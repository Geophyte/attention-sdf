{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.983333333333333,
  "eval_steps": 500,
  "global_step": 239500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003333333333333333,
      "grad_norm": 1.1560782194137573,
      "learning_rate": 4.999791666666667e-05,
      "loss": 0.0205,
      "step": 10
    },
    {
      "epoch": 0.0006666666666666666,
      "grad_norm": 0.9420439004898071,
      "learning_rate": 4.999583333333333e-05,
      "loss": 0.0088,
      "step": 20
    },
    {
      "epoch": 0.001,
      "grad_norm": 0.6023547649383545,
      "learning_rate": 4.999375e-05,
      "loss": 0.008,
      "step": 30
    },
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 0.2019413411617279,
      "learning_rate": 4.999166666666667e-05,
      "loss": 0.0055,
      "step": 40
    },
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.4030986726284027,
      "learning_rate": 4.9989583333333337e-05,
      "loss": 0.0082,
      "step": 50
    },
    {
      "epoch": 0.002,
      "grad_norm": 0.26906415820121765,
      "learning_rate": 4.99875e-05,
      "loss": 0.0076,
      "step": 60
    },
    {
      "epoch": 0.0023333333333333335,
      "grad_norm": 0.7335054874420166,
      "learning_rate": 4.998541666666667e-05,
      "loss": 0.0068,
      "step": 70
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.02046429179608822,
      "learning_rate": 4.998333333333334e-05,
      "loss": 0.0064,
      "step": 80
    },
    {
      "epoch": 0.003,
      "grad_norm": 0.6006406545639038,
      "learning_rate": 4.998125e-05,
      "loss": 0.0073,
      "step": 90
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.7326744198799133,
      "learning_rate": 4.997916666666667e-05,
      "loss": 0.0051,
      "step": 100
    },
    {
      "epoch": 0.0036666666666666666,
      "grad_norm": 0.7994527220726013,
      "learning_rate": 4.9977083333333336e-05,
      "loss": 0.0066,
      "step": 110
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.33258292078971863,
      "learning_rate": 4.9975e-05,
      "loss": 0.0054,
      "step": 120
    },
    {
      "epoch": 0.004333333333333333,
      "grad_norm": 0.20041616261005402,
      "learning_rate": 4.997291666666667e-05,
      "loss": 0.0068,
      "step": 130
    },
    {
      "epoch": 0.004666666666666667,
      "grad_norm": 0.864481508731842,
      "learning_rate": 4.997083333333333e-05,
      "loss": 0.0074,
      "step": 140
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.2662200331687927,
      "learning_rate": 4.9968750000000005e-05,
      "loss": 0.0061,
      "step": 150
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.012828740291297436,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.0087,
      "step": 160
    },
    {
      "epoch": 0.005666666666666667,
      "grad_norm": 0.20407672226428986,
      "learning_rate": 4.9964583333333336e-05,
      "loss": 0.0055,
      "step": 170
    },
    {
      "epoch": 0.006,
      "grad_norm": 0.3337706923484802,
      "learning_rate": 4.99625e-05,
      "loss": 0.008,
      "step": 180
    },
    {
      "epoch": 0.006333333333333333,
      "grad_norm": 0.019590308889746666,
      "learning_rate": 4.9960416666666674e-05,
      "loss": 0.0067,
      "step": 190
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.20411814749240875,
      "learning_rate": 4.995833333333333e-05,
      "loss": 0.0076,
      "step": 200
    },
    {
      "epoch": 0.007,
      "grad_norm": 0.20059727132320404,
      "learning_rate": 4.9956250000000005e-05,
      "loss": 0.0059,
      "step": 210
    },
    {
      "epoch": 0.007333333333333333,
      "grad_norm": 0.39908847212791443,
      "learning_rate": 4.995416666666667e-05,
      "loss": 0.0074,
      "step": 220
    },
    {
      "epoch": 0.007666666666666666,
      "grad_norm": 0.06759165227413177,
      "learning_rate": 4.9952083333333336e-05,
      "loss": 0.0067,
      "step": 230
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.07101558148860931,
      "learning_rate": 4.995e-05,
      "loss": 0.0058,
      "step": 240
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.02266889624297619,
      "learning_rate": 4.994791666666667e-05,
      "loss": 0.0063,
      "step": 250
    },
    {
      "epoch": 0.008666666666666666,
      "grad_norm": 0.3322678804397583,
      "learning_rate": 4.994583333333334e-05,
      "loss": 0.0085,
      "step": 260
    },
    {
      "epoch": 0.009,
      "grad_norm": 0.06699078530073166,
      "learning_rate": 4.994375e-05,
      "loss": 0.0064,
      "step": 270
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 0.3322764039039612,
      "learning_rate": 4.994166666666667e-05,
      "loss": 0.0071,
      "step": 280
    },
    {
      "epoch": 0.009666666666666667,
      "grad_norm": 0.26560649275779724,
      "learning_rate": 4.9939583333333335e-05,
      "loss": 0.0073,
      "step": 290
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.4639890193939209,
      "learning_rate": 4.99375e-05,
      "loss": 0.0057,
      "step": 300
    },
    {
      "epoch": 0.010333333333333333,
      "grad_norm": 0.33165624737739563,
      "learning_rate": 4.9935416666666666e-05,
      "loss": 0.0075,
      "step": 310
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.3313175141811371,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0043,
      "step": 320
    },
    {
      "epoch": 0.011,
      "grad_norm": 0.13575170934200287,
      "learning_rate": 4.9931250000000004e-05,
      "loss": 0.0076,
      "step": 330
    },
    {
      "epoch": 0.011333333333333334,
      "grad_norm": 0.015527193434536457,
      "learning_rate": 4.992916666666667e-05,
      "loss": 0.006,
      "step": 340
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 0.46247559785842896,
      "learning_rate": 4.9927083333333335e-05,
      "loss": 0.0048,
      "step": 350
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.39745426177978516,
      "learning_rate": 4.992500000000001e-05,
      "loss": 0.0098,
      "step": 360
    },
    {
      "epoch": 0.012333333333333333,
      "grad_norm": 0.5949789881706238,
      "learning_rate": 4.9922916666666666e-05,
      "loss": 0.0062,
      "step": 370
    },
    {
      "epoch": 0.012666666666666666,
      "grad_norm": 0.5292698740959167,
      "learning_rate": 4.992083333333333e-05,
      "loss": 0.0061,
      "step": 380
    },
    {
      "epoch": 0.013,
      "grad_norm": 0.013478736393153667,
      "learning_rate": 4.9918750000000004e-05,
      "loss": 0.0054,
      "step": 390
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.3961177468299866,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.0053,
      "step": 400
    },
    {
      "epoch": 0.013666666666666667,
      "grad_norm": 0.5305566191673279,
      "learning_rate": 4.9914583333333335e-05,
      "loss": 0.007,
      "step": 410
    },
    {
      "epoch": 0.014,
      "grad_norm": 0.20122292637825012,
      "learning_rate": 4.99125e-05,
      "loss": 0.007,
      "step": 420
    },
    {
      "epoch": 0.014333333333333333,
      "grad_norm": 0.46275565028190613,
      "learning_rate": 4.991041666666667e-05,
      "loss": 0.0073,
      "step": 430
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 0.4619935154914856,
      "learning_rate": 4.990833333333334e-05,
      "loss": 0.0052,
      "step": 440
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.19981953501701355,
      "learning_rate": 4.9906250000000004e-05,
      "loss": 0.0055,
      "step": 450
    },
    {
      "epoch": 0.015333333333333332,
      "grad_norm": 0.5269908308982849,
      "learning_rate": 4.990416666666667e-05,
      "loss": 0.0074,
      "step": 460
    },
    {
      "epoch": 0.015666666666666666,
      "grad_norm": 0.5929452180862427,
      "learning_rate": 4.990208333333334e-05,
      "loss": 0.0083,
      "step": 470
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.6568933129310608,
      "learning_rate": 4.99e-05,
      "loss": 0.0069,
      "step": 480
    },
    {
      "epoch": 0.01633333333333333,
      "grad_norm": 0.5923436284065247,
      "learning_rate": 4.9897916666666665e-05,
      "loss": 0.0048,
      "step": 490
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.4617859125137329,
      "learning_rate": 4.989583333333334e-05,
      "loss": 0.0072,
      "step": 500
    },
    {
      "epoch": 0.017,
      "grad_norm": 0.3936041593551636,
      "learning_rate": 4.989375e-05,
      "loss": 0.0058,
      "step": 510
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 0.19797441363334656,
      "learning_rate": 4.989166666666667e-05,
      "loss": 0.0082,
      "step": 520
    },
    {
      "epoch": 0.017666666666666667,
      "grad_norm": 0.8563962578773499,
      "learning_rate": 4.9889583333333334e-05,
      "loss": 0.0054,
      "step": 530
    },
    {
      "epoch": 0.018,
      "grad_norm": 0.7872650623321533,
      "learning_rate": 4.9887500000000006e-05,
      "loss": 0.0056,
      "step": 540
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.5252982378005981,
      "learning_rate": 4.9885416666666665e-05,
      "loss": 0.0081,
      "step": 550
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.9844237565994263,
      "learning_rate": 4.988333333333334e-05,
      "loss": 0.0068,
      "step": 560
    },
    {
      "epoch": 0.019,
      "grad_norm": 0.035205595195293427,
      "learning_rate": 4.988125e-05,
      "loss": 0.0071,
      "step": 570
    },
    {
      "epoch": 0.019333333333333334,
      "grad_norm": 0.1976548284292221,
      "learning_rate": 4.987916666666667e-05,
      "loss": 0.0065,
      "step": 580
    },
    {
      "epoch": 0.019666666666666666,
      "grad_norm": 0.5906030535697937,
      "learning_rate": 4.9877083333333334e-05,
      "loss": 0.0061,
      "step": 590
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.6561742424964905,
      "learning_rate": 4.9875000000000006e-05,
      "loss": 0.007,
      "step": 600
    },
    {
      "epoch": 0.02033333333333333,
      "grad_norm": 0.07111760228872299,
      "learning_rate": 4.987291666666667e-05,
      "loss": 0.0055,
      "step": 610
    },
    {
      "epoch": 0.020666666666666667,
      "grad_norm": 0.06835632026195526,
      "learning_rate": 4.987083333333333e-05,
      "loss": 0.0067,
      "step": 620
    },
    {
      "epoch": 0.021,
      "grad_norm": 0.327507346868515,
      "learning_rate": 4.986875e-05,
      "loss": 0.0055,
      "step": 630
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.06693465262651443,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0045,
      "step": 640
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.588023841381073,
      "learning_rate": 4.9864583333333334e-05,
      "loss": 0.0068,
      "step": 650
    },
    {
      "epoch": 0.022,
      "grad_norm": 0.26239013671875,
      "learning_rate": 4.98625e-05,
      "loss": 0.0067,
      "step": 660
    },
    {
      "epoch": 0.022333333333333334,
      "grad_norm": 0.19663524627685547,
      "learning_rate": 4.986041666666667e-05,
      "loss": 0.0052,
      "step": 670
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 0.7852099537849426,
      "learning_rate": 4.985833333333334e-05,
      "loss": 0.0055,
      "step": 680
    },
    {
      "epoch": 0.023,
      "grad_norm": 0.4582890570163727,
      "learning_rate": 4.985625e-05,
      "loss": 0.0059,
      "step": 690
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.39266741275787354,
      "learning_rate": 4.985416666666667e-05,
      "loss": 0.0065,
      "step": 700
    },
    {
      "epoch": 0.023666666666666666,
      "grad_norm": 0.19744490087032318,
      "learning_rate": 4.985208333333334e-05,
      "loss": 0.0058,
      "step": 710
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.26424306631088257,
      "learning_rate": 4.9850000000000006e-05,
      "loss": 0.0056,
      "step": 720
    },
    {
      "epoch": 0.024333333333333332,
      "grad_norm": 0.0684419646859169,
      "learning_rate": 4.9847916666666664e-05,
      "loss": 0.0073,
      "step": 730
    },
    {
      "epoch": 0.024666666666666667,
      "grad_norm": 0.2618856132030487,
      "learning_rate": 4.9845833333333336e-05,
      "loss": 0.0056,
      "step": 740
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.5221850872039795,
      "learning_rate": 4.984375e-05,
      "loss": 0.0073,
      "step": 750
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 0.5231893658638,
      "learning_rate": 4.984166666666667e-05,
      "loss": 0.0074,
      "step": 760
    },
    {
      "epoch": 0.025666666666666667,
      "grad_norm": 0.2619207799434662,
      "learning_rate": 4.983958333333333e-05,
      "loss": 0.0064,
      "step": 770
    },
    {
      "epoch": 0.026,
      "grad_norm": 0.7170276045799255,
      "learning_rate": 4.9837500000000005e-05,
      "loss": 0.0069,
      "step": 780
    },
    {
      "epoch": 0.026333333333333334,
      "grad_norm": 0.3261045217514038,
      "learning_rate": 4.983541666666667e-05,
      "loss": 0.0054,
      "step": 790
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.5221652388572693,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0066,
      "step": 800
    },
    {
      "epoch": 0.027,
      "grad_norm": 0.26066237688064575,
      "learning_rate": 4.983125e-05,
      "loss": 0.0072,
      "step": 810
    },
    {
      "epoch": 0.027333333333333334,
      "grad_norm": 0.3911978006362915,
      "learning_rate": 4.9829166666666674e-05,
      "loss": 0.0052,
      "step": 820
    },
    {
      "epoch": 0.027666666666666666,
      "grad_norm": 0.1965913474559784,
      "learning_rate": 4.982708333333333e-05,
      "loss": 0.0077,
      "step": 830
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.06983150541782379,
      "learning_rate": 4.9825000000000005e-05,
      "loss": 0.0054,
      "step": 840
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.5857415795326233,
      "learning_rate": 4.982291666666667e-05,
      "loss": 0.0063,
      "step": 850
    },
    {
      "epoch": 0.028666666666666667,
      "grad_norm": 0.6520752906799316,
      "learning_rate": 4.9820833333333336e-05,
      "loss": 0.0037,
      "step": 860
    },
    {
      "epoch": 0.029,
      "grad_norm": 0.13258077204227448,
      "learning_rate": 4.981875e-05,
      "loss": 0.0065,
      "step": 870
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.13192257285118103,
      "learning_rate": 4.981666666666667e-05,
      "loss": 0.0079,
      "step": 880
    },
    {
      "epoch": 0.029666666666666668,
      "grad_norm": 0.03416091576218605,
      "learning_rate": 4.981458333333334e-05,
      "loss": 0.0077,
      "step": 890
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.5222176313400269,
      "learning_rate": 4.98125e-05,
      "loss": 0.0063,
      "step": 900
    },
    {
      "epoch": 0.030333333333333334,
      "grad_norm": 0.5231388211250305,
      "learning_rate": 4.981041666666667e-05,
      "loss": 0.0066,
      "step": 910
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 0.5867437124252319,
      "learning_rate": 4.9808333333333336e-05,
      "loss": 0.0048,
      "step": 920
    },
    {
      "epoch": 0.031,
      "grad_norm": 0.45638731122016907,
      "learning_rate": 4.980625e-05,
      "loss": 0.0074,
      "step": 930
    },
    {
      "epoch": 0.03133333333333333,
      "grad_norm": 0.13129685819149017,
      "learning_rate": 4.9804166666666667e-05,
      "loss": 0.0065,
      "step": 940
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.19624538719654083,
      "learning_rate": 4.980208333333334e-05,
      "loss": 0.0061,
      "step": 950
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.45596277713775635,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0081,
      "step": 960
    },
    {
      "epoch": 0.03233333333333333,
      "grad_norm": 0.8464704155921936,
      "learning_rate": 4.979791666666666e-05,
      "loss": 0.0056,
      "step": 970
    },
    {
      "epoch": 0.03266666666666666,
      "grad_norm": 0.6509230732917786,
      "learning_rate": 4.9795833333333335e-05,
      "loss": 0.0067,
      "step": 980
    },
    {
      "epoch": 0.033,
      "grad_norm": 0.1986609250307083,
      "learning_rate": 4.979375e-05,
      "loss": 0.0073,
      "step": 990
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.4556359052658081,
      "learning_rate": 4.979166666666667e-05,
      "loss": 0.0065,
      "step": 1000
    },
    {
      "epoch": 0.033666666666666664,
      "grad_norm": 0.2611868977546692,
      "learning_rate": 4.978958333333333e-05,
      "loss": 0.0051,
      "step": 1010
    },
    {
      "epoch": 0.034,
      "grad_norm": 0.20064610242843628,
      "learning_rate": 4.9787500000000004e-05,
      "loss": 0.0075,
      "step": 1020
    },
    {
      "epoch": 0.034333333333333334,
      "grad_norm": 0.458599716424942,
      "learning_rate": 4.978541666666667e-05,
      "loss": 0.0056,
      "step": 1030
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.38909465074539185,
      "learning_rate": 4.9783333333333335e-05,
      "loss": 0.0056,
      "step": 1040
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.3271828591823578,
      "learning_rate": 4.978125e-05,
      "loss": 0.0037,
      "step": 1050
    },
    {
      "epoch": 0.035333333333333335,
      "grad_norm": 0.8448014259338379,
      "learning_rate": 4.977916666666667e-05,
      "loss": 0.0047,
      "step": 1060
    },
    {
      "epoch": 0.035666666666666666,
      "grad_norm": 0.08516541123390198,
      "learning_rate": 4.977708333333334e-05,
      "loss": 0.0067,
      "step": 1070
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.013719220645725727,
      "learning_rate": 4.9775000000000004e-05,
      "loss": 0.0062,
      "step": 1080
    },
    {
      "epoch": 0.036333333333333336,
      "grad_norm": 0.7155659198760986,
      "learning_rate": 4.977291666666667e-05,
      "loss": 0.009,
      "step": 1090
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.5847353935241699,
      "learning_rate": 4.9770833333333335e-05,
      "loss": 0.0073,
      "step": 1100
    },
    {
      "epoch": 0.037,
      "grad_norm": 0.06614751368761063,
      "learning_rate": 4.976875e-05,
      "loss": 0.0068,
      "step": 1110
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.5194298028945923,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0055,
      "step": 1120
    },
    {
      "epoch": 0.03766666666666667,
      "grad_norm": 0.5844583511352539,
      "learning_rate": 4.976458333333334e-05,
      "loss": 0.0072,
      "step": 1130
    },
    {
      "epoch": 0.038,
      "grad_norm": 0.38861095905303955,
      "learning_rate": 4.9762500000000003e-05,
      "loss": 0.0059,
      "step": 1140
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.8402889966964722,
      "learning_rate": 4.976041666666667e-05,
      "loss": 0.0058,
      "step": 1150
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 0.027911432087421417,
      "learning_rate": 4.9758333333333334e-05,
      "loss": 0.0076,
      "step": 1160
    },
    {
      "epoch": 0.039,
      "grad_norm": 0.39055976271629333,
      "learning_rate": 4.975625000000001e-05,
      "loss": 0.0062,
      "step": 1170
    },
    {
      "epoch": 0.03933333333333333,
      "grad_norm": 0.5193172097206116,
      "learning_rate": 4.9754166666666665e-05,
      "loss": 0.006,
      "step": 1180
    },
    {
      "epoch": 0.03966666666666667,
      "grad_norm": 0.06907301396131516,
      "learning_rate": 4.975208333333334e-05,
      "loss": 0.0059,
      "step": 1190
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.1398271769285202,
      "learning_rate": 4.975e-05,
      "loss": 0.005,
      "step": 1200
    },
    {
      "epoch": 0.04033333333333333,
      "grad_norm": 0.26002269983291626,
      "learning_rate": 4.974791666666667e-05,
      "loss": 0.0047,
      "step": 1210
    },
    {
      "epoch": 0.04066666666666666,
      "grad_norm": 0.522407591342926,
      "learning_rate": 4.9745833333333334e-05,
      "loss": 0.0056,
      "step": 1220
    },
    {
      "epoch": 0.041,
      "grad_norm": 0.4531668424606323,
      "learning_rate": 4.974375e-05,
      "loss": 0.0073,
      "step": 1230
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 0.07259860634803772,
      "learning_rate": 4.974166666666667e-05,
      "loss": 0.0062,
      "step": 1240
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.2662637233734131,
      "learning_rate": 4.973958333333333e-05,
      "loss": 0.0062,
      "step": 1250
    },
    {
      "epoch": 0.042,
      "grad_norm": 0.3238826394081116,
      "learning_rate": 4.97375e-05,
      "loss": 0.0045,
      "step": 1260
    },
    {
      "epoch": 0.042333333333333334,
      "grad_norm": 0.6490402817726135,
      "learning_rate": 4.973541666666667e-05,
      "loss": 0.0045,
      "step": 1270
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 1.0356569290161133,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0048,
      "step": 1280
    },
    {
      "epoch": 0.043,
      "grad_norm": 0.075617715716362,
      "learning_rate": 4.973125e-05,
      "loss": 0.0058,
      "step": 1290
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.13985513150691986,
      "learning_rate": 4.972916666666667e-05,
      "loss": 0.0049,
      "step": 1300
    },
    {
      "epoch": 0.043666666666666666,
      "grad_norm": 0.3271350562572479,
      "learning_rate": 4.972708333333334e-05,
      "loss": 0.0061,
      "step": 1310
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.13328143954277039,
      "learning_rate": 4.9725e-05,
      "loss": 0.0049,
      "step": 1320
    },
    {
      "epoch": 0.044333333333333336,
      "grad_norm": 0.07577519863843918,
      "learning_rate": 4.972291666666667e-05,
      "loss": 0.005,
      "step": 1330
    },
    {
      "epoch": 0.04466666666666667,
      "grad_norm": 0.33228981494903564,
      "learning_rate": 4.9720833333333333e-05,
      "loss": 0.0053,
      "step": 1340
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.8366556167602539,
      "learning_rate": 4.9718750000000006e-05,
      "loss": 0.0064,
      "step": 1350
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.1346662938594818,
      "learning_rate": 4.9716666666666664e-05,
      "loss": 0.0059,
      "step": 1360
    },
    {
      "epoch": 0.04566666666666667,
      "grad_norm": 0.07182318717241287,
      "learning_rate": 4.971458333333334e-05,
      "loss": 0.0044,
      "step": 1370
    },
    {
      "epoch": 0.046,
      "grad_norm": 0.19837211072444916,
      "learning_rate": 4.97125e-05,
      "loss": 0.0039,
      "step": 1380
    },
    {
      "epoch": 0.04633333333333333,
      "grad_norm": 0.1340838223695755,
      "learning_rate": 4.971041666666667e-05,
      "loss": 0.0054,
      "step": 1390
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.3860113024711609,
      "learning_rate": 4.970833333333333e-05,
      "loss": 0.006,
      "step": 1400
    },
    {
      "epoch": 0.047,
      "grad_norm": 0.15820243954658508,
      "learning_rate": 4.9706250000000005e-05,
      "loss": 0.0052,
      "step": 1410
    },
    {
      "epoch": 0.04733333333333333,
      "grad_norm": 0.5489374995231628,
      "learning_rate": 4.970416666666667e-05,
      "loss": 0.0053,
      "step": 1420
    },
    {
      "epoch": 0.04766666666666667,
      "grad_norm": 0.6450271606445312,
      "learning_rate": 4.9702083333333336e-05,
      "loss": 0.0052,
      "step": 1430
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.4502617120742798,
      "learning_rate": 4.97e-05,
      "loss": 0.0052,
      "step": 1440
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.11062382906675339,
      "learning_rate": 4.9697916666666674e-05,
      "loss": 0.0043,
      "step": 1450
    },
    {
      "epoch": 0.048666666666666664,
      "grad_norm": 0.291288822889328,
      "learning_rate": 4.969583333333333e-05,
      "loss": 0.0053,
      "step": 1460
    },
    {
      "epoch": 0.049,
      "grad_norm": 0.6717855930328369,
      "learning_rate": 4.969375e-05,
      "loss": 0.005,
      "step": 1470
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 0.15003736317157745,
      "learning_rate": 4.969166666666667e-05,
      "loss": 0.0053,
      "step": 1480
    },
    {
      "epoch": 0.049666666666666665,
      "grad_norm": 0.423093318939209,
      "learning_rate": 4.9689583333333336e-05,
      "loss": 0.0045,
      "step": 1490
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.23361484706401825,
      "learning_rate": 4.96875e-05,
      "loss": 0.0036,
      "step": 1500
    },
    {
      "epoch": 0.050333333333333334,
      "grad_norm": 0.08618046343326569,
      "learning_rate": 4.968541666666667e-05,
      "loss": 0.0036,
      "step": 1510
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.1954684555530548,
      "learning_rate": 4.968333333333334e-05,
      "loss": 0.0046,
      "step": 1520
    },
    {
      "epoch": 0.051,
      "grad_norm": 0.13464301824569702,
      "learning_rate": 4.968125e-05,
      "loss": 0.0039,
      "step": 1530
    },
    {
      "epoch": 0.051333333333333335,
      "grad_norm": 0.4888894557952881,
      "learning_rate": 4.967916666666667e-05,
      "loss": 0.0053,
      "step": 1540
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.9040284156799316,
      "learning_rate": 4.9677083333333336e-05,
      "loss": 0.0051,
      "step": 1550
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.5472292304039001,
      "learning_rate": 4.967500000000001e-05,
      "loss": 0.005,
      "step": 1560
    },
    {
      "epoch": 0.052333333333333336,
      "grad_norm": 0.11697584390640259,
      "learning_rate": 4.967291666666667e-05,
      "loss": 0.0048,
      "step": 1570
    },
    {
      "epoch": 0.05266666666666667,
      "grad_norm": 0.1373615562915802,
      "learning_rate": 4.967083333333333e-05,
      "loss": 0.0057,
      "step": 1580
    },
    {
      "epoch": 0.053,
      "grad_norm": 0.4172351360321045,
      "learning_rate": 4.9668750000000005e-05,
      "loss": 0.0043,
      "step": 1590
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.20362579822540283,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0054,
      "step": 1600
    },
    {
      "epoch": 0.05366666666666667,
      "grad_norm": 0.14036163687705994,
      "learning_rate": 4.9664583333333335e-05,
      "loss": 0.0037,
      "step": 1610
    },
    {
      "epoch": 0.054,
      "grad_norm": 0.11526485532522202,
      "learning_rate": 4.96625e-05,
      "loss": 0.0052,
      "step": 1620
    },
    {
      "epoch": 0.05433333333333333,
      "grad_norm": 0.32095956802368164,
      "learning_rate": 4.966041666666667e-05,
      "loss": 0.0037,
      "step": 1630
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 0.5830098986625671,
      "learning_rate": 4.965833333333333e-05,
      "loss": 0.0057,
      "step": 1640
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.3281971216201782,
      "learning_rate": 4.9656250000000004e-05,
      "loss": 0.0045,
      "step": 1650
    },
    {
      "epoch": 0.05533333333333333,
      "grad_norm": 0.11900261044502258,
      "learning_rate": 4.965416666666667e-05,
      "loss": 0.004,
      "step": 1660
    },
    {
      "epoch": 0.05566666666666667,
      "grad_norm": 0.6094550490379333,
      "learning_rate": 4.9652083333333335e-05,
      "loss": 0.0044,
      "step": 1670
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.6461173892021179,
      "learning_rate": 4.965e-05,
      "loss": 0.0055,
      "step": 1680
    },
    {
      "epoch": 0.05633333333333333,
      "grad_norm": 0.19362176954746246,
      "learning_rate": 4.964791666666667e-05,
      "loss": 0.0061,
      "step": 1690
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.22991956770420074,
      "learning_rate": 4.964583333333334e-05,
      "loss": 0.0042,
      "step": 1700
    },
    {
      "epoch": 0.057,
      "grad_norm": 0.20678889751434326,
      "learning_rate": 4.964375e-05,
      "loss": 0.0039,
      "step": 1710
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 0.385103315114975,
      "learning_rate": 4.964166666666667e-05,
      "loss": 0.0053,
      "step": 1720
    },
    {
      "epoch": 0.057666666666666665,
      "grad_norm": 0.09936319291591644,
      "learning_rate": 4.9639583333333335e-05,
      "loss": 0.0058,
      "step": 1730
    },
    {
      "epoch": 0.058,
      "grad_norm": 0.5210345387458801,
      "learning_rate": 4.96375e-05,
      "loss": 0.0037,
      "step": 1740
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.5163517594337463,
      "learning_rate": 4.9635416666666666e-05,
      "loss": 0.0064,
      "step": 1750
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.22571086883544922,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0062,
      "step": 1760
    },
    {
      "epoch": 0.059,
      "grad_norm": 0.5517333745956421,
      "learning_rate": 4.9631250000000004e-05,
      "loss": 0.0059,
      "step": 1770
    },
    {
      "epoch": 0.059333333333333335,
      "grad_norm": 0.17314164340496063,
      "learning_rate": 4.962916666666667e-05,
      "loss": 0.0052,
      "step": 1780
    },
    {
      "epoch": 0.059666666666666666,
      "grad_norm": 0.3353174030780792,
      "learning_rate": 4.9627083333333335e-05,
      "loss": 0.0029,
      "step": 1790
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.38493359088897705,
      "learning_rate": 4.962500000000001e-05,
      "loss": 0.0046,
      "step": 1800
    },
    {
      "epoch": 0.060333333333333336,
      "grad_norm": 0.06598777323961258,
      "learning_rate": 4.962291666666667e-05,
      "loss": 0.0047,
      "step": 1810
    },
    {
      "epoch": 0.06066666666666667,
      "grad_norm": 0.13745039701461792,
      "learning_rate": 4.962083333333333e-05,
      "loss": 0.0039,
      "step": 1820
    },
    {
      "epoch": 0.061,
      "grad_norm": 0.06780484318733215,
      "learning_rate": 4.961875e-05,
      "loss": 0.0046,
      "step": 1830
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.45849621295928955,
      "learning_rate": 4.961666666666667e-05,
      "loss": 0.0044,
      "step": 1840
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.2481345534324646,
      "learning_rate": 4.9614583333333334e-05,
      "loss": 0.0043,
      "step": 1850
    },
    {
      "epoch": 0.062,
      "grad_norm": 0.3913742005825043,
      "learning_rate": 4.96125e-05,
      "loss": 0.006,
      "step": 1860
    },
    {
      "epoch": 0.06233333333333333,
      "grad_norm": 0.297727108001709,
      "learning_rate": 4.961041666666667e-05,
      "loss": 0.006,
      "step": 1870
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 0.193990558385849,
      "learning_rate": 4.960833333333334e-05,
      "loss": 0.0044,
      "step": 1880
    },
    {
      "epoch": 0.063,
      "grad_norm": 0.21532703936100006,
      "learning_rate": 4.960625e-05,
      "loss": 0.0046,
      "step": 1890
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.06665056198835373,
      "learning_rate": 4.960416666666667e-05,
      "loss": 0.0037,
      "step": 1900
    },
    {
      "epoch": 0.06366666666666666,
      "grad_norm": 0.4831749498844147,
      "learning_rate": 4.960208333333334e-05,
      "loss": 0.0053,
      "step": 1910
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.1796010285615921,
      "learning_rate": 4.96e-05,
      "loss": 0.0042,
      "step": 1920
    },
    {
      "epoch": 0.06433333333333334,
      "grad_norm": 0.2252778559923172,
      "learning_rate": 4.959791666666667e-05,
      "loss": 0.0029,
      "step": 1930
    },
    {
      "epoch": 0.06466666666666666,
      "grad_norm": 0.23372359573841095,
      "learning_rate": 4.959583333333334e-05,
      "loss": 0.0039,
      "step": 1940
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.10314218699932098,
      "learning_rate": 4.959375e-05,
      "loss": 0.0056,
      "step": 1950
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 0.8332918882369995,
      "learning_rate": 4.959166666666667e-05,
      "loss": 0.005,
      "step": 1960
    },
    {
      "epoch": 0.06566666666666666,
      "grad_norm": 0.34528297185897827,
      "learning_rate": 4.9589583333333334e-05,
      "loss": 0.0057,
      "step": 1970
    },
    {
      "epoch": 0.066,
      "grad_norm": 0.03149644657969475,
      "learning_rate": 4.9587500000000006e-05,
      "loss": 0.0053,
      "step": 1980
    },
    {
      "epoch": 0.06633333333333333,
      "grad_norm": 0.2717788815498352,
      "learning_rate": 4.9585416666666665e-05,
      "loss": 0.0053,
      "step": 1990
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.1404891312122345,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.005,
      "step": 2000
    },
    {
      "epoch": 0.067,
      "grad_norm": 0.4212668240070343,
      "learning_rate": 4.958125e-05,
      "loss": 0.0049,
      "step": 2010
    },
    {
      "epoch": 0.06733333333333333,
      "grad_norm": 0.1293582171201706,
      "learning_rate": 4.957916666666667e-05,
      "loss": 0.0037,
      "step": 2020
    },
    {
      "epoch": 0.06766666666666667,
      "grad_norm": 0.3748474419116974,
      "learning_rate": 4.957708333333333e-05,
      "loss": 0.005,
      "step": 2030
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.8000255227088928,
      "learning_rate": 4.9575000000000006e-05,
      "loss": 0.0041,
      "step": 2040
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.48000001907348633,
      "learning_rate": 4.957291666666667e-05,
      "loss": 0.005,
      "step": 2050
    },
    {
      "epoch": 0.06866666666666667,
      "grad_norm": 0.4480859637260437,
      "learning_rate": 4.957083333333333e-05,
      "loss": 0.0056,
      "step": 2060
    },
    {
      "epoch": 0.069,
      "grad_norm": 0.3533848226070404,
      "learning_rate": 4.956875e-05,
      "loss": 0.005,
      "step": 2070
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.1923290342092514,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.0053,
      "step": 2080
    },
    {
      "epoch": 0.06966666666666667,
      "grad_norm": 0.3849145472049713,
      "learning_rate": 4.956458333333334e-05,
      "loss": 0.0046,
      "step": 2090
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.3310149312019348,
      "learning_rate": 4.95625e-05,
      "loss": 0.0035,
      "step": 2100
    },
    {
      "epoch": 0.07033333333333333,
      "grad_norm": 0.5502054691314697,
      "learning_rate": 4.956041666666667e-05,
      "loss": 0.0042,
      "step": 2110
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 0.34363576769828796,
      "learning_rate": 4.9558333333333336e-05,
      "loss": 0.0042,
      "step": 2120
    },
    {
      "epoch": 0.071,
      "grad_norm": 0.0229243952780962,
      "learning_rate": 4.955625e-05,
      "loss": 0.0049,
      "step": 2130
    },
    {
      "epoch": 0.07133333333333333,
      "grad_norm": 0.6388123631477356,
      "learning_rate": 4.955416666666667e-05,
      "loss": 0.0044,
      "step": 2140
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.11937497556209564,
      "learning_rate": 4.955208333333334e-05,
      "loss": 0.005,
      "step": 2150
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.19530639052391052,
      "learning_rate": 4.9550000000000005e-05,
      "loss": 0.0054,
      "step": 2160
    },
    {
      "epoch": 0.07233333333333333,
      "grad_norm": 0.510834276676178,
      "learning_rate": 4.954791666666667e-05,
      "loss": 0.0054,
      "step": 2170
    },
    {
      "epoch": 0.07266666666666667,
      "grad_norm": 0.19248689711093903,
      "learning_rate": 4.9545833333333336e-05,
      "loss": 0.0036,
      "step": 2180
    },
    {
      "epoch": 0.073,
      "grad_norm": 0.3205944001674652,
      "learning_rate": 4.954375e-05,
      "loss": 0.0055,
      "step": 2190
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.08823877573013306,
      "learning_rate": 4.954166666666667e-05,
      "loss": 0.0036,
      "step": 2200
    },
    {
      "epoch": 0.07366666666666667,
      "grad_norm": 0.7023934125900269,
      "learning_rate": 4.953958333333333e-05,
      "loss": 0.0053,
      "step": 2210
    },
    {
      "epoch": 0.074,
      "grad_norm": 0.5150061249732971,
      "learning_rate": 4.9537500000000005e-05,
      "loss": 0.0056,
      "step": 2220
    },
    {
      "epoch": 0.07433333333333333,
      "grad_norm": 0.06882192194461823,
      "learning_rate": 4.953541666666667e-05,
      "loss": 0.005,
      "step": 2230
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.09715693444013596,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0033,
      "step": 2240
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.3191872537136078,
      "learning_rate": 4.953125e-05,
      "loss": 0.003,
      "step": 2250
    },
    {
      "epoch": 0.07533333333333334,
      "grad_norm": 0.13651348650455475,
      "learning_rate": 4.9529166666666673e-05,
      "loss": 0.0053,
      "step": 2260
    },
    {
      "epoch": 0.07566666666666666,
      "grad_norm": 0.2804230749607086,
      "learning_rate": 4.952708333333333e-05,
      "loss": 0.0062,
      "step": 2270
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.2576817274093628,
      "learning_rate": 4.9525000000000004e-05,
      "loss": 0.0035,
      "step": 2280
    },
    {
      "epoch": 0.07633333333333334,
      "grad_norm": 0.4592675566673279,
      "learning_rate": 4.952291666666667e-05,
      "loss": 0.0046,
      "step": 2290
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.7339469194412231,
      "learning_rate": 4.9520833333333335e-05,
      "loss": 0.0044,
      "step": 2300
    },
    {
      "epoch": 0.077,
      "grad_norm": 0.316115140914917,
      "learning_rate": 4.951875e-05,
      "loss": 0.0048,
      "step": 2310
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.1587202101945877,
      "learning_rate": 4.9516666666666666e-05,
      "loss": 0.0047,
      "step": 2320
    },
    {
      "epoch": 0.07766666666666666,
      "grad_norm": 0.09593091160058975,
      "learning_rate": 4.951458333333334e-05,
      "loss": 0.003,
      "step": 2330
    },
    {
      "epoch": 0.078,
      "grad_norm": 0.7641980051994324,
      "learning_rate": 4.95125e-05,
      "loss": 0.0045,
      "step": 2340
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.13955314457416534,
      "learning_rate": 4.951041666666667e-05,
      "loss": 0.0034,
      "step": 2350
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 0.16141445934772491,
      "learning_rate": 4.9508333333333335e-05,
      "loss": 0.0043,
      "step": 2360
    },
    {
      "epoch": 0.079,
      "grad_norm": 0.7318776249885559,
      "learning_rate": 4.950625000000001e-05,
      "loss": 0.0035,
      "step": 2370
    },
    {
      "epoch": 0.07933333333333334,
      "grad_norm": 0.6355565190315247,
      "learning_rate": 4.9504166666666666e-05,
      "loss": 0.0036,
      "step": 2380
    },
    {
      "epoch": 0.07966666666666666,
      "grad_norm": 0.07384679466485977,
      "learning_rate": 4.950208333333334e-05,
      "loss": 0.004,
      "step": 2390
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.6080827713012695,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.0042,
      "step": 2400
    },
    {
      "epoch": 0.08033333333333334,
      "grad_norm": 0.12463077902793884,
      "learning_rate": 4.949791666666667e-05,
      "loss": 0.0041,
      "step": 2410
    },
    {
      "epoch": 0.08066666666666666,
      "grad_norm": 0.10883362591266632,
      "learning_rate": 4.9495833333333335e-05,
      "loss": 0.0052,
      "step": 2420
    },
    {
      "epoch": 0.081,
      "grad_norm": 0.4773370027542114,
      "learning_rate": 4.949375e-05,
      "loss": 0.0032,
      "step": 2430
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 0.22266468405723572,
      "learning_rate": 4.949166666666667e-05,
      "loss": 0.0048,
      "step": 2440
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.07042600959539413,
      "learning_rate": 4.948958333333333e-05,
      "loss": 0.0038,
      "step": 2450
    },
    {
      "epoch": 0.082,
      "grad_norm": 0.34965866804122925,
      "learning_rate": 4.9487500000000003e-05,
      "loss": 0.0045,
      "step": 2460
    },
    {
      "epoch": 0.08233333333333333,
      "grad_norm": 0.6860373020172119,
      "learning_rate": 4.948541666666667e-05,
      "loss": 0.005,
      "step": 2470
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.3424745798110962,
      "learning_rate": 4.9483333333333334e-05,
      "loss": 0.0046,
      "step": 2480
    },
    {
      "epoch": 0.083,
      "grad_norm": 0.16080546379089355,
      "learning_rate": 4.948125e-05,
      "loss": 0.0037,
      "step": 2490
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.1291804015636444,
      "learning_rate": 4.947916666666667e-05,
      "loss": 0.0039,
      "step": 2500
    },
    {
      "epoch": 0.08366666666666667,
      "grad_norm": 0.06632174551486969,
      "learning_rate": 4.947708333333334e-05,
      "loss": 0.0049,
      "step": 2510
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.024814289063215256,
      "learning_rate": 4.9475e-05,
      "loss": 0.0022,
      "step": 2520
    },
    {
      "epoch": 0.08433333333333333,
      "grad_norm": 0.2234770506620407,
      "learning_rate": 4.947291666666667e-05,
      "loss": 0.0025,
      "step": 2530
    },
    {
      "epoch": 0.08466666666666667,
      "grad_norm": 0.15981604158878326,
      "learning_rate": 4.947083333333334e-05,
      "loss": 0.0044,
      "step": 2540
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.13484367728233337,
      "learning_rate": 4.946875e-05,
      "loss": 0.0048,
      "step": 2550
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.26174822449684143,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0043,
      "step": 2560
    },
    {
      "epoch": 0.08566666666666667,
      "grad_norm": 0.16231679916381836,
      "learning_rate": 4.946458333333334e-05,
      "loss": 0.0046,
      "step": 2570
    },
    {
      "epoch": 0.086,
      "grad_norm": 0.12855254113674164,
      "learning_rate": 4.94625e-05,
      "loss": 0.0045,
      "step": 2580
    },
    {
      "epoch": 0.08633333333333333,
      "grad_norm": 0.6128402948379517,
      "learning_rate": 4.946041666666667e-05,
      "loss": 0.0034,
      "step": 2590
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.5076428651809692,
      "learning_rate": 4.9458333333333334e-05,
      "loss": 0.0056,
      "step": 2600
    },
    {
      "epoch": 0.087,
      "grad_norm": 0.5506514310836792,
      "learning_rate": 4.9456250000000006e-05,
      "loss": 0.0039,
      "step": 2610
    },
    {
      "epoch": 0.08733333333333333,
      "grad_norm": 0.4765093922615051,
      "learning_rate": 4.9454166666666665e-05,
      "loss": 0.0055,
      "step": 2620
    },
    {
      "epoch": 0.08766666666666667,
      "grad_norm": 0.444072425365448,
      "learning_rate": 4.945208333333334e-05,
      "loss": 0.003,
      "step": 2630
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.1395445466041565,
      "learning_rate": 4.945e-05,
      "loss": 0.0052,
      "step": 2640
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.1917615681886673,
      "learning_rate": 4.9447916666666675e-05,
      "loss": 0.0035,
      "step": 2650
    },
    {
      "epoch": 0.08866666666666667,
      "grad_norm": 0.3185091018676758,
      "learning_rate": 4.9445833333333334e-05,
      "loss": 0.005,
      "step": 2660
    },
    {
      "epoch": 0.089,
      "grad_norm": 0.3177468180656433,
      "learning_rate": 4.944375e-05,
      "loss": 0.0042,
      "step": 2670
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 0.2279280722141266,
      "learning_rate": 4.944166666666667e-05,
      "loss": 0.0043,
      "step": 2680
    },
    {
      "epoch": 0.08966666666666667,
      "grad_norm": 0.2858153283596039,
      "learning_rate": 4.943958333333334e-05,
      "loss": 0.003,
      "step": 2690
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.47106873989105225,
      "learning_rate": 4.94375e-05,
      "loss": 0.0036,
      "step": 2700
    },
    {
      "epoch": 0.09033333333333333,
      "grad_norm": 0.15484631061553955,
      "learning_rate": 4.943541666666667e-05,
      "loss": 0.0049,
      "step": 2710
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.20474191009998322,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.0031,
      "step": 2720
    },
    {
      "epoch": 0.091,
      "grad_norm": 0.2538955509662628,
      "learning_rate": 4.943125e-05,
      "loss": 0.0041,
      "step": 2730
    },
    {
      "epoch": 0.09133333333333334,
      "grad_norm": 0.2574734091758728,
      "learning_rate": 4.942916666666667e-05,
      "loss": 0.0033,
      "step": 2740
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.25089335441589355,
      "learning_rate": 4.9427083333333336e-05,
      "loss": 0.0046,
      "step": 2750
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.11838020384311676,
      "learning_rate": 4.9425e-05,
      "loss": 0.0042,
      "step": 2760
    },
    {
      "epoch": 0.09233333333333334,
      "grad_norm": 0.18864893913269043,
      "learning_rate": 4.942291666666667e-05,
      "loss": 0.004,
      "step": 2770
    },
    {
      "epoch": 0.09266666666666666,
      "grad_norm": 0.41528090834617615,
      "learning_rate": 4.942083333333334e-05,
      "loss": 0.0046,
      "step": 2780
    },
    {
      "epoch": 0.093,
      "grad_norm": 0.22046418488025665,
      "learning_rate": 4.9418750000000005e-05,
      "loss": 0.0045,
      "step": 2790
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.35535168647766113,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.0048,
      "step": 2800
    },
    {
      "epoch": 0.09366666666666666,
      "grad_norm": 0.137993723154068,
      "learning_rate": 4.9414583333333336e-05,
      "loss": 0.0042,
      "step": 2810
    },
    {
      "epoch": 0.094,
      "grad_norm": 0.5724239945411682,
      "learning_rate": 4.94125e-05,
      "loss": 0.0041,
      "step": 2820
    },
    {
      "epoch": 0.09433333333333334,
      "grad_norm": 0.7557719349861145,
      "learning_rate": 4.941041666666667e-05,
      "loss": 0.0047,
      "step": 2830
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 0.03647584468126297,
      "learning_rate": 4.940833333333333e-05,
      "loss": 0.0042,
      "step": 2840
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.45689353346824646,
      "learning_rate": 4.9406250000000005e-05,
      "loss": 0.0046,
      "step": 2850
    },
    {
      "epoch": 0.09533333333333334,
      "grad_norm": 0.5066913366317749,
      "learning_rate": 4.940416666666667e-05,
      "loss": 0.0045,
      "step": 2860
    },
    {
      "epoch": 0.09566666666666666,
      "grad_norm": 0.5086254477500916,
      "learning_rate": 4.9402083333333336e-05,
      "loss": 0.0048,
      "step": 2870
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.2852989137172699,
      "learning_rate": 4.94e-05,
      "loss": 0.0039,
      "step": 2880
    },
    {
      "epoch": 0.09633333333333334,
      "grad_norm": 0.41213706135749817,
      "learning_rate": 4.9397916666666674e-05,
      "loss": 0.0043,
      "step": 2890
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.3802223801612854,
      "learning_rate": 4.939583333333333e-05,
      "loss": 0.0049,
      "step": 2900
    },
    {
      "epoch": 0.097,
      "grad_norm": 0.3802202641963959,
      "learning_rate": 4.939375e-05,
      "loss": 0.0037,
      "step": 2910
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 0.09776143729686737,
      "learning_rate": 4.939166666666667e-05,
      "loss": 0.0057,
      "step": 2920
    },
    {
      "epoch": 0.09766666666666667,
      "grad_norm": 0.19084177911281586,
      "learning_rate": 4.9389583333333336e-05,
      "loss": 0.0046,
      "step": 2930
    },
    {
      "epoch": 0.098,
      "grad_norm": 0.15909117460250854,
      "learning_rate": 4.93875e-05,
      "loss": 0.004,
      "step": 2940
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.38026008009910583,
      "learning_rate": 4.9385416666666667e-05,
      "loss": 0.0044,
      "step": 2950
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.31647971272468567,
      "learning_rate": 4.938333333333334e-05,
      "loss": 0.0046,
      "step": 2960
    },
    {
      "epoch": 0.099,
      "grad_norm": 0.1280137300491333,
      "learning_rate": 4.9381250000000004e-05,
      "loss": 0.0029,
      "step": 2970
    },
    {
      "epoch": 0.09933333333333333,
      "grad_norm": 0.1932010054588318,
      "learning_rate": 4.937916666666667e-05,
      "loss": 0.0041,
      "step": 2980
    },
    {
      "epoch": 0.09966666666666667,
      "grad_norm": 0.43703970313072205,
      "learning_rate": 4.9377083333333335e-05,
      "loss": 0.0058,
      "step": 2990
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.05083358660340309,
      "learning_rate": 4.937500000000001e-05,
      "loss": 0.0052,
      "step": 3000
    },
    {
      "epoch": 0.10033333333333333,
      "grad_norm": 0.19186259806156158,
      "learning_rate": 4.9372916666666666e-05,
      "loss": 0.0045,
      "step": 3010
    },
    {
      "epoch": 0.10066666666666667,
      "grad_norm": 0.6969831585884094,
      "learning_rate": 4.937083333333334e-05,
      "loss": 0.0039,
      "step": 3020
    },
    {
      "epoch": 0.101,
      "grad_norm": 0.16186875104904175,
      "learning_rate": 4.9368750000000004e-05,
      "loss": 0.003,
      "step": 3030
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.1593826562166214,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.004,
      "step": 3040
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.2848103940486908,
      "learning_rate": 4.9364583333333335e-05,
      "loss": 0.0049,
      "step": 3050
    },
    {
      "epoch": 0.102,
      "grad_norm": 0.6324020028114319,
      "learning_rate": 4.93625e-05,
      "loss": 0.005,
      "step": 3060
    },
    {
      "epoch": 0.10233333333333333,
      "grad_norm": 0.4099213480949402,
      "learning_rate": 4.936041666666667e-05,
      "loss": 0.004,
      "step": 3070
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 0.44175881147384644,
      "learning_rate": 4.935833333333333e-05,
      "loss": 0.0042,
      "step": 3080
    },
    {
      "epoch": 0.103,
      "grad_norm": 0.1285172551870346,
      "learning_rate": 4.9356250000000004e-05,
      "loss": 0.0028,
      "step": 3090
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.4845874607563019,
      "learning_rate": 4.935416666666667e-05,
      "loss": 0.004,
      "step": 3100
    },
    {
      "epoch": 0.10366666666666667,
      "grad_norm": 0.2359638810157776,
      "learning_rate": 4.9352083333333335e-05,
      "loss": 0.0042,
      "step": 3110
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.8872607350349426,
      "learning_rate": 4.935e-05,
      "loss": 0.0035,
      "step": 3120
    },
    {
      "epoch": 0.10433333333333333,
      "grad_norm": 0.16073571145534515,
      "learning_rate": 4.934791666666667e-05,
      "loss": 0.0033,
      "step": 3130
    },
    {
      "epoch": 0.10466666666666667,
      "grad_norm": 0.021504610776901245,
      "learning_rate": 4.934583333333334e-05,
      "loss": 0.0039,
      "step": 3140
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.34752702713012695,
      "learning_rate": 4.9343749999999997e-05,
      "loss": 0.0047,
      "step": 3150
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 0.3468811511993408,
      "learning_rate": 4.934166666666667e-05,
      "loss": 0.0036,
      "step": 3160
    },
    {
      "epoch": 0.10566666666666667,
      "grad_norm": 0.2219468355178833,
      "learning_rate": 4.9339583333333334e-05,
      "loss": 0.0047,
      "step": 3170
    },
    {
      "epoch": 0.106,
      "grad_norm": 0.17648129165172577,
      "learning_rate": 4.93375e-05,
      "loss": 0.0037,
      "step": 3180
    },
    {
      "epoch": 0.10633333333333334,
      "grad_norm": 0.5703324675559998,
      "learning_rate": 4.9335416666666665e-05,
      "loss": 0.0044,
      "step": 3190
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.3796125054359436,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0044,
      "step": 3200
    },
    {
      "epoch": 0.107,
      "grad_norm": 0.448713093996048,
      "learning_rate": 4.933125e-05,
      "loss": 0.0039,
      "step": 3210
    },
    {
      "epoch": 0.10733333333333334,
      "grad_norm": 0.8551533222198486,
      "learning_rate": 4.932916666666667e-05,
      "loss": 0.0054,
      "step": 3220
    },
    {
      "epoch": 0.10766666666666666,
      "grad_norm": 0.8059447407722473,
      "learning_rate": 4.9327083333333334e-05,
      "loss": 0.0043,
      "step": 3230
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.6337237358093262,
      "learning_rate": 4.9325000000000006e-05,
      "loss": 0.0055,
      "step": 3240
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.37993061542510986,
      "learning_rate": 4.932291666666667e-05,
      "loss": 0.0033,
      "step": 3250
    },
    {
      "epoch": 0.10866666666666666,
      "grad_norm": 0.145280659198761,
      "learning_rate": 4.932083333333334e-05,
      "loss": 0.0037,
      "step": 3260
    },
    {
      "epoch": 0.109,
      "grad_norm": 0.36870524287223816,
      "learning_rate": 4.931875e-05,
      "loss": 0.0052,
      "step": 3270
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.22588196396827698,
      "learning_rate": 4.931666666666667e-05,
      "loss": 0.0043,
      "step": 3280
    },
    {
      "epoch": 0.10966666666666666,
      "grad_norm": 0.7889684438705444,
      "learning_rate": 4.9314583333333334e-05,
      "loss": 0.004,
      "step": 3290
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.31797152757644653,
      "learning_rate": 4.93125e-05,
      "loss": 0.0052,
      "step": 3300
    },
    {
      "epoch": 0.11033333333333334,
      "grad_norm": 0.012390399351716042,
      "learning_rate": 4.931041666666667e-05,
      "loss": 0.0051,
      "step": 3310
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 0.41033393144607544,
      "learning_rate": 4.930833333333334e-05,
      "loss": 0.004,
      "step": 3320
    },
    {
      "epoch": 0.111,
      "grad_norm": 0.6322275996208191,
      "learning_rate": 4.930625e-05,
      "loss": 0.0047,
      "step": 3330
    },
    {
      "epoch": 0.11133333333333334,
      "grad_norm": 0.6874474287033081,
      "learning_rate": 4.930416666666667e-05,
      "loss": 0.0064,
      "step": 3340
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.5072318911552429,
      "learning_rate": 4.930208333333334e-05,
      "loss": 0.0048,
      "step": 3350
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.31482991576194763,
      "learning_rate": 4.93e-05,
      "loss": 0.004,
      "step": 3360
    },
    {
      "epoch": 0.11233333333333333,
      "grad_norm": 0.09556090831756592,
      "learning_rate": 4.929791666666667e-05,
      "loss": 0.0045,
      "step": 3370
    },
    {
      "epoch": 0.11266666666666666,
      "grad_norm": 0.26913130283355713,
      "learning_rate": 4.929583333333334e-05,
      "loss": 0.0036,
      "step": 3380
    },
    {
      "epoch": 0.113,
      "grad_norm": 0.4250771701335907,
      "learning_rate": 4.929375e-05,
      "loss": 0.0048,
      "step": 3390
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.19068454205989838,
      "learning_rate": 4.929166666666667e-05,
      "loss": 0.0038,
      "step": 3400
    },
    {
      "epoch": 0.11366666666666667,
      "grad_norm": 0.5671576261520386,
      "learning_rate": 4.928958333333333e-05,
      "loss": 0.0043,
      "step": 3410
    },
    {
      "epoch": 0.114,
      "grad_norm": 0.4093020260334015,
      "learning_rate": 4.9287500000000005e-05,
      "loss": 0.0036,
      "step": 3420
    },
    {
      "epoch": 0.11433333333333333,
      "grad_norm": 0.5047138333320618,
      "learning_rate": 4.9285416666666664e-05,
      "loss": 0.0038,
      "step": 3430
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.09584241360425949,
      "learning_rate": 4.9283333333333336e-05,
      "loss": 0.0028,
      "step": 3440
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.06425562500953674,
      "learning_rate": 4.928125e-05,
      "loss": 0.0033,
      "step": 3450
    },
    {
      "epoch": 0.11533333333333333,
      "grad_norm": 0.16893170773983002,
      "learning_rate": 4.927916666666667e-05,
      "loss": 0.0048,
      "step": 3460
    },
    {
      "epoch": 0.11566666666666667,
      "grad_norm": 0.4444667100906372,
      "learning_rate": 4.927708333333333e-05,
      "loss": 0.0057,
      "step": 3470
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.2542279064655304,
      "learning_rate": 4.9275000000000005e-05,
      "loss": 0.0034,
      "step": 3480
    },
    {
      "epoch": 0.11633333333333333,
      "grad_norm": 0.23780858516693115,
      "learning_rate": 4.927291666666667e-05,
      "loss": 0.0047,
      "step": 3490
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.12649382650852203,
      "learning_rate": 4.9270833333333336e-05,
      "loss": 0.0033,
      "step": 3500
    },
    {
      "epoch": 0.117,
      "grad_norm": 0.7156320810317993,
      "learning_rate": 4.926875e-05,
      "loss": 0.0037,
      "step": 3510
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.4058772027492523,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0042,
      "step": 3520
    },
    {
      "epoch": 0.11766666666666667,
      "grad_norm": 0.407186359167099,
      "learning_rate": 4.926458333333334e-05,
      "loss": 0.005,
      "step": 3530
    },
    {
      "epoch": 0.118,
      "grad_norm": 0.5669880509376526,
      "learning_rate": 4.92625e-05,
      "loss": 0.003,
      "step": 3540
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.6305785179138184,
      "learning_rate": 4.926041666666667e-05,
      "loss": 0.0037,
      "step": 3550
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 0.07565656304359436,
      "learning_rate": 4.9258333333333336e-05,
      "loss": 0.0038,
      "step": 3560
    },
    {
      "epoch": 0.119,
      "grad_norm": 0.5362787842750549,
      "learning_rate": 4.925625e-05,
      "loss": 0.0041,
      "step": 3570
    },
    {
      "epoch": 0.11933333333333333,
      "grad_norm": 0.5355274677276611,
      "learning_rate": 4.925416666666667e-05,
      "loss": 0.0054,
      "step": 3580
    },
    {
      "epoch": 0.11966666666666667,
      "grad_norm": 0.2524704933166504,
      "learning_rate": 4.925208333333334e-05,
      "loss": 0.0036,
      "step": 3590
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.017470087856054306,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.0047,
      "step": 3600
    },
    {
      "epoch": 0.12033333333333333,
      "grad_norm": 0.35315173864364624,
      "learning_rate": 4.924791666666667e-05,
      "loss": 0.0048,
      "step": 3610
    },
    {
      "epoch": 0.12066666666666667,
      "grad_norm": 0.28642675280570984,
      "learning_rate": 4.9245833333333335e-05,
      "loss": 0.0039,
      "step": 3620
    },
    {
      "epoch": 0.121,
      "grad_norm": 0.786462128162384,
      "learning_rate": 4.924375e-05,
      "loss": 0.0032,
      "step": 3630
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 0.6289154291152954,
      "learning_rate": 4.9241666666666666e-05,
      "loss": 0.0039,
      "step": 3640
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.6711592674255371,
      "learning_rate": 4.923958333333333e-05,
      "loss": 0.0038,
      "step": 3650
    },
    {
      "epoch": 0.122,
      "grad_norm": 0.6282477974891663,
      "learning_rate": 4.9237500000000004e-05,
      "loss": 0.0055,
      "step": 3660
    },
    {
      "epoch": 0.12233333333333334,
      "grad_norm": 0.5699324607849121,
      "learning_rate": 4.923541666666667e-05,
      "loss": 0.0051,
      "step": 3670
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.4846060276031494,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.0037,
      "step": 3680
    },
    {
      "epoch": 0.123,
      "grad_norm": 0.6920290589332581,
      "learning_rate": 4.923125e-05,
      "loss": 0.0037,
      "step": 3690
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.2228478044271469,
      "learning_rate": 4.922916666666667e-05,
      "loss": 0.0041,
      "step": 3700
    },
    {
      "epoch": 0.12366666666666666,
      "grad_norm": 0.3447219133377075,
      "learning_rate": 4.922708333333333e-05,
      "loss": 0.003,
      "step": 3710
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.47195714712142944,
      "learning_rate": 4.9225000000000004e-05,
      "loss": 0.004,
      "step": 3720
    },
    {
      "epoch": 0.12433333333333334,
      "grad_norm": 0.09646135568618774,
      "learning_rate": 4.922291666666667e-05,
      "loss": 0.0039,
      "step": 3730
    },
    {
      "epoch": 0.12466666666666666,
      "grad_norm": 0.03332484886050224,
      "learning_rate": 4.9220833333333335e-05,
      "loss": 0.0044,
      "step": 3740
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.06455515325069427,
      "learning_rate": 4.921875e-05,
      "loss": 0.003,
      "step": 3750
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.30090466141700745,
      "learning_rate": 4.9216666666666666e-05,
      "loss": 0.0031,
      "step": 3760
    },
    {
      "epoch": 0.12566666666666668,
      "grad_norm": 0.015665944665670395,
      "learning_rate": 4.921458333333334e-05,
      "loss": 0.0024,
      "step": 3770
    },
    {
      "epoch": 0.126,
      "grad_norm": 0.497540682554245,
      "learning_rate": 4.9212500000000004e-05,
      "loss": 0.0034,
      "step": 3780
    },
    {
      "epoch": 0.12633333333333333,
      "grad_norm": 0.15967202186584473,
      "learning_rate": 4.921041666666667e-05,
      "loss": 0.0056,
      "step": 3790
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.6283894777297974,
      "learning_rate": 4.9208333333333335e-05,
      "loss": 0.0055,
      "step": 3800
    },
    {
      "epoch": 0.127,
      "grad_norm": 0.3930648863315582,
      "learning_rate": 4.920625000000001e-05,
      "loss": 0.0035,
      "step": 3810
    },
    {
      "epoch": 0.12733333333333333,
      "grad_norm": 0.44070953130722046,
      "learning_rate": 4.9204166666666666e-05,
      "loss": 0.0048,
      "step": 3820
    },
    {
      "epoch": 0.12766666666666668,
      "grad_norm": 0.06496822834014893,
      "learning_rate": 4.920208333333334e-05,
      "loss": 0.0051,
      "step": 3830
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5337730646133423,
      "learning_rate": 4.92e-05,
      "loss": 0.0031,
      "step": 3840
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.03539181128144264,
      "learning_rate": 4.919791666666667e-05,
      "loss": 0.0033,
      "step": 3850
    },
    {
      "epoch": 0.12866666666666668,
      "grad_norm": 0.2582651972770691,
      "learning_rate": 4.9195833333333334e-05,
      "loss": 0.0031,
      "step": 3860
    },
    {
      "epoch": 0.129,
      "grad_norm": 0.3142377436161041,
      "learning_rate": 4.9193750000000007e-05,
      "loss": 0.004,
      "step": 3870
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 0.2535240352153778,
      "learning_rate": 4.919166666666667e-05,
      "loss": 0.0043,
      "step": 3880
    },
    {
      "epoch": 0.12966666666666668,
      "grad_norm": 0.5647426247596741,
      "learning_rate": 4.918958333333333e-05,
      "loss": 0.0048,
      "step": 3890
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.2148105651140213,
      "learning_rate": 4.91875e-05,
      "loss": 0.0035,
      "step": 3900
    },
    {
      "epoch": 0.13033333333333333,
      "grad_norm": 0.28371956944465637,
      "learning_rate": 4.918541666666667e-05,
      "loss": 0.0036,
      "step": 3910
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.09634555131196976,
      "learning_rate": 4.9183333333333334e-05,
      "loss": 0.0032,
      "step": 3920
    },
    {
      "epoch": 0.131,
      "grad_norm": 0.4724031388759613,
      "learning_rate": 4.918125e-05,
      "loss": 0.0027,
      "step": 3930
    },
    {
      "epoch": 0.13133333333333333,
      "grad_norm": 0.18962517380714417,
      "learning_rate": 4.917916666666667e-05,
      "loss": 0.0023,
      "step": 3940
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.1262332946062088,
      "learning_rate": 4.917708333333334e-05,
      "loss": 0.0027,
      "step": 3950
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.2876514196395874,
      "learning_rate": 4.9175e-05,
      "loss": 0.0039,
      "step": 3960
    },
    {
      "epoch": 0.13233333333333333,
      "grad_norm": 0.4602525234222412,
      "learning_rate": 4.917291666666667e-05,
      "loss": 0.0025,
      "step": 3970
    },
    {
      "epoch": 0.13266666666666665,
      "grad_norm": 0.07024078071117401,
      "learning_rate": 4.917083333333334e-05,
      "loss": 0.0048,
      "step": 3980
    },
    {
      "epoch": 0.133,
      "grad_norm": 0.06559513509273529,
      "learning_rate": 4.916875e-05,
      "loss": 0.0033,
      "step": 3990
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.37972843647003174,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0034,
      "step": 4000
    },
    {
      "epoch": 0.13366666666666666,
      "grad_norm": 0.24158146977424622,
      "learning_rate": 4.916458333333334e-05,
      "loss": 0.0034,
      "step": 4010
    },
    {
      "epoch": 0.134,
      "grad_norm": 0.5013566017150879,
      "learning_rate": 4.91625e-05,
      "loss": 0.0025,
      "step": 4020
    },
    {
      "epoch": 0.13433333333333333,
      "grad_norm": 0.10745973140001297,
      "learning_rate": 4.916041666666667e-05,
      "loss": 0.0046,
      "step": 4030
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 0.5781049728393555,
      "learning_rate": 4.915833333333333e-05,
      "loss": 0.0035,
      "step": 4040
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.4078693687915802,
      "learning_rate": 4.9156250000000006e-05,
      "loss": 0.0048,
      "step": 4050
    },
    {
      "epoch": 0.13533333333333333,
      "grad_norm": 0.36429253220558167,
      "learning_rate": 4.915416666666667e-05,
      "loss": 0.0039,
      "step": 4060
    },
    {
      "epoch": 0.13566666666666666,
      "grad_norm": 0.03833084926009178,
      "learning_rate": 4.9152083333333337e-05,
      "loss": 0.0043,
      "step": 4070
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.12856659293174744,
      "learning_rate": 4.915e-05,
      "loss": 0.0039,
      "step": 4080
    },
    {
      "epoch": 0.13633333333333333,
      "grad_norm": 0.37627503275871277,
      "learning_rate": 4.9147916666666674e-05,
      "loss": 0.0034,
      "step": 4090
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.44057878851890564,
      "learning_rate": 4.914583333333333e-05,
      "loss": 0.0033,
      "step": 4100
    },
    {
      "epoch": 0.137,
      "grad_norm": 0.3441631495952606,
      "learning_rate": 4.9143750000000005e-05,
      "loss": 0.0035,
      "step": 4110
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 0.18864288926124573,
      "learning_rate": 4.914166666666667e-05,
      "loss": 0.004,
      "step": 4120
    },
    {
      "epoch": 0.13766666666666666,
      "grad_norm": 0.09594547003507614,
      "learning_rate": 4.9139583333333336e-05,
      "loss": 0.0039,
      "step": 4130
    },
    {
      "epoch": 0.138,
      "grad_norm": 0.1276015341281891,
      "learning_rate": 4.91375e-05,
      "loss": 0.0044,
      "step": 4140
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.4427768886089325,
      "learning_rate": 4.913541666666667e-05,
      "loss": 0.0025,
      "step": 4150
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.06366875767707825,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0044,
      "step": 4160
    },
    {
      "epoch": 0.139,
      "grad_norm": 0.43866148591041565,
      "learning_rate": 4.913125e-05,
      "loss": 0.0036,
      "step": 4170
    },
    {
      "epoch": 0.13933333333333334,
      "grad_norm": 0.1768179088830948,
      "learning_rate": 4.912916666666667e-05,
      "loss": 0.0044,
      "step": 4180
    },
    {
      "epoch": 0.13966666666666666,
      "grad_norm": 0.035009123384952545,
      "learning_rate": 4.9127083333333336e-05,
      "loss": 0.0035,
      "step": 4190
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.06509502232074738,
      "learning_rate": 4.9125e-05,
      "loss": 0.004,
      "step": 4200
    },
    {
      "epoch": 0.14033333333333334,
      "grad_norm": 0.033995747566223145,
      "learning_rate": 4.912291666666667e-05,
      "loss": 0.0045,
      "step": 4210
    },
    {
      "epoch": 0.14066666666666666,
      "grad_norm": 0.03493468090891838,
      "learning_rate": 4.912083333333334e-05,
      "loss": 0.0038,
      "step": 4220
    },
    {
      "epoch": 0.141,
      "grad_norm": 0.12605644762516022,
      "learning_rate": 4.9118750000000005e-05,
      "loss": 0.003,
      "step": 4230
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.28181514143943787,
      "learning_rate": 4.9116666666666663e-05,
      "loss": 0.0032,
      "step": 4240
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.09940089285373688,
      "learning_rate": 4.9114583333333336e-05,
      "loss": 0.0043,
      "step": 4250
    },
    {
      "epoch": 0.142,
      "grad_norm": 0.31359803676605225,
      "learning_rate": 4.91125e-05,
      "loss": 0.0036,
      "step": 4260
    },
    {
      "epoch": 0.14233333333333334,
      "grad_norm": 0.5627931356430054,
      "learning_rate": 4.911041666666667e-05,
      "loss": 0.0041,
      "step": 4270
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 0.1269436776638031,
      "learning_rate": 4.910833333333333e-05,
      "loss": 0.0026,
      "step": 4280
    },
    {
      "epoch": 0.143,
      "grad_norm": 0.24151934683322906,
      "learning_rate": 4.9106250000000004e-05,
      "loss": 0.0041,
      "step": 4290
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.32186225056648254,
      "learning_rate": 4.910416666666667e-05,
      "loss": 0.0041,
      "step": 4300
    },
    {
      "epoch": 0.14366666666666666,
      "grad_norm": 0.38383719325065613,
      "learning_rate": 4.9102083333333335e-05,
      "loss": 0.0031,
      "step": 4310
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.5633249282836914,
      "learning_rate": 4.91e-05,
      "loss": 0.0037,
      "step": 4320
    },
    {
      "epoch": 0.14433333333333334,
      "grad_norm": 0.25039583444595337,
      "learning_rate": 4.909791666666667e-05,
      "loss": 0.0042,
      "step": 4330
    },
    {
      "epoch": 0.14466666666666667,
      "grad_norm": 0.1253776103258133,
      "learning_rate": 4.909583333333334e-05,
      "loss": 0.0052,
      "step": 4340
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.4477260410785675,
      "learning_rate": 4.9093750000000004e-05,
      "loss": 0.0054,
      "step": 4350
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 0.18350346386432648,
      "learning_rate": 4.909166666666667e-05,
      "loss": 0.0039,
      "step": 4360
    },
    {
      "epoch": 0.14566666666666667,
      "grad_norm": 0.06576980650424957,
      "learning_rate": 4.9089583333333335e-05,
      "loss": 0.0034,
      "step": 4370
    },
    {
      "epoch": 0.146,
      "grad_norm": 0.4386436939239502,
      "learning_rate": 4.90875e-05,
      "loss": 0.004,
      "step": 4380
    },
    {
      "epoch": 0.14633333333333334,
      "grad_norm": 0.06389611214399338,
      "learning_rate": 4.9085416666666666e-05,
      "loss": 0.0019,
      "step": 4390
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.21896937489509583,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.003,
      "step": 4400
    },
    {
      "epoch": 0.147,
      "grad_norm": 0.187639057636261,
      "learning_rate": 4.9081250000000004e-05,
      "loss": 0.0037,
      "step": 4410
    },
    {
      "epoch": 0.14733333333333334,
      "grad_norm": 0.3136582374572754,
      "learning_rate": 4.907916666666667e-05,
      "loss": 0.0047,
      "step": 4420
    },
    {
      "epoch": 0.14766666666666667,
      "grad_norm": 0.18883739411830902,
      "learning_rate": 4.9077083333333335e-05,
      "loss": 0.0033,
      "step": 4430
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.15634049475193024,
      "learning_rate": 4.907500000000001e-05,
      "loss": 0.0044,
      "step": 4440
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.5319085121154785,
      "learning_rate": 4.9072916666666666e-05,
      "loss": 0.0033,
      "step": 4450
    },
    {
      "epoch": 0.14866666666666667,
      "grad_norm": 0.12594859302043915,
      "learning_rate": 4.907083333333334e-05,
      "loss": 0.0039,
      "step": 4460
    },
    {
      "epoch": 0.149,
      "grad_norm": 0.12040255218744278,
      "learning_rate": 4.9068750000000003e-05,
      "loss": 0.0032,
      "step": 4470
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.22136059403419495,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0036,
      "step": 4480
    },
    {
      "epoch": 0.14966666666666667,
      "grad_norm": 0.406357079744339,
      "learning_rate": 4.9064583333333334e-05,
      "loss": 0.0026,
      "step": 4490
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.035927243530750275,
      "learning_rate": 4.90625e-05,
      "loss": 0.0032,
      "step": 4500
    },
    {
      "epoch": 0.15033333333333335,
      "grad_norm": 0.21872448921203613,
      "learning_rate": 4.906041666666667e-05,
      "loss": 0.004,
      "step": 4510
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 0.0634971484541893,
      "learning_rate": 4.905833333333333e-05,
      "loss": 0.0038,
      "step": 4520
    },
    {
      "epoch": 0.151,
      "grad_norm": 0.06472652405500412,
      "learning_rate": 4.905625e-05,
      "loss": 0.0045,
      "step": 4530
    },
    {
      "epoch": 0.15133333333333332,
      "grad_norm": 0.063743956387043,
      "learning_rate": 4.905416666666667e-05,
      "loss": 0.0033,
      "step": 4540
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.40682464838027954,
      "learning_rate": 4.9052083333333334e-05,
      "loss": 0.0025,
      "step": 4550
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.2507933974266052,
      "learning_rate": 4.905e-05,
      "loss": 0.0034,
      "step": 4560
    },
    {
      "epoch": 0.15233333333333332,
      "grad_norm": 0.8755300641059875,
      "learning_rate": 4.904791666666667e-05,
      "loss": 0.0026,
      "step": 4570
    },
    {
      "epoch": 0.15266666666666667,
      "grad_norm": 0.37481942772865295,
      "learning_rate": 4.904583333333334e-05,
      "loss": 0.0035,
      "step": 4580
    },
    {
      "epoch": 0.153,
      "grad_norm": 0.2443496286869049,
      "learning_rate": 4.904375e-05,
      "loss": 0.004,
      "step": 4590
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.4683712124824524,
      "learning_rate": 4.904166666666667e-05,
      "loss": 0.0036,
      "step": 4600
    },
    {
      "epoch": 0.15366666666666667,
      "grad_norm": 0.5626978874206543,
      "learning_rate": 4.9039583333333334e-05,
      "loss": 0.0037,
      "step": 4610
    },
    {
      "epoch": 0.154,
      "grad_norm": 0.3767285943031311,
      "learning_rate": 4.9037500000000006e-05,
      "loss": 0.0044,
      "step": 4620
    },
    {
      "epoch": 0.15433333333333332,
      "grad_norm": 0.15760307013988495,
      "learning_rate": 4.9035416666666665e-05,
      "loss": 0.0039,
      "step": 4630
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.31384143233299255,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.0036,
      "step": 4640
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.18823355436325073,
      "learning_rate": 4.903125e-05,
      "loss": 0.005,
      "step": 4650
    },
    {
      "epoch": 0.15533333333333332,
      "grad_norm": 0.3754381835460663,
      "learning_rate": 4.902916666666667e-05,
      "loss": 0.0041,
      "step": 4660
    },
    {
      "epoch": 0.15566666666666668,
      "grad_norm": 0.06499570608139038,
      "learning_rate": 4.9027083333333334e-05,
      "loss": 0.0041,
      "step": 4670
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.2815566658973694,
      "learning_rate": 4.9025000000000006e-05,
      "loss": 0.0058,
      "step": 4680
    },
    {
      "epoch": 0.15633333333333332,
      "grad_norm": 0.15715669095516205,
      "learning_rate": 4.902291666666667e-05,
      "loss": 0.0028,
      "step": 4690
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.18966862559318542,
      "learning_rate": 4.902083333333334e-05,
      "loss": 0.0042,
      "step": 4700
    },
    {
      "epoch": 0.157,
      "grad_norm": 0.35615426301956177,
      "learning_rate": 4.901875e-05,
      "loss": 0.0043,
      "step": 4710
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.03386015444993973,
      "learning_rate": 4.901666666666667e-05,
      "loss": 0.0037,
      "step": 4720
    },
    {
      "epoch": 0.15766666666666668,
      "grad_norm": 0.15688171982765198,
      "learning_rate": 4.901458333333333e-05,
      "loss": 0.0039,
      "step": 4730
    },
    {
      "epoch": 0.158,
      "grad_norm": 0.15686605870723724,
      "learning_rate": 4.90125e-05,
      "loss": 0.0038,
      "step": 4740
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.4382678270339966,
      "learning_rate": 4.901041666666667e-05,
      "loss": 0.0038,
      "step": 4750
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 0.20486417412757874,
      "learning_rate": 4.9008333333333336e-05,
      "loss": 0.0028,
      "step": 4760
    },
    {
      "epoch": 0.159,
      "grad_norm": 0.6261517405509949,
      "learning_rate": 4.900625e-05,
      "loss": 0.0042,
      "step": 4770
    },
    {
      "epoch": 0.15933333333333333,
      "grad_norm": 0.31536346673965454,
      "learning_rate": 4.900416666666667e-05,
      "loss": 0.0052,
      "step": 4780
    },
    {
      "epoch": 0.15966666666666668,
      "grad_norm": 0.04265297204256058,
      "learning_rate": 4.900208333333334e-05,
      "loss": 0.0048,
      "step": 4790
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.34556490182876587,
      "learning_rate": 4.9e-05,
      "loss": 0.0032,
      "step": 4800
    },
    {
      "epoch": 0.16033333333333333,
      "grad_norm": 0.6286272406578064,
      "learning_rate": 4.899791666666667e-05,
      "loss": 0.0039,
      "step": 4810
    },
    {
      "epoch": 0.16066666666666668,
      "grad_norm": 0.4428524374961853,
      "learning_rate": 4.8995833333333336e-05,
      "loss": 0.0038,
      "step": 4820
    },
    {
      "epoch": 0.161,
      "grad_norm": 0.0647566094994545,
      "learning_rate": 4.899375e-05,
      "loss": 0.004,
      "step": 4830
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 0.24989748001098633,
      "learning_rate": 4.899166666666667e-05,
      "loss": 0.004,
      "step": 4840
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.15743187069892883,
      "learning_rate": 4.898958333333333e-05,
      "loss": 0.002,
      "step": 4850
    },
    {
      "epoch": 0.162,
      "grad_norm": 0.09542595595121384,
      "learning_rate": 4.8987500000000005e-05,
      "loss": 0.0049,
      "step": 4860
    },
    {
      "epoch": 0.16233333333333333,
      "grad_norm": 0.09804284572601318,
      "learning_rate": 4.8985416666666664e-05,
      "loss": 0.0033,
      "step": 4870
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.2507438361644745,
      "learning_rate": 4.8983333333333336e-05,
      "loss": 0.0041,
      "step": 4880
    },
    {
      "epoch": 0.163,
      "grad_norm": 0.28197717666625977,
      "learning_rate": 4.898125e-05,
      "loss": 0.0026,
      "step": 4890
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.3765258193016052,
      "learning_rate": 4.8979166666666674e-05,
      "loss": 0.0048,
      "step": 4900
    },
    {
      "epoch": 0.16366666666666665,
      "grad_norm": 0.907270073890686,
      "learning_rate": 4.897708333333333e-05,
      "loss": 0.0034,
      "step": 4910
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.624970555305481,
      "learning_rate": 4.8975000000000005e-05,
      "loss": 0.0042,
      "step": 4920
    },
    {
      "epoch": 0.16433333333333333,
      "grad_norm": 0.2565552890300751,
      "learning_rate": 4.897291666666667e-05,
      "loss": 0.0045,
      "step": 4930
    },
    {
      "epoch": 0.16466666666666666,
      "grad_norm": 0.6270419359207153,
      "learning_rate": 4.8970833333333336e-05,
      "loss": 0.0044,
      "step": 4940
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.5608231425285339,
      "learning_rate": 4.896875e-05,
      "loss": 0.0045,
      "step": 4950
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.24947258830070496,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.0039,
      "step": 4960
    },
    {
      "epoch": 0.16566666666666666,
      "grad_norm": 0.2817765474319458,
      "learning_rate": 4.896458333333334e-05,
      "loss": 0.0034,
      "step": 4970
    },
    {
      "epoch": 0.166,
      "grad_norm": 0.9994145631790161,
      "learning_rate": 4.89625e-05,
      "loss": 0.0041,
      "step": 4980
    },
    {
      "epoch": 0.16633333333333333,
      "grad_norm": 0.7209020256996155,
      "learning_rate": 4.896041666666667e-05,
      "loss": 0.0029,
      "step": 4990
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.020158350467681885,
      "learning_rate": 4.8958333333333335e-05,
      "loss": 0.0035,
      "step": 5000
    },
    {
      "epoch": 0.167,
      "grad_norm": 0.12962226569652557,
      "learning_rate": 4.895625e-05,
      "loss": 0.0042,
      "step": 5010
    },
    {
      "epoch": 0.16733333333333333,
      "grad_norm": 0.3121470808982849,
      "learning_rate": 4.8954166666666666e-05,
      "loss": 0.0027,
      "step": 5020
    },
    {
      "epoch": 0.16766666666666666,
      "grad_norm": 0.4048045873641968,
      "learning_rate": 4.895208333333334e-05,
      "loss": 0.0042,
      "step": 5030
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.06579475104808807,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 0.0036,
      "step": 5040
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.5299453139305115,
      "learning_rate": 4.894791666666667e-05,
      "loss": 0.0021,
      "step": 5050
    },
    {
      "epoch": 0.16866666666666666,
      "grad_norm": 0.06554976105690002,
      "learning_rate": 4.8945833333333335e-05,
      "loss": 0.0028,
      "step": 5060
    },
    {
      "epoch": 0.169,
      "grad_norm": 0.43599051237106323,
      "learning_rate": 4.894375000000001e-05,
      "loss": 0.0033,
      "step": 5070
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 0.016186414286494255,
      "learning_rate": 4.8941666666666666e-05,
      "loss": 0.0024,
      "step": 5080
    },
    {
      "epoch": 0.16966666666666666,
      "grad_norm": 0.6318561434745789,
      "learning_rate": 4.893958333333333e-05,
      "loss": 0.0037,
      "step": 5090
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.6505119204521179,
      "learning_rate": 4.8937500000000004e-05,
      "loss": 0.0034,
      "step": 5100
    },
    {
      "epoch": 0.17033333333333334,
      "grad_norm": 0.7107641100883484,
      "learning_rate": 4.893541666666667e-05,
      "loss": 0.0039,
      "step": 5110
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.4675999581813812,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0041,
      "step": 5120
    },
    {
      "epoch": 0.171,
      "grad_norm": 0.18768587708473206,
      "learning_rate": 4.893125e-05,
      "loss": 0.0049,
      "step": 5130
    },
    {
      "epoch": 0.17133333333333334,
      "grad_norm": 0.12615862488746643,
      "learning_rate": 4.892916666666667e-05,
      "loss": 0.0033,
      "step": 5140
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.1946275532245636,
      "learning_rate": 4.892708333333333e-05,
      "loss": 0.0038,
      "step": 5150
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.6828672289848328,
      "learning_rate": 4.8925e-05,
      "loss": 0.0046,
      "step": 5160
    },
    {
      "epoch": 0.17233333333333334,
      "grad_norm": 0.06438674032688141,
      "learning_rate": 4.892291666666667e-05,
      "loss": 0.0035,
      "step": 5170
    },
    {
      "epoch": 0.17266666666666666,
      "grad_norm": 0.10324365645647049,
      "learning_rate": 4.892083333333334e-05,
      "loss": 0.0039,
      "step": 5180
    },
    {
      "epoch": 0.173,
      "grad_norm": 0.21979112923145294,
      "learning_rate": 4.891875e-05,
      "loss": 0.0034,
      "step": 5190
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.2807752192020416,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.0043,
      "step": 5200
    },
    {
      "epoch": 0.17366666666666666,
      "grad_norm": 0.7468556761741638,
      "learning_rate": 4.891458333333334e-05,
      "loss": 0.0038,
      "step": 5210
    },
    {
      "epoch": 0.174,
      "grad_norm": 0.06422734260559082,
      "learning_rate": 4.89125e-05,
      "loss": 0.0033,
      "step": 5220
    },
    {
      "epoch": 0.17433333333333334,
      "grad_norm": 0.43661895394325256,
      "learning_rate": 4.891041666666667e-05,
      "loss": 0.0035,
      "step": 5230
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 0.7803341150283813,
      "learning_rate": 4.8908333333333334e-05,
      "loss": 0.0055,
      "step": 5240
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.7640648484230042,
      "learning_rate": 4.8906250000000006e-05,
      "loss": 0.0036,
      "step": 5250
    },
    {
      "epoch": 0.17533333333333334,
      "grad_norm": 0.3424351215362549,
      "learning_rate": 4.8904166666666665e-05,
      "loss": 0.0042,
      "step": 5260
    },
    {
      "epoch": 0.17566666666666667,
      "grad_norm": 0.09174961596727371,
      "learning_rate": 4.890208333333334e-05,
      "loss": 0.0031,
      "step": 5270
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.34210458397865295,
      "learning_rate": 4.89e-05,
      "loss": 0.0036,
      "step": 5280
    },
    {
      "epoch": 0.17633333333333334,
      "grad_norm": 0.06428591161966324,
      "learning_rate": 4.889791666666667e-05,
      "loss": 0.0027,
      "step": 5290
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.5302072167396545,
      "learning_rate": 4.8895833333333334e-05,
      "loss": 0.0038,
      "step": 5300
    },
    {
      "epoch": 0.177,
      "grad_norm": 0.49825653433799744,
      "learning_rate": 4.8893750000000006e-05,
      "loss": 0.0036,
      "step": 5310
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 0.10727205127477646,
      "learning_rate": 4.889166666666667e-05,
      "loss": 0.0043,
      "step": 5320
    },
    {
      "epoch": 0.17766666666666667,
      "grad_norm": 0.43037986755371094,
      "learning_rate": 4.888958333333333e-05,
      "loss": 0.0034,
      "step": 5330
    },
    {
      "epoch": 0.178,
      "grad_norm": 0.5607953071594238,
      "learning_rate": 4.88875e-05,
      "loss": 0.0025,
      "step": 5340
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.3123464584350586,
      "learning_rate": 4.888541666666667e-05,
      "loss": 0.0026,
      "step": 5350
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.01871185563504696,
      "learning_rate": 4.8883333333333333e-05,
      "loss": 0.0045,
      "step": 5360
    },
    {
      "epoch": 0.179,
      "grad_norm": 0.034650810062885284,
      "learning_rate": 4.888125e-05,
      "loss": 0.0045,
      "step": 5370
    },
    {
      "epoch": 0.17933333333333334,
      "grad_norm": 0.39930444955825806,
      "learning_rate": 4.887916666666667e-05,
      "loss": 0.0039,
      "step": 5380
    },
    {
      "epoch": 0.17966666666666667,
      "grad_norm": 0.1439889520406723,
      "learning_rate": 4.887708333333334e-05,
      "loss": 0.0051,
      "step": 5390
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.21875238418579102,
      "learning_rate": 4.8875e-05,
      "loss": 0.004,
      "step": 5400
    },
    {
      "epoch": 0.18033333333333335,
      "grad_norm": 0.18847286701202393,
      "learning_rate": 4.887291666666667e-05,
      "loss": 0.0038,
      "step": 5410
    },
    {
      "epoch": 0.18066666666666667,
      "grad_norm": 0.5293447375297546,
      "learning_rate": 4.887083333333334e-05,
      "loss": 0.0044,
      "step": 5420
    },
    {
      "epoch": 0.181,
      "grad_norm": 0.2992250323295593,
      "learning_rate": 4.8868750000000005e-05,
      "loss": 0.0047,
      "step": 5430
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.02317412942647934,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0055,
      "step": 5440
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.3859858512878418,
      "learning_rate": 4.8864583333333336e-05,
      "loss": 0.0033,
      "step": 5450
    },
    {
      "epoch": 0.182,
      "grad_norm": 0.2806531488895416,
      "learning_rate": 4.88625e-05,
      "loss": 0.0039,
      "step": 5460
    },
    {
      "epoch": 0.18233333333333332,
      "grad_norm": 0.20084810256958008,
      "learning_rate": 4.886041666666667e-05,
      "loss": 0.0042,
      "step": 5470
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 0.5599703788757324,
      "learning_rate": 4.885833333333333e-05,
      "loss": 0.0031,
      "step": 5480
    },
    {
      "epoch": 0.183,
      "grad_norm": 0.55893474817276,
      "learning_rate": 4.8856250000000005e-05,
      "loss": 0.0048,
      "step": 5490
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.06262441724538803,
      "learning_rate": 4.885416666666667e-05,
      "loss": 0.0029,
      "step": 5500
    },
    {
      "epoch": 0.18366666666666667,
      "grad_norm": 0.43405359983444214,
      "learning_rate": 4.8852083333333336e-05,
      "loss": 0.004,
      "step": 5510
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.18813152611255646,
      "learning_rate": 4.885e-05,
      "loss": 0.0028,
      "step": 5520
    },
    {
      "epoch": 0.18433333333333332,
      "grad_norm": 0.21702788770198822,
      "learning_rate": 4.8847916666666674e-05,
      "loss": 0.0047,
      "step": 5530
    },
    {
      "epoch": 0.18466666666666667,
      "grad_norm": 0.5018444657325745,
      "learning_rate": 4.884583333333333e-05,
      "loss": 0.0042,
      "step": 5540
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.25067925453186035,
      "learning_rate": 4.8843750000000005e-05,
      "loss": 0.0054,
      "step": 5550
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 0.187237948179245,
      "learning_rate": 4.884166666666667e-05,
      "loss": 0.0038,
      "step": 5560
    },
    {
      "epoch": 0.18566666666666667,
      "grad_norm": 0.2804761826992035,
      "learning_rate": 4.8839583333333336e-05,
      "loss": 0.0037,
      "step": 5570
    },
    {
      "epoch": 0.186,
      "grad_norm": 0.8727450966835022,
      "learning_rate": 4.88375e-05,
      "loss": 0.0044,
      "step": 5580
    },
    {
      "epoch": 0.18633333333333332,
      "grad_norm": 0.49925753474235535,
      "learning_rate": 4.883541666666667e-05,
      "loss": 0.0038,
      "step": 5590
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.3124855160713196,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0032,
      "step": 5600
    },
    {
      "epoch": 0.187,
      "grad_norm": 0.16586415469646454,
      "learning_rate": 4.883125e-05,
      "loss": 0.0034,
      "step": 5610
    },
    {
      "epoch": 0.18733333333333332,
      "grad_norm": 0.24074812233448029,
      "learning_rate": 4.882916666666667e-05,
      "loss": 0.0031,
      "step": 5620
    },
    {
      "epoch": 0.18766666666666668,
      "grad_norm": 0.45563745498657227,
      "learning_rate": 4.8827083333333335e-05,
      "loss": 0.0039,
      "step": 5630
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.09551205486059189,
      "learning_rate": 4.8825e-05,
      "loss": 0.0054,
      "step": 5640
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.12600387632846832,
      "learning_rate": 4.8822916666666666e-05,
      "loss": 0.0031,
      "step": 5650
    },
    {
      "epoch": 0.18866666666666668,
      "grad_norm": 0.2812012732028961,
      "learning_rate": 4.882083333333334e-05,
      "loss": 0.0031,
      "step": 5660
    },
    {
      "epoch": 0.189,
      "grad_norm": 0.12564873695373535,
      "learning_rate": 4.8818750000000004e-05,
      "loss": 0.0042,
      "step": 5670
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.5658179521560669,
      "learning_rate": 4.881666666666667e-05,
      "loss": 0.0039,
      "step": 5680
    },
    {
      "epoch": 0.18966666666666668,
      "grad_norm": 0.06445002555847168,
      "learning_rate": 4.8814583333333335e-05,
      "loss": 0.0027,
      "step": 5690
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.2506791651248932,
      "learning_rate": 4.88125e-05,
      "loss": 0.006,
      "step": 5700
    },
    {
      "epoch": 0.19033333333333333,
      "grad_norm": 0.3053800165653229,
      "learning_rate": 4.881041666666667e-05,
      "loss": 0.0031,
      "step": 5710
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 0.24873825907707214,
      "learning_rate": 4.880833333333333e-05,
      "loss": 0.0049,
      "step": 5720
    },
    {
      "epoch": 0.191,
      "grad_norm": 0.21773558855056763,
      "learning_rate": 4.8806250000000004e-05,
      "loss": 0.0034,
      "step": 5730
    },
    {
      "epoch": 0.19133333333333333,
      "grad_norm": 0.4738464057445526,
      "learning_rate": 4.880416666666667e-05,
      "loss": 0.0039,
      "step": 5740
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.3101881444454193,
      "learning_rate": 4.8802083333333335e-05,
      "loss": 0.0031,
      "step": 5750
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.18690751492977142,
      "learning_rate": 4.88e-05,
      "loss": 0.0042,
      "step": 5760
    },
    {
      "epoch": 0.19233333333333333,
      "grad_norm": 0.49550893902778625,
      "learning_rate": 4.879791666666667e-05,
      "loss": 0.0038,
      "step": 5770
    },
    {
      "epoch": 0.19266666666666668,
      "grad_norm": 0.18642914295196533,
      "learning_rate": 4.879583333333334e-05,
      "loss": 0.0054,
      "step": 5780
    },
    {
      "epoch": 0.193,
      "grad_norm": 0.6513448357582092,
      "learning_rate": 4.8793750000000004e-05,
      "loss": 0.0038,
      "step": 5790
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.5038278698921204,
      "learning_rate": 4.879166666666667e-05,
      "loss": 0.0039,
      "step": 5800
    },
    {
      "epoch": 0.19366666666666665,
      "grad_norm": 0.15569137036800385,
      "learning_rate": 4.8789583333333335e-05,
      "loss": 0.003,
      "step": 5810
    },
    {
      "epoch": 0.194,
      "grad_norm": 0.3918145000934601,
      "learning_rate": 4.87875e-05,
      "loss": 0.0044,
      "step": 5820
    },
    {
      "epoch": 0.19433333333333333,
      "grad_norm": 0.4650211036205292,
      "learning_rate": 4.8785416666666666e-05,
      "loss": 0.0042,
      "step": 5830
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.2807236909866333,
      "learning_rate": 4.878333333333334e-05,
      "loss": 0.0045,
      "step": 5840
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.5579731464385986,
      "learning_rate": 4.878125e-05,
      "loss": 0.0033,
      "step": 5850
    },
    {
      "epoch": 0.19533333333333333,
      "grad_norm": 0.08855899423360825,
      "learning_rate": 4.877916666666667e-05,
      "loss": 0.0043,
      "step": 5860
    },
    {
      "epoch": 0.19566666666666666,
      "grad_norm": 0.18652299046516418,
      "learning_rate": 4.8777083333333334e-05,
      "loss": 0.004,
      "step": 5870
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.3736313581466675,
      "learning_rate": 4.8775000000000007e-05,
      "loss": 0.0029,
      "step": 5880
    },
    {
      "epoch": 0.19633333333333333,
      "grad_norm": 0.5897060632705688,
      "learning_rate": 4.8772916666666665e-05,
      "loss": 0.0042,
      "step": 5890
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.18667317926883698,
      "learning_rate": 4.877083333333334e-05,
      "loss": 0.0035,
      "step": 5900
    },
    {
      "epoch": 0.197,
      "grad_norm": 0.09357047080993652,
      "learning_rate": 4.876875e-05,
      "loss": 0.0049,
      "step": 5910
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.06561578065156937,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0036,
      "step": 5920
    },
    {
      "epoch": 0.19766666666666666,
      "grad_norm": 0.3787250220775604,
      "learning_rate": 4.8764583333333334e-05,
      "loss": 0.0037,
      "step": 5930
    },
    {
      "epoch": 0.198,
      "grad_norm": 0.27919837832450867,
      "learning_rate": 4.87625e-05,
      "loss": 0.0043,
      "step": 5940
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.1250685602426529,
      "learning_rate": 4.876041666666667e-05,
      "loss": 0.0046,
      "step": 5950
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 0.03330009803175926,
      "learning_rate": 4.875833333333333e-05,
      "loss": 0.0035,
      "step": 5960
    },
    {
      "epoch": 0.199,
      "grad_norm": 0.28415849804878235,
      "learning_rate": 4.875625e-05,
      "loss": 0.0036,
      "step": 5970
    },
    {
      "epoch": 0.19933333333333333,
      "grad_norm": 0.30266687273979187,
      "learning_rate": 4.875416666666667e-05,
      "loss": 0.0044,
      "step": 5980
    },
    {
      "epoch": 0.19966666666666666,
      "grad_norm": 0.5265659093856812,
      "learning_rate": 4.875208333333334e-05,
      "loss": 0.0054,
      "step": 5990
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2484549582004547,
      "learning_rate": 4.875e-05,
      "loss": 0.0051,
      "step": 6000
    },
    {
      "epoch": 0.20033333333333334,
      "grad_norm": 0.18700529634952545,
      "learning_rate": 4.874791666666667e-05,
      "loss": 0.0037,
      "step": 6010
    },
    {
      "epoch": 0.20066666666666666,
      "grad_norm": 0.5177488923072815,
      "learning_rate": 4.874583333333334e-05,
      "loss": 0.0038,
      "step": 6020
    },
    {
      "epoch": 0.201,
      "grad_norm": 0.5739030838012695,
      "learning_rate": 4.874375e-05,
      "loss": 0.0032,
      "step": 6030
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 0.5894023776054382,
      "learning_rate": 4.874166666666667e-05,
      "loss": 0.0034,
      "step": 6040
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.06338365375995636,
      "learning_rate": 4.873958333333333e-05,
      "loss": 0.0024,
      "step": 6050
    },
    {
      "epoch": 0.202,
      "grad_norm": 0.1660739779472351,
      "learning_rate": 4.8737500000000006e-05,
      "loss": 0.0043,
      "step": 6060
    },
    {
      "epoch": 0.20233333333333334,
      "grad_norm": 0.6817740201950073,
      "learning_rate": 4.8735416666666664e-05,
      "loss": 0.0035,
      "step": 6070
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.5190969109535217,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0038,
      "step": 6080
    },
    {
      "epoch": 0.203,
      "grad_norm": 0.4045826196670532,
      "learning_rate": 4.873125e-05,
      "loss": 0.0031,
      "step": 6090
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.09494958072900772,
      "learning_rate": 4.872916666666667e-05,
      "loss": 0.0044,
      "step": 6100
    },
    {
      "epoch": 0.20366666666666666,
      "grad_norm": 0.342892587184906,
      "learning_rate": 4.872708333333333e-05,
      "loss": 0.0055,
      "step": 6110
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.1860836297273636,
      "learning_rate": 4.8725000000000005e-05,
      "loss": 0.005,
      "step": 6120
    },
    {
      "epoch": 0.20433333333333334,
      "grad_norm": 0.44515734910964966,
      "learning_rate": 4.872291666666667e-05,
      "loss": 0.0058,
      "step": 6130
    },
    {
      "epoch": 0.20466666666666666,
      "grad_norm": 0.25737157464027405,
      "learning_rate": 4.8720833333333336e-05,
      "loss": 0.0053,
      "step": 6140
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.03573329746723175,
      "learning_rate": 4.871875e-05,
      "loss": 0.0046,
      "step": 6150
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.3092966377735138,
      "learning_rate": 4.8716666666666674e-05,
      "loss": 0.0045,
      "step": 6160
    },
    {
      "epoch": 0.20566666666666666,
      "grad_norm": 0.37338370084762573,
      "learning_rate": 4.871458333333333e-05,
      "loss": 0.003,
      "step": 6170
    },
    {
      "epoch": 0.206,
      "grad_norm": 0.1857510656118393,
      "learning_rate": 4.87125e-05,
      "loss": 0.0034,
      "step": 6180
    },
    {
      "epoch": 0.20633333333333334,
      "grad_norm": 0.47240549325942993,
      "learning_rate": 4.871041666666667e-05,
      "loss": 0.0032,
      "step": 6190
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.09429264813661575,
      "learning_rate": 4.8708333333333336e-05,
      "loss": 0.0039,
      "step": 6200
    },
    {
      "epoch": 0.207,
      "grad_norm": 0.3654891848564148,
      "learning_rate": 4.870625e-05,
      "loss": 0.0047,
      "step": 6210
    },
    {
      "epoch": 0.20733333333333334,
      "grad_norm": 0.21019329130649567,
      "learning_rate": 4.870416666666667e-05,
      "loss": 0.0033,
      "step": 6220
    },
    {
      "epoch": 0.20766666666666667,
      "grad_norm": 0.15463560819625854,
      "learning_rate": 4.870208333333334e-05,
      "loss": 0.0036,
      "step": 6230
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.27798065543174744,
      "learning_rate": 4.87e-05,
      "loss": 0.0048,
      "step": 6240
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.3415622413158417,
      "learning_rate": 4.869791666666667e-05,
      "loss": 0.0042,
      "step": 6250
    },
    {
      "epoch": 0.20866666666666667,
      "grad_norm": 0.4958387315273285,
      "learning_rate": 4.8695833333333336e-05,
      "loss": 0.004,
      "step": 6260
    },
    {
      "epoch": 0.209,
      "grad_norm": 0.5869297385215759,
      "learning_rate": 4.869375000000001e-05,
      "loss": 0.0036,
      "step": 6270
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 0.30951255559921265,
      "learning_rate": 4.869166666666667e-05,
      "loss": 0.004,
      "step": 6280
    },
    {
      "epoch": 0.20966666666666667,
      "grad_norm": 0.40176835656166077,
      "learning_rate": 4.868958333333334e-05,
      "loss": 0.0045,
      "step": 6290
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.4190569221973419,
      "learning_rate": 4.8687500000000004e-05,
      "loss": 0.004,
      "step": 6300
    },
    {
      "epoch": 0.21033333333333334,
      "grad_norm": 0.309242844581604,
      "learning_rate": 4.868541666666667e-05,
      "loss": 0.003,
      "step": 6310
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.1438305377960205,
      "learning_rate": 4.8683333333333335e-05,
      "loss": 0.0027,
      "step": 6320
    },
    {
      "epoch": 0.211,
      "grad_norm": 0.7126714587211609,
      "learning_rate": 4.868125e-05,
      "loss": 0.0039,
      "step": 6330
    },
    {
      "epoch": 0.21133333333333335,
      "grad_norm": 0.06448530405759811,
      "learning_rate": 4.867916666666667e-05,
      "loss": 0.0037,
      "step": 6340
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.310433954000473,
      "learning_rate": 4.867708333333333e-05,
      "loss": 0.0059,
      "step": 6350
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.12456341087818146,
      "learning_rate": 4.8675000000000004e-05,
      "loss": 0.0027,
      "step": 6360
    },
    {
      "epoch": 0.21233333333333335,
      "grad_norm": 0.6799136400222778,
      "learning_rate": 4.867291666666667e-05,
      "loss": 0.0044,
      "step": 6370
    },
    {
      "epoch": 0.21266666666666667,
      "grad_norm": 0.2479577213525772,
      "learning_rate": 4.8670833333333335e-05,
      "loss": 0.0028,
      "step": 6380
    },
    {
      "epoch": 0.213,
      "grad_norm": 0.12422299385070801,
      "learning_rate": 4.866875e-05,
      "loss": 0.004,
      "step": 6390
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.3087827265262604,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.005,
      "step": 6400
    },
    {
      "epoch": 0.21366666666666667,
      "grad_norm": 0.18669795989990234,
      "learning_rate": 4.866458333333334e-05,
      "loss": 0.0046,
      "step": 6410
    },
    {
      "epoch": 0.214,
      "grad_norm": 0.3290446400642395,
      "learning_rate": 4.86625e-05,
      "loss": 0.0038,
      "step": 6420
    },
    {
      "epoch": 0.21433333333333332,
      "grad_norm": 0.1858271211385727,
      "learning_rate": 4.866041666666667e-05,
      "loss": 0.0034,
      "step": 6430
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 0.09526021778583527,
      "learning_rate": 4.8658333333333335e-05,
      "loss": 0.0037,
      "step": 6440
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.1867014616727829,
      "learning_rate": 4.865625e-05,
      "loss": 0.0025,
      "step": 6450
    },
    {
      "epoch": 0.21533333333333332,
      "grad_norm": 0.18549899756908417,
      "learning_rate": 4.8654166666666666e-05,
      "loss": 0.0046,
      "step": 6460
    },
    {
      "epoch": 0.21566666666666667,
      "grad_norm": 0.032176535576581955,
      "learning_rate": 4.865208333333334e-05,
      "loss": 0.0033,
      "step": 6470
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.21584106981754303,
      "learning_rate": 4.8650000000000003e-05,
      "loss": 0.0029,
      "step": 6480
    },
    {
      "epoch": 0.21633333333333332,
      "grad_norm": 0.016820112243294716,
      "learning_rate": 4.864791666666667e-05,
      "loss": 0.0047,
      "step": 6490
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.03233794495463371,
      "learning_rate": 4.8645833333333334e-05,
      "loss": 0.0036,
      "step": 6500
    },
    {
      "epoch": 0.217,
      "grad_norm": 0.3710208535194397,
      "learning_rate": 4.864375000000001e-05,
      "loss": 0.0049,
      "step": 6510
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 0.12456876039505005,
      "learning_rate": 4.8641666666666665e-05,
      "loss": 0.0036,
      "step": 6520
    },
    {
      "epoch": 0.21766666666666667,
      "grad_norm": 0.2474195957183838,
      "learning_rate": 4.863958333333334e-05,
      "loss": 0.0024,
      "step": 6530
    },
    {
      "epoch": 0.218,
      "grad_norm": 0.34014731645584106,
      "learning_rate": 4.86375e-05,
      "loss": 0.0033,
      "step": 6540
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.09573603421449661,
      "learning_rate": 4.863541666666667e-05,
      "loss": 0.0034,
      "step": 6550
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.5167742371559143,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0043,
      "step": 6560
    },
    {
      "epoch": 0.219,
      "grad_norm": 0.6864748001098633,
      "learning_rate": 4.863125e-05,
      "loss": 0.0027,
      "step": 6570
    },
    {
      "epoch": 0.21933333333333332,
      "grad_norm": 0.21707899868488312,
      "learning_rate": 4.862916666666667e-05,
      "loss": 0.0024,
      "step": 6580
    },
    {
      "epoch": 0.21966666666666668,
      "grad_norm": 0.6170142292976379,
      "learning_rate": 4.862708333333334e-05,
      "loss": 0.0051,
      "step": 6590
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.3161475956439972,
      "learning_rate": 4.8625e-05,
      "loss": 0.0044,
      "step": 6600
    },
    {
      "epoch": 0.22033333333333333,
      "grad_norm": 0.03426820784807205,
      "learning_rate": 4.862291666666667e-05,
      "loss": 0.0032,
      "step": 6610
    },
    {
      "epoch": 0.22066666666666668,
      "grad_norm": 0.369963675737381,
      "learning_rate": 4.862083333333334e-05,
      "loss": 0.0046,
      "step": 6620
    },
    {
      "epoch": 0.221,
      "grad_norm": 0.15537139773368835,
      "learning_rate": 4.861875e-05,
      "loss": 0.0025,
      "step": 6630
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.21735027432441711,
      "learning_rate": 4.861666666666667e-05,
      "loss": 0.0042,
      "step": 6640
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.09396164864301682,
      "learning_rate": 4.861458333333334e-05,
      "loss": 0.0028,
      "step": 6650
    },
    {
      "epoch": 0.222,
      "grad_norm": 0.01625402644276619,
      "learning_rate": 4.86125e-05,
      "loss": 0.0025,
      "step": 6660
    },
    {
      "epoch": 0.22233333333333333,
      "grad_norm": 0.2466832846403122,
      "learning_rate": 4.861041666666667e-05,
      "loss": 0.0032,
      "step": 6670
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 0.6321831941604614,
      "learning_rate": 4.8608333333333334e-05,
      "loss": 0.0028,
      "step": 6680
    },
    {
      "epoch": 0.223,
      "grad_norm": 0.1546303629875183,
      "learning_rate": 4.8606250000000006e-05,
      "loss": 0.0033,
      "step": 6690
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.06307491660118103,
      "learning_rate": 4.8604166666666664e-05,
      "loss": 0.0036,
      "step": 6700
    },
    {
      "epoch": 0.22366666666666668,
      "grad_norm": 0.15526199340820312,
      "learning_rate": 4.860208333333334e-05,
      "loss": 0.0043,
      "step": 6710
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.4550776779651642,
      "learning_rate": 4.86e-05,
      "loss": 0.0043,
      "step": 6720
    },
    {
      "epoch": 0.22433333333333333,
      "grad_norm": 0.03679225966334343,
      "learning_rate": 4.859791666666667e-05,
      "loss": 0.0053,
      "step": 6730
    },
    {
      "epoch": 0.22466666666666665,
      "grad_norm": 0.6472606658935547,
      "learning_rate": 4.859583333333333e-05,
      "loss": 0.0035,
      "step": 6740
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.05007196217775345,
      "learning_rate": 4.8593750000000005e-05,
      "loss": 0.0026,
      "step": 6750
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 0.41496437788009644,
      "learning_rate": 4.859166666666667e-05,
      "loss": 0.0027,
      "step": 6760
    },
    {
      "epoch": 0.22566666666666665,
      "grad_norm": 0.07154492288827896,
      "learning_rate": 4.8589583333333336e-05,
      "loss": 0.0027,
      "step": 6770
    },
    {
      "epoch": 0.226,
      "grad_norm": 0.15553539991378784,
      "learning_rate": 4.85875e-05,
      "loss": 0.0033,
      "step": 6780
    },
    {
      "epoch": 0.22633333333333333,
      "grad_norm": 0.5076056718826294,
      "learning_rate": 4.858541666666667e-05,
      "loss": 0.0045,
      "step": 6790
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.09446242451667786,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.0027,
      "step": 6800
    },
    {
      "epoch": 0.227,
      "grad_norm": 0.2782084047794342,
      "learning_rate": 4.858125e-05,
      "loss": 0.0042,
      "step": 6810
    },
    {
      "epoch": 0.22733333333333333,
      "grad_norm": 0.7092358469963074,
      "learning_rate": 4.857916666666667e-05,
      "loss": 0.0045,
      "step": 6820
    },
    {
      "epoch": 0.22766666666666666,
      "grad_norm": 0.40200695395469666,
      "learning_rate": 4.8577083333333336e-05,
      "loss": 0.0048,
      "step": 6830
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.21774417161941528,
      "learning_rate": 4.8575e-05,
      "loss": 0.0045,
      "step": 6840
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.5945032835006714,
      "learning_rate": 4.857291666666667e-05,
      "loss": 0.0043,
      "step": 6850
    },
    {
      "epoch": 0.22866666666666666,
      "grad_norm": 0.24727897346019745,
      "learning_rate": 4.857083333333334e-05,
      "loss": 0.0035,
      "step": 6860
    },
    {
      "epoch": 0.229,
      "grad_norm": 0.44328755140304565,
      "learning_rate": 4.8568750000000005e-05,
      "loss": 0.0037,
      "step": 6870
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.21680611371994019,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0034,
      "step": 6880
    },
    {
      "epoch": 0.22966666666666666,
      "grad_norm": 0.4006780982017517,
      "learning_rate": 4.8564583333333336e-05,
      "loss": 0.0043,
      "step": 6890
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.2213529497385025,
      "learning_rate": 4.85625e-05,
      "loss": 0.0047,
      "step": 6900
    },
    {
      "epoch": 0.23033333333333333,
      "grad_norm": 0.03395423665642738,
      "learning_rate": 4.856041666666667e-05,
      "loss": 0.0043,
      "step": 6910
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 0.21755187213420868,
      "learning_rate": 4.855833333333333e-05,
      "loss": 0.0037,
      "step": 6920
    },
    {
      "epoch": 0.231,
      "grad_norm": 0.37065887451171875,
      "learning_rate": 4.8556250000000005e-05,
      "loss": 0.0039,
      "step": 6930
    },
    {
      "epoch": 0.23133333333333334,
      "grad_norm": 0.18508172035217285,
      "learning_rate": 4.855416666666667e-05,
      "loss": 0.0039,
      "step": 6940
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.3312535881996155,
      "learning_rate": 4.8552083333333336e-05,
      "loss": 0.0029,
      "step": 6950
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.12349364906549454,
      "learning_rate": 4.855e-05,
      "loss": 0.003,
      "step": 6960
    },
    {
      "epoch": 0.23233333333333334,
      "grad_norm": 0.060390207916498184,
      "learning_rate": 4.854791666666667e-05,
      "loss": 0.0035,
      "step": 6970
    },
    {
      "epoch": 0.23266666666666666,
      "grad_norm": 0.4815284013748169,
      "learning_rate": 4.854583333333333e-05,
      "loss": 0.0028,
      "step": 6980
    },
    {
      "epoch": 0.233,
      "grad_norm": 0.12405718117952347,
      "learning_rate": 4.8543750000000004e-05,
      "loss": 0.0047,
      "step": 6990
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.3698442876338959,
      "learning_rate": 4.854166666666667e-05,
      "loss": 0.0035,
      "step": 7000
    },
    {
      "epoch": 0.23366666666666666,
      "grad_norm": 0.09356878697872162,
      "learning_rate": 4.8539583333333335e-05,
      "loss": 0.0029,
      "step": 7010
    },
    {
      "epoch": 0.234,
      "grad_norm": 0.26778122782707214,
      "learning_rate": 4.85375e-05,
      "loss": 0.0026,
      "step": 7020
    },
    {
      "epoch": 0.23433333333333334,
      "grad_norm": 0.011866696178913116,
      "learning_rate": 4.8535416666666666e-05,
      "loss": 0.0034,
      "step": 7030
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.6169664859771729,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0045,
      "step": 7040
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.7202953100204468,
      "learning_rate": 4.853125e-05,
      "loss": 0.0047,
      "step": 7050
    },
    {
      "epoch": 0.23533333333333334,
      "grad_norm": 0.03287898749113083,
      "learning_rate": 4.852916666666667e-05,
      "loss": 0.0026,
      "step": 7060
    },
    {
      "epoch": 0.23566666666666666,
      "grad_norm": 0.2733534276485443,
      "learning_rate": 4.8527083333333335e-05,
      "loss": 0.0041,
      "step": 7070
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.2267380654811859,
      "learning_rate": 4.8525e-05,
      "loss": 0.0046,
      "step": 7080
    },
    {
      "epoch": 0.23633333333333334,
      "grad_norm": 0.063200943171978,
      "learning_rate": 4.8522916666666666e-05,
      "loss": 0.0038,
      "step": 7090
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.27700990438461304,
      "learning_rate": 4.852083333333334e-05,
      "loss": 0.0028,
      "step": 7100
    },
    {
      "epoch": 0.237,
      "grad_norm": 0.4020049273967743,
      "learning_rate": 4.8518750000000004e-05,
      "loss": 0.0027,
      "step": 7110
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.368884801864624,
      "learning_rate": 4.851666666666667e-05,
      "loss": 0.0038,
      "step": 7120
    },
    {
      "epoch": 0.23766666666666666,
      "grad_norm": 0.09374751895666122,
      "learning_rate": 4.8514583333333335e-05,
      "loss": 0.0043,
      "step": 7130
    },
    {
      "epoch": 0.238,
      "grad_norm": 0.7703983783721924,
      "learning_rate": 4.85125e-05,
      "loss": 0.0034,
      "step": 7140
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.6157479286193848,
      "learning_rate": 4.851041666666667e-05,
      "loss": 0.0043,
      "step": 7150
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 0.15808439254760742,
      "learning_rate": 4.850833333333333e-05,
      "loss": 0.0022,
      "step": 7160
    },
    {
      "epoch": 0.239,
      "grad_norm": 0.3706829249858856,
      "learning_rate": 4.850625e-05,
      "loss": 0.0051,
      "step": 7170
    },
    {
      "epoch": 0.23933333333333334,
      "grad_norm": 0.08421351760625839,
      "learning_rate": 4.850416666666667e-05,
      "loss": 0.0043,
      "step": 7180
    },
    {
      "epoch": 0.23966666666666667,
      "grad_norm": 0.24783173203468323,
      "learning_rate": 4.8502083333333334e-05,
      "loss": 0.0036,
      "step": 7190
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.3443240225315094,
      "learning_rate": 4.85e-05,
      "loss": 0.0045,
      "step": 7200
    },
    {
      "epoch": 0.24033333333333334,
      "grad_norm": 0.1857728660106659,
      "learning_rate": 4.849791666666667e-05,
      "loss": 0.0047,
      "step": 7210
    },
    {
      "epoch": 0.24066666666666667,
      "grad_norm": 0.12361942231655121,
      "learning_rate": 4.849583333333334e-05,
      "loss": 0.0063,
      "step": 7220
    },
    {
      "epoch": 0.241,
      "grad_norm": 0.28962811827659607,
      "learning_rate": 4.849375e-05,
      "loss": 0.0041,
      "step": 7230
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 0.062354784458875656,
      "learning_rate": 4.849166666666667e-05,
      "loss": 0.005,
      "step": 7240
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.1260736584663391,
      "learning_rate": 4.848958333333334e-05,
      "loss": 0.0024,
      "step": 7250
    },
    {
      "epoch": 0.242,
      "grad_norm": 0.43149906396865845,
      "learning_rate": 4.84875e-05,
      "loss": 0.0048,
      "step": 7260
    },
    {
      "epoch": 0.24233333333333335,
      "grad_norm": 0.4003959596157074,
      "learning_rate": 4.8485416666666665e-05,
      "loss": 0.0031,
      "step": 7270
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.4632018506526947,
      "learning_rate": 4.848333333333334e-05,
      "loss": 0.0038,
      "step": 7280
    },
    {
      "epoch": 0.243,
      "grad_norm": 0.3400718867778778,
      "learning_rate": 4.848125e-05,
      "loss": 0.0036,
      "step": 7290
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.35480090975761414,
      "learning_rate": 4.847916666666667e-05,
      "loss": 0.0039,
      "step": 7300
    },
    {
      "epoch": 0.24366666666666667,
      "grad_norm": 0.32649654150009155,
      "learning_rate": 4.8477083333333334e-05,
      "loss": 0.0045,
      "step": 7310
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.4307471215724945,
      "learning_rate": 4.8475000000000006e-05,
      "loss": 0.0044,
      "step": 7320
    },
    {
      "epoch": 0.24433333333333335,
      "grad_norm": 0.08444105833768845,
      "learning_rate": 4.8472916666666665e-05,
      "loss": 0.0025,
      "step": 7330
    },
    {
      "epoch": 0.24466666666666667,
      "grad_norm": 0.5551208257675171,
      "learning_rate": 4.847083333333334e-05,
      "loss": 0.0038,
      "step": 7340
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.1251395046710968,
      "learning_rate": 4.846875e-05,
      "loss": 0.0026,
      "step": 7350
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.5235596299171448,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0024,
      "step": 7360
    },
    {
      "epoch": 0.24566666666666667,
      "grad_norm": 0.5422682166099548,
      "learning_rate": 4.8464583333333333e-05,
      "loss": 0.0038,
      "step": 7370
    },
    {
      "epoch": 0.246,
      "grad_norm": 0.48789331316947937,
      "learning_rate": 4.84625e-05,
      "loss": 0.0025,
      "step": 7380
    },
    {
      "epoch": 0.24633333333333332,
      "grad_norm": 0.18444111943244934,
      "learning_rate": 4.846041666666667e-05,
      "loss": 0.004,
      "step": 7390
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.30948689579963684,
      "learning_rate": 4.845833333333334e-05,
      "loss": 0.0039,
      "step": 7400
    },
    {
      "epoch": 0.247,
      "grad_norm": 0.15677794814109802,
      "learning_rate": 4.845625e-05,
      "loss": 0.0034,
      "step": 7410
    },
    {
      "epoch": 0.24733333333333332,
      "grad_norm": 0.15461526811122894,
      "learning_rate": 4.845416666666667e-05,
      "loss": 0.0035,
      "step": 7420
    },
    {
      "epoch": 0.24766666666666667,
      "grad_norm": 0.43116074800491333,
      "learning_rate": 4.845208333333334e-05,
      "loss": 0.0044,
      "step": 7430
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.1690138876438141,
      "learning_rate": 4.845e-05,
      "loss": 0.003,
      "step": 7440
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.18505112826824188,
      "learning_rate": 4.844791666666667e-05,
      "loss": 0.0031,
      "step": 7450
    },
    {
      "epoch": 0.24866666666666667,
      "grad_norm": 0.30807965993881226,
      "learning_rate": 4.8445833333333336e-05,
      "loss": 0.0044,
      "step": 7460
    },
    {
      "epoch": 0.249,
      "grad_norm": 0.37309831380844116,
      "learning_rate": 4.844375e-05,
      "loss": 0.0043,
      "step": 7470
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 0.05795200541615486,
      "learning_rate": 4.844166666666667e-05,
      "loss": 0.0032,
      "step": 7480
    },
    {
      "epoch": 0.24966666666666668,
      "grad_norm": 0.4612272381782532,
      "learning_rate": 4.843958333333334e-05,
      "loss": 0.0031,
      "step": 7490
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.3197656273841858,
      "learning_rate": 4.8437500000000005e-05,
      "loss": 0.0037,
      "step": 7500
    },
    {
      "epoch": 0.25033333333333335,
      "grad_norm": 0.612296462059021,
      "learning_rate": 4.8435416666666664e-05,
      "loss": 0.0045,
      "step": 7510
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.3685861825942993,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.0043,
      "step": 7520
    },
    {
      "epoch": 0.251,
      "grad_norm": 0.09354931861162186,
      "learning_rate": 4.843125e-05,
      "loss": 0.0046,
      "step": 7530
    },
    {
      "epoch": 0.25133333333333335,
      "grad_norm": 0.4301563799381256,
      "learning_rate": 4.842916666666667e-05,
      "loss": 0.0044,
      "step": 7540
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.432742714881897,
      "learning_rate": 4.842708333333333e-05,
      "loss": 0.0024,
      "step": 7550
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.5223371386528015,
      "learning_rate": 4.8425000000000005e-05,
      "loss": 0.0041,
      "step": 7560
    },
    {
      "epoch": 0.25233333333333335,
      "grad_norm": 0.012110666371881962,
      "learning_rate": 4.842291666666667e-05,
      "loss": 0.0022,
      "step": 7570
    },
    {
      "epoch": 0.25266666666666665,
      "grad_norm": 0.2519747316837311,
      "learning_rate": 4.8420833333333336e-05,
      "loss": 0.0032,
      "step": 7580
    },
    {
      "epoch": 0.253,
      "grad_norm": 0.21515516936779022,
      "learning_rate": 4.841875e-05,
      "loss": 0.0039,
      "step": 7590
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.40170446038246155,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.0018,
      "step": 7600
    },
    {
      "epoch": 0.25366666666666665,
      "grad_norm": 0.5838482975959778,
      "learning_rate": 4.841458333333333e-05,
      "loss": 0.0039,
      "step": 7610
    },
    {
      "epoch": 0.254,
      "grad_norm": 0.4301324188709259,
      "learning_rate": 4.8412500000000004e-05,
      "loss": 0.0037,
      "step": 7620
    },
    {
      "epoch": 0.25433333333333336,
      "grad_norm": 0.33766448497772217,
      "learning_rate": 4.841041666666667e-05,
      "loss": 0.003,
      "step": 7630
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 0.7364358305931091,
      "learning_rate": 4.8408333333333335e-05,
      "loss": 0.0032,
      "step": 7640
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.09286294132471085,
      "learning_rate": 4.840625e-05,
      "loss": 0.0043,
      "step": 7650
    },
    {
      "epoch": 0.25533333333333336,
      "grad_norm": 0.15502730011940002,
      "learning_rate": 4.8404166666666666e-05,
      "loss": 0.0028,
      "step": 7660
    },
    {
      "epoch": 0.25566666666666665,
      "grad_norm": 0.4614414572715759,
      "learning_rate": 4.840208333333334e-05,
      "loss": 0.0039,
      "step": 7670
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.615116536617279,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0031,
      "step": 7680
    },
    {
      "epoch": 0.25633333333333336,
      "grad_norm": 0.21533501148223877,
      "learning_rate": 4.839791666666667e-05,
      "loss": 0.0037,
      "step": 7690
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.21197354793548584,
      "learning_rate": 4.8395833333333335e-05,
      "loss": 0.0039,
      "step": 7700
    },
    {
      "epoch": 0.257,
      "grad_norm": 0.27969890832901,
      "learning_rate": 4.839375000000001e-05,
      "loss": 0.0034,
      "step": 7710
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 0.38616812229156494,
      "learning_rate": 4.8391666666666666e-05,
      "loss": 0.0039,
      "step": 7720
    },
    {
      "epoch": 0.25766666666666665,
      "grad_norm": 0.3004681468009949,
      "learning_rate": 4.838958333333334e-05,
      "loss": 0.0043,
      "step": 7730
    },
    {
      "epoch": 0.258,
      "grad_norm": 0.12613718211650848,
      "learning_rate": 4.8387500000000004e-05,
      "loss": 0.0033,
      "step": 7740
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.017251355573534966,
      "learning_rate": 4.838541666666667e-05,
      "loss": 0.003,
      "step": 7750
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.3045715093612671,
      "learning_rate": 4.8383333333333335e-05,
      "loss": 0.0026,
      "step": 7760
    },
    {
      "epoch": 0.259,
      "grad_norm": 0.21800190210342407,
      "learning_rate": 4.838125e-05,
      "loss": 0.0031,
      "step": 7770
    },
    {
      "epoch": 0.25933333333333336,
      "grad_norm": 0.6466220617294312,
      "learning_rate": 4.837916666666667e-05,
      "loss": 0.0034,
      "step": 7780
    },
    {
      "epoch": 0.25966666666666666,
      "grad_norm": 0.15477098524570465,
      "learning_rate": 4.837708333333333e-05,
      "loss": 0.0034,
      "step": 7790
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.35405778884887695,
      "learning_rate": 4.8375000000000004e-05,
      "loss": 0.0046,
      "step": 7800
    },
    {
      "epoch": 0.26033333333333336,
      "grad_norm": 0.42182642221450806,
      "learning_rate": 4.837291666666667e-05,
      "loss": 0.0031,
      "step": 7810
    },
    {
      "epoch": 0.26066666666666666,
      "grad_norm": 0.02606537751853466,
      "learning_rate": 4.8370833333333335e-05,
      "loss": 0.0041,
      "step": 7820
    },
    {
      "epoch": 0.261,
      "grad_norm": 0.7550356388092041,
      "learning_rate": 4.836875e-05,
      "loss": 0.0033,
      "step": 7830
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.3125568628311157,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.0046,
      "step": 7840
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.06350064277648926,
      "learning_rate": 4.836458333333334e-05,
      "loss": 0.0029,
      "step": 7850
    },
    {
      "epoch": 0.262,
      "grad_norm": 0.09330207109451294,
      "learning_rate": 4.83625e-05,
      "loss": 0.0047,
      "step": 7860
    },
    {
      "epoch": 0.2623333333333333,
      "grad_norm": 0.29295939207077026,
      "learning_rate": 4.836041666666667e-05,
      "loss": 0.0033,
      "step": 7870
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 0.18448378145694733,
      "learning_rate": 4.8358333333333334e-05,
      "loss": 0.0046,
      "step": 7880
    },
    {
      "epoch": 0.263,
      "grad_norm": 0.40087804198265076,
      "learning_rate": 4.835625e-05,
      "loss": 0.0045,
      "step": 7890
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.3065192997455597,
      "learning_rate": 4.8354166666666665e-05,
      "loss": 0.0013,
      "step": 7900
    },
    {
      "epoch": 0.26366666666666666,
      "grad_norm": 0.43348222970962524,
      "learning_rate": 4.835208333333334e-05,
      "loss": 0.0033,
      "step": 7910
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.15824368596076965,
      "learning_rate": 4.835e-05,
      "loss": 0.0048,
      "step": 7920
    },
    {
      "epoch": 0.2643333333333333,
      "grad_norm": 0.3082501292228699,
      "learning_rate": 4.834791666666667e-05,
      "loss": 0.0047,
      "step": 7930
    },
    {
      "epoch": 0.26466666666666666,
      "grad_norm": 0.1232500746846199,
      "learning_rate": 4.8345833333333334e-05,
      "loss": 0.003,
      "step": 7940
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.32567161321640015,
      "learning_rate": 4.8343750000000006e-05,
      "loss": 0.0036,
      "step": 7950
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 0.15477792918682098,
      "learning_rate": 4.834166666666667e-05,
      "loss": 0.0026,
      "step": 7960
    },
    {
      "epoch": 0.26566666666666666,
      "grad_norm": 0.15390551090240479,
      "learning_rate": 4.833958333333334e-05,
      "loss": 0.0039,
      "step": 7970
    },
    {
      "epoch": 0.266,
      "grad_norm": 0.30709439516067505,
      "learning_rate": 4.83375e-05,
      "loss": 0.0029,
      "step": 7980
    },
    {
      "epoch": 0.2663333333333333,
      "grad_norm": 0.12407775223255157,
      "learning_rate": 4.833541666666667e-05,
      "loss": 0.0037,
      "step": 7990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.3071168065071106,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0027,
      "step": 8000
    },
    {
      "epoch": 0.267,
      "grad_norm": 0.09431207925081253,
      "learning_rate": 4.833125e-05,
      "loss": 0.0035,
      "step": 8010
    },
    {
      "epoch": 0.2673333333333333,
      "grad_norm": 0.4611630141735077,
      "learning_rate": 4.832916666666667e-05,
      "loss": 0.0039,
      "step": 8020
    },
    {
      "epoch": 0.26766666666666666,
      "grad_norm": 0.49165743589401245,
      "learning_rate": 4.832708333333334e-05,
      "loss": 0.0032,
      "step": 8030
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.49203625321388245,
      "learning_rate": 4.8325e-05,
      "loss": 0.0051,
      "step": 8040
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.39940890669822693,
      "learning_rate": 4.832291666666667e-05,
      "loss": 0.0041,
      "step": 8050
    },
    {
      "epoch": 0.26866666666666666,
      "grad_norm": 0.15458643436431885,
      "learning_rate": 4.832083333333334e-05,
      "loss": 0.003,
      "step": 8060
    },
    {
      "epoch": 0.269,
      "grad_norm": 0.24649859964847565,
      "learning_rate": 4.831875e-05,
      "loss": 0.0043,
      "step": 8070
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.17861397564411163,
      "learning_rate": 4.831666666666667e-05,
      "loss": 0.0033,
      "step": 8080
    },
    {
      "epoch": 0.26966666666666667,
      "grad_norm": 0.2258090376853943,
      "learning_rate": 4.8314583333333337e-05,
      "loss": 0.003,
      "step": 8090
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.2651680111885071,
      "learning_rate": 4.83125e-05,
      "loss": 0.0028,
      "step": 8100
    },
    {
      "epoch": 0.2703333333333333,
      "grad_norm": 0.3859097957611084,
      "learning_rate": 4.831041666666667e-05,
      "loss": 0.0028,
      "step": 8110
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 0.21485278010368347,
      "learning_rate": 4.830833333333333e-05,
      "loss": 0.0038,
      "step": 8120
    },
    {
      "epoch": 0.271,
      "grad_norm": 0.12347877770662308,
      "learning_rate": 4.8306250000000005e-05,
      "loss": 0.0033,
      "step": 8130
    },
    {
      "epoch": 0.2713333333333333,
      "grad_norm": 0.46046754717826843,
      "learning_rate": 4.8304166666666664e-05,
      "loss": 0.0036,
      "step": 8140
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.03310071676969528,
      "learning_rate": 4.8302083333333336e-05,
      "loss": 0.0034,
      "step": 8150
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.15387272834777832,
      "learning_rate": 4.83e-05,
      "loss": 0.0029,
      "step": 8160
    },
    {
      "epoch": 0.2723333333333333,
      "grad_norm": 0.3040145933628082,
      "learning_rate": 4.829791666666667e-05,
      "loss": 0.0034,
      "step": 8170
    },
    {
      "epoch": 0.27266666666666667,
      "grad_norm": 0.6136857867240906,
      "learning_rate": 4.829583333333333e-05,
      "loss": 0.0033,
      "step": 8180
    },
    {
      "epoch": 0.273,
      "grad_norm": 0.12397810071706772,
      "learning_rate": 4.8293750000000005e-05,
      "loss": 0.0041,
      "step": 8190
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.6440765261650085,
      "learning_rate": 4.829166666666667e-05,
      "loss": 0.0038,
      "step": 8200
    },
    {
      "epoch": 0.27366666666666667,
      "grad_norm": 0.21476073563098907,
      "learning_rate": 4.8289583333333336e-05,
      "loss": 0.0031,
      "step": 8210
    },
    {
      "epoch": 0.274,
      "grad_norm": 0.7122653126716614,
      "learning_rate": 4.82875e-05,
      "loss": 0.0045,
      "step": 8220
    },
    {
      "epoch": 0.2743333333333333,
      "grad_norm": 0.12315531075000763,
      "learning_rate": 4.828541666666667e-05,
      "loss": 0.0046,
      "step": 8230
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.12447566539049149,
      "learning_rate": 4.828333333333334e-05,
      "loss": 0.0034,
      "step": 8240
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.36823713779449463,
      "learning_rate": 4.828125e-05,
      "loss": 0.0035,
      "step": 8250
    },
    {
      "epoch": 0.2753333333333333,
      "grad_norm": 0.15476198494434357,
      "learning_rate": 4.827916666666667e-05,
      "loss": 0.0041,
      "step": 8260
    },
    {
      "epoch": 0.27566666666666667,
      "grad_norm": 0.01011236384510994,
      "learning_rate": 4.8277083333333336e-05,
      "loss": 0.0043,
      "step": 8270
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.032703183591365814,
      "learning_rate": 4.8275e-05,
      "loss": 0.0035,
      "step": 8280
    },
    {
      "epoch": 0.2763333333333333,
      "grad_norm": 0.33849552273750305,
      "learning_rate": 4.8272916666666667e-05,
      "loss": 0.0035,
      "step": 8290
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.257605642080307,
      "learning_rate": 4.827083333333334e-05,
      "loss": 0.0046,
      "step": 8300
    },
    {
      "epoch": 0.277,
      "grad_norm": 0.2533016502857208,
      "learning_rate": 4.8268750000000004e-05,
      "loss": 0.0046,
      "step": 8310
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.3374301791191101,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0042,
      "step": 8320
    },
    {
      "epoch": 0.2776666666666667,
      "grad_norm": 0.09285857528448105,
      "learning_rate": 4.8264583333333335e-05,
      "loss": 0.0038,
      "step": 8330
    },
    {
      "epoch": 0.278,
      "grad_norm": 0.12524349987506866,
      "learning_rate": 4.826250000000001e-05,
      "loss": 0.003,
      "step": 8340
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.42924830317497253,
      "learning_rate": 4.8260416666666666e-05,
      "loss": 0.0021,
      "step": 8350
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 0.0631265714764595,
      "learning_rate": 4.825833333333333e-05,
      "loss": 0.0042,
      "step": 8360
    },
    {
      "epoch": 0.279,
      "grad_norm": 0.03400371968746185,
      "learning_rate": 4.8256250000000004e-05,
      "loss": 0.0048,
      "step": 8370
    },
    {
      "epoch": 0.2793333333333333,
      "grad_norm": 0.09288769960403442,
      "learning_rate": 4.825416666666667e-05,
      "loss": 0.0031,
      "step": 8380
    },
    {
      "epoch": 0.2796666666666667,
      "grad_norm": 0.1542155146598816,
      "learning_rate": 4.8252083333333335e-05,
      "loss": 0.0049,
      "step": 8390
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.18644832074642181,
      "learning_rate": 4.825e-05,
      "loss": 0.0026,
      "step": 8400
    },
    {
      "epoch": 0.2803333333333333,
      "grad_norm": 0.09227299690246582,
      "learning_rate": 4.824791666666667e-05,
      "loss": 0.0032,
      "step": 8410
    },
    {
      "epoch": 0.2806666666666667,
      "grad_norm": 0.27663061022758484,
      "learning_rate": 4.824583333333333e-05,
      "loss": 0.0027,
      "step": 8420
    },
    {
      "epoch": 0.281,
      "grad_norm": 0.4604087769985199,
      "learning_rate": 4.8243750000000004e-05,
      "loss": 0.0027,
      "step": 8430
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 0.7970209121704102,
      "learning_rate": 4.824166666666667e-05,
      "loss": 0.0039,
      "step": 8440
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.30640918016433716,
      "learning_rate": 4.8239583333333335e-05,
      "loss": 0.0037,
      "step": 8450
    },
    {
      "epoch": 0.282,
      "grad_norm": 0.15660011768341064,
      "learning_rate": 4.82375e-05,
      "loss": 0.0036,
      "step": 8460
    },
    {
      "epoch": 0.2823333333333333,
      "grad_norm": 0.15401405096054077,
      "learning_rate": 4.8235416666666666e-05,
      "loss": 0.0021,
      "step": 8470
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.5024912357330322,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.0032,
      "step": 8480
    },
    {
      "epoch": 0.283,
      "grad_norm": 0.3369104862213135,
      "learning_rate": 4.823125e-05,
      "loss": 0.003,
      "step": 8490
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.3984307050704956,
      "learning_rate": 4.822916666666667e-05,
      "loss": 0.0032,
      "step": 8500
    },
    {
      "epoch": 0.2836666666666667,
      "grad_norm": 0.1537494957447052,
      "learning_rate": 4.8227083333333334e-05,
      "loss": 0.0033,
      "step": 8510
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.033548228442668915,
      "learning_rate": 4.822500000000001e-05,
      "loss": 0.0022,
      "step": 8520
    },
    {
      "epoch": 0.2843333333333333,
      "grad_norm": 0.2759653329849243,
      "learning_rate": 4.8222916666666665e-05,
      "loss": 0.0028,
      "step": 8530
    },
    {
      "epoch": 0.2846666666666667,
      "grad_norm": 0.642954409122467,
      "learning_rate": 4.822083333333334e-05,
      "loss": 0.0024,
      "step": 8540
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.49472007155418396,
      "learning_rate": 4.821875e-05,
      "loss": 0.0032,
      "step": 8550
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.21525117754936218,
      "learning_rate": 4.821666666666667e-05,
      "loss": 0.0029,
      "step": 8560
    },
    {
      "epoch": 0.2856666666666667,
      "grad_norm": 0.6186385154724121,
      "learning_rate": 4.8214583333333334e-05,
      "loss": 0.004,
      "step": 8570
    },
    {
      "epoch": 0.286,
      "grad_norm": 0.6780158281326294,
      "learning_rate": 4.8212500000000006e-05,
      "loss": 0.0026,
      "step": 8580
    },
    {
      "epoch": 0.28633333333333333,
      "grad_norm": 0.3961319029331207,
      "learning_rate": 4.821041666666667e-05,
      "loss": 0.0034,
      "step": 8590
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.5211306810379028,
      "learning_rate": 4.820833333333333e-05,
      "loss": 0.0028,
      "step": 8600
    },
    {
      "epoch": 0.287,
      "grad_norm": 0.2549474835395813,
      "learning_rate": 4.820625e-05,
      "loss": 0.0034,
      "step": 8610
    },
    {
      "epoch": 0.28733333333333333,
      "grad_norm": 0.3697262406349182,
      "learning_rate": 4.820416666666667e-05,
      "loss": 0.0054,
      "step": 8620
    },
    {
      "epoch": 0.2876666666666667,
      "grad_norm": 0.5627744197845459,
      "learning_rate": 4.8202083333333334e-05,
      "loss": 0.0028,
      "step": 8630
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.39373818039894104,
      "learning_rate": 4.82e-05,
      "loss": 0.0022,
      "step": 8640
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.3682829737663269,
      "learning_rate": 4.819791666666667e-05,
      "loss": 0.0032,
      "step": 8650
    },
    {
      "epoch": 0.2886666666666667,
      "grad_norm": 0.033654507249593735,
      "learning_rate": 4.819583333333334e-05,
      "loss": 0.0029,
      "step": 8660
    },
    {
      "epoch": 0.289,
      "grad_norm": 0.01687687449157238,
      "learning_rate": 4.819375e-05,
      "loss": 0.0033,
      "step": 8670
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 0.09253077954053879,
      "learning_rate": 4.819166666666667e-05,
      "loss": 0.0033,
      "step": 8680
    },
    {
      "epoch": 0.2896666666666667,
      "grad_norm": 0.39046987891197205,
      "learning_rate": 4.818958333333334e-05,
      "loss": 0.0024,
      "step": 8690
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.49802133440971375,
      "learning_rate": 4.81875e-05,
      "loss": 0.0031,
      "step": 8700
    },
    {
      "epoch": 0.29033333333333333,
      "grad_norm": 0.306262344121933,
      "learning_rate": 4.8185416666666664e-05,
      "loss": 0.004,
      "step": 8710
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.14695937931537628,
      "learning_rate": 4.818333333333334e-05,
      "loss": 0.0049,
      "step": 8720
    },
    {
      "epoch": 0.291,
      "grad_norm": 0.12353987246751785,
      "learning_rate": 4.818125e-05,
      "loss": 0.0033,
      "step": 8730
    },
    {
      "epoch": 0.29133333333333333,
      "grad_norm": 0.459146648645401,
      "learning_rate": 4.817916666666667e-05,
      "loss": 0.0045,
      "step": 8740
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.7038814425468445,
      "learning_rate": 4.817708333333333e-05,
      "loss": 0.0033,
      "step": 8750
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.30640462040901184,
      "learning_rate": 4.8175000000000005e-05,
      "loss": 0.0033,
      "step": 8760
    },
    {
      "epoch": 0.29233333333333333,
      "grad_norm": 0.09238371253013611,
      "learning_rate": 4.817291666666667e-05,
      "loss": 0.0032,
      "step": 8770
    },
    {
      "epoch": 0.2926666666666667,
      "grad_norm": 0.09281942248344421,
      "learning_rate": 4.8170833333333336e-05,
      "loss": 0.0041,
      "step": 8780
    },
    {
      "epoch": 0.293,
      "grad_norm": 0.01686893403530121,
      "learning_rate": 4.816875e-05,
      "loss": 0.0047,
      "step": 8790
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.03265892714262009,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.0041,
      "step": 8800
    },
    {
      "epoch": 0.2936666666666667,
      "grad_norm": 0.18386660516262054,
      "learning_rate": 4.816458333333333e-05,
      "loss": 0.0045,
      "step": 8810
    },
    {
      "epoch": 0.294,
      "grad_norm": 0.5509389042854309,
      "learning_rate": 4.8162500000000005e-05,
      "loss": 0.0046,
      "step": 8820
    },
    {
      "epoch": 0.29433333333333334,
      "grad_norm": 0.03548618033528328,
      "learning_rate": 4.816041666666667e-05,
      "loss": 0.0045,
      "step": 8830
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 0.27540501952171326,
      "learning_rate": 4.8158333333333336e-05,
      "loss": 0.0027,
      "step": 8840
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.3063960373401642,
      "learning_rate": 4.815625e-05,
      "loss": 0.0027,
      "step": 8850
    },
    {
      "epoch": 0.29533333333333334,
      "grad_norm": 0.3467780351638794,
      "learning_rate": 4.815416666666667e-05,
      "loss": 0.0035,
      "step": 8860
    },
    {
      "epoch": 0.2956666666666667,
      "grad_norm": 0.12354744225740433,
      "learning_rate": 4.815208333333334e-05,
      "loss": 0.0044,
      "step": 8870
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.061956074088811874,
      "learning_rate": 4.815e-05,
      "loss": 0.0039,
      "step": 8880
    },
    {
      "epoch": 0.29633333333333334,
      "grad_norm": 0.1414644569158554,
      "learning_rate": 4.814791666666667e-05,
      "loss": 0.0035,
      "step": 8890
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.09300798922777176,
      "learning_rate": 4.8145833333333336e-05,
      "loss": 0.0026,
      "step": 8900
    },
    {
      "epoch": 0.297,
      "grad_norm": 0.4896831810474396,
      "learning_rate": 4.814375e-05,
      "loss": 0.0038,
      "step": 8910
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 0.012139419093728065,
      "learning_rate": 4.814166666666667e-05,
      "loss": 0.0038,
      "step": 8920
    },
    {
      "epoch": 0.2976666666666667,
      "grad_norm": 0.367078959941864,
      "learning_rate": 4.813958333333334e-05,
      "loss": 0.0046,
      "step": 8930
    },
    {
      "epoch": 0.298,
      "grad_norm": 0.27497127652168274,
      "learning_rate": 4.8137500000000005e-05,
      "loss": 0.003,
      "step": 8940
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.15353474020957947,
      "learning_rate": 4.813541666666667e-05,
      "loss": 0.004,
      "step": 8950
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.01160428300499916,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.004,
      "step": 8960
    },
    {
      "epoch": 0.299,
      "grad_norm": 0.15306586027145386,
      "learning_rate": 4.813125e-05,
      "loss": 0.0035,
      "step": 8970
    },
    {
      "epoch": 0.29933333333333334,
      "grad_norm": 0.2659977078437805,
      "learning_rate": 4.8129166666666667e-05,
      "loss": 0.0039,
      "step": 8980
    },
    {
      "epoch": 0.2996666666666667,
      "grad_norm": 0.18349575996398926,
      "learning_rate": 4.812708333333333e-05,
      "loss": 0.003,
      "step": 8990
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.09173692762851715,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 0.0022,
      "step": 9000
    },
    {
      "epoch": 0.30033333333333334,
      "grad_norm": 0.396696001291275,
      "learning_rate": 4.812291666666667e-05,
      "loss": 0.0028,
      "step": 9010
    },
    {
      "epoch": 0.3006666666666667,
      "grad_norm": 0.32288384437561035,
      "learning_rate": 4.8120833333333335e-05,
      "loss": 0.0033,
      "step": 9020
    },
    {
      "epoch": 0.301,
      "grad_norm": 0.4882791042327881,
      "learning_rate": 4.811875e-05,
      "loss": 0.0035,
      "step": 9030
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.15295612812042236,
      "learning_rate": 4.811666666666667e-05,
      "loss": 0.0032,
      "step": 9040
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.3973299562931061,
      "learning_rate": 4.811458333333334e-05,
      "loss": 0.0038,
      "step": 9050
    },
    {
      "epoch": 0.302,
      "grad_norm": 0.1254555583000183,
      "learning_rate": 4.8112500000000004e-05,
      "loss": 0.0022,
      "step": 9060
    },
    {
      "epoch": 0.30233333333333334,
      "grad_norm": 0.03275720775127411,
      "learning_rate": 4.811041666666667e-05,
      "loss": 0.0025,
      "step": 9070
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 0.15392036736011505,
      "learning_rate": 4.8108333333333335e-05,
      "loss": 0.0041,
      "step": 9080
    },
    {
      "epoch": 0.303,
      "grad_norm": 0.5190077424049377,
      "learning_rate": 4.810625e-05,
      "loss": 0.0024,
      "step": 9090
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.033435527235269547,
      "learning_rate": 4.8104166666666666e-05,
      "loss": 0.003,
      "step": 9100
    },
    {
      "epoch": 0.30366666666666664,
      "grad_norm": 0.33407941460609436,
      "learning_rate": 4.810208333333334e-05,
      "loss": 0.0041,
      "step": 9110
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.3359448313713074,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0028,
      "step": 9120
    },
    {
      "epoch": 0.30433333333333334,
      "grad_norm": 0.12307910621166229,
      "learning_rate": 4.809791666666667e-05,
      "loss": 0.0025,
      "step": 9130
    },
    {
      "epoch": 0.30466666666666664,
      "grad_norm": 0.4611504077911377,
      "learning_rate": 4.8095833333333335e-05,
      "loss": 0.0038,
      "step": 9140
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.18354758620262146,
      "learning_rate": 4.809375000000001e-05,
      "loss": 0.0024,
      "step": 9150
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 0.18330717086791992,
      "learning_rate": 4.8091666666666666e-05,
      "loss": 0.0037,
      "step": 9160
    },
    {
      "epoch": 0.30566666666666664,
      "grad_norm": 0.0628703236579895,
      "learning_rate": 4.808958333333334e-05,
      "loss": 0.0025,
      "step": 9170
    },
    {
      "epoch": 0.306,
      "grad_norm": 0.5205921530723572,
      "learning_rate": 4.80875e-05,
      "loss": 0.0041,
      "step": 9180
    },
    {
      "epoch": 0.30633333333333335,
      "grad_norm": 0.2745791971683502,
      "learning_rate": 4.808541666666667e-05,
      "loss": 0.0026,
      "step": 9190
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.03225547447800636,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.0022,
      "step": 9200
    },
    {
      "epoch": 0.307,
      "grad_norm": 0.45886799693107605,
      "learning_rate": 4.808125e-05,
      "loss": 0.004,
      "step": 9210
    },
    {
      "epoch": 0.30733333333333335,
      "grad_norm": 0.1655758172273636,
      "learning_rate": 4.807916666666667e-05,
      "loss": 0.0029,
      "step": 9220
    },
    {
      "epoch": 0.30766666666666664,
      "grad_norm": 0.15140345692634583,
      "learning_rate": 4.807708333333333e-05,
      "loss": 0.0044,
      "step": 9230
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.153053417801857,
      "learning_rate": 4.8075e-05,
      "loss": 0.0035,
      "step": 9240
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.5194817781448364,
      "learning_rate": 4.807291666666667e-05,
      "loss": 0.0024,
      "step": 9250
    },
    {
      "epoch": 0.30866666666666664,
      "grad_norm": 0.18439944088459015,
      "learning_rate": 4.8070833333333334e-05,
      "loss": 0.0045,
      "step": 9260
    },
    {
      "epoch": 0.309,
      "grad_norm": 0.1831139773130417,
      "learning_rate": 4.806875e-05,
      "loss": 0.0027,
      "step": 9270
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.3618007302284241,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0028,
      "step": 9280
    },
    {
      "epoch": 0.30966666666666665,
      "grad_norm": 0.3059506416320801,
      "learning_rate": 4.806458333333334e-05,
      "loss": 0.0031,
      "step": 9290
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.3443976640701294,
      "learning_rate": 4.80625e-05,
      "loss": 0.003,
      "step": 9300
    },
    {
      "epoch": 0.31033333333333335,
      "grad_norm": 0.18352703750133514,
      "learning_rate": 4.806041666666667e-05,
      "loss": 0.0026,
      "step": 9310
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 0.1840549260377884,
      "learning_rate": 4.8058333333333334e-05,
      "loss": 0.0024,
      "step": 9320
    },
    {
      "epoch": 0.311,
      "grad_norm": 0.03321978822350502,
      "learning_rate": 4.8056250000000006e-05,
      "loss": 0.004,
      "step": 9330
    },
    {
      "epoch": 0.31133333333333335,
      "grad_norm": 0.21445104479789734,
      "learning_rate": 4.8054166666666665e-05,
      "loss": 0.0027,
      "step": 9340
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.12263283878564835,
      "learning_rate": 4.805208333333334e-05,
      "loss": 0.0046,
      "step": 9350
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.4887918531894684,
      "learning_rate": 4.805e-05,
      "loss": 0.0051,
      "step": 9360
    },
    {
      "epoch": 0.31233333333333335,
      "grad_norm": 0.18452559411525726,
      "learning_rate": 4.804791666666667e-05,
      "loss": 0.0032,
      "step": 9370
    },
    {
      "epoch": 0.31266666666666665,
      "grad_norm": 0.27477267384529114,
      "learning_rate": 4.8045833333333333e-05,
      "loss": 0.0048,
      "step": 9380
    },
    {
      "epoch": 0.313,
      "grad_norm": 0.1834472268819809,
      "learning_rate": 4.8043750000000006e-05,
      "loss": 0.0039,
      "step": 9390
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.39776843786239624,
      "learning_rate": 4.804166666666667e-05,
      "loss": 0.0021,
      "step": 9400
    },
    {
      "epoch": 0.31366666666666665,
      "grad_norm": 0.24463097751140594,
      "learning_rate": 4.803958333333334e-05,
      "loss": 0.0038,
      "step": 9410
    },
    {
      "epoch": 0.314,
      "grad_norm": 0.21448127925395966,
      "learning_rate": 4.80375e-05,
      "loss": 0.0027,
      "step": 9420
    },
    {
      "epoch": 0.31433333333333335,
      "grad_norm": 0.06187071278691292,
      "learning_rate": 4.8035416666666674e-05,
      "loss": 0.0033,
      "step": 9430
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.035541072487831116,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.003,
      "step": 9440
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.3659660220146179,
      "learning_rate": 4.803125e-05,
      "loss": 0.0034,
      "step": 9450
    },
    {
      "epoch": 0.31533333333333335,
      "grad_norm": 0.18355286121368408,
      "learning_rate": 4.802916666666667e-05,
      "loss": 0.0045,
      "step": 9460
    },
    {
      "epoch": 0.31566666666666665,
      "grad_norm": 0.15369676053524017,
      "learning_rate": 4.8027083333333336e-05,
      "loss": 0.0031,
      "step": 9470
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.3968369662761688,
      "learning_rate": 4.8025e-05,
      "loss": 0.0048,
      "step": 9480
    },
    {
      "epoch": 0.31633333333333336,
      "grad_norm": 0.5186284184455872,
      "learning_rate": 4.802291666666667e-05,
      "loss": 0.0024,
      "step": 9490
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.7320484519004822,
      "learning_rate": 4.802083333333334e-05,
      "loss": 0.003,
      "step": 9500
    },
    {
      "epoch": 0.317,
      "grad_norm": 0.09259112179279327,
      "learning_rate": 4.801875e-05,
      "loss": 0.0028,
      "step": 9510
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.21384362876415253,
      "learning_rate": 4.801666666666667e-05,
      "loss": 0.0043,
      "step": 9520
    },
    {
      "epoch": 0.31766666666666665,
      "grad_norm": 0.3788885772228241,
      "learning_rate": 4.8014583333333336e-05,
      "loss": 0.0033,
      "step": 9530
    },
    {
      "epoch": 0.318,
      "grad_norm": 0.24486613273620605,
      "learning_rate": 4.80125e-05,
      "loss": 0.0027,
      "step": 9540
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.4721239507198334,
      "learning_rate": 4.801041666666667e-05,
      "loss": 0.0033,
      "step": 9550
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 0.09174121916294098,
      "learning_rate": 4.800833333333333e-05,
      "loss": 0.004,
      "step": 9560
    },
    {
      "epoch": 0.319,
      "grad_norm": 0.24492378532886505,
      "learning_rate": 4.8006250000000005e-05,
      "loss": 0.0039,
      "step": 9570
    },
    {
      "epoch": 0.31933333333333336,
      "grad_norm": 0.12220034748315811,
      "learning_rate": 4.8004166666666663e-05,
      "loss": 0.0027,
      "step": 9580
    },
    {
      "epoch": 0.31966666666666665,
      "grad_norm": 0.21358902752399445,
      "learning_rate": 4.8002083333333336e-05,
      "loss": 0.0041,
      "step": 9590
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.18391133844852448,
      "learning_rate": 4.8e-05,
      "loss": 0.0049,
      "step": 9600
    },
    {
      "epoch": 0.32033333333333336,
      "grad_norm": 0.10395488888025284,
      "learning_rate": 4.7997916666666673e-05,
      "loss": 0.0029,
      "step": 9610
    },
    {
      "epoch": 0.32066666666666666,
      "grad_norm": 0.30202674865722656,
      "learning_rate": 4.799583333333333e-05,
      "loss": 0.0033,
      "step": 9620
    },
    {
      "epoch": 0.321,
      "grad_norm": 0.45818549394607544,
      "learning_rate": 4.7993750000000004e-05,
      "loss": 0.0046,
      "step": 9630
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 0.15260688960552216,
      "learning_rate": 4.799166666666667e-05,
      "loss": 0.0018,
      "step": 9640
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.06448608636856079,
      "learning_rate": 4.7989583333333335e-05,
      "loss": 0.0036,
      "step": 9650
    },
    {
      "epoch": 0.322,
      "grad_norm": 0.21422237157821655,
      "learning_rate": 4.79875e-05,
      "loss": 0.003,
      "step": 9660
    },
    {
      "epoch": 0.32233333333333336,
      "grad_norm": 0.18639549612998962,
      "learning_rate": 4.798541666666667e-05,
      "loss": 0.0033,
      "step": 9670
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.30511993169784546,
      "learning_rate": 4.798333333333334e-05,
      "loss": 0.0031,
      "step": 9680
    },
    {
      "epoch": 0.323,
      "grad_norm": 0.22665636241436005,
      "learning_rate": 4.798125e-05,
      "loss": 0.0049,
      "step": 9690
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.016570983454585075,
      "learning_rate": 4.797916666666667e-05,
      "loss": 0.0023,
      "step": 9700
    },
    {
      "epoch": 0.32366666666666666,
      "grad_norm": 0.33672377467155457,
      "learning_rate": 4.7977083333333335e-05,
      "loss": 0.0031,
      "step": 9710
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.437114953994751,
      "learning_rate": 4.7975e-05,
      "loss": 0.0035,
      "step": 9720
    },
    {
      "epoch": 0.3243333333333333,
      "grad_norm": 0.09355900436639786,
      "learning_rate": 4.7972916666666666e-05,
      "loss": 0.0023,
      "step": 9730
    },
    {
      "epoch": 0.32466666666666666,
      "grad_norm": 0.4583297669887543,
      "learning_rate": 4.797083333333334e-05,
      "loss": 0.0049,
      "step": 9740
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.4277544319629669,
      "learning_rate": 4.7968750000000004e-05,
      "loss": 0.0018,
      "step": 9750
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.3362082540988922,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0036,
      "step": 9760
    },
    {
      "epoch": 0.32566666666666666,
      "grad_norm": 0.06296563148498535,
      "learning_rate": 4.7964583333333335e-05,
      "loss": 0.0042,
      "step": 9770
    },
    {
      "epoch": 0.326,
      "grad_norm": 0.09203217178583145,
      "learning_rate": 4.796250000000001e-05,
      "loss": 0.0025,
      "step": 9780
    },
    {
      "epoch": 0.3263333333333333,
      "grad_norm": 0.3983801007270813,
      "learning_rate": 4.7960416666666666e-05,
      "loss": 0.0035,
      "step": 9790
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.06318582594394684,
      "learning_rate": 4.795833333333333e-05,
      "loss": 0.0028,
      "step": 9800
    },
    {
      "epoch": 0.327,
      "grad_norm": 0.5909467339515686,
      "learning_rate": 4.7956250000000004e-05,
      "loss": 0.0025,
      "step": 9810
    },
    {
      "epoch": 0.3273333333333333,
      "grad_norm": 0.09221825748682022,
      "learning_rate": 4.795416666666667e-05,
      "loss": 0.003,
      "step": 9820
    },
    {
      "epoch": 0.32766666666666666,
      "grad_norm": 0.15306276082992554,
      "learning_rate": 4.7952083333333335e-05,
      "loss": 0.0028,
      "step": 9830
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.12247371673583984,
      "learning_rate": 4.795e-05,
      "loss": 0.0036,
      "step": 9840
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.428541898727417,
      "learning_rate": 4.794791666666667e-05,
      "loss": 0.0041,
      "step": 9850
    },
    {
      "epoch": 0.32866666666666666,
      "grad_norm": 0.2556358575820923,
      "learning_rate": 4.794583333333333e-05,
      "loss": 0.0034,
      "step": 9860
    },
    {
      "epoch": 0.329,
      "grad_norm": 0.4122733473777771,
      "learning_rate": 4.794375e-05,
      "loss": 0.0031,
      "step": 9870
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 0.3520636260509491,
      "learning_rate": 4.794166666666667e-05,
      "loss": 0.0032,
      "step": 9880
    },
    {
      "epoch": 0.32966666666666666,
      "grad_norm": 0.18370594084262848,
      "learning_rate": 4.793958333333334e-05,
      "loss": 0.0034,
      "step": 9890
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.24410369992256165,
      "learning_rate": 4.79375e-05,
      "loss": 0.0033,
      "step": 9900
    },
    {
      "epoch": 0.3303333333333333,
      "grad_norm": 0.36909884214401245,
      "learning_rate": 4.793541666666667e-05,
      "loss": 0.0041,
      "step": 9910
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.4883413314819336,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.004,
      "step": 9920
    },
    {
      "epoch": 0.331,
      "grad_norm": 0.6739711165428162,
      "learning_rate": 4.793125e-05,
      "loss": 0.0036,
      "step": 9930
    },
    {
      "epoch": 0.3313333333333333,
      "grad_norm": 0.09245095402002335,
      "learning_rate": 4.792916666666667e-05,
      "loss": 0.0031,
      "step": 9940
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.06262225657701492,
      "learning_rate": 4.7927083333333334e-05,
      "loss": 0.0035,
      "step": 9950
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.24409480392932892,
      "learning_rate": 4.7925000000000006e-05,
      "loss": 0.0032,
      "step": 9960
    },
    {
      "epoch": 0.3323333333333333,
      "grad_norm": 0.06128353253006935,
      "learning_rate": 4.7922916666666665e-05,
      "loss": 0.0035,
      "step": 9970
    },
    {
      "epoch": 0.33266666666666667,
      "grad_norm": 0.031759586185216904,
      "learning_rate": 4.792083333333334e-05,
      "loss": 0.0035,
      "step": 9980
    },
    {
      "epoch": 0.333,
      "grad_norm": 0.6100642085075378,
      "learning_rate": 4.791875e-05,
      "loss": 0.0052,
      "step": 9990
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.33544960618019104,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.0048,
      "step": 10000
    },
    {
      "epoch": 0.33366666666666667,
      "grad_norm": 0.1532844603061676,
      "learning_rate": 4.7914583333333334e-05,
      "loss": 0.0034,
      "step": 10010
    },
    {
      "epoch": 0.334,
      "grad_norm": 0.2693724036216736,
      "learning_rate": 4.7912500000000006e-05,
      "loss": 0.0038,
      "step": 10020
    },
    {
      "epoch": 0.3343333333333333,
      "grad_norm": 0.4875788986682892,
      "learning_rate": 4.791041666666667e-05,
      "loss": 0.0039,
      "step": 10030
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 0.2442740499973297,
      "learning_rate": 4.790833333333334e-05,
      "loss": 0.0027,
      "step": 10040
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.033804140985012054,
      "learning_rate": 4.790625e-05,
      "loss": 0.0027,
      "step": 10050
    },
    {
      "epoch": 0.3353333333333333,
      "grad_norm": 0.5308453440666199,
      "learning_rate": 4.790416666666667e-05,
      "loss": 0.0047,
      "step": 10060
    },
    {
      "epoch": 0.33566666666666667,
      "grad_norm": 0.5798551440238953,
      "learning_rate": 4.790208333333333e-05,
      "loss": 0.0049,
      "step": 10070
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.2777096927165985,
      "learning_rate": 4.79e-05,
      "loss": 0.0042,
      "step": 10080
    },
    {
      "epoch": 0.3363333333333333,
      "grad_norm": 0.3051970899105072,
      "learning_rate": 4.789791666666667e-05,
      "loss": 0.0041,
      "step": 10090
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.03931862488389015,
      "learning_rate": 4.7895833333333337e-05,
      "loss": 0.004,
      "step": 10100
    },
    {
      "epoch": 0.337,
      "grad_norm": 0.36946335434913635,
      "learning_rate": 4.789375e-05,
      "loss": 0.0041,
      "step": 10110
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 0.4157087802886963,
      "learning_rate": 4.789166666666667e-05,
      "loss": 0.0026,
      "step": 10120
    },
    {
      "epoch": 0.33766666666666667,
      "grad_norm": 0.09293778240680695,
      "learning_rate": 4.788958333333334e-05,
      "loss": 0.005,
      "step": 10130
    },
    {
      "epoch": 0.338,
      "grad_norm": 0.03240068629384041,
      "learning_rate": 4.78875e-05,
      "loss": 0.0029,
      "step": 10140
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.061927374452352524,
      "learning_rate": 4.788541666666667e-05,
      "loss": 0.0047,
      "step": 10150
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.24506311118602753,
      "learning_rate": 4.7883333333333336e-05,
      "loss": 0.0032,
      "step": 10160
    },
    {
      "epoch": 0.339,
      "grad_norm": 0.12215462327003479,
      "learning_rate": 4.788125e-05,
      "loss": 0.0034,
      "step": 10170
    },
    {
      "epoch": 0.3393333333333333,
      "grad_norm": 0.3048028349876404,
      "learning_rate": 4.787916666666667e-05,
      "loss": 0.003,
      "step": 10180
    },
    {
      "epoch": 0.3396666666666667,
      "grad_norm": 0.3657483160495758,
      "learning_rate": 4.787708333333333e-05,
      "loss": 0.0028,
      "step": 10190
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.06228302791714668,
      "learning_rate": 4.7875000000000005e-05,
      "loss": 0.0026,
      "step": 10200
    },
    {
      "epoch": 0.3403333333333333,
      "grad_norm": 0.18359459936618805,
      "learning_rate": 4.787291666666667e-05,
      "loss": 0.0051,
      "step": 10210
    },
    {
      "epoch": 0.3406666666666667,
      "grad_norm": 0.09466850012540817,
      "learning_rate": 4.7870833333333336e-05,
      "loss": 0.0022,
      "step": 10220
    },
    {
      "epoch": 0.341,
      "grad_norm": 0.0645771324634552,
      "learning_rate": 4.786875e-05,
      "loss": 0.0031,
      "step": 10230
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.0091908173635602,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0038,
      "step": 10240
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.30497103929519653,
      "learning_rate": 4.786458333333333e-05,
      "loss": 0.0039,
      "step": 10250
    },
    {
      "epoch": 0.342,
      "grad_norm": 0.4270063042640686,
      "learning_rate": 4.7862500000000005e-05,
      "loss": 0.0023,
      "step": 10260
    },
    {
      "epoch": 0.3423333333333333,
      "grad_norm": 0.6100305914878845,
      "learning_rate": 4.786041666666667e-05,
      "loss": 0.0033,
      "step": 10270
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 0.4573958218097687,
      "learning_rate": 4.7858333333333336e-05,
      "loss": 0.0029,
      "step": 10280
    },
    {
      "epoch": 0.343,
      "grad_norm": 0.2138097733259201,
      "learning_rate": 4.785625e-05,
      "loss": 0.0026,
      "step": 10290
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.5583019256591797,
      "learning_rate": 4.7854166666666667e-05,
      "loss": 0.0042,
      "step": 10300
    },
    {
      "epoch": 0.3436666666666667,
      "grad_norm": 0.09204313904047012,
      "learning_rate": 4.785208333333334e-05,
      "loss": 0.0029,
      "step": 10310
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.24409827589988708,
      "learning_rate": 4.785e-05,
      "loss": 0.0021,
      "step": 10320
    },
    {
      "epoch": 0.3443333333333333,
      "grad_norm": 0.1530729979276657,
      "learning_rate": 4.784791666666667e-05,
      "loss": 0.0023,
      "step": 10330
    },
    {
      "epoch": 0.3446666666666667,
      "grad_norm": 0.2995178699493408,
      "learning_rate": 4.7845833333333335e-05,
      "loss": 0.0035,
      "step": 10340
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.40220966935157776,
      "learning_rate": 4.784375e-05,
      "loss": 0.0035,
      "step": 10350
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 0.8126066327095032,
      "learning_rate": 4.7841666666666666e-05,
      "loss": 0.0046,
      "step": 10360
    },
    {
      "epoch": 0.3456666666666667,
      "grad_norm": 0.09205323457717896,
      "learning_rate": 4.783958333333334e-05,
      "loss": 0.0033,
      "step": 10370
    },
    {
      "epoch": 0.346,
      "grad_norm": 0.38007280230522156,
      "learning_rate": 4.7837500000000004e-05,
      "loss": 0.0031,
      "step": 10380
    },
    {
      "epoch": 0.3463333333333333,
      "grad_norm": 0.36542776226997375,
      "learning_rate": 4.783541666666667e-05,
      "loss": 0.0039,
      "step": 10390
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 1.048340082168579,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0041,
      "step": 10400
    },
    {
      "epoch": 0.347,
      "grad_norm": 0.3962905704975128,
      "learning_rate": 4.783125e-05,
      "loss": 0.0031,
      "step": 10410
    },
    {
      "epoch": 0.3473333333333333,
      "grad_norm": 0.12263251841068268,
      "learning_rate": 4.7829166666666666e-05,
      "loss": 0.0037,
      "step": 10420
    },
    {
      "epoch": 0.3476666666666667,
      "grad_norm": 0.06240653619170189,
      "learning_rate": 4.782708333333333e-05,
      "loss": 0.0027,
      "step": 10430
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.39663711190223694,
      "learning_rate": 4.7825000000000004e-05,
      "loss": 0.0047,
      "step": 10440
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.254893958568573,
      "learning_rate": 4.782291666666667e-05,
      "loss": 0.0038,
      "step": 10450
    },
    {
      "epoch": 0.3486666666666667,
      "grad_norm": 0.48800957202911377,
      "learning_rate": 4.7820833333333335e-05,
      "loss": 0.0038,
      "step": 10460
    },
    {
      "epoch": 0.349,
      "grad_norm": 0.06825804710388184,
      "learning_rate": 4.781875e-05,
      "loss": 0.005,
      "step": 10470
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.45676955580711365,
      "learning_rate": 4.781666666666667e-05,
      "loss": 0.0046,
      "step": 10480
    },
    {
      "epoch": 0.3496666666666667,
      "grad_norm": 0.39579251408576965,
      "learning_rate": 4.781458333333334e-05,
      "loss": 0.0038,
      "step": 10490
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.2134760320186615,
      "learning_rate": 4.7812500000000003e-05,
      "loss": 0.0029,
      "step": 10500
    },
    {
      "epoch": 0.35033333333333333,
      "grad_norm": 0.7928879261016846,
      "learning_rate": 4.781041666666667e-05,
      "loss": 0.0037,
      "step": 10510
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 0.3964191675186157,
      "learning_rate": 4.780833333333334e-05,
      "loss": 0.0029,
      "step": 10520
    },
    {
      "epoch": 0.351,
      "grad_norm": 0.6513062715530396,
      "learning_rate": 4.780625e-05,
      "loss": 0.0055,
      "step": 10530
    },
    {
      "epoch": 0.35133333333333333,
      "grad_norm": 0.06239548325538635,
      "learning_rate": 4.7804166666666665e-05,
      "loss": 0.0043,
      "step": 10540
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.06463470309972763,
      "learning_rate": 4.780208333333334e-05,
      "loss": 0.0039,
      "step": 10550
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.0676334947347641,
      "learning_rate": 4.78e-05,
      "loss": 0.0039,
      "step": 10560
    },
    {
      "epoch": 0.35233333333333333,
      "grad_norm": 0.3529060482978821,
      "learning_rate": 4.779791666666667e-05,
      "loss": 0.0033,
      "step": 10570
    },
    {
      "epoch": 0.3526666666666667,
      "grad_norm": 0.34086358547210693,
      "learning_rate": 4.7795833333333334e-05,
      "loss": 0.0035,
      "step": 10580
    },
    {
      "epoch": 0.353,
      "grad_norm": 0.15384143590927124,
      "learning_rate": 4.7793750000000006e-05,
      "loss": 0.0043,
      "step": 10590
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.12245092540979385,
      "learning_rate": 4.7791666666666665e-05,
      "loss": 0.0036,
      "step": 10600
    },
    {
      "epoch": 0.3536666666666667,
      "grad_norm": 0.03378749266266823,
      "learning_rate": 4.778958333333334e-05,
      "loss": 0.0033,
      "step": 10610
    },
    {
      "epoch": 0.354,
      "grad_norm": 0.5478851199150085,
      "learning_rate": 4.77875e-05,
      "loss": 0.0042,
      "step": 10620
    },
    {
      "epoch": 0.35433333333333333,
      "grad_norm": 0.5977360010147095,
      "learning_rate": 4.778541666666667e-05,
      "loss": 0.0024,
      "step": 10630
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.2472326010465622,
      "learning_rate": 4.7783333333333334e-05,
      "loss": 0.0038,
      "step": 10640
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.2131803184747696,
      "learning_rate": 4.778125e-05,
      "loss": 0.0037,
      "step": 10650
    },
    {
      "epoch": 0.35533333333333333,
      "grad_norm": 0.09405233711004257,
      "learning_rate": 4.777916666666667e-05,
      "loss": 0.0023,
      "step": 10660
    },
    {
      "epoch": 0.3556666666666667,
      "grad_norm": 0.02658408135175705,
      "learning_rate": 4.777708333333333e-05,
      "loss": 0.0036,
      "step": 10670
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.30418112874031067,
      "learning_rate": 4.7775e-05,
      "loss": 0.003,
      "step": 10680
    },
    {
      "epoch": 0.35633333333333334,
      "grad_norm": 0.15286049246788025,
      "learning_rate": 4.777291666666667e-05,
      "loss": 0.0052,
      "step": 10690
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.1838100254535675,
      "learning_rate": 4.7770833333333333e-05,
      "loss": 0.0038,
      "step": 10700
    },
    {
      "epoch": 0.357,
      "grad_norm": 0.36571750044822693,
      "learning_rate": 4.776875e-05,
      "loss": 0.0039,
      "step": 10710
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.15282633900642395,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0028,
      "step": 10720
    },
    {
      "epoch": 0.3576666666666667,
      "grad_norm": 0.12195209413766861,
      "learning_rate": 4.776458333333334e-05,
      "loss": 0.0036,
      "step": 10730
    },
    {
      "epoch": 0.358,
      "grad_norm": 0.5480816960334778,
      "learning_rate": 4.77625e-05,
      "loss": 0.0035,
      "step": 10740
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.7308096289634705,
      "learning_rate": 4.776041666666667e-05,
      "loss": 0.0035,
      "step": 10750
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 0.4272773563861847,
      "learning_rate": 4.775833333333334e-05,
      "loss": 0.0034,
      "step": 10760
    },
    {
      "epoch": 0.359,
      "grad_norm": 0.18416443467140198,
      "learning_rate": 4.7756250000000005e-05,
      "loss": 0.004,
      "step": 10770
    },
    {
      "epoch": 0.35933333333333334,
      "grad_norm": 0.1703568547964096,
      "learning_rate": 4.7754166666666664e-05,
      "loss": 0.004,
      "step": 10780
    },
    {
      "epoch": 0.3596666666666667,
      "grad_norm": 0.36532318592071533,
      "learning_rate": 4.7752083333333336e-05,
      "loss": 0.0034,
      "step": 10790
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.3050250709056854,
      "learning_rate": 4.775e-05,
      "loss": 0.0026,
      "step": 10800
    },
    {
      "epoch": 0.36033333333333334,
      "grad_norm": 0.1829553097486496,
      "learning_rate": 4.774791666666667e-05,
      "loss": 0.0034,
      "step": 10810
    },
    {
      "epoch": 0.3606666666666667,
      "grad_norm": 0.33574146032333374,
      "learning_rate": 4.774583333333333e-05,
      "loss": 0.0037,
      "step": 10820
    },
    {
      "epoch": 0.361,
      "grad_norm": 0.18338608741760254,
      "learning_rate": 4.7743750000000005e-05,
      "loss": 0.0038,
      "step": 10830
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 0.2123311161994934,
      "learning_rate": 4.774166666666667e-05,
      "loss": 0.0037,
      "step": 10840
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.00808919407427311,
      "learning_rate": 4.7739583333333336e-05,
      "loss": 0.0039,
      "step": 10850
    },
    {
      "epoch": 0.362,
      "grad_norm": 0.24396759271621704,
      "learning_rate": 4.77375e-05,
      "loss": 0.0042,
      "step": 10860
    },
    {
      "epoch": 0.36233333333333334,
      "grad_norm": 0.09219896048307419,
      "learning_rate": 4.7735416666666674e-05,
      "loss": 0.0022,
      "step": 10870
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.031080352142453194,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0036,
      "step": 10880
    },
    {
      "epoch": 0.363,
      "grad_norm": 0.09167452901601791,
      "learning_rate": 4.773125e-05,
      "loss": 0.0036,
      "step": 10890
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.21412499248981476,
      "learning_rate": 4.772916666666667e-05,
      "loss": 0.0022,
      "step": 10900
    },
    {
      "epoch": 0.3636666666666667,
      "grad_norm": 0.4267224669456482,
      "learning_rate": 4.7727083333333336e-05,
      "loss": 0.0025,
      "step": 10910
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.4780241847038269,
      "learning_rate": 4.7725e-05,
      "loss": 0.0038,
      "step": 10920
    },
    {
      "epoch": 0.36433333333333334,
      "grad_norm": 0.15343987941741943,
      "learning_rate": 4.772291666666667e-05,
      "loss": 0.0052,
      "step": 10930
    },
    {
      "epoch": 0.36466666666666664,
      "grad_norm": 0.36582934856414795,
      "learning_rate": 4.772083333333334e-05,
      "loss": 0.0036,
      "step": 10940
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.6091800928115845,
      "learning_rate": 4.771875e-05,
      "loss": 0.004,
      "step": 10950
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.33800816535949707,
      "learning_rate": 4.771666666666667e-05,
      "loss": 0.0035,
      "step": 10960
    },
    {
      "epoch": 0.36566666666666664,
      "grad_norm": 0.45725932717323303,
      "learning_rate": 4.7714583333333336e-05,
      "loss": 0.0048,
      "step": 10970
    },
    {
      "epoch": 0.366,
      "grad_norm": 0.12234235554933548,
      "learning_rate": 4.771250000000001e-05,
      "loss": 0.0035,
      "step": 10980
    },
    {
      "epoch": 0.36633333333333334,
      "grad_norm": 0.33474940061569214,
      "learning_rate": 4.7710416666666666e-05,
      "loss": 0.0045,
      "step": 10990
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.4155747890472412,
      "learning_rate": 4.770833333333334e-05,
      "loss": 0.0035,
      "step": 11000
    },
    {
      "epoch": 0.367,
      "grad_norm": 0.2186332792043686,
      "learning_rate": 4.7706250000000004e-05,
      "loss": 0.0041,
      "step": 11010
    },
    {
      "epoch": 0.36733333333333335,
      "grad_norm": 0.1224864050745964,
      "learning_rate": 4.770416666666667e-05,
      "loss": 0.003,
      "step": 11020
    },
    {
      "epoch": 0.36766666666666664,
      "grad_norm": 0.36557018756866455,
      "learning_rate": 4.7702083333333335e-05,
      "loss": 0.0033,
      "step": 11030
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.18306013941764832,
      "learning_rate": 4.77e-05,
      "loss": 0.0037,
      "step": 11040
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.2798324525356293,
      "learning_rate": 4.769791666666667e-05,
      "loss": 0.0038,
      "step": 11050
    },
    {
      "epoch": 0.36866666666666664,
      "grad_norm": 0.4021919369697571,
      "learning_rate": 4.769583333333333e-05,
      "loss": 0.0035,
      "step": 11060
    },
    {
      "epoch": 0.369,
      "grad_norm": 0.46678605675697327,
      "learning_rate": 4.7693750000000004e-05,
      "loss": 0.0034,
      "step": 11070
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 0.3086605966091156,
      "learning_rate": 4.769166666666667e-05,
      "loss": 0.0038,
      "step": 11080
    },
    {
      "epoch": 0.36966666666666664,
      "grad_norm": 0.7303443551063538,
      "learning_rate": 4.7689583333333335e-05,
      "loss": 0.003,
      "step": 11090
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.2742459774017334,
      "learning_rate": 4.76875e-05,
      "loss": 0.0051,
      "step": 11100
    },
    {
      "epoch": 0.37033333333333335,
      "grad_norm": 0.15316540002822876,
      "learning_rate": 4.768541666666667e-05,
      "loss": 0.0035,
      "step": 11110
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.3128263056278229,
      "learning_rate": 4.768333333333334e-05,
      "loss": 0.0031,
      "step": 11120
    },
    {
      "epoch": 0.371,
      "grad_norm": 0.6698792576789856,
      "learning_rate": 4.768125e-05,
      "loss": 0.0042,
      "step": 11130
    },
    {
      "epoch": 0.37133333333333335,
      "grad_norm": 0.12234596908092499,
      "learning_rate": 4.767916666666667e-05,
      "loss": 0.0038,
      "step": 11140
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.03476068750023842,
      "learning_rate": 4.7677083333333335e-05,
      "loss": 0.0022,
      "step": 11150
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.03165748715400696,
      "learning_rate": 4.7675e-05,
      "loss": 0.004,
      "step": 11160
    },
    {
      "epoch": 0.37233333333333335,
      "grad_norm": 0.30462852120399475,
      "learning_rate": 4.7672916666666666e-05,
      "loss": 0.0031,
      "step": 11170
    },
    {
      "epoch": 0.37266666666666665,
      "grad_norm": 0.18314383924007416,
      "learning_rate": 4.767083333333334e-05,
      "loss": 0.0035,
      "step": 11180
    },
    {
      "epoch": 0.373,
      "grad_norm": 0.030841270461678505,
      "learning_rate": 4.766875e-05,
      "loss": 0.0032,
      "step": 11190
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.24408802390098572,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.002,
      "step": 11200
    },
    {
      "epoch": 0.37366666666666665,
      "grad_norm": 0.24458733201026917,
      "learning_rate": 4.7664583333333334e-05,
      "loss": 0.0025,
      "step": 11210
    },
    {
      "epoch": 0.374,
      "grad_norm": 0.45661890506744385,
      "learning_rate": 4.7662500000000007e-05,
      "loss": 0.0023,
      "step": 11220
    },
    {
      "epoch": 0.37433333333333335,
      "grad_norm": 0.22813159227371216,
      "learning_rate": 4.7660416666666665e-05,
      "loss": 0.0029,
      "step": 11230
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 0.09162086248397827,
      "learning_rate": 4.765833333333334e-05,
      "loss": 0.0035,
      "step": 11240
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.18304595351219177,
      "learning_rate": 4.765625e-05,
      "loss": 0.0031,
      "step": 11250
    },
    {
      "epoch": 0.37533333333333335,
      "grad_norm": 0.7153841257095337,
      "learning_rate": 4.765416666666667e-05,
      "loss": 0.0028,
      "step": 11260
    },
    {
      "epoch": 0.37566666666666665,
      "grad_norm": 0.3043825030326843,
      "learning_rate": 4.7652083333333334e-05,
      "loss": 0.0023,
      "step": 11270
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.15236473083496094,
      "learning_rate": 4.765e-05,
      "loss": 0.0024,
      "step": 11280
    },
    {
      "epoch": 0.37633333333333335,
      "grad_norm": 0.274495929479599,
      "learning_rate": 4.764791666666667e-05,
      "loss": 0.0036,
      "step": 11290
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.15238766372203827,
      "learning_rate": 4.764583333333334e-05,
      "loss": 0.0042,
      "step": 11300
    },
    {
      "epoch": 0.377,
      "grad_norm": 0.12223099172115326,
      "learning_rate": 4.764375e-05,
      "loss": 0.0033,
      "step": 11310
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 0.255986750125885,
      "learning_rate": 4.764166666666667e-05,
      "loss": 0.0036,
      "step": 11320
    },
    {
      "epoch": 0.37766666666666665,
      "grad_norm": 0.21414557099342346,
      "learning_rate": 4.763958333333334e-05,
      "loss": 0.0058,
      "step": 11330
    },
    {
      "epoch": 0.378,
      "grad_norm": 0.3048574924468994,
      "learning_rate": 4.76375e-05,
      "loss": 0.0037,
      "step": 11340
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.4567128121852875,
      "learning_rate": 4.763541666666667e-05,
      "loss": 0.0032,
      "step": 11350
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.5184740424156189,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.0024,
      "step": 11360
    },
    {
      "epoch": 0.379,
      "grad_norm": 0.12790468335151672,
      "learning_rate": 4.763125e-05,
      "loss": 0.0039,
      "step": 11370
    },
    {
      "epoch": 0.37933333333333336,
      "grad_norm": 0.18979406356811523,
      "learning_rate": 4.762916666666667e-05,
      "loss": 0.0034,
      "step": 11380
    },
    {
      "epoch": 0.37966666666666665,
      "grad_norm": 0.21357284486293793,
      "learning_rate": 4.7627083333333333e-05,
      "loss": 0.0026,
      "step": 11390
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.12213507294654846,
      "learning_rate": 4.7625000000000006e-05,
      "loss": 0.0034,
      "step": 11400
    },
    {
      "epoch": 0.38033333333333336,
      "grad_norm": 0.18294461071491241,
      "learning_rate": 4.7622916666666664e-05,
      "loss": 0.0026,
      "step": 11410
    },
    {
      "epoch": 0.38066666666666665,
      "grad_norm": 0.6392672657966614,
      "learning_rate": 4.762083333333334e-05,
      "loss": 0.003,
      "step": 11420
    },
    {
      "epoch": 0.381,
      "grad_norm": 0.18109586834907532,
      "learning_rate": 4.761875e-05,
      "loss": 0.0046,
      "step": 11430
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.03143990784883499,
      "learning_rate": 4.761666666666667e-05,
      "loss": 0.0033,
      "step": 11440
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.543379545211792,
      "learning_rate": 4.761458333333333e-05,
      "loss": 0.0042,
      "step": 11450
    },
    {
      "epoch": 0.382,
      "grad_norm": 0.3958589434623718,
      "learning_rate": 4.7612500000000005e-05,
      "loss": 0.0027,
      "step": 11460
    },
    {
      "epoch": 0.38233333333333336,
      "grad_norm": 0.4053601920604706,
      "learning_rate": 4.761041666666667e-05,
      "loss": 0.003,
      "step": 11470
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 0.48673611879348755,
      "learning_rate": 4.7608333333333336e-05,
      "loss": 0.0025,
      "step": 11480
    },
    {
      "epoch": 0.383,
      "grad_norm": 0.1234559491276741,
      "learning_rate": 4.760625e-05,
      "loss": 0.0039,
      "step": 11490
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.03205303102731705,
      "learning_rate": 4.760416666666667e-05,
      "loss": 0.0035,
      "step": 11500
    },
    {
      "epoch": 0.38366666666666666,
      "grad_norm": 0.24391667544841766,
      "learning_rate": 4.760208333333333e-05,
      "loss": 0.0035,
      "step": 11510
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.12494652718305588,
      "learning_rate": 4.76e-05,
      "loss": 0.0038,
      "step": 11520
    },
    {
      "epoch": 0.38433333333333336,
      "grad_norm": 0.18600894510746002,
      "learning_rate": 4.759791666666667e-05,
      "loss": 0.0032,
      "step": 11530
    },
    {
      "epoch": 0.38466666666666666,
      "grad_norm": 0.5482079386711121,
      "learning_rate": 4.7595833333333336e-05,
      "loss": 0.0031,
      "step": 11540
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.09825108200311661,
      "learning_rate": 4.759375e-05,
      "loss": 0.0022,
      "step": 11550
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 0.6087086200714111,
      "learning_rate": 4.759166666666667e-05,
      "loss": 0.0038,
      "step": 11560
    },
    {
      "epoch": 0.38566666666666666,
      "grad_norm": 0.18305976688861847,
      "learning_rate": 4.758958333333334e-05,
      "loss": 0.0041,
      "step": 11570
    },
    {
      "epoch": 0.386,
      "grad_norm": 0.30515214800834656,
      "learning_rate": 4.7587500000000005e-05,
      "loss": 0.0039,
      "step": 11580
    },
    {
      "epoch": 0.3863333333333333,
      "grad_norm": 0.12196706235408783,
      "learning_rate": 4.758541666666667e-05,
      "loss": 0.0031,
      "step": 11590
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.18322525918483734,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.0035,
      "step": 11600
    },
    {
      "epoch": 0.387,
      "grad_norm": 0.15222449600696564,
      "learning_rate": 4.758125000000001e-05,
      "loss": 0.0046,
      "step": 11610
    },
    {
      "epoch": 0.3873333333333333,
      "grad_norm": 0.24391891062259674,
      "learning_rate": 4.757916666666667e-05,
      "loss": 0.0027,
      "step": 11620
    },
    {
      "epoch": 0.38766666666666666,
      "grad_norm": 0.03132123872637749,
      "learning_rate": 4.757708333333333e-05,
      "loss": 0.0032,
      "step": 11630
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.21548429131507874,
      "learning_rate": 4.7575000000000004e-05,
      "loss": 0.0037,
      "step": 11640
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.41511568427085876,
      "learning_rate": 4.757291666666667e-05,
      "loss": 0.003,
      "step": 11650
    },
    {
      "epoch": 0.38866666666666666,
      "grad_norm": 0.02010609768331051,
      "learning_rate": 4.7570833333333335e-05,
      "loss": 0.0034,
      "step": 11660
    },
    {
      "epoch": 0.389,
      "grad_norm": 0.48853448033332825,
      "learning_rate": 4.756875e-05,
      "loss": 0.0043,
      "step": 11670
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.03308045119047165,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.0028,
      "step": 11680
    },
    {
      "epoch": 0.38966666666666666,
      "grad_norm": 0.18349704146385193,
      "learning_rate": 4.756458333333333e-05,
      "loss": 0.0034,
      "step": 11690
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.03601682931184769,
      "learning_rate": 4.7562500000000004e-05,
      "loss": 0.0034,
      "step": 11700
    },
    {
      "epoch": 0.3903333333333333,
      "grad_norm": 0.4087772071361542,
      "learning_rate": 4.756041666666667e-05,
      "loss": 0.0052,
      "step": 11710
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 0.18305763602256775,
      "learning_rate": 4.7558333333333335e-05,
      "loss": 0.0031,
      "step": 11720
    },
    {
      "epoch": 0.391,
      "grad_norm": 0.39582353830337524,
      "learning_rate": 4.755625e-05,
      "loss": 0.0031,
      "step": 11730
    },
    {
      "epoch": 0.3913333333333333,
      "grad_norm": 0.12320984154939651,
      "learning_rate": 4.7554166666666666e-05,
      "loss": 0.0016,
      "step": 11740
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.213611900806427,
      "learning_rate": 4.755208333333334e-05,
      "loss": 0.0039,
      "step": 11750
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.21322979032993317,
      "learning_rate": 4.755e-05,
      "loss": 0.0029,
      "step": 11760
    },
    {
      "epoch": 0.3923333333333333,
      "grad_norm": 0.1872485727071762,
      "learning_rate": 4.754791666666667e-05,
      "loss": 0.0032,
      "step": 11770
    },
    {
      "epoch": 0.39266666666666666,
      "grad_norm": 0.5474710464477539,
      "learning_rate": 4.7545833333333335e-05,
      "loss": 0.0026,
      "step": 11780
    },
    {
      "epoch": 0.393,
      "grad_norm": 0.09231378883123398,
      "learning_rate": 4.754375e-05,
      "loss": 0.003,
      "step": 11790
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.12238647043704987,
      "learning_rate": 4.7541666666666666e-05,
      "loss": 0.0025,
      "step": 11800
    },
    {
      "epoch": 0.39366666666666666,
      "grad_norm": 0.18254704773426056,
      "learning_rate": 4.753958333333334e-05,
      "loss": 0.0046,
      "step": 11810
    },
    {
      "epoch": 0.394,
      "grad_norm": 0.33517712354660034,
      "learning_rate": 4.7537500000000004e-05,
      "loss": 0.0028,
      "step": 11820
    },
    {
      "epoch": 0.3943333333333333,
      "grad_norm": 0.10393454879522324,
      "learning_rate": 4.753541666666667e-05,
      "loss": 0.003,
      "step": 11830
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.07581313699483871,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0027,
      "step": 11840
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.1524934321641922,
      "learning_rate": 4.753125000000001e-05,
      "loss": 0.0033,
      "step": 11850
    },
    {
      "epoch": 0.3953333333333333,
      "grad_norm": 0.30471426248550415,
      "learning_rate": 4.752916666666667e-05,
      "loss": 0.0031,
      "step": 11860
    },
    {
      "epoch": 0.39566666666666667,
      "grad_norm": 0.09178828448057175,
      "learning_rate": 4.752708333333333e-05,
      "loss": 0.0028,
      "step": 11870
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.09174342453479767,
      "learning_rate": 4.7525e-05,
      "loss": 0.0041,
      "step": 11880
    },
    {
      "epoch": 0.3963333333333333,
      "grad_norm": 0.03336688131093979,
      "learning_rate": 4.752291666666667e-05,
      "loss": 0.0033,
      "step": 11890
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.03357407823204994,
      "learning_rate": 4.7520833333333334e-05,
      "loss": 0.0036,
      "step": 11900
    },
    {
      "epoch": 0.397,
      "grad_norm": 0.3496731221675873,
      "learning_rate": 4.751875e-05,
      "loss": 0.0026,
      "step": 11910
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.03288542851805687,
      "learning_rate": 4.751666666666667e-05,
      "loss": 0.0029,
      "step": 11920
    },
    {
      "epoch": 0.39766666666666667,
      "grad_norm": 0.6076082587242126,
      "learning_rate": 4.751458333333334e-05,
      "loss": 0.0032,
      "step": 11930
    },
    {
      "epoch": 0.398,
      "grad_norm": 0.5780261158943176,
      "learning_rate": 4.75125e-05,
      "loss": 0.0031,
      "step": 11940
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.66900634765625,
      "learning_rate": 4.751041666666667e-05,
      "loss": 0.0028,
      "step": 11950
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 0.5472531914710999,
      "learning_rate": 4.750833333333334e-05,
      "loss": 0.0027,
      "step": 11960
    },
    {
      "epoch": 0.399,
      "grad_norm": 0.26930147409439087,
      "learning_rate": 4.750625e-05,
      "loss": 0.0033,
      "step": 11970
    },
    {
      "epoch": 0.3993333333333333,
      "grad_norm": 0.03234799578785896,
      "learning_rate": 4.7504166666666665e-05,
      "loss": 0.0029,
      "step": 11980
    },
    {
      "epoch": 0.39966666666666667,
      "grad_norm": 0.5276429057121277,
      "learning_rate": 4.750208333333334e-05,
      "loss": 0.0031,
      "step": 11990
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.21325065195560455,
      "learning_rate": 4.75e-05,
      "loss": 0.0041,
      "step": 12000
    },
    {
      "epoch": 0.4003333333333333,
      "grad_norm": 0.5507403016090393,
      "learning_rate": 4.749791666666667e-05,
      "loss": 0.0038,
      "step": 12010
    },
    {
      "epoch": 0.40066666666666667,
      "grad_norm": 0.24370814859867096,
      "learning_rate": 4.7495833333333334e-05,
      "loss": 0.0037,
      "step": 12020
    },
    {
      "epoch": 0.401,
      "grad_norm": 0.033745963126420975,
      "learning_rate": 4.7493750000000006e-05,
      "loss": 0.0042,
      "step": 12030
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 0.0641898363828659,
      "learning_rate": 4.7491666666666665e-05,
      "loss": 0.0032,
      "step": 12040
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.6075791716575623,
      "learning_rate": 4.748958333333334e-05,
      "loss": 0.0046,
      "step": 12050
    },
    {
      "epoch": 0.402,
      "grad_norm": 0.01022146362811327,
      "learning_rate": 4.74875e-05,
      "loss": 0.0022,
      "step": 12060
    },
    {
      "epoch": 0.4023333333333333,
      "grad_norm": 0.18208527565002441,
      "learning_rate": 4.748541666666667e-05,
      "loss": 0.004,
      "step": 12070
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.5713702440261841,
      "learning_rate": 4.748333333333333e-05,
      "loss": 0.0023,
      "step": 12080
    },
    {
      "epoch": 0.403,
      "grad_norm": 0.607559859752655,
      "learning_rate": 4.7481250000000006e-05,
      "loss": 0.0031,
      "step": 12090
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.7294561266899109,
      "learning_rate": 4.747916666666667e-05,
      "loss": 0.0035,
      "step": 12100
    },
    {
      "epoch": 0.4036666666666667,
      "grad_norm": 0.37497439980506897,
      "learning_rate": 4.7477083333333337e-05,
      "loss": 0.0024,
      "step": 12110
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.0625312551856041,
      "learning_rate": 4.7475e-05,
      "loss": 0.0027,
      "step": 12120
    },
    {
      "epoch": 0.4043333333333333,
      "grad_norm": 0.24251271784305573,
      "learning_rate": 4.747291666666667e-05,
      "loss": 0.0038,
      "step": 12130
    },
    {
      "epoch": 0.4046666666666667,
      "grad_norm": 0.32250961661338806,
      "learning_rate": 4.747083333333334e-05,
      "loss": 0.0018,
      "step": 12140
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.2738340497016907,
      "learning_rate": 4.746875e-05,
      "loss": 0.0021,
      "step": 12150
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.3049512207508087,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0037,
      "step": 12160
    },
    {
      "epoch": 0.4056666666666667,
      "grad_norm": 0.01825603097677231,
      "learning_rate": 4.7464583333333336e-05,
      "loss": 0.0027,
      "step": 12170
    },
    {
      "epoch": 0.406,
      "grad_norm": 0.2736421823501587,
      "learning_rate": 4.74625e-05,
      "loss": 0.0026,
      "step": 12180
    },
    {
      "epoch": 0.4063333333333333,
      "grad_norm": 0.09164803475141525,
      "learning_rate": 4.746041666666667e-05,
      "loss": 0.0045,
      "step": 12190
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.03188379481434822,
      "learning_rate": 4.745833333333334e-05,
      "loss": 0.003,
      "step": 12200
    },
    {
      "epoch": 0.407,
      "grad_norm": 0.27667236328125,
      "learning_rate": 4.7456250000000005e-05,
      "loss": 0.0026,
      "step": 12210
    },
    {
      "epoch": 0.4073333333333333,
      "grad_norm": 0.7315954566001892,
      "learning_rate": 4.7454166666666664e-05,
      "loss": 0.0033,
      "step": 12220
    },
    {
      "epoch": 0.4076666666666667,
      "grad_norm": 0.4265812933444977,
      "learning_rate": 4.7452083333333336e-05,
      "loss": 0.0026,
      "step": 12230
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.1828973889350891,
      "learning_rate": 4.745e-05,
      "loss": 0.0032,
      "step": 12240
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.376389741897583,
      "learning_rate": 4.744791666666667e-05,
      "loss": 0.0039,
      "step": 12250
    },
    {
      "epoch": 0.4086666666666667,
      "grad_norm": 0.3663831949234009,
      "learning_rate": 4.744583333333333e-05,
      "loss": 0.0025,
      "step": 12260
    },
    {
      "epoch": 0.409,
      "grad_norm": 0.2620771527290344,
      "learning_rate": 4.7443750000000005e-05,
      "loss": 0.0032,
      "step": 12270
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 0.27713537216186523,
      "learning_rate": 4.744166666666667e-05,
      "loss": 0.003,
      "step": 12280
    },
    {
      "epoch": 0.4096666666666667,
      "grad_norm": 0.03168505057692528,
      "learning_rate": 4.7439583333333336e-05,
      "loss": 0.003,
      "step": 12290
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.4957193434238434,
      "learning_rate": 4.74375e-05,
      "loss": 0.0037,
      "step": 12300
    },
    {
      "epoch": 0.4103333333333333,
      "grad_norm": 0.18292324244976044,
      "learning_rate": 4.743541666666667e-05,
      "loss": 0.0034,
      "step": 12310
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.062284912914037704,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0039,
      "step": 12320
    },
    {
      "epoch": 0.411,
      "grad_norm": 0.6685532927513123,
      "learning_rate": 4.7431250000000004e-05,
      "loss": 0.0031,
      "step": 12330
    },
    {
      "epoch": 0.41133333333333333,
      "grad_norm": 0.3891035318374634,
      "learning_rate": 4.742916666666667e-05,
      "loss": 0.0043,
      "step": 12340
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.16044281423091888,
      "learning_rate": 4.7427083333333335e-05,
      "loss": 0.0033,
      "step": 12350
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.5955324172973633,
      "learning_rate": 4.7425e-05,
      "loss": 0.0043,
      "step": 12360
    },
    {
      "epoch": 0.41233333333333333,
      "grad_norm": 0.031578708440065384,
      "learning_rate": 4.7422916666666666e-05,
      "loss": 0.0036,
      "step": 12370
    },
    {
      "epoch": 0.4126666666666667,
      "grad_norm": 0.1524633765220642,
      "learning_rate": 4.742083333333334e-05,
      "loss": 0.0036,
      "step": 12380
    },
    {
      "epoch": 0.413,
      "grad_norm": 0.1828150749206543,
      "learning_rate": 4.7418750000000004e-05,
      "loss": 0.0045,
      "step": 12390
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.09308681637048721,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.0022,
      "step": 12400
    },
    {
      "epoch": 0.4136666666666667,
      "grad_norm": 0.35879966616630554,
      "learning_rate": 4.7414583333333335e-05,
      "loss": 0.003,
      "step": 12410
    },
    {
      "epoch": 0.414,
      "grad_norm": 0.32397639751434326,
      "learning_rate": 4.741250000000001e-05,
      "loss": 0.0034,
      "step": 12420
    },
    {
      "epoch": 0.41433333333333333,
      "grad_norm": 0.3038087785243988,
      "learning_rate": 4.7410416666666666e-05,
      "loss": 0.0024,
      "step": 12430
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 0.24346138536930084,
      "learning_rate": 4.740833333333334e-05,
      "loss": 0.0023,
      "step": 12440
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.21244344115257263,
      "learning_rate": 4.7406250000000004e-05,
      "loss": 0.0021,
      "step": 12450
    },
    {
      "epoch": 0.41533333333333333,
      "grad_norm": 0.5463905334472656,
      "learning_rate": 4.740416666666667e-05,
      "loss": 0.0032,
      "step": 12460
    },
    {
      "epoch": 0.4156666666666667,
      "grad_norm": 0.7934361696243286,
      "learning_rate": 4.7402083333333335e-05,
      "loss": 0.0038,
      "step": 12470
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.39459192752838135,
      "learning_rate": 4.74e-05,
      "loss": 0.0023,
      "step": 12480
    },
    {
      "epoch": 0.41633333333333333,
      "grad_norm": 0.3043546676635742,
      "learning_rate": 4.739791666666667e-05,
      "loss": 0.0024,
      "step": 12490
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.18479447066783905,
      "learning_rate": 4.739583333333333e-05,
      "loss": 0.0035,
      "step": 12500
    },
    {
      "epoch": 0.417,
      "grad_norm": 0.24333913624286652,
      "learning_rate": 4.7393750000000003e-05,
      "loss": 0.0026,
      "step": 12510
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 0.3954021632671356,
      "learning_rate": 4.739166666666667e-05,
      "loss": 0.0043,
      "step": 12520
    },
    {
      "epoch": 0.4176666666666667,
      "grad_norm": 0.42520657181739807,
      "learning_rate": 4.7389583333333334e-05,
      "loss": 0.0034,
      "step": 12530
    },
    {
      "epoch": 0.418,
      "grad_norm": 0.12200750410556793,
      "learning_rate": 4.73875e-05,
      "loss": 0.0032,
      "step": 12540
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.27429071068763733,
      "learning_rate": 4.738541666666667e-05,
      "loss": 0.004,
      "step": 12550
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.21300996840000153,
      "learning_rate": 4.738333333333334e-05,
      "loss": 0.004,
      "step": 12560
    },
    {
      "epoch": 0.419,
      "grad_norm": 0.12205666303634644,
      "learning_rate": 4.738125e-05,
      "loss": 0.0029,
      "step": 12570
    },
    {
      "epoch": 0.41933333333333334,
      "grad_norm": 0.3949713110923767,
      "learning_rate": 4.737916666666667e-05,
      "loss": 0.0037,
      "step": 12580
    },
    {
      "epoch": 0.4196666666666667,
      "grad_norm": 0.33351773023605347,
      "learning_rate": 4.7377083333333334e-05,
      "loss": 0.004,
      "step": 12590
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.06192287057638168,
      "learning_rate": 4.7375e-05,
      "loss": 0.0034,
      "step": 12600
    },
    {
      "epoch": 0.42033333333333334,
      "grad_norm": 0.03161672502756119,
      "learning_rate": 4.7372916666666665e-05,
      "loss": 0.0035,
      "step": 12610
    },
    {
      "epoch": 0.4206666666666667,
      "grad_norm": 0.1532951444387436,
      "learning_rate": 4.737083333333334e-05,
      "loss": 0.0031,
      "step": 12620
    },
    {
      "epoch": 0.421,
      "grad_norm": 0.36420488357543945,
      "learning_rate": 4.736875e-05,
      "loss": 0.0027,
      "step": 12630
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.38064873218536377,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.0034,
      "step": 12640
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.06181938573718071,
      "learning_rate": 4.7364583333333334e-05,
      "loss": 0.003,
      "step": 12650
    },
    {
      "epoch": 0.422,
      "grad_norm": 0.364632785320282,
      "learning_rate": 4.7362500000000006e-05,
      "loss": 0.0036,
      "step": 12660
    },
    {
      "epoch": 0.42233333333333334,
      "grad_norm": 0.36460864543914795,
      "learning_rate": 4.736041666666667e-05,
      "loss": 0.0034,
      "step": 12670
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 0.3137913942337036,
      "learning_rate": 4.735833333333334e-05,
      "loss": 0.0041,
      "step": 12680
    },
    {
      "epoch": 0.423,
      "grad_norm": 0.09234681725502014,
      "learning_rate": 4.735625e-05,
      "loss": 0.0027,
      "step": 12690
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.27076518535614014,
      "learning_rate": 4.7354166666666675e-05,
      "loss": 0.0024,
      "step": 12700
    },
    {
      "epoch": 0.4236666666666667,
      "grad_norm": 0.062137749046087265,
      "learning_rate": 4.7352083333333333e-05,
      "loss": 0.0033,
      "step": 12710
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.06318920850753784,
      "learning_rate": 4.735e-05,
      "loss": 0.0048,
      "step": 12720
    },
    {
      "epoch": 0.42433333333333334,
      "grad_norm": 0.1218329593539238,
      "learning_rate": 4.734791666666667e-05,
      "loss": 0.0028,
      "step": 12730
    },
    {
      "epoch": 0.4246666666666667,
      "grad_norm": 0.03168544918298721,
      "learning_rate": 4.734583333333334e-05,
      "loss": 0.0028,
      "step": 12740
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.09165868908166885,
      "learning_rate": 4.734375e-05,
      "loss": 0.0034,
      "step": 12750
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 0.3643511235713959,
      "learning_rate": 4.734166666666667e-05,
      "loss": 0.0034,
      "step": 12760
    },
    {
      "epoch": 0.4256666666666667,
      "grad_norm": 0.033669546246528625,
      "learning_rate": 4.733958333333334e-05,
      "loss": 0.0027,
      "step": 12770
    },
    {
      "epoch": 0.426,
      "grad_norm": 0.09184927493333817,
      "learning_rate": 4.73375e-05,
      "loss": 0.0019,
      "step": 12780
    },
    {
      "epoch": 0.42633333333333334,
      "grad_norm": 0.427428662776947,
      "learning_rate": 4.733541666666667e-05,
      "loss": 0.0037,
      "step": 12790
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.39445802569389343,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.003,
      "step": 12800
    },
    {
      "epoch": 0.427,
      "grad_norm": 0.1821681708097458,
      "learning_rate": 4.733125e-05,
      "loss": 0.0043,
      "step": 12810
    },
    {
      "epoch": 0.42733333333333334,
      "grad_norm": 0.18292108178138733,
      "learning_rate": 4.732916666666667e-05,
      "loss": 0.0028,
      "step": 12820
    },
    {
      "epoch": 0.42766666666666664,
      "grad_norm": 0.4919548034667969,
      "learning_rate": 4.732708333333333e-05,
      "loss": 0.0028,
      "step": 12830
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.18273894488811493,
      "learning_rate": 4.7325000000000005e-05,
      "loss": 0.0026,
      "step": 12840
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.5461113452911377,
      "learning_rate": 4.7322916666666664e-05,
      "loss": 0.0037,
      "step": 12850
    },
    {
      "epoch": 0.42866666666666664,
      "grad_norm": 0.36351633071899414,
      "learning_rate": 4.7320833333333336e-05,
      "loss": 0.003,
      "step": 12860
    },
    {
      "epoch": 0.429,
      "grad_norm": 0.3937947750091553,
      "learning_rate": 4.731875e-05,
      "loss": 0.0032,
      "step": 12870
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.18176288902759552,
      "learning_rate": 4.731666666666667e-05,
      "loss": 0.003,
      "step": 12880
    },
    {
      "epoch": 0.42966666666666664,
      "grad_norm": 0.4072636365890503,
      "learning_rate": 4.731458333333333e-05,
      "loss": 0.0024,
      "step": 12890
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.27276426553726196,
      "learning_rate": 4.7312500000000005e-05,
      "loss": 0.0037,
      "step": 12900
    },
    {
      "epoch": 0.43033333333333335,
      "grad_norm": 0.4255322813987732,
      "learning_rate": 4.731041666666667e-05,
      "loss": 0.0022,
      "step": 12910
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 0.18207275867462158,
      "learning_rate": 4.7308333333333336e-05,
      "loss": 0.003,
      "step": 12920
    },
    {
      "epoch": 0.431,
      "grad_norm": 0.09187132865190506,
      "learning_rate": 4.730625e-05,
      "loss": 0.0032,
      "step": 12930
    },
    {
      "epoch": 0.43133333333333335,
      "grad_norm": 0.2133469581604004,
      "learning_rate": 4.7304166666666674e-05,
      "loss": 0.0018,
      "step": 12940
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.546191394329071,
      "learning_rate": 4.730208333333334e-05,
      "loss": 0.0049,
      "step": 12950
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.4554673135280609,
      "learning_rate": 4.73e-05,
      "loss": 0.0042,
      "step": 12960
    },
    {
      "epoch": 0.43233333333333335,
      "grad_norm": 0.23725596070289612,
      "learning_rate": 4.729791666666667e-05,
      "loss": 0.0047,
      "step": 12970
    },
    {
      "epoch": 0.43266666666666664,
      "grad_norm": 0.31248992681503296,
      "learning_rate": 4.7295833333333336e-05,
      "loss": 0.0028,
      "step": 12980
    },
    {
      "epoch": 0.433,
      "grad_norm": 0.18256890773773193,
      "learning_rate": 4.729375e-05,
      "loss": 0.002,
      "step": 12990
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.4067436158657074,
      "learning_rate": 4.7291666666666666e-05,
      "loss": 0.0049,
      "step": 13000
    },
    {
      "epoch": 0.43366666666666664,
      "grad_norm": 0.1542774885892868,
      "learning_rate": 4.728958333333334e-05,
      "loss": 0.0032,
      "step": 13010
    },
    {
      "epoch": 0.434,
      "grad_norm": 0.09184843301773071,
      "learning_rate": 4.7287500000000004e-05,
      "loss": 0.0033,
      "step": 13020
    },
    {
      "epoch": 0.43433333333333335,
      "grad_norm": 0.27333155274391174,
      "learning_rate": 4.728541666666667e-05,
      "loss": 0.0029,
      "step": 13030
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.30363908410072327,
      "learning_rate": 4.7283333333333335e-05,
      "loss": 0.0026,
      "step": 13040
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.2293417751789093,
      "learning_rate": 4.728125000000001e-05,
      "loss": 0.004,
      "step": 13050
    },
    {
      "epoch": 0.43533333333333335,
      "grad_norm": 0.13445863127708435,
      "learning_rate": 4.7279166666666666e-05,
      "loss": 0.0049,
      "step": 13060
    },
    {
      "epoch": 0.43566666666666665,
      "grad_norm": 0.6993130445480347,
      "learning_rate": 4.727708333333333e-05,
      "loss": 0.0038,
      "step": 13070
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.2125011682510376,
      "learning_rate": 4.7275000000000004e-05,
      "loss": 0.0029,
      "step": 13080
    },
    {
      "epoch": 0.43633333333333335,
      "grad_norm": 0.4748225212097168,
      "learning_rate": 4.727291666666667e-05,
      "loss": 0.003,
      "step": 13090
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.012811882421374321,
      "learning_rate": 4.7270833333333335e-05,
      "loss": 0.0038,
      "step": 13100
    },
    {
      "epoch": 0.437,
      "grad_norm": 0.21266746520996094,
      "learning_rate": 4.726875e-05,
      "loss": 0.0036,
      "step": 13110
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.3033878803253174,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0028,
      "step": 13120
    },
    {
      "epoch": 0.43766666666666665,
      "grad_norm": 0.30406785011291504,
      "learning_rate": 4.726458333333333e-05,
      "loss": 0.0033,
      "step": 13130
    },
    {
      "epoch": 0.438,
      "grad_norm": 0.5767424702644348,
      "learning_rate": 4.7262500000000004e-05,
      "loss": 0.0031,
      "step": 13140
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.06135532632470131,
      "learning_rate": 4.726041666666667e-05,
      "loss": 0.0031,
      "step": 13150
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 0.06216058135032654,
      "learning_rate": 4.7258333333333335e-05,
      "loss": 0.0046,
      "step": 13160
    },
    {
      "epoch": 0.439,
      "grad_norm": 0.575739860534668,
      "learning_rate": 4.725625e-05,
      "loss": 0.0023,
      "step": 13170
    },
    {
      "epoch": 0.43933333333333335,
      "grad_norm": 0.26699385046958923,
      "learning_rate": 4.725416666666667e-05,
      "loss": 0.0039,
      "step": 13180
    },
    {
      "epoch": 0.43966666666666665,
      "grad_norm": 0.666912853717804,
      "learning_rate": 4.725208333333334e-05,
      "loss": 0.0044,
      "step": 13190
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.5878170132637024,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.0037,
      "step": 13200
    },
    {
      "epoch": 0.44033333333333335,
      "grad_norm": 0.21284319460391998,
      "learning_rate": 4.724791666666667e-05,
      "loss": 0.0041,
      "step": 13210
    },
    {
      "epoch": 0.44066666666666665,
      "grad_norm": 0.40470433235168457,
      "learning_rate": 4.7245833333333334e-05,
      "loss": 0.0029,
      "step": 13220
    },
    {
      "epoch": 0.441,
      "grad_norm": 0.5668863654136658,
      "learning_rate": 4.7243750000000007e-05,
      "loss": 0.0042,
      "step": 13230
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 0.06162737309932709,
      "learning_rate": 4.7241666666666665e-05,
      "loss": 0.0031,
      "step": 13240
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.27283361554145813,
      "learning_rate": 4.723958333333334e-05,
      "loss": 0.0041,
      "step": 13250
    },
    {
      "epoch": 0.442,
      "grad_norm": 0.30325356125831604,
      "learning_rate": 4.72375e-05,
      "loss": 0.004,
      "step": 13260
    },
    {
      "epoch": 0.44233333333333336,
      "grad_norm": 0.11261026561260223,
      "learning_rate": 4.723541666666667e-05,
      "loss": 0.0023,
      "step": 13270
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.03225339576601982,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0024,
      "step": 13280
    },
    {
      "epoch": 0.443,
      "grad_norm": 0.36723583936691284,
      "learning_rate": 4.7231250000000006e-05,
      "loss": 0.0022,
      "step": 13290
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.061189886182546616,
      "learning_rate": 4.722916666666667e-05,
      "loss": 0.0019,
      "step": 13300
    },
    {
      "epoch": 0.44366666666666665,
      "grad_norm": 0.4247422218322754,
      "learning_rate": 4.722708333333333e-05,
      "loss": 0.0027,
      "step": 13310
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.2130640298128128,
      "learning_rate": 4.7225e-05,
      "loss": 0.0034,
      "step": 13320
    },
    {
      "epoch": 0.44433333333333336,
      "grad_norm": 0.4338735342025757,
      "learning_rate": 4.722291666666667e-05,
      "loss": 0.0031,
      "step": 13330
    },
    {
      "epoch": 0.44466666666666665,
      "grad_norm": 0.27296045422554016,
      "learning_rate": 4.7220833333333334e-05,
      "loss": 0.0035,
      "step": 13340
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.7877846360206604,
      "learning_rate": 4.721875e-05,
      "loss": 0.0019,
      "step": 13350
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.30303049087524414,
      "learning_rate": 4.721666666666667e-05,
      "loss": 0.0029,
      "step": 13360
    },
    {
      "epoch": 0.44566666666666666,
      "grad_norm": 0.12280157953500748,
      "learning_rate": 4.721458333333334e-05,
      "loss": 0.0035,
      "step": 13370
    },
    {
      "epoch": 0.446,
      "grad_norm": 0.5152952075004578,
      "learning_rate": 4.72125e-05,
      "loss": 0.0035,
      "step": 13380
    },
    {
      "epoch": 0.44633333333333336,
      "grad_norm": 0.5312824249267578,
      "learning_rate": 4.721041666666667e-05,
      "loss": 0.0021,
      "step": 13390
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.12177520990371704,
      "learning_rate": 4.720833333333334e-05,
      "loss": 0.0027,
      "step": 13400
    },
    {
      "epoch": 0.447,
      "grad_norm": 0.3045050799846649,
      "learning_rate": 4.720625e-05,
      "loss": 0.0018,
      "step": 13410
    },
    {
      "epoch": 0.44733333333333336,
      "grad_norm": 0.20653760433197021,
      "learning_rate": 4.720416666666667e-05,
      "loss": 0.0033,
      "step": 13420
    },
    {
      "epoch": 0.44766666666666666,
      "grad_norm": 0.21272297203540802,
      "learning_rate": 4.720208333333334e-05,
      "loss": 0.0027,
      "step": 13430
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.12144815921783447,
      "learning_rate": 4.72e-05,
      "loss": 0.0033,
      "step": 13440
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.09208530187606812,
      "learning_rate": 4.719791666666667e-05,
      "loss": 0.0024,
      "step": 13450
    },
    {
      "epoch": 0.44866666666666666,
      "grad_norm": 0.49882712960243225,
      "learning_rate": 4.719583333333333e-05,
      "loss": 0.003,
      "step": 13460
    },
    {
      "epoch": 0.449,
      "grad_norm": 0.27913355827331543,
      "learning_rate": 4.7193750000000005e-05,
      "loss": 0.0026,
      "step": 13470
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 0.12315952777862549,
      "learning_rate": 4.7191666666666664e-05,
      "loss": 0.0024,
      "step": 13480
    },
    {
      "epoch": 0.44966666666666666,
      "grad_norm": 0.15940535068511963,
      "learning_rate": 4.7189583333333336e-05,
      "loss": 0.0032,
      "step": 13490
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.06272429972887039,
      "learning_rate": 4.71875e-05,
      "loss": 0.0025,
      "step": 13500
    },
    {
      "epoch": 0.4503333333333333,
      "grad_norm": 0.12149234116077423,
      "learning_rate": 4.7185416666666674e-05,
      "loss": 0.0038,
      "step": 13510
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.27696213126182556,
      "learning_rate": 4.718333333333333e-05,
      "loss": 0.0036,
      "step": 13520
    },
    {
      "epoch": 0.451,
      "grad_norm": 0.06165161356329918,
      "learning_rate": 4.7181250000000005e-05,
      "loss": 0.004,
      "step": 13530
    },
    {
      "epoch": 0.4513333333333333,
      "grad_norm": 0.06206769496202469,
      "learning_rate": 4.717916666666667e-05,
      "loss": 0.0028,
      "step": 13540
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.09275942295789719,
      "learning_rate": 4.7177083333333336e-05,
      "loss": 0.0028,
      "step": 13550
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.363563597202301,
      "learning_rate": 4.7175e-05,
      "loss": 0.0026,
      "step": 13560
    },
    {
      "epoch": 0.4523333333333333,
      "grad_norm": 0.15316268801689148,
      "learning_rate": 4.717291666666667e-05,
      "loss": 0.0035,
      "step": 13570
    },
    {
      "epoch": 0.45266666666666666,
      "grad_norm": 0.281254380941391,
      "learning_rate": 4.717083333333334e-05,
      "loss": 0.0034,
      "step": 13580
    },
    {
      "epoch": 0.453,
      "grad_norm": 0.09227194637060165,
      "learning_rate": 4.716875e-05,
      "loss": 0.0037,
      "step": 13590
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.09113427996635437,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0038,
      "step": 13600
    },
    {
      "epoch": 0.45366666666666666,
      "grad_norm": 0.15322449803352356,
      "learning_rate": 4.7164583333333336e-05,
      "loss": 0.0024,
      "step": 13610
    },
    {
      "epoch": 0.454,
      "grad_norm": 0.03319227695465088,
      "learning_rate": 4.71625e-05,
      "loss": 0.0034,
      "step": 13620
    },
    {
      "epoch": 0.4543333333333333,
      "grad_norm": 0.4237644374370575,
      "learning_rate": 4.716041666666667e-05,
      "loss": 0.0028,
      "step": 13630
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 0.365987092256546,
      "learning_rate": 4.715833333333334e-05,
      "loss": 0.0035,
      "step": 13640
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.514706552028656,
      "learning_rate": 4.7156250000000004e-05,
      "loss": 0.0031,
      "step": 13650
    },
    {
      "epoch": 0.4553333333333333,
      "grad_norm": 0.09770071506500244,
      "learning_rate": 4.715416666666667e-05,
      "loss": 0.0024,
      "step": 13660
    },
    {
      "epoch": 0.45566666666666666,
      "grad_norm": 0.22814403474330902,
      "learning_rate": 4.7152083333333335e-05,
      "loss": 0.004,
      "step": 13670
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.22175805270671844,
      "learning_rate": 4.715e-05,
      "loss": 0.0039,
      "step": 13680
    },
    {
      "epoch": 0.4563333333333333,
      "grad_norm": 0.32587090134620667,
      "learning_rate": 4.7147916666666666e-05,
      "loss": 0.0035,
      "step": 13690
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.24293294548988342,
      "learning_rate": 4.714583333333333e-05,
      "loss": 0.0026,
      "step": 13700
    },
    {
      "epoch": 0.457,
      "grad_norm": 0.2728445529937744,
      "learning_rate": 4.7143750000000004e-05,
      "loss": 0.0024,
      "step": 13710
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 0.2741452157497406,
      "learning_rate": 4.714166666666667e-05,
      "loss": 0.0029,
      "step": 13720
    },
    {
      "epoch": 0.45766666666666667,
      "grad_norm": 0.2723845839500427,
      "learning_rate": 4.7139583333333335e-05,
      "loss": 0.003,
      "step": 13730
    },
    {
      "epoch": 0.458,
      "grad_norm": 0.3947686553001404,
      "learning_rate": 4.71375e-05,
      "loss": 0.0029,
      "step": 13740
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.2756780683994293,
      "learning_rate": 4.713541666666667e-05,
      "loss": 0.0028,
      "step": 13750
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.18152838945388794,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.004,
      "step": 13760
    },
    {
      "epoch": 0.459,
      "grad_norm": 0.2403448224067688,
      "learning_rate": 4.7131250000000004e-05,
      "loss": 0.0022,
      "step": 13770
    },
    {
      "epoch": 0.4593333333333333,
      "grad_norm": 0.21343152225017548,
      "learning_rate": 4.712916666666667e-05,
      "loss": 0.0027,
      "step": 13780
    },
    {
      "epoch": 0.45966666666666667,
      "grad_norm": 0.4340047836303711,
      "learning_rate": 4.712708333333334e-05,
      "loss": 0.0034,
      "step": 13790
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.12358643859624863,
      "learning_rate": 4.7125e-05,
      "loss": 0.0021,
      "step": 13800
    },
    {
      "epoch": 0.4603333333333333,
      "grad_norm": 0.602344810962677,
      "learning_rate": 4.7122916666666666e-05,
      "loss": 0.0027,
      "step": 13810
    },
    {
      "epoch": 0.46066666666666667,
      "grad_norm": 0.3334696590900421,
      "learning_rate": 4.712083333333334e-05,
      "loss": 0.003,
      "step": 13820
    },
    {
      "epoch": 0.461,
      "grad_norm": 0.2232586145401001,
      "learning_rate": 4.7118750000000004e-05,
      "loss": 0.0032,
      "step": 13830
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.36326491832733154,
      "learning_rate": 4.711666666666667e-05,
      "loss": 0.0034,
      "step": 13840
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.5005956888198853,
      "learning_rate": 4.7114583333333334e-05,
      "loss": 0.0033,
      "step": 13850
    },
    {
      "epoch": 0.462,
      "grad_norm": 0.12841381132602692,
      "learning_rate": 4.711250000000001e-05,
      "loss": 0.0055,
      "step": 13860
    },
    {
      "epoch": 0.4623333333333333,
      "grad_norm": 0.15250273048877716,
      "learning_rate": 4.7110416666666665e-05,
      "loss": 0.0022,
      "step": 13870
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 0.3635866940021515,
      "learning_rate": 4.710833333333334e-05,
      "loss": 0.0038,
      "step": 13880
    },
    {
      "epoch": 0.463,
      "grad_norm": 0.42497897148132324,
      "learning_rate": 4.710625e-05,
      "loss": 0.0038,
      "step": 13890
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.0616401731967926,
      "learning_rate": 4.710416666666667e-05,
      "loss": 0.0034,
      "step": 13900
    },
    {
      "epoch": 0.46366666666666667,
      "grad_norm": 0.4542604982852936,
      "learning_rate": 4.7102083333333334e-05,
      "loss": 0.0028,
      "step": 13910
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.5151271820068359,
      "learning_rate": 4.71e-05,
      "loss": 0.0032,
      "step": 13920
    },
    {
      "epoch": 0.4643333333333333,
      "grad_norm": 0.3552311956882477,
      "learning_rate": 4.709791666666667e-05,
      "loss": 0.0036,
      "step": 13930
    },
    {
      "epoch": 0.4646666666666667,
      "grad_norm": 0.3944043219089508,
      "learning_rate": 4.709583333333333e-05,
      "loss": 0.0031,
      "step": 13940
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.03071817196905613,
      "learning_rate": 4.709375e-05,
      "loss": 0.0034,
      "step": 13950
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 0.43601828813552856,
      "learning_rate": 4.709166666666667e-05,
      "loss": 0.0035,
      "step": 13960
    },
    {
      "epoch": 0.4656666666666667,
      "grad_norm": 0.21240849792957306,
      "learning_rate": 4.7089583333333334e-05,
      "loss": 0.0025,
      "step": 13970
    },
    {
      "epoch": 0.466,
      "grad_norm": 0.09160664677619934,
      "learning_rate": 4.70875e-05,
      "loss": 0.0025,
      "step": 13980
    },
    {
      "epoch": 0.4663333333333333,
      "grad_norm": 0.06092722713947296,
      "learning_rate": 4.708541666666667e-05,
      "loss": 0.0034,
      "step": 13990
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.11825598031282425,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.0014,
      "step": 14000
    },
    {
      "epoch": 0.467,
      "grad_norm": 0.12257526814937592,
      "learning_rate": 4.708125e-05,
      "loss": 0.0025,
      "step": 14010
    },
    {
      "epoch": 0.4673333333333333,
      "grad_norm": 0.3029744625091553,
      "learning_rate": 4.707916666666667e-05,
      "loss": 0.0033,
      "step": 14020
    },
    {
      "epoch": 0.4676666666666667,
      "grad_norm": 0.1516677588224411,
      "learning_rate": 4.707708333333334e-05,
      "loss": 0.0022,
      "step": 14030
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.03209725767374039,
      "learning_rate": 4.7075e-05,
      "loss": 0.0024,
      "step": 14040
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.6063690185546875,
      "learning_rate": 4.7072916666666665e-05,
      "loss": 0.0023,
      "step": 14050
    },
    {
      "epoch": 0.4686666666666667,
      "grad_norm": 0.6057616472244263,
      "learning_rate": 4.707083333333334e-05,
      "loss": 0.0029,
      "step": 14060
    },
    {
      "epoch": 0.469,
      "grad_norm": 0.18570657074451447,
      "learning_rate": 4.706875e-05,
      "loss": 0.0032,
      "step": 14070
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.13087351620197296,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0033,
      "step": 14080
    },
    {
      "epoch": 0.4696666666666667,
      "grad_norm": 0.5143841505050659,
      "learning_rate": 4.706458333333333e-05,
      "loss": 0.0029,
      "step": 14090
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.27268075942993164,
      "learning_rate": 4.7062500000000006e-05,
      "loss": 0.0024,
      "step": 14100
    },
    {
      "epoch": 0.4703333333333333,
      "grad_norm": 0.010154261253774166,
      "learning_rate": 4.706041666666667e-05,
      "loss": 0.0043,
      "step": 14110
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 0.12222479283809662,
      "learning_rate": 4.7058333333333337e-05,
      "loss": 0.0024,
      "step": 14120
    },
    {
      "epoch": 0.471,
      "grad_norm": 0.032878849655389786,
      "learning_rate": 4.705625e-05,
      "loss": 0.0026,
      "step": 14130
    },
    {
      "epoch": 0.4713333333333333,
      "grad_norm": 0.18188884854316711,
      "learning_rate": 4.7054166666666674e-05,
      "loss": 0.0042,
      "step": 14140
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.6079850792884827,
      "learning_rate": 4.705208333333333e-05,
      "loss": 0.0021,
      "step": 14150
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.03119444102048874,
      "learning_rate": 4.705e-05,
      "loss": 0.0029,
      "step": 14160
    },
    {
      "epoch": 0.4723333333333333,
      "grad_norm": 0.3028779625892639,
      "learning_rate": 4.704791666666667e-05,
      "loss": 0.0025,
      "step": 14170
    },
    {
      "epoch": 0.4726666666666667,
      "grad_norm": 0.3886486291885376,
      "learning_rate": 4.7045833333333336e-05,
      "loss": 0.003,
      "step": 14180
    },
    {
      "epoch": 0.473,
      "grad_norm": 0.5746887922286987,
      "learning_rate": 4.704375e-05,
      "loss": 0.0028,
      "step": 14190
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.15118418633937836,
      "learning_rate": 4.704166666666667e-05,
      "loss": 0.0025,
      "step": 14200
    },
    {
      "epoch": 0.4736666666666667,
      "grad_norm": 0.3939701020717621,
      "learning_rate": 4.703958333333334e-05,
      "loss": 0.003,
      "step": 14210
    },
    {
      "epoch": 0.474,
      "grad_norm": 0.45402592420578003,
      "learning_rate": 4.70375e-05,
      "loss": 0.0034,
      "step": 14220
    },
    {
      "epoch": 0.47433333333333333,
      "grad_norm": 0.2382064312696457,
      "learning_rate": 4.703541666666667e-05,
      "loss": 0.0025,
      "step": 14230
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.24370387196540833,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.0033,
      "step": 14240
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.21371746063232422,
      "learning_rate": 4.703125e-05,
      "loss": 0.0022,
      "step": 14250
    },
    {
      "epoch": 0.47533333333333333,
      "grad_norm": 0.09134001284837723,
      "learning_rate": 4.702916666666667e-05,
      "loss": 0.0036,
      "step": 14260
    },
    {
      "epoch": 0.4756666666666667,
      "grad_norm": 0.15152165293693542,
      "learning_rate": 4.702708333333334e-05,
      "loss": 0.0033,
      "step": 14270
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.21314916014671326,
      "learning_rate": 4.7025000000000005e-05,
      "loss": 0.0034,
      "step": 14280
    },
    {
      "epoch": 0.47633333333333333,
      "grad_norm": 0.48316067457199097,
      "learning_rate": 4.702291666666666e-05,
      "loss": 0.003,
      "step": 14290
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.1226375550031662,
      "learning_rate": 4.7020833333333336e-05,
      "loss": 0.0026,
      "step": 14300
    },
    {
      "epoch": 0.477,
      "grad_norm": 0.30936890840530396,
      "learning_rate": 4.701875e-05,
      "loss": 0.0016,
      "step": 14310
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.0912853553891182,
      "learning_rate": 4.701666666666667e-05,
      "loss": 0.0029,
      "step": 14320
    },
    {
      "epoch": 0.4776666666666667,
      "grad_norm": 0.06322001665830612,
      "learning_rate": 4.701458333333333e-05,
      "loss": 0.0037,
      "step": 14330
    },
    {
      "epoch": 0.478,
      "grad_norm": 0.09120432287454605,
      "learning_rate": 4.7012500000000004e-05,
      "loss": 0.0033,
      "step": 14340
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.5762662291526794,
      "learning_rate": 4.701041666666667e-05,
      "loss": 0.0025,
      "step": 14350
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 0.554497241973877,
      "learning_rate": 4.7008333333333335e-05,
      "loss": 0.0032,
      "step": 14360
    },
    {
      "epoch": 0.479,
      "grad_norm": 0.12134701013565063,
      "learning_rate": 4.700625e-05,
      "loss": 0.0039,
      "step": 14370
    },
    {
      "epoch": 0.47933333333333333,
      "grad_norm": 0.09160393476486206,
      "learning_rate": 4.700416666666667e-05,
      "loss": 0.0029,
      "step": 14380
    },
    {
      "epoch": 0.4796666666666667,
      "grad_norm": 0.07756166905164719,
      "learning_rate": 4.700208333333334e-05,
      "loss": 0.003,
      "step": 14390
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.24216228723526,
      "learning_rate": 4.7e-05,
      "loss": 0.0014,
      "step": 14400
    },
    {
      "epoch": 0.48033333333333333,
      "grad_norm": 0.45381200313568115,
      "learning_rate": 4.699791666666667e-05,
      "loss": 0.0032,
      "step": 14410
    },
    {
      "epoch": 0.4806666666666667,
      "grad_norm": 0.0918421521782875,
      "learning_rate": 4.6995833333333335e-05,
      "loss": 0.0046,
      "step": 14420
    },
    {
      "epoch": 0.481,
      "grad_norm": 0.09139040112495422,
      "learning_rate": 4.699375e-05,
      "loss": 0.0037,
      "step": 14430
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 0.06129005551338196,
      "learning_rate": 4.6991666666666666e-05,
      "loss": 0.0043,
      "step": 14440
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.09137193858623505,
      "learning_rate": 4.698958333333334e-05,
      "loss": 0.0031,
      "step": 14450
    },
    {
      "epoch": 0.482,
      "grad_norm": 0.1216716542840004,
      "learning_rate": 4.6987500000000004e-05,
      "loss": 0.0041,
      "step": 14460
    },
    {
      "epoch": 0.48233333333333334,
      "grad_norm": 0.18184366822242737,
      "learning_rate": 4.698541666666667e-05,
      "loss": 0.0024,
      "step": 14470
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.03130907937884331,
      "learning_rate": 4.6983333333333335e-05,
      "loss": 0.0027,
      "step": 14480
    },
    {
      "epoch": 0.483,
      "grad_norm": 0.44136184453964233,
      "learning_rate": 4.698125000000001e-05,
      "loss": 0.0053,
      "step": 14490
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.23783551156520844,
      "learning_rate": 4.6979166666666666e-05,
      "loss": 0.0035,
      "step": 14500
    },
    {
      "epoch": 0.4836666666666667,
      "grad_norm": 0.06288312375545502,
      "learning_rate": 4.697708333333334e-05,
      "loss": 0.0032,
      "step": 14510
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.21176710724830627,
      "learning_rate": 4.6975000000000003e-05,
      "loss": 0.0026,
      "step": 14520
    },
    {
      "epoch": 0.48433333333333334,
      "grad_norm": 0.03396528586745262,
      "learning_rate": 4.697291666666667e-05,
      "loss": 0.003,
      "step": 14530
    },
    {
      "epoch": 0.4846666666666667,
      "grad_norm": 0.44587957859039307,
      "learning_rate": 4.6970833333333334e-05,
      "loss": 0.0022,
      "step": 14540
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.12117764353752136,
      "learning_rate": 4.696875e-05,
      "loss": 0.0026,
      "step": 14550
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.09111150354146957,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0036,
      "step": 14560
    },
    {
      "epoch": 0.4856666666666667,
      "grad_norm": 0.24278295040130615,
      "learning_rate": 4.696458333333333e-05,
      "loss": 0.0035,
      "step": 14570
    },
    {
      "epoch": 0.486,
      "grad_norm": 0.12140955030918121,
      "learning_rate": 4.69625e-05,
      "loss": 0.0027,
      "step": 14580
    },
    {
      "epoch": 0.48633333333333334,
      "grad_norm": 0.09073905646800995,
      "learning_rate": 4.696041666666667e-05,
      "loss": 0.0021,
      "step": 14590
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.2720319330692291,
      "learning_rate": 4.695833333333334e-05,
      "loss": 0.0029,
      "step": 14600
    },
    {
      "epoch": 0.487,
      "grad_norm": 0.42387574911117554,
      "learning_rate": 4.695625e-05,
      "loss": 0.0026,
      "step": 14610
    },
    {
      "epoch": 0.48733333333333334,
      "grad_norm": 0.18168017268180847,
      "learning_rate": 4.695416666666667e-05,
      "loss": 0.0027,
      "step": 14620
    },
    {
      "epoch": 0.4876666666666667,
      "grad_norm": 0.5619274377822876,
      "learning_rate": 4.695208333333334e-05,
      "loss": 0.0031,
      "step": 14630
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.3989678919315338,
      "learning_rate": 4.695e-05,
      "loss": 0.0033,
      "step": 14640
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.3023711144924164,
      "learning_rate": 4.694791666666667e-05,
      "loss": 0.0022,
      "step": 14650
    },
    {
      "epoch": 0.4886666666666667,
      "grad_norm": 0.15163995325565338,
      "learning_rate": 4.6945833333333334e-05,
      "loss": 0.0024,
      "step": 14660
    },
    {
      "epoch": 0.489,
      "grad_norm": 0.39273518323898315,
      "learning_rate": 4.6943750000000006e-05,
      "loss": 0.0032,
      "step": 14670
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 0.44985446333885193,
      "learning_rate": 4.6941666666666665e-05,
      "loss": 0.0038,
      "step": 14680
    },
    {
      "epoch": 0.48966666666666664,
      "grad_norm": 0.12188354134559631,
      "learning_rate": 4.693958333333334e-05,
      "loss": 0.004,
      "step": 14690
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.015915634110569954,
      "learning_rate": 4.69375e-05,
      "loss": 0.0022,
      "step": 14700
    },
    {
      "epoch": 0.49033333333333334,
      "grad_norm": 0.15171480178833008,
      "learning_rate": 4.693541666666667e-05,
      "loss": 0.0034,
      "step": 14710
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.09175268560647964,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0043,
      "step": 14720
    },
    {
      "epoch": 0.491,
      "grad_norm": 0.06107863038778305,
      "learning_rate": 4.6931250000000006e-05,
      "loss": 0.0024,
      "step": 14730
    },
    {
      "epoch": 0.49133333333333334,
      "grad_norm": 0.21214188635349274,
      "learning_rate": 4.692916666666667e-05,
      "loss": 0.0034,
      "step": 14740
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.3027890920639038,
      "learning_rate": 4.692708333333334e-05,
      "loss": 0.0039,
      "step": 14750
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.39286699891090393,
      "learning_rate": 4.6925e-05,
      "loss": 0.0039,
      "step": 14760
    },
    {
      "epoch": 0.49233333333333335,
      "grad_norm": 0.12117194384336472,
      "learning_rate": 4.692291666666667e-05,
      "loss": 0.0035,
      "step": 14770
    },
    {
      "epoch": 0.49266666666666664,
      "grad_norm": 0.2719220519065857,
      "learning_rate": 4.692083333333333e-05,
      "loss": 0.0045,
      "step": 14780
    },
    {
      "epoch": 0.493,
      "grad_norm": 0.30249473452568054,
      "learning_rate": 4.691875e-05,
      "loss": 0.0026,
      "step": 14790
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.6697683334350586,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.003,
      "step": 14800
    },
    {
      "epoch": 0.49366666666666664,
      "grad_norm": 0.7577656507492065,
      "learning_rate": 4.6914583333333336e-05,
      "loss": 0.0031,
      "step": 14810
    },
    {
      "epoch": 0.494,
      "grad_norm": 0.03235896676778793,
      "learning_rate": 4.69125e-05,
      "loss": 0.003,
      "step": 14820
    },
    {
      "epoch": 0.49433333333333335,
      "grad_norm": 0.27350232005119324,
      "learning_rate": 4.691041666666667e-05,
      "loss": 0.0041,
      "step": 14830
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 0.3633640706539154,
      "learning_rate": 4.690833333333334e-05,
      "loss": 0.0026,
      "step": 14840
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.03261961042881012,
      "learning_rate": 4.690625e-05,
      "loss": 0.0034,
      "step": 14850
    },
    {
      "epoch": 0.49533333333333335,
      "grad_norm": 0.30207642912864685,
      "learning_rate": 4.690416666666667e-05,
      "loss": 0.0027,
      "step": 14860
    },
    {
      "epoch": 0.49566666666666664,
      "grad_norm": 0.1586165428161621,
      "learning_rate": 4.6902083333333336e-05,
      "loss": 0.0047,
      "step": 14870
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.3038603365421295,
      "learning_rate": 4.69e-05,
      "loss": 0.0038,
      "step": 14880
    },
    {
      "epoch": 0.49633333333333335,
      "grad_norm": 0.4540269076824188,
      "learning_rate": 4.689791666666667e-05,
      "loss": 0.0031,
      "step": 14890
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.27470481395721436,
      "learning_rate": 4.689583333333333e-05,
      "loss": 0.0027,
      "step": 14900
    },
    {
      "epoch": 0.497,
      "grad_norm": 0.06142504885792732,
      "learning_rate": 4.6893750000000005e-05,
      "loss": 0.0029,
      "step": 14910
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 0.2127062976360321,
      "learning_rate": 4.689166666666667e-05,
      "loss": 0.003,
      "step": 14920
    },
    {
      "epoch": 0.49766666666666665,
      "grad_norm": 0.42429274320602417,
      "learning_rate": 4.6889583333333336e-05,
      "loss": 0.0032,
      "step": 14930
    },
    {
      "epoch": 0.498,
      "grad_norm": 0.39347994327545166,
      "learning_rate": 4.68875e-05,
      "loss": 0.0025,
      "step": 14940
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.013121777214109898,
      "learning_rate": 4.6885416666666674e-05,
      "loss": 0.0035,
      "step": 14950
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.1833907812833786,
      "learning_rate": 4.688333333333333e-05,
      "loss": 0.0027,
      "step": 14960
    },
    {
      "epoch": 0.499,
      "grad_norm": 0.1812877207994461,
      "learning_rate": 4.6881250000000005e-05,
      "loss": 0.0021,
      "step": 14970
    },
    {
      "epoch": 0.49933333333333335,
      "grad_norm": 0.09091689437627792,
      "learning_rate": 4.687916666666667e-05,
      "loss": 0.004,
      "step": 14980
    },
    {
      "epoch": 0.49966666666666665,
      "grad_norm": 0.06328146159648895,
      "learning_rate": 4.6877083333333335e-05,
      "loss": 0.0028,
      "step": 14990
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.24268916249275208,
      "learning_rate": 4.6875e-05,
      "loss": 0.0019,
      "step": 15000
    },
    {
      "epoch": 0.5003333333333333,
      "grad_norm": 0.016389770433306694,
      "learning_rate": 4.6872916666666666e-05,
      "loss": 0.0027,
      "step": 15010
    },
    {
      "epoch": 0.5006666666666667,
      "grad_norm": 0.0908932164311409,
      "learning_rate": 4.687083333333334e-05,
      "loss": 0.0033,
      "step": 15020
    },
    {
      "epoch": 0.501,
      "grad_norm": 0.12205374985933304,
      "learning_rate": 4.686875e-05,
      "loss": 0.0038,
      "step": 15030
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.008749779313802719,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0034,
      "step": 15040
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.2413579374551773,
      "learning_rate": 4.6864583333333335e-05,
      "loss": 0.0025,
      "step": 15050
    },
    {
      "epoch": 0.502,
      "grad_norm": 0.12225393205881119,
      "learning_rate": 4.68625e-05,
      "loss": 0.0029,
      "step": 15060
    },
    {
      "epoch": 0.5023333333333333,
      "grad_norm": 0.18313579261302948,
      "learning_rate": 4.6860416666666666e-05,
      "loss": 0.0043,
      "step": 15070
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 0.24166986346244812,
      "learning_rate": 4.685833333333334e-05,
      "loss": 0.0043,
      "step": 15080
    },
    {
      "epoch": 0.503,
      "grad_norm": 0.03116234391927719,
      "learning_rate": 4.6856250000000004e-05,
      "loss": 0.0043,
      "step": 15090
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.15167702734470367,
      "learning_rate": 4.685416666666667e-05,
      "loss": 0.0026,
      "step": 15100
    },
    {
      "epoch": 0.5036666666666667,
      "grad_norm": 0.5127337574958801,
      "learning_rate": 4.6852083333333335e-05,
      "loss": 0.0032,
      "step": 15110
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.22706976532936096,
      "learning_rate": 4.685000000000001e-05,
      "loss": 0.0035,
      "step": 15120
    },
    {
      "epoch": 0.5043333333333333,
      "grad_norm": 0.5140789747238159,
      "learning_rate": 4.6847916666666666e-05,
      "loss": 0.0042,
      "step": 15130
    },
    {
      "epoch": 0.5046666666666667,
      "grad_norm": 0.24387559294700623,
      "learning_rate": 4.684583333333333e-05,
      "loss": 0.0042,
      "step": 15140
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.30202439427375793,
      "learning_rate": 4.6843750000000004e-05,
      "loss": 0.0042,
      "step": 15150
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 0.06239345669746399,
      "learning_rate": 4.684166666666667e-05,
      "loss": 0.0024,
      "step": 15160
    },
    {
      "epoch": 0.5056666666666667,
      "grad_norm": 0.48362410068511963,
      "learning_rate": 4.6839583333333335e-05,
      "loss": 0.0039,
      "step": 15170
    },
    {
      "epoch": 0.506,
      "grad_norm": 0.42288529872894287,
      "learning_rate": 4.68375e-05,
      "loss": 0.0029,
      "step": 15180
    },
    {
      "epoch": 0.5063333333333333,
      "grad_norm": 0.09352920949459076,
      "learning_rate": 4.683541666666667e-05,
      "loss": 0.0036,
      "step": 15190
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.21292929351329803,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0021,
      "step": 15200
    },
    {
      "epoch": 0.507,
      "grad_norm": 0.4842492640018463,
      "learning_rate": 4.683125e-05,
      "loss": 0.0035,
      "step": 15210
    },
    {
      "epoch": 0.5073333333333333,
      "grad_norm": 0.2760615646839142,
      "learning_rate": 4.682916666666667e-05,
      "loss": 0.003,
      "step": 15220
    },
    {
      "epoch": 0.5076666666666667,
      "grad_norm": 0.48395827412605286,
      "learning_rate": 4.682708333333334e-05,
      "loss": 0.004,
      "step": 15230
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.27272093296051025,
      "learning_rate": 4.6825e-05,
      "loss": 0.0027,
      "step": 15240
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.16533663868904114,
      "learning_rate": 4.6822916666666665e-05,
      "loss": 0.0034,
      "step": 15250
    },
    {
      "epoch": 0.5086666666666667,
      "grad_norm": 0.41950881481170654,
      "learning_rate": 4.682083333333334e-05,
      "loss": 0.0027,
      "step": 15260
    },
    {
      "epoch": 0.509,
      "grad_norm": 0.09051701426506042,
      "learning_rate": 4.681875e-05,
      "loss": 0.0023,
      "step": 15270
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.2718033790588379,
      "learning_rate": 4.681666666666667e-05,
      "loss": 0.0031,
      "step": 15280
    },
    {
      "epoch": 0.5096666666666667,
      "grad_norm": 0.21205157041549683,
      "learning_rate": 4.6814583333333334e-05,
      "loss": 0.0036,
      "step": 15290
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.2722841501235962,
      "learning_rate": 4.6812500000000006e-05,
      "loss": 0.0029,
      "step": 15300
    },
    {
      "epoch": 0.5103333333333333,
      "grad_norm": 0.42345690727233887,
      "learning_rate": 4.6810416666666665e-05,
      "loss": 0.0025,
      "step": 15310
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 0.21119526028633118,
      "learning_rate": 4.680833333333334e-05,
      "loss": 0.0037,
      "step": 15320
    },
    {
      "epoch": 0.511,
      "grad_norm": 0.15086378157138824,
      "learning_rate": 4.680625e-05,
      "loss": 0.0025,
      "step": 15330
    },
    {
      "epoch": 0.5113333333333333,
      "grad_norm": 0.4231610894203186,
      "learning_rate": 4.680416666666667e-05,
      "loss": 0.0024,
      "step": 15340
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.061025287955999374,
      "learning_rate": 4.6802083333333334e-05,
      "loss": 0.0037,
      "step": 15350
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.39225929975509644,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0029,
      "step": 15360
    },
    {
      "epoch": 0.5123333333333333,
      "grad_norm": 0.48312848806381226,
      "learning_rate": 4.679791666666667e-05,
      "loss": 0.0017,
      "step": 15370
    },
    {
      "epoch": 0.5126666666666667,
      "grad_norm": 0.06606936454772949,
      "learning_rate": 4.679583333333333e-05,
      "loss": 0.0025,
      "step": 15380
    },
    {
      "epoch": 0.513,
      "grad_norm": 0.2723389267921448,
      "learning_rate": 4.679375e-05,
      "loss": 0.0031,
      "step": 15390
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.09122715145349503,
      "learning_rate": 4.679166666666667e-05,
      "loss": 0.0029,
      "step": 15400
    },
    {
      "epoch": 0.5136666666666667,
      "grad_norm": 0.3935908079147339,
      "learning_rate": 4.678958333333333e-05,
      "loss": 0.0023,
      "step": 15410
    },
    {
      "epoch": 0.514,
      "grad_norm": 0.19460158050060272,
      "learning_rate": 4.67875e-05,
      "loss": 0.003,
      "step": 15420
    },
    {
      "epoch": 0.5143333333333333,
      "grad_norm": 0.015209624543786049,
      "learning_rate": 4.678541666666667e-05,
      "loss": 0.003,
      "step": 15430
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.5653328895568848,
      "learning_rate": 4.6783333333333337e-05,
      "loss": 0.0023,
      "step": 15440
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.5737451910972595,
      "learning_rate": 4.678125e-05,
      "loss": 0.0034,
      "step": 15450
    },
    {
      "epoch": 0.5153333333333333,
      "grad_norm": 0.06197885051369667,
      "learning_rate": 4.677916666666667e-05,
      "loss": 0.0021,
      "step": 15460
    },
    {
      "epoch": 0.5156666666666667,
      "grad_norm": 0.061351630836725235,
      "learning_rate": 4.677708333333334e-05,
      "loss": 0.0032,
      "step": 15470
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.2412562370300293,
      "learning_rate": 4.6775000000000005e-05,
      "loss": 0.003,
      "step": 15480
    },
    {
      "epoch": 0.5163333333333333,
      "grad_norm": 0.21214266121387482,
      "learning_rate": 4.6772916666666664e-05,
      "loss": 0.0033,
      "step": 15490
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.30209267139434814,
      "learning_rate": 4.6770833333333336e-05,
      "loss": 0.0023,
      "step": 15500
    },
    {
      "epoch": 0.517,
      "grad_norm": 0.060667965561151505,
      "learning_rate": 4.676875e-05,
      "loss": 0.0029,
      "step": 15510
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.03096039593219757,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.0023,
      "step": 15520
    },
    {
      "epoch": 0.5176666666666667,
      "grad_norm": 0.48266392946243286,
      "learning_rate": 4.676458333333333e-05,
      "loss": 0.0024,
      "step": 15530
    },
    {
      "epoch": 0.518,
      "grad_norm": 0.2412761151790619,
      "learning_rate": 4.6762500000000005e-05,
      "loss": 0.003,
      "step": 15540
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.21193663775920868,
      "learning_rate": 4.676041666666667e-05,
      "loss": 0.003,
      "step": 15550
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 0.03194791078567505,
      "learning_rate": 4.6758333333333336e-05,
      "loss": 0.0024,
      "step": 15560
    },
    {
      "epoch": 0.519,
      "grad_norm": 0.2115924209356308,
      "learning_rate": 4.675625e-05,
      "loss": 0.003,
      "step": 15570
    },
    {
      "epoch": 0.5193333333333333,
      "grad_norm": 0.2036469578742981,
      "learning_rate": 4.6754166666666674e-05,
      "loss": 0.0035,
      "step": 15580
    },
    {
      "epoch": 0.5196666666666667,
      "grad_norm": 0.013219223357737064,
      "learning_rate": 4.675208333333333e-05,
      "loss": 0.0025,
      "step": 15590
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.03173960745334625,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.0033,
      "step": 15600
    },
    {
      "epoch": 0.5203333333333333,
      "grad_norm": 0.30341631174087524,
      "learning_rate": 4.674791666666667e-05,
      "loss": 0.0032,
      "step": 15610
    },
    {
      "epoch": 0.5206666666666667,
      "grad_norm": 0.24204087257385254,
      "learning_rate": 4.6745833333333336e-05,
      "loss": 0.003,
      "step": 15620
    },
    {
      "epoch": 0.521,
      "grad_norm": 0.5741227269172668,
      "learning_rate": 4.674375e-05,
      "loss": 0.0032,
      "step": 15630
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 0.24151495099067688,
      "learning_rate": 4.674166666666667e-05,
      "loss": 0.0032,
      "step": 15640
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.1817014515399933,
      "learning_rate": 4.673958333333334e-05,
      "loss": 0.0033,
      "step": 15650
    },
    {
      "epoch": 0.522,
      "grad_norm": 0.4232882857322693,
      "learning_rate": 4.67375e-05,
      "loss": 0.0028,
      "step": 15660
    },
    {
      "epoch": 0.5223333333333333,
      "grad_norm": 0.12199097871780396,
      "learning_rate": 4.673541666666667e-05,
      "loss": 0.0028,
      "step": 15670
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.2520766854286194,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0025,
      "step": 15680
    },
    {
      "epoch": 0.523,
      "grad_norm": 0.333513081073761,
      "learning_rate": 4.673125e-05,
      "loss": 0.003,
      "step": 15690
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.2714272141456604,
      "learning_rate": 4.6729166666666666e-05,
      "loss": 0.0029,
      "step": 15700
    },
    {
      "epoch": 0.5236666666666666,
      "grad_norm": 0.03830842301249504,
      "learning_rate": 4.672708333333334e-05,
      "loss": 0.0027,
      "step": 15710
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.18184775114059448,
      "learning_rate": 4.6725000000000004e-05,
      "loss": 0.0022,
      "step": 15720
    },
    {
      "epoch": 0.5243333333333333,
      "grad_norm": 0.4922037124633789,
      "learning_rate": 4.672291666666667e-05,
      "loss": 0.003,
      "step": 15730
    },
    {
      "epoch": 0.5246666666666666,
      "grad_norm": 0.09905453026294708,
      "learning_rate": 4.6720833333333335e-05,
      "loss": 0.0022,
      "step": 15740
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.09126771241426468,
      "learning_rate": 4.671875e-05,
      "loss": 0.0042,
      "step": 15750
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.27237626910209656,
      "learning_rate": 4.671666666666667e-05,
      "loss": 0.0021,
      "step": 15760
    },
    {
      "epoch": 0.5256666666666666,
      "grad_norm": 0.01174552645534277,
      "learning_rate": 4.671458333333333e-05,
      "loss": 0.0036,
      "step": 15770
    },
    {
      "epoch": 0.526,
      "grad_norm": 0.06247716769576073,
      "learning_rate": 4.6712500000000004e-05,
      "loss": 0.003,
      "step": 15780
    },
    {
      "epoch": 0.5263333333333333,
      "grad_norm": 0.48296067118644714,
      "learning_rate": 4.671041666666667e-05,
      "loss": 0.0026,
      "step": 15790
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.42286956310272217,
      "learning_rate": 4.6708333333333335e-05,
      "loss": 0.003,
      "step": 15800
    },
    {
      "epoch": 0.527,
      "grad_norm": 0.15619443356990814,
      "learning_rate": 4.670625e-05,
      "loss": 0.0028,
      "step": 15810
    },
    {
      "epoch": 0.5273333333333333,
      "grad_norm": 0.30240383744239807,
      "learning_rate": 4.670416666666667e-05,
      "loss": 0.0024,
      "step": 15820
    },
    {
      "epoch": 0.5276666666666666,
      "grad_norm": 0.24225904047489166,
      "learning_rate": 4.670208333333334e-05,
      "loss": 0.0029,
      "step": 15830
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.2711019515991211,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.0022,
      "step": 15840
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.573172390460968,
      "learning_rate": 4.669791666666667e-05,
      "loss": 0.0024,
      "step": 15850
    },
    {
      "epoch": 0.5286666666666666,
      "grad_norm": 0.4476037621498108,
      "learning_rate": 4.6695833333333334e-05,
      "loss": 0.0031,
      "step": 15860
    },
    {
      "epoch": 0.529,
      "grad_norm": 0.022365933284163475,
      "learning_rate": 4.669375e-05,
      "loss": 0.0034,
      "step": 15870
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 0.06150994822382927,
      "learning_rate": 4.6691666666666665e-05,
      "loss": 0.0025,
      "step": 15880
    },
    {
      "epoch": 0.5296666666666666,
      "grad_norm": 0.18078337609767914,
      "learning_rate": 4.668958333333334e-05,
      "loss": 0.0018,
      "step": 15890
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.5133910179138184,
      "learning_rate": 4.66875e-05,
      "loss": 0.0026,
      "step": 15900
    },
    {
      "epoch": 0.5303333333333333,
      "grad_norm": 0.39340001344680786,
      "learning_rate": 4.668541666666667e-05,
      "loss": 0.0041,
      "step": 15910
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.2728356719017029,
      "learning_rate": 4.6683333333333334e-05,
      "loss": 0.0044,
      "step": 15920
    },
    {
      "epoch": 0.531,
      "grad_norm": 0.15122784674167633,
      "learning_rate": 4.6681250000000006e-05,
      "loss": 0.0039,
      "step": 15930
    },
    {
      "epoch": 0.5313333333333333,
      "grad_norm": 0.717560350894928,
      "learning_rate": 4.6679166666666665e-05,
      "loss": 0.0031,
      "step": 15940
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.4246736168861389,
      "learning_rate": 4.667708333333334e-05,
      "loss": 0.003,
      "step": 15950
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.15302257239818573,
      "learning_rate": 4.6675e-05,
      "loss": 0.0035,
      "step": 15960
    },
    {
      "epoch": 0.5323333333333333,
      "grad_norm": 0.010672085918486118,
      "learning_rate": 4.667291666666667e-05,
      "loss": 0.0026,
      "step": 15970
    },
    {
      "epoch": 0.5326666666666666,
      "grad_norm": 0.36845430731773376,
      "learning_rate": 4.6670833333333334e-05,
      "loss": 0.0033,
      "step": 15980
    },
    {
      "epoch": 0.533,
      "grad_norm": 0.2722596228122711,
      "learning_rate": 4.666875e-05,
      "loss": 0.0033,
      "step": 15990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.06241852045059204,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0027,
      "step": 16000
    },
    {
      "epoch": 0.5336666666666666,
      "grad_norm": 0.1811474859714508,
      "learning_rate": 4.666458333333334e-05,
      "loss": 0.0024,
      "step": 16010
    },
    {
      "epoch": 0.534,
      "grad_norm": 0.4221706986427307,
      "learning_rate": 4.66625e-05,
      "loss": 0.004,
      "step": 16020
    },
    {
      "epoch": 0.5343333333333333,
      "grad_norm": 0.22715163230895996,
      "learning_rate": 4.666041666666667e-05,
      "loss": 0.003,
      "step": 16030
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 0.15085667371749878,
      "learning_rate": 4.665833333333334e-05,
      "loss": 0.0036,
      "step": 16040
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.005584847182035446,
      "learning_rate": 4.665625e-05,
      "loss": 0.0025,
      "step": 16050
    },
    {
      "epoch": 0.5353333333333333,
      "grad_norm": 0.06043124571442604,
      "learning_rate": 4.665416666666667e-05,
      "loss": 0.0041,
      "step": 16060
    },
    {
      "epoch": 0.5356666666666666,
      "grad_norm": 0.3629898428916931,
      "learning_rate": 4.665208333333334e-05,
      "loss": 0.0031,
      "step": 16070
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.09383441507816315,
      "learning_rate": 4.665e-05,
      "loss": 0.0032,
      "step": 16080
    },
    {
      "epoch": 0.5363333333333333,
      "grad_norm": 0.1813407987356186,
      "learning_rate": 4.664791666666667e-05,
      "loss": 0.0029,
      "step": 16090
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.16523928940296173,
      "learning_rate": 4.664583333333333e-05,
      "loss": 0.0037,
      "step": 16100
    },
    {
      "epoch": 0.537,
      "grad_norm": 0.4267418086528778,
      "learning_rate": 4.6643750000000006e-05,
      "loss": 0.0026,
      "step": 16110
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 0.12176114320755005,
      "learning_rate": 4.6641666666666664e-05,
      "loss": 0.0031,
      "step": 16120
    },
    {
      "epoch": 0.5376666666666666,
      "grad_norm": 0.1213475838303566,
      "learning_rate": 4.6639583333333336e-05,
      "loss": 0.0031,
      "step": 16130
    },
    {
      "epoch": 0.538,
      "grad_norm": 0.24103891849517822,
      "learning_rate": 4.66375e-05,
      "loss": 0.0032,
      "step": 16140
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.33209696412086487,
      "learning_rate": 4.663541666666667e-05,
      "loss": 0.0043,
      "step": 16150
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.2717210352420807,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.0024,
      "step": 16160
    },
    {
      "epoch": 0.539,
      "grad_norm": 0.09389030188322067,
      "learning_rate": 4.6631250000000005e-05,
      "loss": 0.0028,
      "step": 16170
    },
    {
      "epoch": 0.5393333333333333,
      "grad_norm": 0.42367956042289734,
      "learning_rate": 4.662916666666667e-05,
      "loss": 0.0038,
      "step": 16180
    },
    {
      "epoch": 0.5396666666666666,
      "grad_norm": 0.338946133852005,
      "learning_rate": 4.6627083333333336e-05,
      "loss": 0.0039,
      "step": 16190
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.06150169298052788,
      "learning_rate": 4.6625e-05,
      "loss": 0.0036,
      "step": 16200
    },
    {
      "epoch": 0.5403333333333333,
      "grad_norm": 0.3229486644268036,
      "learning_rate": 4.662291666666667e-05,
      "loss": 0.0025,
      "step": 16210
    },
    {
      "epoch": 0.5406666666666666,
      "grad_norm": 0.30294686555862427,
      "learning_rate": 4.662083333333333e-05,
      "loss": 0.0026,
      "step": 16220
    },
    {
      "epoch": 0.541,
      "grad_norm": 0.2803390324115753,
      "learning_rate": 4.661875e-05,
      "loss": 0.0033,
      "step": 16230
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.09117591381072998,
      "learning_rate": 4.661666666666667e-05,
      "loss": 0.0027,
      "step": 16240
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.09081214666366577,
      "learning_rate": 4.6614583333333336e-05,
      "loss": 0.0032,
      "step": 16250
    },
    {
      "epoch": 0.542,
      "grad_norm": 0.15116776525974274,
      "learning_rate": 4.66125e-05,
      "loss": 0.0035,
      "step": 16260
    },
    {
      "epoch": 0.5423333333333333,
      "grad_norm": 0.06096509099006653,
      "learning_rate": 4.661041666666667e-05,
      "loss": 0.0044,
      "step": 16270
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 0.15111377835273743,
      "learning_rate": 4.660833333333334e-05,
      "loss": 0.0028,
      "step": 16280
    },
    {
      "epoch": 0.543,
      "grad_norm": 0.18098948895931244,
      "learning_rate": 4.6606250000000005e-05,
      "loss": 0.0028,
      "step": 16290
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.19416822493076324,
      "learning_rate": 4.660416666666667e-05,
      "loss": 0.003,
      "step": 16300
    },
    {
      "epoch": 0.5436666666666666,
      "grad_norm": 0.24109870195388794,
      "learning_rate": 4.6602083333333336e-05,
      "loss": 0.0033,
      "step": 16310
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.030775543302297592,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0022,
      "step": 16320
    },
    {
      "epoch": 0.5443333333333333,
      "grad_norm": 0.06047419086098671,
      "learning_rate": 4.6597916666666667e-05,
      "loss": 0.0026,
      "step": 16330
    },
    {
      "epoch": 0.5446666666666666,
      "grad_norm": 0.06080928072333336,
      "learning_rate": 4.659583333333333e-05,
      "loss": 0.003,
      "step": 16340
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.2729508876800537,
      "learning_rate": 4.6593750000000004e-05,
      "loss": 0.0033,
      "step": 16350
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 0.3316042423248291,
      "learning_rate": 4.659166666666667e-05,
      "loss": 0.0033,
      "step": 16360
    },
    {
      "epoch": 0.5456666666666666,
      "grad_norm": 0.39268481731414795,
      "learning_rate": 4.6589583333333335e-05,
      "loss": 0.0033,
      "step": 16370
    },
    {
      "epoch": 0.546,
      "grad_norm": 0.846653938293457,
      "learning_rate": 4.65875e-05,
      "loss": 0.0031,
      "step": 16380
    },
    {
      "epoch": 0.5463333333333333,
      "grad_norm": 0.5441489219665527,
      "learning_rate": 4.658541666666667e-05,
      "loss": 0.0032,
      "step": 16390
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.30189377069473267,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.0034,
      "step": 16400
    },
    {
      "epoch": 0.547,
      "grad_norm": 0.06140328198671341,
      "learning_rate": 4.6581250000000004e-05,
      "loss": 0.004,
      "step": 16410
    },
    {
      "epoch": 0.5473333333333333,
      "grad_norm": 0.6631414890289307,
      "learning_rate": 4.657916666666667e-05,
      "loss": 0.0021,
      "step": 16420
    },
    {
      "epoch": 0.5476666666666666,
      "grad_norm": 0.4523088335990906,
      "learning_rate": 4.6577083333333335e-05,
      "loss": 0.0023,
      "step": 16430
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.3927709758281708,
      "learning_rate": 4.6575e-05,
      "loss": 0.0031,
      "step": 16440
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.03149747475981712,
      "learning_rate": 4.657291666666667e-05,
      "loss": 0.0027,
      "step": 16450
    },
    {
      "epoch": 0.5486666666666666,
      "grad_norm": 0.9348229169845581,
      "learning_rate": 4.657083333333334e-05,
      "loss": 0.0032,
      "step": 16460
    },
    {
      "epoch": 0.549,
      "grad_norm": 0.1812802255153656,
      "learning_rate": 4.656875e-05,
      "loss": 0.0032,
      "step": 16470
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.0609130784869194,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0037,
      "step": 16480
    },
    {
      "epoch": 0.5496666666666666,
      "grad_norm": 0.3533584773540497,
      "learning_rate": 4.6564583333333335e-05,
      "loss": 0.0025,
      "step": 16490
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.29297369718551636,
      "learning_rate": 4.65625e-05,
      "loss": 0.004,
      "step": 16500
    },
    {
      "epoch": 0.5503333333333333,
      "grad_norm": 0.06080881506204605,
      "learning_rate": 4.6560416666666666e-05,
      "loss": 0.0031,
      "step": 16510
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 0.2409459799528122,
      "learning_rate": 4.655833333333334e-05,
      "loss": 0.003,
      "step": 16520
    },
    {
      "epoch": 0.551,
      "grad_norm": 0.27109187841415405,
      "learning_rate": 4.6556250000000003e-05,
      "loss": 0.0048,
      "step": 16530
    },
    {
      "epoch": 0.5513333333333333,
      "grad_norm": 0.03337344527244568,
      "learning_rate": 4.655416666666667e-05,
      "loss": 0.0019,
      "step": 16540
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.21103337407112122,
      "learning_rate": 4.6552083333333334e-05,
      "loss": 0.0029,
      "step": 16550
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.21097321808338165,
      "learning_rate": 4.655000000000001e-05,
      "loss": 0.0028,
      "step": 16560
    },
    {
      "epoch": 0.5523333333333333,
      "grad_norm": 0.18125863373279572,
      "learning_rate": 4.654791666666667e-05,
      "loss": 0.0033,
      "step": 16570
    },
    {
      "epoch": 0.5526666666666666,
      "grad_norm": 0.33075082302093506,
      "learning_rate": 4.654583333333333e-05,
      "loss": 0.0031,
      "step": 16580
    },
    {
      "epoch": 0.553,
      "grad_norm": 0.3920552134513855,
      "learning_rate": 4.654375e-05,
      "loss": 0.0025,
      "step": 16590
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.12148207426071167,
      "learning_rate": 4.654166666666667e-05,
      "loss": 0.0026,
      "step": 16600
    },
    {
      "epoch": 0.5536666666666666,
      "grad_norm": 0.1525501161813736,
      "learning_rate": 4.6539583333333334e-05,
      "loss": 0.0023,
      "step": 16610
    },
    {
      "epoch": 0.554,
      "grad_norm": 0.48232269287109375,
      "learning_rate": 4.65375e-05,
      "loss": 0.0035,
      "step": 16620
    },
    {
      "epoch": 0.5543333333333333,
      "grad_norm": 0.004155856091529131,
      "learning_rate": 4.653541666666667e-05,
      "loss": 0.0027,
      "step": 16630
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.06197177991271019,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0026,
      "step": 16640
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.2717434763908386,
      "learning_rate": 4.653125e-05,
      "loss": 0.0019,
      "step": 16650
    },
    {
      "epoch": 0.5553333333333333,
      "grad_norm": 0.24264219403266907,
      "learning_rate": 4.652916666666667e-05,
      "loss": 0.0019,
      "step": 16660
    },
    {
      "epoch": 0.5556666666666666,
      "grad_norm": 0.30206286907196045,
      "learning_rate": 4.652708333333334e-05,
      "loss": 0.0023,
      "step": 16670
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.15021103620529175,
      "learning_rate": 4.6525e-05,
      "loss": 0.0032,
      "step": 16680
    },
    {
      "epoch": 0.5563333333333333,
      "grad_norm": 0.11003538966178894,
      "learning_rate": 4.652291666666667e-05,
      "loss": 0.0032,
      "step": 16690
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.08765304088592529,
      "learning_rate": 4.652083333333334e-05,
      "loss": 0.0027,
      "step": 16700
    },
    {
      "epoch": 0.557,
      "grad_norm": 0.25703665614128113,
      "learning_rate": 4.651875e-05,
      "loss": 0.0025,
      "step": 16710
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.39543819427490234,
      "learning_rate": 4.651666666666667e-05,
      "loss": 0.0042,
      "step": 16720
    },
    {
      "epoch": 0.5576666666666666,
      "grad_norm": 0.813136637210846,
      "learning_rate": 4.6514583333333333e-05,
      "loss": 0.0025,
      "step": 16730
    },
    {
      "epoch": 0.558,
      "grad_norm": 0.5142640471458435,
      "learning_rate": 4.6512500000000006e-05,
      "loss": 0.0024,
      "step": 16740
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.3913939595222473,
      "learning_rate": 4.6510416666666664e-05,
      "loss": 0.0036,
      "step": 16750
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 0.21444271504878998,
      "learning_rate": 4.650833333333334e-05,
      "loss": 0.0034,
      "step": 16760
    },
    {
      "epoch": 0.559,
      "grad_norm": 0.09114590287208557,
      "learning_rate": 4.650625e-05,
      "loss": 0.0027,
      "step": 16770
    },
    {
      "epoch": 0.5593333333333333,
      "grad_norm": 0.24615316092967987,
      "learning_rate": 4.650416666666667e-05,
      "loss": 0.0035,
      "step": 16780
    },
    {
      "epoch": 0.5596666666666666,
      "grad_norm": 0.4231322705745697,
      "learning_rate": 4.650208333333333e-05,
      "loss": 0.0023,
      "step": 16790
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.12178955227136612,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0031,
      "step": 16800
    },
    {
      "epoch": 0.5603333333333333,
      "grad_norm": 0.6886524558067322,
      "learning_rate": 4.649791666666667e-05,
      "loss": 0.0033,
      "step": 16810
    },
    {
      "epoch": 0.5606666666666666,
      "grad_norm": 0.5124388933181763,
      "learning_rate": 4.649583333333333e-05,
      "loss": 0.0027,
      "step": 16820
    },
    {
      "epoch": 0.561,
      "grad_norm": 0.21040648221969604,
      "learning_rate": 4.649375e-05,
      "loss": 0.0028,
      "step": 16830
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 0.24082030355930328,
      "learning_rate": 4.649166666666667e-05,
      "loss": 0.0033,
      "step": 16840
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.33124610781669617,
      "learning_rate": 4.648958333333334e-05,
      "loss": 0.0033,
      "step": 16850
    },
    {
      "epoch": 0.562,
      "grad_norm": 0.3014887869358063,
      "learning_rate": 4.64875e-05,
      "loss": 0.0029,
      "step": 16860
    },
    {
      "epoch": 0.5623333333333334,
      "grad_norm": 0.18051712214946747,
      "learning_rate": 4.648541666666667e-05,
      "loss": 0.002,
      "step": 16870
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.12114957720041275,
      "learning_rate": 4.6483333333333336e-05,
      "loss": 0.0028,
      "step": 16880
    },
    {
      "epoch": 0.563,
      "grad_norm": 0.4867342710494995,
      "learning_rate": 4.648125e-05,
      "loss": 0.0049,
      "step": 16890
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.21099314093589783,
      "learning_rate": 4.647916666666667e-05,
      "loss": 0.0023,
      "step": 16900
    },
    {
      "epoch": 0.5636666666666666,
      "grad_norm": 0.27092671394348145,
      "learning_rate": 4.647708333333334e-05,
      "loss": 0.0024,
      "step": 16910
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.21150760352611542,
      "learning_rate": 4.6475000000000005e-05,
      "loss": 0.0042,
      "step": 16920
    },
    {
      "epoch": 0.5643333333333334,
      "grad_norm": 0.18092183768749237,
      "learning_rate": 4.647291666666667e-05,
      "loss": 0.0025,
      "step": 16930
    },
    {
      "epoch": 0.5646666666666667,
      "grad_norm": 0.42206594347953796,
      "learning_rate": 4.6470833333333336e-05,
      "loss": 0.004,
      "step": 16940
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.4507905840873718,
      "learning_rate": 4.646875e-05,
      "loss": 0.0027,
      "step": 16950
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.1806173324584961,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0029,
      "step": 16960
    },
    {
      "epoch": 0.5656666666666667,
      "grad_norm": 0.21227140724658966,
      "learning_rate": 4.646458333333333e-05,
      "loss": 0.0028,
      "step": 16970
    },
    {
      "epoch": 0.566,
      "grad_norm": 0.06050672382116318,
      "learning_rate": 4.6462500000000005e-05,
      "loss": 0.003,
      "step": 16980
    },
    {
      "epoch": 0.5663333333333334,
      "grad_norm": 0.27066853642463684,
      "learning_rate": 4.646041666666667e-05,
      "loss": 0.0019,
      "step": 16990
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.03272264823317528,
      "learning_rate": 4.6458333333333335e-05,
      "loss": 0.0032,
      "step": 17000
    },
    {
      "epoch": 0.567,
      "grad_norm": 0.008587861433625221,
      "learning_rate": 4.645625e-05,
      "loss": 0.0032,
      "step": 17010
    },
    {
      "epoch": 0.5673333333333334,
      "grad_norm": 0.2708107531070709,
      "learning_rate": 4.645416666666667e-05,
      "loss": 0.0031,
      "step": 17020
    },
    {
      "epoch": 0.5676666666666667,
      "grad_norm": 0.09013433754444122,
      "learning_rate": 4.645208333333333e-05,
      "loss": 0.0036,
      "step": 17030
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.12127289921045303,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.0027,
      "step": 17040
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.40493956208229065,
      "learning_rate": 4.644791666666667e-05,
      "loss": 0.0034,
      "step": 17050
    },
    {
      "epoch": 0.5686666666666667,
      "grad_norm": 0.6027978658676147,
      "learning_rate": 4.6445833333333335e-05,
      "loss": 0.0021,
      "step": 17060
    },
    {
      "epoch": 0.569,
      "grad_norm": 0.3906867206096649,
      "learning_rate": 4.644375e-05,
      "loss": 0.0028,
      "step": 17070
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 0.061416707932949066,
      "learning_rate": 4.6441666666666666e-05,
      "loss": 0.0033,
      "step": 17080
    },
    {
      "epoch": 0.5696666666666667,
      "grad_norm": 0.030928155407309532,
      "learning_rate": 4.643958333333334e-05,
      "loss": 0.0029,
      "step": 17090
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.0913313701748848,
      "learning_rate": 4.64375e-05,
      "loss": 0.0034,
      "step": 17100
    },
    {
      "epoch": 0.5703333333333334,
      "grad_norm": 0.009198920801281929,
      "learning_rate": 4.643541666666667e-05,
      "loss": 0.0028,
      "step": 17110
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.3272653818130493,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.0018,
      "step": 17120
    },
    {
      "epoch": 0.571,
      "grad_norm": 0.26548513770103455,
      "learning_rate": 4.643125000000001e-05,
      "loss": 0.003,
      "step": 17130
    },
    {
      "epoch": 0.5713333333333334,
      "grad_norm": 0.150452122092247,
      "learning_rate": 4.6429166666666666e-05,
      "loss": 0.0023,
      "step": 17140
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.24098548293113708,
      "learning_rate": 4.642708333333334e-05,
      "loss": 0.0019,
      "step": 17150
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.21942898631095886,
      "learning_rate": 4.6425000000000004e-05,
      "loss": 0.0028,
      "step": 17160
    },
    {
      "epoch": 0.5723333333333334,
      "grad_norm": 0.2710324227809906,
      "learning_rate": 4.642291666666667e-05,
      "loss": 0.0021,
      "step": 17170
    },
    {
      "epoch": 0.5726666666666667,
      "grad_norm": 0.18133194744586945,
      "learning_rate": 4.6420833333333335e-05,
      "loss": 0.0032,
      "step": 17180
    },
    {
      "epoch": 0.573,
      "grad_norm": 0.30045008659362793,
      "learning_rate": 4.641875e-05,
      "loss": 0.0029,
      "step": 17190
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.10668300092220306,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.0036,
      "step": 17200
    },
    {
      "epoch": 0.5736666666666667,
      "grad_norm": 0.12048350274562836,
      "learning_rate": 4.641458333333333e-05,
      "loss": 0.0026,
      "step": 17210
    },
    {
      "epoch": 0.574,
      "grad_norm": 0.18297302722930908,
      "learning_rate": 4.64125e-05,
      "loss": 0.0025,
      "step": 17220
    },
    {
      "epoch": 0.5743333333333334,
      "grad_norm": 0.1854684203863144,
      "learning_rate": 4.641041666666667e-05,
      "loss": 0.0023,
      "step": 17230
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 0.49812862277030945,
      "learning_rate": 4.6408333333333334e-05,
      "loss": 0.0035,
      "step": 17240
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.09060841053724289,
      "learning_rate": 4.640625e-05,
      "loss": 0.0028,
      "step": 17250
    },
    {
      "epoch": 0.5753333333333334,
      "grad_norm": 0.15076620876789093,
      "learning_rate": 4.640416666666667e-05,
      "loss": 0.0035,
      "step": 17260
    },
    {
      "epoch": 0.5756666666666667,
      "grad_norm": 0.23997114598751068,
      "learning_rate": 4.640208333333334e-05,
      "loss": 0.0035,
      "step": 17270
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.3922871947288513,
      "learning_rate": 4.64e-05,
      "loss": 0.0031,
      "step": 17280
    },
    {
      "epoch": 0.5763333333333334,
      "grad_norm": 0.2104259878396988,
      "learning_rate": 4.639791666666667e-05,
      "loss": 0.0026,
      "step": 17290
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.25515133142471313,
      "learning_rate": 4.6395833333333334e-05,
      "loss": 0.0026,
      "step": 17300
    },
    {
      "epoch": 0.577,
      "grad_norm": 0.5654280781745911,
      "learning_rate": 4.639375e-05,
      "loss": 0.0033,
      "step": 17310
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 0.09094803035259247,
      "learning_rate": 4.6391666666666665e-05,
      "loss": 0.0027,
      "step": 17320
    },
    {
      "epoch": 0.5776666666666667,
      "grad_norm": 0.12122270464897156,
      "learning_rate": 4.638958333333334e-05,
      "loss": 0.0028,
      "step": 17330
    },
    {
      "epoch": 0.578,
      "grad_norm": 0.09176818281412125,
      "learning_rate": 4.63875e-05,
      "loss": 0.0031,
      "step": 17340
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.543224573135376,
      "learning_rate": 4.638541666666667e-05,
      "loss": 0.0022,
      "step": 17350
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.15050280094146729,
      "learning_rate": 4.6383333333333334e-05,
      "loss": 0.0025,
      "step": 17360
    },
    {
      "epoch": 0.579,
      "grad_norm": 0.33118578791618347,
      "learning_rate": 4.6381250000000006e-05,
      "loss": 0.0035,
      "step": 17370
    },
    {
      "epoch": 0.5793333333333334,
      "grad_norm": 0.27048686146736145,
      "learning_rate": 4.6379166666666665e-05,
      "loss": 0.0021,
      "step": 17380
    },
    {
      "epoch": 0.5796666666666667,
      "grad_norm": 0.12027855962514877,
      "learning_rate": 4.637708333333334e-05,
      "loss": 0.0024,
      "step": 17390
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.5104601383209229,
      "learning_rate": 4.6375e-05,
      "loss": 0.0027,
      "step": 17400
    },
    {
      "epoch": 0.5803333333333334,
      "grad_norm": 0.031899344176054,
      "learning_rate": 4.6372916666666675e-05,
      "loss": 0.0023,
      "step": 17410
    },
    {
      "epoch": 0.5806666666666667,
      "grad_norm": 0.24137020111083984,
      "learning_rate": 4.637083333333333e-05,
      "loss": 0.003,
      "step": 17420
    },
    {
      "epoch": 0.581,
      "grad_norm": 0.9323718547821045,
      "learning_rate": 4.636875e-05,
      "loss": 0.0031,
      "step": 17430
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.24203166365623474,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0022,
      "step": 17440
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.6610751748085022,
      "learning_rate": 4.6364583333333337e-05,
      "loss": 0.0038,
      "step": 17450
    },
    {
      "epoch": 0.582,
      "grad_norm": 0.4204968810081482,
      "learning_rate": 4.63625e-05,
      "loss": 0.0025,
      "step": 17460
    },
    {
      "epoch": 0.5823333333333334,
      "grad_norm": 0.031879082322120667,
      "learning_rate": 4.636041666666667e-05,
      "loss": 0.0025,
      "step": 17470
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 0.11997485905885696,
      "learning_rate": 4.635833333333334e-05,
      "loss": 0.0019,
      "step": 17480
    },
    {
      "epoch": 0.583,
      "grad_norm": 0.3607180714607239,
      "learning_rate": 4.635625e-05,
      "loss": 0.0026,
      "step": 17490
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.4500384032726288,
      "learning_rate": 4.635416666666667e-05,
      "loss": 0.0023,
      "step": 17500
    },
    {
      "epoch": 0.5836666666666667,
      "grad_norm": 0.23456153273582458,
      "learning_rate": 4.6352083333333336e-05,
      "loss": 0.0035,
      "step": 17510
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.030680079013109207,
      "learning_rate": 4.635e-05,
      "loss": 0.0025,
      "step": 17520
    },
    {
      "epoch": 0.5843333333333334,
      "grad_norm": 0.15057522058486938,
      "learning_rate": 4.634791666666667e-05,
      "loss": 0.0033,
      "step": 17530
    },
    {
      "epoch": 0.5846666666666667,
      "grad_norm": 0.6383459568023682,
      "learning_rate": 4.634583333333334e-05,
      "loss": 0.0034,
      "step": 17540
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.03387843817472458,
      "learning_rate": 4.6343750000000005e-05,
      "loss": 0.0032,
      "step": 17550
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 0.09090878814458847,
      "learning_rate": 4.6341666666666664e-05,
      "loss": 0.0034,
      "step": 17560
    },
    {
      "epoch": 0.5856666666666667,
      "grad_norm": 0.631149172782898,
      "learning_rate": 4.6339583333333336e-05,
      "loss": 0.0017,
      "step": 17570
    },
    {
      "epoch": 0.586,
      "grad_norm": 0.2788989245891571,
      "learning_rate": 4.63375e-05,
      "loss": 0.0032,
      "step": 17580
    },
    {
      "epoch": 0.5863333333333334,
      "grad_norm": 0.3015851378440857,
      "learning_rate": 4.633541666666667e-05,
      "loss": 0.0035,
      "step": 17590
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.2412828505039215,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0032,
      "step": 17600
    },
    {
      "epoch": 0.587,
      "grad_norm": 0.2701893746852875,
      "learning_rate": 4.6331250000000005e-05,
      "loss": 0.002,
      "step": 17610
    },
    {
      "epoch": 0.5873333333333334,
      "grad_norm": 0.03103017807006836,
      "learning_rate": 4.632916666666667e-05,
      "loss": 0.0019,
      "step": 17620
    },
    {
      "epoch": 0.5876666666666667,
      "grad_norm": 0.0608258992433548,
      "learning_rate": 4.6327083333333336e-05,
      "loss": 0.0037,
      "step": 17630
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.1201569065451622,
      "learning_rate": 4.6325e-05,
      "loss": 0.0039,
      "step": 17640
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.06119818240404129,
      "learning_rate": 4.6322916666666673e-05,
      "loss": 0.0025,
      "step": 17650
    },
    {
      "epoch": 0.5886666666666667,
      "grad_norm": 0.18043193221092224,
      "learning_rate": 4.632083333333334e-05,
      "loss": 0.0035,
      "step": 17660
    },
    {
      "epoch": 0.589,
      "grad_norm": 0.4818270802497864,
      "learning_rate": 4.631875e-05,
      "loss": 0.0031,
      "step": 17670
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.24126338958740234,
      "learning_rate": 4.631666666666667e-05,
      "loss": 0.0028,
      "step": 17680
    },
    {
      "epoch": 0.5896666666666667,
      "grad_norm": 0.07626696676015854,
      "learning_rate": 4.6314583333333335e-05,
      "loss": 0.003,
      "step": 17690
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.8099319338798523,
      "learning_rate": 4.63125e-05,
      "loss": 0.0045,
      "step": 17700
    },
    {
      "epoch": 0.5903333333333334,
      "grad_norm": 0.27132827043533325,
      "learning_rate": 4.6310416666666666e-05,
      "loss": 0.0027,
      "step": 17710
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 0.2111102044582367,
      "learning_rate": 4.630833333333334e-05,
      "loss": 0.003,
      "step": 17720
    },
    {
      "epoch": 0.591,
      "grad_norm": 0.4224292039871216,
      "learning_rate": 4.6306250000000004e-05,
      "loss": 0.0036,
      "step": 17730
    },
    {
      "epoch": 0.5913333333333334,
      "grad_norm": 0.48164039850234985,
      "learning_rate": 4.630416666666667e-05,
      "loss": 0.0043,
      "step": 17740
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.21080201864242554,
      "learning_rate": 4.6302083333333335e-05,
      "loss": 0.0043,
      "step": 17750
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.2359941452741623,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0033,
      "step": 17760
    },
    {
      "epoch": 0.5923333333333334,
      "grad_norm": 0.12081846594810486,
      "learning_rate": 4.6297916666666666e-05,
      "loss": 0.004,
      "step": 17770
    },
    {
      "epoch": 0.5926666666666667,
      "grad_norm": 0.33111169934272766,
      "learning_rate": 4.629583333333334e-05,
      "loss": 0.0036,
      "step": 17780
    },
    {
      "epoch": 0.593,
      "grad_norm": 0.18064819276332855,
      "learning_rate": 4.6293750000000004e-05,
      "loss": 0.0033,
      "step": 17790
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.06161230802536011,
      "learning_rate": 4.629166666666667e-05,
      "loss": 0.0029,
      "step": 17800
    },
    {
      "epoch": 0.5936666666666667,
      "grad_norm": 0.1824912130832672,
      "learning_rate": 4.6289583333333335e-05,
      "loss": 0.0035,
      "step": 17810
    },
    {
      "epoch": 0.594,
      "grad_norm": 0.4818892478942871,
      "learning_rate": 4.62875e-05,
      "loss": 0.0022,
      "step": 17820
    },
    {
      "epoch": 0.5943333333333334,
      "grad_norm": 0.12033197283744812,
      "learning_rate": 4.628541666666667e-05,
      "loss": 0.0021,
      "step": 17830
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.21276652812957764,
      "learning_rate": 4.628333333333333e-05,
      "loss": 0.0027,
      "step": 17840
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.5319036245346069,
      "learning_rate": 4.6281250000000003e-05,
      "loss": 0.0031,
      "step": 17850
    },
    {
      "epoch": 0.5953333333333334,
      "grad_norm": 0.06009651720523834,
      "learning_rate": 4.627916666666667e-05,
      "loss": 0.0033,
      "step": 17860
    },
    {
      "epoch": 0.5956666666666667,
      "grad_norm": 0.527064859867096,
      "learning_rate": 4.6277083333333334e-05,
      "loss": 0.0032,
      "step": 17870
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.060937460511922836,
      "learning_rate": 4.6275e-05,
      "loss": 0.0025,
      "step": 17880
    },
    {
      "epoch": 0.5963333333333334,
      "grad_norm": 0.39126595854759216,
      "learning_rate": 4.627291666666667e-05,
      "loss": 0.002,
      "step": 17890
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.27121707797050476,
      "learning_rate": 4.627083333333334e-05,
      "loss": 0.0024,
      "step": 17900
    },
    {
      "epoch": 0.597,
      "grad_norm": 0.12176655232906342,
      "learning_rate": 4.6268749999999996e-05,
      "loss": 0.002,
      "step": 17910
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.15124720335006714,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0035,
      "step": 17920
    },
    {
      "epoch": 0.5976666666666667,
      "grad_norm": 0.5416873097419739,
      "learning_rate": 4.6264583333333334e-05,
      "loss": 0.0024,
      "step": 17930
    },
    {
      "epoch": 0.598,
      "grad_norm": 0.12100709974765778,
      "learning_rate": 4.6262500000000006e-05,
      "loss": 0.0028,
      "step": 17940
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.5263444781303406,
      "learning_rate": 4.6260416666666665e-05,
      "loss": 0.0023,
      "step": 17950
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 0.8787066340446472,
      "learning_rate": 4.625833333333334e-05,
      "loss": 0.0037,
      "step": 17960
    },
    {
      "epoch": 0.599,
      "grad_norm": 0.06087980791926384,
      "learning_rate": 4.625625e-05,
      "loss": 0.0028,
      "step": 17970
    },
    {
      "epoch": 0.5993333333333334,
      "grad_norm": 0.3610551357269287,
      "learning_rate": 4.625416666666667e-05,
      "loss": 0.0029,
      "step": 17980
    },
    {
      "epoch": 0.5996666666666667,
      "grad_norm": 0.3604026436805725,
      "learning_rate": 4.6252083333333334e-05,
      "loss": 0.0038,
      "step": 17990
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.3007209300994873,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.0031,
      "step": 18000
    },
    {
      "epoch": 0.6003333333333334,
      "grad_norm": 0.12075456231832504,
      "learning_rate": 4.624791666666667e-05,
      "loss": 0.0025,
      "step": 18010
    },
    {
      "epoch": 0.6006666666666667,
      "grad_norm": 0.06278953701257706,
      "learning_rate": 4.624583333333334e-05,
      "loss": 0.0047,
      "step": 18020
    },
    {
      "epoch": 0.601,
      "grad_norm": 0.09087370336055756,
      "learning_rate": 4.624375e-05,
      "loss": 0.0034,
      "step": 18030
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 0.7480658292770386,
      "learning_rate": 4.624166666666667e-05,
      "loss": 0.0042,
      "step": 18040
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.06064153462648392,
      "learning_rate": 4.6239583333333334e-05,
      "loss": 0.0021,
      "step": 18050
    },
    {
      "epoch": 0.602,
      "grad_norm": 0.09075216948986053,
      "learning_rate": 4.62375e-05,
      "loss": 0.0026,
      "step": 18060
    },
    {
      "epoch": 0.6023333333333334,
      "grad_norm": 0.23583538830280304,
      "learning_rate": 4.623541666666667e-05,
      "loss": 0.0027,
      "step": 18070
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.27079135179519653,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.0031,
      "step": 18080
    },
    {
      "epoch": 0.603,
      "grad_norm": 0.4808078706264496,
      "learning_rate": 4.623125e-05,
      "loss": 0.0032,
      "step": 18090
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.2704150080680847,
      "learning_rate": 4.622916666666667e-05,
      "loss": 0.0029,
      "step": 18100
    },
    {
      "epoch": 0.6036666666666667,
      "grad_norm": 0.08984703570604324,
      "learning_rate": 4.622708333333334e-05,
      "loss": 0.0025,
      "step": 18110
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.301596999168396,
      "learning_rate": 4.6225e-05,
      "loss": 0.0022,
      "step": 18120
    },
    {
      "epoch": 0.6043333333333333,
      "grad_norm": 0.21188782155513763,
      "learning_rate": 4.622291666666667e-05,
      "loss": 0.0026,
      "step": 18130
    },
    {
      "epoch": 0.6046666666666667,
      "grad_norm": 0.0609264075756073,
      "learning_rate": 4.6220833333333336e-05,
      "loss": 0.0022,
      "step": 18140
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.06115534529089928,
      "learning_rate": 4.621875e-05,
      "loss": 0.0037,
      "step": 18150
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.45109066367149353,
      "learning_rate": 4.621666666666667e-05,
      "loss": 0.0029,
      "step": 18160
    },
    {
      "epoch": 0.6056666666666667,
      "grad_norm": 0.30082616209983826,
      "learning_rate": 4.621458333333333e-05,
      "loss": 0.0036,
      "step": 18170
    },
    {
      "epoch": 0.606,
      "grad_norm": 0.12026887387037277,
      "learning_rate": 4.6212500000000005e-05,
      "loss": 0.0028,
      "step": 18180
    },
    {
      "epoch": 0.6063333333333333,
      "grad_norm": 0.3305211365222931,
      "learning_rate": 4.6210416666666664e-05,
      "loss": 0.003,
      "step": 18190
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.18050014972686768,
      "learning_rate": 4.6208333333333336e-05,
      "loss": 0.0023,
      "step": 18200
    },
    {
      "epoch": 0.607,
      "grad_norm": 0.33072078227996826,
      "learning_rate": 4.620625e-05,
      "loss": 0.0032,
      "step": 18210
    },
    {
      "epoch": 0.6073333333333333,
      "grad_norm": 0.24560801684856415,
      "learning_rate": 4.6204166666666674e-05,
      "loss": 0.0023,
      "step": 18220
    },
    {
      "epoch": 0.6076666666666667,
      "grad_norm": 0.09061715751886368,
      "learning_rate": 4.620208333333333e-05,
      "loss": 0.0034,
      "step": 18230
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.27075234055519104,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0032,
      "step": 18240
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.15018948912620544,
      "learning_rate": 4.619791666666667e-05,
      "loss": 0.0041,
      "step": 18250
    },
    {
      "epoch": 0.6086666666666667,
      "grad_norm": 0.36396023631095886,
      "learning_rate": 4.6195833333333336e-05,
      "loss": 0.0036,
      "step": 18260
    },
    {
      "epoch": 0.609,
      "grad_norm": 0.2703540325164795,
      "learning_rate": 4.619375e-05,
      "loss": 0.004,
      "step": 18270
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 0.030303996056318283,
      "learning_rate": 4.619166666666667e-05,
      "loss": 0.0022,
      "step": 18280
    },
    {
      "epoch": 0.6096666666666667,
      "grad_norm": 0.27055609226226807,
      "learning_rate": 4.618958333333334e-05,
      "loss": 0.004,
      "step": 18290
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.38238880038261414,
      "learning_rate": 4.61875e-05,
      "loss": 0.0027,
      "step": 18300
    },
    {
      "epoch": 0.6103333333333333,
      "grad_norm": 0.4206252098083496,
      "learning_rate": 4.618541666666667e-05,
      "loss": 0.0033,
      "step": 18310
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.5403187870979309,
      "learning_rate": 4.6183333333333336e-05,
      "loss": 0.004,
      "step": 18320
    },
    {
      "epoch": 0.611,
      "grad_norm": 0.060616932809352875,
      "learning_rate": 4.618125e-05,
      "loss": 0.0024,
      "step": 18330
    },
    {
      "epoch": 0.6113333333333333,
      "grad_norm": 0.06046752259135246,
      "learning_rate": 4.6179166666666667e-05,
      "loss": 0.002,
      "step": 18340
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.032053522765636444,
      "learning_rate": 4.617708333333334e-05,
      "loss": 0.0033,
      "step": 18350
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.0097802197560668,
      "learning_rate": 4.6175000000000004e-05,
      "loss": 0.002,
      "step": 18360
    },
    {
      "epoch": 0.6123333333333333,
      "grad_norm": 0.14998742938041687,
      "learning_rate": 4.617291666666667e-05,
      "loss": 0.0025,
      "step": 18370
    },
    {
      "epoch": 0.6126666666666667,
      "grad_norm": 0.09023632854223251,
      "learning_rate": 4.6170833333333335e-05,
      "loss": 0.0029,
      "step": 18380
    },
    {
      "epoch": 0.613,
      "grad_norm": 0.5406939387321472,
      "learning_rate": 4.616875e-05,
      "loss": 0.0032,
      "step": 18390
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.007486402057111263,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.004,
      "step": 18400
    },
    {
      "epoch": 0.6136666666666667,
      "grad_norm": 0.03112379088997841,
      "learning_rate": 4.616458333333333e-05,
      "loss": 0.0015,
      "step": 18410
    },
    {
      "epoch": 0.614,
      "grad_norm": 0.06054718792438507,
      "learning_rate": 4.6162500000000004e-05,
      "loss": 0.0017,
      "step": 18420
    },
    {
      "epoch": 0.6143333333333333,
      "grad_norm": 0.3002088665962219,
      "learning_rate": 4.616041666666667e-05,
      "loss": 0.0033,
      "step": 18430
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 0.33996865153312683,
      "learning_rate": 4.6158333333333335e-05,
      "loss": 0.0027,
      "step": 18440
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.03149711340665817,
      "learning_rate": 4.615625e-05,
      "loss": 0.003,
      "step": 18450
    },
    {
      "epoch": 0.6153333333333333,
      "grad_norm": 0.24209065735340118,
      "learning_rate": 4.615416666666667e-05,
      "loss": 0.0035,
      "step": 18460
    },
    {
      "epoch": 0.6156666666666667,
      "grad_norm": 0.4008300006389618,
      "learning_rate": 4.615208333333333e-05,
      "loss": 0.0038,
      "step": 18470
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.4024125635623932,
      "learning_rate": 4.6150000000000004e-05,
      "loss": 0.0034,
      "step": 18480
    },
    {
      "epoch": 0.6163333333333333,
      "grad_norm": 0.026173105463385582,
      "learning_rate": 4.614791666666667e-05,
      "loss": 0.0049,
      "step": 18490
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.27020779252052307,
      "learning_rate": 4.614583333333334e-05,
      "loss": 0.0032,
      "step": 18500
    },
    {
      "epoch": 0.617,
      "grad_norm": 0.15055671334266663,
      "learning_rate": 4.614375e-05,
      "loss": 0.0037,
      "step": 18510
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 0.03084365278482437,
      "learning_rate": 4.6141666666666666e-05,
      "loss": 0.0038,
      "step": 18520
    },
    {
      "epoch": 0.6176666666666667,
      "grad_norm": 0.032338034361600876,
      "learning_rate": 4.613958333333334e-05,
      "loss": 0.0042,
      "step": 18530
    },
    {
      "epoch": 0.618,
      "grad_norm": 0.20478494465351105,
      "learning_rate": 4.61375e-05,
      "loss": 0.0036,
      "step": 18540
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.09042955935001373,
      "learning_rate": 4.613541666666667e-05,
      "loss": 0.0027,
      "step": 18550
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.007510984316468239,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0039,
      "step": 18560
    },
    {
      "epoch": 0.619,
      "grad_norm": 0.2166147381067276,
      "learning_rate": 4.613125000000001e-05,
      "loss": 0.0033,
      "step": 18570
    },
    {
      "epoch": 0.6193333333333333,
      "grad_norm": 0.5404506921768188,
      "learning_rate": 4.6129166666666665e-05,
      "loss": 0.0031,
      "step": 18580
    },
    {
      "epoch": 0.6196666666666667,
      "grad_norm": 0.21711218357086182,
      "learning_rate": 4.612708333333334e-05,
      "loss": 0.0045,
      "step": 18590
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.33033984899520874,
      "learning_rate": 4.6125e-05,
      "loss": 0.0028,
      "step": 18600
    },
    {
      "epoch": 0.6203333333333333,
      "grad_norm": 0.48134058713912964,
      "learning_rate": 4.612291666666667e-05,
      "loss": 0.0035,
      "step": 18610
    },
    {
      "epoch": 0.6206666666666667,
      "grad_norm": 0.12029087543487549,
      "learning_rate": 4.6120833333333334e-05,
      "loss": 0.0034,
      "step": 18620
    },
    {
      "epoch": 0.621,
      "grad_norm": 0.3302557170391083,
      "learning_rate": 4.611875e-05,
      "loss": 0.003,
      "step": 18630
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.191936194896698,
      "learning_rate": 4.611666666666667e-05,
      "loss": 0.0012,
      "step": 18640
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.5552636981010437,
      "learning_rate": 4.611458333333333e-05,
      "loss": 0.0031,
      "step": 18650
    },
    {
      "epoch": 0.622,
      "grad_norm": 0.09034324437379837,
      "learning_rate": 4.61125e-05,
      "loss": 0.0023,
      "step": 18660
    },
    {
      "epoch": 0.6223333333333333,
      "grad_norm": 0.20969615876674652,
      "learning_rate": 4.611041666666667e-05,
      "loss": 0.0029,
      "step": 18670
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 0.00606864457949996,
      "learning_rate": 4.6108333333333334e-05,
      "loss": 0.0028,
      "step": 18680
    },
    {
      "epoch": 0.623,
      "grad_norm": 0.15096262097358704,
      "learning_rate": 4.610625e-05,
      "loss": 0.0036,
      "step": 18690
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.015527489595115185,
      "learning_rate": 4.610416666666667e-05,
      "loss": 0.0027,
      "step": 18700
    },
    {
      "epoch": 0.6236666666666667,
      "grad_norm": 0.24061310291290283,
      "learning_rate": 4.610208333333334e-05,
      "loss": 0.0027,
      "step": 18710
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.1812467724084854,
      "learning_rate": 4.61e-05,
      "loss": 0.003,
      "step": 18720
    },
    {
      "epoch": 0.6243333333333333,
      "grad_norm": 0.09989161789417267,
      "learning_rate": 4.609791666666667e-05,
      "loss": 0.0039,
      "step": 18730
    },
    {
      "epoch": 0.6246666666666667,
      "grad_norm": 0.15012365579605103,
      "learning_rate": 4.609583333333334e-05,
      "loss": 0.0029,
      "step": 18740
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.5929983258247375,
      "learning_rate": 4.609375e-05,
      "loss": 0.0032,
      "step": 18750
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 0.42093777656555176,
      "learning_rate": 4.6091666666666664e-05,
      "loss": 0.0031,
      "step": 18760
    },
    {
      "epoch": 0.6256666666666667,
      "grad_norm": 0.06147991493344307,
      "learning_rate": 4.608958333333334e-05,
      "loss": 0.0018,
      "step": 18770
    },
    {
      "epoch": 0.626,
      "grad_norm": 0.3904712498188019,
      "learning_rate": 4.60875e-05,
      "loss": 0.0035,
      "step": 18780
    },
    {
      "epoch": 0.6263333333333333,
      "grad_norm": 0.24072398245334625,
      "learning_rate": 4.608541666666667e-05,
      "loss": 0.0035,
      "step": 18790
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.2505120038986206,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.0027,
      "step": 18800
    },
    {
      "epoch": 0.627,
      "grad_norm": 0.5634874105453491,
      "learning_rate": 4.6081250000000005e-05,
      "loss": 0.0024,
      "step": 18810
    },
    {
      "epoch": 0.6273333333333333,
      "grad_norm": 0.0339587964117527,
      "learning_rate": 4.607916666666667e-05,
      "loss": 0.0028,
      "step": 18820
    },
    {
      "epoch": 0.6276666666666667,
      "grad_norm": 0.03538794070482254,
      "learning_rate": 4.6077083333333336e-05,
      "loss": 0.0017,
      "step": 18830
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.3302275836467743,
      "learning_rate": 4.6075e-05,
      "loss": 0.0029,
      "step": 18840
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.06029513478279114,
      "learning_rate": 4.6072916666666674e-05,
      "loss": 0.0022,
      "step": 18850
    },
    {
      "epoch": 0.6286666666666667,
      "grad_norm": 0.3596140146255493,
      "learning_rate": 4.607083333333333e-05,
      "loss": 0.0039,
      "step": 18860
    },
    {
      "epoch": 0.629,
      "grad_norm": 0.031915366649627686,
      "learning_rate": 4.6068750000000005e-05,
      "loss": 0.0028,
      "step": 18870
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.14992518723011017,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.003,
      "step": 18880
    },
    {
      "epoch": 0.6296666666666667,
      "grad_norm": 0.46029818058013916,
      "learning_rate": 4.6064583333333336e-05,
      "loss": 0.0034,
      "step": 18890
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.6560149192810059,
      "learning_rate": 4.60625e-05,
      "loss": 0.0038,
      "step": 18900
    },
    {
      "epoch": 0.6303333333333333,
      "grad_norm": 0.33531683683395386,
      "learning_rate": 4.606041666666667e-05,
      "loss": 0.002,
      "step": 18910
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 0.06262549757957458,
      "learning_rate": 4.605833333333334e-05,
      "loss": 0.0028,
      "step": 18920
    },
    {
      "epoch": 0.631,
      "grad_norm": 0.35995110869407654,
      "learning_rate": 4.605625e-05,
      "loss": 0.0025,
      "step": 18930
    },
    {
      "epoch": 0.6313333333333333,
      "grad_norm": 0.30038443207740784,
      "learning_rate": 4.605416666666667e-05,
      "loss": 0.0029,
      "step": 18940
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.2698499858379364,
      "learning_rate": 4.6052083333333336e-05,
      "loss": 0.0026,
      "step": 18950
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.24025355279445648,
      "learning_rate": 4.605e-05,
      "loss": 0.0028,
      "step": 18960
    },
    {
      "epoch": 0.6323333333333333,
      "grad_norm": 0.6901259422302246,
      "learning_rate": 4.604791666666667e-05,
      "loss": 0.0029,
      "step": 18970
    },
    {
      "epoch": 0.6326666666666667,
      "grad_norm": 0.3302266597747803,
      "learning_rate": 4.604583333333334e-05,
      "loss": 0.0037,
      "step": 18980
    },
    {
      "epoch": 0.633,
      "grad_norm": 0.47982102632522583,
      "learning_rate": 4.6043750000000004e-05,
      "loss": 0.0027,
      "step": 18990
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.4800337851047516,
      "learning_rate": 4.604166666666666e-05,
      "loss": 0.0019,
      "step": 19000
    },
    {
      "epoch": 0.6336666666666667,
      "grad_norm": 0.06289135664701462,
      "learning_rate": 4.6039583333333335e-05,
      "loss": 0.0029,
      "step": 19010
    },
    {
      "epoch": 0.634,
      "grad_norm": 0.24019721150398254,
      "learning_rate": 4.60375e-05,
      "loss": 0.0033,
      "step": 19020
    },
    {
      "epoch": 0.6343333333333333,
      "grad_norm": 0.2403932809829712,
      "learning_rate": 4.6035416666666666e-05,
      "loss": 0.003,
      "step": 19030
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.1796211451292038,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.003,
      "step": 19040
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.38979053497314453,
      "learning_rate": 4.6031250000000004e-05,
      "loss": 0.0025,
      "step": 19050
    },
    {
      "epoch": 0.6353333333333333,
      "grad_norm": 0.33040425181388855,
      "learning_rate": 4.602916666666667e-05,
      "loss": 0.0036,
      "step": 19060
    },
    {
      "epoch": 0.6356666666666667,
      "grad_norm": 0.39151889085769653,
      "learning_rate": 4.6027083333333335e-05,
      "loss": 0.0023,
      "step": 19070
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.6633229851722717,
      "learning_rate": 4.6025e-05,
      "loss": 0.0022,
      "step": 19080
    },
    {
      "epoch": 0.6363333333333333,
      "grad_norm": 0.060239773243665695,
      "learning_rate": 4.602291666666667e-05,
      "loss": 0.0027,
      "step": 19090
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.09054767340421677,
      "learning_rate": 4.602083333333334e-05,
      "loss": 0.0031,
      "step": 19100
    },
    {
      "epoch": 0.637,
      "grad_norm": 0.33057746291160583,
      "learning_rate": 4.6018750000000004e-05,
      "loss": 0.0025,
      "step": 19110
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.15065917372703552,
      "learning_rate": 4.601666666666667e-05,
      "loss": 0.0039,
      "step": 19120
    },
    {
      "epoch": 0.6376666666666667,
      "grad_norm": 0.6930310726165771,
      "learning_rate": 4.6014583333333335e-05,
      "loss": 0.0028,
      "step": 19130
    },
    {
      "epoch": 0.638,
      "grad_norm": 0.6013939380645752,
      "learning_rate": 4.60125e-05,
      "loss": 0.0027,
      "step": 19140
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.2707657516002655,
      "learning_rate": 4.6010416666666666e-05,
      "loss": 0.002,
      "step": 19150
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 0.6602230668067932,
      "learning_rate": 4.600833333333334e-05,
      "loss": 0.0049,
      "step": 19160
    },
    {
      "epoch": 0.639,
      "grad_norm": 0.42025119066238403,
      "learning_rate": 4.6006250000000004e-05,
      "loss": 0.0026,
      "step": 19170
    },
    {
      "epoch": 0.6393333333333333,
      "grad_norm": 0.12040567398071289,
      "learning_rate": 4.600416666666667e-05,
      "loss": 0.0021,
      "step": 19180
    },
    {
      "epoch": 0.6396666666666667,
      "grad_norm": 0.24038611352443695,
      "learning_rate": 4.6002083333333335e-05,
      "loss": 0.005,
      "step": 19190
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.5698230266571045,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0042,
      "step": 19200
    },
    {
      "epoch": 0.6403333333333333,
      "grad_norm": 0.17970947921276093,
      "learning_rate": 4.5997916666666666e-05,
      "loss": 0.0034,
      "step": 19210
    },
    {
      "epoch": 0.6406666666666667,
      "grad_norm": 0.060019947588443756,
      "learning_rate": 4.599583333333334e-05,
      "loss": 0.0024,
      "step": 19220
    },
    {
      "epoch": 0.641,
      "grad_norm": 0.3600279986858368,
      "learning_rate": 4.599375e-05,
      "loss": 0.002,
      "step": 19230
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 0.2695907950401306,
      "learning_rate": 4.599166666666667e-05,
      "loss": 0.0042,
      "step": 19240
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.18036344647407532,
      "learning_rate": 4.5989583333333334e-05,
      "loss": 0.0026,
      "step": 19250
    },
    {
      "epoch": 0.642,
      "grad_norm": 0.27833881974220276,
      "learning_rate": 4.59875e-05,
      "loss": 0.0032,
      "step": 19260
    },
    {
      "epoch": 0.6423333333333333,
      "grad_norm": 0.030733991414308548,
      "learning_rate": 4.598541666666667e-05,
      "loss": 0.0028,
      "step": 19270
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.03465031087398529,
      "learning_rate": 4.598333333333333e-05,
      "loss": 0.0027,
      "step": 19280
    },
    {
      "epoch": 0.643,
      "grad_norm": 0.6893089413642883,
      "learning_rate": 4.598125e-05,
      "loss": 0.0024,
      "step": 19290
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.23966069519519806,
      "learning_rate": 4.597916666666667e-05,
      "loss": 0.0029,
      "step": 19300
    },
    {
      "epoch": 0.6436666666666667,
      "grad_norm": 0.18024703860282898,
      "learning_rate": 4.5977083333333334e-05,
      "loss": 0.0032,
      "step": 19310
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.09016843140125275,
      "learning_rate": 4.5975e-05,
      "loss": 0.0036,
      "step": 19320
    },
    {
      "epoch": 0.6443333333333333,
      "grad_norm": 0.14945024251937866,
      "learning_rate": 4.597291666666667e-05,
      "loss": 0.003,
      "step": 19330
    },
    {
      "epoch": 0.6446666666666667,
      "grad_norm": 0.014089347794651985,
      "learning_rate": 4.597083333333334e-05,
      "loss": 0.0027,
      "step": 19340
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.30044740438461304,
      "learning_rate": 4.596875e-05,
      "loss": 0.0037,
      "step": 19350
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.2701556384563446,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.0032,
      "step": 19360
    },
    {
      "epoch": 0.6456666666666667,
      "grad_norm": 0.36007165908813477,
      "learning_rate": 4.5964583333333334e-05,
      "loss": 0.0025,
      "step": 19370
    },
    {
      "epoch": 0.646,
      "grad_norm": 0.25202134251594543,
      "learning_rate": 4.5962500000000006e-05,
      "loss": 0.0036,
      "step": 19380
    },
    {
      "epoch": 0.6463333333333333,
      "grad_norm": 0.23939277231693268,
      "learning_rate": 4.5960416666666665e-05,
      "loss": 0.0038,
      "step": 19390
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.2591720521450043,
      "learning_rate": 4.595833333333334e-05,
      "loss": 0.0026,
      "step": 19400
    },
    {
      "epoch": 0.647,
      "grad_norm": 0.2698174715042114,
      "learning_rate": 4.595625e-05,
      "loss": 0.0026,
      "step": 19410
    },
    {
      "epoch": 0.6473333333333333,
      "grad_norm": 0.3897492289543152,
      "learning_rate": 4.595416666666667e-05,
      "loss": 0.0044,
      "step": 19420
    },
    {
      "epoch": 0.6476666666666666,
      "grad_norm": 0.00563791673630476,
      "learning_rate": 4.595208333333333e-05,
      "loss": 0.0027,
      "step": 19430
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.030718639492988586,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 0.003,
      "step": 19440
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.091275155544281,
      "learning_rate": 4.594791666666667e-05,
      "loss": 0.003,
      "step": 19450
    },
    {
      "epoch": 0.6486666666666666,
      "grad_norm": 0.03057228960096836,
      "learning_rate": 4.5945833333333337e-05,
      "loss": 0.0036,
      "step": 19460
    },
    {
      "epoch": 0.649,
      "grad_norm": 0.06017020344734192,
      "learning_rate": 4.594375e-05,
      "loss": 0.0033,
      "step": 19470
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 0.3599604368209839,
      "learning_rate": 4.594166666666667e-05,
      "loss": 0.004,
      "step": 19480
    },
    {
      "epoch": 0.6496666666666666,
      "grad_norm": 0.15043707191944122,
      "learning_rate": 4.593958333333333e-05,
      "loss": 0.0038,
      "step": 19490
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.4722713530063629,
      "learning_rate": 4.59375e-05,
      "loss": 0.0042,
      "step": 19500
    },
    {
      "epoch": 0.6503333333333333,
      "grad_norm": 0.4792650640010834,
      "learning_rate": 4.593541666666667e-05,
      "loss": 0.003,
      "step": 19510
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.48365065455436707,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0028,
      "step": 19520
    },
    {
      "epoch": 0.651,
      "grad_norm": 0.2714650630950928,
      "learning_rate": 4.593125e-05,
      "loss": 0.0025,
      "step": 19530
    },
    {
      "epoch": 0.6513333333333333,
      "grad_norm": 0.7965813875198364,
      "learning_rate": 4.592916666666667e-05,
      "loss": 0.0036,
      "step": 19540
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.032697804272174835,
      "learning_rate": 4.592708333333334e-05,
      "loss": 0.0024,
      "step": 19550
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.18043215572834015,
      "learning_rate": 4.5925e-05,
      "loss": 0.0034,
      "step": 19560
    },
    {
      "epoch": 0.6523333333333333,
      "grad_norm": 0.41665905714035034,
      "learning_rate": 4.592291666666667e-05,
      "loss": 0.0039,
      "step": 19570
    },
    {
      "epoch": 0.6526666666666666,
      "grad_norm": 0.299852192401886,
      "learning_rate": 4.5920833333333336e-05,
      "loss": 0.0044,
      "step": 19580
    },
    {
      "epoch": 0.653,
      "grad_norm": 0.31878548860549927,
      "learning_rate": 4.591875e-05,
      "loss": 0.004,
      "step": 19590
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.09073568880558014,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.0025,
      "step": 19600
    },
    {
      "epoch": 0.6536666666666666,
      "grad_norm": 0.23991389572620392,
      "learning_rate": 4.591458333333333e-05,
      "loss": 0.0032,
      "step": 19610
    },
    {
      "epoch": 0.654,
      "grad_norm": 0.06039166823029518,
      "learning_rate": 4.5912500000000005e-05,
      "loss": 0.002,
      "step": 19620
    },
    {
      "epoch": 0.6543333333333333,
      "grad_norm": 0.21025900542736053,
      "learning_rate": 4.591041666666667e-05,
      "loss": 0.0026,
      "step": 19630
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 0.21040180325508118,
      "learning_rate": 4.5908333333333336e-05,
      "loss": 0.0028,
      "step": 19640
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.06014515832066536,
      "learning_rate": 4.590625e-05,
      "loss": 0.0015,
      "step": 19650
    },
    {
      "epoch": 0.6553333333333333,
      "grad_norm": 0.35868319869041443,
      "learning_rate": 4.5904166666666673e-05,
      "loss": 0.0019,
      "step": 19660
    },
    {
      "epoch": 0.6556666666666666,
      "grad_norm": 0.08974743634462357,
      "learning_rate": 4.590208333333333e-05,
      "loss": 0.0033,
      "step": 19670
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.3590587079524994,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0026,
      "step": 19680
    },
    {
      "epoch": 0.6563333333333333,
      "grad_norm": 0.8256836533546448,
      "learning_rate": 4.589791666666667e-05,
      "loss": 0.0031,
      "step": 19690
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.383338063955307,
      "learning_rate": 4.5895833333333335e-05,
      "loss": 0.0034,
      "step": 19700
    },
    {
      "epoch": 0.657,
      "grad_norm": 0.6784031391143799,
      "learning_rate": 4.589375e-05,
      "loss": 0.0029,
      "step": 19710
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 0.7789005637168884,
      "learning_rate": 4.5891666666666666e-05,
      "loss": 0.0028,
      "step": 19720
    },
    {
      "epoch": 0.6576666666666666,
      "grad_norm": 0.33772897720336914,
      "learning_rate": 4.588958333333334e-05,
      "loss": 0.0035,
      "step": 19730
    },
    {
      "epoch": 0.658,
      "grad_norm": 0.2995340824127197,
      "learning_rate": 4.58875e-05,
      "loss": 0.0028,
      "step": 19740
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.30520397424697876,
      "learning_rate": 4.588541666666667e-05,
      "loss": 0.0057,
      "step": 19750
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.2619318664073944,
      "learning_rate": 4.5883333333333335e-05,
      "loss": 0.0044,
      "step": 19760
    },
    {
      "epoch": 0.659,
      "grad_norm": 0.20989231765270233,
      "learning_rate": 4.588125e-05,
      "loss": 0.0035,
      "step": 19770
    },
    {
      "epoch": 0.6593333333333333,
      "grad_norm": 0.12336885184049606,
      "learning_rate": 4.5879166666666666e-05,
      "loss": 0.0036,
      "step": 19780
    },
    {
      "epoch": 0.6596666666666666,
      "grad_norm": 0.24374495446681976,
      "learning_rate": 4.587708333333334e-05,
      "loss": 0.0041,
      "step": 19790
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.477659672498703,
      "learning_rate": 4.5875000000000004e-05,
      "loss": 0.0028,
      "step": 19800
    },
    {
      "epoch": 0.6603333333333333,
      "grad_norm": 0.2698127031326294,
      "learning_rate": 4.587291666666667e-05,
      "loss": 0.0026,
      "step": 19810
    },
    {
      "epoch": 0.6606666666666666,
      "grad_norm": 0.35844773054122925,
      "learning_rate": 4.5870833333333335e-05,
      "loss": 0.0027,
      "step": 19820
    },
    {
      "epoch": 0.661,
      "grad_norm": 0.23984110355377197,
      "learning_rate": 4.586875000000001e-05,
      "loss": 0.0031,
      "step": 19830
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.38888320326805115,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0035,
      "step": 19840
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.20942090451717377,
      "learning_rate": 4.586458333333333e-05,
      "loss": 0.0042,
      "step": 19850
    },
    {
      "epoch": 0.662,
      "grad_norm": 0.1831977218389511,
      "learning_rate": 4.5862500000000003e-05,
      "loss": 0.0028,
      "step": 19860
    },
    {
      "epoch": 0.6623333333333333,
      "grad_norm": 0.44998592138290405,
      "learning_rate": 4.586041666666667e-05,
      "loss": 0.0033,
      "step": 19870
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 0.030731668695807457,
      "learning_rate": 4.5858333333333334e-05,
      "loss": 0.0033,
      "step": 19880
    },
    {
      "epoch": 0.663,
      "grad_norm": 0.39305663108825684,
      "learning_rate": 4.585625e-05,
      "loss": 0.0021,
      "step": 19890
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.5361893773078918,
      "learning_rate": 4.585416666666667e-05,
      "loss": 0.0027,
      "step": 19900
    },
    {
      "epoch": 0.6636666666666666,
      "grad_norm": 0.20820073783397675,
      "learning_rate": 4.585208333333334e-05,
      "loss": 0.0025,
      "step": 19910
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.2700117230415344,
      "learning_rate": 4.585e-05,
      "loss": 0.0045,
      "step": 19920
    },
    {
      "epoch": 0.6643333333333333,
      "grad_norm": 0.06049424037337303,
      "learning_rate": 4.584791666666667e-05,
      "loss": 0.0022,
      "step": 19930
    },
    {
      "epoch": 0.6646666666666666,
      "grad_norm": 0.42399802803993225,
      "learning_rate": 4.584583333333334e-05,
      "loss": 0.0028,
      "step": 19940
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.3028288185596466,
      "learning_rate": 4.584375e-05,
      "loss": 0.0036,
      "step": 19950
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 0.30020102858543396,
      "learning_rate": 4.5841666666666665e-05,
      "loss": 0.0033,
      "step": 19960
    },
    {
      "epoch": 0.6656666666666666,
      "grad_norm": 0.3301874101161957,
      "learning_rate": 4.583958333333334e-05,
      "loss": 0.0031,
      "step": 19970
    },
    {
      "epoch": 0.666,
      "grad_norm": 0.2990070879459381,
      "learning_rate": 4.58375e-05,
      "loss": 0.0032,
      "step": 19980
    },
    {
      "epoch": 0.6663333333333333,
      "grad_norm": 0.12006223201751709,
      "learning_rate": 4.583541666666667e-05,
      "loss": 0.0023,
      "step": 19990
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.21026425063610077,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0039,
      "step": 20000
    },
    {
      "epoch": 0.667,
      "grad_norm": 0.0069280690513551235,
      "learning_rate": 4.5831250000000006e-05,
      "loss": 0.0024,
      "step": 20010
    },
    {
      "epoch": 0.6673333333333333,
      "grad_norm": 0.26902157068252563,
      "learning_rate": 4.5829166666666665e-05,
      "loss": 0.0024,
      "step": 20020
    },
    {
      "epoch": 0.6676666666666666,
      "grad_norm": 0.10090222954750061,
      "learning_rate": 4.582708333333334e-05,
      "loss": 0.0027,
      "step": 20030
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.3590124845504761,
      "learning_rate": 4.5825e-05,
      "loss": 0.0026,
      "step": 20040
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.3094624876976013,
      "learning_rate": 4.582291666666667e-05,
      "loss": 0.003,
      "step": 20050
    },
    {
      "epoch": 0.6686666666666666,
      "grad_norm": 0.03190068155527115,
      "learning_rate": 4.5820833333333334e-05,
      "loss": 0.0033,
      "step": 20060
    },
    {
      "epoch": 0.669,
      "grad_norm": 0.12069661915302277,
      "learning_rate": 4.5818750000000006e-05,
      "loss": 0.0034,
      "step": 20070
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.4193362295627594,
      "learning_rate": 4.581666666666667e-05,
      "loss": 0.0033,
      "step": 20080
    },
    {
      "epoch": 0.6696666666666666,
      "grad_norm": 0.06025393307209015,
      "learning_rate": 4.581458333333333e-05,
      "loss": 0.0036,
      "step": 20090
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.06042102724313736,
      "learning_rate": 4.58125e-05,
      "loss": 0.0024,
      "step": 20100
    },
    {
      "epoch": 0.6703333333333333,
      "grad_norm": 0.03063124604523182,
      "learning_rate": 4.581041666666667e-05,
      "loss": 0.0035,
      "step": 20110
    },
    {
      "epoch": 0.6706666666666666,
      "grad_norm": 0.0059801312163472176,
      "learning_rate": 4.580833333333333e-05,
      "loss": 0.0033,
      "step": 20120
    },
    {
      "epoch": 0.671,
      "grad_norm": 0.3886314034461975,
      "learning_rate": 4.580625e-05,
      "loss": 0.003,
      "step": 20130
    },
    {
      "epoch": 0.6713333333333333,
      "grad_norm": 0.11998298764228821,
      "learning_rate": 4.580416666666667e-05,
      "loss": 0.0033,
      "step": 20140
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.20961225032806396,
      "learning_rate": 4.5802083333333336e-05,
      "loss": 0.0035,
      "step": 20150
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.14959728717803955,
      "learning_rate": 4.58e-05,
      "loss": 0.0037,
      "step": 20160
    },
    {
      "epoch": 0.6723333333333333,
      "grad_norm": 0.11963821947574615,
      "learning_rate": 4.579791666666667e-05,
      "loss": 0.0031,
      "step": 20170
    },
    {
      "epoch": 0.6726666666666666,
      "grad_norm": 0.059843190014362335,
      "learning_rate": 4.579583333333334e-05,
      "loss": 0.003,
      "step": 20180
    },
    {
      "epoch": 0.673,
      "grad_norm": 0.17993830144405365,
      "learning_rate": 4.5793750000000005e-05,
      "loss": 0.0029,
      "step": 20190
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.2095293253660202,
      "learning_rate": 4.579166666666667e-05,
      "loss": 0.0029,
      "step": 20200
    },
    {
      "epoch": 0.6736666666666666,
      "grad_norm": 0.06029278412461281,
      "learning_rate": 4.5789583333333336e-05,
      "loss": 0.0033,
      "step": 20210
    },
    {
      "epoch": 0.674,
      "grad_norm": 0.04181995615363121,
      "learning_rate": 4.57875e-05,
      "loss": 0.0031,
      "step": 20220
    },
    {
      "epoch": 0.6743333333333333,
      "grad_norm": 0.3054887652397156,
      "learning_rate": 4.578541666666667e-05,
      "loss": 0.0021,
      "step": 20230
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.5085864663124084,
      "learning_rate": 4.578333333333333e-05,
      "loss": 0.0022,
      "step": 20240
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.6124866008758545,
      "learning_rate": 4.5781250000000005e-05,
      "loss": 0.0043,
      "step": 20250
    },
    {
      "epoch": 0.6753333333333333,
      "grad_norm": 0.031667184084653854,
      "learning_rate": 4.577916666666667e-05,
      "loss": 0.0034,
      "step": 20260
    },
    {
      "epoch": 0.6756666666666666,
      "grad_norm": 0.060332994908094406,
      "learning_rate": 4.5777083333333336e-05,
      "loss": 0.004,
      "step": 20270
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.11965329945087433,
      "learning_rate": 4.5775e-05,
      "loss": 0.003,
      "step": 20280
    },
    {
      "epoch": 0.6763333333333333,
      "grad_norm": 0.6769739985466003,
      "learning_rate": 4.5772916666666674e-05,
      "loss": 0.0035,
      "step": 20290
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.527802050113678,
      "learning_rate": 4.577083333333333e-05,
      "loss": 0.0034,
      "step": 20300
    },
    {
      "epoch": 0.677,
      "grad_norm": 0.239207461476326,
      "learning_rate": 4.5768750000000005e-05,
      "loss": 0.0018,
      "step": 20310
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.09073658287525177,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.0031,
      "step": 20320
    },
    {
      "epoch": 0.6776666666666666,
      "grad_norm": 0.20907527208328247,
      "learning_rate": 4.5764583333333336e-05,
      "loss": 0.0027,
      "step": 20330
    },
    {
      "epoch": 0.678,
      "grad_norm": 0.20994895696640015,
      "learning_rate": 4.57625e-05,
      "loss": 0.0021,
      "step": 20340
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.09005892276763916,
      "learning_rate": 4.5760416666666667e-05,
      "loss": 0.0019,
      "step": 20350
    },
    {
      "epoch": 0.6786666666666666,
      "grad_norm": 0.09065361320972443,
      "learning_rate": 4.575833333333334e-05,
      "loss": 0.0028,
      "step": 20360
    },
    {
      "epoch": 0.679,
      "grad_norm": 0.29939356446266174,
      "learning_rate": 4.575625e-05,
      "loss": 0.0021,
      "step": 20370
    },
    {
      "epoch": 0.6793333333333333,
      "grad_norm": 0.08988753706216812,
      "learning_rate": 4.575416666666667e-05,
      "loss": 0.0027,
      "step": 20380
    },
    {
      "epoch": 0.6796666666666666,
      "grad_norm": 0.11995284259319305,
      "learning_rate": 4.5752083333333335e-05,
      "loss": 0.0028,
      "step": 20390
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.15016785264015198,
      "learning_rate": 4.575e-05,
      "loss": 0.0029,
      "step": 20400
    },
    {
      "epoch": 0.6803333333333333,
      "grad_norm": 0.47889938950538635,
      "learning_rate": 4.5747916666666666e-05,
      "loss": 0.0034,
      "step": 20410
    },
    {
      "epoch": 0.6806666666666666,
      "grad_norm": 0.3591538071632385,
      "learning_rate": 4.574583333333334e-05,
      "loss": 0.0027,
      "step": 20420
    },
    {
      "epoch": 0.681,
      "grad_norm": 0.23947854340076447,
      "learning_rate": 4.5743750000000004e-05,
      "loss": 0.0032,
      "step": 20430
    },
    {
      "epoch": 0.6813333333333333,
      "grad_norm": 0.010204200632870197,
      "learning_rate": 4.574166666666667e-05,
      "loss": 0.0031,
      "step": 20440
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.34680166840553284,
      "learning_rate": 4.5739583333333335e-05,
      "loss": 0.0026,
      "step": 20450
    },
    {
      "epoch": 0.682,
      "grad_norm": 0.23972894251346588,
      "learning_rate": 4.57375e-05,
      "loss": 0.0024,
      "step": 20460
    },
    {
      "epoch": 0.6823333333333333,
      "grad_norm": 0.23947571218013763,
      "learning_rate": 4.573541666666667e-05,
      "loss": 0.0022,
      "step": 20470
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.3957952857017517,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0022,
      "step": 20480
    },
    {
      "epoch": 0.683,
      "grad_norm": 0.17972955107688904,
      "learning_rate": 4.5731250000000004e-05,
      "loss": 0.0016,
      "step": 20490
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.3892439007759094,
      "learning_rate": 4.572916666666667e-05,
      "loss": 0.0034,
      "step": 20500
    },
    {
      "epoch": 0.6836666666666666,
      "grad_norm": 0.0902877002954483,
      "learning_rate": 4.5727083333333335e-05,
      "loss": 0.0044,
      "step": 20510
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.0896557867527008,
      "learning_rate": 4.5725e-05,
      "loss": 0.0034,
      "step": 20520
    },
    {
      "epoch": 0.6843333333333333,
      "grad_norm": 0.7181281447410583,
      "learning_rate": 4.572291666666667e-05,
      "loss": 0.0036,
      "step": 20530
    },
    {
      "epoch": 0.6846666666666666,
      "grad_norm": 0.06017342954874039,
      "learning_rate": 4.572083333333334e-05,
      "loss": 0.0033,
      "step": 20540
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.40479278564453125,
      "learning_rate": 4.571875e-05,
      "loss": 0.0027,
      "step": 20550
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.17978742718696594,
      "learning_rate": 4.571666666666667e-05,
      "loss": 0.0028,
      "step": 20560
    },
    {
      "epoch": 0.6856666666666666,
      "grad_norm": 0.06050492823123932,
      "learning_rate": 4.5714583333333334e-05,
      "loss": 0.0026,
      "step": 20570
    },
    {
      "epoch": 0.686,
      "grad_norm": 0.5837447047233582,
      "learning_rate": 4.57125e-05,
      "loss": 0.0033,
      "step": 20580
    },
    {
      "epoch": 0.6863333333333334,
      "grad_norm": 0.6676428914070129,
      "learning_rate": 4.5710416666666665e-05,
      "loss": 0.0024,
      "step": 20590
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.5386356115341187,
      "learning_rate": 4.570833333333334e-05,
      "loss": 0.003,
      "step": 20600
    },
    {
      "epoch": 0.687,
      "grad_norm": 0.8928620219230652,
      "learning_rate": 4.570625e-05,
      "loss": 0.005,
      "step": 20610
    },
    {
      "epoch": 0.6873333333333334,
      "grad_norm": 0.5388551950454712,
      "learning_rate": 4.570416666666667e-05,
      "loss": 0.0025,
      "step": 20620
    },
    {
      "epoch": 0.6876666666666666,
      "grad_norm": 0.23937685787677765,
      "learning_rate": 4.5702083333333334e-05,
      "loss": 0.0047,
      "step": 20630
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.4792550802230835,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.0025,
      "step": 20640
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.008832359686493874,
      "learning_rate": 4.5697916666666665e-05,
      "loss": 0.0026,
      "step": 20650
    },
    {
      "epoch": 0.6886666666666666,
      "grad_norm": 0.18079954385757446,
      "learning_rate": 4.569583333333334e-05,
      "loss": 0.0032,
      "step": 20660
    },
    {
      "epoch": 0.689,
      "grad_norm": 0.269410103559494,
      "learning_rate": 4.569375e-05,
      "loss": 0.003,
      "step": 20670
    },
    {
      "epoch": 0.6893333333333334,
      "grad_norm": 0.23931890726089478,
      "learning_rate": 4.569166666666667e-05,
      "loss": 0.0021,
      "step": 20680
    },
    {
      "epoch": 0.6896666666666667,
      "grad_norm": 0.14982829988002777,
      "learning_rate": 4.5689583333333334e-05,
      "loss": 0.004,
      "step": 20690
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.006611155346035957,
      "learning_rate": 4.56875e-05,
      "loss": 0.0034,
      "step": 20700
    },
    {
      "epoch": 0.6903333333333334,
      "grad_norm": 0.12009096890687943,
      "learning_rate": 4.568541666666667e-05,
      "loss": 0.0043,
      "step": 20710
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.30066418647766113,
      "learning_rate": 4.568333333333333e-05,
      "loss": 0.0031,
      "step": 20720
    },
    {
      "epoch": 0.691,
      "grad_norm": 0.17967846989631653,
      "learning_rate": 4.568125e-05,
      "loss": 0.003,
      "step": 20730
    },
    {
      "epoch": 0.6913333333333334,
      "grad_norm": 0.5541740655899048,
      "learning_rate": 4.567916666666667e-05,
      "loss": 0.0029,
      "step": 20740
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.41914865374565125,
      "learning_rate": 4.567708333333334e-05,
      "loss": 0.0033,
      "step": 20750
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.538520097732544,
      "learning_rate": 4.5675e-05,
      "loss": 0.0039,
      "step": 20760
    },
    {
      "epoch": 0.6923333333333334,
      "grad_norm": 0.24147282540798187,
      "learning_rate": 4.567291666666667e-05,
      "loss": 0.0037,
      "step": 20770
    },
    {
      "epoch": 0.6926666666666667,
      "grad_norm": 0.2989538609981537,
      "learning_rate": 4.567083333333334e-05,
      "loss": 0.003,
      "step": 20780
    },
    {
      "epoch": 0.693,
      "grad_norm": 0.35911521315574646,
      "learning_rate": 4.566875e-05,
      "loss": 0.0018,
      "step": 20790
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.03066934272646904,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0028,
      "step": 20800
    },
    {
      "epoch": 0.6936666666666667,
      "grad_norm": 0.49099257588386536,
      "learning_rate": 4.566458333333333e-05,
      "loss": 0.0032,
      "step": 20810
    },
    {
      "epoch": 0.694,
      "grad_norm": 0.1796429455280304,
      "learning_rate": 4.5662500000000005e-05,
      "loss": 0.0019,
      "step": 20820
    },
    {
      "epoch": 0.6943333333333334,
      "grad_norm": 0.1197434663772583,
      "learning_rate": 4.5660416666666664e-05,
      "loss": 0.0033,
      "step": 20830
    },
    {
      "epoch": 0.6946666666666667,
      "grad_norm": 0.36404556035995483,
      "learning_rate": 4.5658333333333336e-05,
      "loss": 0.0048,
      "step": 20840
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.15893076360225677,
      "learning_rate": 4.565625e-05,
      "loss": 0.0037,
      "step": 20850
    },
    {
      "epoch": 0.6953333333333334,
      "grad_norm": 0.39988118410110474,
      "learning_rate": 4.565416666666667e-05,
      "loss": 0.0029,
      "step": 20860
    },
    {
      "epoch": 0.6956666666666667,
      "grad_norm": 0.05864987149834633,
      "learning_rate": 4.565208333333333e-05,
      "loss": 0.004,
      "step": 20870
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.09006458520889282,
      "learning_rate": 4.5650000000000005e-05,
      "loss": 0.0024,
      "step": 20880
    },
    {
      "epoch": 0.6963333333333334,
      "grad_norm": 0.08982188254594803,
      "learning_rate": 4.564791666666667e-05,
      "loss": 0.0032,
      "step": 20890
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.03049067035317421,
      "learning_rate": 4.5645833333333336e-05,
      "loss": 0.0035,
      "step": 20900
    },
    {
      "epoch": 0.697,
      "grad_norm": 0.14990541338920593,
      "learning_rate": 4.564375e-05,
      "loss": 0.0033,
      "step": 20910
    },
    {
      "epoch": 0.6973333333333334,
      "grad_norm": 0.08945254981517792,
      "learning_rate": 4.5641666666666674e-05,
      "loss": 0.0026,
      "step": 20920
    },
    {
      "epoch": 0.6976666666666667,
      "grad_norm": 0.2711400091648102,
      "learning_rate": 4.563958333333333e-05,
      "loss": 0.002,
      "step": 20930
    },
    {
      "epoch": 0.698,
      "grad_norm": 0.2391761839389801,
      "learning_rate": 4.56375e-05,
      "loss": 0.0046,
      "step": 20940
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.6945165395736694,
      "learning_rate": 4.563541666666667e-05,
      "loss": 0.0036,
      "step": 20950
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.08998389542102814,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.0028,
      "step": 20960
    },
    {
      "epoch": 0.699,
      "grad_norm": 0.1491178721189499,
      "learning_rate": 4.563125e-05,
      "loss": 0.0026,
      "step": 20970
    },
    {
      "epoch": 0.6993333333333334,
      "grad_norm": 0.2984430491924286,
      "learning_rate": 4.562916666666667e-05,
      "loss": 0.003,
      "step": 20980
    },
    {
      "epoch": 0.6996666666666667,
      "grad_norm": 0.0599205456674099,
      "learning_rate": 4.562708333333334e-05,
      "loss": 0.0024,
      "step": 20990
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.09049204736948013,
      "learning_rate": 4.5625e-05,
      "loss": 0.0029,
      "step": 21000
    },
    {
      "epoch": 0.7003333333333334,
      "grad_norm": 0.549888551235199,
      "learning_rate": 4.562291666666667e-05,
      "loss": 0.0032,
      "step": 21010
    },
    {
      "epoch": 0.7006666666666667,
      "grad_norm": 0.060813333839178085,
      "learning_rate": 4.5620833333333335e-05,
      "loss": 0.0031,
      "step": 21020
    },
    {
      "epoch": 0.701,
      "grad_norm": 0.2390889972448349,
      "learning_rate": 4.561875000000001e-05,
      "loss": 0.0031,
      "step": 21030
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.03151994198560715,
      "learning_rate": 4.5616666666666666e-05,
      "loss": 0.0019,
      "step": 21040
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.8587555885314941,
      "learning_rate": 4.561458333333333e-05,
      "loss": 0.0033,
      "step": 21050
    },
    {
      "epoch": 0.702,
      "grad_norm": 0.11944039165973663,
      "learning_rate": 4.5612500000000004e-05,
      "loss": 0.0023,
      "step": 21060
    },
    {
      "epoch": 0.7023333333333334,
      "grad_norm": 0.2689802646636963,
      "learning_rate": 4.561041666666667e-05,
      "loss": 0.0031,
      "step": 21070
    },
    {
      "epoch": 0.7026666666666667,
      "grad_norm": 0.17939932644367218,
      "learning_rate": 4.5608333333333335e-05,
      "loss": 0.0027,
      "step": 21080
    },
    {
      "epoch": 0.703,
      "grad_norm": 0.14960671961307526,
      "learning_rate": 4.560625e-05,
      "loss": 0.0036,
      "step": 21090
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.32199162244796753,
      "learning_rate": 4.560416666666667e-05,
      "loss": 0.0023,
      "step": 21100
    },
    {
      "epoch": 0.7036666666666667,
      "grad_norm": 0.06154355779290199,
      "learning_rate": 4.560208333333333e-05,
      "loss": 0.0019,
      "step": 21110
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.0933447852730751,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0031,
      "step": 21120
    },
    {
      "epoch": 0.7043333333333334,
      "grad_norm": 0.5903558731079102,
      "learning_rate": 4.559791666666667e-05,
      "loss": 0.003,
      "step": 21130
    },
    {
      "epoch": 0.7046666666666667,
      "grad_norm": 0.43183037638664246,
      "learning_rate": 4.5595833333333335e-05,
      "loss": 0.0042,
      "step": 21140
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.2688797414302826,
      "learning_rate": 4.559375e-05,
      "loss": 0.0028,
      "step": 21150
    },
    {
      "epoch": 0.7053333333333334,
      "grad_norm": 0.449539452791214,
      "learning_rate": 4.559166666666667e-05,
      "loss": 0.0027,
      "step": 21160
    },
    {
      "epoch": 0.7056666666666667,
      "grad_norm": 0.03334333747625351,
      "learning_rate": 4.558958333333334e-05,
      "loss": 0.003,
      "step": 21170
    },
    {
      "epoch": 0.706,
      "grad_norm": 0.14969636499881744,
      "learning_rate": 4.55875e-05,
      "loss": 0.0016,
      "step": 21180
    },
    {
      "epoch": 0.7063333333333334,
      "grad_norm": 0.12070170044898987,
      "learning_rate": 4.558541666666667e-05,
      "loss": 0.0022,
      "step": 21190
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.5378426909446716,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.0029,
      "step": 21200
    },
    {
      "epoch": 0.707,
      "grad_norm": 0.08993673324584961,
      "learning_rate": 4.558125e-05,
      "loss": 0.0029,
      "step": 21210
    },
    {
      "epoch": 0.7073333333333334,
      "grad_norm": 0.23938016593456268,
      "learning_rate": 4.5579166666666666e-05,
      "loss": 0.0028,
      "step": 21220
    },
    {
      "epoch": 0.7076666666666667,
      "grad_norm": 0.1196412667632103,
      "learning_rate": 4.557708333333334e-05,
      "loss": 0.0028,
      "step": 21230
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.7365654110908508,
      "learning_rate": 4.5575e-05,
      "loss": 0.0017,
      "step": 21240
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.20929475128650665,
      "learning_rate": 4.557291666666667e-05,
      "loss": 0.0023,
      "step": 21250
    },
    {
      "epoch": 0.7086666666666667,
      "grad_norm": 0.34040290117263794,
      "learning_rate": 4.5570833333333334e-05,
      "loss": 0.0026,
      "step": 21260
    },
    {
      "epoch": 0.709,
      "grad_norm": 0.08982690423727036,
      "learning_rate": 4.5568750000000006e-05,
      "loss": 0.0028,
      "step": 21270
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.11982984095811844,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0025,
      "step": 21280
    },
    {
      "epoch": 0.7096666666666667,
      "grad_norm": 0.2690971791744232,
      "learning_rate": 4.556458333333334e-05,
      "loss": 0.0034,
      "step": 21290
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.11973526328802109,
      "learning_rate": 4.55625e-05,
      "loss": 0.0026,
      "step": 21300
    },
    {
      "epoch": 0.7103333333333334,
      "grad_norm": 0.06080733984708786,
      "learning_rate": 4.556041666666667e-05,
      "loss": 0.0018,
      "step": 21310
    },
    {
      "epoch": 0.7106666666666667,
      "grad_norm": 0.29838258028030396,
      "learning_rate": 4.5558333333333334e-05,
      "loss": 0.0022,
      "step": 21320
    },
    {
      "epoch": 0.711,
      "grad_norm": 0.3288983702659607,
      "learning_rate": 4.555625e-05,
      "loss": 0.0025,
      "step": 21330
    },
    {
      "epoch": 0.7113333333333334,
      "grad_norm": 0.41737666726112366,
      "learning_rate": 4.555416666666667e-05,
      "loss": 0.0029,
      "step": 21340
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.47780469059944153,
      "learning_rate": 4.555208333333334e-05,
      "loss": 0.0036,
      "step": 21350
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.2204810231924057,
      "learning_rate": 4.555e-05,
      "loss": 0.0027,
      "step": 21360
    },
    {
      "epoch": 0.7123333333333334,
      "grad_norm": 0.29850518703460693,
      "learning_rate": 4.554791666666667e-05,
      "loss": 0.0035,
      "step": 21370
    },
    {
      "epoch": 0.7126666666666667,
      "grad_norm": 0.14989040791988373,
      "learning_rate": 4.554583333333334e-05,
      "loss": 0.003,
      "step": 21380
    },
    {
      "epoch": 0.713,
      "grad_norm": 0.03068353980779648,
      "learning_rate": 4.554375e-05,
      "loss": 0.0028,
      "step": 21390
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.2695399224758148,
      "learning_rate": 4.554166666666667e-05,
      "loss": 0.0044,
      "step": 21400
    },
    {
      "epoch": 0.7136666666666667,
      "grad_norm": 0.03089022822678089,
      "learning_rate": 4.553958333333334e-05,
      "loss": 0.0034,
      "step": 21410
    },
    {
      "epoch": 0.714,
      "grad_norm": 0.2987537682056427,
      "learning_rate": 4.55375e-05,
      "loss": 0.003,
      "step": 21420
    },
    {
      "epoch": 0.7143333333333334,
      "grad_norm": 0.1789478361606598,
      "learning_rate": 4.553541666666667e-05,
      "loss": 0.0027,
      "step": 21430
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.41798582673072815,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0032,
      "step": 21440
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.5080793499946594,
      "learning_rate": 4.5531250000000006e-05,
      "loss": 0.0021,
      "step": 21450
    },
    {
      "epoch": 0.7153333333333334,
      "grad_norm": 0.00406506797298789,
      "learning_rate": 4.5529166666666664e-05,
      "loss": 0.003,
      "step": 21460
    },
    {
      "epoch": 0.7156666666666667,
      "grad_norm": 0.030476048588752747,
      "learning_rate": 4.5527083333333337e-05,
      "loss": 0.0029,
      "step": 21470
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.6985876560211182,
      "learning_rate": 4.5525e-05,
      "loss": 0.0021,
      "step": 21480
    },
    {
      "epoch": 0.7163333333333334,
      "grad_norm": 0.4776439368724823,
      "learning_rate": 4.552291666666667e-05,
      "loss": 0.0028,
      "step": 21490
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.23934581875801086,
      "learning_rate": 4.552083333333333e-05,
      "loss": 0.0038,
      "step": 21500
    },
    {
      "epoch": 0.717,
      "grad_norm": 0.03170611709356308,
      "learning_rate": 4.5518750000000005e-05,
      "loss": 0.0022,
      "step": 21510
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.419577032327652,
      "learning_rate": 4.551666666666667e-05,
      "loss": 0.0036,
      "step": 21520
    },
    {
      "epoch": 0.7176666666666667,
      "grad_norm": 0.11985835433006287,
      "learning_rate": 4.5514583333333336e-05,
      "loss": 0.0032,
      "step": 21530
    },
    {
      "epoch": 0.718,
      "grad_norm": 0.18027715384960175,
      "learning_rate": 4.55125e-05,
      "loss": 0.0026,
      "step": 21540
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.3883063793182373,
      "learning_rate": 4.551041666666667e-05,
      "loss": 0.0033,
      "step": 21550
    },
    {
      "epoch": 0.7186666666666667,
      "grad_norm": 0.3282179534435272,
      "learning_rate": 4.550833333333334e-05,
      "loss": 0.0026,
      "step": 21560
    },
    {
      "epoch": 0.719,
      "grad_norm": 0.14902375638484955,
      "learning_rate": 4.550625e-05,
      "loss": 0.0026,
      "step": 21570
    },
    {
      "epoch": 0.7193333333333334,
      "grad_norm": 0.08974101394414902,
      "learning_rate": 4.550416666666667e-05,
      "loss": 0.0045,
      "step": 21580
    },
    {
      "epoch": 0.7196666666666667,
      "grad_norm": 0.35814476013183594,
      "learning_rate": 4.5502083333333336e-05,
      "loss": 0.0015,
      "step": 21590
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.0897471159696579,
      "learning_rate": 4.55e-05,
      "loss": 0.0033,
      "step": 21600
    },
    {
      "epoch": 0.7203333333333334,
      "grad_norm": 0.17905092239379883,
      "learning_rate": 4.549791666666667e-05,
      "loss": 0.0025,
      "step": 21610
    },
    {
      "epoch": 0.7206666666666667,
      "grad_norm": 0.2392817586660385,
      "learning_rate": 4.549583333333334e-05,
      "loss": 0.0022,
      "step": 21620
    },
    {
      "epoch": 0.721,
      "grad_norm": 0.119589664041996,
      "learning_rate": 4.5493750000000005e-05,
      "loss": 0.002,
      "step": 21630
    },
    {
      "epoch": 0.7213333333333334,
      "grad_norm": 0.5671336054801941,
      "learning_rate": 4.549166666666667e-05,
      "loss": 0.0028,
      "step": 21640
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.32865023612976074,
      "learning_rate": 4.5489583333333336e-05,
      "loss": 0.0029,
      "step": 21650
    },
    {
      "epoch": 0.722,
      "grad_norm": 0.25316983461380005,
      "learning_rate": 4.54875e-05,
      "loss": 0.0036,
      "step": 21660
    },
    {
      "epoch": 0.7223333333333334,
      "grad_norm": 0.3582337200641632,
      "learning_rate": 4.548541666666667e-05,
      "loss": 0.0029,
      "step": 21670
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.3290015757083893,
      "learning_rate": 4.548333333333333e-05,
      "loss": 0.0039,
      "step": 21680
    },
    {
      "epoch": 0.723,
      "grad_norm": 0.06272640824317932,
      "learning_rate": 4.5481250000000004e-05,
      "loss": 0.003,
      "step": 21690
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.3588319420814514,
      "learning_rate": 4.547916666666667e-05,
      "loss": 0.0034,
      "step": 21700
    },
    {
      "epoch": 0.7236666666666667,
      "grad_norm": 0.06052858754992485,
      "learning_rate": 4.5477083333333335e-05,
      "loss": 0.0031,
      "step": 21710
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.057054974138736725,
      "learning_rate": 4.5475e-05,
      "loss": 0.0016,
      "step": 21720
    },
    {
      "epoch": 0.7243333333333334,
      "grad_norm": 0.060553595423698425,
      "learning_rate": 4.547291666666667e-05,
      "loss": 0.0025,
      "step": 21730
    },
    {
      "epoch": 0.7246666666666667,
      "grad_norm": 0.0060275583527982235,
      "learning_rate": 4.547083333333333e-05,
      "loss": 0.0034,
      "step": 21740
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.1497223824262619,
      "learning_rate": 4.5468750000000004e-05,
      "loss": 0.0014,
      "step": 21750
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.008412239141762257,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0028,
      "step": 21760
    },
    {
      "epoch": 0.7256666666666667,
      "grad_norm": 0.14935502409934998,
      "learning_rate": 4.5464583333333335e-05,
      "loss": 0.0023,
      "step": 21770
    },
    {
      "epoch": 0.726,
      "grad_norm": 0.38738033175468445,
      "learning_rate": 4.54625e-05,
      "loss": 0.0029,
      "step": 21780
    },
    {
      "epoch": 0.7263333333333334,
      "grad_norm": 0.35847654938697815,
      "learning_rate": 4.5460416666666666e-05,
      "loss": 0.0031,
      "step": 21790
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.6268734931945801,
      "learning_rate": 4.545833333333334e-05,
      "loss": 0.0035,
      "step": 21800
    },
    {
      "epoch": 0.727,
      "grad_norm": 0.06008007004857063,
      "learning_rate": 4.545625e-05,
      "loss": 0.0026,
      "step": 21810
    },
    {
      "epoch": 0.7273333333333334,
      "grad_norm": 0.11959506571292877,
      "learning_rate": 4.545416666666667e-05,
      "loss": 0.0029,
      "step": 21820
    },
    {
      "epoch": 0.7276666666666667,
      "grad_norm": 0.19991149008274078,
      "learning_rate": 4.5452083333333335e-05,
      "loss": 0.003,
      "step": 21830
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.2696358263492584,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.0036,
      "step": 21840
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.11967801302671432,
      "learning_rate": 4.5447916666666666e-05,
      "loss": 0.0029,
      "step": 21850
    },
    {
      "epoch": 0.7286666666666667,
      "grad_norm": 0.26872286200523376,
      "learning_rate": 4.544583333333334e-05,
      "loss": 0.0039,
      "step": 21860
    },
    {
      "epoch": 0.729,
      "grad_norm": 0.06119387224316597,
      "learning_rate": 4.5443750000000003e-05,
      "loss": 0.0033,
      "step": 21870
    },
    {
      "epoch": 0.7293333333333333,
      "grad_norm": 0.061717599630355835,
      "learning_rate": 4.544166666666667e-05,
      "loss": 0.0032,
      "step": 21880
    },
    {
      "epoch": 0.7296666666666667,
      "grad_norm": 0.4476465582847595,
      "learning_rate": 4.5439583333333334e-05,
      "loss": 0.0026,
      "step": 21890
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.2658906579017639,
      "learning_rate": 4.54375e-05,
      "loss": 0.0023,
      "step": 21900
    },
    {
      "epoch": 0.7303333333333333,
      "grad_norm": 0.20907846093177795,
      "learning_rate": 4.543541666666667e-05,
      "loss": 0.0031,
      "step": 21910
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.26872527599334717,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.0022,
      "step": 21920
    },
    {
      "epoch": 0.731,
      "grad_norm": 0.11965680867433548,
      "learning_rate": 4.543125e-05,
      "loss": 0.0037,
      "step": 21930
    },
    {
      "epoch": 0.7313333333333333,
      "grad_norm": 0.11964147537946701,
      "learning_rate": 4.542916666666667e-05,
      "loss": 0.0032,
      "step": 21940
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.20937766134738922,
      "learning_rate": 4.5427083333333334e-05,
      "loss": 0.0022,
      "step": 21950
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.08987890183925629,
      "learning_rate": 4.5425e-05,
      "loss": 0.0037,
      "step": 21960
    },
    {
      "epoch": 0.7323333333333333,
      "grad_norm": 0.4181962311267853,
      "learning_rate": 4.542291666666667e-05,
      "loss": 0.0027,
      "step": 21970
    },
    {
      "epoch": 0.7326666666666667,
      "grad_norm": 0.09011649340391159,
      "learning_rate": 4.542083333333334e-05,
      "loss": 0.0027,
      "step": 21980
    },
    {
      "epoch": 0.733,
      "grad_norm": 0.08961924910545349,
      "learning_rate": 4.541875e-05,
      "loss": 0.0033,
      "step": 21990
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.537580132484436,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.0029,
      "step": 22000
    },
    {
      "epoch": 0.7336666666666667,
      "grad_norm": 0.11939690262079239,
      "learning_rate": 4.541458333333334e-05,
      "loss": 0.0028,
      "step": 22010
    },
    {
      "epoch": 0.734,
      "grad_norm": 0.23884320259094238,
      "learning_rate": 4.54125e-05,
      "loss": 0.0034,
      "step": 22020
    },
    {
      "epoch": 0.7343333333333333,
      "grad_norm": 0.26911720633506775,
      "learning_rate": 4.5410416666666665e-05,
      "loss": 0.0036,
      "step": 22030
    },
    {
      "epoch": 0.7346666666666667,
      "grad_norm": 0.24925203621387482,
      "learning_rate": 4.540833333333334e-05,
      "loss": 0.0031,
      "step": 22040
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.09160114079713821,
      "learning_rate": 4.540625e-05,
      "loss": 0.0025,
      "step": 22050
    },
    {
      "epoch": 0.7353333333333333,
      "grad_norm": 0.09005717188119888,
      "learning_rate": 4.540416666666667e-05,
      "loss": 0.0033,
      "step": 22060
    },
    {
      "epoch": 0.7356666666666667,
      "grad_norm": 0.5074405670166016,
      "learning_rate": 4.5402083333333334e-05,
      "loss": 0.003,
      "step": 22070
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.06033144146203995,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0019,
      "step": 22080
    },
    {
      "epoch": 0.7363333333333333,
      "grad_norm": 0.4776971638202667,
      "learning_rate": 4.5397916666666664e-05,
      "loss": 0.005,
      "step": 22090
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.06077529489994049,
      "learning_rate": 4.539583333333334e-05,
      "loss": 0.0023,
      "step": 22100
    },
    {
      "epoch": 0.737,
      "grad_norm": 0.03150161728262901,
      "learning_rate": 4.539375e-05,
      "loss": 0.0037,
      "step": 22110
    },
    {
      "epoch": 0.7373333333333333,
      "grad_norm": 0.1193881705403328,
      "learning_rate": 4.5391666666666675e-05,
      "loss": 0.0031,
      "step": 22120
    },
    {
      "epoch": 0.7376666666666667,
      "grad_norm": 0.2678595185279846,
      "learning_rate": 4.538958333333333e-05,
      "loss": 0.0035,
      "step": 22130
    },
    {
      "epoch": 0.738,
      "grad_norm": 0.32827821373939514,
      "learning_rate": 4.53875e-05,
      "loss": 0.0028,
      "step": 22140
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.6377758979797363,
      "learning_rate": 4.538541666666667e-05,
      "loss": 0.0031,
      "step": 22150
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.09021806716918945,
      "learning_rate": 4.5383333333333336e-05,
      "loss": 0.0023,
      "step": 22160
    },
    {
      "epoch": 0.739,
      "grad_norm": 0.2386288195848465,
      "learning_rate": 4.538125e-05,
      "loss": 0.0037,
      "step": 22170
    },
    {
      "epoch": 0.7393333333333333,
      "grad_norm": 0.18070141971111298,
      "learning_rate": 4.537916666666667e-05,
      "loss": 0.0023,
      "step": 22180
    },
    {
      "epoch": 0.7396666666666667,
      "grad_norm": 0.06025928258895874,
      "learning_rate": 4.537708333333334e-05,
      "loss": 0.0039,
      "step": 22190
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.059968817979097366,
      "learning_rate": 4.5375e-05,
      "loss": 0.0033,
      "step": 22200
    },
    {
      "epoch": 0.7403333333333333,
      "grad_norm": 0.20860698819160461,
      "learning_rate": 4.537291666666667e-05,
      "loss": 0.0028,
      "step": 22210
    },
    {
      "epoch": 0.7406666666666667,
      "grad_norm": 0.32818686962127686,
      "learning_rate": 4.5370833333333336e-05,
      "loss": 0.003,
      "step": 22220
    },
    {
      "epoch": 0.741,
      "grad_norm": 0.7748333811759949,
      "learning_rate": 4.536875e-05,
      "loss": 0.0031,
      "step": 22230
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.26792362332344055,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.0038,
      "step": 22240
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.05985403060913086,
      "learning_rate": 4.536458333333334e-05,
      "loss": 0.0041,
      "step": 22250
    },
    {
      "epoch": 0.742,
      "grad_norm": 0.05997953936457634,
      "learning_rate": 4.5362500000000005e-05,
      "loss": 0.0021,
      "step": 22260
    },
    {
      "epoch": 0.7423333333333333,
      "grad_norm": 0.3277897834777832,
      "learning_rate": 4.5360416666666664e-05,
      "loss": 0.0021,
      "step": 22270
    },
    {
      "epoch": 0.7426666666666667,
      "grad_norm": 0.2982047498226166,
      "learning_rate": 4.5358333333333336e-05,
      "loss": 0.0036,
      "step": 22280
    },
    {
      "epoch": 0.743,
      "grad_norm": 0.6469497084617615,
      "learning_rate": 4.535625e-05,
      "loss": 0.0034,
      "step": 22290
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.007942596450448036,
      "learning_rate": 4.535416666666667e-05,
      "loss": 0.002,
      "step": 22300
    },
    {
      "epoch": 0.7436666666666667,
      "grad_norm": 0.38813623785972595,
      "learning_rate": 4.535208333333333e-05,
      "loss": 0.0024,
      "step": 22310
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.5525224208831787,
      "learning_rate": 4.5350000000000005e-05,
      "loss": 0.0045,
      "step": 22320
    },
    {
      "epoch": 0.7443333333333333,
      "grad_norm": 0.16632211208343506,
      "learning_rate": 4.534791666666667e-05,
      "loss": 0.002,
      "step": 22330
    },
    {
      "epoch": 0.7446666666666667,
      "grad_norm": 0.061708275228738785,
      "learning_rate": 4.5345833333333336e-05,
      "loss": 0.0023,
      "step": 22340
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.0603991262614727,
      "learning_rate": 4.534375e-05,
      "loss": 0.0042,
      "step": 22350
    },
    {
      "epoch": 0.7453333333333333,
      "grad_norm": 0.06092376261949539,
      "learning_rate": 4.534166666666667e-05,
      "loss": 0.0028,
      "step": 22360
    },
    {
      "epoch": 0.7456666666666667,
      "grad_norm": 0.29981914162635803,
      "learning_rate": 4.533958333333333e-05,
      "loss": 0.0027,
      "step": 22370
    },
    {
      "epoch": 0.746,
      "grad_norm": 0.4195007085800171,
      "learning_rate": 4.53375e-05,
      "loss": 0.0037,
      "step": 22380
    },
    {
      "epoch": 0.7463333333333333,
      "grad_norm": 0.025436460971832275,
      "learning_rate": 4.533541666666667e-05,
      "loss": 0.0034,
      "step": 22390
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.1496257781982422,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0023,
      "step": 22400
    },
    {
      "epoch": 0.747,
      "grad_norm": 0.23821793496608734,
      "learning_rate": 4.533125e-05,
      "loss": 0.0032,
      "step": 22410
    },
    {
      "epoch": 0.7473333333333333,
      "grad_norm": 0.35785430669784546,
      "learning_rate": 4.5329166666666666e-05,
      "loss": 0.0033,
      "step": 22420
    },
    {
      "epoch": 0.7476666666666667,
      "grad_norm": 0.23875601589679718,
      "learning_rate": 4.532708333333334e-05,
      "loss": 0.0029,
      "step": 22430
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.5064153075218201,
      "learning_rate": 4.5325000000000004e-05,
      "loss": 0.0035,
      "step": 22440
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.09000970423221588,
      "learning_rate": 4.532291666666667e-05,
      "loss": 0.0026,
      "step": 22450
    },
    {
      "epoch": 0.7486666666666667,
      "grad_norm": 0.09058365225791931,
      "learning_rate": 4.5320833333333335e-05,
      "loss": 0.0047,
      "step": 22460
    },
    {
      "epoch": 0.749,
      "grad_norm": 0.059709541499614716,
      "learning_rate": 4.531875000000001e-05,
      "loss": 0.0034,
      "step": 22470
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.20872682332992554,
      "learning_rate": 4.5316666666666666e-05,
      "loss": 0.0028,
      "step": 22480
    },
    {
      "epoch": 0.7496666666666667,
      "grad_norm": 0.059879716485738754,
      "learning_rate": 4.531458333333334e-05,
      "loss": 0.0018,
      "step": 22490
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.6851561069488525,
      "learning_rate": 4.5312500000000004e-05,
      "loss": 0.0033,
      "step": 22500
    },
    {
      "epoch": 0.7503333333333333,
      "grad_norm": 0.030111163854599,
      "learning_rate": 4.531041666666667e-05,
      "loss": 0.0022,
      "step": 22510
    },
    {
      "epoch": 0.7506666666666667,
      "grad_norm": 0.009568324312567711,
      "learning_rate": 4.5308333333333335e-05,
      "loss": 0.0017,
      "step": 22520
    },
    {
      "epoch": 0.751,
      "grad_norm": 0.060890134423971176,
      "learning_rate": 4.530625e-05,
      "loss": 0.0039,
      "step": 22530
    },
    {
      "epoch": 0.7513333333333333,
      "grad_norm": 0.11978401988744736,
      "learning_rate": 4.530416666666667e-05,
      "loss": 0.0033,
      "step": 22540
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.06046367064118385,
      "learning_rate": 4.530208333333333e-05,
      "loss": 0.004,
      "step": 22550
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.6267116069793701,
      "learning_rate": 4.53e-05,
      "loss": 0.0027,
      "step": 22560
    },
    {
      "epoch": 0.7523333333333333,
      "grad_norm": 0.2684524655342102,
      "learning_rate": 4.529791666666667e-05,
      "loss": 0.0027,
      "step": 22570
    },
    {
      "epoch": 0.7526666666666667,
      "grad_norm": 0.030379626899957657,
      "learning_rate": 4.5295833333333334e-05,
      "loss": 0.003,
      "step": 22580
    },
    {
      "epoch": 0.753,
      "grad_norm": 0.012249105609953403,
      "learning_rate": 4.529375e-05,
      "loss": 0.0025,
      "step": 22590
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.5572522282600403,
      "learning_rate": 4.529166666666667e-05,
      "loss": 0.0053,
      "step": 22600
    },
    {
      "epoch": 0.7536666666666667,
      "grad_norm": 0.061152588576078415,
      "learning_rate": 4.528958333333334e-05,
      "loss": 0.0034,
      "step": 22610
    },
    {
      "epoch": 0.754,
      "grad_norm": 0.41772907972335815,
      "learning_rate": 4.52875e-05,
      "loss": 0.0032,
      "step": 22620
    },
    {
      "epoch": 0.7543333333333333,
      "grad_norm": 0.11988087743520737,
      "learning_rate": 4.528541666666667e-05,
      "loss": 0.0026,
      "step": 22630
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.268823504447937,
      "learning_rate": 4.5283333333333334e-05,
      "loss": 0.0031,
      "step": 22640
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.08973968029022217,
      "learning_rate": 4.528125e-05,
      "loss": 0.002,
      "step": 22650
    },
    {
      "epoch": 0.7553333333333333,
      "grad_norm": 0.32797855138778687,
      "learning_rate": 4.5279166666666665e-05,
      "loss": 0.0038,
      "step": 22660
    },
    {
      "epoch": 0.7556666666666667,
      "grad_norm": 0.08987704664468765,
      "learning_rate": 4.527708333333334e-05,
      "loss": 0.0031,
      "step": 22670
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.4779745936393738,
      "learning_rate": 4.5275e-05,
      "loss": 0.004,
      "step": 22680
    },
    {
      "epoch": 0.7563333333333333,
      "grad_norm": 0.5089967846870422,
      "learning_rate": 4.527291666666667e-05,
      "loss": 0.0032,
      "step": 22690
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.5174350738525391,
      "learning_rate": 4.5270833333333334e-05,
      "loss": 0.0036,
      "step": 22700
    },
    {
      "epoch": 0.757,
      "grad_norm": 0.2988109588623047,
      "learning_rate": 4.5268750000000006e-05,
      "loss": 0.0034,
      "step": 22710
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.26839956641197205,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0039,
      "step": 22720
    },
    {
      "epoch": 0.7576666666666667,
      "grad_norm": 0.2089931219816208,
      "learning_rate": 4.526458333333334e-05,
      "loss": 0.0031,
      "step": 22730
    },
    {
      "epoch": 0.758,
      "grad_norm": 0.388121634721756,
      "learning_rate": 4.52625e-05,
      "loss": 0.0031,
      "step": 22740
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.030352022498846054,
      "learning_rate": 4.526041666666667e-05,
      "loss": 0.0026,
      "step": 22750
    },
    {
      "epoch": 0.7586666666666667,
      "grad_norm": 0.5303334593772888,
      "learning_rate": 4.5258333333333333e-05,
      "loss": 0.0024,
      "step": 22760
    },
    {
      "epoch": 0.759,
      "grad_norm": 0.41806983947753906,
      "learning_rate": 4.525625e-05,
      "loss": 0.0033,
      "step": 22770
    },
    {
      "epoch": 0.7593333333333333,
      "grad_norm": 0.06008051708340645,
      "learning_rate": 4.525416666666667e-05,
      "loss": 0.004,
      "step": 22780
    },
    {
      "epoch": 0.7596666666666667,
      "grad_norm": 0.32797250151634216,
      "learning_rate": 4.525208333333334e-05,
      "loss": 0.003,
      "step": 22790
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.06047169491648674,
      "learning_rate": 4.525e-05,
      "loss": 0.0035,
      "step": 22800
    },
    {
      "epoch": 0.7603333333333333,
      "grad_norm": 0.17888973653316498,
      "learning_rate": 4.524791666666667e-05,
      "loss": 0.0036,
      "step": 22810
    },
    {
      "epoch": 0.7606666666666667,
      "grad_norm": 0.09020362049341202,
      "learning_rate": 4.524583333333334e-05,
      "loss": 0.0017,
      "step": 22820
    },
    {
      "epoch": 0.761,
      "grad_norm": 0.35812583565711975,
      "learning_rate": 4.524375e-05,
      "loss": 0.0025,
      "step": 22830
    },
    {
      "epoch": 0.7613333333333333,
      "grad_norm": 0.060024846345186234,
      "learning_rate": 4.524166666666667e-05,
      "loss": 0.0031,
      "step": 22840
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.42805564403533936,
      "learning_rate": 4.5239583333333336e-05,
      "loss": 0.0031,
      "step": 22850
    },
    {
      "epoch": 0.762,
      "grad_norm": 0.047722227871418,
      "learning_rate": 4.52375e-05,
      "loss": 0.002,
      "step": 22860
    },
    {
      "epoch": 0.7623333333333333,
      "grad_norm": 0.17904223501682281,
      "learning_rate": 4.523541666666667e-05,
      "loss": 0.0028,
      "step": 22870
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.3282134234905243,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.0036,
      "step": 22880
    },
    {
      "epoch": 0.763,
      "grad_norm": 0.08952169120311737,
      "learning_rate": 4.5231250000000005e-05,
      "loss": 0.0028,
      "step": 22890
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.01263437233865261,
      "learning_rate": 4.5229166666666664e-05,
      "loss": 0.0031,
      "step": 22900
    },
    {
      "epoch": 0.7636666666666667,
      "grad_norm": 0.18169057369232178,
      "learning_rate": 4.5227083333333336e-05,
      "loss": 0.0029,
      "step": 22910
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.12020636349916458,
      "learning_rate": 4.5225e-05,
      "loss": 0.0032,
      "step": 22920
    },
    {
      "epoch": 0.7643333333333333,
      "grad_norm": 0.03085731714963913,
      "learning_rate": 4.522291666666667e-05,
      "loss": 0.0039,
      "step": 22930
    },
    {
      "epoch": 0.7646666666666667,
      "grad_norm": 0.3645320236682892,
      "learning_rate": 4.522083333333333e-05,
      "loss": 0.0039,
      "step": 22940
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.03106810711324215,
      "learning_rate": 4.5218750000000005e-05,
      "loss": 0.0016,
      "step": 22950
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.6261252760887146,
      "learning_rate": 4.521666666666667e-05,
      "loss": 0.0043,
      "step": 22960
    },
    {
      "epoch": 0.7656666666666667,
      "grad_norm": 0.14927583932876587,
      "learning_rate": 4.5214583333333336e-05,
      "loss": 0.0047,
      "step": 22970
    },
    {
      "epoch": 0.766,
      "grad_norm": 0.20908160507678986,
      "learning_rate": 4.52125e-05,
      "loss": 0.0025,
      "step": 22980
    },
    {
      "epoch": 0.7663333333333333,
      "grad_norm": 0.1197354644536972,
      "learning_rate": 4.521041666666667e-05,
      "loss": 0.0035,
      "step": 22990
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.567497730255127,
      "learning_rate": 4.520833333333334e-05,
      "loss": 0.003,
      "step": 23000
    },
    {
      "epoch": 0.767,
      "grad_norm": 0.29826948046684265,
      "learning_rate": 4.520625e-05,
      "loss": 0.0022,
      "step": 23010
    },
    {
      "epoch": 0.7673333333333333,
      "grad_norm": 0.07559014111757278,
      "learning_rate": 4.520416666666667e-05,
      "loss": 0.0027,
      "step": 23020
    },
    {
      "epoch": 0.7676666666666667,
      "grad_norm": 0.1794220507144928,
      "learning_rate": 4.5202083333333335e-05,
      "loss": 0.0031,
      "step": 23030
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.14904122054576874,
      "learning_rate": 4.52e-05,
      "loss": 0.0031,
      "step": 23040
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.3872394263744354,
      "learning_rate": 4.5197916666666666e-05,
      "loss": 0.0042,
      "step": 23050
    },
    {
      "epoch": 0.7686666666666667,
      "grad_norm": 0.2986544668674469,
      "learning_rate": 4.519583333333334e-05,
      "loss": 0.0027,
      "step": 23060
    },
    {
      "epoch": 0.769,
      "grad_norm": 0.1491672843694687,
      "learning_rate": 4.5193750000000004e-05,
      "loss": 0.0035,
      "step": 23070
    },
    {
      "epoch": 0.7693333333333333,
      "grad_norm": 0.49404528737068176,
      "learning_rate": 4.519166666666667e-05,
      "loss": 0.0031,
      "step": 23080
    },
    {
      "epoch": 0.7696666666666667,
      "grad_norm": 0.2652386724948883,
      "learning_rate": 4.5189583333333335e-05,
      "loss": 0.0026,
      "step": 23090
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.41747817397117615,
      "learning_rate": 4.518750000000001e-05,
      "loss": 0.0048,
      "step": 23100
    },
    {
      "epoch": 0.7703333333333333,
      "grad_norm": 0.4479479193687439,
      "learning_rate": 4.5185416666666666e-05,
      "loss": 0.002,
      "step": 23110
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.32836198806762695,
      "learning_rate": 4.518333333333333e-05,
      "loss": 0.0024,
      "step": 23120
    },
    {
      "epoch": 0.771,
      "grad_norm": 0.030587226152420044,
      "learning_rate": 4.5181250000000004e-05,
      "loss": 0.0039,
      "step": 23130
    },
    {
      "epoch": 0.7713333333333333,
      "grad_norm": 0.05996643006801605,
      "learning_rate": 4.517916666666667e-05,
      "loss": 0.0023,
      "step": 23140
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.14896278083324432,
      "learning_rate": 4.5177083333333335e-05,
      "loss": 0.0026,
      "step": 23150
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.08401743322610855,
      "learning_rate": 4.5175e-05,
      "loss": 0.0018,
      "step": 23160
    },
    {
      "epoch": 0.7723333333333333,
      "grad_norm": 0.03127789869904518,
      "learning_rate": 4.517291666666667e-05,
      "loss": 0.0027,
      "step": 23170
    },
    {
      "epoch": 0.7726666666666666,
      "grad_norm": 0.4938458502292633,
      "learning_rate": 4.517083333333333e-05,
      "loss": 0.0029,
      "step": 23180
    },
    {
      "epoch": 0.773,
      "grad_norm": 0.29996225237846375,
      "learning_rate": 4.5168750000000004e-05,
      "loss": 0.0034,
      "step": 23190
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.03936167433857918,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0021,
      "step": 23200
    },
    {
      "epoch": 0.7736666666666666,
      "grad_norm": 0.2681778073310852,
      "learning_rate": 4.516458333333334e-05,
      "loss": 0.0034,
      "step": 23210
    },
    {
      "epoch": 0.774,
      "grad_norm": 0.2681865096092224,
      "learning_rate": 4.51625e-05,
      "loss": 0.0023,
      "step": 23220
    },
    {
      "epoch": 0.7743333333333333,
      "grad_norm": 0.14904052019119263,
      "learning_rate": 4.5160416666666665e-05,
      "loss": 0.0032,
      "step": 23230
    },
    {
      "epoch": 0.7746666666666666,
      "grad_norm": 2.098473310470581,
      "learning_rate": 4.515833333333334e-05,
      "loss": 0.0045,
      "step": 23240
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.23805290460586548,
      "learning_rate": 4.515625e-05,
      "loss": 0.0029,
      "step": 23250
    },
    {
      "epoch": 0.7753333333333333,
      "grad_norm": 0.3065524101257324,
      "learning_rate": 4.515416666666667e-05,
      "loss": 0.0023,
      "step": 23260
    },
    {
      "epoch": 0.7756666666666666,
      "grad_norm": 0.14930939674377441,
      "learning_rate": 4.5152083333333334e-05,
      "loss": 0.0021,
      "step": 23270
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.20845945179462433,
      "learning_rate": 4.5150000000000006e-05,
      "loss": 0.0038,
      "step": 23280
    },
    {
      "epoch": 0.7763333333333333,
      "grad_norm": 0.29922568798065186,
      "learning_rate": 4.5147916666666665e-05,
      "loss": 0.003,
      "step": 23290
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.298383891582489,
      "learning_rate": 4.514583333333334e-05,
      "loss": 0.0026,
      "step": 23300
    },
    {
      "epoch": 0.777,
      "grad_norm": 0.2981826364994049,
      "learning_rate": 4.514375e-05,
      "loss": 0.0039,
      "step": 23310
    },
    {
      "epoch": 0.7773333333333333,
      "grad_norm": 0.12903204560279846,
      "learning_rate": 4.514166666666667e-05,
      "loss": 0.0033,
      "step": 23320
    },
    {
      "epoch": 0.7776666666666666,
      "grad_norm": 0.41723841428756714,
      "learning_rate": 4.5139583333333334e-05,
      "loss": 0.0024,
      "step": 23330
    },
    {
      "epoch": 0.778,
      "grad_norm": 0.23968945443630219,
      "learning_rate": 4.5137500000000006e-05,
      "loss": 0.003,
      "step": 23340
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.3279862105846405,
      "learning_rate": 4.513541666666667e-05,
      "loss": 0.0022,
      "step": 23350
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.1493290811777115,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0035,
      "step": 23360
    },
    {
      "epoch": 0.779,
      "grad_norm": 0.3052768111228943,
      "learning_rate": 4.513125e-05,
      "loss": 0.0042,
      "step": 23370
    },
    {
      "epoch": 0.7793333333333333,
      "grad_norm": 0.2680860459804535,
      "learning_rate": 4.512916666666667e-05,
      "loss": 0.0026,
      "step": 23380
    },
    {
      "epoch": 0.7796666666666666,
      "grad_norm": 0.00588682247325778,
      "learning_rate": 4.5127083333333334e-05,
      "loss": 0.0022,
      "step": 23390
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.08930175006389618,
      "learning_rate": 4.5125e-05,
      "loss": 0.0029,
      "step": 23400
    },
    {
      "epoch": 0.7803333333333333,
      "grad_norm": 0.11938212811946869,
      "learning_rate": 4.512291666666667e-05,
      "loss": 0.003,
      "step": 23410
    },
    {
      "epoch": 0.7806666666666666,
      "grad_norm": 0.19277949631214142,
      "learning_rate": 4.512083333333334e-05,
      "loss": 0.0034,
      "step": 23420
    },
    {
      "epoch": 0.781,
      "grad_norm": 0.44182804226875305,
      "learning_rate": 4.511875e-05,
      "loss": 0.0036,
      "step": 23430
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.03101622685790062,
      "learning_rate": 4.511666666666667e-05,
      "loss": 0.0038,
      "step": 23440
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.11879317462444305,
      "learning_rate": 4.511458333333334e-05,
      "loss": 0.0032,
      "step": 23450
    },
    {
      "epoch": 0.782,
      "grad_norm": 0.059989530593156815,
      "learning_rate": 4.51125e-05,
      "loss": 0.0024,
      "step": 23460
    },
    {
      "epoch": 0.7823333333333333,
      "grad_norm": 0.5963714718818665,
      "learning_rate": 4.5110416666666664e-05,
      "loss": 0.0038,
      "step": 23470
    },
    {
      "epoch": 0.7826666666666666,
      "grad_norm": 0.06006782874464989,
      "learning_rate": 4.5108333333333337e-05,
      "loss": 0.0033,
      "step": 23480
    },
    {
      "epoch": 0.783,
      "grad_norm": 0.14897677302360535,
      "learning_rate": 4.510625e-05,
      "loss": 0.0038,
      "step": 23490
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.1793188452720642,
      "learning_rate": 4.510416666666667e-05,
      "loss": 0.0024,
      "step": 23500
    },
    {
      "epoch": 0.7836666666666666,
      "grad_norm": 0.1198543831706047,
      "learning_rate": 4.510208333333333e-05,
      "loss": 0.0041,
      "step": 23510
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.17875628173351288,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0038,
      "step": 23520
    },
    {
      "epoch": 0.7843333333333333,
      "grad_norm": 0.05996735021471977,
      "learning_rate": 4.509791666666667e-05,
      "loss": 0.0033,
      "step": 23530
    },
    {
      "epoch": 0.7846666666666666,
      "grad_norm": 0.40298569202423096,
      "learning_rate": 4.5095833333333336e-05,
      "loss": 0.0035,
      "step": 23540
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.14912517368793488,
      "learning_rate": 4.509375e-05,
      "loss": 0.0043,
      "step": 23550
    },
    {
      "epoch": 0.7853333333333333,
      "grad_norm": 0.23817332088947296,
      "learning_rate": 4.5091666666666674e-05,
      "loss": 0.003,
      "step": 23560
    },
    {
      "epoch": 0.7856666666666666,
      "grad_norm": 0.2981690764427185,
      "learning_rate": 4.508958333333333e-05,
      "loss": 0.0032,
      "step": 23570
    },
    {
      "epoch": 0.786,
      "grad_norm": 0.23815341293811798,
      "learning_rate": 4.5087500000000005e-05,
      "loss": 0.0032,
      "step": 23580
    },
    {
      "epoch": 0.7863333333333333,
      "grad_norm": 0.12151170521974564,
      "learning_rate": 4.508541666666667e-05,
      "loss": 0.0016,
      "step": 23590
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.26824304461479187,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.0035,
      "step": 23600
    },
    {
      "epoch": 0.787,
      "grad_norm": 0.20893923938274384,
      "learning_rate": 4.508125e-05,
      "loss": 0.0034,
      "step": 23610
    },
    {
      "epoch": 0.7873333333333333,
      "grad_norm": 0.26880958676338196,
      "learning_rate": 4.507916666666667e-05,
      "loss": 0.0045,
      "step": 23620
    },
    {
      "epoch": 0.7876666666666666,
      "grad_norm": 0.08928439766168594,
      "learning_rate": 4.507708333333334e-05,
      "loss": 0.003,
      "step": 23630
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.020162224769592285,
      "learning_rate": 4.5075e-05,
      "loss": 0.0036,
      "step": 23640
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.5359072089195251,
      "learning_rate": 4.507291666666667e-05,
      "loss": 0.0024,
      "step": 23650
    },
    {
      "epoch": 0.7886666666666666,
      "grad_norm": 0.20882543921470642,
      "learning_rate": 4.5070833333333336e-05,
      "loss": 0.002,
      "step": 23660
    },
    {
      "epoch": 0.789,
      "grad_norm": 0.008211149834096432,
      "learning_rate": 4.506875e-05,
      "loss": 0.0018,
      "step": 23670
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.32836219668388367,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0033,
      "step": 23680
    },
    {
      "epoch": 0.7896666666666666,
      "grad_norm": 0.17849761247634888,
      "learning_rate": 4.506458333333334e-05,
      "loss": 0.0031,
      "step": 23690
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.5364281535148621,
      "learning_rate": 4.5062500000000004e-05,
      "loss": 0.0016,
      "step": 23700
    },
    {
      "epoch": 0.7903333333333333,
      "grad_norm": 0.2603888511657715,
      "learning_rate": 4.506041666666666e-05,
      "loss": 0.0031,
      "step": 23710
    },
    {
      "epoch": 0.7906666666666666,
      "grad_norm": 0.2688223123550415,
      "learning_rate": 4.5058333333333335e-05,
      "loss": 0.0026,
      "step": 23720
    },
    {
      "epoch": 0.791,
      "grad_norm": 0.17832373082637787,
      "learning_rate": 4.505625e-05,
      "loss": 0.0027,
      "step": 23730
    },
    {
      "epoch": 0.7913333333333333,
      "grad_norm": 0.46912527084350586,
      "learning_rate": 4.5054166666666666e-05,
      "loss": 0.0038,
      "step": 23740
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.23844358325004578,
      "learning_rate": 4.505208333333333e-05,
      "loss": 0.0024,
      "step": 23750
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.32813674211502075,
      "learning_rate": 4.5050000000000004e-05,
      "loss": 0.0025,
      "step": 23760
    },
    {
      "epoch": 0.7923333333333333,
      "grad_norm": 0.08947131782770157,
      "learning_rate": 4.504791666666667e-05,
      "loss": 0.0031,
      "step": 23770
    },
    {
      "epoch": 0.7926666666666666,
      "grad_norm": 0.2981576919555664,
      "learning_rate": 4.5045833333333335e-05,
      "loss": 0.0028,
      "step": 23780
    },
    {
      "epoch": 0.793,
      "grad_norm": 0.11920888721942902,
      "learning_rate": 4.504375e-05,
      "loss": 0.002,
      "step": 23790
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.011090925894677639,
      "learning_rate": 4.504166666666667e-05,
      "loss": 0.0024,
      "step": 23800
    },
    {
      "epoch": 0.7936666666666666,
      "grad_norm": 0.14917519688606262,
      "learning_rate": 4.503958333333334e-05,
      "loss": 0.002,
      "step": 23810
    },
    {
      "epoch": 0.794,
      "grad_norm": 0.3874810039997101,
      "learning_rate": 4.5037500000000004e-05,
      "loss": 0.0031,
      "step": 23820
    },
    {
      "epoch": 0.7943333333333333,
      "grad_norm": 0.031304407864809036,
      "learning_rate": 4.503541666666667e-05,
      "loss": 0.003,
      "step": 23830
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.1190354973077774,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0037,
      "step": 23840
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.30046844482421875,
      "learning_rate": 4.503125e-05,
      "loss": 0.003,
      "step": 23850
    },
    {
      "epoch": 0.7953333333333333,
      "grad_norm": 0.04832175746560097,
      "learning_rate": 4.5029166666666666e-05,
      "loss": 0.0042,
      "step": 23860
    },
    {
      "epoch": 0.7956666666666666,
      "grad_norm": 0.41754111647605896,
      "learning_rate": 4.502708333333334e-05,
      "loss": 0.0021,
      "step": 23870
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.20876698195934296,
      "learning_rate": 4.5025000000000003e-05,
      "loss": 0.0031,
      "step": 23880
    },
    {
      "epoch": 0.7963333333333333,
      "grad_norm": 0.42212778329849243,
      "learning_rate": 4.502291666666667e-05,
      "loss": 0.003,
      "step": 23890
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.030989335849881172,
      "learning_rate": 4.5020833333333334e-05,
      "loss": 0.0031,
      "step": 23900
    },
    {
      "epoch": 0.797,
      "grad_norm": 0.5960408449172974,
      "learning_rate": 4.501875000000001e-05,
      "loss": 0.0024,
      "step": 23910
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.06030634418129921,
      "learning_rate": 4.5016666666666665e-05,
      "loss": 0.0018,
      "step": 23920
    },
    {
      "epoch": 0.7976666666666666,
      "grad_norm": 0.6249336004257202,
      "learning_rate": 4.501458333333334e-05,
      "loss": 0.0019,
      "step": 23930
    },
    {
      "epoch": 0.798,
      "grad_norm": 0.6999245882034302,
      "learning_rate": 4.50125e-05,
      "loss": 0.0022,
      "step": 23940
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.1797557920217514,
      "learning_rate": 4.501041666666667e-05,
      "loss": 0.003,
      "step": 23950
    },
    {
      "epoch": 0.7986666666666666,
      "grad_norm": 0.26835745573043823,
      "learning_rate": 4.5008333333333334e-05,
      "loss": 0.0021,
      "step": 23960
    },
    {
      "epoch": 0.799,
      "grad_norm": 0.09033980220556259,
      "learning_rate": 4.500625e-05,
      "loss": 0.0037,
      "step": 23970
    },
    {
      "epoch": 0.7993333333333333,
      "grad_norm": 0.3094862401485443,
      "learning_rate": 4.500416666666667e-05,
      "loss": 0.0033,
      "step": 23980
    },
    {
      "epoch": 0.7996666666666666,
      "grad_norm": 0.26833075284957886,
      "learning_rate": 4.500208333333333e-05,
      "loss": 0.0021,
      "step": 23990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.41223108768463135,
      "learning_rate": 4.5e-05,
      "loss": 0.0026,
      "step": 24000
    },
    {
      "epoch": 0.8003333333333333,
      "grad_norm": 0.06043587252497673,
      "learning_rate": 4.499791666666667e-05,
      "loss": 0.0031,
      "step": 24010
    },
    {
      "epoch": 0.8006666666666666,
      "grad_norm": 0.11972439289093018,
      "learning_rate": 4.4995833333333334e-05,
      "loss": 0.0045,
      "step": 24020
    },
    {
      "epoch": 0.801,
      "grad_norm": 0.47656306624412537,
      "learning_rate": 4.499375e-05,
      "loss": 0.0047,
      "step": 24030
    },
    {
      "epoch": 0.8013333333333333,
      "grad_norm": 0.2382165491580963,
      "learning_rate": 4.499166666666667e-05,
      "loss": 0.0031,
      "step": 24040
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.5368900895118713,
      "learning_rate": 4.498958333333334e-05,
      "loss": 0.0029,
      "step": 24050
    },
    {
      "epoch": 0.802,
      "grad_norm": 0.059784095734357834,
      "learning_rate": 4.49875e-05,
      "loss": 0.0035,
      "step": 24060
    },
    {
      "epoch": 0.8023333333333333,
      "grad_norm": 0.506740391254425,
      "learning_rate": 4.498541666666667e-05,
      "loss": 0.0043,
      "step": 24070
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.41820281744003296,
      "learning_rate": 4.4983333333333334e-05,
      "loss": 0.0027,
      "step": 24080
    },
    {
      "epoch": 0.803,
      "grad_norm": 0.47787031531333923,
      "learning_rate": 4.4981250000000006e-05,
      "loss": 0.0047,
      "step": 24090
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.11901448667049408,
      "learning_rate": 4.4979166666666664e-05,
      "loss": 0.0033,
      "step": 24100
    },
    {
      "epoch": 0.8036666666666666,
      "grad_norm": 0.05996181443333626,
      "learning_rate": 4.497708333333334e-05,
      "loss": 0.0031,
      "step": 24110
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.47748658061027527,
      "learning_rate": 4.4975e-05,
      "loss": 0.0025,
      "step": 24120
    },
    {
      "epoch": 0.8043333333333333,
      "grad_norm": 0.11970151960849762,
      "learning_rate": 4.497291666666667e-05,
      "loss": 0.003,
      "step": 24130
    },
    {
      "epoch": 0.8046666666666666,
      "grad_norm": 0.007903533056378365,
      "learning_rate": 4.497083333333333e-05,
      "loss": 0.002,
      "step": 24140
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.6874218583106995,
      "learning_rate": 4.4968750000000005e-05,
      "loss": 0.0034,
      "step": 24150
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.03055506944656372,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.0036,
      "step": 24160
    },
    {
      "epoch": 0.8056666666666666,
      "grad_norm": 0.08965910971164703,
      "learning_rate": 4.4964583333333336e-05,
      "loss": 0.0029,
      "step": 24170
    },
    {
      "epoch": 0.806,
      "grad_norm": 0.2979497015476227,
      "learning_rate": 4.49625e-05,
      "loss": 0.003,
      "step": 24180
    },
    {
      "epoch": 0.8063333333333333,
      "grad_norm": 0.7752090096473694,
      "learning_rate": 4.4960416666666674e-05,
      "loss": 0.0031,
      "step": 24190
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.4765486717224121,
      "learning_rate": 4.495833333333333e-05,
      "loss": 0.0017,
      "step": 24200
    },
    {
      "epoch": 0.807,
      "grad_norm": 0.14936217665672302,
      "learning_rate": 4.495625e-05,
      "loss": 0.0026,
      "step": 24210
    },
    {
      "epoch": 0.8073333333333333,
      "grad_norm": 0.06018576771020889,
      "learning_rate": 4.495416666666667e-05,
      "loss": 0.0021,
      "step": 24220
    },
    {
      "epoch": 0.8076666666666666,
      "grad_norm": 0.26820656657218933,
      "learning_rate": 4.4952083333333336e-05,
      "loss": 0.0027,
      "step": 24230
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.059370968490839005,
      "learning_rate": 4.495e-05,
      "loss": 0.0027,
      "step": 24240
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.149539053440094,
      "learning_rate": 4.494791666666667e-05,
      "loss": 0.0028,
      "step": 24250
    },
    {
      "epoch": 0.8086666666666666,
      "grad_norm": 0.14913903176784515,
      "learning_rate": 4.494583333333334e-05,
      "loss": 0.0023,
      "step": 24260
    },
    {
      "epoch": 0.809,
      "grad_norm": 0.3577730655670166,
      "learning_rate": 4.494375e-05,
      "loss": 0.0023,
      "step": 24270
    },
    {
      "epoch": 0.8093333333333333,
      "grad_norm": 0.20862708985805511,
      "learning_rate": 4.494166666666667e-05,
      "loss": 0.0025,
      "step": 24280
    },
    {
      "epoch": 0.8096666666666666,
      "grad_norm": 0.005632221698760986,
      "learning_rate": 4.4939583333333336e-05,
      "loss": 0.0028,
      "step": 24290
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.005039281211793423,
      "learning_rate": 4.49375e-05,
      "loss": 0.0028,
      "step": 24300
    },
    {
      "epoch": 0.8103333333333333,
      "grad_norm": 0.23859399557113647,
      "learning_rate": 4.493541666666667e-05,
      "loss": 0.0027,
      "step": 24310
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.2682536244392395,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.003,
      "step": 24320
    },
    {
      "epoch": 0.811,
      "grad_norm": 0.08980534225702286,
      "learning_rate": 4.4931250000000005e-05,
      "loss": 0.0033,
      "step": 24330
    },
    {
      "epoch": 0.8113333333333334,
      "grad_norm": 0.5359591245651245,
      "learning_rate": 4.492916666666666e-05,
      "loss": 0.002,
      "step": 24340
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.32745587825775146,
      "learning_rate": 4.4927083333333336e-05,
      "loss": 0.0034,
      "step": 24350
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.0989694818854332,
      "learning_rate": 4.4925e-05,
      "loss": 0.0048,
      "step": 24360
    },
    {
      "epoch": 0.8123333333333334,
      "grad_norm": 0.46484342217445374,
      "learning_rate": 4.492291666666667e-05,
      "loss": 0.0023,
      "step": 24370
    },
    {
      "epoch": 0.8126666666666666,
      "grad_norm": 0.23841658234596252,
      "learning_rate": 4.492083333333333e-05,
      "loss": 0.0022,
      "step": 24380
    },
    {
      "epoch": 0.813,
      "grad_norm": 0.17881828546524048,
      "learning_rate": 4.4918750000000004e-05,
      "loss": 0.0024,
      "step": 24390
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.14948832988739014,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.0022,
      "step": 24400
    },
    {
      "epoch": 0.8136666666666666,
      "grad_norm": 0.007975513115525246,
      "learning_rate": 4.4914583333333335e-05,
      "loss": 0.0028,
      "step": 24410
    },
    {
      "epoch": 0.814,
      "grad_norm": 0.41681942343711853,
      "learning_rate": 4.49125e-05,
      "loss": 0.0035,
      "step": 24420
    },
    {
      "epoch": 0.8143333333333334,
      "grad_norm": 0.38738754391670227,
      "learning_rate": 4.491041666666667e-05,
      "loss": 0.0033,
      "step": 24430
    },
    {
      "epoch": 0.8146666666666667,
      "grad_norm": 0.20872339606285095,
      "learning_rate": 4.490833333333334e-05,
      "loss": 0.0021,
      "step": 24440
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.060504455119371414,
      "learning_rate": 4.490625e-05,
      "loss": 0.003,
      "step": 24450
    },
    {
      "epoch": 0.8153333333333334,
      "grad_norm": 0.03123364970088005,
      "learning_rate": 4.490416666666667e-05,
      "loss": 0.0021,
      "step": 24460
    },
    {
      "epoch": 0.8156666666666667,
      "grad_norm": 0.29745858907699585,
      "learning_rate": 4.4902083333333335e-05,
      "loss": 0.0025,
      "step": 24470
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.38692376017570496,
      "learning_rate": 4.49e-05,
      "loss": 0.003,
      "step": 24480
    },
    {
      "epoch": 0.8163333333333334,
      "grad_norm": 0.17879144847393036,
      "learning_rate": 4.4897916666666666e-05,
      "loss": 0.0033,
      "step": 24490
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.1752457618713379,
      "learning_rate": 4.489583333333334e-05,
      "loss": 0.0031,
      "step": 24500
    },
    {
      "epoch": 0.817,
      "grad_norm": 0.059868570417165756,
      "learning_rate": 4.4893750000000004e-05,
      "loss": 0.0023,
      "step": 24510
    },
    {
      "epoch": 0.8173333333333334,
      "grad_norm": 0.20900042355060577,
      "learning_rate": 4.489166666666667e-05,
      "loss": 0.003,
      "step": 24520
    },
    {
      "epoch": 0.8176666666666667,
      "grad_norm": 0.543980598449707,
      "learning_rate": 4.4889583333333335e-05,
      "loss": 0.0028,
      "step": 24530
    },
    {
      "epoch": 0.818,
      "grad_norm": 0.4561518430709839,
      "learning_rate": 4.488750000000001e-05,
      "loss": 0.0039,
      "step": 24540
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.14958399534225464,
      "learning_rate": 4.4885416666666666e-05,
      "loss": 0.0034,
      "step": 24550
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.12045320868492126,
      "learning_rate": 4.488333333333333e-05,
      "loss": 0.0022,
      "step": 24560
    },
    {
      "epoch": 0.819,
      "grad_norm": 0.19888457655906677,
      "learning_rate": 4.488125e-05,
      "loss": 0.0026,
      "step": 24570
    },
    {
      "epoch": 0.8193333333333334,
      "grad_norm": 0.17876137793064117,
      "learning_rate": 4.487916666666667e-05,
      "loss": 0.002,
      "step": 24580
    },
    {
      "epoch": 0.8196666666666667,
      "grad_norm": 0.16890525817871094,
      "learning_rate": 4.4877083333333334e-05,
      "loss": 0.0033,
      "step": 24590
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.006492923013865948,
      "learning_rate": 4.4875e-05,
      "loss": 0.0028,
      "step": 24600
    },
    {
      "epoch": 0.8203333333333334,
      "grad_norm": 0.14879682660102844,
      "learning_rate": 4.487291666666667e-05,
      "loss": 0.0041,
      "step": 24610
    },
    {
      "epoch": 0.8206666666666667,
      "grad_norm": 0.11935729533433914,
      "learning_rate": 4.487083333333334e-05,
      "loss": 0.0023,
      "step": 24620
    },
    {
      "epoch": 0.821,
      "grad_norm": 0.17892344295978546,
      "learning_rate": 4.486875e-05,
      "loss": 0.0026,
      "step": 24630
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.0894596204161644,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0037,
      "step": 24640
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.32745832204818726,
      "learning_rate": 4.486458333333334e-05,
      "loss": 0.0021,
      "step": 24650
    },
    {
      "epoch": 0.822,
      "grad_norm": 0.4761420488357544,
      "learning_rate": 4.48625e-05,
      "loss": 0.0027,
      "step": 24660
    },
    {
      "epoch": 0.8223333333333334,
      "grad_norm": 0.032602708786726,
      "learning_rate": 4.486041666666667e-05,
      "loss": 0.0037,
      "step": 24670
    },
    {
      "epoch": 0.8226666666666667,
      "grad_norm": 0.4173945486545563,
      "learning_rate": 4.485833333333334e-05,
      "loss": 0.0033,
      "step": 24680
    },
    {
      "epoch": 0.823,
      "grad_norm": 0.17848524451255798,
      "learning_rate": 4.485625e-05,
      "loss": 0.0023,
      "step": 24690
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.09177447855472565,
      "learning_rate": 4.485416666666667e-05,
      "loss": 0.0029,
      "step": 24700
    },
    {
      "epoch": 0.8236666666666667,
      "grad_norm": 0.14969013631343842,
      "learning_rate": 4.4852083333333334e-05,
      "loss": 0.0031,
      "step": 24710
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.03212993964552879,
      "learning_rate": 4.4850000000000006e-05,
      "loss": 0.002,
      "step": 24720
    },
    {
      "epoch": 0.8243333333333334,
      "grad_norm": 0.6375843286514282,
      "learning_rate": 4.4847916666666665e-05,
      "loss": 0.0023,
      "step": 24730
    },
    {
      "epoch": 0.8246666666666667,
      "grad_norm": 0.06056007370352745,
      "learning_rate": 4.484583333333334e-05,
      "loss": 0.0024,
      "step": 24740
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.030288124457001686,
      "learning_rate": 4.484375e-05,
      "loss": 0.0022,
      "step": 24750
    },
    {
      "epoch": 0.8253333333333334,
      "grad_norm": 0.47634509205818176,
      "learning_rate": 4.484166666666667e-05,
      "loss": 0.0022,
      "step": 24760
    },
    {
      "epoch": 0.8256666666666667,
      "grad_norm": 0.08917427808046341,
      "learning_rate": 4.4839583333333333e-05,
      "loss": 0.0027,
      "step": 24770
    },
    {
      "epoch": 0.826,
      "grad_norm": 0.7786098718643188,
      "learning_rate": 4.4837500000000006e-05,
      "loss": 0.0061,
      "step": 24780
    },
    {
      "epoch": 0.8263333333333334,
      "grad_norm": 0.030242126435041428,
      "learning_rate": 4.483541666666667e-05,
      "loss": 0.0031,
      "step": 24790
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.6468290090560913,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.003,
      "step": 24800
    },
    {
      "epoch": 0.827,
      "grad_norm": 0.23785445094108582,
      "learning_rate": 4.483125e-05,
      "loss": 0.0029,
      "step": 24810
    },
    {
      "epoch": 0.8273333333333334,
      "grad_norm": 0.6946207880973816,
      "learning_rate": 4.482916666666667e-05,
      "loss": 0.0018,
      "step": 24820
    },
    {
      "epoch": 0.8276666666666667,
      "grad_norm": 0.23797039687633514,
      "learning_rate": 4.482708333333333e-05,
      "loss": 0.0024,
      "step": 24830
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.6409488916397095,
      "learning_rate": 4.4825e-05,
      "loss": 0.0031,
      "step": 24840
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.32609236240386963,
      "learning_rate": 4.482291666666667e-05,
      "loss": 0.0026,
      "step": 24850
    },
    {
      "epoch": 0.8286666666666667,
      "grad_norm": 0.05952863395214081,
      "learning_rate": 4.4820833333333336e-05,
      "loss": 0.0022,
      "step": 24860
    },
    {
      "epoch": 0.829,
      "grad_norm": 0.1486438661813736,
      "learning_rate": 4.481875e-05,
      "loss": 0.0022,
      "step": 24870
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.059687428176403046,
      "learning_rate": 4.481666666666667e-05,
      "loss": 0.0025,
      "step": 24880
    },
    {
      "epoch": 0.8296666666666667,
      "grad_norm": 0.2975640296936035,
      "learning_rate": 4.481458333333334e-05,
      "loss": 0.0032,
      "step": 24890
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.12546129524707794,
      "learning_rate": 4.4812500000000005e-05,
      "loss": 0.0022,
      "step": 24900
    },
    {
      "epoch": 0.8303333333333334,
      "grad_norm": 0.2390279322862625,
      "learning_rate": 4.481041666666667e-05,
      "loss": 0.003,
      "step": 24910
    },
    {
      "epoch": 0.8306666666666667,
      "grad_norm": 0.12005108594894409,
      "learning_rate": 4.4808333333333336e-05,
      "loss": 0.0021,
      "step": 24920
    },
    {
      "epoch": 0.831,
      "grad_norm": 0.20890596508979797,
      "learning_rate": 4.480625e-05,
      "loss": 0.0031,
      "step": 24930
    },
    {
      "epoch": 0.8313333333333334,
      "grad_norm": 0.060639530420303345,
      "learning_rate": 4.480416666666667e-05,
      "loss": 0.0031,
      "step": 24940
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.5100791454315186,
      "learning_rate": 4.480208333333333e-05,
      "loss": 0.0048,
      "step": 24950
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.05966363474726677,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0018,
      "step": 24960
    },
    {
      "epoch": 0.8323333333333334,
      "grad_norm": 0.4166420102119446,
      "learning_rate": 4.479791666666667e-05,
      "loss": 0.0021,
      "step": 24970
    },
    {
      "epoch": 0.8326666666666667,
      "grad_norm": 0.17886459827423096,
      "learning_rate": 4.4795833333333336e-05,
      "loss": 0.004,
      "step": 24980
    },
    {
      "epoch": 0.833,
      "grad_norm": 0.44637346267700195,
      "learning_rate": 4.479375e-05,
      "loss": 0.003,
      "step": 24990
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.17829079926013947,
      "learning_rate": 4.4791666666666673e-05,
      "loss": 0.0031,
      "step": 25000
    },
    {
      "epoch": 0.8336666666666667,
      "grad_norm": 0.17863328754901886,
      "learning_rate": 4.478958333333333e-05,
      "loss": 0.0038,
      "step": 25010
    },
    {
      "epoch": 0.834,
      "grad_norm": 0.3273671567440033,
      "learning_rate": 4.4787500000000004e-05,
      "loss": 0.0029,
      "step": 25020
    },
    {
      "epoch": 0.8343333333333334,
      "grad_norm": 0.2085316926240921,
      "learning_rate": 4.478541666666667e-05,
      "loss": 0.003,
      "step": 25030
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.5051609873771667,
      "learning_rate": 4.4783333333333335e-05,
      "loss": 0.003,
      "step": 25040
    },
    {
      "epoch": 0.835,
      "grad_norm": 0.6245806813240051,
      "learning_rate": 4.478125e-05,
      "loss": 0.0032,
      "step": 25050
    },
    {
      "epoch": 0.8353333333333334,
      "grad_norm": 0.7136843204498291,
      "learning_rate": 4.4779166666666666e-05,
      "loss": 0.0045,
      "step": 25060
    },
    {
      "epoch": 0.8356666666666667,
      "grad_norm": 0.030468754470348358,
      "learning_rate": 4.477708333333334e-05,
      "loss": 0.0036,
      "step": 25070
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.5934193730354309,
      "learning_rate": 4.4775e-05,
      "loss": 0.0028,
      "step": 25080
    },
    {
      "epoch": 0.8363333333333334,
      "grad_norm": 0.08987917751073837,
      "learning_rate": 4.477291666666667e-05,
      "loss": 0.004,
      "step": 25090
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.009008745662868023,
      "learning_rate": 4.4770833333333335e-05,
      "loss": 0.0025,
      "step": 25100
    },
    {
      "epoch": 0.837,
      "grad_norm": 0.3003706634044647,
      "learning_rate": 4.476875e-05,
      "loss": 0.0023,
      "step": 25110
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.44640907645225525,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0027,
      "step": 25120
    },
    {
      "epoch": 0.8376666666666667,
      "grad_norm": 0.14840032160282135,
      "learning_rate": 4.476458333333334e-05,
      "loss": 0.0044,
      "step": 25130
    },
    {
      "epoch": 0.838,
      "grad_norm": 0.2384813278913498,
      "learning_rate": 4.4762500000000004e-05,
      "loss": 0.0018,
      "step": 25140
    },
    {
      "epoch": 0.8383333333333334,
      "grad_norm": 0.23817336559295654,
      "learning_rate": 4.476041666666667e-05,
      "loss": 0.0026,
      "step": 25150
    },
    {
      "epoch": 0.8386666666666667,
      "grad_norm": 0.17962920665740967,
      "learning_rate": 4.4758333333333335e-05,
      "loss": 0.003,
      "step": 25160
    },
    {
      "epoch": 0.839,
      "grad_norm": 0.11945540457963943,
      "learning_rate": 4.475625e-05,
      "loss": 0.004,
      "step": 25170
    },
    {
      "epoch": 0.8393333333333334,
      "grad_norm": 0.148702934384346,
      "learning_rate": 4.475416666666667e-05,
      "loss": 0.0022,
      "step": 25180
    },
    {
      "epoch": 0.8396666666666667,
      "grad_norm": 0.1492227166891098,
      "learning_rate": 4.475208333333333e-05,
      "loss": 0.0018,
      "step": 25190
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.4167470932006836,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.0028,
      "step": 25200
    },
    {
      "epoch": 0.8403333333333334,
      "grad_norm": 0.13850077986717224,
      "learning_rate": 4.474791666666667e-05,
      "loss": 0.0022,
      "step": 25210
    },
    {
      "epoch": 0.8406666666666667,
      "grad_norm": 0.12050268054008484,
      "learning_rate": 4.4745833333333335e-05,
      "loss": 0.0021,
      "step": 25220
    },
    {
      "epoch": 0.841,
      "grad_norm": 0.21884042024612427,
      "learning_rate": 4.474375e-05,
      "loss": 0.0027,
      "step": 25230
    },
    {
      "epoch": 0.8413333333333334,
      "grad_norm": 0.0065706404857337475,
      "learning_rate": 4.474166666666667e-05,
      "loss": 0.0029,
      "step": 25240
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.14889398217201233,
      "learning_rate": 4.473958333333334e-05,
      "loss": 0.0018,
      "step": 25250
    },
    {
      "epoch": 0.842,
      "grad_norm": 0.03180527687072754,
      "learning_rate": 4.47375e-05,
      "loss": 0.0029,
      "step": 25260
    },
    {
      "epoch": 0.8423333333333334,
      "grad_norm": 0.17898517847061157,
      "learning_rate": 4.473541666666667e-05,
      "loss": 0.0031,
      "step": 25270
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.3569096028804779,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0034,
      "step": 25280
    },
    {
      "epoch": 0.843,
      "grad_norm": 0.3874185383319855,
      "learning_rate": 4.473125e-05,
      "loss": 0.0031,
      "step": 25290
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.20835791528224945,
      "learning_rate": 4.4729166666666665e-05,
      "loss": 0.0028,
      "step": 25300
    },
    {
      "epoch": 0.8436666666666667,
      "grad_norm": 0.05990469455718994,
      "learning_rate": 4.472708333333334e-05,
      "loss": 0.0025,
      "step": 25310
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.14887899160385132,
      "learning_rate": 4.4725e-05,
      "loss": 0.0037,
      "step": 25320
    },
    {
      "epoch": 0.8443333333333334,
      "grad_norm": 0.1783675104379654,
      "learning_rate": 4.472291666666667e-05,
      "loss": 0.0028,
      "step": 25330
    },
    {
      "epoch": 0.8446666666666667,
      "grad_norm": 0.45174357295036316,
      "learning_rate": 4.4720833333333334e-05,
      "loss": 0.0028,
      "step": 25340
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.20800119638442993,
      "learning_rate": 4.4718750000000006e-05,
      "loss": 0.0027,
      "step": 25350
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.11906855553388596,
      "learning_rate": 4.4716666666666665e-05,
      "loss": 0.0029,
      "step": 25360
    },
    {
      "epoch": 0.8456666666666667,
      "grad_norm": 0.1196720153093338,
      "learning_rate": 4.471458333333334e-05,
      "loss": 0.0032,
      "step": 25370
    },
    {
      "epoch": 0.846,
      "grad_norm": 0.05988417938351631,
      "learning_rate": 4.47125e-05,
      "loss": 0.0039,
      "step": 25380
    },
    {
      "epoch": 0.8463333333333334,
      "grad_norm": 0.38603338599205017,
      "learning_rate": 4.471041666666667e-05,
      "loss": 0.0023,
      "step": 25390
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.237740620970726,
      "learning_rate": 4.4708333333333334e-05,
      "loss": 0.0042,
      "step": 25400
    },
    {
      "epoch": 0.847,
      "grad_norm": 0.47516700625419617,
      "learning_rate": 4.470625e-05,
      "loss": 0.0034,
      "step": 25410
    },
    {
      "epoch": 0.8473333333333334,
      "grad_norm": 0.1191883236169815,
      "learning_rate": 4.470416666666667e-05,
      "loss": 0.0031,
      "step": 25420
    },
    {
      "epoch": 0.8476666666666667,
      "grad_norm": 0.0894116535782814,
      "learning_rate": 4.470208333333333e-05,
      "loss": 0.0029,
      "step": 25430
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.3863404393196106,
      "learning_rate": 4.47e-05,
      "loss": 0.0023,
      "step": 25440
    },
    {
      "epoch": 0.8483333333333334,
      "grad_norm": 0.20858310163021088,
      "learning_rate": 4.469791666666667e-05,
      "loss": 0.0031,
      "step": 25450
    },
    {
      "epoch": 0.8486666666666667,
      "grad_norm": 0.28914833068847656,
      "learning_rate": 4.469583333333334e-05,
      "loss": 0.002,
      "step": 25460
    },
    {
      "epoch": 0.849,
      "grad_norm": 0.594501256942749,
      "learning_rate": 4.469375e-05,
      "loss": 0.003,
      "step": 25470
    },
    {
      "epoch": 0.8493333333333334,
      "grad_norm": 0.386691689491272,
      "learning_rate": 4.469166666666667e-05,
      "loss": 0.0024,
      "step": 25480
    },
    {
      "epoch": 0.8496666666666667,
      "grad_norm": 0.44670483469963074,
      "learning_rate": 4.4689583333333337e-05,
      "loss": 0.0027,
      "step": 25490
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.38624441623687744,
      "learning_rate": 4.46875e-05,
      "loss": 0.0029,
      "step": 25500
    },
    {
      "epoch": 0.8503333333333334,
      "grad_norm": 0.2674877643585205,
      "learning_rate": 4.468541666666667e-05,
      "loss": 0.0025,
      "step": 25510
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.10570991784334183,
      "learning_rate": 4.468333333333334e-05,
      "loss": 0.0038,
      "step": 25520
    },
    {
      "epoch": 0.851,
      "grad_norm": 0.7741670608520508,
      "learning_rate": 4.4681250000000005e-05,
      "loss": 0.0022,
      "step": 25530
    },
    {
      "epoch": 0.8513333333333334,
      "grad_norm": 0.4654022455215454,
      "learning_rate": 4.4679166666666664e-05,
      "loss": 0.0032,
      "step": 25540
    },
    {
      "epoch": 0.8516666666666667,
      "grad_norm": 0.2678595781326294,
      "learning_rate": 4.4677083333333336e-05,
      "loss": 0.0035,
      "step": 25550
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.31886932253837585,
      "learning_rate": 4.4675e-05,
      "loss": 0.0031,
      "step": 25560
    },
    {
      "epoch": 0.8523333333333334,
      "grad_norm": 0.08999277651309967,
      "learning_rate": 4.467291666666667e-05,
      "loss": 0.0031,
      "step": 25570
    },
    {
      "epoch": 0.8526666666666667,
      "grad_norm": 0.38599255681037903,
      "learning_rate": 4.467083333333333e-05,
      "loss": 0.0023,
      "step": 25580
    },
    {
      "epoch": 0.853,
      "grad_norm": 0.23758868873119354,
      "learning_rate": 4.4668750000000005e-05,
      "loss": 0.0027,
      "step": 25590
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.0893537700176239,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0032,
      "step": 25600
    },
    {
      "epoch": 0.8536666666666667,
      "grad_norm": 0.17833705246448517,
      "learning_rate": 4.4664583333333336e-05,
      "loss": 0.0033,
      "step": 25610
    },
    {
      "epoch": 0.854,
      "grad_norm": 0.00757606653496623,
      "learning_rate": 4.46625e-05,
      "loss": 0.0024,
      "step": 25620
    },
    {
      "epoch": 0.8543333333333333,
      "grad_norm": 0.06008649244904518,
      "learning_rate": 4.4660416666666674e-05,
      "loss": 0.0026,
      "step": 25630
    },
    {
      "epoch": 0.8546666666666667,
      "grad_norm": 0.05986320227384567,
      "learning_rate": 4.465833333333333e-05,
      "loss": 0.003,
      "step": 25640
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.5798451900482178,
      "learning_rate": 4.465625e-05,
      "loss": 0.0032,
      "step": 25650
    },
    {
      "epoch": 0.8553333333333333,
      "grad_norm": 0.1787910759449005,
      "learning_rate": 4.465416666666667e-05,
      "loss": 0.0022,
      "step": 25660
    },
    {
      "epoch": 0.8556666666666667,
      "grad_norm": 0.5652730464935303,
      "learning_rate": 4.4652083333333336e-05,
      "loss": 0.0035,
      "step": 25670
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.2973422706127167,
      "learning_rate": 4.465e-05,
      "loss": 0.0036,
      "step": 25680
    },
    {
      "epoch": 0.8563333333333333,
      "grad_norm": 0.23824872076511383,
      "learning_rate": 4.4647916666666667e-05,
      "loss": 0.0022,
      "step": 25690
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 0.2980588376522064,
      "learning_rate": 4.464583333333334e-05,
      "loss": 0.0034,
      "step": 25700
    },
    {
      "epoch": 0.857,
      "grad_norm": 0.26758235692977905,
      "learning_rate": 4.464375e-05,
      "loss": 0.0029,
      "step": 25710
    },
    {
      "epoch": 0.8573333333333333,
      "grad_norm": 0.564859926700592,
      "learning_rate": 4.464166666666667e-05,
      "loss": 0.0026,
      "step": 25720
    },
    {
      "epoch": 0.8576666666666667,
      "grad_norm": 0.006975963711738586,
      "learning_rate": 4.4639583333333335e-05,
      "loss": 0.0022,
      "step": 25730
    },
    {
      "epoch": 0.858,
      "grad_norm": 0.08923590928316116,
      "learning_rate": 4.463750000000001e-05,
      "loss": 0.0033,
      "step": 25740
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.20835307240486145,
      "learning_rate": 4.4635416666666666e-05,
      "loss": 0.0028,
      "step": 25750
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.2677622437477112,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0036,
      "step": 25760
    },
    {
      "epoch": 0.859,
      "grad_norm": 0.15424160659313202,
      "learning_rate": 4.4631250000000004e-05,
      "loss": 0.0016,
      "step": 25770
    },
    {
      "epoch": 0.8593333333333333,
      "grad_norm": 0.32766416668891907,
      "learning_rate": 4.462916666666667e-05,
      "loss": 0.002,
      "step": 25780
    },
    {
      "epoch": 0.8596666666666667,
      "grad_norm": 0.2383921593427658,
      "learning_rate": 4.4627083333333335e-05,
      "loss": 0.0035,
      "step": 25790
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.22558148205280304,
      "learning_rate": 4.4625e-05,
      "loss": 0.0022,
      "step": 25800
    },
    {
      "epoch": 0.8603333333333333,
      "grad_norm": 0.612471342086792,
      "learning_rate": 4.462291666666667e-05,
      "loss": 0.0028,
      "step": 25810
    },
    {
      "epoch": 0.8606666666666667,
      "grad_norm": 0.007306122221052647,
      "learning_rate": 4.462083333333333e-05,
      "loss": 0.0038,
      "step": 25820
    },
    {
      "epoch": 0.861,
      "grad_norm": 0.23777185380458832,
      "learning_rate": 4.4618750000000004e-05,
      "loss": 0.0024,
      "step": 25830
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.2381584346294403,
      "learning_rate": 4.461666666666667e-05,
      "loss": 0.0021,
      "step": 25840
    },
    {
      "epoch": 0.8616666666666667,
      "grad_norm": 0.1785546839237213,
      "learning_rate": 4.4614583333333335e-05,
      "loss": 0.0024,
      "step": 25850
    },
    {
      "epoch": 0.862,
      "grad_norm": 0.350816935300827,
      "learning_rate": 4.46125e-05,
      "loss": 0.0036,
      "step": 25860
    },
    {
      "epoch": 0.8623333333333333,
      "grad_norm": 0.3381936550140381,
      "learning_rate": 4.461041666666667e-05,
      "loss": 0.0031,
      "step": 25870
    },
    {
      "epoch": 0.8626666666666667,
      "grad_norm": 0.14838825166225433,
      "learning_rate": 4.460833333333334e-05,
      "loss": 0.0018,
      "step": 25880
    },
    {
      "epoch": 0.863,
      "grad_norm": 0.43526530265808105,
      "learning_rate": 4.460625e-05,
      "loss": 0.0024,
      "step": 25890
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 0.535308301448822,
      "learning_rate": 4.460416666666667e-05,
      "loss": 0.003,
      "step": 25900
    },
    {
      "epoch": 0.8636666666666667,
      "grad_norm": 0.4464515149593353,
      "learning_rate": 4.4602083333333334e-05,
      "loss": 0.0024,
      "step": 25910
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.03092203289270401,
      "learning_rate": 4.46e-05,
      "loss": 0.0036,
      "step": 25920
    },
    {
      "epoch": 0.8643333333333333,
      "grad_norm": 0.0058416943065822124,
      "learning_rate": 4.4597916666666665e-05,
      "loss": 0.0028,
      "step": 25930
    },
    {
      "epoch": 0.8646666666666667,
      "grad_norm": 0.17851173877716064,
      "learning_rate": 4.459583333333334e-05,
      "loss": 0.0036,
      "step": 25940
    },
    {
      "epoch": 0.865,
      "grad_norm": 0.08930157870054245,
      "learning_rate": 4.459375e-05,
      "loss": 0.0028,
      "step": 25950
    },
    {
      "epoch": 0.8653333333333333,
      "grad_norm": 0.08938553184270859,
      "learning_rate": 4.459166666666667e-05,
      "loss": 0.0032,
      "step": 25960
    },
    {
      "epoch": 0.8656666666666667,
      "grad_norm": 0.5940847992897034,
      "learning_rate": 4.4589583333333334e-05,
      "loss": 0.0031,
      "step": 25970
    },
    {
      "epoch": 0.866,
      "grad_norm": 0.2382350116968155,
      "learning_rate": 4.4587500000000006e-05,
      "loss": 0.0026,
      "step": 25980
    },
    {
      "epoch": 0.8663333333333333,
      "grad_norm": 0.350262314081192,
      "learning_rate": 4.4585416666666665e-05,
      "loss": 0.0027,
      "step": 25990
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.30128341913223267,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.0033,
      "step": 26000
    },
    {
      "epoch": 0.867,
      "grad_norm": 0.1432648003101349,
      "learning_rate": 4.458125e-05,
      "loss": 0.0023,
      "step": 26010
    },
    {
      "epoch": 0.8673333333333333,
      "grad_norm": 0.26803070306777954,
      "learning_rate": 4.457916666666667e-05,
      "loss": 0.0041,
      "step": 26020
    },
    {
      "epoch": 0.8676666666666667,
      "grad_norm": 0.41018277406692505,
      "learning_rate": 4.4577083333333334e-05,
      "loss": 0.0018,
      "step": 26030
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.6539607644081116,
      "learning_rate": 4.4575e-05,
      "loss": 0.0022,
      "step": 26040
    },
    {
      "epoch": 0.8683333333333333,
      "grad_norm": 0.1492302119731903,
      "learning_rate": 4.457291666666667e-05,
      "loss": 0.0017,
      "step": 26050
    },
    {
      "epoch": 0.8686666666666667,
      "grad_norm": 0.2247418612241745,
      "learning_rate": 4.457083333333334e-05,
      "loss": 0.0022,
      "step": 26060
    },
    {
      "epoch": 0.869,
      "grad_norm": 0.6030188202857971,
      "learning_rate": 4.456875e-05,
      "loss": 0.0026,
      "step": 26070
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.17821389436721802,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0032,
      "step": 26080
    },
    {
      "epoch": 0.8696666666666667,
      "grad_norm": 0.2082429677248001,
      "learning_rate": 4.456458333333334e-05,
      "loss": 0.0033,
      "step": 26090
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.42230135202407837,
      "learning_rate": 4.45625e-05,
      "loss": 0.0017,
      "step": 26100
    },
    {
      "epoch": 0.8703333333333333,
      "grad_norm": 0.33221569657325745,
      "learning_rate": 4.456041666666667e-05,
      "loss": 0.0025,
      "step": 26110
    },
    {
      "epoch": 0.8706666666666667,
      "grad_norm": 0.5347894430160522,
      "learning_rate": 4.455833333333334e-05,
      "loss": 0.0039,
      "step": 26120
    },
    {
      "epoch": 0.871,
      "grad_norm": 0.4457738697528839,
      "learning_rate": 4.455625e-05,
      "loss": 0.0021,
      "step": 26130
    },
    {
      "epoch": 0.8713333333333333,
      "grad_norm": 0.29690033197402954,
      "learning_rate": 4.455416666666667e-05,
      "loss": 0.0034,
      "step": 26140
    },
    {
      "epoch": 0.8716666666666667,
      "grad_norm": 0.08942268788814545,
      "learning_rate": 4.455208333333333e-05,
      "loss": 0.0026,
      "step": 26150
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.08929687738418579,
      "learning_rate": 4.4550000000000005e-05,
      "loss": 0.0026,
      "step": 26160
    },
    {
      "epoch": 0.8723333333333333,
      "grad_norm": 0.44602179527282715,
      "learning_rate": 4.4547916666666664e-05,
      "loss": 0.0028,
      "step": 26170
    },
    {
      "epoch": 0.8726666666666667,
      "grad_norm": 0.08936842530965805,
      "learning_rate": 4.4545833333333336e-05,
      "loss": 0.003,
      "step": 26180
    },
    {
      "epoch": 0.873,
      "grad_norm": 0.059296004474163055,
      "learning_rate": 4.454375e-05,
      "loss": 0.0022,
      "step": 26190
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.39009228348731995,
      "learning_rate": 4.454166666666667e-05,
      "loss": 0.0022,
      "step": 26200
    },
    {
      "epoch": 0.8736666666666667,
      "grad_norm": 0.4158387780189514,
      "learning_rate": 4.453958333333333e-05,
      "loss": 0.0035,
      "step": 26210
    },
    {
      "epoch": 0.874,
      "grad_norm": 0.05980709567666054,
      "learning_rate": 4.4537500000000005e-05,
      "loss": 0.0034,
      "step": 26220
    },
    {
      "epoch": 0.8743333333333333,
      "grad_norm": 0.32718098163604736,
      "learning_rate": 4.453541666666667e-05,
      "loss": 0.0029,
      "step": 26230
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.11899685859680176,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0024,
      "step": 26240
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.267127126455307,
      "learning_rate": 4.453125e-05,
      "loss": 0.0038,
      "step": 26250
    },
    {
      "epoch": 0.8753333333333333,
      "grad_norm": 0.08940158784389496,
      "learning_rate": 4.452916666666667e-05,
      "loss": 0.0031,
      "step": 26260
    },
    {
      "epoch": 0.8756666666666667,
      "grad_norm": 0.8022533655166626,
      "learning_rate": 4.452708333333333e-05,
      "loss": 0.0026,
      "step": 26270
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.030769720673561096,
      "learning_rate": 4.4525e-05,
      "loss": 0.0023,
      "step": 26280
    },
    {
      "epoch": 0.8763333333333333,
      "grad_norm": 0.059459537267684937,
      "learning_rate": 4.452291666666667e-05,
      "loss": 0.003,
      "step": 26290
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 0.3566623032093048,
      "learning_rate": 4.4520833333333336e-05,
      "loss": 0.0029,
      "step": 26300
    },
    {
      "epoch": 0.877,
      "grad_norm": 0.3316410183906555,
      "learning_rate": 4.451875e-05,
      "loss": 0.0039,
      "step": 26310
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.23805832862854004,
      "learning_rate": 4.451666666666667e-05,
      "loss": 0.0026,
      "step": 26320
    },
    {
      "epoch": 0.8776666666666667,
      "grad_norm": 0.2083357572555542,
      "learning_rate": 4.451458333333334e-05,
      "loss": 0.0031,
      "step": 26330
    },
    {
      "epoch": 0.878,
      "grad_norm": 0.08942006528377533,
      "learning_rate": 4.4512500000000005e-05,
      "loss": 0.0029,
      "step": 26340
    },
    {
      "epoch": 0.8783333333333333,
      "grad_norm": 0.11907529830932617,
      "learning_rate": 4.451041666666667e-05,
      "loss": 0.0019,
      "step": 26350
    },
    {
      "epoch": 0.8786666666666667,
      "grad_norm": 0.011126256547868252,
      "learning_rate": 4.4508333333333336e-05,
      "loss": 0.0018,
      "step": 26360
    },
    {
      "epoch": 0.879,
      "grad_norm": 0.7385327219963074,
      "learning_rate": 4.450625000000001e-05,
      "loss": 0.002,
      "step": 26370
    },
    {
      "epoch": 0.8793333333333333,
      "grad_norm": 0.06003061309456825,
      "learning_rate": 4.4504166666666666e-05,
      "loss": 0.0024,
      "step": 26380
    },
    {
      "epoch": 0.8796666666666667,
      "grad_norm": 0.4758359491825104,
      "learning_rate": 4.450208333333333e-05,
      "loss": 0.0032,
      "step": 26390
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.43369001150131226,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.0019,
      "step": 26400
    },
    {
      "epoch": 0.8803333333333333,
      "grad_norm": 0.26746687293052673,
      "learning_rate": 4.449791666666667e-05,
      "loss": 0.0022,
      "step": 26410
    },
    {
      "epoch": 0.8806666666666667,
      "grad_norm": 0.1786142885684967,
      "learning_rate": 4.4495833333333335e-05,
      "loss": 0.0032,
      "step": 26420
    },
    {
      "epoch": 0.881,
      "grad_norm": 0.14881405234336853,
      "learning_rate": 4.449375e-05,
      "loss": 0.0023,
      "step": 26430
    },
    {
      "epoch": 0.8813333333333333,
      "grad_norm": 0.47624850273132324,
      "learning_rate": 4.449166666666667e-05,
      "loss": 0.0017,
      "step": 26440
    },
    {
      "epoch": 0.8816666666666667,
      "grad_norm": 0.14898894727230072,
      "learning_rate": 4.448958333333333e-05,
      "loss": 0.0026,
      "step": 26450
    },
    {
      "epoch": 0.882,
      "grad_norm": 0.35712388157844543,
      "learning_rate": 4.4487500000000004e-05,
      "loss": 0.0025,
      "step": 26460
    },
    {
      "epoch": 0.8823333333333333,
      "grad_norm": 0.11908313632011414,
      "learning_rate": 4.448541666666667e-05,
      "loss": 0.0021,
      "step": 26470
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.23823443055152893,
      "learning_rate": 4.4483333333333335e-05,
      "loss": 0.003,
      "step": 26480
    },
    {
      "epoch": 0.883,
      "grad_norm": 0.35656943917274475,
      "learning_rate": 4.448125e-05,
      "loss": 0.0029,
      "step": 26490
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.11922305822372437,
      "learning_rate": 4.4479166666666666e-05,
      "loss": 0.0026,
      "step": 26500
    },
    {
      "epoch": 0.8836666666666667,
      "grad_norm": 0.03037995845079422,
      "learning_rate": 4.447708333333334e-05,
      "loss": 0.003,
      "step": 26510
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.14857572317123413,
      "learning_rate": 4.4475e-05,
      "loss": 0.0027,
      "step": 26520
    },
    {
      "epoch": 0.8843333333333333,
      "grad_norm": 0.40827053785324097,
      "learning_rate": 4.447291666666667e-05,
      "loss": 0.0032,
      "step": 26530
    },
    {
      "epoch": 0.8846666666666667,
      "grad_norm": 0.2979213297367096,
      "learning_rate": 4.4470833333333335e-05,
      "loss": 0.0025,
      "step": 26540
    },
    {
      "epoch": 0.885,
      "grad_norm": 0.3866150677204132,
      "learning_rate": 4.446875e-05,
      "loss": 0.0019,
      "step": 26550
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.2379949539899826,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0026,
      "step": 26560
    },
    {
      "epoch": 0.8856666666666667,
      "grad_norm": 0.007208328228443861,
      "learning_rate": 4.446458333333334e-05,
      "loss": 0.0027,
      "step": 26570
    },
    {
      "epoch": 0.886,
      "grad_norm": 0.37029799818992615,
      "learning_rate": 4.44625e-05,
      "loss": 0.0033,
      "step": 26580
    },
    {
      "epoch": 0.8863333333333333,
      "grad_norm": 0.2380395531654358,
      "learning_rate": 4.446041666666667e-05,
      "loss": 0.003,
      "step": 26590
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.4161522090435028,
      "learning_rate": 4.4458333333333334e-05,
      "loss": 0.0019,
      "step": 26600
    },
    {
      "epoch": 0.887,
      "grad_norm": 0.29742082953453064,
      "learning_rate": 4.4456250000000007e-05,
      "loss": 0.0032,
      "step": 26610
    },
    {
      "epoch": 0.8873333333333333,
      "grad_norm": 0.23804709315299988,
      "learning_rate": 4.445416666666667e-05,
      "loss": 0.0033,
      "step": 26620
    },
    {
      "epoch": 0.8876666666666667,
      "grad_norm": 0.17832346260547638,
      "learning_rate": 4.445208333333333e-05,
      "loss": 0.0023,
      "step": 26630
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.05984841659665108,
      "learning_rate": 4.445e-05,
      "loss": 0.0022,
      "step": 26640
    },
    {
      "epoch": 0.8883333333333333,
      "grad_norm": 0.14859315752983093,
      "learning_rate": 4.444791666666667e-05,
      "loss": 0.0027,
      "step": 26650
    },
    {
      "epoch": 0.8886666666666667,
      "grad_norm": 0.41604292392730713,
      "learning_rate": 4.4445833333333334e-05,
      "loss": 0.0018,
      "step": 26660
    },
    {
      "epoch": 0.889,
      "grad_norm": 0.18753443658351898,
      "learning_rate": 4.444375e-05,
      "loss": 0.0029,
      "step": 26670
    },
    {
      "epoch": 0.8893333333333333,
      "grad_norm": 0.006518366746604443,
      "learning_rate": 4.444166666666667e-05,
      "loss": 0.0023,
      "step": 26680
    },
    {
      "epoch": 0.8896666666666667,
      "grad_norm": 0.4885074496269226,
      "learning_rate": 4.443958333333334e-05,
      "loss": 0.0017,
      "step": 26690
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.2082650363445282,
      "learning_rate": 4.44375e-05,
      "loss": 0.0024,
      "step": 26700
    },
    {
      "epoch": 0.8903333333333333,
      "grad_norm": 0.16358356177806854,
      "learning_rate": 4.443541666666667e-05,
      "loss": 0.0024,
      "step": 26710
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.060277070850133896,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.003,
      "step": 26720
    },
    {
      "epoch": 0.891,
      "grad_norm": 0.2368156760931015,
      "learning_rate": 4.443125e-05,
      "loss": 0.0023,
      "step": 26730
    },
    {
      "epoch": 0.8913333333333333,
      "grad_norm": 0.1788010448217392,
      "learning_rate": 4.4429166666666665e-05,
      "loss": 0.0022,
      "step": 26740
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 0.6840320825576782,
      "learning_rate": 4.442708333333334e-05,
      "loss": 0.0026,
      "step": 26750
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.17841599881649017,
      "learning_rate": 4.4425e-05,
      "loss": 0.0024,
      "step": 26760
    },
    {
      "epoch": 0.8923333333333333,
      "grad_norm": 0.0077922772616147995,
      "learning_rate": 4.442291666666667e-05,
      "loss": 0.0024,
      "step": 26770
    },
    {
      "epoch": 0.8926666666666667,
      "grad_norm": 0.4164665639400482,
      "learning_rate": 4.442083333333333e-05,
      "loss": 0.0026,
      "step": 26780
    },
    {
      "epoch": 0.893,
      "grad_norm": 0.20811913907527924,
      "learning_rate": 4.4418750000000006e-05,
      "loss": 0.0021,
      "step": 26790
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.30924326181411743,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.0023,
      "step": 26800
    },
    {
      "epoch": 0.8936666666666667,
      "grad_norm": 0.4457663893699646,
      "learning_rate": 4.441458333333334e-05,
      "loss": 0.0015,
      "step": 26810
    },
    {
      "epoch": 0.894,
      "grad_norm": 0.23800700902938843,
      "learning_rate": 4.44125e-05,
      "loss": 0.0032,
      "step": 26820
    },
    {
      "epoch": 0.8943333333333333,
      "grad_norm": 0.0902259573340416,
      "learning_rate": 4.4410416666666674e-05,
      "loss": 0.0033,
      "step": 26830
    },
    {
      "epoch": 0.8946666666666667,
      "grad_norm": 0.3867497444152832,
      "learning_rate": 4.440833333333333e-05,
      "loss": 0.0023,
      "step": 26840
    },
    {
      "epoch": 0.895,
      "grad_norm": 0.08913085609674454,
      "learning_rate": 4.4406250000000005e-05,
      "loss": 0.0028,
      "step": 26850
    },
    {
      "epoch": 0.8953333333333333,
      "grad_norm": 0.26764801144599915,
      "learning_rate": 4.440416666666667e-05,
      "loss": 0.0026,
      "step": 26860
    },
    {
      "epoch": 0.8956666666666667,
      "grad_norm": 0.1782218962907791,
      "learning_rate": 4.4402083333333336e-05,
      "loss": 0.002,
      "step": 26870
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.32655176520347595,
      "learning_rate": 4.44e-05,
      "loss": 0.0015,
      "step": 26880
    },
    {
      "epoch": 0.8963333333333333,
      "grad_norm": 0.5319515466690063,
      "learning_rate": 4.439791666666667e-05,
      "loss": 0.0036,
      "step": 26890
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.010890052653849125,
      "learning_rate": 4.439583333333334e-05,
      "loss": 0.0017,
      "step": 26900
    },
    {
      "epoch": 0.897,
      "grad_norm": 0.1486339420080185,
      "learning_rate": 4.439375e-05,
      "loss": 0.0019,
      "step": 26910
    },
    {
      "epoch": 0.8973333333333333,
      "grad_norm": 0.11909263581037521,
      "learning_rate": 4.439166666666667e-05,
      "loss": 0.0039,
      "step": 26920
    },
    {
      "epoch": 0.8976666666666666,
      "grad_norm": 0.5944679975509644,
      "learning_rate": 4.4389583333333336e-05,
      "loss": 0.0029,
      "step": 26930
    },
    {
      "epoch": 0.898,
      "grad_norm": 0.2677367627620697,
      "learning_rate": 4.43875e-05,
      "loss": 0.0023,
      "step": 26940
    },
    {
      "epoch": 0.8983333333333333,
      "grad_norm": 0.00583851570263505,
      "learning_rate": 4.438541666666667e-05,
      "loss": 0.0034,
      "step": 26950
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.4750584363937378,
      "learning_rate": 4.438333333333334e-05,
      "loss": 0.0031,
      "step": 26960
    },
    {
      "epoch": 0.899,
      "grad_norm": 0.2970077693462372,
      "learning_rate": 4.4381250000000005e-05,
      "loss": 0.0031,
      "step": 26970
    },
    {
      "epoch": 0.8993333333333333,
      "grad_norm": 0.44547080993652344,
      "learning_rate": 4.4379166666666663e-05,
      "loss": 0.0021,
      "step": 26980
    },
    {
      "epoch": 0.8996666666666666,
      "grad_norm": 0.27139371633529663,
      "learning_rate": 4.4377083333333336e-05,
      "loss": 0.0024,
      "step": 26990
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.585870623588562,
      "learning_rate": 4.4375e-05,
      "loss": 0.0041,
      "step": 27000
    },
    {
      "epoch": 0.9003333333333333,
      "grad_norm": 0.2075556069612503,
      "learning_rate": 4.437291666666667e-05,
      "loss": 0.0019,
      "step": 27010
    },
    {
      "epoch": 0.9006666666666666,
      "grad_norm": 0.060428813099861145,
      "learning_rate": 4.437083333333333e-05,
      "loss": 0.0023,
      "step": 27020
    },
    {
      "epoch": 0.901,
      "grad_norm": 0.5049702525138855,
      "learning_rate": 4.4368750000000004e-05,
      "loss": 0.0031,
      "step": 27030
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.030627461150288582,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.0027,
      "step": 27040
    },
    {
      "epoch": 0.9016666666666666,
      "grad_norm": 0.11888410896062851,
      "learning_rate": 4.4364583333333335e-05,
      "loss": 0.0027,
      "step": 27050
    },
    {
      "epoch": 0.902,
      "grad_norm": 0.44260382652282715,
      "learning_rate": 4.43625e-05,
      "loss": 0.0021,
      "step": 27060
    },
    {
      "epoch": 0.9023333333333333,
      "grad_norm": 0.4519120752811432,
      "learning_rate": 4.436041666666667e-05,
      "loss": 0.0032,
      "step": 27070
    },
    {
      "epoch": 0.9026666666666666,
      "grad_norm": 0.17826321721076965,
      "learning_rate": 4.435833333333333e-05,
      "loss": 0.002,
      "step": 27080
    },
    {
      "epoch": 0.903,
      "grad_norm": 0.17822815477848053,
      "learning_rate": 4.4356250000000004e-05,
      "loss": 0.0033,
      "step": 27090
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.7810311317443848,
      "learning_rate": 4.435416666666667e-05,
      "loss": 0.0017,
      "step": 27100
    },
    {
      "epoch": 0.9036666666666666,
      "grad_norm": 0.234138622879982,
      "learning_rate": 4.4352083333333335e-05,
      "loss": 0.0054,
      "step": 27110
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.178602397441864,
      "learning_rate": 4.435e-05,
      "loss": 0.0035,
      "step": 27120
    },
    {
      "epoch": 0.9043333333333333,
      "grad_norm": 0.17862015962600708,
      "learning_rate": 4.4347916666666666e-05,
      "loss": 0.0019,
      "step": 27130
    },
    {
      "epoch": 0.9046666666666666,
      "grad_norm": 0.26744669675827026,
      "learning_rate": 4.434583333333334e-05,
      "loss": 0.0027,
      "step": 27140
    },
    {
      "epoch": 0.905,
      "grad_norm": 0.26717662811279297,
      "learning_rate": 4.4343750000000004e-05,
      "loss": 0.0028,
      "step": 27150
    },
    {
      "epoch": 0.9053333333333333,
      "grad_norm": 0.09120185673236847,
      "learning_rate": 4.434166666666667e-05,
      "loss": 0.0038,
      "step": 27160
    },
    {
      "epoch": 0.9056666666666666,
      "grad_norm": 0.28286486864089966,
      "learning_rate": 4.4339583333333335e-05,
      "loss": 0.0026,
      "step": 27170
    },
    {
      "epoch": 0.906,
      "grad_norm": 0.24185478687286377,
      "learning_rate": 4.433750000000001e-05,
      "loss": 0.0029,
      "step": 27180
    },
    {
      "epoch": 0.9063333333333333,
      "grad_norm": 0.08916367590427399,
      "learning_rate": 4.4335416666666666e-05,
      "loss": 0.0039,
      "step": 27190
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.2077174186706543,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0031,
      "step": 27200
    },
    {
      "epoch": 0.907,
      "grad_norm": 0.20795099437236786,
      "learning_rate": 4.4331250000000004e-05,
      "loss": 0.0024,
      "step": 27210
    },
    {
      "epoch": 0.9073333333333333,
      "grad_norm": 0.05975769832730293,
      "learning_rate": 4.432916666666667e-05,
      "loss": 0.0032,
      "step": 27220
    },
    {
      "epoch": 0.9076666666666666,
      "grad_norm": 0.46700188517570496,
      "learning_rate": 4.4327083333333334e-05,
      "loss": 0.0026,
      "step": 27230
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.5583869814872742,
      "learning_rate": 4.4325e-05,
      "loss": 0.0018,
      "step": 27240
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.17854025959968567,
      "learning_rate": 4.432291666666667e-05,
      "loss": 0.002,
      "step": 27250
    },
    {
      "epoch": 0.9086666666666666,
      "grad_norm": 0.1485244780778885,
      "learning_rate": 4.432083333333333e-05,
      "loss": 0.0038,
      "step": 27260
    },
    {
      "epoch": 0.909,
      "grad_norm": 0.0605904683470726,
      "learning_rate": 4.431875e-05,
      "loss": 0.0028,
      "step": 27270
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.059688448905944824,
      "learning_rate": 4.431666666666667e-05,
      "loss": 0.0021,
      "step": 27280
    },
    {
      "epoch": 0.9096666666666666,
      "grad_norm": 0.11913493275642395,
      "learning_rate": 4.4314583333333334e-05,
      "loss": 0.0025,
      "step": 27290
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.17850100994110107,
      "learning_rate": 4.43125e-05,
      "loss": 0.0028,
      "step": 27300
    },
    {
      "epoch": 0.9103333333333333,
      "grad_norm": 0.6832664608955383,
      "learning_rate": 4.431041666666667e-05,
      "loss": 0.0028,
      "step": 27310
    },
    {
      "epoch": 0.9106666666666666,
      "grad_norm": 0.17839254438877106,
      "learning_rate": 4.430833333333334e-05,
      "loss": 0.0021,
      "step": 27320
    },
    {
      "epoch": 0.911,
      "grad_norm": 0.3565910756587982,
      "learning_rate": 4.430625e-05,
      "loss": 0.0026,
      "step": 27330
    },
    {
      "epoch": 0.9113333333333333,
      "grad_norm": 0.0051923273131251335,
      "learning_rate": 4.430416666666667e-05,
      "loss": 0.0023,
      "step": 27340
    },
    {
      "epoch": 0.9116666666666666,
      "grad_norm": 0.38626793026924133,
      "learning_rate": 4.4302083333333334e-05,
      "loss": 0.0037,
      "step": 27350
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.2376602739095688,
      "learning_rate": 4.43e-05,
      "loss": 0.0039,
      "step": 27360
    },
    {
      "epoch": 0.9123333333333333,
      "grad_norm": 0.03051227144896984,
      "learning_rate": 4.4297916666666665e-05,
      "loss": 0.0022,
      "step": 27370
    },
    {
      "epoch": 0.9126666666666666,
      "grad_norm": 0.06039046496152878,
      "learning_rate": 4.429583333333334e-05,
      "loss": 0.0024,
      "step": 27380
    },
    {
      "epoch": 0.913,
      "grad_norm": 0.11851947009563446,
      "learning_rate": 4.429375e-05,
      "loss": 0.0026,
      "step": 27390
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.6535330414772034,
      "learning_rate": 4.429166666666667e-05,
      "loss": 0.0028,
      "step": 27400
    },
    {
      "epoch": 0.9136666666666666,
      "grad_norm": 0.007493074517697096,
      "learning_rate": 4.4289583333333334e-05,
      "loss": 0.002,
      "step": 27410
    },
    {
      "epoch": 0.914,
      "grad_norm": 0.5643419027328491,
      "learning_rate": 4.4287500000000006e-05,
      "loss": 0.0027,
      "step": 27420
    },
    {
      "epoch": 0.9143333333333333,
      "grad_norm": 0.17810487747192383,
      "learning_rate": 4.428541666666667e-05,
      "loss": 0.0028,
      "step": 27430
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.1788153052330017,
      "learning_rate": 4.428333333333334e-05,
      "loss": 0.0025,
      "step": 27440
    },
    {
      "epoch": 0.915,
      "grad_norm": 0.4453847408294678,
      "learning_rate": 4.428125e-05,
      "loss": 0.004,
      "step": 27450
    },
    {
      "epoch": 0.9153333333333333,
      "grad_norm": 0.2698404788970947,
      "learning_rate": 4.427916666666667e-05,
      "loss": 0.0016,
      "step": 27460
    },
    {
      "epoch": 0.9156666666666666,
      "grad_norm": 0.1487227976322174,
      "learning_rate": 4.427708333333333e-05,
      "loss": 0.0026,
      "step": 27470
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.500244140625,
      "learning_rate": 4.4275e-05,
      "loss": 0.0027,
      "step": 27480
    },
    {
      "epoch": 0.9163333333333333,
      "grad_norm": 0.11118090152740479,
      "learning_rate": 4.427291666666667e-05,
      "loss": 0.0025,
      "step": 27490
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.059603117406368256,
      "learning_rate": 4.4270833333333337e-05,
      "loss": 0.0034,
      "step": 27500
    },
    {
      "epoch": 0.917,
      "grad_norm": 0.11319532245397568,
      "learning_rate": 4.426875e-05,
      "loss": 0.0032,
      "step": 27510
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.1782950907945633,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0041,
      "step": 27520
    },
    {
      "epoch": 0.9176666666666666,
      "grad_norm": 0.14846400916576385,
      "learning_rate": 4.426458333333334e-05,
      "loss": 0.0027,
      "step": 27530
    },
    {
      "epoch": 0.918,
      "grad_norm": 0.1784358024597168,
      "learning_rate": 4.42625e-05,
      "loss": 0.0027,
      "step": 27540
    },
    {
      "epoch": 0.9183333333333333,
      "grad_norm": 0.2081839144229889,
      "learning_rate": 4.426041666666667e-05,
      "loss": 0.0029,
      "step": 27550
    },
    {
      "epoch": 0.9186666666666666,
      "grad_norm": 0.20806282758712769,
      "learning_rate": 4.4258333333333336e-05,
      "loss": 0.0023,
      "step": 27560
    },
    {
      "epoch": 0.919,
      "grad_norm": 0.11916253715753555,
      "learning_rate": 4.425625e-05,
      "loss": 0.003,
      "step": 27570
    },
    {
      "epoch": 0.9193333333333333,
      "grad_norm": 0.1784173548221588,
      "learning_rate": 4.425416666666667e-05,
      "loss": 0.0021,
      "step": 27580
    },
    {
      "epoch": 0.9196666666666666,
      "grad_norm": 0.08906270563602448,
      "learning_rate": 4.425208333333333e-05,
      "loss": 0.0025,
      "step": 27590
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.20768992602825165,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.0027,
      "step": 27600
    },
    {
      "epoch": 0.9203333333333333,
      "grad_norm": 0.17833489179611206,
      "learning_rate": 4.4247916666666664e-05,
      "loss": 0.0029,
      "step": 27610
    },
    {
      "epoch": 0.9206666666666666,
      "grad_norm": 0.18452048301696777,
      "learning_rate": 4.4245833333333336e-05,
      "loss": 0.003,
      "step": 27620
    },
    {
      "epoch": 0.921,
      "grad_norm": 0.08999651670455933,
      "learning_rate": 4.424375e-05,
      "loss": 0.0024,
      "step": 27630
    },
    {
      "epoch": 0.9213333333333333,
      "grad_norm": 0.4454458951950073,
      "learning_rate": 4.424166666666667e-05,
      "loss": 0.0028,
      "step": 27640
    },
    {
      "epoch": 0.9216666666666666,
      "grad_norm": 0.11928516626358032,
      "learning_rate": 4.423958333333333e-05,
      "loss": 0.002,
      "step": 27650
    },
    {
      "epoch": 0.922,
      "grad_norm": 0.007483953144401312,
      "learning_rate": 4.4237500000000005e-05,
      "loss": 0.0032,
      "step": 27660
    },
    {
      "epoch": 0.9223333333333333,
      "grad_norm": 0.2377505749464035,
      "learning_rate": 4.423541666666667e-05,
      "loss": 0.0025,
      "step": 27670
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.08951570838689804,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0021,
      "step": 27680
    },
    {
      "epoch": 0.923,
      "grad_norm": 0.6226397752761841,
      "learning_rate": 4.423125e-05,
      "loss": 0.0022,
      "step": 27690
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 0.0299371425062418,
      "learning_rate": 4.422916666666667e-05,
      "loss": 0.0026,
      "step": 27700
    },
    {
      "epoch": 0.9236666666666666,
      "grad_norm": 0.44533640146255493,
      "learning_rate": 4.422708333333334e-05,
      "loss": 0.0043,
      "step": 27710
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.38582080602645874,
      "learning_rate": 4.4225e-05,
      "loss": 0.0028,
      "step": 27720
    },
    {
      "epoch": 0.9243333333333333,
      "grad_norm": 0.03097558207809925,
      "learning_rate": 4.422291666666667e-05,
      "loss": 0.003,
      "step": 27730
    },
    {
      "epoch": 0.9246666666666666,
      "grad_norm": 0.207777202129364,
      "learning_rate": 4.4220833333333335e-05,
      "loss": 0.0029,
      "step": 27740
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.5680376887321472,
      "learning_rate": 4.421875e-05,
      "loss": 0.0039,
      "step": 27750
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.11890306323766708,
      "learning_rate": 4.4216666666666666e-05,
      "loss": 0.0018,
      "step": 27760
    },
    {
      "epoch": 0.9256666666666666,
      "grad_norm": 0.14867889881134033,
      "learning_rate": 4.421458333333334e-05,
      "loss": 0.0038,
      "step": 27770
    },
    {
      "epoch": 0.926,
      "grad_norm": 0.14898796379566193,
      "learning_rate": 4.4212500000000004e-05,
      "loss": 0.0021,
      "step": 27780
    },
    {
      "epoch": 0.9263333333333333,
      "grad_norm": 0.17802193760871887,
      "learning_rate": 4.421041666666667e-05,
      "loss": 0.0029,
      "step": 27790
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.3267636001110077,
      "learning_rate": 4.4208333333333335e-05,
      "loss": 0.003,
      "step": 27800
    },
    {
      "epoch": 0.927,
      "grad_norm": 0.03051174432039261,
      "learning_rate": 4.420625000000001e-05,
      "loss": 0.0031,
      "step": 27810
    },
    {
      "epoch": 0.9273333333333333,
      "grad_norm": 0.5347958207130432,
      "learning_rate": 4.4204166666666666e-05,
      "loss": 0.0025,
      "step": 27820
    },
    {
      "epoch": 0.9276666666666666,
      "grad_norm": 0.2969251871109009,
      "learning_rate": 4.420208333333333e-05,
      "loss": 0.0028,
      "step": 27830
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.11926903575658798,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0027,
      "step": 27840
    },
    {
      "epoch": 0.9283333333333333,
      "grad_norm": 0.23836156725883484,
      "learning_rate": 4.419791666666667e-05,
      "loss": 0.0038,
      "step": 27850
    },
    {
      "epoch": 0.9286666666666666,
      "grad_norm": 0.11895201355218887,
      "learning_rate": 4.4195833333333335e-05,
      "loss": 0.0034,
      "step": 27860
    },
    {
      "epoch": 0.929,
      "grad_norm": 0.030032504349946976,
      "learning_rate": 4.419375e-05,
      "loss": 0.0026,
      "step": 27870
    },
    {
      "epoch": 0.9293333333333333,
      "grad_norm": 0.05930570885539055,
      "learning_rate": 4.419166666666667e-05,
      "loss": 0.0024,
      "step": 27880
    },
    {
      "epoch": 0.9296666666666666,
      "grad_norm": 0.12039125710725784,
      "learning_rate": 4.418958333333333e-05,
      "loss": 0.0036,
      "step": 27890
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.35570967197418213,
      "learning_rate": 4.4187500000000003e-05,
      "loss": 0.0034,
      "step": 27900
    },
    {
      "epoch": 0.9303333333333333,
      "grad_norm": 0.41516000032424927,
      "learning_rate": 4.418541666666667e-05,
      "loss": 0.0025,
      "step": 27910
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.36548179388046265,
      "learning_rate": 4.4183333333333334e-05,
      "loss": 0.0023,
      "step": 27920
    },
    {
      "epoch": 0.931,
      "grad_norm": 0.44547292590141296,
      "learning_rate": 4.418125e-05,
      "loss": 0.0041,
      "step": 27930
    },
    {
      "epoch": 0.9313333333333333,
      "grad_norm": 0.0069690486416220665,
      "learning_rate": 4.417916666666667e-05,
      "loss": 0.0033,
      "step": 27940
    },
    {
      "epoch": 0.9316666666666666,
      "grad_norm": 0.23770210146903992,
      "learning_rate": 4.417708333333334e-05,
      "loss": 0.0022,
      "step": 27950
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.06053944677114487,
      "learning_rate": 4.4174999999999996e-05,
      "loss": 0.0028,
      "step": 27960
    },
    {
      "epoch": 0.9323333333333333,
      "grad_norm": 0.0891072154045105,
      "learning_rate": 4.417291666666667e-05,
      "loss": 0.0022,
      "step": 27970
    },
    {
      "epoch": 0.9326666666666666,
      "grad_norm": 0.23733851313591003,
      "learning_rate": 4.4170833333333334e-05,
      "loss": 0.0023,
      "step": 27980
    },
    {
      "epoch": 0.933,
      "grad_norm": 0.26658549904823303,
      "learning_rate": 4.4168750000000006e-05,
      "loss": 0.0017,
      "step": 27990
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.2082851380109787,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0034,
      "step": 28000
    },
    {
      "epoch": 0.9336666666666666,
      "grad_norm": 0.08894294500350952,
      "learning_rate": 4.416458333333334e-05,
      "loss": 0.0036,
      "step": 28010
    },
    {
      "epoch": 0.934,
      "grad_norm": 0.23758819699287415,
      "learning_rate": 4.41625e-05,
      "loss": 0.0021,
      "step": 28020
    },
    {
      "epoch": 0.9343333333333333,
      "grad_norm": 0.6221771240234375,
      "learning_rate": 4.416041666666667e-05,
      "loss": 0.0033,
      "step": 28030
    },
    {
      "epoch": 0.9346666666666666,
      "grad_norm": 0.1786791831254959,
      "learning_rate": 4.4158333333333334e-05,
      "loss": 0.0024,
      "step": 28040
    },
    {
      "epoch": 0.935,
      "grad_norm": 0.1482914388179779,
      "learning_rate": 4.4156250000000006e-05,
      "loss": 0.0025,
      "step": 28050
    },
    {
      "epoch": 0.9353333333333333,
      "grad_norm": 0.32601654529571533,
      "learning_rate": 4.415416666666667e-05,
      "loss": 0.0027,
      "step": 28060
    },
    {
      "epoch": 0.9356666666666666,
      "grad_norm": 0.5130163431167603,
      "learning_rate": 4.415208333333333e-05,
      "loss": 0.0028,
      "step": 28070
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.2674267888069153,
      "learning_rate": 4.415e-05,
      "loss": 0.0029,
      "step": 28080
    },
    {
      "epoch": 0.9363333333333334,
      "grad_norm": 0.44518113136291504,
      "learning_rate": 4.414791666666667e-05,
      "loss": 0.0039,
      "step": 28090
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.2672739028930664,
      "learning_rate": 4.4145833333333333e-05,
      "loss": 0.0026,
      "step": 28100
    },
    {
      "epoch": 0.937,
      "grad_norm": 0.6230546236038208,
      "learning_rate": 4.414375e-05,
      "loss": 0.0028,
      "step": 28110
    },
    {
      "epoch": 0.9373333333333334,
      "grad_norm": 0.14859306812286377,
      "learning_rate": 4.414166666666667e-05,
      "loss": 0.003,
      "step": 28120
    },
    {
      "epoch": 0.9376666666666666,
      "grad_norm": 0.029810871928930283,
      "learning_rate": 4.413958333333334e-05,
      "loss": 0.0023,
      "step": 28130
    },
    {
      "epoch": 0.938,
      "grad_norm": 0.3264125883579254,
      "learning_rate": 4.41375e-05,
      "loss": 0.0023,
      "step": 28140
    },
    {
      "epoch": 0.9383333333333334,
      "grad_norm": 0.20796629786491394,
      "learning_rate": 4.413541666666667e-05,
      "loss": 0.0025,
      "step": 28150
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.11873224377632141,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0024,
      "step": 28160
    },
    {
      "epoch": 0.939,
      "grad_norm": 0.1779879629611969,
      "learning_rate": 4.413125e-05,
      "loss": 0.0027,
      "step": 28170
    },
    {
      "epoch": 0.9393333333333334,
      "grad_norm": 0.06030771881341934,
      "learning_rate": 4.412916666666667e-05,
      "loss": 0.0027,
      "step": 28180
    },
    {
      "epoch": 0.9396666666666667,
      "grad_norm": 0.08912386000156403,
      "learning_rate": 4.4127083333333336e-05,
      "loss": 0.0031,
      "step": 28190
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.47339028120040894,
      "learning_rate": 4.4125e-05,
      "loss": 0.0024,
      "step": 28200
    },
    {
      "epoch": 0.9403333333333334,
      "grad_norm": 0.05977298691868782,
      "learning_rate": 4.412291666666667e-05,
      "loss": 0.0036,
      "step": 28210
    },
    {
      "epoch": 0.9406666666666667,
      "grad_norm": 0.19561231136322021,
      "learning_rate": 4.412083333333333e-05,
      "loss": 0.0032,
      "step": 28220
    },
    {
      "epoch": 0.941,
      "grad_norm": 0.20797792077064514,
      "learning_rate": 4.4118750000000005e-05,
      "loss": 0.0029,
      "step": 28230
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.1485918015241623,
      "learning_rate": 4.411666666666667e-05,
      "loss": 0.0028,
      "step": 28240
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.2373897135257721,
      "learning_rate": 4.4114583333333336e-05,
      "loss": 0.0034,
      "step": 28250
    },
    {
      "epoch": 0.942,
      "grad_norm": 0.17896832525730133,
      "learning_rate": 4.41125e-05,
      "loss": 0.0032,
      "step": 28260
    },
    {
      "epoch": 0.9423333333333334,
      "grad_norm": 0.08958736807107925,
      "learning_rate": 4.4110416666666674e-05,
      "loss": 0.0034,
      "step": 28270
    },
    {
      "epoch": 0.9426666666666667,
      "grad_norm": 0.2375764399766922,
      "learning_rate": 4.410833333333333e-05,
      "loss": 0.002,
      "step": 28280
    },
    {
      "epoch": 0.943,
      "grad_norm": 0.47571390867233276,
      "learning_rate": 4.4106250000000005e-05,
      "loss": 0.0034,
      "step": 28290
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.35595136880874634,
      "learning_rate": 4.410416666666667e-05,
      "loss": 0.0027,
      "step": 28300
    },
    {
      "epoch": 0.9436666666666667,
      "grad_norm": 0.4158424437046051,
      "learning_rate": 4.4102083333333336e-05,
      "loss": 0.003,
      "step": 28310
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.20785795152187347,
      "learning_rate": 4.41e-05,
      "loss": 0.0027,
      "step": 28320
    },
    {
      "epoch": 0.9443333333333334,
      "grad_norm": 0.030499674379825592,
      "learning_rate": 4.409791666666667e-05,
      "loss": 0.0036,
      "step": 28330
    },
    {
      "epoch": 0.9446666666666667,
      "grad_norm": 0.17817474901676178,
      "learning_rate": 4.409583333333334e-05,
      "loss": 0.0023,
      "step": 28340
    },
    {
      "epoch": 0.945,
      "grad_norm": 0.34606802463531494,
      "learning_rate": 4.409375e-05,
      "loss": 0.0023,
      "step": 28350
    },
    {
      "epoch": 0.9453333333333334,
      "grad_norm": 0.030454469844698906,
      "learning_rate": 4.409166666666667e-05,
      "loss": 0.003,
      "step": 28360
    },
    {
      "epoch": 0.9456666666666667,
      "grad_norm": 0.17882394790649414,
      "learning_rate": 4.4089583333333335e-05,
      "loss": 0.0024,
      "step": 28370
    },
    {
      "epoch": 0.946,
      "grad_norm": 0.0888206735253334,
      "learning_rate": 4.40875e-05,
      "loss": 0.0022,
      "step": 28380
    },
    {
      "epoch": 0.9463333333333334,
      "grad_norm": 0.4832479655742645,
      "learning_rate": 4.4085416666666666e-05,
      "loss": 0.0039,
      "step": 28390
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.4153176546096802,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.0038,
      "step": 28400
    },
    {
      "epoch": 0.947,
      "grad_norm": 0.4455499053001404,
      "learning_rate": 4.4081250000000004e-05,
      "loss": 0.003,
      "step": 28410
    },
    {
      "epoch": 0.9473333333333334,
      "grad_norm": 0.23714688420295715,
      "learning_rate": 4.407916666666667e-05,
      "loss": 0.0019,
      "step": 28420
    },
    {
      "epoch": 0.9476666666666667,
      "grad_norm": 0.3267526626586914,
      "learning_rate": 4.4077083333333335e-05,
      "loss": 0.0026,
      "step": 28430
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.6234194040298462,
      "learning_rate": 4.4075e-05,
      "loss": 0.0021,
      "step": 28440
    },
    {
      "epoch": 0.9483333333333334,
      "grad_norm": 0.17832159996032715,
      "learning_rate": 4.4072916666666666e-05,
      "loss": 0.0026,
      "step": 28450
    },
    {
      "epoch": 0.9486666666666667,
      "grad_norm": 0.5043085813522339,
      "learning_rate": 4.407083333333333e-05,
      "loss": 0.0019,
      "step": 28460
    },
    {
      "epoch": 0.949,
      "grad_norm": 0.32639840245246887,
      "learning_rate": 4.4068750000000004e-05,
      "loss": 0.0023,
      "step": 28470
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.4074876308441162,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0042,
      "step": 28480
    },
    {
      "epoch": 0.9496666666666667,
      "grad_norm": 0.08896209299564362,
      "learning_rate": 4.4064583333333335e-05,
      "loss": 0.0032,
      "step": 28490
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.4467523694038391,
      "learning_rate": 4.40625e-05,
      "loss": 0.0037,
      "step": 28500
    },
    {
      "epoch": 0.9503333333333334,
      "grad_norm": 0.04880770295858383,
      "learning_rate": 4.406041666666667e-05,
      "loss": 0.0024,
      "step": 28510
    },
    {
      "epoch": 0.9506666666666667,
      "grad_norm": 0.6228517293930054,
      "learning_rate": 4.405833333333334e-05,
      "loss": 0.0035,
      "step": 28520
    },
    {
      "epoch": 0.951,
      "grad_norm": 0.03043053299188614,
      "learning_rate": 4.4056250000000004e-05,
      "loss": 0.0021,
      "step": 28530
    },
    {
      "epoch": 0.9513333333333334,
      "grad_norm": 0.06010599806904793,
      "learning_rate": 4.405416666666667e-05,
      "loss": 0.0026,
      "step": 28540
    },
    {
      "epoch": 0.9516666666666667,
      "grad_norm": 0.6228323578834534,
      "learning_rate": 4.4052083333333335e-05,
      "loss": 0.0028,
      "step": 28550
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.17799526453018188,
      "learning_rate": 4.405e-05,
      "loss": 0.0025,
      "step": 28560
    },
    {
      "epoch": 0.9523333333333334,
      "grad_norm": 0.1783192902803421,
      "learning_rate": 4.4047916666666666e-05,
      "loss": 0.0022,
      "step": 28570
    },
    {
      "epoch": 0.9526666666666667,
      "grad_norm": 0.2964625358581543,
      "learning_rate": 4.404583333333334e-05,
      "loss": 0.0023,
      "step": 28580
    },
    {
      "epoch": 0.953,
      "grad_norm": 0.11870910227298737,
      "learning_rate": 4.404375e-05,
      "loss": 0.0018,
      "step": 28590
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.11905581504106522,
      "learning_rate": 4.404166666666667e-05,
      "loss": 0.0026,
      "step": 28600
    },
    {
      "epoch": 0.9536666666666667,
      "grad_norm": 0.08930344879627228,
      "learning_rate": 4.4039583333333334e-05,
      "loss": 0.0021,
      "step": 28610
    },
    {
      "epoch": 0.954,
      "grad_norm": 0.08885069191455841,
      "learning_rate": 4.4037500000000007e-05,
      "loss": 0.0025,
      "step": 28620
    },
    {
      "epoch": 0.9543333333333334,
      "grad_norm": 0.059947334229946136,
      "learning_rate": 4.4035416666666665e-05,
      "loss": 0.0027,
      "step": 28630
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.005902701523154974,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.0026,
      "step": 28640
    },
    {
      "epoch": 0.955,
      "grad_norm": 0.4446551203727722,
      "learning_rate": 4.403125e-05,
      "loss": 0.0033,
      "step": 28650
    },
    {
      "epoch": 0.9553333333333334,
      "grad_norm": 0.1778138428926468,
      "learning_rate": 4.402916666666667e-05,
      "loss": 0.0028,
      "step": 28660
    },
    {
      "epoch": 0.9556666666666667,
      "grad_norm": 0.3854597806930542,
      "learning_rate": 4.4027083333333334e-05,
      "loss": 0.0028,
      "step": 28670
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.06139811873435974,
      "learning_rate": 4.4025e-05,
      "loss": 0.0027,
      "step": 28680
    },
    {
      "epoch": 0.9563333333333334,
      "grad_norm": 0.5637004375457764,
      "learning_rate": 4.402291666666667e-05,
      "loss": 0.0034,
      "step": 28690
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.11898361891508102,
      "learning_rate": 4.402083333333333e-05,
      "loss": 0.0029,
      "step": 28700
    },
    {
      "epoch": 0.957,
      "grad_norm": 0.5933594107627869,
      "learning_rate": 4.401875e-05,
      "loss": 0.0027,
      "step": 28710
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.11919940263032913,
      "learning_rate": 4.401666666666667e-05,
      "loss": 0.0026,
      "step": 28720
    },
    {
      "epoch": 0.9576666666666667,
      "grad_norm": 0.004645865876227617,
      "learning_rate": 4.4014583333333334e-05,
      "loss": 0.0017,
      "step": 28730
    },
    {
      "epoch": 0.958,
      "grad_norm": 0.03100602515041828,
      "learning_rate": 4.40125e-05,
      "loss": 0.0021,
      "step": 28740
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.23723694682121277,
      "learning_rate": 4.401041666666667e-05,
      "loss": 0.0029,
      "step": 28750
    },
    {
      "epoch": 0.9586666666666667,
      "grad_norm": 0.005874902941286564,
      "learning_rate": 4.400833333333334e-05,
      "loss": 0.003,
      "step": 28760
    },
    {
      "epoch": 0.959,
      "grad_norm": 0.030538031831383705,
      "learning_rate": 4.400625e-05,
      "loss": 0.0036,
      "step": 28770
    },
    {
      "epoch": 0.9593333333333334,
      "grad_norm": 0.1777516007423401,
      "learning_rate": 4.400416666666667e-05,
      "loss": 0.0034,
      "step": 28780
    },
    {
      "epoch": 0.9596666666666667,
      "grad_norm": 0.5337690114974976,
      "learning_rate": 4.400208333333334e-05,
      "loss": 0.0027,
      "step": 28790
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.3850797116756439,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0029,
      "step": 28800
    },
    {
      "epoch": 0.9603333333333334,
      "grad_norm": 0.11884836107492447,
      "learning_rate": 4.3997916666666664e-05,
      "loss": 0.0027,
      "step": 28810
    },
    {
      "epoch": 0.9606666666666667,
      "grad_norm": 0.2373378574848175,
      "learning_rate": 4.3995833333333337e-05,
      "loss": 0.0022,
      "step": 28820
    },
    {
      "epoch": 0.961,
      "grad_norm": 0.08926887065172195,
      "learning_rate": 4.399375e-05,
      "loss": 0.0035,
      "step": 28830
    },
    {
      "epoch": 0.9613333333333334,
      "grad_norm": 0.14827322959899902,
      "learning_rate": 4.399166666666667e-05,
      "loss": 0.0021,
      "step": 28840
    },
    {
      "epoch": 0.9616666666666667,
      "grad_norm": 0.23698818683624268,
      "learning_rate": 4.398958333333333e-05,
      "loss": 0.0037,
      "step": 28850
    },
    {
      "epoch": 0.962,
      "grad_norm": 0.14784276485443115,
      "learning_rate": 4.3987500000000005e-05,
      "loss": 0.0022,
      "step": 28860
    },
    {
      "epoch": 0.9623333333333334,
      "grad_norm": 0.013311007060110569,
      "learning_rate": 4.398541666666667e-05,
      "loss": 0.0026,
      "step": 28870
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.3015880882740021,
      "learning_rate": 4.3983333333333336e-05,
      "loss": 0.0044,
      "step": 28880
    },
    {
      "epoch": 0.963,
      "grad_norm": 0.11882025003433228,
      "learning_rate": 4.398125e-05,
      "loss": 0.0029,
      "step": 28890
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 0.10402277112007141,
      "learning_rate": 4.3979166666666674e-05,
      "loss": 0.0035,
      "step": 28900
    },
    {
      "epoch": 0.9636666666666667,
      "grad_norm": 0.7925308346748352,
      "learning_rate": 4.397708333333333e-05,
      "loss": 0.0036,
      "step": 28910
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.44513973593711853,
      "learning_rate": 4.3975e-05,
      "loss": 0.0027,
      "step": 28920
    },
    {
      "epoch": 0.9643333333333334,
      "grad_norm": 0.23774877190589905,
      "learning_rate": 4.397291666666667e-05,
      "loss": 0.0035,
      "step": 28930
    },
    {
      "epoch": 0.9646666666666667,
      "grad_norm": 0.08327241986989975,
      "learning_rate": 4.3970833333333336e-05,
      "loss": 0.0031,
      "step": 28940
    },
    {
      "epoch": 0.965,
      "grad_norm": 0.03033846989274025,
      "learning_rate": 4.396875e-05,
      "loss": 0.0025,
      "step": 28950
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.2966917157173157,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0031,
      "step": 28960
    },
    {
      "epoch": 0.9656666666666667,
      "grad_norm": 0.212783083319664,
      "learning_rate": 4.396458333333334e-05,
      "loss": 0.0039,
      "step": 28970
    },
    {
      "epoch": 0.966,
      "grad_norm": 0.05956537649035454,
      "learning_rate": 4.39625e-05,
      "loss": 0.0039,
      "step": 28980
    },
    {
      "epoch": 0.9663333333333334,
      "grad_norm": 0.11879510432481766,
      "learning_rate": 4.396041666666667e-05,
      "loss": 0.0025,
      "step": 28990
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.4741247594356537,
      "learning_rate": 4.3958333333333336e-05,
      "loss": 0.0037,
      "step": 29000
    },
    {
      "epoch": 0.967,
      "grad_norm": 0.5336823463439941,
      "learning_rate": 4.395625e-05,
      "loss": 0.002,
      "step": 29010
    },
    {
      "epoch": 0.9673333333333334,
      "grad_norm": 0.5496200323104858,
      "learning_rate": 4.395416666666667e-05,
      "loss": 0.0028,
      "step": 29020
    },
    {
      "epoch": 0.9676666666666667,
      "grad_norm": 0.009827754460275173,
      "learning_rate": 4.395208333333334e-05,
      "loss": 0.0028,
      "step": 29030
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.20762209594249725,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 0.0032,
      "step": 29040
    },
    {
      "epoch": 0.9683333333333334,
      "grad_norm": 0.20734356343746185,
      "learning_rate": 4.394791666666666e-05,
      "loss": 0.0026,
      "step": 29050
    },
    {
      "epoch": 0.9686666666666667,
      "grad_norm": 0.4356175363063812,
      "learning_rate": 4.3945833333333335e-05,
      "loss": 0.0027,
      "step": 29060
    },
    {
      "epoch": 0.969,
      "grad_norm": 0.26719748973846436,
      "learning_rate": 4.394375e-05,
      "loss": 0.0015,
      "step": 29070
    },
    {
      "epoch": 0.9693333333333334,
      "grad_norm": 0.030079107731580734,
      "learning_rate": 4.394166666666667e-05,
      "loss": 0.0033,
      "step": 29080
    },
    {
      "epoch": 0.9696666666666667,
      "grad_norm": 0.060530174523591995,
      "learning_rate": 4.393958333333333e-05,
      "loss": 0.0034,
      "step": 29090
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.08985310792922974,
      "learning_rate": 4.3937500000000004e-05,
      "loss": 0.0023,
      "step": 29100
    },
    {
      "epoch": 0.9703333333333334,
      "grad_norm": 0.2845270335674286,
      "learning_rate": 4.393541666666667e-05,
      "loss": 0.0025,
      "step": 29110
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.2074975073337555,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0021,
      "step": 29120
    },
    {
      "epoch": 0.971,
      "grad_norm": 0.14873084425926208,
      "learning_rate": 4.393125e-05,
      "loss": 0.0024,
      "step": 29130
    },
    {
      "epoch": 0.9713333333333334,
      "grad_norm": 0.1782018542289734,
      "learning_rate": 4.392916666666667e-05,
      "loss": 0.0023,
      "step": 29140
    },
    {
      "epoch": 0.9716666666666667,
      "grad_norm": 0.11923198401927948,
      "learning_rate": 4.392708333333334e-05,
      "loss": 0.0025,
      "step": 29150
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.2665751278400421,
      "learning_rate": 4.3925e-05,
      "loss": 0.0038,
      "step": 29160
    },
    {
      "epoch": 0.9723333333333334,
      "grad_norm": 0.089292012155056,
      "learning_rate": 4.392291666666667e-05,
      "loss": 0.0032,
      "step": 29170
    },
    {
      "epoch": 0.9726666666666667,
      "grad_norm": 0.3852119743824005,
      "learning_rate": 4.3920833333333335e-05,
      "loss": 0.0024,
      "step": 29180
    },
    {
      "epoch": 0.973,
      "grad_norm": 0.006133696064352989,
      "learning_rate": 4.391875e-05,
      "loss": 0.0038,
      "step": 29190
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.4449526071548462,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.0023,
      "step": 29200
    },
    {
      "epoch": 0.9736666666666667,
      "grad_norm": 0.8948394656181335,
      "learning_rate": 4.391458333333334e-05,
      "loss": 0.0035,
      "step": 29210
    },
    {
      "epoch": 0.974,
      "grad_norm": 0.6816076636314392,
      "learning_rate": 4.3912500000000004e-05,
      "loss": 0.0021,
      "step": 29220
    },
    {
      "epoch": 0.9743333333333334,
      "grad_norm": 0.4444708228111267,
      "learning_rate": 4.391041666666667e-05,
      "loss": 0.0023,
      "step": 29230
    },
    {
      "epoch": 0.9746666666666667,
      "grad_norm": 0.14848847687244415,
      "learning_rate": 4.3908333333333334e-05,
      "loss": 0.0024,
      "step": 29240
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.17799851298332214,
      "learning_rate": 4.390625000000001e-05,
      "loss": 0.0027,
      "step": 29250
    },
    {
      "epoch": 0.9753333333333334,
      "grad_norm": 0.30579349398612976,
      "learning_rate": 4.3904166666666665e-05,
      "loss": 0.003,
      "step": 29260
    },
    {
      "epoch": 0.9756666666666667,
      "grad_norm": 0.045817140489816666,
      "learning_rate": 4.390208333333334e-05,
      "loss": 0.0028,
      "step": 29270
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.3853847086429596,
      "learning_rate": 4.39e-05,
      "loss": 0.0032,
      "step": 29280
    },
    {
      "epoch": 0.9763333333333334,
      "grad_norm": 0.08896750956773758,
      "learning_rate": 4.389791666666667e-05,
      "loss": 0.0018,
      "step": 29290
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 0.14859311282634735,
      "learning_rate": 4.3895833333333334e-05,
      "loss": 0.0032,
      "step": 29300
    },
    {
      "epoch": 0.977,
      "grad_norm": 0.23725678026676178,
      "learning_rate": 4.389375e-05,
      "loss": 0.0017,
      "step": 29310
    },
    {
      "epoch": 0.9773333333333334,
      "grad_norm": 0.08908344805240631,
      "learning_rate": 4.389166666666667e-05,
      "loss": 0.0016,
      "step": 29320
    },
    {
      "epoch": 0.9776666666666667,
      "grad_norm": 0.29736611247062683,
      "learning_rate": 4.388958333333333e-05,
      "loss": 0.0033,
      "step": 29330
    },
    {
      "epoch": 0.978,
      "grad_norm": 0.08900098502635956,
      "learning_rate": 4.38875e-05,
      "loss": 0.0038,
      "step": 29340
    },
    {
      "epoch": 0.9783333333333334,
      "grad_norm": 0.2967855632305145,
      "learning_rate": 4.388541666666667e-05,
      "loss": 0.0021,
      "step": 29350
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.23719041049480438,
      "learning_rate": 4.388333333333334e-05,
      "loss": 0.0036,
      "step": 29360
    },
    {
      "epoch": 0.979,
      "grad_norm": 0.14840976893901825,
      "learning_rate": 4.388125e-05,
      "loss": 0.0033,
      "step": 29370
    },
    {
      "epoch": 0.9793333333333333,
      "grad_norm": 0.05929078534245491,
      "learning_rate": 4.387916666666667e-05,
      "loss": 0.0028,
      "step": 29380
    },
    {
      "epoch": 0.9796666666666667,
      "grad_norm": 0.20758262276649475,
      "learning_rate": 4.387708333333334e-05,
      "loss": 0.0028,
      "step": 29390
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.148268923163414,
      "learning_rate": 4.3875e-05,
      "loss": 0.0028,
      "step": 29400
    },
    {
      "epoch": 0.9803333333333333,
      "grad_norm": 0.030014606192708015,
      "learning_rate": 4.387291666666667e-05,
      "loss": 0.0037,
      "step": 29410
    },
    {
      "epoch": 0.9806666666666667,
      "grad_norm": 0.030630599707365036,
      "learning_rate": 4.3870833333333334e-05,
      "loss": 0.0055,
      "step": 29420
    },
    {
      "epoch": 0.981,
      "grad_norm": 0.4731604754924774,
      "learning_rate": 4.3868750000000006e-05,
      "loss": 0.0028,
      "step": 29430
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.3855167627334595,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0023,
      "step": 29440
    },
    {
      "epoch": 0.9816666666666667,
      "grad_norm": 0.2454574853181839,
      "learning_rate": 4.386458333333334e-05,
      "loss": 0.0037,
      "step": 29450
    },
    {
      "epoch": 0.982,
      "grad_norm": 0.006006509531289339,
      "learning_rate": 4.38625e-05,
      "loss": 0.0021,
      "step": 29460
    },
    {
      "epoch": 0.9823333333333333,
      "grad_norm": 0.11929292976856232,
      "learning_rate": 4.386041666666667e-05,
      "loss": 0.0034,
      "step": 29470
    },
    {
      "epoch": 0.9826666666666667,
      "grad_norm": 0.8750891089439392,
      "learning_rate": 4.385833333333333e-05,
      "loss": 0.003,
      "step": 29480
    },
    {
      "epoch": 0.983,
      "grad_norm": 0.14843522012233734,
      "learning_rate": 4.3856250000000006e-05,
      "loss": 0.0031,
      "step": 29490
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.4148258566856384,
      "learning_rate": 4.385416666666667e-05,
      "loss": 0.0021,
      "step": 29500
    },
    {
      "epoch": 0.9836666666666667,
      "grad_norm": 0.03049449436366558,
      "learning_rate": 4.3852083333333336e-05,
      "loss": 0.0035,
      "step": 29510
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.40591010451316833,
      "learning_rate": 4.385e-05,
      "loss": 0.0033,
      "step": 29520
    },
    {
      "epoch": 0.9843333333333333,
      "grad_norm": 0.2965899407863617,
      "learning_rate": 4.384791666666667e-05,
      "loss": 0.0033,
      "step": 29530
    },
    {
      "epoch": 0.9846666666666667,
      "grad_norm": 0.14910788834095,
      "learning_rate": 4.384583333333333e-05,
      "loss": 0.0028,
      "step": 29540
    },
    {
      "epoch": 0.985,
      "grad_norm": 0.6806474924087524,
      "learning_rate": 4.384375e-05,
      "loss": 0.0039,
      "step": 29550
    },
    {
      "epoch": 0.9853333333333333,
      "grad_norm": 0.08947868645191193,
      "learning_rate": 4.384166666666667e-05,
      "loss": 0.0046,
      "step": 29560
    },
    {
      "epoch": 0.9856666666666667,
      "grad_norm": 0.20763947069644928,
      "learning_rate": 4.3839583333333336e-05,
      "loss": 0.0024,
      "step": 29570
    },
    {
      "epoch": 0.986,
      "grad_norm": 0.029973570257425308,
      "learning_rate": 4.38375e-05,
      "loss": 0.0028,
      "step": 29580
    },
    {
      "epoch": 0.9863333333333333,
      "grad_norm": 0.2966158390045166,
      "learning_rate": 4.383541666666667e-05,
      "loss": 0.0028,
      "step": 29590
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.08986148238182068,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.003,
      "step": 29600
    },
    {
      "epoch": 0.987,
      "grad_norm": 0.35609766840934753,
      "learning_rate": 4.383125e-05,
      "loss": 0.0023,
      "step": 29610
    },
    {
      "epoch": 0.9873333333333333,
      "grad_norm": 0.38553446531295776,
      "learning_rate": 4.382916666666667e-05,
      "loss": 0.0025,
      "step": 29620
    },
    {
      "epoch": 0.9876666666666667,
      "grad_norm": 0.23752130568027496,
      "learning_rate": 4.3827083333333336e-05,
      "loss": 0.0021,
      "step": 29630
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.03146688640117645,
      "learning_rate": 4.3825e-05,
      "loss": 0.0029,
      "step": 29640
    },
    {
      "epoch": 0.9883333333333333,
      "grad_norm": 0.089528888463974,
      "learning_rate": 4.382291666666667e-05,
      "loss": 0.0025,
      "step": 29650
    },
    {
      "epoch": 0.9886666666666667,
      "grad_norm": 0.05950654670596123,
      "learning_rate": 4.382083333333333e-05,
      "loss": 0.0019,
      "step": 29660
    },
    {
      "epoch": 0.989,
      "grad_norm": 0.3563072085380554,
      "learning_rate": 4.3818750000000005e-05,
      "loss": 0.0036,
      "step": 29670
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.20748768746852875,
      "learning_rate": 4.381666666666667e-05,
      "loss": 0.0024,
      "step": 29680
    },
    {
      "epoch": 0.9896666666666667,
      "grad_norm": 0.004833804443478584,
      "learning_rate": 4.3814583333333336e-05,
      "loss": 0.0032,
      "step": 29690
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.11870744079351425,
      "learning_rate": 4.38125e-05,
      "loss": 0.0029,
      "step": 29700
    },
    {
      "epoch": 0.9903333333333333,
      "grad_norm": 0.14815907180309296,
      "learning_rate": 4.381041666666667e-05,
      "loss": 0.0024,
      "step": 29710
    },
    {
      "epoch": 0.9906666666666667,
      "grad_norm": 0.415294885635376,
      "learning_rate": 4.380833333333333e-05,
      "loss": 0.0022,
      "step": 29720
    },
    {
      "epoch": 0.991,
      "grad_norm": 0.3266003131866455,
      "learning_rate": 4.3806250000000004e-05,
      "loss": 0.0033,
      "step": 29730
    },
    {
      "epoch": 0.9913333333333333,
      "grad_norm": 0.20773547887802124,
      "learning_rate": 4.380416666666667e-05,
      "loss": 0.0032,
      "step": 29740
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.20787987112998962,
      "learning_rate": 4.3802083333333335e-05,
      "loss": 0.0034,
      "step": 29750
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.2077162265777588,
      "learning_rate": 4.38e-05,
      "loss": 0.0026,
      "step": 29760
    },
    {
      "epoch": 0.9923333333333333,
      "grad_norm": 0.14861857891082764,
      "learning_rate": 4.3797916666666666e-05,
      "loss": 0.0026,
      "step": 29770
    },
    {
      "epoch": 0.9926666666666667,
      "grad_norm": 0.38602256774902344,
      "learning_rate": 4.379583333333334e-05,
      "loss": 0.0033,
      "step": 29780
    },
    {
      "epoch": 0.993,
      "grad_norm": 0.059899430721998215,
      "learning_rate": 4.379375e-05,
      "loss": 0.0022,
      "step": 29790
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.16019004583358765,
      "learning_rate": 4.379166666666667e-05,
      "loss": 0.0022,
      "step": 29800
    },
    {
      "epoch": 0.9936666666666667,
      "grad_norm": 0.5176617503166199,
      "learning_rate": 4.3789583333333335e-05,
      "loss": 0.0018,
      "step": 29810
    },
    {
      "epoch": 0.994,
      "grad_norm": 0.060162197798490524,
      "learning_rate": 4.37875e-05,
      "loss": 0.0033,
      "step": 29820
    },
    {
      "epoch": 0.9943333333333333,
      "grad_norm": 0.47445234656333923,
      "learning_rate": 4.3785416666666666e-05,
      "loss": 0.0022,
      "step": 29830
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.2670220732688904,
      "learning_rate": 4.378333333333334e-05,
      "loss": 0.0043,
      "step": 29840
    },
    {
      "epoch": 0.995,
      "grad_norm": 0.2666660249233246,
      "learning_rate": 4.3781250000000004e-05,
      "loss": 0.0025,
      "step": 29850
    },
    {
      "epoch": 0.9953333333333333,
      "grad_norm": 0.24123230576515198,
      "learning_rate": 4.377916666666667e-05,
      "loss": 0.0025,
      "step": 29860
    },
    {
      "epoch": 0.9956666666666667,
      "grad_norm": 0.1488235592842102,
      "learning_rate": 4.3777083333333335e-05,
      "loss": 0.0039,
      "step": 29870
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.030664047226309776,
      "learning_rate": 4.3775e-05,
      "loss": 0.0027,
      "step": 29880
    },
    {
      "epoch": 0.9963333333333333,
      "grad_norm": 0.6141971349716187,
      "learning_rate": 4.3772916666666666e-05,
      "loss": 0.0028,
      "step": 29890
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.44497138261795044,
      "learning_rate": 4.377083333333333e-05,
      "loss": 0.002,
      "step": 29900
    },
    {
      "epoch": 0.997,
      "grad_norm": 0.0595288947224617,
      "learning_rate": 4.3768750000000003e-05,
      "loss": 0.0032,
      "step": 29910
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.5037286877632141,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0026,
      "step": 29920
    },
    {
      "epoch": 0.9976666666666667,
      "grad_norm": 0.3851604759693146,
      "learning_rate": 4.3764583333333334e-05,
      "loss": 0.0025,
      "step": 29930
    },
    {
      "epoch": 0.998,
      "grad_norm": 0.031524475663900375,
      "learning_rate": 4.37625e-05,
      "loss": 0.0026,
      "step": 29940
    },
    {
      "epoch": 0.9983333333333333,
      "grad_norm": 0.2663378119468689,
      "learning_rate": 4.376041666666667e-05,
      "loss": 0.0022,
      "step": 29950
    },
    {
      "epoch": 0.9986666666666667,
      "grad_norm": 0.0598716102540493,
      "learning_rate": 4.375833333333334e-05,
      "loss": 0.0026,
      "step": 29960
    },
    {
      "epoch": 0.999,
      "grad_norm": 0.29647570848464966,
      "learning_rate": 4.375625e-05,
      "loss": 0.0022,
      "step": 29970
    },
    {
      "epoch": 0.9993333333333333,
      "grad_norm": 0.030392205342650414,
      "learning_rate": 4.375416666666667e-05,
      "loss": 0.0032,
      "step": 29980
    },
    {
      "epoch": 0.9996666666666667,
      "grad_norm": 0.08959333598613739,
      "learning_rate": 4.375208333333334e-05,
      "loss": 0.0016,
      "step": 29990
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.32593804597854614,
      "learning_rate": 4.375e-05,
      "loss": 0.0035,
      "step": 30000
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0024574368726462126,
      "eval_runtime": 129.084,
      "eval_samples_per_second": 1549.378,
      "eval_steps_per_second": 38.734,
      "step": 30000
    },
    {
      "epoch": 1.0003333333333333,
      "grad_norm": 0.44488587975502014,
      "learning_rate": 4.3747916666666665e-05,
      "loss": 0.0042,
      "step": 30010
    },
    {
      "epoch": 1.0006666666666666,
      "grad_norm": 0.2966344356536865,
      "learning_rate": 4.374583333333334e-05,
      "loss": 0.0027,
      "step": 30020
    },
    {
      "epoch": 1.001,
      "grad_norm": 0.11855438351631165,
      "learning_rate": 4.374375e-05,
      "loss": 0.0015,
      "step": 30030
    },
    {
      "epoch": 1.0013333333333334,
      "grad_norm": 0.41567957401275635,
      "learning_rate": 4.374166666666667e-05,
      "loss": 0.003,
      "step": 30040
    },
    {
      "epoch": 1.0016666666666667,
      "grad_norm": 0.4744091033935547,
      "learning_rate": 4.3739583333333334e-05,
      "loss": 0.0023,
      "step": 30050
    },
    {
      "epoch": 1.002,
      "grad_norm": 0.031479187309741974,
      "learning_rate": 4.3737500000000006e-05,
      "loss": 0.0028,
      "step": 30060
    },
    {
      "epoch": 1.0023333333333333,
      "grad_norm": 0.8706613183021545,
      "learning_rate": 4.3735416666666665e-05,
      "loss": 0.003,
      "step": 30070
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.208566814661026,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0028,
      "step": 30080
    },
    {
      "epoch": 1.003,
      "grad_norm": 0.0890434980392456,
      "learning_rate": 4.373125e-05,
      "loss": 0.0035,
      "step": 30090
    },
    {
      "epoch": 1.0033333333333334,
      "grad_norm": 0.17834731936454773,
      "learning_rate": 4.372916666666667e-05,
      "loss": 0.0033,
      "step": 30100
    },
    {
      "epoch": 1.0036666666666667,
      "grad_norm": 0.6243354678153992,
      "learning_rate": 4.3727083333333333e-05,
      "loss": 0.0028,
      "step": 30110
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.005276830401271582,
      "learning_rate": 4.3725000000000006e-05,
      "loss": 0.0035,
      "step": 30120
    },
    {
      "epoch": 1.0043333333333333,
      "grad_norm": 0.12607255578041077,
      "learning_rate": 4.372291666666667e-05,
      "loss": 0.005,
      "step": 30130
    },
    {
      "epoch": 1.0046666666666666,
      "grad_norm": 0.08923594653606415,
      "learning_rate": 4.372083333333333e-05,
      "loss": 0.0026,
      "step": 30140
    },
    {
      "epoch": 1.005,
      "grad_norm": 0.11870425939559937,
      "learning_rate": 4.371875e-05,
      "loss": 0.0037,
      "step": 30150
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.11841154098510742,
      "learning_rate": 4.371666666666667e-05,
      "loss": 0.0023,
      "step": 30160
    },
    {
      "epoch": 1.0056666666666667,
      "grad_norm": 0.3494410514831543,
      "learning_rate": 4.371458333333334e-05,
      "loss": 0.0024,
      "step": 30170
    },
    {
      "epoch": 1.006,
      "grad_norm": 0.1189185306429863,
      "learning_rate": 4.37125e-05,
      "loss": 0.0029,
      "step": 30180
    },
    {
      "epoch": 1.0063333333333333,
      "grad_norm": 0.38566797971725464,
      "learning_rate": 4.371041666666667e-05,
      "loss": 0.0031,
      "step": 30190
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.44484931230545044,
      "learning_rate": 4.3708333333333336e-05,
      "loss": 0.0015,
      "step": 30200
    },
    {
      "epoch": 1.007,
      "grad_norm": 0.8302298188209534,
      "learning_rate": 4.370625e-05,
      "loss": 0.0016,
      "step": 30210
    },
    {
      "epoch": 1.0073333333333334,
      "grad_norm": 0.00779620511457324,
      "learning_rate": 4.370416666666667e-05,
      "loss": 0.0018,
      "step": 30220
    },
    {
      "epoch": 1.0076666666666667,
      "grad_norm": 0.1790233850479126,
      "learning_rate": 4.370208333333334e-05,
      "loss": 0.0036,
      "step": 30230
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.3257981836795807,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.003,
      "step": 30240
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 0.13434630632400513,
      "learning_rate": 4.3697916666666664e-05,
      "loss": 0.0029,
      "step": 30250
    },
    {
      "epoch": 1.0086666666666666,
      "grad_norm": 0.44459572434425354,
      "learning_rate": 4.3695833333333336e-05,
      "loss": 0.0035,
      "step": 30260
    },
    {
      "epoch": 1.009,
      "grad_norm": 0.8621795773506165,
      "learning_rate": 4.369375e-05,
      "loss": 0.0024,
      "step": 30270
    },
    {
      "epoch": 1.0093333333333334,
      "grad_norm": 0.030338561162352562,
      "learning_rate": 4.369166666666667e-05,
      "loss": 0.002,
      "step": 30280
    },
    {
      "epoch": 1.0096666666666667,
      "grad_norm": 0.7112953662872314,
      "learning_rate": 4.368958333333333e-05,
      "loss": 0.0024,
      "step": 30290
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.1779298335313797,
      "learning_rate": 4.3687500000000005e-05,
      "loss": 0.003,
      "step": 30300
    },
    {
      "epoch": 1.0103333333333333,
      "grad_norm": 0.38558945059776306,
      "learning_rate": 4.368541666666667e-05,
      "loss": 0.0037,
      "step": 30310
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.1782149374485016,
      "learning_rate": 4.3683333333333336e-05,
      "loss": 0.0025,
      "step": 30320
    },
    {
      "epoch": 1.011,
      "grad_norm": 0.029729416593909264,
      "learning_rate": 4.368125e-05,
      "loss": 0.0032,
      "step": 30330
    },
    {
      "epoch": 1.0113333333333334,
      "grad_norm": 0.9387956857681274,
      "learning_rate": 4.3679166666666674e-05,
      "loss": 0.003,
      "step": 30340
    },
    {
      "epoch": 1.0116666666666667,
      "grad_norm": 0.7234084010124207,
      "learning_rate": 4.367708333333333e-05,
      "loss": 0.0028,
      "step": 30350
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.20736812055110931,
      "learning_rate": 4.3675000000000005e-05,
      "loss": 0.0039,
      "step": 30360
    },
    {
      "epoch": 1.0123333333333333,
      "grad_norm": 0.05944424867630005,
      "learning_rate": 4.367291666666667e-05,
      "loss": 0.0021,
      "step": 30370
    },
    {
      "epoch": 1.0126666666666666,
      "grad_norm": 0.14810270071029663,
      "learning_rate": 4.3670833333333335e-05,
      "loss": 0.0031,
      "step": 30380
    },
    {
      "epoch": 1.013,
      "grad_norm": 0.20705044269561768,
      "learning_rate": 4.366875e-05,
      "loss": 0.0024,
      "step": 30390
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.9837324619293213,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0041,
      "step": 30400
    },
    {
      "epoch": 1.0136666666666667,
      "grad_norm": 0.32607710361480713,
      "learning_rate": 4.366458333333334e-05,
      "loss": 0.0037,
      "step": 30410
    },
    {
      "epoch": 1.014,
      "grad_norm": 0.03010876290500164,
      "learning_rate": 4.36625e-05,
      "loss": 0.004,
      "step": 30420
    },
    {
      "epoch": 1.0143333333333333,
      "grad_norm": 0.2075783610343933,
      "learning_rate": 4.366041666666667e-05,
      "loss": 0.0021,
      "step": 30430
    },
    {
      "epoch": 1.0146666666666666,
      "grad_norm": 0.1983756124973297,
      "learning_rate": 4.3658333333333335e-05,
      "loss": 0.0038,
      "step": 30440
    },
    {
      "epoch": 1.015,
      "grad_norm": 0.088838130235672,
      "learning_rate": 4.365625000000001e-05,
      "loss": 0.0021,
      "step": 30450
    },
    {
      "epoch": 1.0153333333333334,
      "grad_norm": 0.05958627909421921,
      "learning_rate": 4.3654166666666666e-05,
      "loss": 0.0034,
      "step": 30460
    },
    {
      "epoch": 1.0156666666666667,
      "grad_norm": 0.05990051478147507,
      "learning_rate": 4.365208333333334e-05,
      "loss": 0.0017,
      "step": 30470
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.20767174661159515,
      "learning_rate": 4.3650000000000004e-05,
      "loss": 0.0028,
      "step": 30480
    },
    {
      "epoch": 1.0163333333333333,
      "grad_norm": 0.20806828141212463,
      "learning_rate": 4.364791666666667e-05,
      "loss": 0.0025,
      "step": 30490
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.6285558342933655,
      "learning_rate": 4.3645833333333335e-05,
      "loss": 0.0017,
      "step": 30500
    },
    {
      "epoch": 1.017,
      "grad_norm": 0.34699898958206177,
      "learning_rate": 4.364375e-05,
      "loss": 0.003,
      "step": 30510
    },
    {
      "epoch": 1.0173333333333334,
      "grad_norm": 0.030990194529294968,
      "learning_rate": 4.364166666666667e-05,
      "loss": 0.0027,
      "step": 30520
    },
    {
      "epoch": 1.0176666666666667,
      "grad_norm": 0.20761077105998993,
      "learning_rate": 4.363958333333333e-05,
      "loss": 0.0023,
      "step": 30530
    },
    {
      "epoch": 1.018,
      "grad_norm": 0.2966577708721161,
      "learning_rate": 4.3637500000000004e-05,
      "loss": 0.0027,
      "step": 30540
    },
    {
      "epoch": 1.0183333333333333,
      "grad_norm": 0.030464183539152145,
      "learning_rate": 4.363541666666667e-05,
      "loss": 0.0023,
      "step": 30550
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.6326807141304016,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0026,
      "step": 30560
    },
    {
      "epoch": 1.019,
      "grad_norm": 0.41493895649909973,
      "learning_rate": 4.363125e-05,
      "loss": 0.0033,
      "step": 30570
    },
    {
      "epoch": 1.0193333333333334,
      "grad_norm": 0.7109454870223999,
      "learning_rate": 4.362916666666667e-05,
      "loss": 0.0025,
      "step": 30580
    },
    {
      "epoch": 1.0196666666666667,
      "grad_norm": 0.029718980193138123,
      "learning_rate": 4.362708333333334e-05,
      "loss": 0.0033,
      "step": 30590
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.00786898285150528,
      "learning_rate": 4.3625e-05,
      "loss": 0.0029,
      "step": 30600
    },
    {
      "epoch": 1.0203333333333333,
      "grad_norm": 0.2667720317840576,
      "learning_rate": 4.362291666666667e-05,
      "loss": 0.0015,
      "step": 30610
    },
    {
      "epoch": 1.0206666666666666,
      "grad_norm": 0.2964349091053009,
      "learning_rate": 4.3620833333333334e-05,
      "loss": 0.0019,
      "step": 30620
    },
    {
      "epoch": 1.021,
      "grad_norm": 0.295982301235199,
      "learning_rate": 4.361875e-05,
      "loss": 0.0029,
      "step": 30630
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.11842325329780579,
      "learning_rate": 4.3616666666666665e-05,
      "loss": 0.0021,
      "step": 30640
    },
    {
      "epoch": 1.0216666666666667,
      "grad_norm": 0.05966874957084656,
      "learning_rate": 4.361458333333334e-05,
      "loss": 0.0034,
      "step": 30650
    },
    {
      "epoch": 1.022,
      "grad_norm": 0.059858325868844986,
      "learning_rate": 4.36125e-05,
      "loss": 0.0034,
      "step": 30660
    },
    {
      "epoch": 1.0223333333333333,
      "grad_norm": 0.05952451750636101,
      "learning_rate": 4.361041666666667e-05,
      "loss": 0.003,
      "step": 30670
    },
    {
      "epoch": 1.0226666666666666,
      "grad_norm": 0.5629647970199585,
      "learning_rate": 4.3608333333333334e-05,
      "loss": 0.0024,
      "step": 30680
    },
    {
      "epoch": 1.023,
      "grad_norm": 0.15573565661907196,
      "learning_rate": 4.3606250000000006e-05,
      "loss": 0.0026,
      "step": 30690
    },
    {
      "epoch": 1.0233333333333334,
      "grad_norm": 0.23722274601459503,
      "learning_rate": 4.3604166666666665e-05,
      "loss": 0.0026,
      "step": 30700
    },
    {
      "epoch": 1.0236666666666667,
      "grad_norm": 0.08884620666503906,
      "learning_rate": 4.360208333333334e-05,
      "loss": 0.0017,
      "step": 30710
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.14806325733661652,
      "learning_rate": 4.36e-05,
      "loss": 0.0031,
      "step": 30720
    },
    {
      "epoch": 1.0243333333333333,
      "grad_norm": 0.29598069190979004,
      "learning_rate": 4.359791666666667e-05,
      "loss": 0.002,
      "step": 30730
    },
    {
      "epoch": 1.0246666666666666,
      "grad_norm": 0.414699524641037,
      "learning_rate": 4.3595833333333334e-05,
      "loss": 0.0035,
      "step": 30740
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.08878543972969055,
      "learning_rate": 4.359375e-05,
      "loss": 0.0021,
      "step": 30750
    },
    {
      "epoch": 1.0253333333333334,
      "grad_norm": 0.7348398566246033,
      "learning_rate": 4.359166666666667e-05,
      "loss": 0.0028,
      "step": 30760
    },
    {
      "epoch": 1.0256666666666667,
      "grad_norm": 0.12877240777015686,
      "learning_rate": 4.358958333333334e-05,
      "loss": 0.0027,
      "step": 30770
    },
    {
      "epoch": 1.026,
      "grad_norm": 0.11840847879648209,
      "learning_rate": 4.35875e-05,
      "loss": 0.0024,
      "step": 30780
    },
    {
      "epoch": 1.0263333333333333,
      "grad_norm": 0.05930667743086815,
      "learning_rate": 4.358541666666667e-05,
      "loss": 0.0035,
      "step": 30790
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.3265211880207062,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.0024,
      "step": 30800
    },
    {
      "epoch": 1.027,
      "grad_norm": 0.32594117522239685,
      "learning_rate": 4.358125e-05,
      "loss": 0.0025,
      "step": 30810
    },
    {
      "epoch": 1.0273333333333334,
      "grad_norm": 0.059488702565431595,
      "learning_rate": 4.357916666666667e-05,
      "loss": 0.0031,
      "step": 30820
    },
    {
      "epoch": 1.0276666666666667,
      "grad_norm": 0.11840621381998062,
      "learning_rate": 4.3577083333333337e-05,
      "loss": 0.002,
      "step": 30830
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.2368413209915161,
      "learning_rate": 4.3575e-05,
      "loss": 0.0027,
      "step": 30840
    },
    {
      "epoch": 1.0283333333333333,
      "grad_norm": 0.14820650219917297,
      "learning_rate": 4.357291666666667e-05,
      "loss": 0.0027,
      "step": 30850
    },
    {
      "epoch": 1.0286666666666666,
      "grad_norm": 0.14796139299869537,
      "learning_rate": 4.357083333333333e-05,
      "loss": 0.0026,
      "step": 30860
    },
    {
      "epoch": 1.029,
      "grad_norm": 0.20726564526557922,
      "learning_rate": 4.3568750000000005e-05,
      "loss": 0.0018,
      "step": 30870
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.12629544734954834,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.0021,
      "step": 30880
    },
    {
      "epoch": 1.0296666666666667,
      "grad_norm": 0.05938239023089409,
      "learning_rate": 4.3564583333333336e-05,
      "loss": 0.0037,
      "step": 30890
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.23703879117965698,
      "learning_rate": 4.35625e-05,
      "loss": 0.0029,
      "step": 30900
    },
    {
      "epoch": 1.0303333333333333,
      "grad_norm": 0.47389090061187744,
      "learning_rate": 4.356041666666667e-05,
      "loss": 0.0023,
      "step": 30910
    },
    {
      "epoch": 1.0306666666666666,
      "grad_norm": 0.47367846965789795,
      "learning_rate": 4.355833333333333e-05,
      "loss": 0.0033,
      "step": 30920
    },
    {
      "epoch": 1.031,
      "grad_norm": 0.35690900683403015,
      "learning_rate": 4.3556250000000005e-05,
      "loss": 0.0025,
      "step": 30930
    },
    {
      "epoch": 1.0313333333333334,
      "grad_norm": 0.44763875007629395,
      "learning_rate": 4.355416666666667e-05,
      "loss": 0.0023,
      "step": 30940
    },
    {
      "epoch": 1.0316666666666667,
      "grad_norm": 0.5629286170005798,
      "learning_rate": 4.3552083333333336e-05,
      "loss": 0.0029,
      "step": 30950
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.44728559255599976,
      "learning_rate": 4.355e-05,
      "loss": 0.0028,
      "step": 30960
    },
    {
      "epoch": 1.0323333333333333,
      "grad_norm": 0.05969652906060219,
      "learning_rate": 4.354791666666667e-05,
      "loss": 0.0028,
      "step": 30970
    },
    {
      "epoch": 1.0326666666666666,
      "grad_norm": 0.030598893761634827,
      "learning_rate": 4.354583333333333e-05,
      "loss": 0.0016,
      "step": 30980
    },
    {
      "epoch": 1.033,
      "grad_norm": 0.26678353548049927,
      "learning_rate": 4.354375e-05,
      "loss": 0.0032,
      "step": 30990
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.17773625254631042,
      "learning_rate": 4.354166666666667e-05,
      "loss": 0.0026,
      "step": 31000
    },
    {
      "epoch": 1.0336666666666667,
      "grad_norm": 0.2370283007621765,
      "learning_rate": 4.3539583333333336e-05,
      "loss": 0.0025,
      "step": 31010
    },
    {
      "epoch": 1.034,
      "grad_norm": 0.059615232050418854,
      "learning_rate": 4.35375e-05,
      "loss": 0.0024,
      "step": 31020
    },
    {
      "epoch": 1.0343333333333333,
      "grad_norm": 0.08896999806165695,
      "learning_rate": 4.353541666666667e-05,
      "loss": 0.0022,
      "step": 31030
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.9580997228622437,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0028,
      "step": 31040
    },
    {
      "epoch": 1.035,
      "grad_norm": 0.06067903712391853,
      "learning_rate": 4.3531250000000004e-05,
      "loss": 0.0027,
      "step": 31050
    },
    {
      "epoch": 1.0353333333333334,
      "grad_norm": 0.15158440172672272,
      "learning_rate": 4.352916666666667e-05,
      "loss": 0.0017,
      "step": 31060
    },
    {
      "epoch": 1.0356666666666667,
      "grad_norm": 0.262238472700119,
      "learning_rate": 4.3527083333333335e-05,
      "loss": 0.0035,
      "step": 31070
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.029859688133001328,
      "learning_rate": 4.352500000000001e-05,
      "loss": 0.0036,
      "step": 31080
    },
    {
      "epoch": 1.0363333333333333,
      "grad_norm": 0.44520705938339233,
      "learning_rate": 4.3522916666666666e-05,
      "loss": 0.004,
      "step": 31090
    },
    {
      "epoch": 1.0366666666666666,
      "grad_norm": 0.14789465069770813,
      "learning_rate": 4.352083333333333e-05,
      "loss": 0.0015,
      "step": 31100
    },
    {
      "epoch": 1.037,
      "grad_norm": 0.059706129133701324,
      "learning_rate": 4.3518750000000004e-05,
      "loss": 0.0031,
      "step": 31110
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.11867222934961319,
      "learning_rate": 4.351666666666667e-05,
      "loss": 0.0034,
      "step": 31120
    },
    {
      "epoch": 1.0376666666666667,
      "grad_norm": 0.08904388546943665,
      "learning_rate": 4.3514583333333335e-05,
      "loss": 0.0024,
      "step": 31130
    },
    {
      "epoch": 1.038,
      "grad_norm": 0.23732809722423553,
      "learning_rate": 4.35125e-05,
      "loss": 0.0028,
      "step": 31140
    },
    {
      "epoch": 1.0383333333333333,
      "grad_norm": 0.14861485362052917,
      "learning_rate": 4.351041666666667e-05,
      "loss": 0.0023,
      "step": 31150
    },
    {
      "epoch": 1.0386666666666666,
      "grad_norm": 0.17778043448925018,
      "learning_rate": 4.350833333333333e-05,
      "loss": 0.0031,
      "step": 31160
    },
    {
      "epoch": 1.039,
      "grad_norm": 0.17808322608470917,
      "learning_rate": 4.3506250000000004e-05,
      "loss": 0.002,
      "step": 31170
    },
    {
      "epoch": 1.0393333333333334,
      "grad_norm": 0.2840029001235962,
      "learning_rate": 4.350416666666667e-05,
      "loss": 0.0019,
      "step": 31180
    },
    {
      "epoch": 1.0396666666666667,
      "grad_norm": 0.23680345714092255,
      "learning_rate": 4.3502083333333335e-05,
      "loss": 0.0031,
      "step": 31190
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.08909803628921509,
      "learning_rate": 4.35e-05,
      "loss": 0.0023,
      "step": 31200
    },
    {
      "epoch": 1.0403333333333333,
      "grad_norm": 0.3553447425365448,
      "learning_rate": 4.3497916666666666e-05,
      "loss": 0.0027,
      "step": 31210
    },
    {
      "epoch": 1.0406666666666666,
      "grad_norm": 0.41500017046928406,
      "learning_rate": 4.349583333333334e-05,
      "loss": 0.0025,
      "step": 31220
    },
    {
      "epoch": 1.041,
      "grad_norm": 0.05982320010662079,
      "learning_rate": 4.349375e-05,
      "loss": 0.0022,
      "step": 31230
    },
    {
      "epoch": 1.0413333333333332,
      "grad_norm": 0.2666807174682617,
      "learning_rate": 4.349166666666667e-05,
      "loss": 0.0029,
      "step": 31240
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.08883640915155411,
      "learning_rate": 4.3489583333333334e-05,
      "loss": 0.0029,
      "step": 31250
    },
    {
      "epoch": 1.042,
      "grad_norm": 0.08899793028831482,
      "learning_rate": 4.34875e-05,
      "loss": 0.0034,
      "step": 31260
    },
    {
      "epoch": 1.0423333333333333,
      "grad_norm": 0.03017432987689972,
      "learning_rate": 4.3485416666666665e-05,
      "loss": 0.0036,
      "step": 31270
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.2958720624446869,
      "learning_rate": 4.348333333333334e-05,
      "loss": 0.0024,
      "step": 31280
    },
    {
      "epoch": 1.043,
      "grad_norm": 0.4735732674598694,
      "learning_rate": 4.348125e-05,
      "loss": 0.0039,
      "step": 31290
    },
    {
      "epoch": 1.0433333333333334,
      "grad_norm": 0.5329151153564453,
      "learning_rate": 4.347916666666667e-05,
      "loss": 0.0022,
      "step": 31300
    },
    {
      "epoch": 1.0436666666666667,
      "grad_norm": 0.08898647874593735,
      "learning_rate": 4.3477083333333334e-05,
      "loss": 0.0024,
      "step": 31310
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.2667655944824219,
      "learning_rate": 4.3475000000000006e-05,
      "loss": 0.0021,
      "step": 31320
    },
    {
      "epoch": 1.0443333333333333,
      "grad_norm": 0.03015410155057907,
      "learning_rate": 4.347291666666667e-05,
      "loss": 0.0021,
      "step": 31330
    },
    {
      "epoch": 1.0446666666666666,
      "grad_norm": 0.20744016766548157,
      "learning_rate": 4.347083333333333e-05,
      "loss": 0.0017,
      "step": 31340
    },
    {
      "epoch": 1.045,
      "grad_norm": 0.644841730594635,
      "learning_rate": 4.346875e-05,
      "loss": 0.0014,
      "step": 31350
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.3258820176124573,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.002,
      "step": 31360
    },
    {
      "epoch": 1.0456666666666667,
      "grad_norm": 0.14814074337482452,
      "learning_rate": 4.3464583333333334e-05,
      "loss": 0.0025,
      "step": 31370
    },
    {
      "epoch": 1.046,
      "grad_norm": 0.2961443066596985,
      "learning_rate": 4.34625e-05,
      "loss": 0.0027,
      "step": 31380
    },
    {
      "epoch": 1.0463333333333333,
      "grad_norm": 0.11846432834863663,
      "learning_rate": 4.346041666666667e-05,
      "loss": 0.0024,
      "step": 31390
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.08901695907115936,
      "learning_rate": 4.345833333333334e-05,
      "loss": 0.0028,
      "step": 31400
    },
    {
      "epoch": 1.047,
      "grad_norm": 0.030192945152521133,
      "learning_rate": 4.345625e-05,
      "loss": 0.0016,
      "step": 31410
    },
    {
      "epoch": 1.0473333333333332,
      "grad_norm": 0.2366696298122406,
      "learning_rate": 4.345416666666667e-05,
      "loss": 0.0018,
      "step": 31420
    },
    {
      "epoch": 1.0476666666666667,
      "grad_norm": 0.11842416971921921,
      "learning_rate": 4.345208333333334e-05,
      "loss": 0.0032,
      "step": 31430
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.20712146162986755,
      "learning_rate": 4.345e-05,
      "loss": 0.0029,
      "step": 31440
    },
    {
      "epoch": 1.0483333333333333,
      "grad_norm": 0.35591116547584534,
      "learning_rate": 4.344791666666667e-05,
      "loss": 0.0027,
      "step": 31450
    },
    {
      "epoch": 1.0486666666666666,
      "grad_norm": 0.4146290421485901,
      "learning_rate": 4.344583333333334e-05,
      "loss": 0.0027,
      "step": 31460
    },
    {
      "epoch": 1.049,
      "grad_norm": 0.5098239183425903,
      "learning_rate": 4.344375e-05,
      "loss": 0.003,
      "step": 31470
    },
    {
      "epoch": 1.0493333333333332,
      "grad_norm": 0.14884015917778015,
      "learning_rate": 4.344166666666667e-05,
      "loss": 0.0027,
      "step": 31480
    },
    {
      "epoch": 1.0496666666666667,
      "grad_norm": 0.030116485431790352,
      "learning_rate": 4.343958333333333e-05,
      "loss": 0.0033,
      "step": 31490
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.29587307572364807,
      "learning_rate": 4.3437500000000006e-05,
      "loss": 0.0031,
      "step": 31500
    },
    {
      "epoch": 1.0503333333333333,
      "grad_norm": 0.11878685653209686,
      "learning_rate": 4.3435416666666664e-05,
      "loss": 0.0037,
      "step": 31510
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.2962527871131897,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0019,
      "step": 31520
    },
    {
      "epoch": 1.051,
      "grad_norm": 0.14793744683265686,
      "learning_rate": 4.343125e-05,
      "loss": 0.0037,
      "step": 31530
    },
    {
      "epoch": 1.0513333333333332,
      "grad_norm": 0.20749908685684204,
      "learning_rate": 4.342916666666667e-05,
      "loss": 0.0024,
      "step": 31540
    },
    {
      "epoch": 1.0516666666666667,
      "grad_norm": 0.29609957337379456,
      "learning_rate": 4.342708333333333e-05,
      "loss": 0.002,
      "step": 31550
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.5339192152023315,
      "learning_rate": 4.3425000000000005e-05,
      "loss": 0.003,
      "step": 31560
    },
    {
      "epoch": 1.0523333333333333,
      "grad_norm": 0.5623044967651367,
      "learning_rate": 4.342291666666667e-05,
      "loss": 0.0039,
      "step": 31570
    },
    {
      "epoch": 1.0526666666666666,
      "grad_norm": 0.005385482218116522,
      "learning_rate": 4.3420833333333336e-05,
      "loss": 0.0021,
      "step": 31580
    },
    {
      "epoch": 1.053,
      "grad_norm": 0.20776814222335815,
      "learning_rate": 4.341875e-05,
      "loss": 0.0021,
      "step": 31590
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.059986721724271774,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.0033,
      "step": 31600
    },
    {
      "epoch": 1.0536666666666668,
      "grad_norm": 0.11860135942697525,
      "learning_rate": 4.341458333333334e-05,
      "loss": 0.002,
      "step": 31610
    },
    {
      "epoch": 1.054,
      "grad_norm": 0.5643018484115601,
      "learning_rate": 4.34125e-05,
      "loss": 0.0024,
      "step": 31620
    },
    {
      "epoch": 1.0543333333333333,
      "grad_norm": 1.0296472311019897,
      "learning_rate": 4.341041666666667e-05,
      "loss": 0.0024,
      "step": 31630
    },
    {
      "epoch": 1.0546666666666666,
      "grad_norm": 0.1957990527153015,
      "learning_rate": 4.3408333333333336e-05,
      "loss": 0.0026,
      "step": 31640
    },
    {
      "epoch": 1.055,
      "grad_norm": 0.11813688278198242,
      "learning_rate": 4.340625e-05,
      "loss": 0.0026,
      "step": 31650
    },
    {
      "epoch": 1.0553333333333332,
      "grad_norm": 0.22387385368347168,
      "learning_rate": 4.340416666666667e-05,
      "loss": 0.0026,
      "step": 31660
    },
    {
      "epoch": 1.0556666666666668,
      "grad_norm": 0.05943378061056137,
      "learning_rate": 4.340208333333334e-05,
      "loss": 0.0027,
      "step": 31670
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.030257439240813255,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0026,
      "step": 31680
    },
    {
      "epoch": 1.0563333333333333,
      "grad_norm": 0.2959176301956177,
      "learning_rate": 4.339791666666667e-05,
      "loss": 0.0035,
      "step": 31690
    },
    {
      "epoch": 1.0566666666666666,
      "grad_norm": 0.32548120617866516,
      "learning_rate": 4.3395833333333336e-05,
      "loss": 0.0026,
      "step": 31700
    },
    {
      "epoch": 1.057,
      "grad_norm": 0.05955418199300766,
      "learning_rate": 4.339375e-05,
      "loss": 0.0028,
      "step": 31710
    },
    {
      "epoch": 1.0573333333333332,
      "grad_norm": 0.059470757842063904,
      "learning_rate": 4.3391666666666667e-05,
      "loss": 0.0024,
      "step": 31720
    },
    {
      "epoch": 1.0576666666666668,
      "grad_norm": 0.20128466188907623,
      "learning_rate": 4.338958333333333e-05,
      "loss": 0.0025,
      "step": 31730
    },
    {
      "epoch": 1.058,
      "grad_norm": 0.48354220390319824,
      "learning_rate": 4.3387500000000004e-05,
      "loss": 0.0017,
      "step": 31740
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 0.3018362820148468,
      "learning_rate": 4.338541666666667e-05,
      "loss": 0.0035,
      "step": 31750
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.4749870300292969,
      "learning_rate": 4.3383333333333335e-05,
      "loss": 0.0032,
      "step": 31760
    },
    {
      "epoch": 1.059,
      "grad_norm": 0.030430186539888382,
      "learning_rate": 4.338125e-05,
      "loss": 0.002,
      "step": 31770
    },
    {
      "epoch": 1.0593333333333332,
      "grad_norm": 1.0892629623413086,
      "learning_rate": 4.337916666666667e-05,
      "loss": 0.0019,
      "step": 31780
    },
    {
      "epoch": 1.0596666666666668,
      "grad_norm": 0.06292448937892914,
      "learning_rate": 4.337708333333333e-05,
      "loss": 0.0028,
      "step": 31790
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.11849109083414078,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 0.0036,
      "step": 31800
    },
    {
      "epoch": 1.0603333333333333,
      "grad_norm": 0.059182822704315186,
      "learning_rate": 4.337291666666667e-05,
      "loss": 0.0032,
      "step": 31810
    },
    {
      "epoch": 1.0606666666666666,
      "grad_norm": 0.5031536817550659,
      "learning_rate": 4.3370833333333335e-05,
      "loss": 0.0025,
      "step": 31820
    },
    {
      "epoch": 1.061,
      "grad_norm": 0.5921547412872314,
      "learning_rate": 4.336875e-05,
      "loss": 0.0027,
      "step": 31830
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.08865365386009216,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0028,
      "step": 31840
    },
    {
      "epoch": 1.0616666666666668,
      "grad_norm": 0.05998373031616211,
      "learning_rate": 4.336458333333334e-05,
      "loss": 0.0038,
      "step": 31850
    },
    {
      "epoch": 1.062,
      "grad_norm": 0.0596003383398056,
      "learning_rate": 4.3362500000000004e-05,
      "loss": 0.0022,
      "step": 31860
    },
    {
      "epoch": 1.0623333333333334,
      "grad_norm": 0.44595327973365784,
      "learning_rate": 4.336041666666667e-05,
      "loss": 0.0029,
      "step": 31870
    },
    {
      "epoch": 1.0626666666666666,
      "grad_norm": 0.47336676716804504,
      "learning_rate": 4.3358333333333335e-05,
      "loss": 0.0026,
      "step": 31880
    },
    {
      "epoch": 1.063,
      "grad_norm": 0.05952880159020424,
      "learning_rate": 4.335625000000001e-05,
      "loss": 0.0031,
      "step": 31890
    },
    {
      "epoch": 1.0633333333333332,
      "grad_norm": 0.2766631245613098,
      "learning_rate": 4.3354166666666666e-05,
      "loss": 0.0023,
      "step": 31900
    },
    {
      "epoch": 1.0636666666666668,
      "grad_norm": 0.36147215962409973,
      "learning_rate": 4.335208333333334e-05,
      "loss": 0.0024,
      "step": 31910
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.14827047288417816,
      "learning_rate": 4.335e-05,
      "loss": 0.0017,
      "step": 31920
    },
    {
      "epoch": 1.0643333333333334,
      "grad_norm": 0.06035895645618439,
      "learning_rate": 4.334791666666667e-05,
      "loss": 0.0032,
      "step": 31930
    },
    {
      "epoch": 1.0646666666666667,
      "grad_norm": 0.26626649498939514,
      "learning_rate": 4.3345833333333334e-05,
      "loss": 0.0023,
      "step": 31940
    },
    {
      "epoch": 1.065,
      "grad_norm": 0.2958384156227112,
      "learning_rate": 4.334375e-05,
      "loss": 0.0036,
      "step": 31950
    },
    {
      "epoch": 1.0653333333333332,
      "grad_norm": 0.08894830197095871,
      "learning_rate": 4.334166666666667e-05,
      "loss": 0.0018,
      "step": 31960
    },
    {
      "epoch": 1.0656666666666668,
      "grad_norm": 0.26651445031166077,
      "learning_rate": 4.333958333333333e-05,
      "loss": 0.0035,
      "step": 31970
    },
    {
      "epoch": 1.066,
      "grad_norm": 0.030511626973748207,
      "learning_rate": 4.33375e-05,
      "loss": 0.0021,
      "step": 31980
    },
    {
      "epoch": 1.0663333333333334,
      "grad_norm": 0.0592794343829155,
      "learning_rate": 4.333541666666667e-05,
      "loss": 0.0031,
      "step": 31990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.1184348613023758,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0029,
      "step": 32000
    },
    {
      "epoch": 1.067,
      "grad_norm": 0.26625189185142517,
      "learning_rate": 4.333125e-05,
      "loss": 0.0024,
      "step": 32010
    },
    {
      "epoch": 1.0673333333333332,
      "grad_norm": 0.2367241382598877,
      "learning_rate": 4.332916666666667e-05,
      "loss": 0.0026,
      "step": 32020
    },
    {
      "epoch": 1.0676666666666668,
      "grad_norm": 0.1477690488100052,
      "learning_rate": 4.332708333333334e-05,
      "loss": 0.0028,
      "step": 32030
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.16770075261592865,
      "learning_rate": 4.3325e-05,
      "loss": 0.0025,
      "step": 32040
    },
    {
      "epoch": 1.0683333333333334,
      "grad_norm": 0.08917310833930969,
      "learning_rate": 4.332291666666667e-05,
      "loss": 0.0021,
      "step": 32050
    },
    {
      "epoch": 1.0686666666666667,
      "grad_norm": 0.2370874136686325,
      "learning_rate": 4.3320833333333334e-05,
      "loss": 0.0023,
      "step": 32060
    },
    {
      "epoch": 1.069,
      "grad_norm": 0.17778931558132172,
      "learning_rate": 4.331875e-05,
      "loss": 0.0029,
      "step": 32070
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.11961223185062408,
      "learning_rate": 4.3316666666666665e-05,
      "loss": 0.0036,
      "step": 32080
    },
    {
      "epoch": 1.0696666666666668,
      "grad_norm": 0.08889327943325043,
      "learning_rate": 4.331458333333334e-05,
      "loss": 0.0025,
      "step": 32090
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.03153476119041443,
      "learning_rate": 4.33125e-05,
      "loss": 0.0042,
      "step": 32100
    },
    {
      "epoch": 1.0703333333333334,
      "grad_norm": 0.29569631814956665,
      "learning_rate": 4.331041666666667e-05,
      "loss": 0.0018,
      "step": 32110
    },
    {
      "epoch": 1.0706666666666667,
      "grad_norm": 0.2073991447687149,
      "learning_rate": 4.3308333333333333e-05,
      "loss": 0.0027,
      "step": 32120
    },
    {
      "epoch": 1.071,
      "grad_norm": 0.2663443088531494,
      "learning_rate": 4.3306250000000006e-05,
      "loss": 0.0027,
      "step": 32130
    },
    {
      "epoch": 1.0713333333333332,
      "grad_norm": 0.5345696210861206,
      "learning_rate": 4.330416666666667e-05,
      "loss": 0.0031,
      "step": 32140
    },
    {
      "epoch": 1.0716666666666668,
      "grad_norm": 0.26593533158302307,
      "learning_rate": 4.330208333333334e-05,
      "loss": 0.003,
      "step": 32150
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.32534506916999817,
      "learning_rate": 4.33e-05,
      "loss": 0.0035,
      "step": 32160
    },
    {
      "epoch": 1.0723333333333334,
      "grad_norm": 0.20705898106098175,
      "learning_rate": 4.3297916666666674e-05,
      "loss": 0.0024,
      "step": 32170
    },
    {
      "epoch": 1.0726666666666667,
      "grad_norm": 0.2073018103837967,
      "learning_rate": 4.329583333333333e-05,
      "loss": 0.0034,
      "step": 32180
    },
    {
      "epoch": 1.073,
      "grad_norm": 0.05936267599463463,
      "learning_rate": 4.329375e-05,
      "loss": 0.0018,
      "step": 32190
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.05928470939397812,
      "learning_rate": 4.329166666666667e-05,
      "loss": 0.0018,
      "step": 32200
    },
    {
      "epoch": 1.0736666666666668,
      "grad_norm": 0.3843269646167755,
      "learning_rate": 4.3289583333333336e-05,
      "loss": 0.0024,
      "step": 32210
    },
    {
      "epoch": 1.074,
      "grad_norm": 0.0888177901506424,
      "learning_rate": 4.32875e-05,
      "loss": 0.0027,
      "step": 32220
    },
    {
      "epoch": 1.0743333333333334,
      "grad_norm": 0.41417697072029114,
      "learning_rate": 4.328541666666667e-05,
      "loss": 0.0033,
      "step": 32230
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.030382107943296432,
      "learning_rate": 4.328333333333334e-05,
      "loss": 0.0019,
      "step": 32240
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.02986127883195877,
      "learning_rate": 4.328125e-05,
      "loss": 0.0023,
      "step": 32250
    },
    {
      "epoch": 1.0753333333333333,
      "grad_norm": 0.17779509723186493,
      "learning_rate": 4.327916666666667e-05,
      "loss": 0.002,
      "step": 32260
    },
    {
      "epoch": 1.0756666666666668,
      "grad_norm": 0.6276097297668457,
      "learning_rate": 4.3277083333333336e-05,
      "loss": 0.0024,
      "step": 32270
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.5032820105552673,
      "learning_rate": 4.3275e-05,
      "loss": 0.003,
      "step": 32280
    },
    {
      "epoch": 1.0763333333333334,
      "grad_norm": 0.5022933483123779,
      "learning_rate": 4.327291666666667e-05,
      "loss": 0.0035,
      "step": 32290
    },
    {
      "epoch": 1.0766666666666667,
      "grad_norm": 0.7469766736030579,
      "learning_rate": 4.327083333333333e-05,
      "loss": 0.0022,
      "step": 32300
    },
    {
      "epoch": 1.077,
      "grad_norm": 0.26661133766174316,
      "learning_rate": 4.3268750000000005e-05,
      "loss": 0.0028,
      "step": 32310
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.5025283694267273,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0026,
      "step": 32320
    },
    {
      "epoch": 1.0776666666666666,
      "grad_norm": 0.23696932196617126,
      "learning_rate": 4.3264583333333336e-05,
      "loss": 0.0032,
      "step": 32330
    },
    {
      "epoch": 1.078,
      "grad_norm": 0.4128338098526001,
      "learning_rate": 4.32625e-05,
      "loss": 0.0035,
      "step": 32340
    },
    {
      "epoch": 1.0783333333333334,
      "grad_norm": 0.11919009685516357,
      "learning_rate": 4.326041666666667e-05,
      "loss": 0.0029,
      "step": 32350
    },
    {
      "epoch": 1.0786666666666667,
      "grad_norm": 0.2953622341156006,
      "learning_rate": 4.325833333333333e-05,
      "loss": 0.0033,
      "step": 32360
    },
    {
      "epoch": 1.079,
      "grad_norm": 0.5913093090057373,
      "learning_rate": 4.3256250000000004e-05,
      "loss": 0.0023,
      "step": 32370
    },
    {
      "epoch": 1.0793333333333333,
      "grad_norm": 0.6508899331092834,
      "learning_rate": 4.325416666666667e-05,
      "loss": 0.0036,
      "step": 32380
    },
    {
      "epoch": 1.0796666666666668,
      "grad_norm": 0.2658484876155853,
      "learning_rate": 4.3252083333333335e-05,
      "loss": 0.0025,
      "step": 32390
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.08954937756061554,
      "learning_rate": 4.325e-05,
      "loss": 0.0025,
      "step": 32400
    },
    {
      "epoch": 1.0803333333333334,
      "grad_norm": 0.06049489974975586,
      "learning_rate": 4.324791666666667e-05,
      "loss": 0.0035,
      "step": 32410
    },
    {
      "epoch": 1.0806666666666667,
      "grad_norm": 0.2662869691848755,
      "learning_rate": 4.324583333333334e-05,
      "loss": 0.0041,
      "step": 32420
    },
    {
      "epoch": 1.081,
      "grad_norm": 0.11868958175182343,
      "learning_rate": 4.324375e-05,
      "loss": 0.0026,
      "step": 32430
    },
    {
      "epoch": 1.0813333333333333,
      "grad_norm": 0.0690615177154541,
      "learning_rate": 4.324166666666667e-05,
      "loss": 0.0026,
      "step": 32440
    },
    {
      "epoch": 1.0816666666666666,
      "grad_norm": 0.08927501738071442,
      "learning_rate": 4.3239583333333335e-05,
      "loss": 0.0033,
      "step": 32450
    },
    {
      "epoch": 1.082,
      "grad_norm": 0.3850606679916382,
      "learning_rate": 4.32375e-05,
      "loss": 0.0024,
      "step": 32460
    },
    {
      "epoch": 1.0823333333333334,
      "grad_norm": 0.031034935265779495,
      "learning_rate": 4.3235416666666666e-05,
      "loss": 0.0024,
      "step": 32470
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.11852739006280899,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0046,
      "step": 32480
    },
    {
      "epoch": 1.083,
      "grad_norm": 0.05950998142361641,
      "learning_rate": 4.3231250000000004e-05,
      "loss": 0.0013,
      "step": 32490
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.26616957783699036,
      "learning_rate": 4.322916666666667e-05,
      "loss": 0.0027,
      "step": 32500
    },
    {
      "epoch": 1.0836666666666668,
      "grad_norm": 0.0370015874505043,
      "learning_rate": 4.3227083333333335e-05,
      "loss": 0.0026,
      "step": 32510
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.20717832446098328,
      "learning_rate": 4.322500000000001e-05,
      "loss": 0.004,
      "step": 32520
    },
    {
      "epoch": 1.0843333333333334,
      "grad_norm": 0.8432798981666565,
      "learning_rate": 4.3222916666666666e-05,
      "loss": 0.0028,
      "step": 32530
    },
    {
      "epoch": 1.0846666666666667,
      "grad_norm": 0.11831355839967728,
      "learning_rate": 4.322083333333334e-05,
      "loss": 0.0024,
      "step": 32540
    },
    {
      "epoch": 1.085,
      "grad_norm": 0.1773158311843872,
      "learning_rate": 4.3218750000000004e-05,
      "loss": 0.0029,
      "step": 32550
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.08882175385951996,
      "learning_rate": 4.321666666666667e-05,
      "loss": 0.0045,
      "step": 32560
    },
    {
      "epoch": 1.0856666666666666,
      "grad_norm": 0.5544617772102356,
      "learning_rate": 4.3214583333333335e-05,
      "loss": 0.0035,
      "step": 32570
    },
    {
      "epoch": 1.086,
      "grad_norm": 0.05917027220129967,
      "learning_rate": 4.32125e-05,
      "loss": 0.0017,
      "step": 32580
    },
    {
      "epoch": 1.0863333333333334,
      "grad_norm": 0.1478847712278366,
      "learning_rate": 4.321041666666667e-05,
      "loss": 0.0035,
      "step": 32590
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.029677271842956543,
      "learning_rate": 4.320833333333333e-05,
      "loss": 0.0033,
      "step": 32600
    },
    {
      "epoch": 1.087,
      "grad_norm": 0.14790380001068115,
      "learning_rate": 4.320625e-05,
      "loss": 0.0041,
      "step": 32610
    },
    {
      "epoch": 1.0873333333333333,
      "grad_norm": 0.13594628870487213,
      "learning_rate": 4.320416666666667e-05,
      "loss": 0.0034,
      "step": 32620
    },
    {
      "epoch": 1.0876666666666668,
      "grad_norm": 0.11848368495702744,
      "learning_rate": 4.3202083333333334e-05,
      "loss": 0.0026,
      "step": 32630
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.030072622001171112,
      "learning_rate": 4.32e-05,
      "loss": 0.0044,
      "step": 32640
    },
    {
      "epoch": 1.0883333333333334,
      "grad_norm": 0.5030115246772766,
      "learning_rate": 4.319791666666667e-05,
      "loss": 0.0027,
      "step": 32650
    },
    {
      "epoch": 1.0886666666666667,
      "grad_norm": 0.29561617970466614,
      "learning_rate": 4.319583333333334e-05,
      "loss": 0.002,
      "step": 32660
    },
    {
      "epoch": 1.089,
      "grad_norm": 0.2073582261800766,
      "learning_rate": 4.3193749999999996e-05,
      "loss": 0.0029,
      "step": 32670
    },
    {
      "epoch": 1.0893333333333333,
      "grad_norm": 0.17764036357402802,
      "learning_rate": 4.319166666666667e-05,
      "loss": 0.003,
      "step": 32680
    },
    {
      "epoch": 1.0896666666666666,
      "grad_norm": 0.5494261384010315,
      "learning_rate": 4.3189583333333334e-05,
      "loss": 0.0021,
      "step": 32690
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.3555336594581604,
      "learning_rate": 4.3187500000000006e-05,
      "loss": 0.0023,
      "step": 32700
    },
    {
      "epoch": 1.0903333333333334,
      "grad_norm": 0.3850303888320923,
      "learning_rate": 4.3185416666666665e-05,
      "loss": 0.0024,
      "step": 32710
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.455436110496521,
      "learning_rate": 4.318333333333334e-05,
      "loss": 0.0025,
      "step": 32720
    },
    {
      "epoch": 1.091,
      "grad_norm": 0.26650846004486084,
      "learning_rate": 4.318125e-05,
      "loss": 0.0023,
      "step": 32730
    },
    {
      "epoch": 1.0913333333333333,
      "grad_norm": 0.41388261318206787,
      "learning_rate": 4.317916666666667e-05,
      "loss": 0.0018,
      "step": 32740
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 0.23672564327716827,
      "learning_rate": 4.3177083333333334e-05,
      "loss": 0.002,
      "step": 32750
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.4061072766780853,
      "learning_rate": 4.3175000000000006e-05,
      "loss": 0.0034,
      "step": 32760
    },
    {
      "epoch": 1.0923333333333334,
      "grad_norm": 0.4601259231567383,
      "learning_rate": 4.317291666666667e-05,
      "loss": 0.0044,
      "step": 32770
    },
    {
      "epoch": 1.0926666666666667,
      "grad_norm": 0.18255820870399475,
      "learning_rate": 4.317083333333334e-05,
      "loss": 0.0039,
      "step": 32780
    },
    {
      "epoch": 1.093,
      "grad_norm": 0.009195555932819843,
      "learning_rate": 4.316875e-05,
      "loss": 0.0033,
      "step": 32790
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.17725281417369843,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0016,
      "step": 32800
    },
    {
      "epoch": 1.0936666666666666,
      "grad_norm": 0.2363714575767517,
      "learning_rate": 4.316458333333333e-05,
      "loss": 0.0025,
      "step": 32810
    },
    {
      "epoch": 1.094,
      "grad_norm": 0.5617207884788513,
      "learning_rate": 4.31625e-05,
      "loss": 0.0035,
      "step": 32820
    },
    {
      "epoch": 1.0943333333333334,
      "grad_norm": 0.11863973736763,
      "learning_rate": 4.316041666666667e-05,
      "loss": 0.0021,
      "step": 32830
    },
    {
      "epoch": 1.0946666666666667,
      "grad_norm": 0.005031950771808624,
      "learning_rate": 4.3158333333333337e-05,
      "loss": 0.0039,
      "step": 32840
    },
    {
      "epoch": 1.095,
      "grad_norm": 0.4440150558948517,
      "learning_rate": 4.315625e-05,
      "loss": 0.0031,
      "step": 32850
    },
    {
      "epoch": 1.0953333333333333,
      "grad_norm": 0.059793733060359955,
      "learning_rate": 4.315416666666667e-05,
      "loss": 0.0033,
      "step": 32860
    },
    {
      "epoch": 1.0956666666666666,
      "grad_norm": 0.5321987867355347,
      "learning_rate": 4.315208333333334e-05,
      "loss": 0.0022,
      "step": 32870
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.5328404307365417,
      "learning_rate": 4.315e-05,
      "loss": 0.0019,
      "step": 32880
    },
    {
      "epoch": 1.0963333333333334,
      "grad_norm": 0.17750698328018188,
      "learning_rate": 4.314791666666667e-05,
      "loss": 0.0027,
      "step": 32890
    },
    {
      "epoch": 1.0966666666666667,
      "grad_norm": 0.4138866066932678,
      "learning_rate": 4.3145833333333336e-05,
      "loss": 0.002,
      "step": 32900
    },
    {
      "epoch": 1.097,
      "grad_norm": 0.14789645373821259,
      "learning_rate": 4.314375e-05,
      "loss": 0.0031,
      "step": 32910
    },
    {
      "epoch": 1.0973333333333333,
      "grad_norm": 0.030031004920601845,
      "learning_rate": 4.314166666666667e-05,
      "loss": 0.002,
      "step": 32920
    },
    {
      "epoch": 1.0976666666666666,
      "grad_norm": 0.05928020924329758,
      "learning_rate": 4.313958333333333e-05,
      "loss": 0.0024,
      "step": 32930
    },
    {
      "epoch": 1.098,
      "grad_norm": 0.3845435678958893,
      "learning_rate": 4.3137500000000005e-05,
      "loss": 0.0021,
      "step": 32940
    },
    {
      "epoch": 1.0983333333333334,
      "grad_norm": 0.14767569303512573,
      "learning_rate": 4.3135416666666664e-05,
      "loss": 0.002,
      "step": 32950
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.20697441697120667,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.002,
      "step": 32960
    },
    {
      "epoch": 1.099,
      "grad_norm": 0.3546975255012512,
      "learning_rate": 4.313125e-05,
      "loss": 0.0028,
      "step": 32970
    },
    {
      "epoch": 1.0993333333333333,
      "grad_norm": 0.2067965269088745,
      "learning_rate": 4.3129166666666674e-05,
      "loss": 0.0025,
      "step": 32980
    },
    {
      "epoch": 1.0996666666666666,
      "grad_norm": 0.3553718030452728,
      "learning_rate": 4.312708333333333e-05,
      "loss": 0.0022,
      "step": 32990
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.4146784842014313,
      "learning_rate": 4.3125000000000005e-05,
      "loss": 0.0023,
      "step": 33000
    },
    {
      "epoch": 1.1003333333333334,
      "grad_norm": 0.05978524684906006,
      "learning_rate": 4.312291666666667e-05,
      "loss": 0.0027,
      "step": 33010
    },
    {
      "epoch": 1.1006666666666667,
      "grad_norm": 0.20685434341430664,
      "learning_rate": 4.3120833333333336e-05,
      "loss": 0.0026,
      "step": 33020
    },
    {
      "epoch": 1.101,
      "grad_norm": 0.7393272519111633,
      "learning_rate": 4.311875e-05,
      "loss": 0.0036,
      "step": 33030
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.03084222599864006,
      "learning_rate": 4.311666666666667e-05,
      "loss": 0.0037,
      "step": 33040
    },
    {
      "epoch": 1.1016666666666666,
      "grad_norm": 0.4248161315917969,
      "learning_rate": 4.311458333333334e-05,
      "loss": 0.003,
      "step": 33050
    },
    {
      "epoch": 1.102,
      "grad_norm": 0.5161815285682678,
      "learning_rate": 4.31125e-05,
      "loss": 0.002,
      "step": 33060
    },
    {
      "epoch": 1.1023333333333334,
      "grad_norm": 0.05923110246658325,
      "learning_rate": 4.311041666666667e-05,
      "loss": 0.0021,
      "step": 33070
    },
    {
      "epoch": 1.1026666666666667,
      "grad_norm": 0.05957968905568123,
      "learning_rate": 4.3108333333333335e-05,
      "loss": 0.0031,
      "step": 33080
    },
    {
      "epoch": 1.103,
      "grad_norm": 0.5097594857215881,
      "learning_rate": 4.310625e-05,
      "loss": 0.0023,
      "step": 33090
    },
    {
      "epoch": 1.1033333333333333,
      "grad_norm": 0.4864175319671631,
      "learning_rate": 4.3104166666666666e-05,
      "loss": 0.0029,
      "step": 33100
    },
    {
      "epoch": 1.1036666666666666,
      "grad_norm": 0.31297391653060913,
      "learning_rate": 4.310208333333334e-05,
      "loss": 0.002,
      "step": 33110
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.23637321591377258,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.002,
      "step": 33120
    },
    {
      "epoch": 1.1043333333333334,
      "grad_norm": 0.11871976405382156,
      "learning_rate": 4.309791666666667e-05,
      "loss": 0.0018,
      "step": 33130
    },
    {
      "epoch": 1.1046666666666667,
      "grad_norm": 0.23655329644680023,
      "learning_rate": 4.3095833333333335e-05,
      "loss": 0.0025,
      "step": 33140
    },
    {
      "epoch": 1.105,
      "grad_norm": 0.005387912504374981,
      "learning_rate": 4.309375e-05,
      "loss": 0.0038,
      "step": 33150
    },
    {
      "epoch": 1.1053333333333333,
      "grad_norm": 0.5907601714134216,
      "learning_rate": 4.3091666666666666e-05,
      "loss": 0.0026,
      "step": 33160
    },
    {
      "epoch": 1.1056666666666666,
      "grad_norm": 0.2659178078174591,
      "learning_rate": 4.308958333333333e-05,
      "loss": 0.0038,
      "step": 33170
    },
    {
      "epoch": 1.106,
      "grad_norm": 0.40692904591560364,
      "learning_rate": 4.3087500000000004e-05,
      "loss": 0.0027,
      "step": 33180
    },
    {
      "epoch": 1.1063333333333334,
      "grad_norm": 0.23630844056606293,
      "learning_rate": 4.308541666666667e-05,
      "loss": 0.0027,
      "step": 33190
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.14746098220348358,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.0027,
      "step": 33200
    },
    {
      "epoch": 1.107,
      "grad_norm": 0.23649939894676208,
      "learning_rate": 4.308125e-05,
      "loss": 0.0033,
      "step": 33210
    },
    {
      "epoch": 1.1073333333333333,
      "grad_norm": 0.0300802793353796,
      "learning_rate": 4.307916666666667e-05,
      "loss": 0.0034,
      "step": 33220
    },
    {
      "epoch": 1.1076666666666666,
      "grad_norm": 0.03080568090081215,
      "learning_rate": 4.307708333333333e-05,
      "loss": 0.0025,
      "step": 33230
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.14785830676555634,
      "learning_rate": 4.3075000000000003e-05,
      "loss": 0.0018,
      "step": 33240
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.029832687228918076,
      "learning_rate": 4.307291666666667e-05,
      "loss": 0.0022,
      "step": 33250
    },
    {
      "epoch": 1.1086666666666667,
      "grad_norm": 0.4435291290283203,
      "learning_rate": 4.307083333333334e-05,
      "loss": 0.004,
      "step": 33260
    },
    {
      "epoch": 1.109,
      "grad_norm": 0.20704931020736694,
      "learning_rate": 4.306875e-05,
      "loss": 0.0028,
      "step": 33270
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.26532989740371704,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0017,
      "step": 33280
    },
    {
      "epoch": 1.1096666666666666,
      "grad_norm": 0.5910542607307434,
      "learning_rate": 4.306458333333334e-05,
      "loss": 0.0041,
      "step": 33290
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.3248205780982971,
      "learning_rate": 4.30625e-05,
      "loss": 0.0024,
      "step": 33300
    },
    {
      "epoch": 1.1103333333333334,
      "grad_norm": 0.2068563997745514,
      "learning_rate": 4.306041666666667e-05,
      "loss": 0.0033,
      "step": 33310
    },
    {
      "epoch": 1.1106666666666667,
      "grad_norm": 0.4135049879550934,
      "learning_rate": 4.3058333333333334e-05,
      "loss": 0.0028,
      "step": 33320
    },
    {
      "epoch": 1.111,
      "grad_norm": 0.14793218672275543,
      "learning_rate": 4.3056250000000006e-05,
      "loss": 0.0021,
      "step": 33330
    },
    {
      "epoch": 1.1113333333333333,
      "grad_norm": 0.17771880328655243,
      "learning_rate": 4.3054166666666665e-05,
      "loss": 0.0029,
      "step": 33340
    },
    {
      "epoch": 1.1116666666666666,
      "grad_norm": 0.11815270781517029,
      "learning_rate": 4.305208333333334e-05,
      "loss": 0.0025,
      "step": 33350
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.23827007412910461,
      "learning_rate": 4.305e-05,
      "loss": 0.0041,
      "step": 33360
    },
    {
      "epoch": 1.1123333333333334,
      "grad_norm": 0.007245031651109457,
      "learning_rate": 4.304791666666667e-05,
      "loss": 0.0027,
      "step": 33370
    },
    {
      "epoch": 1.1126666666666667,
      "grad_norm": 0.2952745258808136,
      "learning_rate": 4.3045833333333334e-05,
      "loss": 0.0027,
      "step": 33380
    },
    {
      "epoch": 1.113,
      "grad_norm": 0.11804788559675217,
      "learning_rate": 4.304375e-05,
      "loss": 0.0033,
      "step": 33390
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.08866614103317261,
      "learning_rate": 4.304166666666667e-05,
      "loss": 0.003,
      "step": 33400
    },
    {
      "epoch": 1.1136666666666666,
      "grad_norm": 0.06052543222904205,
      "learning_rate": 4.303958333333333e-05,
      "loss": 0.0022,
      "step": 33410
    },
    {
      "epoch": 1.114,
      "grad_norm": 0.26592111587524414,
      "learning_rate": 4.30375e-05,
      "loss": 0.0019,
      "step": 33420
    },
    {
      "epoch": 1.1143333333333334,
      "grad_norm": 0.14799770712852478,
      "learning_rate": 4.303541666666667e-05,
      "loss": 0.0034,
      "step": 33430
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.5874984860420227,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.0043,
      "step": 33440
    },
    {
      "epoch": 1.115,
      "grad_norm": 0.004790621344000101,
      "learning_rate": 4.303125e-05,
      "loss": 0.0033,
      "step": 33450
    },
    {
      "epoch": 1.1153333333333333,
      "grad_norm": 0.030263079330325127,
      "learning_rate": 4.302916666666667e-05,
      "loss": 0.0013,
      "step": 33460
    },
    {
      "epoch": 1.1156666666666666,
      "grad_norm": 0.11879672110080719,
      "learning_rate": 4.302708333333334e-05,
      "loss": 0.0022,
      "step": 33470
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.11820758134126663,
      "learning_rate": 4.3025e-05,
      "loss": 0.0041,
      "step": 33480
    },
    {
      "epoch": 1.1163333333333334,
      "grad_norm": 0.059358786791563034,
      "learning_rate": 4.302291666666667e-05,
      "loss": 0.0019,
      "step": 33490
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.15322540700435638,
      "learning_rate": 4.302083333333334e-05,
      "loss": 0.0025,
      "step": 33500
    },
    {
      "epoch": 1.117,
      "grad_norm": 0.20665374398231506,
      "learning_rate": 4.301875e-05,
      "loss": 0.0026,
      "step": 33510
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.030322829261422157,
      "learning_rate": 4.3016666666666664e-05,
      "loss": 0.0025,
      "step": 33520
    },
    {
      "epoch": 1.1176666666666666,
      "grad_norm": 0.25313085317611694,
      "learning_rate": 4.3014583333333336e-05,
      "loss": 0.0023,
      "step": 33530
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 0.17725171148777008,
      "learning_rate": 4.30125e-05,
      "loss": 0.0023,
      "step": 33540
    },
    {
      "epoch": 1.1183333333333334,
      "grad_norm": 0.05914910137653351,
      "learning_rate": 4.301041666666667e-05,
      "loss": 0.0027,
      "step": 33550
    },
    {
      "epoch": 1.1186666666666667,
      "grad_norm": 0.05933699011802673,
      "learning_rate": 4.300833333333333e-05,
      "loss": 0.0029,
      "step": 33560
    },
    {
      "epoch": 1.119,
      "grad_norm": 0.3552713096141815,
      "learning_rate": 4.3006250000000005e-05,
      "loss": 0.0022,
      "step": 33570
    },
    {
      "epoch": 1.1193333333333333,
      "grad_norm": 0.01567297987639904,
      "learning_rate": 4.300416666666667e-05,
      "loss": 0.0026,
      "step": 33580
    },
    {
      "epoch": 1.1196666666666666,
      "grad_norm": 0.08895081281661987,
      "learning_rate": 4.3002083333333336e-05,
      "loss": 0.0036,
      "step": 33590
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.4731476902961731,
      "learning_rate": 4.3e-05,
      "loss": 0.0029,
      "step": 33600
    },
    {
      "epoch": 1.1203333333333334,
      "grad_norm": 0.1185159832239151,
      "learning_rate": 4.2997916666666674e-05,
      "loss": 0.0028,
      "step": 33610
    },
    {
      "epoch": 1.1206666666666667,
      "grad_norm": 0.08881223946809769,
      "learning_rate": 4.299583333333333e-05,
      "loss": 0.0014,
      "step": 33620
    },
    {
      "epoch": 1.121,
      "grad_norm": 0.06297144293785095,
      "learning_rate": 4.299375e-05,
      "loss": 0.0023,
      "step": 33630
    },
    {
      "epoch": 1.1213333333333333,
      "grad_norm": 0.5911084413528442,
      "learning_rate": 4.299166666666667e-05,
      "loss": 0.0023,
      "step": 33640
    },
    {
      "epoch": 1.1216666666666666,
      "grad_norm": 0.24776111543178558,
      "learning_rate": 4.2989583333333336e-05,
      "loss": 0.0027,
      "step": 33650
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 0.17702801525592804,
      "learning_rate": 4.29875e-05,
      "loss": 0.0024,
      "step": 33660
    },
    {
      "epoch": 1.1223333333333334,
      "grad_norm": 0.35395556688308716,
      "learning_rate": 4.298541666666667e-05,
      "loss": 0.0024,
      "step": 33670
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.563887357711792,
      "learning_rate": 4.298333333333334e-05,
      "loss": 0.0027,
      "step": 33680
    },
    {
      "epoch": 1.123,
      "grad_norm": 0.17736245691776276,
      "learning_rate": 4.298125e-05,
      "loss": 0.0037,
      "step": 33690
    },
    {
      "epoch": 1.1233333333333333,
      "grad_norm": 0.4135400056838989,
      "learning_rate": 4.297916666666667e-05,
      "loss": 0.0021,
      "step": 33700
    },
    {
      "epoch": 1.1236666666666666,
      "grad_norm": 0.005801044870167971,
      "learning_rate": 4.2977083333333336e-05,
      "loss": 0.0025,
      "step": 33710
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.11824864149093628,
      "learning_rate": 4.2975e-05,
      "loss": 0.0027,
      "step": 33720
    },
    {
      "epoch": 1.1243333333333334,
      "grad_norm": 0.26585566997528076,
      "learning_rate": 4.2972916666666667e-05,
      "loss": 0.0024,
      "step": 33730
    },
    {
      "epoch": 1.1246666666666667,
      "grad_norm": 0.4156467914581299,
      "learning_rate": 4.297083333333334e-05,
      "loss": 0.0033,
      "step": 33740
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.1971920132637024,
      "learning_rate": 4.2968750000000004e-05,
      "loss": 0.0029,
      "step": 33750
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.2952185273170471,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.002,
      "step": 33760
    },
    {
      "epoch": 1.1256666666666666,
      "grad_norm": 0.20700564980506897,
      "learning_rate": 4.2964583333333335e-05,
      "loss": 0.0021,
      "step": 33770
    },
    {
      "epoch": 1.126,
      "grad_norm": 0.2069544792175293,
      "learning_rate": 4.29625e-05,
      "loss": 0.0023,
      "step": 33780
    },
    {
      "epoch": 1.1263333333333334,
      "grad_norm": 0.44304198026657104,
      "learning_rate": 4.296041666666667e-05,
      "loss": 0.002,
      "step": 33790
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.2957269251346588,
      "learning_rate": 4.295833333333333e-05,
      "loss": 0.0022,
      "step": 33800
    },
    {
      "epoch": 1.127,
      "grad_norm": 0.26598677039146423,
      "learning_rate": 4.2956250000000004e-05,
      "loss": 0.003,
      "step": 33810
    },
    {
      "epoch": 1.1273333333333333,
      "grad_norm": 0.147963747382164,
      "learning_rate": 4.295416666666667e-05,
      "loss": 0.0024,
      "step": 33820
    },
    {
      "epoch": 1.1276666666666666,
      "grad_norm": 0.5613671541213989,
      "learning_rate": 4.2952083333333335e-05,
      "loss": 0.003,
      "step": 33830
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.0299916323274374,
      "learning_rate": 4.295e-05,
      "loss": 0.0049,
      "step": 33840
    },
    {
      "epoch": 1.1283333333333334,
      "grad_norm": 0.4432311952114105,
      "learning_rate": 4.294791666666667e-05,
      "loss": 0.0029,
      "step": 33850
    },
    {
      "epoch": 1.1286666666666667,
      "grad_norm": 0.08885964006185532,
      "learning_rate": 4.294583333333334e-05,
      "loss": 0.0021,
      "step": 33860
    },
    {
      "epoch": 1.129,
      "grad_norm": 0.1771373301744461,
      "learning_rate": 4.2943750000000004e-05,
      "loss": 0.0022,
      "step": 33870
    },
    {
      "epoch": 1.1293333333333333,
      "grad_norm": 0.20694488286972046,
      "learning_rate": 4.294166666666667e-05,
      "loss": 0.0022,
      "step": 33880
    },
    {
      "epoch": 1.1296666666666666,
      "grad_norm": 0.20670683681964874,
      "learning_rate": 4.2939583333333335e-05,
      "loss": 0.0039,
      "step": 33890
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.05933532491326332,
      "learning_rate": 4.29375e-05,
      "loss": 0.0035,
      "step": 33900
    },
    {
      "epoch": 1.1303333333333334,
      "grad_norm": 0.05949244648218155,
      "learning_rate": 4.2935416666666666e-05,
      "loss": 0.0024,
      "step": 33910
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.3846890330314636,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0027,
      "step": 33920
    },
    {
      "epoch": 1.131,
      "grad_norm": 0.2653760015964508,
      "learning_rate": 4.293125e-05,
      "loss": 0.0047,
      "step": 33930
    },
    {
      "epoch": 1.1313333333333333,
      "grad_norm": 0.08866136521100998,
      "learning_rate": 4.292916666666667e-05,
      "loss": 0.0029,
      "step": 33940
    },
    {
      "epoch": 1.1316666666666666,
      "grad_norm": 0.05893971025943756,
      "learning_rate": 4.2927083333333334e-05,
      "loss": 0.0035,
      "step": 33950
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.32544922828674316,
      "learning_rate": 4.2925000000000007e-05,
      "loss": 0.0035,
      "step": 33960
    },
    {
      "epoch": 1.1323333333333334,
      "grad_norm": 0.5023828744888306,
      "learning_rate": 4.2922916666666665e-05,
      "loss": 0.0038,
      "step": 33970
    },
    {
      "epoch": 1.1326666666666667,
      "grad_norm": 0.059381209313869476,
      "learning_rate": 4.292083333333334e-05,
      "loss": 0.0027,
      "step": 33980
    },
    {
      "epoch": 1.133,
      "grad_norm": 0.7427665591239929,
      "learning_rate": 4.291875e-05,
      "loss": 0.0026,
      "step": 33990
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.2365882247686386,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.0027,
      "step": 34000
    },
    {
      "epoch": 1.1336666666666666,
      "grad_norm": 0.3253534138202667,
      "learning_rate": 4.2914583333333334e-05,
      "loss": 0.0034,
      "step": 34010
    },
    {
      "epoch": 1.134,
      "grad_norm": 0.38414788246154785,
      "learning_rate": 4.29125e-05,
      "loss": 0.0037,
      "step": 34020
    },
    {
      "epoch": 1.1343333333333334,
      "grad_norm": 0.1777910590171814,
      "learning_rate": 4.291041666666667e-05,
      "loss": 0.0023,
      "step": 34030
    },
    {
      "epoch": 1.1346666666666667,
      "grad_norm": 0.1768745481967926,
      "learning_rate": 4.290833333333333e-05,
      "loss": 0.0032,
      "step": 34040
    },
    {
      "epoch": 1.135,
      "grad_norm": 0.26549869775772095,
      "learning_rate": 4.290625e-05,
      "loss": 0.0024,
      "step": 34050
    },
    {
      "epoch": 1.1353333333333333,
      "grad_norm": 0.2659679353237152,
      "learning_rate": 4.290416666666667e-05,
      "loss": 0.003,
      "step": 34060
    },
    {
      "epoch": 1.1356666666666666,
      "grad_norm": 0.17721134424209595,
      "learning_rate": 4.290208333333334e-05,
      "loss": 0.0032,
      "step": 34070
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.14755311608314514,
      "learning_rate": 4.29e-05,
      "loss": 0.0032,
      "step": 34080
    },
    {
      "epoch": 1.1363333333333334,
      "grad_norm": 0.20659317076206207,
      "learning_rate": 4.289791666666667e-05,
      "loss": 0.0031,
      "step": 34090
    },
    {
      "epoch": 1.1366666666666667,
      "grad_norm": 0.03008236549794674,
      "learning_rate": 4.289583333333334e-05,
      "loss": 0.0028,
      "step": 34100
    },
    {
      "epoch": 1.137,
      "grad_norm": 0.5936679840087891,
      "learning_rate": 4.289375e-05,
      "loss": 0.0027,
      "step": 34110
    },
    {
      "epoch": 1.1373333333333333,
      "grad_norm": 0.29523250460624695,
      "learning_rate": 4.289166666666667e-05,
      "loss": 0.0019,
      "step": 34120
    },
    {
      "epoch": 1.1376666666666666,
      "grad_norm": 0.08869881927967072,
      "learning_rate": 4.2889583333333333e-05,
      "loss": 0.0022,
      "step": 34130
    },
    {
      "epoch": 1.138,
      "grad_norm": 0.26590266823768616,
      "learning_rate": 4.2887500000000006e-05,
      "loss": 0.002,
      "step": 34140
    },
    {
      "epoch": 1.1383333333333334,
      "grad_norm": 0.14783330261707306,
      "learning_rate": 4.2885416666666664e-05,
      "loss": 0.0034,
      "step": 34150
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.26597872376441956,
      "learning_rate": 4.288333333333334e-05,
      "loss": 0.0021,
      "step": 34160
    },
    {
      "epoch": 1.139,
      "grad_norm": 0.26585933566093445,
      "learning_rate": 4.288125e-05,
      "loss": 0.002,
      "step": 34170
    },
    {
      "epoch": 1.1393333333333333,
      "grad_norm": 0.6803895235061646,
      "learning_rate": 4.287916666666667e-05,
      "loss": 0.0027,
      "step": 34180
    },
    {
      "epoch": 1.1396666666666666,
      "grad_norm": 0.3245200514793396,
      "learning_rate": 4.287708333333333e-05,
      "loss": 0.0024,
      "step": 34190
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.15137110650539398,
      "learning_rate": 4.2875000000000005e-05,
      "loss": 0.002,
      "step": 34200
    },
    {
      "epoch": 1.1403333333333334,
      "grad_norm": 0.23733340203762054,
      "learning_rate": 4.287291666666667e-05,
      "loss": 0.0026,
      "step": 34210
    },
    {
      "epoch": 1.1406666666666667,
      "grad_norm": 0.5022163391113281,
      "learning_rate": 4.2870833333333336e-05,
      "loss": 0.0021,
      "step": 34220
    },
    {
      "epoch": 1.141,
      "grad_norm": 0.2659290134906769,
      "learning_rate": 4.286875e-05,
      "loss": 0.0019,
      "step": 34230
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.40253639221191406,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0022,
      "step": 34240
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 0.14755110442638397,
      "learning_rate": 4.286458333333333e-05,
      "loss": 0.0021,
      "step": 34250
    },
    {
      "epoch": 1.142,
      "grad_norm": 0.3252052962779999,
      "learning_rate": 4.28625e-05,
      "loss": 0.0024,
      "step": 34260
    },
    {
      "epoch": 1.1423333333333334,
      "grad_norm": 0.0070035080425441265,
      "learning_rate": 4.286041666666667e-05,
      "loss": 0.0022,
      "step": 34270
    },
    {
      "epoch": 1.1426666666666667,
      "grad_norm": 0.3852595090866089,
      "learning_rate": 4.2858333333333336e-05,
      "loss": 0.0033,
      "step": 34280
    },
    {
      "epoch": 1.143,
      "grad_norm": 0.030526118353009224,
      "learning_rate": 4.285625e-05,
      "loss": 0.0025,
      "step": 34290
    },
    {
      "epoch": 1.1433333333333333,
      "grad_norm": 0.1479499489068985,
      "learning_rate": 4.285416666666667e-05,
      "loss": 0.0033,
      "step": 34300
    },
    {
      "epoch": 1.1436666666666666,
      "grad_norm": 0.2656517028808594,
      "learning_rate": 4.285208333333334e-05,
      "loss": 0.0033,
      "step": 34310
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.005216515157371759,
      "learning_rate": 4.285e-05,
      "loss": 0.003,
      "step": 34320
    },
    {
      "epoch": 1.1443333333333334,
      "grad_norm": 0.06446851044893265,
      "learning_rate": 4.284791666666667e-05,
      "loss": 0.0022,
      "step": 34330
    },
    {
      "epoch": 1.1446666666666667,
      "grad_norm": 0.004305305425077677,
      "learning_rate": 4.2845833333333336e-05,
      "loss": 0.002,
      "step": 34340
    },
    {
      "epoch": 1.145,
      "grad_norm": 0.03063119761645794,
      "learning_rate": 4.284375000000001e-05,
      "loss": 0.0021,
      "step": 34350
    },
    {
      "epoch": 1.1453333333333333,
      "grad_norm": 0.5021274089813232,
      "learning_rate": 4.284166666666667e-05,
      "loss": 0.0033,
      "step": 34360
    },
    {
      "epoch": 1.1456666666666666,
      "grad_norm": 0.029844338074326515,
      "learning_rate": 4.283958333333333e-05,
      "loss": 0.0037,
      "step": 34370
    },
    {
      "epoch": 1.146,
      "grad_norm": 0.009229669347405434,
      "learning_rate": 4.2837500000000004e-05,
      "loss": 0.0028,
      "step": 34380
    },
    {
      "epoch": 1.1463333333333334,
      "grad_norm": 0.41352272033691406,
      "learning_rate": 4.283541666666667e-05,
      "loss": 0.0038,
      "step": 34390
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.008293826133012772,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.003,
      "step": 34400
    },
    {
      "epoch": 1.147,
      "grad_norm": 0.2066648155450821,
      "learning_rate": 4.283125e-05,
      "loss": 0.0022,
      "step": 34410
    },
    {
      "epoch": 1.1473333333333333,
      "grad_norm": 0.05922868475317955,
      "learning_rate": 4.282916666666667e-05,
      "loss": 0.0033,
      "step": 34420
    },
    {
      "epoch": 1.1476666666666666,
      "grad_norm": 0.23636984825134277,
      "learning_rate": 4.282708333333333e-05,
      "loss": 0.0025,
      "step": 34430
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.17709660530090332,
      "learning_rate": 4.2825000000000004e-05,
      "loss": 0.0031,
      "step": 34440
    },
    {
      "epoch": 1.1483333333333334,
      "grad_norm": 0.23637588322162628,
      "learning_rate": 4.282291666666667e-05,
      "loss": 0.0021,
      "step": 34450
    },
    {
      "epoch": 1.1486666666666667,
      "grad_norm": 0.415277361869812,
      "learning_rate": 4.2820833333333335e-05,
      "loss": 0.0029,
      "step": 34460
    },
    {
      "epoch": 1.149,
      "grad_norm": 1.1365647315979004,
      "learning_rate": 4.281875e-05,
      "loss": 0.0027,
      "step": 34470
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.2955171763896942,
      "learning_rate": 4.2816666666666666e-05,
      "loss": 0.0035,
      "step": 34480
    },
    {
      "epoch": 1.1496666666666666,
      "grad_norm": 0.17734286189079285,
      "learning_rate": 4.281458333333334e-05,
      "loss": 0.0022,
      "step": 34490
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.2957428991794586,
      "learning_rate": 4.28125e-05,
      "loss": 0.0022,
      "step": 34500
    },
    {
      "epoch": 1.1503333333333334,
      "grad_norm": 0.08930452167987823,
      "learning_rate": 4.281041666666667e-05,
      "loss": 0.003,
      "step": 34510
    },
    {
      "epoch": 1.1506666666666667,
      "grad_norm": 0.11877970397472382,
      "learning_rate": 4.2808333333333335e-05,
      "loss": 0.0026,
      "step": 34520
    },
    {
      "epoch": 1.151,
      "grad_norm": 0.0891752690076828,
      "learning_rate": 4.280625e-05,
      "loss": 0.0019,
      "step": 34530
    },
    {
      "epoch": 1.1513333333333333,
      "grad_norm": 0.17747409641742706,
      "learning_rate": 4.2804166666666666e-05,
      "loss": 0.0024,
      "step": 34540
    },
    {
      "epoch": 1.1516666666666666,
      "grad_norm": 0.29573291540145874,
      "learning_rate": 4.280208333333334e-05,
      "loss": 0.0022,
      "step": 34550
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.17753005027770996,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.002,
      "step": 34560
    },
    {
      "epoch": 1.1523333333333334,
      "grad_norm": 0.08857810497283936,
      "learning_rate": 4.279791666666667e-05,
      "loss": 0.0021,
      "step": 34570
    },
    {
      "epoch": 1.1526666666666667,
      "grad_norm": 0.23672108352184296,
      "learning_rate": 4.2795833333333335e-05,
      "loss": 0.0022,
      "step": 34580
    },
    {
      "epoch": 1.153,
      "grad_norm": 0.0892503634095192,
      "learning_rate": 4.279375000000001e-05,
      "loss": 0.0022,
      "step": 34590
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.20706872642040253,
      "learning_rate": 4.2791666666666666e-05,
      "loss": 0.0021,
      "step": 34600
    },
    {
      "epoch": 1.1536666666666666,
      "grad_norm": 0.3255934715270996,
      "learning_rate": 4.278958333333333e-05,
      "loss": 0.0033,
      "step": 34610
    },
    {
      "epoch": 1.154,
      "grad_norm": 0.03056148998439312,
      "learning_rate": 4.27875e-05,
      "loss": 0.0024,
      "step": 34620
    },
    {
      "epoch": 1.1543333333333332,
      "grad_norm": 0.14780199527740479,
      "learning_rate": 4.278541666666667e-05,
      "loss": 0.0019,
      "step": 34630
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.030147762969136238,
      "learning_rate": 4.2783333333333334e-05,
      "loss": 0.0026,
      "step": 34640
    },
    {
      "epoch": 1.155,
      "grad_norm": 0.2954934239387512,
      "learning_rate": 4.278125e-05,
      "loss": 0.0032,
      "step": 34650
    },
    {
      "epoch": 1.1553333333333333,
      "grad_norm": 0.14747783541679382,
      "learning_rate": 4.277916666666667e-05,
      "loss": 0.0017,
      "step": 34660
    },
    {
      "epoch": 1.1556666666666666,
      "grad_norm": 0.2956649363040924,
      "learning_rate": 4.277708333333334e-05,
      "loss": 0.0016,
      "step": 34670
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.09283972531557083,
      "learning_rate": 4.2775e-05,
      "loss": 0.0021,
      "step": 34680
    },
    {
      "epoch": 1.1563333333333334,
      "grad_norm": 0.1768140196800232,
      "learning_rate": 4.277291666666667e-05,
      "loss": 0.0027,
      "step": 34690
    },
    {
      "epoch": 1.1566666666666667,
      "grad_norm": 0.1479952335357666,
      "learning_rate": 4.277083333333334e-05,
      "loss": 0.0028,
      "step": 34700
    },
    {
      "epoch": 1.157,
      "grad_norm": 0.23680928349494934,
      "learning_rate": 4.276875e-05,
      "loss": 0.0029,
      "step": 34710
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.059714481234550476,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0021,
      "step": 34720
    },
    {
      "epoch": 1.1576666666666666,
      "grad_norm": 0.07326020300388336,
      "learning_rate": 4.276458333333334e-05,
      "loss": 0.0011,
      "step": 34730
    },
    {
      "epoch": 1.158,
      "grad_norm": 0.38433417677879333,
      "learning_rate": 4.27625e-05,
      "loss": 0.0029,
      "step": 34740
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.11853086948394775,
      "learning_rate": 4.276041666666667e-05,
      "loss": 0.0031,
      "step": 34750
    },
    {
      "epoch": 1.1586666666666667,
      "grad_norm": 0.569054365158081,
      "learning_rate": 4.2758333333333334e-05,
      "loss": 0.0023,
      "step": 34760
    },
    {
      "epoch": 1.159,
      "grad_norm": 0.14781929552555084,
      "learning_rate": 4.2756250000000006e-05,
      "loss": 0.0016,
      "step": 34770
    },
    {
      "epoch": 1.1593333333333333,
      "grad_norm": 0.11857569962739944,
      "learning_rate": 4.2754166666666665e-05,
      "loss": 0.0026,
      "step": 34780
    },
    {
      "epoch": 1.1596666666666666,
      "grad_norm": 0.050008926540613174,
      "learning_rate": 4.275208333333334e-05,
      "loss": 0.0025,
      "step": 34790
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.32526156306266785,
      "learning_rate": 4.275e-05,
      "loss": 0.0016,
      "step": 34800
    },
    {
      "epoch": 1.1603333333333334,
      "grad_norm": 0.029973167926073074,
      "learning_rate": 4.274791666666667e-05,
      "loss": 0.0028,
      "step": 34810
    },
    {
      "epoch": 1.1606666666666667,
      "grad_norm": 0.029760630801320076,
      "learning_rate": 4.274583333333333e-05,
      "loss": 0.002,
      "step": 34820
    },
    {
      "epoch": 1.161,
      "grad_norm": 0.41387662291526794,
      "learning_rate": 4.2743750000000006e-05,
      "loss": 0.0024,
      "step": 34830
    },
    {
      "epoch": 1.1613333333333333,
      "grad_norm": 0.2665160596370697,
      "learning_rate": 4.274166666666667e-05,
      "loss": 0.0029,
      "step": 34840
    },
    {
      "epoch": 1.1616666666666666,
      "grad_norm": 0.7094376683235168,
      "learning_rate": 4.273958333333333e-05,
      "loss": 0.0026,
      "step": 34850
    },
    {
      "epoch": 1.162,
      "grad_norm": 0.4985627233982086,
      "learning_rate": 4.27375e-05,
      "loss": 0.0028,
      "step": 34860
    },
    {
      "epoch": 1.1623333333333332,
      "grad_norm": 0.4433865249156952,
      "learning_rate": 4.273541666666667e-05,
      "loss": 0.0036,
      "step": 34870
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.4136100709438324,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0022,
      "step": 34880
    },
    {
      "epoch": 1.163,
      "grad_norm": 0.3249794542789459,
      "learning_rate": 4.273125e-05,
      "loss": 0.0024,
      "step": 34890
    },
    {
      "epoch": 1.1633333333333333,
      "grad_norm": 0.10224922746419907,
      "learning_rate": 4.272916666666667e-05,
      "loss": 0.0018,
      "step": 34900
    },
    {
      "epoch": 1.1636666666666666,
      "grad_norm": 0.2954569458961487,
      "learning_rate": 4.2727083333333336e-05,
      "loss": 0.0029,
      "step": 34910
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.2953949272632599,
      "learning_rate": 4.2725e-05,
      "loss": 0.0023,
      "step": 34920
    },
    {
      "epoch": 1.1643333333333334,
      "grad_norm": 0.5023676156997681,
      "learning_rate": 4.272291666666667e-05,
      "loss": 0.0022,
      "step": 34930
    },
    {
      "epoch": 1.1646666666666667,
      "grad_norm": 0.05918337404727936,
      "learning_rate": 4.272083333333334e-05,
      "loss": 0.0025,
      "step": 34940
    },
    {
      "epoch": 1.165,
      "grad_norm": 0.620302677154541,
      "learning_rate": 4.2718750000000005e-05,
      "loss": 0.0031,
      "step": 34950
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.03026367537677288,
      "learning_rate": 4.2716666666666664e-05,
      "loss": 0.0021,
      "step": 34960
    },
    {
      "epoch": 1.1656666666666666,
      "grad_norm": 0.11806157231330872,
      "learning_rate": 4.2714583333333336e-05,
      "loss": 0.0024,
      "step": 34970
    },
    {
      "epoch": 1.166,
      "grad_norm": 0.2659173607826233,
      "learning_rate": 4.27125e-05,
      "loss": 0.0028,
      "step": 34980
    },
    {
      "epoch": 1.1663333333333332,
      "grad_norm": 0.08880053460597992,
      "learning_rate": 4.271041666666667e-05,
      "loss": 0.0022,
      "step": 34990
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.35401955246925354,
      "learning_rate": 4.270833333333333e-05,
      "loss": 0.0019,
      "step": 35000
    },
    {
      "epoch": 1.167,
      "grad_norm": 0.2664884328842163,
      "learning_rate": 4.2706250000000005e-05,
      "loss": 0.002,
      "step": 35010
    },
    {
      "epoch": 1.1673333333333333,
      "grad_norm": 0.295156866312027,
      "learning_rate": 4.270416666666667e-05,
      "loss": 0.0031,
      "step": 35020
    },
    {
      "epoch": 1.1676666666666666,
      "grad_norm": 0.2662428319454193,
      "learning_rate": 4.2702083333333336e-05,
      "loss": 0.0018,
      "step": 35030
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.20737139880657196,
      "learning_rate": 4.27e-05,
      "loss": 0.0023,
      "step": 35040
    },
    {
      "epoch": 1.1683333333333334,
      "grad_norm": 0.06002774462103844,
      "learning_rate": 4.2697916666666673e-05,
      "loss": 0.0025,
      "step": 35050
    },
    {
      "epoch": 1.1686666666666667,
      "grad_norm": 0.11845549196004868,
      "learning_rate": 4.269583333333333e-05,
      "loss": 0.0026,
      "step": 35060
    },
    {
      "epoch": 1.169,
      "grad_norm": 0.17743517458438873,
      "learning_rate": 4.2693750000000004e-05,
      "loss": 0.0027,
      "step": 35070
    },
    {
      "epoch": 1.1693333333333333,
      "grad_norm": 0.11825796216726303,
      "learning_rate": 4.269166666666667e-05,
      "loss": 0.0022,
      "step": 35080
    },
    {
      "epoch": 1.1696666666666666,
      "grad_norm": 0.17717953026294708,
      "learning_rate": 4.2689583333333335e-05,
      "loss": 0.003,
      "step": 35090
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.8227889537811279,
      "learning_rate": 4.26875e-05,
      "loss": 0.002,
      "step": 35100
    },
    {
      "epoch": 1.1703333333333332,
      "grad_norm": 0.2662274241447449,
      "learning_rate": 4.2685416666666666e-05,
      "loss": 0.0028,
      "step": 35110
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.32475554943084717,
      "learning_rate": 4.268333333333334e-05,
      "loss": 0.0019,
      "step": 35120
    },
    {
      "epoch": 1.171,
      "grad_norm": 0.02982005663216114,
      "learning_rate": 4.268125e-05,
      "loss": 0.0022,
      "step": 35130
    },
    {
      "epoch": 1.1713333333333333,
      "grad_norm": 0.4427785873413086,
      "learning_rate": 4.267916666666667e-05,
      "loss": 0.0023,
      "step": 35140
    },
    {
      "epoch": 1.1716666666666666,
      "grad_norm": 0.05920925736427307,
      "learning_rate": 4.2677083333333335e-05,
      "loss": 0.0036,
      "step": 35150
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.007162580266594887,
      "learning_rate": 4.2675e-05,
      "loss": 0.0027,
      "step": 35160
    },
    {
      "epoch": 1.1723333333333334,
      "grad_norm": 0.11784294992685318,
      "learning_rate": 4.2672916666666666e-05,
      "loss": 0.0023,
      "step": 35170
    },
    {
      "epoch": 1.1726666666666667,
      "grad_norm": 0.11791416257619858,
      "learning_rate": 4.267083333333334e-05,
      "loss": 0.0023,
      "step": 35180
    },
    {
      "epoch": 1.173,
      "grad_norm": 0.118828184902668,
      "learning_rate": 4.2668750000000004e-05,
      "loss": 0.0031,
      "step": 35190
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.07883259654045105,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0026,
      "step": 35200
    },
    {
      "epoch": 1.1736666666666666,
      "grad_norm": 0.08907972276210785,
      "learning_rate": 4.2664583333333335e-05,
      "loss": 0.0017,
      "step": 35210
    },
    {
      "epoch": 1.174,
      "grad_norm": 0.17742836475372314,
      "learning_rate": 4.26625e-05,
      "loss": 0.0034,
      "step": 35220
    },
    {
      "epoch": 1.1743333333333332,
      "grad_norm": 0.2660226821899414,
      "learning_rate": 4.266041666666667e-05,
      "loss": 0.0036,
      "step": 35230
    },
    {
      "epoch": 1.1746666666666667,
      "grad_norm": 0.17728637158870697,
      "learning_rate": 4.265833333333333e-05,
      "loss": 0.0027,
      "step": 35240
    },
    {
      "epoch": 1.175,
      "grad_norm": 0.14743618667125702,
      "learning_rate": 4.2656250000000003e-05,
      "loss": 0.0037,
      "step": 35250
    },
    {
      "epoch": 1.1753333333333333,
      "grad_norm": 0.08862258493900299,
      "learning_rate": 4.265416666666667e-05,
      "loss": 0.0032,
      "step": 35260
    },
    {
      "epoch": 1.1756666666666666,
      "grad_norm": 0.08890244364738464,
      "learning_rate": 4.2652083333333334e-05,
      "loss": 0.0027,
      "step": 35270
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.0884949341416359,
      "learning_rate": 4.265e-05,
      "loss": 0.0031,
      "step": 35280
    },
    {
      "epoch": 1.1763333333333335,
      "grad_norm": 0.38517165184020996,
      "learning_rate": 4.264791666666667e-05,
      "loss": 0.0033,
      "step": 35290
    },
    {
      "epoch": 1.1766666666666667,
      "grad_norm": 0.20666369795799255,
      "learning_rate": 4.264583333333334e-05,
      "loss": 0.0034,
      "step": 35300
    },
    {
      "epoch": 1.177,
      "grad_norm": 0.7106466293334961,
      "learning_rate": 4.264375e-05,
      "loss": 0.0026,
      "step": 35310
    },
    {
      "epoch": 1.1773333333333333,
      "grad_norm": 0.08874029666185379,
      "learning_rate": 4.264166666666667e-05,
      "loss": 0.0041,
      "step": 35320
    },
    {
      "epoch": 1.1776666666666666,
      "grad_norm": 0.5904065370559692,
      "learning_rate": 4.2639583333333334e-05,
      "loss": 0.0025,
      "step": 35330
    },
    {
      "epoch": 1.178,
      "grad_norm": 0.26566872000694275,
      "learning_rate": 4.26375e-05,
      "loss": 0.0025,
      "step": 35340
    },
    {
      "epoch": 1.1783333333333332,
      "grad_norm": 0.38373616337776184,
      "learning_rate": 4.2635416666666665e-05,
      "loss": 0.0025,
      "step": 35350
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.1474199891090393,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0027,
      "step": 35360
    },
    {
      "epoch": 1.179,
      "grad_norm": 0.20661070942878723,
      "learning_rate": 4.263125e-05,
      "loss": 0.0027,
      "step": 35370
    },
    {
      "epoch": 1.1793333333333333,
      "grad_norm": 0.47222816944122314,
      "learning_rate": 4.262916666666667e-05,
      "loss": 0.0027,
      "step": 35380
    },
    {
      "epoch": 1.1796666666666666,
      "grad_norm": 0.47211647033691406,
      "learning_rate": 4.2627083333333334e-05,
      "loss": 0.0023,
      "step": 35390
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.1772747039794922,
      "learning_rate": 4.2625000000000006e-05,
      "loss": 0.003,
      "step": 35400
    },
    {
      "epoch": 1.1803333333333335,
      "grad_norm": 0.147616907954216,
      "learning_rate": 4.2622916666666665e-05,
      "loss": 0.0036,
      "step": 35410
    },
    {
      "epoch": 1.1806666666666668,
      "grad_norm": 0.3855922818183899,
      "learning_rate": 4.262083333333334e-05,
      "loss": 0.0026,
      "step": 35420
    },
    {
      "epoch": 1.181,
      "grad_norm": 0.177043616771698,
      "learning_rate": 4.261875e-05,
      "loss": 0.0024,
      "step": 35430
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.004490590188652277,
      "learning_rate": 4.261666666666667e-05,
      "loss": 0.0019,
      "step": 35440
    },
    {
      "epoch": 1.1816666666666666,
      "grad_norm": 0.029694849625229836,
      "learning_rate": 4.2614583333333334e-05,
      "loss": 0.0018,
      "step": 35450
    },
    {
      "epoch": 1.182,
      "grad_norm": 0.08873230963945389,
      "learning_rate": 4.26125e-05,
      "loss": 0.0026,
      "step": 35460
    },
    {
      "epoch": 1.1823333333333332,
      "grad_norm": 0.08865022659301758,
      "learning_rate": 4.261041666666667e-05,
      "loss": 0.0017,
      "step": 35470
    },
    {
      "epoch": 1.1826666666666668,
      "grad_norm": 0.33306485414505005,
      "learning_rate": 4.260833333333334e-05,
      "loss": 0.0032,
      "step": 35480
    },
    {
      "epoch": 1.183,
      "grad_norm": 0.11838933825492859,
      "learning_rate": 4.260625e-05,
      "loss": 0.0016,
      "step": 35490
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.5505744218826294,
      "learning_rate": 4.260416666666667e-05,
      "loss": 0.0042,
      "step": 35500
    },
    {
      "epoch": 1.1836666666666666,
      "grad_norm": 0.11612574011087418,
      "learning_rate": 4.260208333333334e-05,
      "loss": 0.0028,
      "step": 35510
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.3566080927848816,
      "learning_rate": 4.26e-05,
      "loss": 0.0051,
      "step": 35520
    },
    {
      "epoch": 1.1843333333333332,
      "grad_norm": 0.29524219036102295,
      "learning_rate": 4.259791666666667e-05,
      "loss": 0.0035,
      "step": 35530
    },
    {
      "epoch": 1.1846666666666668,
      "grad_norm": 0.08853963017463684,
      "learning_rate": 4.2595833333333336e-05,
      "loss": 0.0035,
      "step": 35540
    },
    {
      "epoch": 1.185,
      "grad_norm": 0.6198630332946777,
      "learning_rate": 4.259375e-05,
      "loss": 0.0019,
      "step": 35550
    },
    {
      "epoch": 1.1853333333333333,
      "grad_norm": 0.6785781383514404,
      "learning_rate": 4.259166666666667e-05,
      "loss": 0.0027,
      "step": 35560
    },
    {
      "epoch": 1.1856666666666666,
      "grad_norm": 0.4132827818393707,
      "learning_rate": 4.258958333333333e-05,
      "loss": 0.004,
      "step": 35570
    },
    {
      "epoch": 1.186,
      "grad_norm": 0.029753994196653366,
      "learning_rate": 4.2587500000000005e-05,
      "loss": 0.0033,
      "step": 35580
    },
    {
      "epoch": 1.1863333333333332,
      "grad_norm": 0.006129569374024868,
      "learning_rate": 4.2585416666666664e-05,
      "loss": 0.003,
      "step": 35590
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.26595690846443176,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.0021,
      "step": 35600
    },
    {
      "epoch": 1.187,
      "grad_norm": 0.06532922387123108,
      "learning_rate": 4.258125e-05,
      "loss": 0.0041,
      "step": 35610
    },
    {
      "epoch": 1.1873333333333334,
      "grad_norm": 0.29542648792266846,
      "learning_rate": 4.257916666666667e-05,
      "loss": 0.0024,
      "step": 35620
    },
    {
      "epoch": 1.1876666666666666,
      "grad_norm": 0.059142760932445526,
      "learning_rate": 4.257708333333333e-05,
      "loss": 0.0021,
      "step": 35630
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.23632411658763885,
      "learning_rate": 4.2575000000000005e-05,
      "loss": 0.0036,
      "step": 35640
    },
    {
      "epoch": 1.1883333333333332,
      "grad_norm": 0.0298849456012249,
      "learning_rate": 4.257291666666667e-05,
      "loss": 0.0026,
      "step": 35650
    },
    {
      "epoch": 1.1886666666666668,
      "grad_norm": 0.1771216094493866,
      "learning_rate": 4.2570833333333336e-05,
      "loss": 0.0026,
      "step": 35660
    },
    {
      "epoch": 1.189,
      "grad_norm": 0.1773635745048523,
      "learning_rate": 4.256875e-05,
      "loss": 0.0022,
      "step": 35670
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.06011606752872467,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0035,
      "step": 35680
    },
    {
      "epoch": 1.1896666666666667,
      "grad_norm": 0.1768220216035843,
      "learning_rate": 4.256458333333333e-05,
      "loss": 0.0015,
      "step": 35690
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.004829138517379761,
      "learning_rate": 4.25625e-05,
      "loss": 0.0037,
      "step": 35700
    },
    {
      "epoch": 1.1903333333333332,
      "grad_norm": 0.35447239875793457,
      "learning_rate": 4.256041666666667e-05,
      "loss": 0.0017,
      "step": 35710
    },
    {
      "epoch": 1.1906666666666668,
      "grad_norm": 0.029844487085938454,
      "learning_rate": 4.2558333333333336e-05,
      "loss": 0.002,
      "step": 35720
    },
    {
      "epoch": 1.191,
      "grad_norm": 0.20744115114212036,
      "learning_rate": 4.255625e-05,
      "loss": 0.004,
      "step": 35730
    },
    {
      "epoch": 1.1913333333333334,
      "grad_norm": 0.8459535837173462,
      "learning_rate": 4.2554166666666667e-05,
      "loss": 0.0023,
      "step": 35740
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 0.38356542587280273,
      "learning_rate": 4.255208333333334e-05,
      "loss": 0.0038,
      "step": 35750
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.2064310759305954,
      "learning_rate": 4.2550000000000004e-05,
      "loss": 0.0026,
      "step": 35760
    },
    {
      "epoch": 1.1923333333333332,
      "grad_norm": 0.295024573802948,
      "learning_rate": 4.254791666666667e-05,
      "loss": 0.0028,
      "step": 35770
    },
    {
      "epoch": 1.1926666666666668,
      "grad_norm": 0.11826223134994507,
      "learning_rate": 4.2545833333333335e-05,
      "loss": 0.0027,
      "step": 35780
    },
    {
      "epoch": 1.193,
      "grad_norm": 0.05968865007162094,
      "learning_rate": 4.254375000000001e-05,
      "loss": 0.0034,
      "step": 35790
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.17703475058078766,
      "learning_rate": 4.2541666666666666e-05,
      "loss": 0.0025,
      "step": 35800
    },
    {
      "epoch": 1.1936666666666667,
      "grad_norm": 0.38369569182395935,
      "learning_rate": 4.253958333333333e-05,
      "loss": 0.0028,
      "step": 35810
    },
    {
      "epoch": 1.194,
      "grad_norm": 0.35431209206581116,
      "learning_rate": 4.2537500000000004e-05,
      "loss": 0.0023,
      "step": 35820
    },
    {
      "epoch": 1.1943333333333332,
      "grad_norm": 0.2655571401119232,
      "learning_rate": 4.253541666666667e-05,
      "loss": 0.0021,
      "step": 35830
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.5024008750915527,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0018,
      "step": 35840
    },
    {
      "epoch": 1.195,
      "grad_norm": 0.9125619530677795,
      "learning_rate": 4.253125e-05,
      "loss": 0.003,
      "step": 35850
    },
    {
      "epoch": 1.1953333333333334,
      "grad_norm": 0.004762433934956789,
      "learning_rate": 4.252916666666667e-05,
      "loss": 0.0022,
      "step": 35860
    },
    {
      "epoch": 1.1956666666666667,
      "grad_norm": 0.11797667294740677,
      "learning_rate": 4.252708333333333e-05,
      "loss": 0.002,
      "step": 35870
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.08879632502794266,
      "learning_rate": 4.2525000000000004e-05,
      "loss": 0.0042,
      "step": 35880
    },
    {
      "epoch": 1.1963333333333332,
      "grad_norm": 0.08855031430721283,
      "learning_rate": 4.252291666666667e-05,
      "loss": 0.007,
      "step": 35890
    },
    {
      "epoch": 1.1966666666666668,
      "grad_norm": 0.47702381014823914,
      "learning_rate": 4.2520833333333335e-05,
      "loss": 0.0056,
      "step": 35900
    },
    {
      "epoch": 1.197,
      "grad_norm": 0.4428449273109436,
      "learning_rate": 4.251875e-05,
      "loss": 0.0045,
      "step": 35910
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.14769233763217926,
      "learning_rate": 4.251666666666667e-05,
      "loss": 0.003,
      "step": 35920
    },
    {
      "epoch": 1.1976666666666667,
      "grad_norm": 0.412830114364624,
      "learning_rate": 4.251458333333334e-05,
      "loss": 0.0026,
      "step": 35930
    },
    {
      "epoch": 1.198,
      "grad_norm": 0.17687153816223145,
      "learning_rate": 4.2512499999999997e-05,
      "loss": 0.0043,
      "step": 35940
    },
    {
      "epoch": 1.1983333333333333,
      "grad_norm": 0.5898876786231995,
      "learning_rate": 4.251041666666667e-05,
      "loss": 0.0026,
      "step": 35950
    },
    {
      "epoch": 1.1986666666666665,
      "grad_norm": 0.4522780478000641,
      "learning_rate": 4.2508333333333334e-05,
      "loss": 0.0046,
      "step": 35960
    },
    {
      "epoch": 1.199,
      "grad_norm": 0.5607527494430542,
      "learning_rate": 4.250625e-05,
      "loss": 0.0026,
      "step": 35970
    },
    {
      "epoch": 1.1993333333333334,
      "grad_norm": 0.05958535149693489,
      "learning_rate": 4.2504166666666665e-05,
      "loss": 0.0036,
      "step": 35980
    },
    {
      "epoch": 1.1996666666666667,
      "grad_norm": 0.3541061282157898,
      "learning_rate": 4.250208333333334e-05,
      "loss": 0.0017,
      "step": 35990
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.2657254636287689,
      "learning_rate": 4.25e-05,
      "loss": 0.002,
      "step": 36000
    },
    {
      "epoch": 1.2003333333333333,
      "grad_norm": 0.05969827622175217,
      "learning_rate": 4.249791666666667e-05,
      "loss": 0.0031,
      "step": 36010
    },
    {
      "epoch": 1.2006666666666668,
      "grad_norm": 0.2655209004878998,
      "learning_rate": 4.2495833333333334e-05,
      "loss": 0.0038,
      "step": 36020
    },
    {
      "epoch": 1.201,
      "grad_norm": 0.5895987153053284,
      "learning_rate": 4.2493750000000006e-05,
      "loss": 0.0027,
      "step": 36030
    },
    {
      "epoch": 1.2013333333333334,
      "grad_norm": 0.600607693195343,
      "learning_rate": 4.249166666666667e-05,
      "loss": 0.002,
      "step": 36040
    },
    {
      "epoch": 1.2016666666666667,
      "grad_norm": 0.5309355854988098,
      "learning_rate": 4.248958333333333e-05,
      "loss": 0.0032,
      "step": 36050
    },
    {
      "epoch": 1.202,
      "grad_norm": 0.29523780941963196,
      "learning_rate": 4.24875e-05,
      "loss": 0.0031,
      "step": 36060
    },
    {
      "epoch": 1.2023333333333333,
      "grad_norm": 0.1478058248758316,
      "learning_rate": 4.248541666666667e-05,
      "loss": 0.0028,
      "step": 36070
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.23630431294441223,
      "learning_rate": 4.2483333333333334e-05,
      "loss": 0.0039,
      "step": 36080
    },
    {
      "epoch": 1.203,
      "grad_norm": 0.05924814194440842,
      "learning_rate": 4.248125e-05,
      "loss": 0.0033,
      "step": 36090
    },
    {
      "epoch": 1.2033333333333334,
      "grad_norm": 0.008689562790095806,
      "learning_rate": 4.247916666666667e-05,
      "loss": 0.0035,
      "step": 36100
    },
    {
      "epoch": 1.2036666666666667,
      "grad_norm": 0.5646119713783264,
      "learning_rate": 4.247708333333334e-05,
      "loss": 0.0025,
      "step": 36110
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.2360224425792694,
      "learning_rate": 4.2475e-05,
      "loss": 0.0025,
      "step": 36120
    },
    {
      "epoch": 1.2043333333333333,
      "grad_norm": 0.05942068248987198,
      "learning_rate": 4.247291666666667e-05,
      "loss": 0.0021,
      "step": 36130
    },
    {
      "epoch": 1.2046666666666668,
      "grad_norm": 0.35380980372428894,
      "learning_rate": 4.247083333333334e-05,
      "loss": 0.003,
      "step": 36140
    },
    {
      "epoch": 1.205,
      "grad_norm": 0.14738519489765167,
      "learning_rate": 4.246875e-05,
      "loss": 0.0033,
      "step": 36150
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.08874718099832535,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0023,
      "step": 36160
    },
    {
      "epoch": 1.2056666666666667,
      "grad_norm": 0.05978332832455635,
      "learning_rate": 4.246458333333334e-05,
      "loss": 0.0019,
      "step": 36170
    },
    {
      "epoch": 1.206,
      "grad_norm": 0.3531639277935028,
      "learning_rate": 4.24625e-05,
      "loss": 0.0026,
      "step": 36180
    },
    {
      "epoch": 1.2063333333333333,
      "grad_norm": 0.2655407190322876,
      "learning_rate": 4.246041666666667e-05,
      "loss": 0.0016,
      "step": 36190
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.23608921468257904,
      "learning_rate": 4.245833333333333e-05,
      "loss": 0.0026,
      "step": 36200
    },
    {
      "epoch": 1.207,
      "grad_norm": 0.32477322220802307,
      "learning_rate": 4.2456250000000005e-05,
      "loss": 0.003,
      "step": 36210
    },
    {
      "epoch": 1.2073333333333334,
      "grad_norm": 0.030777400359511375,
      "learning_rate": 4.2454166666666664e-05,
      "loss": 0.0022,
      "step": 36220
    },
    {
      "epoch": 1.2076666666666667,
      "grad_norm": 0.030053626745939255,
      "learning_rate": 4.2452083333333336e-05,
      "loss": 0.0028,
      "step": 36230
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.5943909287452698,
      "learning_rate": 4.245e-05,
      "loss": 0.0032,
      "step": 36240
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.20069485902786255,
      "learning_rate": 4.244791666666667e-05,
      "loss": 0.0018,
      "step": 36250
    },
    {
      "epoch": 1.2086666666666668,
      "grad_norm": 0.02960841730237007,
      "learning_rate": 4.244583333333333e-05,
      "loss": 0.0015,
      "step": 36260
    },
    {
      "epoch": 1.209,
      "grad_norm": 0.05932477489113808,
      "learning_rate": 4.2443750000000005e-05,
      "loss": 0.0044,
      "step": 36270
    },
    {
      "epoch": 1.2093333333333334,
      "grad_norm": 0.32463428378105164,
      "learning_rate": 4.244166666666667e-05,
      "loss": 0.0025,
      "step": 36280
    },
    {
      "epoch": 1.2096666666666667,
      "grad_norm": 0.0593121275305748,
      "learning_rate": 4.2439583333333336e-05,
      "loss": 0.002,
      "step": 36290
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.6714777946472168,
      "learning_rate": 4.24375e-05,
      "loss": 0.0024,
      "step": 36300
    },
    {
      "epoch": 1.2103333333333333,
      "grad_norm": 0.029693657532334328,
      "learning_rate": 4.243541666666667e-05,
      "loss": 0.0022,
      "step": 36310
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.05914749577641487,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.003,
      "step": 36320
    },
    {
      "epoch": 1.211,
      "grad_norm": 0.14756856858730316,
      "learning_rate": 4.243125e-05,
      "loss": 0.0018,
      "step": 36330
    },
    {
      "epoch": 1.2113333333333334,
      "grad_norm": 0.26559707522392273,
      "learning_rate": 4.242916666666667e-05,
      "loss": 0.0031,
      "step": 36340
    },
    {
      "epoch": 1.2116666666666667,
      "grad_norm": 0.41349223256111145,
      "learning_rate": 4.2427083333333336e-05,
      "loss": 0.0029,
      "step": 36350
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.08919622004032135,
      "learning_rate": 4.2425e-05,
      "loss": 0.0025,
      "step": 36360
    },
    {
      "epoch": 1.2123333333333333,
      "grad_norm": 0.030561279505491257,
      "learning_rate": 4.242291666666667e-05,
      "loss": 0.0028,
      "step": 36370
    },
    {
      "epoch": 1.2126666666666668,
      "grad_norm": 0.08888448029756546,
      "learning_rate": 4.242083333333334e-05,
      "loss": 0.0033,
      "step": 36380
    },
    {
      "epoch": 1.213,
      "grad_norm": 0.05899520963430405,
      "learning_rate": 4.2418750000000004e-05,
      "loss": 0.0027,
      "step": 36390
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.0592680461704731,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.0033,
      "step": 36400
    },
    {
      "epoch": 1.2136666666666667,
      "grad_norm": 0.1482439935207367,
      "learning_rate": 4.2414583333333335e-05,
      "loss": 0.0027,
      "step": 36410
    },
    {
      "epoch": 1.214,
      "grad_norm": 0.20672214031219482,
      "learning_rate": 4.24125e-05,
      "loss": 0.0018,
      "step": 36420
    },
    {
      "epoch": 1.2143333333333333,
      "grad_norm": 0.14752070605754852,
      "learning_rate": 4.2410416666666666e-05,
      "loss": 0.0025,
      "step": 36430
    },
    {
      "epoch": 1.2146666666666666,
      "grad_norm": 0.5604068636894226,
      "learning_rate": 4.240833333333333e-05,
      "loss": 0.0039,
      "step": 36440
    },
    {
      "epoch": 1.215,
      "grad_norm": 0.4133813977241516,
      "learning_rate": 4.2406250000000004e-05,
      "loss": 0.0027,
      "step": 36450
    },
    {
      "epoch": 1.2153333333333334,
      "grad_norm": 0.35406431555747986,
      "learning_rate": 4.240416666666667e-05,
      "loss": 0.0027,
      "step": 36460
    },
    {
      "epoch": 1.2156666666666667,
      "grad_norm": 0.14734983444213867,
      "learning_rate": 4.2402083333333335e-05,
      "loss": 0.0022,
      "step": 36470
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.08861556649208069,
      "learning_rate": 4.24e-05,
      "loss": 0.0026,
      "step": 36480
    },
    {
      "epoch": 1.2163333333333333,
      "grad_norm": 0.41233161091804504,
      "learning_rate": 4.239791666666667e-05,
      "loss": 0.0024,
      "step": 36490
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.030356518924236298,
      "learning_rate": 4.239583333333333e-05,
      "loss": 0.0039,
      "step": 36500
    },
    {
      "epoch": 1.217,
      "grad_norm": 0.03023502603173256,
      "learning_rate": 4.2393750000000004e-05,
      "loss": 0.0024,
      "step": 36510
    },
    {
      "epoch": 1.2173333333333334,
      "grad_norm": 0.11805464327335358,
      "learning_rate": 4.239166666666667e-05,
      "loss": 0.003,
      "step": 36520
    },
    {
      "epoch": 1.2176666666666667,
      "grad_norm": 0.5308729410171509,
      "learning_rate": 4.2389583333333335e-05,
      "loss": 0.0032,
      "step": 36530
    },
    {
      "epoch": 1.218,
      "grad_norm": 0.2943960130214691,
      "learning_rate": 4.23875e-05,
      "loss": 0.003,
      "step": 36540
    },
    {
      "epoch": 1.2183333333333333,
      "grad_norm": 0.25739040970802307,
      "learning_rate": 4.2385416666666666e-05,
      "loss": 0.0022,
      "step": 36550
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.3243948817253113,
      "learning_rate": 4.238333333333334e-05,
      "loss": 0.0035,
      "step": 36560
    },
    {
      "epoch": 1.219,
      "grad_norm": 0.17718033492565155,
      "learning_rate": 4.238125e-05,
      "loss": 0.0028,
      "step": 36570
    },
    {
      "epoch": 1.2193333333333334,
      "grad_norm": 0.006669850088655949,
      "learning_rate": 4.237916666666667e-05,
      "loss": 0.0023,
      "step": 36580
    },
    {
      "epoch": 1.2196666666666667,
      "grad_norm": 0.44207125902175903,
      "learning_rate": 4.2377083333333335e-05,
      "loss": 0.0034,
      "step": 36590
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.1178925409913063,
      "learning_rate": 4.237500000000001e-05,
      "loss": 0.0024,
      "step": 36600
    },
    {
      "epoch": 1.2203333333333333,
      "grad_norm": 0.029927263036370277,
      "learning_rate": 4.2372916666666666e-05,
      "loss": 0.0023,
      "step": 36610
    },
    {
      "epoch": 1.2206666666666668,
      "grad_norm": 0.1472773551940918,
      "learning_rate": 4.237083333333334e-05,
      "loss": 0.0023,
      "step": 36620
    },
    {
      "epoch": 1.221,
      "grad_norm": 0.0887075662612915,
      "learning_rate": 4.236875e-05,
      "loss": 0.0031,
      "step": 36630
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.029950736090540886,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0029,
      "step": 36640
    },
    {
      "epoch": 1.2216666666666667,
      "grad_norm": 0.5601698756217957,
      "learning_rate": 4.2364583333333334e-05,
      "loss": 0.0022,
      "step": 36650
    },
    {
      "epoch": 1.222,
      "grad_norm": 0.2116241306066513,
      "learning_rate": 4.23625e-05,
      "loss": 0.003,
      "step": 36660
    },
    {
      "epoch": 1.2223333333333333,
      "grad_norm": 0.1182202696800232,
      "learning_rate": 4.236041666666667e-05,
      "loss": 0.002,
      "step": 36670
    },
    {
      "epoch": 1.2226666666666666,
      "grad_norm": 0.41779419779777527,
      "learning_rate": 4.235833333333333e-05,
      "loss": 0.0027,
      "step": 36680
    },
    {
      "epoch": 1.223,
      "grad_norm": 0.20635870099067688,
      "learning_rate": 4.235625e-05,
      "loss": 0.0032,
      "step": 36690
    },
    {
      "epoch": 1.2233333333333334,
      "grad_norm": 0.4293375015258789,
      "learning_rate": 4.235416666666667e-05,
      "loss": 0.0028,
      "step": 36700
    },
    {
      "epoch": 1.2236666666666667,
      "grad_norm": 0.0055785104632377625,
      "learning_rate": 4.2352083333333334e-05,
      "loss": 0.003,
      "step": 36710
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.1473531872034073,
      "learning_rate": 4.235e-05,
      "loss": 0.0018,
      "step": 36720
    },
    {
      "epoch": 1.2243333333333333,
      "grad_norm": 0.7078624367713928,
      "learning_rate": 4.234791666666667e-05,
      "loss": 0.0026,
      "step": 36730
    },
    {
      "epoch": 1.2246666666666666,
      "grad_norm": 0.44242411851882935,
      "learning_rate": 4.234583333333334e-05,
      "loss": 0.0025,
      "step": 36740
    },
    {
      "epoch": 1.225,
      "grad_norm": 0.17665567994117737,
      "learning_rate": 4.234375e-05,
      "loss": 0.003,
      "step": 36750
    },
    {
      "epoch": 1.2253333333333334,
      "grad_norm": 0.14791078865528107,
      "learning_rate": 4.234166666666667e-05,
      "loss": 0.0021,
      "step": 36760
    },
    {
      "epoch": 1.2256666666666667,
      "grad_norm": 0.060155488550662994,
      "learning_rate": 4.233958333333334e-05,
      "loss": 0.0025,
      "step": 36770
    },
    {
      "epoch": 1.226,
      "grad_norm": 0.6041613221168518,
      "learning_rate": 4.23375e-05,
      "loss": 0.0017,
      "step": 36780
    },
    {
      "epoch": 1.2263333333333333,
      "grad_norm": 0.2209492176771164,
      "learning_rate": 4.2335416666666665e-05,
      "loss": 0.003,
      "step": 36790
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.7752796411514282,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0022,
      "step": 36800
    },
    {
      "epoch": 1.227,
      "grad_norm": 0.5801509618759155,
      "learning_rate": 4.233125e-05,
      "loss": 0.0029,
      "step": 36810
    },
    {
      "epoch": 1.2273333333333334,
      "grad_norm": 0.11792021989822388,
      "learning_rate": 4.232916666666667e-05,
      "loss": 0.0034,
      "step": 36820
    },
    {
      "epoch": 1.2276666666666667,
      "grad_norm": 0.35368821024894714,
      "learning_rate": 4.232708333333333e-05,
      "loss": 0.002,
      "step": 36830
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.2654014527797699,
      "learning_rate": 4.2325000000000006e-05,
      "loss": 0.0031,
      "step": 36840
    },
    {
      "epoch": 1.2283333333333333,
      "grad_norm": 0.14746251702308655,
      "learning_rate": 4.2322916666666664e-05,
      "loss": 0.0039,
      "step": 36850
    },
    {
      "epoch": 1.2286666666666666,
      "grad_norm": 0.20627690851688385,
      "learning_rate": 4.2320833333333337e-05,
      "loss": 0.0035,
      "step": 36860
    },
    {
      "epoch": 1.229,
      "grad_norm": 0.3536643385887146,
      "learning_rate": 4.231875e-05,
      "loss": 0.0025,
      "step": 36870
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.006516664754599333,
      "learning_rate": 4.2316666666666674e-05,
      "loss": 0.0025,
      "step": 36880
    },
    {
      "epoch": 1.2296666666666667,
      "grad_norm": 0.1769665628671646,
      "learning_rate": 4.231458333333333e-05,
      "loss": 0.0024,
      "step": 36890
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.3242860436439514,
      "learning_rate": 4.23125e-05,
      "loss": 0.0025,
      "step": 36900
    },
    {
      "epoch": 1.2303333333333333,
      "grad_norm": 0.3241822421550751,
      "learning_rate": 4.231041666666667e-05,
      "loss": 0.0015,
      "step": 36910
    },
    {
      "epoch": 1.2306666666666666,
      "grad_norm": 0.14741644263267517,
      "learning_rate": 4.2308333333333336e-05,
      "loss": 0.0031,
      "step": 36920
    },
    {
      "epoch": 1.231,
      "grad_norm": 0.8469077944755554,
      "learning_rate": 4.230625e-05,
      "loss": 0.0022,
      "step": 36930
    },
    {
      "epoch": 1.2313333333333334,
      "grad_norm": 0.2356942594051361,
      "learning_rate": 4.230416666666667e-05,
      "loss": 0.0018,
      "step": 36940
    },
    {
      "epoch": 1.2316666666666667,
      "grad_norm": 0.004356455057859421,
      "learning_rate": 4.230208333333334e-05,
      "loss": 0.0014,
      "step": 36950
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.0904804989695549,
      "learning_rate": 4.23e-05,
      "loss": 0.0027,
      "step": 36960
    },
    {
      "epoch": 1.2323333333333333,
      "grad_norm": 0.3244045376777649,
      "learning_rate": 4.229791666666667e-05,
      "loss": 0.0031,
      "step": 36970
    },
    {
      "epoch": 1.2326666666666666,
      "grad_norm": 0.3243090510368347,
      "learning_rate": 4.2295833333333336e-05,
      "loss": 0.0017,
      "step": 36980
    },
    {
      "epoch": 1.233,
      "grad_norm": 0.03069927543401718,
      "learning_rate": 4.229375e-05,
      "loss": 0.0028,
      "step": 36990
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.03616652637720108,
      "learning_rate": 4.229166666666667e-05,
      "loss": 0.0018,
      "step": 37000
    },
    {
      "epoch": 1.2336666666666667,
      "grad_norm": 0.32445067167282104,
      "learning_rate": 4.228958333333334e-05,
      "loss": 0.0023,
      "step": 37010
    },
    {
      "epoch": 1.234,
      "grad_norm": 0.23608902096748352,
      "learning_rate": 4.2287500000000005e-05,
      "loss": 0.0019,
      "step": 37020
    },
    {
      "epoch": 1.2343333333333333,
      "grad_norm": 0.11831356585025787,
      "learning_rate": 4.228541666666666e-05,
      "loss": 0.0028,
      "step": 37030
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.35387563705444336,
      "learning_rate": 4.2283333333333336e-05,
      "loss": 0.0019,
      "step": 37040
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 0.1765364110469818,
      "learning_rate": 4.228125e-05,
      "loss": 0.0031,
      "step": 37050
    },
    {
      "epoch": 1.2353333333333334,
      "grad_norm": 0.6331924796104431,
      "learning_rate": 4.2279166666666667e-05,
      "loss": 0.0025,
      "step": 37060
    },
    {
      "epoch": 1.2356666666666667,
      "grad_norm": 0.38350963592529297,
      "learning_rate": 4.227708333333333e-05,
      "loss": 0.0014,
      "step": 37070
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.08940354734659195,
      "learning_rate": 4.2275000000000004e-05,
      "loss": 0.0012,
      "step": 37080
    },
    {
      "epoch": 1.2363333333333333,
      "grad_norm": 0.4125839173793793,
      "learning_rate": 4.227291666666667e-05,
      "loss": 0.0027,
      "step": 37090
    },
    {
      "epoch": 1.2366666666666666,
      "grad_norm": 0.3830724060535431,
      "learning_rate": 4.2270833333333335e-05,
      "loss": 0.0031,
      "step": 37100
    },
    {
      "epoch": 1.237,
      "grad_norm": 0.11845169216394424,
      "learning_rate": 4.226875e-05,
      "loss": 0.0024,
      "step": 37110
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.5894089937210083,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0032,
      "step": 37120
    },
    {
      "epoch": 1.2376666666666667,
      "grad_norm": 0.08842702209949493,
      "learning_rate": 4.226458333333334e-05,
      "loss": 0.0025,
      "step": 37130
    },
    {
      "epoch": 1.238,
      "grad_norm": 0.2946297824382782,
      "learning_rate": 4.22625e-05,
      "loss": 0.0037,
      "step": 37140
    },
    {
      "epoch": 1.2383333333333333,
      "grad_norm": 0.03012564592063427,
      "learning_rate": 4.226041666666667e-05,
      "loss": 0.0031,
      "step": 37150
    },
    {
      "epoch": 1.2386666666666666,
      "grad_norm": 0.2821183502674103,
      "learning_rate": 4.2258333333333335e-05,
      "loss": 0.0035,
      "step": 37160
    },
    {
      "epoch": 1.2389999999999999,
      "grad_norm": 0.2066689282655716,
      "learning_rate": 4.225625e-05,
      "loss": 0.0032,
      "step": 37170
    },
    {
      "epoch": 1.2393333333333334,
      "grad_norm": 0.14749868214130402,
      "learning_rate": 4.2254166666666666e-05,
      "loss": 0.0025,
      "step": 37180
    },
    {
      "epoch": 1.2396666666666667,
      "grad_norm": 0.38324305415153503,
      "learning_rate": 4.225208333333334e-05,
      "loss": 0.0043,
      "step": 37190
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.2949865758419037,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.0028,
      "step": 37200
    },
    {
      "epoch": 1.2403333333333333,
      "grad_norm": 0.2067103087902069,
      "learning_rate": 4.224791666666667e-05,
      "loss": 0.0031,
      "step": 37210
    },
    {
      "epoch": 1.2406666666666666,
      "grad_norm": 0.02974911965429783,
      "learning_rate": 4.2245833333333335e-05,
      "loss": 0.0023,
      "step": 37220
    },
    {
      "epoch": 1.241,
      "grad_norm": 0.058938439935445786,
      "learning_rate": 4.224375000000001e-05,
      "loss": 0.0033,
      "step": 37230
    },
    {
      "epoch": 1.2413333333333334,
      "grad_norm": 0.35369881987571716,
      "learning_rate": 4.2241666666666666e-05,
      "loss": 0.0028,
      "step": 37240
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 0.17782855033874512,
      "learning_rate": 4.223958333333334e-05,
      "loss": 0.0021,
      "step": 37250
    },
    {
      "epoch": 1.242,
      "grad_norm": 0.23591379821300507,
      "learning_rate": 4.2237500000000003e-05,
      "loss": 0.0035,
      "step": 37260
    },
    {
      "epoch": 1.2423333333333333,
      "grad_norm": 0.1769399791955948,
      "learning_rate": 4.223541666666667e-05,
      "loss": 0.0022,
      "step": 37270
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.059374287724494934,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.002,
      "step": 37280
    },
    {
      "epoch": 1.2429999999999999,
      "grad_norm": 0.17713958024978638,
      "learning_rate": 4.223125e-05,
      "loss": 0.0049,
      "step": 37290
    },
    {
      "epoch": 1.2433333333333334,
      "grad_norm": 0.3241268992424011,
      "learning_rate": 4.222916666666667e-05,
      "loss": 0.0025,
      "step": 37300
    },
    {
      "epoch": 1.2436666666666667,
      "grad_norm": 0.17684198915958405,
      "learning_rate": 4.222708333333333e-05,
      "loss": 0.0023,
      "step": 37310
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.32431119680404663,
      "learning_rate": 4.2225e-05,
      "loss": 0.003,
      "step": 37320
    },
    {
      "epoch": 1.2443333333333333,
      "grad_norm": 0.029734443873167038,
      "learning_rate": 4.222291666666667e-05,
      "loss": 0.0025,
      "step": 37330
    },
    {
      "epoch": 1.2446666666666666,
      "grad_norm": 0.11789395660161972,
      "learning_rate": 4.2220833333333334e-05,
      "loss": 0.0025,
      "step": 37340
    },
    {
      "epoch": 1.245,
      "grad_norm": 0.02950388938188553,
      "learning_rate": 4.221875e-05,
      "loss": 0.003,
      "step": 37350
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.14710445702075958,
      "learning_rate": 4.221666666666667e-05,
      "loss": 0.002,
      "step": 37360
    },
    {
      "epoch": 1.2456666666666667,
      "grad_norm": 0.03010210581123829,
      "learning_rate": 4.221458333333334e-05,
      "loss": 0.0027,
      "step": 37370
    },
    {
      "epoch": 1.246,
      "grad_norm": 0.382950097322464,
      "learning_rate": 4.2212499999999996e-05,
      "loss": 0.0021,
      "step": 37380
    },
    {
      "epoch": 1.2463333333333333,
      "grad_norm": 0.05925305560231209,
      "learning_rate": 4.221041666666667e-05,
      "loss": 0.0023,
      "step": 37390
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.004716450348496437,
      "learning_rate": 4.2208333333333334e-05,
      "loss": 0.0034,
      "step": 37400
    },
    {
      "epoch": 1.2469999999999999,
      "grad_norm": 0.0361725278198719,
      "learning_rate": 4.2206250000000006e-05,
      "loss": 0.0023,
      "step": 37410
    },
    {
      "epoch": 1.2473333333333334,
      "grad_norm": 0.2063141018152237,
      "learning_rate": 4.2204166666666665e-05,
      "loss": 0.0028,
      "step": 37420
    },
    {
      "epoch": 1.2476666666666667,
      "grad_norm": 0.47083455324172974,
      "learning_rate": 4.220208333333334e-05,
      "loss": 0.0022,
      "step": 37430
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.1766391545534134,
      "learning_rate": 4.22e-05,
      "loss": 0.0026,
      "step": 37440
    },
    {
      "epoch": 1.2483333333333333,
      "grad_norm": 0.206192746758461,
      "learning_rate": 4.219791666666667e-05,
      "loss": 0.002,
      "step": 37450
    },
    {
      "epoch": 1.2486666666666666,
      "grad_norm": 0.20624928176403046,
      "learning_rate": 4.2195833333333334e-05,
      "loss": 0.003,
      "step": 37460
    },
    {
      "epoch": 1.249,
      "grad_norm": 0.25005432963371277,
      "learning_rate": 4.2193750000000006e-05,
      "loss": 0.0036,
      "step": 37470
    },
    {
      "epoch": 1.2493333333333334,
      "grad_norm": 0.26484423875808716,
      "learning_rate": 4.219166666666667e-05,
      "loss": 0.0028,
      "step": 37480
    },
    {
      "epoch": 1.2496666666666667,
      "grad_norm": 0.5349276065826416,
      "learning_rate": 4.218958333333334e-05,
      "loss": 0.0027,
      "step": 37490
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.029561907052993774,
      "learning_rate": 4.21875e-05,
      "loss": 0.0027,
      "step": 37500
    },
    {
      "epoch": 1.2503333333333333,
      "grad_norm": 0.08819419890642166,
      "learning_rate": 4.218541666666667e-05,
      "loss": 0.0031,
      "step": 37510
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.18761217594146729,
      "learning_rate": 4.218333333333333e-05,
      "loss": 0.0028,
      "step": 37520
    },
    {
      "epoch": 1.251,
      "grad_norm": 0.0891040787100792,
      "learning_rate": 4.218125e-05,
      "loss": 0.0019,
      "step": 37530
    },
    {
      "epoch": 1.2513333333333334,
      "grad_norm": 0.14722241461277008,
      "learning_rate": 4.217916666666667e-05,
      "loss": 0.0023,
      "step": 37540
    },
    {
      "epoch": 1.2516666666666667,
      "grad_norm": 0.11811791360378265,
      "learning_rate": 4.2177083333333336e-05,
      "loss": 0.0022,
      "step": 37550
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.029905611649155617,
      "learning_rate": 4.2175e-05,
      "loss": 0.002,
      "step": 37560
    },
    {
      "epoch": 1.2523333333333333,
      "grad_norm": 0.5308069586753845,
      "learning_rate": 4.217291666666667e-05,
      "loss": 0.0025,
      "step": 37570
    },
    {
      "epoch": 1.2526666666666666,
      "grad_norm": 0.8818318247795105,
      "learning_rate": 4.217083333333334e-05,
      "loss": 0.0027,
      "step": 37580
    },
    {
      "epoch": 1.2530000000000001,
      "grad_norm": 0.03054633177816868,
      "learning_rate": 4.216875e-05,
      "loss": 0.0019,
      "step": 37590
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.26508113741874695,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.003,
      "step": 37600
    },
    {
      "epoch": 1.2536666666666667,
      "grad_norm": 0.030244911089539528,
      "learning_rate": 4.2164583333333336e-05,
      "loss": 0.004,
      "step": 37610
    },
    {
      "epoch": 1.254,
      "grad_norm": 0.003850409761071205,
      "learning_rate": 4.21625e-05,
      "loss": 0.0018,
      "step": 37620
    },
    {
      "epoch": 1.2543333333333333,
      "grad_norm": 0.2170242816209793,
      "learning_rate": 4.216041666666667e-05,
      "loss": 0.0036,
      "step": 37630
    },
    {
      "epoch": 1.2546666666666666,
      "grad_norm": 0.059056319296360016,
      "learning_rate": 4.215833333333333e-05,
      "loss": 0.0026,
      "step": 37640
    },
    {
      "epoch": 1.255,
      "grad_norm": 0.5301638245582581,
      "learning_rate": 4.2156250000000005e-05,
      "loss": 0.0026,
      "step": 37650
    },
    {
      "epoch": 1.2553333333333334,
      "grad_norm": 0.0917614996433258,
      "learning_rate": 4.2154166666666664e-05,
      "loss": 0.0019,
      "step": 37660
    },
    {
      "epoch": 1.2556666666666667,
      "grad_norm": 0.29495754837989807,
      "learning_rate": 4.2152083333333336e-05,
      "loss": 0.002,
      "step": 37670
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.6186721324920654,
      "learning_rate": 4.215e-05,
      "loss": 0.0019,
      "step": 37680
    },
    {
      "epoch": 1.2563333333333333,
      "grad_norm": 0.11767949163913727,
      "learning_rate": 4.2147916666666674e-05,
      "loss": 0.002,
      "step": 37690
    },
    {
      "epoch": 1.2566666666666666,
      "grad_norm": 0.030481554567813873,
      "learning_rate": 4.214583333333333e-05,
      "loss": 0.0021,
      "step": 37700
    },
    {
      "epoch": 1.2570000000000001,
      "grad_norm": 0.584003746509552,
      "learning_rate": 4.2143750000000005e-05,
      "loss": 0.0023,
      "step": 37710
    },
    {
      "epoch": 1.2573333333333334,
      "grad_norm": 0.38329124450683594,
      "learning_rate": 4.214166666666667e-05,
      "loss": 0.0029,
      "step": 37720
    },
    {
      "epoch": 1.2576666666666667,
      "grad_norm": 0.17651280760765076,
      "learning_rate": 4.2139583333333336e-05,
      "loss": 0.0026,
      "step": 37730
    },
    {
      "epoch": 1.258,
      "grad_norm": 0.05911372974514961,
      "learning_rate": 4.21375e-05,
      "loss": 0.0017,
      "step": 37740
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.2062702625989914,
      "learning_rate": 4.2135416666666667e-05,
      "loss": 0.0017,
      "step": 37750
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.11791694164276123,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0026,
      "step": 37760
    },
    {
      "epoch": 1.259,
      "grad_norm": 0.05939735844731331,
      "learning_rate": 4.213125e-05,
      "loss": 0.0032,
      "step": 37770
    },
    {
      "epoch": 1.2593333333333334,
      "grad_norm": 0.176828995347023,
      "learning_rate": 4.212916666666667e-05,
      "loss": 0.0026,
      "step": 37780
    },
    {
      "epoch": 1.2596666666666667,
      "grad_norm": 0.34636664390563965,
      "learning_rate": 4.2127083333333335e-05,
      "loss": 0.0022,
      "step": 37790
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.6473538875579834,
      "learning_rate": 4.2125e-05,
      "loss": 0.0031,
      "step": 37800
    },
    {
      "epoch": 1.2603333333333333,
      "grad_norm": 0.03828752040863037,
      "learning_rate": 4.2122916666666666e-05,
      "loss": 0.0021,
      "step": 37810
    },
    {
      "epoch": 1.2606666666666666,
      "grad_norm": 0.7678542733192444,
      "learning_rate": 4.212083333333334e-05,
      "loss": 0.0029,
      "step": 37820
    },
    {
      "epoch": 1.2610000000000001,
      "grad_norm": 0.17676083743572235,
      "learning_rate": 4.2118750000000004e-05,
      "loss": 0.0028,
      "step": 37830
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.059318725019693375,
      "learning_rate": 4.211666666666667e-05,
      "loss": 0.0023,
      "step": 37840
    },
    {
      "epoch": 1.2616666666666667,
      "grad_norm": 0.05905112996697426,
      "learning_rate": 4.2114583333333335e-05,
      "loss": 0.0025,
      "step": 37850
    },
    {
      "epoch": 1.262,
      "grad_norm": 0.6183403730392456,
      "learning_rate": 4.211250000000001e-05,
      "loss": 0.0018,
      "step": 37860
    },
    {
      "epoch": 1.2623333333333333,
      "grad_norm": 0.030264144763350487,
      "learning_rate": 4.2110416666666666e-05,
      "loss": 0.0032,
      "step": 37870
    },
    {
      "epoch": 1.2626666666666666,
      "grad_norm": 0.028183648362755775,
      "learning_rate": 4.210833333333333e-05,
      "loss": 0.0026,
      "step": 37880
    },
    {
      "epoch": 1.263,
      "grad_norm": 0.6476479172706604,
      "learning_rate": 4.2106250000000004e-05,
      "loss": 0.0026,
      "step": 37890
    },
    {
      "epoch": 1.2633333333333332,
      "grad_norm": 0.35361248254776,
      "learning_rate": 4.210416666666667e-05,
      "loss": 0.0031,
      "step": 37900
    },
    {
      "epoch": 1.2636666666666667,
      "grad_norm": 0.20693865418434143,
      "learning_rate": 4.2102083333333335e-05,
      "loss": 0.0017,
      "step": 37910
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.38272371888160706,
      "learning_rate": 4.21e-05,
      "loss": 0.0025,
      "step": 37920
    },
    {
      "epoch": 1.2643333333333333,
      "grad_norm": 0.08832596987485886,
      "learning_rate": 4.209791666666667e-05,
      "loss": 0.0031,
      "step": 37930
    },
    {
      "epoch": 1.2646666666666666,
      "grad_norm": 0.23570165038108826,
      "learning_rate": 4.209583333333333e-05,
      "loss": 0.0013,
      "step": 37940
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 0.029411431401968002,
      "learning_rate": 4.209375e-05,
      "loss": 0.003,
      "step": 37950
    },
    {
      "epoch": 1.2653333333333334,
      "grad_norm": 0.35346719622612,
      "learning_rate": 4.209166666666667e-05,
      "loss": 0.0025,
      "step": 37960
    },
    {
      "epoch": 1.2656666666666667,
      "grad_norm": 0.23582245409488678,
      "learning_rate": 4.208958333333334e-05,
      "loss": 0.0019,
      "step": 37970
    },
    {
      "epoch": 1.266,
      "grad_norm": 0.23533232510089874,
      "learning_rate": 4.20875e-05,
      "loss": 0.0021,
      "step": 37980
    },
    {
      "epoch": 1.2663333333333333,
      "grad_norm": 0.14746417105197906,
      "learning_rate": 4.2085416666666665e-05,
      "loss": 0.0026,
      "step": 37990
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.4271533191204071,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.0035,
      "step": 38000
    },
    {
      "epoch": 1.267,
      "grad_norm": 0.23535919189453125,
      "learning_rate": 4.208125e-05,
      "loss": 0.0022,
      "step": 38010
    },
    {
      "epoch": 1.2673333333333332,
      "grad_norm": 0.004211037885397673,
      "learning_rate": 4.207916666666667e-05,
      "loss": 0.0037,
      "step": 38020
    },
    {
      "epoch": 1.2676666666666667,
      "grad_norm": 0.059027109295129776,
      "learning_rate": 4.2077083333333334e-05,
      "loss": 0.0027,
      "step": 38030
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.14760781824588776,
      "learning_rate": 4.2075000000000006e-05,
      "loss": 0.0026,
      "step": 38040
    },
    {
      "epoch": 1.2683333333333333,
      "grad_norm": 0.3826656937599182,
      "learning_rate": 4.2072916666666665e-05,
      "loss": 0.0033,
      "step": 38050
    },
    {
      "epoch": 1.2686666666666666,
      "grad_norm": 0.03017951361835003,
      "learning_rate": 4.207083333333334e-05,
      "loss": 0.0028,
      "step": 38060
    },
    {
      "epoch": 1.2690000000000001,
      "grad_norm": 0.21715602278709412,
      "learning_rate": 4.206875e-05,
      "loss": 0.0039,
      "step": 38070
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.05950380489230156,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0037,
      "step": 38080
    },
    {
      "epoch": 1.2696666666666667,
      "grad_norm": 0.29548385739326477,
      "learning_rate": 4.2064583333333334e-05,
      "loss": 0.0031,
      "step": 38090
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.23590292036533356,
      "learning_rate": 4.2062500000000006e-05,
      "loss": 0.0022,
      "step": 38100
    },
    {
      "epoch": 1.2703333333333333,
      "grad_norm": 0.14706040918827057,
      "learning_rate": 4.206041666666667e-05,
      "loss": 0.0024,
      "step": 38110
    },
    {
      "epoch": 1.2706666666666666,
      "grad_norm": 0.08856102079153061,
      "learning_rate": 4.205833333333333e-05,
      "loss": 0.0026,
      "step": 38120
    },
    {
      "epoch": 1.271,
      "grad_norm": 0.11776059865951538,
      "learning_rate": 4.205625e-05,
      "loss": 0.0026,
      "step": 38130
    },
    {
      "epoch": 1.2713333333333332,
      "grad_norm": 0.2101273238658905,
      "learning_rate": 4.205416666666667e-05,
      "loss": 0.0027,
      "step": 38140
    },
    {
      "epoch": 1.2716666666666667,
      "grad_norm": 0.09598894417285919,
      "learning_rate": 4.2052083333333333e-05,
      "loss": 0.0019,
      "step": 38150
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.15410150587558746,
      "learning_rate": 4.205e-05,
      "loss": 0.0022,
      "step": 38160
    },
    {
      "epoch": 1.2723333333333333,
      "grad_norm": 0.11898289620876312,
      "learning_rate": 4.204791666666667e-05,
      "loss": 0.0024,
      "step": 38170
    },
    {
      "epoch": 1.2726666666666666,
      "grad_norm": 0.41206932067871094,
      "learning_rate": 4.204583333333334e-05,
      "loss": 0.0029,
      "step": 38180
    },
    {
      "epoch": 1.2730000000000001,
      "grad_norm": 0.08879698812961578,
      "learning_rate": 4.204375e-05,
      "loss": 0.0027,
      "step": 38190
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.3827255666255951,
      "learning_rate": 4.204166666666667e-05,
      "loss": 0.002,
      "step": 38200
    },
    {
      "epoch": 1.2736666666666667,
      "grad_norm": 0.00976350624114275,
      "learning_rate": 4.203958333333334e-05,
      "loss": 0.0022,
      "step": 38210
    },
    {
      "epoch": 1.274,
      "grad_norm": 0.4127032160758972,
      "learning_rate": 4.20375e-05,
      "loss": 0.0025,
      "step": 38220
    },
    {
      "epoch": 1.2743333333333333,
      "grad_norm": 0.08840753138065338,
      "learning_rate": 4.2035416666666664e-05,
      "loss": 0.0017,
      "step": 38230
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.32427504658699036,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0023,
      "step": 38240
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.3533322215080261,
      "learning_rate": 4.203125e-05,
      "loss": 0.0019,
      "step": 38250
    },
    {
      "epoch": 1.2753333333333332,
      "grad_norm": 0.29469379782676697,
      "learning_rate": 4.202916666666667e-05,
      "loss": 0.0032,
      "step": 38260
    },
    {
      "epoch": 1.2756666666666667,
      "grad_norm": 0.11806954443454742,
      "learning_rate": 4.202708333333333e-05,
      "loss": 0.002,
      "step": 38270
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.1912640631198883,
      "learning_rate": 4.2025000000000005e-05,
      "loss": 0.0024,
      "step": 38280
    },
    {
      "epoch": 1.2763333333333333,
      "grad_norm": 0.06007058545947075,
      "learning_rate": 4.202291666666667e-05,
      "loss": 0.0033,
      "step": 38290
    },
    {
      "epoch": 1.2766666666666666,
      "grad_norm": 0.4432576894760132,
      "learning_rate": 4.2020833333333336e-05,
      "loss": 0.0021,
      "step": 38300
    },
    {
      "epoch": 1.2770000000000001,
      "grad_norm": 0.08889888226985931,
      "learning_rate": 4.201875e-05,
      "loss": 0.0017,
      "step": 38310
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.2356279492378235,
      "learning_rate": 4.2016666666666674e-05,
      "loss": 0.003,
      "step": 38320
    },
    {
      "epoch": 1.2776666666666667,
      "grad_norm": 0.176422119140625,
      "learning_rate": 4.201458333333333e-05,
      "loss": 0.0027,
      "step": 38330
    },
    {
      "epoch": 1.278,
      "grad_norm": 0.3241935074329376,
      "learning_rate": 4.2012500000000005e-05,
      "loss": 0.0024,
      "step": 38340
    },
    {
      "epoch": 1.2783333333333333,
      "grad_norm": 0.5237530469894409,
      "learning_rate": 4.201041666666667e-05,
      "loss": 0.0026,
      "step": 38350
    },
    {
      "epoch": 1.2786666666666666,
      "grad_norm": 0.1771858185529709,
      "learning_rate": 4.2008333333333336e-05,
      "loss": 0.0031,
      "step": 38360
    },
    {
      "epoch": 1.279,
      "grad_norm": 0.23560726642608643,
      "learning_rate": 4.200625e-05,
      "loss": 0.0022,
      "step": 38370
    },
    {
      "epoch": 1.2793333333333332,
      "grad_norm": 0.029628116637468338,
      "learning_rate": 4.200416666666667e-05,
      "loss": 0.0016,
      "step": 38380
    },
    {
      "epoch": 1.2796666666666667,
      "grad_norm": 0.3532506823539734,
      "learning_rate": 4.200208333333334e-05,
      "loss": 0.003,
      "step": 38390
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.11778394877910614,
      "learning_rate": 4.2e-05,
      "loss": 0.0026,
      "step": 38400
    },
    {
      "epoch": 1.2803333333333333,
      "grad_norm": 0.4706127941608429,
      "learning_rate": 4.199791666666667e-05,
      "loss": 0.003,
      "step": 38410
    },
    {
      "epoch": 1.2806666666666666,
      "grad_norm": 0.361147403717041,
      "learning_rate": 4.1995833333333335e-05,
      "loss": 0.0026,
      "step": 38420
    },
    {
      "epoch": 1.2810000000000001,
      "grad_norm": 0.9463948607444763,
      "learning_rate": 4.199375e-05,
      "loss": 0.0028,
      "step": 38430
    },
    {
      "epoch": 1.2813333333333334,
      "grad_norm": 0.4415722191333771,
      "learning_rate": 4.1991666666666666e-05,
      "loss": 0.0027,
      "step": 38440
    },
    {
      "epoch": 1.2816666666666667,
      "grad_norm": 0.029789341613650322,
      "learning_rate": 4.198958333333334e-05,
      "loss": 0.0034,
      "step": 38450
    },
    {
      "epoch": 1.282,
      "grad_norm": 0.32381898164749146,
      "learning_rate": 4.1987500000000004e-05,
      "loss": 0.0028,
      "step": 38460
    },
    {
      "epoch": 1.2823333333333333,
      "grad_norm": 0.1767507791519165,
      "learning_rate": 4.198541666666666e-05,
      "loss": 0.0042,
      "step": 38470
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.26506495475769043,
      "learning_rate": 4.1983333333333335e-05,
      "loss": 0.0026,
      "step": 38480
    },
    {
      "epoch": 1.283,
      "grad_norm": 0.2652510702610016,
      "learning_rate": 4.198125e-05,
      "loss": 0.0031,
      "step": 38490
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.02975505217909813,
      "learning_rate": 4.1979166666666666e-05,
      "loss": 0.0025,
      "step": 38500
    },
    {
      "epoch": 1.2836666666666667,
      "grad_norm": 0.38245201110839844,
      "learning_rate": 4.197708333333333e-05,
      "loss": 0.0023,
      "step": 38510
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.529502809047699,
      "learning_rate": 4.1975000000000004e-05,
      "loss": 0.0021,
      "step": 38520
    },
    {
      "epoch": 1.2843333333333333,
      "grad_norm": 0.1766536384820938,
      "learning_rate": 4.197291666666667e-05,
      "loss": 0.002,
      "step": 38530
    },
    {
      "epoch": 1.2846666666666666,
      "grad_norm": 0.06269705295562744,
      "learning_rate": 4.1970833333333335e-05,
      "loss": 0.0024,
      "step": 38540
    },
    {
      "epoch": 1.285,
      "grad_norm": 0.2652715742588043,
      "learning_rate": 4.196875e-05,
      "loss": 0.0017,
      "step": 38550
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.21480463445186615,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0025,
      "step": 38560
    },
    {
      "epoch": 1.2856666666666667,
      "grad_norm": 0.14700205624103546,
      "learning_rate": 4.196458333333334e-05,
      "loss": 0.0017,
      "step": 38570
    },
    {
      "epoch": 1.286,
      "grad_norm": 0.05903260409832001,
      "learning_rate": 4.1962500000000004e-05,
      "loss": 0.0027,
      "step": 38580
    },
    {
      "epoch": 1.2863333333333333,
      "grad_norm": 0.3530222773551941,
      "learning_rate": 4.196041666666667e-05,
      "loss": 0.0023,
      "step": 38590
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.26466259360313416,
      "learning_rate": 4.1958333333333335e-05,
      "loss": 0.0022,
      "step": 38600
    },
    {
      "epoch": 1.287,
      "grad_norm": 0.08852943032979965,
      "learning_rate": 4.195625e-05,
      "loss": 0.0027,
      "step": 38610
    },
    {
      "epoch": 1.2873333333333332,
      "grad_norm": 0.059473901987075806,
      "learning_rate": 4.1954166666666665e-05,
      "loss": 0.0019,
      "step": 38620
    },
    {
      "epoch": 1.2876666666666667,
      "grad_norm": 0.26489633321762085,
      "learning_rate": 4.195208333333334e-05,
      "loss": 0.0027,
      "step": 38630
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.0299956314265728,
      "learning_rate": 4.195e-05,
      "loss": 0.0024,
      "step": 38640
    },
    {
      "epoch": 1.2883333333333333,
      "grad_norm": 0.5006214380264282,
      "learning_rate": 4.194791666666667e-05,
      "loss": 0.0032,
      "step": 38650
    },
    {
      "epoch": 1.2886666666666666,
      "grad_norm": 0.11785584688186646,
      "learning_rate": 4.1945833333333334e-05,
      "loss": 0.0016,
      "step": 38660
    },
    {
      "epoch": 1.289,
      "grad_norm": 0.4117048680782318,
      "learning_rate": 4.1943750000000006e-05,
      "loss": 0.0026,
      "step": 38670
    },
    {
      "epoch": 1.2893333333333334,
      "grad_norm": 0.08843208849430084,
      "learning_rate": 4.1941666666666665e-05,
      "loss": 0.0025,
      "step": 38680
    },
    {
      "epoch": 1.2896666666666667,
      "grad_norm": 0.3529244363307953,
      "learning_rate": 4.193958333333334e-05,
      "loss": 0.0024,
      "step": 38690
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.23523470759391785,
      "learning_rate": 4.19375e-05,
      "loss": 0.0019,
      "step": 38700
    },
    {
      "epoch": 1.2903333333333333,
      "grad_norm": 0.3530430495738983,
      "learning_rate": 4.193541666666667e-05,
      "loss": 0.0028,
      "step": 38710
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.5294283032417297,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0037,
      "step": 38720
    },
    {
      "epoch": 1.291,
      "grad_norm": 0.4121949374675751,
      "learning_rate": 4.193125e-05,
      "loss": 0.0014,
      "step": 38730
    },
    {
      "epoch": 1.2913333333333332,
      "grad_norm": 0.08858104050159454,
      "learning_rate": 4.192916666666667e-05,
      "loss": 0.0019,
      "step": 38740
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.17677977681159973,
      "learning_rate": 4.192708333333333e-05,
      "loss": 0.0027,
      "step": 38750
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.030055111274123192,
      "learning_rate": 4.1925e-05,
      "loss": 0.0027,
      "step": 38760
    },
    {
      "epoch": 1.2923333333333333,
      "grad_norm": 0.32361480593681335,
      "learning_rate": 4.192291666666667e-05,
      "loss": 0.0023,
      "step": 38770
    },
    {
      "epoch": 1.2926666666666666,
      "grad_norm": 0.05926300585269928,
      "learning_rate": 4.1920833333333334e-05,
      "loss": 0.0017,
      "step": 38780
    },
    {
      "epoch": 1.293,
      "grad_norm": 0.35316070914268494,
      "learning_rate": 4.191875e-05,
      "loss": 0.0021,
      "step": 38790
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.2065291702747345,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.0015,
      "step": 38800
    },
    {
      "epoch": 1.2936666666666667,
      "grad_norm": 0.32347166538238525,
      "learning_rate": 4.191458333333334e-05,
      "loss": 0.0027,
      "step": 38810
    },
    {
      "epoch": 1.294,
      "grad_norm": 0.08844348043203354,
      "learning_rate": 4.19125e-05,
      "loss": 0.0027,
      "step": 38820
    },
    {
      "epoch": 1.2943333333333333,
      "grad_norm": 0.35280823707580566,
      "learning_rate": 4.191041666666667e-05,
      "loss": 0.0028,
      "step": 38830
    },
    {
      "epoch": 1.2946666666666666,
      "grad_norm": 0.17670486867427826,
      "learning_rate": 4.190833333333333e-05,
      "loss": 0.0023,
      "step": 38840
    },
    {
      "epoch": 1.295,
      "grad_norm": 0.32397225499153137,
      "learning_rate": 4.1906250000000006e-05,
      "loss": 0.0049,
      "step": 38850
    },
    {
      "epoch": 1.2953333333333332,
      "grad_norm": 0.4486597776412964,
      "learning_rate": 4.1904166666666664e-05,
      "loss": 0.0027,
      "step": 38860
    },
    {
      "epoch": 1.2956666666666667,
      "grad_norm": 0.2941993474960327,
      "learning_rate": 4.1902083333333337e-05,
      "loss": 0.0021,
      "step": 38870
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.005151804070919752,
      "learning_rate": 4.19e-05,
      "loss": 0.002,
      "step": 38880
    },
    {
      "epoch": 1.2963333333333333,
      "grad_norm": 0.7666537761688232,
      "learning_rate": 4.189791666666667e-05,
      "loss": 0.004,
      "step": 38890
    },
    {
      "epoch": 1.2966666666666666,
      "grad_norm": 0.30125319957733154,
      "learning_rate": 4.189583333333333e-05,
      "loss": 0.0026,
      "step": 38900
    },
    {
      "epoch": 1.297,
      "grad_norm": 0.031129419803619385,
      "learning_rate": 4.1893750000000005e-05,
      "loss": 0.0034,
      "step": 38910
    },
    {
      "epoch": 1.2973333333333334,
      "grad_norm": 0.2871723473072052,
      "learning_rate": 4.189166666666667e-05,
      "loss": 0.0024,
      "step": 38920
    },
    {
      "epoch": 1.2976666666666667,
      "grad_norm": 0.20564128458499908,
      "learning_rate": 4.1889583333333336e-05,
      "loss": 0.0025,
      "step": 38930
    },
    {
      "epoch": 1.298,
      "grad_norm": 0.41157275438308716,
      "learning_rate": 4.18875e-05,
      "loss": 0.0033,
      "step": 38940
    },
    {
      "epoch": 1.2983333333333333,
      "grad_norm": 0.44100022315979004,
      "learning_rate": 4.1885416666666674e-05,
      "loss": 0.0028,
      "step": 38950
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.17623509466648102,
      "learning_rate": 4.188333333333333e-05,
      "loss": 0.0018,
      "step": 38960
    },
    {
      "epoch": 1.299,
      "grad_norm": 0.29438379406929016,
      "learning_rate": 4.188125e-05,
      "loss": 0.0033,
      "step": 38970
    },
    {
      "epoch": 1.2993333333333332,
      "grad_norm": 0.030230874195694923,
      "learning_rate": 4.187916666666667e-05,
      "loss": 0.004,
      "step": 38980
    },
    {
      "epoch": 1.2996666666666667,
      "grad_norm": 0.11830493062734604,
      "learning_rate": 4.1877083333333336e-05,
      "loss": 0.0029,
      "step": 38990
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.38226768374443054,
      "learning_rate": 4.1875e-05,
      "loss": 0.0026,
      "step": 39000
    },
    {
      "epoch": 1.3003333333333333,
      "grad_norm": 0.4075278043746948,
      "learning_rate": 4.187291666666667e-05,
      "loss": 0.0036,
      "step": 39010
    },
    {
      "epoch": 1.3006666666666666,
      "grad_norm": 0.1179155707359314,
      "learning_rate": 4.187083333333334e-05,
      "loss": 0.0035,
      "step": 39020
    },
    {
      "epoch": 1.301,
      "grad_norm": 0.17614582180976868,
      "learning_rate": 4.186875e-05,
      "loss": 0.0023,
      "step": 39030
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.29379770159721375,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0025,
      "step": 39040
    },
    {
      "epoch": 1.3016666666666667,
      "grad_norm": 0.7641153931617737,
      "learning_rate": 4.1864583333333336e-05,
      "loss": 0.0023,
      "step": 39050
    },
    {
      "epoch": 1.302,
      "grad_norm": 0.7346363067626953,
      "learning_rate": 4.18625e-05,
      "loss": 0.0033,
      "step": 39060
    },
    {
      "epoch": 1.3023333333333333,
      "grad_norm": 0.23518194258213043,
      "learning_rate": 4.1860416666666667e-05,
      "loss": 0.0021,
      "step": 39070
    },
    {
      "epoch": 1.3026666666666666,
      "grad_norm": 0.5297523736953735,
      "learning_rate": 4.185833333333333e-05,
      "loss": 0.0031,
      "step": 39080
    },
    {
      "epoch": 1.303,
      "grad_norm": 0.029955770820379257,
      "learning_rate": 4.1856250000000004e-05,
      "loss": 0.0028,
      "step": 39090
    },
    {
      "epoch": 1.3033333333333332,
      "grad_norm": 0.0593968890607357,
      "learning_rate": 4.185416666666667e-05,
      "loss": 0.0028,
      "step": 39100
    },
    {
      "epoch": 1.3036666666666665,
      "grad_norm": 0.05977007374167442,
      "learning_rate": 4.1852083333333335e-05,
      "loss": 0.0022,
      "step": 39110
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.352613240480423,
      "learning_rate": 4.185e-05,
      "loss": 0.0029,
      "step": 39120
    },
    {
      "epoch": 1.3043333333333333,
      "grad_norm": 0.35318246483802795,
      "learning_rate": 4.184791666666667e-05,
      "loss": 0.0023,
      "step": 39130
    },
    {
      "epoch": 1.3046666666666666,
      "grad_norm": 0.7352635860443115,
      "learning_rate": 4.184583333333333e-05,
      "loss": 0.003,
      "step": 39140
    },
    {
      "epoch": 1.305,
      "grad_norm": 0.3236050605773926,
      "learning_rate": 4.1843750000000004e-05,
      "loss": 0.0026,
      "step": 39150
    },
    {
      "epoch": 1.3053333333333335,
      "grad_norm": 0.14682543277740479,
      "learning_rate": 4.184166666666667e-05,
      "loss": 0.0039,
      "step": 39160
    },
    {
      "epoch": 1.3056666666666668,
      "grad_norm": 0.1178346499800682,
      "learning_rate": 4.1839583333333335e-05,
      "loss": 0.0021,
      "step": 39170
    },
    {
      "epoch": 1.306,
      "grad_norm": 0.17666210234165192,
      "learning_rate": 4.18375e-05,
      "loss": 0.0035,
      "step": 39180
    },
    {
      "epoch": 1.3063333333333333,
      "grad_norm": 0.49970564246177673,
      "learning_rate": 4.183541666666667e-05,
      "loss": 0.0024,
      "step": 39190
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.11839327216148376,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0024,
      "step": 39200
    },
    {
      "epoch": 1.307,
      "grad_norm": 0.14743372797966003,
      "learning_rate": 4.183125e-05,
      "loss": 0.0028,
      "step": 39210
    },
    {
      "epoch": 1.3073333333333332,
      "grad_norm": 0.03173073008656502,
      "learning_rate": 4.182916666666667e-05,
      "loss": 0.002,
      "step": 39220
    },
    {
      "epoch": 1.3076666666666665,
      "grad_norm": 0.06238590180873871,
      "learning_rate": 4.1827083333333335e-05,
      "loss": 0.0026,
      "step": 39230
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.6969671845436096,
      "learning_rate": 4.1825e-05,
      "loss": 0.0028,
      "step": 39240
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.17633377015590668,
      "learning_rate": 4.1822916666666666e-05,
      "loss": 0.0031,
      "step": 39250
    },
    {
      "epoch": 1.3086666666666666,
      "grad_norm": 0.1176835298538208,
      "learning_rate": 4.182083333333334e-05,
      "loss": 0.0018,
      "step": 39260
    },
    {
      "epoch": 1.309,
      "grad_norm": 0.17626667022705078,
      "learning_rate": 4.1818750000000003e-05,
      "loss": 0.0018,
      "step": 39270
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.14692071080207825,
      "learning_rate": 4.181666666666667e-05,
      "loss": 0.0024,
      "step": 39280
    },
    {
      "epoch": 1.3096666666666668,
      "grad_norm": 0.05955883488059044,
      "learning_rate": 4.1814583333333334e-05,
      "loss": 0.002,
      "step": 39290
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.00460724625736475,
      "learning_rate": 4.181250000000001e-05,
      "loss": 0.0024,
      "step": 39300
    },
    {
      "epoch": 1.3103333333333333,
      "grad_norm": 0.443607896566391,
      "learning_rate": 4.1810416666666665e-05,
      "loss": 0.0026,
      "step": 39310
    },
    {
      "epoch": 1.3106666666666666,
      "grad_norm": 0.1467500925064087,
      "learning_rate": 4.180833333333333e-05,
      "loss": 0.0013,
      "step": 39320
    },
    {
      "epoch": 1.311,
      "grad_norm": 0.0880899578332901,
      "learning_rate": 4.180625e-05,
      "loss": 0.0022,
      "step": 39330
    },
    {
      "epoch": 1.3113333333333332,
      "grad_norm": 0.27085456252098083,
      "learning_rate": 4.180416666666667e-05,
      "loss": 0.0028,
      "step": 39340
    },
    {
      "epoch": 1.3116666666666665,
      "grad_norm": 0.25138503313064575,
      "learning_rate": 4.1802083333333334e-05,
      "loss": 0.0032,
      "step": 39350
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.02970312535762787,
      "learning_rate": 4.18e-05,
      "loss": 0.0019,
      "step": 39360
    },
    {
      "epoch": 1.3123333333333334,
      "grad_norm": 0.294308066368103,
      "learning_rate": 4.179791666666667e-05,
      "loss": 0.0027,
      "step": 39370
    },
    {
      "epoch": 1.3126666666666666,
      "grad_norm": 0.058630261570215225,
      "learning_rate": 4.179583333333334e-05,
      "loss": 0.0028,
      "step": 39380
    },
    {
      "epoch": 1.313,
      "grad_norm": 0.499468594789505,
      "learning_rate": 4.179375e-05,
      "loss": 0.0025,
      "step": 39390
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.0598350390791893,
      "learning_rate": 4.179166666666667e-05,
      "loss": 0.0027,
      "step": 39400
    },
    {
      "epoch": 1.3136666666666668,
      "grad_norm": 0.030591700226068497,
      "learning_rate": 4.178958333333334e-05,
      "loss": 0.0029,
      "step": 39410
    },
    {
      "epoch": 1.314,
      "grad_norm": 0.08836027979850769,
      "learning_rate": 4.17875e-05,
      "loss": 0.0016,
      "step": 39420
    },
    {
      "epoch": 1.3143333333333334,
      "grad_norm": 0.4114304482936859,
      "learning_rate": 4.178541666666667e-05,
      "loss": 0.0021,
      "step": 39430
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.23563294112682343,
      "learning_rate": 4.178333333333334e-05,
      "loss": 0.0028,
      "step": 39440
    },
    {
      "epoch": 1.315,
      "grad_norm": 0.17677490413188934,
      "learning_rate": 4.178125e-05,
      "loss": 0.0025,
      "step": 39450
    },
    {
      "epoch": 1.3153333333333332,
      "grad_norm": 0.11789417266845703,
      "learning_rate": 4.177916666666667e-05,
      "loss": 0.0033,
      "step": 39460
    },
    {
      "epoch": 1.3156666666666665,
      "grad_norm": 0.2938276529312134,
      "learning_rate": 4.1777083333333333e-05,
      "loss": 0.0022,
      "step": 39470
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.5880570411682129,
      "learning_rate": 4.1775000000000006e-05,
      "loss": 0.0018,
      "step": 39480
    },
    {
      "epoch": 1.3163333333333334,
      "grad_norm": 0.20590592920780182,
      "learning_rate": 4.1772916666666664e-05,
      "loss": 0.0038,
      "step": 39490
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.14678888022899628,
      "learning_rate": 4.177083333333334e-05,
      "loss": 0.0017,
      "step": 39500
    },
    {
      "epoch": 1.317,
      "grad_norm": 0.20550000667572021,
      "learning_rate": 4.176875e-05,
      "loss": 0.002,
      "step": 39510
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.059141259640455246,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0036,
      "step": 39520
    },
    {
      "epoch": 1.3176666666666668,
      "grad_norm": 0.0059057422913610935,
      "learning_rate": 4.176458333333333e-05,
      "loss": 0.0027,
      "step": 39530
    },
    {
      "epoch": 1.318,
      "grad_norm": 0.05897451564669609,
      "learning_rate": 4.1762500000000005e-05,
      "loss": 0.0024,
      "step": 39540
    },
    {
      "epoch": 1.3183333333333334,
      "grad_norm": 0.059073932468891144,
      "learning_rate": 4.176041666666667e-05,
      "loss": 0.0025,
      "step": 39550
    },
    {
      "epoch": 1.3186666666666667,
      "grad_norm": 0.2352127730846405,
      "learning_rate": 4.175833333333333e-05,
      "loss": 0.0023,
      "step": 39560
    },
    {
      "epoch": 1.319,
      "grad_norm": 0.20684319734573364,
      "learning_rate": 4.175625e-05,
      "loss": 0.0028,
      "step": 39570
    },
    {
      "epoch": 1.3193333333333332,
      "grad_norm": 0.08848579227924347,
      "learning_rate": 4.175416666666667e-05,
      "loss": 0.0021,
      "step": 39580
    },
    {
      "epoch": 1.3196666666666665,
      "grad_norm": 0.02990874834358692,
      "learning_rate": 4.175208333333333e-05,
      "loss": 0.0014,
      "step": 39590
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.14713972806930542,
      "learning_rate": 4.175e-05,
      "loss": 0.0032,
      "step": 39600
    },
    {
      "epoch": 1.3203333333333334,
      "grad_norm": 0.3820284307003021,
      "learning_rate": 4.174791666666667e-05,
      "loss": 0.0016,
      "step": 39610
    },
    {
      "epoch": 1.3206666666666667,
      "grad_norm": 0.0590953454375267,
      "learning_rate": 4.1745833333333336e-05,
      "loss": 0.0016,
      "step": 39620
    },
    {
      "epoch": 1.321,
      "grad_norm": 0.5875383019447327,
      "learning_rate": 4.174375e-05,
      "loss": 0.0026,
      "step": 39630
    },
    {
      "epoch": 1.3213333333333335,
      "grad_norm": 0.29380425810813904,
      "learning_rate": 4.174166666666667e-05,
      "loss": 0.0024,
      "step": 39640
    },
    {
      "epoch": 1.3216666666666668,
      "grad_norm": 0.3819237947463989,
      "learning_rate": 4.173958333333334e-05,
      "loss": 0.0024,
      "step": 39650
    },
    {
      "epoch": 1.322,
      "grad_norm": 0.029625650495290756,
      "learning_rate": 4.1737500000000005e-05,
      "loss": 0.0026,
      "step": 39660
    },
    {
      "epoch": 1.3223333333333334,
      "grad_norm": 0.08864723891019821,
      "learning_rate": 4.173541666666667e-05,
      "loss": 0.0022,
      "step": 39670
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.14372137188911438,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0028,
      "step": 39680
    },
    {
      "epoch": 1.323,
      "grad_norm": 0.20553915202617645,
      "learning_rate": 4.173125e-05,
      "loss": 0.0017,
      "step": 39690
    },
    {
      "epoch": 1.3233333333333333,
      "grad_norm": 0.1469099223613739,
      "learning_rate": 4.172916666666667e-05,
      "loss": 0.0033,
      "step": 39700
    },
    {
      "epoch": 1.3236666666666665,
      "grad_norm": 0.087933748960495,
      "learning_rate": 4.172708333333333e-05,
      "loss": 0.0033,
      "step": 39710
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.059412144124507904,
      "learning_rate": 4.1725000000000005e-05,
      "loss": 0.003,
      "step": 39720
    },
    {
      "epoch": 1.3243333333333334,
      "grad_norm": 0.3239080309867859,
      "learning_rate": 4.172291666666667e-05,
      "loss": 0.002,
      "step": 39730
    },
    {
      "epoch": 1.3246666666666667,
      "grad_norm": 0.3231409192085266,
      "learning_rate": 4.1720833333333336e-05,
      "loss": 0.0025,
      "step": 39740
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.35237917304039,
      "learning_rate": 4.171875e-05,
      "loss": 0.0023,
      "step": 39750
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.17665459215641022,
      "learning_rate": 4.171666666666667e-05,
      "loss": 0.0029,
      "step": 39760
    },
    {
      "epoch": 1.3256666666666668,
      "grad_norm": 0.2941725552082062,
      "learning_rate": 4.171458333333333e-05,
      "loss": 0.0024,
      "step": 39770
    },
    {
      "epoch": 1.326,
      "grad_norm": 0.030544616281986237,
      "learning_rate": 4.1712500000000004e-05,
      "loss": 0.0025,
      "step": 39780
    },
    {
      "epoch": 1.3263333333333334,
      "grad_norm": 0.0596470981836319,
      "learning_rate": 4.171041666666667e-05,
      "loss": 0.0028,
      "step": 39790
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.11848241090774536,
      "learning_rate": 4.1708333333333335e-05,
      "loss": 0.0031,
      "step": 39800
    },
    {
      "epoch": 1.327,
      "grad_norm": 0.5379996299743652,
      "learning_rate": 4.170625e-05,
      "loss": 0.003,
      "step": 39810
    },
    {
      "epoch": 1.3273333333333333,
      "grad_norm": 0.6172589063644409,
      "learning_rate": 4.1704166666666666e-05,
      "loss": 0.0019,
      "step": 39820
    },
    {
      "epoch": 1.3276666666666666,
      "grad_norm": 0.5967909097671509,
      "learning_rate": 4.170208333333334e-05,
      "loss": 0.0033,
      "step": 39830
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.2055189460515976,
      "learning_rate": 4.17e-05,
      "loss": 0.0023,
      "step": 39840
    },
    {
      "epoch": 1.3283333333333334,
      "grad_norm": 0.33988314867019653,
      "learning_rate": 4.169791666666667e-05,
      "loss": 0.0028,
      "step": 39850
    },
    {
      "epoch": 1.3286666666666667,
      "grad_norm": 0.08878882229328156,
      "learning_rate": 4.1695833333333335e-05,
      "loss": 0.0017,
      "step": 39860
    },
    {
      "epoch": 1.329,
      "grad_norm": 0.5585578680038452,
      "learning_rate": 4.169375e-05,
      "loss": 0.0017,
      "step": 39870
    },
    {
      "epoch": 1.3293333333333333,
      "grad_norm": 0.47024115920066833,
      "learning_rate": 4.1691666666666666e-05,
      "loss": 0.0021,
      "step": 39880
    },
    {
      "epoch": 1.3296666666666668,
      "grad_norm": 0.06035013124346733,
      "learning_rate": 4.168958333333334e-05,
      "loss": 0.0029,
      "step": 39890
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.2647642195224762,
      "learning_rate": 4.1687500000000004e-05,
      "loss": 0.0018,
      "step": 39900
    },
    {
      "epoch": 1.3303333333333334,
      "grad_norm": 0.17636598646640778,
      "learning_rate": 4.168541666666667e-05,
      "loss": 0.003,
      "step": 39910
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.059127483516931534,
      "learning_rate": 4.1683333333333335e-05,
      "loss": 0.0027,
      "step": 39920
    },
    {
      "epoch": 1.331,
      "grad_norm": 0.029966874048113823,
      "learning_rate": 4.168125e-05,
      "loss": 0.0018,
      "step": 39930
    },
    {
      "epoch": 1.3313333333333333,
      "grad_norm": 0.11810441315174103,
      "learning_rate": 4.167916666666667e-05,
      "loss": 0.003,
      "step": 39940
    },
    {
      "epoch": 1.3316666666666666,
      "grad_norm": 0.17633658647537231,
      "learning_rate": 4.167708333333333e-05,
      "loss": 0.003,
      "step": 39950
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.7863913774490356,
      "learning_rate": 4.1675e-05,
      "loss": 0.0042,
      "step": 39960
    },
    {
      "epoch": 1.3323333333333334,
      "grad_norm": 0.35261261463165283,
      "learning_rate": 4.167291666666667e-05,
      "loss": 0.0023,
      "step": 39970
    },
    {
      "epoch": 1.3326666666666667,
      "grad_norm": 0.11785580962896347,
      "learning_rate": 4.1670833333333334e-05,
      "loss": 0.0032,
      "step": 39980
    },
    {
      "epoch": 1.333,
      "grad_norm": 0.20525066554546356,
      "learning_rate": 4.166875e-05,
      "loss": 0.0039,
      "step": 39990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.0587533675134182,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0026,
      "step": 40000
    },
    {
      "epoch": 1.3336666666666668,
      "grad_norm": 0.11833587288856506,
      "learning_rate": 4.166458333333334e-05,
      "loss": 0.0027,
      "step": 40010
    },
    {
      "epoch": 1.334,
      "grad_norm": 0.1765570342540741,
      "learning_rate": 4.16625e-05,
      "loss": 0.0015,
      "step": 40020
    },
    {
      "epoch": 1.3343333333333334,
      "grad_norm": 0.11757596582174301,
      "learning_rate": 4.166041666666667e-05,
      "loss": 0.0024,
      "step": 40030
    },
    {
      "epoch": 1.3346666666666667,
      "grad_norm": 0.6898376941680908,
      "learning_rate": 4.165833333333334e-05,
      "loss": 0.0028,
      "step": 40040
    },
    {
      "epoch": 1.335,
      "grad_norm": 0.610344648361206,
      "learning_rate": 4.165625e-05,
      "loss": 0.0031,
      "step": 40050
    },
    {
      "epoch": 1.3353333333333333,
      "grad_norm": 0.11829264461994171,
      "learning_rate": 4.1654166666666665e-05,
      "loss": 0.0026,
      "step": 40060
    },
    {
      "epoch": 1.3356666666666666,
      "grad_norm": 0.9408488273620605,
      "learning_rate": 4.165208333333334e-05,
      "loss": 0.0028,
      "step": 40070
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.3142211437225342,
      "learning_rate": 4.165e-05,
      "loss": 0.0029,
      "step": 40080
    },
    {
      "epoch": 1.3363333333333334,
      "grad_norm": 0.05893939360976219,
      "learning_rate": 4.164791666666667e-05,
      "loss": 0.0026,
      "step": 40090
    },
    {
      "epoch": 1.3366666666666667,
      "grad_norm": 0.06805060058832169,
      "learning_rate": 4.1645833333333334e-05,
      "loss": 0.0031,
      "step": 40100
    },
    {
      "epoch": 1.337,
      "grad_norm": 0.2702963054180145,
      "learning_rate": 4.1643750000000006e-05,
      "loss": 0.0031,
      "step": 40110
    },
    {
      "epoch": 1.3373333333333333,
      "grad_norm": 0.05878039821982384,
      "learning_rate": 4.1641666666666665e-05,
      "loss": 0.0024,
      "step": 40120
    },
    {
      "epoch": 1.3376666666666668,
      "grad_norm": 0.11798509955406189,
      "learning_rate": 4.163958333333334e-05,
      "loss": 0.0024,
      "step": 40130
    },
    {
      "epoch": 1.338,
      "grad_norm": 0.20599119365215302,
      "learning_rate": 4.16375e-05,
      "loss": 0.003,
      "step": 40140
    },
    {
      "epoch": 1.3383333333333334,
      "grad_norm": 0.029873058199882507,
      "learning_rate": 4.163541666666667e-05,
      "loss": 0.0034,
      "step": 40150
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.4125925898551941,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0035,
      "step": 40160
    },
    {
      "epoch": 1.339,
      "grad_norm": 0.20592671632766724,
      "learning_rate": 4.163125e-05,
      "loss": 0.0031,
      "step": 40170
    },
    {
      "epoch": 1.3393333333333333,
      "grad_norm": 0.20737262070178986,
      "learning_rate": 4.162916666666667e-05,
      "loss": 0.0022,
      "step": 40180
    },
    {
      "epoch": 1.3396666666666666,
      "grad_norm": 0.23512674868106842,
      "learning_rate": 4.162708333333333e-05,
      "loss": 0.0029,
      "step": 40190
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.41146141290664673,
      "learning_rate": 4.1625e-05,
      "loss": 0.0032,
      "step": 40200
    },
    {
      "epoch": 1.3403333333333334,
      "grad_norm": 0.005763962399214506,
      "learning_rate": 4.162291666666667e-05,
      "loss": 0.003,
      "step": 40210
    },
    {
      "epoch": 1.3406666666666667,
      "grad_norm": 0.059237975627183914,
      "learning_rate": 4.162083333333334e-05,
      "loss": 0.0033,
      "step": 40220
    },
    {
      "epoch": 1.341,
      "grad_norm": 0.23509959876537323,
      "learning_rate": 4.161875e-05,
      "loss": 0.0024,
      "step": 40230
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.11798601597547531,
      "learning_rate": 4.161666666666667e-05,
      "loss": 0.0031,
      "step": 40240
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 0.1470687985420227,
      "learning_rate": 4.1614583333333336e-05,
      "loss": 0.0033,
      "step": 40250
    },
    {
      "epoch": 1.342,
      "grad_norm": 0.3556794226169586,
      "learning_rate": 4.16125e-05,
      "loss": 0.0033,
      "step": 40260
    },
    {
      "epoch": 1.3423333333333334,
      "grad_norm": 0.24059945344924927,
      "learning_rate": 4.161041666666667e-05,
      "loss": 0.0027,
      "step": 40270
    },
    {
      "epoch": 1.3426666666666667,
      "grad_norm": 0.11762821674346924,
      "learning_rate": 4.160833333333334e-05,
      "loss": 0.0026,
      "step": 40280
    },
    {
      "epoch": 1.343,
      "grad_norm": 0.1769769936800003,
      "learning_rate": 4.1606250000000005e-05,
      "loss": 0.0027,
      "step": 40290
    },
    {
      "epoch": 1.3433333333333333,
      "grad_norm": 0.05954388529062271,
      "learning_rate": 4.1604166666666664e-05,
      "loss": 0.0021,
      "step": 40300
    },
    {
      "epoch": 1.3436666666666666,
      "grad_norm": 0.49977099895477295,
      "learning_rate": 4.1602083333333336e-05,
      "loss": 0.0026,
      "step": 40310
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.1781831830739975,
      "learning_rate": 4.16e-05,
      "loss": 0.003,
      "step": 40320
    },
    {
      "epoch": 1.3443333333333334,
      "grad_norm": 0.5879069566726685,
      "learning_rate": 4.159791666666667e-05,
      "loss": 0.0021,
      "step": 40330
    },
    {
      "epoch": 1.3446666666666667,
      "grad_norm": 0.058887507766485214,
      "learning_rate": 4.159583333333333e-05,
      "loss": 0.0017,
      "step": 40340
    },
    {
      "epoch": 1.345,
      "grad_norm": 0.17662526667118073,
      "learning_rate": 4.1593750000000005e-05,
      "loss": 0.0025,
      "step": 40350
    },
    {
      "epoch": 1.3453333333333333,
      "grad_norm": 0.23528125882148743,
      "learning_rate": 4.159166666666667e-05,
      "loss": 0.0021,
      "step": 40360
    },
    {
      "epoch": 1.3456666666666668,
      "grad_norm": 0.5293739438056946,
      "learning_rate": 4.1589583333333336e-05,
      "loss": 0.0033,
      "step": 40370
    },
    {
      "epoch": 1.346,
      "grad_norm": 0.088230662047863,
      "learning_rate": 4.15875e-05,
      "loss": 0.0017,
      "step": 40380
    },
    {
      "epoch": 1.3463333333333334,
      "grad_norm": 0.11791396141052246,
      "learning_rate": 4.1585416666666673e-05,
      "loss": 0.0025,
      "step": 40390
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.35289788246154785,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.0032,
      "step": 40400
    },
    {
      "epoch": 1.347,
      "grad_norm": 0.5163071155548096,
      "learning_rate": 4.158125e-05,
      "loss": 0.0022,
      "step": 40410
    },
    {
      "epoch": 1.3473333333333333,
      "grad_norm": 0.47012749314308167,
      "learning_rate": 4.157916666666667e-05,
      "loss": 0.0033,
      "step": 40420
    },
    {
      "epoch": 1.3476666666666666,
      "grad_norm": 0.14700336754322052,
      "learning_rate": 4.1577083333333335e-05,
      "loss": 0.0026,
      "step": 40430
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.08868356794118881,
      "learning_rate": 4.1575e-05,
      "loss": 0.0023,
      "step": 40440
    },
    {
      "epoch": 1.3483333333333334,
      "grad_norm": 0.4408590793609619,
      "learning_rate": 4.1572916666666666e-05,
      "loss": 0.0029,
      "step": 40450
    },
    {
      "epoch": 1.3486666666666667,
      "grad_norm": 0.2057477831840515,
      "learning_rate": 4.157083333333334e-05,
      "loss": 0.0026,
      "step": 40460
    },
    {
      "epoch": 1.349,
      "grad_norm": 0.35267511010169983,
      "learning_rate": 4.1568750000000004e-05,
      "loss": 0.0021,
      "step": 40470
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.2940700054168701,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.0036,
      "step": 40480
    },
    {
      "epoch": 1.3496666666666668,
      "grad_norm": 0.44481804966926575,
      "learning_rate": 4.1564583333333335e-05,
      "loss": 0.0022,
      "step": 40490
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.67280513048172,
      "learning_rate": 4.156250000000001e-05,
      "loss": 0.0025,
      "step": 40500
    },
    {
      "epoch": 1.3503333333333334,
      "grad_norm": 0.08882473409175873,
      "learning_rate": 4.1560416666666666e-05,
      "loss": 0.0022,
      "step": 40510
    },
    {
      "epoch": 1.3506666666666667,
      "grad_norm": 0.2055191844701767,
      "learning_rate": 4.155833333333334e-05,
      "loss": 0.0028,
      "step": 40520
    },
    {
      "epoch": 1.351,
      "grad_norm": 0.17671629786491394,
      "learning_rate": 4.1556250000000004e-05,
      "loss": 0.0023,
      "step": 40530
    },
    {
      "epoch": 1.3513333333333333,
      "grad_norm": 0.20613011717796326,
      "learning_rate": 4.155416666666667e-05,
      "loss": 0.0027,
      "step": 40540
    },
    {
      "epoch": 1.3516666666666666,
      "grad_norm": 0.2650068402290344,
      "learning_rate": 4.1552083333333335e-05,
      "loss": 0.0018,
      "step": 40550
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.17674610018730164,
      "learning_rate": 4.155e-05,
      "loss": 0.0023,
      "step": 40560
    },
    {
      "epoch": 1.3523333333333334,
      "grad_norm": 0.08898290246725082,
      "learning_rate": 4.154791666666667e-05,
      "loss": 0.0019,
      "step": 40570
    },
    {
      "epoch": 1.3526666666666667,
      "grad_norm": 0.26450061798095703,
      "learning_rate": 4.154583333333333e-05,
      "loss": 0.0033,
      "step": 40580
    },
    {
      "epoch": 1.353,
      "grad_norm": 0.05891677364706993,
      "learning_rate": 4.1543750000000004e-05,
      "loss": 0.003,
      "step": 40590
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.294366717338562,
      "learning_rate": 4.154166666666667e-05,
      "loss": 0.0021,
      "step": 40600
    },
    {
      "epoch": 1.3536666666666668,
      "grad_norm": 0.8597387671470642,
      "learning_rate": 4.1539583333333334e-05,
      "loss": 0.0016,
      "step": 40610
    },
    {
      "epoch": 1.354,
      "grad_norm": 0.04509583115577698,
      "learning_rate": 4.15375e-05,
      "loss": 0.0028,
      "step": 40620
    },
    {
      "epoch": 1.3543333333333334,
      "grad_norm": 0.7055379748344421,
      "learning_rate": 4.153541666666667e-05,
      "loss": 0.0031,
      "step": 40630
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.3358091413974762,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0031,
      "step": 40640
    },
    {
      "epoch": 1.355,
      "grad_norm": 0.17747832834720612,
      "learning_rate": 4.1531249999999996e-05,
      "loss": 0.002,
      "step": 40650
    },
    {
      "epoch": 1.3553333333333333,
      "grad_norm": 0.14751313626766205,
      "learning_rate": 4.152916666666667e-05,
      "loss": 0.0036,
      "step": 40660
    },
    {
      "epoch": 1.3556666666666666,
      "grad_norm": 0.08808895945549011,
      "learning_rate": 4.1527083333333334e-05,
      "loss": 0.0024,
      "step": 40670
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.17658039927482605,
      "learning_rate": 4.1525e-05,
      "loss": 0.0021,
      "step": 40680
    },
    {
      "epoch": 1.3563333333333334,
      "grad_norm": 0.17666879296302795,
      "learning_rate": 4.1522916666666665e-05,
      "loss": 0.0031,
      "step": 40690
    },
    {
      "epoch": 1.3566666666666667,
      "grad_norm": 0.3705620765686035,
      "learning_rate": 4.152083333333334e-05,
      "loss": 0.0022,
      "step": 40700
    },
    {
      "epoch": 1.357,
      "grad_norm": 0.17592307925224304,
      "learning_rate": 4.151875e-05,
      "loss": 0.0037,
      "step": 40710
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.17621764540672302,
      "learning_rate": 4.151666666666667e-05,
      "loss": 0.0023,
      "step": 40720
    },
    {
      "epoch": 1.3576666666666668,
      "grad_norm": 0.08875105530023575,
      "learning_rate": 4.1514583333333334e-05,
      "loss": 0.0027,
      "step": 40730
    },
    {
      "epoch": 1.358,
      "grad_norm": 0.14738251268863678,
      "learning_rate": 4.1512500000000006e-05,
      "loss": 0.0029,
      "step": 40740
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.17629587650299072,
      "learning_rate": 4.151041666666667e-05,
      "loss": 0.0026,
      "step": 40750
    },
    {
      "epoch": 1.3586666666666667,
      "grad_norm": 0.14716669917106628,
      "learning_rate": 4.150833333333334e-05,
      "loss": 0.0023,
      "step": 40760
    },
    {
      "epoch": 1.359,
      "grad_norm": 0.029613861814141273,
      "learning_rate": 4.150625e-05,
      "loss": 0.0031,
      "step": 40770
    },
    {
      "epoch": 1.3593333333333333,
      "grad_norm": 0.38607603311538696,
      "learning_rate": 4.150416666666667e-05,
      "loss": 0.0036,
      "step": 40780
    },
    {
      "epoch": 1.3596666666666666,
      "grad_norm": 0.004301487933844328,
      "learning_rate": 4.1502083333333334e-05,
      "loss": 0.0035,
      "step": 40790
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.0038069235160946846,
      "learning_rate": 4.15e-05,
      "loss": 0.002,
      "step": 40800
    },
    {
      "epoch": 1.3603333333333334,
      "grad_norm": 0.32325246930122375,
      "learning_rate": 4.149791666666667e-05,
      "loss": 0.0028,
      "step": 40810
    },
    {
      "epoch": 1.3606666666666667,
      "grad_norm": 0.20580391585826874,
      "learning_rate": 4.149583333333334e-05,
      "loss": 0.0026,
      "step": 40820
    },
    {
      "epoch": 1.361,
      "grad_norm": 0.4411352276802063,
      "learning_rate": 4.149375e-05,
      "loss": 0.0029,
      "step": 40830
    },
    {
      "epoch": 1.3613333333333333,
      "grad_norm": 0.2643285095691681,
      "learning_rate": 4.149166666666667e-05,
      "loss": 0.0029,
      "step": 40840
    },
    {
      "epoch": 1.3616666666666668,
      "grad_norm": 0.11771973967552185,
      "learning_rate": 4.148958333333334e-05,
      "loss": 0.0042,
      "step": 40850
    },
    {
      "epoch": 1.362,
      "grad_norm": 0.6169760227203369,
      "learning_rate": 4.14875e-05,
      "loss": 0.0024,
      "step": 40860
    },
    {
      "epoch": 1.3623333333333334,
      "grad_norm": 0.2946319878101349,
      "learning_rate": 4.148541666666667e-05,
      "loss": 0.0023,
      "step": 40870
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.11760490387678146,
      "learning_rate": 4.1483333333333337e-05,
      "loss": 0.003,
      "step": 40880
    },
    {
      "epoch": 1.363,
      "grad_norm": 0.2057131677865982,
      "learning_rate": 4.148125e-05,
      "loss": 0.0024,
      "step": 40890
    },
    {
      "epoch": 1.3633333333333333,
      "grad_norm": 0.008354134857654572,
      "learning_rate": 4.147916666666667e-05,
      "loss": 0.0029,
      "step": 40900
    },
    {
      "epoch": 1.3636666666666666,
      "grad_norm": 0.11758208274841309,
      "learning_rate": 4.147708333333333e-05,
      "loss": 0.0024,
      "step": 40910
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.08880520612001419,
      "learning_rate": 4.1475000000000005e-05,
      "loss": 0.0018,
      "step": 40920
    },
    {
      "epoch": 1.3643333333333334,
      "grad_norm": 0.059015803039073944,
      "learning_rate": 4.1472916666666664e-05,
      "loss": 0.0017,
      "step": 40930
    },
    {
      "epoch": 1.3646666666666667,
      "grad_norm": 0.029560720548033714,
      "learning_rate": 4.1470833333333336e-05,
      "loss": 0.0014,
      "step": 40940
    },
    {
      "epoch": 1.365,
      "grad_norm": 0.0883483961224556,
      "learning_rate": 4.146875e-05,
      "loss": 0.0024,
      "step": 40950
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.3527904450893402,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0031,
      "step": 40960
    },
    {
      "epoch": 1.3656666666666666,
      "grad_norm": 0.05938638374209404,
      "learning_rate": 4.146458333333333e-05,
      "loss": 0.0027,
      "step": 40970
    },
    {
      "epoch": 1.366,
      "grad_norm": 0.35250672698020935,
      "learning_rate": 4.1462500000000005e-05,
      "loss": 0.0037,
      "step": 40980
    },
    {
      "epoch": 1.3663333333333334,
      "grad_norm": 0.5290783643722534,
      "learning_rate": 4.146041666666667e-05,
      "loss": 0.003,
      "step": 40990
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.17610175907611847,
      "learning_rate": 4.1458333333333336e-05,
      "loss": 0.0024,
      "step": 41000
    },
    {
      "epoch": 1.367,
      "grad_norm": 0.1178421899676323,
      "learning_rate": 4.145625e-05,
      "loss": 0.0027,
      "step": 41010
    },
    {
      "epoch": 1.3673333333333333,
      "grad_norm": 0.2937219440937042,
      "learning_rate": 4.145416666666667e-05,
      "loss": 0.0027,
      "step": 41020
    },
    {
      "epoch": 1.3676666666666666,
      "grad_norm": 0.013736085966229439,
      "learning_rate": 4.145208333333334e-05,
      "loss": 0.0022,
      "step": 41030
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.08840858936309814,
      "learning_rate": 4.145e-05,
      "loss": 0.0028,
      "step": 41040
    },
    {
      "epoch": 1.3683333333333334,
      "grad_norm": 0.030119692906737328,
      "learning_rate": 4.144791666666667e-05,
      "loss": 0.0029,
      "step": 41050
    },
    {
      "epoch": 1.3686666666666667,
      "grad_norm": 1.0052403211593628,
      "learning_rate": 4.1445833333333336e-05,
      "loss": 0.0018,
      "step": 41060
    },
    {
      "epoch": 1.369,
      "grad_norm": 0.5925853252410889,
      "learning_rate": 4.144375e-05,
      "loss": 0.0024,
      "step": 41070
    },
    {
      "epoch": 1.3693333333333333,
      "grad_norm": 0.29420799016952515,
      "learning_rate": 4.1441666666666667e-05,
      "loss": 0.0023,
      "step": 41080
    },
    {
      "epoch": 1.3696666666666666,
      "grad_norm": 0.14683052897453308,
      "learning_rate": 4.143958333333334e-05,
      "loss": 0.0023,
      "step": 41090
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.23545053601264954,
      "learning_rate": 4.1437500000000004e-05,
      "loss": 0.0024,
      "step": 41100
    },
    {
      "epoch": 1.3703333333333334,
      "grad_norm": 0.18347622454166412,
      "learning_rate": 4.143541666666667e-05,
      "loss": 0.0036,
      "step": 41110
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.2355426400899887,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0024,
      "step": 41120
    },
    {
      "epoch": 1.371,
      "grad_norm": 0.050755370408296585,
      "learning_rate": 4.143125e-05,
      "loss": 0.0025,
      "step": 41130
    },
    {
      "epoch": 1.3713333333333333,
      "grad_norm": 0.007239450700581074,
      "learning_rate": 4.1429166666666666e-05,
      "loss": 0.002,
      "step": 41140
    },
    {
      "epoch": 1.3716666666666666,
      "grad_norm": 0.23489682376384735,
      "learning_rate": 4.142708333333333e-05,
      "loss": 0.0027,
      "step": 41150
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.003609106643125415,
      "learning_rate": 4.1425000000000004e-05,
      "loss": 0.0035,
      "step": 41160
    },
    {
      "epoch": 1.3723333333333334,
      "grad_norm": 0.2940313518047333,
      "learning_rate": 4.142291666666667e-05,
      "loss": 0.0035,
      "step": 41170
    },
    {
      "epoch": 1.3726666666666667,
      "grad_norm": 0.26475003361701965,
      "learning_rate": 4.1420833333333335e-05,
      "loss": 0.0022,
      "step": 41180
    },
    {
      "epoch": 1.373,
      "grad_norm": 0.02970026060938835,
      "learning_rate": 4.141875e-05,
      "loss": 0.0026,
      "step": 41190
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.00544933695346117,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.0017,
      "step": 41200
    },
    {
      "epoch": 1.3736666666666666,
      "grad_norm": 0.17706340551376343,
      "learning_rate": 4.141458333333333e-05,
      "loss": 0.0036,
      "step": 41210
    },
    {
      "epoch": 1.374,
      "grad_norm": 0.2056729942560196,
      "learning_rate": 4.1412500000000004e-05,
      "loss": 0.0023,
      "step": 41220
    },
    {
      "epoch": 1.3743333333333334,
      "grad_norm": 0.2061152309179306,
      "learning_rate": 4.141041666666667e-05,
      "loss": 0.0021,
      "step": 41230
    },
    {
      "epoch": 1.3746666666666667,
      "grad_norm": 0.20601947605609894,
      "learning_rate": 4.1408333333333335e-05,
      "loss": 0.0022,
      "step": 41240
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.11929834634065628,
      "learning_rate": 4.140625e-05,
      "loss": 0.0033,
      "step": 41250
    },
    {
      "epoch": 1.3753333333333333,
      "grad_norm": 0.18031801283359528,
      "learning_rate": 4.1404166666666666e-05,
      "loss": 0.0018,
      "step": 41260
    },
    {
      "epoch": 1.3756666666666666,
      "grad_norm": 0.23679514229297638,
      "learning_rate": 4.140208333333334e-05,
      "loss": 0.0026,
      "step": 41270
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.02994871698319912,
      "learning_rate": 4.14e-05,
      "loss": 0.002,
      "step": 41280
    },
    {
      "epoch": 1.3763333333333334,
      "grad_norm": 0.4119966924190521,
      "learning_rate": 4.139791666666667e-05,
      "loss": 0.0032,
      "step": 41290
    },
    {
      "epoch": 1.3766666666666667,
      "grad_norm": 0.14676088094711304,
      "learning_rate": 4.1395833333333334e-05,
      "loss": 0.0032,
      "step": 41300
    },
    {
      "epoch": 1.377,
      "grad_norm": 0.05943289399147034,
      "learning_rate": 4.139375000000001e-05,
      "loss": 0.0025,
      "step": 41310
    },
    {
      "epoch": 1.3773333333333333,
      "grad_norm": 0.14713160693645477,
      "learning_rate": 4.1391666666666665e-05,
      "loss": 0.0028,
      "step": 41320
    },
    {
      "epoch": 1.3776666666666666,
      "grad_norm": 0.007763537112623453,
      "learning_rate": 4.138958333333334e-05,
      "loss": 0.0033,
      "step": 41330
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 0.2737334072589874,
      "learning_rate": 4.13875e-05,
      "loss": 0.0018,
      "step": 41340
    },
    {
      "epoch": 1.3783333333333334,
      "grad_norm": 0.16131091117858887,
      "learning_rate": 4.138541666666667e-05,
      "loss": 0.0019,
      "step": 41350
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.3643553853034973,
      "learning_rate": 4.1383333333333334e-05,
      "loss": 0.0038,
      "step": 41360
    },
    {
      "epoch": 1.379,
      "grad_norm": 0.2937737703323364,
      "learning_rate": 4.1381250000000006e-05,
      "loss": 0.0025,
      "step": 41370
    },
    {
      "epoch": 1.3793333333333333,
      "grad_norm": 0.11753637343645096,
      "learning_rate": 4.137916666666667e-05,
      "loss": 0.0024,
      "step": 41380
    },
    {
      "epoch": 1.3796666666666666,
      "grad_norm": 0.05876011773943901,
      "learning_rate": 4.137708333333333e-05,
      "loss": 0.0021,
      "step": 41390
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.1175074651837349,
      "learning_rate": 4.1375e-05,
      "loss": 0.003,
      "step": 41400
    },
    {
      "epoch": 1.3803333333333334,
      "grad_norm": 0.32304975390434265,
      "learning_rate": 4.137291666666667e-05,
      "loss": 0.003,
      "step": 41410
    },
    {
      "epoch": 1.3806666666666667,
      "grad_norm": 0.14676529169082642,
      "learning_rate": 4.1370833333333334e-05,
      "loss": 0.0023,
      "step": 41420
    },
    {
      "epoch": 1.381,
      "grad_norm": 0.17616590857505798,
      "learning_rate": 4.136875e-05,
      "loss": 0.0023,
      "step": 41430
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.4413626194000244,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0028,
      "step": 41440
    },
    {
      "epoch": 1.3816666666666666,
      "grad_norm": 0.08858955651521683,
      "learning_rate": 4.136458333333334e-05,
      "loss": 0.0035,
      "step": 41450
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 0.0313209593296051,
      "learning_rate": 4.13625e-05,
      "loss": 0.003,
      "step": 41460
    },
    {
      "epoch": 1.3823333333333334,
      "grad_norm": 0.3228156566619873,
      "learning_rate": 4.136041666666667e-05,
      "loss": 0.0025,
      "step": 41470
    },
    {
      "epoch": 1.3826666666666667,
      "grad_norm": 0.0881740152835846,
      "learning_rate": 4.135833333333334e-05,
      "loss": 0.0027,
      "step": 41480
    },
    {
      "epoch": 1.383,
      "grad_norm": 0.11781618744134903,
      "learning_rate": 4.135625e-05,
      "loss": 0.002,
      "step": 41490
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.23499849438667297,
      "learning_rate": 4.1354166666666664e-05,
      "loss": 0.0027,
      "step": 41500
    },
    {
      "epoch": 1.3836666666666666,
      "grad_norm": 0.005885982420295477,
      "learning_rate": 4.135208333333334e-05,
      "loss": 0.0019,
      "step": 41510
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.11824984848499298,
      "learning_rate": 4.135e-05,
      "loss": 0.0022,
      "step": 41520
    },
    {
      "epoch": 1.3843333333333334,
      "grad_norm": 0.49495792388916016,
      "learning_rate": 4.134791666666667e-05,
      "loss": 0.004,
      "step": 41530
    },
    {
      "epoch": 1.3846666666666667,
      "grad_norm": 0.2937925457954407,
      "learning_rate": 4.134583333333333e-05,
      "loss": 0.0024,
      "step": 41540
    },
    {
      "epoch": 1.385,
      "grad_norm": 0.005757794715464115,
      "learning_rate": 4.1343750000000005e-05,
      "loss": 0.003,
      "step": 41550
    },
    {
      "epoch": 1.3853333333333333,
      "grad_norm": 0.11905345320701599,
      "learning_rate": 4.1341666666666664e-05,
      "loss": 0.0018,
      "step": 41560
    },
    {
      "epoch": 1.3856666666666666,
      "grad_norm": 0.11762760579586029,
      "learning_rate": 4.1339583333333336e-05,
      "loss": 0.0021,
      "step": 41570
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 0.2231675684452057,
      "learning_rate": 4.13375e-05,
      "loss": 0.0027,
      "step": 41580
    },
    {
      "epoch": 1.3863333333333334,
      "grad_norm": 0.14900949597358704,
      "learning_rate": 4.1335416666666674e-05,
      "loss": 0.0022,
      "step": 41590
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.23527845740318298,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0019,
      "step": 41600
    },
    {
      "epoch": 1.387,
      "grad_norm": 0.14708994328975677,
      "learning_rate": 4.1331250000000005e-05,
      "loss": 0.0029,
      "step": 41610
    },
    {
      "epoch": 1.3873333333333333,
      "grad_norm": 0.33701092004776,
      "learning_rate": 4.132916666666667e-05,
      "loss": 0.0018,
      "step": 41620
    },
    {
      "epoch": 1.3876666666666666,
      "grad_norm": 0.3817625343799591,
      "learning_rate": 4.1327083333333336e-05,
      "loss": 0.0016,
      "step": 41630
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.02952725440263748,
      "learning_rate": 4.1325e-05,
      "loss": 0.0024,
      "step": 41640
    },
    {
      "epoch": 1.3883333333333332,
      "grad_norm": 0.058724429458379745,
      "learning_rate": 4.132291666666667e-05,
      "loss": 0.0026,
      "step": 41650
    },
    {
      "epoch": 1.3886666666666667,
      "grad_norm": 0.17625154554843903,
      "learning_rate": 4.132083333333334e-05,
      "loss": 0.0022,
      "step": 41660
    },
    {
      "epoch": 1.389,
      "grad_norm": 0.7418517470359802,
      "learning_rate": 4.131875e-05,
      "loss": 0.0019,
      "step": 41670
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.05398093909025192,
      "learning_rate": 4.131666666666667e-05,
      "loss": 0.0036,
      "step": 41680
    },
    {
      "epoch": 1.3896666666666666,
      "grad_norm": 0.03010304644703865,
      "learning_rate": 4.1314583333333336e-05,
      "loss": 0.0022,
      "step": 41690
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.41165614128112793,
      "learning_rate": 4.13125e-05,
      "loss": 0.0018,
      "step": 41700
    },
    {
      "epoch": 1.3903333333333334,
      "grad_norm": 0.029986826702952385,
      "learning_rate": 4.131041666666667e-05,
      "loss": 0.0017,
      "step": 41710
    },
    {
      "epoch": 1.3906666666666667,
      "grad_norm": 0.14707481861114502,
      "learning_rate": 4.130833333333334e-05,
      "loss": 0.003,
      "step": 41720
    },
    {
      "epoch": 1.391,
      "grad_norm": 0.23487432301044464,
      "learning_rate": 4.1306250000000005e-05,
      "loss": 0.0033,
      "step": 41730
    },
    {
      "epoch": 1.3913333333333333,
      "grad_norm": 0.2935926616191864,
      "learning_rate": 4.130416666666666e-05,
      "loss": 0.0025,
      "step": 41740
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 0.11812715232372284,
      "learning_rate": 4.1302083333333336e-05,
      "loss": 0.0021,
      "step": 41750
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.6165969371795654,
      "learning_rate": 4.13e-05,
      "loss": 0.0015,
      "step": 41760
    },
    {
      "epoch": 1.3923333333333332,
      "grad_norm": 0.11748488247394562,
      "learning_rate": 4.1297916666666666e-05,
      "loss": 0.0019,
      "step": 41770
    },
    {
      "epoch": 1.3926666666666667,
      "grad_norm": 0.10332994908094406,
      "learning_rate": 4.129583333333333e-05,
      "loss": 0.0023,
      "step": 41780
    },
    {
      "epoch": 1.393,
      "grad_norm": 0.029459644109010696,
      "learning_rate": 4.1293750000000004e-05,
      "loss": 0.0013,
      "step": 41790
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.38172611594200134,
      "learning_rate": 4.129166666666667e-05,
      "loss": 0.0025,
      "step": 41800
    },
    {
      "epoch": 1.3936666666666666,
      "grad_norm": 0.02950403094291687,
      "learning_rate": 4.1289583333333335e-05,
      "loss": 0.0023,
      "step": 41810
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 0.004876348655670881,
      "learning_rate": 4.12875e-05,
      "loss": 0.0023,
      "step": 41820
    },
    {
      "epoch": 1.3943333333333334,
      "grad_norm": 0.08795405179262161,
      "learning_rate": 4.128541666666667e-05,
      "loss": 0.0022,
      "step": 41830
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.11744990944862366,
      "learning_rate": 4.128333333333333e-05,
      "loss": 0.0018,
      "step": 41840
    },
    {
      "epoch": 1.395,
      "grad_norm": 0.2938366234302521,
      "learning_rate": 4.1281250000000004e-05,
      "loss": 0.0026,
      "step": 41850
    },
    {
      "epoch": 1.3953333333333333,
      "grad_norm": 0.12081724405288696,
      "learning_rate": 4.127916666666667e-05,
      "loss": 0.0015,
      "step": 41860
    },
    {
      "epoch": 1.3956666666666666,
      "grad_norm": 0.1761268973350525,
      "learning_rate": 4.1277083333333335e-05,
      "loss": 0.0015,
      "step": 41870
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.08805403113365173,
      "learning_rate": 4.1275e-05,
      "loss": 0.0019,
      "step": 41880
    },
    {
      "epoch": 1.3963333333333332,
      "grad_norm": 0.5592909455299377,
      "learning_rate": 4.1272916666666666e-05,
      "loss": 0.0033,
      "step": 41890
    },
    {
      "epoch": 1.3966666666666667,
      "grad_norm": 0.2643323242664337,
      "learning_rate": 4.127083333333334e-05,
      "loss": 0.0024,
      "step": 41900
    },
    {
      "epoch": 1.397,
      "grad_norm": 0.14665977656841278,
      "learning_rate": 4.1268750000000004e-05,
      "loss": 0.0018,
      "step": 41910
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.35257580876350403,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0027,
      "step": 41920
    },
    {
      "epoch": 1.3976666666666666,
      "grad_norm": 0.03023364581167698,
      "learning_rate": 4.1264583333333335e-05,
      "loss": 0.003,
      "step": 41930
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 0.26436373591423035,
      "learning_rate": 4.126250000000001e-05,
      "loss": 0.0033,
      "step": 41940
    },
    {
      "epoch": 1.3983333333333334,
      "grad_norm": 0.05862009897828102,
      "learning_rate": 4.1260416666666666e-05,
      "loss": 0.0022,
      "step": 41950
    },
    {
      "epoch": 1.3986666666666667,
      "grad_norm": 0.11784050613641739,
      "learning_rate": 4.125833333333334e-05,
      "loss": 0.0024,
      "step": 41960
    },
    {
      "epoch": 1.399,
      "grad_norm": 0.3819269835948944,
      "learning_rate": 4.125625e-05,
      "loss": 0.0027,
      "step": 41970
    },
    {
      "epoch": 1.3993333333333333,
      "grad_norm": 0.4403686821460724,
      "learning_rate": 4.125416666666667e-05,
      "loss": 0.0036,
      "step": 41980
    },
    {
      "epoch": 1.3996666666666666,
      "grad_norm": 0.32297253608703613,
      "learning_rate": 4.1252083333333334e-05,
      "loss": 0.0021,
      "step": 41990
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.2937677204608917,
      "learning_rate": 4.125e-05,
      "loss": 0.0026,
      "step": 42000
    },
    {
      "epoch": 1.4003333333333332,
      "grad_norm": 0.6646348834037781,
      "learning_rate": 4.124791666666667e-05,
      "loss": 0.0022,
      "step": 42010
    },
    {
      "epoch": 1.4006666666666667,
      "grad_norm": 0.0593096986413002,
      "learning_rate": 4.124583333333333e-05,
      "loss": 0.0034,
      "step": 42020
    },
    {
      "epoch": 1.401,
      "grad_norm": 0.766872227191925,
      "learning_rate": 4.124375e-05,
      "loss": 0.0023,
      "step": 42030
    },
    {
      "epoch": 1.4013333333333333,
      "grad_norm": 0.08808280527591705,
      "learning_rate": 4.124166666666667e-05,
      "loss": 0.0024,
      "step": 42040
    },
    {
      "epoch": 1.4016666666666666,
      "grad_norm": 0.03237319365143776,
      "learning_rate": 4.1239583333333334e-05,
      "loss": 0.0042,
      "step": 42050
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 0.35219329595565796,
      "learning_rate": 4.12375e-05,
      "loss": 0.0027,
      "step": 42060
    },
    {
      "epoch": 1.4023333333333334,
      "grad_norm": 0.29349789023399353,
      "learning_rate": 4.123541666666667e-05,
      "loss": 0.002,
      "step": 42070
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.2347640097141266,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.0015,
      "step": 42080
    },
    {
      "epoch": 1.403,
      "grad_norm": 0.5867476463317871,
      "learning_rate": 4.123125e-05,
      "loss": 0.0029,
      "step": 42090
    },
    {
      "epoch": 1.4033333333333333,
      "grad_norm": 0.1172058954834938,
      "learning_rate": 4.122916666666667e-05,
      "loss": 0.003,
      "step": 42100
    },
    {
      "epoch": 1.4036666666666666,
      "grad_norm": 0.46991389989852905,
      "learning_rate": 4.1227083333333334e-05,
      "loss": 0.0023,
      "step": 42110
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.3227809965610504,
      "learning_rate": 4.1225e-05,
      "loss": 0.0027,
      "step": 42120
    },
    {
      "epoch": 1.4043333333333332,
      "grad_norm": 0.05866442248225212,
      "learning_rate": 4.1222916666666665e-05,
      "loss": 0.0019,
      "step": 42130
    },
    {
      "epoch": 1.4046666666666667,
      "grad_norm": 0.6163187026977539,
      "learning_rate": 4.122083333333334e-05,
      "loss": 0.0027,
      "step": 42140
    },
    {
      "epoch": 1.405,
      "grad_norm": 0.23487918078899384,
      "learning_rate": 4.121875e-05,
      "loss": 0.0017,
      "step": 42150
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.029512150213122368,
      "learning_rate": 4.121666666666667e-05,
      "loss": 0.0031,
      "step": 42160
    },
    {
      "epoch": 1.4056666666666666,
      "grad_norm": 0.5416434407234192,
      "learning_rate": 4.121458333333333e-05,
      "loss": 0.0038,
      "step": 42170
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 0.11725728958845139,
      "learning_rate": 4.1212500000000006e-05,
      "loss": 0.0026,
      "step": 42180
    },
    {
      "epoch": 1.4063333333333334,
      "grad_norm": 0.004323618020862341,
      "learning_rate": 4.121041666666667e-05,
      "loss": 0.0028,
      "step": 42190
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.26396769285202026,
      "learning_rate": 4.120833333333334e-05,
      "loss": 0.0029,
      "step": 42200
    },
    {
      "epoch": 1.407,
      "grad_norm": 0.14670777320861816,
      "learning_rate": 4.120625e-05,
      "loss": 0.0019,
      "step": 42210
    },
    {
      "epoch": 1.4073333333333333,
      "grad_norm": 0.11746080219745636,
      "learning_rate": 4.120416666666667e-05,
      "loss": 0.002,
      "step": 42220
    },
    {
      "epoch": 1.4076666666666666,
      "grad_norm": 0.12031438946723938,
      "learning_rate": 4.120208333333333e-05,
      "loss": 0.0027,
      "step": 42230
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.2649812698364258,
      "learning_rate": 4.12e-05,
      "loss": 0.0024,
      "step": 42240
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 0.1173381432890892,
      "learning_rate": 4.119791666666667e-05,
      "loss": 0.0022,
      "step": 42250
    },
    {
      "epoch": 1.4086666666666667,
      "grad_norm": 0.4987640976905823,
      "learning_rate": 4.1195833333333336e-05,
      "loss": 0.003,
      "step": 42260
    },
    {
      "epoch": 1.409,
      "grad_norm": 0.6162384152412415,
      "learning_rate": 4.119375e-05,
      "loss": 0.0025,
      "step": 42270
    },
    {
      "epoch": 1.4093333333333333,
      "grad_norm": 0.08799430727958679,
      "learning_rate": 4.119166666666667e-05,
      "loss": 0.0016,
      "step": 42280
    },
    {
      "epoch": 1.4096666666666666,
      "grad_norm": 0.44041913747787476,
      "learning_rate": 4.118958333333334e-05,
      "loss": 0.0018,
      "step": 42290
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.05866444483399391,
      "learning_rate": 4.11875e-05,
      "loss": 0.0029,
      "step": 42300
    },
    {
      "epoch": 1.4103333333333334,
      "grad_norm": 0.3814404308795929,
      "learning_rate": 4.118541666666667e-05,
      "loss": 0.0031,
      "step": 42310
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.009271427989006042,
      "learning_rate": 4.1183333333333336e-05,
      "loss": 0.0023,
      "step": 42320
    },
    {
      "epoch": 1.411,
      "grad_norm": 0.20547173917293549,
      "learning_rate": 4.118125e-05,
      "loss": 0.0023,
      "step": 42330
    },
    {
      "epoch": 1.4113333333333333,
      "grad_norm": 0.05913389474153519,
      "learning_rate": 4.117916666666667e-05,
      "loss": 0.0022,
      "step": 42340
    },
    {
      "epoch": 1.4116666666666666,
      "grad_norm": 0.06756407022476196,
      "learning_rate": 4.117708333333333e-05,
      "loss": 0.0029,
      "step": 42350
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.0588032603263855,
      "learning_rate": 4.1175000000000005e-05,
      "loss": 0.002,
      "step": 42360
    },
    {
      "epoch": 1.4123333333333332,
      "grad_norm": 0.05881647765636444,
      "learning_rate": 4.1172916666666663e-05,
      "loss": 0.002,
      "step": 42370
    },
    {
      "epoch": 1.4126666666666667,
      "grad_norm": 0.41084524989128113,
      "learning_rate": 4.1170833333333336e-05,
      "loss": 0.0029,
      "step": 42380
    },
    {
      "epoch": 1.413,
      "grad_norm": 0.32299676537513733,
      "learning_rate": 4.116875e-05,
      "loss": 0.0028,
      "step": 42390
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.20574550330638885,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0041,
      "step": 42400
    },
    {
      "epoch": 1.4136666666666666,
      "grad_norm": 0.5580316781997681,
      "learning_rate": 4.116458333333333e-05,
      "loss": 0.0029,
      "step": 42410
    },
    {
      "epoch": 1.414,
      "grad_norm": 0.14670707285404205,
      "learning_rate": 4.1162500000000004e-05,
      "loss": 0.0024,
      "step": 42420
    },
    {
      "epoch": 1.4143333333333334,
      "grad_norm": 0.11744596809148788,
      "learning_rate": 4.116041666666667e-05,
      "loss": 0.0015,
      "step": 42430
    },
    {
      "epoch": 1.4146666666666667,
      "grad_norm": 0.1628754884004593,
      "learning_rate": 4.1158333333333335e-05,
      "loss": 0.0021,
      "step": 42440
    },
    {
      "epoch": 1.415,
      "grad_norm": 0.059356167912483215,
      "learning_rate": 4.115625e-05,
      "loss": 0.002,
      "step": 42450
    },
    {
      "epoch": 1.4153333333333333,
      "grad_norm": 0.2941271960735321,
      "learning_rate": 4.1154166666666666e-05,
      "loss": 0.0031,
      "step": 42460
    },
    {
      "epoch": 1.4156666666666666,
      "grad_norm": 0.5578504800796509,
      "learning_rate": 4.115208333333334e-05,
      "loss": 0.003,
      "step": 42470
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.23480680584907532,
      "learning_rate": 4.115e-05,
      "loss": 0.003,
      "step": 42480
    },
    {
      "epoch": 1.4163333333333332,
      "grad_norm": 0.23505397140979767,
      "learning_rate": 4.114791666666667e-05,
      "loss": 0.0022,
      "step": 42490
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.11741573363542557,
      "learning_rate": 4.1145833333333335e-05,
      "loss": 0.0027,
      "step": 42500
    },
    {
      "epoch": 1.417,
      "grad_norm": 0.17599491775035858,
      "learning_rate": 4.114375e-05,
      "loss": 0.0024,
      "step": 42510
    },
    {
      "epoch": 1.4173333333333333,
      "grad_norm": 0.440180242061615,
      "learning_rate": 4.1141666666666666e-05,
      "loss": 0.0018,
      "step": 42520
    },
    {
      "epoch": 1.4176666666666666,
      "grad_norm": 0.1761365532875061,
      "learning_rate": 4.113958333333334e-05,
      "loss": 0.0026,
      "step": 42530
    },
    {
      "epoch": 1.418,
      "grad_norm": 0.23464898765087128,
      "learning_rate": 4.1137500000000004e-05,
      "loss": 0.0029,
      "step": 42540
    },
    {
      "epoch": 1.4183333333333334,
      "grad_norm": 0.35231414437294006,
      "learning_rate": 4.113541666666667e-05,
      "loss": 0.0022,
      "step": 42550
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.6584162712097168,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0026,
      "step": 42560
    },
    {
      "epoch": 1.419,
      "grad_norm": 0.5864311456680298,
      "learning_rate": 4.113125000000001e-05,
      "loss": 0.0018,
      "step": 42570
    },
    {
      "epoch": 1.4193333333333333,
      "grad_norm": 0.23522910475730896,
      "learning_rate": 4.1129166666666666e-05,
      "loss": 0.0031,
      "step": 42580
    },
    {
      "epoch": 1.4196666666666666,
      "grad_norm": 0.5202270746231079,
      "learning_rate": 4.112708333333333e-05,
      "loss": 0.0034,
      "step": 42590
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.2057248055934906,
      "learning_rate": 4.1125000000000004e-05,
      "loss": 0.003,
      "step": 42600
    },
    {
      "epoch": 1.4203333333333332,
      "grad_norm": 0.14699015021324158,
      "learning_rate": 4.112291666666667e-05,
      "loss": 0.0027,
      "step": 42610
    },
    {
      "epoch": 1.4206666666666667,
      "grad_norm": 0.14659978449344635,
      "learning_rate": 4.1120833333333334e-05,
      "loss": 0.0028,
      "step": 42620
    },
    {
      "epoch": 1.421,
      "grad_norm": 0.23455262184143066,
      "learning_rate": 4.111875e-05,
      "loss": 0.0047,
      "step": 42630
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.08866618573665619,
      "learning_rate": 4.111666666666667e-05,
      "loss": 0.0031,
      "step": 42640
    },
    {
      "epoch": 1.4216666666666666,
      "grad_norm": 0.08800382167100906,
      "learning_rate": 4.111458333333333e-05,
      "loss": 0.0024,
      "step": 42650
    },
    {
      "epoch": 1.422,
      "grad_norm": 0.007019403390586376,
      "learning_rate": 4.11125e-05,
      "loss": 0.0016,
      "step": 42660
    },
    {
      "epoch": 1.4223333333333334,
      "grad_norm": 0.059138767421245575,
      "learning_rate": 4.111041666666667e-05,
      "loss": 0.0022,
      "step": 42670
    },
    {
      "epoch": 1.4226666666666667,
      "grad_norm": 0.0880652442574501,
      "learning_rate": 4.110833333333334e-05,
      "loss": 0.0031,
      "step": 42680
    },
    {
      "epoch": 1.423,
      "grad_norm": 0.2377457469701767,
      "learning_rate": 4.110625e-05,
      "loss": 0.0032,
      "step": 42690
    },
    {
      "epoch": 1.4233333333333333,
      "grad_norm": 0.029622767120599747,
      "learning_rate": 4.110416666666667e-05,
      "loss": 0.0022,
      "step": 42700
    },
    {
      "epoch": 1.4236666666666666,
      "grad_norm": 0.11754711717367172,
      "learning_rate": 4.110208333333334e-05,
      "loss": 0.0023,
      "step": 42710
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.006851626560091972,
      "learning_rate": 4.11e-05,
      "loss": 0.0027,
      "step": 42720
    },
    {
      "epoch": 1.4243333333333332,
      "grad_norm": 0.26422807574272156,
      "learning_rate": 4.109791666666667e-05,
      "loss": 0.003,
      "step": 42730
    },
    {
      "epoch": 1.4246666666666667,
      "grad_norm": 0.07088492810726166,
      "learning_rate": 4.1095833333333334e-05,
      "loss": 0.0022,
      "step": 42740
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.05881569907069206,
      "learning_rate": 4.1093750000000006e-05,
      "loss": 0.0027,
      "step": 42750
    },
    {
      "epoch": 1.4253333333333333,
      "grad_norm": 0.7140117287635803,
      "learning_rate": 4.1091666666666665e-05,
      "loss": 0.0027,
      "step": 42760
    },
    {
      "epoch": 1.4256666666666666,
      "grad_norm": 0.3816206753253937,
      "learning_rate": 4.108958333333334e-05,
      "loss": 0.0031,
      "step": 42770
    },
    {
      "epoch": 1.426,
      "grad_norm": 0.8759476542472839,
      "learning_rate": 4.10875e-05,
      "loss": 0.0027,
      "step": 42780
    },
    {
      "epoch": 1.4263333333333335,
      "grad_norm": 0.11721570044755936,
      "learning_rate": 4.108541666666667e-05,
      "loss": 0.0016,
      "step": 42790
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.2950543761253357,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.0033,
      "step": 42800
    },
    {
      "epoch": 1.427,
      "grad_norm": 0.9587239623069763,
      "learning_rate": 4.1081250000000006e-05,
      "loss": 0.0032,
      "step": 42810
    },
    {
      "epoch": 1.4273333333333333,
      "grad_norm": 0.02948303148150444,
      "learning_rate": 4.107916666666667e-05,
      "loss": 0.0029,
      "step": 42820
    },
    {
      "epoch": 1.4276666666666666,
      "grad_norm": 0.6774553656578064,
      "learning_rate": 4.107708333333333e-05,
      "loss": 0.0032,
      "step": 42830
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.029576020315289497,
      "learning_rate": 4.1075e-05,
      "loss": 0.0021,
      "step": 42840
    },
    {
      "epoch": 1.4283333333333332,
      "grad_norm": 0.11740519851446152,
      "learning_rate": 4.107291666666667e-05,
      "loss": 0.0032,
      "step": 42850
    },
    {
      "epoch": 1.4286666666666665,
      "grad_norm": 0.002788156969472766,
      "learning_rate": 4.107083333333333e-05,
      "loss": 0.0026,
      "step": 42860
    },
    {
      "epoch": 1.429,
      "grad_norm": 0.2346978634595871,
      "learning_rate": 4.106875e-05,
      "loss": 0.0016,
      "step": 42870
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.2640366554260254,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0033,
      "step": 42880
    },
    {
      "epoch": 1.4296666666666666,
      "grad_norm": 0.05888906121253967,
      "learning_rate": 4.1064583333333337e-05,
      "loss": 0.0027,
      "step": 42890
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.11759063601493835,
      "learning_rate": 4.10625e-05,
      "loss": 0.0012,
      "step": 42900
    },
    {
      "epoch": 1.4303333333333335,
      "grad_norm": 0.41058653593063354,
      "learning_rate": 4.106041666666667e-05,
      "loss": 0.0038,
      "step": 42910
    },
    {
      "epoch": 1.4306666666666668,
      "grad_norm": 0.1756872981786728,
      "learning_rate": 4.105833333333334e-05,
      "loss": 0.0027,
      "step": 42920
    },
    {
      "epoch": 1.431,
      "grad_norm": 0.1173134446144104,
      "learning_rate": 4.105625e-05,
      "loss": 0.003,
      "step": 42930
    },
    {
      "epoch": 1.4313333333333333,
      "grad_norm": 0.14697913825511932,
      "learning_rate": 4.105416666666667e-05,
      "loss": 0.0026,
      "step": 42940
    },
    {
      "epoch": 1.4316666666666666,
      "grad_norm": 0.031468987464904785,
      "learning_rate": 4.1052083333333336e-05,
      "loss": 0.0016,
      "step": 42950
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.06001105159521103,
      "learning_rate": 4.105e-05,
      "loss": 0.0022,
      "step": 42960
    },
    {
      "epoch": 1.4323333333333332,
      "grad_norm": 0.46990758180618286,
      "learning_rate": 4.104791666666667e-05,
      "loss": 0.0023,
      "step": 42970
    },
    {
      "epoch": 1.4326666666666665,
      "grad_norm": 0.14688418805599213,
      "learning_rate": 4.104583333333333e-05,
      "loss": 0.0023,
      "step": 42980
    },
    {
      "epoch": 1.433,
      "grad_norm": 0.3518486022949219,
      "learning_rate": 4.1043750000000005e-05,
      "loss": 0.0031,
      "step": 42990
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.4695846736431122,
      "learning_rate": 4.104166666666667e-05,
      "loss": 0.0026,
      "step": 43000
    },
    {
      "epoch": 1.4336666666666666,
      "grad_norm": 0.6541517972946167,
      "learning_rate": 4.1039583333333336e-05,
      "loss": 0.0031,
      "step": 43010
    },
    {
      "epoch": 1.434,
      "grad_norm": 0.32247620820999146,
      "learning_rate": 4.10375e-05,
      "loss": 0.0032,
      "step": 43020
    },
    {
      "epoch": 1.4343333333333335,
      "grad_norm": 0.23553696274757385,
      "learning_rate": 4.1035416666666674e-05,
      "loss": 0.0023,
      "step": 43030
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.43986231088638306,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0021,
      "step": 43040
    },
    {
      "epoch": 1.435,
      "grad_norm": 0.586489200592041,
      "learning_rate": 4.1031250000000005e-05,
      "loss": 0.0038,
      "step": 43050
    },
    {
      "epoch": 1.4353333333333333,
      "grad_norm": 0.08813103288412094,
      "learning_rate": 4.102916666666667e-05,
      "loss": 0.0022,
      "step": 43060
    },
    {
      "epoch": 1.4356666666666666,
      "grad_norm": 0.46960920095443726,
      "learning_rate": 4.1027083333333336e-05,
      "loss": 0.0022,
      "step": 43070
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.05940956249833107,
      "learning_rate": 4.1025e-05,
      "loss": 0.0026,
      "step": 43080
    },
    {
      "epoch": 1.4363333333333332,
      "grad_norm": 0.058881137520074844,
      "learning_rate": 4.1022916666666667e-05,
      "loss": 0.0027,
      "step": 43090
    },
    {
      "epoch": 1.4366666666666665,
      "grad_norm": 0.3229556977748871,
      "learning_rate": 4.102083333333334e-05,
      "loss": 0.0023,
      "step": 43100
    },
    {
      "epoch": 1.437,
      "grad_norm": 0.3230285942554474,
      "learning_rate": 4.101875e-05,
      "loss": 0.002,
      "step": 43110
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.08822248131036758,
      "learning_rate": 4.101666666666667e-05,
      "loss": 0.0028,
      "step": 43120
    },
    {
      "epoch": 1.4376666666666666,
      "grad_norm": 0.1777142584323883,
      "learning_rate": 4.1014583333333335e-05,
      "loss": 0.0042,
      "step": 43130
    },
    {
      "epoch": 1.438,
      "grad_norm": 0.08808618783950806,
      "learning_rate": 4.10125e-05,
      "loss": 0.0028,
      "step": 43140
    },
    {
      "epoch": 1.4383333333333335,
      "grad_norm": 0.029615050181746483,
      "learning_rate": 4.1010416666666666e-05,
      "loss": 0.0018,
      "step": 43150
    },
    {
      "epoch": 1.4386666666666668,
      "grad_norm": 0.3229459524154663,
      "learning_rate": 4.100833333333334e-05,
      "loss": 0.0026,
      "step": 43160
    },
    {
      "epoch": 1.439,
      "grad_norm": 0.1761588603258133,
      "learning_rate": 4.1006250000000004e-05,
      "loss": 0.0016,
      "step": 43170
    },
    {
      "epoch": 1.4393333333333334,
      "grad_norm": 0.20584194362163544,
      "learning_rate": 4.100416666666667e-05,
      "loss": 0.0021,
      "step": 43180
    },
    {
      "epoch": 1.4396666666666667,
      "grad_norm": 0.1468934416770935,
      "learning_rate": 4.1002083333333335e-05,
      "loss": 0.0028,
      "step": 43190
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.15770941972732544,
      "learning_rate": 4.1e-05,
      "loss": 0.0033,
      "step": 43200
    },
    {
      "epoch": 1.4403333333333332,
      "grad_norm": 0.05920635163784027,
      "learning_rate": 4.0997916666666666e-05,
      "loss": 0.0031,
      "step": 43210
    },
    {
      "epoch": 1.4406666666666665,
      "grad_norm": 0.4401053786277771,
      "learning_rate": 4.099583333333333e-05,
      "loss": 0.0026,
      "step": 43220
    },
    {
      "epoch": 1.441,
      "grad_norm": 0.08774232864379883,
      "learning_rate": 4.0993750000000004e-05,
      "loss": 0.0017,
      "step": 43230
    },
    {
      "epoch": 1.4413333333333334,
      "grad_norm": 0.1758510172367096,
      "learning_rate": 4.099166666666667e-05,
      "loss": 0.0024,
      "step": 43240
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.14647839963436127,
      "learning_rate": 4.0989583333333335e-05,
      "loss": 0.0014,
      "step": 43250
    },
    {
      "epoch": 1.442,
      "grad_norm": 0.14722175896167755,
      "learning_rate": 4.09875e-05,
      "loss": 0.0023,
      "step": 43260
    },
    {
      "epoch": 1.4423333333333335,
      "grad_norm": 0.12260854989290237,
      "learning_rate": 4.098541666666667e-05,
      "loss": 0.0026,
      "step": 43270
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.14719629287719727,
      "learning_rate": 4.098333333333334e-05,
      "loss": 0.0023,
      "step": 43280
    },
    {
      "epoch": 1.443,
      "grad_norm": 0.11716165393590927,
      "learning_rate": 4.0981250000000003e-05,
      "loss": 0.0028,
      "step": 43290
    },
    {
      "epoch": 1.4433333333333334,
      "grad_norm": 0.26382386684417725,
      "learning_rate": 4.097916666666667e-05,
      "loss": 0.0025,
      "step": 43300
    },
    {
      "epoch": 1.4436666666666667,
      "grad_norm": 0.4104592502117157,
      "learning_rate": 4.0977083333333334e-05,
      "loss": 0.0032,
      "step": 43310
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.11739131063222885,
      "learning_rate": 4.0975e-05,
      "loss": 0.0021,
      "step": 43320
    },
    {
      "epoch": 1.4443333333333332,
      "grad_norm": 0.32220545411109924,
      "learning_rate": 4.0972916666666665e-05,
      "loss": 0.0033,
      "step": 43330
    },
    {
      "epoch": 1.4446666666666665,
      "grad_norm": 0.12362942844629288,
      "learning_rate": 4.097083333333334e-05,
      "loss": 0.003,
      "step": 43340
    },
    {
      "epoch": 1.445,
      "grad_norm": 0.2345736175775528,
      "learning_rate": 4.096875e-05,
      "loss": 0.0024,
      "step": 43350
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.4105067253112793,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0017,
      "step": 43360
    },
    {
      "epoch": 1.4456666666666667,
      "grad_norm": 0.029637007042765617,
      "learning_rate": 4.0964583333333334e-05,
      "loss": 0.003,
      "step": 43370
    },
    {
      "epoch": 1.446,
      "grad_norm": 0.11744513362646103,
      "learning_rate": 4.0962500000000006e-05,
      "loss": 0.0021,
      "step": 43380
    },
    {
      "epoch": 1.4463333333333335,
      "grad_norm": 0.711745023727417,
      "learning_rate": 4.0960416666666665e-05,
      "loss": 0.0031,
      "step": 43390
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.4044109582901001,
      "learning_rate": 4.095833333333334e-05,
      "loss": 0.003,
      "step": 43400
    },
    {
      "epoch": 1.447,
      "grad_norm": 0.11723171919584274,
      "learning_rate": 4.095625e-05,
      "loss": 0.0031,
      "step": 43410
    },
    {
      "epoch": 1.4473333333333334,
      "grad_norm": 0.14628855884075165,
      "learning_rate": 4.095416666666667e-05,
      "loss": 0.0024,
      "step": 43420
    },
    {
      "epoch": 1.4476666666666667,
      "grad_norm": 0.2636672556400299,
      "learning_rate": 4.0952083333333334e-05,
      "loss": 0.0027,
      "step": 43430
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.20533743500709534,
      "learning_rate": 4.095e-05,
      "loss": 0.0017,
      "step": 43440
    },
    {
      "epoch": 1.4483333333333333,
      "grad_norm": 0.5570239424705505,
      "learning_rate": 4.094791666666667e-05,
      "loss": 0.0032,
      "step": 43450
    },
    {
      "epoch": 1.4486666666666665,
      "grad_norm": 0.4693014621734619,
      "learning_rate": 4.094583333333333e-05,
      "loss": 0.0025,
      "step": 43460
    },
    {
      "epoch": 1.449,
      "grad_norm": 0.4152653217315674,
      "learning_rate": 4.094375e-05,
      "loss": 0.0051,
      "step": 43470
    },
    {
      "epoch": 1.4493333333333334,
      "grad_norm": 0.05895674228668213,
      "learning_rate": 4.094166666666667e-05,
      "loss": 0.0033,
      "step": 43480
    },
    {
      "epoch": 1.4496666666666667,
      "grad_norm": 0.030676255002617836,
      "learning_rate": 4.0939583333333333e-05,
      "loss": 0.0021,
      "step": 43490
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.20528556406497955,
      "learning_rate": 4.09375e-05,
      "loss": 0.0027,
      "step": 43500
    },
    {
      "epoch": 1.4503333333333333,
      "grad_norm": 0.05891469866037369,
      "learning_rate": 4.093541666666667e-05,
      "loss": 0.003,
      "step": 43510
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.2641942501068115,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0037,
      "step": 43520
    },
    {
      "epoch": 1.451,
      "grad_norm": 0.9522111415863037,
      "learning_rate": 4.093125e-05,
      "loss": 0.0024,
      "step": 43530
    },
    {
      "epoch": 1.4513333333333334,
      "grad_norm": 0.2710791230201721,
      "learning_rate": 4.092916666666667e-05,
      "loss": 0.0032,
      "step": 43540
    },
    {
      "epoch": 1.4516666666666667,
      "grad_norm": 0.26388299465179443,
      "learning_rate": 4.092708333333333e-05,
      "loss": 0.0019,
      "step": 43550
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.20542317628860474,
      "learning_rate": 4.0925000000000005e-05,
      "loss": 0.0027,
      "step": 43560
    },
    {
      "epoch": 1.4523333333333333,
      "grad_norm": 0.007075607776641846,
      "learning_rate": 4.0922916666666664e-05,
      "loss": 0.0033,
      "step": 43570
    },
    {
      "epoch": 1.4526666666666666,
      "grad_norm": 0.05871959030628204,
      "learning_rate": 4.0920833333333336e-05,
      "loss": 0.0021,
      "step": 43580
    },
    {
      "epoch": 1.453,
      "grad_norm": 0.3223249018192291,
      "learning_rate": 4.091875e-05,
      "loss": 0.0022,
      "step": 43590
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.23476938903331757,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.0035,
      "step": 43600
    },
    {
      "epoch": 1.4536666666666667,
      "grad_norm": 0.32256272435188293,
      "learning_rate": 4.091458333333333e-05,
      "loss": 0.0015,
      "step": 43610
    },
    {
      "epoch": 1.454,
      "grad_norm": 0.03248199075460434,
      "learning_rate": 4.0912500000000005e-05,
      "loss": 0.0023,
      "step": 43620
    },
    {
      "epoch": 1.4543333333333333,
      "grad_norm": 0.674062967300415,
      "learning_rate": 4.091041666666667e-05,
      "loss": 0.0035,
      "step": 43630
    },
    {
      "epoch": 1.4546666666666668,
      "grad_norm": 0.05875915661454201,
      "learning_rate": 4.0908333333333336e-05,
      "loss": 0.003,
      "step": 43640
    },
    {
      "epoch": 1.455,
      "grad_norm": 0.23457859456539154,
      "learning_rate": 4.090625e-05,
      "loss": 0.0021,
      "step": 43650
    },
    {
      "epoch": 1.4553333333333334,
      "grad_norm": 0.4688062071800232,
      "learning_rate": 4.0904166666666674e-05,
      "loss": 0.0021,
      "step": 43660
    },
    {
      "epoch": 1.4556666666666667,
      "grad_norm": 0.2054661065340042,
      "learning_rate": 4.090208333333333e-05,
      "loss": 0.0043,
      "step": 43670
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.5573768019676208,
      "learning_rate": 4.09e-05,
      "loss": 0.0033,
      "step": 43680
    },
    {
      "epoch": 1.4563333333333333,
      "grad_norm": 0.7917972207069397,
      "learning_rate": 4.089791666666667e-05,
      "loss": 0.0024,
      "step": 43690
    },
    {
      "epoch": 1.4566666666666666,
      "grad_norm": 0.11750300228595734,
      "learning_rate": 4.0895833333333336e-05,
      "loss": 0.0021,
      "step": 43700
    },
    {
      "epoch": 1.457,
      "grad_norm": 0.11726132035255432,
      "learning_rate": 4.089375e-05,
      "loss": 0.0018,
      "step": 43710
    },
    {
      "epoch": 1.4573333333333334,
      "grad_norm": 0.29286420345306396,
      "learning_rate": 4.089166666666667e-05,
      "loss": 0.0022,
      "step": 43720
    },
    {
      "epoch": 1.4576666666666667,
      "grad_norm": 0.11728781461715698,
      "learning_rate": 4.088958333333334e-05,
      "loss": 0.0033,
      "step": 43730
    },
    {
      "epoch": 1.458,
      "grad_norm": 0.14655481278896332,
      "learning_rate": 4.08875e-05,
      "loss": 0.003,
      "step": 43740
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.11733806133270264,
      "learning_rate": 4.088541666666667e-05,
      "loss": 0.0028,
      "step": 43750
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.14671854674816132,
      "learning_rate": 4.0883333333333335e-05,
      "loss": 0.0024,
      "step": 43760
    },
    {
      "epoch": 1.459,
      "grad_norm": 0.12822318077087402,
      "learning_rate": 4.088125e-05,
      "loss": 0.0032,
      "step": 43770
    },
    {
      "epoch": 1.4593333333333334,
      "grad_norm": 0.17649240791797638,
      "learning_rate": 4.0879166666666666e-05,
      "loss": 0.0036,
      "step": 43780
    },
    {
      "epoch": 1.4596666666666667,
      "grad_norm": 0.030308730900287628,
      "learning_rate": 4.087708333333334e-05,
      "loss": 0.0027,
      "step": 43790
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.17610324919223785,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 0.0024,
      "step": 43800
    },
    {
      "epoch": 1.4603333333333333,
      "grad_norm": 0.38098713755607605,
      "learning_rate": 4.087291666666666e-05,
      "loss": 0.0021,
      "step": 43810
    },
    {
      "epoch": 1.4606666666666666,
      "grad_norm": 0.11734504997730255,
      "learning_rate": 4.0870833333333335e-05,
      "loss": 0.0031,
      "step": 43820
    },
    {
      "epoch": 1.461,
      "grad_norm": 0.49143192172050476,
      "learning_rate": 4.086875e-05,
      "loss": 0.0019,
      "step": 43830
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.36755672097206116,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0027,
      "step": 43840
    },
    {
      "epoch": 1.4616666666666667,
      "grad_norm": 0.05884183198213577,
      "learning_rate": 4.086458333333333e-05,
      "loss": 0.0037,
      "step": 43850
    },
    {
      "epoch": 1.462,
      "grad_norm": 0.20920751988887787,
      "learning_rate": 4.0862500000000004e-05,
      "loss": 0.0025,
      "step": 43860
    },
    {
      "epoch": 1.4623333333333333,
      "grad_norm": 0.14705215394496918,
      "learning_rate": 4.086041666666667e-05,
      "loss": 0.0026,
      "step": 43870
    },
    {
      "epoch": 1.4626666666666668,
      "grad_norm": 0.40992164611816406,
      "learning_rate": 4.0858333333333335e-05,
      "loss": 0.0026,
      "step": 43880
    },
    {
      "epoch": 1.463,
      "grad_norm": 0.4397217035293579,
      "learning_rate": 4.085625e-05,
      "loss": 0.0033,
      "step": 43890
    },
    {
      "epoch": 1.4633333333333334,
      "grad_norm": 0.17593826353549957,
      "learning_rate": 4.085416666666667e-05,
      "loss": 0.0022,
      "step": 43900
    },
    {
      "epoch": 1.4636666666666667,
      "grad_norm": 0.20584595203399658,
      "learning_rate": 4.085208333333334e-05,
      "loss": 0.0037,
      "step": 43910
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.08819915354251862,
      "learning_rate": 4.085e-05,
      "loss": 0.0031,
      "step": 43920
    },
    {
      "epoch": 1.4643333333333333,
      "grad_norm": 0.1764504760503769,
      "learning_rate": 4.084791666666667e-05,
      "loss": 0.0035,
      "step": 43930
    },
    {
      "epoch": 1.4646666666666666,
      "grad_norm": 0.32279348373413086,
      "learning_rate": 4.0845833333333335e-05,
      "loss": 0.003,
      "step": 43940
    },
    {
      "epoch": 1.465,
      "grad_norm": 0.1500401496887207,
      "learning_rate": 4.084375e-05,
      "loss": 0.0028,
      "step": 43950
    },
    {
      "epoch": 1.4653333333333334,
      "grad_norm": 0.05874406173825264,
      "learning_rate": 4.0841666666666666e-05,
      "loss": 0.0034,
      "step": 43960
    },
    {
      "epoch": 1.4656666666666667,
      "grad_norm": 0.35211488604545593,
      "learning_rate": 4.083958333333334e-05,
      "loss": 0.0032,
      "step": 43970
    },
    {
      "epoch": 1.466,
      "grad_norm": 0.26678478717803955,
      "learning_rate": 4.08375e-05,
      "loss": 0.0029,
      "step": 43980
    },
    {
      "epoch": 1.4663333333333333,
      "grad_norm": 0.3437884449958801,
      "learning_rate": 4.083541666666667e-05,
      "loss": 0.0038,
      "step": 43990
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.23941545188426971,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.003,
      "step": 44000
    },
    {
      "epoch": 1.467,
      "grad_norm": 0.32245126366615295,
      "learning_rate": 4.0831250000000007e-05,
      "loss": 0.0018,
      "step": 44010
    },
    {
      "epoch": 1.4673333333333334,
      "grad_norm": 0.5083631277084351,
      "learning_rate": 4.0829166666666665e-05,
      "loss": 0.0022,
      "step": 44020
    },
    {
      "epoch": 1.4676666666666667,
      "grad_norm": 0.23404033482074738,
      "learning_rate": 4.082708333333334e-05,
      "loss": 0.0023,
      "step": 44030
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.11703391373157501,
      "learning_rate": 4.0825e-05,
      "loss": 0.0025,
      "step": 44040
    },
    {
      "epoch": 1.4683333333333333,
      "grad_norm": 0.26356860995292664,
      "learning_rate": 4.082291666666667e-05,
      "loss": 0.0024,
      "step": 44050
    },
    {
      "epoch": 1.4686666666666666,
      "grad_norm": 0.11730466037988663,
      "learning_rate": 4.0820833333333334e-05,
      "loss": 0.002,
      "step": 44060
    },
    {
      "epoch": 1.4689999999999999,
      "grad_norm": 0.08795361965894699,
      "learning_rate": 4.081875e-05,
      "loss": 0.003,
      "step": 44070
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.17567220330238342,
      "learning_rate": 4.081666666666667e-05,
      "loss": 0.0028,
      "step": 44080
    },
    {
      "epoch": 1.4696666666666667,
      "grad_norm": 0.11765526235103607,
      "learning_rate": 4.081458333333334e-05,
      "loss": 0.0031,
      "step": 44090
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.23425815999507904,
      "learning_rate": 4.08125e-05,
      "loss": 0.0024,
      "step": 44100
    },
    {
      "epoch": 1.4703333333333333,
      "grad_norm": 0.031530190259218216,
      "learning_rate": 4.081041666666667e-05,
      "loss": 0.003,
      "step": 44110
    },
    {
      "epoch": 1.4706666666666668,
      "grad_norm": 0.26399949193000793,
      "learning_rate": 4.080833333333334e-05,
      "loss": 0.0024,
      "step": 44120
    },
    {
      "epoch": 1.471,
      "grad_norm": 0.3226516544818878,
      "learning_rate": 4.080625e-05,
      "loss": 0.0022,
      "step": 44130
    },
    {
      "epoch": 1.4713333333333334,
      "grad_norm": 0.14691486954689026,
      "learning_rate": 4.080416666666667e-05,
      "loss": 0.0028,
      "step": 44140
    },
    {
      "epoch": 1.4716666666666667,
      "grad_norm": 0.3291938900947571,
      "learning_rate": 4.080208333333334e-05,
      "loss": 0.0043,
      "step": 44150
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.44023504853248596,
      "learning_rate": 4.08e-05,
      "loss": 0.0029,
      "step": 44160
    },
    {
      "epoch": 1.4723333333333333,
      "grad_norm": 1.0433578491210938,
      "learning_rate": 4.079791666666667e-05,
      "loss": 0.0034,
      "step": 44170
    },
    {
      "epoch": 1.4726666666666666,
      "grad_norm": 0.1469438076019287,
      "learning_rate": 4.079583333333333e-05,
      "loss": 0.0029,
      "step": 44180
    },
    {
      "epoch": 1.4729999999999999,
      "grad_norm": 0.058617230504751205,
      "learning_rate": 4.0793750000000006e-05,
      "loss": 0.0024,
      "step": 44190
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.17566722631454468,
      "learning_rate": 4.0791666666666664e-05,
      "loss": 0.003,
      "step": 44200
    },
    {
      "epoch": 1.4736666666666667,
      "grad_norm": 0.058995284140110016,
      "learning_rate": 4.0789583333333337e-05,
      "loss": 0.0022,
      "step": 44210
    },
    {
      "epoch": 1.474,
      "grad_norm": 0.20532085001468658,
      "learning_rate": 4.07875e-05,
      "loss": 0.0034,
      "step": 44220
    },
    {
      "epoch": 1.4743333333333333,
      "grad_norm": 0.02960691973567009,
      "learning_rate": 4.078541666666667e-05,
      "loss": 0.0023,
      "step": 44230
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.23455458879470825,
      "learning_rate": 4.078333333333333e-05,
      "loss": 0.0017,
      "step": 44240
    },
    {
      "epoch": 1.475,
      "grad_norm": 0.03043689765036106,
      "learning_rate": 4.0781250000000005e-05,
      "loss": 0.0027,
      "step": 44250
    },
    {
      "epoch": 1.4753333333333334,
      "grad_norm": 0.05858498811721802,
      "learning_rate": 4.077916666666667e-05,
      "loss": 0.0027,
      "step": 44260
    },
    {
      "epoch": 1.4756666666666667,
      "grad_norm": 0.14670433104038239,
      "learning_rate": 4.0777083333333336e-05,
      "loss": 0.0029,
      "step": 44270
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.17611756920814514,
      "learning_rate": 4.0775e-05,
      "loss": 0.0034,
      "step": 44280
    },
    {
      "epoch": 1.4763333333333333,
      "grad_norm": 0.11775833368301392,
      "learning_rate": 4.077291666666667e-05,
      "loss": 0.0035,
      "step": 44290
    },
    {
      "epoch": 1.4766666666666666,
      "grad_norm": 0.26375365257263184,
      "learning_rate": 4.077083333333333e-05,
      "loss": 0.0022,
      "step": 44300
    },
    {
      "epoch": 1.4769999999999999,
      "grad_norm": 0.1759311705827713,
      "learning_rate": 4.076875e-05,
      "loss": 0.0036,
      "step": 44310
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.2642216384410858,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.0033,
      "step": 44320
    },
    {
      "epoch": 1.4776666666666667,
      "grad_norm": 0.17577296495437622,
      "learning_rate": 4.0764583333333336e-05,
      "loss": 0.0026,
      "step": 44330
    },
    {
      "epoch": 1.478,
      "grad_norm": 0.08801774680614471,
      "learning_rate": 4.07625e-05,
      "loss": 0.0025,
      "step": 44340
    },
    {
      "epoch": 1.4783333333333333,
      "grad_norm": 0.4102421700954437,
      "learning_rate": 4.076041666666667e-05,
      "loss": 0.0026,
      "step": 44350
    },
    {
      "epoch": 1.4786666666666668,
      "grad_norm": 0.058813124895095825,
      "learning_rate": 4.075833333333334e-05,
      "loss": 0.0023,
      "step": 44360
    },
    {
      "epoch": 1.479,
      "grad_norm": 0.26359260082244873,
      "learning_rate": 4.0756250000000005e-05,
      "loss": 0.0032,
      "step": 44370
    },
    {
      "epoch": 1.4793333333333334,
      "grad_norm": 0.32279983162879944,
      "learning_rate": 4.075416666666667e-05,
      "loss": 0.0018,
      "step": 44380
    },
    {
      "epoch": 1.4796666666666667,
      "grad_norm": 0.2929683029651642,
      "learning_rate": 4.0752083333333336e-05,
      "loss": 0.0014,
      "step": 44390
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.030341316014528275,
      "learning_rate": 4.075e-05,
      "loss": 0.0018,
      "step": 44400
    },
    {
      "epoch": 1.4803333333333333,
      "grad_norm": 0.19357021152973175,
      "learning_rate": 4.074791666666667e-05,
      "loss": 0.0027,
      "step": 44410
    },
    {
      "epoch": 1.4806666666666666,
      "grad_norm": 0.2932594120502472,
      "learning_rate": 4.074583333333333e-05,
      "loss": 0.0022,
      "step": 44420
    },
    {
      "epoch": 1.4809999999999999,
      "grad_norm": 0.41060274839401245,
      "learning_rate": 4.0743750000000004e-05,
      "loss": 0.0017,
      "step": 44430
    },
    {
      "epoch": 1.4813333333333334,
      "grad_norm": 0.23473197221755981,
      "learning_rate": 4.074166666666667e-05,
      "loss": 0.0029,
      "step": 44440
    },
    {
      "epoch": 1.4816666666666667,
      "grad_norm": 0.5272400975227356,
      "learning_rate": 4.0739583333333335e-05,
      "loss": 0.0018,
      "step": 44450
    },
    {
      "epoch": 1.482,
      "grad_norm": 0.05915326252579689,
      "learning_rate": 4.07375e-05,
      "loss": 0.0022,
      "step": 44460
    },
    {
      "epoch": 1.4823333333333333,
      "grad_norm": 0.007619874551892281,
      "learning_rate": 4.073541666666667e-05,
      "loss": 0.0024,
      "step": 44470
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.23477767407894135,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0019,
      "step": 44480
    },
    {
      "epoch": 1.483,
      "grad_norm": 0.20553842186927795,
      "learning_rate": 4.0731250000000004e-05,
      "loss": 0.0021,
      "step": 44490
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.5575428009033203,
      "learning_rate": 4.072916666666667e-05,
      "loss": 0.0031,
      "step": 44500
    },
    {
      "epoch": 1.4836666666666667,
      "grad_norm": 0.2638789713382721,
      "learning_rate": 4.0727083333333335e-05,
      "loss": 0.0022,
      "step": 44510
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.030356116592884064,
      "learning_rate": 4.0725e-05,
      "loss": 0.0033,
      "step": 44520
    },
    {
      "epoch": 1.4843333333333333,
      "grad_norm": 0.31864050030708313,
      "learning_rate": 4.0722916666666666e-05,
      "loss": 0.0027,
      "step": 44530
    },
    {
      "epoch": 1.4846666666666666,
      "grad_norm": 0.26424646377563477,
      "learning_rate": 4.072083333333334e-05,
      "loss": 0.0027,
      "step": 44540
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 0.39527735114097595,
      "learning_rate": 4.071875e-05,
      "loss": 0.0026,
      "step": 44550
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.11763456463813782,
      "learning_rate": 4.071666666666667e-05,
      "loss": 0.0025,
      "step": 44560
    },
    {
      "epoch": 1.4856666666666667,
      "grad_norm": 0.030145613476634026,
      "learning_rate": 4.0714583333333335e-05,
      "loss": 0.0019,
      "step": 44570
    },
    {
      "epoch": 1.486,
      "grad_norm": 0.29364675283432007,
      "learning_rate": 4.07125e-05,
      "loss": 0.0021,
      "step": 44580
    },
    {
      "epoch": 1.4863333333333333,
      "grad_norm": 0.24393846094608307,
      "learning_rate": 4.0710416666666666e-05,
      "loss": 0.0028,
      "step": 44590
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.029760513454675674,
      "learning_rate": 4.070833333333334e-05,
      "loss": 0.0028,
      "step": 44600
    },
    {
      "epoch": 1.487,
      "grad_norm": 0.08818954229354858,
      "learning_rate": 4.0706250000000003e-05,
      "loss": 0.0035,
      "step": 44610
    },
    {
      "epoch": 1.4873333333333334,
      "grad_norm": 0.4397808611392975,
      "learning_rate": 4.070416666666667e-05,
      "loss": 0.0023,
      "step": 44620
    },
    {
      "epoch": 1.4876666666666667,
      "grad_norm": 0.3812679648399353,
      "learning_rate": 4.0702083333333334e-05,
      "loss": 0.0023,
      "step": 44630
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.23456645011901855,
      "learning_rate": 4.07e-05,
      "loss": 0.0019,
      "step": 44640
    },
    {
      "epoch": 1.4883333333333333,
      "grad_norm": 0.11743487417697906,
      "learning_rate": 4.069791666666667e-05,
      "loss": 0.0017,
      "step": 44650
    },
    {
      "epoch": 1.4886666666666666,
      "grad_norm": 0.17627853155136108,
      "learning_rate": 4.069583333333333e-05,
      "loss": 0.0019,
      "step": 44660
    },
    {
      "epoch": 1.4889999999999999,
      "grad_norm": 0.29268908500671387,
      "learning_rate": 4.069375e-05,
      "loss": 0.0016,
      "step": 44670
    },
    {
      "epoch": 1.4893333333333334,
      "grad_norm": 0.5569907426834106,
      "learning_rate": 4.069166666666667e-05,
      "loss": 0.0031,
      "step": 44680
    },
    {
      "epoch": 1.4896666666666667,
      "grad_norm": 0.10635390877723694,
      "learning_rate": 4.0689583333333334e-05,
      "loss": 0.0018,
      "step": 44690
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.35150307416915894,
      "learning_rate": 4.06875e-05,
      "loss": 0.0016,
      "step": 44700
    },
    {
      "epoch": 1.4903333333333333,
      "grad_norm": 0.029757842421531677,
      "learning_rate": 4.068541666666667e-05,
      "loss": 0.0014,
      "step": 44710
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.1464666873216629,
      "learning_rate": 4.068333333333334e-05,
      "loss": 0.0019,
      "step": 44720
    },
    {
      "epoch": 1.491,
      "grad_norm": 0.2639828324317932,
      "learning_rate": 4.068125e-05,
      "loss": 0.0025,
      "step": 44730
    },
    {
      "epoch": 1.4913333333333334,
      "grad_norm": 0.17583328485488892,
      "learning_rate": 4.067916666666667e-05,
      "loss": 0.0025,
      "step": 44740
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.17582903802394867,
      "learning_rate": 4.067708333333334e-05,
      "loss": 0.0027,
      "step": 44750
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.2635689377784729,
      "learning_rate": 4.0675e-05,
      "loss": 0.0023,
      "step": 44760
    },
    {
      "epoch": 1.4923333333333333,
      "grad_norm": 0.1467762291431427,
      "learning_rate": 4.0672916666666665e-05,
      "loss": 0.0027,
      "step": 44770
    },
    {
      "epoch": 1.4926666666666666,
      "grad_norm": 0.20564785599708557,
      "learning_rate": 4.067083333333334e-05,
      "loss": 0.0026,
      "step": 44780
    },
    {
      "epoch": 1.4929999999999999,
      "grad_norm": 0.3225339353084564,
      "learning_rate": 4.066875e-05,
      "loss": 0.0025,
      "step": 44790
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.11801553517580032,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0023,
      "step": 44800
    },
    {
      "epoch": 1.4936666666666667,
      "grad_norm": 0.1169673278927803,
      "learning_rate": 4.0664583333333334e-05,
      "loss": 0.0023,
      "step": 44810
    },
    {
      "epoch": 1.494,
      "grad_norm": 0.029636720195412636,
      "learning_rate": 4.0662500000000006e-05,
      "loss": 0.0021,
      "step": 44820
    },
    {
      "epoch": 1.4943333333333333,
      "grad_norm": 0.061233632266521454,
      "learning_rate": 4.0660416666666665e-05,
      "loss": 0.0018,
      "step": 44830
    },
    {
      "epoch": 1.4946666666666666,
      "grad_norm": 0.41035526990890503,
      "learning_rate": 4.065833333333334e-05,
      "loss": 0.0027,
      "step": 44840
    },
    {
      "epoch": 1.495,
      "grad_norm": 0.11801989376544952,
      "learning_rate": 4.065625e-05,
      "loss": 0.0026,
      "step": 44850
    },
    {
      "epoch": 1.4953333333333334,
      "grad_norm": 0.1203889548778534,
      "learning_rate": 4.065416666666667e-05,
      "loss": 0.0027,
      "step": 44860
    },
    {
      "epoch": 1.4956666666666667,
      "grad_norm": 0.059082888066768646,
      "learning_rate": 4.065208333333333e-05,
      "loss": 0.003,
      "step": 44870
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.1464569866657257,
      "learning_rate": 4.065e-05,
      "loss": 0.0015,
      "step": 44880
    },
    {
      "epoch": 1.4963333333333333,
      "grad_norm": 0.4981823265552521,
      "learning_rate": 4.064791666666667e-05,
      "loss": 0.0028,
      "step": 44890
    },
    {
      "epoch": 1.4966666666666666,
      "grad_norm": 0.0882442370057106,
      "learning_rate": 4.064583333333333e-05,
      "loss": 0.0021,
      "step": 44900
    },
    {
      "epoch": 1.4969999999999999,
      "grad_norm": 0.030008789151906967,
      "learning_rate": 4.064375e-05,
      "loss": 0.0019,
      "step": 44910
    },
    {
      "epoch": 1.4973333333333334,
      "grad_norm": 0.058880604803562164,
      "learning_rate": 4.064166666666667e-05,
      "loss": 0.0027,
      "step": 44920
    },
    {
      "epoch": 1.4976666666666667,
      "grad_norm": 0.11798027157783508,
      "learning_rate": 4.063958333333334e-05,
      "loss": 0.0021,
      "step": 44930
    },
    {
      "epoch": 1.498,
      "grad_norm": 0.08796285837888718,
      "learning_rate": 4.06375e-05,
      "loss": 0.0022,
      "step": 44940
    },
    {
      "epoch": 1.4983333333333333,
      "grad_norm": 0.2342865914106369,
      "learning_rate": 4.063541666666667e-05,
      "loss": 0.0026,
      "step": 44950
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.1763632446527481,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.004,
      "step": 44960
    },
    {
      "epoch": 1.499,
      "grad_norm": 0.005205958150327206,
      "learning_rate": 4.063125e-05,
      "loss": 0.0022,
      "step": 44970
    },
    {
      "epoch": 1.4993333333333334,
      "grad_norm": 0.10584830492734909,
      "learning_rate": 4.062916666666667e-05,
      "loss": 0.0018,
      "step": 44980
    },
    {
      "epoch": 1.4996666666666667,
      "grad_norm": 0.058956339955329895,
      "learning_rate": 4.062708333333334e-05,
      "loss": 0.0017,
      "step": 44990
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.3959934711456299,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 0.0021,
      "step": 45000
    },
    {
      "epoch": 1.5003333333333333,
      "grad_norm": 0.351863294839859,
      "learning_rate": 4.0622916666666664e-05,
      "loss": 0.0035,
      "step": 45010
    },
    {
      "epoch": 1.5006666666666666,
      "grad_norm": 0.41018396615982056,
      "learning_rate": 4.0620833333333336e-05,
      "loss": 0.0028,
      "step": 45020
    },
    {
      "epoch": 1.501,
      "grad_norm": 0.26375043392181396,
      "learning_rate": 4.061875e-05,
      "loss": 0.0021,
      "step": 45030
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.11755523830652237,
      "learning_rate": 4.061666666666667e-05,
      "loss": 0.0033,
      "step": 45040
    },
    {
      "epoch": 1.5016666666666667,
      "grad_norm": 0.12086154520511627,
      "learning_rate": 4.061458333333333e-05,
      "loss": 0.0026,
      "step": 45050
    },
    {
      "epoch": 1.502,
      "grad_norm": 0.05890011787414551,
      "learning_rate": 4.0612500000000005e-05,
      "loss": 0.0027,
      "step": 45060
    },
    {
      "epoch": 1.5023333333333333,
      "grad_norm": 0.3513951599597931,
      "learning_rate": 4.061041666666667e-05,
      "loss": 0.0023,
      "step": 45070
    },
    {
      "epoch": 1.5026666666666668,
      "grad_norm": 0.1174272671341896,
      "learning_rate": 4.0608333333333336e-05,
      "loss": 0.0031,
      "step": 45080
    },
    {
      "epoch": 1.5030000000000001,
      "grad_norm": 0.4102611243724823,
      "learning_rate": 4.060625e-05,
      "loss": 0.0022,
      "step": 45090
    },
    {
      "epoch": 1.5033333333333334,
      "grad_norm": 0.2930522859096527,
      "learning_rate": 4.060416666666667e-05,
      "loss": 0.0022,
      "step": 45100
    },
    {
      "epoch": 1.5036666666666667,
      "grad_norm": 0.5946877598762512,
      "learning_rate": 4.060208333333333e-05,
      "loss": 0.0029,
      "step": 45110
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.4688399136066437,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0025,
      "step": 45120
    },
    {
      "epoch": 1.5043333333333333,
      "grad_norm": 0.14620859920978546,
      "learning_rate": 4.059791666666667e-05,
      "loss": 0.0024,
      "step": 45130
    },
    {
      "epoch": 1.5046666666666666,
      "grad_norm": 0.23486363887786865,
      "learning_rate": 4.0595833333333335e-05,
      "loss": 0.0033,
      "step": 45140
    },
    {
      "epoch": 1.505,
      "grad_norm": 0.1170620322227478,
      "learning_rate": 4.059375e-05,
      "loss": 0.0028,
      "step": 45150
    },
    {
      "epoch": 1.5053333333333332,
      "grad_norm": 0.17574642598628998,
      "learning_rate": 4.0591666666666666e-05,
      "loss": 0.0029,
      "step": 45160
    },
    {
      "epoch": 1.5056666666666667,
      "grad_norm": 0.03003956936299801,
      "learning_rate": 4.058958333333334e-05,
      "loss": 0.0016,
      "step": 45170
    },
    {
      "epoch": 1.506,
      "grad_norm": 0.1673596054315567,
      "learning_rate": 4.05875e-05,
      "loss": 0.0025,
      "step": 45180
    },
    {
      "epoch": 1.5063333333333333,
      "grad_norm": 0.3809328079223633,
      "learning_rate": 4.058541666666667e-05,
      "loss": 0.0022,
      "step": 45190
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.17553484439849854,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.0026,
      "step": 45200
    },
    {
      "epoch": 1.5070000000000001,
      "grad_norm": 0.3814719021320343,
      "learning_rate": 4.058125000000001e-05,
      "loss": 0.0025,
      "step": 45210
    },
    {
      "epoch": 1.5073333333333334,
      "grad_norm": 0.20470090210437775,
      "learning_rate": 4.0579166666666666e-05,
      "loss": 0.0025,
      "step": 45220
    },
    {
      "epoch": 1.5076666666666667,
      "grad_norm": 0.4173615872859955,
      "learning_rate": 4.057708333333334e-05,
      "loss": 0.0025,
      "step": 45230
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.2928850054740906,
      "learning_rate": 4.0575000000000004e-05,
      "loss": 0.0027,
      "step": 45240
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 0.26384618878364563,
      "learning_rate": 4.057291666666667e-05,
      "loss": 0.0022,
      "step": 45250
    },
    {
      "epoch": 1.5086666666666666,
      "grad_norm": 0.29297274351119995,
      "learning_rate": 4.0570833333333335e-05,
      "loss": 0.0033,
      "step": 45260
    },
    {
      "epoch": 1.509,
      "grad_norm": 0.11738143861293793,
      "learning_rate": 4.056875e-05,
      "loss": 0.0021,
      "step": 45270
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.7547982335090637,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0033,
      "step": 45280
    },
    {
      "epoch": 1.5096666666666667,
      "grad_norm": 0.11722400784492493,
      "learning_rate": 4.056458333333333e-05,
      "loss": 0.0014,
      "step": 45290
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.41160714626312256,
      "learning_rate": 4.0562500000000003e-05,
      "loss": 0.0024,
      "step": 45300
    },
    {
      "epoch": 1.5103333333333333,
      "grad_norm": 0.05915643647313118,
      "learning_rate": 4.056041666666667e-05,
      "loss": 0.0019,
      "step": 45310
    },
    {
      "epoch": 1.5106666666666668,
      "grad_norm": 0.38113144040107727,
      "learning_rate": 4.0558333333333334e-05,
      "loss": 0.0028,
      "step": 45320
    },
    {
      "epoch": 1.5110000000000001,
      "grad_norm": 0.35174062848091125,
      "learning_rate": 4.055625e-05,
      "loss": 0.0022,
      "step": 45330
    },
    {
      "epoch": 1.5113333333333334,
      "grad_norm": 0.4753928780555725,
      "learning_rate": 4.055416666666667e-05,
      "loss": 0.0027,
      "step": 45340
    },
    {
      "epoch": 1.5116666666666667,
      "grad_norm": 0.11754277348518372,
      "learning_rate": 4.055208333333334e-05,
      "loss": 0.0026,
      "step": 45350
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.03502895310521126,
      "learning_rate": 4.055e-05,
      "loss": 0.0021,
      "step": 45360
    },
    {
      "epoch": 1.5123333333333333,
      "grad_norm": 0.2933572232723236,
      "learning_rate": 4.054791666666667e-05,
      "loss": 0.002,
      "step": 45370
    },
    {
      "epoch": 1.5126666666666666,
      "grad_norm": 0.03090755082666874,
      "learning_rate": 4.0545833333333334e-05,
      "loss": 0.0021,
      "step": 45380
    },
    {
      "epoch": 1.513,
      "grad_norm": 0.059263188391923904,
      "learning_rate": 4.054375e-05,
      "loss": 0.002,
      "step": 45390
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.4689740240573883,
      "learning_rate": 4.0541666666666665e-05,
      "loss": 0.0025,
      "step": 45400
    },
    {
      "epoch": 1.5136666666666667,
      "grad_norm": 0.05866800993680954,
      "learning_rate": 4.053958333333334e-05,
      "loss": 0.0022,
      "step": 45410
    },
    {
      "epoch": 1.514,
      "grad_norm": 0.11791376024484634,
      "learning_rate": 4.05375e-05,
      "loss": 0.0025,
      "step": 45420
    },
    {
      "epoch": 1.5143333333333333,
      "grad_norm": 0.005896143615245819,
      "learning_rate": 4.053541666666667e-05,
      "loss": 0.0016,
      "step": 45430
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.08811954408884048,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0021,
      "step": 45440
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 0.2640126347541809,
      "learning_rate": 4.0531250000000006e-05,
      "loss": 0.0035,
      "step": 45450
    },
    {
      "epoch": 1.5153333333333334,
      "grad_norm": 1.1384073495864868,
      "learning_rate": 4.0529166666666665e-05,
      "loss": 0.0037,
      "step": 45460
    },
    {
      "epoch": 1.5156666666666667,
      "grad_norm": 0.43434393405914307,
      "learning_rate": 4.052708333333334e-05,
      "loss": 0.0024,
      "step": 45470
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.005918513983488083,
      "learning_rate": 4.0525e-05,
      "loss": 0.0031,
      "step": 45480
    },
    {
      "epoch": 1.5163333333333333,
      "grad_norm": 0.06400617957115173,
      "learning_rate": 4.052291666666667e-05,
      "loss": 0.0022,
      "step": 45490
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.20535804331302643,
      "learning_rate": 4.0520833333333333e-05,
      "loss": 0.0019,
      "step": 45500
    },
    {
      "epoch": 1.517,
      "grad_norm": 0.17569877207279205,
      "learning_rate": 4.051875e-05,
      "loss": 0.0028,
      "step": 45510
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.4697432816028595,
      "learning_rate": 4.051666666666667e-05,
      "loss": 0.0029,
      "step": 45520
    },
    {
      "epoch": 1.5176666666666667,
      "grad_norm": 0.21075886487960815,
      "learning_rate": 4.051458333333334e-05,
      "loss": 0.0026,
      "step": 45530
    },
    {
      "epoch": 1.518,
      "grad_norm": 0.381392240524292,
      "learning_rate": 4.05125e-05,
      "loss": 0.0019,
      "step": 45540
    },
    {
      "epoch": 1.5183333333333333,
      "grad_norm": 0.11738399416208267,
      "learning_rate": 4.051041666666667e-05,
      "loss": 0.0033,
      "step": 45550
    },
    {
      "epoch": 1.5186666666666668,
      "grad_norm": 0.205148845911026,
      "learning_rate": 4.050833333333334e-05,
      "loss": 0.0035,
      "step": 45560
    },
    {
      "epoch": 1.5190000000000001,
      "grad_norm": 0.235050231218338,
      "learning_rate": 4.050625e-05,
      "loss": 0.0032,
      "step": 45570
    },
    {
      "epoch": 1.5193333333333334,
      "grad_norm": 0.05861503258347511,
      "learning_rate": 4.050416666666667e-05,
      "loss": 0.0032,
      "step": 45580
    },
    {
      "epoch": 1.5196666666666667,
      "grad_norm": 0.08789312839508057,
      "learning_rate": 4.0502083333333336e-05,
      "loss": 0.0025,
      "step": 45590
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.35180413722991943,
      "learning_rate": 4.05e-05,
      "loss": 0.0019,
      "step": 45600
    },
    {
      "epoch": 1.5203333333333333,
      "grad_norm": 0.2052701860666275,
      "learning_rate": 4.049791666666667e-05,
      "loss": 0.0034,
      "step": 45610
    },
    {
      "epoch": 1.5206666666666666,
      "grad_norm": 0.05900251120328903,
      "learning_rate": 4.049583333333333e-05,
      "loss": 0.0027,
      "step": 45620
    },
    {
      "epoch": 1.521,
      "grad_norm": 0.4396541118621826,
      "learning_rate": 4.0493750000000005e-05,
      "loss": 0.0024,
      "step": 45630
    },
    {
      "epoch": 1.5213333333333332,
      "grad_norm": 0.43975093960762024,
      "learning_rate": 4.0491666666666664e-05,
      "loss": 0.0028,
      "step": 45640
    },
    {
      "epoch": 1.5216666666666665,
      "grad_norm": 0.08824653178453445,
      "learning_rate": 4.0489583333333336e-05,
      "loss": 0.0028,
      "step": 45650
    },
    {
      "epoch": 1.522,
      "grad_norm": 0.556495726108551,
      "learning_rate": 4.04875e-05,
      "loss": 0.0027,
      "step": 45660
    },
    {
      "epoch": 1.5223333333333333,
      "grad_norm": 0.059049513190984726,
      "learning_rate": 4.048541666666667e-05,
      "loss": 0.0037,
      "step": 45670
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.5860159397125244,
      "learning_rate": 4.048333333333333e-05,
      "loss": 0.0023,
      "step": 45680
    },
    {
      "epoch": 1.5230000000000001,
      "grad_norm": 0.4396427869796753,
      "learning_rate": 4.0481250000000005e-05,
      "loss": 0.0036,
      "step": 45690
    },
    {
      "epoch": 1.5233333333333334,
      "grad_norm": 0.38083675503730774,
      "learning_rate": 4.047916666666667e-05,
      "loss": 0.0024,
      "step": 45700
    },
    {
      "epoch": 1.5236666666666667,
      "grad_norm": 0.23438745737075806,
      "learning_rate": 4.0477083333333336e-05,
      "loss": 0.0034,
      "step": 45710
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.26356497406959534,
      "learning_rate": 4.0475e-05,
      "loss": 0.0014,
      "step": 45720
    },
    {
      "epoch": 1.5243333333333333,
      "grad_norm": 0.17551974952220917,
      "learning_rate": 4.047291666666667e-05,
      "loss": 0.002,
      "step": 45730
    },
    {
      "epoch": 1.5246666666666666,
      "grad_norm": 0.11698000878095627,
      "learning_rate": 4.047083333333333e-05,
      "loss": 0.0033,
      "step": 45740
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.2631843090057373,
      "learning_rate": 4.046875e-05,
      "loss": 0.005,
      "step": 45750
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.20501519739627838,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0022,
      "step": 45760
    },
    {
      "epoch": 1.5256666666666665,
      "grad_norm": 0.20515470206737518,
      "learning_rate": 4.0464583333333335e-05,
      "loss": 0.0021,
      "step": 45770
    },
    {
      "epoch": 1.526,
      "grad_norm": 0.29307934641838074,
      "learning_rate": 4.04625e-05,
      "loss": 0.0028,
      "step": 45780
    },
    {
      "epoch": 1.5263333333333333,
      "grad_norm": 0.4689227342605591,
      "learning_rate": 4.0460416666666666e-05,
      "loss": 0.0024,
      "step": 45790
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.20468878746032715,
      "learning_rate": 4.045833333333334e-05,
      "loss": 0.0035,
      "step": 45800
    },
    {
      "epoch": 1.5270000000000001,
      "grad_norm": 0.7893018126487732,
      "learning_rate": 4.0456250000000004e-05,
      "loss": 0.0031,
      "step": 45810
    },
    {
      "epoch": 1.5273333333333334,
      "grad_norm": 0.2635456621646881,
      "learning_rate": 4.045416666666667e-05,
      "loss": 0.0036,
      "step": 45820
    },
    {
      "epoch": 1.5276666666666667,
      "grad_norm": 0.26368385553359985,
      "learning_rate": 4.0452083333333335e-05,
      "loss": 0.0023,
      "step": 45830
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.8121872544288635,
      "learning_rate": 4.045000000000001e-05,
      "loss": 0.0022,
      "step": 45840
    },
    {
      "epoch": 1.5283333333333333,
      "grad_norm": 0.05879862979054451,
      "learning_rate": 4.0447916666666666e-05,
      "loss": 0.0042,
      "step": 45850
    },
    {
      "epoch": 1.5286666666666666,
      "grad_norm": 0.43919235467910767,
      "learning_rate": 4.044583333333333e-05,
      "loss": 0.0024,
      "step": 45860
    },
    {
      "epoch": 1.529,
      "grad_norm": 0.08838390558958054,
      "learning_rate": 4.0443750000000004e-05,
      "loss": 0.0024,
      "step": 45870
    },
    {
      "epoch": 1.5293333333333332,
      "grad_norm": 0.1758425086736679,
      "learning_rate": 4.044166666666667e-05,
      "loss": 0.0029,
      "step": 45880
    },
    {
      "epoch": 1.5296666666666665,
      "grad_norm": 0.17574790120124817,
      "learning_rate": 4.0439583333333335e-05,
      "loss": 0.003,
      "step": 45890
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.20507100224494934,
      "learning_rate": 4.04375e-05,
      "loss": 0.0036,
      "step": 45900
    },
    {
      "epoch": 1.5303333333333333,
      "grad_norm": 0.32222941517829895,
      "learning_rate": 4.043541666666667e-05,
      "loss": 0.0021,
      "step": 45910
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.008312137797474861,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0033,
      "step": 45920
    },
    {
      "epoch": 1.5310000000000001,
      "grad_norm": 0.6488920450210571,
      "learning_rate": 4.0431250000000004e-05,
      "loss": 0.0031,
      "step": 45930
    },
    {
      "epoch": 1.5313333333333334,
      "grad_norm": 0.030107982456684113,
      "learning_rate": 4.042916666666667e-05,
      "loss": 0.0033,
      "step": 45940
    },
    {
      "epoch": 1.5316666666666667,
      "grad_norm": 0.00588664785027504,
      "learning_rate": 4.0427083333333335e-05,
      "loss": 0.0023,
      "step": 45950
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.08823253959417343,
      "learning_rate": 4.0425e-05,
      "loss": 0.0019,
      "step": 45960
    },
    {
      "epoch": 1.5323333333333333,
      "grad_norm": 0.32222896814346313,
      "learning_rate": 4.0422916666666666e-05,
      "loss": 0.0027,
      "step": 45970
    },
    {
      "epoch": 1.5326666666666666,
      "grad_norm": 0.8386664986610413,
      "learning_rate": 4.042083333333334e-05,
      "loss": 0.0027,
      "step": 45980
    },
    {
      "epoch": 1.533,
      "grad_norm": 0.23472826182842255,
      "learning_rate": 4.0418749999999997e-05,
      "loss": 0.0024,
      "step": 45990
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.513879120349884,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.0032,
      "step": 46000
    },
    {
      "epoch": 1.5336666666666665,
      "grad_norm": 0.2340981811285019,
      "learning_rate": 4.0414583333333334e-05,
      "loss": 0.0023,
      "step": 46010
    },
    {
      "epoch": 1.534,
      "grad_norm": 0.41009509563446045,
      "learning_rate": 4.0412500000000007e-05,
      "loss": 0.0021,
      "step": 46020
    },
    {
      "epoch": 1.5343333333333333,
      "grad_norm": 0.14681117236614227,
      "learning_rate": 4.0410416666666665e-05,
      "loss": 0.0033,
      "step": 46030
    },
    {
      "epoch": 1.5346666666666666,
      "grad_norm": 0.17581351101398468,
      "learning_rate": 4.040833333333334e-05,
      "loss": 0.0015,
      "step": 46040
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 0.23435986042022705,
      "learning_rate": 4.040625e-05,
      "loss": 0.0024,
      "step": 46050
    },
    {
      "epoch": 1.5353333333333334,
      "grad_norm": 0.147018164396286,
      "learning_rate": 4.040416666666667e-05,
      "loss": 0.0026,
      "step": 46060
    },
    {
      "epoch": 1.5356666666666667,
      "grad_norm": 0.2051326334476471,
      "learning_rate": 4.0402083333333334e-05,
      "loss": 0.0025,
      "step": 46070
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.16080889105796814,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.003,
      "step": 46080
    },
    {
      "epoch": 1.5363333333333333,
      "grad_norm": 0.05891581252217293,
      "learning_rate": 4.039791666666667e-05,
      "loss": 0.0023,
      "step": 46090
    },
    {
      "epoch": 1.5366666666666666,
      "grad_norm": 0.17634262144565582,
      "learning_rate": 4.039583333333333e-05,
      "loss": 0.0022,
      "step": 46100
    },
    {
      "epoch": 1.537,
      "grad_norm": 0.20545467734336853,
      "learning_rate": 4.039375e-05,
      "loss": 0.0039,
      "step": 46110
    },
    {
      "epoch": 1.5373333333333332,
      "grad_norm": 0.4777439534664154,
      "learning_rate": 4.039166666666667e-05,
      "loss": 0.0028,
      "step": 46120
    },
    {
      "epoch": 1.5376666666666665,
      "grad_norm": 0.1741868406534195,
      "learning_rate": 4.0389583333333334e-05,
      "loss": 0.0021,
      "step": 46130
    },
    {
      "epoch": 1.538,
      "grad_norm": 0.11747398972511292,
      "learning_rate": 4.03875e-05,
      "loss": 0.0012,
      "step": 46140
    },
    {
      "epoch": 1.5383333333333333,
      "grad_norm": 0.03033297508955002,
      "learning_rate": 4.038541666666667e-05,
      "loss": 0.0017,
      "step": 46150
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.3814716339111328,
      "learning_rate": 4.038333333333334e-05,
      "loss": 0.0018,
      "step": 46160
    },
    {
      "epoch": 1.5390000000000001,
      "grad_norm": 0.23452293872833252,
      "learning_rate": 4.038125e-05,
      "loss": 0.0023,
      "step": 46170
    },
    {
      "epoch": 1.5393333333333334,
      "grad_norm": 0.23476308584213257,
      "learning_rate": 4.037916666666667e-05,
      "loss": 0.0017,
      "step": 46180
    },
    {
      "epoch": 1.5396666666666667,
      "grad_norm": 0.35192084312438965,
      "learning_rate": 4.037708333333334e-05,
      "loss": 0.0024,
      "step": 46190
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.19768421351909637,
      "learning_rate": 4.0375e-05,
      "loss": 0.0015,
      "step": 46200
    },
    {
      "epoch": 1.5403333333333333,
      "grad_norm": 0.23507770895957947,
      "learning_rate": 4.0372916666666664e-05,
      "loss": 0.0027,
      "step": 46210
    },
    {
      "epoch": 1.5406666666666666,
      "grad_norm": 0.14627139270305634,
      "learning_rate": 4.0370833333333337e-05,
      "loss": 0.0021,
      "step": 46220
    },
    {
      "epoch": 1.541,
      "grad_norm": 0.32207757234573364,
      "learning_rate": 4.036875e-05,
      "loss": 0.0024,
      "step": 46230
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.49822506308555603,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.003,
      "step": 46240
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.059011608362197876,
      "learning_rate": 4.036458333333333e-05,
      "loss": 0.002,
      "step": 46250
    },
    {
      "epoch": 1.542,
      "grad_norm": 0.11754842102527618,
      "learning_rate": 4.0362500000000005e-05,
      "loss": 0.0018,
      "step": 46260
    },
    {
      "epoch": 1.5423333333333333,
      "grad_norm": 0.05882343277335167,
      "learning_rate": 4.0360416666666664e-05,
      "loss": 0.003,
      "step": 46270
    },
    {
      "epoch": 1.5426666666666666,
      "grad_norm": 0.7614306211471558,
      "learning_rate": 4.0358333333333336e-05,
      "loss": 0.0022,
      "step": 46280
    },
    {
      "epoch": 1.5430000000000001,
      "grad_norm": 0.46907562017440796,
      "learning_rate": 4.035625e-05,
      "loss": 0.0029,
      "step": 46290
    },
    {
      "epoch": 1.5433333333333334,
      "grad_norm": 0.1755673885345459,
      "learning_rate": 4.0354166666666674e-05,
      "loss": 0.002,
      "step": 46300
    },
    {
      "epoch": 1.5436666666666667,
      "grad_norm": 0.030300015583634377,
      "learning_rate": 4.035208333333333e-05,
      "loss": 0.0028,
      "step": 46310
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.35151106119155884,
      "learning_rate": 4.0350000000000005e-05,
      "loss": 0.0021,
      "step": 46320
    },
    {
      "epoch": 1.5443333333333333,
      "grad_norm": 0.23473350703716278,
      "learning_rate": 4.034791666666667e-05,
      "loss": 0.0031,
      "step": 46330
    },
    {
      "epoch": 1.5446666666666666,
      "grad_norm": 0.5276919603347778,
      "learning_rate": 4.0345833333333336e-05,
      "loss": 0.0022,
      "step": 46340
    },
    {
      "epoch": 1.545,
      "grad_norm": 0.33059731125831604,
      "learning_rate": 4.034375e-05,
      "loss": 0.0037,
      "step": 46350
    },
    {
      "epoch": 1.5453333333333332,
      "grad_norm": 0.05869135633111,
      "learning_rate": 4.034166666666667e-05,
      "loss": 0.0027,
      "step": 46360
    },
    {
      "epoch": 1.5456666666666665,
      "grad_norm": 0.3515443205833435,
      "learning_rate": 4.033958333333334e-05,
      "loss": 0.0025,
      "step": 46370
    },
    {
      "epoch": 1.546,
      "grad_norm": 0.1757829785346985,
      "learning_rate": 4.03375e-05,
      "loss": 0.0028,
      "step": 46380
    },
    {
      "epoch": 1.5463333333333333,
      "grad_norm": 0.38093289732933044,
      "learning_rate": 4.033541666666667e-05,
      "loss": 0.0031,
      "step": 46390
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.4623706340789795,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0022,
      "step": 46400
    },
    {
      "epoch": 1.5470000000000002,
      "grad_norm": 0.23388969898223877,
      "learning_rate": 4.033125e-05,
      "loss": 0.0023,
      "step": 46410
    },
    {
      "epoch": 1.5473333333333334,
      "grad_norm": 0.2928919494152069,
      "learning_rate": 4.032916666666667e-05,
      "loss": 0.0033,
      "step": 46420
    },
    {
      "epoch": 1.5476666666666667,
      "grad_norm": 0.058666180819272995,
      "learning_rate": 4.032708333333334e-05,
      "loss": 0.0032,
      "step": 46430
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.08778686821460724,
      "learning_rate": 4.0325000000000004e-05,
      "loss": 0.0019,
      "step": 46440
    },
    {
      "epoch": 1.5483333333333333,
      "grad_norm": 0.058961592614650726,
      "learning_rate": 4.032291666666667e-05,
      "loss": 0.0021,
      "step": 46450
    },
    {
      "epoch": 1.5486666666666666,
      "grad_norm": 0.4100181758403778,
      "learning_rate": 4.0320833333333335e-05,
      "loss": 0.0032,
      "step": 46460
    },
    {
      "epoch": 1.549,
      "grad_norm": 0.17579735815525055,
      "learning_rate": 4.031875e-05,
      "loss": 0.0021,
      "step": 46470
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.11711885780096054,
      "learning_rate": 4.0316666666666666e-05,
      "loss": 0.0022,
      "step": 46480
    },
    {
      "epoch": 1.5496666666666665,
      "grad_norm": 0.05869169533252716,
      "learning_rate": 4.031458333333333e-05,
      "loss": 0.0021,
      "step": 46490
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.3805871307849884,
      "learning_rate": 4.0312500000000004e-05,
      "loss": 0.002,
      "step": 46500
    },
    {
      "epoch": 1.5503333333333333,
      "grad_norm": 0.3223940134048462,
      "learning_rate": 4.031041666666667e-05,
      "loss": 0.0029,
      "step": 46510
    },
    {
      "epoch": 1.5506666666666666,
      "grad_norm": 0.14675748348236084,
      "learning_rate": 4.0308333333333335e-05,
      "loss": 0.002,
      "step": 46520
    },
    {
      "epoch": 1.5510000000000002,
      "grad_norm": 0.01636440120637417,
      "learning_rate": 4.030625e-05,
      "loss": 0.0032,
      "step": 46530
    },
    {
      "epoch": 1.5513333333333335,
      "grad_norm": 0.8541539311408997,
      "learning_rate": 4.030416666666667e-05,
      "loss": 0.0054,
      "step": 46540
    },
    {
      "epoch": 1.5516666666666667,
      "grad_norm": 0.3521119952201843,
      "learning_rate": 4.030208333333333e-05,
      "loss": 0.0031,
      "step": 46550
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.11814768612384796,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0024,
      "step": 46560
    },
    {
      "epoch": 1.5523333333333333,
      "grad_norm": 0.3530747890472412,
      "learning_rate": 4.029791666666667e-05,
      "loss": 0.0028,
      "step": 46570
    },
    {
      "epoch": 1.5526666666666666,
      "grad_norm": 0.23462709784507751,
      "learning_rate": 4.0295833333333335e-05,
      "loss": 0.0025,
      "step": 46580
    },
    {
      "epoch": 1.553,
      "grad_norm": 0.029774874448776245,
      "learning_rate": 4.029375e-05,
      "loss": 0.0021,
      "step": 46590
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.5268962979316711,
      "learning_rate": 4.0291666666666666e-05,
      "loss": 0.0012,
      "step": 46600
    },
    {
      "epoch": 1.5536666666666665,
      "grad_norm": 0.38079169392585754,
      "learning_rate": 4.028958333333334e-05,
      "loss": 0.0026,
      "step": 46610
    },
    {
      "epoch": 1.554,
      "grad_norm": 0.08775664865970612,
      "learning_rate": 4.0287500000000003e-05,
      "loss": 0.0027,
      "step": 46620
    },
    {
      "epoch": 1.5543333333333333,
      "grad_norm": 0.029819220304489136,
      "learning_rate": 4.028541666666667e-05,
      "loss": 0.0018,
      "step": 46630
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.11726189404726028,
      "learning_rate": 4.0283333333333334e-05,
      "loss": 0.0028,
      "step": 46640
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 0.0045858826488256454,
      "learning_rate": 4.028125000000001e-05,
      "loss": 0.0032,
      "step": 46650
    },
    {
      "epoch": 1.5553333333333335,
      "grad_norm": 0.029697781428694725,
      "learning_rate": 4.0279166666666665e-05,
      "loss": 0.0029,
      "step": 46660
    },
    {
      "epoch": 1.5556666666666668,
      "grad_norm": 0.17569105327129364,
      "learning_rate": 4.027708333333334e-05,
      "loss": 0.0031,
      "step": 46670
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.29290303587913513,
      "learning_rate": 4.0275e-05,
      "loss": 0.0025,
      "step": 46680
    },
    {
      "epoch": 1.5563333333333333,
      "grad_norm": 0.10074929147958755,
      "learning_rate": 4.027291666666667e-05,
      "loss": 0.0028,
      "step": 46690
    },
    {
      "epoch": 1.5566666666666666,
      "grad_norm": 0.3930846154689789,
      "learning_rate": 4.0270833333333334e-05,
      "loss": 0.003,
      "step": 46700
    },
    {
      "epoch": 1.557,
      "grad_norm": 0.029309580102562904,
      "learning_rate": 4.026875e-05,
      "loss": 0.0026,
      "step": 46710
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.11745709925889969,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0011,
      "step": 46720
    },
    {
      "epoch": 1.5576666666666665,
      "grad_norm": 0.08791400492191315,
      "learning_rate": 4.026458333333333e-05,
      "loss": 0.0019,
      "step": 46730
    },
    {
      "epoch": 1.558,
      "grad_norm": 0.08786636590957642,
      "learning_rate": 4.02625e-05,
      "loss": 0.0017,
      "step": 46740
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 0.029520928859710693,
      "learning_rate": 4.026041666666667e-05,
      "loss": 0.0034,
      "step": 46750
    },
    {
      "epoch": 1.5586666666666666,
      "grad_norm": 0.006506078876554966,
      "learning_rate": 4.0258333333333334e-05,
      "loss": 0.0029,
      "step": 46760
    },
    {
      "epoch": 1.5590000000000002,
      "grad_norm": 0.3803226351737976,
      "learning_rate": 4.025625e-05,
      "loss": 0.0037,
      "step": 46770
    },
    {
      "epoch": 1.5593333333333335,
      "grad_norm": 0.14645592868328094,
      "learning_rate": 4.025416666666667e-05,
      "loss": 0.0024,
      "step": 46780
    },
    {
      "epoch": 1.5596666666666668,
      "grad_norm": 0.11711318790912628,
      "learning_rate": 4.025208333333334e-05,
      "loss": 0.003,
      "step": 46790
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.26337099075317383,
      "learning_rate": 4.025e-05,
      "loss": 0.0021,
      "step": 46800
    },
    {
      "epoch": 1.5603333333333333,
      "grad_norm": 0.05869191512465477,
      "learning_rate": 4.024791666666667e-05,
      "loss": 0.0034,
      "step": 46810
    },
    {
      "epoch": 1.5606666666666666,
      "grad_norm": 0.08835595101118088,
      "learning_rate": 4.0245833333333334e-05,
      "loss": 0.0028,
      "step": 46820
    },
    {
      "epoch": 1.561,
      "grad_norm": 0.14616766571998596,
      "learning_rate": 4.024375e-05,
      "loss": 0.0018,
      "step": 46830
    },
    {
      "epoch": 1.5613333333333332,
      "grad_norm": 0.08756933361291885,
      "learning_rate": 4.0241666666666665e-05,
      "loss": 0.0022,
      "step": 46840
    },
    {
      "epoch": 1.5616666666666665,
      "grad_norm": 0.17551951110363007,
      "learning_rate": 4.023958333333334e-05,
      "loss": 0.0019,
      "step": 46850
    },
    {
      "epoch": 1.562,
      "grad_norm": 0.08792398124933243,
      "learning_rate": 4.02375e-05,
      "loss": 0.0023,
      "step": 46860
    },
    {
      "epoch": 1.5623333333333334,
      "grad_norm": 0.11706070601940155,
      "learning_rate": 4.023541666666667e-05,
      "loss": 0.0031,
      "step": 46870
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.29284974932670593,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.0029,
      "step": 46880
    },
    {
      "epoch": 1.563,
      "grad_norm": 0.14621645212173462,
      "learning_rate": 4.0231250000000006e-05,
      "loss": 0.0039,
      "step": 46890
    },
    {
      "epoch": 1.5633333333333335,
      "grad_norm": 0.058778584003448486,
      "learning_rate": 4.022916666666667e-05,
      "loss": 0.0023,
      "step": 46900
    },
    {
      "epoch": 1.5636666666666668,
      "grad_norm": 0.05859598144888878,
      "learning_rate": 4.0227083333333336e-05,
      "loss": 0.0026,
      "step": 46910
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.46933579444885254,
      "learning_rate": 4.0225e-05,
      "loss": 0.0019,
      "step": 46920
    },
    {
      "epoch": 1.5643333333333334,
      "grad_norm": 0.5522056818008423,
      "learning_rate": 4.0222916666666674e-05,
      "loss": 0.0018,
      "step": 46930
    },
    {
      "epoch": 1.5646666666666667,
      "grad_norm": 0.058995120227336884,
      "learning_rate": 4.022083333333333e-05,
      "loss": 0.0028,
      "step": 46940
    },
    {
      "epoch": 1.565,
      "grad_norm": 0.29255539178848267,
      "learning_rate": 4.021875e-05,
      "loss": 0.002,
      "step": 46950
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.3217781186103821,
      "learning_rate": 4.021666666666667e-05,
      "loss": 0.0023,
      "step": 46960
    },
    {
      "epoch": 1.5656666666666665,
      "grad_norm": 0.4387248456478119,
      "learning_rate": 4.0214583333333336e-05,
      "loss": 0.0022,
      "step": 46970
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 0.2926538288593292,
      "learning_rate": 4.02125e-05,
      "loss": 0.0022,
      "step": 46980
    },
    {
      "epoch": 1.5663333333333334,
      "grad_norm": 0.17552687227725983,
      "learning_rate": 4.021041666666667e-05,
      "loss": 0.0025,
      "step": 46990
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.02954385243356228,
      "learning_rate": 4.020833333333334e-05,
      "loss": 0.0026,
      "step": 47000
    },
    {
      "epoch": 1.567,
      "grad_norm": 0.11709694564342499,
      "learning_rate": 4.020625e-05,
      "loss": 0.0026,
      "step": 47010
    },
    {
      "epoch": 1.5673333333333335,
      "grad_norm": 0.05877106264233589,
      "learning_rate": 4.020416666666667e-05,
      "loss": 0.0031,
      "step": 47020
    },
    {
      "epoch": 1.5676666666666668,
      "grad_norm": 0.22937527298927307,
      "learning_rate": 4.0202083333333336e-05,
      "loss": 0.0026,
      "step": 47030
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.14629490673542023,
      "learning_rate": 4.02e-05,
      "loss": 0.0026,
      "step": 47040
    },
    {
      "epoch": 1.5683333333333334,
      "grad_norm": 0.2342602014541626,
      "learning_rate": 4.019791666666667e-05,
      "loss": 0.0027,
      "step": 47050
    },
    {
      "epoch": 1.5686666666666667,
      "grad_norm": 0.7926318049430847,
      "learning_rate": 4.019583333333333e-05,
      "loss": 0.0021,
      "step": 47060
    },
    {
      "epoch": 1.569,
      "grad_norm": 0.2785630226135254,
      "learning_rate": 4.0193750000000005e-05,
      "loss": 0.0023,
      "step": 47070
    },
    {
      "epoch": 1.5693333333333332,
      "grad_norm": 0.3804309070110321,
      "learning_rate": 4.019166666666666e-05,
      "loss": 0.0026,
      "step": 47080
    },
    {
      "epoch": 1.5696666666666665,
      "grad_norm": 0.1757775992155075,
      "learning_rate": 4.0189583333333336e-05,
      "loss": 0.0021,
      "step": 47090
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.11715846508741379,
      "learning_rate": 4.01875e-05,
      "loss": 0.0038,
      "step": 47100
    },
    {
      "epoch": 1.5703333333333334,
      "grad_norm": 0.20485350489616394,
      "learning_rate": 4.0185416666666667e-05,
      "loss": 0.0027,
      "step": 47110
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.35262081027030945,
      "learning_rate": 4.018333333333333e-05,
      "loss": 0.0035,
      "step": 47120
    },
    {
      "epoch": 1.571,
      "grad_norm": 0.05876634642481804,
      "learning_rate": 4.0181250000000004e-05,
      "loss": 0.0024,
      "step": 47130
    },
    {
      "epoch": 1.5713333333333335,
      "grad_norm": 0.1462680846452713,
      "learning_rate": 4.017916666666667e-05,
      "loss": 0.0031,
      "step": 47140
    },
    {
      "epoch": 1.5716666666666668,
      "grad_norm": 0.4390948414802551,
      "learning_rate": 4.0177083333333335e-05,
      "loss": 0.0026,
      "step": 47150
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.23422566056251526,
      "learning_rate": 4.0175e-05,
      "loss": 0.0022,
      "step": 47160
    },
    {
      "epoch": 1.5723333333333334,
      "grad_norm": 0.141977459192276,
      "learning_rate": 4.017291666666667e-05,
      "loss": 0.0026,
      "step": 47170
    },
    {
      "epoch": 1.5726666666666667,
      "grad_norm": 0.05918607860803604,
      "learning_rate": 4.017083333333334e-05,
      "loss": 0.003,
      "step": 47180
    },
    {
      "epoch": 1.573,
      "grad_norm": 0.43921342492103577,
      "learning_rate": 4.016875e-05,
      "loss": 0.0022,
      "step": 47190
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.5517588257789612,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0033,
      "step": 47200
    },
    {
      "epoch": 1.5736666666666665,
      "grad_norm": 0.23306913673877716,
      "learning_rate": 4.0164583333333335e-05,
      "loss": 0.0025,
      "step": 47210
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 0.4390636384487152,
      "learning_rate": 4.01625e-05,
      "loss": 0.0026,
      "step": 47220
    },
    {
      "epoch": 1.5743333333333334,
      "grad_norm": 0.08817683160305023,
      "learning_rate": 4.0160416666666666e-05,
      "loss": 0.0024,
      "step": 47230
    },
    {
      "epoch": 1.5746666666666667,
      "grad_norm": 0.0383426696062088,
      "learning_rate": 4.015833333333334e-05,
      "loss": 0.0028,
      "step": 47240
    },
    {
      "epoch": 1.575,
      "grad_norm": 0.11712197959423065,
      "learning_rate": 4.0156250000000004e-05,
      "loss": 0.0025,
      "step": 47250
    },
    {
      "epoch": 1.5753333333333335,
      "grad_norm": 0.2930407226085663,
      "learning_rate": 4.015416666666667e-05,
      "loss": 0.0023,
      "step": 47260
    },
    {
      "epoch": 1.5756666666666668,
      "grad_norm": 0.7319143414497375,
      "learning_rate": 4.0152083333333335e-05,
      "loss": 0.0024,
      "step": 47270
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.23438462615013123,
      "learning_rate": 4.015000000000001e-05,
      "loss": 0.0015,
      "step": 47280
    },
    {
      "epoch": 1.5763333333333334,
      "grad_norm": 0.3223581314086914,
      "learning_rate": 4.0147916666666666e-05,
      "loss": 0.0022,
      "step": 47290
    },
    {
      "epoch": 1.5766666666666667,
      "grad_norm": 0.26363420486450195,
      "learning_rate": 4.014583333333333e-05,
      "loss": 0.0029,
      "step": 47300
    },
    {
      "epoch": 1.577,
      "grad_norm": 0.23394817113876343,
      "learning_rate": 4.014375e-05,
      "loss": 0.0026,
      "step": 47310
    },
    {
      "epoch": 1.5773333333333333,
      "grad_norm": 0.17560388147830963,
      "learning_rate": 4.014166666666667e-05,
      "loss": 0.0023,
      "step": 47320
    },
    {
      "epoch": 1.5776666666666666,
      "grad_norm": 0.6366887092590332,
      "learning_rate": 4.0139583333333334e-05,
      "loss": 0.0021,
      "step": 47330
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 0.030408095568418503,
      "learning_rate": 4.01375e-05,
      "loss": 0.0022,
      "step": 47340
    },
    {
      "epoch": 1.5783333333333334,
      "grad_norm": 0.0056863585487008095,
      "learning_rate": 4.013541666666667e-05,
      "loss": 0.0026,
      "step": 47350
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.5271551012992859,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0026,
      "step": 47360
    },
    {
      "epoch": 1.579,
      "grad_norm": 0.14647787809371948,
      "learning_rate": 4.013125e-05,
      "loss": 0.0026,
      "step": 47370
    },
    {
      "epoch": 1.5793333333333335,
      "grad_norm": 0.05971531569957733,
      "learning_rate": 4.012916666666667e-05,
      "loss": 0.0022,
      "step": 47380
    },
    {
      "epoch": 1.5796666666666668,
      "grad_norm": 0.38073527812957764,
      "learning_rate": 4.0127083333333334e-05,
      "loss": 0.0022,
      "step": 47390
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.43922075629234314,
      "learning_rate": 4.0125e-05,
      "loss": 0.0034,
      "step": 47400
    },
    {
      "epoch": 1.5803333333333334,
      "grad_norm": 0.11667322367429733,
      "learning_rate": 4.012291666666667e-05,
      "loss": 0.0026,
      "step": 47410
    },
    {
      "epoch": 1.5806666666666667,
      "grad_norm": 0.08809099346399307,
      "learning_rate": 4.012083333333334e-05,
      "loss": 0.003,
      "step": 47420
    },
    {
      "epoch": 1.581,
      "grad_norm": 1.3340615034103394,
      "learning_rate": 4.011875e-05,
      "loss": 0.003,
      "step": 47430
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.029581483453512192,
      "learning_rate": 4.011666666666667e-05,
      "loss": 0.0027,
      "step": 47440
    },
    {
      "epoch": 1.5816666666666666,
      "grad_norm": 0.1463823914527893,
      "learning_rate": 4.0114583333333334e-05,
      "loss": 0.0025,
      "step": 47450
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 0.059035420417785645,
      "learning_rate": 4.0112500000000006e-05,
      "loss": 0.0031,
      "step": 47460
    },
    {
      "epoch": 1.5823333333333334,
      "grad_norm": 0.05615706741809845,
      "learning_rate": 4.0110416666666665e-05,
      "loss": 0.0028,
      "step": 47470
    },
    {
      "epoch": 1.5826666666666667,
      "grad_norm": 0.3027270436286926,
      "learning_rate": 4.010833333333334e-05,
      "loss": 0.0019,
      "step": 47480
    },
    {
      "epoch": 1.583,
      "grad_norm": 0.05881037563085556,
      "learning_rate": 4.010625e-05,
      "loss": 0.0033,
      "step": 47490
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.29279226064682007,
      "learning_rate": 4.010416666666667e-05,
      "loss": 0.0026,
      "step": 47500
    },
    {
      "epoch": 1.5836666666666668,
      "grad_norm": 0.20520062744617462,
      "learning_rate": 4.0102083333333333e-05,
      "loss": 0.0036,
      "step": 47510
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.11705180257558823,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.0017,
      "step": 47520
    },
    {
      "epoch": 1.5843333333333334,
      "grad_norm": 0.05916371941566467,
      "learning_rate": 4.009791666666667e-05,
      "loss": 0.0014,
      "step": 47530
    },
    {
      "epoch": 1.5846666666666667,
      "grad_norm": 0.1467454880475998,
      "learning_rate": 4.009583333333334e-05,
      "loss": 0.0018,
      "step": 47540
    },
    {
      "epoch": 1.585,
      "grad_norm": 0.11717930436134338,
      "learning_rate": 4.009375e-05,
      "loss": 0.0023,
      "step": 47550
    },
    {
      "epoch": 1.5853333333333333,
      "grad_norm": 0.03371988609433174,
      "learning_rate": 4.009166666666667e-05,
      "loss": 0.0017,
      "step": 47560
    },
    {
      "epoch": 1.5856666666666666,
      "grad_norm": 0.0594022311270237,
      "learning_rate": 4.008958333333333e-05,
      "loss": 0.0026,
      "step": 47570
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 0.27391517162323,
      "learning_rate": 4.00875e-05,
      "loss": 0.003,
      "step": 47580
    },
    {
      "epoch": 1.5863333333333334,
      "grad_norm": 0.2425951212644577,
      "learning_rate": 4.008541666666667e-05,
      "loss": 0.0022,
      "step": 47590
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.11757700145244598,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.0027,
      "step": 47600
    },
    {
      "epoch": 1.587,
      "grad_norm": 0.2931200861930847,
      "learning_rate": 4.008125e-05,
      "loss": 0.0018,
      "step": 47610
    },
    {
      "epoch": 1.5873333333333335,
      "grad_norm": 0.08809968084096909,
      "learning_rate": 4.007916666666667e-05,
      "loss": 0.0024,
      "step": 47620
    },
    {
      "epoch": 1.5876666666666668,
      "grad_norm": 0.058701399713754654,
      "learning_rate": 4.007708333333334e-05,
      "loss": 0.0021,
      "step": 47630
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.17588095366954803,
      "learning_rate": 4.0075e-05,
      "loss": 0.0017,
      "step": 47640
    },
    {
      "epoch": 1.5883333333333334,
      "grad_norm": 0.32263797521591187,
      "learning_rate": 4.007291666666667e-05,
      "loss": 0.002,
      "step": 47650
    },
    {
      "epoch": 1.5886666666666667,
      "grad_norm": 0.3809097707271576,
      "learning_rate": 4.0070833333333336e-05,
      "loss": 0.0028,
      "step": 47660
    },
    {
      "epoch": 1.589,
      "grad_norm": 0.11739473044872284,
      "learning_rate": 4.006875e-05,
      "loss": 0.0029,
      "step": 47670
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.035865623503923416,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0023,
      "step": 47680
    },
    {
      "epoch": 1.5896666666666666,
      "grad_norm": 0.5564683079719543,
      "learning_rate": 4.006458333333333e-05,
      "loss": 0.0023,
      "step": 47690
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.11729826033115387,
      "learning_rate": 4.0062500000000005e-05,
      "loss": 0.0025,
      "step": 47700
    },
    {
      "epoch": 1.5903333333333334,
      "grad_norm": 0.059291958808898926,
      "learning_rate": 4.006041666666667e-05,
      "loss": 0.0023,
      "step": 47710
    },
    {
      "epoch": 1.5906666666666667,
      "grad_norm": 0.4100031554698944,
      "learning_rate": 4.0058333333333336e-05,
      "loss": 0.0031,
      "step": 47720
    },
    {
      "epoch": 1.591,
      "grad_norm": 0.0879824236035347,
      "learning_rate": 4.005625e-05,
      "loss": 0.0031,
      "step": 47730
    },
    {
      "epoch": 1.5913333333333335,
      "grad_norm": 0.3091707229614258,
      "learning_rate": 4.0054166666666674e-05,
      "loss": 0.0015,
      "step": 47740
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.11746886372566223,
      "learning_rate": 4.005208333333333e-05,
      "loss": 0.0021,
      "step": 47750
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.14648310840129852,
      "learning_rate": 4.0050000000000004e-05,
      "loss": 0.0019,
      "step": 47760
    },
    {
      "epoch": 1.5923333333333334,
      "grad_norm": 0.05861740931868553,
      "learning_rate": 4.004791666666667e-05,
      "loss": 0.0045,
      "step": 47770
    },
    {
      "epoch": 1.5926666666666667,
      "grad_norm": 0.1172521635890007,
      "learning_rate": 4.0045833333333335e-05,
      "loss": 0.003,
      "step": 47780
    },
    {
      "epoch": 1.593,
      "grad_norm": 0.08814924210309982,
      "learning_rate": 4.004375e-05,
      "loss": 0.0022,
      "step": 47790
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.0878153145313263,
      "learning_rate": 4.0041666666666666e-05,
      "loss": 0.0031,
      "step": 47800
    },
    {
      "epoch": 1.5936666666666666,
      "grad_norm": 0.2051350623369217,
      "learning_rate": 4.003958333333334e-05,
      "loss": 0.0028,
      "step": 47810
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 0.1470053642988205,
      "learning_rate": 4.00375e-05,
      "loss": 0.0022,
      "step": 47820
    },
    {
      "epoch": 1.5943333333333334,
      "grad_norm": 0.26338472962379456,
      "learning_rate": 4.003541666666667e-05,
      "loss": 0.0028,
      "step": 47830
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.029218679293990135,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.002,
      "step": 47840
    },
    {
      "epoch": 1.595,
      "grad_norm": 0.18978045880794525,
      "learning_rate": 4.003125e-05,
      "loss": 0.0033,
      "step": 47850
    },
    {
      "epoch": 1.5953333333333335,
      "grad_norm": 0.14656449854373932,
      "learning_rate": 4.0029166666666666e-05,
      "loss": 0.0031,
      "step": 47860
    },
    {
      "epoch": 1.5956666666666668,
      "grad_norm": 0.18118426203727722,
      "learning_rate": 4.002708333333334e-05,
      "loss": 0.0033,
      "step": 47870
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.26365843415260315,
      "learning_rate": 4.0025000000000004e-05,
      "loss": 0.0028,
      "step": 47880
    },
    {
      "epoch": 1.5963333333333334,
      "grad_norm": 0.20511627197265625,
      "learning_rate": 4.002291666666667e-05,
      "loss": 0.0022,
      "step": 47890
    },
    {
      "epoch": 1.5966666666666667,
      "grad_norm": 0.0300438329577446,
      "learning_rate": 4.0020833333333335e-05,
      "loss": 0.0024,
      "step": 47900
    },
    {
      "epoch": 1.597,
      "grad_norm": 0.029822928830981255,
      "learning_rate": 4.001875e-05,
      "loss": 0.0016,
      "step": 47910
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.46534526348114014,
      "learning_rate": 4.0016666666666666e-05,
      "loss": 0.0037,
      "step": 47920
    },
    {
      "epoch": 1.5976666666666666,
      "grad_norm": 0.1467025727033615,
      "learning_rate": 4.001458333333333e-05,
      "loss": 0.0031,
      "step": 47930
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 0.3509965240955353,
      "learning_rate": 4.0012500000000004e-05,
      "loss": 0.0027,
      "step": 47940
    },
    {
      "epoch": 1.5983333333333334,
      "grad_norm": 0.020449472591280937,
      "learning_rate": 4.001041666666667e-05,
      "loss": 0.0026,
      "step": 47950
    },
    {
      "epoch": 1.5986666666666667,
      "grad_norm": 0.38047730922698975,
      "learning_rate": 4.0008333333333335e-05,
      "loss": 0.0024,
      "step": 47960
    },
    {
      "epoch": 1.599,
      "grad_norm": 0.26314935088157654,
      "learning_rate": 4.000625e-05,
      "loss": 0.0026,
      "step": 47970
    },
    {
      "epoch": 1.5993333333333335,
      "grad_norm": 0.1756058931350708,
      "learning_rate": 4.000416666666667e-05,
      "loss": 0.003,
      "step": 47980
    },
    {
      "epoch": 1.5996666666666668,
      "grad_norm": 0.14666405320167542,
      "learning_rate": 4.000208333333334e-05,
      "loss": 0.0027,
      "step": 47990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.5269949436187744,
      "learning_rate": 4e-05,
      "loss": 0.0022,
      "step": 48000
    },
    {
      "epoch": 1.6003333333333334,
      "grad_norm": 0.05866251885890961,
      "learning_rate": 3.999791666666667e-05,
      "loss": 0.0026,
      "step": 48010
    },
    {
      "epoch": 1.6006666666666667,
      "grad_norm": 0.02965416945517063,
      "learning_rate": 3.999583333333334e-05,
      "loss": 0.0021,
      "step": 48020
    },
    {
      "epoch": 1.601,
      "grad_norm": 0.249200701713562,
      "learning_rate": 3.999375e-05,
      "loss": 0.0023,
      "step": 48030
    },
    {
      "epoch": 1.6013333333333333,
      "grad_norm": 0.4099564254283905,
      "learning_rate": 3.9991666666666665e-05,
      "loss": 0.0025,
      "step": 48040
    },
    {
      "epoch": 1.6016666666666666,
      "grad_norm": 0.20496706664562225,
      "learning_rate": 3.998958333333334e-05,
      "loss": 0.0026,
      "step": 48050
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 0.08797632157802582,
      "learning_rate": 3.99875e-05,
      "loss": 0.0022,
      "step": 48060
    },
    {
      "epoch": 1.6023333333333334,
      "grad_norm": 0.32200562953948975,
      "learning_rate": 3.998541666666667e-05,
      "loss": 0.0027,
      "step": 48070
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.1462508887052536,
      "learning_rate": 3.9983333333333334e-05,
      "loss": 0.002,
      "step": 48080
    },
    {
      "epoch": 1.603,
      "grad_norm": 0.2048509269952774,
      "learning_rate": 3.9981250000000006e-05,
      "loss": 0.0038,
      "step": 48090
    },
    {
      "epoch": 1.6033333333333335,
      "grad_norm": 0.4682844281196594,
      "learning_rate": 3.9979166666666665e-05,
      "loss": 0.0023,
      "step": 48100
    },
    {
      "epoch": 1.6036666666666668,
      "grad_norm": 0.2634182572364807,
      "learning_rate": 3.997708333333334e-05,
      "loss": 0.0022,
      "step": 48110
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.46853333711624146,
      "learning_rate": 3.9975e-05,
      "loss": 0.0025,
      "step": 48120
    },
    {
      "epoch": 1.6043333333333334,
      "grad_norm": 0.35121452808380127,
      "learning_rate": 3.997291666666667e-05,
      "loss": 0.0028,
      "step": 48130
    },
    {
      "epoch": 1.6046666666666667,
      "grad_norm": 0.3517400622367859,
      "learning_rate": 3.9970833333333334e-05,
      "loss": 0.0036,
      "step": 48140
    },
    {
      "epoch": 1.605,
      "grad_norm": 0.08812345564365387,
      "learning_rate": 3.996875e-05,
      "loss": 0.0022,
      "step": 48150
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.3806416690349579,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.0027,
      "step": 48160
    },
    {
      "epoch": 1.6056666666666666,
      "grad_norm": 0.4387719929218292,
      "learning_rate": 3.996458333333333e-05,
      "loss": 0.0034,
      "step": 48170
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 0.17555174231529236,
      "learning_rate": 3.99625e-05,
      "loss": 0.0023,
      "step": 48180
    },
    {
      "epoch": 1.6063333333333332,
      "grad_norm": 0.2045806348323822,
      "learning_rate": 3.996041666666667e-05,
      "loss": 0.0026,
      "step": 48190
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.26346734166145325,
      "learning_rate": 3.995833333333333e-05,
      "loss": 0.0019,
      "step": 48200
    },
    {
      "epoch": 1.607,
      "grad_norm": 1.424063801765442,
      "learning_rate": 3.995625e-05,
      "loss": 0.0027,
      "step": 48210
    },
    {
      "epoch": 1.6073333333333333,
      "grad_norm": 0.1081816628575325,
      "learning_rate": 3.995416666666667e-05,
      "loss": 0.0037,
      "step": 48220
    },
    {
      "epoch": 1.6076666666666668,
      "grad_norm": 0.1657591611146927,
      "learning_rate": 3.9952083333333337e-05,
      "loss": 0.0019,
      "step": 48230
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.5852915048599243,
      "learning_rate": 3.995e-05,
      "loss": 0.0025,
      "step": 48240
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.647164523601532,
      "learning_rate": 3.994791666666667e-05,
      "loss": 0.0036,
      "step": 48250
    },
    {
      "epoch": 1.6086666666666667,
      "grad_norm": 0.2049395889043808,
      "learning_rate": 3.994583333333334e-05,
      "loss": 0.0043,
      "step": 48260
    },
    {
      "epoch": 1.609,
      "grad_norm": 0.3803790509700775,
      "learning_rate": 3.9943750000000005e-05,
      "loss": 0.0033,
      "step": 48270
    },
    {
      "epoch": 1.6093333333333333,
      "grad_norm": 0.16598686575889587,
      "learning_rate": 3.9941666666666664e-05,
      "loss": 0.0032,
      "step": 48280
    },
    {
      "epoch": 1.6096666666666666,
      "grad_norm": 0.2631682753562927,
      "learning_rate": 3.9939583333333336e-05,
      "loss": 0.0019,
      "step": 48290
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.08789326250553131,
      "learning_rate": 3.99375e-05,
      "loss": 0.0028,
      "step": 48300
    },
    {
      "epoch": 1.6103333333333332,
      "grad_norm": 0.23423714935779572,
      "learning_rate": 3.993541666666667e-05,
      "loss": 0.0022,
      "step": 48310
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.7201089262962341,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0026,
      "step": 48320
    },
    {
      "epoch": 1.611,
      "grad_norm": 0.4390828311443329,
      "learning_rate": 3.9931250000000005e-05,
      "loss": 0.0036,
      "step": 48330
    },
    {
      "epoch": 1.6113333333333333,
      "grad_norm": 0.17579731345176697,
      "learning_rate": 3.992916666666667e-05,
      "loss": 0.003,
      "step": 48340
    },
    {
      "epoch": 1.6116666666666668,
      "grad_norm": 0.0310228131711483,
      "learning_rate": 3.9927083333333336e-05,
      "loss": 0.0026,
      "step": 48350
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.18774646520614624,
      "learning_rate": 3.9925e-05,
      "loss": 0.0029,
      "step": 48360
    },
    {
      "epoch": 1.6123333333333334,
      "grad_norm": 0.29255518317222595,
      "learning_rate": 3.9922916666666674e-05,
      "loss": 0.0032,
      "step": 48370
    },
    {
      "epoch": 1.6126666666666667,
      "grad_norm": 0.20506900548934937,
      "learning_rate": 3.992083333333333e-05,
      "loss": 0.0033,
      "step": 48380
    },
    {
      "epoch": 1.613,
      "grad_norm": 0.03157762065529823,
      "learning_rate": 3.991875e-05,
      "loss": 0.0027,
      "step": 48390
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.008280646987259388,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.0029,
      "step": 48400
    },
    {
      "epoch": 1.6136666666666666,
      "grad_norm": 0.29274147748947144,
      "learning_rate": 3.9914583333333336e-05,
      "loss": 0.0025,
      "step": 48410
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 0.030166424810886383,
      "learning_rate": 3.99125e-05,
      "loss": 0.0034,
      "step": 48420
    },
    {
      "epoch": 1.6143333333333332,
      "grad_norm": 0.23521709442138672,
      "learning_rate": 3.991041666666667e-05,
      "loss": 0.0031,
      "step": 48430
    },
    {
      "epoch": 1.6146666666666667,
      "grad_norm": 0.12221772223711014,
      "learning_rate": 3.990833333333334e-05,
      "loss": 0.002,
      "step": 48440
    },
    {
      "epoch": 1.615,
      "grad_norm": 0.20513729751110077,
      "learning_rate": 3.990625e-05,
      "loss": 0.0024,
      "step": 48450
    },
    {
      "epoch": 1.6153333333333333,
      "grad_norm": 0.29265296459198,
      "learning_rate": 3.990416666666667e-05,
      "loss": 0.0038,
      "step": 48460
    },
    {
      "epoch": 1.6156666666666668,
      "grad_norm": 0.1757293939590454,
      "learning_rate": 3.9902083333333335e-05,
      "loss": 0.0031,
      "step": 48470
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.40968114137649536,
      "learning_rate": 3.99e-05,
      "loss": 0.0027,
      "step": 48480
    },
    {
      "epoch": 1.6163333333333334,
      "grad_norm": 0.03033195436000824,
      "learning_rate": 3.9897916666666666e-05,
      "loss": 0.0028,
      "step": 48490
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.351331502199173,
      "learning_rate": 3.989583333333334e-05,
      "loss": 0.0038,
      "step": 48500
    },
    {
      "epoch": 1.617,
      "grad_norm": 0.088512122631073,
      "learning_rate": 3.9893750000000004e-05,
      "loss": 0.0027,
      "step": 48510
    },
    {
      "epoch": 1.6173333333333333,
      "grad_norm": 0.40970033407211304,
      "learning_rate": 3.989166666666666e-05,
      "loss": 0.0032,
      "step": 48520
    },
    {
      "epoch": 1.6176666666666666,
      "grad_norm": 0.23839285969734192,
      "learning_rate": 3.9889583333333335e-05,
      "loss": 0.0026,
      "step": 48530
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 0.1755441427230835,
      "learning_rate": 3.98875e-05,
      "loss": 0.002,
      "step": 48540
    },
    {
      "epoch": 1.6183333333333332,
      "grad_norm": 0.32147157192230225,
      "learning_rate": 3.988541666666667e-05,
      "loss": 0.003,
      "step": 48550
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.6137058138847351,
      "learning_rate": 3.988333333333333e-05,
      "loss": 0.0026,
      "step": 48560
    },
    {
      "epoch": 1.619,
      "grad_norm": 0.5262033343315125,
      "learning_rate": 3.9881250000000004e-05,
      "loss": 0.0024,
      "step": 48570
    },
    {
      "epoch": 1.6193333333333333,
      "grad_norm": 0.17539125680923462,
      "learning_rate": 3.987916666666667e-05,
      "loss": 0.0027,
      "step": 48580
    },
    {
      "epoch": 1.6196666666666668,
      "grad_norm": 0.23399510979652405,
      "learning_rate": 3.9877083333333335e-05,
      "loss": 0.0025,
      "step": 48590
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.1755116879940033,
      "learning_rate": 3.9875e-05,
      "loss": 0.0022,
      "step": 48600
    },
    {
      "epoch": 1.6203333333333334,
      "grad_norm": 0.20449134707450867,
      "learning_rate": 3.987291666666667e-05,
      "loss": 0.0017,
      "step": 48610
    },
    {
      "epoch": 1.6206666666666667,
      "grad_norm": 0.059137485921382904,
      "learning_rate": 3.987083333333334e-05,
      "loss": 0.0018,
      "step": 48620
    },
    {
      "epoch": 1.621,
      "grad_norm": 0.23405338823795319,
      "learning_rate": 3.986875e-05,
      "loss": 0.002,
      "step": 48630
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.17541836202144623,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0025,
      "step": 48640
    },
    {
      "epoch": 1.6216666666666666,
      "grad_norm": 0.20489312708377838,
      "learning_rate": 3.9864583333333334e-05,
      "loss": 0.003,
      "step": 48650
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 0.006214557681232691,
      "learning_rate": 3.98625e-05,
      "loss": 0.0022,
      "step": 48660
    },
    {
      "epoch": 1.6223333333333332,
      "grad_norm": 0.4681932330131531,
      "learning_rate": 3.9860416666666665e-05,
      "loss": 0.003,
      "step": 48670
    },
    {
      "epoch": 1.6226666666666667,
      "grad_norm": 0.5206254720687866,
      "learning_rate": 3.985833333333334e-05,
      "loss": 0.0036,
      "step": 48680
    },
    {
      "epoch": 1.623,
      "grad_norm": 0.44520333409309387,
      "learning_rate": 3.985625e-05,
      "loss": 0.0026,
      "step": 48690
    },
    {
      "epoch": 1.6233333333333333,
      "grad_norm": 0.14730699360370636,
      "learning_rate": 3.985416666666667e-05,
      "loss": 0.0022,
      "step": 48700
    },
    {
      "epoch": 1.6236666666666668,
      "grad_norm": 0.030394138768315315,
      "learning_rate": 3.9852083333333334e-05,
      "loss": 0.0025,
      "step": 48710
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.11733828485012054,
      "learning_rate": 3.9850000000000006e-05,
      "loss": 0.0023,
      "step": 48720
    },
    {
      "epoch": 1.6243333333333334,
      "grad_norm": 0.14700466394424438,
      "learning_rate": 3.9847916666666665e-05,
      "loss": 0.0017,
      "step": 48730
    },
    {
      "epoch": 1.6246666666666667,
      "grad_norm": 0.2053096443414688,
      "learning_rate": 3.984583333333334e-05,
      "loss": 0.0021,
      "step": 48740
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.26305198669433594,
      "learning_rate": 3.984375e-05,
      "loss": 0.0014,
      "step": 48750
    },
    {
      "epoch": 1.6253333333333333,
      "grad_norm": 0.05865858122706413,
      "learning_rate": 3.984166666666667e-05,
      "loss": 0.0016,
      "step": 48760
    },
    {
      "epoch": 1.6256666666666666,
      "grad_norm": 0.05982542783021927,
      "learning_rate": 3.9839583333333334e-05,
      "loss": 0.0019,
      "step": 48770
    },
    {
      "epoch": 1.626,
      "grad_norm": 0.3605499863624573,
      "learning_rate": 3.98375e-05,
      "loss": 0.0027,
      "step": 48780
    },
    {
      "epoch": 1.6263333333333332,
      "grad_norm": 0.08808067440986633,
      "learning_rate": 3.983541666666667e-05,
      "loss": 0.0024,
      "step": 48790
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.3402339518070221,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0024,
      "step": 48800
    },
    {
      "epoch": 1.627,
      "grad_norm": 0.23422688245773315,
      "learning_rate": 3.983125e-05,
      "loss": 0.0029,
      "step": 48810
    },
    {
      "epoch": 1.6273333333333333,
      "grad_norm": 0.02982284128665924,
      "learning_rate": 3.982916666666667e-05,
      "loss": 0.0024,
      "step": 48820
    },
    {
      "epoch": 1.6276666666666668,
      "grad_norm": 0.11724012345075607,
      "learning_rate": 3.982708333333334e-05,
      "loss": 0.0025,
      "step": 48830
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.496924489736557,
      "learning_rate": 3.9825e-05,
      "loss": 0.0017,
      "step": 48840
    },
    {
      "epoch": 1.6283333333333334,
      "grad_norm": 0.4386427402496338,
      "learning_rate": 3.982291666666667e-05,
      "loss": 0.0027,
      "step": 48850
    },
    {
      "epoch": 1.6286666666666667,
      "grad_norm": 0.35085198283195496,
      "learning_rate": 3.982083333333334e-05,
      "loss": 0.002,
      "step": 48860
    },
    {
      "epoch": 1.629,
      "grad_norm": 0.26298558712005615,
      "learning_rate": 3.981875e-05,
      "loss": 0.0032,
      "step": 48870
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.3802158534526825,
      "learning_rate": 3.981666666666667e-05,
      "loss": 0.0027,
      "step": 48880
    },
    {
      "epoch": 1.6296666666666666,
      "grad_norm": 0.17574138939380646,
      "learning_rate": 3.981458333333333e-05,
      "loss": 0.0027,
      "step": 48890
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.030035333707928658,
      "learning_rate": 3.9812500000000005e-05,
      "loss": 0.0019,
      "step": 48900
    },
    {
      "epoch": 1.6303333333333332,
      "grad_norm": 0.17543230950832367,
      "learning_rate": 3.9810416666666664e-05,
      "loss": 0.0016,
      "step": 48910
    },
    {
      "epoch": 1.6306666666666667,
      "grad_norm": 0.14630384743213654,
      "learning_rate": 3.9808333333333336e-05,
      "loss": 0.002,
      "step": 48920
    },
    {
      "epoch": 1.631,
      "grad_norm": 0.011951154097914696,
      "learning_rate": 3.980625e-05,
      "loss": 0.003,
      "step": 48930
    },
    {
      "epoch": 1.6313333333333333,
      "grad_norm": 0.08763740211725235,
      "learning_rate": 3.980416666666667e-05,
      "loss": 0.0012,
      "step": 48940
    },
    {
      "epoch": 1.6316666666666668,
      "grad_norm": 0.08771549165248871,
      "learning_rate": 3.980208333333333e-05,
      "loss": 0.0037,
      "step": 48950
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.2923319935798645,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.003,
      "step": 48960
    },
    {
      "epoch": 1.6323333333333334,
      "grad_norm": 0.08789460361003876,
      "learning_rate": 3.979791666666667e-05,
      "loss": 0.0031,
      "step": 48970
    },
    {
      "epoch": 1.6326666666666667,
      "grad_norm": 0.11685007810592651,
      "learning_rate": 3.9795833333333336e-05,
      "loss": 0.0025,
      "step": 48980
    },
    {
      "epoch": 1.633,
      "grad_norm": 0.3508399724960327,
      "learning_rate": 3.979375e-05,
      "loss": 0.0024,
      "step": 48990
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.20487159490585327,
      "learning_rate": 3.979166666666667e-05,
      "loss": 0.0024,
      "step": 49000
    },
    {
      "epoch": 1.6336666666666666,
      "grad_norm": 0.32610824704170227,
      "learning_rate": 3.978958333333333e-05,
      "loss": 0.0025,
      "step": 49010
    },
    {
      "epoch": 1.634,
      "grad_norm": 0.2631187438964844,
      "learning_rate": 3.97875e-05,
      "loss": 0.0025,
      "step": 49020
    },
    {
      "epoch": 1.6343333333333332,
      "grad_norm": 0.5259865522384644,
      "learning_rate": 3.978541666666667e-05,
      "loss": 0.0035,
      "step": 49030
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.058589570224285126,
      "learning_rate": 3.9783333333333336e-05,
      "loss": 0.0015,
      "step": 49040
    },
    {
      "epoch": 1.635,
      "grad_norm": 0.11787986755371094,
      "learning_rate": 3.978125e-05,
      "loss": 0.0031,
      "step": 49050
    },
    {
      "epoch": 1.6353333333333333,
      "grad_norm": 0.031499091535806656,
      "learning_rate": 3.977916666666667e-05,
      "loss": 0.0023,
      "step": 49060
    },
    {
      "epoch": 1.6356666666666668,
      "grad_norm": 0.1461450606584549,
      "learning_rate": 3.977708333333334e-05,
      "loss": 0.0018,
      "step": 49070
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.4680105149745941,
      "learning_rate": 3.9775e-05,
      "loss": 0.0022,
      "step": 49080
    },
    {
      "epoch": 1.6363333333333334,
      "grad_norm": 0.29238006472587585,
      "learning_rate": 3.977291666666667e-05,
      "loss": 0.0029,
      "step": 49090
    },
    {
      "epoch": 1.6366666666666667,
      "grad_norm": 0.1760241985321045,
      "learning_rate": 3.9770833333333336e-05,
      "loss": 0.0017,
      "step": 49100
    },
    {
      "epoch": 1.637,
      "grad_norm": 0.3214969336986542,
      "learning_rate": 3.976875000000001e-05,
      "loss": 0.0027,
      "step": 49110
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.3715091645717621,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.002,
      "step": 49120
    },
    {
      "epoch": 1.6376666666666666,
      "grad_norm": 0.2856469750404358,
      "learning_rate": 3.976458333333333e-05,
      "loss": 0.003,
      "step": 49130
    },
    {
      "epoch": 1.638,
      "grad_norm": 0.05903593450784683,
      "learning_rate": 3.9762500000000004e-05,
      "loss": 0.0022,
      "step": 49140
    },
    {
      "epoch": 1.6383333333333332,
      "grad_norm": 0.058807067573070526,
      "learning_rate": 3.976041666666667e-05,
      "loss": 0.0022,
      "step": 49150
    },
    {
      "epoch": 1.6386666666666667,
      "grad_norm": 0.11718889325857162,
      "learning_rate": 3.9758333333333335e-05,
      "loss": 0.0017,
      "step": 49160
    },
    {
      "epoch": 1.639,
      "grad_norm": 0.11745622009038925,
      "learning_rate": 3.975625e-05,
      "loss": 0.0025,
      "step": 49170
    },
    {
      "epoch": 1.6393333333333333,
      "grad_norm": 0.030420292168855667,
      "learning_rate": 3.975416666666667e-05,
      "loss": 0.0022,
      "step": 49180
    },
    {
      "epoch": 1.6396666666666668,
      "grad_norm": 0.11696294695138931,
      "learning_rate": 3.975208333333333e-05,
      "loss": 0.002,
      "step": 49190
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.005101904738694429,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.0024,
      "step": 49200
    },
    {
      "epoch": 1.6403333333333334,
      "grad_norm": 0.12183655798435211,
      "learning_rate": 3.974791666666667e-05,
      "loss": 0.0023,
      "step": 49210
    },
    {
      "epoch": 1.6406666666666667,
      "grad_norm": 0.14640364050865173,
      "learning_rate": 3.9745833333333335e-05,
      "loss": 0.002,
      "step": 49220
    },
    {
      "epoch": 1.641,
      "grad_norm": 0.059008773416280746,
      "learning_rate": 3.974375e-05,
      "loss": 0.0018,
      "step": 49230
    },
    {
      "epoch": 1.6413333333333333,
      "grad_norm": 0.08276811987161636,
      "learning_rate": 3.9741666666666666e-05,
      "loss": 0.0018,
      "step": 49240
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 0.46778443455696106,
      "learning_rate": 3.973958333333334e-05,
      "loss": 0.0016,
      "step": 49250
    },
    {
      "epoch": 1.642,
      "grad_norm": 0.2612278163433075,
      "learning_rate": 3.97375e-05,
      "loss": 0.0026,
      "step": 49260
    },
    {
      "epoch": 1.6423333333333332,
      "grad_norm": 0.20527182519435883,
      "learning_rate": 3.973541666666667e-05,
      "loss": 0.0018,
      "step": 49270
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.2933139503002167,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.003,
      "step": 49280
    },
    {
      "epoch": 1.643,
      "grad_norm": 0.14614996314048767,
      "learning_rate": 3.973125e-05,
      "loss": 0.0024,
      "step": 49290
    },
    {
      "epoch": 1.6433333333333333,
      "grad_norm": 0.11714360862970352,
      "learning_rate": 3.9729166666666666e-05,
      "loss": 0.0016,
      "step": 49300
    },
    {
      "epoch": 1.6436666666666668,
      "grad_norm": 0.20477642118930817,
      "learning_rate": 3.972708333333334e-05,
      "loss": 0.0027,
      "step": 49310
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.08795563131570816,
      "learning_rate": 3.9725e-05,
      "loss": 0.0023,
      "step": 49320
    },
    {
      "epoch": 1.6443333333333334,
      "grad_norm": 0.14609433710575104,
      "learning_rate": 3.972291666666667e-05,
      "loss": 0.0028,
      "step": 49330
    },
    {
      "epoch": 1.6446666666666667,
      "grad_norm": 0.08780977129936218,
      "learning_rate": 3.9720833333333334e-05,
      "loss": 0.0025,
      "step": 49340
    },
    {
      "epoch": 1.645,
      "grad_norm": 0.08773108571767807,
      "learning_rate": 3.9718750000000007e-05,
      "loss": 0.0024,
      "step": 49350
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.058781132102012634,
      "learning_rate": 3.9716666666666665e-05,
      "loss": 0.0029,
      "step": 49360
    },
    {
      "epoch": 1.6456666666666666,
      "grad_norm": 0.14630131423473358,
      "learning_rate": 3.971458333333333e-05,
      "loss": 0.0019,
      "step": 49370
    },
    {
      "epoch": 1.646,
      "grad_norm": 0.20471426844596863,
      "learning_rate": 3.97125e-05,
      "loss": 0.0025,
      "step": 49380
    },
    {
      "epoch": 1.6463333333333332,
      "grad_norm": 0.05880199745297432,
      "learning_rate": 3.971041666666667e-05,
      "loss": 0.0022,
      "step": 49390
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.14592429995536804,
      "learning_rate": 3.9708333333333334e-05,
      "loss": 0.0023,
      "step": 49400
    },
    {
      "epoch": 1.647,
      "grad_norm": 0.2041134238243103,
      "learning_rate": 3.970625e-05,
      "loss": 0.0021,
      "step": 49410
    },
    {
      "epoch": 1.6473333333333333,
      "grad_norm": 0.08814867585897446,
      "learning_rate": 3.970416666666667e-05,
      "loss": 0.0023,
      "step": 49420
    },
    {
      "epoch": 1.6476666666666666,
      "grad_norm": 0.11731532961130142,
      "learning_rate": 3.970208333333334e-05,
      "loss": 0.0038,
      "step": 49430
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.4973214864730835,
      "learning_rate": 3.97e-05,
      "loss": 0.0024,
      "step": 49440
    },
    {
      "epoch": 1.6483333333333334,
      "grad_norm": 0.7119408249855042,
      "learning_rate": 3.969791666666667e-05,
      "loss": 0.0026,
      "step": 49450
    },
    {
      "epoch": 1.6486666666666667,
      "grad_norm": 0.20488794147968292,
      "learning_rate": 3.969583333333334e-05,
      "loss": 0.0016,
      "step": 49460
    },
    {
      "epoch": 1.649,
      "grad_norm": 0.3215422034263611,
      "learning_rate": 3.969375e-05,
      "loss": 0.0016,
      "step": 49470
    },
    {
      "epoch": 1.6493333333333333,
      "grad_norm": 0.3802970349788666,
      "learning_rate": 3.9691666666666665e-05,
      "loss": 0.0031,
      "step": 49480
    },
    {
      "epoch": 1.6496666666666666,
      "grad_norm": 0.5518985986709595,
      "learning_rate": 3.968958333333334e-05,
      "loss": 0.0032,
      "step": 49490
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.11735543608665466,
      "learning_rate": 3.96875e-05,
      "loss": 0.0027,
      "step": 49500
    },
    {
      "epoch": 1.6503333333333332,
      "grad_norm": 0.39891359210014343,
      "learning_rate": 3.968541666666667e-05,
      "loss": 0.0045,
      "step": 49510
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.1755697876214981,
      "learning_rate": 3.9683333333333333e-05,
      "loss": 0.0021,
      "step": 49520
    },
    {
      "epoch": 1.651,
      "grad_norm": 0.02962772361934185,
      "learning_rate": 3.9681250000000006e-05,
      "loss": 0.003,
      "step": 49530
    },
    {
      "epoch": 1.6513333333333333,
      "grad_norm": 0.14617383480072021,
      "learning_rate": 3.9679166666666664e-05,
      "loss": 0.0027,
      "step": 49540
    },
    {
      "epoch": 1.6516666666666666,
      "grad_norm": 0.00814275536686182,
      "learning_rate": 3.967708333333334e-05,
      "loss": 0.002,
      "step": 49550
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.38678881525993347,
      "learning_rate": 3.9675e-05,
      "loss": 0.002,
      "step": 49560
    },
    {
      "epoch": 1.6523333333333334,
      "grad_norm": 0.24647539854049683,
      "learning_rate": 3.967291666666667e-05,
      "loss": 0.0025,
      "step": 49570
    },
    {
      "epoch": 1.6526666666666667,
      "grad_norm": 0.03046094812452793,
      "learning_rate": 3.967083333333333e-05,
      "loss": 0.0032,
      "step": 49580
    },
    {
      "epoch": 1.653,
      "grad_norm": 0.11681699007749557,
      "learning_rate": 3.9668750000000005e-05,
      "loss": 0.0018,
      "step": 49590
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.17548218369483948,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0036,
      "step": 49600
    },
    {
      "epoch": 1.6536666666666666,
      "grad_norm": 0.029779888689517975,
      "learning_rate": 3.966458333333333e-05,
      "loss": 0.002,
      "step": 49610
    },
    {
      "epoch": 1.654,
      "grad_norm": 0.2338004857301712,
      "learning_rate": 3.96625e-05,
      "loss": 0.0023,
      "step": 49620
    },
    {
      "epoch": 1.6543333333333332,
      "grad_norm": 0.8082107901573181,
      "learning_rate": 3.966041666666667e-05,
      "loss": 0.0018,
      "step": 49630
    },
    {
      "epoch": 1.6546666666666665,
      "grad_norm": 0.17659184336662292,
      "learning_rate": 3.965833333333334e-05,
      "loss": 0.0035,
      "step": 49640
    },
    {
      "epoch": 1.655,
      "grad_norm": 0.23399066925048828,
      "learning_rate": 3.965625e-05,
      "loss": 0.0024,
      "step": 49650
    },
    {
      "epoch": 1.6553333333333333,
      "grad_norm": 0.029789576306939125,
      "learning_rate": 3.965416666666667e-05,
      "loss": 0.0018,
      "step": 49660
    },
    {
      "epoch": 1.6556666666666666,
      "grad_norm": 0.2339455485343933,
      "learning_rate": 3.9652083333333336e-05,
      "loss": 0.0028,
      "step": 49670
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.14592748880386353,
      "learning_rate": 3.965e-05,
      "loss": 0.0045,
      "step": 49680
    },
    {
      "epoch": 1.6563333333333334,
      "grad_norm": 0.3217543065547943,
      "learning_rate": 3.964791666666667e-05,
      "loss": 0.0022,
      "step": 49690
    },
    {
      "epoch": 1.6566666666666667,
      "grad_norm": 0.08805590867996216,
      "learning_rate": 3.964583333333334e-05,
      "loss": 0.0025,
      "step": 49700
    },
    {
      "epoch": 1.657,
      "grad_norm": 0.17581062018871307,
      "learning_rate": 3.9643750000000005e-05,
      "loss": 0.0023,
      "step": 49710
    },
    {
      "epoch": 1.6573333333333333,
      "grad_norm": 0.17551612854003906,
      "learning_rate": 3.9641666666666663e-05,
      "loss": 0.0018,
      "step": 49720
    },
    {
      "epoch": 1.6576666666666666,
      "grad_norm": 0.05912516266107559,
      "learning_rate": 3.9639583333333336e-05,
      "loss": 0.0028,
      "step": 49730
    },
    {
      "epoch": 1.658,
      "grad_norm": 0.031547971069812775,
      "learning_rate": 3.96375e-05,
      "loss": 0.0045,
      "step": 49740
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 0.2046699821949005,
      "learning_rate": 3.963541666666667e-05,
      "loss": 0.002,
      "step": 49750
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.17548634111881256,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0022,
      "step": 49760
    },
    {
      "epoch": 1.659,
      "grad_norm": 0.11709950864315033,
      "learning_rate": 3.9631250000000004e-05,
      "loss": 0.0028,
      "step": 49770
    },
    {
      "epoch": 1.6593333333333333,
      "grad_norm": 0.20482110977172852,
      "learning_rate": 3.962916666666667e-05,
      "loss": 0.0035,
      "step": 49780
    },
    {
      "epoch": 1.6596666666666666,
      "grad_norm": 0.2339961975812912,
      "learning_rate": 3.9627083333333335e-05,
      "loss": 0.0024,
      "step": 49790
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.029958391562104225,
      "learning_rate": 3.9625e-05,
      "loss": 0.0022,
      "step": 49800
    },
    {
      "epoch": 1.6603333333333334,
      "grad_norm": 0.5844296813011169,
      "learning_rate": 3.962291666666667e-05,
      "loss": 0.002,
      "step": 49810
    },
    {
      "epoch": 1.6606666666666667,
      "grad_norm": 0.2926138937473297,
      "learning_rate": 3.962083333333333e-05,
      "loss": 0.0025,
      "step": 49820
    },
    {
      "epoch": 1.661,
      "grad_norm": 0.2922302782535553,
      "learning_rate": 3.9618750000000004e-05,
      "loss": 0.0034,
      "step": 49830
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.2897327244281769,
      "learning_rate": 3.961666666666667e-05,
      "loss": 0.0027,
      "step": 49840
    },
    {
      "epoch": 1.6616666666666666,
      "grad_norm": 0.05857907235622406,
      "learning_rate": 3.9614583333333335e-05,
      "loss": 0.002,
      "step": 49850
    },
    {
      "epoch": 1.662,
      "grad_norm": 0.26356714963912964,
      "learning_rate": 3.96125e-05,
      "loss": 0.0026,
      "step": 49860
    },
    {
      "epoch": 1.6623333333333332,
      "grad_norm": 0.14613361656665802,
      "learning_rate": 3.9610416666666666e-05,
      "loss": 0.0028,
      "step": 49870
    },
    {
      "epoch": 1.6626666666666665,
      "grad_norm": 0.08789470046758652,
      "learning_rate": 3.960833333333334e-05,
      "loss": 0.0026,
      "step": 49880
    },
    {
      "epoch": 1.663,
      "grad_norm": 0.4969603717327118,
      "learning_rate": 3.960625e-05,
      "loss": 0.0022,
      "step": 49890
    },
    {
      "epoch": 1.6633333333333333,
      "grad_norm": 0.5262240171432495,
      "learning_rate": 3.960416666666667e-05,
      "loss": 0.0027,
      "step": 49900
    },
    {
      "epoch": 1.6636666666666666,
      "grad_norm": 0.3510509729385376,
      "learning_rate": 3.9602083333333335e-05,
      "loss": 0.002,
      "step": 49910
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.05902893468737602,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0027,
      "step": 49920
    },
    {
      "epoch": 1.6643333333333334,
      "grad_norm": 0.05860219523310661,
      "learning_rate": 3.9597916666666666e-05,
      "loss": 0.0019,
      "step": 49930
    },
    {
      "epoch": 1.6646666666666667,
      "grad_norm": 0.11689946055412292,
      "learning_rate": 3.959583333333334e-05,
      "loss": 0.0023,
      "step": 49940
    },
    {
      "epoch": 1.665,
      "grad_norm": 0.35056784749031067,
      "learning_rate": 3.9593750000000004e-05,
      "loss": 0.003,
      "step": 49950
    },
    {
      "epoch": 1.6653333333333333,
      "grad_norm": 0.17533078789710999,
      "learning_rate": 3.959166666666667e-05,
      "loss": 0.0018,
      "step": 49960
    },
    {
      "epoch": 1.6656666666666666,
      "grad_norm": 0.42703479528427124,
      "learning_rate": 3.9589583333333335e-05,
      "loss": 0.0029,
      "step": 49970
    },
    {
      "epoch": 1.666,
      "grad_norm": 0.006352283526211977,
      "learning_rate": 3.95875e-05,
      "loss": 0.0022,
      "step": 49980
    },
    {
      "epoch": 1.6663333333333332,
      "grad_norm": 0.30783551931381226,
      "learning_rate": 3.958541666666667e-05,
      "loss": 0.0019,
      "step": 49990
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.4380679130554199,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.0018,
      "step": 50000
    },
    {
      "epoch": 1.667,
      "grad_norm": 0.058791548013687134,
      "learning_rate": 3.958125e-05,
      "loss": 0.0011,
      "step": 50010
    },
    {
      "epoch": 1.6673333333333333,
      "grad_norm": 0.3301961421966553,
      "learning_rate": 3.957916666666667e-05,
      "loss": 0.0024,
      "step": 50020
    },
    {
      "epoch": 1.6676666666666666,
      "grad_norm": 0.30442777276039124,
      "learning_rate": 3.9577083333333334e-05,
      "loss": 0.0021,
      "step": 50030
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.05909394100308418,
      "learning_rate": 3.9575e-05,
      "loss": 0.0022,
      "step": 50040
    },
    {
      "epoch": 1.6683333333333334,
      "grad_norm": 0.3800451159477234,
      "learning_rate": 3.957291666666667e-05,
      "loss": 0.0021,
      "step": 50050
    },
    {
      "epoch": 1.6686666666666667,
      "grad_norm": 0.0874725729227066,
      "learning_rate": 3.957083333333334e-05,
      "loss": 0.0028,
      "step": 50060
    },
    {
      "epoch": 1.669,
      "grad_norm": 0.7014513611793518,
      "learning_rate": 3.956875e-05,
      "loss": 0.0022,
      "step": 50070
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.11672966182231903,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.0025,
      "step": 50080
    },
    {
      "epoch": 1.6696666666666666,
      "grad_norm": 1.061760663986206,
      "learning_rate": 3.9564583333333334e-05,
      "loss": 0.0061,
      "step": 50090
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.11693378537893295,
      "learning_rate": 3.95625e-05,
      "loss": 0.003,
      "step": 50100
    },
    {
      "epoch": 1.6703333333333332,
      "grad_norm": 0.20446766912937164,
      "learning_rate": 3.9560416666666665e-05,
      "loss": 0.0021,
      "step": 50110
    },
    {
      "epoch": 1.6706666666666665,
      "grad_norm": 0.05861092731356621,
      "learning_rate": 3.955833333333334e-05,
      "loss": 0.0035,
      "step": 50120
    },
    {
      "epoch": 1.671,
      "grad_norm": 0.11694097518920898,
      "learning_rate": 3.955625e-05,
      "loss": 0.0025,
      "step": 50130
    },
    {
      "epoch": 1.6713333333333333,
      "grad_norm": 0.029553277418017387,
      "learning_rate": 3.955416666666667e-05,
      "loss": 0.0033,
      "step": 50140
    },
    {
      "epoch": 1.6716666666666666,
      "grad_norm": 0.11686567962169647,
      "learning_rate": 3.9552083333333334e-05,
      "loss": 0.0027,
      "step": 50150
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.23393353819847107,
      "learning_rate": 3.9550000000000006e-05,
      "loss": 0.0024,
      "step": 50160
    },
    {
      "epoch": 1.6723333333333334,
      "grad_norm": 0.5845980644226074,
      "learning_rate": 3.9547916666666665e-05,
      "loss": 0.002,
      "step": 50170
    },
    {
      "epoch": 1.6726666666666667,
      "grad_norm": 0.00970727950334549,
      "learning_rate": 3.954583333333334e-05,
      "loss": 0.0028,
      "step": 50180
    },
    {
      "epoch": 1.673,
      "grad_norm": 0.26325398683547974,
      "learning_rate": 3.954375e-05,
      "loss": 0.0028,
      "step": 50190
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.08786739408969879,
      "learning_rate": 3.9541666666666675e-05,
      "loss": 0.0019,
      "step": 50200
    },
    {
      "epoch": 1.6736666666666666,
      "grad_norm": 0.02965172752737999,
      "learning_rate": 3.953958333333333e-05,
      "loss": 0.002,
      "step": 50210
    },
    {
      "epoch": 1.674,
      "grad_norm": 0.3210722804069519,
      "learning_rate": 3.95375e-05,
      "loss": 0.0019,
      "step": 50220
    },
    {
      "epoch": 1.6743333333333332,
      "grad_norm": 0.11693368852138519,
      "learning_rate": 3.953541666666667e-05,
      "loss": 0.0031,
      "step": 50230
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.4668135643005371,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0037,
      "step": 50240
    },
    {
      "epoch": 1.675,
      "grad_norm": 0.005099080037325621,
      "learning_rate": 3.953125e-05,
      "loss": 0.0024,
      "step": 50250
    },
    {
      "epoch": 1.6753333333333333,
      "grad_norm": 0.2043984830379486,
      "learning_rate": 3.952916666666667e-05,
      "loss": 0.0026,
      "step": 50260
    },
    {
      "epoch": 1.6756666666666666,
      "grad_norm": 0.058894410729408264,
      "learning_rate": 3.952708333333334e-05,
      "loss": 0.0034,
      "step": 50270
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.2921642065048218,
      "learning_rate": 3.9525e-05,
      "loss": 0.0026,
      "step": 50280
    },
    {
      "epoch": 1.6763333333333335,
      "grad_norm": 0.02946823462843895,
      "learning_rate": 3.952291666666667e-05,
      "loss": 0.0016,
      "step": 50290
    },
    {
      "epoch": 1.6766666666666667,
      "grad_norm": 0.029656769707798958,
      "learning_rate": 3.9520833333333336e-05,
      "loss": 0.0018,
      "step": 50300
    },
    {
      "epoch": 1.677,
      "grad_norm": 1.6528511047363281,
      "learning_rate": 3.951875e-05,
      "loss": 0.0028,
      "step": 50310
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.11727143824100494,
      "learning_rate": 3.951666666666667e-05,
      "loss": 0.002,
      "step": 50320
    },
    {
      "epoch": 1.6776666666666666,
      "grad_norm": 0.1461493819952011,
      "learning_rate": 3.951458333333333e-05,
      "loss": 0.004,
      "step": 50330
    },
    {
      "epoch": 1.678,
      "grad_norm": 0.10549266636371613,
      "learning_rate": 3.9512500000000005e-05,
      "loss": 0.0025,
      "step": 50340
    },
    {
      "epoch": 1.6783333333333332,
      "grad_norm": 0.11769569665193558,
      "learning_rate": 3.9510416666666664e-05,
      "loss": 0.0015,
      "step": 50350
    },
    {
      "epoch": 1.6786666666666665,
      "grad_norm": 0.030390944331884384,
      "learning_rate": 3.9508333333333336e-05,
      "loss": 0.0026,
      "step": 50360
    },
    {
      "epoch": 1.679,
      "grad_norm": 0.08785548806190491,
      "learning_rate": 3.950625e-05,
      "loss": 0.0023,
      "step": 50370
    },
    {
      "epoch": 1.6793333333333333,
      "grad_norm": 0.2044503539800644,
      "learning_rate": 3.950416666666667e-05,
      "loss": 0.0022,
      "step": 50380
    },
    {
      "epoch": 1.6796666666666666,
      "grad_norm": 0.0583273246884346,
      "learning_rate": 3.950208333333333e-05,
      "loss": 0.0025,
      "step": 50390
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.17562317848205566,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0024,
      "step": 50400
    },
    {
      "epoch": 1.6803333333333335,
      "grad_norm": 0.08785215020179749,
      "learning_rate": 3.949791666666667e-05,
      "loss": 0.0031,
      "step": 50410
    },
    {
      "epoch": 1.6806666666666668,
      "grad_norm": 0.08782695233821869,
      "learning_rate": 3.9495833333333336e-05,
      "loss": 0.0018,
      "step": 50420
    },
    {
      "epoch": 1.681,
      "grad_norm": 0.2924219071865082,
      "learning_rate": 3.949375e-05,
      "loss": 0.0026,
      "step": 50430
    },
    {
      "epoch": 1.6813333333333333,
      "grad_norm": 0.02984531782567501,
      "learning_rate": 3.9491666666666673e-05,
      "loss": 0.0024,
      "step": 50440
    },
    {
      "epoch": 1.6816666666666666,
      "grad_norm": 0.14629273116588593,
      "learning_rate": 3.948958333333333e-05,
      "loss": 0.0021,
      "step": 50450
    },
    {
      "epoch": 1.682,
      "grad_norm": 0.08773817121982574,
      "learning_rate": 3.94875e-05,
      "loss": 0.0024,
      "step": 50460
    },
    {
      "epoch": 1.6823333333333332,
      "grad_norm": 0.4964323937892914,
      "learning_rate": 3.948541666666667e-05,
      "loss": 0.0026,
      "step": 50470
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.029681220650672913,
      "learning_rate": 3.9483333333333335e-05,
      "loss": 0.0022,
      "step": 50480
    },
    {
      "epoch": 1.683,
      "grad_norm": 0.438483327627182,
      "learning_rate": 3.948125e-05,
      "loss": 0.0026,
      "step": 50490
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.508512556552887,
      "learning_rate": 3.9479166666666666e-05,
      "loss": 0.0031,
      "step": 50500
    },
    {
      "epoch": 1.6836666666666666,
      "grad_norm": 0.08805699646472931,
      "learning_rate": 3.947708333333334e-05,
      "loss": 0.0016,
      "step": 50510
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.029370669275522232,
      "learning_rate": 3.9475000000000004e-05,
      "loss": 0.0024,
      "step": 50520
    },
    {
      "epoch": 1.6843333333333335,
      "grad_norm": 0.029465220868587494,
      "learning_rate": 3.947291666666667e-05,
      "loss": 0.0024,
      "step": 50530
    },
    {
      "epoch": 1.6846666666666668,
      "grad_norm": 0.17540031671524048,
      "learning_rate": 3.9470833333333335e-05,
      "loss": 0.0028,
      "step": 50540
    },
    {
      "epoch": 1.685,
      "grad_norm": 0.2622545659542084,
      "learning_rate": 3.946875000000001e-05,
      "loss": 0.0029,
      "step": 50550
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.08783534914255142,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0029,
      "step": 50560
    },
    {
      "epoch": 1.6856666666666666,
      "grad_norm": 0.20430058240890503,
      "learning_rate": 3.946458333333333e-05,
      "loss": 0.0025,
      "step": 50570
    },
    {
      "epoch": 1.686,
      "grad_norm": 0.03096577525138855,
      "learning_rate": 3.9462500000000004e-05,
      "loss": 0.0032,
      "step": 50580
    },
    {
      "epoch": 1.6863333333333332,
      "grad_norm": 0.029899319633841515,
      "learning_rate": 3.946041666666667e-05,
      "loss": 0.0024,
      "step": 50590
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.5549642443656921,
      "learning_rate": 3.9458333333333335e-05,
      "loss": 0.0033,
      "step": 50600
    },
    {
      "epoch": 1.687,
      "grad_norm": 0.4088324010372162,
      "learning_rate": 3.945625e-05,
      "loss": 0.0028,
      "step": 50610
    },
    {
      "epoch": 1.6873333333333334,
      "grad_norm": 0.6720353960990906,
      "learning_rate": 3.945416666666667e-05,
      "loss": 0.002,
      "step": 50620
    },
    {
      "epoch": 1.6876666666666666,
      "grad_norm": 0.17557096481323242,
      "learning_rate": 3.945208333333333e-05,
      "loss": 0.0035,
      "step": 50630
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.029498104006052017,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 0.0024,
      "step": 50640
    },
    {
      "epoch": 1.6883333333333335,
      "grad_norm": 0.26298844814300537,
      "learning_rate": 3.944791666666667e-05,
      "loss": 0.0027,
      "step": 50650
    },
    {
      "epoch": 1.6886666666666668,
      "grad_norm": 0.29189664125442505,
      "learning_rate": 3.9445833333333334e-05,
      "loss": 0.0027,
      "step": 50660
    },
    {
      "epoch": 1.689,
      "grad_norm": 0.006286003161221743,
      "learning_rate": 3.944375e-05,
      "loss": 0.0026,
      "step": 50670
    },
    {
      "epoch": 1.6893333333333334,
      "grad_norm": 0.05904068797826767,
      "learning_rate": 3.944166666666667e-05,
      "loss": 0.002,
      "step": 50680
    },
    {
      "epoch": 1.6896666666666667,
      "grad_norm": 0.11666881293058395,
      "learning_rate": 3.943958333333334e-05,
      "loss": 0.0024,
      "step": 50690
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.11701396107673645,
      "learning_rate": 3.9437499999999996e-05,
      "loss": 0.002,
      "step": 50700
    },
    {
      "epoch": 1.6903333333333332,
      "grad_norm": 0.23312917351722717,
      "learning_rate": 3.943541666666667e-05,
      "loss": 0.0032,
      "step": 50710
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.5628330111503601,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.0018,
      "step": 50720
    },
    {
      "epoch": 1.6909999999999998,
      "grad_norm": 0.015689929947257042,
      "learning_rate": 3.943125e-05,
      "loss": 0.003,
      "step": 50730
    },
    {
      "epoch": 1.6913333333333334,
      "grad_norm": 0.1466110795736313,
      "learning_rate": 3.9429166666666665e-05,
      "loss": 0.0032,
      "step": 50740
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 0.24257706105709076,
      "learning_rate": 3.942708333333334e-05,
      "loss": 0.005,
      "step": 50750
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.29225876927375793,
      "learning_rate": 3.9425e-05,
      "loss": 0.0026,
      "step": 50760
    },
    {
      "epoch": 1.6923333333333335,
      "grad_norm": 0.37971869111061096,
      "learning_rate": 3.942291666666667e-05,
      "loss": 0.0018,
      "step": 50770
    },
    {
      "epoch": 1.6926666666666668,
      "grad_norm": 0.029542354866862297,
      "learning_rate": 3.9420833333333334e-05,
      "loss": 0.0019,
      "step": 50780
    },
    {
      "epoch": 1.693,
      "grad_norm": 0.05882968753576279,
      "learning_rate": 3.9418750000000006e-05,
      "loss": 0.0025,
      "step": 50790
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.08792561292648315,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.0032,
      "step": 50800
    },
    {
      "epoch": 1.6936666666666667,
      "grad_norm": 0.40851715207099915,
      "learning_rate": 3.941458333333333e-05,
      "loss": 0.0034,
      "step": 50810
    },
    {
      "epoch": 1.694,
      "grad_norm": 0.29204124212265015,
      "learning_rate": 3.94125e-05,
      "loss": 0.0021,
      "step": 50820
    },
    {
      "epoch": 1.6943333333333332,
      "grad_norm": 0.5841735005378723,
      "learning_rate": 3.941041666666667e-05,
      "loss": 0.0032,
      "step": 50830
    },
    {
      "epoch": 1.6946666666666665,
      "grad_norm": 0.2922918498516083,
      "learning_rate": 3.9408333333333334e-05,
      "loss": 0.0033,
      "step": 50840
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 0.25593283772468567,
      "learning_rate": 3.940625e-05,
      "loss": 0.0028,
      "step": 50850
    },
    {
      "epoch": 1.6953333333333334,
      "grad_norm": 0.9537473320960999,
      "learning_rate": 3.940416666666667e-05,
      "loss": 0.0034,
      "step": 50860
    },
    {
      "epoch": 1.6956666666666667,
      "grad_norm": 0.17554552853107452,
      "learning_rate": 3.940208333333334e-05,
      "loss": 0.0031,
      "step": 50870
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.029422679916024208,
      "learning_rate": 3.94e-05,
      "loss": 0.0035,
      "step": 50880
    },
    {
      "epoch": 1.6963333333333335,
      "grad_norm": 0.14654436707496643,
      "learning_rate": 3.939791666666667e-05,
      "loss": 0.002,
      "step": 50890
    },
    {
      "epoch": 1.6966666666666668,
      "grad_norm": 0.4667343199253082,
      "learning_rate": 3.939583333333334e-05,
      "loss": 0.0023,
      "step": 50900
    },
    {
      "epoch": 1.697,
      "grad_norm": 0.17526309192180634,
      "learning_rate": 3.939375e-05,
      "loss": 0.0022,
      "step": 50910
    },
    {
      "epoch": 1.6973333333333334,
      "grad_norm": 0.14611868560314178,
      "learning_rate": 3.939166666666667e-05,
      "loss": 0.0027,
      "step": 50920
    },
    {
      "epoch": 1.6976666666666667,
      "grad_norm": 0.34540843963623047,
      "learning_rate": 3.9389583333333336e-05,
      "loss": 0.0038,
      "step": 50930
    },
    {
      "epoch": 1.698,
      "grad_norm": 0.05887778475880623,
      "learning_rate": 3.93875e-05,
      "loss": 0.0026,
      "step": 50940
    },
    {
      "epoch": 1.6983333333333333,
      "grad_norm": 0.029838912189006805,
      "learning_rate": 3.938541666666667e-05,
      "loss": 0.0025,
      "step": 50950
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.26286306977272034,
      "learning_rate": 3.938333333333333e-05,
      "loss": 0.0018,
      "step": 50960
    },
    {
      "epoch": 1.6989999999999998,
      "grad_norm": 0.11665212363004684,
      "learning_rate": 3.9381250000000005e-05,
      "loss": 0.0017,
      "step": 50970
    },
    {
      "epoch": 1.6993333333333334,
      "grad_norm": 0.09587278962135315,
      "learning_rate": 3.9379166666666664e-05,
      "loss": 0.0023,
      "step": 50980
    },
    {
      "epoch": 1.6996666666666667,
      "grad_norm": 0.43796730041503906,
      "learning_rate": 3.9377083333333336e-05,
      "loss": 0.0016,
      "step": 50990
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.029957206919789314,
      "learning_rate": 3.9375e-05,
      "loss": 0.002,
      "step": 51000
    },
    {
      "epoch": 1.7003333333333335,
      "grad_norm": 0.14602075517177582,
      "learning_rate": 3.937291666666667e-05,
      "loss": 0.0035,
      "step": 51010
    },
    {
      "epoch": 1.7006666666666668,
      "grad_norm": 0.08741068094968796,
      "learning_rate": 3.937083333333333e-05,
      "loss": 0.002,
      "step": 51020
    },
    {
      "epoch": 1.701,
      "grad_norm": 0.029585134238004684,
      "learning_rate": 3.9368750000000005e-05,
      "loss": 0.0022,
      "step": 51030
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.08774926513433456,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.0018,
      "step": 51040
    },
    {
      "epoch": 1.7016666666666667,
      "grad_norm": 0.05856974050402641,
      "learning_rate": 3.9364583333333336e-05,
      "loss": 0.0026,
      "step": 51050
    },
    {
      "epoch": 1.702,
      "grad_norm": 0.11691166460514069,
      "learning_rate": 3.93625e-05,
      "loss": 0.003,
      "step": 51060
    },
    {
      "epoch": 1.7023333333333333,
      "grad_norm": 0.1166420504450798,
      "learning_rate": 3.936041666666667e-05,
      "loss": 0.0022,
      "step": 51070
    },
    {
      "epoch": 1.7026666666666666,
      "grad_norm": 0.14637188613414764,
      "learning_rate": 3.935833333333334e-05,
      "loss": 0.0013,
      "step": 51080
    },
    {
      "epoch": 1.7029999999999998,
      "grad_norm": 0.3503090739250183,
      "learning_rate": 3.935625e-05,
      "loss": 0.0029,
      "step": 51090
    },
    {
      "epoch": 1.7033333333333334,
      "grad_norm": 0.41009047627449036,
      "learning_rate": 3.935416666666667e-05,
      "loss": 0.0024,
      "step": 51100
    },
    {
      "epoch": 1.7036666666666667,
      "grad_norm": 0.14595453441143036,
      "learning_rate": 3.9352083333333336e-05,
      "loss": 0.002,
      "step": 51110
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.11660824716091156,
      "learning_rate": 3.935e-05,
      "loss": 0.0026,
      "step": 51120
    },
    {
      "epoch": 1.7043333333333335,
      "grad_norm": 0.05853772535920143,
      "learning_rate": 3.9347916666666667e-05,
      "loss": 0.0023,
      "step": 51130
    },
    {
      "epoch": 1.7046666666666668,
      "grad_norm": 0.005718878470361233,
      "learning_rate": 3.934583333333334e-05,
      "loss": 0.0025,
      "step": 51140
    },
    {
      "epoch": 1.705,
      "grad_norm": 0.0879744291305542,
      "learning_rate": 3.9343750000000004e-05,
      "loss": 0.0021,
      "step": 51150
    },
    {
      "epoch": 1.7053333333333334,
      "grad_norm": 0.1460898518562317,
      "learning_rate": 3.934166666666667e-05,
      "loss": 0.0033,
      "step": 51160
    },
    {
      "epoch": 1.7056666666666667,
      "grad_norm": 0.004349338822066784,
      "learning_rate": 3.9339583333333335e-05,
      "loss": 0.0028,
      "step": 51170
    },
    {
      "epoch": 1.706,
      "grad_norm": 0.20401924848556519,
      "learning_rate": 3.93375e-05,
      "loss": 0.0031,
      "step": 51180
    },
    {
      "epoch": 1.7063333333333333,
      "grad_norm": 0.20396214723587036,
      "learning_rate": 3.9335416666666666e-05,
      "loss": 0.0017,
      "step": 51190
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.23421774804592133,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0021,
      "step": 51200
    },
    {
      "epoch": 1.7069999999999999,
      "grad_norm": 0.14579246938228607,
      "learning_rate": 3.9331250000000004e-05,
      "loss": 0.0019,
      "step": 51210
    },
    {
      "epoch": 1.7073333333333334,
      "grad_norm": 0.40851664543151855,
      "learning_rate": 3.932916666666667e-05,
      "loss": 0.0027,
      "step": 51220
    },
    {
      "epoch": 1.7076666666666667,
      "grad_norm": 0.292044997215271,
      "learning_rate": 3.9327083333333335e-05,
      "loss": 0.0027,
      "step": 51230
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.02961692400276661,
      "learning_rate": 3.9325e-05,
      "loss": 0.0028,
      "step": 51240
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.4960364103317261,
      "learning_rate": 3.932291666666667e-05,
      "loss": 0.0018,
      "step": 51250
    },
    {
      "epoch": 1.7086666666666668,
      "grad_norm": 0.17542608082294464,
      "learning_rate": 3.932083333333333e-05,
      "loss": 0.0016,
      "step": 51260
    },
    {
      "epoch": 1.709,
      "grad_norm": 0.08775263279676437,
      "learning_rate": 3.9318750000000004e-05,
      "loss": 0.0018,
      "step": 51270
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.3505868911743164,
      "learning_rate": 3.931666666666667e-05,
      "loss": 0.0022,
      "step": 51280
    },
    {
      "epoch": 1.7096666666666667,
      "grad_norm": 0.17529672384262085,
      "learning_rate": 3.9314583333333335e-05,
      "loss": 0.0023,
      "step": 51290
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.262443482875824,
      "learning_rate": 3.93125e-05,
      "loss": 0.0017,
      "step": 51300
    },
    {
      "epoch": 1.7103333333333333,
      "grad_norm": 0.05847004055976868,
      "learning_rate": 3.9310416666666666e-05,
      "loss": 0.0037,
      "step": 51310
    },
    {
      "epoch": 1.7106666666666666,
      "grad_norm": 0.39822104573249817,
      "learning_rate": 3.930833333333334e-05,
      "loss": 0.0025,
      "step": 51320
    },
    {
      "epoch": 1.7109999999999999,
      "grad_norm": 0.05911122262477875,
      "learning_rate": 3.930625e-05,
      "loss": 0.0027,
      "step": 51330
    },
    {
      "epoch": 1.7113333333333334,
      "grad_norm": 0.08840187638998032,
      "learning_rate": 3.930416666666667e-05,
      "loss": 0.0017,
      "step": 51340
    },
    {
      "epoch": 1.7116666666666667,
      "grad_norm": 0.005866308230906725,
      "learning_rate": 3.9302083333333334e-05,
      "loss": 0.0026,
      "step": 51350
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.4095046818256378,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.0016,
      "step": 51360
    },
    {
      "epoch": 1.7123333333333335,
      "grad_norm": 0.6128592491149902,
      "learning_rate": 3.9297916666666665e-05,
      "loss": 0.0026,
      "step": 51370
    },
    {
      "epoch": 1.7126666666666668,
      "grad_norm": 0.05872838944196701,
      "learning_rate": 3.929583333333334e-05,
      "loss": 0.0022,
      "step": 51380
    },
    {
      "epoch": 1.713,
      "grad_norm": 0.029537823051214218,
      "learning_rate": 3.929375e-05,
      "loss": 0.0028,
      "step": 51390
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.26278504729270935,
      "learning_rate": 3.929166666666667e-05,
      "loss": 0.0023,
      "step": 51400
    },
    {
      "epoch": 1.7136666666666667,
      "grad_norm": 0.0586995854973793,
      "learning_rate": 3.9289583333333334e-05,
      "loss": 0.0029,
      "step": 51410
    },
    {
      "epoch": 1.714,
      "grad_norm": 0.08762199431657791,
      "learning_rate": 3.92875e-05,
      "loss": 0.0019,
      "step": 51420
    },
    {
      "epoch": 1.7143333333333333,
      "grad_norm": 0.4670093357563019,
      "learning_rate": 3.928541666666667e-05,
      "loss": 0.0014,
      "step": 51430
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.20454272627830505,
      "learning_rate": 3.928333333333333e-05,
      "loss": 0.0023,
      "step": 51440
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 0.14637398719787598,
      "learning_rate": 3.928125e-05,
      "loss": 0.0028,
      "step": 51450
    },
    {
      "epoch": 1.7153333333333334,
      "grad_norm": 0.0948343500494957,
      "learning_rate": 3.927916666666667e-05,
      "loss": 0.0027,
      "step": 51460
    },
    {
      "epoch": 1.7156666666666667,
      "grad_norm": 0.33799606561660767,
      "learning_rate": 3.9277083333333334e-05,
      "loss": 0.0031,
      "step": 51470
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.11667768657207489,
      "learning_rate": 3.9275e-05,
      "loss": 0.002,
      "step": 51480
    },
    {
      "epoch": 1.7163333333333335,
      "grad_norm": 0.6710789203643799,
      "learning_rate": 3.927291666666667e-05,
      "loss": 0.0032,
      "step": 51490
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.23378834128379822,
      "learning_rate": 3.927083333333334e-05,
      "loss": 0.0025,
      "step": 51500
    },
    {
      "epoch": 1.717,
      "grad_norm": 0.29178792238235474,
      "learning_rate": 3.926875e-05,
      "loss": 0.0029,
      "step": 51510
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.058383695781230927,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0025,
      "step": 51520
    },
    {
      "epoch": 1.7176666666666667,
      "grad_norm": 0.23302613198757172,
      "learning_rate": 3.926458333333334e-05,
      "loss": 0.0017,
      "step": 51530
    },
    {
      "epoch": 1.718,
      "grad_norm": 0.2919037640094757,
      "learning_rate": 3.92625e-05,
      "loss": 0.0025,
      "step": 51540
    },
    {
      "epoch": 1.7183333333333333,
      "grad_norm": 0.2918550968170166,
      "learning_rate": 3.9260416666666664e-05,
      "loss": 0.0035,
      "step": 51550
    },
    {
      "epoch": 1.7186666666666666,
      "grad_norm": 0.23316408693790436,
      "learning_rate": 3.925833333333334e-05,
      "loss": 0.0016,
      "step": 51560
    },
    {
      "epoch": 1.7189999999999999,
      "grad_norm": 0.11748960614204407,
      "learning_rate": 3.925625e-05,
      "loss": 0.0026,
      "step": 51570
    },
    {
      "epoch": 1.7193333333333334,
      "grad_norm": 0.11959642916917801,
      "learning_rate": 3.925416666666667e-05,
      "loss": 0.0024,
      "step": 51580
    },
    {
      "epoch": 1.7196666666666667,
      "grad_norm": 0.0033042735885828733,
      "learning_rate": 3.925208333333333e-05,
      "loss": 0.0032,
      "step": 51590
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.17482678592205048,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.0036,
      "step": 51600
    },
    {
      "epoch": 1.7203333333333335,
      "grad_norm": 0.23336617648601532,
      "learning_rate": 3.924791666666667e-05,
      "loss": 0.0023,
      "step": 51610
    },
    {
      "epoch": 1.7206666666666668,
      "grad_norm": 0.2624044120311737,
      "learning_rate": 3.9245833333333336e-05,
      "loss": 0.0019,
      "step": 51620
    },
    {
      "epoch": 1.721,
      "grad_norm": 0.058348946273326874,
      "learning_rate": 3.924375e-05,
      "loss": 0.0019,
      "step": 51630
    },
    {
      "epoch": 1.7213333333333334,
      "grad_norm": 0.20457398891448975,
      "learning_rate": 3.9241666666666674e-05,
      "loss": 0.003,
      "step": 51640
    },
    {
      "epoch": 1.7216666666666667,
      "grad_norm": 0.029771270230412483,
      "learning_rate": 3.923958333333333e-05,
      "loss": 0.0028,
      "step": 51650
    },
    {
      "epoch": 1.722,
      "grad_norm": 0.2803042531013489,
      "learning_rate": 3.92375e-05,
      "loss": 0.0025,
      "step": 51660
    },
    {
      "epoch": 1.7223333333333333,
      "grad_norm": 0.11678676307201385,
      "learning_rate": 3.923541666666667e-05,
      "loss": 0.0023,
      "step": 51670
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.05885099247097969,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0023,
      "step": 51680
    },
    {
      "epoch": 1.7229999999999999,
      "grad_norm": 0.5544509291648865,
      "learning_rate": 3.923125e-05,
      "loss": 0.0024,
      "step": 51690
    },
    {
      "epoch": 1.7233333333333334,
      "grad_norm": 0.35015448927879333,
      "learning_rate": 3.922916666666667e-05,
      "loss": 0.0021,
      "step": 51700
    },
    {
      "epoch": 1.7236666666666667,
      "grad_norm": 0.018528224900364876,
      "learning_rate": 3.922708333333334e-05,
      "loss": 0.002,
      "step": 51710
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.46673429012298584,
      "learning_rate": 3.9225e-05,
      "loss": 0.0023,
      "step": 51720
    },
    {
      "epoch": 1.7243333333333335,
      "grad_norm": 0.2047189623117447,
      "learning_rate": 3.922291666666667e-05,
      "loss": 0.0016,
      "step": 51730
    },
    {
      "epoch": 1.7246666666666668,
      "grad_norm": 0.0294144656509161,
      "learning_rate": 3.9220833333333336e-05,
      "loss": 0.0024,
      "step": 51740
    },
    {
      "epoch": 1.725,
      "grad_norm": 0.11675092577934265,
      "learning_rate": 3.921875e-05,
      "loss": 0.0017,
      "step": 51750
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.0883229672908783,
      "learning_rate": 3.921666666666667e-05,
      "loss": 0.0025,
      "step": 51760
    },
    {
      "epoch": 1.7256666666666667,
      "grad_norm": 0.3846573531627655,
      "learning_rate": 3.921458333333334e-05,
      "loss": 0.0024,
      "step": 51770
    },
    {
      "epoch": 1.726,
      "grad_norm": 0.379697322845459,
      "learning_rate": 3.9212500000000004e-05,
      "loss": 0.0026,
      "step": 51780
    },
    {
      "epoch": 1.7263333333333333,
      "grad_norm": 0.14605067670345306,
      "learning_rate": 3.921041666666666e-05,
      "loss": 0.0032,
      "step": 51790
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.17526787519454956,
      "learning_rate": 3.9208333333333335e-05,
      "loss": 0.002,
      "step": 51800
    },
    {
      "epoch": 1.7269999999999999,
      "grad_norm": 0.27315017580986023,
      "learning_rate": 3.920625e-05,
      "loss": 0.0026,
      "step": 51810
    },
    {
      "epoch": 1.7273333333333334,
      "grad_norm": 0.029338784515857697,
      "learning_rate": 3.9204166666666666e-05,
      "loss": 0.003,
      "step": 51820
    },
    {
      "epoch": 1.7276666666666667,
      "grad_norm": 0.11681142449378967,
      "learning_rate": 3.920208333333333e-05,
      "loss": 0.0024,
      "step": 51830
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.08787950128316879,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.003,
      "step": 51840
    },
    {
      "epoch": 1.7283333333333335,
      "grad_norm": 0.058413393795490265,
      "learning_rate": 3.919791666666667e-05,
      "loss": 0.0026,
      "step": 51850
    },
    {
      "epoch": 1.7286666666666668,
      "grad_norm": 0.4671172499656677,
      "learning_rate": 3.9195833333333335e-05,
      "loss": 0.002,
      "step": 51860
    },
    {
      "epoch": 1.729,
      "grad_norm": 0.2918754518032074,
      "learning_rate": 3.919375e-05,
      "loss": 0.0018,
      "step": 51870
    },
    {
      "epoch": 1.7293333333333334,
      "grad_norm": 0.5298659205436707,
      "learning_rate": 3.919166666666667e-05,
      "loss": 0.0019,
      "step": 51880
    },
    {
      "epoch": 1.7296666666666667,
      "grad_norm": 0.1461084634065628,
      "learning_rate": 3.918958333333334e-05,
      "loss": 0.0023,
      "step": 51890
    },
    {
      "epoch": 1.73,
      "grad_norm": 1.1333980560302734,
      "learning_rate": 3.91875e-05,
      "loss": 0.0023,
      "step": 51900
    },
    {
      "epoch": 1.7303333333333333,
      "grad_norm": 0.17505629360675812,
      "learning_rate": 3.918541666666667e-05,
      "loss": 0.0029,
      "step": 51910
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.1462889015674591,
      "learning_rate": 3.9183333333333335e-05,
      "loss": 0.0025,
      "step": 51920
    },
    {
      "epoch": 1.7309999999999999,
      "grad_norm": 0.3626985251903534,
      "learning_rate": 3.918125e-05,
      "loss": 0.0016,
      "step": 51930
    },
    {
      "epoch": 1.7313333333333332,
      "grad_norm": 0.005743472371250391,
      "learning_rate": 3.9179166666666666e-05,
      "loss": 0.0028,
      "step": 51940
    },
    {
      "epoch": 1.7316666666666667,
      "grad_norm": 0.08769059926271439,
      "learning_rate": 3.917708333333334e-05,
      "loss": 0.0015,
      "step": 51950
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.32124218344688416,
      "learning_rate": 3.9175000000000004e-05,
      "loss": 0.0022,
      "step": 51960
    },
    {
      "epoch": 1.7323333333333333,
      "grad_norm": 0.17558631300926208,
      "learning_rate": 3.917291666666667e-05,
      "loss": 0.0026,
      "step": 51970
    },
    {
      "epoch": 1.7326666666666668,
      "grad_norm": 0.3800802528858185,
      "learning_rate": 3.9170833333333335e-05,
      "loss": 0.0022,
      "step": 51980
    },
    {
      "epoch": 1.733,
      "grad_norm": 0.29201292991638184,
      "learning_rate": 3.916875000000001e-05,
      "loss": 0.0021,
      "step": 51990
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.01408451423048973,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0031,
      "step": 52000
    },
    {
      "epoch": 1.7336666666666667,
      "grad_norm": 0.29307714104652405,
      "learning_rate": 3.916458333333334e-05,
      "loss": 0.0024,
      "step": 52010
    },
    {
      "epoch": 1.734,
      "grad_norm": 0.2047794759273529,
      "learning_rate": 3.91625e-05,
      "loss": 0.0018,
      "step": 52020
    },
    {
      "epoch": 1.7343333333333333,
      "grad_norm": 0.029979776591062546,
      "learning_rate": 3.916041666666667e-05,
      "loss": 0.003,
      "step": 52030
    },
    {
      "epoch": 1.7346666666666666,
      "grad_norm": 0.05883030220866203,
      "learning_rate": 3.9158333333333334e-05,
      "loss": 0.0011,
      "step": 52040
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 0.1459113508462906,
      "learning_rate": 3.915625e-05,
      "loss": 0.0023,
      "step": 52050
    },
    {
      "epoch": 1.7353333333333332,
      "grad_norm": 0.2338029444217682,
      "learning_rate": 3.915416666666667e-05,
      "loss": 0.0015,
      "step": 52060
    },
    {
      "epoch": 1.7356666666666667,
      "grad_norm": 0.11744663119316101,
      "learning_rate": 3.915208333333333e-05,
      "loss": 0.0026,
      "step": 52070
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.7590545415878296,
      "learning_rate": 3.915e-05,
      "loss": 0.0022,
      "step": 52080
    },
    {
      "epoch": 1.7363333333333333,
      "grad_norm": 0.26258158683776855,
      "learning_rate": 3.914791666666667e-05,
      "loss": 0.0034,
      "step": 52090
    },
    {
      "epoch": 1.7366666666666668,
      "grad_norm": 0.20430608093738556,
      "learning_rate": 3.9145833333333334e-05,
      "loss": 0.0023,
      "step": 52100
    },
    {
      "epoch": 1.737,
      "grad_norm": 0.32080966234207153,
      "learning_rate": 3.914375e-05,
      "loss": 0.0013,
      "step": 52110
    },
    {
      "epoch": 1.7373333333333334,
      "grad_norm": 0.11675550788640976,
      "learning_rate": 3.914166666666667e-05,
      "loss": 0.0025,
      "step": 52120
    },
    {
      "epoch": 1.7376666666666667,
      "grad_norm": 0.05835958942770958,
      "learning_rate": 3.913958333333334e-05,
      "loss": 0.0018,
      "step": 52130
    },
    {
      "epoch": 1.738,
      "grad_norm": 0.05865315720438957,
      "learning_rate": 3.9137499999999996e-05,
      "loss": 0.0013,
      "step": 52140
    },
    {
      "epoch": 1.7383333333333333,
      "grad_norm": 0.14596903324127197,
      "learning_rate": 3.913541666666667e-05,
      "loss": 0.0019,
      "step": 52150
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.11636927723884583,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0029,
      "step": 52160
    },
    {
      "epoch": 1.7389999999999999,
      "grad_norm": 0.11673588305711746,
      "learning_rate": 3.9131250000000006e-05,
      "loss": 0.0027,
      "step": 52170
    },
    {
      "epoch": 1.7393333333333332,
      "grad_norm": 0.20446035265922546,
      "learning_rate": 3.9129166666666665e-05,
      "loss": 0.0022,
      "step": 52180
    },
    {
      "epoch": 1.7396666666666667,
      "grad_norm": 0.030253564938902855,
      "learning_rate": 3.912708333333334e-05,
      "loss": 0.0025,
      "step": 52190
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.004276168067008257,
      "learning_rate": 3.9125e-05,
      "loss": 0.0021,
      "step": 52200
    },
    {
      "epoch": 1.7403333333333333,
      "grad_norm": 0.5208922624588013,
      "learning_rate": 3.912291666666667e-05,
      "loss": 0.0019,
      "step": 52210
    },
    {
      "epoch": 1.7406666666666668,
      "grad_norm": 0.3792215883731842,
      "learning_rate": 3.912083333333333e-05,
      "loss": 0.0026,
      "step": 52220
    },
    {
      "epoch": 1.741,
      "grad_norm": 0.23315295577049255,
      "learning_rate": 3.9118750000000006e-05,
      "loss": 0.0033,
      "step": 52230
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.08758753538131714,
      "learning_rate": 3.911666666666667e-05,
      "loss": 0.0024,
      "step": 52240
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 0.09230431914329529,
      "learning_rate": 3.9114583333333337e-05,
      "loss": 0.0039,
      "step": 52250
    },
    {
      "epoch": 1.742,
      "grad_norm": 0.17525112628936768,
      "learning_rate": 3.91125e-05,
      "loss": 0.0033,
      "step": 52260
    },
    {
      "epoch": 1.7423333333333333,
      "grad_norm": 0.058530084788799286,
      "learning_rate": 3.911041666666667e-05,
      "loss": 0.0019,
      "step": 52270
    },
    {
      "epoch": 1.7426666666666666,
      "grad_norm": 0.08790948987007141,
      "learning_rate": 3.910833333333333e-05,
      "loss": 0.0019,
      "step": 52280
    },
    {
      "epoch": 1.7429999999999999,
      "grad_norm": 0.006816523149609566,
      "learning_rate": 3.910625e-05,
      "loss": 0.0018,
      "step": 52290
    },
    {
      "epoch": 1.7433333333333332,
      "grad_norm": 0.2917393743991852,
      "learning_rate": 3.910416666666667e-05,
      "loss": 0.0026,
      "step": 52300
    },
    {
      "epoch": 1.7436666666666667,
      "grad_norm": 0.030019447207450867,
      "learning_rate": 3.9102083333333336e-05,
      "loss": 0.0027,
      "step": 52310
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.058482494205236435,
      "learning_rate": 3.91e-05,
      "loss": 0.0024,
      "step": 52320
    },
    {
      "epoch": 1.7443333333333333,
      "grad_norm": 0.11738911271095276,
      "learning_rate": 3.909791666666667e-05,
      "loss": 0.0036,
      "step": 52330
    },
    {
      "epoch": 1.7446666666666668,
      "grad_norm": 0.08761346340179443,
      "learning_rate": 3.909583333333334e-05,
      "loss": 0.0036,
      "step": 52340
    },
    {
      "epoch": 1.745,
      "grad_norm": 0.20430970191955566,
      "learning_rate": 3.909375e-05,
      "loss": 0.0023,
      "step": 52350
    },
    {
      "epoch": 1.7453333333333334,
      "grad_norm": 0.3511660695075989,
      "learning_rate": 3.909166666666667e-05,
      "loss": 0.0028,
      "step": 52360
    },
    {
      "epoch": 1.7456666666666667,
      "grad_norm": 0.08807707577943802,
      "learning_rate": 3.9089583333333336e-05,
      "loss": 0.0029,
      "step": 52370
    },
    {
      "epoch": 1.746,
      "grad_norm": 0.029616069048643112,
      "learning_rate": 3.90875e-05,
      "loss": 0.0028,
      "step": 52380
    },
    {
      "epoch": 1.7463333333333333,
      "grad_norm": 0.08750303089618683,
      "learning_rate": 3.908541666666667e-05,
      "loss": 0.0018,
      "step": 52390
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.11672721058130264,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.0016,
      "step": 52400
    },
    {
      "epoch": 1.7469999999999999,
      "grad_norm": 0.4964650869369507,
      "learning_rate": 3.9081250000000005e-05,
      "loss": 0.0027,
      "step": 52410
    },
    {
      "epoch": 1.7473333333333332,
      "grad_norm": 0.0027370702009648085,
      "learning_rate": 3.907916666666666e-05,
      "loss": 0.0025,
      "step": 52420
    },
    {
      "epoch": 1.7476666666666667,
      "grad_norm": 0.2919423282146454,
      "learning_rate": 3.9077083333333336e-05,
      "loss": 0.0022,
      "step": 52430
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.0294465571641922,
      "learning_rate": 3.9075e-05,
      "loss": 0.0028,
      "step": 52440
    },
    {
      "epoch": 1.7483333333333333,
      "grad_norm": 0.05852643772959709,
      "learning_rate": 3.907291666666667e-05,
      "loss": 0.0019,
      "step": 52450
    },
    {
      "epoch": 1.7486666666666668,
      "grad_norm": 0.20445258915424347,
      "learning_rate": 3.907083333333333e-05,
      "loss": 0.0026,
      "step": 52460
    },
    {
      "epoch": 1.749,
      "grad_norm": 0.02960227057337761,
      "learning_rate": 3.9068750000000004e-05,
      "loss": 0.004,
      "step": 52470
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.08781062066555023,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0037,
      "step": 52480
    },
    {
      "epoch": 1.7496666666666667,
      "grad_norm": 0.3510100543498993,
      "learning_rate": 3.9064583333333335e-05,
      "loss": 0.0026,
      "step": 52490
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.17540228366851807,
      "learning_rate": 3.90625e-05,
      "loss": 0.0024,
      "step": 52500
    },
    {
      "epoch": 1.7503333333333333,
      "grad_norm": 0.6132421493530273,
      "learning_rate": 3.9060416666666666e-05,
      "loss": 0.0022,
      "step": 52510
    },
    {
      "epoch": 1.7506666666666666,
      "grad_norm": 0.26275938749313354,
      "learning_rate": 3.905833333333334e-05,
      "loss": 0.0024,
      "step": 52520
    },
    {
      "epoch": 1.751,
      "grad_norm": 0.02933153323829174,
      "learning_rate": 3.905625e-05,
      "loss": 0.0025,
      "step": 52530
    },
    {
      "epoch": 1.7513333333333332,
      "grad_norm": 0.005687833297997713,
      "learning_rate": 3.905416666666667e-05,
      "loss": 0.0024,
      "step": 52540
    },
    {
      "epoch": 1.7516666666666667,
      "grad_norm": 0.20440258085727692,
      "learning_rate": 3.9052083333333335e-05,
      "loss": 0.004,
      "step": 52550
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.29234474897384644,
      "learning_rate": 3.905e-05,
      "loss": 0.0044,
      "step": 52560
    },
    {
      "epoch": 1.7523333333333333,
      "grad_norm": 0.17499221861362457,
      "learning_rate": 3.9047916666666666e-05,
      "loss": 0.0036,
      "step": 52570
    },
    {
      "epoch": 1.7526666666666668,
      "grad_norm": 0.7973052859306335,
      "learning_rate": 3.904583333333334e-05,
      "loss": 0.0056,
      "step": 52580
    },
    {
      "epoch": 1.7530000000000001,
      "grad_norm": 0.3791118264198303,
      "learning_rate": 3.9043750000000004e-05,
      "loss": 0.0023,
      "step": 52590
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.23404483497142792,
      "learning_rate": 3.904166666666667e-05,
      "loss": 0.002,
      "step": 52600
    },
    {
      "epoch": 1.7536666666666667,
      "grad_norm": 0.6130861043930054,
      "learning_rate": 3.9039583333333335e-05,
      "loss": 0.0019,
      "step": 52610
    },
    {
      "epoch": 1.754,
      "grad_norm": 0.1753700226545334,
      "learning_rate": 3.903750000000001e-05,
      "loss": 0.0026,
      "step": 52620
    },
    {
      "epoch": 1.7543333333333333,
      "grad_norm": 0.4964537024497986,
      "learning_rate": 3.9035416666666666e-05,
      "loss": 0.0028,
      "step": 52630
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.05874073505401611,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0027,
      "step": 52640
    },
    {
      "epoch": 1.755,
      "grad_norm": 0.3504185974597931,
      "learning_rate": 3.9031250000000003e-05,
      "loss": 0.0027,
      "step": 52650
    },
    {
      "epoch": 1.7553333333333332,
      "grad_norm": 0.08749621361494064,
      "learning_rate": 3.902916666666667e-05,
      "loss": 0.0031,
      "step": 52660
    },
    {
      "epoch": 1.7556666666666667,
      "grad_norm": 0.38030725717544556,
      "learning_rate": 3.9027083333333334e-05,
      "loss": 0.0026,
      "step": 52670
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.38610103726387024,
      "learning_rate": 3.9025e-05,
      "loss": 0.0037,
      "step": 52680
    },
    {
      "epoch": 1.7563333333333333,
      "grad_norm": 0.03041534125804901,
      "learning_rate": 3.902291666666667e-05,
      "loss": 0.0026,
      "step": 52690
    },
    {
      "epoch": 1.7566666666666668,
      "grad_norm": 0.11702703684568405,
      "learning_rate": 3.902083333333333e-05,
      "loss": 0.0026,
      "step": 52700
    },
    {
      "epoch": 1.7570000000000001,
      "grad_norm": 0.029469510540366173,
      "learning_rate": 3.901875e-05,
      "loss": 0.0024,
      "step": 52710
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.32106128334999084,
      "learning_rate": 3.901666666666667e-05,
      "loss": 0.0027,
      "step": 52720
    },
    {
      "epoch": 1.7576666666666667,
      "grad_norm": 0.11662475019693375,
      "learning_rate": 3.901458333333334e-05,
      "loss": 0.0021,
      "step": 52730
    },
    {
      "epoch": 1.758,
      "grad_norm": 0.1459907740354538,
      "learning_rate": 3.90125e-05,
      "loss": 0.0024,
      "step": 52740
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 0.17532947659492493,
      "learning_rate": 3.9010416666666665e-05,
      "loss": 0.0023,
      "step": 52750
    },
    {
      "epoch": 1.7586666666666666,
      "grad_norm": 0.350190132856369,
      "learning_rate": 3.900833333333334e-05,
      "loss": 0.003,
      "step": 52760
    },
    {
      "epoch": 1.759,
      "grad_norm": 0.11664895713329315,
      "learning_rate": 3.900625e-05,
      "loss": 0.002,
      "step": 52770
    },
    {
      "epoch": 1.7593333333333332,
      "grad_norm": 0.058467984199523926,
      "learning_rate": 3.900416666666667e-05,
      "loss": 0.0024,
      "step": 52780
    },
    {
      "epoch": 1.7596666666666667,
      "grad_norm": 0.08774890750646591,
      "learning_rate": 3.9002083333333334e-05,
      "loss": 0.002,
      "step": 52790
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.029547275975346565,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0016,
      "step": 52800
    },
    {
      "epoch": 1.7603333333333333,
      "grad_norm": 0.29221466183662415,
      "learning_rate": 3.8997916666666665e-05,
      "loss": 0.0018,
      "step": 52810
    },
    {
      "epoch": 1.7606666666666668,
      "grad_norm": 0.2336367964744568,
      "learning_rate": 3.899583333333334e-05,
      "loss": 0.002,
      "step": 52820
    },
    {
      "epoch": 1.7610000000000001,
      "grad_norm": 0.2042699158191681,
      "learning_rate": 3.899375e-05,
      "loss": 0.0014,
      "step": 52830
    },
    {
      "epoch": 1.7613333333333334,
      "grad_norm": 0.6718356013298035,
      "learning_rate": 3.899166666666667e-05,
      "loss": 0.0023,
      "step": 52840
    },
    {
      "epoch": 1.7616666666666667,
      "grad_norm": 0.29199209809303284,
      "learning_rate": 3.8989583333333334e-05,
      "loss": 0.0019,
      "step": 52850
    },
    {
      "epoch": 1.762,
      "grad_norm": 0.5840808153152466,
      "learning_rate": 3.8987500000000006e-05,
      "loss": 0.0015,
      "step": 52860
    },
    {
      "epoch": 1.7623333333333333,
      "grad_norm": 0.20455136895179749,
      "learning_rate": 3.898541666666667e-05,
      "loss": 0.002,
      "step": 52870
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.2631017565727234,
      "learning_rate": 3.898333333333333e-05,
      "loss": 0.0024,
      "step": 52880
    },
    {
      "epoch": 1.763,
      "grad_norm": 0.14600075781345367,
      "learning_rate": 3.898125e-05,
      "loss": 0.0015,
      "step": 52890
    },
    {
      "epoch": 1.7633333333333332,
      "grad_norm": 0.29216986894607544,
      "learning_rate": 3.897916666666667e-05,
      "loss": 0.0034,
      "step": 52900
    },
    {
      "epoch": 1.7636666666666667,
      "grad_norm": 0.11679627001285553,
      "learning_rate": 3.897708333333333e-05,
      "loss": 0.0036,
      "step": 52910
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.029536189511418343,
      "learning_rate": 3.8975e-05,
      "loss": 0.0021,
      "step": 52920
    },
    {
      "epoch": 1.7643333333333333,
      "grad_norm": 0.08761735260486603,
      "learning_rate": 3.897291666666667e-05,
      "loss": 0.0026,
      "step": 52930
    },
    {
      "epoch": 1.7646666666666668,
      "grad_norm": 0.058543216437101364,
      "learning_rate": 3.8970833333333336e-05,
      "loss": 0.0018,
      "step": 52940
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 0.3210310637950897,
      "learning_rate": 3.896875e-05,
      "loss": 0.0018,
      "step": 52950
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.11687582731246948,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.003,
      "step": 52960
    },
    {
      "epoch": 1.7656666666666667,
      "grad_norm": 0.14635545015335083,
      "learning_rate": 3.896458333333334e-05,
      "loss": 0.0029,
      "step": 52970
    },
    {
      "epoch": 1.766,
      "grad_norm": 0.14579346776008606,
      "learning_rate": 3.8962500000000005e-05,
      "loss": 0.002,
      "step": 52980
    },
    {
      "epoch": 1.7663333333333333,
      "grad_norm": 0.34115368127822876,
      "learning_rate": 3.8960416666666664e-05,
      "loss": 0.0027,
      "step": 52990
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.05832916498184204,
      "learning_rate": 3.8958333333333336e-05,
      "loss": 0.0022,
      "step": 53000
    },
    {
      "epoch": 1.767,
      "grad_norm": 0.5253026485443115,
      "learning_rate": 3.895625e-05,
      "loss": 0.0021,
      "step": 53010
    },
    {
      "epoch": 1.7673333333333332,
      "grad_norm": 0.1450565755367279,
      "learning_rate": 3.895416666666667e-05,
      "loss": 0.003,
      "step": 53020
    },
    {
      "epoch": 1.7676666666666667,
      "grad_norm": 0.14613556861877441,
      "learning_rate": 3.895208333333333e-05,
      "loss": 0.002,
      "step": 53030
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.3611299395561218,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 0.0036,
      "step": 53040
    },
    {
      "epoch": 1.7683333333333333,
      "grad_norm": 0.14600291848182678,
      "learning_rate": 3.894791666666667e-05,
      "loss": 0.0025,
      "step": 53050
    },
    {
      "epoch": 1.7686666666666668,
      "grad_norm": 0.03262070566415787,
      "learning_rate": 3.8945833333333336e-05,
      "loss": 0.0026,
      "step": 53060
    },
    {
      "epoch": 1.7690000000000001,
      "grad_norm": 0.23388515412807465,
      "learning_rate": 3.894375e-05,
      "loss": 0.0024,
      "step": 53070
    },
    {
      "epoch": 1.7693333333333334,
      "grad_norm": 0.08785869926214218,
      "learning_rate": 3.8941666666666674e-05,
      "loss": 0.003,
      "step": 53080
    },
    {
      "epoch": 1.7696666666666667,
      "grad_norm": 0.2920520305633545,
      "learning_rate": 3.893958333333333e-05,
      "loss": 0.0017,
      "step": 53090
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.08775026351213455,
      "learning_rate": 3.8937500000000005e-05,
      "loss": 0.0024,
      "step": 53100
    },
    {
      "epoch": 1.7703333333333333,
      "grad_norm": 0.2044864147901535,
      "learning_rate": 3.893541666666667e-05,
      "loss": 0.0025,
      "step": 53110
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.43368616700172424,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0027,
      "step": 53120
    },
    {
      "epoch": 1.771,
      "grad_norm": 0.005037327762693167,
      "learning_rate": 3.893125e-05,
      "loss": 0.0026,
      "step": 53130
    },
    {
      "epoch": 1.7713333333333332,
      "grad_norm": 0.17511104047298431,
      "learning_rate": 3.8929166666666666e-05,
      "loss": 0.0027,
      "step": 53140
    },
    {
      "epoch": 1.7716666666666665,
      "grad_norm": 0.06093595176935196,
      "learning_rate": 3.892708333333334e-05,
      "loss": 0.0031,
      "step": 53150
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.23690533638000488,
      "learning_rate": 3.8925e-05,
      "loss": 0.002,
      "step": 53160
    },
    {
      "epoch": 1.7723333333333333,
      "grad_norm": 0.20432284474372864,
      "learning_rate": 3.892291666666667e-05,
      "loss": 0.0016,
      "step": 53170
    },
    {
      "epoch": 1.7726666666666666,
      "grad_norm": 0.08853273838758469,
      "learning_rate": 3.8920833333333335e-05,
      "loss": 0.0025,
      "step": 53180
    },
    {
      "epoch": 1.7730000000000001,
      "grad_norm": 0.4961714446544647,
      "learning_rate": 3.891875e-05,
      "loss": 0.0026,
      "step": 53190
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.029974009841680527,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.0016,
      "step": 53200
    },
    {
      "epoch": 1.7736666666666667,
      "grad_norm": 0.37959688901901245,
      "learning_rate": 3.891458333333334e-05,
      "loss": 0.0032,
      "step": 53210
    },
    {
      "epoch": 1.774,
      "grad_norm": 0.23365074396133423,
      "learning_rate": 3.8912500000000004e-05,
      "loss": 0.0027,
      "step": 53220
    },
    {
      "epoch": 1.7743333333333333,
      "grad_norm": 0.29201701283454895,
      "learning_rate": 3.891041666666666e-05,
      "loss": 0.0018,
      "step": 53230
    },
    {
      "epoch": 1.7746666666666666,
      "grad_norm": 0.07445497810840607,
      "learning_rate": 3.8908333333333335e-05,
      "loss": 0.0026,
      "step": 53240
    },
    {
      "epoch": 1.775,
      "grad_norm": 0.08763298392295837,
      "learning_rate": 3.890625e-05,
      "loss": 0.0025,
      "step": 53250
    },
    {
      "epoch": 1.7753333333333332,
      "grad_norm": 0.35042619705200195,
      "learning_rate": 3.890416666666667e-05,
      "loss": 0.0026,
      "step": 53260
    },
    {
      "epoch": 1.7756666666666665,
      "grad_norm": 0.08781209588050842,
      "learning_rate": 3.890208333333333e-05,
      "loss": 0.002,
      "step": 53270
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.25166288018226624,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.0033,
      "step": 53280
    },
    {
      "epoch": 1.7763333333333333,
      "grad_norm": 0.08730290830135345,
      "learning_rate": 3.889791666666667e-05,
      "loss": 0.0029,
      "step": 53290
    },
    {
      "epoch": 1.7766666666666666,
      "grad_norm": 0.5835679173469543,
      "learning_rate": 3.8895833333333335e-05,
      "loss": 0.0016,
      "step": 53300
    },
    {
      "epoch": 1.7770000000000001,
      "grad_norm": 0.08761235326528549,
      "learning_rate": 3.889375e-05,
      "loss": 0.0037,
      "step": 53310
    },
    {
      "epoch": 1.7773333333333334,
      "grad_norm": 0.5545188188552856,
      "learning_rate": 3.889166666666667e-05,
      "loss": 0.0026,
      "step": 53320
    },
    {
      "epoch": 1.7776666666666667,
      "grad_norm": 0.058525677770376205,
      "learning_rate": 3.888958333333334e-05,
      "loss": 0.0025,
      "step": 53330
    },
    {
      "epoch": 1.778,
      "grad_norm": 0.11659640818834305,
      "learning_rate": 3.88875e-05,
      "loss": 0.0023,
      "step": 53340
    },
    {
      "epoch": 1.7783333333333333,
      "grad_norm": 0.11681760847568512,
      "learning_rate": 3.888541666666667e-05,
      "loss": 0.0022,
      "step": 53350
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.2331799417734146,
      "learning_rate": 3.8883333333333334e-05,
      "loss": 0.0024,
      "step": 53360
    },
    {
      "epoch": 1.779,
      "grad_norm": 0.029552415013313293,
      "learning_rate": 3.888125e-05,
      "loss": 0.0019,
      "step": 53370
    },
    {
      "epoch": 1.7793333333333332,
      "grad_norm": 0.3224326968193054,
      "learning_rate": 3.8879166666666665e-05,
      "loss": 0.0026,
      "step": 53380
    },
    {
      "epoch": 1.7796666666666665,
      "grad_norm": 0.262029767036438,
      "learning_rate": 3.887708333333334e-05,
      "loss": 0.0024,
      "step": 53390
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.17507939040660858,
      "learning_rate": 3.8875e-05,
      "loss": 0.0039,
      "step": 53400
    },
    {
      "epoch": 1.7803333333333333,
      "grad_norm": 0.058222874999046326,
      "learning_rate": 3.887291666666667e-05,
      "loss": 0.0024,
      "step": 53410
    },
    {
      "epoch": 1.7806666666666666,
      "grad_norm": 0.23324422538280487,
      "learning_rate": 3.8870833333333334e-05,
      "loss": 0.0032,
      "step": 53420
    },
    {
      "epoch": 1.7810000000000001,
      "grad_norm": 0.6317700147628784,
      "learning_rate": 3.8868750000000006e-05,
      "loss": 0.0032,
      "step": 53430
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.34998568892478943,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0023,
      "step": 53440
    },
    {
      "epoch": 1.7816666666666667,
      "grad_norm": 0.05854975804686546,
      "learning_rate": 3.886458333333334e-05,
      "loss": 0.0031,
      "step": 53450
    },
    {
      "epoch": 1.782,
      "grad_norm": 0.11686190962791443,
      "learning_rate": 3.88625e-05,
      "loss": 0.0028,
      "step": 53460
    },
    {
      "epoch": 1.7823333333333333,
      "grad_norm": 0.2039240598678589,
      "learning_rate": 3.886041666666667e-05,
      "loss": 0.002,
      "step": 53470
    },
    {
      "epoch": 1.7826666666666666,
      "grad_norm": 0.6422922611236572,
      "learning_rate": 3.8858333333333334e-05,
      "loss": 0.0029,
      "step": 53480
    },
    {
      "epoch": 1.783,
      "grad_norm": 0.0603017583489418,
      "learning_rate": 3.885625e-05,
      "loss": 0.0025,
      "step": 53490
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.08767833560705185,
      "learning_rate": 3.885416666666667e-05,
      "loss": 0.0015,
      "step": 53500
    },
    {
      "epoch": 1.7836666666666665,
      "grad_norm": 0.005073178093880415,
      "learning_rate": 3.885208333333333e-05,
      "loss": 0.0034,
      "step": 53510
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.08763617277145386,
      "learning_rate": 3.885e-05,
      "loss": 0.0026,
      "step": 53520
    },
    {
      "epoch": 1.7843333333333333,
      "grad_norm": 0.637639582157135,
      "learning_rate": 3.884791666666667e-05,
      "loss": 0.0017,
      "step": 53530
    },
    {
      "epoch": 1.7846666666666666,
      "grad_norm": 0.17546437680721283,
      "learning_rate": 3.884583333333334e-05,
      "loss": 0.0024,
      "step": 53540
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 0.35025593638420105,
      "learning_rate": 3.884375e-05,
      "loss": 0.0017,
      "step": 53550
    },
    {
      "epoch": 1.7853333333333334,
      "grad_norm": 0.4375927746295929,
      "learning_rate": 3.884166666666667e-05,
      "loss": 0.0022,
      "step": 53560
    },
    {
      "epoch": 1.7856666666666667,
      "grad_norm": 0.029368828982114792,
      "learning_rate": 3.883958333333334e-05,
      "loss": 0.0025,
      "step": 53570
    },
    {
      "epoch": 1.786,
      "grad_norm": 0.17568723857402802,
      "learning_rate": 3.88375e-05,
      "loss": 0.0026,
      "step": 53580
    },
    {
      "epoch": 1.7863333333333333,
      "grad_norm": 0.11699486523866653,
      "learning_rate": 3.883541666666667e-05,
      "loss": 0.0022,
      "step": 53590
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.1175527349114418,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0023,
      "step": 53600
    },
    {
      "epoch": 1.787,
      "grad_norm": 0.20407046377658844,
      "learning_rate": 3.8831250000000005e-05,
      "loss": 0.0017,
      "step": 53610
    },
    {
      "epoch": 1.7873333333333332,
      "grad_norm": 0.20393574237823486,
      "learning_rate": 3.8829166666666664e-05,
      "loss": 0.0032,
      "step": 53620
    },
    {
      "epoch": 1.7876666666666665,
      "grad_norm": 0.9202257990837097,
      "learning_rate": 3.8827083333333336e-05,
      "loss": 0.0022,
      "step": 53630
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.17522601783275604,
      "learning_rate": 3.8825e-05,
      "loss": 0.0019,
      "step": 53640
    },
    {
      "epoch": 1.7883333333333333,
      "grad_norm": 0.2672850489616394,
      "learning_rate": 3.882291666666667e-05,
      "loss": 0.0037,
      "step": 53650
    },
    {
      "epoch": 1.7886666666666666,
      "grad_norm": 0.40863463282585144,
      "learning_rate": 3.882083333333333e-05,
      "loss": 0.0023,
      "step": 53660
    },
    {
      "epoch": 1.7890000000000001,
      "grad_norm": 0.2919239103794098,
      "learning_rate": 3.8818750000000005e-05,
      "loss": 0.0031,
      "step": 53670
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.11671193689107895,
      "learning_rate": 3.881666666666667e-05,
      "loss": 0.0023,
      "step": 53680
    },
    {
      "epoch": 1.7896666666666667,
      "grad_norm": 0.5252167582511902,
      "learning_rate": 3.8814583333333336e-05,
      "loss": 0.0026,
      "step": 53690
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.233384370803833,
      "learning_rate": 3.88125e-05,
      "loss": 0.0017,
      "step": 53700
    },
    {
      "epoch": 1.7903333333333333,
      "grad_norm": 0.28761667013168335,
      "learning_rate": 3.881041666666667e-05,
      "loss": 0.0028,
      "step": 53710
    },
    {
      "epoch": 1.7906666666666666,
      "grad_norm": 0.058486469089984894,
      "learning_rate": 3.880833333333333e-05,
      "loss": 0.0016,
      "step": 53720
    },
    {
      "epoch": 1.791,
      "grad_norm": 0.20405080914497375,
      "learning_rate": 3.880625e-05,
      "loss": 0.0024,
      "step": 53730
    },
    {
      "epoch": 1.7913333333333332,
      "grad_norm": 0.43735817074775696,
      "learning_rate": 3.880416666666667e-05,
      "loss": 0.0019,
      "step": 53740
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.11656148731708527,
      "learning_rate": 3.8802083333333336e-05,
      "loss": 0.0023,
      "step": 53750
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.34970828890800476,
      "learning_rate": 3.88e-05,
      "loss": 0.0018,
      "step": 53760
    },
    {
      "epoch": 1.7923333333333333,
      "grad_norm": 0.3960224986076355,
      "learning_rate": 3.879791666666667e-05,
      "loss": 0.0032,
      "step": 53770
    },
    {
      "epoch": 1.7926666666666666,
      "grad_norm": 0.11663680523633957,
      "learning_rate": 3.879583333333334e-05,
      "loss": 0.0021,
      "step": 53780
    },
    {
      "epoch": 1.7930000000000001,
      "grad_norm": 0.23326179385185242,
      "learning_rate": 3.879375e-05,
      "loss": 0.0027,
      "step": 53790
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.1750878244638443,
      "learning_rate": 3.879166666666667e-05,
      "loss": 0.0028,
      "step": 53800
    },
    {
      "epoch": 1.7936666666666667,
      "grad_norm": 0.11711359769105911,
      "learning_rate": 3.8789583333333335e-05,
      "loss": 0.0023,
      "step": 53810
    },
    {
      "epoch": 1.794,
      "grad_norm": 0.4376603364944458,
      "learning_rate": 3.878750000000001e-05,
      "loss": 0.0035,
      "step": 53820
    },
    {
      "epoch": 1.7943333333333333,
      "grad_norm": 0.4334162175655365,
      "learning_rate": 3.8785416666666666e-05,
      "loss": 0.0026,
      "step": 53830
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.5942288041114807,
      "learning_rate": 3.878333333333333e-05,
      "loss": 0.0028,
      "step": 53840
    },
    {
      "epoch": 1.795,
      "grad_norm": 0.2622651755809784,
      "learning_rate": 3.8781250000000004e-05,
      "loss": 0.0031,
      "step": 53850
    },
    {
      "epoch": 1.7953333333333332,
      "grad_norm": 0.6253926753997803,
      "learning_rate": 3.877916666666667e-05,
      "loss": 0.0019,
      "step": 53860
    },
    {
      "epoch": 1.7956666666666665,
      "grad_norm": 0.0883423388004303,
      "learning_rate": 3.8777083333333335e-05,
      "loss": 0.002,
      "step": 53870
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.2040620595216751,
      "learning_rate": 3.8775e-05,
      "loss": 0.0034,
      "step": 53880
    },
    {
      "epoch": 1.7963333333333333,
      "grad_norm": 0.00408724183216691,
      "learning_rate": 3.877291666666667e-05,
      "loss": 0.0042,
      "step": 53890
    },
    {
      "epoch": 1.7966666666666666,
      "grad_norm": 0.4664629399776459,
      "learning_rate": 3.877083333333333e-05,
      "loss": 0.0024,
      "step": 53900
    },
    {
      "epoch": 1.7970000000000002,
      "grad_norm": 0.2635265290737152,
      "learning_rate": 3.8768750000000004e-05,
      "loss": 0.0027,
      "step": 53910
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.2919149696826935,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.0021,
      "step": 53920
    },
    {
      "epoch": 1.7976666666666667,
      "grad_norm": 0.006452099420130253,
      "learning_rate": 3.8764583333333335e-05,
      "loss": 0.0014,
      "step": 53930
    },
    {
      "epoch": 1.798,
      "grad_norm": 0.08766520023345947,
      "learning_rate": 3.87625e-05,
      "loss": 0.0025,
      "step": 53940
    },
    {
      "epoch": 1.7983333333333333,
      "grad_norm": 0.5254141688346863,
      "learning_rate": 3.876041666666667e-05,
      "loss": 0.0021,
      "step": 53950
    },
    {
      "epoch": 1.7986666666666666,
      "grad_norm": 0.1754114329814911,
      "learning_rate": 3.875833333333334e-05,
      "loss": 0.0032,
      "step": 53960
    },
    {
      "epoch": 1.799,
      "grad_norm": 0.5249765515327454,
      "learning_rate": 3.875625e-05,
      "loss": 0.0025,
      "step": 53970
    },
    {
      "epoch": 1.7993333333333332,
      "grad_norm": 0.40915921330451965,
      "learning_rate": 3.875416666666667e-05,
      "loss": 0.0031,
      "step": 53980
    },
    {
      "epoch": 1.7996666666666665,
      "grad_norm": 0.23398061096668243,
      "learning_rate": 3.8752083333333335e-05,
      "loss": 0.0033,
      "step": 53990
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.03002709150314331,
      "learning_rate": 3.875e-05,
      "loss": 0.0019,
      "step": 54000
    },
    {
      "epoch": 1.8003333333333333,
      "grad_norm": 0.29145368933677673,
      "learning_rate": 3.8747916666666665e-05,
      "loss": 0.002,
      "step": 54010
    },
    {
      "epoch": 1.8006666666666666,
      "grad_norm": 0.11023376137018204,
      "learning_rate": 3.874583333333334e-05,
      "loss": 0.0024,
      "step": 54020
    },
    {
      "epoch": 1.8010000000000002,
      "grad_norm": 0.05872895196080208,
      "learning_rate": 3.874375e-05,
      "loss": 0.0024,
      "step": 54030
    },
    {
      "epoch": 1.8013333333333335,
      "grad_norm": 0.08910442143678665,
      "learning_rate": 3.874166666666667e-05,
      "loss": 0.0031,
      "step": 54040
    },
    {
      "epoch": 1.8016666666666667,
      "grad_norm": 0.2919006943702698,
      "learning_rate": 3.8739583333333334e-05,
      "loss": 0.0027,
      "step": 54050
    },
    {
      "epoch": 1.802,
      "grad_norm": 0.1752471625804901,
      "learning_rate": 3.8737500000000006e-05,
      "loss": 0.0019,
      "step": 54060
    },
    {
      "epoch": 1.8023333333333333,
      "grad_norm": 0.20619873702526093,
      "learning_rate": 3.8735416666666665e-05,
      "loss": 0.0017,
      "step": 54070
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.26218119263648987,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0027,
      "step": 54080
    },
    {
      "epoch": 1.803,
      "grad_norm": 0.11672689765691757,
      "learning_rate": 3.873125e-05,
      "loss": 0.0023,
      "step": 54090
    },
    {
      "epoch": 1.8033333333333332,
      "grad_norm": 0.4670300781726837,
      "learning_rate": 3.872916666666667e-05,
      "loss": 0.0028,
      "step": 54100
    },
    {
      "epoch": 1.8036666666666665,
      "grad_norm": 0.02955450303852558,
      "learning_rate": 3.8727083333333334e-05,
      "loss": 0.0023,
      "step": 54110
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.291778564453125,
      "learning_rate": 3.8725e-05,
      "loss": 0.0029,
      "step": 54120
    },
    {
      "epoch": 1.8043333333333333,
      "grad_norm": 0.23355312645435333,
      "learning_rate": 3.872291666666667e-05,
      "loss": 0.0031,
      "step": 54130
    },
    {
      "epoch": 1.8046666666666666,
      "grad_norm": 0.207981675863266,
      "learning_rate": 3.872083333333334e-05,
      "loss": 0.0019,
      "step": 54140
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 0.2630584239959717,
      "learning_rate": 3.871875e-05,
      "loss": 0.0021,
      "step": 54150
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.4080369174480438,
      "learning_rate": 3.871666666666667e-05,
      "loss": 0.0021,
      "step": 54160
    },
    {
      "epoch": 1.8056666666666668,
      "grad_norm": 0.08764323592185974,
      "learning_rate": 3.871458333333334e-05,
      "loss": 0.0029,
      "step": 54170
    },
    {
      "epoch": 1.806,
      "grad_norm": 0.0888715609908104,
      "learning_rate": 3.87125e-05,
      "loss": 0.0025,
      "step": 54180
    },
    {
      "epoch": 1.8063333333333333,
      "grad_norm": 0.17494943737983704,
      "learning_rate": 3.871041666666667e-05,
      "loss": 0.0023,
      "step": 54190
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.26260024309158325,
      "learning_rate": 3.870833333333334e-05,
      "loss": 0.0026,
      "step": 54200
    },
    {
      "epoch": 1.807,
      "grad_norm": 0.0877002477645874,
      "learning_rate": 3.870625e-05,
      "loss": 0.0017,
      "step": 54210
    },
    {
      "epoch": 1.8073333333333332,
      "grad_norm": 0.6995211839675903,
      "learning_rate": 3.870416666666667e-05,
      "loss": 0.0024,
      "step": 54220
    },
    {
      "epoch": 1.8076666666666665,
      "grad_norm": 0.08972474932670593,
      "learning_rate": 3.870208333333333e-05,
      "loss": 0.0019,
      "step": 54230
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.17460913956165314,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0026,
      "step": 54240
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 0.6995790004730225,
      "learning_rate": 3.8697916666666664e-05,
      "loss": 0.0029,
      "step": 54250
    },
    {
      "epoch": 1.8086666666666666,
      "grad_norm": 0.29140982031822205,
      "learning_rate": 3.8695833333333337e-05,
      "loss": 0.0019,
      "step": 54260
    },
    {
      "epoch": 1.8090000000000002,
      "grad_norm": 0.3793007433414459,
      "learning_rate": 3.869375e-05,
      "loss": 0.002,
      "step": 54270
    },
    {
      "epoch": 1.8093333333333335,
      "grad_norm": 0.030324241146445274,
      "learning_rate": 3.869166666666667e-05,
      "loss": 0.0029,
      "step": 54280
    },
    {
      "epoch": 1.8096666666666668,
      "grad_norm": 0.11686664819717407,
      "learning_rate": 3.868958333333333e-05,
      "loss": 0.0022,
      "step": 54290
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.6416094899177551,
      "learning_rate": 3.8687500000000005e-05,
      "loss": 0.0029,
      "step": 54300
    },
    {
      "epoch": 1.8103333333333333,
      "grad_norm": 0.17505668103694916,
      "learning_rate": 3.868541666666667e-05,
      "loss": 0.0027,
      "step": 54310
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.24178479611873627,
      "learning_rate": 3.868333333333333e-05,
      "loss": 0.0023,
      "step": 54320
    },
    {
      "epoch": 1.811,
      "grad_norm": 0.05903046578168869,
      "learning_rate": 3.868125e-05,
      "loss": 0.0024,
      "step": 54330
    },
    {
      "epoch": 1.8113333333333332,
      "grad_norm": 0.5243100523948669,
      "learning_rate": 3.867916666666667e-05,
      "loss": 0.0018,
      "step": 54340
    },
    {
      "epoch": 1.8116666666666665,
      "grad_norm": 0.20402108132839203,
      "learning_rate": 3.867708333333333e-05,
      "loss": 0.0022,
      "step": 54350
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.08716435730457306,
      "learning_rate": 3.8675e-05,
      "loss": 0.0022,
      "step": 54360
    },
    {
      "epoch": 1.8123333333333334,
      "grad_norm": 0.2622487545013428,
      "learning_rate": 3.867291666666667e-05,
      "loss": 0.0017,
      "step": 54370
    },
    {
      "epoch": 1.8126666666666666,
      "grad_norm": 0.1457045078277588,
      "learning_rate": 3.8670833333333336e-05,
      "loss": 0.0016,
      "step": 54380
    },
    {
      "epoch": 1.813,
      "grad_norm": 0.08758581429719925,
      "learning_rate": 3.866875e-05,
      "loss": 0.0023,
      "step": 54390
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.5537517070770264,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0032,
      "step": 54400
    },
    {
      "epoch": 1.8136666666666668,
      "grad_norm": 0.05852452665567398,
      "learning_rate": 3.866458333333334e-05,
      "loss": 0.0029,
      "step": 54410
    },
    {
      "epoch": 1.814,
      "grad_norm": 0.029224127531051636,
      "learning_rate": 3.8662500000000005e-05,
      "loss": 0.0026,
      "step": 54420
    },
    {
      "epoch": 1.8143333333333334,
      "grad_norm": 0.05832010135054588,
      "learning_rate": 3.866041666666667e-05,
      "loss": 0.0031,
      "step": 54430
    },
    {
      "epoch": 1.8146666666666667,
      "grad_norm": 0.007601411547511816,
      "learning_rate": 3.8658333333333336e-05,
      "loss": 0.002,
      "step": 54440
    },
    {
      "epoch": 1.815,
      "grad_norm": 0.05907553806900978,
      "learning_rate": 3.865625e-05,
      "loss": 0.0029,
      "step": 54450
    },
    {
      "epoch": 1.8153333333333332,
      "grad_norm": 0.2973579466342926,
      "learning_rate": 3.8654166666666667e-05,
      "loss": 0.0026,
      "step": 54460
    },
    {
      "epoch": 1.8156666666666665,
      "grad_norm": 0.5829558968544006,
      "learning_rate": 3.865208333333333e-05,
      "loss": 0.0033,
      "step": 54470
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.08773626387119293,
      "learning_rate": 3.8650000000000004e-05,
      "loss": 0.0027,
      "step": 54480
    },
    {
      "epoch": 1.8163333333333334,
      "grad_norm": 1.1646593809127808,
      "learning_rate": 3.864791666666667e-05,
      "loss": 0.002,
      "step": 54490
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.7196712493896484,
      "learning_rate": 3.8645833333333335e-05,
      "loss": 0.0044,
      "step": 54500
    },
    {
      "epoch": 1.817,
      "grad_norm": 0.08756141364574432,
      "learning_rate": 3.864375e-05,
      "loss": 0.0024,
      "step": 54510
    },
    {
      "epoch": 1.8173333333333335,
      "grad_norm": 0.5535578727722168,
      "learning_rate": 3.864166666666667e-05,
      "loss": 0.0024,
      "step": 54520
    },
    {
      "epoch": 1.8176666666666668,
      "grad_norm": 0.4376547932624817,
      "learning_rate": 3.863958333333333e-05,
      "loss": 0.0014,
      "step": 54530
    },
    {
      "epoch": 1.818,
      "grad_norm": 0.005435505881905556,
      "learning_rate": 3.8637500000000004e-05,
      "loss": 0.0033,
      "step": 54540
    },
    {
      "epoch": 1.8183333333333334,
      "grad_norm": 0.17488886415958405,
      "learning_rate": 3.863541666666667e-05,
      "loss": 0.0029,
      "step": 54550
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.03811721131205559,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.0029,
      "step": 54560
    },
    {
      "epoch": 1.819,
      "grad_norm": 0.08774428814649582,
      "learning_rate": 3.863125e-05,
      "loss": 0.0029,
      "step": 54570
    },
    {
      "epoch": 1.8193333333333332,
      "grad_norm": 0.16997136175632477,
      "learning_rate": 3.8629166666666666e-05,
      "loss": 0.0038,
      "step": 54580
    },
    {
      "epoch": 1.8196666666666665,
      "grad_norm": 0.2331424504518509,
      "learning_rate": 3.862708333333334e-05,
      "loss": 0.0042,
      "step": 54590
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.29169294238090515,
      "learning_rate": 3.8625e-05,
      "loss": 0.0022,
      "step": 54600
    },
    {
      "epoch": 1.8203333333333334,
      "grad_norm": 0.029284745454788208,
      "learning_rate": 3.862291666666667e-05,
      "loss": 0.0026,
      "step": 54610
    },
    {
      "epoch": 1.8206666666666667,
      "grad_norm": 0.11680493503808975,
      "learning_rate": 3.8620833333333335e-05,
      "loss": 0.0027,
      "step": 54620
    },
    {
      "epoch": 1.821,
      "grad_norm": 0.1748088151216507,
      "learning_rate": 3.861875e-05,
      "loss": 0.0039,
      "step": 54630
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.20426036417484283,
      "learning_rate": 3.8616666666666666e-05,
      "loss": 0.0025,
      "step": 54640
    },
    {
      "epoch": 1.8216666666666668,
      "grad_norm": 0.11667905747890472,
      "learning_rate": 3.861458333333334e-05,
      "loss": 0.0021,
      "step": 54650
    },
    {
      "epoch": 1.822,
      "grad_norm": 0.17496804893016815,
      "learning_rate": 3.8612500000000003e-05,
      "loss": 0.0025,
      "step": 54660
    },
    {
      "epoch": 1.8223333333333334,
      "grad_norm": 0.009101053699851036,
      "learning_rate": 3.861041666666667e-05,
      "loss": 0.0025,
      "step": 54670
    },
    {
      "epoch": 1.8226666666666667,
      "grad_norm": 0.43712180852890015,
      "learning_rate": 3.8608333333333334e-05,
      "loss": 0.0028,
      "step": 54680
    },
    {
      "epoch": 1.823,
      "grad_norm": 0.0056340573355555534,
      "learning_rate": 3.860625e-05,
      "loss": 0.0023,
      "step": 54690
    },
    {
      "epoch": 1.8233333333333333,
      "grad_norm": 0.030227655544877052,
      "learning_rate": 3.860416666666667e-05,
      "loss": 0.0021,
      "step": 54700
    },
    {
      "epoch": 1.8236666666666665,
      "grad_norm": 0.05950147658586502,
      "learning_rate": 3.860208333333333e-05,
      "loss": 0.0029,
      "step": 54710
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.058814164251089096,
      "learning_rate": 3.86e-05,
      "loss": 0.0022,
      "step": 54720
    },
    {
      "epoch": 1.8243333333333334,
      "grad_norm": 0.11664501577615738,
      "learning_rate": 3.859791666666667e-05,
      "loss": 0.0038,
      "step": 54730
    },
    {
      "epoch": 1.8246666666666667,
      "grad_norm": 0.14589077234268188,
      "learning_rate": 3.8595833333333334e-05,
      "loss": 0.0024,
      "step": 54740
    },
    {
      "epoch": 1.825,
      "grad_norm": 0.08849699050188065,
      "learning_rate": 3.859375e-05,
      "loss": 0.0019,
      "step": 54750
    },
    {
      "epoch": 1.8253333333333335,
      "grad_norm": 0.2623721659183502,
      "learning_rate": 3.859166666666667e-05,
      "loss": 0.0027,
      "step": 54760
    },
    {
      "epoch": 1.8256666666666668,
      "grad_norm": 0.08729643374681473,
      "learning_rate": 3.858958333333334e-05,
      "loss": 0.0024,
      "step": 54770
    },
    {
      "epoch": 1.826,
      "grad_norm": 0.20392736792564392,
      "learning_rate": 3.85875e-05,
      "loss": 0.0026,
      "step": 54780
    },
    {
      "epoch": 1.8263333333333334,
      "grad_norm": 0.005314467009156942,
      "learning_rate": 3.858541666666667e-05,
      "loss": 0.0034,
      "step": 54790
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.4369959533214569,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.0025,
      "step": 54800
    },
    {
      "epoch": 1.827,
      "grad_norm": 0.0294415932148695,
      "learning_rate": 3.858125e-05,
      "loss": 0.0026,
      "step": 54810
    },
    {
      "epoch": 1.8273333333333333,
      "grad_norm": 0.320271760225296,
      "learning_rate": 3.8579166666666665e-05,
      "loss": 0.0028,
      "step": 54820
    },
    {
      "epoch": 1.8276666666666666,
      "grad_norm": 0.262518972158432,
      "learning_rate": 3.857708333333334e-05,
      "loss": 0.0036,
      "step": 54830
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.145589679479599,
      "learning_rate": 3.8575e-05,
      "loss": 0.0034,
      "step": 54840
    },
    {
      "epoch": 1.8283333333333334,
      "grad_norm": 0.320610374212265,
      "learning_rate": 3.857291666666667e-05,
      "loss": 0.003,
      "step": 54850
    },
    {
      "epoch": 1.8286666666666667,
      "grad_norm": 0.14549949765205383,
      "learning_rate": 3.8570833333333333e-05,
      "loss": 0.0027,
      "step": 54860
    },
    {
      "epoch": 1.829,
      "grad_norm": 0.40762603282928467,
      "learning_rate": 3.8568750000000006e-05,
      "loss": 0.0019,
      "step": 54870
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.46225014328956604,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.0026,
      "step": 54880
    },
    {
      "epoch": 1.8296666666666668,
      "grad_norm": 0.03000580705702305,
      "learning_rate": 3.856458333333334e-05,
      "loss": 0.0027,
      "step": 54890
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.5574601292610168,
      "learning_rate": 3.85625e-05,
      "loss": 0.0026,
      "step": 54900
    },
    {
      "epoch": 1.8303333333333334,
      "grad_norm": 0.3496982753276825,
      "learning_rate": 3.856041666666667e-05,
      "loss": 0.0033,
      "step": 54910
    },
    {
      "epoch": 1.8306666666666667,
      "grad_norm": 0.29145607352256775,
      "learning_rate": 3.855833333333333e-05,
      "loss": 0.0018,
      "step": 54920
    },
    {
      "epoch": 1.831,
      "grad_norm": 0.3791860044002533,
      "learning_rate": 3.855625e-05,
      "loss": 0.003,
      "step": 54930
    },
    {
      "epoch": 1.8313333333333333,
      "grad_norm": 0.0874701589345932,
      "learning_rate": 3.855416666666667e-05,
      "loss": 0.0017,
      "step": 54940
    },
    {
      "epoch": 1.8316666666666666,
      "grad_norm": 0.5436325073242188,
      "learning_rate": 3.8552083333333336e-05,
      "loss": 0.0033,
      "step": 54950
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.029591582715511322,
      "learning_rate": 3.855e-05,
      "loss": 0.0031,
      "step": 54960
    },
    {
      "epoch": 1.8323333333333334,
      "grad_norm": 0.0031979235354810953,
      "learning_rate": 3.854791666666667e-05,
      "loss": 0.0025,
      "step": 54970
    },
    {
      "epoch": 1.8326666666666667,
      "grad_norm": 0.5248538255691528,
      "learning_rate": 3.854583333333334e-05,
      "loss": 0.0025,
      "step": 54980
    },
    {
      "epoch": 1.833,
      "grad_norm": 0.6700670123100281,
      "learning_rate": 3.854375e-05,
      "loss": 0.0019,
      "step": 54990
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.5244282484054565,
      "learning_rate": 3.854166666666667e-05,
      "loss": 0.0047,
      "step": 55000
    },
    {
      "epoch": 1.8336666666666668,
      "grad_norm": 0.291143536567688,
      "learning_rate": 3.8539583333333336e-05,
      "loss": 0.0017,
      "step": 55010
    },
    {
      "epoch": 1.834,
      "grad_norm": 0.20396554470062256,
      "learning_rate": 3.85375e-05,
      "loss": 0.0026,
      "step": 55020
    },
    {
      "epoch": 1.8343333333333334,
      "grad_norm": 0.262191504240036,
      "learning_rate": 3.853541666666667e-05,
      "loss": 0.0036,
      "step": 55030
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.7278542518615723,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.002,
      "step": 55040
    },
    {
      "epoch": 1.835,
      "grad_norm": 0.14573274552822113,
      "learning_rate": 3.8531250000000005e-05,
      "loss": 0.0019,
      "step": 55050
    },
    {
      "epoch": 1.8353333333333333,
      "grad_norm": 0.5241814851760864,
      "learning_rate": 3.8529166666666664e-05,
      "loss": 0.0021,
      "step": 55060
    },
    {
      "epoch": 1.8356666666666666,
      "grad_norm": 0.00291166128590703,
      "learning_rate": 3.8527083333333336e-05,
      "loss": 0.0018,
      "step": 55070
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.2327273190021515,
      "learning_rate": 3.8525e-05,
      "loss": 0.0026,
      "step": 55080
    },
    {
      "epoch": 1.8363333333333334,
      "grad_norm": 0.02968243509531021,
      "learning_rate": 3.852291666666667e-05,
      "loss": 0.0031,
      "step": 55090
    },
    {
      "epoch": 1.8366666666666667,
      "grad_norm": 0.0872516855597496,
      "learning_rate": 3.852083333333333e-05,
      "loss": 0.003,
      "step": 55100
    },
    {
      "epoch": 1.837,
      "grad_norm": 0.1749366968870163,
      "learning_rate": 3.8518750000000005e-05,
      "loss": 0.0023,
      "step": 55110
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.05871082842350006,
      "learning_rate": 3.851666666666667e-05,
      "loss": 0.0029,
      "step": 55120
    },
    {
      "epoch": 1.8376666666666668,
      "grad_norm": 0.0034364734310656786,
      "learning_rate": 3.8514583333333336e-05,
      "loss": 0.0024,
      "step": 55130
    },
    {
      "epoch": 1.838,
      "grad_norm": 0.2621725797653198,
      "learning_rate": 3.85125e-05,
      "loss": 0.0024,
      "step": 55140
    },
    {
      "epoch": 1.8383333333333334,
      "grad_norm": 0.1460086852312088,
      "learning_rate": 3.851041666666667e-05,
      "loss": 0.0027,
      "step": 55150
    },
    {
      "epoch": 1.8386666666666667,
      "grad_norm": 0.0045668282546103,
      "learning_rate": 3.850833333333333e-05,
      "loss": 0.0023,
      "step": 55160
    },
    {
      "epoch": 1.839,
      "grad_norm": 0.2332909107208252,
      "learning_rate": 3.850625e-05,
      "loss": 0.0026,
      "step": 55170
    },
    {
      "epoch": 1.8393333333333333,
      "grad_norm": 0.14595046639442444,
      "learning_rate": 3.850416666666667e-05,
      "loss": 0.0019,
      "step": 55180
    },
    {
      "epoch": 1.8396666666666666,
      "grad_norm": 0.1751192957162857,
      "learning_rate": 3.8502083333333335e-05,
      "loss": 0.0023,
      "step": 55190
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.3819873332977295,
      "learning_rate": 3.85e-05,
      "loss": 0.0024,
      "step": 55200
    },
    {
      "epoch": 1.8403333333333334,
      "grad_norm": 0.02985229529440403,
      "learning_rate": 3.8497916666666666e-05,
      "loss": 0.0031,
      "step": 55210
    },
    {
      "epoch": 1.8406666666666667,
      "grad_norm": 0.26225510239601135,
      "learning_rate": 3.849583333333334e-05,
      "loss": 0.0021,
      "step": 55220
    },
    {
      "epoch": 1.841,
      "grad_norm": 0.02995925024151802,
      "learning_rate": 3.8493750000000004e-05,
      "loss": 0.0024,
      "step": 55230
    },
    {
      "epoch": 1.8413333333333335,
      "grad_norm": 0.2619037926197052,
      "learning_rate": 3.849166666666667e-05,
      "loss": 0.0021,
      "step": 55240
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 0.233005091547966,
      "learning_rate": 3.8489583333333335e-05,
      "loss": 0.0031,
      "step": 55250
    },
    {
      "epoch": 1.842,
      "grad_norm": 0.20388266444206238,
      "learning_rate": 3.848750000000001e-05,
      "loss": 0.0026,
      "step": 55260
    },
    {
      "epoch": 1.8423333333333334,
      "grad_norm": 0.20394942164421082,
      "learning_rate": 3.8485416666666666e-05,
      "loss": 0.0024,
      "step": 55270
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.11660381406545639,
      "learning_rate": 3.848333333333334e-05,
      "loss": 0.0025,
      "step": 55280
    },
    {
      "epoch": 1.843,
      "grad_norm": 0.2333349585533142,
      "learning_rate": 3.8481250000000004e-05,
      "loss": 0.0021,
      "step": 55290
    },
    {
      "epoch": 1.8433333333333333,
      "grad_norm": 0.07883179187774658,
      "learning_rate": 3.847916666666667e-05,
      "loss": 0.0027,
      "step": 55300
    },
    {
      "epoch": 1.8436666666666666,
      "grad_norm": 0.029624009504914284,
      "learning_rate": 3.8477083333333335e-05,
      "loss": 0.0025,
      "step": 55310
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.05849334970116615,
      "learning_rate": 3.8475e-05,
      "loss": 0.0018,
      "step": 55320
    },
    {
      "epoch": 1.8443333333333334,
      "grad_norm": 0.030040662735700607,
      "learning_rate": 3.847291666666667e-05,
      "loss": 0.0034,
      "step": 55330
    },
    {
      "epoch": 1.8446666666666667,
      "grad_norm": 0.08747411519289017,
      "learning_rate": 3.847083333333333e-05,
      "loss": 0.0021,
      "step": 55340
    },
    {
      "epoch": 1.845,
      "grad_norm": 0.1746811717748642,
      "learning_rate": 3.846875e-05,
      "loss": 0.0039,
      "step": 55350
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.11645645648241043,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.002,
      "step": 55360
    },
    {
      "epoch": 1.8456666666666668,
      "grad_norm": 0.0874285027384758,
      "learning_rate": 3.8464583333333334e-05,
      "loss": 0.0021,
      "step": 55370
    },
    {
      "epoch": 1.846,
      "grad_norm": 0.15299953520298004,
      "learning_rate": 3.84625e-05,
      "loss": 0.0023,
      "step": 55380
    },
    {
      "epoch": 1.8463333333333334,
      "grad_norm": 0.08737564086914062,
      "learning_rate": 3.846041666666667e-05,
      "loss": 0.0029,
      "step": 55390
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.7137020230293274,
      "learning_rate": 3.845833333333334e-05,
      "loss": 0.0029,
      "step": 55400
    },
    {
      "epoch": 1.847,
      "grad_norm": 0.3211425542831421,
      "learning_rate": 3.8456249999999996e-05,
      "loss": 0.0028,
      "step": 55410
    },
    {
      "epoch": 1.8473333333333333,
      "grad_norm": 0.20384451746940613,
      "learning_rate": 3.845416666666667e-05,
      "loss": 0.0029,
      "step": 55420
    },
    {
      "epoch": 1.8476666666666666,
      "grad_norm": 0.2912149429321289,
      "learning_rate": 3.8452083333333334e-05,
      "loss": 0.0027,
      "step": 55430
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.08723767101764679,
      "learning_rate": 3.845e-05,
      "loss": 0.0026,
      "step": 55440
    },
    {
      "epoch": 1.8483333333333334,
      "grad_norm": 0.6367120742797852,
      "learning_rate": 3.8447916666666665e-05,
      "loss": 0.0025,
      "step": 55450
    },
    {
      "epoch": 1.8486666666666667,
      "grad_norm": 0.14562788605690002,
      "learning_rate": 3.844583333333334e-05,
      "loss": 0.0028,
      "step": 55460
    },
    {
      "epoch": 1.849,
      "grad_norm": 0.11651776731014252,
      "learning_rate": 3.844375e-05,
      "loss": 0.0023,
      "step": 55470
    },
    {
      "epoch": 1.8493333333333335,
      "grad_norm": 0.2911696135997772,
      "learning_rate": 3.844166666666667e-05,
      "loss": 0.0021,
      "step": 55480
    },
    {
      "epoch": 1.8496666666666668,
      "grad_norm": 0.20375961065292358,
      "learning_rate": 3.8439583333333334e-05,
      "loss": 0.0021,
      "step": 55490
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.2330419421195984,
      "learning_rate": 3.8437500000000006e-05,
      "loss": 0.002,
      "step": 55500
    },
    {
      "epoch": 1.8503333333333334,
      "grad_norm": 0.08723058551549911,
      "learning_rate": 3.843541666666667e-05,
      "loss": 0.0029,
      "step": 55510
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.08741314709186554,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0025,
      "step": 55520
    },
    {
      "epoch": 1.851,
      "grad_norm": 0.11661553382873535,
      "learning_rate": 3.843125e-05,
      "loss": 0.0036,
      "step": 55530
    },
    {
      "epoch": 1.8513333333333333,
      "grad_norm": 0.05838729813694954,
      "learning_rate": 3.842916666666667e-05,
      "loss": 0.0028,
      "step": 55540
    },
    {
      "epoch": 1.8516666666666666,
      "grad_norm": 0.0584842823445797,
      "learning_rate": 3.842708333333333e-05,
      "loss": 0.0023,
      "step": 55550
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.20398220419883728,
      "learning_rate": 3.8425e-05,
      "loss": 0.0027,
      "step": 55560
    },
    {
      "epoch": 1.8523333333333334,
      "grad_norm": 0.6494501829147339,
      "learning_rate": 3.842291666666667e-05,
      "loss": 0.0025,
      "step": 55570
    },
    {
      "epoch": 1.8526666666666667,
      "grad_norm": 0.26206058263778687,
      "learning_rate": 3.842083333333334e-05,
      "loss": 0.0025,
      "step": 55580
    },
    {
      "epoch": 1.853,
      "grad_norm": 0.2980962097644806,
      "learning_rate": 3.841875e-05,
      "loss": 0.0022,
      "step": 55590
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.3204735815525055,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.0026,
      "step": 55600
    },
    {
      "epoch": 1.8536666666666668,
      "grad_norm": 0.26219093799591064,
      "learning_rate": 3.841458333333334e-05,
      "loss": 0.0022,
      "step": 55610
    },
    {
      "epoch": 1.854,
      "grad_norm": 0.199193075299263,
      "learning_rate": 3.84125e-05,
      "loss": 0.0022,
      "step": 55620
    },
    {
      "epoch": 1.8543333333333334,
      "grad_norm": 0.47553208470344543,
      "learning_rate": 3.841041666666667e-05,
      "loss": 0.0025,
      "step": 55630
    },
    {
      "epoch": 1.8546666666666667,
      "grad_norm": 1.080338478088379,
      "learning_rate": 3.8408333333333336e-05,
      "loss": 0.0046,
      "step": 55640
    },
    {
      "epoch": 1.855,
      "grad_norm": 0.43649354577064514,
      "learning_rate": 3.840625e-05,
      "loss": 0.0028,
      "step": 55650
    },
    {
      "epoch": 1.8553333333333333,
      "grad_norm": 0.23287156224250793,
      "learning_rate": 3.840416666666667e-05,
      "loss": 0.0039,
      "step": 55660
    },
    {
      "epoch": 1.8556666666666666,
      "grad_norm": 0.262079119682312,
      "learning_rate": 3.840208333333333e-05,
      "loss": 0.0021,
      "step": 55670
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.0587037093937397,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0022,
      "step": 55680
    },
    {
      "epoch": 1.8563333333333332,
      "grad_norm": 0.6754441261291504,
      "learning_rate": 3.8397916666666664e-05,
      "loss": 0.0023,
      "step": 55690
    },
    {
      "epoch": 1.8566666666666667,
      "grad_norm": 0.659266471862793,
      "learning_rate": 3.8395833333333336e-05,
      "loss": 0.0035,
      "step": 55700
    },
    {
      "epoch": 1.857,
      "grad_norm": 0.09603022783994675,
      "learning_rate": 3.839375e-05,
      "loss": 0.002,
      "step": 55710
    },
    {
      "epoch": 1.8573333333333333,
      "grad_norm": 0.37853217124938965,
      "learning_rate": 3.839166666666667e-05,
      "loss": 0.002,
      "step": 55720
    },
    {
      "epoch": 1.8576666666666668,
      "grad_norm": 0.05839729309082031,
      "learning_rate": 3.838958333333333e-05,
      "loss": 0.0032,
      "step": 55730
    },
    {
      "epoch": 1.858,
      "grad_norm": 0.262040376663208,
      "learning_rate": 3.8387500000000005e-05,
      "loss": 0.0024,
      "step": 55740
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 0.06199346110224724,
      "learning_rate": 3.838541666666667e-05,
      "loss": 0.002,
      "step": 55750
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.6098572015762329,
      "learning_rate": 3.8383333333333336e-05,
      "loss": 0.0019,
      "step": 55760
    },
    {
      "epoch": 1.859,
      "grad_norm": 0.2617371678352356,
      "learning_rate": 3.838125e-05,
      "loss": 0.0017,
      "step": 55770
    },
    {
      "epoch": 1.8593333333333333,
      "grad_norm": 0.029871055856347084,
      "learning_rate": 3.837916666666667e-05,
      "loss": 0.0029,
      "step": 55780
    },
    {
      "epoch": 1.8596666666666666,
      "grad_norm": 0.029556820169091225,
      "learning_rate": 3.837708333333334e-05,
      "loss": 0.0018,
      "step": 55790
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.17477208375930786,
      "learning_rate": 3.8375e-05,
      "loss": 0.0023,
      "step": 55800
    },
    {
      "epoch": 1.8603333333333332,
      "grad_norm": 0.11715038865804672,
      "learning_rate": 3.837291666666667e-05,
      "loss": 0.0023,
      "step": 55810
    },
    {
      "epoch": 1.8606666666666667,
      "grad_norm": 0.029891593381762505,
      "learning_rate": 3.8370833333333335e-05,
      "loss": 0.0033,
      "step": 55820
    },
    {
      "epoch": 1.861,
      "grad_norm": 0.029925722628831863,
      "learning_rate": 3.836875e-05,
      "loss": 0.0021,
      "step": 55830
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.2910606861114502,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.0027,
      "step": 55840
    },
    {
      "epoch": 1.8616666666666668,
      "grad_norm": 0.14574265480041504,
      "learning_rate": 3.836458333333334e-05,
      "loss": 0.0021,
      "step": 55850
    },
    {
      "epoch": 1.862,
      "grad_norm": 0.2618177831172943,
      "learning_rate": 3.8362500000000004e-05,
      "loss": 0.0024,
      "step": 55860
    },
    {
      "epoch": 1.8623333333333334,
      "grad_norm": 0.14559872448444366,
      "learning_rate": 3.836041666666667e-05,
      "loss": 0.0027,
      "step": 55870
    },
    {
      "epoch": 1.8626666666666667,
      "grad_norm": 0.02997339889407158,
      "learning_rate": 3.8358333333333335e-05,
      "loss": 0.0019,
      "step": 55880
    },
    {
      "epoch": 1.863,
      "grad_norm": 0.1745978146791458,
      "learning_rate": 3.835625e-05,
      "loss": 0.0023,
      "step": 55890
    },
    {
      "epoch": 1.8633333333333333,
      "grad_norm": 0.3040389120578766,
      "learning_rate": 3.8354166666666666e-05,
      "loss": 0.0031,
      "step": 55900
    },
    {
      "epoch": 1.8636666666666666,
      "grad_norm": 0.2621161937713623,
      "learning_rate": 3.835208333333333e-05,
      "loss": 0.0021,
      "step": 55910
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.349582314491272,
      "learning_rate": 3.8350000000000004e-05,
      "loss": 0.0021,
      "step": 55920
    },
    {
      "epoch": 1.8643333333333332,
      "grad_norm": 0.31996840238571167,
      "learning_rate": 3.834791666666667e-05,
      "loss": 0.0022,
      "step": 55930
    },
    {
      "epoch": 1.8646666666666667,
      "grad_norm": 0.029510997235774994,
      "learning_rate": 3.8345833333333335e-05,
      "loss": 0.0031,
      "step": 55940
    },
    {
      "epoch": 1.865,
      "grad_norm": 0.0038807790260761976,
      "learning_rate": 3.834375e-05,
      "loss": 0.0021,
      "step": 55950
    },
    {
      "epoch": 1.8653333333333333,
      "grad_norm": 0.26210153102874756,
      "learning_rate": 3.834166666666667e-05,
      "loss": 0.0025,
      "step": 55960
    },
    {
      "epoch": 1.8656666666666668,
      "grad_norm": 0.23277530074119568,
      "learning_rate": 3.833958333333333e-05,
      "loss": 0.0028,
      "step": 55970
    },
    {
      "epoch": 1.866,
      "grad_norm": 0.029515929520130157,
      "learning_rate": 3.8337500000000004e-05,
      "loss": 0.0023,
      "step": 55980
    },
    {
      "epoch": 1.8663333333333334,
      "grad_norm": 0.02952728606760502,
      "learning_rate": 3.833541666666667e-05,
      "loss": 0.0028,
      "step": 55990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.7282277941703796,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0039,
      "step": 56000
    },
    {
      "epoch": 1.867,
      "grad_norm": 0.46543627977371216,
      "learning_rate": 3.833125e-05,
      "loss": 0.0028,
      "step": 56010
    },
    {
      "epoch": 1.8673333333333333,
      "grad_norm": 0.058615848422050476,
      "learning_rate": 3.8329166666666665e-05,
      "loss": 0.0042,
      "step": 56020
    },
    {
      "epoch": 1.8676666666666666,
      "grad_norm": 0.006205807439982891,
      "learning_rate": 3.832708333333334e-05,
      "loss": 0.0027,
      "step": 56030
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.2035968005657196,
      "learning_rate": 3.8324999999999996e-05,
      "loss": 0.0027,
      "step": 56040
    },
    {
      "epoch": 1.8683333333333332,
      "grad_norm": 0.11669502407312393,
      "learning_rate": 3.832291666666667e-05,
      "loss": 0.0028,
      "step": 56050
    },
    {
      "epoch": 1.8686666666666667,
      "grad_norm": 0.1456506997346878,
      "learning_rate": 3.8320833333333334e-05,
      "loss": 0.0042,
      "step": 56060
    },
    {
      "epoch": 1.869,
      "grad_norm": 0.0589599646627903,
      "learning_rate": 3.8318750000000006e-05,
      "loss": 0.0026,
      "step": 56070
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.029949145391583443,
      "learning_rate": 3.8316666666666665e-05,
      "loss": 0.0017,
      "step": 56080
    },
    {
      "epoch": 1.8696666666666668,
      "grad_norm": 0.26132383942604065,
      "learning_rate": 3.831458333333334e-05,
      "loss": 0.0025,
      "step": 56090
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.11651334911584854,
      "learning_rate": 3.83125e-05,
      "loss": 0.0021,
      "step": 56100
    },
    {
      "epoch": 1.8703333333333334,
      "grad_norm": 0.08760514855384827,
      "learning_rate": 3.831041666666667e-05,
      "loss": 0.0024,
      "step": 56110
    },
    {
      "epoch": 1.8706666666666667,
      "grad_norm": 0.02972438372671604,
      "learning_rate": 3.8308333333333334e-05,
      "loss": 0.0022,
      "step": 56120
    },
    {
      "epoch": 1.871,
      "grad_norm": 0.3491610884666443,
      "learning_rate": 3.830625e-05,
      "loss": 0.0021,
      "step": 56130
    },
    {
      "epoch": 1.8713333333333333,
      "grad_norm": 0.11610272526741028,
      "learning_rate": 3.830416666666667e-05,
      "loss": 0.0023,
      "step": 56140
    },
    {
      "epoch": 1.8716666666666666,
      "grad_norm": 0.11683517694473267,
      "learning_rate": 3.830208333333333e-05,
      "loss": 0.0033,
      "step": 56150
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.3199951648712158,
      "learning_rate": 3.83e-05,
      "loss": 0.0023,
      "step": 56160
    },
    {
      "epoch": 1.8723333333333332,
      "grad_norm": 0.26745742559432983,
      "learning_rate": 3.829791666666667e-05,
      "loss": 0.0021,
      "step": 56170
    },
    {
      "epoch": 1.8726666666666667,
      "grad_norm": 0.29140785336494446,
      "learning_rate": 3.8295833333333334e-05,
      "loss": 0.0025,
      "step": 56180
    },
    {
      "epoch": 1.873,
      "grad_norm": 0.4100055694580078,
      "learning_rate": 3.829375e-05,
      "loss": 0.0018,
      "step": 56190
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.23307806253433228,
      "learning_rate": 3.829166666666667e-05,
      "loss": 0.0039,
      "step": 56200
    },
    {
      "epoch": 1.8736666666666668,
      "grad_norm": 0.6402738094329834,
      "learning_rate": 3.828958333333334e-05,
      "loss": 0.0034,
      "step": 56210
    },
    {
      "epoch": 1.874,
      "grad_norm": 0.17502158880233765,
      "learning_rate": 3.82875e-05,
      "loss": 0.0042,
      "step": 56220
    },
    {
      "epoch": 1.8743333333333334,
      "grad_norm": 0.08755919337272644,
      "learning_rate": 3.828541666666667e-05,
      "loss": 0.0025,
      "step": 56230
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.509149968624115,
      "learning_rate": 3.828333333333334e-05,
      "loss": 0.004,
      "step": 56240
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.1929713487625122,
      "learning_rate": 3.828125e-05,
      "loss": 0.0017,
      "step": 56250
    },
    {
      "epoch": 1.8753333333333333,
      "grad_norm": 0.14578600227832794,
      "learning_rate": 3.8279166666666664e-05,
      "loss": 0.0018,
      "step": 56260
    },
    {
      "epoch": 1.8756666666666666,
      "grad_norm": 0.26207610964775085,
      "learning_rate": 3.8277083333333337e-05,
      "loss": 0.0024,
      "step": 56270
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.23296019434928894,
      "learning_rate": 3.8275e-05,
      "loss": 0.0027,
      "step": 56280
    },
    {
      "epoch": 1.8763333333333332,
      "grad_norm": 0.0039631640538573265,
      "learning_rate": 3.827291666666667e-05,
      "loss": 0.003,
      "step": 56290
    },
    {
      "epoch": 1.8766666666666667,
      "grad_norm": 0.14577604830265045,
      "learning_rate": 3.827083333333333e-05,
      "loss": 0.0028,
      "step": 56300
    },
    {
      "epoch": 1.877,
      "grad_norm": 0.007134921848773956,
      "learning_rate": 3.8268750000000005e-05,
      "loss": 0.0031,
      "step": 56310
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.05857843533158302,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.003,
      "step": 56320
    },
    {
      "epoch": 1.8776666666666668,
      "grad_norm": 0.290903776884079,
      "learning_rate": 3.8264583333333336e-05,
      "loss": 0.0029,
      "step": 56330
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 0.4368579685688019,
      "learning_rate": 3.82625e-05,
      "loss": 0.0024,
      "step": 56340
    },
    {
      "epoch": 1.8783333333333334,
      "grad_norm": 0.11657164245843887,
      "learning_rate": 3.8260416666666674e-05,
      "loss": 0.0028,
      "step": 56350
    },
    {
      "epoch": 1.8786666666666667,
      "grad_norm": 0.005069854203611612,
      "learning_rate": 3.825833333333333e-05,
      "loss": 0.0023,
      "step": 56360
    },
    {
      "epoch": 1.879,
      "grad_norm": 0.0049277376383543015,
      "learning_rate": 3.8256250000000005e-05,
      "loss": 0.0023,
      "step": 56370
    },
    {
      "epoch": 1.8793333333333333,
      "grad_norm": 0.14585530757904053,
      "learning_rate": 3.825416666666667e-05,
      "loss": 0.0022,
      "step": 56380
    },
    {
      "epoch": 1.8796666666666666,
      "grad_norm": 0.17528119683265686,
      "learning_rate": 3.8252083333333336e-05,
      "loss": 0.0022,
      "step": 56390
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.15283137559890747,
      "learning_rate": 3.825e-05,
      "loss": 0.0018,
      "step": 56400
    },
    {
      "epoch": 1.8803333333333332,
      "grad_norm": 0.43330127000808716,
      "learning_rate": 3.824791666666667e-05,
      "loss": 0.0019,
      "step": 56410
    },
    {
      "epoch": 1.8806666666666667,
      "grad_norm": 0.02572631649672985,
      "learning_rate": 3.824583333333334e-05,
      "loss": 0.0018,
      "step": 56420
    },
    {
      "epoch": 1.881,
      "grad_norm": 0.1254071295261383,
      "learning_rate": 3.824375e-05,
      "loss": 0.002,
      "step": 56430
    },
    {
      "epoch": 1.8813333333333333,
      "grad_norm": 0.0874033123254776,
      "learning_rate": 3.824166666666667e-05,
      "loss": 0.0025,
      "step": 56440
    },
    {
      "epoch": 1.8816666666666668,
      "grad_norm": 0.23330071568489075,
      "learning_rate": 3.8239583333333336e-05,
      "loss": 0.0014,
      "step": 56450
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 0.02974519319832325,
      "learning_rate": 3.82375e-05,
      "loss": 0.0022,
      "step": 56460
    },
    {
      "epoch": 1.8823333333333334,
      "grad_norm": 0.05835835263133049,
      "learning_rate": 3.8235416666666667e-05,
      "loss": 0.0025,
      "step": 56470
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.6830041408538818,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.0021,
      "step": 56480
    },
    {
      "epoch": 1.883,
      "grad_norm": 0.0868515744805336,
      "learning_rate": 3.8231250000000004e-05,
      "loss": 0.0016,
      "step": 56490
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.1743362694978714,
      "learning_rate": 3.822916666666666e-05,
      "loss": 0.0025,
      "step": 56500
    },
    {
      "epoch": 1.8836666666666666,
      "grad_norm": 0.5244399309158325,
      "learning_rate": 3.8227083333333335e-05,
      "loss": 0.0026,
      "step": 56510
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.11693627387285233,
      "learning_rate": 3.8225e-05,
      "loss": 0.0025,
      "step": 56520
    },
    {
      "epoch": 1.8843333333333332,
      "grad_norm": 0.030719511210918427,
      "learning_rate": 3.8222916666666666e-05,
      "loss": 0.0021,
      "step": 56530
    },
    {
      "epoch": 1.8846666666666667,
      "grad_norm": 0.11657650768756866,
      "learning_rate": 3.822083333333333e-05,
      "loss": 0.0033,
      "step": 56540
    },
    {
      "epoch": 1.885,
      "grad_norm": 0.08768194913864136,
      "learning_rate": 3.8218750000000004e-05,
      "loss": 0.0014,
      "step": 56550
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.43673110008239746,
      "learning_rate": 3.821666666666667e-05,
      "loss": 0.0022,
      "step": 56560
    },
    {
      "epoch": 1.8856666666666668,
      "grad_norm": 0.3495652675628662,
      "learning_rate": 3.8214583333333335e-05,
      "loss": 0.0027,
      "step": 56570
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 0.3575223684310913,
      "learning_rate": 3.82125e-05,
      "loss": 0.0017,
      "step": 56580
    },
    {
      "epoch": 1.8863333333333334,
      "grad_norm": 0.029558952897787094,
      "learning_rate": 3.821041666666667e-05,
      "loss": 0.0032,
      "step": 56590
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.3494976758956909,
      "learning_rate": 3.820833333333334e-05,
      "loss": 0.0029,
      "step": 56600
    },
    {
      "epoch": 1.887,
      "grad_norm": 0.11629606783390045,
      "learning_rate": 3.8206250000000004e-05,
      "loss": 0.0024,
      "step": 56610
    },
    {
      "epoch": 1.8873333333333333,
      "grad_norm": 0.2366832196712494,
      "learning_rate": 3.820416666666667e-05,
      "loss": 0.0023,
      "step": 56620
    },
    {
      "epoch": 1.8876666666666666,
      "grad_norm": 0.008666189387440681,
      "learning_rate": 3.8202083333333335e-05,
      "loss": 0.0017,
      "step": 56630
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.08823996782302856,
      "learning_rate": 3.82e-05,
      "loss": 0.0024,
      "step": 56640
    },
    {
      "epoch": 1.8883333333333332,
      "grad_norm": 0.08781206607818604,
      "learning_rate": 3.8197916666666666e-05,
      "loss": 0.002,
      "step": 56650
    },
    {
      "epoch": 1.8886666666666667,
      "grad_norm": 0.3489927649497986,
      "learning_rate": 3.819583333333334e-05,
      "loss": 0.0027,
      "step": 56660
    },
    {
      "epoch": 1.889,
      "grad_norm": 0.1749790608882904,
      "learning_rate": 3.8193750000000003e-05,
      "loss": 0.0023,
      "step": 56670
    },
    {
      "epoch": 1.8893333333333333,
      "grad_norm": 0.004168019630014896,
      "learning_rate": 3.819166666666667e-05,
      "loss": 0.0018,
      "step": 56680
    },
    {
      "epoch": 1.8896666666666668,
      "grad_norm": 0.05836397036910057,
      "learning_rate": 3.8189583333333334e-05,
      "loss": 0.0015,
      "step": 56690
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.5527682900428772,
      "learning_rate": 3.818750000000001e-05,
      "loss": 0.0023,
      "step": 56700
    },
    {
      "epoch": 1.8903333333333334,
      "grad_norm": 0.19938261806964874,
      "learning_rate": 3.8185416666666665e-05,
      "loss": 0.0028,
      "step": 56710
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.2620573937892914,
      "learning_rate": 3.818333333333334e-05,
      "loss": 0.0028,
      "step": 56720
    },
    {
      "epoch": 1.891,
      "grad_norm": 0.029323142021894455,
      "learning_rate": 3.818125e-05,
      "loss": 0.0019,
      "step": 56730
    },
    {
      "epoch": 1.8913333333333333,
      "grad_norm": 0.05850551649928093,
      "learning_rate": 3.817916666666667e-05,
      "loss": 0.0026,
      "step": 56740
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 0.0291889738291502,
      "learning_rate": 3.8177083333333334e-05,
      "loss": 0.002,
      "step": 56750
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.2040429711341858,
      "learning_rate": 3.8175e-05,
      "loss": 0.0027,
      "step": 56760
    },
    {
      "epoch": 1.8923333333333332,
      "grad_norm": 0.1747831255197525,
      "learning_rate": 3.817291666666667e-05,
      "loss": 0.002,
      "step": 56770
    },
    {
      "epoch": 1.8926666666666667,
      "grad_norm": 0.3490149974822998,
      "learning_rate": 3.817083333333333e-05,
      "loss": 0.0027,
      "step": 56780
    },
    {
      "epoch": 1.893,
      "grad_norm": 0.11639713495969772,
      "learning_rate": 3.816875e-05,
      "loss": 0.0025,
      "step": 56790
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.058973927050828934,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0022,
      "step": 56800
    },
    {
      "epoch": 1.8936666666666668,
      "grad_norm": 0.10051339864730835,
      "learning_rate": 3.8164583333333334e-05,
      "loss": 0.0017,
      "step": 56810
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 0.3385074734687805,
      "learning_rate": 3.81625e-05,
      "loss": 0.0031,
      "step": 56820
    },
    {
      "epoch": 1.8943333333333334,
      "grad_norm": 0.14558672904968262,
      "learning_rate": 3.816041666666667e-05,
      "loss": 0.0019,
      "step": 56830
    },
    {
      "epoch": 1.8946666666666667,
      "grad_norm": 0.4658377468585968,
      "learning_rate": 3.815833333333334e-05,
      "loss": 0.0041,
      "step": 56840
    },
    {
      "epoch": 1.895,
      "grad_norm": 0.20352023839950562,
      "learning_rate": 3.815625e-05,
      "loss": 0.0018,
      "step": 56850
    },
    {
      "epoch": 1.8953333333333333,
      "grad_norm": 0.03023580089211464,
      "learning_rate": 3.815416666666667e-05,
      "loss": 0.0026,
      "step": 56860
    },
    {
      "epoch": 1.8956666666666666,
      "grad_norm": 0.029739299789071083,
      "learning_rate": 3.8152083333333333e-05,
      "loss": 0.002,
      "step": 56870
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.43827328085899353,
      "learning_rate": 3.8150000000000006e-05,
      "loss": 0.0033,
      "step": 56880
    },
    {
      "epoch": 1.8963333333333332,
      "grad_norm": 0.23283828794956207,
      "learning_rate": 3.8147916666666664e-05,
      "loss": 0.0015,
      "step": 56890
    },
    {
      "epoch": 1.8966666666666665,
      "grad_norm": 0.1748332977294922,
      "learning_rate": 3.814583333333334e-05,
      "loss": 0.002,
      "step": 56900
    },
    {
      "epoch": 1.897,
      "grad_norm": 0.26173803210258484,
      "learning_rate": 3.814375e-05,
      "loss": 0.0027,
      "step": 56910
    },
    {
      "epoch": 1.8973333333333333,
      "grad_norm": 0.05869545042514801,
      "learning_rate": 3.814166666666667e-05,
      "loss": 0.0022,
      "step": 56920
    },
    {
      "epoch": 1.8976666666666666,
      "grad_norm": 0.4944964349269867,
      "learning_rate": 3.813958333333333e-05,
      "loss": 0.0019,
      "step": 56930
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 0.14773593842983246,
      "learning_rate": 3.8137500000000005e-05,
      "loss": 0.0027,
      "step": 56940
    },
    {
      "epoch": 1.8983333333333334,
      "grad_norm": 0.17453797161579132,
      "learning_rate": 3.813541666666667e-05,
      "loss": 0.0017,
      "step": 56950
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.02933347038924694,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0027,
      "step": 56960
    },
    {
      "epoch": 1.899,
      "grad_norm": 0.0299493670463562,
      "learning_rate": 3.813125e-05,
      "loss": 0.0019,
      "step": 56970
    },
    {
      "epoch": 1.8993333333333333,
      "grad_norm": 0.030040664598345757,
      "learning_rate": 3.812916666666667e-05,
      "loss": 0.0023,
      "step": 56980
    },
    {
      "epoch": 1.8996666666666666,
      "grad_norm": 0.23289956152439117,
      "learning_rate": 3.812708333333333e-05,
      "loss": 0.002,
      "step": 56990
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.3654624819755554,
      "learning_rate": 3.8125e-05,
      "loss": 0.0023,
      "step": 57000
    },
    {
      "epoch": 1.9003333333333332,
      "grad_norm": 0.6400993466377258,
      "learning_rate": 3.812291666666667e-05,
      "loss": 0.0021,
      "step": 57010
    },
    {
      "epoch": 1.9006666666666665,
      "grad_norm": 0.02956490032374859,
      "learning_rate": 3.8120833333333336e-05,
      "loss": 0.0022,
      "step": 57020
    },
    {
      "epoch": 1.901,
      "grad_norm": 0.029462601989507675,
      "learning_rate": 3.811875e-05,
      "loss": 0.0019,
      "step": 57030
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.05865778774023056,
      "learning_rate": 3.811666666666667e-05,
      "loss": 0.002,
      "step": 57040
    },
    {
      "epoch": 1.9016666666666666,
      "grad_norm": 0.37834230065345764,
      "learning_rate": 3.811458333333334e-05,
      "loss": 0.0023,
      "step": 57050
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 0.31982970237731934,
      "learning_rate": 3.81125e-05,
      "loss": 0.0028,
      "step": 57060
    },
    {
      "epoch": 1.9023333333333334,
      "grad_norm": 0.23333965241909027,
      "learning_rate": 3.811041666666667e-05,
      "loss": 0.0022,
      "step": 57070
    },
    {
      "epoch": 1.9026666666666667,
      "grad_norm": 0.1744581013917923,
      "learning_rate": 3.8108333333333336e-05,
      "loss": 0.0021,
      "step": 57080
    },
    {
      "epoch": 1.903,
      "grad_norm": 0.37834277749061584,
      "learning_rate": 3.810625e-05,
      "loss": 0.0024,
      "step": 57090
    },
    {
      "epoch": 1.9033333333333333,
      "grad_norm": 0.11651192605495453,
      "learning_rate": 3.810416666666667e-05,
      "loss": 0.0027,
      "step": 57100
    },
    {
      "epoch": 1.9036666666666666,
      "grad_norm": 0.4790804088115692,
      "learning_rate": 3.810208333333333e-05,
      "loss": 0.0022,
      "step": 57110
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.029436687007546425,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.003,
      "step": 57120
    },
    {
      "epoch": 1.9043333333333332,
      "grad_norm": 0.23481839895248413,
      "learning_rate": 3.809791666666666e-05,
      "loss": 0.0015,
      "step": 57130
    },
    {
      "epoch": 1.9046666666666665,
      "grad_norm": 0.029544441029429436,
      "learning_rate": 3.8095833333333335e-05,
      "loss": 0.0024,
      "step": 57140
    },
    {
      "epoch": 1.905,
      "grad_norm": 0.32043397426605225,
      "learning_rate": 3.809375e-05,
      "loss": 0.0016,
      "step": 57150
    },
    {
      "epoch": 1.9053333333333333,
      "grad_norm": 0.3782521188259125,
      "learning_rate": 3.809166666666667e-05,
      "loss": 0.0029,
      "step": 57160
    },
    {
      "epoch": 1.9056666666666666,
      "grad_norm": 0.14570599794387817,
      "learning_rate": 3.808958333333333e-05,
      "loss": 0.003,
      "step": 57170
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 0.407680481672287,
      "learning_rate": 3.8087500000000004e-05,
      "loss": 0.0021,
      "step": 57180
    },
    {
      "epoch": 1.9063333333333334,
      "grad_norm": 0.08751386404037476,
      "learning_rate": 3.808541666666667e-05,
      "loss": 0.0015,
      "step": 57190
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.14553306996822357,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.0019,
      "step": 57200
    },
    {
      "epoch": 1.907,
      "grad_norm": 0.11651165783405304,
      "learning_rate": 3.808125e-05,
      "loss": 0.0022,
      "step": 57210
    },
    {
      "epoch": 1.9073333333333333,
      "grad_norm": 0.2619514465332031,
      "learning_rate": 3.8079166666666666e-05,
      "loss": 0.0014,
      "step": 57220
    },
    {
      "epoch": 1.9076666666666666,
      "grad_norm": 0.20377594232559204,
      "learning_rate": 3.807708333333334e-05,
      "loss": 0.0021,
      "step": 57230
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.46589434146881104,
      "learning_rate": 3.8075e-05,
      "loss": 0.0016,
      "step": 57240
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 0.038357075303792953,
      "learning_rate": 3.807291666666667e-05,
      "loss": 0.0025,
      "step": 57250
    },
    {
      "epoch": 1.9086666666666665,
      "grad_norm": 0.5173498392105103,
      "learning_rate": 3.8070833333333335e-05,
      "loss": 0.0018,
      "step": 57260
    },
    {
      "epoch": 1.909,
      "grad_norm": 0.2956438362598419,
      "learning_rate": 3.806875e-05,
      "loss": 0.0023,
      "step": 57270
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.26210805773735046,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0028,
      "step": 57280
    },
    {
      "epoch": 1.9096666666666666,
      "grad_norm": 0.4077579379081726,
      "learning_rate": 3.806458333333334e-05,
      "loss": 0.0026,
      "step": 57290
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.1460212618112564,
      "learning_rate": 3.8062500000000004e-05,
      "loss": 0.0019,
      "step": 57300
    },
    {
      "epoch": 1.9103333333333334,
      "grad_norm": 0.46595004200935364,
      "learning_rate": 3.806041666666667e-05,
      "loss": 0.0022,
      "step": 57310
    },
    {
      "epoch": 1.9106666666666667,
      "grad_norm": 0.6401782035827637,
      "learning_rate": 3.8058333333333335e-05,
      "loss": 0.0022,
      "step": 57320
    },
    {
      "epoch": 1.911,
      "grad_norm": 0.058695368468761444,
      "learning_rate": 3.805625000000001e-05,
      "loss": 0.0032,
      "step": 57330
    },
    {
      "epoch": 1.9113333333333333,
      "grad_norm": 0.1456749141216278,
      "learning_rate": 3.8054166666666666e-05,
      "loss": 0.002,
      "step": 57340
    },
    {
      "epoch": 1.9116666666666666,
      "grad_norm": 0.008257403038442135,
      "learning_rate": 3.805208333333333e-05,
      "loss": 0.0024,
      "step": 57350
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.7567348480224609,
      "learning_rate": 3.805e-05,
      "loss": 0.0021,
      "step": 57360
    },
    {
      "epoch": 1.9123333333333332,
      "grad_norm": 0.08770930767059326,
      "learning_rate": 3.804791666666667e-05,
      "loss": 0.0024,
      "step": 57370
    },
    {
      "epoch": 1.9126666666666665,
      "grad_norm": 0.005427367519587278,
      "learning_rate": 3.8045833333333334e-05,
      "loss": 0.0025,
      "step": 57380
    },
    {
      "epoch": 1.913,
      "grad_norm": 0.32009071111679077,
      "learning_rate": 3.804375e-05,
      "loss": 0.0014,
      "step": 57390
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.011085846461355686,
      "learning_rate": 3.804166666666667e-05,
      "loss": 0.0023,
      "step": 57400
    },
    {
      "epoch": 1.9136666666666666,
      "grad_norm": 0.3199748992919922,
      "learning_rate": 3.803958333333333e-05,
      "loss": 0.0023,
      "step": 57410
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 0.29126080870628357,
      "learning_rate": 3.80375e-05,
      "loss": 0.0017,
      "step": 57420
    },
    {
      "epoch": 1.9143333333333334,
      "grad_norm": 0.30650365352630615,
      "learning_rate": 3.803541666666667e-05,
      "loss": 0.0032,
      "step": 57430
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.17459729313850403,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0032,
      "step": 57440
    },
    {
      "epoch": 1.915,
      "grad_norm": 0.05869598314166069,
      "learning_rate": 3.803125e-05,
      "loss": 0.0027,
      "step": 57450
    },
    {
      "epoch": 1.9153333333333333,
      "grad_norm": 0.0339064747095108,
      "learning_rate": 3.8029166666666665e-05,
      "loss": 0.0023,
      "step": 57460
    },
    {
      "epoch": 1.9156666666666666,
      "grad_norm": 0.06836975365877151,
      "learning_rate": 3.802708333333334e-05,
      "loss": 0.0024,
      "step": 57470
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.2623502016067505,
      "learning_rate": 3.8025e-05,
      "loss": 0.0031,
      "step": 57480
    },
    {
      "epoch": 1.9163333333333332,
      "grad_norm": 0.11653219908475876,
      "learning_rate": 3.802291666666667e-05,
      "loss": 0.0031,
      "step": 57490
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.145486980676651,
      "learning_rate": 3.8020833333333334e-05,
      "loss": 0.0027,
      "step": 57500
    },
    {
      "epoch": 1.917,
      "grad_norm": 0.058387771248817444,
      "learning_rate": 3.8018750000000006e-05,
      "loss": 0.0022,
      "step": 57510
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.3201678395271301,
      "learning_rate": 3.8016666666666665e-05,
      "loss": 0.0023,
      "step": 57520
    },
    {
      "epoch": 1.9176666666666666,
      "grad_norm": 0.1747819036245346,
      "learning_rate": 3.801458333333334e-05,
      "loss": 0.0037,
      "step": 57530
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 0.20393162965774536,
      "learning_rate": 3.80125e-05,
      "loss": 0.0028,
      "step": 57540
    },
    {
      "epoch": 1.9183333333333334,
      "grad_norm": 0.058809686452150345,
      "learning_rate": 3.801041666666667e-05,
      "loss": 0.0028,
      "step": 57550
    },
    {
      "epoch": 1.9186666666666667,
      "grad_norm": 0.32021841406822205,
      "learning_rate": 3.800833333333333e-05,
      "loss": 0.0025,
      "step": 57560
    },
    {
      "epoch": 1.919,
      "grad_norm": 0.2910892367362976,
      "learning_rate": 3.8006250000000006e-05,
      "loss": 0.0031,
      "step": 57570
    },
    {
      "epoch": 1.9193333333333333,
      "grad_norm": 0.4699770212173462,
      "learning_rate": 3.800416666666667e-05,
      "loss": 0.0026,
      "step": 57580
    },
    {
      "epoch": 1.9196666666666666,
      "grad_norm": 0.0586097426712513,
      "learning_rate": 3.800208333333333e-05,
      "loss": 0.0023,
      "step": 57590
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.14795663952827454,
      "learning_rate": 3.8e-05,
      "loss": 0.0033,
      "step": 57600
    },
    {
      "epoch": 1.9203333333333332,
      "grad_norm": 0.11656840145587921,
      "learning_rate": 3.799791666666667e-05,
      "loss": 0.0019,
      "step": 57610
    },
    {
      "epoch": 1.9206666666666665,
      "grad_norm": 0.203660249710083,
      "learning_rate": 3.799583333333333e-05,
      "loss": 0.0021,
      "step": 57620
    },
    {
      "epoch": 1.921,
      "grad_norm": 0.17487691342830658,
      "learning_rate": 3.799375e-05,
      "loss": 0.0024,
      "step": 57630
    },
    {
      "epoch": 1.9213333333333333,
      "grad_norm": 0.11636414378881454,
      "learning_rate": 3.799166666666667e-05,
      "loss": 0.0022,
      "step": 57640
    },
    {
      "epoch": 1.9216666666666666,
      "grad_norm": 0.32002055644989014,
      "learning_rate": 3.7989583333333336e-05,
      "loss": 0.0024,
      "step": 57650
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 0.059590697288513184,
      "learning_rate": 3.79875e-05,
      "loss": 0.003,
      "step": 57660
    },
    {
      "epoch": 1.9223333333333334,
      "grad_norm": 0.3198951184749603,
      "learning_rate": 3.798541666666667e-05,
      "loss": 0.0028,
      "step": 57670
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.17452991008758545,
      "learning_rate": 3.798333333333334e-05,
      "loss": 0.0031,
      "step": 57680
    },
    {
      "epoch": 1.923,
      "grad_norm": 0.11642653495073318,
      "learning_rate": 3.798125e-05,
      "loss": 0.0029,
      "step": 57690
    },
    {
      "epoch": 1.9233333333333333,
      "grad_norm": 0.02936466783285141,
      "learning_rate": 3.797916666666667e-05,
      "loss": 0.0023,
      "step": 57700
    },
    {
      "epoch": 1.9236666666666666,
      "grad_norm": 0.26325884461402893,
      "learning_rate": 3.7977083333333336e-05,
      "loss": 0.0024,
      "step": 57710
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.14549966156482697,
      "learning_rate": 3.7975e-05,
      "loss": 0.0019,
      "step": 57720
    },
    {
      "epoch": 1.9243333333333332,
      "grad_norm": 0.1457948088645935,
      "learning_rate": 3.797291666666667e-05,
      "loss": 0.0018,
      "step": 57730
    },
    {
      "epoch": 1.9246666666666665,
      "grad_norm": 0.030170071870088577,
      "learning_rate": 3.797083333333333e-05,
      "loss": 0.0024,
      "step": 57740
    },
    {
      "epoch": 1.925,
      "grad_norm": 0.3490005433559418,
      "learning_rate": 3.7968750000000005e-05,
      "loss": 0.0016,
      "step": 57750
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.0055757202208042145,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.0016,
      "step": 57760
    },
    {
      "epoch": 1.9256666666666666,
      "grad_norm": 0.14596478641033173,
      "learning_rate": 3.7964583333333336e-05,
      "loss": 0.0023,
      "step": 57770
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 0.43653228878974915,
      "learning_rate": 3.79625e-05,
      "loss": 0.0032,
      "step": 57780
    },
    {
      "epoch": 1.9263333333333335,
      "grad_norm": 0.46576306223869324,
      "learning_rate": 3.7960416666666673e-05,
      "loss": 0.0019,
      "step": 57790
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.38981589674949646,
      "learning_rate": 3.795833333333333e-05,
      "loss": 0.0026,
      "step": 57800
    },
    {
      "epoch": 1.927,
      "grad_norm": 0.029371993616223335,
      "learning_rate": 3.7956250000000004e-05,
      "loss": 0.0021,
      "step": 57810
    },
    {
      "epoch": 1.9273333333333333,
      "grad_norm": 0.2036387324333191,
      "learning_rate": 3.795416666666667e-05,
      "loss": 0.0032,
      "step": 57820
    },
    {
      "epoch": 1.9276666666666666,
      "grad_norm": 0.17487822473049164,
      "learning_rate": 3.7952083333333335e-05,
      "loss": 0.002,
      "step": 57830
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.041731104254722595,
      "learning_rate": 3.795e-05,
      "loss": 0.0027,
      "step": 57840
    },
    {
      "epoch": 1.9283333333333332,
      "grad_norm": 0.05851350352168083,
      "learning_rate": 3.7947916666666666e-05,
      "loss": 0.0029,
      "step": 57850
    },
    {
      "epoch": 1.9286666666666665,
      "grad_norm": 0.446653813123703,
      "learning_rate": 3.794583333333334e-05,
      "loss": 0.0021,
      "step": 57860
    },
    {
      "epoch": 1.929,
      "grad_norm": 0.05112732574343681,
      "learning_rate": 3.794375e-05,
      "loss": 0.0038,
      "step": 57870
    },
    {
      "epoch": 1.9293333333333333,
      "grad_norm": 0.05840596184134483,
      "learning_rate": 3.794166666666667e-05,
      "loss": 0.0027,
      "step": 57880
    },
    {
      "epoch": 1.9296666666666666,
      "grad_norm": 0.37881481647491455,
      "learning_rate": 3.7939583333333335e-05,
      "loss": 0.0021,
      "step": 57890
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.06105588376522064,
      "learning_rate": 3.79375e-05,
      "loss": 0.0028,
      "step": 57900
    },
    {
      "epoch": 1.9303333333333335,
      "grad_norm": 0.20385350286960602,
      "learning_rate": 3.7935416666666666e-05,
      "loss": 0.0024,
      "step": 57910
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.08754190802574158,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0023,
      "step": 57920
    },
    {
      "epoch": 1.931,
      "grad_norm": 0.20356880128383636,
      "learning_rate": 3.7931250000000004e-05,
      "loss": 0.0019,
      "step": 57930
    },
    {
      "epoch": 1.9313333333333333,
      "grad_norm": 0.40715277194976807,
      "learning_rate": 3.792916666666667e-05,
      "loss": 0.0022,
      "step": 57940
    },
    {
      "epoch": 1.9316666666666666,
      "grad_norm": 0.14584937691688538,
      "learning_rate": 3.7927083333333335e-05,
      "loss": 0.0028,
      "step": 57950
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.029342710971832275,
      "learning_rate": 3.7925e-05,
      "loss": 0.003,
      "step": 57960
    },
    {
      "epoch": 1.9323333333333332,
      "grad_norm": 0.26210275292396545,
      "learning_rate": 3.7922916666666666e-05,
      "loss": 0.0018,
      "step": 57970
    },
    {
      "epoch": 1.9326666666666665,
      "grad_norm": 0.029255356639623642,
      "learning_rate": 3.792083333333333e-05,
      "loss": 0.0021,
      "step": 57980
    },
    {
      "epoch": 1.933,
      "grad_norm": 0.12560506165027618,
      "learning_rate": 3.7918750000000004e-05,
      "loss": 0.002,
      "step": 57990
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.008965438231825829,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.0016,
      "step": 58000
    },
    {
      "epoch": 1.9336666666666666,
      "grad_norm": 0.1748618334531784,
      "learning_rate": 3.7914583333333334e-05,
      "loss": 0.0017,
      "step": 58010
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 0.3201521635055542,
      "learning_rate": 3.79125e-05,
      "loss": 0.0015,
      "step": 58020
    },
    {
      "epoch": 1.9343333333333335,
      "grad_norm": 0.23260051012039185,
      "learning_rate": 3.791041666666667e-05,
      "loss": 0.0031,
      "step": 58030
    },
    {
      "epoch": 1.9346666666666668,
      "grad_norm": 0.08743274211883545,
      "learning_rate": 3.790833333333334e-05,
      "loss": 0.0027,
      "step": 58040
    },
    {
      "epoch": 1.935,
      "grad_norm": 0.058185823261737823,
      "learning_rate": 3.790625e-05,
      "loss": 0.0029,
      "step": 58050
    },
    {
      "epoch": 1.9353333333333333,
      "grad_norm": 0.557595431804657,
      "learning_rate": 3.790416666666667e-05,
      "loss": 0.0019,
      "step": 58060
    },
    {
      "epoch": 1.9356666666666666,
      "grad_norm": 0.261911541223526,
      "learning_rate": 3.7902083333333334e-05,
      "loss": 0.0027,
      "step": 58070
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.4659899175167084,
      "learning_rate": 3.79e-05,
      "loss": 0.0018,
      "step": 58080
    },
    {
      "epoch": 1.9363333333333332,
      "grad_norm": 0.05898682400584221,
      "learning_rate": 3.7897916666666665e-05,
      "loss": 0.0027,
      "step": 58090
    },
    {
      "epoch": 1.9366666666666665,
      "grad_norm": 0.37858858704566956,
      "learning_rate": 3.789583333333334e-05,
      "loss": 0.0026,
      "step": 58100
    },
    {
      "epoch": 1.937,
      "grad_norm": 0.058109574019908905,
      "learning_rate": 3.789375e-05,
      "loss": 0.0035,
      "step": 58110
    },
    {
      "epoch": 1.9373333333333334,
      "grad_norm": 0.1745886355638504,
      "learning_rate": 3.789166666666667e-05,
      "loss": 0.003,
      "step": 58120
    },
    {
      "epoch": 1.9376666666666666,
      "grad_norm": 0.11645837873220444,
      "learning_rate": 3.7889583333333334e-05,
      "loss": 0.0022,
      "step": 58130
    },
    {
      "epoch": 1.938,
      "grad_norm": 0.1453946977853775,
      "learning_rate": 3.7887500000000006e-05,
      "loss": 0.0026,
      "step": 58140
    },
    {
      "epoch": 1.9383333333333335,
      "grad_norm": 0.05837363004684448,
      "learning_rate": 3.7885416666666665e-05,
      "loss": 0.0017,
      "step": 58150
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.23258210718631744,
      "learning_rate": 3.788333333333334e-05,
      "loss": 0.0021,
      "step": 58160
    },
    {
      "epoch": 1.939,
      "grad_norm": 0.23308315873146057,
      "learning_rate": 3.788125e-05,
      "loss": 0.0022,
      "step": 58170
    },
    {
      "epoch": 1.9393333333333334,
      "grad_norm": 0.3782210946083069,
      "learning_rate": 3.787916666666667e-05,
      "loss": 0.0014,
      "step": 58180
    },
    {
      "epoch": 1.9396666666666667,
      "grad_norm": 0.11657103896141052,
      "learning_rate": 3.7877083333333334e-05,
      "loss": 0.002,
      "step": 58190
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.08724091202020645,
      "learning_rate": 3.7875e-05,
      "loss": 0.0024,
      "step": 58200
    },
    {
      "epoch": 1.9403333333333332,
      "grad_norm": 0.1745380163192749,
      "learning_rate": 3.787291666666667e-05,
      "loss": 0.0032,
      "step": 58210
    },
    {
      "epoch": 1.9406666666666665,
      "grad_norm": 0.23256328701972961,
      "learning_rate": 3.787083333333333e-05,
      "loss": 0.0025,
      "step": 58220
    },
    {
      "epoch": 1.9409999999999998,
      "grad_norm": 0.05858374014496803,
      "learning_rate": 3.786875e-05,
      "loss": 0.0025,
      "step": 58230
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.05881251022219658,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0026,
      "step": 58240
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 0.20426435768604279,
      "learning_rate": 3.786458333333333e-05,
      "loss": 0.0022,
      "step": 58250
    },
    {
      "epoch": 1.942,
      "grad_norm": 0.1455921232700348,
      "learning_rate": 3.78625e-05,
      "loss": 0.0017,
      "step": 58260
    },
    {
      "epoch": 1.9423333333333335,
      "grad_norm": 0.05849464237689972,
      "learning_rate": 3.786041666666667e-05,
      "loss": 0.0024,
      "step": 58270
    },
    {
      "epoch": 1.9426666666666668,
      "grad_norm": 0.2037653774023056,
      "learning_rate": 3.7858333333333336e-05,
      "loss": 0.0025,
      "step": 58280
    },
    {
      "epoch": 1.943,
      "grad_norm": 0.26174888014793396,
      "learning_rate": 3.785625e-05,
      "loss": 0.0024,
      "step": 58290
    },
    {
      "epoch": 1.9433333333333334,
      "grad_norm": 0.17456871271133423,
      "learning_rate": 3.785416666666667e-05,
      "loss": 0.0014,
      "step": 58300
    },
    {
      "epoch": 1.9436666666666667,
      "grad_norm": 0.34927886724472046,
      "learning_rate": 3.785208333333333e-05,
      "loss": 0.0024,
      "step": 58310
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.029215814545750618,
      "learning_rate": 3.7850000000000005e-05,
      "loss": 0.0024,
      "step": 58320
    },
    {
      "epoch": 1.9443333333333332,
      "grad_norm": 0.2621314227581024,
      "learning_rate": 3.7847916666666664e-05,
      "loss": 0.0035,
      "step": 58330
    },
    {
      "epoch": 1.9446666666666665,
      "grad_norm": 0.610888659954071,
      "learning_rate": 3.7845833333333336e-05,
      "loss": 0.0019,
      "step": 58340
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 0.17455138266086578,
      "learning_rate": 3.784375e-05,
      "loss": 0.0027,
      "step": 58350
    },
    {
      "epoch": 1.9453333333333334,
      "grad_norm": 0.14554718136787415,
      "learning_rate": 3.784166666666667e-05,
      "loss": 0.0023,
      "step": 58360
    },
    {
      "epoch": 1.9456666666666667,
      "grad_norm": 0.08748379349708557,
      "learning_rate": 3.783958333333333e-05,
      "loss": 0.0029,
      "step": 58370
    },
    {
      "epoch": 1.946,
      "grad_norm": 0.14565396308898926,
      "learning_rate": 3.7837500000000005e-05,
      "loss": 0.0028,
      "step": 58380
    },
    {
      "epoch": 1.9463333333333335,
      "grad_norm": 0.3205541670322418,
      "learning_rate": 3.783541666666667e-05,
      "loss": 0.0041,
      "step": 58390
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.32031095027923584,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0034,
      "step": 58400
    },
    {
      "epoch": 1.947,
      "grad_norm": 0.0582776814699173,
      "learning_rate": 3.783125e-05,
      "loss": 0.0018,
      "step": 58410
    },
    {
      "epoch": 1.9473333333333334,
      "grad_norm": 0.4947689175605774,
      "learning_rate": 3.7829166666666674e-05,
      "loss": 0.0027,
      "step": 58420
    },
    {
      "epoch": 1.9476666666666667,
      "grad_norm": 0.029621263965964317,
      "learning_rate": 3.782708333333333e-05,
      "loss": 0.0031,
      "step": 58430
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.2911924421787262,
      "learning_rate": 3.7825e-05,
      "loss": 0.0036,
      "step": 58440
    },
    {
      "epoch": 1.9483333333333333,
      "grad_norm": 0.05848891660571098,
      "learning_rate": 3.782291666666667e-05,
      "loss": 0.0028,
      "step": 58450
    },
    {
      "epoch": 1.9486666666666665,
      "grad_norm": 0.02933344803750515,
      "learning_rate": 3.7820833333333336e-05,
      "loss": 0.0022,
      "step": 58460
    },
    {
      "epoch": 1.9489999999999998,
      "grad_norm": 0.6113241314888,
      "learning_rate": 3.781875e-05,
      "loss": 0.0018,
      "step": 58470
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.14560918509960175,
      "learning_rate": 3.7816666666666667e-05,
      "loss": 0.0024,
      "step": 58480
    },
    {
      "epoch": 1.9496666666666667,
      "grad_norm": 0.26185157895088196,
      "learning_rate": 3.781458333333334e-05,
      "loss": 0.0031,
      "step": 58490
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.6400800943374634,
      "learning_rate": 3.78125e-05,
      "loss": 0.0024,
      "step": 58500
    },
    {
      "epoch": 1.9503333333333335,
      "grad_norm": 0.2909633219242096,
      "learning_rate": 3.781041666666667e-05,
      "loss": 0.003,
      "step": 58510
    },
    {
      "epoch": 1.9506666666666668,
      "grad_norm": 0.11635299772024155,
      "learning_rate": 3.7808333333333335e-05,
      "loss": 0.0023,
      "step": 58520
    },
    {
      "epoch": 1.951,
      "grad_norm": 0.20382730662822723,
      "learning_rate": 3.780625000000001e-05,
      "loss": 0.0023,
      "step": 58530
    },
    {
      "epoch": 1.9513333333333334,
      "grad_norm": 0.029345616698265076,
      "learning_rate": 3.7804166666666666e-05,
      "loss": 0.0022,
      "step": 58540
    },
    {
      "epoch": 1.9516666666666667,
      "grad_norm": 0.17461982369422913,
      "learning_rate": 3.780208333333333e-05,
      "loss": 0.002,
      "step": 58550
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.11655434966087341,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0028,
      "step": 58560
    },
    {
      "epoch": 1.9523333333333333,
      "grad_norm": 0.15758103132247925,
      "learning_rate": 3.779791666666667e-05,
      "loss": 0.0027,
      "step": 58570
    },
    {
      "epoch": 1.9526666666666666,
      "grad_norm": 0.6688348650932312,
      "learning_rate": 3.7795833333333335e-05,
      "loss": 0.0031,
      "step": 58580
    },
    {
      "epoch": 1.9529999999999998,
      "grad_norm": 0.3782171607017517,
      "learning_rate": 3.779375e-05,
      "loss": 0.0025,
      "step": 58590
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.37788426876068115,
      "learning_rate": 3.779166666666667e-05,
      "loss": 0.0024,
      "step": 58600
    },
    {
      "epoch": 1.9536666666666667,
      "grad_norm": 0.24996910989284515,
      "learning_rate": 3.778958333333333e-05,
      "loss": 0.0049,
      "step": 58610
    },
    {
      "epoch": 1.954,
      "grad_norm": 0.03036033920943737,
      "learning_rate": 3.7787500000000004e-05,
      "loss": 0.0038,
      "step": 58620
    },
    {
      "epoch": 1.9543333333333335,
      "grad_norm": 0.3198409080505371,
      "learning_rate": 3.778541666666667e-05,
      "loss": 0.0025,
      "step": 58630
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.08733547478914261,
      "learning_rate": 3.7783333333333335e-05,
      "loss": 0.0021,
      "step": 58640
    },
    {
      "epoch": 1.955,
      "grad_norm": 0.2038668841123581,
      "learning_rate": 3.778125e-05,
      "loss": 0.0023,
      "step": 58650
    },
    {
      "epoch": 1.9553333333333334,
      "grad_norm": 0.6232738494873047,
      "learning_rate": 3.777916666666667e-05,
      "loss": 0.0032,
      "step": 58660
    },
    {
      "epoch": 1.9556666666666667,
      "grad_norm": 0.09344659000635147,
      "learning_rate": 3.777708333333334e-05,
      "loss": 0.0029,
      "step": 58670
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.029485143721103668,
      "learning_rate": 3.7775e-05,
      "loss": 0.0023,
      "step": 58680
    },
    {
      "epoch": 1.9563333333333333,
      "grad_norm": 0.37808069586753845,
      "learning_rate": 3.777291666666667e-05,
      "loss": 0.0018,
      "step": 58690
    },
    {
      "epoch": 1.9566666666666666,
      "grad_norm": 0.029281122609972954,
      "learning_rate": 3.7770833333333334e-05,
      "loss": 0.0024,
      "step": 58700
    },
    {
      "epoch": 1.9569999999999999,
      "grad_norm": 0.26185962557792664,
      "learning_rate": 3.776875e-05,
      "loss": 0.0019,
      "step": 58710
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.26177555322647095,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0032,
      "step": 58720
    },
    {
      "epoch": 1.9576666666666667,
      "grad_norm": 0.11661764234304428,
      "learning_rate": 3.776458333333334e-05,
      "loss": 0.0021,
      "step": 58730
    },
    {
      "epoch": 1.958,
      "grad_norm": 0.2906498312950134,
      "learning_rate": 3.77625e-05,
      "loss": 0.0024,
      "step": 58740
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.1455247402191162,
      "learning_rate": 3.776041666666667e-05,
      "loss": 0.0018,
      "step": 58750
    },
    {
      "epoch": 1.9586666666666668,
      "grad_norm": 0.08715187013149261,
      "learning_rate": 3.7758333333333334e-05,
      "loss": 0.0039,
      "step": 58760
    },
    {
      "epoch": 1.959,
      "grad_norm": 0.11664775758981705,
      "learning_rate": 3.7756250000000006e-05,
      "loss": 0.0025,
      "step": 58770
    },
    {
      "epoch": 1.9593333333333334,
      "grad_norm": 0.3779711127281189,
      "learning_rate": 3.7754166666666665e-05,
      "loss": 0.0027,
      "step": 58780
    },
    {
      "epoch": 1.9596666666666667,
      "grad_norm": 0.17473553121089935,
      "learning_rate": 3.775208333333334e-05,
      "loss": 0.0035,
      "step": 58790
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.23235729336738586,
      "learning_rate": 3.775e-05,
      "loss": 0.0035,
      "step": 58800
    },
    {
      "epoch": 1.9603333333333333,
      "grad_norm": 0.11634344607591629,
      "learning_rate": 3.774791666666667e-05,
      "loss": 0.0015,
      "step": 58810
    },
    {
      "epoch": 1.9606666666666666,
      "grad_norm": 0.09100871533155441,
      "learning_rate": 3.7745833333333334e-05,
      "loss": 0.0021,
      "step": 58820
    },
    {
      "epoch": 1.9609999999999999,
      "grad_norm": 0.5080832242965698,
      "learning_rate": 3.774375e-05,
      "loss": 0.002,
      "step": 58830
    },
    {
      "epoch": 1.9613333333333334,
      "grad_norm": 0.029298406094312668,
      "learning_rate": 3.774166666666667e-05,
      "loss": 0.0022,
      "step": 58840
    },
    {
      "epoch": 1.9616666666666667,
      "grad_norm": 0.2613823711872101,
      "learning_rate": 3.773958333333334e-05,
      "loss": 0.0021,
      "step": 58850
    },
    {
      "epoch": 1.962,
      "grad_norm": 0.232529878616333,
      "learning_rate": 3.77375e-05,
      "loss": 0.0025,
      "step": 58860
    },
    {
      "epoch": 1.9623333333333335,
      "grad_norm": 0.0874268114566803,
      "learning_rate": 3.773541666666667e-05,
      "loss": 0.003,
      "step": 58870
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.08736266195774078,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0025,
      "step": 58880
    },
    {
      "epoch": 1.963,
      "grad_norm": 0.029383022338151932,
      "learning_rate": 3.773125e-05,
      "loss": 0.0034,
      "step": 58890
    },
    {
      "epoch": 1.9633333333333334,
      "grad_norm": 0.08894452452659607,
      "learning_rate": 3.772916666666667e-05,
      "loss": 0.0024,
      "step": 58900
    },
    {
      "epoch": 1.9636666666666667,
      "grad_norm": 0.34926554560661316,
      "learning_rate": 3.772708333333334e-05,
      "loss": 0.0025,
      "step": 58910
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.05825909227132797,
      "learning_rate": 3.7725e-05,
      "loss": 0.0021,
      "step": 58920
    },
    {
      "epoch": 1.9643333333333333,
      "grad_norm": 0.14556245505809784,
      "learning_rate": 3.772291666666667e-05,
      "loss": 0.0016,
      "step": 58930
    },
    {
      "epoch": 1.9646666666666666,
      "grad_norm": 0.17484842240810394,
      "learning_rate": 3.772083333333333e-05,
      "loss": 0.0023,
      "step": 58940
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 0.145326167345047,
      "learning_rate": 3.7718750000000005e-05,
      "loss": 0.0022,
      "step": 58950
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.1743868738412857,
      "learning_rate": 3.7716666666666664e-05,
      "loss": 0.0025,
      "step": 58960
    },
    {
      "epoch": 1.9656666666666667,
      "grad_norm": 0.29049304127693176,
      "learning_rate": 3.7714583333333336e-05,
      "loss": 0.0027,
      "step": 58970
    },
    {
      "epoch": 1.966,
      "grad_norm": 0.26221445202827454,
      "learning_rate": 3.77125e-05,
      "loss": 0.0025,
      "step": 58980
    },
    {
      "epoch": 1.9663333333333335,
      "grad_norm": 0.523308277130127,
      "learning_rate": 3.771041666666667e-05,
      "loss": 0.0021,
      "step": 58990
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.2618124186992645,
      "learning_rate": 3.770833333333333e-05,
      "loss": 0.0027,
      "step": 59000
    },
    {
      "epoch": 1.967,
      "grad_norm": 0.11624102294445038,
      "learning_rate": 3.7706250000000005e-05,
      "loss": 0.003,
      "step": 59010
    },
    {
      "epoch": 1.9673333333333334,
      "grad_norm": 0.2325408011674881,
      "learning_rate": 3.770416666666667e-05,
      "loss": 0.0022,
      "step": 59020
    },
    {
      "epoch": 1.9676666666666667,
      "grad_norm": 0.2037026584148407,
      "learning_rate": 3.7702083333333336e-05,
      "loss": 0.0026,
      "step": 59030
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.08724763989448547,
      "learning_rate": 3.77e-05,
      "loss": 0.0034,
      "step": 59040
    },
    {
      "epoch": 1.9683333333333333,
      "grad_norm": 0.34872040152549744,
      "learning_rate": 3.769791666666667e-05,
      "loss": 0.003,
      "step": 59050
    },
    {
      "epoch": 1.9686666666666666,
      "grad_norm": 0.23247335851192474,
      "learning_rate": 3.769583333333333e-05,
      "loss": 0.0032,
      "step": 59060
    },
    {
      "epoch": 1.9689999999999999,
      "grad_norm": 0.08769591897726059,
      "learning_rate": 3.769375e-05,
      "loss": 0.003,
      "step": 59070
    },
    {
      "epoch": 1.9693333333333334,
      "grad_norm": 0.31977763772010803,
      "learning_rate": 3.769166666666667e-05,
      "loss": 0.0036,
      "step": 59080
    },
    {
      "epoch": 1.9696666666666667,
      "grad_norm": 0.17458248138427734,
      "learning_rate": 3.7689583333333336e-05,
      "loss": 0.0031,
      "step": 59090
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.17453265190124512,
      "learning_rate": 3.76875e-05,
      "loss": 0.0019,
      "step": 59100
    },
    {
      "epoch": 1.9703333333333335,
      "grad_norm": 0.7265340685844421,
      "learning_rate": 3.768541666666667e-05,
      "loss": 0.0026,
      "step": 59110
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.23232512176036835,
      "learning_rate": 3.768333333333334e-05,
      "loss": 0.002,
      "step": 59120
    },
    {
      "epoch": 1.971,
      "grad_norm": 0.40679362416267395,
      "learning_rate": 3.7681250000000005e-05,
      "loss": 0.0028,
      "step": 59130
    },
    {
      "epoch": 1.9713333333333334,
      "grad_norm": 0.14535364508628845,
      "learning_rate": 3.767916666666667e-05,
      "loss": 0.0029,
      "step": 59140
    },
    {
      "epoch": 1.9716666666666667,
      "grad_norm": 0.34864145517349243,
      "learning_rate": 3.7677083333333335e-05,
      "loss": 0.0027,
      "step": 59150
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.17447125911712646,
      "learning_rate": 3.7675e-05,
      "loss": 0.0023,
      "step": 59160
    },
    {
      "epoch": 1.9723333333333333,
      "grad_norm": 0.08738674968481064,
      "learning_rate": 3.7672916666666666e-05,
      "loss": 0.002,
      "step": 59170
    },
    {
      "epoch": 1.9726666666666666,
      "grad_norm": 0.11607609689235687,
      "learning_rate": 3.767083333333333e-05,
      "loss": 0.0022,
      "step": 59180
    },
    {
      "epoch": 1.9729999999999999,
      "grad_norm": 0.3115399181842804,
      "learning_rate": 3.7668750000000004e-05,
      "loss": 0.0031,
      "step": 59190
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.005110078025609255,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0023,
      "step": 59200
    },
    {
      "epoch": 1.9736666666666667,
      "grad_norm": 0.5231570601463318,
      "learning_rate": 3.7664583333333335e-05,
      "loss": 0.0015,
      "step": 59210
    },
    {
      "epoch": 1.974,
      "grad_norm": 0.14541253447532654,
      "learning_rate": 3.76625e-05,
      "loss": 0.0021,
      "step": 59220
    },
    {
      "epoch": 1.9743333333333335,
      "grad_norm": 0.37764042615890503,
      "learning_rate": 3.766041666666667e-05,
      "loss": 0.0021,
      "step": 59230
    },
    {
      "epoch": 1.9746666666666668,
      "grad_norm": 0.4659244120121002,
      "learning_rate": 3.765833333333333e-05,
      "loss": 0.0018,
      "step": 59240
    },
    {
      "epoch": 1.975,
      "grad_norm": 0.6970057487487793,
      "learning_rate": 3.7656250000000004e-05,
      "loss": 0.0038,
      "step": 59250
    },
    {
      "epoch": 1.9753333333333334,
      "grad_norm": 0.08800386637449265,
      "learning_rate": 3.765416666666667e-05,
      "loss": 0.0022,
      "step": 59260
    },
    {
      "epoch": 1.9756666666666667,
      "grad_norm": 0.058419119566679,
      "learning_rate": 3.7652083333333335e-05,
      "loss": 0.0025,
      "step": 59270
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.174614816904068,
      "learning_rate": 3.765e-05,
      "loss": 0.0023,
      "step": 59280
    },
    {
      "epoch": 1.9763333333333333,
      "grad_norm": 0.536781907081604,
      "learning_rate": 3.7647916666666666e-05,
      "loss": 0.0034,
      "step": 59290
    },
    {
      "epoch": 1.9766666666666666,
      "grad_norm": 0.8291332721710205,
      "learning_rate": 3.764583333333334e-05,
      "loss": 0.002,
      "step": 59300
    },
    {
      "epoch": 1.9769999999999999,
      "grad_norm": 0.11610625684261322,
      "learning_rate": 3.764375e-05,
      "loss": 0.003,
      "step": 59310
    },
    {
      "epoch": 1.9773333333333334,
      "grad_norm": 0.14528264105319977,
      "learning_rate": 3.764166666666667e-05,
      "loss": 0.002,
      "step": 59320
    },
    {
      "epoch": 1.9776666666666667,
      "grad_norm": 0.20342282950878143,
      "learning_rate": 3.7639583333333335e-05,
      "loss": 0.0024,
      "step": 59330
    },
    {
      "epoch": 1.978,
      "grad_norm": 0.11625140905380249,
      "learning_rate": 3.76375e-05,
      "loss": 0.0017,
      "step": 59340
    },
    {
      "epoch": 1.9783333333333335,
      "grad_norm": 0.23247064650058746,
      "learning_rate": 3.7635416666666666e-05,
      "loss": 0.0024,
      "step": 59350
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.05829831585288048,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0023,
      "step": 59360
    },
    {
      "epoch": 1.979,
      "grad_norm": 0.11645705252885818,
      "learning_rate": 3.763125e-05,
      "loss": 0.0034,
      "step": 59370
    },
    {
      "epoch": 1.9793333333333334,
      "grad_norm": 0.058246221393346786,
      "learning_rate": 3.762916666666667e-05,
      "loss": 0.0024,
      "step": 59380
    },
    {
      "epoch": 1.9796666666666667,
      "grad_norm": 0.6395693421363831,
      "learning_rate": 3.7627083333333334e-05,
      "loss": 0.0022,
      "step": 59390
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.46498018503189087,
      "learning_rate": 3.7625e-05,
      "loss": 0.0022,
      "step": 59400
    },
    {
      "epoch": 1.9803333333333333,
      "grad_norm": 0.05859369412064552,
      "learning_rate": 3.762291666666667e-05,
      "loss": 0.0017,
      "step": 59410
    },
    {
      "epoch": 1.9806666666666666,
      "grad_norm": 0.17455336451530457,
      "learning_rate": 3.762083333333333e-05,
      "loss": 0.0025,
      "step": 59420
    },
    {
      "epoch": 1.9809999999999999,
      "grad_norm": 0.08742757886648178,
      "learning_rate": 3.761875e-05,
      "loss": 0.0021,
      "step": 59430
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.5641353726387024,
      "learning_rate": 3.761666666666667e-05,
      "loss": 0.0019,
      "step": 59440
    },
    {
      "epoch": 1.9816666666666667,
      "grad_norm": 0.2906952202320099,
      "learning_rate": 3.7614583333333334e-05,
      "loss": 0.0025,
      "step": 59450
    },
    {
      "epoch": 1.982,
      "grad_norm": 0.6650246381759644,
      "learning_rate": 3.76125e-05,
      "loss": 0.0018,
      "step": 59460
    },
    {
      "epoch": 1.9823333333333333,
      "grad_norm": 0.08726754784584045,
      "learning_rate": 3.761041666666667e-05,
      "loss": 0.0019,
      "step": 59470
    },
    {
      "epoch": 1.9826666666666668,
      "grad_norm": 0.6102424263954163,
      "learning_rate": 3.760833333333334e-05,
      "loss": 0.002,
      "step": 59480
    },
    {
      "epoch": 1.983,
      "grad_norm": 0.08756003528833389,
      "learning_rate": 3.760625e-05,
      "loss": 0.0015,
      "step": 59490
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.2324228733778,
      "learning_rate": 3.760416666666667e-05,
      "loss": 0.0023,
      "step": 59500
    },
    {
      "epoch": 1.9836666666666667,
      "grad_norm": 0.029381247237324715,
      "learning_rate": 3.760208333333334e-05,
      "loss": 0.0022,
      "step": 59510
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.444263219833374,
      "learning_rate": 3.76e-05,
      "loss": 0.0026,
      "step": 59520
    },
    {
      "epoch": 1.9843333333333333,
      "grad_norm": 0.43586692214012146,
      "learning_rate": 3.7597916666666665e-05,
      "loss": 0.0022,
      "step": 59530
    },
    {
      "epoch": 1.9846666666666666,
      "grad_norm": 0.7821112871170044,
      "learning_rate": 3.759583333333334e-05,
      "loss": 0.0017,
      "step": 59540
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 0.1742672324180603,
      "learning_rate": 3.759375e-05,
      "loss": 0.0022,
      "step": 59550
    },
    {
      "epoch": 1.9853333333333332,
      "grad_norm": 0.08714375644922256,
      "learning_rate": 3.759166666666667e-05,
      "loss": 0.0023,
      "step": 59560
    },
    {
      "epoch": 1.9856666666666667,
      "grad_norm": 0.08792652189731598,
      "learning_rate": 3.758958333333333e-05,
      "loss": 0.0018,
      "step": 59570
    },
    {
      "epoch": 1.986,
      "grad_norm": 0.5809624195098877,
      "learning_rate": 3.7587500000000006e-05,
      "loss": 0.0037,
      "step": 59580
    },
    {
      "epoch": 1.9863333333333333,
      "grad_norm": 0.20344923436641693,
      "learning_rate": 3.7585416666666664e-05,
      "loss": 0.0018,
      "step": 59590
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.14538371562957764,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.0028,
      "step": 59600
    },
    {
      "epoch": 1.987,
      "grad_norm": 0.058350082486867905,
      "learning_rate": 3.758125e-05,
      "loss": 0.0022,
      "step": 59610
    },
    {
      "epoch": 1.9873333333333334,
      "grad_norm": 0.14525309205055237,
      "learning_rate": 3.757916666666667e-05,
      "loss": 0.0017,
      "step": 59620
    },
    {
      "epoch": 1.9876666666666667,
      "grad_norm": 0.08727605640888214,
      "learning_rate": 3.757708333333333e-05,
      "loss": 0.0028,
      "step": 59630
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.004029980394989252,
      "learning_rate": 3.7575e-05,
      "loss": 0.0021,
      "step": 59640
    },
    {
      "epoch": 1.9883333333333333,
      "grad_norm": 0.31947270035743713,
      "learning_rate": 3.757291666666667e-05,
      "loss": 0.0023,
      "step": 59650
    },
    {
      "epoch": 1.9886666666666666,
      "grad_norm": 0.0873841792345047,
      "learning_rate": 3.757083333333333e-05,
      "loss": 0.0019,
      "step": 59660
    },
    {
      "epoch": 1.9889999999999999,
      "grad_norm": 0.26156243681907654,
      "learning_rate": 3.756875e-05,
      "loss": 0.0018,
      "step": 59670
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.02989722415804863,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.0015,
      "step": 59680
    },
    {
      "epoch": 1.9896666666666667,
      "grad_norm": 0.261106014251709,
      "learning_rate": 3.756458333333334e-05,
      "loss": 0.0019,
      "step": 59690
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.5669428706169128,
      "learning_rate": 3.75625e-05,
      "loss": 0.0021,
      "step": 59700
    },
    {
      "epoch": 1.9903333333333333,
      "grad_norm": 0.08725805580615997,
      "learning_rate": 3.756041666666667e-05,
      "loss": 0.002,
      "step": 59710
    },
    {
      "epoch": 1.9906666666666668,
      "grad_norm": 0.20349916815757751,
      "learning_rate": 3.7558333333333336e-05,
      "loss": 0.0021,
      "step": 59720
    },
    {
      "epoch": 1.991,
      "grad_norm": 0.20360296964645386,
      "learning_rate": 3.755625e-05,
      "loss": 0.0019,
      "step": 59730
    },
    {
      "epoch": 1.9913333333333334,
      "grad_norm": 0.4938361346721649,
      "learning_rate": 3.755416666666667e-05,
      "loss": 0.0024,
      "step": 59740
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 0.05815915763378143,
      "learning_rate": 3.755208333333334e-05,
      "loss": 0.0027,
      "step": 59750
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.08729074895381927,
      "learning_rate": 3.7550000000000005e-05,
      "loss": 0.0033,
      "step": 59760
    },
    {
      "epoch": 1.9923333333333333,
      "grad_norm": 0.08497167378664017,
      "learning_rate": 3.7547916666666663e-05,
      "loss": 0.0016,
      "step": 59770
    },
    {
      "epoch": 1.9926666666666666,
      "grad_norm": 0.17418363690376282,
      "learning_rate": 3.7545833333333336e-05,
      "loss": 0.0014,
      "step": 59780
    },
    {
      "epoch": 1.9929999999999999,
      "grad_norm": 0.6975363492965698,
      "learning_rate": 3.754375e-05,
      "loss": 0.0017,
      "step": 59790
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.17434214055538177,
      "learning_rate": 3.754166666666667e-05,
      "loss": 0.0035,
      "step": 59800
    },
    {
      "epoch": 1.9936666666666667,
      "grad_norm": 0.31953445076942444,
      "learning_rate": 3.753958333333333e-05,
      "loss": 0.0019,
      "step": 59810
    },
    {
      "epoch": 1.994,
      "grad_norm": 0.40701356530189514,
      "learning_rate": 3.7537500000000004e-05,
      "loss": 0.0017,
      "step": 59820
    },
    {
      "epoch": 1.9943333333333333,
      "grad_norm": 0.1454705148935318,
      "learning_rate": 3.753541666666667e-05,
      "loss": 0.0029,
      "step": 59830
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.05820811912417412,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0031,
      "step": 59840
    },
    {
      "epoch": 1.995,
      "grad_norm": 0.17436076700687408,
      "learning_rate": 3.753125e-05,
      "loss": 0.003,
      "step": 59850
    },
    {
      "epoch": 1.9953333333333334,
      "grad_norm": 0.1453225165605545,
      "learning_rate": 3.752916666666667e-05,
      "loss": 0.0017,
      "step": 59860
    },
    {
      "epoch": 1.9956666666666667,
      "grad_norm": 0.0875169038772583,
      "learning_rate": 3.752708333333333e-05,
      "loss": 0.0024,
      "step": 59870
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.08759210258722305,
      "learning_rate": 3.7525e-05,
      "loss": 0.0035,
      "step": 59880
    },
    {
      "epoch": 1.9963333333333333,
      "grad_norm": 0.2324305921792984,
      "learning_rate": 3.752291666666667e-05,
      "loss": 0.0021,
      "step": 59890
    },
    {
      "epoch": 1.9966666666666666,
      "grad_norm": 0.08726312965154648,
      "learning_rate": 3.7520833333333335e-05,
      "loss": 0.0028,
      "step": 59900
    },
    {
      "epoch": 1.9969999999999999,
      "grad_norm": 0.46509677171707153,
      "learning_rate": 3.751875e-05,
      "loss": 0.0029,
      "step": 59910
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.05825436860322952,
      "learning_rate": 3.7516666666666666e-05,
      "loss": 0.0035,
      "step": 59920
    },
    {
      "epoch": 1.9976666666666667,
      "grad_norm": 0.030283113941550255,
      "learning_rate": 3.751458333333334e-05,
      "loss": 0.0029,
      "step": 59930
    },
    {
      "epoch": 1.998,
      "grad_norm": 0.2903231382369995,
      "learning_rate": 3.7512500000000004e-05,
      "loss": 0.0022,
      "step": 59940
    },
    {
      "epoch": 1.9983333333333333,
      "grad_norm": 0.17438343167304993,
      "learning_rate": 3.751041666666667e-05,
      "loss": 0.0028,
      "step": 59950
    },
    {
      "epoch": 1.9986666666666668,
      "grad_norm": 0.11624130606651306,
      "learning_rate": 3.7508333333333335e-05,
      "loss": 0.0023,
      "step": 59960
    },
    {
      "epoch": 1.999,
      "grad_norm": 0.14554400742053986,
      "learning_rate": 3.750625000000001e-05,
      "loss": 0.0022,
      "step": 59970
    },
    {
      "epoch": 1.9993333333333334,
      "grad_norm": 0.029505373910069466,
      "learning_rate": 3.7504166666666666e-05,
      "loss": 0.0028,
      "step": 59980
    },
    {
      "epoch": 1.9996666666666667,
      "grad_norm": 0.7694029808044434,
      "learning_rate": 3.750208333333334e-05,
      "loss": 0.0029,
      "step": 59990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.29056358337402344,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0032,
      "step": 60000
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.002387440763413906,
      "eval_runtime": 133.7555,
      "eval_samples_per_second": 1495.265,
      "eval_steps_per_second": 37.382,
      "step": 60000
    },
    {
      "epoch": 2.0003333333333333,
      "grad_norm": 0.29052743315696716,
      "learning_rate": 3.749791666666667e-05,
      "loss": 0.002,
      "step": 60010
    },
    {
      "epoch": 2.0006666666666666,
      "grad_norm": 0.5811926126480103,
      "learning_rate": 3.7495833333333334e-05,
      "loss": 0.0024,
      "step": 60020
    },
    {
      "epoch": 2.001,
      "grad_norm": 0.11604063957929611,
      "learning_rate": 3.749375e-05,
      "loss": 0.002,
      "step": 60030
    },
    {
      "epoch": 2.001333333333333,
      "grad_norm": 0.6026103496551514,
      "learning_rate": 3.749166666666667e-05,
      "loss": 0.0035,
      "step": 60040
    },
    {
      "epoch": 2.0016666666666665,
      "grad_norm": 0.029780138283967972,
      "learning_rate": 3.748958333333333e-05,
      "loss": 0.0038,
      "step": 60050
    },
    {
      "epoch": 2.002,
      "grad_norm": 0.08713049441576004,
      "learning_rate": 3.74875e-05,
      "loss": 0.0047,
      "step": 60060
    },
    {
      "epoch": 2.0023333333333335,
      "grad_norm": 0.2030973881483078,
      "learning_rate": 3.748541666666667e-05,
      "loss": 0.0023,
      "step": 60070
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.3193824589252472,
      "learning_rate": 3.7483333333333334e-05,
      "loss": 0.0014,
      "step": 60080
    },
    {
      "epoch": 2.003,
      "grad_norm": 0.3740662932395935,
      "learning_rate": 3.748125e-05,
      "loss": 0.0029,
      "step": 60090
    },
    {
      "epoch": 2.0033333333333334,
      "grad_norm": 0.20355314016342163,
      "learning_rate": 3.747916666666667e-05,
      "loss": 0.0036,
      "step": 60100
    },
    {
      "epoch": 2.0036666666666667,
      "grad_norm": 0.11627242714166641,
      "learning_rate": 3.747708333333334e-05,
      "loss": 0.0029,
      "step": 60110
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.2324211448431015,
      "learning_rate": 3.7475e-05,
      "loss": 0.0034,
      "step": 60120
    },
    {
      "epoch": 2.0043333333333333,
      "grad_norm": 0.029376985505223274,
      "learning_rate": 3.747291666666667e-05,
      "loss": 0.0021,
      "step": 60130
    },
    {
      "epoch": 2.0046666666666666,
      "grad_norm": 0.05815320089459419,
      "learning_rate": 3.7470833333333334e-05,
      "loss": 0.0032,
      "step": 60140
    },
    {
      "epoch": 2.005,
      "grad_norm": 0.05810989812016487,
      "learning_rate": 3.746875e-05,
      "loss": 0.0027,
      "step": 60150
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.08719713985919952,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0019,
      "step": 60160
    },
    {
      "epoch": 2.0056666666666665,
      "grad_norm": 0.696887731552124,
      "learning_rate": 3.746458333333334e-05,
      "loss": 0.0018,
      "step": 60170
    },
    {
      "epoch": 2.006,
      "grad_norm": 0.31945672631263733,
      "learning_rate": 3.74625e-05,
      "loss": 0.0029,
      "step": 60180
    },
    {
      "epoch": 2.0063333333333335,
      "grad_norm": 0.20325259864330292,
      "learning_rate": 3.746041666666667e-05,
      "loss": 0.0023,
      "step": 60190
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.1162458285689354,
      "learning_rate": 3.7458333333333334e-05,
      "loss": 0.0025,
      "step": 60200
    },
    {
      "epoch": 2.007,
      "grad_norm": 0.0041793896816670895,
      "learning_rate": 3.7456250000000006e-05,
      "loss": 0.0022,
      "step": 60210
    },
    {
      "epoch": 2.0073333333333334,
      "grad_norm": 0.06066153198480606,
      "learning_rate": 3.745416666666667e-05,
      "loss": 0.0032,
      "step": 60220
    },
    {
      "epoch": 2.0076666666666667,
      "grad_norm": 1.0495680570602417,
      "learning_rate": 3.745208333333334e-05,
      "loss": 0.0034,
      "step": 60230
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.14573146402835846,
      "learning_rate": 3.745e-05,
      "loss": 0.0023,
      "step": 60240
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 0.2690756916999817,
      "learning_rate": 3.744791666666667e-05,
      "loss": 0.0027,
      "step": 60250
    },
    {
      "epoch": 2.0086666666666666,
      "grad_norm": 0.2618360221385956,
      "learning_rate": 3.744583333333333e-05,
      "loss": 0.0029,
      "step": 60260
    },
    {
      "epoch": 2.009,
      "grad_norm": 0.2612382471561432,
      "learning_rate": 3.744375e-05,
      "loss": 0.0027,
      "step": 60270
    },
    {
      "epoch": 2.009333333333333,
      "grad_norm": 0.1452232301235199,
      "learning_rate": 3.744166666666667e-05,
      "loss": 0.0019,
      "step": 60280
    },
    {
      "epoch": 2.0096666666666665,
      "grad_norm": 0.1453099399805069,
      "learning_rate": 3.7439583333333336e-05,
      "loss": 0.0027,
      "step": 60290
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.11637509614229202,
      "learning_rate": 3.74375e-05,
      "loss": 0.0023,
      "step": 60300
    },
    {
      "epoch": 2.0103333333333335,
      "grad_norm": 0.23243556916713715,
      "learning_rate": 3.743541666666667e-05,
      "loss": 0.003,
      "step": 60310
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.38492950797080994,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.0034,
      "step": 60320
    },
    {
      "epoch": 2.011,
      "grad_norm": 0.17427146434783936,
      "learning_rate": 3.743125e-05,
      "loss": 0.0026,
      "step": 60330
    },
    {
      "epoch": 2.0113333333333334,
      "grad_norm": 0.17470838129520416,
      "learning_rate": 3.742916666666667e-05,
      "loss": 0.0024,
      "step": 60340
    },
    {
      "epoch": 2.0116666666666667,
      "grad_norm": 0.2615402638912201,
      "learning_rate": 3.7427083333333336e-05,
      "loss": 0.0023,
      "step": 60350
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.26153165102005005,
      "learning_rate": 3.7425e-05,
      "loss": 0.0038,
      "step": 60360
    },
    {
      "epoch": 2.0123333333333333,
      "grad_norm": 0.3485737144947052,
      "learning_rate": 3.742291666666667e-05,
      "loss": 0.0019,
      "step": 60370
    },
    {
      "epoch": 2.0126666666666666,
      "grad_norm": 0.5517638325691223,
      "learning_rate": 3.742083333333333e-05,
      "loss": 0.0035,
      "step": 60380
    },
    {
      "epoch": 2.013,
      "grad_norm": 0.05849788710474968,
      "learning_rate": 3.7418750000000005e-05,
      "loss": 0.0016,
      "step": 60390
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.08716076612472534,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.0019,
      "step": 60400
    },
    {
      "epoch": 2.0136666666666665,
      "grad_norm": 0.3774074614048004,
      "learning_rate": 3.7414583333333336e-05,
      "loss": 0.0028,
      "step": 60410
    },
    {
      "epoch": 2.014,
      "grad_norm": 0.029465826228260994,
      "learning_rate": 3.74125e-05,
      "loss": 0.0024,
      "step": 60420
    },
    {
      "epoch": 2.0143333333333335,
      "grad_norm": 0.003957932349294424,
      "learning_rate": 3.741041666666667e-05,
      "loss": 0.003,
      "step": 60430
    },
    {
      "epoch": 2.014666666666667,
      "grad_norm": 0.17430336773395538,
      "learning_rate": 3.740833333333333e-05,
      "loss": 0.0016,
      "step": 60440
    },
    {
      "epoch": 2.015,
      "grad_norm": 0.11646899580955505,
      "learning_rate": 3.7406250000000005e-05,
      "loss": 0.0016,
      "step": 60450
    },
    {
      "epoch": 2.0153333333333334,
      "grad_norm": 0.05824039503931999,
      "learning_rate": 3.740416666666667e-05,
      "loss": 0.0029,
      "step": 60460
    },
    {
      "epoch": 2.0156666666666667,
      "grad_norm": 0.2615096867084503,
      "learning_rate": 3.7402083333333336e-05,
      "loss": 0.002,
      "step": 60470
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.2903915345668793,
      "learning_rate": 3.74e-05,
      "loss": 0.0021,
      "step": 60480
    },
    {
      "epoch": 2.0163333333333333,
      "grad_norm": 0.0296509750187397,
      "learning_rate": 3.7397916666666667e-05,
      "loss": 0.0033,
      "step": 60490
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.029568281024694443,
      "learning_rate": 3.739583333333334e-05,
      "loss": 0.0023,
      "step": 60500
    },
    {
      "epoch": 2.017,
      "grad_norm": 0.005305535160005093,
      "learning_rate": 3.739375e-05,
      "loss": 0.0029,
      "step": 60510
    },
    {
      "epoch": 2.017333333333333,
      "grad_norm": 0.31928062438964844,
      "learning_rate": 3.739166666666667e-05,
      "loss": 0.0024,
      "step": 60520
    },
    {
      "epoch": 2.0176666666666665,
      "grad_norm": 0.02930935099720955,
      "learning_rate": 3.7389583333333335e-05,
      "loss": 0.0029,
      "step": 60530
    },
    {
      "epoch": 2.018,
      "grad_norm": 0.371049165725708,
      "learning_rate": 3.73875e-05,
      "loss": 0.0025,
      "step": 60540
    },
    {
      "epoch": 2.0183333333333335,
      "grad_norm": 0.31928637623786926,
      "learning_rate": 3.7385416666666666e-05,
      "loss": 0.0024,
      "step": 60550
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.23199479281902313,
      "learning_rate": 3.738333333333334e-05,
      "loss": 0.0022,
      "step": 60560
    },
    {
      "epoch": 2.019,
      "grad_norm": 0.20336085557937622,
      "learning_rate": 3.7381250000000004e-05,
      "loss": 0.0027,
      "step": 60570
    },
    {
      "epoch": 2.0193333333333334,
      "grad_norm": 0.14511843025684357,
      "learning_rate": 3.737916666666667e-05,
      "loss": 0.0031,
      "step": 60580
    },
    {
      "epoch": 2.0196666666666667,
      "grad_norm": 0.05847976356744766,
      "learning_rate": 3.7377083333333335e-05,
      "loss": 0.0021,
      "step": 60590
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.030440060421824455,
      "learning_rate": 3.737500000000001e-05,
      "loss": 0.0024,
      "step": 60600
    },
    {
      "epoch": 2.0203333333333333,
      "grad_norm": 0.23236005008220673,
      "learning_rate": 3.7372916666666666e-05,
      "loss": 0.0021,
      "step": 60610
    },
    {
      "epoch": 2.0206666666666666,
      "grad_norm": 0.290054053068161,
      "learning_rate": 3.737083333333333e-05,
      "loss": 0.0032,
      "step": 60620
    },
    {
      "epoch": 2.021,
      "grad_norm": 0.4361395537853241,
      "learning_rate": 3.7368750000000004e-05,
      "loss": 0.0026,
      "step": 60630
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.3126579821109772,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.0021,
      "step": 60640
    },
    {
      "epoch": 2.0216666666666665,
      "grad_norm": 0.20358504354953766,
      "learning_rate": 3.7364583333333335e-05,
      "loss": 0.0019,
      "step": 60650
    },
    {
      "epoch": 2.022,
      "grad_norm": 0.02927161380648613,
      "learning_rate": 3.73625e-05,
      "loss": 0.002,
      "step": 60660
    },
    {
      "epoch": 2.0223333333333335,
      "grad_norm": 0.030036423355340958,
      "learning_rate": 3.736041666666667e-05,
      "loss": 0.0019,
      "step": 60670
    },
    {
      "epoch": 2.022666666666667,
      "grad_norm": 0.0870446190237999,
      "learning_rate": 3.735833333333333e-05,
      "loss": 0.0024,
      "step": 60680
    },
    {
      "epoch": 2.023,
      "grad_norm": 0.11619090288877487,
      "learning_rate": 3.735625e-05,
      "loss": 0.003,
      "step": 60690
    },
    {
      "epoch": 2.0233333333333334,
      "grad_norm": 0.17422325909137726,
      "learning_rate": 3.735416666666667e-05,
      "loss": 0.0023,
      "step": 60700
    },
    {
      "epoch": 2.0236666666666667,
      "grad_norm": 0.4062839150428772,
      "learning_rate": 3.7352083333333334e-05,
      "loss": 0.0025,
      "step": 60710
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.24043318629264832,
      "learning_rate": 3.735e-05,
      "loss": 0.0025,
      "step": 60720
    },
    {
      "epoch": 2.0243333333333333,
      "grad_norm": 0.4934557378292084,
      "learning_rate": 3.7347916666666665e-05,
      "loss": 0.0036,
      "step": 60730
    },
    {
      "epoch": 2.0246666666666666,
      "grad_norm": 0.20382718741893768,
      "learning_rate": 3.734583333333334e-05,
      "loss": 0.0018,
      "step": 60740
    },
    {
      "epoch": 2.025,
      "grad_norm": 0.029343487694859505,
      "learning_rate": 3.7343749999999996e-05,
      "loss": 0.0031,
      "step": 60750
    },
    {
      "epoch": 2.025333333333333,
      "grad_norm": 0.11730615049600601,
      "learning_rate": 3.734166666666667e-05,
      "loss": 0.003,
      "step": 60760
    },
    {
      "epoch": 2.0256666666666665,
      "grad_norm": 0.14519470930099487,
      "learning_rate": 3.7339583333333334e-05,
      "loss": 0.003,
      "step": 60770
    },
    {
      "epoch": 2.026,
      "grad_norm": 0.7468770742416382,
      "learning_rate": 3.7337500000000006e-05,
      "loss": 0.0017,
      "step": 60780
    },
    {
      "epoch": 2.0263333333333335,
      "grad_norm": 0.14490751922130585,
      "learning_rate": 3.7335416666666665e-05,
      "loss": 0.0026,
      "step": 60790
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.46439599990844727,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0024,
      "step": 60800
    },
    {
      "epoch": 2.027,
      "grad_norm": 0.23243828117847443,
      "learning_rate": 3.733125e-05,
      "loss": 0.0017,
      "step": 60810
    },
    {
      "epoch": 2.0273333333333334,
      "grad_norm": 0.029699064791202545,
      "learning_rate": 3.732916666666667e-05,
      "loss": 0.0025,
      "step": 60820
    },
    {
      "epoch": 2.0276666666666667,
      "grad_norm": 0.29008814692497253,
      "learning_rate": 3.7327083333333334e-05,
      "loss": 0.0026,
      "step": 60830
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.05817648023366928,
      "learning_rate": 3.7325000000000006e-05,
      "loss": 0.0018,
      "step": 60840
    },
    {
      "epoch": 2.0283333333333333,
      "grad_norm": 0.23225265741348267,
      "learning_rate": 3.732291666666667e-05,
      "loss": 0.0017,
      "step": 60850
    },
    {
      "epoch": 2.0286666666666666,
      "grad_norm": 0.1457654982805252,
      "learning_rate": 3.732083333333333e-05,
      "loss": 0.0041,
      "step": 60860
    },
    {
      "epoch": 2.029,
      "grad_norm": 0.11623094975948334,
      "learning_rate": 3.731875e-05,
      "loss": 0.0025,
      "step": 60870
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.5514702796936035,
      "learning_rate": 3.731666666666667e-05,
      "loss": 0.0023,
      "step": 60880
    },
    {
      "epoch": 2.0296666666666665,
      "grad_norm": 0.6092449426651001,
      "learning_rate": 3.7314583333333333e-05,
      "loss": 0.0027,
      "step": 60890
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.022006575018167496,
      "learning_rate": 3.73125e-05,
      "loss": 0.0025,
      "step": 60900
    },
    {
      "epoch": 2.0303333333333335,
      "grad_norm": 0.16568666696548462,
      "learning_rate": 3.731041666666667e-05,
      "loss": 0.0023,
      "step": 60910
    },
    {
      "epoch": 2.030666666666667,
      "grad_norm": 0.46449291706085205,
      "learning_rate": 3.730833333333334e-05,
      "loss": 0.0017,
      "step": 60920
    },
    {
      "epoch": 2.031,
      "grad_norm": 0.1741202473640442,
      "learning_rate": 3.730625e-05,
      "loss": 0.0021,
      "step": 60930
    },
    {
      "epoch": 2.0313333333333334,
      "grad_norm": 0.3086428940296173,
      "learning_rate": 3.730416666666667e-05,
      "loss": 0.0026,
      "step": 60940
    },
    {
      "epoch": 2.0316666666666667,
      "grad_norm": 0.11598467081785202,
      "learning_rate": 3.730208333333334e-05,
      "loss": 0.0035,
      "step": 60950
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.17443688213825226,
      "learning_rate": 3.73e-05,
      "loss": 0.0028,
      "step": 60960
    },
    {
      "epoch": 2.0323333333333333,
      "grad_norm": 0.14534065127372742,
      "learning_rate": 3.7297916666666664e-05,
      "loss": 0.0017,
      "step": 60970
    },
    {
      "epoch": 2.0326666666666666,
      "grad_norm": 0.5364734530448914,
      "learning_rate": 3.7295833333333336e-05,
      "loss": 0.0014,
      "step": 60980
    },
    {
      "epoch": 2.033,
      "grad_norm": 0.004064171575009823,
      "learning_rate": 3.729375e-05,
      "loss": 0.0021,
      "step": 60990
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.7196623682975769,
      "learning_rate": 3.729166666666667e-05,
      "loss": 0.0025,
      "step": 61000
    },
    {
      "epoch": 2.0336666666666665,
      "grad_norm": 0.14536096155643463,
      "learning_rate": 3.728958333333333e-05,
      "loss": 0.0022,
      "step": 61010
    },
    {
      "epoch": 2.034,
      "grad_norm": 0.20303264260292053,
      "learning_rate": 3.7287500000000005e-05,
      "loss": 0.0021,
      "step": 61020
    },
    {
      "epoch": 2.0343333333333335,
      "grad_norm": 0.14516209065914154,
      "learning_rate": 3.7285416666666664e-05,
      "loss": 0.0026,
      "step": 61030
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.3034590482711792,
      "learning_rate": 3.7283333333333336e-05,
      "loss": 0.0029,
      "step": 61040
    },
    {
      "epoch": 2.035,
      "grad_norm": 0.553105890750885,
      "learning_rate": 3.728125e-05,
      "loss": 0.0032,
      "step": 61050
    },
    {
      "epoch": 2.0353333333333334,
      "grad_norm": 0.1868419498205185,
      "learning_rate": 3.7279166666666674e-05,
      "loss": 0.0024,
      "step": 61060
    },
    {
      "epoch": 2.0356666666666667,
      "grad_norm": 0.20305651426315308,
      "learning_rate": 3.727708333333333e-05,
      "loss": 0.0027,
      "step": 61070
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.3192470669746399,
      "learning_rate": 3.7275000000000005e-05,
      "loss": 0.0026,
      "step": 61080
    },
    {
      "epoch": 2.0363333333333333,
      "grad_norm": 0.3480384051799774,
      "learning_rate": 3.727291666666667e-05,
      "loss": 0.0023,
      "step": 61090
    },
    {
      "epoch": 2.0366666666666666,
      "grad_norm": 0.2610873878002167,
      "learning_rate": 3.7270833333333336e-05,
      "loss": 0.002,
      "step": 61100
    },
    {
      "epoch": 2.037,
      "grad_norm": 0.05872153490781784,
      "learning_rate": 3.726875e-05,
      "loss": 0.002,
      "step": 61110
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.3009139895439148,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0038,
      "step": 61120
    },
    {
      "epoch": 2.0376666666666665,
      "grad_norm": 0.14524349570274353,
      "learning_rate": 3.726458333333334e-05,
      "loss": 0.0017,
      "step": 61130
    },
    {
      "epoch": 2.038,
      "grad_norm": 0.1453704684972763,
      "learning_rate": 3.72625e-05,
      "loss": 0.0022,
      "step": 61140
    },
    {
      "epoch": 2.038333333333333,
      "grad_norm": 0.1747109442949295,
      "learning_rate": 3.726041666666667e-05,
      "loss": 0.0021,
      "step": 61150
    },
    {
      "epoch": 2.038666666666667,
      "grad_norm": 0.058002762496471405,
      "learning_rate": 3.7258333333333335e-05,
      "loss": 0.002,
      "step": 61160
    },
    {
      "epoch": 2.039,
      "grad_norm": 0.6963753700256348,
      "learning_rate": 3.725625e-05,
      "loss": 0.0012,
      "step": 61170
    },
    {
      "epoch": 2.0393333333333334,
      "grad_norm": 0.2899421453475952,
      "learning_rate": 3.7254166666666666e-05,
      "loss": 0.0022,
      "step": 61180
    },
    {
      "epoch": 2.0396666666666667,
      "grad_norm": 0.17407318949699402,
      "learning_rate": 3.725208333333334e-05,
      "loss": 0.002,
      "step": 61190
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.14593073725700378,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.0031,
      "step": 61200
    },
    {
      "epoch": 2.0403333333333333,
      "grad_norm": 0.11684173345565796,
      "learning_rate": 3.724791666666666e-05,
      "loss": 0.0032,
      "step": 61210
    },
    {
      "epoch": 2.0406666666666666,
      "grad_norm": 0.11703170090913773,
      "learning_rate": 3.7245833333333335e-05,
      "loss": 0.002,
      "step": 61220
    },
    {
      "epoch": 2.041,
      "grad_norm": 0.5513273477554321,
      "learning_rate": 3.724375e-05,
      "loss": 0.002,
      "step": 61230
    },
    {
      "epoch": 2.041333333333333,
      "grad_norm": 0.0872345045208931,
      "learning_rate": 3.7241666666666666e-05,
      "loss": 0.0025,
      "step": 61240
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 0.05857912078499794,
      "learning_rate": 3.723958333333333e-05,
      "loss": 0.0024,
      "step": 61250
    },
    {
      "epoch": 2.042,
      "grad_norm": 0.6781279444694519,
      "learning_rate": 3.7237500000000004e-05,
      "loss": 0.0035,
      "step": 61260
    },
    {
      "epoch": 2.0423333333333336,
      "grad_norm": 0.029650967568159103,
      "learning_rate": 3.723541666666667e-05,
      "loss": 0.0022,
      "step": 61270
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.20299367606639862,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0035,
      "step": 61280
    },
    {
      "epoch": 2.043,
      "grad_norm": 0.05807754397392273,
      "learning_rate": 3.723125e-05,
      "loss": 0.002,
      "step": 61290
    },
    {
      "epoch": 2.0433333333333334,
      "grad_norm": 0.2616787552833557,
      "learning_rate": 3.722916666666667e-05,
      "loss": 0.0021,
      "step": 61300
    },
    {
      "epoch": 2.0436666666666667,
      "grad_norm": 0.20306693017482758,
      "learning_rate": 3.722708333333333e-05,
      "loss": 0.0017,
      "step": 61310
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.49330127239227295,
      "learning_rate": 3.7225000000000004e-05,
      "loss": 0.0023,
      "step": 61320
    },
    {
      "epoch": 2.0443333333333333,
      "grad_norm": 0.0581195093691349,
      "learning_rate": 3.722291666666667e-05,
      "loss": 0.0026,
      "step": 61330
    },
    {
      "epoch": 2.0446666666666666,
      "grad_norm": 0.05852154642343521,
      "learning_rate": 3.7220833333333335e-05,
      "loss": 0.0023,
      "step": 61340
    },
    {
      "epoch": 2.045,
      "grad_norm": 0.08709769695997238,
      "learning_rate": 3.721875e-05,
      "loss": 0.0017,
      "step": 61350
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.17421138286590576,
      "learning_rate": 3.7216666666666666e-05,
      "loss": 0.0023,
      "step": 61360
    },
    {
      "epoch": 2.0456666666666665,
      "grad_norm": 0.513024628162384,
      "learning_rate": 3.721458333333334e-05,
      "loss": 0.0025,
      "step": 61370
    },
    {
      "epoch": 2.046,
      "grad_norm": 0.08709230273962021,
      "learning_rate": 3.72125e-05,
      "loss": 0.0022,
      "step": 61380
    },
    {
      "epoch": 2.046333333333333,
      "grad_norm": 0.006782453041523695,
      "learning_rate": 3.721041666666667e-05,
      "loss": 0.0018,
      "step": 61390
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.11621126532554626,
      "learning_rate": 3.7208333333333334e-05,
      "loss": 0.0021,
      "step": 61400
    },
    {
      "epoch": 2.047,
      "grad_norm": 0.34848907589912415,
      "learning_rate": 3.7206250000000007e-05,
      "loss": 0.0028,
      "step": 61410
    },
    {
      "epoch": 2.0473333333333334,
      "grad_norm": 0.23230676352977753,
      "learning_rate": 3.7204166666666665e-05,
      "loss": 0.0021,
      "step": 61420
    },
    {
      "epoch": 2.0476666666666667,
      "grad_norm": 0.20312128961086273,
      "learning_rate": 3.720208333333334e-05,
      "loss": 0.0023,
      "step": 61430
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.14506648480892181,
      "learning_rate": 3.72e-05,
      "loss": 0.0031,
      "step": 61440
    },
    {
      "epoch": 2.0483333333333333,
      "grad_norm": 0.20359134674072266,
      "learning_rate": 3.719791666666667e-05,
      "loss": 0.0026,
      "step": 61450
    },
    {
      "epoch": 2.0486666666666666,
      "grad_norm": 0.05817658454179764,
      "learning_rate": 3.7195833333333334e-05,
      "loss": 0.0024,
      "step": 61460
    },
    {
      "epoch": 2.049,
      "grad_norm": 0.37775275111198425,
      "learning_rate": 3.719375e-05,
      "loss": 0.0025,
      "step": 61470
    },
    {
      "epoch": 2.0493333333333332,
      "grad_norm": 0.20306362211704254,
      "learning_rate": 3.719166666666667e-05,
      "loss": 0.0026,
      "step": 61480
    },
    {
      "epoch": 2.0496666666666665,
      "grad_norm": 0.2905196249485016,
      "learning_rate": 3.718958333333333e-05,
      "loss": 0.0023,
      "step": 61490
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.15603533387184143,
      "learning_rate": 3.71875e-05,
      "loss": 0.0026,
      "step": 61500
    },
    {
      "epoch": 2.050333333333333,
      "grad_norm": 0.005442972760647535,
      "learning_rate": 3.718541666666667e-05,
      "loss": 0.0024,
      "step": 61510
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.40659987926483154,
      "learning_rate": 3.7183333333333334e-05,
      "loss": 0.002,
      "step": 61520
    },
    {
      "epoch": 2.051,
      "grad_norm": 0.10940691083669662,
      "learning_rate": 3.718125e-05,
      "loss": 0.0032,
      "step": 61530
    },
    {
      "epoch": 2.0513333333333335,
      "grad_norm": 0.05819263309240341,
      "learning_rate": 3.717916666666667e-05,
      "loss": 0.0025,
      "step": 61540
    },
    {
      "epoch": 2.0516666666666667,
      "grad_norm": 0.2612992525100708,
      "learning_rate": 3.717708333333334e-05,
      "loss": 0.0033,
      "step": 61550
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.5221624374389648,
      "learning_rate": 3.7175e-05,
      "loss": 0.003,
      "step": 61560
    },
    {
      "epoch": 2.0523333333333333,
      "grad_norm": 0.17411111295223236,
      "learning_rate": 3.717291666666667e-05,
      "loss": 0.0031,
      "step": 61570
    },
    {
      "epoch": 2.0526666666666666,
      "grad_norm": 0.1449868232011795,
      "learning_rate": 3.717083333333333e-05,
      "loss": 0.0031,
      "step": 61580
    },
    {
      "epoch": 2.053,
      "grad_norm": 0.8001347780227661,
      "learning_rate": 3.716875e-05,
      "loss": 0.0022,
      "step": 61590
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.05867252126336098,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.003,
      "step": 61600
    },
    {
      "epoch": 2.0536666666666665,
      "grad_norm": 0.030339960008859634,
      "learning_rate": 3.7164583333333337e-05,
      "loss": 0.0023,
      "step": 61610
    },
    {
      "epoch": 2.054,
      "grad_norm": 0.5679790377616882,
      "learning_rate": 3.71625e-05,
      "loss": 0.0028,
      "step": 61620
    },
    {
      "epoch": 2.054333333333333,
      "grad_norm": 0.0053313011303544044,
      "learning_rate": 3.716041666666667e-05,
      "loss": 0.0024,
      "step": 61630
    },
    {
      "epoch": 2.054666666666667,
      "grad_norm": 0.14547158777713776,
      "learning_rate": 3.715833333333333e-05,
      "loss": 0.0038,
      "step": 61640
    },
    {
      "epoch": 2.055,
      "grad_norm": 0.3777940571308136,
      "learning_rate": 3.7156250000000005e-05,
      "loss": 0.0021,
      "step": 61650
    },
    {
      "epoch": 2.0553333333333335,
      "grad_norm": 0.1453174650669098,
      "learning_rate": 3.715416666666667e-05,
      "loss": 0.0024,
      "step": 61660
    },
    {
      "epoch": 2.0556666666666668,
      "grad_norm": 0.11596439778804779,
      "learning_rate": 3.7152083333333336e-05,
      "loss": 0.0021,
      "step": 61670
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.3485789895057678,
      "learning_rate": 3.715e-05,
      "loss": 0.0024,
      "step": 61680
    },
    {
      "epoch": 2.0563333333333333,
      "grad_norm": 0.08738873898983002,
      "learning_rate": 3.7147916666666674e-05,
      "loss": 0.0028,
      "step": 61690
    },
    {
      "epoch": 2.0566666666666666,
      "grad_norm": 0.20325028896331787,
      "learning_rate": 3.714583333333333e-05,
      "loss": 0.0018,
      "step": 61700
    },
    {
      "epoch": 2.057,
      "grad_norm": 0.6388429999351501,
      "learning_rate": 3.714375e-05,
      "loss": 0.002,
      "step": 61710
    },
    {
      "epoch": 2.0573333333333332,
      "grad_norm": 0.11619633436203003,
      "learning_rate": 3.714166666666667e-05,
      "loss": 0.002,
      "step": 61720
    },
    {
      "epoch": 2.0576666666666665,
      "grad_norm": 0.2901497483253479,
      "learning_rate": 3.7139583333333336e-05,
      "loss": 0.0024,
      "step": 61730
    },
    {
      "epoch": 2.058,
      "grad_norm": 0.2906734347343445,
      "learning_rate": 3.71375e-05,
      "loss": 0.0029,
      "step": 61740
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 0.1448938250541687,
      "learning_rate": 3.713541666666667e-05,
      "loss": 0.0027,
      "step": 61750
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.46440449357032776,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0028,
      "step": 61760
    },
    {
      "epoch": 2.059,
      "grad_norm": 0.4643004238605499,
      "learning_rate": 3.713125e-05,
      "loss": 0.002,
      "step": 61770
    },
    {
      "epoch": 2.0593333333333335,
      "grad_norm": 0.14527782797813416,
      "learning_rate": 3.712916666666667e-05,
      "loss": 0.0021,
      "step": 61780
    },
    {
      "epoch": 2.0596666666666668,
      "grad_norm": 0.2902410328388214,
      "learning_rate": 3.7127083333333336e-05,
      "loss": 0.0028,
      "step": 61790
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.08723385632038116,
      "learning_rate": 3.7125e-05,
      "loss": 0.0023,
      "step": 61800
    },
    {
      "epoch": 2.0603333333333333,
      "grad_norm": 0.005494394805282354,
      "learning_rate": 3.712291666666667e-05,
      "loss": 0.0024,
      "step": 61810
    },
    {
      "epoch": 2.0606666666666666,
      "grad_norm": 0.058181848376989365,
      "learning_rate": 3.712083333333333e-05,
      "loss": 0.0027,
      "step": 61820
    },
    {
      "epoch": 2.061,
      "grad_norm": 0.08699728548526764,
      "learning_rate": 3.7118750000000004e-05,
      "loss": 0.003,
      "step": 61830
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.02979263663291931,
      "learning_rate": 3.711666666666666e-05,
      "loss": 0.0022,
      "step": 61840
    },
    {
      "epoch": 2.0616666666666665,
      "grad_norm": 0.007603568956255913,
      "learning_rate": 3.7114583333333335e-05,
      "loss": 0.0026,
      "step": 61850
    },
    {
      "epoch": 2.062,
      "grad_norm": 0.3478701114654541,
      "learning_rate": 3.71125e-05,
      "loss": 0.0026,
      "step": 61860
    },
    {
      "epoch": 2.062333333333333,
      "grad_norm": 0.6963008642196655,
      "learning_rate": 3.7110416666666666e-05,
      "loss": 0.0019,
      "step": 61870
    },
    {
      "epoch": 2.062666666666667,
      "grad_norm": 0.08743143081665039,
      "learning_rate": 3.710833333333333e-05,
      "loss": 0.0024,
      "step": 61880
    },
    {
      "epoch": 2.063,
      "grad_norm": 0.20333163440227509,
      "learning_rate": 3.7106250000000004e-05,
      "loss": 0.0015,
      "step": 61890
    },
    {
      "epoch": 2.0633333333333335,
      "grad_norm": 0.0585484653711319,
      "learning_rate": 3.710416666666667e-05,
      "loss": 0.0026,
      "step": 61900
    },
    {
      "epoch": 2.0636666666666668,
      "grad_norm": 0.02926456183195114,
      "learning_rate": 3.7102083333333335e-05,
      "loss": 0.0022,
      "step": 61910
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.05895194038748741,
      "learning_rate": 3.71e-05,
      "loss": 0.0021,
      "step": 61920
    },
    {
      "epoch": 2.0643333333333334,
      "grad_norm": 0.08725970983505249,
      "learning_rate": 3.709791666666667e-05,
      "loss": 0.0025,
      "step": 61930
    },
    {
      "epoch": 2.0646666666666667,
      "grad_norm": 0.0593009814620018,
      "learning_rate": 3.709583333333334e-05,
      "loss": 0.0026,
      "step": 61940
    },
    {
      "epoch": 2.065,
      "grad_norm": 0.17399878799915314,
      "learning_rate": 3.709375e-05,
      "loss": 0.0022,
      "step": 61950
    },
    {
      "epoch": 2.0653333333333332,
      "grad_norm": 0.26121625304222107,
      "learning_rate": 3.709166666666667e-05,
      "loss": 0.0026,
      "step": 61960
    },
    {
      "epoch": 2.0656666666666665,
      "grad_norm": 0.22332347929477692,
      "learning_rate": 3.7089583333333335e-05,
      "loss": 0.0032,
      "step": 61970
    },
    {
      "epoch": 2.066,
      "grad_norm": 0.11626914143562317,
      "learning_rate": 3.70875e-05,
      "loss": 0.0022,
      "step": 61980
    },
    {
      "epoch": 2.066333333333333,
      "grad_norm": 0.144950270652771,
      "learning_rate": 3.7085416666666666e-05,
      "loss": 0.0027,
      "step": 61990
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.2033764272928238,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.0026,
      "step": 62000
    },
    {
      "epoch": 2.067,
      "grad_norm": 0.08770272880792618,
      "learning_rate": 3.7081250000000003e-05,
      "loss": 0.0019,
      "step": 62010
    },
    {
      "epoch": 2.0673333333333335,
      "grad_norm": 0.058160681277513504,
      "learning_rate": 3.707916666666667e-05,
      "loss": 0.002,
      "step": 62020
    },
    {
      "epoch": 2.0676666666666668,
      "grad_norm": 0.3103352189064026,
      "learning_rate": 3.7077083333333334e-05,
      "loss": 0.0019,
      "step": 62030
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.1450708657503128,
      "learning_rate": 3.707500000000001e-05,
      "loss": 0.0022,
      "step": 62040
    },
    {
      "epoch": 2.0683333333333334,
      "grad_norm": 0.35637980699539185,
      "learning_rate": 3.7072916666666665e-05,
      "loss": 0.0015,
      "step": 62050
    },
    {
      "epoch": 2.0686666666666667,
      "grad_norm": 0.29014644026756287,
      "learning_rate": 3.707083333333333e-05,
      "loss": 0.0029,
      "step": 62060
    },
    {
      "epoch": 2.069,
      "grad_norm": 0.05805113539099693,
      "learning_rate": 3.706875e-05,
      "loss": 0.0028,
      "step": 62070
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.11588411778211594,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0027,
      "step": 62080
    },
    {
      "epoch": 2.0696666666666665,
      "grad_norm": 0.14516200125217438,
      "learning_rate": 3.7064583333333334e-05,
      "loss": 0.0016,
      "step": 62090
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.1740397810935974,
      "learning_rate": 3.70625e-05,
      "loss": 0.0019,
      "step": 62100
    },
    {
      "epoch": 2.070333333333333,
      "grad_norm": 0.1759117990732193,
      "learning_rate": 3.706041666666667e-05,
      "loss": 0.0026,
      "step": 62110
    },
    {
      "epoch": 2.070666666666667,
      "grad_norm": 0.49315306544303894,
      "learning_rate": 3.705833333333333e-05,
      "loss": 0.0024,
      "step": 62120
    },
    {
      "epoch": 2.071,
      "grad_norm": 0.2611180543899536,
      "learning_rate": 3.705625e-05,
      "loss": 0.0025,
      "step": 62130
    },
    {
      "epoch": 2.0713333333333335,
      "grad_norm": 0.1163158193230629,
      "learning_rate": 3.705416666666667e-05,
      "loss": 0.0022,
      "step": 62140
    },
    {
      "epoch": 2.0716666666666668,
      "grad_norm": 0.2613508105278015,
      "learning_rate": 3.705208333333334e-05,
      "loss": 0.0023,
      "step": 62150
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.02915676310658455,
      "learning_rate": 3.705e-05,
      "loss": 0.0034,
      "step": 62160
    },
    {
      "epoch": 2.0723333333333334,
      "grad_norm": 0.008100357837975025,
      "learning_rate": 3.704791666666667e-05,
      "loss": 0.0026,
      "step": 62170
    },
    {
      "epoch": 2.0726666666666667,
      "grad_norm": 0.058827318251132965,
      "learning_rate": 3.704583333333334e-05,
      "loss": 0.002,
      "step": 62180
    },
    {
      "epoch": 2.073,
      "grad_norm": 0.029235757887363434,
      "learning_rate": 3.704375e-05,
      "loss": 0.0027,
      "step": 62190
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.058201864361763,
      "learning_rate": 3.704166666666667e-05,
      "loss": 0.0029,
      "step": 62200
    },
    {
      "epoch": 2.0736666666666665,
      "grad_norm": 0.11634732782840729,
      "learning_rate": 3.7039583333333334e-05,
      "loss": 0.0026,
      "step": 62210
    },
    {
      "epoch": 2.074,
      "grad_norm": 0.20310789346694946,
      "learning_rate": 3.7037500000000006e-05,
      "loss": 0.0019,
      "step": 62220
    },
    {
      "epoch": 2.074333333333333,
      "grad_norm": 0.26152658462524414,
      "learning_rate": 3.7035416666666665e-05,
      "loss": 0.0025,
      "step": 62230
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.17422126233577728,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0021,
      "step": 62240
    },
    {
      "epoch": 2.075,
      "grad_norm": 0.029586244374513626,
      "learning_rate": 3.703125e-05,
      "loss": 0.0025,
      "step": 62250
    },
    {
      "epoch": 2.0753333333333335,
      "grad_norm": 0.029373614117503166,
      "learning_rate": 3.702916666666667e-05,
      "loss": 0.0025,
      "step": 62260
    },
    {
      "epoch": 2.0756666666666668,
      "grad_norm": 0.20316126942634583,
      "learning_rate": 3.702708333333333e-05,
      "loss": 0.002,
      "step": 62270
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.14528793096542358,
      "learning_rate": 3.7025000000000005e-05,
      "loss": 0.0022,
      "step": 62280
    },
    {
      "epoch": 2.0763333333333334,
      "grad_norm": 0.05808278173208237,
      "learning_rate": 3.702291666666667e-05,
      "loss": 0.0023,
      "step": 62290
    },
    {
      "epoch": 2.0766666666666667,
      "grad_norm": 0.05809946730732918,
      "learning_rate": 3.702083333333333e-05,
      "loss": 0.0026,
      "step": 62300
    },
    {
      "epoch": 2.077,
      "grad_norm": 0.029220743104815483,
      "learning_rate": 3.701875e-05,
      "loss": 0.002,
      "step": 62310
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.14523428678512573,
      "learning_rate": 3.701666666666667e-05,
      "loss": 0.0021,
      "step": 62320
    },
    {
      "epoch": 2.0776666666666666,
      "grad_norm": 0.030288103967905045,
      "learning_rate": 3.701458333333333e-05,
      "loss": 0.0023,
      "step": 62330
    },
    {
      "epoch": 2.078,
      "grad_norm": 0.029247621074318886,
      "learning_rate": 3.70125e-05,
      "loss": 0.003,
      "step": 62340
    },
    {
      "epoch": 2.078333333333333,
      "grad_norm": 0.5464569926261902,
      "learning_rate": 3.701041666666667e-05,
      "loss": 0.0018,
      "step": 62350
    },
    {
      "epoch": 2.078666666666667,
      "grad_norm": 0.14529088139533997,
      "learning_rate": 3.7008333333333336e-05,
      "loss": 0.0023,
      "step": 62360
    },
    {
      "epoch": 2.079,
      "grad_norm": 0.49347051978111267,
      "learning_rate": 3.700625e-05,
      "loss": 0.002,
      "step": 62370
    },
    {
      "epoch": 2.0793333333333335,
      "grad_norm": 0.029288852587342262,
      "learning_rate": 3.700416666666667e-05,
      "loss": 0.0019,
      "step": 62380
    },
    {
      "epoch": 2.0796666666666668,
      "grad_norm": 0.26125577092170715,
      "learning_rate": 3.700208333333334e-05,
      "loss": 0.0022,
      "step": 62390
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.03501930087804794,
      "learning_rate": 3.7e-05,
      "loss": 0.0029,
      "step": 62400
    },
    {
      "epoch": 2.0803333333333334,
      "grad_norm": 0.4931199848651886,
      "learning_rate": 3.699791666666667e-05,
      "loss": 0.0022,
      "step": 62410
    },
    {
      "epoch": 2.0806666666666667,
      "grad_norm": 0.20313693583011627,
      "learning_rate": 3.6995833333333336e-05,
      "loss": 0.0022,
      "step": 62420
    },
    {
      "epoch": 2.081,
      "grad_norm": 0.3484646677970886,
      "learning_rate": 3.699375e-05,
      "loss": 0.0034,
      "step": 62430
    },
    {
      "epoch": 2.0813333333333333,
      "grad_norm": 0.1741149127483368,
      "learning_rate": 3.699166666666667e-05,
      "loss": 0.0019,
      "step": 62440
    },
    {
      "epoch": 2.0816666666666666,
      "grad_norm": 0.866289496421814,
      "learning_rate": 3.698958333333333e-05,
      "loss": 0.0025,
      "step": 62450
    },
    {
      "epoch": 2.082,
      "grad_norm": 0.2030916064977646,
      "learning_rate": 3.6987500000000005e-05,
      "loss": 0.0022,
      "step": 62460
    },
    {
      "epoch": 2.082333333333333,
      "grad_norm": 0.20303471386432648,
      "learning_rate": 3.698541666666667e-05,
      "loss": 0.0021,
      "step": 62470
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.2031441479921341,
      "learning_rate": 3.6983333333333336e-05,
      "loss": 0.0019,
      "step": 62480
    },
    {
      "epoch": 2.083,
      "grad_norm": 0.1744317263364792,
      "learning_rate": 3.698125e-05,
      "loss": 0.0022,
      "step": 62490
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.1741090714931488,
      "learning_rate": 3.697916666666667e-05,
      "loss": 0.002,
      "step": 62500
    },
    {
      "epoch": 2.083666666666667,
      "grad_norm": 0.30623653531074524,
      "learning_rate": 3.697708333333333e-05,
      "loss": 0.0027,
      "step": 62510
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.2321965992450714,
      "learning_rate": 3.6975000000000004e-05,
      "loss": 0.0022,
      "step": 62520
    },
    {
      "epoch": 2.0843333333333334,
      "grad_norm": 0.43531933426856995,
      "learning_rate": 3.697291666666667e-05,
      "loss": 0.0016,
      "step": 62530
    },
    {
      "epoch": 2.0846666666666667,
      "grad_norm": 0.03012141026556492,
      "learning_rate": 3.6970833333333335e-05,
      "loss": 0.0026,
      "step": 62540
    },
    {
      "epoch": 2.085,
      "grad_norm": 0.6391487717628479,
      "learning_rate": 3.696875e-05,
      "loss": 0.0021,
      "step": 62550
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.030493343248963356,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0023,
      "step": 62560
    },
    {
      "epoch": 2.0856666666666666,
      "grad_norm": 0.14499877393245697,
      "learning_rate": 3.696458333333334e-05,
      "loss": 0.0018,
      "step": 62570
    },
    {
      "epoch": 2.086,
      "grad_norm": 0.02969188056886196,
      "learning_rate": 3.69625e-05,
      "loss": 0.0025,
      "step": 62580
    },
    {
      "epoch": 2.086333333333333,
      "grad_norm": 0.6149052381515503,
      "learning_rate": 3.696041666666667e-05,
      "loss": 0.0026,
      "step": 62590
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.2611124813556671,
      "learning_rate": 3.6958333333333335e-05,
      "loss": 0.0015,
      "step": 62600
    },
    {
      "epoch": 2.087,
      "grad_norm": 0.029096517711877823,
      "learning_rate": 3.695625e-05,
      "loss": 0.0026,
      "step": 62610
    },
    {
      "epoch": 2.0873333333333335,
      "grad_norm": 0.04995450749993324,
      "learning_rate": 3.6954166666666666e-05,
      "loss": 0.0028,
      "step": 62620
    },
    {
      "epoch": 2.087666666666667,
      "grad_norm": 0.29054251313209534,
      "learning_rate": 3.695208333333334e-05,
      "loss": 0.0023,
      "step": 62630
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.004252390470355749,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 0.002,
      "step": 62640
    },
    {
      "epoch": 2.0883333333333334,
      "grad_norm": 0.08687333017587662,
      "learning_rate": 3.694791666666667e-05,
      "loss": 0.0023,
      "step": 62650
    },
    {
      "epoch": 2.0886666666666667,
      "grad_norm": 0.05809662491083145,
      "learning_rate": 3.6945833333333335e-05,
      "loss": 0.0028,
      "step": 62660
    },
    {
      "epoch": 2.089,
      "grad_norm": 0.11596841365098953,
      "learning_rate": 3.694375e-05,
      "loss": 0.0022,
      "step": 62670
    },
    {
      "epoch": 2.0893333333333333,
      "grad_norm": 0.05810350179672241,
      "learning_rate": 3.6941666666666666e-05,
      "loss": 0.0028,
      "step": 62680
    },
    {
      "epoch": 2.0896666666666666,
      "grad_norm": 0.17396046221256256,
      "learning_rate": 3.693958333333333e-05,
      "loss": 0.0023,
      "step": 62690
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.17445218563079834,
      "learning_rate": 3.69375e-05,
      "loss": 0.0026,
      "step": 62700
    },
    {
      "epoch": 2.090333333333333,
      "grad_norm": 0.05814247950911522,
      "learning_rate": 3.693541666666667e-05,
      "loss": 0.0023,
      "step": 62710
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.26120567321777344,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0022,
      "step": 62720
    },
    {
      "epoch": 2.091,
      "grad_norm": 0.3200138211250305,
      "learning_rate": 3.693125e-05,
      "loss": 0.0024,
      "step": 62730
    },
    {
      "epoch": 2.0913333333333335,
      "grad_norm": 0.006978453136980534,
      "learning_rate": 3.692916666666667e-05,
      "loss": 0.0016,
      "step": 62740
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 0.05835993215441704,
      "learning_rate": 3.692708333333334e-05,
      "loss": 0.002,
      "step": 62750
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.3769039511680603,
      "learning_rate": 3.6925e-05,
      "loss": 0.0028,
      "step": 62760
    },
    {
      "epoch": 2.0923333333333334,
      "grad_norm": 0.346384733915329,
      "learning_rate": 3.692291666666667e-05,
      "loss": 0.0028,
      "step": 62770
    },
    {
      "epoch": 2.0926666666666667,
      "grad_norm": 0.20310454070568085,
      "learning_rate": 3.692083333333334e-05,
      "loss": 0.0016,
      "step": 62780
    },
    {
      "epoch": 2.093,
      "grad_norm": 0.17400018870830536,
      "learning_rate": 3.691875e-05,
      "loss": 0.0027,
      "step": 62790
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.08766505122184753,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.0021,
      "step": 62800
    },
    {
      "epoch": 2.0936666666666666,
      "grad_norm": 0.2612709403038025,
      "learning_rate": 3.691458333333334e-05,
      "loss": 0.0016,
      "step": 62810
    },
    {
      "epoch": 2.094,
      "grad_norm": 0.14515094459056854,
      "learning_rate": 3.69125e-05,
      "loss": 0.0024,
      "step": 62820
    },
    {
      "epoch": 2.094333333333333,
      "grad_norm": 0.2608698308467865,
      "learning_rate": 3.691041666666667e-05,
      "loss": 0.002,
      "step": 62830
    },
    {
      "epoch": 2.0946666666666665,
      "grad_norm": 0.2034512609243393,
      "learning_rate": 3.6908333333333334e-05,
      "loss": 0.0023,
      "step": 62840
    },
    {
      "epoch": 2.095,
      "grad_norm": 0.3771785497665405,
      "learning_rate": 3.6906250000000006e-05,
      "loss": 0.0021,
      "step": 62850
    },
    {
      "epoch": 2.0953333333333335,
      "grad_norm": 0.058279767632484436,
      "learning_rate": 3.6904166666666665e-05,
      "loss": 0.0018,
      "step": 62860
    },
    {
      "epoch": 2.095666666666667,
      "grad_norm": 0.0869230255484581,
      "learning_rate": 3.690208333333334e-05,
      "loss": 0.0029,
      "step": 62870
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.0870693102478981,
      "learning_rate": 3.69e-05,
      "loss": 0.0023,
      "step": 62880
    },
    {
      "epoch": 2.0963333333333334,
      "grad_norm": 0.08755230158567429,
      "learning_rate": 3.689791666666667e-05,
      "loss": 0.0019,
      "step": 62890
    },
    {
      "epoch": 2.0966666666666667,
      "grad_norm": 0.23376689851284027,
      "learning_rate": 3.6895833333333333e-05,
      "loss": 0.0023,
      "step": 62900
    },
    {
      "epoch": 2.097,
      "grad_norm": 0.006488383747637272,
      "learning_rate": 3.689375e-05,
      "loss": 0.0019,
      "step": 62910
    },
    {
      "epoch": 2.0973333333333333,
      "grad_norm": 0.232008159160614,
      "learning_rate": 3.689166666666667e-05,
      "loss": 0.003,
      "step": 62920
    },
    {
      "epoch": 2.0976666666666666,
      "grad_norm": 0.20350457727909088,
      "learning_rate": 3.688958333333333e-05,
      "loss": 0.003,
      "step": 62930
    },
    {
      "epoch": 2.098,
      "grad_norm": 0.0873858705163002,
      "learning_rate": 3.68875e-05,
      "loss": 0.0027,
      "step": 62940
    },
    {
      "epoch": 2.098333333333333,
      "grad_norm": 0.11614769697189331,
      "learning_rate": 3.688541666666667e-05,
      "loss": 0.0019,
      "step": 62950
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.08685629069805145,
      "learning_rate": 3.688333333333333e-05,
      "loss": 0.0022,
      "step": 62960
    },
    {
      "epoch": 2.099,
      "grad_norm": 0.11632740497589111,
      "learning_rate": 3.688125e-05,
      "loss": 0.0024,
      "step": 62970
    },
    {
      "epoch": 2.0993333333333335,
      "grad_norm": 0.004506137687712908,
      "learning_rate": 3.687916666666667e-05,
      "loss": 0.0027,
      "step": 62980
    },
    {
      "epoch": 2.099666666666667,
      "grad_norm": 0.09056701511144638,
      "learning_rate": 3.6877083333333336e-05,
      "loss": 0.0027,
      "step": 62990
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.3482446074485779,
      "learning_rate": 3.6875e-05,
      "loss": 0.0026,
      "step": 63000
    },
    {
      "epoch": 2.1003333333333334,
      "grad_norm": 0.23186418414115906,
      "learning_rate": 3.687291666666667e-05,
      "loss": 0.0021,
      "step": 63010
    },
    {
      "epoch": 2.1006666666666667,
      "grad_norm": 0.26087701320648193,
      "learning_rate": 3.687083333333334e-05,
      "loss": 0.0019,
      "step": 63020
    },
    {
      "epoch": 2.101,
      "grad_norm": 0.11644929647445679,
      "learning_rate": 3.6868750000000005e-05,
      "loss": 0.0023,
      "step": 63030
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.08702688664197922,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0021,
      "step": 63040
    },
    {
      "epoch": 2.1016666666666666,
      "grad_norm": 0.29007476568222046,
      "learning_rate": 3.6864583333333336e-05,
      "loss": 0.0026,
      "step": 63050
    },
    {
      "epoch": 2.102,
      "grad_norm": 0.08671984821557999,
      "learning_rate": 3.68625e-05,
      "loss": 0.002,
      "step": 63060
    },
    {
      "epoch": 2.102333333333333,
      "grad_norm": 0.11667443811893463,
      "learning_rate": 3.686041666666667e-05,
      "loss": 0.0019,
      "step": 63070
    },
    {
      "epoch": 2.1026666666666665,
      "grad_norm": 0.20289121568202972,
      "learning_rate": 3.685833333333333e-05,
      "loss": 0.0012,
      "step": 63080
    },
    {
      "epoch": 2.103,
      "grad_norm": 0.029874805361032486,
      "learning_rate": 3.6856250000000005e-05,
      "loss": 0.0022,
      "step": 63090
    },
    {
      "epoch": 2.1033333333333335,
      "grad_norm": 0.058113183826208115,
      "learning_rate": 3.685416666666667e-05,
      "loss": 0.002,
      "step": 63100
    },
    {
      "epoch": 2.103666666666667,
      "grad_norm": 0.2902163863182068,
      "learning_rate": 3.6852083333333336e-05,
      "loss": 0.0021,
      "step": 63110
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.261067658662796,
      "learning_rate": 3.685e-05,
      "loss": 0.0019,
      "step": 63120
    },
    {
      "epoch": 2.1043333333333334,
      "grad_norm": 0.34818607568740845,
      "learning_rate": 3.6847916666666674e-05,
      "loss": 0.0029,
      "step": 63130
    },
    {
      "epoch": 2.1046666666666667,
      "grad_norm": 0.20326554775238037,
      "learning_rate": 3.684583333333333e-05,
      "loss": 0.0015,
      "step": 63140
    },
    {
      "epoch": 2.105,
      "grad_norm": 0.03481539338827133,
      "learning_rate": 3.684375e-05,
      "loss": 0.0021,
      "step": 63150
    },
    {
      "epoch": 2.1053333333333333,
      "grad_norm": 0.4929618835449219,
      "learning_rate": 3.684166666666667e-05,
      "loss": 0.0031,
      "step": 63160
    },
    {
      "epoch": 2.1056666666666666,
      "grad_norm": 0.14554990828037262,
      "learning_rate": 3.6839583333333335e-05,
      "loss": 0.0022,
      "step": 63170
    },
    {
      "epoch": 2.106,
      "grad_norm": 0.377055287361145,
      "learning_rate": 3.68375e-05,
      "loss": 0.0022,
      "step": 63180
    },
    {
      "epoch": 2.106333333333333,
      "grad_norm": 0.46378853917121887,
      "learning_rate": 3.6835416666666666e-05,
      "loss": 0.0023,
      "step": 63190
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.14512330293655396,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0019,
      "step": 63200
    },
    {
      "epoch": 2.107,
      "grad_norm": 0.11601638793945312,
      "learning_rate": 3.683125e-05,
      "loss": 0.0022,
      "step": 63210
    },
    {
      "epoch": 2.1073333333333335,
      "grad_norm": 0.23191553354263306,
      "learning_rate": 3.682916666666667e-05,
      "loss": 0.0029,
      "step": 63220
    },
    {
      "epoch": 2.107666666666667,
      "grad_norm": 0.5800980925559998,
      "learning_rate": 3.6827083333333335e-05,
      "loss": 0.0019,
      "step": 63230
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.14497961103916168,
      "learning_rate": 3.6825e-05,
      "loss": 0.003,
      "step": 63240
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 0.05956270918250084,
      "learning_rate": 3.6822916666666666e-05,
      "loss": 0.0021,
      "step": 63250
    },
    {
      "epoch": 2.1086666666666667,
      "grad_norm": 0.0038431130815297365,
      "learning_rate": 3.682083333333334e-05,
      "loss": 0.0034,
      "step": 63260
    },
    {
      "epoch": 2.109,
      "grad_norm": 0.17380072176456451,
      "learning_rate": 3.6818750000000004e-05,
      "loss": 0.0022,
      "step": 63270
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.0584491603076458,
      "learning_rate": 3.681666666666667e-05,
      "loss": 0.0015,
      "step": 63280
    },
    {
      "epoch": 2.1096666666666666,
      "grad_norm": 0.003832837101072073,
      "learning_rate": 3.6814583333333335e-05,
      "loss": 0.0021,
      "step": 63290
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.17375323176383972,
      "learning_rate": 3.68125e-05,
      "loss": 0.0027,
      "step": 63300
    },
    {
      "epoch": 2.110333333333333,
      "grad_norm": 0.3037441670894623,
      "learning_rate": 3.681041666666667e-05,
      "loss": 0.0023,
      "step": 63310
    },
    {
      "epoch": 2.1106666666666665,
      "grad_norm": 0.029291300103068352,
      "learning_rate": 3.680833333333333e-05,
      "loss": 0.0021,
      "step": 63320
    },
    {
      "epoch": 2.111,
      "grad_norm": 0.14513887465000153,
      "learning_rate": 3.6806250000000004e-05,
      "loss": 0.0031,
      "step": 63330
    },
    {
      "epoch": 2.1113333333333335,
      "grad_norm": 0.004586396273225546,
      "learning_rate": 3.680416666666667e-05,
      "loss": 0.0021,
      "step": 63340
    },
    {
      "epoch": 2.111666666666667,
      "grad_norm": 0.08746912330389023,
      "learning_rate": 3.6802083333333335e-05,
      "loss": 0.0028,
      "step": 63350
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.0580742172896862,
      "learning_rate": 3.68e-05,
      "loss": 0.0015,
      "step": 63360
    },
    {
      "epoch": 2.1123333333333334,
      "grad_norm": 0.029576817527413368,
      "learning_rate": 3.679791666666667e-05,
      "loss": 0.0015,
      "step": 63370
    },
    {
      "epoch": 2.1126666666666667,
      "grad_norm": 0.14506074786186218,
      "learning_rate": 3.679583333333334e-05,
      "loss": 0.003,
      "step": 63380
    },
    {
      "epoch": 2.113,
      "grad_norm": 0.14540055394172668,
      "learning_rate": 3.6793749999999996e-05,
      "loss": 0.0024,
      "step": 63390
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.174343541264534,
      "learning_rate": 3.679166666666667e-05,
      "loss": 0.0021,
      "step": 63400
    },
    {
      "epoch": 2.1136666666666666,
      "grad_norm": 0.1449321061372757,
      "learning_rate": 3.6789583333333334e-05,
      "loss": 0.0015,
      "step": 63410
    },
    {
      "epoch": 2.114,
      "grad_norm": 0.2706238627433777,
      "learning_rate": 3.67875e-05,
      "loss": 0.0017,
      "step": 63420
    },
    {
      "epoch": 2.114333333333333,
      "grad_norm": 0.029453612864017487,
      "learning_rate": 3.6785416666666665e-05,
      "loss": 0.0027,
      "step": 63430
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.11580419540405273,
      "learning_rate": 3.678333333333334e-05,
      "loss": 0.0018,
      "step": 63440
    },
    {
      "epoch": 2.115,
      "grad_norm": 0.3190310001373291,
      "learning_rate": 3.678125e-05,
      "loss": 0.0028,
      "step": 63450
    },
    {
      "epoch": 2.1153333333333335,
      "grad_norm": 0.1159820631146431,
      "learning_rate": 3.677916666666667e-05,
      "loss": 0.0024,
      "step": 63460
    },
    {
      "epoch": 2.115666666666667,
      "grad_norm": 0.004940533544868231,
      "learning_rate": 3.6777083333333334e-05,
      "loss": 0.0017,
      "step": 63470
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.3190547525882721,
      "learning_rate": 3.6775000000000006e-05,
      "loss": 0.002,
      "step": 63480
    },
    {
      "epoch": 2.1163333333333334,
      "grad_norm": 0.11613497138023376,
      "learning_rate": 3.6772916666666665e-05,
      "loss": 0.0021,
      "step": 63490
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.14485283195972443,
      "learning_rate": 3.677083333333334e-05,
      "loss": 0.0022,
      "step": 63500
    },
    {
      "epoch": 2.117,
      "grad_norm": 0.2317993938922882,
      "learning_rate": 3.676875e-05,
      "loss": 0.0025,
      "step": 63510
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.11642415821552277,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.0024,
      "step": 63520
    },
    {
      "epoch": 2.1176666666666666,
      "grad_norm": 0.10103245824575424,
      "learning_rate": 3.6764583333333334e-05,
      "loss": 0.0024,
      "step": 63530
    },
    {
      "epoch": 2.118,
      "grad_norm": 0.0869307890534401,
      "learning_rate": 3.67625e-05,
      "loss": 0.0018,
      "step": 63540
    },
    {
      "epoch": 2.118333333333333,
      "grad_norm": 0.11604592204093933,
      "learning_rate": 3.676041666666667e-05,
      "loss": 0.0025,
      "step": 63550
    },
    {
      "epoch": 2.1186666666666665,
      "grad_norm": 0.40571925044059753,
      "learning_rate": 3.675833333333334e-05,
      "loss": 0.0016,
      "step": 63560
    },
    {
      "epoch": 2.1189999999999998,
      "grad_norm": 0.14500196278095245,
      "learning_rate": 3.675625e-05,
      "loss": 0.0023,
      "step": 63570
    },
    {
      "epoch": 2.1193333333333335,
      "grad_norm": 0.2028866410255432,
      "learning_rate": 3.675416666666667e-05,
      "loss": 0.0015,
      "step": 63580
    },
    {
      "epoch": 2.119666666666667,
      "grad_norm": 0.21687722206115723,
      "learning_rate": 3.675208333333334e-05,
      "loss": 0.0033,
      "step": 63590
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.08706162124872208,
      "learning_rate": 3.675e-05,
      "loss": 0.0025,
      "step": 63600
    },
    {
      "epoch": 2.1203333333333334,
      "grad_norm": 0.029687335714697838,
      "learning_rate": 3.674791666666667e-05,
      "loss": 0.0018,
      "step": 63610
    },
    {
      "epoch": 2.1206666666666667,
      "grad_norm": 0.20296339690685272,
      "learning_rate": 3.6745833333333337e-05,
      "loss": 0.0023,
      "step": 63620
    },
    {
      "epoch": 2.121,
      "grad_norm": 0.2322564423084259,
      "learning_rate": 3.674375e-05,
      "loss": 0.0026,
      "step": 63630
    },
    {
      "epoch": 2.1213333333333333,
      "grad_norm": 0.029407348483800888,
      "learning_rate": 3.674166666666667e-05,
      "loss": 0.0021,
      "step": 63640
    },
    {
      "epoch": 2.1216666666666666,
      "grad_norm": 0.08683927357196808,
      "learning_rate": 3.673958333333333e-05,
      "loss": 0.0022,
      "step": 63650
    },
    {
      "epoch": 2.122,
      "grad_norm": 0.0075414711609482765,
      "learning_rate": 3.6737500000000005e-05,
      "loss": 0.0024,
      "step": 63660
    },
    {
      "epoch": 2.122333333333333,
      "grad_norm": 0.26110100746154785,
      "learning_rate": 3.6735416666666664e-05,
      "loss": 0.002,
      "step": 63670
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.08736658841371536,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0019,
      "step": 63680
    },
    {
      "epoch": 2.123,
      "grad_norm": 0.239315003156662,
      "learning_rate": 3.673125e-05,
      "loss": 0.0029,
      "step": 63690
    },
    {
      "epoch": 2.1233333333333335,
      "grad_norm": 0.08755180984735489,
      "learning_rate": 3.672916666666667e-05,
      "loss": 0.0026,
      "step": 63700
    },
    {
      "epoch": 2.123666666666667,
      "grad_norm": 0.0872093066573143,
      "learning_rate": 3.672708333333333e-05,
      "loss": 0.0014,
      "step": 63710
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.3480917513370514,
      "learning_rate": 3.6725000000000005e-05,
      "loss": 0.0026,
      "step": 63720
    },
    {
      "epoch": 2.1243333333333334,
      "grad_norm": 0.40583738684654236,
      "learning_rate": 3.672291666666667e-05,
      "loss": 0.0022,
      "step": 63730
    },
    {
      "epoch": 2.1246666666666667,
      "grad_norm": 0.521934986114502,
      "learning_rate": 3.6720833333333336e-05,
      "loss": 0.0019,
      "step": 63740
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.4349737763404846,
      "learning_rate": 3.671875e-05,
      "loss": 0.002,
      "step": 63750
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.14583559334278107,
      "learning_rate": 3.671666666666667e-05,
      "loss": 0.0028,
      "step": 63760
    },
    {
      "epoch": 2.1256666666666666,
      "grad_norm": 0.1740010380744934,
      "learning_rate": 3.671458333333333e-05,
      "loss": 0.0017,
      "step": 63770
    },
    {
      "epoch": 2.126,
      "grad_norm": 0.3188442885875702,
      "learning_rate": 3.67125e-05,
      "loss": 0.0026,
      "step": 63780
    },
    {
      "epoch": 2.126333333333333,
      "grad_norm": 0.1453545093536377,
      "learning_rate": 3.671041666666667e-05,
      "loss": 0.0027,
      "step": 63790
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.1456250697374344,
      "learning_rate": 3.6708333333333336e-05,
      "loss": 0.0017,
      "step": 63800
    },
    {
      "epoch": 2.127,
      "grad_norm": 0.14529523253440857,
      "learning_rate": 3.670625e-05,
      "loss": 0.0019,
      "step": 63810
    },
    {
      "epoch": 2.1273333333333335,
      "grad_norm": 0.46384015679359436,
      "learning_rate": 3.670416666666667e-05,
      "loss": 0.0021,
      "step": 63820
    },
    {
      "epoch": 2.127666666666667,
      "grad_norm": 0.23187242448329926,
      "learning_rate": 3.670208333333334e-05,
      "loss": 0.0022,
      "step": 63830
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.11637824028730392,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0028,
      "step": 63840
    },
    {
      "epoch": 2.1283333333333334,
      "grad_norm": 0.11630785465240479,
      "learning_rate": 3.669791666666667e-05,
      "loss": 0.0021,
      "step": 63850
    },
    {
      "epoch": 2.1286666666666667,
      "grad_norm": 0.05871707573533058,
      "learning_rate": 3.6695833333333335e-05,
      "loss": 0.0023,
      "step": 63860
    },
    {
      "epoch": 2.129,
      "grad_norm": 0.2836257815361023,
      "learning_rate": 3.669375000000001e-05,
      "loss": 0.004,
      "step": 63870
    },
    {
      "epoch": 2.1293333333333333,
      "grad_norm": 0.2902758717536926,
      "learning_rate": 3.6691666666666666e-05,
      "loss": 0.0027,
      "step": 63880
    },
    {
      "epoch": 2.1296666666666666,
      "grad_norm": 0.11620885878801346,
      "learning_rate": 3.668958333333333e-05,
      "loss": 0.0017,
      "step": 63890
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.23200787603855133,
      "learning_rate": 3.6687500000000004e-05,
      "loss": 0.0021,
      "step": 63900
    },
    {
      "epoch": 2.130333333333333,
      "grad_norm": 0.6655371189117432,
      "learning_rate": 3.668541666666667e-05,
      "loss": 0.0013,
      "step": 63910
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.46378469467163086,
      "learning_rate": 3.6683333333333335e-05,
      "loss": 0.0017,
      "step": 63920
    },
    {
      "epoch": 2.1310000000000002,
      "grad_norm": 0.2031489461660385,
      "learning_rate": 3.668125e-05,
      "loss": 0.0027,
      "step": 63930
    },
    {
      "epoch": 2.1313333333333335,
      "grad_norm": 0.05802178010344505,
      "learning_rate": 3.667916666666667e-05,
      "loss": 0.0018,
      "step": 63940
    },
    {
      "epoch": 2.131666666666667,
      "grad_norm": 0.11602316051721573,
      "learning_rate": 3.667708333333333e-05,
      "loss": 0.0022,
      "step": 63950
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.2898588180541992,
      "learning_rate": 3.6675000000000004e-05,
      "loss": 0.0026,
      "step": 63960
    },
    {
      "epoch": 2.1323333333333334,
      "grad_norm": 0.23189318180084229,
      "learning_rate": 3.667291666666667e-05,
      "loss": 0.0026,
      "step": 63970
    },
    {
      "epoch": 2.1326666666666667,
      "grad_norm": 0.05805475637316704,
      "learning_rate": 3.6670833333333335e-05,
      "loss": 0.0026,
      "step": 63980
    },
    {
      "epoch": 2.133,
      "grad_norm": 0.1450147181749344,
      "learning_rate": 3.666875e-05,
      "loss": 0.0025,
      "step": 63990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.17407727241516113,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0026,
      "step": 64000
    },
    {
      "epoch": 2.1336666666666666,
      "grad_norm": 0.34835144877433777,
      "learning_rate": 3.666458333333334e-05,
      "loss": 0.005,
      "step": 64010
    },
    {
      "epoch": 2.134,
      "grad_norm": 0.2610529363155365,
      "learning_rate": 3.66625e-05,
      "loss": 0.002,
      "step": 64020
    },
    {
      "epoch": 2.134333333333333,
      "grad_norm": 0.11579608172178268,
      "learning_rate": 3.666041666666667e-05,
      "loss": 0.0027,
      "step": 64030
    },
    {
      "epoch": 2.1346666666666665,
      "grad_norm": 0.2171105295419693,
      "learning_rate": 3.6658333333333334e-05,
      "loss": 0.0027,
      "step": 64040
    },
    {
      "epoch": 2.135,
      "grad_norm": 0.08708681166172028,
      "learning_rate": 3.665625e-05,
      "loss": 0.0026,
      "step": 64050
    },
    {
      "epoch": 2.1353333333333335,
      "grad_norm": 0.23189033567905426,
      "learning_rate": 3.6654166666666665e-05,
      "loss": 0.0033,
      "step": 64060
    },
    {
      "epoch": 2.135666666666667,
      "grad_norm": 0.05820706486701965,
      "learning_rate": 3.665208333333334e-05,
      "loss": 0.0033,
      "step": 64070
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.5217016935348511,
      "learning_rate": 3.665e-05,
      "loss": 0.0028,
      "step": 64080
    },
    {
      "epoch": 2.1363333333333334,
      "grad_norm": 0.5509498715400696,
      "learning_rate": 3.664791666666667e-05,
      "loss": 0.0028,
      "step": 64090
    },
    {
      "epoch": 2.1366666666666667,
      "grad_norm": 0.2317235767841339,
      "learning_rate": 3.6645833333333334e-05,
      "loss": 0.0028,
      "step": 64100
    },
    {
      "epoch": 2.137,
      "grad_norm": 0.05824217572808266,
      "learning_rate": 3.6643750000000006e-05,
      "loss": 0.0034,
      "step": 64110
    },
    {
      "epoch": 2.1373333333333333,
      "grad_norm": 0.260960191488266,
      "learning_rate": 3.664166666666667e-05,
      "loss": 0.0018,
      "step": 64120
    },
    {
      "epoch": 2.1376666666666666,
      "grad_norm": 0.2319285124540329,
      "learning_rate": 3.663958333333333e-05,
      "loss": 0.0029,
      "step": 64130
    },
    {
      "epoch": 2.138,
      "grad_norm": 0.5508392453193665,
      "learning_rate": 3.66375e-05,
      "loss": 0.0022,
      "step": 64140
    },
    {
      "epoch": 2.138333333333333,
      "grad_norm": 0.05878005549311638,
      "learning_rate": 3.663541666666667e-05,
      "loss": 0.0023,
      "step": 64150
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.05791917443275452,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.0027,
      "step": 64160
    },
    {
      "epoch": 2.1390000000000002,
      "grad_norm": 0.02942517213523388,
      "learning_rate": 3.663125e-05,
      "loss": 0.0024,
      "step": 64170
    },
    {
      "epoch": 2.1393333333333335,
      "grad_norm": 0.20306865870952606,
      "learning_rate": 3.662916666666667e-05,
      "loss": 0.0025,
      "step": 64180
    },
    {
      "epoch": 2.139666666666667,
      "grad_norm": 0.02929910272359848,
      "learning_rate": 3.662708333333334e-05,
      "loss": 0.0024,
      "step": 64190
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.0583006925880909,
      "learning_rate": 3.6625e-05,
      "loss": 0.0027,
      "step": 64200
    },
    {
      "epoch": 2.1403333333333334,
      "grad_norm": 0.11674626916646957,
      "learning_rate": 3.662291666666667e-05,
      "loss": 0.0022,
      "step": 64210
    },
    {
      "epoch": 2.1406666666666667,
      "grad_norm": 0.029661117121577263,
      "learning_rate": 3.662083333333334e-05,
      "loss": 0.0025,
      "step": 64220
    },
    {
      "epoch": 2.141,
      "grad_norm": 0.42624595761299133,
      "learning_rate": 3.661875e-05,
      "loss": 0.0051,
      "step": 64230
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.05817333608865738,
      "learning_rate": 3.6616666666666664e-05,
      "loss": 0.0025,
      "step": 64240
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 0.05827353149652481,
      "learning_rate": 3.661458333333334e-05,
      "loss": 0.0019,
      "step": 64250
    },
    {
      "epoch": 2.142,
      "grad_norm": 0.5519089102745056,
      "learning_rate": 3.66125e-05,
      "loss": 0.0029,
      "step": 64260
    },
    {
      "epoch": 2.142333333333333,
      "grad_norm": 0.3769592046737671,
      "learning_rate": 3.661041666666667e-05,
      "loss": 0.0031,
      "step": 64270
    },
    {
      "epoch": 2.1426666666666665,
      "grad_norm": 0.2898005247116089,
      "learning_rate": 3.660833333333333e-05,
      "loss": 0.0026,
      "step": 64280
    },
    {
      "epoch": 2.143,
      "grad_norm": 0.08731624484062195,
      "learning_rate": 3.6606250000000005e-05,
      "loss": 0.003,
      "step": 64290
    },
    {
      "epoch": 2.1433333333333335,
      "grad_norm": 0.17374826967716217,
      "learning_rate": 3.6604166666666664e-05,
      "loss": 0.0019,
      "step": 64300
    },
    {
      "epoch": 2.143666666666667,
      "grad_norm": 0.029486127197742462,
      "learning_rate": 3.6602083333333336e-05,
      "loss": 0.003,
      "step": 64310
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.5193611979484558,
      "learning_rate": 3.66e-05,
      "loss": 0.004,
      "step": 64320
    },
    {
      "epoch": 2.1443333333333334,
      "grad_norm": 0.3187487721443176,
      "learning_rate": 3.659791666666667e-05,
      "loss": 0.002,
      "step": 64330
    },
    {
      "epoch": 2.1446666666666667,
      "grad_norm": 0.32531851530075073,
      "learning_rate": 3.659583333333333e-05,
      "loss": 0.0028,
      "step": 64340
    },
    {
      "epoch": 2.145,
      "grad_norm": 0.20330099761486053,
      "learning_rate": 3.6593750000000005e-05,
      "loss": 0.0033,
      "step": 64350
    },
    {
      "epoch": 2.1453333333333333,
      "grad_norm": 0.058182235807180405,
      "learning_rate": 3.659166666666667e-05,
      "loss": 0.0025,
      "step": 64360
    },
    {
      "epoch": 2.1456666666666666,
      "grad_norm": 0.2899477779865265,
      "learning_rate": 3.658958333333333e-05,
      "loss": 0.0026,
      "step": 64370
    },
    {
      "epoch": 2.146,
      "grad_norm": 0.08708011358976364,
      "learning_rate": 3.65875e-05,
      "loss": 0.0032,
      "step": 64380
    },
    {
      "epoch": 2.146333333333333,
      "grad_norm": 0.029649823904037476,
      "learning_rate": 3.658541666666667e-05,
      "loss": 0.0029,
      "step": 64390
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.4349675178527832,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.0022,
      "step": 64400
    },
    {
      "epoch": 2.147,
      "grad_norm": 0.1470920443534851,
      "learning_rate": 3.658125e-05,
      "loss": 0.0032,
      "step": 64410
    },
    {
      "epoch": 2.1473333333333335,
      "grad_norm": 0.08736897259950638,
      "learning_rate": 3.657916666666667e-05,
      "loss": 0.0017,
      "step": 64420
    },
    {
      "epoch": 2.147666666666667,
      "grad_norm": 0.16745644807815552,
      "learning_rate": 3.6577083333333336e-05,
      "loss": 0.0024,
      "step": 64430
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.0870644822716713,
      "learning_rate": 3.6575e-05,
      "loss": 0.0016,
      "step": 64440
    },
    {
      "epoch": 2.1483333333333334,
      "grad_norm": 0.20310571789741516,
      "learning_rate": 3.657291666666667e-05,
      "loss": 0.002,
      "step": 64450
    },
    {
      "epoch": 2.1486666666666667,
      "grad_norm": 0.2771094739437103,
      "learning_rate": 3.657083333333334e-05,
      "loss": 0.0032,
      "step": 64460
    },
    {
      "epoch": 2.149,
      "grad_norm": 0.23209600150585175,
      "learning_rate": 3.6568750000000005e-05,
      "loss": 0.0025,
      "step": 64470
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.13092920184135437,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.0036,
      "step": 64480
    },
    {
      "epoch": 2.1496666666666666,
      "grad_norm": 0.17395372688770294,
      "learning_rate": 3.6564583333333336e-05,
      "loss": 0.0029,
      "step": 64490
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.1449754685163498,
      "learning_rate": 3.65625e-05,
      "loss": 0.0022,
      "step": 64500
    },
    {
      "epoch": 2.150333333333333,
      "grad_norm": 0.3482287526130676,
      "learning_rate": 3.6560416666666667e-05,
      "loss": 0.0017,
      "step": 64510
    },
    {
      "epoch": 2.1506666666666665,
      "grad_norm": 0.08721268177032471,
      "learning_rate": 3.655833333333333e-05,
      "loss": 0.0028,
      "step": 64520
    },
    {
      "epoch": 2.151,
      "grad_norm": 0.31879493594169617,
      "learning_rate": 3.6556250000000004e-05,
      "loss": 0.0026,
      "step": 64530
    },
    {
      "epoch": 2.1513333333333335,
      "grad_norm": 0.05818384140729904,
      "learning_rate": 3.655416666666667e-05,
      "loss": 0.0031,
      "step": 64540
    },
    {
      "epoch": 2.151666666666667,
      "grad_norm": 0.6961091160774231,
      "learning_rate": 3.6552083333333335e-05,
      "loss": 0.0026,
      "step": 64550
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.14524027705192566,
      "learning_rate": 3.655e-05,
      "loss": 0.0027,
      "step": 64560
    },
    {
      "epoch": 2.1523333333333334,
      "grad_norm": 0.3193226158618927,
      "learning_rate": 3.654791666666667e-05,
      "loss": 0.002,
      "step": 64570
    },
    {
      "epoch": 2.1526666666666667,
      "grad_norm": 0.2033686488866806,
      "learning_rate": 3.654583333333333e-05,
      "loss": 0.002,
      "step": 64580
    },
    {
      "epoch": 2.153,
      "grad_norm": 0.030960164964199066,
      "learning_rate": 3.6543750000000004e-05,
      "loss": 0.0017,
      "step": 64590
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.11587103456258774,
      "learning_rate": 3.654166666666667e-05,
      "loss": 0.003,
      "step": 64600
    },
    {
      "epoch": 2.1536666666666666,
      "grad_norm": 0.23209893703460693,
      "learning_rate": 3.6539583333333335e-05,
      "loss": 0.002,
      "step": 64610
    },
    {
      "epoch": 2.154,
      "grad_norm": 0.1451054811477661,
      "learning_rate": 3.65375e-05,
      "loss": 0.0024,
      "step": 64620
    },
    {
      "epoch": 2.154333333333333,
      "grad_norm": 0.08697059750556946,
      "learning_rate": 3.6535416666666666e-05,
      "loss": 0.0019,
      "step": 64630
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.0057578496634960175,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0023,
      "step": 64640
    },
    {
      "epoch": 2.155,
      "grad_norm": 0.2900007665157318,
      "learning_rate": 3.653125e-05,
      "loss": 0.0032,
      "step": 64650
    },
    {
      "epoch": 2.155333333333333,
      "grad_norm": 0.3480963110923767,
      "learning_rate": 3.652916666666667e-05,
      "loss": 0.0028,
      "step": 64660
    },
    {
      "epoch": 2.155666666666667,
      "grad_norm": 0.08721019327640533,
      "learning_rate": 3.6527083333333335e-05,
      "loss": 0.0016,
      "step": 64670
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.0292454082518816,
      "learning_rate": 3.652500000000001e-05,
      "loss": 0.0028,
      "step": 64680
    },
    {
      "epoch": 2.1563333333333334,
      "grad_norm": 0.5507725477218628,
      "learning_rate": 3.6522916666666666e-05,
      "loss": 0.0019,
      "step": 64690
    },
    {
      "epoch": 2.1566666666666667,
      "grad_norm": 0.08711490780115128,
      "learning_rate": 3.652083333333334e-05,
      "loss": 0.0027,
      "step": 64700
    },
    {
      "epoch": 2.157,
      "grad_norm": 0.05837703496217728,
      "learning_rate": 3.651875e-05,
      "loss": 0.0025,
      "step": 64710
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.02927582897245884,
      "learning_rate": 3.651666666666667e-05,
      "loss": 0.0019,
      "step": 64720
    },
    {
      "epoch": 2.1576666666666666,
      "grad_norm": 0.058305688202381134,
      "learning_rate": 3.6514583333333334e-05,
      "loss": 0.0027,
      "step": 64730
    },
    {
      "epoch": 2.158,
      "grad_norm": 0.3479319214820862,
      "learning_rate": 3.65125e-05,
      "loss": 0.0028,
      "step": 64740
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 0.43499556183815,
      "learning_rate": 3.651041666666667e-05,
      "loss": 0.0025,
      "step": 64750
    },
    {
      "epoch": 2.1586666666666665,
      "grad_norm": 0.029914477840065956,
      "learning_rate": 3.650833333333333e-05,
      "loss": 0.0027,
      "step": 64760
    },
    {
      "epoch": 2.159,
      "grad_norm": 0.08782833814620972,
      "learning_rate": 3.650625e-05,
      "loss": 0.0025,
      "step": 64770
    },
    {
      "epoch": 2.1593333333333335,
      "grad_norm": 0.14509455859661102,
      "learning_rate": 3.650416666666667e-05,
      "loss": 0.0025,
      "step": 64780
    },
    {
      "epoch": 2.159666666666667,
      "grad_norm": 0.029573336243629456,
      "learning_rate": 3.6502083333333334e-05,
      "loss": 0.0018,
      "step": 64790
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.207238107919693,
      "learning_rate": 3.65e-05,
      "loss": 0.0033,
      "step": 64800
    },
    {
      "epoch": 2.1603333333333334,
      "grad_norm": 0.05836222693324089,
      "learning_rate": 3.649791666666667e-05,
      "loss": 0.0033,
      "step": 64810
    },
    {
      "epoch": 2.1606666666666667,
      "grad_norm": 0.058030277490615845,
      "learning_rate": 3.649583333333334e-05,
      "loss": 0.0029,
      "step": 64820
    },
    {
      "epoch": 2.161,
      "grad_norm": 0.4059849977493286,
      "learning_rate": 3.649375e-05,
      "loss": 0.0029,
      "step": 64830
    },
    {
      "epoch": 2.1613333333333333,
      "grad_norm": 0.1399017721414566,
      "learning_rate": 3.649166666666667e-05,
      "loss": 0.003,
      "step": 64840
    },
    {
      "epoch": 2.1616666666666666,
      "grad_norm": 0.31904470920562744,
      "learning_rate": 3.6489583333333334e-05,
      "loss": 0.0033,
      "step": 64850
    },
    {
      "epoch": 2.162,
      "grad_norm": 0.0883515477180481,
      "learning_rate": 3.64875e-05,
      "loss": 0.0019,
      "step": 64860
    },
    {
      "epoch": 2.162333333333333,
      "grad_norm": 0.029389338567852974,
      "learning_rate": 3.6485416666666665e-05,
      "loss": 0.0018,
      "step": 64870
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.01034638099372387,
      "learning_rate": 3.648333333333334e-05,
      "loss": 0.0021,
      "step": 64880
    },
    {
      "epoch": 2.163,
      "grad_norm": 0.08701375871896744,
      "learning_rate": 3.648125e-05,
      "loss": 0.0019,
      "step": 64890
    },
    {
      "epoch": 2.163333333333333,
      "grad_norm": 0.058197472244501114,
      "learning_rate": 3.647916666666667e-05,
      "loss": 0.0025,
      "step": 64900
    },
    {
      "epoch": 2.163666666666667,
      "grad_norm": 0.3768860697746277,
      "learning_rate": 3.6477083333333333e-05,
      "loss": 0.002,
      "step": 64910
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.5512990951538086,
      "learning_rate": 3.6475000000000006e-05,
      "loss": 0.0021,
      "step": 64920
    },
    {
      "epoch": 2.1643333333333334,
      "grad_norm": 0.14505206048488617,
      "learning_rate": 3.6472916666666664e-05,
      "loss": 0.0032,
      "step": 64930
    },
    {
      "epoch": 2.1646666666666667,
      "grad_norm": 0.3634487986564636,
      "learning_rate": 3.647083333333334e-05,
      "loss": 0.0038,
      "step": 64940
    },
    {
      "epoch": 2.165,
      "grad_norm": 0.34841540455818176,
      "learning_rate": 3.646875e-05,
      "loss": 0.0028,
      "step": 64950
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.08711527287960052,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0022,
      "step": 64960
    },
    {
      "epoch": 2.1656666666666666,
      "grad_norm": 0.3771207630634308,
      "learning_rate": 3.646458333333333e-05,
      "loss": 0.0025,
      "step": 64970
    },
    {
      "epoch": 2.166,
      "grad_norm": 0.08692482858896255,
      "learning_rate": 3.64625e-05,
      "loss": 0.0028,
      "step": 64980
    },
    {
      "epoch": 2.166333333333333,
      "grad_norm": 0.2898370921611786,
      "learning_rate": 3.646041666666667e-05,
      "loss": 0.0024,
      "step": 64990
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.2995105981826782,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.0021,
      "step": 65000
    },
    {
      "epoch": 2.167,
      "grad_norm": 0.11635386198759079,
      "learning_rate": 3.645625e-05,
      "loss": 0.003,
      "step": 65010
    },
    {
      "epoch": 2.1673333333333336,
      "grad_norm": 0.34795889258384705,
      "learning_rate": 3.645416666666667e-05,
      "loss": 0.0021,
      "step": 65020
    },
    {
      "epoch": 2.167666666666667,
      "grad_norm": 0.11587847769260406,
      "learning_rate": 3.645208333333334e-05,
      "loss": 0.0019,
      "step": 65030
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.2898617088794708,
      "learning_rate": 3.645e-05,
      "loss": 0.0023,
      "step": 65040
    },
    {
      "epoch": 2.1683333333333334,
      "grad_norm": 0.20309141278266907,
      "learning_rate": 3.644791666666667e-05,
      "loss": 0.0014,
      "step": 65050
    },
    {
      "epoch": 2.1686666666666667,
      "grad_norm": 0.11635920405387878,
      "learning_rate": 3.6445833333333336e-05,
      "loss": 0.0027,
      "step": 65060
    },
    {
      "epoch": 2.169,
      "grad_norm": 0.17130079865455627,
      "learning_rate": 3.644375e-05,
      "loss": 0.0026,
      "step": 65070
    },
    {
      "epoch": 2.1693333333333333,
      "grad_norm": 0.5219249129295349,
      "learning_rate": 3.644166666666667e-05,
      "loss": 0.0023,
      "step": 65080
    },
    {
      "epoch": 2.1696666666666666,
      "grad_norm": 0.23213158547878265,
      "learning_rate": 3.643958333333333e-05,
      "loss": 0.0022,
      "step": 65090
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.11611371487379074,
      "learning_rate": 3.6437500000000005e-05,
      "loss": 0.0018,
      "step": 65100
    },
    {
      "epoch": 2.1703333333333332,
      "grad_norm": 0.14533883333206177,
      "learning_rate": 3.6435416666666663e-05,
      "loss": 0.0025,
      "step": 65110
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.0732007548213005,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0023,
      "step": 65120
    },
    {
      "epoch": 2.171,
      "grad_norm": 0.029129749163985252,
      "learning_rate": 3.643125e-05,
      "loss": 0.0027,
      "step": 65130
    },
    {
      "epoch": 2.171333333333333,
      "grad_norm": 0.2029494345188141,
      "learning_rate": 3.642916666666667e-05,
      "loss": 0.0019,
      "step": 65140
    },
    {
      "epoch": 2.171666666666667,
      "grad_norm": 0.26090744137763977,
      "learning_rate": 3.642708333333333e-05,
      "loss": 0.0028,
      "step": 65150
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.057964589446783066,
      "learning_rate": 3.6425000000000004e-05,
      "loss": 0.0025,
      "step": 65160
    },
    {
      "epoch": 2.1723333333333334,
      "grad_norm": 0.23178322613239288,
      "learning_rate": 3.642291666666667e-05,
      "loss": 0.0016,
      "step": 65170
    },
    {
      "epoch": 2.1726666666666667,
      "grad_norm": 0.058071572333574295,
      "learning_rate": 3.6420833333333335e-05,
      "loss": 0.0026,
      "step": 65180
    },
    {
      "epoch": 2.173,
      "grad_norm": 0.3477572798728943,
      "learning_rate": 3.641875e-05,
      "loss": 0.0023,
      "step": 65190
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.2027754783630371,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.0029,
      "step": 65200
    },
    {
      "epoch": 2.1736666666666666,
      "grad_norm": 0.290078341960907,
      "learning_rate": 3.641458333333333e-05,
      "loss": 0.002,
      "step": 65210
    },
    {
      "epoch": 2.174,
      "grad_norm": 0.0030906193424016237,
      "learning_rate": 3.64125e-05,
      "loss": 0.0035,
      "step": 65220
    },
    {
      "epoch": 2.1743333333333332,
      "grad_norm": 0.08698839694261551,
      "learning_rate": 3.641041666666667e-05,
      "loss": 0.0037,
      "step": 65230
    },
    {
      "epoch": 2.1746666666666665,
      "grad_norm": 0.2316904515028,
      "learning_rate": 3.6408333333333335e-05,
      "loss": 0.0038,
      "step": 65240
    },
    {
      "epoch": 2.175,
      "grad_norm": 0.057950183749198914,
      "learning_rate": 3.640625e-05,
      "loss": 0.0036,
      "step": 65250
    },
    {
      "epoch": 2.1753333333333336,
      "grad_norm": 0.029107622802257538,
      "learning_rate": 3.6404166666666666e-05,
      "loss": 0.0022,
      "step": 65260
    },
    {
      "epoch": 2.175666666666667,
      "grad_norm": 0.058001816272735596,
      "learning_rate": 3.640208333333334e-05,
      "loss": 0.002,
      "step": 65270
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.006269738543778658,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0022,
      "step": 65280
    },
    {
      "epoch": 2.1763333333333335,
      "grad_norm": 0.3773112893104553,
      "learning_rate": 3.639791666666667e-05,
      "loss": 0.0019,
      "step": 65290
    },
    {
      "epoch": 2.1766666666666667,
      "grad_norm": 0.11605215817689896,
      "learning_rate": 3.6395833333333335e-05,
      "loss": 0.002,
      "step": 65300
    },
    {
      "epoch": 2.177,
      "grad_norm": 0.08685655146837234,
      "learning_rate": 3.639375000000001e-05,
      "loss": 0.0031,
      "step": 65310
    },
    {
      "epoch": 2.1773333333333333,
      "grad_norm": 0.08700237423181534,
      "learning_rate": 3.6391666666666666e-05,
      "loss": 0.0029,
      "step": 65320
    },
    {
      "epoch": 2.1776666666666666,
      "grad_norm": 0.00287904916331172,
      "learning_rate": 3.638958333333333e-05,
      "loss": 0.0025,
      "step": 65330
    },
    {
      "epoch": 2.178,
      "grad_norm": 0.5510632991790771,
      "learning_rate": 3.6387500000000004e-05,
      "loss": 0.0028,
      "step": 65340
    },
    {
      "epoch": 2.1783333333333332,
      "grad_norm": 0.11625449359416962,
      "learning_rate": 3.638541666666667e-05,
      "loss": 0.0018,
      "step": 65350
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.17433087527751923,
      "learning_rate": 3.6383333333333335e-05,
      "loss": 0.0028,
      "step": 65360
    },
    {
      "epoch": 2.179,
      "grad_norm": 0.20354606211185455,
      "learning_rate": 3.638125e-05,
      "loss": 0.0034,
      "step": 65370
    },
    {
      "epoch": 2.179333333333333,
      "grad_norm": 0.05820458009839058,
      "learning_rate": 3.637916666666667e-05,
      "loss": 0.0027,
      "step": 65380
    },
    {
      "epoch": 2.179666666666667,
      "grad_norm": 0.20321251451969147,
      "learning_rate": 3.637708333333333e-05,
      "loss": 0.0027,
      "step": 65390
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.2900591492652893,
      "learning_rate": 3.6375e-05,
      "loss": 0.0024,
      "step": 65400
    },
    {
      "epoch": 2.1803333333333335,
      "grad_norm": 0.17429980635643005,
      "learning_rate": 3.637291666666667e-05,
      "loss": 0.0032,
      "step": 65410
    },
    {
      "epoch": 2.1806666666666668,
      "grad_norm": 0.3479154109954834,
      "learning_rate": 3.6370833333333334e-05,
      "loss": 0.002,
      "step": 65420
    },
    {
      "epoch": 2.181,
      "grad_norm": 0.40603095293045044,
      "learning_rate": 3.636875e-05,
      "loss": 0.0032,
      "step": 65430
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.003709089010953903,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.0031,
      "step": 65440
    },
    {
      "epoch": 2.1816666666666666,
      "grad_norm": 0.08720526844263077,
      "learning_rate": 3.636458333333334e-05,
      "loss": 0.0013,
      "step": 65450
    },
    {
      "epoch": 2.182,
      "grad_norm": 0.29007115960121155,
      "learning_rate": 3.6362499999999996e-05,
      "loss": 0.0036,
      "step": 65460
    },
    {
      "epoch": 2.1823333333333332,
      "grad_norm": 0.23183906078338623,
      "learning_rate": 3.636041666666667e-05,
      "loss": 0.003,
      "step": 65470
    },
    {
      "epoch": 2.1826666666666665,
      "grad_norm": 0.06438498944044113,
      "learning_rate": 3.6358333333333334e-05,
      "loss": 0.0026,
      "step": 65480
    },
    {
      "epoch": 2.183,
      "grad_norm": 0.14492519199848175,
      "learning_rate": 3.6356250000000006e-05,
      "loss": 0.0022,
      "step": 65490
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.005208717193454504,
      "learning_rate": 3.6354166666666665e-05,
      "loss": 0.0016,
      "step": 65500
    },
    {
      "epoch": 2.183666666666667,
      "grad_norm": 0.46385443210601807,
      "learning_rate": 3.635208333333334e-05,
      "loss": 0.003,
      "step": 65510
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.5508196353912354,
      "learning_rate": 3.635e-05,
      "loss": 0.0038,
      "step": 65520
    },
    {
      "epoch": 2.1843333333333335,
      "grad_norm": 0.5217649936676025,
      "learning_rate": 3.634791666666667e-05,
      "loss": 0.0029,
      "step": 65530
    },
    {
      "epoch": 2.1846666666666668,
      "grad_norm": 0.08733931183815002,
      "learning_rate": 3.6345833333333334e-05,
      "loss": 0.0025,
      "step": 65540
    },
    {
      "epoch": 2.185,
      "grad_norm": 0.05808860808610916,
      "learning_rate": 3.6343750000000006e-05,
      "loss": 0.0026,
      "step": 65550
    },
    {
      "epoch": 2.1853333333333333,
      "grad_norm": 0.17400361597537994,
      "learning_rate": 3.634166666666667e-05,
      "loss": 0.0026,
      "step": 65560
    },
    {
      "epoch": 2.1856666666666666,
      "grad_norm": 0.14493905007839203,
      "learning_rate": 3.633958333333333e-05,
      "loss": 0.002,
      "step": 65570
    },
    {
      "epoch": 2.186,
      "grad_norm": 0.31887269020080566,
      "learning_rate": 3.63375e-05,
      "loss": 0.0021,
      "step": 65580
    },
    {
      "epoch": 2.1863333333333332,
      "grad_norm": 0.14514552056789398,
      "learning_rate": 3.633541666666667e-05,
      "loss": 0.0021,
      "step": 65590
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 1.3230409622192383,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0026,
      "step": 65600
    },
    {
      "epoch": 2.187,
      "grad_norm": 0.17396056652069092,
      "learning_rate": 3.633125e-05,
      "loss": 0.0018,
      "step": 65610
    },
    {
      "epoch": 2.187333333333333,
      "grad_norm": 0.029420889914035797,
      "learning_rate": 3.632916666666667e-05,
      "loss": 0.0035,
      "step": 65620
    },
    {
      "epoch": 2.187666666666667,
      "grad_norm": 0.17454130947589874,
      "learning_rate": 3.6327083333333337e-05,
      "loss": 0.0029,
      "step": 65630
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.08685806393623352,
      "learning_rate": 3.6325e-05,
      "loss": 0.0026,
      "step": 65640
    },
    {
      "epoch": 2.1883333333333335,
      "grad_norm": 0.2028382569551468,
      "learning_rate": 3.632291666666667e-05,
      "loss": 0.0016,
      "step": 65650
    },
    {
      "epoch": 2.1886666666666668,
      "grad_norm": 0.4924313426017761,
      "learning_rate": 3.632083333333334e-05,
      "loss": 0.0027,
      "step": 65660
    },
    {
      "epoch": 2.189,
      "grad_norm": 0.43481361865997314,
      "learning_rate": 3.631875e-05,
      "loss": 0.0025,
      "step": 65670
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.2028169333934784,
      "learning_rate": 3.631666666666667e-05,
      "loss": 0.0022,
      "step": 65680
    },
    {
      "epoch": 2.1896666666666667,
      "grad_norm": 0.354493647813797,
      "learning_rate": 3.6314583333333336e-05,
      "loss": 0.0024,
      "step": 65690
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.029595132917165756,
      "learning_rate": 3.63125e-05,
      "loss": 0.0032,
      "step": 65700
    },
    {
      "epoch": 2.1903333333333332,
      "grad_norm": 0.5504414439201355,
      "learning_rate": 3.631041666666667e-05,
      "loss": 0.0025,
      "step": 65710
    },
    {
      "epoch": 2.1906666666666665,
      "grad_norm": 0.2895563244819641,
      "learning_rate": 3.630833333333333e-05,
      "loss": 0.0024,
      "step": 65720
    },
    {
      "epoch": 2.191,
      "grad_norm": 0.5215556025505066,
      "learning_rate": 3.6306250000000005e-05,
      "loss": 0.0021,
      "step": 65730
    },
    {
      "epoch": 2.191333333333333,
      "grad_norm": 0.05811513215303421,
      "learning_rate": 3.6304166666666664e-05,
      "loss": 0.0032,
      "step": 65740
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 0.28958195447921753,
      "learning_rate": 3.6302083333333336e-05,
      "loss": 0.002,
      "step": 65750
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.16419163346290588,
      "learning_rate": 3.63e-05,
      "loss": 0.0023,
      "step": 65760
    },
    {
      "epoch": 2.1923333333333335,
      "grad_norm": 0.11602358520030975,
      "learning_rate": 3.6297916666666674e-05,
      "loss": 0.0023,
      "step": 65770
    },
    {
      "epoch": 2.1926666666666668,
      "grad_norm": 0.6956078410148621,
      "learning_rate": 3.629583333333333e-05,
      "loss": 0.0023,
      "step": 65780
    },
    {
      "epoch": 2.193,
      "grad_norm": 0.08693565428256989,
      "learning_rate": 3.6293750000000005e-05,
      "loss": 0.0041,
      "step": 65790
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.029611919075250626,
      "learning_rate": 3.629166666666667e-05,
      "loss": 0.0031,
      "step": 65800
    },
    {
      "epoch": 2.1936666666666667,
      "grad_norm": 0.004404223058372736,
      "learning_rate": 3.6289583333333336e-05,
      "loss": 0.0032,
      "step": 65810
    },
    {
      "epoch": 2.194,
      "grad_norm": 0.23170877993106842,
      "learning_rate": 3.62875e-05,
      "loss": 0.0031,
      "step": 65820
    },
    {
      "epoch": 2.1943333333333332,
      "grad_norm": 0.1449355185031891,
      "learning_rate": 3.6285416666666667e-05,
      "loss": 0.0022,
      "step": 65830
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.28988566994667053,
      "learning_rate": 3.628333333333334e-05,
      "loss": 0.0025,
      "step": 65840
    },
    {
      "epoch": 2.195,
      "grad_norm": 0.005332313012331724,
      "learning_rate": 3.628125e-05,
      "loss": 0.0028,
      "step": 65850
    },
    {
      "epoch": 2.195333333333333,
      "grad_norm": 0.268917977809906,
      "learning_rate": 3.627916666666667e-05,
      "loss": 0.0025,
      "step": 65860
    },
    {
      "epoch": 2.195666666666667,
      "grad_norm": 0.02961464412510395,
      "learning_rate": 3.6277083333333335e-05,
      "loss": 0.0028,
      "step": 65870
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.029456108808517456,
      "learning_rate": 3.6275e-05,
      "loss": 0.0023,
      "step": 65880
    },
    {
      "epoch": 2.1963333333333335,
      "grad_norm": 0.3475673496723175,
      "learning_rate": 3.6272916666666666e-05,
      "loss": 0.0024,
      "step": 65890
    },
    {
      "epoch": 2.1966666666666668,
      "grad_norm": 0.23160333931446075,
      "learning_rate": 3.627083333333334e-05,
      "loss": 0.0019,
      "step": 65900
    },
    {
      "epoch": 2.197,
      "grad_norm": 0.1447606384754181,
      "learning_rate": 3.6268750000000004e-05,
      "loss": 0.002,
      "step": 65910
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.17372368276119232,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0016,
      "step": 65920
    },
    {
      "epoch": 2.1976666666666667,
      "grad_norm": 0.31913042068481445,
      "learning_rate": 3.6264583333333335e-05,
      "loss": 0.0027,
      "step": 65930
    },
    {
      "epoch": 2.198,
      "grad_norm": 0.34784334897994995,
      "learning_rate": 3.62625e-05,
      "loss": 0.0024,
      "step": 65940
    },
    {
      "epoch": 2.1983333333333333,
      "grad_norm": 0.05807061493396759,
      "learning_rate": 3.6260416666666666e-05,
      "loss": 0.0025,
      "step": 65950
    },
    {
      "epoch": 2.1986666666666665,
      "grad_norm": 0.31855934858322144,
      "learning_rate": 3.625833333333333e-05,
      "loss": 0.0024,
      "step": 65960
    },
    {
      "epoch": 2.199,
      "grad_norm": 0.08685626834630966,
      "learning_rate": 3.6256250000000004e-05,
      "loss": 0.0022,
      "step": 65970
    },
    {
      "epoch": 2.199333333333333,
      "grad_norm": 0.20309187471866608,
      "learning_rate": 3.625416666666667e-05,
      "loss": 0.0026,
      "step": 65980
    },
    {
      "epoch": 2.1996666666666664,
      "grad_norm": 0.20265290141105652,
      "learning_rate": 3.6252083333333335e-05,
      "loss": 0.0028,
      "step": 65990
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.23206369578838348,
      "learning_rate": 3.625e-05,
      "loss": 0.0019,
      "step": 66000
    },
    {
      "epoch": 2.2003333333333335,
      "grad_norm": 0.28963473439216614,
      "learning_rate": 3.624791666666667e-05,
      "loss": 0.0019,
      "step": 66010
    },
    {
      "epoch": 2.2006666666666668,
      "grad_norm": 0.3766980469226837,
      "learning_rate": 3.624583333333333e-05,
      "loss": 0.0018,
      "step": 66020
    },
    {
      "epoch": 2.201,
      "grad_norm": 0.7679803371429443,
      "learning_rate": 3.6243750000000003e-05,
      "loss": 0.0034,
      "step": 66030
    },
    {
      "epoch": 2.2013333333333334,
      "grad_norm": 0.14506885409355164,
      "learning_rate": 3.624166666666667e-05,
      "loss": 0.0019,
      "step": 66040
    },
    {
      "epoch": 2.2016666666666667,
      "grad_norm": 0.49271097779273987,
      "learning_rate": 3.6239583333333334e-05,
      "loss": 0.0017,
      "step": 66050
    },
    {
      "epoch": 2.202,
      "grad_norm": 0.05803578346967697,
      "learning_rate": 3.62375e-05,
      "loss": 0.0026,
      "step": 66060
    },
    {
      "epoch": 2.2023333333333333,
      "grad_norm": 0.5083363056182861,
      "learning_rate": 3.6235416666666665e-05,
      "loss": 0.0027,
      "step": 66070
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.202651247382164,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.002,
      "step": 66080
    },
    {
      "epoch": 2.203,
      "grad_norm": 0.05843636021018028,
      "learning_rate": 3.623125e-05,
      "loss": 0.0033,
      "step": 66090
    },
    {
      "epoch": 2.203333333333333,
      "grad_norm": 0.14488160610198975,
      "learning_rate": 3.622916666666667e-05,
      "loss": 0.003,
      "step": 66100
    },
    {
      "epoch": 2.203666666666667,
      "grad_norm": 0.08714374154806137,
      "learning_rate": 3.6227083333333334e-05,
      "loss": 0.003,
      "step": 66110
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.23199297487735748,
      "learning_rate": 3.6225000000000006e-05,
      "loss": 0.0026,
      "step": 66120
    },
    {
      "epoch": 2.2043333333333335,
      "grad_norm": 0.38520050048828125,
      "learning_rate": 3.6222916666666665e-05,
      "loss": 0.0025,
      "step": 66130
    },
    {
      "epoch": 2.2046666666666668,
      "grad_norm": 0.11609494686126709,
      "learning_rate": 3.622083333333334e-05,
      "loss": 0.0019,
      "step": 66140
    },
    {
      "epoch": 2.205,
      "grad_norm": 0.6951788067817688,
      "learning_rate": 3.621875e-05,
      "loss": 0.0016,
      "step": 66150
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.05819760262966156,
      "learning_rate": 3.621666666666667e-05,
      "loss": 0.0025,
      "step": 66160
    },
    {
      "epoch": 2.2056666666666667,
      "grad_norm": 0.7121812105178833,
      "learning_rate": 3.6214583333333334e-05,
      "loss": 0.0019,
      "step": 66170
    },
    {
      "epoch": 2.206,
      "grad_norm": 0.05868864431977272,
      "learning_rate": 3.62125e-05,
      "loss": 0.0021,
      "step": 66180
    },
    {
      "epoch": 2.2063333333333333,
      "grad_norm": 0.23179680109024048,
      "learning_rate": 3.621041666666667e-05,
      "loss": 0.0021,
      "step": 66190
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.14484867453575134,
      "learning_rate": 3.620833333333333e-05,
      "loss": 0.0018,
      "step": 66200
    },
    {
      "epoch": 2.207,
      "grad_norm": 0.20306023955345154,
      "learning_rate": 3.620625e-05,
      "loss": 0.0022,
      "step": 66210
    },
    {
      "epoch": 2.207333333333333,
      "grad_norm": 0.11592352390289307,
      "learning_rate": 3.620416666666667e-05,
      "loss": 0.002,
      "step": 66220
    },
    {
      "epoch": 2.2076666666666664,
      "grad_norm": 0.029654953628778458,
      "learning_rate": 3.6202083333333334e-05,
      "loss": 0.0023,
      "step": 66230
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.08696819841861725,
      "learning_rate": 3.62e-05,
      "loss": 0.0025,
      "step": 66240
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 0.4055626094341278,
      "learning_rate": 3.619791666666667e-05,
      "loss": 0.0029,
      "step": 66250
    },
    {
      "epoch": 2.208666666666667,
      "grad_norm": 0.28982096910476685,
      "learning_rate": 3.619583333333334e-05,
      "loss": 0.0019,
      "step": 66260
    },
    {
      "epoch": 2.209,
      "grad_norm": 0.3755761682987213,
      "learning_rate": 3.619375e-05,
      "loss": 0.0022,
      "step": 66270
    },
    {
      "epoch": 2.2093333333333334,
      "grad_norm": 0.2028239220380783,
      "learning_rate": 3.619166666666667e-05,
      "loss": 0.002,
      "step": 66280
    },
    {
      "epoch": 2.2096666666666667,
      "grad_norm": 0.02949899062514305,
      "learning_rate": 3.618958333333334e-05,
      "loss": 0.0026,
      "step": 66290
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.11592578887939453,
      "learning_rate": 3.61875e-05,
      "loss": 0.0025,
      "step": 66300
    },
    {
      "epoch": 2.2103333333333333,
      "grad_norm": 0.4637816250324249,
      "learning_rate": 3.6185416666666664e-05,
      "loss": 0.0019,
      "step": 66310
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.08686820417642593,
      "learning_rate": 3.6183333333333336e-05,
      "loss": 0.0018,
      "step": 66320
    },
    {
      "epoch": 2.211,
      "grad_norm": 0.23167625069618225,
      "learning_rate": 3.618125e-05,
      "loss": 0.0029,
      "step": 66330
    },
    {
      "epoch": 2.211333333333333,
      "grad_norm": 0.1457539051771164,
      "learning_rate": 3.617916666666667e-05,
      "loss": 0.0017,
      "step": 66340
    },
    {
      "epoch": 2.211666666666667,
      "grad_norm": 0.05803835391998291,
      "learning_rate": 3.617708333333333e-05,
      "loss": 0.0033,
      "step": 66350
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.06257009506225586,
      "learning_rate": 3.6175000000000005e-05,
      "loss": 0.0018,
      "step": 66360
    },
    {
      "epoch": 2.2123333333333335,
      "grad_norm": 0.5264912247657776,
      "learning_rate": 3.617291666666667e-05,
      "loss": 0.004,
      "step": 66370
    },
    {
      "epoch": 2.212666666666667,
      "grad_norm": 0.08707302808761597,
      "learning_rate": 3.6170833333333336e-05,
      "loss": 0.0031,
      "step": 66380
    },
    {
      "epoch": 2.213,
      "grad_norm": 0.058950334787368774,
      "learning_rate": 3.616875e-05,
      "loss": 0.0023,
      "step": 66390
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.08717924356460571,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0022,
      "step": 66400
    },
    {
      "epoch": 2.2136666666666667,
      "grad_norm": 0.11585945636034012,
      "learning_rate": 3.616458333333333e-05,
      "loss": 0.0026,
      "step": 66410
    },
    {
      "epoch": 2.214,
      "grad_norm": 0.4922555088996887,
      "learning_rate": 3.61625e-05,
      "loss": 0.0012,
      "step": 66420
    },
    {
      "epoch": 2.2143333333333333,
      "grad_norm": 0.464042991399765,
      "learning_rate": 3.616041666666667e-05,
      "loss": 0.0026,
      "step": 66430
    },
    {
      "epoch": 2.2146666666666666,
      "grad_norm": 0.029340190812945366,
      "learning_rate": 3.6158333333333336e-05,
      "loss": 0.0024,
      "step": 66440
    },
    {
      "epoch": 2.215,
      "grad_norm": 0.1450658142566681,
      "learning_rate": 3.615625e-05,
      "loss": 0.0025,
      "step": 66450
    },
    {
      "epoch": 2.215333333333333,
      "grad_norm": 0.058105915784835815,
      "learning_rate": 3.615416666666667e-05,
      "loss": 0.0018,
      "step": 66460
    },
    {
      "epoch": 2.2156666666666665,
      "grad_norm": 0.02929968759417534,
      "learning_rate": 3.615208333333334e-05,
      "loss": 0.002,
      "step": 66470
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.20286321640014648,
      "learning_rate": 3.615e-05,
      "loss": 0.002,
      "step": 66480
    },
    {
      "epoch": 2.2163333333333335,
      "grad_norm": 0.08688914775848389,
      "learning_rate": 3.614791666666667e-05,
      "loss": 0.0025,
      "step": 66490
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.20372088253498077,
      "learning_rate": 3.6145833333333336e-05,
      "loss": 0.0026,
      "step": 66500
    },
    {
      "epoch": 2.217,
      "grad_norm": 0.17469939589500427,
      "learning_rate": 3.614375e-05,
      "loss": 0.0026,
      "step": 66510
    },
    {
      "epoch": 2.2173333333333334,
      "grad_norm": 0.03130744770169258,
      "learning_rate": 3.6141666666666667e-05,
      "loss": 0.0016,
      "step": 66520
    },
    {
      "epoch": 2.2176666666666667,
      "grad_norm": 0.022236142307519913,
      "learning_rate": 3.613958333333334e-05,
      "loss": 0.0035,
      "step": 66530
    },
    {
      "epoch": 2.218,
      "grad_norm": 0.1740431934595108,
      "learning_rate": 3.6137500000000004e-05,
      "loss": 0.002,
      "step": 66540
    },
    {
      "epoch": 2.2183333333333333,
      "grad_norm": 0.05799040570855141,
      "learning_rate": 3.613541666666666e-05,
      "loss": 0.0027,
      "step": 66550
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.05828461796045303,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0038,
      "step": 66560
    },
    {
      "epoch": 2.219,
      "grad_norm": 0.14539586007595062,
      "learning_rate": 3.613125e-05,
      "loss": 0.0025,
      "step": 66570
    },
    {
      "epoch": 2.219333333333333,
      "grad_norm": 0.005564373917877674,
      "learning_rate": 3.6129166666666666e-05,
      "loss": 0.0017,
      "step": 66580
    },
    {
      "epoch": 2.219666666666667,
      "grad_norm": 0.05845722183585167,
      "learning_rate": 3.612708333333333e-05,
      "loss": 0.0027,
      "step": 66590
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.05829240754246712,
      "learning_rate": 3.6125000000000004e-05,
      "loss": 0.0022,
      "step": 66600
    },
    {
      "epoch": 2.2203333333333335,
      "grad_norm": 0.4056395888328552,
      "learning_rate": 3.612291666666667e-05,
      "loss": 0.0025,
      "step": 66610
    },
    {
      "epoch": 2.220666666666667,
      "grad_norm": 0.3066067099571228,
      "learning_rate": 3.6120833333333335e-05,
      "loss": 0.002,
      "step": 66620
    },
    {
      "epoch": 2.221,
      "grad_norm": 0.029393771663308144,
      "learning_rate": 3.611875e-05,
      "loss": 0.0025,
      "step": 66630
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.05801751837134361,
      "learning_rate": 3.611666666666667e-05,
      "loss": 0.0026,
      "step": 66640
    },
    {
      "epoch": 2.2216666666666667,
      "grad_norm": 0.11592309921979904,
      "learning_rate": 3.611458333333334e-05,
      "loss": 0.0025,
      "step": 66650
    },
    {
      "epoch": 2.222,
      "grad_norm": 0.11675690859556198,
      "learning_rate": 3.61125e-05,
      "loss": 0.003,
      "step": 66660
    },
    {
      "epoch": 2.2223333333333333,
      "grad_norm": 0.006848831661045551,
      "learning_rate": 3.611041666666667e-05,
      "loss": 0.0023,
      "step": 66670
    },
    {
      "epoch": 2.2226666666666666,
      "grad_norm": 0.05819397792220116,
      "learning_rate": 3.6108333333333335e-05,
      "loss": 0.003,
      "step": 66680
    },
    {
      "epoch": 2.223,
      "grad_norm": 0.05810636654496193,
      "learning_rate": 3.610625e-05,
      "loss": 0.0026,
      "step": 66690
    },
    {
      "epoch": 2.223333333333333,
      "grad_norm": 0.26076623797416687,
      "learning_rate": 3.6104166666666666e-05,
      "loss": 0.0025,
      "step": 66700
    },
    {
      "epoch": 2.2236666666666665,
      "grad_norm": 0.029650410637259483,
      "learning_rate": 3.610208333333334e-05,
      "loss": 0.0025,
      "step": 66710
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.029314972460269928,
      "learning_rate": 3.61e-05,
      "loss": 0.0028,
      "step": 66720
    },
    {
      "epoch": 2.2243333333333335,
      "grad_norm": 0.14512906968593597,
      "learning_rate": 3.609791666666667e-05,
      "loss": 0.0024,
      "step": 66730
    },
    {
      "epoch": 2.224666666666667,
      "grad_norm": 0.11620524525642395,
      "learning_rate": 3.6095833333333334e-05,
      "loss": 0.0026,
      "step": 66740
    },
    {
      "epoch": 2.225,
      "grad_norm": 0.05808064714074135,
      "learning_rate": 3.6093750000000007e-05,
      "loss": 0.002,
      "step": 66750
    },
    {
      "epoch": 2.2253333333333334,
      "grad_norm": 0.058519523590803146,
      "learning_rate": 3.6091666666666665e-05,
      "loss": 0.0023,
      "step": 66760
    },
    {
      "epoch": 2.2256666666666667,
      "grad_norm": 0.05810902640223503,
      "learning_rate": 3.608958333333334e-05,
      "loss": 0.002,
      "step": 66770
    },
    {
      "epoch": 2.226,
      "grad_norm": 0.43479588627815247,
      "learning_rate": 3.60875e-05,
      "loss": 0.0026,
      "step": 66780
    },
    {
      "epoch": 2.2263333333333333,
      "grad_norm": 0.28935304284095764,
      "learning_rate": 3.608541666666667e-05,
      "loss": 0.0028,
      "step": 66790
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.2027963250875473,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.0022,
      "step": 66800
    },
    {
      "epoch": 2.227,
      "grad_norm": 0.14593768119812012,
      "learning_rate": 3.608125e-05,
      "loss": 0.0021,
      "step": 66810
    },
    {
      "epoch": 2.227333333333333,
      "grad_norm": 0.0875021442770958,
      "learning_rate": 3.607916666666667e-05,
      "loss": 0.0029,
      "step": 66820
    },
    {
      "epoch": 2.2276666666666665,
      "grad_norm": 0.12318753451108932,
      "learning_rate": 3.607708333333333e-05,
      "loss": 0.0023,
      "step": 66830
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.20284488797187805,
      "learning_rate": 3.6075e-05,
      "loss": 0.0017,
      "step": 66840
    },
    {
      "epoch": 2.2283333333333335,
      "grad_norm": 0.15625959634780884,
      "learning_rate": 3.607291666666667e-05,
      "loss": 0.0023,
      "step": 66850
    },
    {
      "epoch": 2.228666666666667,
      "grad_norm": 0.14619982242584229,
      "learning_rate": 3.6070833333333334e-05,
      "loss": 0.0022,
      "step": 66860
    },
    {
      "epoch": 2.229,
      "grad_norm": 0.23186346888542175,
      "learning_rate": 3.606875e-05,
      "loss": 0.0018,
      "step": 66870
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.20273378491401672,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0037,
      "step": 66880
    },
    {
      "epoch": 2.2296666666666667,
      "grad_norm": 0.4052143394947052,
      "learning_rate": 3.606458333333334e-05,
      "loss": 0.0023,
      "step": 66890
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.029109036549925804,
      "learning_rate": 3.60625e-05,
      "loss": 0.0028,
      "step": 66900
    },
    {
      "epoch": 2.2303333333333333,
      "grad_norm": 0.5503758788108826,
      "learning_rate": 3.606041666666667e-05,
      "loss": 0.0017,
      "step": 66910
    },
    {
      "epoch": 2.2306666666666666,
      "grad_norm": 0.11586012691259384,
      "learning_rate": 3.6058333333333333e-05,
      "loss": 0.003,
      "step": 66920
    },
    {
      "epoch": 2.231,
      "grad_norm": 0.003905339166522026,
      "learning_rate": 3.6056250000000006e-05,
      "loss": 0.0021,
      "step": 66930
    },
    {
      "epoch": 2.231333333333333,
      "grad_norm": 0.4051513373851776,
      "learning_rate": 3.6054166666666664e-05,
      "loss": 0.0028,
      "step": 66940
    },
    {
      "epoch": 2.2316666666666665,
      "grad_norm": 0.005313768517225981,
      "learning_rate": 3.605208333333334e-05,
      "loss": 0.0033,
      "step": 66950
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.3170032203197479,
      "learning_rate": 3.605e-05,
      "loss": 0.0029,
      "step": 66960
    },
    {
      "epoch": 2.2323333333333335,
      "grad_norm": 0.8639723062515259,
      "learning_rate": 3.604791666666667e-05,
      "loss": 0.0028,
      "step": 66970
    },
    {
      "epoch": 2.232666666666667,
      "grad_norm": 0.14481540024280548,
      "learning_rate": 3.604583333333333e-05,
      "loss": 0.0025,
      "step": 66980
    },
    {
      "epoch": 2.233,
      "grad_norm": 0.347234308719635,
      "learning_rate": 3.6043750000000005e-05,
      "loss": 0.0036,
      "step": 66990
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.14491896331310272,
      "learning_rate": 3.604166666666667e-05,
      "loss": 0.0025,
      "step": 67000
    },
    {
      "epoch": 2.2336666666666667,
      "grad_norm": 0.20264102518558502,
      "learning_rate": 3.6039583333333336e-05,
      "loss": 0.0013,
      "step": 67010
    },
    {
      "epoch": 2.234,
      "grad_norm": 0.4632686972618103,
      "learning_rate": 3.60375e-05,
      "loss": 0.0016,
      "step": 67020
    },
    {
      "epoch": 2.2343333333333333,
      "grad_norm": 0.14487606287002563,
      "learning_rate": 3.603541666666667e-05,
      "loss": 0.0026,
      "step": 67030
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.23167404532432556,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0024,
      "step": 67040
    },
    {
      "epoch": 2.235,
      "grad_norm": 0.029998699203133583,
      "learning_rate": 3.603125e-05,
      "loss": 0.0043,
      "step": 67050
    },
    {
      "epoch": 2.235333333333333,
      "grad_norm": 0.08668653666973114,
      "learning_rate": 3.602916666666667e-05,
      "loss": 0.0035,
      "step": 67060
    },
    {
      "epoch": 2.2356666666666665,
      "grad_norm": 0.3316972255706787,
      "learning_rate": 3.6027083333333336e-05,
      "loss": 0.0021,
      "step": 67070
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.4053134024143219,
      "learning_rate": 3.6025e-05,
      "loss": 0.0024,
      "step": 67080
    },
    {
      "epoch": 2.2363333333333335,
      "grad_norm": 0.11611369997262955,
      "learning_rate": 3.602291666666667e-05,
      "loss": 0.0021,
      "step": 67090
    },
    {
      "epoch": 2.236666666666667,
      "grad_norm": 0.05834604427218437,
      "learning_rate": 3.602083333333334e-05,
      "loss": 0.0017,
      "step": 67100
    },
    {
      "epoch": 2.237,
      "grad_norm": 0.14504744112491608,
      "learning_rate": 3.601875e-05,
      "loss": 0.0019,
      "step": 67110
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.05788592994213104,
      "learning_rate": 3.601666666666667e-05,
      "loss": 0.0029,
      "step": 67120
    },
    {
      "epoch": 2.2376666666666667,
      "grad_norm": 0.7417480945587158,
      "learning_rate": 3.6014583333333336e-05,
      "loss": 0.0029,
      "step": 67130
    },
    {
      "epoch": 2.238,
      "grad_norm": 0.5615096688270569,
      "learning_rate": 3.60125e-05,
      "loss": 0.0021,
      "step": 67140
    },
    {
      "epoch": 2.2383333333333333,
      "grad_norm": 0.9514060020446777,
      "learning_rate": 3.601041666666667e-05,
      "loss": 0.0022,
      "step": 67150
    },
    {
      "epoch": 2.2386666666666666,
      "grad_norm": 0.029440969228744507,
      "learning_rate": 3.600833333333333e-05,
      "loss": 0.0036,
      "step": 67160
    },
    {
      "epoch": 2.239,
      "grad_norm": 0.2897266745567322,
      "learning_rate": 3.6006250000000004e-05,
      "loss": 0.0024,
      "step": 67170
    },
    {
      "epoch": 2.239333333333333,
      "grad_norm": 0.724152684211731,
      "learning_rate": 3.600416666666667e-05,
      "loss": 0.0031,
      "step": 67180
    },
    {
      "epoch": 2.2396666666666665,
      "grad_norm": 0.3763403296470642,
      "learning_rate": 3.6002083333333335e-05,
      "loss": 0.003,
      "step": 67190
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.17374834418296814,
      "learning_rate": 3.6e-05,
      "loss": 0.0027,
      "step": 67200
    },
    {
      "epoch": 2.2403333333333335,
      "grad_norm": 0.5015414953231812,
      "learning_rate": 3.599791666666667e-05,
      "loss": 0.0032,
      "step": 67210
    },
    {
      "epoch": 2.240666666666667,
      "grad_norm": 0.1740807741880417,
      "learning_rate": 3.599583333333333e-05,
      "loss": 0.0027,
      "step": 67220
    },
    {
      "epoch": 2.241,
      "grad_norm": 0.0295416209846735,
      "learning_rate": 3.5993750000000004e-05,
      "loss": 0.0038,
      "step": 67230
    },
    {
      "epoch": 2.2413333333333334,
      "grad_norm": 0.2608526051044464,
      "learning_rate": 3.599166666666667e-05,
      "loss": 0.0024,
      "step": 67240
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 0.2895437180995941,
      "learning_rate": 3.5989583333333335e-05,
      "loss": 0.002,
      "step": 67250
    },
    {
      "epoch": 2.242,
      "grad_norm": 0.376655638217926,
      "learning_rate": 3.59875e-05,
      "loss": 0.0025,
      "step": 67260
    },
    {
      "epoch": 2.2423333333333333,
      "grad_norm": 0.23159156739711761,
      "learning_rate": 3.5985416666666666e-05,
      "loss": 0.0033,
      "step": 67270
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.08704181760549545,
      "learning_rate": 3.598333333333334e-05,
      "loss": 0.0032,
      "step": 67280
    },
    {
      "epoch": 2.243,
      "grad_norm": 0.40531590580940247,
      "learning_rate": 3.598125e-05,
      "loss": 0.0031,
      "step": 67290
    },
    {
      "epoch": 2.243333333333333,
      "grad_norm": 0.20260371267795563,
      "learning_rate": 3.597916666666667e-05,
      "loss": 0.0025,
      "step": 67300
    },
    {
      "epoch": 2.2436666666666665,
      "grad_norm": 0.20278239250183105,
      "learning_rate": 3.5977083333333335e-05,
      "loss": 0.0025,
      "step": 67310
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.20264595746994019,
      "learning_rate": 3.5975e-05,
      "loss": 0.0025,
      "step": 67320
    },
    {
      "epoch": 2.2443333333333335,
      "grad_norm": 0.20256873965263367,
      "learning_rate": 3.5972916666666666e-05,
      "loss": 0.0026,
      "step": 67330
    },
    {
      "epoch": 2.244666666666667,
      "grad_norm": 0.23161686956882477,
      "learning_rate": 3.597083333333334e-05,
      "loss": 0.002,
      "step": 67340
    },
    {
      "epoch": 2.245,
      "grad_norm": 0.058167580515146255,
      "learning_rate": 3.5968750000000004e-05,
      "loss": 0.0022,
      "step": 67350
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.08692464232444763,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.0026,
      "step": 67360
    },
    {
      "epoch": 2.2456666666666667,
      "grad_norm": 0.08716714382171631,
      "learning_rate": 3.5964583333333335e-05,
      "loss": 0.0016,
      "step": 67370
    },
    {
      "epoch": 2.246,
      "grad_norm": 0.28936195373535156,
      "learning_rate": 3.59625e-05,
      "loss": 0.0017,
      "step": 67380
    },
    {
      "epoch": 2.2463333333333333,
      "grad_norm": 0.23184189200401306,
      "learning_rate": 3.5960416666666665e-05,
      "loss": 0.0027,
      "step": 67390
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.28958848118782043,
      "learning_rate": 3.595833333333333e-05,
      "loss": 0.003,
      "step": 67400
    },
    {
      "epoch": 2.247,
      "grad_norm": 0.14476600289344788,
      "learning_rate": 3.595625e-05,
      "loss": 0.0029,
      "step": 67410
    },
    {
      "epoch": 2.247333333333333,
      "grad_norm": 0.006297841668128967,
      "learning_rate": 3.595416666666667e-05,
      "loss": 0.0024,
      "step": 67420
    },
    {
      "epoch": 2.2476666666666665,
      "grad_norm": 0.49225735664367676,
      "learning_rate": 3.5952083333333334e-05,
      "loss": 0.003,
      "step": 67430
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.2028568983078003,
      "learning_rate": 3.595e-05,
      "loss": 0.0033,
      "step": 67440
    },
    {
      "epoch": 2.2483333333333335,
      "grad_norm": 0.26064562797546387,
      "learning_rate": 3.594791666666667e-05,
      "loss": 0.0026,
      "step": 67450
    },
    {
      "epoch": 2.248666666666667,
      "grad_norm": 0.23186488449573517,
      "learning_rate": 3.594583333333334e-05,
      "loss": 0.0023,
      "step": 67460
    },
    {
      "epoch": 2.249,
      "grad_norm": 0.23157837986946106,
      "learning_rate": 3.594375e-05,
      "loss": 0.0023,
      "step": 67470
    },
    {
      "epoch": 2.2493333333333334,
      "grad_norm": 0.17375490069389343,
      "learning_rate": 3.594166666666667e-05,
      "loss": 0.0022,
      "step": 67480
    },
    {
      "epoch": 2.2496666666666667,
      "grad_norm": 0.20079568028450012,
      "learning_rate": 3.593958333333334e-05,
      "loss": 0.003,
      "step": 67490
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.029489366337656975,
      "learning_rate": 3.59375e-05,
      "loss": 0.0021,
      "step": 67500
    },
    {
      "epoch": 2.2503333333333333,
      "grad_norm": 0.2028464674949646,
      "learning_rate": 3.5935416666666665e-05,
      "loss": 0.0031,
      "step": 67510
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.029101667925715446,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0032,
      "step": 67520
    },
    {
      "epoch": 2.251,
      "grad_norm": 0.3181239366531372,
      "learning_rate": 3.593125e-05,
      "loss": 0.0029,
      "step": 67530
    },
    {
      "epoch": 2.251333333333333,
      "grad_norm": 0.1737295538187027,
      "learning_rate": 3.592916666666667e-05,
      "loss": 0.0027,
      "step": 67540
    },
    {
      "epoch": 2.2516666666666665,
      "grad_norm": 0.9101858139038086,
      "learning_rate": 3.5927083333333334e-05,
      "loss": 0.0028,
      "step": 67550
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.4246634244918823,
      "learning_rate": 3.5925000000000006e-05,
      "loss": 0.0026,
      "step": 67560
    },
    {
      "epoch": 2.2523333333333335,
      "grad_norm": 0.08713284879922867,
      "learning_rate": 3.5922916666666665e-05,
      "loss": 0.0023,
      "step": 67570
    },
    {
      "epoch": 2.252666666666667,
      "grad_norm": 0.20262138545513153,
      "learning_rate": 3.592083333333334e-05,
      "loss": 0.0018,
      "step": 67580
    },
    {
      "epoch": 2.253,
      "grad_norm": 0.2317870557308197,
      "learning_rate": 3.591875e-05,
      "loss": 0.0031,
      "step": 67590
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.26051220297813416,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.0023,
      "step": 67600
    },
    {
      "epoch": 2.2536666666666667,
      "grad_norm": 0.4920930564403534,
      "learning_rate": 3.591458333333333e-05,
      "loss": 0.0023,
      "step": 67610
    },
    {
      "epoch": 2.254,
      "grad_norm": 0.05830240994691849,
      "learning_rate": 3.5912500000000006e-05,
      "loss": 0.0029,
      "step": 67620
    },
    {
      "epoch": 2.2543333333333333,
      "grad_norm": 0.08716914802789688,
      "learning_rate": 3.591041666666667e-05,
      "loss": 0.0024,
      "step": 67630
    },
    {
      "epoch": 2.2546666666666666,
      "grad_norm": 0.08712618052959442,
      "learning_rate": 3.590833333333333e-05,
      "loss": 0.0028,
      "step": 67640
    },
    {
      "epoch": 2.255,
      "grad_norm": 0.23172160983085632,
      "learning_rate": 3.590625e-05,
      "loss": 0.0024,
      "step": 67650
    },
    {
      "epoch": 2.255333333333333,
      "grad_norm": 0.005184196401387453,
      "learning_rate": 3.590416666666667e-05,
      "loss": 0.0013,
      "step": 67660
    },
    {
      "epoch": 2.2556666666666665,
      "grad_norm": 0.520709753036499,
      "learning_rate": 3.590208333333333e-05,
      "loss": 0.0026,
      "step": 67670
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.08688424527645111,
      "learning_rate": 3.59e-05,
      "loss": 0.0031,
      "step": 67680
    },
    {
      "epoch": 2.2563333333333335,
      "grad_norm": 0.17365606129169464,
      "learning_rate": 3.589791666666667e-05,
      "loss": 0.0029,
      "step": 67690
    },
    {
      "epoch": 2.256666666666667,
      "grad_norm": 0.029422691091895103,
      "learning_rate": 3.5895833333333336e-05,
      "loss": 0.0021,
      "step": 67700
    },
    {
      "epoch": 2.257,
      "grad_norm": 0.6664069294929504,
      "learning_rate": 3.589375e-05,
      "loss": 0.0018,
      "step": 67710
    },
    {
      "epoch": 2.2573333333333334,
      "grad_norm": 0.26061302423477173,
      "learning_rate": 3.589166666666667e-05,
      "loss": 0.0029,
      "step": 67720
    },
    {
      "epoch": 2.2576666666666667,
      "grad_norm": 0.1735038161277771,
      "learning_rate": 3.588958333333334e-05,
      "loss": 0.0025,
      "step": 67730
    },
    {
      "epoch": 2.258,
      "grad_norm": 0.08707503229379654,
      "learning_rate": 3.5887500000000005e-05,
      "loss": 0.0015,
      "step": 67740
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 0.03026217222213745,
      "learning_rate": 3.5885416666666664e-05,
      "loss": 0.0031,
      "step": 67750
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.42558544874191284,
      "learning_rate": 3.5883333333333336e-05,
      "loss": 0.0028,
      "step": 67760
    },
    {
      "epoch": 2.259,
      "grad_norm": 0.23200638592243195,
      "learning_rate": 3.588125e-05,
      "loss": 0.002,
      "step": 67770
    },
    {
      "epoch": 2.259333333333333,
      "grad_norm": 0.05841505154967308,
      "learning_rate": 3.587916666666667e-05,
      "loss": 0.0024,
      "step": 67780
    },
    {
      "epoch": 2.2596666666666665,
      "grad_norm": 0.058310188353061676,
      "learning_rate": 3.587708333333333e-05,
      "loss": 0.0025,
      "step": 67790
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.3475927710533142,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 0.0024,
      "step": 67800
    },
    {
      "epoch": 2.2603333333333335,
      "grad_norm": 0.0063436455093324184,
      "learning_rate": 3.587291666666667e-05,
      "loss": 0.0025,
      "step": 67810
    },
    {
      "epoch": 2.260666666666667,
      "grad_norm": 0.4056689739227295,
      "learning_rate": 3.5870833333333336e-05,
      "loss": 0.0018,
      "step": 67820
    },
    {
      "epoch": 2.261,
      "grad_norm": 0.827194094657898,
      "learning_rate": 3.586875e-05,
      "loss": 0.0025,
      "step": 67830
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.40554940700531006,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0026,
      "step": 67840
    },
    {
      "epoch": 2.2616666666666667,
      "grad_norm": 0.08684723824262619,
      "learning_rate": 3.586458333333333e-05,
      "loss": 0.0019,
      "step": 67850
    },
    {
      "epoch": 2.262,
      "grad_norm": 0.05810302868485451,
      "learning_rate": 3.5862500000000004e-05,
      "loss": 0.0024,
      "step": 67860
    },
    {
      "epoch": 2.2623333333333333,
      "grad_norm": 0.14479005336761475,
      "learning_rate": 3.586041666666667e-05,
      "loss": 0.0029,
      "step": 67870
    },
    {
      "epoch": 2.2626666666666666,
      "grad_norm": 0.11599839478731155,
      "learning_rate": 3.5858333333333335e-05,
      "loss": 0.0027,
      "step": 67880
    },
    {
      "epoch": 2.263,
      "grad_norm": 0.031005410477519035,
      "learning_rate": 3.585625e-05,
      "loss": 0.0024,
      "step": 67890
    },
    {
      "epoch": 2.263333333333333,
      "grad_norm": 0.1743813455104828,
      "learning_rate": 3.5854166666666666e-05,
      "loss": 0.0022,
      "step": 67900
    },
    {
      "epoch": 2.2636666666666665,
      "grad_norm": 0.8977689743041992,
      "learning_rate": 3.585208333333334e-05,
      "loss": 0.0021,
      "step": 67910
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.40539804100990295,
      "learning_rate": 3.585e-05,
      "loss": 0.002,
      "step": 67920
    },
    {
      "epoch": 2.264333333333333,
      "grad_norm": 0.22017419338226318,
      "learning_rate": 3.584791666666667e-05,
      "loss": 0.0018,
      "step": 67930
    },
    {
      "epoch": 2.264666666666667,
      "grad_norm": 0.26067787408828735,
      "learning_rate": 3.5845833333333335e-05,
      "loss": 0.0025,
      "step": 67940
    },
    {
      "epoch": 2.265,
      "grad_norm": 0.029056021943688393,
      "learning_rate": 3.584375e-05,
      "loss": 0.0021,
      "step": 67950
    },
    {
      "epoch": 2.2653333333333334,
      "grad_norm": 0.00506672402843833,
      "learning_rate": 3.5841666666666666e-05,
      "loss": 0.0022,
      "step": 67960
    },
    {
      "epoch": 2.2656666666666667,
      "grad_norm": 0.02916477620601654,
      "learning_rate": 3.583958333333334e-05,
      "loss": 0.0021,
      "step": 67970
    },
    {
      "epoch": 2.266,
      "grad_norm": 0.1735820770263672,
      "learning_rate": 3.5837500000000004e-05,
      "loss": 0.0024,
      "step": 67980
    },
    {
      "epoch": 2.2663333333333333,
      "grad_norm": 0.4921207129955292,
      "learning_rate": 3.583541666666666e-05,
      "loss": 0.0025,
      "step": 67990
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.37627604603767395,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0025,
      "step": 68000
    },
    {
      "epoch": 2.267,
      "grad_norm": 0.318550169467926,
      "learning_rate": 3.583125e-05,
      "loss": 0.0028,
      "step": 68010
    },
    {
      "epoch": 2.267333333333333,
      "grad_norm": 0.40531110763549805,
      "learning_rate": 3.582916666666667e-05,
      "loss": 0.0015,
      "step": 68020
    },
    {
      "epoch": 2.2676666666666665,
      "grad_norm": 0.11600317060947418,
      "learning_rate": 3.582708333333333e-05,
      "loss": 0.0027,
      "step": 68030
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.08716011792421341,
      "learning_rate": 3.5825000000000003e-05,
      "loss": 0.0022,
      "step": 68040
    },
    {
      "epoch": 2.2683333333333335,
      "grad_norm": 0.23201677203178406,
      "learning_rate": 3.582291666666667e-05,
      "loss": 0.0019,
      "step": 68050
    },
    {
      "epoch": 2.268666666666667,
      "grad_norm": 0.08733077347278595,
      "learning_rate": 3.5820833333333334e-05,
      "loss": 0.0028,
      "step": 68060
    },
    {
      "epoch": 2.269,
      "grad_norm": 0.1450389176607132,
      "learning_rate": 3.581875e-05,
      "loss": 0.0027,
      "step": 68070
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.1735633909702301,
      "learning_rate": 3.581666666666667e-05,
      "loss": 0.0023,
      "step": 68080
    },
    {
      "epoch": 2.2696666666666667,
      "grad_norm": 0.11588422954082489,
      "learning_rate": 3.581458333333334e-05,
      "loss": 0.0014,
      "step": 68090
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.17342792451381683,
      "learning_rate": 3.58125e-05,
      "loss": 0.003,
      "step": 68100
    },
    {
      "epoch": 2.2703333333333333,
      "grad_norm": 0.029417160898447037,
      "learning_rate": 3.581041666666667e-05,
      "loss": 0.0022,
      "step": 68110
    },
    {
      "epoch": 2.2706666666666666,
      "grad_norm": 0.49195396900177,
      "learning_rate": 3.5808333333333334e-05,
      "loss": 0.0032,
      "step": 68120
    },
    {
      "epoch": 2.271,
      "grad_norm": 0.46320101618766785,
      "learning_rate": 3.580625e-05,
      "loss": 0.0018,
      "step": 68130
    },
    {
      "epoch": 2.271333333333333,
      "grad_norm": 0.17373640835285187,
      "learning_rate": 3.5804166666666665e-05,
      "loss": 0.002,
      "step": 68140
    },
    {
      "epoch": 2.2716666666666665,
      "grad_norm": 0.09863905608654022,
      "learning_rate": 3.580208333333334e-05,
      "loss": 0.0024,
      "step": 68150
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.1736181378364563,
      "learning_rate": 3.58e-05,
      "loss": 0.0034,
      "step": 68160
    },
    {
      "epoch": 2.272333333333333,
      "grad_norm": 0.11585628986358643,
      "learning_rate": 3.579791666666667e-05,
      "loss": 0.0018,
      "step": 68170
    },
    {
      "epoch": 2.272666666666667,
      "grad_norm": 0.029314784333109856,
      "learning_rate": 3.5795833333333334e-05,
      "loss": 0.0026,
      "step": 68180
    },
    {
      "epoch": 2.273,
      "grad_norm": 0.05814139544963837,
      "learning_rate": 3.5793750000000006e-05,
      "loss": 0.0025,
      "step": 68190
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.058342114090919495,
      "learning_rate": 3.5791666666666665e-05,
      "loss": 0.0025,
      "step": 68200
    },
    {
      "epoch": 2.2736666666666667,
      "grad_norm": 0.14557211101055145,
      "learning_rate": 3.578958333333334e-05,
      "loss": 0.0023,
      "step": 68210
    },
    {
      "epoch": 2.274,
      "grad_norm": 0.4176314175128937,
      "learning_rate": 3.57875e-05,
      "loss": 0.0027,
      "step": 68220
    },
    {
      "epoch": 2.2743333333333333,
      "grad_norm": 0.23168015480041504,
      "learning_rate": 3.578541666666667e-05,
      "loss": 0.0029,
      "step": 68230
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.23162506520748138,
      "learning_rate": 3.5783333333333333e-05,
      "loss": 0.0027,
      "step": 68240
    },
    {
      "epoch": 2.275,
      "grad_norm": 0.14484550058841705,
      "learning_rate": 3.578125e-05,
      "loss": 0.0024,
      "step": 68250
    },
    {
      "epoch": 2.275333333333333,
      "grad_norm": 0.43408718705177307,
      "learning_rate": 3.577916666666667e-05,
      "loss": 0.0027,
      "step": 68260
    },
    {
      "epoch": 2.2756666666666665,
      "grad_norm": 0.08690803498029709,
      "learning_rate": 3.577708333333333e-05,
      "loss": 0.0021,
      "step": 68270
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.1448122262954712,
      "learning_rate": 3.5775e-05,
      "loss": 0.0015,
      "step": 68280
    },
    {
      "epoch": 2.2763333333333335,
      "grad_norm": 0.05788334459066391,
      "learning_rate": 3.577291666666667e-05,
      "loss": 0.0024,
      "step": 68290
    },
    {
      "epoch": 2.276666666666667,
      "grad_norm": 0.00467017712071538,
      "learning_rate": 3.577083333333334e-05,
      "loss": 0.0027,
      "step": 68300
    },
    {
      "epoch": 2.277,
      "grad_norm": 0.004984257277101278,
      "learning_rate": 3.576875e-05,
      "loss": 0.002,
      "step": 68310
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.11603306233882904,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.0021,
      "step": 68320
    },
    {
      "epoch": 2.2776666666666667,
      "grad_norm": 0.23169346153736115,
      "learning_rate": 3.5764583333333336e-05,
      "loss": 0.0026,
      "step": 68330
    },
    {
      "epoch": 2.278,
      "grad_norm": 0.008116201497614384,
      "learning_rate": 3.57625e-05,
      "loss": 0.0025,
      "step": 68340
    },
    {
      "epoch": 2.2783333333333333,
      "grad_norm": 0.45082423090934753,
      "learning_rate": 3.576041666666667e-05,
      "loss": 0.0032,
      "step": 68350
    },
    {
      "epoch": 2.2786666666666666,
      "grad_norm": 0.08712998777627945,
      "learning_rate": 3.575833333333333e-05,
      "loss": 0.0035,
      "step": 68360
    },
    {
      "epoch": 2.279,
      "grad_norm": 0.3184334635734558,
      "learning_rate": 3.5756250000000005e-05,
      "loss": 0.0021,
      "step": 68370
    },
    {
      "epoch": 2.279333333333333,
      "grad_norm": 0.14507272839546204,
      "learning_rate": 3.5754166666666664e-05,
      "loss": 0.0023,
      "step": 68380
    },
    {
      "epoch": 2.2796666666666665,
      "grad_norm": 0.43415120244026184,
      "learning_rate": 3.5752083333333336e-05,
      "loss": 0.0019,
      "step": 68390
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.008584556169807911,
      "learning_rate": 3.575e-05,
      "loss": 0.0024,
      "step": 68400
    },
    {
      "epoch": 2.280333333333333,
      "grad_norm": 0.6950571537017822,
      "learning_rate": 3.574791666666667e-05,
      "loss": 0.0019,
      "step": 68410
    },
    {
      "epoch": 2.280666666666667,
      "grad_norm": 0.6164689660072327,
      "learning_rate": 3.574583333333333e-05,
      "loss": 0.002,
      "step": 68420
    },
    {
      "epoch": 2.281,
      "grad_norm": 0.059983253479003906,
      "learning_rate": 3.5743750000000005e-05,
      "loss": 0.002,
      "step": 68430
    },
    {
      "epoch": 2.2813333333333334,
      "grad_norm": 0.6280400156974792,
      "learning_rate": 3.574166666666667e-05,
      "loss": 0.0031,
      "step": 68440
    },
    {
      "epoch": 2.2816666666666667,
      "grad_norm": 0.26052674651145935,
      "learning_rate": 3.5739583333333336e-05,
      "loss": 0.0026,
      "step": 68450
    },
    {
      "epoch": 2.282,
      "grad_norm": 0.434552401304245,
      "learning_rate": 3.57375e-05,
      "loss": 0.0032,
      "step": 68460
    },
    {
      "epoch": 2.2823333333333333,
      "grad_norm": 0.007017694879323244,
      "learning_rate": 3.573541666666667e-05,
      "loss": 0.0025,
      "step": 68470
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.46301132440567017,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0031,
      "step": 68480
    },
    {
      "epoch": 2.283,
      "grad_norm": 0.23174136877059937,
      "learning_rate": 3.573125e-05,
      "loss": 0.0017,
      "step": 68490
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.05838388949632645,
      "learning_rate": 3.572916666666667e-05,
      "loss": 0.0023,
      "step": 68500
    },
    {
      "epoch": 2.2836666666666665,
      "grad_norm": 0.08717940002679825,
      "learning_rate": 3.5727083333333336e-05,
      "loss": 0.0017,
      "step": 68510
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.3184875547885895,
      "learning_rate": 3.5725e-05,
      "loss": 0.0035,
      "step": 68520
    },
    {
      "epoch": 2.2843333333333335,
      "grad_norm": 0.0868154913187027,
      "learning_rate": 3.5722916666666666e-05,
      "loss": 0.0028,
      "step": 68530
    },
    {
      "epoch": 2.284666666666667,
      "grad_norm": 0.1449097841978073,
      "learning_rate": 3.572083333333334e-05,
      "loss": 0.0023,
      "step": 68540
    },
    {
      "epoch": 2.285,
      "grad_norm": 0.14455106854438782,
      "learning_rate": 3.571875e-05,
      "loss": 0.0024,
      "step": 68550
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.02911992557346821,
      "learning_rate": 3.571666666666667e-05,
      "loss": 0.0023,
      "step": 68560
    },
    {
      "epoch": 2.2856666666666667,
      "grad_norm": 0.0868913009762764,
      "learning_rate": 3.5714583333333335e-05,
      "loss": 0.002,
      "step": 68570
    },
    {
      "epoch": 2.286,
      "grad_norm": 0.08706459403038025,
      "learning_rate": 3.571250000000001e-05,
      "loss": 0.0026,
      "step": 68580
    },
    {
      "epoch": 2.2863333333333333,
      "grad_norm": 0.1160973310470581,
      "learning_rate": 3.5710416666666666e-05,
      "loss": 0.0028,
      "step": 68590
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.23192152380943298,
      "learning_rate": 3.570833333333333e-05,
      "loss": 0.0029,
      "step": 68600
    },
    {
      "epoch": 2.287,
      "grad_norm": 0.20273974537849426,
      "learning_rate": 3.5706250000000004e-05,
      "loss": 0.0023,
      "step": 68610
    },
    {
      "epoch": 2.287333333333333,
      "grad_norm": 0.030446285381913185,
      "learning_rate": 3.570416666666667e-05,
      "loss": 0.0019,
      "step": 68620
    },
    {
      "epoch": 2.2876666666666665,
      "grad_norm": 0.5504254698753357,
      "learning_rate": 3.5702083333333335e-05,
      "loss": 0.0028,
      "step": 68630
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.318211168050766,
      "learning_rate": 3.57e-05,
      "loss": 0.0025,
      "step": 68640
    },
    {
      "epoch": 2.288333333333333,
      "grad_norm": 0.2303123027086258,
      "learning_rate": 3.569791666666667e-05,
      "loss": 0.0029,
      "step": 68650
    },
    {
      "epoch": 2.288666666666667,
      "grad_norm": 0.3761102557182312,
      "learning_rate": 3.569583333333333e-05,
      "loss": 0.0028,
      "step": 68660
    },
    {
      "epoch": 2.289,
      "grad_norm": 0.2604656219482422,
      "learning_rate": 3.5693750000000004e-05,
      "loss": 0.0026,
      "step": 68670
    },
    {
      "epoch": 2.2893333333333334,
      "grad_norm": 0.40498948097229004,
      "learning_rate": 3.569166666666667e-05,
      "loss": 0.0017,
      "step": 68680
    },
    {
      "epoch": 2.2896666666666667,
      "grad_norm": 0.437132865190506,
      "learning_rate": 3.5689583333333335e-05,
      "loss": 0.0026,
      "step": 68690
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.1446085274219513,
      "learning_rate": 3.56875e-05,
      "loss": 0.0025,
      "step": 68700
    },
    {
      "epoch": 2.2903333333333333,
      "grad_norm": 0.1450260877609253,
      "learning_rate": 3.5685416666666666e-05,
      "loss": 0.0018,
      "step": 68710
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.34737539291381836,
      "learning_rate": 3.568333333333334e-05,
      "loss": 0.0017,
      "step": 68720
    },
    {
      "epoch": 2.291,
      "grad_norm": 0.3862484395503998,
      "learning_rate": 3.5681249999999997e-05,
      "loss": 0.0018,
      "step": 68730
    },
    {
      "epoch": 2.291333333333333,
      "grad_norm": 0.010172328911721706,
      "learning_rate": 3.567916666666667e-05,
      "loss": 0.0019,
      "step": 68740
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 0.5495790839195251,
      "learning_rate": 3.5677083333333334e-05,
      "loss": 0.003,
      "step": 68750
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.37602153420448303,
      "learning_rate": 3.5675e-05,
      "loss": 0.0027,
      "step": 68760
    },
    {
      "epoch": 2.2923333333333336,
      "grad_norm": 0.5087072849273682,
      "learning_rate": 3.5672916666666665e-05,
      "loss": 0.0023,
      "step": 68770
    },
    {
      "epoch": 2.292666666666667,
      "grad_norm": 0.405015230178833,
      "learning_rate": 3.567083333333334e-05,
      "loss": 0.0019,
      "step": 68780
    },
    {
      "epoch": 2.293,
      "grad_norm": 0.004755010362714529,
      "learning_rate": 3.566875e-05,
      "loss": 0.0035,
      "step": 68790
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.05908668786287308,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0021,
      "step": 68800
    },
    {
      "epoch": 2.2936666666666667,
      "grad_norm": 0.26049092411994934,
      "learning_rate": 3.5664583333333334e-05,
      "loss": 0.0026,
      "step": 68810
    },
    {
      "epoch": 2.294,
      "grad_norm": 0.40503498911857605,
      "learning_rate": 3.5662500000000006e-05,
      "loss": 0.0025,
      "step": 68820
    },
    {
      "epoch": 2.2943333333333333,
      "grad_norm": 0.23142538964748383,
      "learning_rate": 3.566041666666667e-05,
      "loss": 0.0018,
      "step": 68830
    },
    {
      "epoch": 2.2946666666666666,
      "grad_norm": 0.02921185828745365,
      "learning_rate": 3.565833333333333e-05,
      "loss": 0.0017,
      "step": 68840
    },
    {
      "epoch": 2.295,
      "grad_norm": 0.17390385270118713,
      "learning_rate": 3.565625e-05,
      "loss": 0.0017,
      "step": 68850
    },
    {
      "epoch": 2.2953333333333332,
      "grad_norm": 0.23149850964546204,
      "learning_rate": 3.565416666666667e-05,
      "loss": 0.0017,
      "step": 68860
    },
    {
      "epoch": 2.2956666666666665,
      "grad_norm": 0.23137155175209045,
      "learning_rate": 3.5652083333333334e-05,
      "loss": 0.0025,
      "step": 68870
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.2606422007083893,
      "learning_rate": 3.565e-05,
      "loss": 0.0024,
      "step": 68880
    },
    {
      "epoch": 2.296333333333333,
      "grad_norm": 0.0293685644865036,
      "learning_rate": 3.564791666666667e-05,
      "loss": 0.0015,
      "step": 68890
    },
    {
      "epoch": 2.296666666666667,
      "grad_norm": 0.20265036821365356,
      "learning_rate": 3.564583333333334e-05,
      "loss": 0.0025,
      "step": 68900
    },
    {
      "epoch": 2.297,
      "grad_norm": 0.0035409871488809586,
      "learning_rate": 3.564375e-05,
      "loss": 0.0022,
      "step": 68910
    },
    {
      "epoch": 2.2973333333333334,
      "grad_norm": 0.02948855236172676,
      "learning_rate": 3.564166666666667e-05,
      "loss": 0.0019,
      "step": 68920
    },
    {
      "epoch": 2.2976666666666667,
      "grad_norm": 0.41183310747146606,
      "learning_rate": 3.563958333333334e-05,
      "loss": 0.0018,
      "step": 68930
    },
    {
      "epoch": 2.298,
      "grad_norm": 0.2891925573348999,
      "learning_rate": 3.56375e-05,
      "loss": 0.0027,
      "step": 68940
    },
    {
      "epoch": 2.2983333333333333,
      "grad_norm": 0.26064983010292053,
      "learning_rate": 3.563541666666667e-05,
      "loss": 0.0023,
      "step": 68950
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.14504677057266235,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0025,
      "step": 68960
    },
    {
      "epoch": 2.299,
      "grad_norm": 0.11581604182720184,
      "learning_rate": 3.563125e-05,
      "loss": 0.0026,
      "step": 68970
    },
    {
      "epoch": 2.2993333333333332,
      "grad_norm": 0.6448616981506348,
      "learning_rate": 3.562916666666667e-05,
      "loss": 0.0027,
      "step": 68980
    },
    {
      "epoch": 2.2996666666666665,
      "grad_norm": 0.28950372338294983,
      "learning_rate": 3.562708333333333e-05,
      "loss": 0.0027,
      "step": 68990
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.17392407357692719,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 0.002,
      "step": 69000
    },
    {
      "epoch": 2.3003333333333336,
      "grad_norm": 0.20260410010814667,
      "learning_rate": 3.5622916666666664e-05,
      "loss": 0.0016,
      "step": 69010
    },
    {
      "epoch": 2.300666666666667,
      "grad_norm": 0.2026028335094452,
      "learning_rate": 3.5620833333333336e-05,
      "loss": 0.0021,
      "step": 69020
    },
    {
      "epoch": 2.301,
      "grad_norm": 0.4053203761577606,
      "learning_rate": 3.561875e-05,
      "loss": 0.0025,
      "step": 69030
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.260593056678772,
      "learning_rate": 3.561666666666667e-05,
      "loss": 0.0021,
      "step": 69040
    },
    {
      "epoch": 2.3016666666666667,
      "grad_norm": 0.11574020981788635,
      "learning_rate": 3.561458333333333e-05,
      "loss": 0.0022,
      "step": 69050
    },
    {
      "epoch": 2.302,
      "grad_norm": 0.37588274478912354,
      "learning_rate": 3.5612500000000005e-05,
      "loss": 0.0023,
      "step": 69060
    },
    {
      "epoch": 2.3023333333333333,
      "grad_norm": 0.14468960464000702,
      "learning_rate": 3.561041666666667e-05,
      "loss": 0.0018,
      "step": 69070
    },
    {
      "epoch": 2.3026666666666666,
      "grad_norm": 0.05809590965509415,
      "learning_rate": 3.560833333333333e-05,
      "loss": 0.0026,
      "step": 69080
    },
    {
      "epoch": 2.303,
      "grad_norm": 0.03793114423751831,
      "learning_rate": 3.560625e-05,
      "loss": 0.0023,
      "step": 69090
    },
    {
      "epoch": 2.3033333333333332,
      "grad_norm": 0.2892887592315674,
      "learning_rate": 3.560416666666667e-05,
      "loss": 0.0028,
      "step": 69100
    },
    {
      "epoch": 2.3036666666666665,
      "grad_norm": 0.404792457818985,
      "learning_rate": 3.560208333333334e-05,
      "loss": 0.0019,
      "step": 69110
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.3181535005569458,
      "learning_rate": 3.56e-05,
      "loss": 0.0017,
      "step": 69120
    },
    {
      "epoch": 2.304333333333333,
      "grad_norm": 0.08687034249305725,
      "learning_rate": 3.559791666666667e-05,
      "loss": 0.0024,
      "step": 69130
    },
    {
      "epoch": 2.304666666666667,
      "grad_norm": 0.5119404196739197,
      "learning_rate": 3.5595833333333336e-05,
      "loss": 0.003,
      "step": 69140
    },
    {
      "epoch": 2.305,
      "grad_norm": 0.08713535964488983,
      "learning_rate": 3.559375e-05,
      "loss": 0.0028,
      "step": 69150
    },
    {
      "epoch": 2.3053333333333335,
      "grad_norm": 0.006641062907874584,
      "learning_rate": 3.559166666666667e-05,
      "loss": 0.0031,
      "step": 69160
    },
    {
      "epoch": 2.3056666666666668,
      "grad_norm": 0.029397519305348396,
      "learning_rate": 3.558958333333334e-05,
      "loss": 0.0023,
      "step": 69170
    },
    {
      "epoch": 2.306,
      "grad_norm": 0.08710265159606934,
      "learning_rate": 3.5587500000000004e-05,
      "loss": 0.0024,
      "step": 69180
    },
    {
      "epoch": 2.3063333333333333,
      "grad_norm": 0.14497384428977966,
      "learning_rate": 3.558541666666667e-05,
      "loss": 0.0026,
      "step": 69190
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.2608579397201538,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.0023,
      "step": 69200
    },
    {
      "epoch": 2.307,
      "grad_norm": 0.3759998381137848,
      "learning_rate": 3.558125e-05,
      "loss": 0.0022,
      "step": 69210
    },
    {
      "epoch": 2.3073333333333332,
      "grad_norm": 0.31851327419281006,
      "learning_rate": 3.5579166666666666e-05,
      "loss": 0.0016,
      "step": 69220
    },
    {
      "epoch": 2.3076666666666665,
      "grad_norm": 0.06486575305461884,
      "learning_rate": 3.557708333333333e-05,
      "loss": 0.0022,
      "step": 69230
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.05832023173570633,
      "learning_rate": 3.5575000000000004e-05,
      "loss": 0.0025,
      "step": 69240
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 0.31805869936943054,
      "learning_rate": 3.557291666666667e-05,
      "loss": 0.0022,
      "step": 69250
    },
    {
      "epoch": 2.3086666666666664,
      "grad_norm": 0.14442859590053558,
      "learning_rate": 3.5570833333333335e-05,
      "loss": 0.0032,
      "step": 69260
    },
    {
      "epoch": 2.309,
      "grad_norm": 0.11570852249860764,
      "learning_rate": 3.556875e-05,
      "loss": 0.0031,
      "step": 69270
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.37592825293540955,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.0024,
      "step": 69280
    },
    {
      "epoch": 2.3096666666666668,
      "grad_norm": 0.7885609269142151,
      "learning_rate": 3.556458333333333e-05,
      "loss": 0.003,
      "step": 69290
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.2314339131116867,
      "learning_rate": 3.5562500000000004e-05,
      "loss": 0.0022,
      "step": 69300
    },
    {
      "epoch": 2.3103333333333333,
      "grad_norm": 0.5500247478485107,
      "learning_rate": 3.556041666666667e-05,
      "loss": 0.0027,
      "step": 69310
    },
    {
      "epoch": 2.3106666666666666,
      "grad_norm": 0.2023659497499466,
      "learning_rate": 3.5558333333333335e-05,
      "loss": 0.0039,
      "step": 69320
    },
    {
      "epoch": 2.311,
      "grad_norm": 0.11577269434928894,
      "learning_rate": 3.555625e-05,
      "loss": 0.0025,
      "step": 69330
    },
    {
      "epoch": 2.3113333333333332,
      "grad_norm": 0.029231946915388107,
      "learning_rate": 3.5554166666666666e-05,
      "loss": 0.0022,
      "step": 69340
    },
    {
      "epoch": 2.3116666666666665,
      "grad_norm": 0.2311898171901703,
      "learning_rate": 3.555208333333334e-05,
      "loss": 0.0028,
      "step": 69350
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.31813448667526245,
      "learning_rate": 3.555e-05,
      "loss": 0.0024,
      "step": 69360
    },
    {
      "epoch": 2.312333333333333,
      "grad_norm": 0.11608085036277771,
      "learning_rate": 3.554791666666667e-05,
      "loss": 0.0022,
      "step": 69370
    },
    {
      "epoch": 2.312666666666667,
      "grad_norm": 0.20228883624076843,
      "learning_rate": 3.5545833333333334e-05,
      "loss": 0.003,
      "step": 69380
    },
    {
      "epoch": 2.313,
      "grad_norm": 0.6940966844558716,
      "learning_rate": 3.554375000000001e-05,
      "loss": 0.0029,
      "step": 69390
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.4341448247432709,
      "learning_rate": 3.5541666666666665e-05,
      "loss": 0.0015,
      "step": 69400
    },
    {
      "epoch": 2.3136666666666668,
      "grad_norm": 0.34717607498168945,
      "learning_rate": 3.553958333333334e-05,
      "loss": 0.0028,
      "step": 69410
    },
    {
      "epoch": 2.314,
      "grad_norm": 0.14460709691047668,
      "learning_rate": 3.55375e-05,
      "loss": 0.0017,
      "step": 69420
    },
    {
      "epoch": 2.3143333333333334,
      "grad_norm": 0.3182049095630646,
      "learning_rate": 3.553541666666667e-05,
      "loss": 0.0026,
      "step": 69430
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.49168816208839417,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0025,
      "step": 69440
    },
    {
      "epoch": 2.315,
      "grad_norm": 0.2902003526687622,
      "learning_rate": 3.553125e-05,
      "loss": 0.0024,
      "step": 69450
    },
    {
      "epoch": 2.3153333333333332,
      "grad_norm": 0.6655434370040894,
      "learning_rate": 3.552916666666667e-05,
      "loss": 0.0032,
      "step": 69460
    },
    {
      "epoch": 2.3156666666666665,
      "grad_norm": 0.11573659628629684,
      "learning_rate": 3.552708333333333e-05,
      "loss": 0.003,
      "step": 69470
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.17359133064746857,
      "learning_rate": 3.5525e-05,
      "loss": 0.0018,
      "step": 69480
    },
    {
      "epoch": 2.3163333333333336,
      "grad_norm": 0.11587167531251907,
      "learning_rate": 3.552291666666667e-05,
      "loss": 0.0024,
      "step": 69490
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.1446286290884018,
      "learning_rate": 3.5520833333333334e-05,
      "loss": 0.0021,
      "step": 69500
    },
    {
      "epoch": 2.317,
      "grad_norm": 0.3760567903518677,
      "learning_rate": 3.551875e-05,
      "loss": 0.0016,
      "step": 69510
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.029987581074237823,
      "learning_rate": 3.551666666666667e-05,
      "loss": 0.0023,
      "step": 69520
    },
    {
      "epoch": 2.3176666666666668,
      "grad_norm": 0.17351777851581573,
      "learning_rate": 3.551458333333334e-05,
      "loss": 0.0034,
      "step": 69530
    },
    {
      "epoch": 2.318,
      "grad_norm": 0.05922596901655197,
      "learning_rate": 3.55125e-05,
      "loss": 0.0021,
      "step": 69540
    },
    {
      "epoch": 2.3183333333333334,
      "grad_norm": 0.11669087409973145,
      "learning_rate": 3.551041666666667e-05,
      "loss": 0.0021,
      "step": 69550
    },
    {
      "epoch": 2.3186666666666667,
      "grad_norm": 0.20296384394168854,
      "learning_rate": 3.5508333333333334e-05,
      "loss": 0.0021,
      "step": 69560
    },
    {
      "epoch": 2.319,
      "grad_norm": 0.37625616788864136,
      "learning_rate": 3.550625e-05,
      "loss": 0.0024,
      "step": 69570
    },
    {
      "epoch": 2.3193333333333332,
      "grad_norm": 0.5204719305038452,
      "learning_rate": 3.5504166666666665e-05,
      "loss": 0.0026,
      "step": 69580
    },
    {
      "epoch": 2.3196666666666665,
      "grad_norm": 0.14471109211444855,
      "learning_rate": 3.550208333333334e-05,
      "loss": 0.0015,
      "step": 69590
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.0580042339861393,
      "learning_rate": 3.55e-05,
      "loss": 0.0026,
      "step": 69600
    },
    {
      "epoch": 2.320333333333333,
      "grad_norm": 0.5497945547103882,
      "learning_rate": 3.549791666666667e-05,
      "loss": 0.0022,
      "step": 69610
    },
    {
      "epoch": 2.320666666666667,
      "grad_norm": 0.43372756242752075,
      "learning_rate": 3.549583333333333e-05,
      "loss": 0.0032,
      "step": 69620
    },
    {
      "epoch": 2.321,
      "grad_norm": 0.14471638202667236,
      "learning_rate": 3.5493750000000006e-05,
      "loss": 0.0022,
      "step": 69630
    },
    {
      "epoch": 2.3213333333333335,
      "grad_norm": 0.2895073890686035,
      "learning_rate": 3.5491666666666664e-05,
      "loss": 0.0023,
      "step": 69640
    },
    {
      "epoch": 2.3216666666666668,
      "grad_norm": 0.029116516932845116,
      "learning_rate": 3.5489583333333337e-05,
      "loss": 0.0021,
      "step": 69650
    },
    {
      "epoch": 2.322,
      "grad_norm": 0.20271210372447968,
      "learning_rate": 3.54875e-05,
      "loss": 0.003,
      "step": 69660
    },
    {
      "epoch": 2.3223333333333334,
      "grad_norm": 0.4050018787384033,
      "learning_rate": 3.5485416666666674e-05,
      "loss": 0.0025,
      "step": 69670
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.02937549538910389,
      "learning_rate": 3.548333333333333e-05,
      "loss": 0.003,
      "step": 69680
    },
    {
      "epoch": 2.323,
      "grad_norm": 0.1735214740037918,
      "learning_rate": 3.548125e-05,
      "loss": 0.0019,
      "step": 69690
    },
    {
      "epoch": 2.3233333333333333,
      "grad_norm": 0.2892976999282837,
      "learning_rate": 3.547916666666667e-05,
      "loss": 0.0018,
      "step": 69700
    },
    {
      "epoch": 2.3236666666666665,
      "grad_norm": 0.06845036149024963,
      "learning_rate": 3.5477083333333336e-05,
      "loss": 0.0022,
      "step": 69710
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.4340921938419342,
      "learning_rate": 3.5475e-05,
      "loss": 0.0032,
      "step": 69720
    },
    {
      "epoch": 2.324333333333333,
      "grad_norm": 0.058004770427942276,
      "learning_rate": 3.547291666666667e-05,
      "loss": 0.0015,
      "step": 69730
    },
    {
      "epoch": 2.3246666666666664,
      "grad_norm": 0.17409071326255798,
      "learning_rate": 3.547083333333334e-05,
      "loss": 0.0025,
      "step": 69740
    },
    {
      "epoch": 2.325,
      "grad_norm": 0.020751943811774254,
      "learning_rate": 3.546875e-05,
      "loss": 0.0024,
      "step": 69750
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.14494824409484863,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0034,
      "step": 69760
    },
    {
      "epoch": 2.3256666666666668,
      "grad_norm": 0.4920351803302765,
      "learning_rate": 3.5464583333333336e-05,
      "loss": 0.002,
      "step": 69770
    },
    {
      "epoch": 2.326,
      "grad_norm": 0.20263737440109253,
      "learning_rate": 3.54625e-05,
      "loss": 0.0017,
      "step": 69780
    },
    {
      "epoch": 2.3263333333333334,
      "grad_norm": 0.1448146253824234,
      "learning_rate": 3.546041666666667e-05,
      "loss": 0.0025,
      "step": 69790
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.14480982720851898,
      "learning_rate": 3.545833333333333e-05,
      "loss": 0.0019,
      "step": 69800
    },
    {
      "epoch": 2.327,
      "grad_norm": 0.28922420740127563,
      "learning_rate": 3.5456250000000005e-05,
      "loss": 0.0018,
      "step": 69810
    },
    {
      "epoch": 2.3273333333333333,
      "grad_norm": 0.0043823570013046265,
      "learning_rate": 3.545416666666666e-05,
      "loss": 0.0028,
      "step": 69820
    },
    {
      "epoch": 2.3276666666666666,
      "grad_norm": 0.2606392204761505,
      "learning_rate": 3.5452083333333336e-05,
      "loss": 0.0028,
      "step": 69830
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.5387387275695801,
      "learning_rate": 3.545e-05,
      "loss": 0.0033,
      "step": 69840
    },
    {
      "epoch": 2.328333333333333,
      "grad_norm": 0.05809897184371948,
      "learning_rate": 3.5447916666666667e-05,
      "loss": 0.0017,
      "step": 69850
    },
    {
      "epoch": 2.328666666666667,
      "grad_norm": 0.11567947268486023,
      "learning_rate": 3.544583333333333e-05,
      "loss": 0.0029,
      "step": 69860
    },
    {
      "epoch": 2.329,
      "grad_norm": 0.17364533245563507,
      "learning_rate": 3.5443750000000004e-05,
      "loss": 0.0019,
      "step": 69870
    },
    {
      "epoch": 2.3293333333333335,
      "grad_norm": 0.2606423497200012,
      "learning_rate": 3.544166666666667e-05,
      "loss": 0.0029,
      "step": 69880
    },
    {
      "epoch": 2.3296666666666668,
      "grad_norm": 0.4630407989025116,
      "learning_rate": 3.5439583333333335e-05,
      "loss": 0.0022,
      "step": 69890
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.058562930673360825,
      "learning_rate": 3.54375e-05,
      "loss": 0.0021,
      "step": 69900
    },
    {
      "epoch": 2.3303333333333334,
      "grad_norm": 0.029626624658703804,
      "learning_rate": 3.543541666666667e-05,
      "loss": 0.0015,
      "step": 69910
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.02935834601521492,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.0022,
      "step": 69920
    },
    {
      "epoch": 2.331,
      "grad_norm": 0.17370496690273285,
      "learning_rate": 3.543125e-05,
      "loss": 0.0029,
      "step": 69930
    },
    {
      "epoch": 2.3313333333333333,
      "grad_norm": 0.029008768498897552,
      "learning_rate": 3.542916666666667e-05,
      "loss": 0.0019,
      "step": 69940
    },
    {
      "epoch": 2.3316666666666666,
      "grad_norm": 0.17380021512508392,
      "learning_rate": 3.5427083333333335e-05,
      "loss": 0.0024,
      "step": 69950
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.23175647854804993,
      "learning_rate": 3.5425e-05,
      "loss": 0.0015,
      "step": 69960
    },
    {
      "epoch": 2.332333333333333,
      "grad_norm": 0.2604881823062897,
      "learning_rate": 3.5422916666666666e-05,
      "loss": 0.0035,
      "step": 69970
    },
    {
      "epoch": 2.3326666666666664,
      "grad_norm": 0.2025502622127533,
      "learning_rate": 3.542083333333334e-05,
      "loss": 0.002,
      "step": 69980
    },
    {
      "epoch": 2.333,
      "grad_norm": 0.23161497712135315,
      "learning_rate": 3.5418750000000004e-05,
      "loss": 0.0026,
      "step": 69990
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.20246393978595734,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.0031,
      "step": 70000
    },
    {
      "epoch": 2.333666666666667,
      "grad_norm": 0.05830248445272446,
      "learning_rate": 3.5414583333333335e-05,
      "loss": 0.0025,
      "step": 70010
    },
    {
      "epoch": 2.334,
      "grad_norm": 0.14477501809597015,
      "learning_rate": 3.541250000000001e-05,
      "loss": 0.0023,
      "step": 70020
    },
    {
      "epoch": 2.3343333333333334,
      "grad_norm": 0.0579574815928936,
      "learning_rate": 3.5410416666666666e-05,
      "loss": 0.0021,
      "step": 70030
    },
    {
      "epoch": 2.3346666666666667,
      "grad_norm": 0.08704715967178345,
      "learning_rate": 3.540833333333334e-05,
      "loss": 0.0033,
      "step": 70040
    },
    {
      "epoch": 2.335,
      "grad_norm": 0.2607201635837555,
      "learning_rate": 3.5406250000000003e-05,
      "loss": 0.0024,
      "step": 70050
    },
    {
      "epoch": 2.3353333333333333,
      "grad_norm": 0.11587253212928772,
      "learning_rate": 3.540416666666667e-05,
      "loss": 0.0026,
      "step": 70060
    },
    {
      "epoch": 2.3356666666666666,
      "grad_norm": 0.434062659740448,
      "learning_rate": 3.5402083333333334e-05,
      "loss": 0.0021,
      "step": 70070
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.23163694143295288,
      "learning_rate": 3.54e-05,
      "loss": 0.0023,
      "step": 70080
    },
    {
      "epoch": 2.336333333333333,
      "grad_norm": 0.2605084180831909,
      "learning_rate": 3.539791666666667e-05,
      "loss": 0.003,
      "step": 70090
    },
    {
      "epoch": 2.336666666666667,
      "grad_norm": 0.11578845232725143,
      "learning_rate": 3.539583333333333e-05,
      "loss": 0.0019,
      "step": 70100
    },
    {
      "epoch": 2.337,
      "grad_norm": 0.43389877676963806,
      "learning_rate": 3.539375e-05,
      "loss": 0.0027,
      "step": 70110
    },
    {
      "epoch": 2.3373333333333335,
      "grad_norm": 0.3472083806991577,
      "learning_rate": 3.539166666666667e-05,
      "loss": 0.0017,
      "step": 70120
    },
    {
      "epoch": 2.337666666666667,
      "grad_norm": 0.11571837961673737,
      "learning_rate": 3.5389583333333334e-05,
      "loss": 0.0018,
      "step": 70130
    },
    {
      "epoch": 2.338,
      "grad_norm": 0.029137277975678444,
      "learning_rate": 3.53875e-05,
      "loss": 0.0032,
      "step": 70140
    },
    {
      "epoch": 2.3383333333333334,
      "grad_norm": 0.058101534843444824,
      "learning_rate": 3.538541666666667e-05,
      "loss": 0.0026,
      "step": 70150
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.43388107419013977,
      "learning_rate": 3.538333333333334e-05,
      "loss": 0.0039,
      "step": 70160
    },
    {
      "epoch": 2.339,
      "grad_norm": 0.6073693037033081,
      "learning_rate": 3.5381249999999996e-05,
      "loss": 0.0017,
      "step": 70170
    },
    {
      "epoch": 2.3393333333333333,
      "grad_norm": 0.1444288194179535,
      "learning_rate": 3.537916666666667e-05,
      "loss": 0.0021,
      "step": 70180
    },
    {
      "epoch": 2.3396666666666666,
      "grad_norm": 0.8548805713653564,
      "learning_rate": 3.5377083333333334e-05,
      "loss": 0.0036,
      "step": 70190
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.1448095142841339,
      "learning_rate": 3.5375e-05,
      "loss": 0.0024,
      "step": 70200
    },
    {
      "epoch": 2.340333333333333,
      "grad_norm": 0.007470131851732731,
      "learning_rate": 3.5372916666666665e-05,
      "loss": 0.0024,
      "step": 70210
    },
    {
      "epoch": 2.3406666666666665,
      "grad_norm": 0.26037368178367615,
      "learning_rate": 3.537083333333334e-05,
      "loss": 0.0021,
      "step": 70220
    },
    {
      "epoch": 2.341,
      "grad_norm": 0.31804725527763367,
      "learning_rate": 3.536875e-05,
      "loss": 0.0018,
      "step": 70230
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.26021790504455566,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0018,
      "step": 70240
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 0.031609851866960526,
      "learning_rate": 3.5364583333333333e-05,
      "loss": 0.0026,
      "step": 70250
    },
    {
      "epoch": 2.342,
      "grad_norm": 0.02989388071000576,
      "learning_rate": 3.5362500000000006e-05,
      "loss": 0.0028,
      "step": 70260
    },
    {
      "epoch": 2.3423333333333334,
      "grad_norm": 0.4916955530643463,
      "learning_rate": 3.536041666666667e-05,
      "loss": 0.0043,
      "step": 70270
    },
    {
      "epoch": 2.3426666666666667,
      "grad_norm": 0.31820932030677795,
      "learning_rate": 3.535833333333334e-05,
      "loss": 0.0027,
      "step": 70280
    },
    {
      "epoch": 2.343,
      "grad_norm": 0.20252569019794464,
      "learning_rate": 3.535625e-05,
      "loss": 0.0023,
      "step": 70290
    },
    {
      "epoch": 2.3433333333333333,
      "grad_norm": 0.4047103524208069,
      "learning_rate": 3.535416666666667e-05,
      "loss": 0.0019,
      "step": 70300
    },
    {
      "epoch": 2.3436666666666666,
      "grad_norm": 0.2893238961696625,
      "learning_rate": 3.535208333333333e-05,
      "loss": 0.0024,
      "step": 70310
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.029101397842168808,
      "learning_rate": 3.535e-05,
      "loss": 0.0028,
      "step": 70320
    },
    {
      "epoch": 2.344333333333333,
      "grad_norm": 0.20925745368003845,
      "learning_rate": 3.534791666666667e-05,
      "loss": 0.0029,
      "step": 70330
    },
    {
      "epoch": 2.344666666666667,
      "grad_norm": 0.08695152401924133,
      "learning_rate": 3.5345833333333336e-05,
      "loss": 0.0029,
      "step": 70340
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 0.3470783829689026,
      "learning_rate": 3.534375e-05,
      "loss": 0.003,
      "step": 70350
    },
    {
      "epoch": 2.3453333333333335,
      "grad_norm": 0.4338129758834839,
      "learning_rate": 3.534166666666667e-05,
      "loss": 0.0027,
      "step": 70360
    },
    {
      "epoch": 2.345666666666667,
      "grad_norm": 0.02916954644024372,
      "learning_rate": 3.533958333333334e-05,
      "loss": 0.0026,
      "step": 70370
    },
    {
      "epoch": 2.346,
      "grad_norm": 0.05793176218867302,
      "learning_rate": 3.53375e-05,
      "loss": 0.0032,
      "step": 70380
    },
    {
      "epoch": 2.3463333333333334,
      "grad_norm": 0.14462998509407043,
      "learning_rate": 3.533541666666667e-05,
      "loss": 0.002,
      "step": 70390
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.11602084338665009,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0021,
      "step": 70400
    },
    {
      "epoch": 2.347,
      "grad_norm": 0.030306411907076836,
      "learning_rate": 3.533125e-05,
      "loss": 0.002,
      "step": 70410
    },
    {
      "epoch": 2.3473333333333333,
      "grad_norm": 1.0280344486236572,
      "learning_rate": 3.532916666666667e-05,
      "loss": 0.0036,
      "step": 70420
    },
    {
      "epoch": 2.3476666666666666,
      "grad_norm": 0.23168861865997314,
      "learning_rate": 3.532708333333333e-05,
      "loss": 0.0036,
      "step": 70430
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.029652055352926254,
      "learning_rate": 3.5325000000000005e-05,
      "loss": 0.0025,
      "step": 70440
    },
    {
      "epoch": 2.348333333333333,
      "grad_norm": 0.17352686822414398,
      "learning_rate": 3.5322916666666664e-05,
      "loss": 0.0026,
      "step": 70450
    },
    {
      "epoch": 2.3486666666666665,
      "grad_norm": 0.029047861695289612,
      "learning_rate": 3.5320833333333336e-05,
      "loss": 0.0021,
      "step": 70460
    },
    {
      "epoch": 2.349,
      "grad_norm": 0.23147161304950714,
      "learning_rate": 3.531875e-05,
      "loss": 0.0013,
      "step": 70470
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.05833219364285469,
      "learning_rate": 3.531666666666667e-05,
      "loss": 0.0031,
      "step": 70480
    },
    {
      "epoch": 2.349666666666667,
      "grad_norm": 0.23154667019844055,
      "learning_rate": 3.531458333333333e-05,
      "loss": 0.0018,
      "step": 70490
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.20237557590007782,
      "learning_rate": 3.5312500000000005e-05,
      "loss": 0.0025,
      "step": 70500
    },
    {
      "epoch": 2.3503333333333334,
      "grad_norm": 0.28916507959365845,
      "learning_rate": 3.531041666666667e-05,
      "loss": 0.0019,
      "step": 70510
    },
    {
      "epoch": 2.3506666666666667,
      "grad_norm": 0.3471076190471649,
      "learning_rate": 3.5308333333333336e-05,
      "loss": 0.0017,
      "step": 70520
    },
    {
      "epoch": 2.351,
      "grad_norm": 0.029406407848000526,
      "learning_rate": 3.530625e-05,
      "loss": 0.0022,
      "step": 70530
    },
    {
      "epoch": 2.3513333333333333,
      "grad_norm": 0.11593008786439896,
      "learning_rate": 3.5304166666666666e-05,
      "loss": 0.0027,
      "step": 70540
    },
    {
      "epoch": 2.3516666666666666,
      "grad_norm": 0.34686315059661865,
      "learning_rate": 3.530208333333334e-05,
      "loss": 0.0018,
      "step": 70550
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.17359326779842377,
      "learning_rate": 3.53e-05,
      "loss": 0.002,
      "step": 70560
    },
    {
      "epoch": 2.352333333333333,
      "grad_norm": 0.11559730023145676,
      "learning_rate": 3.529791666666667e-05,
      "loss": 0.002,
      "step": 70570
    },
    {
      "epoch": 2.352666666666667,
      "grad_norm": 0.08716250956058502,
      "learning_rate": 3.5295833333333335e-05,
      "loss": 0.0024,
      "step": 70580
    },
    {
      "epoch": 2.3529999999999998,
      "grad_norm": 0.14468468725681305,
      "learning_rate": 3.529375e-05,
      "loss": 0.0019,
      "step": 70590
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.004850001540035009,
      "learning_rate": 3.5291666666666666e-05,
      "loss": 0.0018,
      "step": 70600
    },
    {
      "epoch": 2.353666666666667,
      "grad_norm": 0.2889541983604431,
      "learning_rate": 3.528958333333334e-05,
      "loss": 0.0019,
      "step": 70610
    },
    {
      "epoch": 2.354,
      "grad_norm": 0.14461679756641388,
      "learning_rate": 3.5287500000000004e-05,
      "loss": 0.0017,
      "step": 70620
    },
    {
      "epoch": 2.3543333333333334,
      "grad_norm": 0.144584059715271,
      "learning_rate": 3.528541666666667e-05,
      "loss": 0.0016,
      "step": 70630
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.02910042740404606,
      "learning_rate": 3.5283333333333335e-05,
      "loss": 0.0031,
      "step": 70640
    },
    {
      "epoch": 2.355,
      "grad_norm": 0.46270614862442017,
      "learning_rate": 3.528125e-05,
      "loss": 0.002,
      "step": 70650
    },
    {
      "epoch": 2.3553333333333333,
      "grad_norm": 0.17349763214588165,
      "learning_rate": 3.5279166666666666e-05,
      "loss": 0.0028,
      "step": 70660
    },
    {
      "epoch": 2.3556666666666666,
      "grad_norm": 0.26032570004463196,
      "learning_rate": 3.527708333333333e-05,
      "loss": 0.0021,
      "step": 70670
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.05791863426566124,
      "learning_rate": 3.5275000000000004e-05,
      "loss": 0.0022,
      "step": 70680
    },
    {
      "epoch": 2.356333333333333,
      "grad_norm": 0.05806302651762962,
      "learning_rate": 3.527291666666667e-05,
      "loss": 0.0024,
      "step": 70690
    },
    {
      "epoch": 2.3566666666666665,
      "grad_norm": 0.02926480770111084,
      "learning_rate": 3.5270833333333335e-05,
      "loss": 0.0019,
      "step": 70700
    },
    {
      "epoch": 2.357,
      "grad_norm": 0.08695404976606369,
      "learning_rate": 3.526875e-05,
      "loss": 0.0038,
      "step": 70710
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.17345577478408813,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.003,
      "step": 70720
    },
    {
      "epoch": 2.357666666666667,
      "grad_norm": 0.029224511235952377,
      "learning_rate": 3.526458333333333e-05,
      "loss": 0.003,
      "step": 70730
    },
    {
      "epoch": 2.358,
      "grad_norm": 0.05796024575829506,
      "learning_rate": 3.52625e-05,
      "loss": 0.0027,
      "step": 70740
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 0.20451340079307556,
      "learning_rate": 3.526041666666667e-05,
      "loss": 0.0022,
      "step": 70750
    },
    {
      "epoch": 2.3586666666666667,
      "grad_norm": 0.08690667897462845,
      "learning_rate": 3.5258333333333334e-05,
      "loss": 0.0017,
      "step": 70760
    },
    {
      "epoch": 2.359,
      "grad_norm": 0.2027612179517746,
      "learning_rate": 3.525625e-05,
      "loss": 0.0028,
      "step": 70770
    },
    {
      "epoch": 2.3593333333333333,
      "grad_norm": 0.08689496666193008,
      "learning_rate": 3.5254166666666665e-05,
      "loss": 0.0027,
      "step": 70780
    },
    {
      "epoch": 2.3596666666666666,
      "grad_norm": 0.14450763165950775,
      "learning_rate": 3.525208333333334e-05,
      "loss": 0.0028,
      "step": 70790
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.17377881705760956,
      "learning_rate": 3.525e-05,
      "loss": 0.0029,
      "step": 70800
    },
    {
      "epoch": 2.360333333333333,
      "grad_norm": 0.2027590572834015,
      "learning_rate": 3.524791666666667e-05,
      "loss": 0.0025,
      "step": 70810
    },
    {
      "epoch": 2.360666666666667,
      "grad_norm": 0.05783623084425926,
      "learning_rate": 3.5245833333333334e-05,
      "loss": 0.0023,
      "step": 70820
    },
    {
      "epoch": 2.3609999999999998,
      "grad_norm": 0.02932041510939598,
      "learning_rate": 3.5243750000000006e-05,
      "loss": 0.0018,
      "step": 70830
    },
    {
      "epoch": 2.3613333333333335,
      "grad_norm": 0.14464734494686127,
      "learning_rate": 3.5241666666666665e-05,
      "loss": 0.0019,
      "step": 70840
    },
    {
      "epoch": 2.361666666666667,
      "grad_norm": 0.2603190839290619,
      "learning_rate": 3.523958333333334e-05,
      "loss": 0.0016,
      "step": 70850
    },
    {
      "epoch": 2.362,
      "grad_norm": 0.6419469714164734,
      "learning_rate": 3.52375e-05,
      "loss": 0.0028,
      "step": 70860
    },
    {
      "epoch": 2.3623333333333334,
      "grad_norm": 0.40510308742523193,
      "learning_rate": 3.523541666666667e-05,
      "loss": 0.0018,
      "step": 70870
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.05780439451336861,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.0028,
      "step": 70880
    },
    {
      "epoch": 2.363,
      "grad_norm": 0.08670845627784729,
      "learning_rate": 3.523125e-05,
      "loss": 0.0027,
      "step": 70890
    },
    {
      "epoch": 2.3633333333333333,
      "grad_norm": 0.2892202138900757,
      "learning_rate": 3.522916666666667e-05,
      "loss": 0.0016,
      "step": 70900
    },
    {
      "epoch": 2.3636666666666666,
      "grad_norm": 0.2026890069246292,
      "learning_rate": 3.522708333333333e-05,
      "loss": 0.0023,
      "step": 70910
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.17368070781230927,
      "learning_rate": 3.5225e-05,
      "loss": 0.0018,
      "step": 70920
    },
    {
      "epoch": 2.364333333333333,
      "grad_norm": 0.11791130155324936,
      "learning_rate": 3.522291666666667e-05,
      "loss": 0.0024,
      "step": 70930
    },
    {
      "epoch": 2.3646666666666665,
      "grad_norm": 0.4048057794570923,
      "learning_rate": 3.522083333333333e-05,
      "loss": 0.0025,
      "step": 70940
    },
    {
      "epoch": 2.365,
      "grad_norm": 0.1443508267402649,
      "learning_rate": 3.521875e-05,
      "loss": 0.002,
      "step": 70950
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.16675402224063873,
      "learning_rate": 3.521666666666667e-05,
      "loss": 0.002,
      "step": 70960
    },
    {
      "epoch": 2.365666666666667,
      "grad_norm": 0.2600902020931244,
      "learning_rate": 3.521458333333334e-05,
      "loss": 0.0035,
      "step": 70970
    },
    {
      "epoch": 2.366,
      "grad_norm": 0.02897198125720024,
      "learning_rate": 3.52125e-05,
      "loss": 0.0028,
      "step": 70980
    },
    {
      "epoch": 2.3663333333333334,
      "grad_norm": 0.4625432789325714,
      "learning_rate": 3.521041666666667e-05,
      "loss": 0.0017,
      "step": 70990
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.17351141571998596,
      "learning_rate": 3.520833333333334e-05,
      "loss": 0.003,
      "step": 71000
    },
    {
      "epoch": 2.367,
      "grad_norm": 0.26026177406311035,
      "learning_rate": 3.520625e-05,
      "loss": 0.0023,
      "step": 71010
    },
    {
      "epoch": 2.3673333333333333,
      "grad_norm": 0.0872252881526947,
      "learning_rate": 3.5204166666666664e-05,
      "loss": 0.0027,
      "step": 71020
    },
    {
      "epoch": 2.3676666666666666,
      "grad_norm": 0.003584477584809065,
      "learning_rate": 3.5202083333333336e-05,
      "loss": 0.0028,
      "step": 71030
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.231368288397789,
      "learning_rate": 3.52e-05,
      "loss": 0.0034,
      "step": 71040
    },
    {
      "epoch": 2.368333333333333,
      "grad_norm": 0.28725141286849976,
      "learning_rate": 3.519791666666667e-05,
      "loss": 0.0036,
      "step": 71050
    },
    {
      "epoch": 2.3686666666666665,
      "grad_norm": 0.17344866693019867,
      "learning_rate": 3.519583333333333e-05,
      "loss": 0.0023,
      "step": 71060
    },
    {
      "epoch": 2.3689999999999998,
      "grad_norm": 0.057843271642923355,
      "learning_rate": 3.5193750000000005e-05,
      "loss": 0.0025,
      "step": 71070
    },
    {
      "epoch": 2.3693333333333335,
      "grad_norm": 0.0047430056147277355,
      "learning_rate": 3.519166666666667e-05,
      "loss": 0.0031,
      "step": 71080
    },
    {
      "epoch": 2.369666666666667,
      "grad_norm": 0.2605508863925934,
      "learning_rate": 3.5189583333333336e-05,
      "loss": 0.0025,
      "step": 71090
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.0868934616446495,
      "learning_rate": 3.51875e-05,
      "loss": 0.0016,
      "step": 71100
    },
    {
      "epoch": 2.3703333333333334,
      "grad_norm": 0.14468470215797424,
      "learning_rate": 3.5185416666666674e-05,
      "loss": 0.0025,
      "step": 71110
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.3180162012577057,
      "learning_rate": 3.518333333333333e-05,
      "loss": 0.0021,
      "step": 71120
    },
    {
      "epoch": 2.371,
      "grad_norm": 0.20256172120571136,
      "learning_rate": 3.518125e-05,
      "loss": 0.002,
      "step": 71130
    },
    {
      "epoch": 2.3713333333333333,
      "grad_norm": 0.058218732476234436,
      "learning_rate": 3.517916666666667e-05,
      "loss": 0.0021,
      "step": 71140
    },
    {
      "epoch": 2.3716666666666666,
      "grad_norm": 0.17345575988292694,
      "learning_rate": 3.5177083333333336e-05,
      "loss": 0.0028,
      "step": 71150
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.4625628888607025,
      "learning_rate": 3.5175e-05,
      "loss": 0.0029,
      "step": 71160
    },
    {
      "epoch": 2.372333333333333,
      "grad_norm": 0.08711660653352737,
      "learning_rate": 3.517291666666667e-05,
      "loss": 0.0033,
      "step": 71170
    },
    {
      "epoch": 2.3726666666666665,
      "grad_norm": 0.02942018397152424,
      "learning_rate": 3.517083333333334e-05,
      "loss": 0.0029,
      "step": 71180
    },
    {
      "epoch": 2.373,
      "grad_norm": 0.40517112612724304,
      "learning_rate": 3.516875e-05,
      "loss": 0.002,
      "step": 71190
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.26018720865249634,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0024,
      "step": 71200
    },
    {
      "epoch": 2.373666666666667,
      "grad_norm": 0.14453572034835815,
      "learning_rate": 3.5164583333333335e-05,
      "loss": 0.0027,
      "step": 71210
    },
    {
      "epoch": 2.374,
      "grad_norm": 0.23116102814674377,
      "learning_rate": 3.51625e-05,
      "loss": 0.0026,
      "step": 71220
    },
    {
      "epoch": 2.3743333333333334,
      "grad_norm": 0.3337208926677704,
      "learning_rate": 3.5160416666666666e-05,
      "loss": 0.0015,
      "step": 71230
    },
    {
      "epoch": 2.3746666666666667,
      "grad_norm": 0.05771621689200401,
      "learning_rate": 3.515833333333334e-05,
      "loss": 0.0015,
      "step": 71240
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.0883050486445427,
      "learning_rate": 3.5156250000000004e-05,
      "loss": 0.0021,
      "step": 71250
    },
    {
      "epoch": 2.3753333333333333,
      "grad_norm": 0.2312387079000473,
      "learning_rate": 3.515416666666666e-05,
      "loss": 0.0025,
      "step": 71260
    },
    {
      "epoch": 2.3756666666666666,
      "grad_norm": 0.20247600972652435,
      "learning_rate": 3.5152083333333335e-05,
      "loss": 0.0038,
      "step": 71270
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.02918594889342785,
      "learning_rate": 3.515e-05,
      "loss": 0.0016,
      "step": 71280
    },
    {
      "epoch": 2.376333333333333,
      "grad_norm": 0.1450996994972229,
      "learning_rate": 3.5147916666666666e-05,
      "loss": 0.0022,
      "step": 71290
    },
    {
      "epoch": 2.3766666666666665,
      "grad_norm": 0.1445591002702713,
      "learning_rate": 3.514583333333333e-05,
      "loss": 0.0022,
      "step": 71300
    },
    {
      "epoch": 2.377,
      "grad_norm": 0.1735265702009201,
      "learning_rate": 3.5143750000000004e-05,
      "loss": 0.0026,
      "step": 71310
    },
    {
      "epoch": 2.3773333333333335,
      "grad_norm": 0.5205019116401672,
      "learning_rate": 3.514166666666667e-05,
      "loss": 0.0021,
      "step": 71320
    },
    {
      "epoch": 2.377666666666667,
      "grad_norm": 0.26055216789245605,
      "learning_rate": 3.5139583333333335e-05,
      "loss": 0.0021,
      "step": 71330
    },
    {
      "epoch": 2.378,
      "grad_norm": 0.08703846484422684,
      "learning_rate": 3.51375e-05,
      "loss": 0.0014,
      "step": 71340
    },
    {
      "epoch": 2.3783333333333334,
      "grad_norm": 0.20226728916168213,
      "learning_rate": 3.513541666666667e-05,
      "loss": 0.0018,
      "step": 71350
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.4623626470565796,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.002,
      "step": 71360
    },
    {
      "epoch": 2.379,
      "grad_norm": 0.058011602610349655,
      "learning_rate": 3.5131250000000004e-05,
      "loss": 0.003,
      "step": 71370
    },
    {
      "epoch": 2.3793333333333333,
      "grad_norm": 0.02910809963941574,
      "learning_rate": 3.512916666666667e-05,
      "loss": 0.0017,
      "step": 71380
    },
    {
      "epoch": 2.3796666666666666,
      "grad_norm": 0.20249897241592407,
      "learning_rate": 3.5127083333333334e-05,
      "loss": 0.0023,
      "step": 71390
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.2313610017299652,
      "learning_rate": 3.5125e-05,
      "loss": 0.0012,
      "step": 71400
    },
    {
      "epoch": 2.380333333333333,
      "grad_norm": 0.20266766846179962,
      "learning_rate": 3.5122916666666665e-05,
      "loss": 0.0021,
      "step": 71410
    },
    {
      "epoch": 2.3806666666666665,
      "grad_norm": 0.17340213060379028,
      "learning_rate": 3.512083333333334e-05,
      "loss": 0.0023,
      "step": 71420
    },
    {
      "epoch": 2.3810000000000002,
      "grad_norm": 0.1738639622926712,
      "learning_rate": 3.511875e-05,
      "loss": 0.0018,
      "step": 71430
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.4045906662940979,
      "learning_rate": 3.511666666666667e-05,
      "loss": 0.0021,
      "step": 71440
    },
    {
      "epoch": 2.381666666666667,
      "grad_norm": 0.6398349404335022,
      "learning_rate": 3.5114583333333334e-05,
      "loss": 0.0029,
      "step": 71450
    },
    {
      "epoch": 2.382,
      "grad_norm": 0.14460675418376923,
      "learning_rate": 3.5112500000000006e-05,
      "loss": 0.0019,
      "step": 71460
    },
    {
      "epoch": 2.3823333333333334,
      "grad_norm": 0.20253068208694458,
      "learning_rate": 3.5110416666666665e-05,
      "loss": 0.0019,
      "step": 71470
    },
    {
      "epoch": 2.3826666666666667,
      "grad_norm": 0.37572479248046875,
      "learning_rate": 3.510833333333334e-05,
      "loss": 0.0016,
      "step": 71480
    },
    {
      "epoch": 2.383,
      "grad_norm": 0.26020535826683044,
      "learning_rate": 3.510625e-05,
      "loss": 0.002,
      "step": 71490
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.029155148193240166,
      "learning_rate": 3.510416666666667e-05,
      "loss": 0.0016,
      "step": 71500
    },
    {
      "epoch": 2.3836666666666666,
      "grad_norm": 0.058208826929330826,
      "learning_rate": 3.5102083333333334e-05,
      "loss": 0.0016,
      "step": 71510
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.20251427590847015,
      "learning_rate": 3.51e-05,
      "loss": 0.0024,
      "step": 71520
    },
    {
      "epoch": 2.384333333333333,
      "grad_norm": 0.029219038784503937,
      "learning_rate": 3.509791666666667e-05,
      "loss": 0.0022,
      "step": 71530
    },
    {
      "epoch": 2.3846666666666665,
      "grad_norm": 0.39823463559150696,
      "learning_rate": 3.509583333333333e-05,
      "loss": 0.0027,
      "step": 71540
    },
    {
      "epoch": 2.385,
      "grad_norm": 0.08735845983028412,
      "learning_rate": 3.509375e-05,
      "loss": 0.0025,
      "step": 71550
    },
    {
      "epoch": 2.3853333333333335,
      "grad_norm": 0.08698705583810806,
      "learning_rate": 3.509166666666667e-05,
      "loss": 0.0025,
      "step": 71560
    },
    {
      "epoch": 2.385666666666667,
      "grad_norm": 0.23121577501296997,
      "learning_rate": 3.5089583333333334e-05,
      "loss": 0.0013,
      "step": 71570
    },
    {
      "epoch": 2.386,
      "grad_norm": 0.057957131415605545,
      "learning_rate": 3.50875e-05,
      "loss": 0.0024,
      "step": 71580
    },
    {
      "epoch": 2.3863333333333334,
      "grad_norm": 0.5779977440834045,
      "learning_rate": 3.508541666666667e-05,
      "loss": 0.0036,
      "step": 71590
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.20226432383060455,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.0028,
      "step": 71600
    },
    {
      "epoch": 2.387,
      "grad_norm": 0.4914107620716095,
      "learning_rate": 3.508125e-05,
      "loss": 0.0017,
      "step": 71610
    },
    {
      "epoch": 2.3873333333333333,
      "grad_norm": 0.004874171689152718,
      "learning_rate": 3.507916666666667e-05,
      "loss": 0.0026,
      "step": 71620
    },
    {
      "epoch": 2.3876666666666666,
      "grad_norm": 0.2312389761209488,
      "learning_rate": 3.507708333333333e-05,
      "loss": 0.002,
      "step": 71630
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.14461956918239594,
      "learning_rate": 3.5075000000000006e-05,
      "loss": 0.0016,
      "step": 71640
    },
    {
      "epoch": 2.388333333333333,
      "grad_norm": 0.31813132762908936,
      "learning_rate": 3.5072916666666664e-05,
      "loss": 0.0019,
      "step": 71650
    },
    {
      "epoch": 2.3886666666666665,
      "grad_norm": 0.05845336243510246,
      "learning_rate": 3.5070833333333337e-05,
      "loss": 0.0018,
      "step": 71660
    },
    {
      "epoch": 2.3890000000000002,
      "grad_norm": 0.4047802686691284,
      "learning_rate": 3.506875e-05,
      "loss": 0.0031,
      "step": 71670
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.05825776606798172,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0022,
      "step": 71680
    },
    {
      "epoch": 2.389666666666667,
      "grad_norm": 0.05787714570760727,
      "learning_rate": 3.506458333333333e-05,
      "loss": 0.0028,
      "step": 71690
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.16481170058250427,
      "learning_rate": 3.5062500000000005e-05,
      "loss": 0.0023,
      "step": 71700
    },
    {
      "epoch": 2.3903333333333334,
      "grad_norm": 0.1447451263666153,
      "learning_rate": 3.506041666666667e-05,
      "loss": 0.0021,
      "step": 71710
    },
    {
      "epoch": 2.3906666666666667,
      "grad_norm": 0.14491797983646393,
      "learning_rate": 3.5058333333333336e-05,
      "loss": 0.0024,
      "step": 71720
    },
    {
      "epoch": 2.391,
      "grad_norm": 0.08700613677501678,
      "learning_rate": 3.505625e-05,
      "loss": 0.0029,
      "step": 71730
    },
    {
      "epoch": 2.3913333333333333,
      "grad_norm": 0.05828039348125458,
      "learning_rate": 3.505416666666667e-05,
      "loss": 0.0018,
      "step": 71740
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 0.029187137261033058,
      "learning_rate": 3.505208333333333e-05,
      "loss": 0.002,
      "step": 71750
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.3470114469528198,
      "learning_rate": 3.505e-05,
      "loss": 0.0023,
      "step": 71760
    },
    {
      "epoch": 2.392333333333333,
      "grad_norm": 0.8364740014076233,
      "learning_rate": 3.504791666666667e-05,
      "loss": 0.0039,
      "step": 71770
    },
    {
      "epoch": 2.3926666666666665,
      "grad_norm": 0.3265947997570038,
      "learning_rate": 3.5045833333333336e-05,
      "loss": 0.0032,
      "step": 71780
    },
    {
      "epoch": 2.393,
      "grad_norm": 0.2888518273830414,
      "learning_rate": 3.504375e-05,
      "loss": 0.0021,
      "step": 71790
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.2312462478876114,
      "learning_rate": 3.504166666666667e-05,
      "loss": 0.0029,
      "step": 71800
    },
    {
      "epoch": 2.393666666666667,
      "grad_norm": 0.004613316617906094,
      "learning_rate": 3.503958333333334e-05,
      "loss": 0.0017,
      "step": 71810
    },
    {
      "epoch": 2.394,
      "grad_norm": 0.08697430789470673,
      "learning_rate": 3.50375e-05,
      "loss": 0.0017,
      "step": 71820
    },
    {
      "epoch": 2.3943333333333334,
      "grad_norm": 0.3182017505168915,
      "learning_rate": 3.503541666666667e-05,
      "loss": 0.0027,
      "step": 71830
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.08681479096412659,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.002,
      "step": 71840
    },
    {
      "epoch": 2.395,
      "grad_norm": 0.058262936770915985,
      "learning_rate": 3.503125e-05,
      "loss": 0.0025,
      "step": 71850
    },
    {
      "epoch": 2.3953333333333333,
      "grad_norm": 0.3178350329399109,
      "learning_rate": 3.5029166666666667e-05,
      "loss": 0.0023,
      "step": 71860
    },
    {
      "epoch": 2.3956666666666666,
      "grad_norm": 0.0866004154086113,
      "learning_rate": 3.502708333333333e-05,
      "loss": 0.0027,
      "step": 71870
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.057884301990270615,
      "learning_rate": 3.5025000000000004e-05,
      "loss": 0.0027,
      "step": 71880
    },
    {
      "epoch": 2.396333333333333,
      "grad_norm": 0.058065179735422134,
      "learning_rate": 3.502291666666666e-05,
      "loss": 0.0026,
      "step": 71890
    },
    {
      "epoch": 2.3966666666666665,
      "grad_norm": 0.26042428612709045,
      "learning_rate": 3.5020833333333335e-05,
      "loss": 0.0028,
      "step": 71900
    },
    {
      "epoch": 2.3970000000000002,
      "grad_norm": 0.4334469139575958,
      "learning_rate": 3.501875e-05,
      "loss": 0.0022,
      "step": 71910
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.14462481439113617,
      "learning_rate": 3.501666666666667e-05,
      "loss": 0.0028,
      "step": 71920
    },
    {
      "epoch": 2.397666666666667,
      "grad_norm": 0.3178623616695404,
      "learning_rate": 3.501458333333333e-05,
      "loss": 0.0028,
      "step": 71930
    },
    {
      "epoch": 2.398,
      "grad_norm": 0.17346549034118652,
      "learning_rate": 3.5012500000000004e-05,
      "loss": 0.0029,
      "step": 71940
    },
    {
      "epoch": 2.3983333333333334,
      "grad_norm": 0.202581986784935,
      "learning_rate": 3.501041666666667e-05,
      "loss": 0.0018,
      "step": 71950
    },
    {
      "epoch": 2.3986666666666667,
      "grad_norm": 0.08688609302043915,
      "learning_rate": 3.5008333333333335e-05,
      "loss": 0.003,
      "step": 71960
    },
    {
      "epoch": 2.399,
      "grad_norm": 0.029182586818933487,
      "learning_rate": 3.500625e-05,
      "loss": 0.0025,
      "step": 71970
    },
    {
      "epoch": 2.3993333333333333,
      "grad_norm": 0.13853676617145538,
      "learning_rate": 3.5004166666666666e-05,
      "loss": 0.0019,
      "step": 71980
    },
    {
      "epoch": 2.3996666666666666,
      "grad_norm": 0.3470023572444916,
      "learning_rate": 3.500208333333334e-05,
      "loss": 0.0021,
      "step": 71990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.029128700494766235,
      "learning_rate": 3.5e-05,
      "loss": 0.0016,
      "step": 72000
    },
    {
      "epoch": 2.400333333333333,
      "grad_norm": 0.317897766828537,
      "learning_rate": 3.499791666666667e-05,
      "loss": 0.0027,
      "step": 72010
    },
    {
      "epoch": 2.4006666666666665,
      "grad_norm": 0.08693167567253113,
      "learning_rate": 3.4995833333333335e-05,
      "loss": 0.002,
      "step": 72020
    },
    {
      "epoch": 2.401,
      "grad_norm": 0.20232264697551727,
      "learning_rate": 3.499375e-05,
      "loss": 0.0023,
      "step": 72030
    },
    {
      "epoch": 2.4013333333333335,
      "grad_norm": 0.14441069960594177,
      "learning_rate": 3.4991666666666666e-05,
      "loss": 0.0024,
      "step": 72040
    },
    {
      "epoch": 2.401666666666667,
      "grad_norm": 0.20225192606449127,
      "learning_rate": 3.498958333333334e-05,
      "loss": 0.0027,
      "step": 72050
    },
    {
      "epoch": 2.402,
      "grad_norm": 0.08776597678661346,
      "learning_rate": 3.4987500000000003e-05,
      "loss": 0.0021,
      "step": 72060
    },
    {
      "epoch": 2.4023333333333334,
      "grad_norm": 0.11547151207923889,
      "learning_rate": 3.498541666666667e-05,
      "loss": 0.0028,
      "step": 72070
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.11582878977060318,
      "learning_rate": 3.4983333333333334e-05,
      "loss": 0.0028,
      "step": 72080
    },
    {
      "epoch": 2.403,
      "grad_norm": 0.3473148047924042,
      "learning_rate": 3.498125000000001e-05,
      "loss": 0.0033,
      "step": 72090
    },
    {
      "epoch": 2.4033333333333333,
      "grad_norm": 0.26021286845207214,
      "learning_rate": 3.4979166666666665e-05,
      "loss": 0.0023,
      "step": 72100
    },
    {
      "epoch": 2.4036666666666666,
      "grad_norm": 0.0320608876645565,
      "learning_rate": 3.497708333333333e-05,
      "loss": 0.0026,
      "step": 72110
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.029353540390729904,
      "learning_rate": 3.4975e-05,
      "loss": 0.0026,
      "step": 72120
    },
    {
      "epoch": 2.404333333333333,
      "grad_norm": 0.3179754316806793,
      "learning_rate": 3.497291666666667e-05,
      "loss": 0.0024,
      "step": 72130
    },
    {
      "epoch": 2.4046666666666665,
      "grad_norm": 0.28908517956733704,
      "learning_rate": 3.4970833333333334e-05,
      "loss": 0.0018,
      "step": 72140
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 0.37584295868873596,
      "learning_rate": 3.496875e-05,
      "loss": 0.0026,
      "step": 72150
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.43368083238601685,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0027,
      "step": 72160
    },
    {
      "epoch": 2.405666666666667,
      "grad_norm": 0.23130735754966736,
      "learning_rate": 3.496458333333333e-05,
      "loss": 0.0025,
      "step": 72170
    },
    {
      "epoch": 2.406,
      "grad_norm": 0.20245644450187683,
      "learning_rate": 3.49625e-05,
      "loss": 0.003,
      "step": 72180
    },
    {
      "epoch": 2.4063333333333334,
      "grad_norm": 0.08683865517377853,
      "learning_rate": 3.496041666666667e-05,
      "loss": 0.0036,
      "step": 72190
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.31775134801864624,
      "learning_rate": 3.495833333333334e-05,
      "loss": 0.0026,
      "step": 72200
    },
    {
      "epoch": 2.407,
      "grad_norm": 0.11556132137775421,
      "learning_rate": 3.495625e-05,
      "loss": 0.0025,
      "step": 72210
    },
    {
      "epoch": 2.4073333333333333,
      "grad_norm": 0.20261020958423615,
      "learning_rate": 3.4954166666666665e-05,
      "loss": 0.0025,
      "step": 72220
    },
    {
      "epoch": 2.4076666666666666,
      "grad_norm": 0.057874880731105804,
      "learning_rate": 3.495208333333334e-05,
      "loss": 0.0021,
      "step": 72230
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.11554746329784393,
      "learning_rate": 3.495e-05,
      "loss": 0.0018,
      "step": 72240
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 0.1443255990743637,
      "learning_rate": 3.494791666666667e-05,
      "loss": 0.0029,
      "step": 72250
    },
    {
      "epoch": 2.4086666666666665,
      "grad_norm": 0.4621168375015259,
      "learning_rate": 3.4945833333333333e-05,
      "loss": 0.0022,
      "step": 72260
    },
    {
      "epoch": 2.409,
      "grad_norm": 0.1447763741016388,
      "learning_rate": 3.4943750000000006e-05,
      "loss": 0.0025,
      "step": 72270
    },
    {
      "epoch": 2.4093333333333335,
      "grad_norm": 0.14545468986034393,
      "learning_rate": 3.4941666666666664e-05,
      "loss": 0.0022,
      "step": 72280
    },
    {
      "epoch": 2.409666666666667,
      "grad_norm": 0.31789982318878174,
      "learning_rate": 3.493958333333334e-05,
      "loss": 0.0019,
      "step": 72290
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.4044952392578125,
      "learning_rate": 3.49375e-05,
      "loss": 0.0027,
      "step": 72300
    },
    {
      "epoch": 2.4103333333333334,
      "grad_norm": 0.029272029176354408,
      "learning_rate": 3.493541666666667e-05,
      "loss": 0.0022,
      "step": 72310
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.11578453332185745,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0028,
      "step": 72320
    },
    {
      "epoch": 2.411,
      "grad_norm": 0.23105256259441376,
      "learning_rate": 3.4931250000000005e-05,
      "loss": 0.0022,
      "step": 72330
    },
    {
      "epoch": 2.4113333333333333,
      "grad_norm": 0.2599286139011383,
      "learning_rate": 3.492916666666667e-05,
      "loss": 0.0022,
      "step": 72340
    },
    {
      "epoch": 2.4116666666666666,
      "grad_norm": 0.1444627046585083,
      "learning_rate": 3.492708333333333e-05,
      "loss": 0.0032,
      "step": 72350
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.20217755436897278,
      "learning_rate": 3.4925e-05,
      "loss": 0.0017,
      "step": 72360
    },
    {
      "epoch": 2.412333333333333,
      "grad_norm": 0.029522063210606575,
      "learning_rate": 3.492291666666667e-05,
      "loss": 0.0025,
      "step": 72370
    },
    {
      "epoch": 2.4126666666666665,
      "grad_norm": 0.14522337913513184,
      "learning_rate": 3.492083333333333e-05,
      "loss": 0.0019,
      "step": 72380
    },
    {
      "epoch": 2.413,
      "grad_norm": 0.1736774891614914,
      "learning_rate": 3.491875e-05,
      "loss": 0.0021,
      "step": 72390
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.259976863861084,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.0022,
      "step": 72400
    },
    {
      "epoch": 2.413666666666667,
      "grad_norm": 0.0292385034263134,
      "learning_rate": 3.4914583333333336e-05,
      "loss": 0.0015,
      "step": 72410
    },
    {
      "epoch": 2.414,
      "grad_norm": 0.14433777332305908,
      "learning_rate": 3.49125e-05,
      "loss": 0.0023,
      "step": 72420
    },
    {
      "epoch": 2.4143333333333334,
      "grad_norm": 0.34715116024017334,
      "learning_rate": 3.491041666666667e-05,
      "loss": 0.0025,
      "step": 72430
    },
    {
      "epoch": 2.4146666666666667,
      "grad_norm": 0.11575444042682648,
      "learning_rate": 3.490833333333334e-05,
      "loss": 0.0024,
      "step": 72440
    },
    {
      "epoch": 2.415,
      "grad_norm": 0.14460235834121704,
      "learning_rate": 3.4906250000000005e-05,
      "loss": 0.002,
      "step": 72450
    },
    {
      "epoch": 2.4153333333333333,
      "grad_norm": 0.17347922921180725,
      "learning_rate": 3.4904166666666664e-05,
      "loss": 0.0047,
      "step": 72460
    },
    {
      "epoch": 2.4156666666666666,
      "grad_norm": 0.2316855490207672,
      "learning_rate": 3.4902083333333336e-05,
      "loss": 0.0029,
      "step": 72470
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.028942717239260674,
      "learning_rate": 3.49e-05,
      "loss": 0.0022,
      "step": 72480
    },
    {
      "epoch": 2.416333333333333,
      "grad_norm": 0.05813778564333916,
      "learning_rate": 3.489791666666667e-05,
      "loss": 0.0017,
      "step": 72490
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.31788167357444763,
      "learning_rate": 3.489583333333333e-05,
      "loss": 0.0028,
      "step": 72500
    },
    {
      "epoch": 2.417,
      "grad_norm": 0.202287495136261,
      "learning_rate": 3.4893750000000005e-05,
      "loss": 0.0032,
      "step": 72510
    },
    {
      "epoch": 2.4173333333333336,
      "grad_norm": 0.1451369822025299,
      "learning_rate": 3.489166666666667e-05,
      "loss": 0.0036,
      "step": 72520
    },
    {
      "epoch": 2.417666666666667,
      "grad_norm": 0.116350457072258,
      "learning_rate": 3.4889583333333335e-05,
      "loss": 0.0032,
      "step": 72530
    },
    {
      "epoch": 2.418,
      "grad_norm": 0.02915540710091591,
      "learning_rate": 3.48875e-05,
      "loss": 0.0037,
      "step": 72540
    },
    {
      "epoch": 2.4183333333333334,
      "grad_norm": 0.15359672904014587,
      "learning_rate": 3.488541666666667e-05,
      "loss": 0.0028,
      "step": 72550
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.23108014464378357,
      "learning_rate": 3.488333333333333e-05,
      "loss": 0.002,
      "step": 72560
    },
    {
      "epoch": 2.419,
      "grad_norm": 0.14438815414905548,
      "learning_rate": 3.4881250000000004e-05,
      "loss": 0.0021,
      "step": 72570
    },
    {
      "epoch": 2.4193333333333333,
      "grad_norm": 0.20228056609630585,
      "learning_rate": 3.487916666666667e-05,
      "loss": 0.002,
      "step": 72580
    },
    {
      "epoch": 2.4196666666666666,
      "grad_norm": 0.05796932801604271,
      "learning_rate": 3.4877083333333335e-05,
      "loss": 0.0015,
      "step": 72590
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.3469288647174835,
      "learning_rate": 3.4875e-05,
      "loss": 0.0024,
      "step": 72600
    },
    {
      "epoch": 2.4203333333333332,
      "grad_norm": 0.0052578686736524105,
      "learning_rate": 3.4872916666666666e-05,
      "loss": 0.0022,
      "step": 72610
    },
    {
      "epoch": 2.4206666666666665,
      "grad_norm": 0.007847501896321774,
      "learning_rate": 3.487083333333334e-05,
      "loss": 0.0039,
      "step": 72620
    },
    {
      "epoch": 2.421,
      "grad_norm": 0.3468157649040222,
      "learning_rate": 3.486875e-05,
      "loss": 0.0017,
      "step": 72630
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.2891085743904114,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0027,
      "step": 72640
    },
    {
      "epoch": 2.421666666666667,
      "grad_norm": 0.23098193109035492,
      "learning_rate": 3.4864583333333335e-05,
      "loss": 0.0017,
      "step": 72650
    },
    {
      "epoch": 2.422,
      "grad_norm": 0.28885790705680847,
      "learning_rate": 3.48625e-05,
      "loss": 0.0018,
      "step": 72660
    },
    {
      "epoch": 2.4223333333333334,
      "grad_norm": 0.43346238136291504,
      "learning_rate": 3.4860416666666666e-05,
      "loss": 0.0023,
      "step": 72670
    },
    {
      "epoch": 2.4226666666666667,
      "grad_norm": 0.11590536683797836,
      "learning_rate": 3.485833333333334e-05,
      "loss": 0.0029,
      "step": 72680
    },
    {
      "epoch": 2.423,
      "grad_norm": 0.34706631302833557,
      "learning_rate": 3.4856250000000004e-05,
      "loss": 0.0021,
      "step": 72690
    },
    {
      "epoch": 2.4233333333333333,
      "grad_norm": 0.058232542127370834,
      "learning_rate": 3.485416666666667e-05,
      "loss": 0.0022,
      "step": 72700
    },
    {
      "epoch": 2.4236666666666666,
      "grad_norm": 0.1155204176902771,
      "learning_rate": 3.4852083333333335e-05,
      "loss": 0.003,
      "step": 72710
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.25986766815185547,
      "learning_rate": 3.485e-05,
      "loss": 0.0022,
      "step": 72720
    },
    {
      "epoch": 2.4243333333333332,
      "grad_norm": 0.17328660190105438,
      "learning_rate": 3.484791666666667e-05,
      "loss": 0.0024,
      "step": 72730
    },
    {
      "epoch": 2.4246666666666665,
      "grad_norm": 0.14459232985973358,
      "learning_rate": 3.484583333333333e-05,
      "loss": 0.0024,
      "step": 72740
    },
    {
      "epoch": 2.425,
      "grad_norm": 0.11552885174751282,
      "learning_rate": 3.484375e-05,
      "loss": 0.0022,
      "step": 72750
    },
    {
      "epoch": 2.4253333333333336,
      "grad_norm": 0.03582923114299774,
      "learning_rate": 3.484166666666667e-05,
      "loss": 0.0015,
      "step": 72760
    },
    {
      "epoch": 2.425666666666667,
      "grad_norm": 0.5490384101867676,
      "learning_rate": 3.4839583333333334e-05,
      "loss": 0.0022,
      "step": 72770
    },
    {
      "epoch": 2.426,
      "grad_norm": 0.08704232424497604,
      "learning_rate": 3.48375e-05,
      "loss": 0.0026,
      "step": 72780
    },
    {
      "epoch": 2.4263333333333335,
      "grad_norm": 0.2915465533733368,
      "learning_rate": 3.483541666666667e-05,
      "loss": 0.0035,
      "step": 72790
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.11562503129243851,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0022,
      "step": 72800
    },
    {
      "epoch": 2.427,
      "grad_norm": 0.20225000381469727,
      "learning_rate": 3.483125e-05,
      "loss": 0.0025,
      "step": 72810
    },
    {
      "epoch": 2.4273333333333333,
      "grad_norm": 0.05811581015586853,
      "learning_rate": 3.482916666666667e-05,
      "loss": 0.0015,
      "step": 72820
    },
    {
      "epoch": 2.4276666666666666,
      "grad_norm": 0.010471220128238201,
      "learning_rate": 3.4827083333333334e-05,
      "loss": 0.0023,
      "step": 72830
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.4916505515575409,
      "learning_rate": 3.4825e-05,
      "loss": 0.0022,
      "step": 72840
    },
    {
      "epoch": 2.4283333333333332,
      "grad_norm": 0.08677718043327332,
      "learning_rate": 3.4822916666666665e-05,
      "loss": 0.0017,
      "step": 72850
    },
    {
      "epoch": 2.4286666666666665,
      "grad_norm": 0.029312079772353172,
      "learning_rate": 3.482083333333334e-05,
      "loss": 0.0015,
      "step": 72860
    },
    {
      "epoch": 2.429,
      "grad_norm": 0.34660372138023376,
      "learning_rate": 3.481875e-05,
      "loss": 0.0023,
      "step": 72870
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.144335076212883,
      "learning_rate": 3.481666666666667e-05,
      "loss": 0.002,
      "step": 72880
    },
    {
      "epoch": 2.429666666666667,
      "grad_norm": 0.2890010476112366,
      "learning_rate": 3.4814583333333334e-05,
      "loss": 0.0031,
      "step": 72890
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.25998491048812866,
      "learning_rate": 3.4812500000000006e-05,
      "loss": 0.0014,
      "step": 72900
    },
    {
      "epoch": 2.4303333333333335,
      "grad_norm": 0.2059723436832428,
      "learning_rate": 3.4810416666666665e-05,
      "loss": 0.0027,
      "step": 72910
    },
    {
      "epoch": 2.4306666666666668,
      "grad_norm": 0.029518837109208107,
      "learning_rate": 3.480833333333334e-05,
      "loss": 0.0027,
      "step": 72920
    },
    {
      "epoch": 2.431,
      "grad_norm": 0.17345848679542542,
      "learning_rate": 3.480625e-05,
      "loss": 0.0015,
      "step": 72930
    },
    {
      "epoch": 2.4313333333333333,
      "grad_norm": 0.2311139553785324,
      "learning_rate": 3.480416666666667e-05,
      "loss": 0.0024,
      "step": 72940
    },
    {
      "epoch": 2.4316666666666666,
      "grad_norm": 0.28913116455078125,
      "learning_rate": 3.480208333333333e-05,
      "loss": 0.0026,
      "step": 72950
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.17348183691501617,
      "learning_rate": 3.48e-05,
      "loss": 0.0018,
      "step": 72960
    },
    {
      "epoch": 2.4323333333333332,
      "grad_norm": 0.23145586252212524,
      "learning_rate": 3.479791666666667e-05,
      "loss": 0.0017,
      "step": 72970
    },
    {
      "epoch": 2.4326666666666665,
      "grad_norm": 0.005514902528375387,
      "learning_rate": 3.479583333333333e-05,
      "loss": 0.0023,
      "step": 72980
    },
    {
      "epoch": 2.433,
      "grad_norm": 0.28884175419807434,
      "learning_rate": 3.479375e-05,
      "loss": 0.0029,
      "step": 72990
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.7135334610939026,
      "learning_rate": 3.479166666666667e-05,
      "loss": 0.0026,
      "step": 73000
    },
    {
      "epoch": 2.4336666666666664,
      "grad_norm": 0.23120225965976715,
      "learning_rate": 3.478958333333334e-05,
      "loss": 0.0022,
      "step": 73010
    },
    {
      "epoch": 2.434,
      "grad_norm": 0.4332174062728882,
      "learning_rate": 3.47875e-05,
      "loss": 0.0016,
      "step": 73020
    },
    {
      "epoch": 2.4343333333333335,
      "grad_norm": 0.11538977921009064,
      "learning_rate": 3.478541666666667e-05,
      "loss": 0.0017,
      "step": 73030
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.0036207058001309633,
      "learning_rate": 3.4783333333333336e-05,
      "loss": 0.0026,
      "step": 73040
    },
    {
      "epoch": 2.435,
      "grad_norm": 0.231023907661438,
      "learning_rate": 3.478125e-05,
      "loss": 0.0021,
      "step": 73050
    },
    {
      "epoch": 2.4353333333333333,
      "grad_norm": 0.11553540825843811,
      "learning_rate": 3.477916666666667e-05,
      "loss": 0.0028,
      "step": 73060
    },
    {
      "epoch": 2.4356666666666666,
      "grad_norm": 0.6068076491355896,
      "learning_rate": 3.477708333333333e-05,
      "loss": 0.0029,
      "step": 73070
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.31769371032714844,
      "learning_rate": 3.4775000000000005e-05,
      "loss": 0.0023,
      "step": 73080
    },
    {
      "epoch": 2.4363333333333332,
      "grad_norm": 0.43323615193367004,
      "learning_rate": 3.4772916666666664e-05,
      "loss": 0.0019,
      "step": 73090
    },
    {
      "epoch": 2.4366666666666665,
      "grad_norm": 0.20254606008529663,
      "learning_rate": 3.4770833333333336e-05,
      "loss": 0.002,
      "step": 73100
    },
    {
      "epoch": 2.437,
      "grad_norm": 0.02911831997334957,
      "learning_rate": 3.476875e-05,
      "loss": 0.0026,
      "step": 73110
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.3176993429660797,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0031,
      "step": 73120
    },
    {
      "epoch": 2.437666666666667,
      "grad_norm": 0.5361748337745667,
      "learning_rate": 3.476458333333333e-05,
      "loss": 0.0028,
      "step": 73130
    },
    {
      "epoch": 2.438,
      "grad_norm": 0.28890088200569153,
      "learning_rate": 3.4762500000000005e-05,
      "loss": 0.0025,
      "step": 73140
    },
    {
      "epoch": 2.4383333333333335,
      "grad_norm": 0.0867057666182518,
      "learning_rate": 3.476041666666667e-05,
      "loss": 0.0021,
      "step": 73150
    },
    {
      "epoch": 2.4386666666666668,
      "grad_norm": 0.26010841131210327,
      "learning_rate": 3.4758333333333336e-05,
      "loss": 0.0016,
      "step": 73160
    },
    {
      "epoch": 2.439,
      "grad_norm": 0.31792354583740234,
      "learning_rate": 3.475625e-05,
      "loss": 0.0024,
      "step": 73170
    },
    {
      "epoch": 2.4393333333333334,
      "grad_norm": 0.20252637565135956,
      "learning_rate": 3.4754166666666673e-05,
      "loss": 0.0025,
      "step": 73180
    },
    {
      "epoch": 2.4396666666666667,
      "grad_norm": 0.1737816482782364,
      "learning_rate": 3.475208333333333e-05,
      "loss": 0.0026,
      "step": 73190
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.17367273569107056,
      "learning_rate": 3.475e-05,
      "loss": 0.0033,
      "step": 73200
    },
    {
      "epoch": 2.4403333333333332,
      "grad_norm": 0.14437651634216309,
      "learning_rate": 3.474791666666667e-05,
      "loss": 0.0028,
      "step": 73210
    },
    {
      "epoch": 2.4406666666666665,
      "grad_norm": 0.3175622522830963,
      "learning_rate": 3.4745833333333335e-05,
      "loss": 0.0021,
      "step": 73220
    },
    {
      "epoch": 2.441,
      "grad_norm": 0.029074443504214287,
      "learning_rate": 3.474375e-05,
      "loss": 0.0026,
      "step": 73230
    },
    {
      "epoch": 2.4413333333333336,
      "grad_norm": 0.029214829206466675,
      "learning_rate": 3.4741666666666666e-05,
      "loss": 0.0021,
      "step": 73240
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 0.1732506901025772,
      "learning_rate": 3.473958333333334e-05,
      "loss": 0.0021,
      "step": 73250
    },
    {
      "epoch": 2.442,
      "grad_norm": 0.20237764716148376,
      "learning_rate": 3.47375e-05,
      "loss": 0.0021,
      "step": 73260
    },
    {
      "epoch": 2.4423333333333335,
      "grad_norm": 0.16595034301280975,
      "learning_rate": 3.473541666666667e-05,
      "loss": 0.0026,
      "step": 73270
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.0869402140378952,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.002,
      "step": 73280
    },
    {
      "epoch": 2.443,
      "grad_norm": 0.46227893233299255,
      "learning_rate": 3.473125000000001e-05,
      "loss": 0.0028,
      "step": 73290
    },
    {
      "epoch": 2.4433333333333334,
      "grad_norm": 0.26003149151802063,
      "learning_rate": 3.4729166666666666e-05,
      "loss": 0.002,
      "step": 73300
    },
    {
      "epoch": 2.4436666666666667,
      "grad_norm": 0.4626099169254303,
      "learning_rate": 3.472708333333333e-05,
      "loss": 0.0024,
      "step": 73310
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.34682103991508484,
      "learning_rate": 3.4725000000000004e-05,
      "loss": 0.0016,
      "step": 73320
    },
    {
      "epoch": 2.4443333333333332,
      "grad_norm": 0.05845893919467926,
      "learning_rate": 3.472291666666667e-05,
      "loss": 0.0022,
      "step": 73330
    },
    {
      "epoch": 2.4446666666666665,
      "grad_norm": 0.05836896598339081,
      "learning_rate": 3.4720833333333335e-05,
      "loss": 0.0023,
      "step": 73340
    },
    {
      "epoch": 2.445,
      "grad_norm": 0.4625234603881836,
      "learning_rate": 3.471875e-05,
      "loss": 0.0038,
      "step": 73350
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.14443379640579224,
      "learning_rate": 3.471666666666667e-05,
      "loss": 0.0021,
      "step": 73360
    },
    {
      "epoch": 2.445666666666667,
      "grad_norm": 0.20224007964134216,
      "learning_rate": 3.471458333333333e-05,
      "loss": 0.0026,
      "step": 73370
    },
    {
      "epoch": 2.446,
      "grad_norm": 0.4332580268383026,
      "learning_rate": 3.4712500000000003e-05,
      "loss": 0.0031,
      "step": 73380
    },
    {
      "epoch": 2.4463333333333335,
      "grad_norm": 0.23125402629375458,
      "learning_rate": 3.471041666666667e-05,
      "loss": 0.0025,
      "step": 73390
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.23106107115745544,
      "learning_rate": 3.4708333333333334e-05,
      "loss": 0.0029,
      "step": 73400
    },
    {
      "epoch": 2.447,
      "grad_norm": 0.08669078350067139,
      "learning_rate": 3.470625e-05,
      "loss": 0.0036,
      "step": 73410
    },
    {
      "epoch": 2.4473333333333334,
      "grad_norm": 0.17342281341552734,
      "learning_rate": 3.470416666666667e-05,
      "loss": 0.0025,
      "step": 73420
    },
    {
      "epoch": 2.4476666666666667,
      "grad_norm": 0.17333850264549255,
      "learning_rate": 3.470208333333334e-05,
      "loss": 0.002,
      "step": 73430
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.08683247119188309,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.002,
      "step": 73440
    },
    {
      "epoch": 2.4483333333333333,
      "grad_norm": 0.4043157398700714,
      "learning_rate": 3.469791666666667e-05,
      "loss": 0.0034,
      "step": 73450
    },
    {
      "epoch": 2.4486666666666665,
      "grad_norm": 0.2888338565826416,
      "learning_rate": 3.4695833333333334e-05,
      "loss": 0.0022,
      "step": 73460
    },
    {
      "epoch": 2.449,
      "grad_norm": 0.2026955783367157,
      "learning_rate": 3.469375e-05,
      "loss": 0.0027,
      "step": 73470
    },
    {
      "epoch": 2.449333333333333,
      "grad_norm": 0.11583799123764038,
      "learning_rate": 3.4691666666666665e-05,
      "loss": 0.0017,
      "step": 73480
    },
    {
      "epoch": 2.4496666666666664,
      "grad_norm": 0.08700358867645264,
      "learning_rate": 3.468958333333334e-05,
      "loss": 0.0018,
      "step": 73490
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.02920054830610752,
      "learning_rate": 3.46875e-05,
      "loss": 0.002,
      "step": 73500
    },
    {
      "epoch": 2.4503333333333335,
      "grad_norm": 0.05788775905966759,
      "learning_rate": 3.468541666666667e-05,
      "loss": 0.0016,
      "step": 73510
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.17338860034942627,
      "learning_rate": 3.4683333333333334e-05,
      "loss": 0.0025,
      "step": 73520
    },
    {
      "epoch": 2.451,
      "grad_norm": 0.059040702879428864,
      "learning_rate": 3.4681250000000006e-05,
      "loss": 0.0034,
      "step": 73530
    },
    {
      "epoch": 2.4513333333333334,
      "grad_norm": 0.17329426109790802,
      "learning_rate": 3.4679166666666665e-05,
      "loss": 0.0035,
      "step": 73540
    },
    {
      "epoch": 2.4516666666666667,
      "grad_norm": 0.3951559364795685,
      "learning_rate": 3.467708333333333e-05,
      "loss": 0.0038,
      "step": 73550
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.14443650841712952,
      "learning_rate": 3.4675e-05,
      "loss": 0.0023,
      "step": 73560
    },
    {
      "epoch": 2.4523333333333333,
      "grad_norm": 0.17792963981628418,
      "learning_rate": 3.467291666666667e-05,
      "loss": 0.0018,
      "step": 73570
    },
    {
      "epoch": 2.4526666666666666,
      "grad_norm": 0.31795042753219604,
      "learning_rate": 3.4670833333333334e-05,
      "loss": 0.0025,
      "step": 73580
    },
    {
      "epoch": 2.453,
      "grad_norm": 0.08663910627365112,
      "learning_rate": 3.466875e-05,
      "loss": 0.0033,
      "step": 73590
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.5353537201881409,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0035,
      "step": 73600
    },
    {
      "epoch": 2.453666666666667,
      "grad_norm": 0.19630542397499084,
      "learning_rate": 3.466458333333334e-05,
      "loss": 0.0025,
      "step": 73610
    },
    {
      "epoch": 2.454,
      "grad_norm": 0.0033418142702430487,
      "learning_rate": 3.46625e-05,
      "loss": 0.0019,
      "step": 73620
    },
    {
      "epoch": 2.4543333333333335,
      "grad_norm": 0.029614681378006935,
      "learning_rate": 3.466041666666667e-05,
      "loss": 0.0024,
      "step": 73630
    },
    {
      "epoch": 2.4546666666666668,
      "grad_norm": 0.0865495353937149,
      "learning_rate": 3.465833333333334e-05,
      "loss": 0.0019,
      "step": 73640
    },
    {
      "epoch": 2.455,
      "grad_norm": 0.17324097454547882,
      "learning_rate": 3.465625e-05,
      "loss": 0.0015,
      "step": 73650
    },
    {
      "epoch": 2.4553333333333334,
      "grad_norm": 0.0293527003377676,
      "learning_rate": 3.465416666666667e-05,
      "loss": 0.003,
      "step": 73660
    },
    {
      "epoch": 2.4556666666666667,
      "grad_norm": 0.11547042429447174,
      "learning_rate": 3.4652083333333336e-05,
      "loss": 0.0021,
      "step": 73670
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.25992923974990845,
      "learning_rate": 3.465e-05,
      "loss": 0.0034,
      "step": 73680
    },
    {
      "epoch": 2.4563333333333333,
      "grad_norm": 0.40473973751068115,
      "learning_rate": 3.464791666666667e-05,
      "loss": 0.0038,
      "step": 73690
    },
    {
      "epoch": 2.4566666666666666,
      "grad_norm": 0.23108801245689392,
      "learning_rate": 3.464583333333333e-05,
      "loss": 0.0022,
      "step": 73700
    },
    {
      "epoch": 2.457,
      "grad_norm": 0.7389865517616272,
      "learning_rate": 3.4643750000000005e-05,
      "loss": 0.0031,
      "step": 73710
    },
    {
      "epoch": 2.457333333333333,
      "grad_norm": 0.1734032928943634,
      "learning_rate": 3.4641666666666664e-05,
      "loss": 0.0016,
      "step": 73720
    },
    {
      "epoch": 2.4576666666666664,
      "grad_norm": 0.02916036918759346,
      "learning_rate": 3.4639583333333336e-05,
      "loss": 0.0017,
      "step": 73730
    },
    {
      "epoch": 2.458,
      "grad_norm": 0.17337611317634583,
      "learning_rate": 3.46375e-05,
      "loss": 0.0021,
      "step": 73740
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 0.14445233345031738,
      "learning_rate": 3.463541666666667e-05,
      "loss": 0.003,
      "step": 73750
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.05798463523387909,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0024,
      "step": 73760
    },
    {
      "epoch": 2.459,
      "grad_norm": 0.4137800931930542,
      "learning_rate": 3.4631250000000005e-05,
      "loss": 0.0033,
      "step": 73770
    },
    {
      "epoch": 2.4593333333333334,
      "grad_norm": 0.5606251358985901,
      "learning_rate": 3.462916666666667e-05,
      "loss": 0.003,
      "step": 73780
    },
    {
      "epoch": 2.4596666666666667,
      "grad_norm": 0.288502961397171,
      "learning_rate": 3.4627083333333336e-05,
      "loss": 0.0026,
      "step": 73790
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.1686864197254181,
      "learning_rate": 3.4625e-05,
      "loss": 0.0016,
      "step": 73800
    },
    {
      "epoch": 2.4603333333333333,
      "grad_norm": 0.058035578578710556,
      "learning_rate": 3.462291666666667e-05,
      "loss": 0.0022,
      "step": 73810
    },
    {
      "epoch": 2.4606666666666666,
      "grad_norm": 0.11600477248430252,
      "learning_rate": 3.462083333333333e-05,
      "loss": 0.0026,
      "step": 73820
    },
    {
      "epoch": 2.461,
      "grad_norm": 0.490676611661911,
      "learning_rate": 3.461875e-05,
      "loss": 0.0035,
      "step": 73830
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.0866507738828659,
      "learning_rate": 3.461666666666667e-05,
      "loss": 0.0017,
      "step": 73840
    },
    {
      "epoch": 2.461666666666667,
      "grad_norm": 0.029390519484877586,
      "learning_rate": 3.4614583333333336e-05,
      "loss": 0.0023,
      "step": 73850
    },
    {
      "epoch": 2.462,
      "grad_norm": 0.11568044126033783,
      "learning_rate": 3.46125e-05,
      "loss": 0.002,
      "step": 73860
    },
    {
      "epoch": 2.4623333333333335,
      "grad_norm": 0.09555082023143768,
      "learning_rate": 3.4610416666666667e-05,
      "loss": 0.0027,
      "step": 73870
    },
    {
      "epoch": 2.462666666666667,
      "grad_norm": 0.5489368438720703,
      "learning_rate": 3.460833333333334e-05,
      "loss": 0.0024,
      "step": 73880
    },
    {
      "epoch": 2.463,
      "grad_norm": 0.15528376400470734,
      "learning_rate": 3.4606250000000004e-05,
      "loss": 0.0024,
      "step": 73890
    },
    {
      "epoch": 2.4633333333333334,
      "grad_norm": 0.005677195731550455,
      "learning_rate": 3.460416666666667e-05,
      "loss": 0.0025,
      "step": 73900
    },
    {
      "epoch": 2.4636666666666667,
      "grad_norm": 0.37522757053375244,
      "learning_rate": 3.4602083333333335e-05,
      "loss": 0.0023,
      "step": 73910
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.14509667456150055,
      "learning_rate": 3.46e-05,
      "loss": 0.0025,
      "step": 73920
    },
    {
      "epoch": 2.4643333333333333,
      "grad_norm": 0.05802283436059952,
      "learning_rate": 3.4597916666666666e-05,
      "loss": 0.0019,
      "step": 73930
    },
    {
      "epoch": 2.4646666666666666,
      "grad_norm": 0.17363134026527405,
      "learning_rate": 3.459583333333333e-05,
      "loss": 0.0021,
      "step": 73940
    },
    {
      "epoch": 2.465,
      "grad_norm": 0.37535732984542847,
      "learning_rate": 3.4593750000000004e-05,
      "loss": 0.0023,
      "step": 73950
    },
    {
      "epoch": 2.465333333333333,
      "grad_norm": 0.2034628540277481,
      "learning_rate": 3.459166666666667e-05,
      "loss": 0.0024,
      "step": 73960
    },
    {
      "epoch": 2.4656666666666665,
      "grad_norm": 0.23132814466953278,
      "learning_rate": 3.4589583333333335e-05,
      "loss": 0.0025,
      "step": 73970
    },
    {
      "epoch": 2.466,
      "grad_norm": 0.1732245683670044,
      "learning_rate": 3.45875e-05,
      "loss": 0.0023,
      "step": 73980
    },
    {
      "epoch": 2.4663333333333335,
      "grad_norm": 0.3470174968242645,
      "learning_rate": 3.458541666666667e-05,
      "loss": 0.0037,
      "step": 73990
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.2230626344680786,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.0021,
      "step": 74000
    },
    {
      "epoch": 2.467,
      "grad_norm": 0.37563034892082214,
      "learning_rate": 3.4581250000000004e-05,
      "loss": 0.0018,
      "step": 74010
    },
    {
      "epoch": 2.4673333333333334,
      "grad_norm": 0.4335319399833679,
      "learning_rate": 3.457916666666667e-05,
      "loss": 0.0021,
      "step": 74020
    },
    {
      "epoch": 2.4676666666666667,
      "grad_norm": 0.17368188500404358,
      "learning_rate": 3.4577083333333335e-05,
      "loss": 0.0016,
      "step": 74030
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.17327748239040375,
      "learning_rate": 3.4575e-05,
      "loss": 0.0023,
      "step": 74040
    },
    {
      "epoch": 2.4683333333333333,
      "grad_norm": 0.17318564653396606,
      "learning_rate": 3.4572916666666666e-05,
      "loss": 0.0032,
      "step": 74050
    },
    {
      "epoch": 2.4686666666666666,
      "grad_norm": 0.23108337819576263,
      "learning_rate": 3.457083333333334e-05,
      "loss": 0.0019,
      "step": 74060
    },
    {
      "epoch": 2.469,
      "grad_norm": 0.028956253081560135,
      "learning_rate": 3.456875e-05,
      "loss": 0.0024,
      "step": 74070
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.7695918083190918,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0025,
      "step": 74080
    },
    {
      "epoch": 2.469666666666667,
      "grad_norm": 0.03091084398329258,
      "learning_rate": 3.4564583333333334e-05,
      "loss": 0.0024,
      "step": 74090
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.004494826775044203,
      "learning_rate": 3.45625e-05,
      "loss": 0.0021,
      "step": 74100
    },
    {
      "epoch": 2.4703333333333335,
      "grad_norm": 0.05844704806804657,
      "learning_rate": 3.4560416666666665e-05,
      "loss": 0.0019,
      "step": 74110
    },
    {
      "epoch": 2.470666666666667,
      "grad_norm": 0.28896719217300415,
      "learning_rate": 3.455833333333334e-05,
      "loss": 0.0024,
      "step": 74120
    },
    {
      "epoch": 2.471,
      "grad_norm": 0.11550434678792953,
      "learning_rate": 3.455625e-05,
      "loss": 0.0018,
      "step": 74130
    },
    {
      "epoch": 2.4713333333333334,
      "grad_norm": 0.1731884777545929,
      "learning_rate": 3.455416666666667e-05,
      "loss": 0.0028,
      "step": 74140
    },
    {
      "epoch": 2.4716666666666667,
      "grad_norm": 0.37561270594596863,
      "learning_rate": 3.4552083333333334e-05,
      "loss": 0.0034,
      "step": 74150
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.17326223850250244,
      "learning_rate": 3.455e-05,
      "loss": 0.0017,
      "step": 74160
    },
    {
      "epoch": 2.4723333333333333,
      "grad_norm": 0.23102474212646484,
      "learning_rate": 3.454791666666667e-05,
      "loss": 0.0032,
      "step": 74170
    },
    {
      "epoch": 2.4726666666666666,
      "grad_norm": 0.259790301322937,
      "learning_rate": 3.454583333333333e-05,
      "loss": 0.0021,
      "step": 74180
    },
    {
      "epoch": 2.473,
      "grad_norm": 0.6078611016273499,
      "learning_rate": 3.454375e-05,
      "loss": 0.0026,
      "step": 74190
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.20882394909858704,
      "learning_rate": 3.454166666666667e-05,
      "loss": 0.0021,
      "step": 74200
    },
    {
      "epoch": 2.4736666666666665,
      "grad_norm": 0.3180846869945526,
      "learning_rate": 3.4539583333333334e-05,
      "loss": 0.0021,
      "step": 74210
    },
    {
      "epoch": 2.474,
      "grad_norm": 0.40467068552970886,
      "learning_rate": 3.45375e-05,
      "loss": 0.0021,
      "step": 74220
    },
    {
      "epoch": 2.4743333333333335,
      "grad_norm": 0.38089197874069214,
      "learning_rate": 3.453541666666667e-05,
      "loss": 0.0035,
      "step": 74230
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.17713899910449982,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0032,
      "step": 74240
    },
    {
      "epoch": 2.475,
      "grad_norm": 0.14468048512935638,
      "learning_rate": 3.453125e-05,
      "loss": 0.0019,
      "step": 74250
    },
    {
      "epoch": 2.4753333333333334,
      "grad_norm": 0.11593446880578995,
      "learning_rate": 3.452916666666667e-05,
      "loss": 0.002,
      "step": 74260
    },
    {
      "epoch": 2.4756666666666667,
      "grad_norm": 0.029436742886900902,
      "learning_rate": 3.452708333333334e-05,
      "loss": 0.0022,
      "step": 74270
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.8156716823577881,
      "learning_rate": 3.4525e-05,
      "loss": 0.0023,
      "step": 74280
    },
    {
      "epoch": 2.4763333333333333,
      "grad_norm": 0.05790895223617554,
      "learning_rate": 3.4522916666666664e-05,
      "loss": 0.0024,
      "step": 74290
    },
    {
      "epoch": 2.4766666666666666,
      "grad_norm": 0.23088866472244263,
      "learning_rate": 3.452083333333334e-05,
      "loss": 0.0018,
      "step": 74300
    },
    {
      "epoch": 2.477,
      "grad_norm": 0.346764475107193,
      "learning_rate": 3.451875e-05,
      "loss": 0.0017,
      "step": 74310
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.49066779017448425,
      "learning_rate": 3.451666666666667e-05,
      "loss": 0.0023,
      "step": 74320
    },
    {
      "epoch": 2.477666666666667,
      "grad_norm": 0.005837703589349985,
      "learning_rate": 3.451458333333333e-05,
      "loss": 0.0017,
      "step": 74330
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 0.02912997081875801,
      "learning_rate": 3.4512500000000005e-05,
      "loss": 0.0024,
      "step": 74340
    },
    {
      "epoch": 2.4783333333333335,
      "grad_norm": 0.14458976686000824,
      "learning_rate": 3.4510416666666664e-05,
      "loss": 0.0013,
      "step": 74350
    },
    {
      "epoch": 2.478666666666667,
      "grad_norm": 0.14468373358249664,
      "learning_rate": 3.4508333333333336e-05,
      "loss": 0.0027,
      "step": 74360
    },
    {
      "epoch": 2.479,
      "grad_norm": 0.2324093133211136,
      "learning_rate": 3.450625e-05,
      "loss": 0.0017,
      "step": 74370
    },
    {
      "epoch": 2.4793333333333334,
      "grad_norm": 0.1446831077337265,
      "learning_rate": 3.4504166666666674e-05,
      "loss": 0.0026,
      "step": 74380
    },
    {
      "epoch": 2.4796666666666667,
      "grad_norm": 0.08655700087547302,
      "learning_rate": 3.450208333333333e-05,
      "loss": 0.0032,
      "step": 74390
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.08676622807979584,
      "learning_rate": 3.45e-05,
      "loss": 0.0024,
      "step": 74400
    },
    {
      "epoch": 2.4803333333333333,
      "grad_norm": 0.4621568024158478,
      "learning_rate": 3.449791666666667e-05,
      "loss": 0.0021,
      "step": 74410
    },
    {
      "epoch": 2.4806666666666666,
      "grad_norm": 0.31786155700683594,
      "learning_rate": 3.4495833333333336e-05,
      "loss": 0.0026,
      "step": 74420
    },
    {
      "epoch": 2.481,
      "grad_norm": 0.1732451468706131,
      "learning_rate": 3.449375e-05,
      "loss": 0.0018,
      "step": 74430
    },
    {
      "epoch": 2.481333333333333,
      "grad_norm": 0.004962701350450516,
      "learning_rate": 3.449166666666667e-05,
      "loss": 0.002,
      "step": 74440
    },
    {
      "epoch": 2.4816666666666665,
      "grad_norm": 0.20225411653518677,
      "learning_rate": 3.448958333333334e-05,
      "loss": 0.0026,
      "step": 74450
    },
    {
      "epoch": 2.482,
      "grad_norm": 0.2310263216495514,
      "learning_rate": 3.44875e-05,
      "loss": 0.0023,
      "step": 74460
    },
    {
      "epoch": 2.4823333333333335,
      "grad_norm": 0.26001888513565063,
      "learning_rate": 3.448541666666667e-05,
      "loss": 0.0027,
      "step": 74470
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.086787648499012,
      "learning_rate": 3.4483333333333336e-05,
      "loss": 0.0021,
      "step": 74480
    },
    {
      "epoch": 2.483,
      "grad_norm": 0.20213772356510162,
      "learning_rate": 3.448125e-05,
      "loss": 0.0028,
      "step": 74490
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.3753431737422943,
      "learning_rate": 3.447916666666667e-05,
      "loss": 0.0025,
      "step": 74500
    },
    {
      "epoch": 2.4836666666666667,
      "grad_norm": 0.08679281920194626,
      "learning_rate": 3.447708333333334e-05,
      "loss": 0.0025,
      "step": 74510
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.17319004237651825,
      "learning_rate": 3.4475000000000005e-05,
      "loss": 0.0022,
      "step": 74520
    },
    {
      "epoch": 2.4843333333333333,
      "grad_norm": 0.5195953845977783,
      "learning_rate": 3.447291666666666e-05,
      "loss": 0.0026,
      "step": 74530
    },
    {
      "epoch": 2.4846666666666666,
      "grad_norm": 0.08697669953107834,
      "learning_rate": 3.4470833333333335e-05,
      "loss": 0.0021,
      "step": 74540
    },
    {
      "epoch": 2.485,
      "grad_norm": 0.6654115319252014,
      "learning_rate": 3.446875e-05,
      "loss": 0.0027,
      "step": 74550
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.23079867660999298,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0028,
      "step": 74560
    },
    {
      "epoch": 2.485666666666667,
      "grad_norm": 0.23087064921855927,
      "learning_rate": 3.446458333333333e-05,
      "loss": 0.0025,
      "step": 74570
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 0.5937996506690979,
      "learning_rate": 3.4462500000000004e-05,
      "loss": 0.0019,
      "step": 74580
    },
    {
      "epoch": 2.4863333333333335,
      "grad_norm": 0.3464483916759491,
      "learning_rate": 3.446041666666667e-05,
      "loss": 0.002,
      "step": 74590
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.1337909996509552,
      "learning_rate": 3.4458333333333335e-05,
      "loss": 0.0024,
      "step": 74600
    },
    {
      "epoch": 2.487,
      "grad_norm": 0.05784463882446289,
      "learning_rate": 3.445625e-05,
      "loss": 0.0024,
      "step": 74610
    },
    {
      "epoch": 2.4873333333333334,
      "grad_norm": 0.06221400201320648,
      "learning_rate": 3.445416666666667e-05,
      "loss": 0.0038,
      "step": 74620
    },
    {
      "epoch": 2.4876666666666667,
      "grad_norm": 0.005397791042923927,
      "learning_rate": 3.445208333333333e-05,
      "loss": 0.0024,
      "step": 74630
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.3464016616344452,
      "learning_rate": 3.445e-05,
      "loss": 0.0015,
      "step": 74640
    },
    {
      "epoch": 2.4883333333333333,
      "grad_norm": 0.17339494824409485,
      "learning_rate": 3.444791666666667e-05,
      "loss": 0.0028,
      "step": 74650
    },
    {
      "epoch": 2.4886666666666666,
      "grad_norm": 0.005264644511044025,
      "learning_rate": 3.4445833333333335e-05,
      "loss": 0.0027,
      "step": 74660
    },
    {
      "epoch": 2.489,
      "grad_norm": 0.37548747658729553,
      "learning_rate": 3.444375e-05,
      "loss": 0.0026,
      "step": 74670
    },
    {
      "epoch": 2.489333333333333,
      "grad_norm": 0.03150857985019684,
      "learning_rate": 3.4441666666666666e-05,
      "loss": 0.0026,
      "step": 74680
    },
    {
      "epoch": 2.4896666666666665,
      "grad_norm": 0.05789809674024582,
      "learning_rate": 3.443958333333334e-05,
      "loss": 0.0031,
      "step": 74690
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.057966116815805435,
      "learning_rate": 3.4437500000000004e-05,
      "loss": 0.0029,
      "step": 74700
    },
    {
      "epoch": 2.4903333333333335,
      "grad_norm": 0.0056157344952225685,
      "learning_rate": 3.443541666666667e-05,
      "loss": 0.0019,
      "step": 74710
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.0023530691396445036,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0018,
      "step": 74720
    },
    {
      "epoch": 2.491,
      "grad_norm": 0.20234812796115875,
      "learning_rate": 3.443125000000001e-05,
      "loss": 0.0024,
      "step": 74730
    },
    {
      "epoch": 2.4913333333333334,
      "grad_norm": 0.31735020875930786,
      "learning_rate": 3.4429166666666666e-05,
      "loss": 0.0027,
      "step": 74740
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 0.2026176154613495,
      "learning_rate": 3.442708333333334e-05,
      "loss": 0.0037,
      "step": 74750
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.0297895148396492,
      "learning_rate": 3.4425e-05,
      "loss": 0.0026,
      "step": 74760
    },
    {
      "epoch": 2.4923333333333333,
      "grad_norm": 0.02893710508942604,
      "learning_rate": 3.442291666666667e-05,
      "loss": 0.0022,
      "step": 74770
    },
    {
      "epoch": 2.4926666666666666,
      "grad_norm": 0.00418626144528389,
      "learning_rate": 3.4420833333333334e-05,
      "loss": 0.0032,
      "step": 74780
    },
    {
      "epoch": 2.493,
      "grad_norm": 0.4894455373287201,
      "learning_rate": 3.441875e-05,
      "loss": 0.0036,
      "step": 74790
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.0579945333302021,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.0023,
      "step": 74800
    },
    {
      "epoch": 2.4936666666666665,
      "grad_norm": 0.20220962166786194,
      "learning_rate": 3.441458333333333e-05,
      "loss": 0.0018,
      "step": 74810
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 0.3678293228149414,
      "learning_rate": 3.44125e-05,
      "loss": 0.0021,
      "step": 74820
    },
    {
      "epoch": 2.4943333333333335,
      "grad_norm": 0.3179808259010315,
      "learning_rate": 3.441041666666667e-05,
      "loss": 0.0024,
      "step": 74830
    },
    {
      "epoch": 2.494666666666667,
      "grad_norm": 0.1444571316242218,
      "learning_rate": 3.4408333333333334e-05,
      "loss": 0.002,
      "step": 74840
    },
    {
      "epoch": 2.495,
      "grad_norm": 0.05793313309550285,
      "learning_rate": 3.440625e-05,
      "loss": 0.0015,
      "step": 74850
    },
    {
      "epoch": 2.4953333333333334,
      "grad_norm": 0.11544456332921982,
      "learning_rate": 3.440416666666667e-05,
      "loss": 0.0031,
      "step": 74860
    },
    {
      "epoch": 2.4956666666666667,
      "grad_norm": 0.11547243595123291,
      "learning_rate": 3.440208333333334e-05,
      "loss": 0.0025,
      "step": 74870
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.08764924854040146,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0026,
      "step": 74880
    },
    {
      "epoch": 2.4963333333333333,
      "grad_norm": 0.2023095339536667,
      "learning_rate": 3.439791666666667e-05,
      "loss": 0.0017,
      "step": 74890
    },
    {
      "epoch": 2.4966666666666666,
      "grad_norm": 0.14437714219093323,
      "learning_rate": 3.4395833333333334e-05,
      "loss": 0.0019,
      "step": 74900
    },
    {
      "epoch": 2.497,
      "grad_norm": 0.05836719274520874,
      "learning_rate": 3.439375e-05,
      "loss": 0.0019,
      "step": 74910
    },
    {
      "epoch": 2.497333333333333,
      "grad_norm": 0.11558454483747482,
      "learning_rate": 3.4391666666666665e-05,
      "loss": 0.0025,
      "step": 74920
    },
    {
      "epoch": 2.4976666666666665,
      "grad_norm": 0.20227602124214172,
      "learning_rate": 3.438958333333334e-05,
      "loss": 0.0028,
      "step": 74930
    },
    {
      "epoch": 2.498,
      "grad_norm": 0.08669476211071014,
      "learning_rate": 3.43875e-05,
      "loss": 0.0032,
      "step": 74940
    },
    {
      "epoch": 2.4983333333333335,
      "grad_norm": 0.231043741106987,
      "learning_rate": 3.438541666666667e-05,
      "loss": 0.0028,
      "step": 74950
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.11598055809736252,
      "learning_rate": 3.438333333333333e-05,
      "loss": 0.0019,
      "step": 74960
    },
    {
      "epoch": 2.499,
      "grad_norm": 0.2598423659801483,
      "learning_rate": 3.4381250000000006e-05,
      "loss": 0.0019,
      "step": 74970
    },
    {
      "epoch": 2.4993333333333334,
      "grad_norm": 0.2886602580547333,
      "learning_rate": 3.437916666666667e-05,
      "loss": 0.0025,
      "step": 74980
    },
    {
      "epoch": 2.4996666666666667,
      "grad_norm": 0.46186378598213196,
      "learning_rate": 3.4377083333333337e-05,
      "loss": 0.0019,
      "step": 74990
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.20199261605739594,
      "learning_rate": 3.4375e-05,
      "loss": 0.0026,
      "step": 75000
    },
    {
      "epoch": 2.5003333333333333,
      "grad_norm": 0.23116737604141235,
      "learning_rate": 3.437291666666667e-05,
      "loss": 0.0023,
      "step": 75010
    },
    {
      "epoch": 2.5006666666666666,
      "grad_norm": 0.3754497170448303,
      "learning_rate": 3.437083333333333e-05,
      "loss": 0.0017,
      "step": 75020
    },
    {
      "epoch": 2.501,
      "grad_norm": 0.3465683162212372,
      "learning_rate": 3.436875e-05,
      "loss": 0.0026,
      "step": 75030
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.05775291472673416,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.0027,
      "step": 75040
    },
    {
      "epoch": 2.501666666666667,
      "grad_norm": 0.2309023141860962,
      "learning_rate": 3.4364583333333336e-05,
      "loss": 0.0017,
      "step": 75050
    },
    {
      "epoch": 2.502,
      "grad_norm": 0.05833125486969948,
      "learning_rate": 3.43625e-05,
      "loss": 0.0022,
      "step": 75060
    },
    {
      "epoch": 2.5023333333333335,
      "grad_norm": 0.11561834067106247,
      "learning_rate": 3.436041666666667e-05,
      "loss": 0.002,
      "step": 75070
    },
    {
      "epoch": 2.502666666666667,
      "grad_norm": 0.14446407556533813,
      "learning_rate": 3.435833333333334e-05,
      "loss": 0.0027,
      "step": 75080
    },
    {
      "epoch": 2.503,
      "grad_norm": 0.2028840035200119,
      "learning_rate": 3.435625e-05,
      "loss": 0.0027,
      "step": 75090
    },
    {
      "epoch": 2.5033333333333334,
      "grad_norm": 0.11569102853536606,
      "learning_rate": 3.435416666666667e-05,
      "loss": 0.0015,
      "step": 75100
    },
    {
      "epoch": 2.5036666666666667,
      "grad_norm": 0.1445005238056183,
      "learning_rate": 3.4352083333333336e-05,
      "loss": 0.0022,
      "step": 75110
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.08662675321102142,
      "learning_rate": 3.435e-05,
      "loss": 0.003,
      "step": 75120
    },
    {
      "epoch": 2.5043333333333333,
      "grad_norm": 0.4038936495780945,
      "learning_rate": 3.434791666666667e-05,
      "loss": 0.0036,
      "step": 75130
    },
    {
      "epoch": 2.5046666666666666,
      "grad_norm": 0.17333029210567474,
      "learning_rate": 3.434583333333333e-05,
      "loss": 0.0018,
      "step": 75140
    },
    {
      "epoch": 2.505,
      "grad_norm": 0.05772055685520172,
      "learning_rate": 3.4343750000000005e-05,
      "loss": 0.003,
      "step": 75150
    },
    {
      "epoch": 2.505333333333333,
      "grad_norm": 0.0074792467057704926,
      "learning_rate": 3.4341666666666663e-05,
      "loss": 0.0023,
      "step": 75160
    },
    {
      "epoch": 2.5056666666666665,
      "grad_norm": 0.26005256175994873,
      "learning_rate": 3.4339583333333336e-05,
      "loss": 0.0036,
      "step": 75170
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 0.1444176733493805,
      "learning_rate": 3.43375e-05,
      "loss": 0.0016,
      "step": 75180
    },
    {
      "epoch": 2.506333333333333,
      "grad_norm": 0.02898937463760376,
      "learning_rate": 3.433541666666667e-05,
      "loss": 0.003,
      "step": 75190
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.23091545701026917,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0021,
      "step": 75200
    },
    {
      "epoch": 2.507,
      "grad_norm": 0.02919720485806465,
      "learning_rate": 3.4331250000000004e-05,
      "loss": 0.0027,
      "step": 75210
    },
    {
      "epoch": 2.5073333333333334,
      "grad_norm": 0.08664049208164215,
      "learning_rate": 3.432916666666667e-05,
      "loss": 0.0027,
      "step": 75220
    },
    {
      "epoch": 2.5076666666666667,
      "grad_norm": 0.11569919437170029,
      "learning_rate": 3.4327083333333335e-05,
      "loss": 0.0011,
      "step": 75230
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.05810641497373581,
      "learning_rate": 3.4325e-05,
      "loss": 0.0021,
      "step": 75240
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 0.2017984390258789,
      "learning_rate": 3.4322916666666666e-05,
      "loss": 0.0023,
      "step": 75250
    },
    {
      "epoch": 2.5086666666666666,
      "grad_norm": 0.38566726446151733,
      "learning_rate": 3.432083333333334e-05,
      "loss": 0.0029,
      "step": 75260
    },
    {
      "epoch": 2.509,
      "grad_norm": 0.6635369658470154,
      "learning_rate": 3.431875e-05,
      "loss": 0.0029,
      "step": 75270
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.2601657509803772,
      "learning_rate": 3.431666666666667e-05,
      "loss": 0.0018,
      "step": 75280
    },
    {
      "epoch": 2.509666666666667,
      "grad_norm": 0.029510604217648506,
      "learning_rate": 3.4314583333333335e-05,
      "loss": 0.0023,
      "step": 75290
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.3177749216556549,
      "learning_rate": 3.43125e-05,
      "loss": 0.0024,
      "step": 75300
    },
    {
      "epoch": 2.5103333333333335,
      "grad_norm": 0.25999170541763306,
      "learning_rate": 3.4310416666666666e-05,
      "loss": 0.0025,
      "step": 75310
    },
    {
      "epoch": 2.510666666666667,
      "grad_norm": 0.25981810688972473,
      "learning_rate": 3.430833333333334e-05,
      "loss": 0.0031,
      "step": 75320
    },
    {
      "epoch": 2.511,
      "grad_norm": 0.029971247538924217,
      "learning_rate": 3.4306250000000004e-05,
      "loss": 0.0026,
      "step": 75330
    },
    {
      "epoch": 2.5113333333333334,
      "grad_norm": 0.14470691978931427,
      "learning_rate": 3.430416666666667e-05,
      "loss": 0.0016,
      "step": 75340
    },
    {
      "epoch": 2.5116666666666667,
      "grad_norm": 0.2598007917404175,
      "learning_rate": 3.4302083333333335e-05,
      "loss": 0.002,
      "step": 75350
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.3475407361984253,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0027,
      "step": 75360
    },
    {
      "epoch": 2.5123333333333333,
      "grad_norm": 0.2021121084690094,
      "learning_rate": 3.4297916666666666e-05,
      "loss": 0.0022,
      "step": 75370
    },
    {
      "epoch": 2.5126666666666666,
      "grad_norm": 0.5775022506713867,
      "learning_rate": 3.429583333333333e-05,
      "loss": 0.0033,
      "step": 75380
    },
    {
      "epoch": 2.513,
      "grad_norm": 0.4538748264312744,
      "learning_rate": 3.4293750000000003e-05,
      "loss": 0.0025,
      "step": 75390
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.0042125931940972805,
      "learning_rate": 3.429166666666667e-05,
      "loss": 0.0018,
      "step": 75400
    },
    {
      "epoch": 2.5136666666666665,
      "grad_norm": 0.264418363571167,
      "learning_rate": 3.4289583333333334e-05,
      "loss": 0.0026,
      "step": 75410
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 0.40597251057624817,
      "learning_rate": 3.42875e-05,
      "loss": 0.0021,
      "step": 75420
    },
    {
      "epoch": 2.514333333333333,
      "grad_norm": 0.40324220061302185,
      "learning_rate": 3.428541666666667e-05,
      "loss": 0.0027,
      "step": 75430
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.34657254815101624,
      "learning_rate": 3.428333333333333e-05,
      "loss": 0.002,
      "step": 75440
    },
    {
      "epoch": 2.515,
      "grad_norm": 0.5196232199668884,
      "learning_rate": 3.428125e-05,
      "loss": 0.0025,
      "step": 75450
    },
    {
      "epoch": 2.5153333333333334,
      "grad_norm": 0.2020898163318634,
      "learning_rate": 3.427916666666667e-05,
      "loss": 0.0019,
      "step": 75460
    },
    {
      "epoch": 2.5156666666666667,
      "grad_norm": 0.057757411152124405,
      "learning_rate": 3.4277083333333334e-05,
      "loss": 0.0018,
      "step": 75470
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.23085623979568481,
      "learning_rate": 3.4275e-05,
      "loss": 0.0021,
      "step": 75480
    },
    {
      "epoch": 2.5163333333333333,
      "grad_norm": 0.08662077784538269,
      "learning_rate": 3.4272916666666665e-05,
      "loss": 0.0019,
      "step": 75490
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 0.0295170359313488,
      "learning_rate": 3.427083333333334e-05,
      "loss": 0.0025,
      "step": 75500
    },
    {
      "epoch": 2.517,
      "grad_norm": 0.11567172408103943,
      "learning_rate": 3.4268749999999996e-05,
      "loss": 0.0027,
      "step": 75510
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.20303836464881897,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.003,
      "step": 75520
    },
    {
      "epoch": 2.517666666666667,
      "grad_norm": 0.14454461634159088,
      "learning_rate": 3.4264583333333334e-05,
      "loss": 0.0031,
      "step": 75530
    },
    {
      "epoch": 2.518,
      "grad_norm": 0.11544274538755417,
      "learning_rate": 3.4262500000000006e-05,
      "loss": 0.0034,
      "step": 75540
    },
    {
      "epoch": 2.5183333333333335,
      "grad_norm": 0.05788061395287514,
      "learning_rate": 3.4260416666666665e-05,
      "loss": 0.002,
      "step": 75550
    },
    {
      "epoch": 2.518666666666667,
      "grad_norm": 0.25991326570510864,
      "learning_rate": 3.425833333333334e-05,
      "loss": 0.0028,
      "step": 75560
    },
    {
      "epoch": 2.519,
      "grad_norm": 0.08717912435531616,
      "learning_rate": 3.425625e-05,
      "loss": 0.003,
      "step": 75570
    },
    {
      "epoch": 2.5193333333333334,
      "grad_norm": 0.3501427173614502,
      "learning_rate": 3.425416666666667e-05,
      "loss": 0.0024,
      "step": 75580
    },
    {
      "epoch": 2.5196666666666667,
      "grad_norm": 0.2021712213754654,
      "learning_rate": 3.4252083333333334e-05,
      "loss": 0.0027,
      "step": 75590
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.029564855620265007,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.0028,
      "step": 75600
    },
    {
      "epoch": 2.5203333333333333,
      "grad_norm": 0.08743300288915634,
      "learning_rate": 3.424791666666667e-05,
      "loss": 0.0024,
      "step": 75610
    },
    {
      "epoch": 2.5206666666666666,
      "grad_norm": 0.8565155863761902,
      "learning_rate": 3.424583333333333e-05,
      "loss": 0.0022,
      "step": 75620
    },
    {
      "epoch": 2.521,
      "grad_norm": 0.11550413072109222,
      "learning_rate": 3.424375e-05,
      "loss": 0.002,
      "step": 75630
    },
    {
      "epoch": 2.521333333333333,
      "grad_norm": 0.8403354287147522,
      "learning_rate": 3.424166666666667e-05,
      "loss": 0.0026,
      "step": 75640
    },
    {
      "epoch": 2.5216666666666665,
      "grad_norm": 0.37549370527267456,
      "learning_rate": 3.423958333333333e-05,
      "loss": 0.0026,
      "step": 75650
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 0.14448051154613495,
      "learning_rate": 3.42375e-05,
      "loss": 0.0016,
      "step": 75660
    },
    {
      "epoch": 2.522333333333333,
      "grad_norm": 0.14424429833889008,
      "learning_rate": 3.423541666666667e-05,
      "loss": 0.0031,
      "step": 75670
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.004118026699870825,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.002,
      "step": 75680
    },
    {
      "epoch": 2.523,
      "grad_norm": 0.20195834338665009,
      "learning_rate": 3.423125e-05,
      "loss": 0.0027,
      "step": 75690
    },
    {
      "epoch": 2.5233333333333334,
      "grad_norm": 0.02929568849503994,
      "learning_rate": 3.422916666666667e-05,
      "loss": 0.0033,
      "step": 75700
    },
    {
      "epoch": 2.5236666666666667,
      "grad_norm": 0.2888994514942169,
      "learning_rate": 3.422708333333334e-05,
      "loss": 0.0026,
      "step": 75710
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.1444234848022461,
      "learning_rate": 3.4225e-05,
      "loss": 0.0025,
      "step": 75720
    },
    {
      "epoch": 2.5243333333333333,
      "grad_norm": 0.20237068831920624,
      "learning_rate": 3.4222916666666664e-05,
      "loss": 0.0028,
      "step": 75730
    },
    {
      "epoch": 2.5246666666666666,
      "grad_norm": 0.002751676132902503,
      "learning_rate": 3.4220833333333336e-05,
      "loss": 0.0025,
      "step": 75740
    },
    {
      "epoch": 2.525,
      "grad_norm": 0.17305566370487213,
      "learning_rate": 3.421875e-05,
      "loss": 0.002,
      "step": 75750
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.08660338073968887,
      "learning_rate": 3.421666666666667e-05,
      "loss": 0.0024,
      "step": 75760
    },
    {
      "epoch": 2.5256666666666665,
      "grad_norm": 0.05777939036488533,
      "learning_rate": 3.421458333333333e-05,
      "loss": 0.0029,
      "step": 75770
    },
    {
      "epoch": 2.526,
      "grad_norm": 0.17312932014465332,
      "learning_rate": 3.4212500000000005e-05,
      "loss": 0.0024,
      "step": 75780
    },
    {
      "epoch": 2.5263333333333335,
      "grad_norm": 0.057948771864175797,
      "learning_rate": 3.421041666666667e-05,
      "loss": 0.003,
      "step": 75790
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.2895532250404358,
      "learning_rate": 3.4208333333333336e-05,
      "loss": 0.0028,
      "step": 75800
    },
    {
      "epoch": 2.527,
      "grad_norm": 0.14427410066127777,
      "learning_rate": 3.420625e-05,
      "loss": 0.0025,
      "step": 75810
    },
    {
      "epoch": 2.5273333333333334,
      "grad_norm": 0.25964733958244324,
      "learning_rate": 3.4204166666666674e-05,
      "loss": 0.002,
      "step": 75820
    },
    {
      "epoch": 2.5276666666666667,
      "grad_norm": 0.14433585107326508,
      "learning_rate": 3.420208333333333e-05,
      "loss": 0.0019,
      "step": 75830
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.17309805750846863,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0033,
      "step": 75840
    },
    {
      "epoch": 2.5283333333333333,
      "grad_norm": 0.012480298988521099,
      "learning_rate": 3.419791666666667e-05,
      "loss": 0.0023,
      "step": 75850
    },
    {
      "epoch": 2.5286666666666666,
      "grad_norm": 0.28859230875968933,
      "learning_rate": 3.4195833333333336e-05,
      "loss": 0.0016,
      "step": 75860
    },
    {
      "epoch": 2.529,
      "grad_norm": 0.05785181373357773,
      "learning_rate": 3.419375e-05,
      "loss": 0.0014,
      "step": 75870
    },
    {
      "epoch": 2.529333333333333,
      "grad_norm": 0.14448441565036774,
      "learning_rate": 3.4191666666666667e-05,
      "loss": 0.0019,
      "step": 75880
    },
    {
      "epoch": 2.5296666666666665,
      "grad_norm": 0.1158691793680191,
      "learning_rate": 3.418958333333334e-05,
      "loss": 0.0022,
      "step": 75890
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 0.03071954846382141,
      "learning_rate": 3.41875e-05,
      "loss": 0.0022,
      "step": 75900
    },
    {
      "epoch": 2.530333333333333,
      "grad_norm": 0.2596380412578583,
      "learning_rate": 3.418541666666667e-05,
      "loss": 0.0019,
      "step": 75910
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.25974878668785095,
      "learning_rate": 3.4183333333333335e-05,
      "loss": 0.0029,
      "step": 75920
    },
    {
      "epoch": 2.531,
      "grad_norm": 0.14434869587421417,
      "learning_rate": 3.418125e-05,
      "loss": 0.0022,
      "step": 75930
    },
    {
      "epoch": 2.5313333333333334,
      "grad_norm": 0.2595844566822052,
      "learning_rate": 3.4179166666666666e-05,
      "loss": 0.0024,
      "step": 75940
    },
    {
      "epoch": 2.5316666666666667,
      "grad_norm": 0.08687973022460938,
      "learning_rate": 3.417708333333334e-05,
      "loss": 0.0025,
      "step": 75950
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.28918829560279846,
      "learning_rate": 3.4175000000000004e-05,
      "loss": 0.0015,
      "step": 75960
    },
    {
      "epoch": 2.5323333333333333,
      "grad_norm": 0.28873494267463684,
      "learning_rate": 3.417291666666666e-05,
      "loss": 0.0017,
      "step": 75970
    },
    {
      "epoch": 2.5326666666666666,
      "grad_norm": 0.8596718311309814,
      "learning_rate": 3.4170833333333335e-05,
      "loss": 0.0024,
      "step": 75980
    },
    {
      "epoch": 2.533,
      "grad_norm": 0.08689247816801071,
      "learning_rate": 3.416875e-05,
      "loss": 0.002,
      "step": 75990
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.006962079554796219,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0018,
      "step": 76000
    },
    {
      "epoch": 2.5336666666666665,
      "grad_norm": 0.17358247935771942,
      "learning_rate": 3.416458333333333e-05,
      "loss": 0.0022,
      "step": 76010
    },
    {
      "epoch": 2.534,
      "grad_norm": 0.28856727480888367,
      "learning_rate": 3.4162500000000004e-05,
      "loss": 0.002,
      "step": 76020
    },
    {
      "epoch": 2.5343333333333335,
      "grad_norm": 0.11541096121072769,
      "learning_rate": 3.416041666666667e-05,
      "loss": 0.0017,
      "step": 76030
    },
    {
      "epoch": 2.5346666666666664,
      "grad_norm": 0.08679629862308502,
      "learning_rate": 3.4158333333333335e-05,
      "loss": 0.0021,
      "step": 76040
    },
    {
      "epoch": 2.535,
      "grad_norm": 0.2309262752532959,
      "learning_rate": 3.415625e-05,
      "loss": 0.0017,
      "step": 76050
    },
    {
      "epoch": 2.5353333333333334,
      "grad_norm": 0.14433568716049194,
      "learning_rate": 3.415416666666667e-05,
      "loss": 0.0015,
      "step": 76060
    },
    {
      "epoch": 2.5356666666666667,
      "grad_norm": 0.2886192202568054,
      "learning_rate": 3.415208333333334e-05,
      "loss": 0.0029,
      "step": 76070
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.029927227646112442,
      "learning_rate": 3.415e-05,
      "loss": 0.0019,
      "step": 76080
    },
    {
      "epoch": 2.5363333333333333,
      "grad_norm": 0.2600793242454529,
      "learning_rate": 3.414791666666667e-05,
      "loss": 0.0022,
      "step": 76090
    },
    {
      "epoch": 2.5366666666666666,
      "grad_norm": 0.4039851427078247,
      "learning_rate": 3.4145833333333334e-05,
      "loss": 0.0023,
      "step": 76100
    },
    {
      "epoch": 2.537,
      "grad_norm": 0.23103582859039307,
      "learning_rate": 3.414375e-05,
      "loss": 0.0023,
      "step": 76110
    },
    {
      "epoch": 2.537333333333333,
      "grad_norm": 0.05794185400009155,
      "learning_rate": 3.4141666666666665e-05,
      "loss": 0.0018,
      "step": 76120
    },
    {
      "epoch": 2.5376666666666665,
      "grad_norm": 0.01394146028906107,
      "learning_rate": 3.413958333333334e-05,
      "loss": 0.0033,
      "step": 76130
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 0.20191366970539093,
      "learning_rate": 3.41375e-05,
      "loss": 0.0029,
      "step": 76140
    },
    {
      "epoch": 2.538333333333333,
      "grad_norm": 0.40447086095809937,
      "learning_rate": 3.413541666666667e-05,
      "loss": 0.0023,
      "step": 76150
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.057769741863012314,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0017,
      "step": 76160
    },
    {
      "epoch": 2.539,
      "grad_norm": 0.17286792397499084,
      "learning_rate": 3.4131250000000006e-05,
      "loss": 0.0018,
      "step": 76170
    },
    {
      "epoch": 2.5393333333333334,
      "grad_norm": 0.20198090374469757,
      "learning_rate": 3.4129166666666665e-05,
      "loss": 0.0023,
      "step": 76180
    },
    {
      "epoch": 2.5396666666666667,
      "grad_norm": 0.14447525143623352,
      "learning_rate": 3.412708333333334e-05,
      "loss": 0.0025,
      "step": 76190
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.26620328426361084,
      "learning_rate": 3.4125e-05,
      "loss": 0.0025,
      "step": 76200
    },
    {
      "epoch": 2.5403333333333333,
      "grad_norm": 0.35668689012527466,
      "learning_rate": 3.412291666666667e-05,
      "loss": 0.0025,
      "step": 76210
    },
    {
      "epoch": 2.5406666666666666,
      "grad_norm": 0.25978943705558777,
      "learning_rate": 3.4120833333333334e-05,
      "loss": 0.0037,
      "step": 76220
    },
    {
      "epoch": 2.541,
      "grad_norm": 0.05809905380010605,
      "learning_rate": 3.411875e-05,
      "loss": 0.0024,
      "step": 76230
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.08682083338499069,
      "learning_rate": 3.411666666666667e-05,
      "loss": 0.0016,
      "step": 76240
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 0.37512338161468506,
      "learning_rate": 3.411458333333333e-05,
      "loss": 0.0027,
      "step": 76250
    },
    {
      "epoch": 2.542,
      "grad_norm": 0.03134680539369583,
      "learning_rate": 3.41125e-05,
      "loss": 0.0025,
      "step": 76260
    },
    {
      "epoch": 2.5423333333333336,
      "grad_norm": 0.2596331536769867,
      "learning_rate": 3.411041666666667e-05,
      "loss": 0.0021,
      "step": 76270
    },
    {
      "epoch": 2.5426666666666664,
      "grad_norm": 0.20208686590194702,
      "learning_rate": 3.4108333333333333e-05,
      "loss": 0.0014,
      "step": 76280
    },
    {
      "epoch": 2.543,
      "grad_norm": 0.6130234003067017,
      "learning_rate": 3.410625e-05,
      "loss": 0.0026,
      "step": 76290
    },
    {
      "epoch": 2.5433333333333334,
      "grad_norm": 0.4618600010871887,
      "learning_rate": 3.410416666666667e-05,
      "loss": 0.0021,
      "step": 76300
    },
    {
      "epoch": 2.5436666666666667,
      "grad_norm": 0.12492415308952332,
      "learning_rate": 3.410208333333334e-05,
      "loss": 0.0032,
      "step": 76310
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.174191415309906,
      "learning_rate": 3.41e-05,
      "loss": 0.0028,
      "step": 76320
    },
    {
      "epoch": 2.5443333333333333,
      "grad_norm": 0.20221368968486786,
      "learning_rate": 3.409791666666667e-05,
      "loss": 0.0021,
      "step": 76330
    },
    {
      "epoch": 2.5446666666666666,
      "grad_norm": 0.17312192916870117,
      "learning_rate": 3.409583333333333e-05,
      "loss": 0.0019,
      "step": 76340
    },
    {
      "epoch": 2.545,
      "grad_norm": 1.279861569404602,
      "learning_rate": 3.4093750000000005e-05,
      "loss": 0.0023,
      "step": 76350
    },
    {
      "epoch": 2.5453333333333332,
      "grad_norm": 0.28869256377220154,
      "learning_rate": 3.4091666666666664e-05,
      "loss": 0.0018,
      "step": 76360
    },
    {
      "epoch": 2.5456666666666665,
      "grad_norm": 0.2020653933286667,
      "learning_rate": 3.4089583333333336e-05,
      "loss": 0.0021,
      "step": 76370
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 0.14425595104694366,
      "learning_rate": 3.40875e-05,
      "loss": 0.0034,
      "step": 76380
    },
    {
      "epoch": 2.546333333333333,
      "grad_norm": 0.14424100518226624,
      "learning_rate": 3.408541666666667e-05,
      "loss": 0.0022,
      "step": 76390
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.14434103667736053,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.0034,
      "step": 76400
    },
    {
      "epoch": 2.547,
      "grad_norm": 0.05781593173742294,
      "learning_rate": 3.4081250000000005e-05,
      "loss": 0.0019,
      "step": 76410
    },
    {
      "epoch": 2.5473333333333334,
      "grad_norm": 0.11568138003349304,
      "learning_rate": 3.407916666666667e-05,
      "loss": 0.0015,
      "step": 76420
    },
    {
      "epoch": 2.5476666666666667,
      "grad_norm": 0.115730881690979,
      "learning_rate": 3.4077083333333336e-05,
      "loss": 0.0029,
      "step": 76430
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.02936335653066635,
      "learning_rate": 3.4075e-05,
      "loss": 0.0025,
      "step": 76440
    },
    {
      "epoch": 2.5483333333333333,
      "grad_norm": 0.11571119725704193,
      "learning_rate": 3.4072916666666674e-05,
      "loss": 0.0027,
      "step": 76450
    },
    {
      "epoch": 2.5486666666666666,
      "grad_norm": 0.40398508310317993,
      "learning_rate": 3.407083333333333e-05,
      "loss": 0.0021,
      "step": 76460
    },
    {
      "epoch": 2.549,
      "grad_norm": 0.23121187090873718,
      "learning_rate": 3.406875e-05,
      "loss": 0.0017,
      "step": 76470
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.17352117598056793,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0029,
      "step": 76480
    },
    {
      "epoch": 2.5496666666666665,
      "grad_norm": 0.029017390683293343,
      "learning_rate": 3.4064583333333336e-05,
      "loss": 0.0038,
      "step": 76490
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.006349058356136084,
      "learning_rate": 3.40625e-05,
      "loss": 0.0018,
      "step": 76500
    },
    {
      "epoch": 2.5503333333333336,
      "grad_norm": 0.029192548245191574,
      "learning_rate": 3.406041666666667e-05,
      "loss": 0.002,
      "step": 76510
    },
    {
      "epoch": 2.5506666666666664,
      "grad_norm": 0.058086324483156204,
      "learning_rate": 3.405833333333334e-05,
      "loss": 0.0024,
      "step": 76520
    },
    {
      "epoch": 2.551,
      "grad_norm": 0.28845086693763733,
      "learning_rate": 3.405625e-05,
      "loss": 0.0027,
      "step": 76530
    },
    {
      "epoch": 2.5513333333333335,
      "grad_norm": 0.005169312469661236,
      "learning_rate": 3.405416666666667e-05,
      "loss": 0.0026,
      "step": 76540
    },
    {
      "epoch": 2.5516666666666667,
      "grad_norm": 0.3462989330291748,
      "learning_rate": 3.4052083333333335e-05,
      "loss": 0.0021,
      "step": 76550
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.08697302639484406,
      "learning_rate": 3.405e-05,
      "loss": 0.0029,
      "step": 76560
    },
    {
      "epoch": 2.5523333333333333,
      "grad_norm": 0.20207063853740692,
      "learning_rate": 3.4047916666666666e-05,
      "loss": 0.0019,
      "step": 76570
    },
    {
      "epoch": 2.5526666666666666,
      "grad_norm": 0.05774283781647682,
      "learning_rate": 3.404583333333333e-05,
      "loss": 0.0029,
      "step": 76580
    },
    {
      "epoch": 2.553,
      "grad_norm": 0.3461059033870697,
      "learning_rate": 3.4043750000000004e-05,
      "loss": 0.0018,
      "step": 76590
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.11542962491512299,
      "learning_rate": 3.404166666666666e-05,
      "loss": 0.0025,
      "step": 76600
    },
    {
      "epoch": 2.5536666666666665,
      "grad_norm": 0.46165600419044495,
      "learning_rate": 3.4039583333333335e-05,
      "loss": 0.0025,
      "step": 76610
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 0.2020905464887619,
      "learning_rate": 3.40375e-05,
      "loss": 0.0028,
      "step": 76620
    },
    {
      "epoch": 2.554333333333333,
      "grad_norm": 0.11551808565855026,
      "learning_rate": 3.403541666666667e-05,
      "loss": 0.0028,
      "step": 76630
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.3175346851348877,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0021,
      "step": 76640
    },
    {
      "epoch": 2.555,
      "grad_norm": 0.1731046885251999,
      "learning_rate": 3.4031250000000004e-05,
      "loss": 0.0016,
      "step": 76650
    },
    {
      "epoch": 2.5553333333333335,
      "grad_norm": 0.11581075191497803,
      "learning_rate": 3.402916666666667e-05,
      "loss": 0.0018,
      "step": 76660
    },
    {
      "epoch": 2.5556666666666668,
      "grad_norm": 0.059349291026592255,
      "learning_rate": 3.4027083333333335e-05,
      "loss": 0.0025,
      "step": 76670
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.7855762243270874,
      "learning_rate": 3.4025e-05,
      "loss": 0.0025,
      "step": 76680
    },
    {
      "epoch": 2.5563333333333333,
      "grad_norm": 0.4904745817184448,
      "learning_rate": 3.402291666666667e-05,
      "loss": 0.0023,
      "step": 76690
    },
    {
      "epoch": 2.5566666666666666,
      "grad_norm": 0.03001825138926506,
      "learning_rate": 3.402083333333334e-05,
      "loss": 0.0018,
      "step": 76700
    },
    {
      "epoch": 2.557,
      "grad_norm": 0.028998279944062233,
      "learning_rate": 3.401875e-05,
      "loss": 0.003,
      "step": 76710
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.00658394955098629,
      "learning_rate": 3.401666666666667e-05,
      "loss": 0.0036,
      "step": 76720
    },
    {
      "epoch": 2.5576666666666665,
      "grad_norm": 0.2308005541563034,
      "learning_rate": 3.4014583333333335e-05,
      "loss": 0.002,
      "step": 76730
    },
    {
      "epoch": 2.558,
      "grad_norm": 0.029358046129345894,
      "learning_rate": 3.40125e-05,
      "loss": 0.0028,
      "step": 76740
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 0.14433903992176056,
      "learning_rate": 3.4010416666666666e-05,
      "loss": 0.0022,
      "step": 76750
    },
    {
      "epoch": 2.5586666666666664,
      "grad_norm": 0.20189963281154633,
      "learning_rate": 3.400833333333334e-05,
      "loss": 0.0031,
      "step": 76760
    },
    {
      "epoch": 2.559,
      "grad_norm": 0.08708269894123077,
      "learning_rate": 3.400625e-05,
      "loss": 0.0015,
      "step": 76770
    },
    {
      "epoch": 2.5593333333333335,
      "grad_norm": 0.05788744240999222,
      "learning_rate": 3.400416666666667e-05,
      "loss": 0.0019,
      "step": 76780
    },
    {
      "epoch": 2.5596666666666668,
      "grad_norm": 0.5482326745986938,
      "learning_rate": 3.4002083333333334e-05,
      "loss": 0.0028,
      "step": 76790
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.14466916024684906,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.003,
      "step": 76800
    },
    {
      "epoch": 2.5603333333333333,
      "grad_norm": 0.2597925662994385,
      "learning_rate": 3.3997916666666665e-05,
      "loss": 0.0024,
      "step": 76810
    },
    {
      "epoch": 2.5606666666666666,
      "grad_norm": 0.17184436321258545,
      "learning_rate": 3.399583333333333e-05,
      "loss": 0.0028,
      "step": 76820
    },
    {
      "epoch": 2.561,
      "grad_norm": 0.20196092128753662,
      "learning_rate": 3.399375e-05,
      "loss": 0.0033,
      "step": 76830
    },
    {
      "epoch": 2.5613333333333332,
      "grad_norm": 0.3619849681854248,
      "learning_rate": 3.399166666666667e-05,
      "loss": 0.0025,
      "step": 76840
    },
    {
      "epoch": 2.5616666666666665,
      "grad_norm": 0.17325350642204285,
      "learning_rate": 3.3989583333333334e-05,
      "loss": 0.0017,
      "step": 76850
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 0.37513792514801025,
      "learning_rate": 3.39875e-05,
      "loss": 0.0021,
      "step": 76860
    },
    {
      "epoch": 2.562333333333333,
      "grad_norm": 0.007216037716716528,
      "learning_rate": 3.398541666666667e-05,
      "loss": 0.0022,
      "step": 76870
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.2596900761127472,
      "learning_rate": 3.398333333333333e-05,
      "loss": 0.0031,
      "step": 76880
    },
    {
      "epoch": 2.5629999999999997,
      "grad_norm": 0.14453530311584473,
      "learning_rate": 3.398125e-05,
      "loss": 0.0019,
      "step": 76890
    },
    {
      "epoch": 2.5633333333333335,
      "grad_norm": 0.3464191257953644,
      "learning_rate": 3.397916666666667e-05,
      "loss": 0.0026,
      "step": 76900
    },
    {
      "epoch": 2.5636666666666668,
      "grad_norm": 0.3460877537727356,
      "learning_rate": 3.397708333333334e-05,
      "loss": 0.0028,
      "step": 76910
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.14457887411117554,
      "learning_rate": 3.3975e-05,
      "loss": 0.0032,
      "step": 76920
    },
    {
      "epoch": 2.5643333333333334,
      "grad_norm": 0.05839110165834427,
      "learning_rate": 3.397291666666667e-05,
      "loss": 0.0029,
      "step": 76930
    },
    {
      "epoch": 2.5646666666666667,
      "grad_norm": 0.11581110209226608,
      "learning_rate": 3.397083333333334e-05,
      "loss": 0.0031,
      "step": 76940
    },
    {
      "epoch": 2.565,
      "grad_norm": 0.006312210578471422,
      "learning_rate": 3.396875e-05,
      "loss": 0.0016,
      "step": 76950
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.36246463656425476,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.0027,
      "step": 76960
    },
    {
      "epoch": 2.5656666666666665,
      "grad_norm": 0.34679797291755676,
      "learning_rate": 3.396458333333333e-05,
      "loss": 0.0026,
      "step": 76970
    },
    {
      "epoch": 2.566,
      "grad_norm": 0.11564971506595612,
      "learning_rate": 3.3962500000000006e-05,
      "loss": 0.0025,
      "step": 76980
    },
    {
      "epoch": 2.5663333333333336,
      "grad_norm": 0.11577372252941132,
      "learning_rate": 3.3960416666666664e-05,
      "loss": 0.002,
      "step": 76990
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.115911103785038,
      "learning_rate": 3.3958333333333337e-05,
      "loss": 0.0017,
      "step": 77000
    },
    {
      "epoch": 2.567,
      "grad_norm": 0.002303930465131998,
      "learning_rate": 3.395625e-05,
      "loss": 0.0019,
      "step": 77010
    },
    {
      "epoch": 2.5673333333333335,
      "grad_norm": 0.20183952152729034,
      "learning_rate": 3.395416666666667e-05,
      "loss": 0.0035,
      "step": 77020
    },
    {
      "epoch": 2.5676666666666668,
      "grad_norm": 0.207976296544075,
      "learning_rate": 3.395208333333333e-05,
      "loss": 0.0024,
      "step": 77030
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.5672716498374939,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 0.002,
      "step": 77040
    },
    {
      "epoch": 2.5683333333333334,
      "grad_norm": 0.46172699332237244,
      "learning_rate": 3.394791666666667e-05,
      "loss": 0.0022,
      "step": 77050
    },
    {
      "epoch": 2.5686666666666667,
      "grad_norm": 0.14448222517967224,
      "learning_rate": 3.394583333333333e-05,
      "loss": 0.0019,
      "step": 77060
    },
    {
      "epoch": 2.569,
      "grad_norm": 0.006302322261035442,
      "learning_rate": 3.394375e-05,
      "loss": 0.0023,
      "step": 77070
    },
    {
      "epoch": 2.5693333333333332,
      "grad_norm": 0.3490450978279114,
      "learning_rate": 3.394166666666667e-05,
      "loss": 0.0033,
      "step": 77080
    },
    {
      "epoch": 2.5696666666666665,
      "grad_norm": 0.48010286688804626,
      "learning_rate": 3.393958333333333e-05,
      "loss": 0.004,
      "step": 77090
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.24547378718852997,
      "learning_rate": 3.39375e-05,
      "loss": 0.0023,
      "step": 77100
    },
    {
      "epoch": 2.570333333333333,
      "grad_norm": 0.14471839368343353,
      "learning_rate": 3.393541666666667e-05,
      "loss": 0.0026,
      "step": 77110
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.3751397132873535,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0027,
      "step": 77120
    },
    {
      "epoch": 2.5709999999999997,
      "grad_norm": 0.20191921293735504,
      "learning_rate": 3.393125e-05,
      "loss": 0.0027,
      "step": 77130
    },
    {
      "epoch": 2.5713333333333335,
      "grad_norm": 0.08634591847658157,
      "learning_rate": 3.392916666666667e-05,
      "loss": 0.0021,
      "step": 77140
    },
    {
      "epoch": 2.5716666666666668,
      "grad_norm": 0.14425764977931976,
      "learning_rate": 3.392708333333334e-05,
      "loss": 0.0018,
      "step": 77150
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.20224127173423767,
      "learning_rate": 3.3925e-05,
      "loss": 0.0031,
      "step": 77160
    },
    {
      "epoch": 2.5723333333333334,
      "grad_norm": 0.5481933355331421,
      "learning_rate": 3.392291666666667e-05,
      "loss": 0.0023,
      "step": 77170
    },
    {
      "epoch": 2.5726666666666667,
      "grad_norm": 0.17336200177669525,
      "learning_rate": 3.3920833333333336e-05,
      "loss": 0.0025,
      "step": 77180
    },
    {
      "epoch": 2.573,
      "grad_norm": 0.029472248628735542,
      "learning_rate": 3.391875e-05,
      "loss": 0.0022,
      "step": 77190
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.17311595380306244,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.0025,
      "step": 77200
    },
    {
      "epoch": 2.5736666666666665,
      "grad_norm": 0.029743531718850136,
      "learning_rate": 3.391458333333333e-05,
      "loss": 0.0017,
      "step": 77210
    },
    {
      "epoch": 2.574,
      "grad_norm": 0.05800352618098259,
      "learning_rate": 3.3912500000000004e-05,
      "loss": 0.0021,
      "step": 77220
    },
    {
      "epoch": 2.5743333333333336,
      "grad_norm": 0.08714131265878677,
      "learning_rate": 3.391041666666667e-05,
      "loss": 0.0014,
      "step": 77230
    },
    {
      "epoch": 2.5746666666666664,
      "grad_norm": 0.23092730343341827,
      "learning_rate": 3.3908333333333335e-05,
      "loss": 0.002,
      "step": 77240
    },
    {
      "epoch": 2.575,
      "grad_norm": 2.2435948848724365,
      "learning_rate": 3.390625e-05,
      "loss": 0.0023,
      "step": 77250
    },
    {
      "epoch": 2.5753333333333335,
      "grad_norm": 0.20233936607837677,
      "learning_rate": 3.390416666666667e-05,
      "loss": 0.0026,
      "step": 77260
    },
    {
      "epoch": 2.5756666666666668,
      "grad_norm": 0.17340312898159027,
      "learning_rate": 3.390208333333333e-05,
      "loss": 0.0028,
      "step": 77270
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.057964883744716644,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.002,
      "step": 77280
    },
    {
      "epoch": 2.5763333333333334,
      "grad_norm": 0.144120991230011,
      "learning_rate": 3.389791666666667e-05,
      "loss": 0.002,
      "step": 77290
    },
    {
      "epoch": 2.5766666666666667,
      "grad_norm": 0.20199277997016907,
      "learning_rate": 3.3895833333333335e-05,
      "loss": 0.0027,
      "step": 77300
    },
    {
      "epoch": 2.577,
      "grad_norm": 0.3172307312488556,
      "learning_rate": 3.389375e-05,
      "loss": 0.0023,
      "step": 77310
    },
    {
      "epoch": 2.5773333333333333,
      "grad_norm": 0.11783380806446075,
      "learning_rate": 3.3891666666666666e-05,
      "loss": 0.0024,
      "step": 77320
    },
    {
      "epoch": 2.5776666666666666,
      "grad_norm": 0.1732243001461029,
      "learning_rate": 3.388958333333334e-05,
      "loss": 0.0022,
      "step": 77330
    },
    {
      "epoch": 2.578,
      "grad_norm": 0.08673883229494095,
      "learning_rate": 3.38875e-05,
      "loss": 0.0026,
      "step": 77340
    },
    {
      "epoch": 2.578333333333333,
      "grad_norm": 0.23596756160259247,
      "learning_rate": 3.388541666666667e-05,
      "loss": 0.0017,
      "step": 77350
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.050855547189712524,
      "learning_rate": 3.3883333333333335e-05,
      "loss": 0.0032,
      "step": 77360
    },
    {
      "epoch": 2.5789999999999997,
      "grad_norm": 0.03045310080051422,
      "learning_rate": 3.388125e-05,
      "loss": 0.0025,
      "step": 77370
    },
    {
      "epoch": 2.5793333333333335,
      "grad_norm": 0.11579474061727524,
      "learning_rate": 3.3879166666666666e-05,
      "loss": 0.0021,
      "step": 77380
    },
    {
      "epoch": 2.5796666666666668,
      "grad_norm": 0.08679661154747009,
      "learning_rate": 3.387708333333334e-05,
      "loss": 0.0019,
      "step": 77390
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.029150359332561493,
      "learning_rate": 3.3875000000000003e-05,
      "loss": 0.0016,
      "step": 77400
    },
    {
      "epoch": 2.5803333333333334,
      "grad_norm": 0.1156075969338417,
      "learning_rate": 3.387291666666667e-05,
      "loss": 0.0012,
      "step": 77410
    },
    {
      "epoch": 2.5806666666666667,
      "grad_norm": 0.23122362792491913,
      "learning_rate": 3.3870833333333334e-05,
      "loss": 0.0022,
      "step": 77420
    },
    {
      "epoch": 2.581,
      "grad_norm": 0.14464138448238373,
      "learning_rate": 3.386875e-05,
      "loss": 0.002,
      "step": 77430
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.31777822971343994,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0016,
      "step": 77440
    },
    {
      "epoch": 2.5816666666666666,
      "grad_norm": 0.11562446504831314,
      "learning_rate": 3.386458333333333e-05,
      "loss": 0.0025,
      "step": 77450
    },
    {
      "epoch": 2.582,
      "grad_norm": 0.8531149625778198,
      "learning_rate": 3.38625e-05,
      "loss": 0.0035,
      "step": 77460
    },
    {
      "epoch": 2.5823333333333336,
      "grad_norm": 0.9952778816223145,
      "learning_rate": 3.386041666666667e-05,
      "loss": 0.0022,
      "step": 77470
    },
    {
      "epoch": 2.5826666666666664,
      "grad_norm": 0.05812850967049599,
      "learning_rate": 3.3858333333333334e-05,
      "loss": 0.0019,
      "step": 77480
    },
    {
      "epoch": 2.583,
      "grad_norm": 0.20194514095783234,
      "learning_rate": 3.385625e-05,
      "loss": 0.0032,
      "step": 77490
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.057688768953084946,
      "learning_rate": 3.385416666666667e-05,
      "loss": 0.0029,
      "step": 77500
    },
    {
      "epoch": 2.583666666666667,
      "grad_norm": 0.11546008288860321,
      "learning_rate": 3.385208333333334e-05,
      "loss": 0.0017,
      "step": 77510
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.20205442607402802,
      "learning_rate": 3.385e-05,
      "loss": 0.0027,
      "step": 77520
    },
    {
      "epoch": 2.5843333333333334,
      "grad_norm": 0.20197921991348267,
      "learning_rate": 3.384791666666667e-05,
      "loss": 0.0019,
      "step": 77530
    },
    {
      "epoch": 2.5846666666666667,
      "grad_norm": 0.05778074264526367,
      "learning_rate": 3.384583333333334e-05,
      "loss": 0.0022,
      "step": 77540
    },
    {
      "epoch": 2.585,
      "grad_norm": 0.17287588119506836,
      "learning_rate": 3.384375e-05,
      "loss": 0.0024,
      "step": 77550
    },
    {
      "epoch": 2.5853333333333333,
      "grad_norm": 0.14415422081947327,
      "learning_rate": 3.3841666666666665e-05,
      "loss": 0.0023,
      "step": 77560
    },
    {
      "epoch": 2.5856666666666666,
      "grad_norm": 0.4038381278514862,
      "learning_rate": 3.383958333333334e-05,
      "loss": 0.0027,
      "step": 77570
    },
    {
      "epoch": 2.586,
      "grad_norm": 0.02898993343114853,
      "learning_rate": 3.38375e-05,
      "loss": 0.0023,
      "step": 77580
    },
    {
      "epoch": 2.586333333333333,
      "grad_norm": 0.5769084692001343,
      "learning_rate": 3.383541666666667e-05,
      "loss": 0.0026,
      "step": 77590
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.2885095179080963,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0029,
      "step": 77600
    },
    {
      "epoch": 2.5869999999999997,
      "grad_norm": 0.006131387315690517,
      "learning_rate": 3.3831250000000006e-05,
      "loss": 0.0026,
      "step": 77610
    },
    {
      "epoch": 2.5873333333333335,
      "grad_norm": 0.490236759185791,
      "learning_rate": 3.3829166666666665e-05,
      "loss": 0.0023,
      "step": 77620
    },
    {
      "epoch": 2.587666666666667,
      "grad_norm": 0.5191330909729004,
      "learning_rate": 3.382708333333334e-05,
      "loss": 0.002,
      "step": 77630
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.08689609169960022,
      "learning_rate": 3.3825e-05,
      "loss": 0.0035,
      "step": 77640
    },
    {
      "epoch": 2.5883333333333334,
      "grad_norm": 0.14425240457057953,
      "learning_rate": 3.382291666666667e-05,
      "loss": 0.0024,
      "step": 77650
    },
    {
      "epoch": 2.5886666666666667,
      "grad_norm": 0.08655371516942978,
      "learning_rate": 3.382083333333333e-05,
      "loss": 0.0024,
      "step": 77660
    },
    {
      "epoch": 2.589,
      "grad_norm": 0.3459795415401459,
      "learning_rate": 3.381875e-05,
      "loss": 0.003,
      "step": 77670
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.20213757455348969,
      "learning_rate": 3.381666666666667e-05,
      "loss": 0.002,
      "step": 77680
    },
    {
      "epoch": 2.5896666666666666,
      "grad_norm": 0.05834372714161873,
      "learning_rate": 3.381458333333333e-05,
      "loss": 0.0036,
      "step": 77690
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.057986486703157425,
      "learning_rate": 3.38125e-05,
      "loss": 0.0024,
      "step": 77700
    },
    {
      "epoch": 2.5903333333333336,
      "grad_norm": 0.20203791558742523,
      "learning_rate": 3.381041666666667e-05,
      "loss": 0.0026,
      "step": 77710
    },
    {
      "epoch": 2.5906666666666665,
      "grad_norm": 0.058071333914995193,
      "learning_rate": 3.380833333333333e-05,
      "loss": 0.0038,
      "step": 77720
    },
    {
      "epoch": 2.591,
      "grad_norm": 0.23085446655750275,
      "learning_rate": 3.380625e-05,
      "loss": 0.0036,
      "step": 77730
    },
    {
      "epoch": 2.5913333333333335,
      "grad_norm": 0.20176367461681366,
      "learning_rate": 3.380416666666667e-05,
      "loss": 0.0036,
      "step": 77740
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 0.40375933051109314,
      "learning_rate": 3.3802083333333336e-05,
      "loss": 0.0034,
      "step": 77750
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.057940565049648285,
      "learning_rate": 3.38e-05,
      "loss": 0.0033,
      "step": 77760
    },
    {
      "epoch": 2.5923333333333334,
      "grad_norm": 0.20189757645130157,
      "learning_rate": 3.379791666666667e-05,
      "loss": 0.0019,
      "step": 77770
    },
    {
      "epoch": 2.5926666666666667,
      "grad_norm": 0.259613037109375,
      "learning_rate": 3.379583333333334e-05,
      "loss": 0.0031,
      "step": 77780
    },
    {
      "epoch": 2.593,
      "grad_norm": 0.23065342009067535,
      "learning_rate": 3.3793750000000005e-05,
      "loss": 0.0022,
      "step": 77790
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.2058435082435608,
      "learning_rate": 3.3791666666666664e-05,
      "loss": 0.0017,
      "step": 77800
    },
    {
      "epoch": 2.5936666666666666,
      "grad_norm": 0.08668631315231323,
      "learning_rate": 3.3789583333333336e-05,
      "loss": 0.0027,
      "step": 77810
    },
    {
      "epoch": 2.594,
      "grad_norm": 0.11531597375869751,
      "learning_rate": 3.37875e-05,
      "loss": 0.0026,
      "step": 77820
    },
    {
      "epoch": 2.594333333333333,
      "grad_norm": 0.17310824990272522,
      "learning_rate": 3.378541666666667e-05,
      "loss": 0.0025,
      "step": 77830
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.08638673275709152,
      "learning_rate": 3.378333333333333e-05,
      "loss": 0.0026,
      "step": 77840
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 0.05785621702671051,
      "learning_rate": 3.3781250000000005e-05,
      "loss": 0.0014,
      "step": 77850
    },
    {
      "epoch": 2.5953333333333335,
      "grad_norm": 0.4327656924724579,
      "learning_rate": 3.377916666666667e-05,
      "loss": 0.0021,
      "step": 77860
    },
    {
      "epoch": 2.595666666666667,
      "grad_norm": 0.1442583054304123,
      "learning_rate": 3.3777083333333336e-05,
      "loss": 0.0033,
      "step": 77870
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.1440095156431198,
      "learning_rate": 3.3775e-05,
      "loss": 0.0019,
      "step": 77880
    },
    {
      "epoch": 2.5963333333333334,
      "grad_norm": 0.27662280201911926,
      "learning_rate": 3.377291666666667e-05,
      "loss": 0.0026,
      "step": 77890
    },
    {
      "epoch": 2.5966666666666667,
      "grad_norm": 0.17298711836338043,
      "learning_rate": 3.377083333333333e-05,
      "loss": 0.002,
      "step": 77900
    },
    {
      "epoch": 2.597,
      "grad_norm": 0.5191323161125183,
      "learning_rate": 3.376875e-05,
      "loss": 0.0019,
      "step": 77910
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.20217888057231903,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0018,
      "step": 77920
    },
    {
      "epoch": 2.5976666666666666,
      "grad_norm": 0.23080980777740479,
      "learning_rate": 3.3764583333333335e-05,
      "loss": 0.0039,
      "step": 77930
    },
    {
      "epoch": 2.598,
      "grad_norm": 0.08666691929101944,
      "learning_rate": 3.37625e-05,
      "loss": 0.0025,
      "step": 77940
    },
    {
      "epoch": 2.5983333333333336,
      "grad_norm": 0.20184728503227234,
      "learning_rate": 3.3760416666666666e-05,
      "loss": 0.0018,
      "step": 77950
    },
    {
      "epoch": 2.5986666666666665,
      "grad_norm": 0.23086772859096527,
      "learning_rate": 3.375833333333334e-05,
      "loss": 0.0018,
      "step": 77960
    },
    {
      "epoch": 2.599,
      "grad_norm": 0.029114149510860443,
      "learning_rate": 3.375625e-05,
      "loss": 0.0018,
      "step": 77970
    },
    {
      "epoch": 2.5993333333333335,
      "grad_norm": 0.25976526737213135,
      "learning_rate": 3.375416666666667e-05,
      "loss": 0.0024,
      "step": 77980
    },
    {
      "epoch": 2.599666666666667,
      "grad_norm": 0.11558827012777328,
      "learning_rate": 3.3752083333333335e-05,
      "loss": 0.0019,
      "step": 77990
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.2600573003292084,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.0026,
      "step": 78000
    },
    {
      "epoch": 2.6003333333333334,
      "grad_norm": 0.3172491788864136,
      "learning_rate": 3.3747916666666666e-05,
      "loss": 0.0016,
      "step": 78010
    },
    {
      "epoch": 2.6006666666666667,
      "grad_norm": 0.02921513468027115,
      "learning_rate": 3.374583333333334e-05,
      "loss": 0.0017,
      "step": 78020
    },
    {
      "epoch": 2.601,
      "grad_norm": 0.3461456000804901,
      "learning_rate": 3.3743750000000004e-05,
      "loss": 0.0022,
      "step": 78030
    },
    {
      "epoch": 2.6013333333333333,
      "grad_norm": 0.31736820936203003,
      "learning_rate": 3.374166666666667e-05,
      "loss": 0.0021,
      "step": 78040
    },
    {
      "epoch": 2.6016666666666666,
      "grad_norm": 0.006798245012760162,
      "learning_rate": 3.3739583333333335e-05,
      "loss": 0.0028,
      "step": 78050
    },
    {
      "epoch": 2.602,
      "grad_norm": 0.08663894981145859,
      "learning_rate": 3.37375e-05,
      "loss": 0.0019,
      "step": 78060
    },
    {
      "epoch": 2.602333333333333,
      "grad_norm": 0.4323279559612274,
      "learning_rate": 3.373541666666667e-05,
      "loss": 0.0025,
      "step": 78070
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.20211109519004822,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0027,
      "step": 78080
    },
    {
      "epoch": 2.6029999999999998,
      "grad_norm": 0.17299577593803406,
      "learning_rate": 3.373125e-05,
      "loss": 0.0029,
      "step": 78090
    },
    {
      "epoch": 2.6033333333333335,
      "grad_norm": 0.029661355540156364,
      "learning_rate": 3.372916666666667e-05,
      "loss": 0.002,
      "step": 78100
    },
    {
      "epoch": 2.603666666666667,
      "grad_norm": 0.2304605394601822,
      "learning_rate": 3.3727083333333334e-05,
      "loss": 0.0026,
      "step": 78110
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.31700921058654785,
      "learning_rate": 3.3725e-05,
      "loss": 0.0022,
      "step": 78120
    },
    {
      "epoch": 2.6043333333333334,
      "grad_norm": 0.3167426884174347,
      "learning_rate": 3.372291666666667e-05,
      "loss": 0.0018,
      "step": 78130
    },
    {
      "epoch": 2.6046666666666667,
      "grad_norm": 0.31254029273986816,
      "learning_rate": 3.372083333333334e-05,
      "loss": 0.0035,
      "step": 78140
    },
    {
      "epoch": 2.605,
      "grad_norm": 0.6333102583885193,
      "learning_rate": 3.3718749999999996e-05,
      "loss": 0.0012,
      "step": 78150
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.2598499357700348,
      "learning_rate": 3.371666666666667e-05,
      "loss": 0.0019,
      "step": 78160
    },
    {
      "epoch": 2.6056666666666666,
      "grad_norm": 0.10068036615848541,
      "learning_rate": 3.3714583333333334e-05,
      "loss": 0.0019,
      "step": 78170
    },
    {
      "epoch": 2.606,
      "grad_norm": 0.086760513484478,
      "learning_rate": 3.37125e-05,
      "loss": 0.0023,
      "step": 78180
    },
    {
      "epoch": 2.606333333333333,
      "grad_norm": 0.08697173744440079,
      "learning_rate": 3.3710416666666665e-05,
      "loss": 0.0019,
      "step": 78190
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.11585292965173721,
      "learning_rate": 3.370833333333334e-05,
      "loss": 0.0016,
      "step": 78200
    },
    {
      "epoch": 2.607,
      "grad_norm": 0.032294780015945435,
      "learning_rate": 3.370625e-05,
      "loss": 0.0018,
      "step": 78210
    },
    {
      "epoch": 2.607333333333333,
      "grad_norm": 0.2885219156742096,
      "learning_rate": 3.370416666666667e-05,
      "loss": 0.0013,
      "step": 78220
    },
    {
      "epoch": 2.607666666666667,
      "grad_norm": 0.08697455376386642,
      "learning_rate": 3.3702083333333334e-05,
      "loss": 0.002,
      "step": 78230
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.029248835518956184,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0025,
      "step": 78240
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 0.28827592730522156,
      "learning_rate": 3.3697916666666665e-05,
      "loss": 0.0023,
      "step": 78250
    },
    {
      "epoch": 2.6086666666666667,
      "grad_norm": 0.23103025555610657,
      "learning_rate": 3.369583333333334e-05,
      "loss": 0.0019,
      "step": 78260
    },
    {
      "epoch": 2.609,
      "grad_norm": 0.10285337269306183,
      "learning_rate": 3.369375e-05,
      "loss": 0.0022,
      "step": 78270
    },
    {
      "epoch": 2.6093333333333333,
      "grad_norm": 0.08681990206241608,
      "learning_rate": 3.369166666666667e-05,
      "loss": 0.0023,
      "step": 78280
    },
    {
      "epoch": 2.6096666666666666,
      "grad_norm": 0.029416395351290703,
      "learning_rate": 3.3689583333333333e-05,
      "loss": 0.0016,
      "step": 78290
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.08643580228090286,
      "learning_rate": 3.36875e-05,
      "loss": 0.0028,
      "step": 78300
    },
    {
      "epoch": 2.610333333333333,
      "grad_norm": 0.1728127896785736,
      "learning_rate": 3.368541666666667e-05,
      "loss": 0.0027,
      "step": 78310
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.20193256437778473,
      "learning_rate": 3.368333333333334e-05,
      "loss": 0.0021,
      "step": 78320
    },
    {
      "epoch": 2.6109999999999998,
      "grad_norm": 0.17289796471595764,
      "learning_rate": 3.368125e-05,
      "loss": 0.0019,
      "step": 78330
    },
    {
      "epoch": 2.6113333333333335,
      "grad_norm": 0.28859245777130127,
      "learning_rate": 3.367916666666667e-05,
      "loss": 0.0024,
      "step": 78340
    },
    {
      "epoch": 2.611666666666667,
      "grad_norm": 0.25953125953674316,
      "learning_rate": 3.367708333333334e-05,
      "loss": 0.002,
      "step": 78350
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.6014129519462585,
      "learning_rate": 3.3675e-05,
      "loss": 0.0029,
      "step": 78360
    },
    {
      "epoch": 2.6123333333333334,
      "grad_norm": 0.22336092591285706,
      "learning_rate": 3.367291666666667e-05,
      "loss": 0.0021,
      "step": 78370
    },
    {
      "epoch": 2.6126666666666667,
      "grad_norm": 0.2309815138578415,
      "learning_rate": 3.3670833333333336e-05,
      "loss": 0.0022,
      "step": 78380
    },
    {
      "epoch": 2.613,
      "grad_norm": 0.864996612071991,
      "learning_rate": 3.366875e-05,
      "loss": 0.0026,
      "step": 78390
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.23068532347679138,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0029,
      "step": 78400
    },
    {
      "epoch": 2.6136666666666666,
      "grad_norm": 0.2020416259765625,
      "learning_rate": 3.366458333333333e-05,
      "loss": 0.003,
      "step": 78410
    },
    {
      "epoch": 2.614,
      "grad_norm": 0.08661457151174545,
      "learning_rate": 3.3662500000000005e-05,
      "loss": 0.0019,
      "step": 78420
    },
    {
      "epoch": 2.614333333333333,
      "grad_norm": 0.007494611665606499,
      "learning_rate": 3.3660416666666664e-05,
      "loss": 0.0025,
      "step": 78430
    },
    {
      "epoch": 2.6146666666666665,
      "grad_norm": 0.02943487837910652,
      "learning_rate": 3.3658333333333336e-05,
      "loss": 0.0035,
      "step": 78440
    },
    {
      "epoch": 2.615,
      "grad_norm": 0.20351630449295044,
      "learning_rate": 3.365625e-05,
      "loss": 0.0024,
      "step": 78450
    },
    {
      "epoch": 2.615333333333333,
      "grad_norm": 0.28822773694992065,
      "learning_rate": 3.365416666666667e-05,
      "loss": 0.0024,
      "step": 78460
    },
    {
      "epoch": 2.615666666666667,
      "grad_norm": 0.1828063726425171,
      "learning_rate": 3.365208333333333e-05,
      "loss": 0.0017,
      "step": 78470
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.030057132244110107,
      "learning_rate": 3.3650000000000005e-05,
      "loss": 0.0017,
      "step": 78480
    },
    {
      "epoch": 2.6163333333333334,
      "grad_norm": 0.1729443222284317,
      "learning_rate": 3.364791666666667e-05,
      "loss": 0.0026,
      "step": 78490
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.5266957879066467,
      "learning_rate": 3.3645833333333336e-05,
      "loss": 0.0021,
      "step": 78500
    },
    {
      "epoch": 2.617,
      "grad_norm": 0.08658263087272644,
      "learning_rate": 3.364375e-05,
      "loss": 0.0026,
      "step": 78510
    },
    {
      "epoch": 2.6173333333333333,
      "grad_norm": 0.5188756585121155,
      "learning_rate": 3.364166666666667e-05,
      "loss": 0.0023,
      "step": 78520
    },
    {
      "epoch": 2.6176666666666666,
      "grad_norm": 0.3170686364173889,
      "learning_rate": 3.363958333333333e-05,
      "loss": 0.002,
      "step": 78530
    },
    {
      "epoch": 2.618,
      "grad_norm": 0.23063911497592926,
      "learning_rate": 3.36375e-05,
      "loss": 0.0027,
      "step": 78540
    },
    {
      "epoch": 2.618333333333333,
      "grad_norm": 0.0578109547495842,
      "learning_rate": 3.363541666666667e-05,
      "loss": 0.0016,
      "step": 78550
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.3752879500389099,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0023,
      "step": 78560
    },
    {
      "epoch": 2.6189999999999998,
      "grad_norm": 0.004504983313381672,
      "learning_rate": 3.363125e-05,
      "loss": 0.002,
      "step": 78570
    },
    {
      "epoch": 2.6193333333333335,
      "grad_norm": 0.05777650699019432,
      "learning_rate": 3.3629166666666666e-05,
      "loss": 0.0018,
      "step": 78580
    },
    {
      "epoch": 2.619666666666667,
      "grad_norm": 0.34609803557395935,
      "learning_rate": 3.362708333333334e-05,
      "loss": 0.0029,
      "step": 78590
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.11533147841691971,
      "learning_rate": 3.3625000000000004e-05,
      "loss": 0.0023,
      "step": 78600
    },
    {
      "epoch": 2.6203333333333334,
      "grad_norm": 0.5369364023208618,
      "learning_rate": 3.362291666666667e-05,
      "loss": 0.0036,
      "step": 78610
    },
    {
      "epoch": 2.6206666666666667,
      "grad_norm": 0.3463747501373291,
      "learning_rate": 3.3620833333333335e-05,
      "loss": 0.002,
      "step": 78620
    },
    {
      "epoch": 2.621,
      "grad_norm": 0.058007631450891495,
      "learning_rate": 3.361875e-05,
      "loss": 0.0024,
      "step": 78630
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.12285515666007996,
      "learning_rate": 3.3616666666666666e-05,
      "loss": 0.0022,
      "step": 78640
    },
    {
      "epoch": 2.6216666666666666,
      "grad_norm": 0.02951100468635559,
      "learning_rate": 3.361458333333333e-05,
      "loss": 0.0022,
      "step": 78650
    },
    {
      "epoch": 2.622,
      "grad_norm": 0.4324461817741394,
      "learning_rate": 3.3612500000000004e-05,
      "loss": 0.0017,
      "step": 78660
    },
    {
      "epoch": 2.622333333333333,
      "grad_norm": 0.2884541153907776,
      "learning_rate": 3.361041666666667e-05,
      "loss": 0.0028,
      "step": 78670
    },
    {
      "epoch": 2.6226666666666665,
      "grad_norm": 0.2596699297428131,
      "learning_rate": 3.3608333333333335e-05,
      "loss": 0.0022,
      "step": 78680
    },
    {
      "epoch": 2.623,
      "grad_norm": 0.2883816659450531,
      "learning_rate": 3.360625e-05,
      "loss": 0.0013,
      "step": 78690
    },
    {
      "epoch": 2.623333333333333,
      "grad_norm": 0.24006322026252747,
      "learning_rate": 3.360416666666667e-05,
      "loss": 0.0027,
      "step": 78700
    },
    {
      "epoch": 2.623666666666667,
      "grad_norm": 0.5771874785423279,
      "learning_rate": 3.360208333333333e-05,
      "loss": 0.0027,
      "step": 78710
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.05775729566812515,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0027,
      "step": 78720
    },
    {
      "epoch": 2.6243333333333334,
      "grad_norm": 0.37512555718421936,
      "learning_rate": 3.359791666666667e-05,
      "loss": 0.0027,
      "step": 78730
    },
    {
      "epoch": 2.6246666666666667,
      "grad_norm": 0.17303137481212616,
      "learning_rate": 3.3595833333333335e-05,
      "loss": 0.0026,
      "step": 78740
    },
    {
      "epoch": 2.625,
      "grad_norm": 0.23097588121891022,
      "learning_rate": 3.359375e-05,
      "loss": 0.0021,
      "step": 78750
    },
    {
      "epoch": 2.6253333333333333,
      "grad_norm": 0.2593685984611511,
      "learning_rate": 3.3591666666666666e-05,
      "loss": 0.0024,
      "step": 78760
    },
    {
      "epoch": 2.6256666666666666,
      "grad_norm": 0.08667972683906555,
      "learning_rate": 3.358958333333334e-05,
      "loss": 0.0022,
      "step": 78770
    },
    {
      "epoch": 2.626,
      "grad_norm": 0.4326633810997009,
      "learning_rate": 3.3587499999999996e-05,
      "loss": 0.0024,
      "step": 78780
    },
    {
      "epoch": 2.626333333333333,
      "grad_norm": 0.23079761862754822,
      "learning_rate": 3.358541666666667e-05,
      "loss": 0.002,
      "step": 78790
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.3508175313472748,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.0019,
      "step": 78800
    },
    {
      "epoch": 2.627,
      "grad_norm": 0.31718361377716064,
      "learning_rate": 3.358125e-05,
      "loss": 0.0033,
      "step": 78810
    },
    {
      "epoch": 2.6273333333333335,
      "grad_norm": 0.08658356964588165,
      "learning_rate": 3.3579166666666665e-05,
      "loss": 0.0025,
      "step": 78820
    },
    {
      "epoch": 2.627666666666667,
      "grad_norm": 0.0036785441916435957,
      "learning_rate": 3.357708333333334e-05,
      "loss": 0.002,
      "step": 78830
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.0867137759923935,
      "learning_rate": 3.3575e-05,
      "loss": 0.0026,
      "step": 78840
    },
    {
      "epoch": 2.6283333333333334,
      "grad_norm": 0.005650052800774574,
      "learning_rate": 3.357291666666667e-05,
      "loss": 0.0019,
      "step": 78850
    },
    {
      "epoch": 2.6286666666666667,
      "grad_norm": 0.28849881887435913,
      "learning_rate": 3.3570833333333334e-05,
      "loss": 0.0018,
      "step": 78860
    },
    {
      "epoch": 2.629,
      "grad_norm": 0.1729385405778885,
      "learning_rate": 3.3568750000000006e-05,
      "loss": 0.002,
      "step": 78870
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.11547008156776428,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0029,
      "step": 78880
    },
    {
      "epoch": 2.6296666666666666,
      "grad_norm": 0.28905048966407776,
      "learning_rate": 3.356458333333333e-05,
      "loss": 0.0019,
      "step": 78890
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.05769774317741394,
      "learning_rate": 3.35625e-05,
      "loss": 0.002,
      "step": 78900
    },
    {
      "epoch": 2.630333333333333,
      "grad_norm": 0.5191316604614258,
      "learning_rate": 3.356041666666667e-05,
      "loss": 0.0016,
      "step": 78910
    },
    {
      "epoch": 2.6306666666666665,
      "grad_norm": 0.14431610703468323,
      "learning_rate": 3.3558333333333334e-05,
      "loss": 0.0036,
      "step": 78920
    },
    {
      "epoch": 2.6310000000000002,
      "grad_norm": 0.17312493920326233,
      "learning_rate": 3.355625e-05,
      "loss": 0.003,
      "step": 78930
    },
    {
      "epoch": 2.631333333333333,
      "grad_norm": 0.08650152385234833,
      "learning_rate": 3.355416666666667e-05,
      "loss": 0.0024,
      "step": 78940
    },
    {
      "epoch": 2.631666666666667,
      "grad_norm": 0.34618106484413147,
      "learning_rate": 3.355208333333334e-05,
      "loss": 0.0018,
      "step": 78950
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.004801574628800154,
      "learning_rate": 3.355e-05,
      "loss": 0.0023,
      "step": 78960
    },
    {
      "epoch": 2.6323333333333334,
      "grad_norm": 0.25948965549468994,
      "learning_rate": 3.354791666666667e-05,
      "loss": 0.002,
      "step": 78970
    },
    {
      "epoch": 2.6326666666666667,
      "grad_norm": 0.03134395554661751,
      "learning_rate": 3.354583333333334e-05,
      "loss": 0.0023,
      "step": 78980
    },
    {
      "epoch": 2.633,
      "grad_norm": 0.2593937814235687,
      "learning_rate": 3.354375e-05,
      "loss": 0.0026,
      "step": 78990
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.40372082591056824,
      "learning_rate": 3.3541666666666664e-05,
      "loss": 0.0019,
      "step": 79000
    },
    {
      "epoch": 2.6336666666666666,
      "grad_norm": 0.34599801898002625,
      "learning_rate": 3.3539583333333337e-05,
      "loss": 0.002,
      "step": 79010
    },
    {
      "epoch": 2.634,
      "grad_norm": 0.08732081204652786,
      "learning_rate": 3.35375e-05,
      "loss": 0.0017,
      "step": 79020
    },
    {
      "epoch": 2.634333333333333,
      "grad_norm": 0.5478411316871643,
      "learning_rate": 3.353541666666667e-05,
      "loss": 0.0026,
      "step": 79030
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.14415699243545532,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0014,
      "step": 79040
    },
    {
      "epoch": 2.635,
      "grad_norm": 0.3458772897720337,
      "learning_rate": 3.3531250000000005e-05,
      "loss": 0.0022,
      "step": 79050
    },
    {
      "epoch": 2.6353333333333335,
      "grad_norm": 0.8280576467514038,
      "learning_rate": 3.3529166666666664e-05,
      "loss": 0.0022,
      "step": 79060
    },
    {
      "epoch": 2.635666666666667,
      "grad_norm": 0.1730668991804123,
      "learning_rate": 3.3527083333333336e-05,
      "loss": 0.0018,
      "step": 79070
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.08706644922494888,
      "learning_rate": 3.3525e-05,
      "loss": 0.002,
      "step": 79080
    },
    {
      "epoch": 2.6363333333333334,
      "grad_norm": 0.05787457525730133,
      "learning_rate": 3.352291666666667e-05,
      "loss": 0.0016,
      "step": 79090
    },
    {
      "epoch": 2.6366666666666667,
      "grad_norm": 0.3754991292953491,
      "learning_rate": 3.352083333333333e-05,
      "loss": 0.0024,
      "step": 79100
    },
    {
      "epoch": 2.637,
      "grad_norm": 0.14411397278308868,
      "learning_rate": 3.3518750000000005e-05,
      "loss": 0.0036,
      "step": 79110
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.6054676175117493,
      "learning_rate": 3.351666666666667e-05,
      "loss": 0.0019,
      "step": 79120
    },
    {
      "epoch": 2.6376666666666666,
      "grad_norm": 0.26107341051101685,
      "learning_rate": 3.351458333333333e-05,
      "loss": 0.0028,
      "step": 79130
    },
    {
      "epoch": 2.638,
      "grad_norm": 0.17290998995304108,
      "learning_rate": 3.35125e-05,
      "loss": 0.0028,
      "step": 79140
    },
    {
      "epoch": 2.638333333333333,
      "grad_norm": 0.2228192239999771,
      "learning_rate": 3.351041666666667e-05,
      "loss": 0.0025,
      "step": 79150
    },
    {
      "epoch": 2.6386666666666665,
      "grad_norm": 0.14442645013332367,
      "learning_rate": 3.350833333333334e-05,
      "loss": 0.0027,
      "step": 79160
    },
    {
      "epoch": 2.6390000000000002,
      "grad_norm": 0.0866708904504776,
      "learning_rate": 3.350625e-05,
      "loss": 0.0021,
      "step": 79170
    },
    {
      "epoch": 2.639333333333333,
      "grad_norm": 0.17316287755966187,
      "learning_rate": 3.350416666666667e-05,
      "loss": 0.0021,
      "step": 79180
    },
    {
      "epoch": 2.639666666666667,
      "grad_norm": 0.029960280284285545,
      "learning_rate": 3.3502083333333336e-05,
      "loss": 0.002,
      "step": 79190
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.11525488644838333,
      "learning_rate": 3.35e-05,
      "loss": 0.0018,
      "step": 79200
    },
    {
      "epoch": 2.6403333333333334,
      "grad_norm": 0.14448627829551697,
      "learning_rate": 3.349791666666667e-05,
      "loss": 0.0027,
      "step": 79210
    },
    {
      "epoch": 2.6406666666666667,
      "grad_norm": 0.2596462666988373,
      "learning_rate": 3.349583333333334e-05,
      "loss": 0.0022,
      "step": 79220
    },
    {
      "epoch": 2.641,
      "grad_norm": 0.37945958971977234,
      "learning_rate": 3.3493750000000004e-05,
      "loss": 0.0018,
      "step": 79230
    },
    {
      "epoch": 2.6413333333333333,
      "grad_norm": 0.35722336173057556,
      "learning_rate": 3.349166666666666e-05,
      "loss": 0.0033,
      "step": 79240
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 0.19965940713882446,
      "learning_rate": 3.3489583333333335e-05,
      "loss": 0.0022,
      "step": 79250
    },
    {
      "epoch": 2.642,
      "grad_norm": 0.5188302993774414,
      "learning_rate": 3.34875e-05,
      "loss": 0.002,
      "step": 79260
    },
    {
      "epoch": 2.642333333333333,
      "grad_norm": 0.2019081860780716,
      "learning_rate": 3.3485416666666666e-05,
      "loss": 0.0029,
      "step": 79270
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.7181003093719482,
      "learning_rate": 3.348333333333333e-05,
      "loss": 0.0027,
      "step": 79280
    },
    {
      "epoch": 2.643,
      "grad_norm": 0.28847530484199524,
      "learning_rate": 3.3481250000000004e-05,
      "loss": 0.0028,
      "step": 79290
    },
    {
      "epoch": 2.6433333333333335,
      "grad_norm": 0.02903004363179207,
      "learning_rate": 3.347916666666667e-05,
      "loss": 0.0024,
      "step": 79300
    },
    {
      "epoch": 2.643666666666667,
      "grad_norm": 0.14060400426387787,
      "learning_rate": 3.3477083333333335e-05,
      "loss": 0.0016,
      "step": 79310
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.058027178049087524,
      "learning_rate": 3.3475e-05,
      "loss": 0.0029,
      "step": 79320
    },
    {
      "epoch": 2.6443333333333334,
      "grad_norm": 0.40351569652557373,
      "learning_rate": 3.347291666666667e-05,
      "loss": 0.0032,
      "step": 79330
    },
    {
      "epoch": 2.6446666666666667,
      "grad_norm": 0.19521209597587585,
      "learning_rate": 3.347083333333333e-05,
      "loss": 0.0017,
      "step": 79340
    },
    {
      "epoch": 2.645,
      "grad_norm": 0.00635534105822444,
      "learning_rate": 3.3468750000000004e-05,
      "loss": 0.0025,
      "step": 79350
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.2884680926799774,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0023,
      "step": 79360
    },
    {
      "epoch": 2.6456666666666666,
      "grad_norm": 0.23081612586975098,
      "learning_rate": 3.3464583333333335e-05,
      "loss": 0.0034,
      "step": 79370
    },
    {
      "epoch": 2.646,
      "grad_norm": 0.4611889123916626,
      "learning_rate": 3.34625e-05,
      "loss": 0.0026,
      "step": 79380
    },
    {
      "epoch": 2.646333333333333,
      "grad_norm": 0.2307155877351761,
      "learning_rate": 3.3460416666666666e-05,
      "loss": 0.0027,
      "step": 79390
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.11530695110559464,
      "learning_rate": 3.345833333333334e-05,
      "loss": 0.0022,
      "step": 79400
    },
    {
      "epoch": 2.6470000000000002,
      "grad_norm": 0.057891275733709335,
      "learning_rate": 3.3456250000000003e-05,
      "loss": 0.0029,
      "step": 79410
    },
    {
      "epoch": 2.647333333333333,
      "grad_norm": 0.4326624572277069,
      "learning_rate": 3.345416666666667e-05,
      "loss": 0.003,
      "step": 79420
    },
    {
      "epoch": 2.647666666666667,
      "grad_norm": 0.0867229551076889,
      "learning_rate": 3.3452083333333334e-05,
      "loss": 0.0027,
      "step": 79430
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.029300631955266,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.0034,
      "step": 79440
    },
    {
      "epoch": 2.6483333333333334,
      "grad_norm": 0.1439662128686905,
      "learning_rate": 3.3447916666666665e-05,
      "loss": 0.0036,
      "step": 79450
    },
    {
      "epoch": 2.6486666666666667,
      "grad_norm": 0.25911378860473633,
      "learning_rate": 3.344583333333334e-05,
      "loss": 0.003,
      "step": 79460
    },
    {
      "epoch": 2.649,
      "grad_norm": 0.14651738107204437,
      "learning_rate": 3.344375e-05,
      "loss": 0.0041,
      "step": 79470
    },
    {
      "epoch": 2.6493333333333333,
      "grad_norm": 0.1154375821352005,
      "learning_rate": 3.344166666666667e-05,
      "loss": 0.0037,
      "step": 79480
    },
    {
      "epoch": 2.6496666666666666,
      "grad_norm": 0.23063182830810547,
      "learning_rate": 3.3439583333333334e-05,
      "loss": 0.0019,
      "step": 79490
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.3579193651676178,
      "learning_rate": 3.34375e-05,
      "loss": 0.0031,
      "step": 79500
    },
    {
      "epoch": 2.650333333333333,
      "grad_norm": 0.11533097922801971,
      "learning_rate": 3.343541666666667e-05,
      "loss": 0.0022,
      "step": 79510
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.4589857757091522,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0018,
      "step": 79520
    },
    {
      "epoch": 2.651,
      "grad_norm": 0.37475451827049255,
      "learning_rate": 3.343125e-05,
      "loss": 0.0021,
      "step": 79530
    },
    {
      "epoch": 2.6513333333333335,
      "grad_norm": 0.23072175681591034,
      "learning_rate": 3.342916666666667e-05,
      "loss": 0.0027,
      "step": 79540
    },
    {
      "epoch": 2.6516666666666664,
      "grad_norm": 0.029677143320441246,
      "learning_rate": 3.3427083333333334e-05,
      "loss": 0.0021,
      "step": 79550
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.43238094449043274,
      "learning_rate": 3.3425e-05,
      "loss": 0.0024,
      "step": 79560
    },
    {
      "epoch": 2.6523333333333334,
      "grad_norm": 0.28840896487236023,
      "learning_rate": 3.342291666666667e-05,
      "loss": 0.0036,
      "step": 79570
    },
    {
      "epoch": 2.6526666666666667,
      "grad_norm": 0.14420048892498016,
      "learning_rate": 3.342083333333334e-05,
      "loss": 0.0019,
      "step": 79580
    },
    {
      "epoch": 2.653,
      "grad_norm": 0.20172438025474548,
      "learning_rate": 3.341875e-05,
      "loss": 0.0024,
      "step": 79590
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.17293758690357208,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.0033,
      "step": 79600
    },
    {
      "epoch": 2.6536666666666666,
      "grad_norm": 0.028956297785043716,
      "learning_rate": 3.3414583333333334e-05,
      "loss": 0.002,
      "step": 79610
    },
    {
      "epoch": 2.654,
      "grad_norm": 0.11526765674352646,
      "learning_rate": 3.34125e-05,
      "loss": 0.0025,
      "step": 79620
    },
    {
      "epoch": 2.654333333333333,
      "grad_norm": 0.05775915086269379,
      "learning_rate": 3.3410416666666664e-05,
      "loss": 0.0023,
      "step": 79630
    },
    {
      "epoch": 2.6546666666666665,
      "grad_norm": 0.2594505250453949,
      "learning_rate": 3.340833333333334e-05,
      "loss": 0.002,
      "step": 79640
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 0.17296743392944336,
      "learning_rate": 3.340625e-05,
      "loss": 0.0013,
      "step": 79650
    },
    {
      "epoch": 2.655333333333333,
      "grad_norm": 0.25952383875846863,
      "learning_rate": 3.340416666666667e-05,
      "loss": 0.0019,
      "step": 79660
    },
    {
      "epoch": 2.655666666666667,
      "grad_norm": 0.17304658889770508,
      "learning_rate": 3.340208333333333e-05,
      "loss": 0.0026,
      "step": 79670
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.005066889338195324,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0018,
      "step": 79680
    },
    {
      "epoch": 2.6563333333333334,
      "grad_norm": 0.25959786772727966,
      "learning_rate": 3.339791666666667e-05,
      "loss": 0.0028,
      "step": 79690
    },
    {
      "epoch": 2.6566666666666667,
      "grad_norm": 0.2595994770526886,
      "learning_rate": 3.3395833333333336e-05,
      "loss": 0.004,
      "step": 79700
    },
    {
      "epoch": 2.657,
      "grad_norm": 0.2880933880805969,
      "learning_rate": 3.339375e-05,
      "loss": 0.0026,
      "step": 79710
    },
    {
      "epoch": 2.6573333333333333,
      "grad_norm": 0.2594750225543976,
      "learning_rate": 3.339166666666667e-05,
      "loss": 0.0022,
      "step": 79720
    },
    {
      "epoch": 2.6576666666666666,
      "grad_norm": 0.05822552740573883,
      "learning_rate": 3.338958333333333e-05,
      "loss": 0.0025,
      "step": 79730
    },
    {
      "epoch": 2.658,
      "grad_norm": 0.2307128608226776,
      "learning_rate": 3.33875e-05,
      "loss": 0.0024,
      "step": 79740
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 0.28459224104881287,
      "learning_rate": 3.338541666666667e-05,
      "loss": 0.0028,
      "step": 79750
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.23085279762744904,
      "learning_rate": 3.3383333333333336e-05,
      "loss": 0.002,
      "step": 79760
    },
    {
      "epoch": 2.659,
      "grad_norm": 0.14428770542144775,
      "learning_rate": 3.338125e-05,
      "loss": 0.002,
      "step": 79770
    },
    {
      "epoch": 2.6593333333333335,
      "grad_norm": 0.40506666898727417,
      "learning_rate": 3.337916666666667e-05,
      "loss": 0.0023,
      "step": 79780
    },
    {
      "epoch": 2.6596666666666664,
      "grad_norm": 0.20412394404411316,
      "learning_rate": 3.337708333333334e-05,
      "loss": 0.0024,
      "step": 79790
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.057918015867471695,
      "learning_rate": 3.3375e-05,
      "loss": 0.0017,
      "step": 79800
    },
    {
      "epoch": 2.6603333333333334,
      "grad_norm": 0.11537247151136398,
      "learning_rate": 3.337291666666667e-05,
      "loss": 0.003,
      "step": 79810
    },
    {
      "epoch": 2.6606666666666667,
      "grad_norm": 0.14413492381572723,
      "learning_rate": 3.3370833333333336e-05,
      "loss": 0.0025,
      "step": 79820
    },
    {
      "epoch": 2.661,
      "grad_norm": 0.28848835825920105,
      "learning_rate": 3.336875e-05,
      "loss": 0.0027,
      "step": 79830
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.05805547907948494,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.0018,
      "step": 79840
    },
    {
      "epoch": 2.6616666666666666,
      "grad_norm": 0.6413791179656982,
      "learning_rate": 3.336458333333333e-05,
      "loss": 0.0018,
      "step": 79850
    },
    {
      "epoch": 2.662,
      "grad_norm": 0.6193315982818604,
      "learning_rate": 3.3362500000000005e-05,
      "loss": 0.0016,
      "step": 79860
    },
    {
      "epoch": 2.662333333333333,
      "grad_norm": 0.48887261748313904,
      "learning_rate": 3.336041666666666e-05,
      "loss": 0.0025,
      "step": 79870
    },
    {
      "epoch": 2.6626666666666665,
      "grad_norm": 0.05803276225924492,
      "learning_rate": 3.3358333333333336e-05,
      "loss": 0.0028,
      "step": 79880
    },
    {
      "epoch": 2.6630000000000003,
      "grad_norm": 0.6337088942527771,
      "learning_rate": 3.335625e-05,
      "loss": 0.0016,
      "step": 79890
    },
    {
      "epoch": 2.663333333333333,
      "grad_norm": 0.20176830887794495,
      "learning_rate": 3.3354166666666667e-05,
      "loss": 0.0018,
      "step": 79900
    },
    {
      "epoch": 2.663666666666667,
      "grad_norm": 0.8005821704864502,
      "learning_rate": 3.335208333333333e-05,
      "loss": 0.0025,
      "step": 79910
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.17313537001609802,
      "learning_rate": 3.3350000000000004e-05,
      "loss": 0.0026,
      "step": 79920
    },
    {
      "epoch": 2.6643333333333334,
      "grad_norm": 0.20171202719211578,
      "learning_rate": 3.334791666666667e-05,
      "loss": 0.003,
      "step": 79930
    },
    {
      "epoch": 2.6646666666666667,
      "grad_norm": 0.030001213774085045,
      "learning_rate": 3.3345833333333335e-05,
      "loss": 0.0029,
      "step": 79940
    },
    {
      "epoch": 2.665,
      "grad_norm": 0.3458622694015503,
      "learning_rate": 3.334375e-05,
      "loss": 0.003,
      "step": 79950
    },
    {
      "epoch": 2.6653333333333333,
      "grad_norm": 0.046177055686712265,
      "learning_rate": 3.3341666666666666e-05,
      "loss": 0.0023,
      "step": 79960
    },
    {
      "epoch": 2.6656666666666666,
      "grad_norm": 0.20195598900318146,
      "learning_rate": 3.333958333333334e-05,
      "loss": 0.0031,
      "step": 79970
    },
    {
      "epoch": 2.666,
      "grad_norm": 0.49014708399772644,
      "learning_rate": 3.33375e-05,
      "loss": 0.0032,
      "step": 79980
    },
    {
      "epoch": 2.666333333333333,
      "grad_norm": 0.34591761231422424,
      "learning_rate": 3.333541666666667e-05,
      "loss": 0.0033,
      "step": 79990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.2020944207906723,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0019,
      "step": 80000
    },
    {
      "epoch": 2.667,
      "grad_norm": 0.004398655612021685,
      "learning_rate": 3.333125e-05,
      "loss": 0.0026,
      "step": 80010
    },
    {
      "epoch": 2.6673333333333336,
      "grad_norm": 0.37490299344062805,
      "learning_rate": 3.3329166666666666e-05,
      "loss": 0.0027,
      "step": 80020
    },
    {
      "epoch": 2.6676666666666664,
      "grad_norm": 0.11551780253648758,
      "learning_rate": 3.332708333333334e-05,
      "loss": 0.0019,
      "step": 80030
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.28818830847740173,
      "learning_rate": 3.3325000000000004e-05,
      "loss": 0.0026,
      "step": 80040
    },
    {
      "epoch": 2.6683333333333334,
      "grad_norm": 0.029953017830848694,
      "learning_rate": 3.332291666666667e-05,
      "loss": 0.0029,
      "step": 80050
    },
    {
      "epoch": 2.6686666666666667,
      "grad_norm": 0.17281055450439453,
      "learning_rate": 3.3320833333333335e-05,
      "loss": 0.0023,
      "step": 80060
    },
    {
      "epoch": 2.669,
      "grad_norm": 0.004551049321889877,
      "learning_rate": 3.331875000000001e-05,
      "loss": 0.002,
      "step": 80070
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.17308588325977325,
      "learning_rate": 3.3316666666666666e-05,
      "loss": 0.0024,
      "step": 80080
    },
    {
      "epoch": 2.6696666666666666,
      "grad_norm": 0.02936740033328533,
      "learning_rate": 3.331458333333333e-05,
      "loss": 0.0026,
      "step": 80090
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.28810831904411316,
      "learning_rate": 3.33125e-05,
      "loss": 0.0017,
      "step": 80100
    },
    {
      "epoch": 2.6703333333333332,
      "grad_norm": 0.02916916087269783,
      "learning_rate": 3.331041666666667e-05,
      "loss": 0.001,
      "step": 80110
    },
    {
      "epoch": 2.6706666666666665,
      "grad_norm": 0.5190326571464539,
      "learning_rate": 3.3308333333333334e-05,
      "loss": 0.0029,
      "step": 80120
    },
    {
      "epoch": 2.6710000000000003,
      "grad_norm": 0.20201626420021057,
      "learning_rate": 3.330625e-05,
      "loss": 0.0021,
      "step": 80130
    },
    {
      "epoch": 2.671333333333333,
      "grad_norm": 0.1441313773393631,
      "learning_rate": 3.330416666666667e-05,
      "loss": 0.0018,
      "step": 80140
    },
    {
      "epoch": 2.671666666666667,
      "grad_norm": 0.3171754777431488,
      "learning_rate": 3.330208333333333e-05,
      "loss": 0.0026,
      "step": 80150
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.11540429294109344,
      "learning_rate": 3.33e-05,
      "loss": 0.0032,
      "step": 80160
    },
    {
      "epoch": 2.6723333333333334,
      "grad_norm": 0.08676343411207199,
      "learning_rate": 3.329791666666667e-05,
      "loss": 0.0027,
      "step": 80170
    },
    {
      "epoch": 2.6726666666666667,
      "grad_norm": 0.02915467880666256,
      "learning_rate": 3.3295833333333334e-05,
      "loss": 0.0034,
      "step": 80180
    },
    {
      "epoch": 2.673,
      "grad_norm": 0.057875096797943115,
      "learning_rate": 3.329375e-05,
      "loss": 0.0032,
      "step": 80190
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.20156967639923096,
      "learning_rate": 3.329166666666667e-05,
      "loss": 0.0026,
      "step": 80200
    },
    {
      "epoch": 2.6736666666666666,
      "grad_norm": 0.28822508454322815,
      "learning_rate": 3.328958333333334e-05,
      "loss": 0.0023,
      "step": 80210
    },
    {
      "epoch": 2.674,
      "grad_norm": 0.1729862540960312,
      "learning_rate": 3.3287499999999996e-05,
      "loss": 0.0029,
      "step": 80220
    },
    {
      "epoch": 2.6743333333333332,
      "grad_norm": 0.10688477754592896,
      "learning_rate": 3.328541666666667e-05,
      "loss": 0.0032,
      "step": 80230
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.19587565958499908,
      "learning_rate": 3.3283333333333334e-05,
      "loss": 0.0025,
      "step": 80240
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.46202775835990906,
      "learning_rate": 3.3281250000000006e-05,
      "loss": 0.0024,
      "step": 80250
    },
    {
      "epoch": 2.6753333333333336,
      "grad_norm": 0.11531998217105865,
      "learning_rate": 3.3279166666666665e-05,
      "loss": 0.0028,
      "step": 80260
    },
    {
      "epoch": 2.6756666666666664,
      "grad_norm": 0.11549698561429977,
      "learning_rate": 3.327708333333334e-05,
      "loss": 0.003,
      "step": 80270
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.08657465130090714,
      "learning_rate": 3.3275e-05,
      "loss": 0.0021,
      "step": 80280
    },
    {
      "epoch": 2.6763333333333335,
      "grad_norm": 0.4567316472530365,
      "learning_rate": 3.327291666666667e-05,
      "loss": 0.0034,
      "step": 80290
    },
    {
      "epoch": 2.6766666666666667,
      "grad_norm": 0.11552740633487701,
      "learning_rate": 3.3270833333333333e-05,
      "loss": 0.0023,
      "step": 80300
    },
    {
      "epoch": 2.677,
      "grad_norm": 0.057991206645965576,
      "learning_rate": 3.3268750000000006e-05,
      "loss": 0.0025,
      "step": 80310
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.23838487267494202,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0023,
      "step": 80320
    },
    {
      "epoch": 2.6776666666666666,
      "grad_norm": 0.11534826457500458,
      "learning_rate": 3.326458333333333e-05,
      "loss": 0.0021,
      "step": 80330
    },
    {
      "epoch": 2.678,
      "grad_norm": 0.37459418177604675,
      "learning_rate": 3.32625e-05,
      "loss": 0.0019,
      "step": 80340
    },
    {
      "epoch": 2.6783333333333332,
      "grad_norm": 0.4034966826438904,
      "learning_rate": 3.326041666666667e-05,
      "loss": 0.0032,
      "step": 80350
    },
    {
      "epoch": 2.6786666666666665,
      "grad_norm": 0.0578148327767849,
      "learning_rate": 3.325833333333333e-05,
      "loss": 0.002,
      "step": 80360
    },
    {
      "epoch": 2.6790000000000003,
      "grad_norm": 0.1729489415884018,
      "learning_rate": 3.325625e-05,
      "loss": 0.0024,
      "step": 80370
    },
    {
      "epoch": 2.679333333333333,
      "grad_norm": 0.05799731984734535,
      "learning_rate": 3.325416666666667e-05,
      "loss": 0.0019,
      "step": 80380
    },
    {
      "epoch": 2.679666666666667,
      "grad_norm": 0.46672773361206055,
      "learning_rate": 3.3252083333333336e-05,
      "loss": 0.0022,
      "step": 80390
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.14440450072288513,
      "learning_rate": 3.325e-05,
      "loss": 0.0023,
      "step": 80400
    },
    {
      "epoch": 2.6803333333333335,
      "grad_norm": 0.31691673398017883,
      "learning_rate": 3.324791666666667e-05,
      "loss": 0.0017,
      "step": 80410
    },
    {
      "epoch": 2.6806666666666668,
      "grad_norm": 0.08644711226224899,
      "learning_rate": 3.324583333333334e-05,
      "loss": 0.003,
      "step": 80420
    },
    {
      "epoch": 2.681,
      "grad_norm": 0.23057343065738678,
      "learning_rate": 3.324375e-05,
      "loss": 0.0019,
      "step": 80430
    },
    {
      "epoch": 2.6813333333333333,
      "grad_norm": 0.05797161906957626,
      "learning_rate": 3.324166666666667e-05,
      "loss": 0.0021,
      "step": 80440
    },
    {
      "epoch": 2.6816666666666666,
      "grad_norm": 0.6050752401351929,
      "learning_rate": 3.3239583333333336e-05,
      "loss": 0.0023,
      "step": 80450
    },
    {
      "epoch": 2.682,
      "grad_norm": 0.7663149833679199,
      "learning_rate": 3.32375e-05,
      "loss": 0.0027,
      "step": 80460
    },
    {
      "epoch": 2.6823333333333332,
      "grad_norm": 0.3457377254962921,
      "learning_rate": 3.323541666666667e-05,
      "loss": 0.0034,
      "step": 80470
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.057838764041662216,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0028,
      "step": 80480
    },
    {
      "epoch": 2.683,
      "grad_norm": 0.378481924533844,
      "learning_rate": 3.3231250000000005e-05,
      "loss": 0.0027,
      "step": 80490
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.05810467153787613,
      "learning_rate": 3.3229166666666663e-05,
      "loss": 0.0011,
      "step": 80500
    },
    {
      "epoch": 2.6836666666666664,
      "grad_norm": 0.31715908646583557,
      "learning_rate": 3.3227083333333336e-05,
      "loss": 0.0027,
      "step": 80510
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.4922798275947571,
      "learning_rate": 3.3225e-05,
      "loss": 0.003,
      "step": 80520
    },
    {
      "epoch": 2.6843333333333335,
      "grad_norm": 0.20204409956932068,
      "learning_rate": 3.3222916666666673e-05,
      "loss": 0.0011,
      "step": 80530
    },
    {
      "epoch": 2.6846666666666668,
      "grad_norm": 0.03003883734345436,
      "learning_rate": 3.322083333333333e-05,
      "loss": 0.0027,
      "step": 80540
    },
    {
      "epoch": 2.685,
      "grad_norm": 0.057756759226322174,
      "learning_rate": 3.3218750000000004e-05,
      "loss": 0.0017,
      "step": 80550
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.1157044991850853,
      "learning_rate": 3.321666666666667e-05,
      "loss": 0.0014,
      "step": 80560
    },
    {
      "epoch": 2.6856666666666666,
      "grad_norm": 0.23115748167037964,
      "learning_rate": 3.3214583333333335e-05,
      "loss": 0.0031,
      "step": 80570
    },
    {
      "epoch": 2.686,
      "grad_norm": 0.4037054181098938,
      "learning_rate": 3.32125e-05,
      "loss": 0.0034,
      "step": 80580
    },
    {
      "epoch": 2.6863333333333332,
      "grad_norm": 0.08672048896551132,
      "learning_rate": 3.3210416666666666e-05,
      "loss": 0.0027,
      "step": 80590
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.05869284272193909,
      "learning_rate": 3.320833333333334e-05,
      "loss": 0.0026,
      "step": 80600
    },
    {
      "epoch": 2.6870000000000003,
      "grad_norm": 0.08660552650690079,
      "learning_rate": 3.320625e-05,
      "loss": 0.0026,
      "step": 80610
    },
    {
      "epoch": 2.687333333333333,
      "grad_norm": 0.08652263879776001,
      "learning_rate": 3.320416666666667e-05,
      "loss": 0.002,
      "step": 80620
    },
    {
      "epoch": 2.687666666666667,
      "grad_norm": 0.02882835641503334,
      "learning_rate": 3.3202083333333335e-05,
      "loss": 0.0017,
      "step": 80630
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.4322175085544586,
      "learning_rate": 3.32e-05,
      "loss": 0.0027,
      "step": 80640
    },
    {
      "epoch": 2.6883333333333335,
      "grad_norm": 0.02909923903644085,
      "learning_rate": 3.3197916666666666e-05,
      "loss": 0.0016,
      "step": 80650
    },
    {
      "epoch": 2.6886666666666668,
      "grad_norm": 0.14406849443912506,
      "learning_rate": 3.319583333333334e-05,
      "loss": 0.0021,
      "step": 80660
    },
    {
      "epoch": 2.689,
      "grad_norm": 0.05775066465139389,
      "learning_rate": 3.3193750000000004e-05,
      "loss": 0.0023,
      "step": 80670
    },
    {
      "epoch": 2.6893333333333334,
      "grad_norm": 0.3170234262943268,
      "learning_rate": 3.319166666666667e-05,
      "loss": 0.0018,
      "step": 80680
    },
    {
      "epoch": 2.6896666666666667,
      "grad_norm": 0.31698310375213623,
      "learning_rate": 3.3189583333333335e-05,
      "loss": 0.0024,
      "step": 80690
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.05789333954453468,
      "learning_rate": 3.31875e-05,
      "loss": 0.0026,
      "step": 80700
    },
    {
      "epoch": 2.6903333333333332,
      "grad_norm": 0.00546879880130291,
      "learning_rate": 3.3185416666666666e-05,
      "loss": 0.0029,
      "step": 80710
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.34598442912101746,
      "learning_rate": 3.318333333333333e-05,
      "loss": 0.0029,
      "step": 80720
    },
    {
      "epoch": 2.691,
      "grad_norm": 0.23042148351669312,
      "learning_rate": 3.3181250000000004e-05,
      "loss": 0.0024,
      "step": 80730
    },
    {
      "epoch": 2.6913333333333336,
      "grad_norm": 0.4365975260734558,
      "learning_rate": 3.317916666666667e-05,
      "loss": 0.003,
      "step": 80740
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 0.4902648329734802,
      "learning_rate": 3.3177083333333335e-05,
      "loss": 0.0034,
      "step": 80750
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.029399659484624863,
      "learning_rate": 3.3175e-05,
      "loss": 0.0024,
      "step": 80760
    },
    {
      "epoch": 2.6923333333333335,
      "grad_norm": 1.6382837295532227,
      "learning_rate": 3.317291666666667e-05,
      "loss": 0.0031,
      "step": 80770
    },
    {
      "epoch": 2.6926666666666668,
      "grad_norm": 0.08682233840227127,
      "learning_rate": 3.317083333333333e-05,
      "loss": 0.0031,
      "step": 80780
    },
    {
      "epoch": 2.693,
      "grad_norm": 0.11543986946344376,
      "learning_rate": 3.316875e-05,
      "loss": 0.0026,
      "step": 80790
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.008172220550477505,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0029,
      "step": 80800
    },
    {
      "epoch": 2.6936666666666667,
      "grad_norm": 0.0865393728017807,
      "learning_rate": 3.3164583333333334e-05,
      "loss": 0.003,
      "step": 80810
    },
    {
      "epoch": 2.694,
      "grad_norm": 0.34598368406295776,
      "learning_rate": 3.31625e-05,
      "loss": 0.0026,
      "step": 80820
    },
    {
      "epoch": 2.6943333333333332,
      "grad_norm": 0.40339550375938416,
      "learning_rate": 3.3160416666666665e-05,
      "loss": 0.0013,
      "step": 80830
    },
    {
      "epoch": 2.6946666666666665,
      "grad_norm": 0.05886323004961014,
      "learning_rate": 3.315833333333334e-05,
      "loss": 0.0014,
      "step": 80840
    },
    {
      "epoch": 2.695,
      "grad_norm": 0.15035921335220337,
      "learning_rate": 3.315625e-05,
      "loss": 0.0021,
      "step": 80850
    },
    {
      "epoch": 2.695333333333333,
      "grad_norm": 0.3170645236968994,
      "learning_rate": 3.315416666666667e-05,
      "loss": 0.0023,
      "step": 80860
    },
    {
      "epoch": 2.695666666666667,
      "grad_norm": 0.1732700914144516,
      "learning_rate": 3.3152083333333334e-05,
      "loss": 0.0029,
      "step": 80870
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.2879957854747772,
      "learning_rate": 3.3150000000000006e-05,
      "loss": 0.0033,
      "step": 80880
    },
    {
      "epoch": 2.6963333333333335,
      "grad_norm": 0.005697358399629593,
      "learning_rate": 3.3147916666666665e-05,
      "loss": 0.0022,
      "step": 80890
    },
    {
      "epoch": 2.6966666666666668,
      "grad_norm": 0.029484478756785393,
      "learning_rate": 3.314583333333334e-05,
      "loss": 0.0016,
      "step": 80900
    },
    {
      "epoch": 2.697,
      "grad_norm": 0.23070700466632843,
      "learning_rate": 3.314375e-05,
      "loss": 0.0029,
      "step": 80910
    },
    {
      "epoch": 2.6973333333333334,
      "grad_norm": 0.08683215081691742,
      "learning_rate": 3.314166666666667e-05,
      "loss": 0.0017,
      "step": 80920
    },
    {
      "epoch": 2.6976666666666667,
      "grad_norm": 0.1730775386095047,
      "learning_rate": 3.3139583333333334e-05,
      "loss": 0.0019,
      "step": 80930
    },
    {
      "epoch": 2.698,
      "grad_norm": 0.02938043512403965,
      "learning_rate": 3.31375e-05,
      "loss": 0.0024,
      "step": 80940
    },
    {
      "epoch": 2.6983333333333333,
      "grad_norm": 0.017020095139741898,
      "learning_rate": 3.313541666666667e-05,
      "loss": 0.0023,
      "step": 80950
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.17295825481414795,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0022,
      "step": 80960
    },
    {
      "epoch": 2.699,
      "grad_norm": 0.08706662803888321,
      "learning_rate": 3.313125e-05,
      "loss": 0.0038,
      "step": 80970
    },
    {
      "epoch": 2.6993333333333336,
      "grad_norm": 0.057981476187705994,
      "learning_rate": 3.312916666666667e-05,
      "loss": 0.0017,
      "step": 80980
    },
    {
      "epoch": 2.6996666666666664,
      "grad_norm": 0.1440427005290985,
      "learning_rate": 3.312708333333333e-05,
      "loss": 0.0031,
      "step": 80990
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.029203543439507484,
      "learning_rate": 3.3125e-05,
      "loss": 0.002,
      "step": 81000
    },
    {
      "epoch": 2.7003333333333335,
      "grad_norm": 0.37451815605163574,
      "learning_rate": 3.312291666666667e-05,
      "loss": 0.0032,
      "step": 81010
    },
    {
      "epoch": 2.7006666666666668,
      "grad_norm": 0.02932812087237835,
      "learning_rate": 3.3120833333333337e-05,
      "loss": 0.0019,
      "step": 81020
    },
    {
      "epoch": 2.701,
      "grad_norm": 0.1441413164138794,
      "learning_rate": 3.311875e-05,
      "loss": 0.0017,
      "step": 81030
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.17311225831508636,
      "learning_rate": 3.311666666666667e-05,
      "loss": 0.0021,
      "step": 81040
    },
    {
      "epoch": 2.7016666666666667,
      "grad_norm": 0.057896096259355545,
      "learning_rate": 3.311458333333333e-05,
      "loss": 0.0016,
      "step": 81050
    },
    {
      "epoch": 2.702,
      "grad_norm": 0.23037753999233246,
      "learning_rate": 3.31125e-05,
      "loss": 0.0026,
      "step": 81060
    },
    {
      "epoch": 2.7023333333333333,
      "grad_norm": 0.40320807695388794,
      "learning_rate": 3.3110416666666664e-05,
      "loss": 0.0022,
      "step": 81070
    },
    {
      "epoch": 2.7026666666666666,
      "grad_norm": 0.029456356540322304,
      "learning_rate": 3.3108333333333336e-05,
      "loss": 0.0026,
      "step": 81080
    },
    {
      "epoch": 2.703,
      "grad_norm": 0.2017458826303482,
      "learning_rate": 3.310625e-05,
      "loss": 0.0018,
      "step": 81090
    },
    {
      "epoch": 2.703333333333333,
      "grad_norm": 0.2304592877626419,
      "learning_rate": 3.310416666666667e-05,
      "loss": 0.0023,
      "step": 81100
    },
    {
      "epoch": 2.703666666666667,
      "grad_norm": 0.11583703011274338,
      "learning_rate": 3.310208333333333e-05,
      "loss": 0.0018,
      "step": 81110
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.11536990851163864,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.004,
      "step": 81120
    },
    {
      "epoch": 2.7043333333333335,
      "grad_norm": 0.2594108581542969,
      "learning_rate": 3.309791666666667e-05,
      "loss": 0.0028,
      "step": 81130
    },
    {
      "epoch": 2.7046666666666668,
      "grad_norm": 0.08661419153213501,
      "learning_rate": 3.3095833333333336e-05,
      "loss": 0.0014,
      "step": 81140
    },
    {
      "epoch": 2.705,
      "grad_norm": 0.34571126103401184,
      "learning_rate": 3.309375e-05,
      "loss": 0.0027,
      "step": 81150
    },
    {
      "epoch": 2.7053333333333334,
      "grad_norm": 0.030318252742290497,
      "learning_rate": 3.3091666666666674e-05,
      "loss": 0.003,
      "step": 81160
    },
    {
      "epoch": 2.7056666666666667,
      "grad_norm": 0.6338489651679993,
      "learning_rate": 3.308958333333333e-05,
      "loss": 0.0026,
      "step": 81170
    },
    {
      "epoch": 2.706,
      "grad_norm": 0.3746717572212219,
      "learning_rate": 3.30875e-05,
      "loss": 0.0026,
      "step": 81180
    },
    {
      "epoch": 2.7063333333333333,
      "grad_norm": 0.43211016058921814,
      "learning_rate": 3.308541666666667e-05,
      "loss": 0.0023,
      "step": 81190
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.3455789089202881,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.0021,
      "step": 81200
    },
    {
      "epoch": 2.707,
      "grad_norm": 0.0039088912308216095,
      "learning_rate": 3.308125e-05,
      "loss": 0.002,
      "step": 81210
    },
    {
      "epoch": 2.7073333333333336,
      "grad_norm": 0.0865805372595787,
      "learning_rate": 3.3079166666666667e-05,
      "loss": 0.0026,
      "step": 81220
    },
    {
      "epoch": 2.7076666666666664,
      "grad_norm": 0.005375250708311796,
      "learning_rate": 3.307708333333334e-05,
      "loss": 0.0028,
      "step": 81230
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.6626909375190735,
      "learning_rate": 3.3075e-05,
      "loss": 0.0019,
      "step": 81240
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 0.11551738530397415,
      "learning_rate": 3.307291666666667e-05,
      "loss": 0.0021,
      "step": 81250
    },
    {
      "epoch": 2.708666666666667,
      "grad_norm": 0.3456537127494812,
      "learning_rate": 3.3070833333333335e-05,
      "loss": 0.0025,
      "step": 81260
    },
    {
      "epoch": 2.709,
      "grad_norm": 0.0873134657740593,
      "learning_rate": 3.306875e-05,
      "loss": 0.0012,
      "step": 81270
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.11545023322105408,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0019,
      "step": 81280
    },
    {
      "epoch": 2.7096666666666667,
      "grad_norm": 0.3671208620071411,
      "learning_rate": 3.306458333333334e-05,
      "loss": 0.002,
      "step": 81290
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.4835076928138733,
      "learning_rate": 3.3062500000000004e-05,
      "loss": 0.0022,
      "step": 81300
    },
    {
      "epoch": 2.7103333333333333,
      "grad_norm": 0.007305935490876436,
      "learning_rate": 3.306041666666666e-05,
      "loss": 0.0028,
      "step": 81310
    },
    {
      "epoch": 2.7106666666666666,
      "grad_norm": 0.3170093595981598,
      "learning_rate": 3.3058333333333335e-05,
      "loss": 0.0018,
      "step": 81320
    },
    {
      "epoch": 2.711,
      "grad_norm": 0.28813767433166504,
      "learning_rate": 3.305625e-05,
      "loss": 0.0027,
      "step": 81330
    },
    {
      "epoch": 2.711333333333333,
      "grad_norm": 0.9331443309783936,
      "learning_rate": 3.305416666666667e-05,
      "loss": 0.0024,
      "step": 81340
    },
    {
      "epoch": 2.711666666666667,
      "grad_norm": 0.2307504564523697,
      "learning_rate": 3.305208333333333e-05,
      "loss": 0.0016,
      "step": 81350
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.028915468603372574,
      "learning_rate": 3.3050000000000004e-05,
      "loss": 0.0023,
      "step": 81360
    },
    {
      "epoch": 2.7123333333333335,
      "grad_norm": 0.05791309475898743,
      "learning_rate": 3.304791666666667e-05,
      "loss": 0.0024,
      "step": 81370
    },
    {
      "epoch": 2.712666666666667,
      "grad_norm": 0.1151939183473587,
      "learning_rate": 3.3045833333333335e-05,
      "loss": 0.0031,
      "step": 81380
    },
    {
      "epoch": 2.713,
      "grad_norm": 0.058022841811180115,
      "learning_rate": 3.304375e-05,
      "loss": 0.0023,
      "step": 81390
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.14421148598194122,
      "learning_rate": 3.304166666666667e-05,
      "loss": 0.0029,
      "step": 81400
    },
    {
      "epoch": 2.7136666666666667,
      "grad_norm": 0.3168179392814636,
      "learning_rate": 3.303958333333334e-05,
      "loss": 0.0022,
      "step": 81410
    },
    {
      "epoch": 2.714,
      "grad_norm": 0.3743618130683899,
      "learning_rate": 3.30375e-05,
      "loss": 0.0021,
      "step": 81420
    },
    {
      "epoch": 2.7143333333333333,
      "grad_norm": 0.1442914605140686,
      "learning_rate": 3.303541666666667e-05,
      "loss": 0.0018,
      "step": 81430
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.02915961854159832,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.002,
      "step": 81440
    },
    {
      "epoch": 2.715,
      "grad_norm": 0.004470840096473694,
      "learning_rate": 3.303125e-05,
      "loss": 0.0021,
      "step": 81450
    },
    {
      "epoch": 2.7153333333333336,
      "grad_norm": 0.11569532006978989,
      "learning_rate": 3.3029166666666665e-05,
      "loss": 0.0021,
      "step": 81460
    },
    {
      "epoch": 2.7156666666666665,
      "grad_norm": 0.02930930256843567,
      "learning_rate": 3.302708333333334e-05,
      "loss": 0.0018,
      "step": 81470
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.08689616620540619,
      "learning_rate": 3.3025e-05,
      "loss": 0.0025,
      "step": 81480
    },
    {
      "epoch": 2.7163333333333335,
      "grad_norm": 0.08687884360551834,
      "learning_rate": 3.302291666666667e-05,
      "loss": 0.0017,
      "step": 81490
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 0.11561213433742523,
      "learning_rate": 3.3020833333333334e-05,
      "loss": 0.0018,
      "step": 81500
    },
    {
      "epoch": 2.717,
      "grad_norm": 0.14428800344467163,
      "learning_rate": 3.3018750000000006e-05,
      "loss": 0.0018,
      "step": 81510
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.2305280864238739,
      "learning_rate": 3.3016666666666665e-05,
      "loss": 0.0022,
      "step": 81520
    },
    {
      "epoch": 2.7176666666666667,
      "grad_norm": 0.461159884929657,
      "learning_rate": 3.301458333333334e-05,
      "loss": 0.0023,
      "step": 81530
    },
    {
      "epoch": 2.718,
      "grad_norm": 0.05753722041845322,
      "learning_rate": 3.30125e-05,
      "loss": 0.0017,
      "step": 81540
    },
    {
      "epoch": 2.7183333333333333,
      "grad_norm": 0.08656001836061478,
      "learning_rate": 3.301041666666667e-05,
      "loss": 0.0025,
      "step": 81550
    },
    {
      "epoch": 2.7186666666666666,
      "grad_norm": 0.5472877621650696,
      "learning_rate": 3.3008333333333334e-05,
      "loss": 0.0013,
      "step": 81560
    },
    {
      "epoch": 2.719,
      "grad_norm": 0.003825069172307849,
      "learning_rate": 3.300625e-05,
      "loss": 0.0022,
      "step": 81570
    },
    {
      "epoch": 2.719333333333333,
      "grad_norm": 0.28824591636657715,
      "learning_rate": 3.300416666666667e-05,
      "loss": 0.002,
      "step": 81580
    },
    {
      "epoch": 2.719666666666667,
      "grad_norm": 0.006615345366299152,
      "learning_rate": 3.300208333333333e-05,
      "loss": 0.0031,
      "step": 81590
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.08651509135961533,
      "learning_rate": 3.3e-05,
      "loss": 0.0017,
      "step": 81600
    },
    {
      "epoch": 2.7203333333333335,
      "grad_norm": 0.23053507506847382,
      "learning_rate": 3.299791666666667e-05,
      "loss": 0.0026,
      "step": 81610
    },
    {
      "epoch": 2.720666666666667,
      "grad_norm": 0.08655371516942978,
      "learning_rate": 3.299583333333334e-05,
      "loss": 0.002,
      "step": 81620
    },
    {
      "epoch": 2.721,
      "grad_norm": 0.08698488771915436,
      "learning_rate": 3.299375e-05,
      "loss": 0.0026,
      "step": 81630
    },
    {
      "epoch": 2.7213333333333334,
      "grad_norm": 0.34615230560302734,
      "learning_rate": 3.299166666666667e-05,
      "loss": 0.0021,
      "step": 81640
    },
    {
      "epoch": 2.7216666666666667,
      "grad_norm": 0.3455660343170166,
      "learning_rate": 3.298958333333334e-05,
      "loss": 0.0021,
      "step": 81650
    },
    {
      "epoch": 2.722,
      "grad_norm": 0.1734379529953003,
      "learning_rate": 3.29875e-05,
      "loss": 0.0023,
      "step": 81660
    },
    {
      "epoch": 2.7223333333333333,
      "grad_norm": 0.02908875234425068,
      "learning_rate": 3.298541666666667e-05,
      "loss": 0.0025,
      "step": 81670
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.345840185880661,
      "learning_rate": 3.298333333333333e-05,
      "loss": 0.0018,
      "step": 81680
    },
    {
      "epoch": 2.723,
      "grad_norm": 0.25920572876930237,
      "learning_rate": 3.2981250000000005e-05,
      "loss": 0.0016,
      "step": 81690
    },
    {
      "epoch": 2.7233333333333336,
      "grad_norm": 0.2302527129650116,
      "learning_rate": 3.2979166666666664e-05,
      "loss": 0.0014,
      "step": 81700
    },
    {
      "epoch": 2.7236666666666665,
      "grad_norm": 0.17278949916362762,
      "learning_rate": 3.2977083333333336e-05,
      "loss": 0.0025,
      "step": 81710
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.11741466075181961,
      "learning_rate": 3.2975e-05,
      "loss": 0.0024,
      "step": 81720
    },
    {
      "epoch": 2.7243333333333335,
      "grad_norm": 0.23120591044425964,
      "learning_rate": 3.297291666666667e-05,
      "loss": 0.0035,
      "step": 81730
    },
    {
      "epoch": 2.724666666666667,
      "grad_norm": 0.31703102588653564,
      "learning_rate": 3.297083333333333e-05,
      "loss": 0.0017,
      "step": 81740
    },
    {
      "epoch": 2.725,
      "grad_norm": 0.17327575385570526,
      "learning_rate": 3.2968750000000005e-05,
      "loss": 0.0021,
      "step": 81750
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.030259033665060997,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0024,
      "step": 81760
    },
    {
      "epoch": 2.7256666666666667,
      "grad_norm": 0.34575897455215454,
      "learning_rate": 3.2964583333333336e-05,
      "loss": 0.0021,
      "step": 81770
    },
    {
      "epoch": 2.726,
      "grad_norm": 0.08647523075342178,
      "learning_rate": 3.29625e-05,
      "loss": 0.0017,
      "step": 81780
    },
    {
      "epoch": 2.7263333333333333,
      "grad_norm": 0.11531662940979004,
      "learning_rate": 3.296041666666667e-05,
      "loss": 0.0022,
      "step": 81790
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.288429856300354,
      "learning_rate": 3.295833333333333e-05,
      "loss": 0.0023,
      "step": 81800
    },
    {
      "epoch": 2.727,
      "grad_norm": 0.28801727294921875,
      "learning_rate": 3.295625e-05,
      "loss": 0.0021,
      "step": 81810
    },
    {
      "epoch": 2.727333333333333,
      "grad_norm": 0.09674510359764099,
      "learning_rate": 3.295416666666667e-05,
      "loss": 0.0018,
      "step": 81820
    },
    {
      "epoch": 2.727666666666667,
      "grad_norm": 0.17339907586574554,
      "learning_rate": 3.2952083333333336e-05,
      "loss": 0.0038,
      "step": 81830
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.057907503098249435,
      "learning_rate": 3.295e-05,
      "loss": 0.0024,
      "step": 81840
    },
    {
      "epoch": 2.7283333333333335,
      "grad_norm": 0.17299841344356537,
      "learning_rate": 3.294791666666667e-05,
      "loss": 0.0035,
      "step": 81850
    },
    {
      "epoch": 2.728666666666667,
      "grad_norm": 0.010836031287908554,
      "learning_rate": 3.294583333333334e-05,
      "loss": 0.0026,
      "step": 81860
    },
    {
      "epoch": 2.729,
      "grad_norm": 0.7673627138137817,
      "learning_rate": 3.294375e-05,
      "loss": 0.0023,
      "step": 81870
    },
    {
      "epoch": 2.7293333333333334,
      "grad_norm": 0.05771137401461601,
      "learning_rate": 3.294166666666667e-05,
      "loss": 0.002,
      "step": 81880
    },
    {
      "epoch": 2.7296666666666667,
      "grad_norm": 0.20151348412036896,
      "learning_rate": 3.2939583333333336e-05,
      "loss": 0.0016,
      "step": 81890
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.08686921745538712,
      "learning_rate": 3.29375e-05,
      "loss": 0.0019,
      "step": 81900
    },
    {
      "epoch": 2.7303333333333333,
      "grad_norm": 0.11536133289337158,
      "learning_rate": 3.2935416666666666e-05,
      "loss": 0.0028,
      "step": 81910
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.20174281299114227,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0026,
      "step": 81920
    },
    {
      "epoch": 2.731,
      "grad_norm": 0.3745131194591522,
      "learning_rate": 3.2931250000000004e-05,
      "loss": 0.0029,
      "step": 81930
    },
    {
      "epoch": 2.731333333333333,
      "grad_norm": 0.029276592656970024,
      "learning_rate": 3.292916666666667e-05,
      "loss": 0.003,
      "step": 81940
    },
    {
      "epoch": 2.7316666666666665,
      "grad_norm": 0.37436676025390625,
      "learning_rate": 3.2927083333333335e-05,
      "loss": 0.0027,
      "step": 81950
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.029531782492995262,
      "learning_rate": 3.2925e-05,
      "loss": 0.0021,
      "step": 81960
    },
    {
      "epoch": 2.732333333333333,
      "grad_norm": 0.11515694111585617,
      "learning_rate": 3.292291666666667e-05,
      "loss": 0.0026,
      "step": 81970
    },
    {
      "epoch": 2.732666666666667,
      "grad_norm": 0.5758782625198364,
      "learning_rate": 3.292083333333333e-05,
      "loss": 0.0021,
      "step": 81980
    },
    {
      "epoch": 2.733,
      "grad_norm": 0.1439497172832489,
      "learning_rate": 3.2918750000000004e-05,
      "loss": 0.0026,
      "step": 81990
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.23060283064842224,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.0017,
      "step": 82000
    },
    {
      "epoch": 2.7336666666666667,
      "grad_norm": 0.0875948816537857,
      "learning_rate": 3.2914583333333335e-05,
      "loss": 0.0022,
      "step": 82010
    },
    {
      "epoch": 2.734,
      "grad_norm": 0.058174386620521545,
      "learning_rate": 3.29125e-05,
      "loss": 0.0022,
      "step": 82020
    },
    {
      "epoch": 2.7343333333333333,
      "grad_norm": 0.5185483694076538,
      "learning_rate": 3.2910416666666666e-05,
      "loss": 0.0032,
      "step": 82030
    },
    {
      "epoch": 2.7346666666666666,
      "grad_norm": 0.08669610321521759,
      "learning_rate": 3.290833333333334e-05,
      "loss": 0.0021,
      "step": 82040
    },
    {
      "epoch": 2.735,
      "grad_norm": 0.1728145182132721,
      "learning_rate": 3.290625e-05,
      "loss": 0.0023,
      "step": 82050
    },
    {
      "epoch": 2.735333333333333,
      "grad_norm": 0.23031465709209442,
      "learning_rate": 3.290416666666667e-05,
      "loss": 0.0021,
      "step": 82060
    },
    {
      "epoch": 2.735666666666667,
      "grad_norm": 0.11525288969278336,
      "learning_rate": 3.2902083333333335e-05,
      "loss": 0.0022,
      "step": 82070
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.08652007579803467,
      "learning_rate": 3.29e-05,
      "loss": 0.0018,
      "step": 82080
    },
    {
      "epoch": 2.7363333333333335,
      "grad_norm": 0.2599088251590729,
      "learning_rate": 3.2897916666666666e-05,
      "loss": 0.0028,
      "step": 82090
    },
    {
      "epoch": 2.736666666666667,
      "grad_norm": 0.20160357654094696,
      "learning_rate": 3.289583333333334e-05,
      "loss": 0.0031,
      "step": 82100
    },
    {
      "epoch": 2.737,
      "grad_norm": 0.14389094710350037,
      "learning_rate": 3.289375e-05,
      "loss": 0.003,
      "step": 82110
    },
    {
      "epoch": 2.7373333333333334,
      "grad_norm": 0.05793706327676773,
      "learning_rate": 3.289166666666667e-05,
      "loss": 0.0019,
      "step": 82120
    },
    {
      "epoch": 2.7376666666666667,
      "grad_norm": 0.14407871663570404,
      "learning_rate": 3.2889583333333334e-05,
      "loss": 0.002,
      "step": 82130
    },
    {
      "epoch": 2.738,
      "grad_norm": 0.20163081586360931,
      "learning_rate": 3.28875e-05,
      "loss": 0.0025,
      "step": 82140
    },
    {
      "epoch": 2.7383333333333333,
      "grad_norm": 0.1154271736741066,
      "learning_rate": 3.2885416666666665e-05,
      "loss": 0.0025,
      "step": 82150
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.4538553059101105,
      "learning_rate": 3.288333333333333e-05,
      "loss": 0.0019,
      "step": 82160
    },
    {
      "epoch": 2.739,
      "grad_norm": 0.23032736778259277,
      "learning_rate": 3.288125e-05,
      "loss": 0.0022,
      "step": 82170
    },
    {
      "epoch": 2.739333333333333,
      "grad_norm": 0.14397121965885162,
      "learning_rate": 3.287916666666667e-05,
      "loss": 0.0017,
      "step": 82180
    },
    {
      "epoch": 2.7396666666666665,
      "grad_norm": 0.057629622519016266,
      "learning_rate": 3.2877083333333334e-05,
      "loss": 0.002,
      "step": 82190
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.08681285381317139,
      "learning_rate": 3.2875e-05,
      "loss": 0.0026,
      "step": 82200
    },
    {
      "epoch": 2.740333333333333,
      "grad_norm": 0.2884291708469391,
      "learning_rate": 3.287291666666667e-05,
      "loss": 0.0023,
      "step": 82210
    },
    {
      "epoch": 2.740666666666667,
      "grad_norm": 0.028966620564460754,
      "learning_rate": 3.287083333333334e-05,
      "loss": 0.0022,
      "step": 82220
    },
    {
      "epoch": 2.741,
      "grad_norm": 0.28814539313316345,
      "learning_rate": 3.286875e-05,
      "loss": 0.0029,
      "step": 82230
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.08658593147993088,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0025,
      "step": 82240
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 0.006465406157076359,
      "learning_rate": 3.286458333333334e-05,
      "loss": 0.0023,
      "step": 82250
    },
    {
      "epoch": 2.742,
      "grad_norm": 0.14401216804981232,
      "learning_rate": 3.28625e-05,
      "loss": 0.002,
      "step": 82260
    },
    {
      "epoch": 2.7423333333333333,
      "grad_norm": 0.23058508336544037,
      "learning_rate": 3.2860416666666665e-05,
      "loss": 0.0013,
      "step": 82270
    },
    {
      "epoch": 2.7426666666666666,
      "grad_norm": 0.19813419878482819,
      "learning_rate": 3.285833333333334e-05,
      "loss": 0.0028,
      "step": 82280
    },
    {
      "epoch": 2.743,
      "grad_norm": 0.05780387297272682,
      "learning_rate": 3.285625e-05,
      "loss": 0.0027,
      "step": 82290
    },
    {
      "epoch": 2.743333333333333,
      "grad_norm": 0.058207958936691284,
      "learning_rate": 3.285416666666667e-05,
      "loss": 0.0028,
      "step": 82300
    },
    {
      "epoch": 2.743666666666667,
      "grad_norm": 0.17324359714984894,
      "learning_rate": 3.2852083333333333e-05,
      "loss": 0.0024,
      "step": 82310
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.11515363305807114,
      "learning_rate": 3.2850000000000006e-05,
      "loss": 0.0026,
      "step": 82320
    },
    {
      "epoch": 2.7443333333333335,
      "grad_norm": 0.08764886856079102,
      "learning_rate": 3.2847916666666664e-05,
      "loss": 0.0019,
      "step": 82330
    },
    {
      "epoch": 2.744666666666667,
      "grad_norm": 0.34576117992401123,
      "learning_rate": 3.284583333333334e-05,
      "loss": 0.0015,
      "step": 82340
    },
    {
      "epoch": 2.745,
      "grad_norm": 0.05797494575381279,
      "learning_rate": 3.284375e-05,
      "loss": 0.0019,
      "step": 82350
    },
    {
      "epoch": 2.7453333333333334,
      "grad_norm": 0.14402183890342712,
      "learning_rate": 3.284166666666667e-05,
      "loss": 0.0021,
      "step": 82360
    },
    {
      "epoch": 2.7456666666666667,
      "grad_norm": 0.20202109217643738,
      "learning_rate": 3.283958333333333e-05,
      "loss": 0.0031,
      "step": 82370
    },
    {
      "epoch": 2.746,
      "grad_norm": 0.030363399535417557,
      "learning_rate": 3.28375e-05,
      "loss": 0.0018,
      "step": 82380
    },
    {
      "epoch": 2.7463333333333333,
      "grad_norm": 0.08672693371772766,
      "learning_rate": 3.283541666666667e-05,
      "loss": 0.002,
      "step": 82390
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.08674389868974686,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0017,
      "step": 82400
    },
    {
      "epoch": 2.747,
      "grad_norm": 0.14424964785575867,
      "learning_rate": 3.283125e-05,
      "loss": 0.0024,
      "step": 82410
    },
    {
      "epoch": 2.747333333333333,
      "grad_norm": 0.14410535991191864,
      "learning_rate": 3.282916666666667e-05,
      "loss": 0.0015,
      "step": 82420
    },
    {
      "epoch": 2.7476666666666665,
      "grad_norm": 0.2878638207912445,
      "learning_rate": 3.282708333333333e-05,
      "loss": 0.0023,
      "step": 82430
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.11557821929454803,
      "learning_rate": 3.2825e-05,
      "loss": 0.0017,
      "step": 82440
    },
    {
      "epoch": 2.748333333333333,
      "grad_norm": 0.2018185406923294,
      "learning_rate": 3.282291666666667e-05,
      "loss": 0.0025,
      "step": 82450
    },
    {
      "epoch": 2.748666666666667,
      "grad_norm": 0.08668100088834763,
      "learning_rate": 3.2820833333333336e-05,
      "loss": 0.0028,
      "step": 82460
    },
    {
      "epoch": 2.749,
      "grad_norm": 0.2592231333255768,
      "learning_rate": 3.281875e-05,
      "loss": 0.0035,
      "step": 82470
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.05769530311226845,
      "learning_rate": 3.281666666666667e-05,
      "loss": 0.0024,
      "step": 82480
    },
    {
      "epoch": 2.7496666666666667,
      "grad_norm": 0.23058030009269714,
      "learning_rate": 3.281458333333334e-05,
      "loss": 0.0026,
      "step": 82490
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.47656843066215515,
      "learning_rate": 3.2812500000000005e-05,
      "loss": 0.0021,
      "step": 82500
    },
    {
      "epoch": 2.7503333333333333,
      "grad_norm": 0.20175929367542267,
      "learning_rate": 3.2810416666666663e-05,
      "loss": 0.0017,
      "step": 82510
    },
    {
      "epoch": 2.7506666666666666,
      "grad_norm": 0.23023203015327454,
      "learning_rate": 3.2808333333333336e-05,
      "loss": 0.0029,
      "step": 82520
    },
    {
      "epoch": 2.751,
      "grad_norm": 0.2593268156051636,
      "learning_rate": 3.280625e-05,
      "loss": 0.0022,
      "step": 82530
    },
    {
      "epoch": 2.751333333333333,
      "grad_norm": 0.5235305428504944,
      "learning_rate": 3.280416666666667e-05,
      "loss": 0.0021,
      "step": 82540
    },
    {
      "epoch": 2.751666666666667,
      "grad_norm": 0.1151302382349968,
      "learning_rate": 3.280208333333333e-05,
      "loss": 0.0022,
      "step": 82550
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.4896458685398102,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0026,
      "step": 82560
    },
    {
      "epoch": 2.7523333333333335,
      "grad_norm": 0.029269682243466377,
      "learning_rate": 3.279791666666667e-05,
      "loss": 0.0022,
      "step": 82570
    },
    {
      "epoch": 2.752666666666667,
      "grad_norm": 0.3457094728946686,
      "learning_rate": 3.2795833333333335e-05,
      "loss": 0.0023,
      "step": 82580
    },
    {
      "epoch": 2.753,
      "grad_norm": 0.1547432243824005,
      "learning_rate": 3.279375e-05,
      "loss": 0.003,
      "step": 82590
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.11528732627630234,
      "learning_rate": 3.279166666666667e-05,
      "loss": 0.0023,
      "step": 82600
    },
    {
      "epoch": 2.7536666666666667,
      "grad_norm": 0.05830506235361099,
      "learning_rate": 3.278958333333333e-05,
      "loss": 0.0022,
      "step": 82610
    },
    {
      "epoch": 2.754,
      "grad_norm": 0.05779287591576576,
      "learning_rate": 3.2787500000000004e-05,
      "loss": 0.0022,
      "step": 82620
    },
    {
      "epoch": 2.7543333333333333,
      "grad_norm": 0.6624984741210938,
      "learning_rate": 3.278541666666667e-05,
      "loss": 0.0025,
      "step": 82630
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.029283666983246803,
      "learning_rate": 3.2783333333333335e-05,
      "loss": 0.0019,
      "step": 82640
    },
    {
      "epoch": 2.755,
      "grad_norm": 0.43201544880867004,
      "learning_rate": 3.278125e-05,
      "loss": 0.0024,
      "step": 82650
    },
    {
      "epoch": 2.755333333333333,
      "grad_norm": 0.029744019731879234,
      "learning_rate": 3.2779166666666666e-05,
      "loss": 0.0025,
      "step": 82660
    },
    {
      "epoch": 2.7556666666666665,
      "grad_norm": 0.14404205977916718,
      "learning_rate": 3.277708333333334e-05,
      "loss": 0.0027,
      "step": 82670
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.2879953980445862,
      "learning_rate": 3.2775e-05,
      "loss": 0.0019,
      "step": 82680
    },
    {
      "epoch": 2.756333333333333,
      "grad_norm": 0.2592957615852356,
      "learning_rate": 3.277291666666667e-05,
      "loss": 0.0022,
      "step": 82690
    },
    {
      "epoch": 2.756666666666667,
      "grad_norm": 0.4032096862792969,
      "learning_rate": 3.2770833333333335e-05,
      "loss": 0.0025,
      "step": 82700
    },
    {
      "epoch": 2.757,
      "grad_norm": 0.5757635235786438,
      "learning_rate": 3.276875e-05,
      "loss": 0.0025,
      "step": 82710
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.17310605943202972,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.0025,
      "step": 82720
    },
    {
      "epoch": 2.7576666666666667,
      "grad_norm": 0.2880670726299286,
      "learning_rate": 3.276458333333334e-05,
      "loss": 0.0033,
      "step": 82730
    },
    {
      "epoch": 2.758,
      "grad_norm": 0.23180294036865234,
      "learning_rate": 3.2762500000000004e-05,
      "loss": 0.0024,
      "step": 82740
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 0.1440911740064621,
      "learning_rate": 3.276041666666667e-05,
      "loss": 0.003,
      "step": 82750
    },
    {
      "epoch": 2.7586666666666666,
      "grad_norm": 0.08675708621740341,
      "learning_rate": 3.2758333333333335e-05,
      "loss": 0.0028,
      "step": 82760
    },
    {
      "epoch": 2.759,
      "grad_norm": 0.05788564682006836,
      "learning_rate": 3.275625e-05,
      "loss": 0.0021,
      "step": 82770
    },
    {
      "epoch": 2.759333333333333,
      "grad_norm": 0.2591564953327179,
      "learning_rate": 3.275416666666667e-05,
      "loss": 0.0022,
      "step": 82780
    },
    {
      "epoch": 2.759666666666667,
      "grad_norm": 0.37436825037002563,
      "learning_rate": 3.275208333333333e-05,
      "loss": 0.0023,
      "step": 82790
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.05777275189757347,
      "learning_rate": 3.275e-05,
      "loss": 0.0019,
      "step": 82800
    },
    {
      "epoch": 2.7603333333333335,
      "grad_norm": 0.6050186157226562,
      "learning_rate": 3.274791666666667e-05,
      "loss": 0.0021,
      "step": 82810
    },
    {
      "epoch": 2.760666666666667,
      "grad_norm": 0.20179876685142517,
      "learning_rate": 3.2745833333333334e-05,
      "loss": 0.0024,
      "step": 82820
    },
    {
      "epoch": 2.761,
      "grad_norm": 0.14420229196548462,
      "learning_rate": 3.274375e-05,
      "loss": 0.0021,
      "step": 82830
    },
    {
      "epoch": 2.7613333333333334,
      "grad_norm": 0.3168550431728363,
      "learning_rate": 3.274166666666667e-05,
      "loss": 0.0037,
      "step": 82840
    },
    {
      "epoch": 2.7616666666666667,
      "grad_norm": 0.17286665737628937,
      "learning_rate": 3.273958333333334e-05,
      "loss": 0.0026,
      "step": 82850
    },
    {
      "epoch": 2.762,
      "grad_norm": 0.5471202731132507,
      "learning_rate": 3.27375e-05,
      "loss": 0.0019,
      "step": 82860
    },
    {
      "epoch": 2.7623333333333333,
      "grad_norm": 0.005251070484519005,
      "learning_rate": 3.273541666666667e-05,
      "loss": 0.0029,
      "step": 82870
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.004825960844755173,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0018,
      "step": 82880
    },
    {
      "epoch": 2.763,
      "grad_norm": 0.11540306359529495,
      "learning_rate": 3.273125e-05,
      "loss": 0.0029,
      "step": 82890
    },
    {
      "epoch": 2.763333333333333,
      "grad_norm": 0.1729256510734558,
      "learning_rate": 3.2729166666666665e-05,
      "loss": 0.0034,
      "step": 82900
    },
    {
      "epoch": 2.7636666666666665,
      "grad_norm": 0.028884101659059525,
      "learning_rate": 3.272708333333334e-05,
      "loss": 0.0019,
      "step": 82910
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.0864962711930275,
      "learning_rate": 3.2725e-05,
      "loss": 0.0022,
      "step": 82920
    },
    {
      "epoch": 2.764333333333333,
      "grad_norm": 0.17670989036560059,
      "learning_rate": 3.272291666666667e-05,
      "loss": 0.0016,
      "step": 82930
    },
    {
      "epoch": 2.764666666666667,
      "grad_norm": 0.17280998826026917,
      "learning_rate": 3.2720833333333334e-05,
      "loss": 0.0029,
      "step": 82940
    },
    {
      "epoch": 2.765,
      "grad_norm": 0.6047291159629822,
      "learning_rate": 3.2718750000000006e-05,
      "loss": 0.0021,
      "step": 82950
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.17288939654827118,
      "learning_rate": 3.2716666666666665e-05,
      "loss": 0.0024,
      "step": 82960
    },
    {
      "epoch": 2.7656666666666667,
      "grad_norm": 0.14409558475017548,
      "learning_rate": 3.271458333333334e-05,
      "loss": 0.0028,
      "step": 82970
    },
    {
      "epoch": 2.766,
      "grad_norm": 0.2246456891298294,
      "learning_rate": 3.27125e-05,
      "loss": 0.0028,
      "step": 82980
    },
    {
      "epoch": 2.7663333333333333,
      "grad_norm": 0.14403633773326874,
      "learning_rate": 3.271041666666667e-05,
      "loss": 0.0027,
      "step": 82990
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.31689539551734924,
      "learning_rate": 3.270833333333333e-05,
      "loss": 0.0018,
      "step": 83000
    },
    {
      "epoch": 2.767,
      "grad_norm": 0.17278915643692017,
      "learning_rate": 3.270625e-05,
      "loss": 0.002,
      "step": 83010
    },
    {
      "epoch": 2.767333333333333,
      "grad_norm": 0.11520246416330338,
      "learning_rate": 3.270416666666667e-05,
      "loss": 0.0026,
      "step": 83020
    },
    {
      "epoch": 2.767666666666667,
      "grad_norm": 0.28820499777793884,
      "learning_rate": 3.2702083333333337e-05,
      "loss": 0.0017,
      "step": 83030
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.02996673993766308,
      "learning_rate": 3.27e-05,
      "loss": 0.002,
      "step": 83040
    },
    {
      "epoch": 2.7683333333333335,
      "grad_norm": 0.2876882255077362,
      "learning_rate": 3.269791666666667e-05,
      "loss": 0.0026,
      "step": 83050
    },
    {
      "epoch": 2.768666666666667,
      "grad_norm": 0.25934305787086487,
      "learning_rate": 3.269583333333334e-05,
      "loss": 0.0019,
      "step": 83060
    },
    {
      "epoch": 2.769,
      "grad_norm": 0.2880581021308899,
      "learning_rate": 3.269375e-05,
      "loss": 0.0026,
      "step": 83070
    },
    {
      "epoch": 2.7693333333333334,
      "grad_norm": 0.11628752201795578,
      "learning_rate": 3.269166666666667e-05,
      "loss": 0.0018,
      "step": 83080
    },
    {
      "epoch": 2.7696666666666667,
      "grad_norm": 0.25928664207458496,
      "learning_rate": 3.2689583333333336e-05,
      "loss": 0.0025,
      "step": 83090
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.08637744188308716,
      "learning_rate": 3.26875e-05,
      "loss": 0.0015,
      "step": 83100
    },
    {
      "epoch": 2.7703333333333333,
      "grad_norm": 0.14408616721630096,
      "learning_rate": 3.268541666666667e-05,
      "loss": 0.0018,
      "step": 83110
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.13534198701381683,
      "learning_rate": 3.268333333333333e-05,
      "loss": 0.0021,
      "step": 83120
    },
    {
      "epoch": 2.771,
      "grad_norm": 0.11545886844396591,
      "learning_rate": 3.2681250000000005e-05,
      "loss": 0.0027,
      "step": 83130
    },
    {
      "epoch": 2.771333333333333,
      "grad_norm": 0.14387942850589752,
      "learning_rate": 3.2679166666666664e-05,
      "loss": 0.0033,
      "step": 83140
    },
    {
      "epoch": 2.7716666666666665,
      "grad_norm": 0.14420641958713531,
      "learning_rate": 3.2677083333333336e-05,
      "loss": 0.0017,
      "step": 83150
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.2594728171825409,
      "learning_rate": 3.2675e-05,
      "loss": 0.0019,
      "step": 83160
    },
    {
      "epoch": 2.772333333333333,
      "grad_norm": 0.11528865247964859,
      "learning_rate": 3.267291666666667e-05,
      "loss": 0.0021,
      "step": 83170
    },
    {
      "epoch": 2.772666666666667,
      "grad_norm": 0.3982970714569092,
      "learning_rate": 3.267083333333333e-05,
      "loss": 0.0017,
      "step": 83180
    },
    {
      "epoch": 2.773,
      "grad_norm": 0.20185120403766632,
      "learning_rate": 3.2668750000000005e-05,
      "loss": 0.0023,
      "step": 83190
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.08673572540283203,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.003,
      "step": 83200
    },
    {
      "epoch": 2.7736666666666667,
      "grad_norm": 0.057644810527563095,
      "learning_rate": 3.2664583333333336e-05,
      "loss": 0.002,
      "step": 83210
    },
    {
      "epoch": 2.774,
      "grad_norm": 0.20207835733890533,
      "learning_rate": 3.26625e-05,
      "loss": 0.0019,
      "step": 83220
    },
    {
      "epoch": 2.7743333333333333,
      "grad_norm": 0.05803156644105911,
      "learning_rate": 3.2660416666666667e-05,
      "loss": 0.0016,
      "step": 83230
    },
    {
      "epoch": 2.7746666666666666,
      "grad_norm": 0.7095944881439209,
      "learning_rate": 3.265833333333333e-05,
      "loss": 0.0021,
      "step": 83240
    },
    {
      "epoch": 2.775,
      "grad_norm": 0.029303545132279396,
      "learning_rate": 3.265625e-05,
      "loss": 0.0026,
      "step": 83250
    },
    {
      "epoch": 2.775333333333333,
      "grad_norm": 0.782160758972168,
      "learning_rate": 3.265416666666667e-05,
      "loss": 0.0034,
      "step": 83260
    },
    {
      "epoch": 2.7756666666666665,
      "grad_norm": 0.11519019305706024,
      "learning_rate": 3.2652083333333335e-05,
      "loss": 0.0019,
      "step": 83270
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.48927438259124756,
      "learning_rate": 3.265e-05,
      "loss": 0.0023,
      "step": 83280
    },
    {
      "epoch": 2.7763333333333335,
      "grad_norm": 0.008298262022435665,
      "learning_rate": 3.2647916666666666e-05,
      "loss": 0.0017,
      "step": 83290
    },
    {
      "epoch": 2.7766666666666664,
      "grad_norm": 0.37493497133255005,
      "learning_rate": 3.264583333333334e-05,
      "loss": 0.003,
      "step": 83300
    },
    {
      "epoch": 2.777,
      "grad_norm": 0.029678624123334885,
      "learning_rate": 3.2643750000000004e-05,
      "loss": 0.0022,
      "step": 83310
    },
    {
      "epoch": 2.7773333333333334,
      "grad_norm": 0.14393605291843414,
      "learning_rate": 3.264166666666667e-05,
      "loss": 0.0019,
      "step": 83320
    },
    {
      "epoch": 2.7776666666666667,
      "grad_norm": 0.2051403522491455,
      "learning_rate": 3.2639583333333335e-05,
      "loss": 0.0029,
      "step": 83330
    },
    {
      "epoch": 2.778,
      "grad_norm": 0.11539297550916672,
      "learning_rate": 3.263750000000001e-05,
      "loss": 0.0032,
      "step": 83340
    },
    {
      "epoch": 2.7783333333333333,
      "grad_norm": 0.42143508791923523,
      "learning_rate": 3.2635416666666666e-05,
      "loss": 0.0018,
      "step": 83350
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.05817727372050285,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.0031,
      "step": 83360
    },
    {
      "epoch": 2.779,
      "grad_norm": 0.46060648560523987,
      "learning_rate": 3.2631250000000004e-05,
      "loss": 0.002,
      "step": 83370
    },
    {
      "epoch": 2.779333333333333,
      "grad_norm": 0.4318494200706482,
      "learning_rate": 3.262916666666667e-05,
      "loss": 0.0024,
      "step": 83380
    },
    {
      "epoch": 2.7796666666666665,
      "grad_norm": 0.23056535422801971,
      "learning_rate": 3.2627083333333335e-05,
      "loss": 0.0027,
      "step": 83390
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.03171580284833908,
      "learning_rate": 3.2625e-05,
      "loss": 0.0023,
      "step": 83400
    },
    {
      "epoch": 2.780333333333333,
      "grad_norm": 0.20194856822490692,
      "learning_rate": 3.262291666666667e-05,
      "loss": 0.0032,
      "step": 83410
    },
    {
      "epoch": 2.780666666666667,
      "grad_norm": 0.5182803869247437,
      "learning_rate": 3.262083333333333e-05,
      "loss": 0.0017,
      "step": 83420
    },
    {
      "epoch": 2.781,
      "grad_norm": 0.4608117938041687,
      "learning_rate": 3.2618750000000003e-05,
      "loss": 0.0028,
      "step": 83430
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.08652421832084656,
      "learning_rate": 3.261666666666667e-05,
      "loss": 0.0029,
      "step": 83440
    },
    {
      "epoch": 2.7816666666666667,
      "grad_norm": 0.14387278258800507,
      "learning_rate": 3.2614583333333334e-05,
      "loss": 0.0024,
      "step": 83450
    },
    {
      "epoch": 2.782,
      "grad_norm": 0.40304994583129883,
      "learning_rate": 3.26125e-05,
      "loss": 0.0018,
      "step": 83460
    },
    {
      "epoch": 2.7823333333333333,
      "grad_norm": 0.11530735343694687,
      "learning_rate": 3.2610416666666665e-05,
      "loss": 0.0018,
      "step": 83470
    },
    {
      "epoch": 2.7826666666666666,
      "grad_norm": 0.0650397315621376,
      "learning_rate": 3.260833333333334e-05,
      "loss": 0.0036,
      "step": 83480
    },
    {
      "epoch": 2.783,
      "grad_norm": 0.08677525818347931,
      "learning_rate": 3.2606249999999996e-05,
      "loss": 0.0017,
      "step": 83490
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 0.05754714086651802,
      "learning_rate": 3.260416666666667e-05,
      "loss": 0.0025,
      "step": 83500
    },
    {
      "epoch": 2.7836666666666665,
      "grad_norm": 0.17288482189178467,
      "learning_rate": 3.2602083333333334e-05,
      "loss": 0.0029,
      "step": 83510
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.17314888536930084,
      "learning_rate": 3.26e-05,
      "loss": 0.0044,
      "step": 83520
    },
    {
      "epoch": 2.7843333333333335,
      "grad_norm": 0.029039131477475166,
      "learning_rate": 3.2597916666666665e-05,
      "loss": 0.0017,
      "step": 83530
    },
    {
      "epoch": 2.7846666666666664,
      "grad_norm": 0.23039856553077698,
      "learning_rate": 3.259583333333334e-05,
      "loss": 0.0023,
      "step": 83540
    },
    {
      "epoch": 2.785,
      "grad_norm": 0.04878590628504753,
      "learning_rate": 3.259375e-05,
      "loss": 0.0032,
      "step": 83550
    },
    {
      "epoch": 2.7853333333333334,
      "grad_norm": 0.030036093667149544,
      "learning_rate": 3.259166666666667e-05,
      "loss": 0.0029,
      "step": 83560
    },
    {
      "epoch": 2.7856666666666667,
      "grad_norm": 0.6545750498771667,
      "learning_rate": 3.2589583333333334e-05,
      "loss": 0.0023,
      "step": 83570
    },
    {
      "epoch": 2.786,
      "grad_norm": 0.37463411688804626,
      "learning_rate": 3.2587500000000006e-05,
      "loss": 0.0014,
      "step": 83580
    },
    {
      "epoch": 2.7863333333333333,
      "grad_norm": 0.3745555877685547,
      "learning_rate": 3.258541666666667e-05,
      "loss": 0.0028,
      "step": 83590
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.2594757378101349,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.0023,
      "step": 83600
    },
    {
      "epoch": 2.787,
      "grad_norm": 0.2306402623653412,
      "learning_rate": 3.258125e-05,
      "loss": 0.0016,
      "step": 83610
    },
    {
      "epoch": 2.787333333333333,
      "grad_norm": 0.00668799364939332,
      "learning_rate": 3.257916666666667e-05,
      "loss": 0.0018,
      "step": 83620
    },
    {
      "epoch": 2.7876666666666665,
      "grad_norm": 0.34541043639183044,
      "learning_rate": 3.2577083333333333e-05,
      "loss": 0.0026,
      "step": 83630
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.08688598871231079,
      "learning_rate": 3.2575e-05,
      "loss": 0.0028,
      "step": 83640
    },
    {
      "epoch": 2.788333333333333,
      "grad_norm": 0.40327081084251404,
      "learning_rate": 3.257291666666667e-05,
      "loss": 0.0028,
      "step": 83650
    },
    {
      "epoch": 2.788666666666667,
      "grad_norm": 0.05791676416993141,
      "learning_rate": 3.257083333333334e-05,
      "loss": 0.0033,
      "step": 83660
    },
    {
      "epoch": 2.789,
      "grad_norm": 0.20189625024795532,
      "learning_rate": 3.256875e-05,
      "loss": 0.003,
      "step": 83670
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.20162227749824524,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0015,
      "step": 83680
    },
    {
      "epoch": 2.7896666666666667,
      "grad_norm": 0.1727573722600937,
      "learning_rate": 3.256458333333334e-05,
      "loss": 0.0026,
      "step": 83690
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.08643025159835815,
      "learning_rate": 3.25625e-05,
      "loss": 0.0023,
      "step": 83700
    },
    {
      "epoch": 2.7903333333333333,
      "grad_norm": 0.14408235251903534,
      "learning_rate": 3.2560416666666664e-05,
      "loss": 0.0021,
      "step": 83710
    },
    {
      "epoch": 2.7906666666666666,
      "grad_norm": 0.1910874992609024,
      "learning_rate": 3.2558333333333336e-05,
      "loss": 0.0024,
      "step": 83720
    },
    {
      "epoch": 2.791,
      "grad_norm": 0.20162706077098846,
      "learning_rate": 3.255625e-05,
      "loss": 0.0031,
      "step": 83730
    },
    {
      "epoch": 2.791333333333333,
      "grad_norm": 0.057808615267276764,
      "learning_rate": 3.255416666666667e-05,
      "loss": 0.0014,
      "step": 83740
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 0.28793951869010925,
      "learning_rate": 3.255208333333333e-05,
      "loss": 0.0025,
      "step": 83750
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.31678473949432373,
      "learning_rate": 3.2550000000000005e-05,
      "loss": 0.0034,
      "step": 83760
    },
    {
      "epoch": 2.7923333333333336,
      "grad_norm": 0.1728297770023346,
      "learning_rate": 3.2547916666666664e-05,
      "loss": 0.002,
      "step": 83770
    },
    {
      "epoch": 2.7926666666666664,
      "grad_norm": 0.11533837765455246,
      "learning_rate": 3.2545833333333336e-05,
      "loss": 0.0021,
      "step": 83780
    },
    {
      "epoch": 2.793,
      "grad_norm": 0.08641248941421509,
      "learning_rate": 3.254375e-05,
      "loss": 0.0014,
      "step": 83790
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.2303885668516159,
      "learning_rate": 3.254166666666667e-05,
      "loss": 0.0023,
      "step": 83800
    },
    {
      "epoch": 2.7936666666666667,
      "grad_norm": 0.3455842137336731,
      "learning_rate": 3.253958333333333e-05,
      "loss": 0.0014,
      "step": 83810
    },
    {
      "epoch": 2.794,
      "grad_norm": 0.3453172445297241,
      "learning_rate": 3.2537500000000005e-05,
      "loss": 0.0024,
      "step": 83820
    },
    {
      "epoch": 2.7943333333333333,
      "grad_norm": 0.034455444663763046,
      "learning_rate": 3.253541666666667e-05,
      "loss": 0.0025,
      "step": 83830
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.05836494266986847,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0018,
      "step": 83840
    },
    {
      "epoch": 2.795,
      "grad_norm": 0.16294990479946136,
      "learning_rate": 3.253125e-05,
      "loss": 0.0019,
      "step": 83850
    },
    {
      "epoch": 2.7953333333333332,
      "grad_norm": 0.05769960954785347,
      "learning_rate": 3.252916666666667e-05,
      "loss": 0.0018,
      "step": 83860
    },
    {
      "epoch": 2.7956666666666665,
      "grad_norm": 0.20160868763923645,
      "learning_rate": 3.252708333333334e-05,
      "loss": 0.003,
      "step": 83870
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.20154407620429993,
      "learning_rate": 3.2525e-05,
      "loss": 0.0018,
      "step": 83880
    },
    {
      "epoch": 2.796333333333333,
      "grad_norm": 0.11533594876527786,
      "learning_rate": 3.252291666666667e-05,
      "loss": 0.0018,
      "step": 83890
    },
    {
      "epoch": 2.796666666666667,
      "grad_norm": 0.08634699881076813,
      "learning_rate": 3.2520833333333336e-05,
      "loss": 0.0019,
      "step": 83900
    },
    {
      "epoch": 2.797,
      "grad_norm": 0.14394477009773254,
      "learning_rate": 3.251875e-05,
      "loss": 0.0018,
      "step": 83910
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.086266428232193,
      "learning_rate": 3.2516666666666666e-05,
      "loss": 0.0018,
      "step": 83920
    },
    {
      "epoch": 2.7976666666666667,
      "grad_norm": 0.1441594511270523,
      "learning_rate": 3.251458333333334e-05,
      "loss": 0.0023,
      "step": 83930
    },
    {
      "epoch": 2.798,
      "grad_norm": 0.05834191292524338,
      "learning_rate": 3.2512500000000004e-05,
      "loss": 0.002,
      "step": 83940
    },
    {
      "epoch": 2.7983333333333333,
      "grad_norm": 0.2593100368976593,
      "learning_rate": 3.251041666666667e-05,
      "loss": 0.0021,
      "step": 83950
    },
    {
      "epoch": 2.7986666666666666,
      "grad_norm": 0.25908493995666504,
      "learning_rate": 3.2508333333333335e-05,
      "loss": 0.0018,
      "step": 83960
    },
    {
      "epoch": 2.799,
      "grad_norm": 0.23060491681098938,
      "learning_rate": 3.250625e-05,
      "loss": 0.0034,
      "step": 83970
    },
    {
      "epoch": 2.7993333333333332,
      "grad_norm": 0.004896520171314478,
      "learning_rate": 3.2504166666666666e-05,
      "loss": 0.0019,
      "step": 83980
    },
    {
      "epoch": 2.7996666666666665,
      "grad_norm": 0.04537535086274147,
      "learning_rate": 3.250208333333333e-05,
      "loss": 0.0016,
      "step": 83990
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.20164722204208374,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0021,
      "step": 84000
    },
    {
      "epoch": 2.8003333333333336,
      "grad_norm": 0.20166683197021484,
      "learning_rate": 3.249791666666667e-05,
      "loss": 0.0027,
      "step": 84010
    },
    {
      "epoch": 2.8006666666666664,
      "grad_norm": 0.05771075189113617,
      "learning_rate": 3.2495833333333335e-05,
      "loss": 0.0017,
      "step": 84020
    },
    {
      "epoch": 2.801,
      "grad_norm": 0.28802651166915894,
      "learning_rate": 3.249375e-05,
      "loss": 0.002,
      "step": 84030
    },
    {
      "epoch": 2.8013333333333335,
      "grad_norm": 0.23044359683990479,
      "learning_rate": 3.249166666666667e-05,
      "loss": 0.0023,
      "step": 84040
    },
    {
      "epoch": 2.8016666666666667,
      "grad_norm": 0.28806397318840027,
      "learning_rate": 3.248958333333333e-05,
      "loss": 0.0017,
      "step": 84050
    },
    {
      "epoch": 2.802,
      "grad_norm": 0.05756106600165367,
      "learning_rate": 3.2487500000000004e-05,
      "loss": 0.0015,
      "step": 84060
    },
    {
      "epoch": 2.8023333333333333,
      "grad_norm": 0.11508285254240036,
      "learning_rate": 3.248541666666667e-05,
      "loss": 0.0028,
      "step": 84070
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.17301273345947266,
      "learning_rate": 3.2483333333333335e-05,
      "loss": 0.0019,
      "step": 84080
    },
    {
      "epoch": 2.803,
      "grad_norm": 0.11513421684503555,
      "learning_rate": 3.248125e-05,
      "loss": 0.0022,
      "step": 84090
    },
    {
      "epoch": 2.8033333333333332,
      "grad_norm": 0.2878912389278412,
      "learning_rate": 3.2479166666666666e-05,
      "loss": 0.0026,
      "step": 84100
    },
    {
      "epoch": 2.8036666666666665,
      "grad_norm": 0.25901225209236145,
      "learning_rate": 3.247708333333334e-05,
      "loss": 0.0021,
      "step": 84110
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.21190248429775238,
      "learning_rate": 3.2474999999999997e-05,
      "loss": 0.0019,
      "step": 84120
    },
    {
      "epoch": 2.804333333333333,
      "grad_norm": 0.17288117110729218,
      "learning_rate": 3.247291666666667e-05,
      "loss": 0.0019,
      "step": 84130
    },
    {
      "epoch": 2.804666666666667,
      "grad_norm": 0.11518105864524841,
      "learning_rate": 3.2470833333333334e-05,
      "loss": 0.0034,
      "step": 84140
    },
    {
      "epoch": 2.805,
      "grad_norm": 0.3743651807308197,
      "learning_rate": 3.2468750000000007e-05,
      "loss": 0.0019,
      "step": 84150
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.0290057510137558,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0026,
      "step": 84160
    },
    {
      "epoch": 2.8056666666666668,
      "grad_norm": 0.5757251381874084,
      "learning_rate": 3.246458333333334e-05,
      "loss": 0.0026,
      "step": 84170
    },
    {
      "epoch": 2.806,
      "grad_norm": 0.2303541898727417,
      "learning_rate": 3.24625e-05,
      "loss": 0.0029,
      "step": 84180
    },
    {
      "epoch": 2.8063333333333333,
      "grad_norm": 0.02900419756770134,
      "learning_rate": 3.246041666666667e-05,
      "loss": 0.0023,
      "step": 84190
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.20177467167377472,
      "learning_rate": 3.2458333333333334e-05,
      "loss": 0.0021,
      "step": 84200
    },
    {
      "epoch": 2.807,
      "grad_norm": 0.43187469244003296,
      "learning_rate": 3.245625e-05,
      "loss": 0.0024,
      "step": 84210
    },
    {
      "epoch": 2.8073333333333332,
      "grad_norm": 0.00726366275921464,
      "learning_rate": 3.245416666666667e-05,
      "loss": 0.0019,
      "step": 84220
    },
    {
      "epoch": 2.8076666666666665,
      "grad_norm": 0.18039526045322418,
      "learning_rate": 3.245208333333333e-05,
      "loss": 0.0028,
      "step": 84230
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.3244194984436035,
      "learning_rate": 3.245e-05,
      "loss": 0.0022,
      "step": 84240
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 0.287795752286911,
      "learning_rate": 3.244791666666667e-05,
      "loss": 0.0025,
      "step": 84250
    },
    {
      "epoch": 2.8086666666666664,
      "grad_norm": 0.013590400107204914,
      "learning_rate": 3.2445833333333334e-05,
      "loss": 0.0035,
      "step": 84260
    },
    {
      "epoch": 2.809,
      "grad_norm": 0.25914427638053894,
      "learning_rate": 3.244375e-05,
      "loss": 0.0022,
      "step": 84270
    },
    {
      "epoch": 2.8093333333333335,
      "grad_norm": 0.31658628582954407,
      "learning_rate": 3.244166666666667e-05,
      "loss": 0.0019,
      "step": 84280
    },
    {
      "epoch": 2.8096666666666668,
      "grad_norm": 0.05770190805196762,
      "learning_rate": 3.243958333333334e-05,
      "loss": 0.0028,
      "step": 84290
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.2877117395401001,
      "learning_rate": 3.24375e-05,
      "loss": 0.0032,
      "step": 84300
    },
    {
      "epoch": 2.8103333333333333,
      "grad_norm": 0.11529303342103958,
      "learning_rate": 3.243541666666667e-05,
      "loss": 0.0018,
      "step": 84310
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.2877308428287506,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0015,
      "step": 84320
    },
    {
      "epoch": 2.811,
      "grad_norm": 0.05817560851573944,
      "learning_rate": 3.243125e-05,
      "loss": 0.002,
      "step": 84330
    },
    {
      "epoch": 2.8113333333333332,
      "grad_norm": 0.6331498622894287,
      "learning_rate": 3.2429166666666664e-05,
      "loss": 0.0042,
      "step": 84340
    },
    {
      "epoch": 2.8116666666666665,
      "grad_norm": 0.31644460558891296,
      "learning_rate": 3.242708333333334e-05,
      "loss": 0.0022,
      "step": 84350
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.5179527401924133,
      "learning_rate": 3.2425e-05,
      "loss": 0.0028,
      "step": 84360
    },
    {
      "epoch": 2.812333333333333,
      "grad_norm": 0.003880472620949149,
      "learning_rate": 3.242291666666667e-05,
      "loss": 0.0024,
      "step": 84370
    },
    {
      "epoch": 2.812666666666667,
      "grad_norm": 0.14383001625537872,
      "learning_rate": 3.242083333333333e-05,
      "loss": 0.0029,
      "step": 84380
    },
    {
      "epoch": 2.8129999999999997,
      "grad_norm": 0.11504099518060684,
      "learning_rate": 3.2418750000000005e-05,
      "loss": 0.0024,
      "step": 84390
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.08651284873485565,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.0016,
      "step": 84400
    },
    {
      "epoch": 2.8136666666666668,
      "grad_norm": 0.2590683102607727,
      "learning_rate": 3.2414583333333336e-05,
      "loss": 0.0019,
      "step": 84410
    },
    {
      "epoch": 2.814,
      "grad_norm": 0.20154084265232086,
      "learning_rate": 3.24125e-05,
      "loss": 0.0024,
      "step": 84420
    },
    {
      "epoch": 2.8143333333333334,
      "grad_norm": 0.11540741473436356,
      "learning_rate": 3.2410416666666674e-05,
      "loss": 0.0023,
      "step": 84430
    },
    {
      "epoch": 2.8146666666666667,
      "grad_norm": 0.08662047237157822,
      "learning_rate": 3.240833333333333e-05,
      "loss": 0.0026,
      "step": 84440
    },
    {
      "epoch": 2.815,
      "grad_norm": 0.6452639698982239,
      "learning_rate": 3.240625e-05,
      "loss": 0.0028,
      "step": 84450
    },
    {
      "epoch": 2.8153333333333332,
      "grad_norm": 0.34539493918418884,
      "learning_rate": 3.240416666666667e-05,
      "loss": 0.002,
      "step": 84460
    },
    {
      "epoch": 2.8156666666666665,
      "grad_norm": 0.14386054873466492,
      "learning_rate": 3.2402083333333336e-05,
      "loss": 0.0027,
      "step": 84470
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.0581316202878952,
      "learning_rate": 3.24e-05,
      "loss": 0.0032,
      "step": 84480
    },
    {
      "epoch": 2.8163333333333336,
      "grad_norm": 0.23020711541175842,
      "learning_rate": 3.239791666666667e-05,
      "loss": 0.0039,
      "step": 84490
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 0.2878848910331726,
      "learning_rate": 3.239583333333334e-05,
      "loss": 0.0029,
      "step": 84500
    },
    {
      "epoch": 2.817,
      "grad_norm": 0.6406939029693604,
      "learning_rate": 3.239375e-05,
      "loss": 0.0024,
      "step": 84510
    },
    {
      "epoch": 2.8173333333333335,
      "grad_norm": 0.48932814598083496,
      "learning_rate": 3.239166666666667e-05,
      "loss": 0.0025,
      "step": 84520
    },
    {
      "epoch": 2.8176666666666668,
      "grad_norm": 0.37431299686431885,
      "learning_rate": 3.2389583333333336e-05,
      "loss": 0.0032,
      "step": 84530
    },
    {
      "epoch": 2.818,
      "grad_norm": 0.23172946274280548,
      "learning_rate": 3.23875e-05,
      "loss": 0.0019,
      "step": 84540
    },
    {
      "epoch": 2.8183333333333334,
      "grad_norm": 0.40323400497436523,
      "learning_rate": 3.238541666666667e-05,
      "loss": 0.0023,
      "step": 84550
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.05769049748778343,
      "learning_rate": 3.238333333333333e-05,
      "loss": 0.0023,
      "step": 84560
    },
    {
      "epoch": 2.819,
      "grad_norm": 0.11499565094709396,
      "learning_rate": 3.2381250000000004e-05,
      "loss": 0.0017,
      "step": 84570
    },
    {
      "epoch": 2.8193333333333332,
      "grad_norm": 0.6182477474212646,
      "learning_rate": 3.237916666666666e-05,
      "loss": 0.0028,
      "step": 84580
    },
    {
      "epoch": 2.8196666666666665,
      "grad_norm": 0.3741225302219391,
      "learning_rate": 3.2377083333333335e-05,
      "loss": 0.0031,
      "step": 84590
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.6517439484596252,
      "learning_rate": 3.2375e-05,
      "loss": 0.0022,
      "step": 84600
    },
    {
      "epoch": 2.820333333333333,
      "grad_norm": 0.1726626306772232,
      "learning_rate": 3.2372916666666666e-05,
      "loss": 0.003,
      "step": 84610
    },
    {
      "epoch": 2.820666666666667,
      "grad_norm": 0.25892719626426697,
      "learning_rate": 3.237083333333333e-05,
      "loss": 0.0025,
      "step": 84620
    },
    {
      "epoch": 2.8209999999999997,
      "grad_norm": 0.4609060287475586,
      "learning_rate": 3.2368750000000004e-05,
      "loss": 0.0036,
      "step": 84630
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.17285938560962677,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0023,
      "step": 84640
    },
    {
      "epoch": 2.8216666666666668,
      "grad_norm": 0.11557147651910782,
      "learning_rate": 3.2364583333333335e-05,
      "loss": 0.0025,
      "step": 84650
    },
    {
      "epoch": 2.822,
      "grad_norm": 0.1681891530752182,
      "learning_rate": 3.23625e-05,
      "loss": 0.0022,
      "step": 84660
    },
    {
      "epoch": 2.8223333333333334,
      "grad_norm": 0.20173965394496918,
      "learning_rate": 3.236041666666667e-05,
      "loss": 0.0022,
      "step": 84670
    },
    {
      "epoch": 2.8226666666666667,
      "grad_norm": 0.08656874299049377,
      "learning_rate": 3.235833333333333e-05,
      "loss": 0.0018,
      "step": 84680
    },
    {
      "epoch": 2.823,
      "grad_norm": 0.029367715120315552,
      "learning_rate": 3.235625e-05,
      "loss": 0.002,
      "step": 84690
    },
    {
      "epoch": 2.8233333333333333,
      "grad_norm": 0.02905554696917534,
      "learning_rate": 3.235416666666667e-05,
      "loss": 0.0012,
      "step": 84700
    },
    {
      "epoch": 2.8236666666666665,
      "grad_norm": 0.005333913490176201,
      "learning_rate": 3.2352083333333335e-05,
      "loss": 0.0017,
      "step": 84710
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.9317950010299683,
      "learning_rate": 3.235e-05,
      "loss": 0.0032,
      "step": 84720
    },
    {
      "epoch": 2.8243333333333336,
      "grad_norm": 0.25913164019584656,
      "learning_rate": 3.2347916666666666e-05,
      "loss": 0.0033,
      "step": 84730
    },
    {
      "epoch": 2.8246666666666664,
      "grad_norm": 0.4317156970500946,
      "learning_rate": 3.234583333333334e-05,
      "loss": 0.0031,
      "step": 84740
    },
    {
      "epoch": 2.825,
      "grad_norm": 0.46000248193740845,
      "learning_rate": 3.2343750000000004e-05,
      "loss": 0.002,
      "step": 84750
    },
    {
      "epoch": 2.8253333333333335,
      "grad_norm": 0.2589300870895386,
      "learning_rate": 3.234166666666667e-05,
      "loss": 0.0019,
      "step": 84760
    },
    {
      "epoch": 2.8256666666666668,
      "grad_norm": 0.20148102939128876,
      "learning_rate": 3.2339583333333334e-05,
      "loss": 0.0035,
      "step": 84770
    },
    {
      "epoch": 2.826,
      "grad_norm": 0.17265437543392181,
      "learning_rate": 3.233750000000001e-05,
      "loss": 0.0034,
      "step": 84780
    },
    {
      "epoch": 2.8263333333333334,
      "grad_norm": 0.172712042927742,
      "learning_rate": 3.2335416666666665e-05,
      "loss": 0.0032,
      "step": 84790
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.25912678241729736,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0023,
      "step": 84800
    },
    {
      "epoch": 2.827,
      "grad_norm": 0.2624669671058655,
      "learning_rate": 3.233125e-05,
      "loss": 0.0025,
      "step": 84810
    },
    {
      "epoch": 2.8273333333333333,
      "grad_norm": 0.43166008591651917,
      "learning_rate": 3.232916666666667e-05,
      "loss": 0.0025,
      "step": 84820
    },
    {
      "epoch": 2.8276666666666666,
      "grad_norm": 0.7480795979499817,
      "learning_rate": 3.2327083333333334e-05,
      "loss": 0.0017,
      "step": 84830
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.029111364856362343,
      "learning_rate": 3.2325e-05,
      "loss": 0.0029,
      "step": 84840
    },
    {
      "epoch": 2.828333333333333,
      "grad_norm": 0.11514812707901001,
      "learning_rate": 3.232291666666667e-05,
      "loss": 0.0018,
      "step": 84850
    },
    {
      "epoch": 2.828666666666667,
      "grad_norm": 0.23046067357063293,
      "learning_rate": 3.232083333333333e-05,
      "loss": 0.002,
      "step": 84860
    },
    {
      "epoch": 2.8289999999999997,
      "grad_norm": 0.11547999829053879,
      "learning_rate": 3.231875e-05,
      "loss": 0.0013,
      "step": 84870
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.17258378863334656,
      "learning_rate": 3.231666666666667e-05,
      "loss": 0.0029,
      "step": 84880
    },
    {
      "epoch": 2.8296666666666668,
      "grad_norm": 0.02969461679458618,
      "learning_rate": 3.2314583333333334e-05,
      "loss": 0.0028,
      "step": 84890
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.17267869412899017,
      "learning_rate": 3.23125e-05,
      "loss": 0.0012,
      "step": 84900
    },
    {
      "epoch": 2.8303333333333334,
      "grad_norm": 0.6831122040748596,
      "learning_rate": 3.231041666666667e-05,
      "loss": 0.0032,
      "step": 84910
    },
    {
      "epoch": 2.8306666666666667,
      "grad_norm": 0.08674754202365875,
      "learning_rate": 3.230833333333334e-05,
      "loss": 0.0022,
      "step": 84920
    },
    {
      "epoch": 2.831,
      "grad_norm": 0.20122624933719635,
      "learning_rate": 3.2306249999999996e-05,
      "loss": 0.003,
      "step": 84930
    },
    {
      "epoch": 2.8313333333333333,
      "grad_norm": 0.2590320408344269,
      "learning_rate": 3.230416666666667e-05,
      "loss": 0.0022,
      "step": 84940
    },
    {
      "epoch": 2.8316666666666666,
      "grad_norm": 0.14394664764404297,
      "learning_rate": 3.2302083333333334e-05,
      "loss": 0.0018,
      "step": 84950
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.2013099640607834,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0022,
      "step": 84960
    },
    {
      "epoch": 2.8323333333333336,
      "grad_norm": 0.5180926322937012,
      "learning_rate": 3.2297916666666665e-05,
      "loss": 0.002,
      "step": 84970
    },
    {
      "epoch": 2.8326666666666664,
      "grad_norm": 0.11556415259838104,
      "learning_rate": 3.229583333333334e-05,
      "loss": 0.0021,
      "step": 84980
    },
    {
      "epoch": 2.833,
      "grad_norm": 0.058021143078804016,
      "learning_rate": 3.229375e-05,
      "loss": 0.0024,
      "step": 84990
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.2301974892616272,
      "learning_rate": 3.229166666666667e-05,
      "loss": 0.0027,
      "step": 85000
    },
    {
      "epoch": 2.833666666666667,
      "grad_norm": 0.2880070209503174,
      "learning_rate": 3.228958333333333e-05,
      "loss": 0.0035,
      "step": 85010
    },
    {
      "epoch": 2.834,
      "grad_norm": 0.3165639638900757,
      "learning_rate": 3.2287500000000006e-05,
      "loss": 0.0032,
      "step": 85020
    },
    {
      "epoch": 2.8343333333333334,
      "grad_norm": 0.2013557255268097,
      "learning_rate": 3.228541666666667e-05,
      "loss": 0.0018,
      "step": 85030
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.2015891820192337,
      "learning_rate": 3.2283333333333337e-05,
      "loss": 0.0024,
      "step": 85040
    },
    {
      "epoch": 2.835,
      "grad_norm": 0.25906217098236084,
      "learning_rate": 3.228125e-05,
      "loss": 0.0017,
      "step": 85050
    },
    {
      "epoch": 2.8353333333333333,
      "grad_norm": 0.20100264251232147,
      "learning_rate": 3.227916666666667e-05,
      "loss": 0.0021,
      "step": 85060
    },
    {
      "epoch": 2.8356666666666666,
      "grad_norm": 0.5752633213996887,
      "learning_rate": 3.227708333333333e-05,
      "loss": 0.0027,
      "step": 85070
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.08649421483278275,
      "learning_rate": 3.2275e-05,
      "loss": 0.0038,
      "step": 85080
    },
    {
      "epoch": 2.836333333333333,
      "grad_norm": 0.20125581324100494,
      "learning_rate": 3.227291666666667e-05,
      "loss": 0.0018,
      "step": 85090
    },
    {
      "epoch": 2.836666666666667,
      "grad_norm": 0.08661847561597824,
      "learning_rate": 3.2270833333333336e-05,
      "loss": 0.0022,
      "step": 85100
    },
    {
      "epoch": 2.8369999999999997,
      "grad_norm": 0.029006734490394592,
      "learning_rate": 3.226875e-05,
      "loss": 0.002,
      "step": 85110
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.6906369924545288,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0028,
      "step": 85120
    },
    {
      "epoch": 2.837666666666667,
      "grad_norm": 0.17256803810596466,
      "learning_rate": 3.226458333333334e-05,
      "loss": 0.0021,
      "step": 85130
    },
    {
      "epoch": 2.838,
      "grad_norm": 0.08648470789194107,
      "learning_rate": 3.22625e-05,
      "loss": 0.0017,
      "step": 85140
    },
    {
      "epoch": 2.8383333333333334,
      "grad_norm": 0.42962807416915894,
      "learning_rate": 3.226041666666667e-05,
      "loss": 0.0027,
      "step": 85150
    },
    {
      "epoch": 2.8386666666666667,
      "grad_norm": 0.11567861586809158,
      "learning_rate": 3.2258333333333336e-05,
      "loss": 0.0025,
      "step": 85160
    },
    {
      "epoch": 2.839,
      "grad_norm": 0.3451845645904541,
      "learning_rate": 3.225625e-05,
      "loss": 0.0014,
      "step": 85170
    },
    {
      "epoch": 2.8393333333333333,
      "grad_norm": 0.20165273547172546,
      "learning_rate": 3.225416666666667e-05,
      "loss": 0.0023,
      "step": 85180
    },
    {
      "epoch": 2.8396666666666666,
      "grad_norm": 0.029518544673919678,
      "learning_rate": 3.225208333333333e-05,
      "loss": 0.0021,
      "step": 85190
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.1438797414302826,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.0026,
      "step": 85200
    },
    {
      "epoch": 2.8403333333333336,
      "grad_norm": 0.14404575526714325,
      "learning_rate": 3.224791666666666e-05,
      "loss": 0.0013,
      "step": 85210
    },
    {
      "epoch": 2.8406666666666665,
      "grad_norm": 0.3659035265445709,
      "learning_rate": 3.2245833333333336e-05,
      "loss": 0.0023,
      "step": 85220
    },
    {
      "epoch": 2.841,
      "grad_norm": 0.2016090750694275,
      "learning_rate": 3.224375e-05,
      "loss": 0.0026,
      "step": 85230
    },
    {
      "epoch": 2.8413333333333335,
      "grad_norm": 0.08634119480848312,
      "learning_rate": 3.224166666666667e-05,
      "loss": 0.0032,
      "step": 85240
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 0.11556529253721237,
      "learning_rate": 3.223958333333333e-05,
      "loss": 0.0028,
      "step": 85250
    },
    {
      "epoch": 2.842,
      "grad_norm": 0.4602203369140625,
      "learning_rate": 3.2237500000000004e-05,
      "loss": 0.0022,
      "step": 85260
    },
    {
      "epoch": 2.8423333333333334,
      "grad_norm": 0.16713374853134155,
      "learning_rate": 3.223541666666667e-05,
      "loss": 0.0027,
      "step": 85270
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.05817312374711037,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.002,
      "step": 85280
    },
    {
      "epoch": 2.843,
      "grad_norm": 0.3770894706249237,
      "learning_rate": 3.223125e-05,
      "loss": 0.0031,
      "step": 85290
    },
    {
      "epoch": 2.8433333333333333,
      "grad_norm": 0.02926900051534176,
      "learning_rate": 3.2229166666666666e-05,
      "loss": 0.0014,
      "step": 85300
    },
    {
      "epoch": 2.8436666666666666,
      "grad_norm": 0.14375942945480347,
      "learning_rate": 3.222708333333334e-05,
      "loss": 0.0021,
      "step": 85310
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.058772601187229156,
      "learning_rate": 3.2225e-05,
      "loss": 0.0016,
      "step": 85320
    },
    {
      "epoch": 2.844333333333333,
      "grad_norm": 0.4025641679763794,
      "learning_rate": 3.222291666666667e-05,
      "loss": 0.0024,
      "step": 85330
    },
    {
      "epoch": 2.844666666666667,
      "grad_norm": 0.4214548170566559,
      "learning_rate": 3.2220833333333335e-05,
      "loss": 0.0031,
      "step": 85340
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 0.5984789729118347,
      "learning_rate": 3.221875e-05,
      "loss": 0.0027,
      "step": 85350
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.5057247281074524,
      "learning_rate": 3.2216666666666666e-05,
      "loss": 0.0052,
      "step": 85360
    },
    {
      "epoch": 2.845666666666667,
      "grad_norm": 0.11598019301891327,
      "learning_rate": 3.221458333333334e-05,
      "loss": 0.0042,
      "step": 85370
    },
    {
      "epoch": 2.846,
      "grad_norm": 0.14420543611049652,
      "learning_rate": 3.2212500000000004e-05,
      "loss": 0.0022,
      "step": 85380
    },
    {
      "epoch": 2.8463333333333334,
      "grad_norm": 0.34590181708335876,
      "learning_rate": 3.221041666666667e-05,
      "loss": 0.003,
      "step": 85390
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.4316238462924957,
      "learning_rate": 3.2208333333333335e-05,
      "loss": 0.0026,
      "step": 85400
    },
    {
      "epoch": 2.847,
      "grad_norm": 0.20161238312721252,
      "learning_rate": 3.220625e-05,
      "loss": 0.002,
      "step": 85410
    },
    {
      "epoch": 2.8473333333333333,
      "grad_norm": 0.05786483362317085,
      "learning_rate": 3.2204166666666666e-05,
      "loss": 0.002,
      "step": 85420
    },
    {
      "epoch": 2.8476666666666666,
      "grad_norm": 0.144039124250412,
      "learning_rate": 3.220208333333333e-05,
      "loss": 0.0022,
      "step": 85430
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.31705671548843384,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0022,
      "step": 85440
    },
    {
      "epoch": 2.8483333333333336,
      "grad_norm": 0.057876765727996826,
      "learning_rate": 3.219791666666667e-05,
      "loss": 0.0029,
      "step": 85450
    },
    {
      "epoch": 2.8486666666666665,
      "grad_norm": 0.009819387458264828,
      "learning_rate": 3.2195833333333334e-05,
      "loss": 0.0023,
      "step": 85460
    },
    {
      "epoch": 2.849,
      "grad_norm": 0.057614024728536606,
      "learning_rate": 3.219375e-05,
      "loss": 0.0032,
      "step": 85470
    },
    {
      "epoch": 2.8493333333333335,
      "grad_norm": 0.2588092088699341,
      "learning_rate": 3.219166666666667e-05,
      "loss": 0.0024,
      "step": 85480
    },
    {
      "epoch": 2.849666666666667,
      "grad_norm": 0.05801624804735184,
      "learning_rate": 3.218958333333333e-05,
      "loss": 0.0024,
      "step": 85490
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.31637200713157654,
      "learning_rate": 3.21875e-05,
      "loss": 0.0029,
      "step": 85500
    },
    {
      "epoch": 2.8503333333333334,
      "grad_norm": 0.0576859675347805,
      "learning_rate": 3.218541666666667e-05,
      "loss": 0.0028,
      "step": 85510
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.08647086471319199,
      "learning_rate": 3.218333333333334e-05,
      "loss": 0.0011,
      "step": 85520
    },
    {
      "epoch": 2.851,
      "grad_norm": 0.08652979135513306,
      "learning_rate": 3.218125e-05,
      "loss": 0.0018,
      "step": 85530
    },
    {
      "epoch": 2.8513333333333333,
      "grad_norm": 0.028931645676493645,
      "learning_rate": 3.2179166666666665e-05,
      "loss": 0.0022,
      "step": 85540
    },
    {
      "epoch": 2.8516666666666666,
      "grad_norm": 0.2592671811580658,
      "learning_rate": 3.217708333333334e-05,
      "loss": 0.0027,
      "step": 85550
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.08640707284212112,
      "learning_rate": 3.2175e-05,
      "loss": 0.003,
      "step": 85560
    },
    {
      "epoch": 2.852333333333333,
      "grad_norm": 0.029106175526976585,
      "learning_rate": 3.217291666666667e-05,
      "loss": 0.0026,
      "step": 85570
    },
    {
      "epoch": 2.852666666666667,
      "grad_norm": 0.28795865178108215,
      "learning_rate": 3.2170833333333334e-05,
      "loss": 0.0021,
      "step": 85580
    },
    {
      "epoch": 2.8529999999999998,
      "grad_norm": 0.02950497716665268,
      "learning_rate": 3.2168750000000006e-05,
      "loss": 0.0035,
      "step": 85590
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.14392349123954773,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0019,
      "step": 85600
    },
    {
      "epoch": 2.853666666666667,
      "grad_norm": 0.681649386882782,
      "learning_rate": 3.216458333333334e-05,
      "loss": 0.0023,
      "step": 85610
    },
    {
      "epoch": 2.854,
      "grad_norm": 0.006672812160104513,
      "learning_rate": 3.21625e-05,
      "loss": 0.0025,
      "step": 85620
    },
    {
      "epoch": 2.8543333333333334,
      "grad_norm": 0.029402954503893852,
      "learning_rate": 3.216041666666667e-05,
      "loss": 0.0022,
      "step": 85630
    },
    {
      "epoch": 2.8546666666666667,
      "grad_norm": 0.2875915765762329,
      "learning_rate": 3.2158333333333333e-05,
      "loss": 0.0022,
      "step": 85640
    },
    {
      "epoch": 2.855,
      "grad_norm": 0.20156806707382202,
      "learning_rate": 3.215625e-05,
      "loss": 0.0024,
      "step": 85650
    },
    {
      "epoch": 2.8553333333333333,
      "grad_norm": 0.3454635441303253,
      "learning_rate": 3.215416666666667e-05,
      "loss": 0.0023,
      "step": 85660
    },
    {
      "epoch": 2.8556666666666666,
      "grad_norm": 0.4604659378528595,
      "learning_rate": 3.215208333333333e-05,
      "loss": 0.0029,
      "step": 85670
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.11567877978086472,
      "learning_rate": 3.215e-05,
      "loss": 0.0019,
      "step": 85680
    },
    {
      "epoch": 2.856333333333333,
      "grad_norm": 0.02989461086690426,
      "learning_rate": 3.214791666666667e-05,
      "loss": 0.0028,
      "step": 85690
    },
    {
      "epoch": 2.8566666666666665,
      "grad_norm": 0.14377498626708984,
      "learning_rate": 3.214583333333333e-05,
      "loss": 0.0015,
      "step": 85700
    },
    {
      "epoch": 2.857,
      "grad_norm": 0.11557450890541077,
      "learning_rate": 3.214375e-05,
      "loss": 0.0027,
      "step": 85710
    },
    {
      "epoch": 2.857333333333333,
      "grad_norm": 0.4030754566192627,
      "learning_rate": 3.214166666666667e-05,
      "loss": 0.002,
      "step": 85720
    },
    {
      "epoch": 2.857666666666667,
      "grad_norm": 0.201669380068779,
      "learning_rate": 3.2139583333333336e-05,
      "loss": 0.0024,
      "step": 85730
    },
    {
      "epoch": 2.858,
      "grad_norm": 0.17534296214580536,
      "learning_rate": 3.21375e-05,
      "loss": 0.0031,
      "step": 85740
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 0.2877660393714905,
      "learning_rate": 3.213541666666667e-05,
      "loss": 0.0015,
      "step": 85750
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.14429423213005066,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0022,
      "step": 85760
    },
    {
      "epoch": 2.859,
      "grad_norm": 0.23014409840106964,
      "learning_rate": 3.213125e-05,
      "loss": 0.0035,
      "step": 85770
    },
    {
      "epoch": 2.8593333333333333,
      "grad_norm": 0.2213372439146042,
      "learning_rate": 3.2129166666666664e-05,
      "loss": 0.0018,
      "step": 85780
    },
    {
      "epoch": 2.8596666666666666,
      "grad_norm": 0.40260112285614014,
      "learning_rate": 3.2127083333333336e-05,
      "loss": 0.0023,
      "step": 85790
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.2301533818244934,
      "learning_rate": 3.2125e-05,
      "loss": 0.0025,
      "step": 85800
    },
    {
      "epoch": 2.860333333333333,
      "grad_norm": 0.0294693261384964,
      "learning_rate": 3.212291666666667e-05,
      "loss": 0.0022,
      "step": 85810
    },
    {
      "epoch": 2.860666666666667,
      "grad_norm": 0.28733018040657043,
      "learning_rate": 3.212083333333333e-05,
      "loss": 0.0017,
      "step": 85820
    },
    {
      "epoch": 2.8609999999999998,
      "grad_norm": 0.9449477791786194,
      "learning_rate": 3.2118750000000005e-05,
      "loss": 0.0028,
      "step": 85830
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.25889649987220764,
      "learning_rate": 3.211666666666667e-05,
      "loss": 0.0026,
      "step": 85840
    },
    {
      "epoch": 2.861666666666667,
      "grad_norm": 0.2585933804512024,
      "learning_rate": 3.2114583333333336e-05,
      "loss": 0.0029,
      "step": 85850
    },
    {
      "epoch": 2.862,
      "grad_norm": 0.7478379011154175,
      "learning_rate": 3.21125e-05,
      "loss": 0.0026,
      "step": 85860
    },
    {
      "epoch": 2.8623333333333334,
      "grad_norm": 0.17241232097148895,
      "learning_rate": 3.2110416666666674e-05,
      "loss": 0.003,
      "step": 85870
    },
    {
      "epoch": 2.8626666666666667,
      "grad_norm": 0.4024500548839569,
      "learning_rate": 3.210833333333333e-05,
      "loss": 0.0018,
      "step": 85880
    },
    {
      "epoch": 2.863,
      "grad_norm": 0.17332430183887482,
      "learning_rate": 3.210625e-05,
      "loss": 0.0017,
      "step": 85890
    },
    {
      "epoch": 2.8633333333333333,
      "grad_norm": 0.11510338634252548,
      "learning_rate": 3.210416666666667e-05,
      "loss": 0.0022,
      "step": 85900
    },
    {
      "epoch": 2.8636666666666666,
      "grad_norm": 0.2877059876918793,
      "learning_rate": 3.2102083333333335e-05,
      "loss": 0.0015,
      "step": 85910
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.11496543884277344,
      "learning_rate": 3.21e-05,
      "loss": 0.0029,
      "step": 85920
    },
    {
      "epoch": 2.864333333333333,
      "grad_norm": 0.05752260237932205,
      "learning_rate": 3.2097916666666666e-05,
      "loss": 0.0019,
      "step": 85930
    },
    {
      "epoch": 2.8646666666666665,
      "grad_norm": 0.05839709937572479,
      "learning_rate": 3.209583333333334e-05,
      "loss": 0.0028,
      "step": 85940
    },
    {
      "epoch": 2.865,
      "grad_norm": 0.028736509382724762,
      "learning_rate": 3.209375e-05,
      "loss": 0.002,
      "step": 85950
    },
    {
      "epoch": 2.865333333333333,
      "grad_norm": 0.1441846489906311,
      "learning_rate": 3.209166666666667e-05,
      "loss": 0.0018,
      "step": 85960
    },
    {
      "epoch": 2.865666666666667,
      "grad_norm": 0.20161934196949005,
      "learning_rate": 3.2089583333333335e-05,
      "loss": 0.0034,
      "step": 85970
    },
    {
      "epoch": 2.866,
      "grad_norm": 0.029816467314958572,
      "learning_rate": 3.20875e-05,
      "loss": 0.0029,
      "step": 85980
    },
    {
      "epoch": 2.8663333333333334,
      "grad_norm": 0.17269515991210938,
      "learning_rate": 3.2085416666666666e-05,
      "loss": 0.0022,
      "step": 85990
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.3737253248691559,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.0017,
      "step": 86000
    },
    {
      "epoch": 2.867,
      "grad_norm": 0.2876245081424713,
      "learning_rate": 3.2081250000000004e-05,
      "loss": 0.0022,
      "step": 86010
    },
    {
      "epoch": 2.8673333333333333,
      "grad_norm": 0.23020954430103302,
      "learning_rate": 3.207916666666666e-05,
      "loss": 0.002,
      "step": 86020
    },
    {
      "epoch": 2.8676666666666666,
      "grad_norm": 0.17264938354492188,
      "learning_rate": 3.2077083333333335e-05,
      "loss": 0.0025,
      "step": 86030
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.029029127210378647,
      "learning_rate": 3.2075e-05,
      "loss": 0.0018,
      "step": 86040
    },
    {
      "epoch": 2.868333333333333,
      "grad_norm": 0.2603817284107208,
      "learning_rate": 3.2072916666666666e-05,
      "loss": 0.0024,
      "step": 86050
    },
    {
      "epoch": 2.868666666666667,
      "grad_norm": 0.115111343562603,
      "learning_rate": 3.207083333333333e-05,
      "loss": 0.0021,
      "step": 86060
    },
    {
      "epoch": 2.8689999999999998,
      "grad_norm": 0.05751032382249832,
      "learning_rate": 3.2068750000000004e-05,
      "loss": 0.0025,
      "step": 86070
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.40247684717178345,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0025,
      "step": 86080
    },
    {
      "epoch": 2.869666666666667,
      "grad_norm": 0.5753631591796875,
      "learning_rate": 3.2064583333333335e-05,
      "loss": 0.0026,
      "step": 86090
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.20185540616512299,
      "learning_rate": 3.20625e-05,
      "loss": 0.0017,
      "step": 86100
    },
    {
      "epoch": 2.8703333333333334,
      "grad_norm": 0.08641526103019714,
      "learning_rate": 3.206041666666667e-05,
      "loss": 0.0039,
      "step": 86110
    },
    {
      "epoch": 2.8706666666666667,
      "grad_norm": 0.029331227764487267,
      "learning_rate": 3.205833333333334e-05,
      "loss": 0.0021,
      "step": 86120
    },
    {
      "epoch": 2.871,
      "grad_norm": 0.05760450288653374,
      "learning_rate": 3.2056249999999997e-05,
      "loss": 0.0019,
      "step": 86130
    },
    {
      "epoch": 2.8713333333333333,
      "grad_norm": 0.1723775565624237,
      "learning_rate": 3.205416666666667e-05,
      "loss": 0.0022,
      "step": 86140
    },
    {
      "epoch": 2.8716666666666666,
      "grad_norm": 0.2876860499382019,
      "learning_rate": 3.2052083333333334e-05,
      "loss": 0.0021,
      "step": 86150
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.23553335666656494,
      "learning_rate": 3.205e-05,
      "loss": 0.0025,
      "step": 86160
    },
    {
      "epoch": 2.872333333333333,
      "grad_norm": 0.08681954443454742,
      "learning_rate": 3.2047916666666665e-05,
      "loss": 0.0019,
      "step": 86170
    },
    {
      "epoch": 2.8726666666666665,
      "grad_norm": 0.1730639785528183,
      "learning_rate": 3.204583333333334e-05,
      "loss": 0.0021,
      "step": 86180
    },
    {
      "epoch": 2.873,
      "grad_norm": 0.057623472064733505,
      "learning_rate": 3.204375e-05,
      "loss": 0.0017,
      "step": 86190
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.20125813782215118,
      "learning_rate": 3.204166666666667e-05,
      "loss": 0.0027,
      "step": 86200
    },
    {
      "epoch": 2.873666666666667,
      "grad_norm": 0.005152777303010225,
      "learning_rate": 3.2039583333333334e-05,
      "loss": 0.003,
      "step": 86210
    },
    {
      "epoch": 2.874,
      "grad_norm": 0.1727258265018463,
      "learning_rate": 3.2037500000000006e-05,
      "loss": 0.002,
      "step": 86220
    },
    {
      "epoch": 2.8743333333333334,
      "grad_norm": 0.009111437946557999,
      "learning_rate": 3.2035416666666665e-05,
      "loss": 0.0017,
      "step": 86230
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.20225518941879272,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0019,
      "step": 86240
    },
    {
      "epoch": 2.875,
      "grad_norm": 0.37368032336235046,
      "learning_rate": 3.203125e-05,
      "loss": 0.0023,
      "step": 86250
    },
    {
      "epoch": 2.8753333333333333,
      "grad_norm": 0.20146183669567108,
      "learning_rate": 3.202916666666667e-05,
      "loss": 0.0023,
      "step": 86260
    },
    {
      "epoch": 2.8756666666666666,
      "grad_norm": 0.02903113327920437,
      "learning_rate": 3.2027083333333334e-05,
      "loss": 0.0023,
      "step": 86270
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.05755825713276863,
      "learning_rate": 3.2025e-05,
      "loss": 0.0012,
      "step": 86280
    },
    {
      "epoch": 2.876333333333333,
      "grad_norm": 0.05759713426232338,
      "learning_rate": 3.202291666666667e-05,
      "loss": 0.0019,
      "step": 86290
    },
    {
      "epoch": 2.876666666666667,
      "grad_norm": 0.057565733790397644,
      "learning_rate": 3.202083333333333e-05,
      "loss": 0.0031,
      "step": 86300
    },
    {
      "epoch": 2.877,
      "grad_norm": 0.20151492953300476,
      "learning_rate": 3.201875e-05,
      "loss": 0.0028,
      "step": 86310
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.006274170242249966,
      "learning_rate": 3.201666666666667e-05,
      "loss": 0.003,
      "step": 86320
    },
    {
      "epoch": 2.877666666666667,
      "grad_norm": 0.0579899363219738,
      "learning_rate": 3.201458333333333e-05,
      "loss": 0.0013,
      "step": 86330
    },
    {
      "epoch": 2.878,
      "grad_norm": 0.14388182759284973,
      "learning_rate": 3.20125e-05,
      "loss": 0.0022,
      "step": 86340
    },
    {
      "epoch": 2.8783333333333334,
      "grad_norm": 0.3453453481197357,
      "learning_rate": 3.201041666666667e-05,
      "loss": 0.0022,
      "step": 86350
    },
    {
      "epoch": 2.8786666666666667,
      "grad_norm": 0.14423532783985138,
      "learning_rate": 3.2008333333333337e-05,
      "loss": 0.0027,
      "step": 86360
    },
    {
      "epoch": 2.879,
      "grad_norm": 0.14378303289413452,
      "learning_rate": 3.200625e-05,
      "loss": 0.0029,
      "step": 86370
    },
    {
      "epoch": 2.8793333333333333,
      "grad_norm": 0.1438433676958084,
      "learning_rate": 3.200416666666667e-05,
      "loss": 0.0021,
      "step": 86380
    },
    {
      "epoch": 2.8796666666666666,
      "grad_norm": 0.230087548494339,
      "learning_rate": 3.200208333333333e-05,
      "loss": 0.0023,
      "step": 86390
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.02898304909467697,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0022,
      "step": 86400
    },
    {
      "epoch": 2.880333333333333,
      "grad_norm": 0.05761883780360222,
      "learning_rate": 3.1997916666666664e-05,
      "loss": 0.0011,
      "step": 86410
    },
    {
      "epoch": 2.8806666666666665,
      "grad_norm": 0.057672932744026184,
      "learning_rate": 3.1995833333333336e-05,
      "loss": 0.0024,
      "step": 86420
    },
    {
      "epoch": 2.8810000000000002,
      "grad_norm": 0.23080578446388245,
      "learning_rate": 3.199375e-05,
      "loss": 0.0029,
      "step": 86430
    },
    {
      "epoch": 2.881333333333333,
      "grad_norm": 0.14357446134090424,
      "learning_rate": 3.199166666666667e-05,
      "loss": 0.0017,
      "step": 86440
    },
    {
      "epoch": 2.881666666666667,
      "grad_norm": 0.11556370556354523,
      "learning_rate": 3.198958333333333e-05,
      "loss": 0.0023,
      "step": 86450
    },
    {
      "epoch": 2.882,
      "grad_norm": 0.34923502802848816,
      "learning_rate": 3.1987500000000005e-05,
      "loss": 0.0026,
      "step": 86460
    },
    {
      "epoch": 2.8823333333333334,
      "grad_norm": 0.11524016410112381,
      "learning_rate": 3.198541666666667e-05,
      "loss": 0.0026,
      "step": 86470
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.08632058650255203,
      "learning_rate": 3.1983333333333336e-05,
      "loss": 0.0019,
      "step": 86480
    },
    {
      "epoch": 2.883,
      "grad_norm": 0.08642302453517914,
      "learning_rate": 3.198125e-05,
      "loss": 0.0025,
      "step": 86490
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 0.2873961627483368,
      "learning_rate": 3.197916666666667e-05,
      "loss": 0.0023,
      "step": 86500
    },
    {
      "epoch": 2.8836666666666666,
      "grad_norm": 0.006159685552120209,
      "learning_rate": 3.197708333333333e-05,
      "loss": 0.0019,
      "step": 86510
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.1153508871793747,
      "learning_rate": 3.1975e-05,
      "loss": 0.0022,
      "step": 86520
    },
    {
      "epoch": 2.884333333333333,
      "grad_norm": 0.6343136429786682,
      "learning_rate": 3.197291666666667e-05,
      "loss": 0.0026,
      "step": 86530
    },
    {
      "epoch": 2.884666666666667,
      "grad_norm": 0.11525905132293701,
      "learning_rate": 3.1970833333333336e-05,
      "loss": 0.0019,
      "step": 86540
    },
    {
      "epoch": 2.885,
      "grad_norm": 0.058767128735780716,
      "learning_rate": 3.196875e-05,
      "loss": 0.0021,
      "step": 86550
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.17274165153503418,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.0019,
      "step": 86560
    },
    {
      "epoch": 2.885666666666667,
      "grad_norm": 0.08648372441530228,
      "learning_rate": 3.196458333333334e-05,
      "loss": 0.0024,
      "step": 86570
    },
    {
      "epoch": 2.886,
      "grad_norm": 0.11502949148416519,
      "learning_rate": 3.19625e-05,
      "loss": 0.0027,
      "step": 86580
    },
    {
      "epoch": 2.8863333333333334,
      "grad_norm": 0.34543606638908386,
      "learning_rate": 3.196041666666667e-05,
      "loss": 0.0024,
      "step": 86590
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.1153547465801239,
      "learning_rate": 3.1958333333333335e-05,
      "loss": 0.0026,
      "step": 86600
    },
    {
      "epoch": 2.887,
      "grad_norm": 0.0865124762058258,
      "learning_rate": 3.195625e-05,
      "loss": 0.0024,
      "step": 86610
    },
    {
      "epoch": 2.8873333333333333,
      "grad_norm": 0.17244231700897217,
      "learning_rate": 3.1954166666666666e-05,
      "loss": 0.0018,
      "step": 86620
    },
    {
      "epoch": 2.8876666666666666,
      "grad_norm": 0.4937829375267029,
      "learning_rate": 3.195208333333333e-05,
      "loss": 0.0019,
      "step": 86630
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.05779631435871124,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.002,
      "step": 86640
    },
    {
      "epoch": 2.888333333333333,
      "grad_norm": 0.9764201045036316,
      "learning_rate": 3.194791666666667e-05,
      "loss": 0.0025,
      "step": 86650
    },
    {
      "epoch": 2.8886666666666665,
      "grad_norm": 0.25897401571273804,
      "learning_rate": 3.1945833333333335e-05,
      "loss": 0.0028,
      "step": 86660
    },
    {
      "epoch": 2.8890000000000002,
      "grad_norm": 0.08699426800012589,
      "learning_rate": 3.194375e-05,
      "loss": 0.0017,
      "step": 86670
    },
    {
      "epoch": 2.889333333333333,
      "grad_norm": 0.05771448090672493,
      "learning_rate": 3.194166666666667e-05,
      "loss": 0.0028,
      "step": 86680
    },
    {
      "epoch": 2.889666666666667,
      "grad_norm": 0.08640924841165543,
      "learning_rate": 3.193958333333333e-05,
      "loss": 0.003,
      "step": 86690
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.17278380692005157,
      "learning_rate": 3.1937500000000004e-05,
      "loss": 0.0021,
      "step": 86700
    },
    {
      "epoch": 2.8903333333333334,
      "grad_norm": 0.057800810784101486,
      "learning_rate": 3.193541666666667e-05,
      "loss": 0.0015,
      "step": 86710
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.0574876070022583,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0024,
      "step": 86720
    },
    {
      "epoch": 2.891,
      "grad_norm": 0.08645766973495483,
      "learning_rate": 3.193125e-05,
      "loss": 0.0022,
      "step": 86730
    },
    {
      "epoch": 2.8913333333333333,
      "grad_norm": 0.2878953516483307,
      "learning_rate": 3.1929166666666666e-05,
      "loss": 0.0031,
      "step": 86740
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 0.058030806481838226,
      "learning_rate": 3.192708333333334e-05,
      "loss": 0.0028,
      "step": 86750
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.25892651081085205,
      "learning_rate": 3.1925e-05,
      "loss": 0.0025,
      "step": 86760
    },
    {
      "epoch": 2.892333333333333,
      "grad_norm": 0.006551598664373159,
      "learning_rate": 3.192291666666667e-05,
      "loss": 0.0019,
      "step": 86770
    },
    {
      "epoch": 2.892666666666667,
      "grad_norm": 0.0863937959074974,
      "learning_rate": 3.1920833333333334e-05,
      "loss": 0.0018,
      "step": 86780
    },
    {
      "epoch": 2.893,
      "grad_norm": 0.05776609480381012,
      "learning_rate": 3.191875e-05,
      "loss": 0.0016,
      "step": 86790
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.25899723172187805,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.0015,
      "step": 86800
    },
    {
      "epoch": 2.893666666666667,
      "grad_norm": 0.40272364020347595,
      "learning_rate": 3.191458333333334e-05,
      "loss": 0.0021,
      "step": 86810
    },
    {
      "epoch": 2.894,
      "grad_norm": 0.05800527334213257,
      "learning_rate": 3.19125e-05,
      "loss": 0.0019,
      "step": 86820
    },
    {
      "epoch": 2.8943333333333334,
      "grad_norm": 0.607190728187561,
      "learning_rate": 3.191041666666667e-05,
      "loss": 0.0018,
      "step": 86830
    },
    {
      "epoch": 2.8946666666666667,
      "grad_norm": 0.34542110562324524,
      "learning_rate": 3.1908333333333334e-05,
      "loss": 0.0021,
      "step": 86840
    },
    {
      "epoch": 2.895,
      "grad_norm": 0.14374646544456482,
      "learning_rate": 3.1906250000000006e-05,
      "loss": 0.0024,
      "step": 86850
    },
    {
      "epoch": 2.8953333333333333,
      "grad_norm": 0.057780880481004715,
      "learning_rate": 3.1904166666666665e-05,
      "loss": 0.0017,
      "step": 86860
    },
    {
      "epoch": 2.8956666666666666,
      "grad_norm": 0.20588834583759308,
      "learning_rate": 3.190208333333333e-05,
      "loss": 0.0024,
      "step": 86870
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.6574616432189941,
      "learning_rate": 3.19e-05,
      "loss": 0.0019,
      "step": 86880
    },
    {
      "epoch": 2.896333333333333,
      "grad_norm": 0.14380501210689545,
      "learning_rate": 3.189791666666667e-05,
      "loss": 0.0019,
      "step": 86890
    },
    {
      "epoch": 2.8966666666666665,
      "grad_norm": 0.2303154617547989,
      "learning_rate": 3.1895833333333334e-05,
      "loss": 0.0024,
      "step": 86900
    },
    {
      "epoch": 2.8970000000000002,
      "grad_norm": 0.23015835881233215,
      "learning_rate": 3.189375e-05,
      "loss": 0.0025,
      "step": 86910
    },
    {
      "epoch": 2.897333333333333,
      "grad_norm": 0.1157769113779068,
      "learning_rate": 3.189166666666667e-05,
      "loss": 0.002,
      "step": 86920
    },
    {
      "epoch": 2.897666666666667,
      "grad_norm": 0.1725197732448578,
      "learning_rate": 3.188958333333334e-05,
      "loss": 0.0025,
      "step": 86930
    },
    {
      "epoch": 2.898,
      "grad_norm": 0.11591726541519165,
      "learning_rate": 3.18875e-05,
      "loss": 0.0026,
      "step": 86940
    },
    {
      "epoch": 2.8983333333333334,
      "grad_norm": 0.20119395852088928,
      "learning_rate": 3.188541666666667e-05,
      "loss": 0.0028,
      "step": 86950
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.34849101305007935,
      "learning_rate": 3.188333333333334e-05,
      "loss": 0.0021,
      "step": 86960
    },
    {
      "epoch": 2.899,
      "grad_norm": 0.11564162373542786,
      "learning_rate": 3.188125e-05,
      "loss": 0.0033,
      "step": 86970
    },
    {
      "epoch": 2.8993333333333333,
      "grad_norm": 0.4026235044002533,
      "learning_rate": 3.1879166666666665e-05,
      "loss": 0.0023,
      "step": 86980
    },
    {
      "epoch": 2.8996666666666666,
      "grad_norm": 0.11564415693283081,
      "learning_rate": 3.187708333333334e-05,
      "loss": 0.0027,
      "step": 86990
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.03142482787370682,
      "learning_rate": 3.1875e-05,
      "loss": 0.0028,
      "step": 87000
    },
    {
      "epoch": 2.900333333333333,
      "grad_norm": 0.06079176813364029,
      "learning_rate": 3.187291666666667e-05,
      "loss": 0.0015,
      "step": 87010
    },
    {
      "epoch": 2.9006666666666665,
      "grad_norm": 0.23058395087718964,
      "learning_rate": 3.187083333333333e-05,
      "loss": 0.0027,
      "step": 87020
    },
    {
      "epoch": 2.901,
      "grad_norm": 0.05811621621251106,
      "learning_rate": 3.1868750000000006e-05,
      "loss": 0.002,
      "step": 87030
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.22982735931873322,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0023,
      "step": 87040
    },
    {
      "epoch": 2.9016666666666664,
      "grad_norm": 0.23027199506759644,
      "learning_rate": 3.1864583333333336e-05,
      "loss": 0.0019,
      "step": 87050
    },
    {
      "epoch": 2.902,
      "grad_norm": 0.22996561229228973,
      "learning_rate": 3.18625e-05,
      "loss": 0.0029,
      "step": 87060
    },
    {
      "epoch": 2.9023333333333334,
      "grad_norm": 0.2301519364118576,
      "learning_rate": 3.186041666666667e-05,
      "loss": 0.0022,
      "step": 87070
    },
    {
      "epoch": 2.9026666666666667,
      "grad_norm": 0.11534272879362106,
      "learning_rate": 3.185833333333333e-05,
      "loss": 0.0026,
      "step": 87080
    },
    {
      "epoch": 2.903,
      "grad_norm": 0.2876172661781311,
      "learning_rate": 3.1856250000000005e-05,
      "loss": 0.002,
      "step": 87090
    },
    {
      "epoch": 2.9033333333333333,
      "grad_norm": 0.11679760366678238,
      "learning_rate": 3.185416666666667e-05,
      "loss": 0.0036,
      "step": 87100
    },
    {
      "epoch": 2.9036666666666666,
      "grad_norm": 0.08615382760763168,
      "learning_rate": 3.185208333333333e-05,
      "loss": 0.0021,
      "step": 87110
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.20114901661872864,
      "learning_rate": 3.185e-05,
      "loss": 0.0021,
      "step": 87120
    },
    {
      "epoch": 2.904333333333333,
      "grad_norm": 0.4887706935405731,
      "learning_rate": 3.184791666666667e-05,
      "loss": 0.0024,
      "step": 87130
    },
    {
      "epoch": 2.9046666666666665,
      "grad_norm": 0.11507895588874817,
      "learning_rate": 3.184583333333333e-05,
      "loss": 0.0022,
      "step": 87140
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 0.02903088927268982,
      "learning_rate": 3.184375e-05,
      "loss": 0.0025,
      "step": 87150
    },
    {
      "epoch": 2.905333333333333,
      "grad_norm": 0.05782441049814224,
      "learning_rate": 3.184166666666667e-05,
      "loss": 0.0015,
      "step": 87160
    },
    {
      "epoch": 2.905666666666667,
      "grad_norm": 0.029121143743395805,
      "learning_rate": 3.1839583333333336e-05,
      "loss": 0.0021,
      "step": 87170
    },
    {
      "epoch": 2.906,
      "grad_norm": 0.20137935876846313,
      "learning_rate": 3.18375e-05,
      "loss": 0.0018,
      "step": 87180
    },
    {
      "epoch": 2.9063333333333334,
      "grad_norm": 0.23003336787223816,
      "learning_rate": 3.183541666666667e-05,
      "loss": 0.0028,
      "step": 87190
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.057532407343387604,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0015,
      "step": 87200
    },
    {
      "epoch": 2.907,
      "grad_norm": 0.1440473347902298,
      "learning_rate": 3.1831250000000005e-05,
      "loss": 0.0017,
      "step": 87210
    },
    {
      "epoch": 2.9073333333333333,
      "grad_norm": 0.004673132207244635,
      "learning_rate": 3.182916666666666e-05,
      "loss": 0.0023,
      "step": 87220
    },
    {
      "epoch": 2.9076666666666666,
      "grad_norm": 0.11596880108118057,
      "learning_rate": 3.1827083333333336e-05,
      "loss": 0.0022,
      "step": 87230
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.11537699401378632,
      "learning_rate": 3.1825e-05,
      "loss": 0.0018,
      "step": 87240
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 0.287587434053421,
      "learning_rate": 3.1822916666666667e-05,
      "loss": 0.003,
      "step": 87250
    },
    {
      "epoch": 2.9086666666666665,
      "grad_norm": 0.20169366896152496,
      "learning_rate": 3.182083333333333e-05,
      "loss": 0.0026,
      "step": 87260
    },
    {
      "epoch": 2.909,
      "grad_norm": 0.3737943172454834,
      "learning_rate": 3.1818750000000004e-05,
      "loss": 0.0026,
      "step": 87270
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.0577622726559639,
      "learning_rate": 3.181666666666667e-05,
      "loss": 0.0023,
      "step": 87280
    },
    {
      "epoch": 2.9096666666666664,
      "grad_norm": 0.14402644336223602,
      "learning_rate": 3.1814583333333335e-05,
      "loss": 0.0024,
      "step": 87290
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.2015509009361267,
      "learning_rate": 3.18125e-05,
      "loss": 0.0024,
      "step": 87300
    },
    {
      "epoch": 2.9103333333333334,
      "grad_norm": 0.08683209121227264,
      "learning_rate": 3.181041666666667e-05,
      "loss": 0.0022,
      "step": 87310
    },
    {
      "epoch": 2.9106666666666667,
      "grad_norm": 0.28787752985954285,
      "learning_rate": 3.180833333333333e-05,
      "loss": 0.0022,
      "step": 87320
    },
    {
      "epoch": 2.911,
      "grad_norm": 0.20142552256584167,
      "learning_rate": 3.1806250000000004e-05,
      "loss": 0.0022,
      "step": 87330
    },
    {
      "epoch": 2.9113333333333333,
      "grad_norm": 0.4315299987792969,
      "learning_rate": 3.180416666666667e-05,
      "loss": 0.0022,
      "step": 87340
    },
    {
      "epoch": 2.9116666666666666,
      "grad_norm": 0.029900703579187393,
      "learning_rate": 3.1802083333333335e-05,
      "loss": 0.0029,
      "step": 87350
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.02939680963754654,
      "learning_rate": 3.18e-05,
      "loss": 0.0014,
      "step": 87360
    },
    {
      "epoch": 2.912333333333333,
      "grad_norm": 0.08649832755327225,
      "learning_rate": 3.1797916666666666e-05,
      "loss": 0.002,
      "step": 87370
    },
    {
      "epoch": 2.9126666666666665,
      "grad_norm": 0.20147082209587097,
      "learning_rate": 3.179583333333334e-05,
      "loss": 0.0018,
      "step": 87380
    },
    {
      "epoch": 2.9130000000000003,
      "grad_norm": 0.6326117515563965,
      "learning_rate": 3.179375e-05,
      "loss": 0.0019,
      "step": 87390
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.37385696172714233,
      "learning_rate": 3.179166666666667e-05,
      "loss": 0.0023,
      "step": 87400
    },
    {
      "epoch": 2.913666666666667,
      "grad_norm": 0.3450441360473633,
      "learning_rate": 3.1789583333333335e-05,
      "loss": 0.002,
      "step": 87410
    },
    {
      "epoch": 2.914,
      "grad_norm": 0.14390593767166138,
      "learning_rate": 3.17875e-05,
      "loss": 0.0017,
      "step": 87420
    },
    {
      "epoch": 2.9143333333333334,
      "grad_norm": 0.08649405092000961,
      "learning_rate": 3.1785416666666666e-05,
      "loss": 0.0023,
      "step": 87430
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.1439368724822998,
      "learning_rate": 3.178333333333334e-05,
      "loss": 0.0026,
      "step": 87440
    },
    {
      "epoch": 2.915,
      "grad_norm": 0.11552032083272934,
      "learning_rate": 3.1781250000000003e-05,
      "loss": 0.0019,
      "step": 87450
    },
    {
      "epoch": 2.9153333333333333,
      "grad_norm": 0.05790313705801964,
      "learning_rate": 3.177916666666666e-05,
      "loss": 0.0031,
      "step": 87460
    },
    {
      "epoch": 2.9156666666666666,
      "grad_norm": 0.11510352045297623,
      "learning_rate": 3.1777083333333334e-05,
      "loss": 0.002,
      "step": 87470
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.14395637810230255,
      "learning_rate": 3.1775e-05,
      "loss": 0.0016,
      "step": 87480
    },
    {
      "epoch": 2.916333333333333,
      "grad_norm": 0.2011546641588211,
      "learning_rate": 3.177291666666667e-05,
      "loss": 0.0029,
      "step": 87490
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.19438250362873077,
      "learning_rate": 3.177083333333333e-05,
      "loss": 0.0022,
      "step": 87500
    },
    {
      "epoch": 2.917,
      "grad_norm": 0.40245237946510315,
      "learning_rate": 3.176875e-05,
      "loss": 0.0025,
      "step": 87510
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.1722976565361023,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.0025,
      "step": 87520
    },
    {
      "epoch": 2.9176666666666664,
      "grad_norm": 0.5007091760635376,
      "learning_rate": 3.1764583333333334e-05,
      "loss": 0.002,
      "step": 87530
    },
    {
      "epoch": 2.918,
      "grad_norm": 0.3737867772579193,
      "learning_rate": 3.17625e-05,
      "loss": 0.0018,
      "step": 87540
    },
    {
      "epoch": 2.9183333333333334,
      "grad_norm": 0.11589772999286652,
      "learning_rate": 3.176041666666667e-05,
      "loss": 0.0023,
      "step": 87550
    },
    {
      "epoch": 2.9186666666666667,
      "grad_norm": 0.22981101274490356,
      "learning_rate": 3.175833333333334e-05,
      "loss": 0.0024,
      "step": 87560
    },
    {
      "epoch": 2.919,
      "grad_norm": 0.11557160317897797,
      "learning_rate": 3.175625e-05,
      "loss": 0.0026,
      "step": 87570
    },
    {
      "epoch": 2.9193333333333333,
      "grad_norm": 0.6547874212265015,
      "learning_rate": 3.175416666666667e-05,
      "loss": 0.0022,
      "step": 87580
    },
    {
      "epoch": 2.9196666666666666,
      "grad_norm": 0.4956981837749481,
      "learning_rate": 3.1752083333333334e-05,
      "loss": 0.0021,
      "step": 87590
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.40592968463897705,
      "learning_rate": 3.175e-05,
      "loss": 0.0031,
      "step": 87600
    },
    {
      "epoch": 2.9203333333333332,
      "grad_norm": 0.34505414962768555,
      "learning_rate": 3.1747916666666665e-05,
      "loss": 0.003,
      "step": 87610
    },
    {
      "epoch": 2.9206666666666665,
      "grad_norm": 0.34495997428894043,
      "learning_rate": 3.174583333333334e-05,
      "loss": 0.0026,
      "step": 87620
    },
    {
      "epoch": 2.9210000000000003,
      "grad_norm": 0.11524193733930588,
      "learning_rate": 3.174375e-05,
      "loss": 0.0019,
      "step": 87630
    },
    {
      "epoch": 2.921333333333333,
      "grad_norm": 0.2012438327074051,
      "learning_rate": 3.174166666666667e-05,
      "loss": 0.002,
      "step": 87640
    },
    {
      "epoch": 2.921666666666667,
      "grad_norm": 0.05813102424144745,
      "learning_rate": 3.1739583333333333e-05,
      "loss": 0.0023,
      "step": 87650
    },
    {
      "epoch": 2.922,
      "grad_norm": 0.17277835309505463,
      "learning_rate": 3.1737500000000006e-05,
      "loss": 0.0019,
      "step": 87660
    },
    {
      "epoch": 2.9223333333333334,
      "grad_norm": 0.05772662162780762,
      "learning_rate": 3.1735416666666664e-05,
      "loss": 0.0028,
      "step": 87670
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.11481086164712906,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0022,
      "step": 87680
    },
    {
      "epoch": 2.923,
      "grad_norm": 0.23007096350193024,
      "learning_rate": 3.173125e-05,
      "loss": 0.0017,
      "step": 87690
    },
    {
      "epoch": 2.9233333333333333,
      "grad_norm": 0.17270877957344055,
      "learning_rate": 3.172916666666667e-05,
      "loss": 0.0035,
      "step": 87700
    },
    {
      "epoch": 2.9236666666666666,
      "grad_norm": 0.00561049347743392,
      "learning_rate": 3.172708333333333e-05,
      "loss": 0.0026,
      "step": 87710
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.05782075598835945,
      "learning_rate": 3.1725e-05,
      "loss": 0.0023,
      "step": 87720
    },
    {
      "epoch": 2.9243333333333332,
      "grad_norm": 0.2298954576253891,
      "learning_rate": 3.172291666666667e-05,
      "loss": 0.002,
      "step": 87730
    },
    {
      "epoch": 2.9246666666666665,
      "grad_norm": 0.0037876323331147432,
      "learning_rate": 3.172083333333333e-05,
      "loss": 0.0018,
      "step": 87740
    },
    {
      "epoch": 2.925,
      "grad_norm": 0.0312375295907259,
      "learning_rate": 3.171875e-05,
      "loss": 0.0015,
      "step": 87750
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.029133278876543045,
      "learning_rate": 3.171666666666667e-05,
      "loss": 0.0022,
      "step": 87760
    },
    {
      "epoch": 2.9256666666666664,
      "grad_norm": 0.05808606371283531,
      "learning_rate": 3.171458333333334e-05,
      "loss": 0.0016,
      "step": 87770
    },
    {
      "epoch": 2.926,
      "grad_norm": 0.344992995262146,
      "learning_rate": 3.17125e-05,
      "loss": 0.0017,
      "step": 87780
    },
    {
      "epoch": 2.9263333333333335,
      "grad_norm": 0.258766233921051,
      "learning_rate": 3.171041666666667e-05,
      "loss": 0.0017,
      "step": 87790
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.12639248371124268,
      "learning_rate": 3.1708333333333336e-05,
      "loss": 0.0026,
      "step": 87800
    },
    {
      "epoch": 2.927,
      "grad_norm": 0.3454470932483673,
      "learning_rate": 3.170625e-05,
      "loss": 0.0022,
      "step": 87810
    },
    {
      "epoch": 2.9273333333333333,
      "grad_norm": 0.37344300746917725,
      "learning_rate": 3.170416666666667e-05,
      "loss": 0.0021,
      "step": 87820
    },
    {
      "epoch": 2.9276666666666666,
      "grad_norm": 0.143683522939682,
      "learning_rate": 3.170208333333333e-05,
      "loss": 0.0016,
      "step": 87830
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.6269175410270691,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0022,
      "step": 87840
    },
    {
      "epoch": 2.9283333333333332,
      "grad_norm": 0.11526454240083694,
      "learning_rate": 3.1697916666666664e-05,
      "loss": 0.002,
      "step": 87850
    },
    {
      "epoch": 2.9286666666666665,
      "grad_norm": 0.057661715894937515,
      "learning_rate": 3.1695833333333336e-05,
      "loss": 0.0018,
      "step": 87860
    },
    {
      "epoch": 2.9290000000000003,
      "grad_norm": 0.05766148865222931,
      "learning_rate": 3.169375e-05,
      "loss": 0.0022,
      "step": 87870
    },
    {
      "epoch": 2.929333333333333,
      "grad_norm": 0.23017002642154694,
      "learning_rate": 3.169166666666667e-05,
      "loss": 0.0019,
      "step": 87880
    },
    {
      "epoch": 2.929666666666667,
      "grad_norm": 0.17278237640857697,
      "learning_rate": 3.168958333333333e-05,
      "loss": 0.002,
      "step": 87890
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.23012754321098328,
      "learning_rate": 3.1687500000000005e-05,
      "loss": 0.0023,
      "step": 87900
    },
    {
      "epoch": 2.9303333333333335,
      "grad_norm": 0.20119328796863556,
      "learning_rate": 3.168541666666667e-05,
      "loss": 0.0021,
      "step": 87910
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.34462037682533264,
      "learning_rate": 3.1683333333333335e-05,
      "loss": 0.002,
      "step": 87920
    },
    {
      "epoch": 2.931,
      "grad_norm": 0.2299882471561432,
      "learning_rate": 3.168125e-05,
      "loss": 0.0021,
      "step": 87930
    },
    {
      "epoch": 2.9313333333333333,
      "grad_norm": 0.20125184953212738,
      "learning_rate": 3.167916666666667e-05,
      "loss": 0.0021,
      "step": 87940
    },
    {
      "epoch": 2.9316666666666666,
      "grad_norm": 0.2011081576347351,
      "learning_rate": 3.167708333333333e-05,
      "loss": 0.0015,
      "step": 87950
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.11488930135965347,
      "learning_rate": 3.1675e-05,
      "loss": 0.0022,
      "step": 87960
    },
    {
      "epoch": 2.9323333333333332,
      "grad_norm": 0.28784269094467163,
      "learning_rate": 3.167291666666667e-05,
      "loss": 0.0031,
      "step": 87970
    },
    {
      "epoch": 2.9326666666666665,
      "grad_norm": 0.5462297201156616,
      "learning_rate": 3.1670833333333335e-05,
      "loss": 0.0023,
      "step": 87980
    },
    {
      "epoch": 2.933,
      "grad_norm": 0.48858124017715454,
      "learning_rate": 3.166875e-05,
      "loss": 0.002,
      "step": 87990
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.25893113017082214,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0025,
      "step": 88000
    },
    {
      "epoch": 2.9336666666666664,
      "grad_norm": 0.029183099046349525,
      "learning_rate": 3.166458333333334e-05,
      "loss": 0.0025,
      "step": 88010
    },
    {
      "epoch": 2.934,
      "grad_norm": 0.08657185733318329,
      "learning_rate": 3.16625e-05,
      "loss": 0.0018,
      "step": 88020
    },
    {
      "epoch": 2.9343333333333335,
      "grad_norm": 0.005503446329385042,
      "learning_rate": 3.166041666666667e-05,
      "loss": 0.002,
      "step": 88030
    },
    {
      "epoch": 2.9346666666666668,
      "grad_norm": 0.23017601668834686,
      "learning_rate": 3.1658333333333335e-05,
      "loss": 0.0026,
      "step": 88040
    },
    {
      "epoch": 2.935,
      "grad_norm": 0.1724400669336319,
      "learning_rate": 3.165625000000001e-05,
      "loss": 0.0025,
      "step": 88050
    },
    {
      "epoch": 2.9353333333333333,
      "grad_norm": 0.23159483075141907,
      "learning_rate": 3.1654166666666666e-05,
      "loss": 0.0025,
      "step": 88060
    },
    {
      "epoch": 2.9356666666666666,
      "grad_norm": 0.02937949076294899,
      "learning_rate": 3.165208333333333e-05,
      "loss": 0.0021,
      "step": 88070
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.11485134810209274,
      "learning_rate": 3.1650000000000004e-05,
      "loss": 0.0017,
      "step": 88080
    },
    {
      "epoch": 2.9363333333333332,
      "grad_norm": 0.40234285593032837,
      "learning_rate": 3.164791666666667e-05,
      "loss": 0.0026,
      "step": 88090
    },
    {
      "epoch": 2.9366666666666665,
      "grad_norm": 1.2489099502563477,
      "learning_rate": 3.1645833333333335e-05,
      "loss": 0.0019,
      "step": 88100
    },
    {
      "epoch": 2.9370000000000003,
      "grad_norm": 0.02923850528895855,
      "learning_rate": 3.164375e-05,
      "loss": 0.0022,
      "step": 88110
    },
    {
      "epoch": 2.937333333333333,
      "grad_norm": 0.0576336570084095,
      "learning_rate": 3.164166666666667e-05,
      "loss": 0.0022,
      "step": 88120
    },
    {
      "epoch": 2.937666666666667,
      "grad_norm": 0.3449742794036865,
      "learning_rate": 3.163958333333333e-05,
      "loss": 0.0023,
      "step": 88130
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 0.14392094314098358,
      "learning_rate": 3.16375e-05,
      "loss": 0.0019,
      "step": 88140
    },
    {
      "epoch": 2.9383333333333335,
      "grad_norm": 0.3738296329975128,
      "learning_rate": 3.163541666666667e-05,
      "loss": 0.0032,
      "step": 88150
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.11211986094713211,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.0021,
      "step": 88160
    },
    {
      "epoch": 2.939,
      "grad_norm": 0.17305928468704224,
      "learning_rate": 3.163125e-05,
      "loss": 0.0017,
      "step": 88170
    },
    {
      "epoch": 2.9393333333333334,
      "grad_norm": 0.4446629285812378,
      "learning_rate": 3.162916666666667e-05,
      "loss": 0.0029,
      "step": 88180
    },
    {
      "epoch": 2.9396666666666667,
      "grad_norm": 0.1441369503736496,
      "learning_rate": 3.162708333333334e-05,
      "loss": 0.0031,
      "step": 88190
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.3446842133998871,
      "learning_rate": 3.1624999999999996e-05,
      "loss": 0.0033,
      "step": 88200
    },
    {
      "epoch": 2.9403333333333332,
      "grad_norm": 0.11495881527662277,
      "learning_rate": 3.162291666666667e-05,
      "loss": 0.0026,
      "step": 88210
    },
    {
      "epoch": 2.9406666666666665,
      "grad_norm": 0.11992814391851425,
      "learning_rate": 3.1620833333333334e-05,
      "loss": 0.0019,
      "step": 88220
    },
    {
      "epoch": 2.941,
      "grad_norm": 0.029580818489193916,
      "learning_rate": 3.161875e-05,
      "loss": 0.002,
      "step": 88230
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.029211103916168213,
      "learning_rate": 3.1616666666666665e-05,
      "loss": 0.0015,
      "step": 88240
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 0.2012009471654892,
      "learning_rate": 3.161458333333334e-05,
      "loss": 0.0026,
      "step": 88250
    },
    {
      "epoch": 2.942,
      "grad_norm": 0.316102534532547,
      "learning_rate": 3.16125e-05,
      "loss": 0.003,
      "step": 88260
    },
    {
      "epoch": 2.9423333333333335,
      "grad_norm": 0.17249467968940735,
      "learning_rate": 3.161041666666667e-05,
      "loss": 0.0016,
      "step": 88270
    },
    {
      "epoch": 2.9426666666666668,
      "grad_norm": 0.17231279611587524,
      "learning_rate": 3.1608333333333334e-05,
      "loss": 0.0033,
      "step": 88280
    },
    {
      "epoch": 2.943,
      "grad_norm": 0.08636568486690521,
      "learning_rate": 3.1606250000000006e-05,
      "loss": 0.0021,
      "step": 88290
    },
    {
      "epoch": 2.9433333333333334,
      "grad_norm": 0.20126406848430634,
      "learning_rate": 3.160416666666667e-05,
      "loss": 0.0028,
      "step": 88300
    },
    {
      "epoch": 2.9436666666666667,
      "grad_norm": 0.008919370360672474,
      "learning_rate": 3.160208333333333e-05,
      "loss": 0.0018,
      "step": 88310
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.2012534886598587,
      "learning_rate": 3.16e-05,
      "loss": 0.0015,
      "step": 88320
    },
    {
      "epoch": 2.9443333333333332,
      "grad_norm": 0.08622478693723679,
      "learning_rate": 3.159791666666667e-05,
      "loss": 0.0018,
      "step": 88330
    },
    {
      "epoch": 2.9446666666666665,
      "grad_norm": 0.20113079249858856,
      "learning_rate": 3.159583333333333e-05,
      "loss": 0.0018,
      "step": 88340
    },
    {
      "epoch": 2.945,
      "grad_norm": 0.6036364436149597,
      "learning_rate": 3.159375e-05,
      "loss": 0.0036,
      "step": 88350
    },
    {
      "epoch": 2.945333333333333,
      "grad_norm": 0.057846710085868835,
      "learning_rate": 3.159166666666667e-05,
      "loss": 0.0022,
      "step": 88360
    },
    {
      "epoch": 2.945666666666667,
      "grad_norm": 0.05771173536777496,
      "learning_rate": 3.1589583333333337e-05,
      "loss": 0.0014,
      "step": 88370
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 0.2584376335144043,
      "learning_rate": 3.15875e-05,
      "loss": 0.002,
      "step": 88380
    },
    {
      "epoch": 2.9463333333333335,
      "grad_norm": 0.11550002545118332,
      "learning_rate": 3.158541666666667e-05,
      "loss": 0.0016,
      "step": 88390
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.20116782188415527,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.0015,
      "step": 88400
    },
    {
      "epoch": 2.947,
      "grad_norm": 0.28739944100379944,
      "learning_rate": 3.158125e-05,
      "loss": 0.0027,
      "step": 88410
    },
    {
      "epoch": 2.9473333333333334,
      "grad_norm": 0.11530183255672455,
      "learning_rate": 3.157916666666667e-05,
      "loss": 0.0015,
      "step": 88420
    },
    {
      "epoch": 2.9476666666666667,
      "grad_norm": 0.25863659381866455,
      "learning_rate": 3.1577083333333336e-05,
      "loss": 0.0025,
      "step": 88430
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.8189848065376282,
      "learning_rate": 3.1575e-05,
      "loss": 0.0024,
      "step": 88440
    },
    {
      "epoch": 2.9483333333333333,
      "grad_norm": 0.12907256186008453,
      "learning_rate": 3.157291666666667e-05,
      "loss": 0.0017,
      "step": 88450
    },
    {
      "epoch": 2.9486666666666665,
      "grad_norm": 0.28742823004722595,
      "learning_rate": 3.157083333333333e-05,
      "loss": 0.0027,
      "step": 88460
    },
    {
      "epoch": 2.949,
      "grad_norm": 0.029216928407549858,
      "learning_rate": 3.1568750000000005e-05,
      "loss": 0.0044,
      "step": 88470
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.02914387173950672,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0026,
      "step": 88480
    },
    {
      "epoch": 2.9496666666666664,
      "grad_norm": 0.1725245714187622,
      "learning_rate": 3.1564583333333336e-05,
      "loss": 0.0018,
      "step": 88490
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.287624329328537,
      "learning_rate": 3.15625e-05,
      "loss": 0.0026,
      "step": 88500
    },
    {
      "epoch": 2.9503333333333335,
      "grad_norm": 0.029148515313863754,
      "learning_rate": 3.156041666666667e-05,
      "loss": 0.0023,
      "step": 88510
    },
    {
      "epoch": 2.9506666666666668,
      "grad_norm": 0.05760222300887108,
      "learning_rate": 3.155833333333333e-05,
      "loss": 0.0025,
      "step": 88520
    },
    {
      "epoch": 2.951,
      "grad_norm": 0.17262700200080872,
      "learning_rate": 3.1556250000000005e-05,
      "loss": 0.003,
      "step": 88530
    },
    {
      "epoch": 2.9513333333333334,
      "grad_norm": 0.04808181896805763,
      "learning_rate": 3.155416666666667e-05,
      "loss": 0.0025,
      "step": 88540
    },
    {
      "epoch": 2.9516666666666667,
      "grad_norm": 0.37357521057128906,
      "learning_rate": 3.155208333333333e-05,
      "loss": 0.0022,
      "step": 88550
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.0648193210363388,
      "learning_rate": 3.155e-05,
      "loss": 0.0028,
      "step": 88560
    },
    {
      "epoch": 2.9523333333333333,
      "grad_norm": 0.003509755712002516,
      "learning_rate": 3.154791666666667e-05,
      "loss": 0.0024,
      "step": 88570
    },
    {
      "epoch": 2.9526666666666666,
      "grad_norm": 0.20139507949352264,
      "learning_rate": 3.154583333333334e-05,
      "loss": 0.0021,
      "step": 88580
    },
    {
      "epoch": 2.953,
      "grad_norm": 0.11492903530597687,
      "learning_rate": 3.154375e-05,
      "loss": 0.0023,
      "step": 88590
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.20147134363651276,
      "learning_rate": 3.154166666666667e-05,
      "loss": 0.0033,
      "step": 88600
    },
    {
      "epoch": 2.953666666666667,
      "grad_norm": 0.05779678374528885,
      "learning_rate": 3.1539583333333335e-05,
      "loss": 0.0018,
      "step": 88610
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 0.17267683148384094,
      "learning_rate": 3.15375e-05,
      "loss": 0.0018,
      "step": 88620
    },
    {
      "epoch": 2.9543333333333335,
      "grad_norm": 0.08615804463624954,
      "learning_rate": 3.1535416666666666e-05,
      "loss": 0.0034,
      "step": 88630
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.0864194855093956,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0026,
      "step": 88640
    },
    {
      "epoch": 2.955,
      "grad_norm": 0.11522657424211502,
      "learning_rate": 3.1531250000000004e-05,
      "loss": 0.0025,
      "step": 88650
    },
    {
      "epoch": 2.9553333333333334,
      "grad_norm": 0.057821694761514664,
      "learning_rate": 3.152916666666667e-05,
      "loss": 0.0015,
      "step": 88660
    },
    {
      "epoch": 2.9556666666666667,
      "grad_norm": 0.2586710751056671,
      "learning_rate": 3.1527083333333335e-05,
      "loss": 0.0026,
      "step": 88670
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.45987939834594727,
      "learning_rate": 3.1525e-05,
      "loss": 0.002,
      "step": 88680
    },
    {
      "epoch": 2.9563333333333333,
      "grad_norm": 0.43113797903060913,
      "learning_rate": 3.1522916666666666e-05,
      "loss": 0.0023,
      "step": 88690
    },
    {
      "epoch": 2.9566666666666666,
      "grad_norm": 0.14388541877269745,
      "learning_rate": 3.152083333333333e-05,
      "loss": 0.002,
      "step": 88700
    },
    {
      "epoch": 2.957,
      "grad_norm": 0.029182491824030876,
      "learning_rate": 3.1518750000000004e-05,
      "loss": 0.0019,
      "step": 88710
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.029251789674162865,
      "learning_rate": 3.151666666666667e-05,
      "loss": 0.0018,
      "step": 88720
    },
    {
      "epoch": 2.9576666666666664,
      "grad_norm": 0.14384602010250092,
      "learning_rate": 3.1514583333333335e-05,
      "loss": 0.0025,
      "step": 88730
    },
    {
      "epoch": 2.958,
      "grad_norm": 0.6321851015090942,
      "learning_rate": 3.15125e-05,
      "loss": 0.0022,
      "step": 88740
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 0.3448742628097534,
      "learning_rate": 3.151041666666667e-05,
      "loss": 0.0014,
      "step": 88750
    },
    {
      "epoch": 2.958666666666667,
      "grad_norm": 0.660781979560852,
      "learning_rate": 3.150833333333333e-05,
      "loss": 0.002,
      "step": 88760
    },
    {
      "epoch": 2.959,
      "grad_norm": 0.0575764961540699,
      "learning_rate": 3.1506250000000003e-05,
      "loss": 0.0022,
      "step": 88770
    },
    {
      "epoch": 2.9593333333333334,
      "grad_norm": 0.08632290363311768,
      "learning_rate": 3.150416666666667e-05,
      "loss": 0.0024,
      "step": 88780
    },
    {
      "epoch": 2.9596666666666667,
      "grad_norm": 0.00885738618671894,
      "learning_rate": 3.1502083333333334e-05,
      "loss": 0.0027,
      "step": 88790
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.23000264167785645,
      "learning_rate": 3.15e-05,
      "loss": 0.002,
      "step": 88800
    },
    {
      "epoch": 2.9603333333333333,
      "grad_norm": 0.1725068837404251,
      "learning_rate": 3.1497916666666665e-05,
      "loss": 0.0022,
      "step": 88810
    },
    {
      "epoch": 2.9606666666666666,
      "grad_norm": 0.08653676509857178,
      "learning_rate": 3.149583333333334e-05,
      "loss": 0.0019,
      "step": 88820
    },
    {
      "epoch": 2.961,
      "grad_norm": 0.11494860053062439,
      "learning_rate": 3.1493749999999996e-05,
      "loss": 0.0022,
      "step": 88830
    },
    {
      "epoch": 2.961333333333333,
      "grad_norm": 0.024869995191693306,
      "learning_rate": 3.149166666666667e-05,
      "loss": 0.0025,
      "step": 88840
    },
    {
      "epoch": 2.961666666666667,
      "grad_norm": 0.058105457574129105,
      "learning_rate": 3.1489583333333334e-05,
      "loss": 0.0023,
      "step": 88850
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 0.3315916657447815,
      "learning_rate": 3.1487500000000006e-05,
      "loss": 0.0018,
      "step": 88860
    },
    {
      "epoch": 2.9623333333333335,
      "grad_norm": 0.7187237739562988,
      "learning_rate": 3.1485416666666665e-05,
      "loss": 0.0031,
      "step": 88870
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.40226873755455017,
      "learning_rate": 3.148333333333334e-05,
      "loss": 0.0025,
      "step": 88880
    },
    {
      "epoch": 2.963,
      "grad_norm": 0.22996197640895844,
      "learning_rate": 3.148125e-05,
      "loss": 0.0017,
      "step": 88890
    },
    {
      "epoch": 2.9633333333333334,
      "grad_norm": 0.05880005657672882,
      "learning_rate": 3.147916666666667e-05,
      "loss": 0.0034,
      "step": 88900
    },
    {
      "epoch": 2.9636666666666667,
      "grad_norm": 0.11502043157815933,
      "learning_rate": 3.1477083333333334e-05,
      "loss": 0.0027,
      "step": 88910
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.20139406621456146,
      "learning_rate": 3.1475e-05,
      "loss": 0.0039,
      "step": 88920
    },
    {
      "epoch": 2.9643333333333333,
      "grad_norm": 0.11501788347959518,
      "learning_rate": 3.147291666666667e-05,
      "loss": 0.0018,
      "step": 88930
    },
    {
      "epoch": 2.9646666666666666,
      "grad_norm": 0.05769090726971626,
      "learning_rate": 3.147083333333333e-05,
      "loss": 0.0018,
      "step": 88940
    },
    {
      "epoch": 2.965,
      "grad_norm": 0.02905733697116375,
      "learning_rate": 3.146875e-05,
      "loss": 0.0031,
      "step": 88950
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.20116393268108368,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0029,
      "step": 88960
    },
    {
      "epoch": 2.9656666666666665,
      "grad_norm": 0.14395692944526672,
      "learning_rate": 3.1464583333333334e-05,
      "loss": 0.0023,
      "step": 88970
    },
    {
      "epoch": 2.966,
      "grad_norm": 0.029471302404999733,
      "learning_rate": 3.14625e-05,
      "loss": 0.0029,
      "step": 88980
    },
    {
      "epoch": 2.9663333333333335,
      "grad_norm": 0.3162401616573334,
      "learning_rate": 3.146041666666667e-05,
      "loss": 0.0018,
      "step": 88990
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.14389660954475403,
      "learning_rate": 3.145833333333334e-05,
      "loss": 0.0024,
      "step": 89000
    },
    {
      "epoch": 2.967,
      "grad_norm": 0.2586965262889862,
      "learning_rate": 3.145625e-05,
      "loss": 0.0023,
      "step": 89010
    },
    {
      "epoch": 2.9673333333333334,
      "grad_norm": 0.28769630193710327,
      "learning_rate": 3.145416666666667e-05,
      "loss": 0.0026,
      "step": 89020
    },
    {
      "epoch": 2.9676666666666667,
      "grad_norm": 0.3737887144088745,
      "learning_rate": 3.145208333333334e-05,
      "loss": 0.0018,
      "step": 89030
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.20126578211784363,
      "learning_rate": 3.145e-05,
      "loss": 0.0013,
      "step": 89040
    },
    {
      "epoch": 2.9683333333333333,
      "grad_norm": 0.11492304503917694,
      "learning_rate": 3.1447916666666664e-05,
      "loss": 0.0018,
      "step": 89050
    },
    {
      "epoch": 2.9686666666666666,
      "grad_norm": 0.029217928647994995,
      "learning_rate": 3.1445833333333336e-05,
      "loss": 0.002,
      "step": 89060
    },
    {
      "epoch": 2.969,
      "grad_norm": 0.31608423590660095,
      "learning_rate": 3.144375e-05,
      "loss": 0.0016,
      "step": 89070
    },
    {
      "epoch": 2.969333333333333,
      "grad_norm": 0.20120194554328918,
      "learning_rate": 3.144166666666667e-05,
      "loss": 0.0022,
      "step": 89080
    },
    {
      "epoch": 2.969666666666667,
      "grad_norm": 0.08665184676647186,
      "learning_rate": 3.143958333333333e-05,
      "loss": 0.0028,
      "step": 89090
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.1725611835718155,
      "learning_rate": 3.1437500000000005e-05,
      "loss": 0.0024,
      "step": 89100
    },
    {
      "epoch": 2.9703333333333335,
      "grad_norm": 0.7224867343902588,
      "learning_rate": 3.1435416666666664e-05,
      "loss": 0.0028,
      "step": 89110
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.14362801611423492,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.0022,
      "step": 89120
    },
    {
      "epoch": 2.971,
      "grad_norm": 0.5953307747840881,
      "learning_rate": 3.143125e-05,
      "loss": 0.002,
      "step": 89130
    },
    {
      "epoch": 2.9713333333333334,
      "grad_norm": 0.17246566712856293,
      "learning_rate": 3.1429166666666674e-05,
      "loss": 0.0029,
      "step": 89140
    },
    {
      "epoch": 2.9716666666666667,
      "grad_norm": 0.18543384969234467,
      "learning_rate": 3.142708333333333e-05,
      "loss": 0.0016,
      "step": 89150
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.5638413429260254,
      "learning_rate": 3.1425e-05,
      "loss": 0.0026,
      "step": 89160
    },
    {
      "epoch": 2.9723333333333333,
      "grad_norm": 0.029239891096949577,
      "learning_rate": 3.142291666666667e-05,
      "loss": 0.0027,
      "step": 89170
    },
    {
      "epoch": 2.9726666666666666,
      "grad_norm": 0.37341541051864624,
      "learning_rate": 3.1420833333333336e-05,
      "loss": 0.0018,
      "step": 89180
    },
    {
      "epoch": 2.973,
      "grad_norm": 0.2873193919658661,
      "learning_rate": 3.141875e-05,
      "loss": 0.0024,
      "step": 89190
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.05754905566573143,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.0024,
      "step": 89200
    },
    {
      "epoch": 2.9736666666666665,
      "grad_norm": 0.5174109935760498,
      "learning_rate": 3.141458333333334e-05,
      "loss": 0.0018,
      "step": 89210
    },
    {
      "epoch": 2.974,
      "grad_norm": 0.14383310079574585,
      "learning_rate": 3.14125e-05,
      "loss": 0.0022,
      "step": 89220
    },
    {
      "epoch": 2.9743333333333335,
      "grad_norm": 0.20124909281730652,
      "learning_rate": 3.141041666666667e-05,
      "loss": 0.0022,
      "step": 89230
    },
    {
      "epoch": 2.974666666666667,
      "grad_norm": 0.14381380379199982,
      "learning_rate": 3.1408333333333336e-05,
      "loss": 0.0016,
      "step": 89240
    },
    {
      "epoch": 2.975,
      "grad_norm": 0.005802489351481199,
      "learning_rate": 3.140625e-05,
      "loss": 0.0019,
      "step": 89250
    },
    {
      "epoch": 2.9753333333333334,
      "grad_norm": 0.08663854002952576,
      "learning_rate": 3.1404166666666667e-05,
      "loss": 0.0024,
      "step": 89260
    },
    {
      "epoch": 2.9756666666666667,
      "grad_norm": 0.5461542010307312,
      "learning_rate": 3.140208333333334e-05,
      "loss": 0.0027,
      "step": 89270
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.08647016435861588,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0021,
      "step": 89280
    },
    {
      "epoch": 2.9763333333333333,
      "grad_norm": 0.05771774426102638,
      "learning_rate": 3.139791666666666e-05,
      "loss": 0.0021,
      "step": 89290
    },
    {
      "epoch": 2.9766666666666666,
      "grad_norm": 0.14362221956253052,
      "learning_rate": 3.1395833333333335e-05,
      "loss": 0.0026,
      "step": 89300
    },
    {
      "epoch": 2.977,
      "grad_norm": 0.1725984513759613,
      "learning_rate": 3.139375e-05,
      "loss": 0.0019,
      "step": 89310
    },
    {
      "epoch": 2.977333333333333,
      "grad_norm": 0.1444532424211502,
      "learning_rate": 3.1391666666666666e-05,
      "loss": 0.0021,
      "step": 89320
    },
    {
      "epoch": 2.977666666666667,
      "grad_norm": 0.20154868066310883,
      "learning_rate": 3.138958333333333e-05,
      "loss": 0.0025,
      "step": 89330
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 0.08646271377801895,
      "learning_rate": 3.1387500000000004e-05,
      "loss": 0.0025,
      "step": 89340
    },
    {
      "epoch": 2.9783333333333335,
      "grad_norm": 0.14409297704696655,
      "learning_rate": 3.138541666666667e-05,
      "loss": 0.0032,
      "step": 89350
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.11508040875196457,
      "learning_rate": 3.1383333333333335e-05,
      "loss": 0.002,
      "step": 89360
    },
    {
      "epoch": 2.979,
      "grad_norm": 0.25870269536972046,
      "learning_rate": 3.138125e-05,
      "loss": 0.0025,
      "step": 89370
    },
    {
      "epoch": 2.9793333333333334,
      "grad_norm": 0.006927842739969492,
      "learning_rate": 3.137916666666667e-05,
      "loss": 0.0025,
      "step": 89380
    },
    {
      "epoch": 2.9796666666666667,
      "grad_norm": 0.14387837052345276,
      "learning_rate": 3.137708333333333e-05,
      "loss": 0.0017,
      "step": 89390
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.3645687401294708,
      "learning_rate": 3.1375e-05,
      "loss": 0.003,
      "step": 89400
    },
    {
      "epoch": 2.9803333333333333,
      "grad_norm": 0.02900555171072483,
      "learning_rate": 3.137291666666667e-05,
      "loss": 0.0017,
      "step": 89410
    },
    {
      "epoch": 2.9806666666666666,
      "grad_norm": 0.08635026961565018,
      "learning_rate": 3.1370833333333335e-05,
      "loss": 0.0022,
      "step": 89420
    },
    {
      "epoch": 2.981,
      "grad_norm": 0.3450586199760437,
      "learning_rate": 3.136875e-05,
      "loss": 0.0021,
      "step": 89430
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.20121604204177856,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0023,
      "step": 89440
    },
    {
      "epoch": 2.9816666666666665,
      "grad_norm": 0.005467612762004137,
      "learning_rate": 3.136458333333334e-05,
      "loss": 0.0022,
      "step": 89450
    },
    {
      "epoch": 2.982,
      "grad_norm": 0.11527811735868454,
      "learning_rate": 3.13625e-05,
      "loss": 0.0028,
      "step": 89460
    },
    {
      "epoch": 2.982333333333333,
      "grad_norm": 0.006120626349002123,
      "learning_rate": 3.136041666666667e-05,
      "loss": 0.0028,
      "step": 89470
    },
    {
      "epoch": 2.982666666666667,
      "grad_norm": 0.1436862349510193,
      "learning_rate": 3.1358333333333334e-05,
      "loss": 0.0022,
      "step": 89480
    },
    {
      "epoch": 2.983,
      "grad_norm": 0.2875412106513977,
      "learning_rate": 3.135625000000001e-05,
      "loss": 0.0029,
      "step": 89490
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 0.1436770260334015,
      "learning_rate": 3.1354166666666665e-05,
      "loss": 0.0019,
      "step": 89500
    },
    {
      "epoch": 2.9836666666666667,
      "grad_norm": 0.05774552747607231,
      "learning_rate": 3.135208333333334e-05,
      "loss": 0.0028,
      "step": 89510
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.4027450680732727,
      "learning_rate": 3.135e-05,
      "loss": 0.0017,
      "step": 89520
    },
    {
      "epoch": 2.9843333333333333,
      "grad_norm": 1.782546043395996,
      "learning_rate": 3.134791666666667e-05,
      "loss": 0.0037,
      "step": 89530
    },
    {
      "epoch": 2.9846666666666666,
      "grad_norm": 0.0030065507162362337,
      "learning_rate": 3.1345833333333334e-05,
      "loss": 0.0024,
      "step": 89540
    },
    {
      "epoch": 2.985,
      "grad_norm": 0.2873893082141876,
      "learning_rate": 3.134375e-05,
      "loss": 0.0026,
      "step": 89550
    },
    {
      "epoch": 2.985333333333333,
      "grad_norm": 0.20117639005184174,
      "learning_rate": 3.134166666666667e-05,
      "loss": 0.0024,
      "step": 89560
    },
    {
      "epoch": 2.985666666666667,
      "grad_norm": 0.14390884339809418,
      "learning_rate": 3.133958333333333e-05,
      "loss": 0.0021,
      "step": 89570
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 0.08655739575624466,
      "learning_rate": 3.13375e-05,
      "loss": 0.0031,
      "step": 89580
    },
    {
      "epoch": 2.9863333333333335,
      "grad_norm": 0.1724170744419098,
      "learning_rate": 3.133541666666667e-05,
      "loss": 0.0023,
      "step": 89590
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.143697127699852,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0022,
      "step": 89600
    },
    {
      "epoch": 2.987,
      "grad_norm": 0.12188664078712463,
      "learning_rate": 3.133125e-05,
      "loss": 0.002,
      "step": 89610
    },
    {
      "epoch": 2.9873333333333334,
      "grad_norm": 0.0862586721777916,
      "learning_rate": 3.132916666666667e-05,
      "loss": 0.002,
      "step": 89620
    },
    {
      "epoch": 2.9876666666666667,
      "grad_norm": 0.08682096004486084,
      "learning_rate": 3.132708333333334e-05,
      "loss": 0.0025,
      "step": 89630
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.25868526101112366,
      "learning_rate": 3.1324999999999996e-05,
      "loss": 0.0027,
      "step": 89640
    },
    {
      "epoch": 2.9883333333333333,
      "grad_norm": 0.006157387513667345,
      "learning_rate": 3.132291666666667e-05,
      "loss": 0.0016,
      "step": 89650
    },
    {
      "epoch": 2.9886666666666666,
      "grad_norm": 0.40667620301246643,
      "learning_rate": 3.1320833333333333e-05,
      "loss": 0.0026,
      "step": 89660
    },
    {
      "epoch": 2.989,
      "grad_norm": 0.20133812725543976,
      "learning_rate": 3.131875e-05,
      "loss": 0.0023,
      "step": 89670
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.27416089177131653,
      "learning_rate": 3.1316666666666664e-05,
      "loss": 0.003,
      "step": 89680
    },
    {
      "epoch": 2.9896666666666665,
      "grad_norm": 0.22988976538181305,
      "learning_rate": 3.131458333333334e-05,
      "loss": 0.0023,
      "step": 89690
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.3310611844062805,
      "learning_rate": 3.13125e-05,
      "loss": 0.0017,
      "step": 89700
    },
    {
      "epoch": 2.990333333333333,
      "grad_norm": 0.20121872425079346,
      "learning_rate": 3.131041666666667e-05,
      "loss": 0.0037,
      "step": 89710
    },
    {
      "epoch": 2.990666666666667,
      "grad_norm": 0.2013256847858429,
      "learning_rate": 3.130833333333333e-05,
      "loss": 0.0023,
      "step": 89720
    },
    {
      "epoch": 2.991,
      "grad_norm": 0.22971442341804504,
      "learning_rate": 3.1306250000000005e-05,
      "loss": 0.0026,
      "step": 89730
    },
    {
      "epoch": 2.9913333333333334,
      "grad_norm": 0.20109815895557404,
      "learning_rate": 3.130416666666667e-05,
      "loss": 0.0015,
      "step": 89740
    },
    {
      "epoch": 2.9916666666666667,
      "grad_norm": 0.029709603637456894,
      "learning_rate": 3.1302083333333336e-05,
      "loss": 0.0028,
      "step": 89750
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.029343748465180397,
      "learning_rate": 3.13e-05,
      "loss": 0.0019,
      "step": 89760
    },
    {
      "epoch": 2.9923333333333333,
      "grad_norm": 0.17290933430194855,
      "learning_rate": 3.129791666666667e-05,
      "loss": 0.0024,
      "step": 89770
    },
    {
      "epoch": 2.9926666666666666,
      "grad_norm": 0.172308087348938,
      "learning_rate": 3.129583333333333e-05,
      "loss": 0.0018,
      "step": 89780
    },
    {
      "epoch": 2.993,
      "grad_norm": 0.3733278512954712,
      "learning_rate": 3.129375e-05,
      "loss": 0.0034,
      "step": 89790
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.08605953305959702,
      "learning_rate": 3.129166666666667e-05,
      "loss": 0.0021,
      "step": 89800
    },
    {
      "epoch": 2.993666666666667,
      "grad_norm": 0.0863579660654068,
      "learning_rate": 3.1289583333333336e-05,
      "loss": 0.0016,
      "step": 89810
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 0.029349546879529953,
      "learning_rate": 3.12875e-05,
      "loss": 0.0025,
      "step": 89820
    },
    {
      "epoch": 2.9943333333333335,
      "grad_norm": 0.37356069684028625,
      "learning_rate": 3.128541666666667e-05,
      "loss": 0.0021,
      "step": 89830
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.086253821849823,
      "learning_rate": 3.128333333333334e-05,
      "loss": 0.0023,
      "step": 89840
    },
    {
      "epoch": 2.995,
      "grad_norm": 0.05773578956723213,
      "learning_rate": 3.128125e-05,
      "loss": 0.0019,
      "step": 89850
    },
    {
      "epoch": 2.9953333333333334,
      "grad_norm": 0.35865819454193115,
      "learning_rate": 3.127916666666667e-05,
      "loss": 0.0025,
      "step": 89860
    },
    {
      "epoch": 2.9956666666666667,
      "grad_norm": 0.14620491862297058,
      "learning_rate": 3.1277083333333336e-05,
      "loss": 0.003,
      "step": 89870
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.008546008728444576,
      "learning_rate": 3.1275e-05,
      "loss": 0.0017,
      "step": 89880
    },
    {
      "epoch": 2.9963333333333333,
      "grad_norm": 0.08627444505691528,
      "learning_rate": 3.127291666666667e-05,
      "loss": 0.0016,
      "step": 89890
    },
    {
      "epoch": 2.9966666666666666,
      "grad_norm": 0.057801149785518646,
      "learning_rate": 3.127083333333333e-05,
      "loss": 0.0022,
      "step": 89900
    },
    {
      "epoch": 2.997,
      "grad_norm": 0.17234617471694946,
      "learning_rate": 3.1268750000000004e-05,
      "loss": 0.0019,
      "step": 89910
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.11524645984172821,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0022,
      "step": 89920
    },
    {
      "epoch": 2.9976666666666665,
      "grad_norm": 0.028799373656511307,
      "learning_rate": 3.1264583333333335e-05,
      "loss": 0.0026,
      "step": 89930
    },
    {
      "epoch": 2.998,
      "grad_norm": 0.11522575467824936,
      "learning_rate": 3.12625e-05,
      "loss": 0.002,
      "step": 89940
    },
    {
      "epoch": 2.998333333333333,
      "grad_norm": 0.05767587199807167,
      "learning_rate": 3.1260416666666666e-05,
      "loss": 0.0012,
      "step": 89950
    },
    {
      "epoch": 2.998666666666667,
      "grad_norm": 0.3161650598049164,
      "learning_rate": 3.125833333333333e-05,
      "loss": 0.0033,
      "step": 89960
    },
    {
      "epoch": 2.999,
      "grad_norm": 0.40224626660346985,
      "learning_rate": 3.1256250000000004e-05,
      "loss": 0.0027,
      "step": 89970
    },
    {
      "epoch": 2.9993333333333334,
      "grad_norm": 0.386360764503479,
      "learning_rate": 3.125416666666667e-05,
      "loss": 0.0017,
      "step": 89980
    },
    {
      "epoch": 2.9996666666666667,
      "grad_norm": 0.23054370284080505,
      "learning_rate": 3.1252083333333335e-05,
      "loss": 0.0035,
      "step": 89990
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.029649997130036354,
      "learning_rate": 3.125e-05,
      "loss": 0.0015,
      "step": 90000
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.0022999930661171675,
      "eval_runtime": 127.2647,
      "eval_samples_per_second": 1571.527,
      "eval_steps_per_second": 39.288,
      "step": 90000
    },
    {
      "epoch": 3.0003333333333333,
      "grad_norm": 0.14391179382801056,
      "learning_rate": 3.1247916666666666e-05,
      "loss": 0.0024,
      "step": 90010
    },
    {
      "epoch": 3.0006666666666666,
      "grad_norm": 0.028905276209115982,
      "learning_rate": 3.124583333333334e-05,
      "loss": 0.0026,
      "step": 90020
    },
    {
      "epoch": 3.001,
      "grad_norm": 0.17226538062095642,
      "learning_rate": 3.124375e-05,
      "loss": 0.0022,
      "step": 90030
    },
    {
      "epoch": 3.001333333333333,
      "grad_norm": 0.17237836122512817,
      "learning_rate": 3.124166666666667e-05,
      "loss": 0.0019,
      "step": 90040
    },
    {
      "epoch": 3.0016666666666665,
      "grad_norm": 0.11531303077936172,
      "learning_rate": 3.1239583333333335e-05,
      "loss": 0.003,
      "step": 90050
    },
    {
      "epoch": 3.002,
      "grad_norm": 0.3159581124782562,
      "learning_rate": 3.12375e-05,
      "loss": 0.0037,
      "step": 90060
    },
    {
      "epoch": 3.0023333333333335,
      "grad_norm": 0.08640685677528381,
      "learning_rate": 3.1235416666666666e-05,
      "loss": 0.0026,
      "step": 90070
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.058215364813804626,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0022,
      "step": 90080
    },
    {
      "epoch": 3.003,
      "grad_norm": 0.057944156229496,
      "learning_rate": 3.1231250000000004e-05,
      "loss": 0.0026,
      "step": 90090
    },
    {
      "epoch": 3.0033333333333334,
      "grad_norm": 0.2870728373527527,
      "learning_rate": 3.122916666666667e-05,
      "loss": 0.0024,
      "step": 90100
    },
    {
      "epoch": 3.0036666666666667,
      "grad_norm": 0.2509690225124359,
      "learning_rate": 3.1227083333333335e-05,
      "loss": 0.002,
      "step": 90110
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.0860646590590477,
      "learning_rate": 3.122500000000001e-05,
      "loss": 0.0033,
      "step": 90120
    },
    {
      "epoch": 3.0043333333333333,
      "grad_norm": 0.20132102072238922,
      "learning_rate": 3.1222916666666666e-05,
      "loss": 0.0025,
      "step": 90130
    },
    {
      "epoch": 3.0046666666666666,
      "grad_norm": 0.28886112570762634,
      "learning_rate": 3.122083333333333e-05,
      "loss": 0.0023,
      "step": 90140
    },
    {
      "epoch": 3.005,
      "grad_norm": 0.05793694406747818,
      "learning_rate": 3.121875e-05,
      "loss": 0.0029,
      "step": 90150
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.17255069315433502,
      "learning_rate": 3.121666666666667e-05,
      "loss": 0.0025,
      "step": 90160
    },
    {
      "epoch": 3.0056666666666665,
      "grad_norm": 0.25851526856422424,
      "learning_rate": 3.1214583333333334e-05,
      "loss": 0.002,
      "step": 90170
    },
    {
      "epoch": 3.006,
      "grad_norm": 0.2588649392127991,
      "learning_rate": 3.12125e-05,
      "loss": 0.0027,
      "step": 90180
    },
    {
      "epoch": 3.0063333333333335,
      "grad_norm": 0.25874531269073486,
      "learning_rate": 3.121041666666667e-05,
      "loss": 0.0023,
      "step": 90190
    },
    {
      "epoch": 3.006666666666667,
      "grad_norm": 0.34500157833099365,
      "learning_rate": 3.120833333333333e-05,
      "loss": 0.0024,
      "step": 90200
    },
    {
      "epoch": 3.007,
      "grad_norm": 0.31570741534233093,
      "learning_rate": 3.120625e-05,
      "loss": 0.0024,
      "step": 90210
    },
    {
      "epoch": 3.0073333333333334,
      "grad_norm": 0.02876666560769081,
      "learning_rate": 3.120416666666667e-05,
      "loss": 0.0025,
      "step": 90220
    },
    {
      "epoch": 3.0076666666666667,
      "grad_norm": 0.17250140011310577,
      "learning_rate": 3.1202083333333334e-05,
      "loss": 0.0015,
      "step": 90230
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.05732434242963791,
      "learning_rate": 3.12e-05,
      "loss": 0.0024,
      "step": 90240
    },
    {
      "epoch": 3.0083333333333333,
      "grad_norm": 0.6319847106933594,
      "learning_rate": 3.1197916666666665e-05,
      "loss": 0.0025,
      "step": 90250
    },
    {
      "epoch": 3.0086666666666666,
      "grad_norm": 0.5171681046485901,
      "learning_rate": 3.119583333333334e-05,
      "loss": 0.0019,
      "step": 90260
    },
    {
      "epoch": 3.009,
      "grad_norm": 0.4309726059436798,
      "learning_rate": 3.119375e-05,
      "loss": 0.0025,
      "step": 90270
    },
    {
      "epoch": 3.009333333333333,
      "grad_norm": 0.11513356119394302,
      "learning_rate": 3.119166666666667e-05,
      "loss": 0.0033,
      "step": 90280
    },
    {
      "epoch": 3.0096666666666665,
      "grad_norm": 0.008946968242526054,
      "learning_rate": 3.1189583333333334e-05,
      "loss": 0.0019,
      "step": 90290
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.004927358124405146,
      "learning_rate": 3.1187500000000006e-05,
      "loss": 0.0024,
      "step": 90300
    },
    {
      "epoch": 3.0103333333333335,
      "grad_norm": 0.48832952976226807,
      "learning_rate": 3.1185416666666665e-05,
      "loss": 0.0018,
      "step": 90310
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.1437380313873291,
      "learning_rate": 3.118333333333334e-05,
      "loss": 0.003,
      "step": 90320
    },
    {
      "epoch": 3.011,
      "grad_norm": 0.20109455287456512,
      "learning_rate": 3.118125e-05,
      "loss": 0.0018,
      "step": 90330
    },
    {
      "epoch": 3.0113333333333334,
      "grad_norm": 0.5172224044799805,
      "learning_rate": 3.117916666666667e-05,
      "loss": 0.0025,
      "step": 90340
    },
    {
      "epoch": 3.0116666666666667,
      "grad_norm": 0.5458998084068298,
      "learning_rate": 3.117708333333333e-05,
      "loss": 0.0021,
      "step": 90350
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.05774294584989548,
      "learning_rate": 3.1175000000000006e-05,
      "loss": 0.0025,
      "step": 90360
    },
    {
      "epoch": 3.0123333333333333,
      "grad_norm": 0.2299290895462036,
      "learning_rate": 3.117291666666667e-05,
      "loss": 0.0035,
      "step": 90370
    },
    {
      "epoch": 3.0126666666666666,
      "grad_norm": 0.12390897423028946,
      "learning_rate": 3.117083333333333e-05,
      "loss": 0.0014,
      "step": 90380
    },
    {
      "epoch": 3.013,
      "grad_norm": 0.14464332163333893,
      "learning_rate": 3.116875e-05,
      "loss": 0.0026,
      "step": 90390
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.25853657722473145,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.0016,
      "step": 90400
    },
    {
      "epoch": 3.0136666666666665,
      "grad_norm": 0.37380003929138184,
      "learning_rate": 3.116458333333333e-05,
      "loss": 0.0021,
      "step": 90410
    },
    {
      "epoch": 3.014,
      "grad_norm": 0.17232076823711395,
      "learning_rate": 3.11625e-05,
      "loss": 0.0021,
      "step": 90420
    },
    {
      "epoch": 3.0143333333333335,
      "grad_norm": 0.3448620140552521,
      "learning_rate": 3.116041666666667e-05,
      "loss": 0.0023,
      "step": 90430
    },
    {
      "epoch": 3.014666666666667,
      "grad_norm": 0.11511272192001343,
      "learning_rate": 3.1158333333333336e-05,
      "loss": 0.0029,
      "step": 90440
    },
    {
      "epoch": 3.015,
      "grad_norm": 0.08628702908754349,
      "learning_rate": 3.115625e-05,
      "loss": 0.0028,
      "step": 90450
    },
    {
      "epoch": 3.0153333333333334,
      "grad_norm": 0.11497265845537186,
      "learning_rate": 3.115416666666667e-05,
      "loss": 0.002,
      "step": 90460
    },
    {
      "epoch": 3.0156666666666667,
      "grad_norm": 0.08650381863117218,
      "learning_rate": 3.115208333333334e-05,
      "loss": 0.0023,
      "step": 90470
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.2297436147928238,
      "learning_rate": 3.115e-05,
      "loss": 0.0019,
      "step": 90480
    },
    {
      "epoch": 3.0163333333333333,
      "grad_norm": 0.14359544217586517,
      "learning_rate": 3.1147916666666664e-05,
      "loss": 0.0019,
      "step": 90490
    },
    {
      "epoch": 3.0166666666666666,
      "grad_norm": 0.287313312292099,
      "learning_rate": 3.1145833333333336e-05,
      "loss": 0.002,
      "step": 90500
    },
    {
      "epoch": 3.017,
      "grad_norm": 0.08620905131101608,
      "learning_rate": 3.114375e-05,
      "loss": 0.0023,
      "step": 90510
    },
    {
      "epoch": 3.017333333333333,
      "grad_norm": 0.3634074926376343,
      "learning_rate": 3.114166666666667e-05,
      "loss": 0.0025,
      "step": 90520
    },
    {
      "epoch": 3.0176666666666665,
      "grad_norm": 0.2871575355529785,
      "learning_rate": 3.113958333333333e-05,
      "loss": 0.0026,
      "step": 90530
    },
    {
      "epoch": 3.018,
      "grad_norm": 0.14386619627475739,
      "learning_rate": 3.1137500000000005e-05,
      "loss": 0.0014,
      "step": 90540
    },
    {
      "epoch": 3.0183333333333335,
      "grad_norm": 0.2298499345779419,
      "learning_rate": 3.113541666666667e-05,
      "loss": 0.0019,
      "step": 90550
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.14355193078517914,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0032,
      "step": 90560
    },
    {
      "epoch": 3.019,
      "grad_norm": 0.008596532046794891,
      "learning_rate": 3.113125e-05,
      "loss": 0.0019,
      "step": 90570
    },
    {
      "epoch": 3.0193333333333334,
      "grad_norm": 0.3734932541847229,
      "learning_rate": 3.1129166666666673e-05,
      "loss": 0.003,
      "step": 90580
    },
    {
      "epoch": 3.0196666666666667,
      "grad_norm": 0.029291771352291107,
      "learning_rate": 3.112708333333333e-05,
      "loss": 0.0026,
      "step": 90590
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.14390221238136292,
      "learning_rate": 3.1125000000000004e-05,
      "loss": 0.0029,
      "step": 90600
    },
    {
      "epoch": 3.0203333333333333,
      "grad_norm": 0.1722072809934616,
      "learning_rate": 3.112291666666667e-05,
      "loss": 0.0016,
      "step": 90610
    },
    {
      "epoch": 3.0206666666666666,
      "grad_norm": 0.23032353818416595,
      "learning_rate": 3.1120833333333335e-05,
      "loss": 0.0024,
      "step": 90620
    },
    {
      "epoch": 3.021,
      "grad_norm": 0.344484806060791,
      "learning_rate": 3.111875e-05,
      "loss": 0.0024,
      "step": 90630
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.1739102602005005,
      "learning_rate": 3.1116666666666666e-05,
      "loss": 0.0023,
      "step": 90640
    },
    {
      "epoch": 3.0216666666666665,
      "grad_norm": 0.38683822751045227,
      "learning_rate": 3.111458333333334e-05,
      "loss": 0.002,
      "step": 90650
    },
    {
      "epoch": 3.022,
      "grad_norm": 0.02914840541779995,
      "learning_rate": 3.11125e-05,
      "loss": 0.002,
      "step": 90660
    },
    {
      "epoch": 3.0223333333333335,
      "grad_norm": 0.45976904034614563,
      "learning_rate": 3.111041666666667e-05,
      "loss": 0.0015,
      "step": 90670
    },
    {
      "epoch": 3.022666666666667,
      "grad_norm": 0.2868560552597046,
      "learning_rate": 3.1108333333333335e-05,
      "loss": 0.0031,
      "step": 90680
    },
    {
      "epoch": 3.023,
      "grad_norm": 0.05796126648783684,
      "learning_rate": 3.110625e-05,
      "loss": 0.0017,
      "step": 90690
    },
    {
      "epoch": 3.0233333333333334,
      "grad_norm": 0.11538518965244293,
      "learning_rate": 3.1104166666666666e-05,
      "loss": 0.0027,
      "step": 90700
    },
    {
      "epoch": 3.0236666666666667,
      "grad_norm": 0.058068592101335526,
      "learning_rate": 3.110208333333334e-05,
      "loss": 0.0018,
      "step": 90710
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.029134241864085197,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.0017,
      "step": 90720
    },
    {
      "epoch": 3.0243333333333333,
      "grad_norm": 0.287386029958725,
      "learning_rate": 3.109791666666666e-05,
      "loss": 0.0037,
      "step": 90730
    },
    {
      "epoch": 3.0246666666666666,
      "grad_norm": 0.2873607277870178,
      "learning_rate": 3.1095833333333335e-05,
      "loss": 0.004,
      "step": 90740
    },
    {
      "epoch": 3.025,
      "grad_norm": 0.029705790802836418,
      "learning_rate": 3.109375e-05,
      "loss": 0.0015,
      "step": 90750
    },
    {
      "epoch": 3.025333333333333,
      "grad_norm": 0.230520561337471,
      "learning_rate": 3.1091666666666666e-05,
      "loss": 0.0027,
      "step": 90760
    },
    {
      "epoch": 3.0256666666666665,
      "grad_norm": 0.08604556322097778,
      "learning_rate": 3.108958333333333e-05,
      "loss": 0.0031,
      "step": 90770
    },
    {
      "epoch": 3.026,
      "grad_norm": 0.17238187789916992,
      "learning_rate": 3.1087500000000003e-05,
      "loss": 0.0031,
      "step": 90780
    },
    {
      "epoch": 3.0263333333333335,
      "grad_norm": 0.08637753129005432,
      "learning_rate": 3.108541666666667e-05,
      "loss": 0.002,
      "step": 90790
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.2586328685283661,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.0025,
      "step": 90800
    },
    {
      "epoch": 3.027,
      "grad_norm": 0.028922678902745247,
      "learning_rate": 3.108125e-05,
      "loss": 0.0027,
      "step": 90810
    },
    {
      "epoch": 3.0273333333333334,
      "grad_norm": 0.22977417707443237,
      "learning_rate": 3.107916666666667e-05,
      "loss": 0.003,
      "step": 90820
    },
    {
      "epoch": 3.0276666666666667,
      "grad_norm": 0.029198935255408287,
      "learning_rate": 3.107708333333334e-05,
      "loss": 0.0019,
      "step": 90830
    },
    {
      "epoch": 3.028,
      "grad_norm": 0.3732026517391205,
      "learning_rate": 3.1075e-05,
      "loss": 0.002,
      "step": 90840
    },
    {
      "epoch": 3.0283333333333333,
      "grad_norm": 0.1726018786430359,
      "learning_rate": 3.107291666666667e-05,
      "loss": 0.0025,
      "step": 90850
    },
    {
      "epoch": 3.0286666666666666,
      "grad_norm": 0.0863318219780922,
      "learning_rate": 3.1070833333333334e-05,
      "loss": 0.0027,
      "step": 90860
    },
    {
      "epoch": 3.029,
      "grad_norm": 0.17255645990371704,
      "learning_rate": 3.106875e-05,
      "loss": 0.0016,
      "step": 90870
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.057769183069467545,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0022,
      "step": 90880
    },
    {
      "epoch": 3.0296666666666665,
      "grad_norm": 0.20103706419467926,
      "learning_rate": 3.106458333333334e-05,
      "loss": 0.0025,
      "step": 90890
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.48848363757133484,
      "learning_rate": 3.10625e-05,
      "loss": 0.0015,
      "step": 90900
    },
    {
      "epoch": 3.0303333333333335,
      "grad_norm": 0.6318585872650146,
      "learning_rate": 3.106041666666667e-05,
      "loss": 0.0034,
      "step": 90910
    },
    {
      "epoch": 3.030666666666667,
      "grad_norm": 0.11504741758108139,
      "learning_rate": 3.1058333333333334e-05,
      "loss": 0.002,
      "step": 90920
    },
    {
      "epoch": 3.031,
      "grad_norm": 0.05811164900660515,
      "learning_rate": 3.1056250000000006e-05,
      "loss": 0.0021,
      "step": 90930
    },
    {
      "epoch": 3.0313333333333334,
      "grad_norm": 0.1723269671201706,
      "learning_rate": 3.1054166666666665e-05,
      "loss": 0.002,
      "step": 90940
    },
    {
      "epoch": 3.0316666666666667,
      "grad_norm": 0.17243905365467072,
      "learning_rate": 3.105208333333334e-05,
      "loss": 0.0023,
      "step": 90950
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.1437848061323166,
      "learning_rate": 3.105e-05,
      "loss": 0.0019,
      "step": 90960
    },
    {
      "epoch": 3.0323333333333333,
      "grad_norm": 0.00551504734903574,
      "learning_rate": 3.104791666666667e-05,
      "loss": 0.0032,
      "step": 90970
    },
    {
      "epoch": 3.0326666666666666,
      "grad_norm": 0.028910620138049126,
      "learning_rate": 3.1045833333333334e-05,
      "loss": 0.0016,
      "step": 90980
    },
    {
      "epoch": 3.033,
      "grad_norm": 0.2872779369354248,
      "learning_rate": 3.104375e-05,
      "loss": 0.0019,
      "step": 90990
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.08628547191619873,
      "learning_rate": 3.104166666666667e-05,
      "loss": 0.002,
      "step": 91000
    },
    {
      "epoch": 3.0336666666666665,
      "grad_norm": 0.42987027764320374,
      "learning_rate": 3.103958333333333e-05,
      "loss": 0.0028,
      "step": 91010
    },
    {
      "epoch": 3.034,
      "grad_norm": 0.20114286243915558,
      "learning_rate": 3.10375e-05,
      "loss": 0.0017,
      "step": 91020
    },
    {
      "epoch": 3.0343333333333335,
      "grad_norm": 0.14471571147441864,
      "learning_rate": 3.103541666666667e-05,
      "loss": 0.0019,
      "step": 91030
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.0892452821135521,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0032,
      "step": 91040
    },
    {
      "epoch": 3.035,
      "grad_norm": 0.2872001528739929,
      "learning_rate": 3.103125e-05,
      "loss": 0.0018,
      "step": 91050
    },
    {
      "epoch": 3.0353333333333334,
      "grad_norm": 0.005429730750620365,
      "learning_rate": 3.102916666666667e-05,
      "loss": 0.0025,
      "step": 91060
    },
    {
      "epoch": 3.0356666666666667,
      "grad_norm": 0.3447625935077667,
      "learning_rate": 3.1027083333333336e-05,
      "loss": 0.0017,
      "step": 91070
    },
    {
      "epoch": 3.036,
      "grad_norm": 0.17246472835540771,
      "learning_rate": 3.1025e-05,
      "loss": 0.0022,
      "step": 91080
    },
    {
      "epoch": 3.0363333333333333,
      "grad_norm": 0.17255671322345734,
      "learning_rate": 3.102291666666667e-05,
      "loss": 0.0031,
      "step": 91090
    },
    {
      "epoch": 3.0366666666666666,
      "grad_norm": 0.20133134722709656,
      "learning_rate": 3.102083333333333e-05,
      "loss": 0.0023,
      "step": 91100
    },
    {
      "epoch": 3.037,
      "grad_norm": 0.20120388269424438,
      "learning_rate": 3.1018750000000005e-05,
      "loss": 0.0025,
      "step": 91110
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.2872750461101532,
      "learning_rate": 3.1016666666666664e-05,
      "loss": 0.0029,
      "step": 91120
    },
    {
      "epoch": 3.0376666666666665,
      "grad_norm": 0.3160587549209595,
      "learning_rate": 3.1014583333333336e-05,
      "loss": 0.0016,
      "step": 91130
    },
    {
      "epoch": 3.038,
      "grad_norm": 0.17258504033088684,
      "learning_rate": 3.10125e-05,
      "loss": 0.0029,
      "step": 91140
    },
    {
      "epoch": 3.038333333333333,
      "grad_norm": 0.2586924731731415,
      "learning_rate": 3.101041666666667e-05,
      "loss": 0.0026,
      "step": 91150
    },
    {
      "epoch": 3.038666666666667,
      "grad_norm": 0.48845306038856506,
      "learning_rate": 3.100833333333333e-05,
      "loss": 0.003,
      "step": 91160
    },
    {
      "epoch": 3.039,
      "grad_norm": 0.08644996583461761,
      "learning_rate": 3.1006250000000005e-05,
      "loss": 0.0022,
      "step": 91170
    },
    {
      "epoch": 3.0393333333333334,
      "grad_norm": 0.4305918216705322,
      "learning_rate": 3.100416666666667e-05,
      "loss": 0.0015,
      "step": 91180
    },
    {
      "epoch": 3.0396666666666667,
      "grad_norm": 0.0052332025952637196,
      "learning_rate": 3.1002083333333336e-05,
      "loss": 0.0023,
      "step": 91190
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.5170377492904663,
      "learning_rate": 3.1e-05,
      "loss": 0.0016,
      "step": 91200
    },
    {
      "epoch": 3.0403333333333333,
      "grad_norm": 0.1151461973786354,
      "learning_rate": 3.099791666666667e-05,
      "loss": 0.0024,
      "step": 91210
    },
    {
      "epoch": 3.0406666666666666,
      "grad_norm": 0.17241044342517853,
      "learning_rate": 3.099583333333333e-05,
      "loss": 0.0019,
      "step": 91220
    },
    {
      "epoch": 3.041,
      "grad_norm": 0.17233990132808685,
      "learning_rate": 3.099375e-05,
      "loss": 0.002,
      "step": 91230
    },
    {
      "epoch": 3.041333333333333,
      "grad_norm": 0.1151350662112236,
      "learning_rate": 3.099166666666667e-05,
      "loss": 0.0028,
      "step": 91240
    },
    {
      "epoch": 3.0416666666666665,
      "grad_norm": 0.28708726167678833,
      "learning_rate": 3.0989583333333336e-05,
      "loss": 0.0028,
      "step": 91250
    },
    {
      "epoch": 3.042,
      "grad_norm": 0.4307917058467865,
      "learning_rate": 3.09875e-05,
      "loss": 0.0021,
      "step": 91260
    },
    {
      "epoch": 3.0423333333333336,
      "grad_norm": 0.14373838901519775,
      "learning_rate": 3.0985416666666667e-05,
      "loss": 0.0023,
      "step": 91270
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.06053163483738899,
      "learning_rate": 3.098333333333334e-05,
      "loss": 0.0031,
      "step": 91280
    },
    {
      "epoch": 3.043,
      "grad_norm": 0.11482567340135574,
      "learning_rate": 3.098125e-05,
      "loss": 0.0016,
      "step": 91290
    },
    {
      "epoch": 3.0433333333333334,
      "grad_norm": 0.11480168998241425,
      "learning_rate": 3.097916666666667e-05,
      "loss": 0.0028,
      "step": 91300
    },
    {
      "epoch": 3.0436666666666667,
      "grad_norm": 0.17248976230621338,
      "learning_rate": 3.0977083333333335e-05,
      "loss": 0.0026,
      "step": 91310
    },
    {
      "epoch": 3.044,
      "grad_norm": 0.22929568588733673,
      "learning_rate": 3.0975e-05,
      "loss": 0.0017,
      "step": 91320
    },
    {
      "epoch": 3.0443333333333333,
      "grad_norm": 0.057976823300123215,
      "learning_rate": 3.0972916666666666e-05,
      "loss": 0.0019,
      "step": 91330
    },
    {
      "epoch": 3.0446666666666666,
      "grad_norm": 0.3160700500011444,
      "learning_rate": 3.097083333333333e-05,
      "loss": 0.0023,
      "step": 91340
    },
    {
      "epoch": 3.045,
      "grad_norm": 0.22574104368686676,
      "learning_rate": 3.0968750000000004e-05,
      "loss": 0.0014,
      "step": 91350
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.20108215510845184,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.002,
      "step": 91360
    },
    {
      "epoch": 3.0456666666666665,
      "grad_norm": 0.05842622369527817,
      "learning_rate": 3.0964583333333335e-05,
      "loss": 0.0027,
      "step": 91370
    },
    {
      "epoch": 3.046,
      "grad_norm": 0.14355629682540894,
      "learning_rate": 3.09625e-05,
      "loss": 0.0028,
      "step": 91380
    },
    {
      "epoch": 3.046333333333333,
      "grad_norm": 0.28719937801361084,
      "learning_rate": 3.096041666666667e-05,
      "loss": 0.0024,
      "step": 91390
    },
    {
      "epoch": 3.046666666666667,
      "grad_norm": 0.14386647939682007,
      "learning_rate": 3.095833333333333e-05,
      "loss": 0.0025,
      "step": 91400
    },
    {
      "epoch": 3.047,
      "grad_norm": 0.057436253875494,
      "learning_rate": 3.0956250000000004e-05,
      "loss": 0.0018,
      "step": 91410
    },
    {
      "epoch": 3.0473333333333334,
      "grad_norm": 0.11481152474880219,
      "learning_rate": 3.095416666666667e-05,
      "loss": 0.0023,
      "step": 91420
    },
    {
      "epoch": 3.0476666666666667,
      "grad_norm": 0.3805670440196991,
      "learning_rate": 3.0952083333333335e-05,
      "loss": 0.0029,
      "step": 91430
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.057947490364313126,
      "learning_rate": 3.095e-05,
      "loss": 0.002,
      "step": 91440
    },
    {
      "epoch": 3.0483333333333333,
      "grad_norm": 0.29160216450691223,
      "learning_rate": 3.094791666666667e-05,
      "loss": 0.0031,
      "step": 91450
    },
    {
      "epoch": 3.0486666666666666,
      "grad_norm": 0.006360847037285566,
      "learning_rate": 3.094583333333334e-05,
      "loss": 0.0028,
      "step": 91460
    },
    {
      "epoch": 3.049,
      "grad_norm": 0.22991156578063965,
      "learning_rate": 3.0943749999999997e-05,
      "loss": 0.0026,
      "step": 91470
    },
    {
      "epoch": 3.0493333333333332,
      "grad_norm": 0.1437501609325409,
      "learning_rate": 3.094166666666667e-05,
      "loss": 0.0017,
      "step": 91480
    },
    {
      "epoch": 3.0496666666666665,
      "grad_norm": 0.02908388525247574,
      "learning_rate": 3.0939583333333334e-05,
      "loss": 0.0028,
      "step": 91490
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.3874496519565582,
      "learning_rate": 3.09375e-05,
      "loss": 0.0023,
      "step": 91500
    },
    {
      "epoch": 3.050333333333333,
      "grad_norm": 0.17222408950328827,
      "learning_rate": 3.0935416666666665e-05,
      "loss": 0.0016,
      "step": 91510
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.115553118288517,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0027,
      "step": 91520
    },
    {
      "epoch": 3.051,
      "grad_norm": 0.20094440877437592,
      "learning_rate": 3.093125e-05,
      "loss": 0.002,
      "step": 91530
    },
    {
      "epoch": 3.0513333333333335,
      "grad_norm": 0.34456875920295715,
      "learning_rate": 3.092916666666667e-05,
      "loss": 0.0017,
      "step": 91540
    },
    {
      "epoch": 3.0516666666666667,
      "grad_norm": 0.02899744175374508,
      "learning_rate": 3.0927083333333334e-05,
      "loss": 0.0017,
      "step": 91550
    },
    {
      "epoch": 3.052,
      "grad_norm": 0.08655519783496857,
      "learning_rate": 3.0925000000000006e-05,
      "loss": 0.0029,
      "step": 91560
    },
    {
      "epoch": 3.0523333333333333,
      "grad_norm": 0.14456605911254883,
      "learning_rate": 3.0922916666666665e-05,
      "loss": 0.0038,
      "step": 91570
    },
    {
      "epoch": 3.0526666666666666,
      "grad_norm": 0.14353196322917938,
      "learning_rate": 3.092083333333333e-05,
      "loss": 0.0029,
      "step": 91580
    },
    {
      "epoch": 3.053,
      "grad_norm": 0.08619845658540726,
      "learning_rate": 3.091875e-05,
      "loss": 0.0016,
      "step": 91590
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.23002813756465912,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.002,
      "step": 91600
    },
    {
      "epoch": 3.0536666666666665,
      "grad_norm": 0.08624813705682755,
      "learning_rate": 3.0914583333333334e-05,
      "loss": 0.0027,
      "step": 91610
    },
    {
      "epoch": 3.054,
      "grad_norm": 0.05819251388311386,
      "learning_rate": 3.09125e-05,
      "loss": 0.0019,
      "step": 91620
    },
    {
      "epoch": 3.054333333333333,
      "grad_norm": 0.22970791161060333,
      "learning_rate": 3.091041666666667e-05,
      "loss": 0.0029,
      "step": 91630
    },
    {
      "epoch": 3.054666666666667,
      "grad_norm": 0.028924712911248207,
      "learning_rate": 3.090833333333334e-05,
      "loss": 0.002,
      "step": 91640
    },
    {
      "epoch": 3.055,
      "grad_norm": 0.20110450685024261,
      "learning_rate": 3.090625e-05,
      "loss": 0.0017,
      "step": 91650
    },
    {
      "epoch": 3.0553333333333335,
      "grad_norm": 0.26090097427368164,
      "learning_rate": 3.090416666666667e-05,
      "loss": 0.0023,
      "step": 91660
    },
    {
      "epoch": 3.0556666666666668,
      "grad_norm": 0.17232809960842133,
      "learning_rate": 3.090208333333334e-05,
      "loss": 0.0022,
      "step": 91670
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.14524081349372864,
      "learning_rate": 3.09e-05,
      "loss": 0.002,
      "step": 91680
    },
    {
      "epoch": 3.0563333333333333,
      "grad_norm": 0.08641307055950165,
      "learning_rate": 3.089791666666667e-05,
      "loss": 0.0026,
      "step": 91690
    },
    {
      "epoch": 3.0566666666666666,
      "grad_norm": 0.20116640627384186,
      "learning_rate": 3.089583333333334e-05,
      "loss": 0.0025,
      "step": 91700
    },
    {
      "epoch": 3.057,
      "grad_norm": 0.2117306888103485,
      "learning_rate": 3.089375e-05,
      "loss": 0.0028,
      "step": 91710
    },
    {
      "epoch": 3.0573333333333332,
      "grad_norm": 0.1437113881111145,
      "learning_rate": 3.089166666666667e-05,
      "loss": 0.0029,
      "step": 91720
    },
    {
      "epoch": 3.0576666666666665,
      "grad_norm": 0.25846657156944275,
      "learning_rate": 3.088958333333333e-05,
      "loss": 0.0025,
      "step": 91730
    },
    {
      "epoch": 3.058,
      "grad_norm": 0.3443543314933777,
      "learning_rate": 3.0887500000000005e-05,
      "loss": 0.0018,
      "step": 91740
    },
    {
      "epoch": 3.058333333333333,
      "grad_norm": 0.14383994042873383,
      "learning_rate": 3.0885416666666664e-05,
      "loss": 0.0022,
      "step": 91750
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.02927514724433422,
      "learning_rate": 3.0883333333333336e-05,
      "loss": 0.0032,
      "step": 91760
    },
    {
      "epoch": 3.059,
      "grad_norm": 0.041985053569078445,
      "learning_rate": 3.088125e-05,
      "loss": 0.0019,
      "step": 91770
    },
    {
      "epoch": 3.0593333333333335,
      "grad_norm": 0.17254576086997986,
      "learning_rate": 3.087916666666667e-05,
      "loss": 0.0019,
      "step": 91780
    },
    {
      "epoch": 3.0596666666666668,
      "grad_norm": 0.05762062966823578,
      "learning_rate": 3.087708333333333e-05,
      "loss": 0.0016,
      "step": 91790
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.31575772166252136,
      "learning_rate": 3.0875000000000005e-05,
      "loss": 0.0016,
      "step": 91800
    },
    {
      "epoch": 3.0603333333333333,
      "grad_norm": 0.4441515803337097,
      "learning_rate": 3.087291666666667e-05,
      "loss": 0.0019,
      "step": 91810
    },
    {
      "epoch": 3.0606666666666666,
      "grad_norm": 0.03169196471571922,
      "learning_rate": 3.087083333333333e-05,
      "loss": 0.0021,
      "step": 91820
    },
    {
      "epoch": 3.061,
      "grad_norm": 0.17294931411743164,
      "learning_rate": 3.086875e-05,
      "loss": 0.002,
      "step": 91830
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.17327016592025757,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0024,
      "step": 91840
    },
    {
      "epoch": 3.0616666666666665,
      "grad_norm": 0.28694671392440796,
      "learning_rate": 3.086458333333333e-05,
      "loss": 0.0017,
      "step": 91850
    },
    {
      "epoch": 3.062,
      "grad_norm": 0.3736267387866974,
      "learning_rate": 3.08625e-05,
      "loss": 0.0022,
      "step": 91860
    },
    {
      "epoch": 3.062333333333333,
      "grad_norm": 0.22974549233913422,
      "learning_rate": 3.086041666666667e-05,
      "loss": 0.0031,
      "step": 91870
    },
    {
      "epoch": 3.062666666666667,
      "grad_norm": 0.11509288847446442,
      "learning_rate": 3.0858333333333336e-05,
      "loss": 0.0017,
      "step": 91880
    },
    {
      "epoch": 3.063,
      "grad_norm": 0.1723874807357788,
      "learning_rate": 3.085625e-05,
      "loss": 0.0019,
      "step": 91890
    },
    {
      "epoch": 3.0633333333333335,
      "grad_norm": 0.11471296101808548,
      "learning_rate": 3.085416666666667e-05,
      "loss": 0.0019,
      "step": 91900
    },
    {
      "epoch": 3.0636666666666668,
      "grad_norm": 0.25848913192749023,
      "learning_rate": 3.085208333333334e-05,
      "loss": 0.0018,
      "step": 91910
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.003867348190397024,
      "learning_rate": 3.0850000000000004e-05,
      "loss": 0.0029,
      "step": 91920
    },
    {
      "epoch": 3.0643333333333334,
      "grad_norm": 0.05742434784770012,
      "learning_rate": 3.084791666666667e-05,
      "loss": 0.0024,
      "step": 91930
    },
    {
      "epoch": 3.0646666666666667,
      "grad_norm": 0.05753811076283455,
      "learning_rate": 3.0845833333333335e-05,
      "loss": 0.0021,
      "step": 91940
    },
    {
      "epoch": 3.065,
      "grad_norm": 0.029017731547355652,
      "learning_rate": 3.084375e-05,
      "loss": 0.0023,
      "step": 91950
    },
    {
      "epoch": 3.0653333333333332,
      "grad_norm": 0.05765795335173607,
      "learning_rate": 3.0841666666666666e-05,
      "loss": 0.0017,
      "step": 91960
    },
    {
      "epoch": 3.0656666666666665,
      "grad_norm": 0.004708022344857454,
      "learning_rate": 3.083958333333333e-05,
      "loss": 0.0015,
      "step": 91970
    },
    {
      "epoch": 3.066,
      "grad_norm": 0.14347894489765167,
      "learning_rate": 3.0837500000000004e-05,
      "loss": 0.0017,
      "step": 91980
    },
    {
      "epoch": 3.066333333333333,
      "grad_norm": 0.5747097730636597,
      "learning_rate": 3.083541666666667e-05,
      "loss": 0.0026,
      "step": 91990
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.029137270525097847,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0027,
      "step": 92000
    },
    {
      "epoch": 3.067,
      "grad_norm": 0.1438724249601364,
      "learning_rate": 3.083125e-05,
      "loss": 0.0021,
      "step": 92010
    },
    {
      "epoch": 3.0673333333333335,
      "grad_norm": 0.2874628007411957,
      "learning_rate": 3.082916666666667e-05,
      "loss": 0.0021,
      "step": 92020
    },
    {
      "epoch": 3.0676666666666668,
      "grad_norm": 0.029682861641049385,
      "learning_rate": 3.082708333333333e-05,
      "loss": 0.0022,
      "step": 92030
    },
    {
      "epoch": 3.068,
      "grad_norm": 0.08601441979408264,
      "learning_rate": 3.0825000000000004e-05,
      "loss": 0.0022,
      "step": 92040
    },
    {
      "epoch": 3.0683333333333334,
      "grad_norm": 0.11485335975885391,
      "learning_rate": 3.082291666666667e-05,
      "loss": 0.003,
      "step": 92050
    },
    {
      "epoch": 3.0686666666666667,
      "grad_norm": 0.029681287705898285,
      "learning_rate": 3.0820833333333335e-05,
      "loss": 0.0019,
      "step": 92060
    },
    {
      "epoch": 3.069,
      "grad_norm": 0.17276278138160706,
      "learning_rate": 3.081875e-05,
      "loss": 0.0021,
      "step": 92070
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.3565219044685364,
      "learning_rate": 3.0816666666666666e-05,
      "loss": 0.0019,
      "step": 92080
    },
    {
      "epoch": 3.0696666666666665,
      "grad_norm": 0.08620345592498779,
      "learning_rate": 3.081458333333334e-05,
      "loss": 0.0029,
      "step": 92090
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.17239712178707123,
      "learning_rate": 3.08125e-05,
      "loss": 0.0024,
      "step": 92100
    },
    {
      "epoch": 3.070333333333333,
      "grad_norm": 0.5020447969436646,
      "learning_rate": 3.081041666666667e-05,
      "loss": 0.0019,
      "step": 92110
    },
    {
      "epoch": 3.070666666666667,
      "grad_norm": 0.20104707777500153,
      "learning_rate": 3.0808333333333335e-05,
      "loss": 0.0024,
      "step": 92120
    },
    {
      "epoch": 3.071,
      "grad_norm": 0.20117369294166565,
      "learning_rate": 3.080625e-05,
      "loss": 0.0023,
      "step": 92130
    },
    {
      "epoch": 3.0713333333333335,
      "grad_norm": 0.43064752221107483,
      "learning_rate": 3.0804166666666666e-05,
      "loss": 0.0029,
      "step": 92140
    },
    {
      "epoch": 3.0716666666666668,
      "grad_norm": 0.17225946485996246,
      "learning_rate": 3.080208333333334e-05,
      "loss": 0.0023,
      "step": 92150
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.28711801767349243,
      "learning_rate": 3.08e-05,
      "loss": 0.0031,
      "step": 92160
    },
    {
      "epoch": 3.0723333333333334,
      "grad_norm": 0.20112021267414093,
      "learning_rate": 3.079791666666667e-05,
      "loss": 0.002,
      "step": 92170
    },
    {
      "epoch": 3.0726666666666667,
      "grad_norm": 0.08673381060361862,
      "learning_rate": 3.0795833333333334e-05,
      "loss": 0.0029,
      "step": 92180
    },
    {
      "epoch": 3.073,
      "grad_norm": 0.2873796224594116,
      "learning_rate": 3.079375e-05,
      "loss": 0.0019,
      "step": 92190
    },
    {
      "epoch": 3.0733333333333333,
      "grad_norm": 0.37317392230033875,
      "learning_rate": 3.079166666666667e-05,
      "loss": 0.0025,
      "step": 92200
    },
    {
      "epoch": 3.0736666666666665,
      "grad_norm": 0.11487917602062225,
      "learning_rate": 3.078958333333333e-05,
      "loss": 0.0023,
      "step": 92210
    },
    {
      "epoch": 3.074,
      "grad_norm": 0.1727471649646759,
      "learning_rate": 3.07875e-05,
      "loss": 0.0033,
      "step": 92220
    },
    {
      "epoch": 3.074333333333333,
      "grad_norm": 0.5170496702194214,
      "learning_rate": 3.078541666666667e-05,
      "loss": 0.0017,
      "step": 92230
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.17227061092853546,
      "learning_rate": 3.0783333333333334e-05,
      "loss": 0.0029,
      "step": 92240
    },
    {
      "epoch": 3.075,
      "grad_norm": 0.2584042549133301,
      "learning_rate": 3.078125e-05,
      "loss": 0.0024,
      "step": 92250
    },
    {
      "epoch": 3.0753333333333335,
      "grad_norm": 0.37345442175865173,
      "learning_rate": 3.077916666666667e-05,
      "loss": 0.0025,
      "step": 92260
    },
    {
      "epoch": 3.0756666666666668,
      "grad_norm": 0.2007429003715515,
      "learning_rate": 3.077708333333334e-05,
      "loss": 0.0023,
      "step": 92270
    },
    {
      "epoch": 3.076,
      "grad_norm": 0.23001085221767426,
      "learning_rate": 3.0775e-05,
      "loss": 0.0024,
      "step": 92280
    },
    {
      "epoch": 3.0763333333333334,
      "grad_norm": 0.17260363698005676,
      "learning_rate": 3.077291666666667e-05,
      "loss": 0.0021,
      "step": 92290
    },
    {
      "epoch": 3.0766666666666667,
      "grad_norm": 0.11514900624752045,
      "learning_rate": 3.0770833333333334e-05,
      "loss": 0.0026,
      "step": 92300
    },
    {
      "epoch": 3.077,
      "grad_norm": 0.08624818921089172,
      "learning_rate": 3.076875e-05,
      "loss": 0.0025,
      "step": 92310
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.0863754153251648,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.002,
      "step": 92320
    },
    {
      "epoch": 3.0776666666666666,
      "grad_norm": 0.029398275539278984,
      "learning_rate": 3.076458333333334e-05,
      "loss": 0.0017,
      "step": 92330
    },
    {
      "epoch": 3.078,
      "grad_norm": 0.058051999658346176,
      "learning_rate": 3.07625e-05,
      "loss": 0.0014,
      "step": 92340
    },
    {
      "epoch": 3.078333333333333,
      "grad_norm": 0.007151974365115166,
      "learning_rate": 3.076041666666667e-05,
      "loss": 0.0023,
      "step": 92350
    },
    {
      "epoch": 3.078666666666667,
      "grad_norm": 0.1436612606048584,
      "learning_rate": 3.075833333333333e-05,
      "loss": 0.0013,
      "step": 92360
    },
    {
      "epoch": 3.079,
      "grad_norm": 0.08618233352899551,
      "learning_rate": 3.0756250000000006e-05,
      "loss": 0.0024,
      "step": 92370
    },
    {
      "epoch": 3.0793333333333335,
      "grad_norm": 0.34475624561309814,
      "learning_rate": 3.0754166666666664e-05,
      "loss": 0.002,
      "step": 92380
    },
    {
      "epoch": 3.0796666666666668,
      "grad_norm": 0.0862407311797142,
      "learning_rate": 3.0752083333333337e-05,
      "loss": 0.0019,
      "step": 92390
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.2584867477416992,
      "learning_rate": 3.075e-05,
      "loss": 0.0017,
      "step": 92400
    },
    {
      "epoch": 3.0803333333333334,
      "grad_norm": 0.1722232550382614,
      "learning_rate": 3.074791666666667e-05,
      "loss": 0.0024,
      "step": 92410
    },
    {
      "epoch": 3.0806666666666667,
      "grad_norm": 0.17239747941493988,
      "learning_rate": 3.074583333333333e-05,
      "loss": 0.0015,
      "step": 92420
    },
    {
      "epoch": 3.081,
      "grad_norm": 0.22969835996627808,
      "learning_rate": 3.074375e-05,
      "loss": 0.0024,
      "step": 92430
    },
    {
      "epoch": 3.0813333333333333,
      "grad_norm": 0.08622220903635025,
      "learning_rate": 3.074166666666667e-05,
      "loss": 0.0028,
      "step": 92440
    },
    {
      "epoch": 3.0816666666666666,
      "grad_norm": 0.057840630412101746,
      "learning_rate": 3.073958333333333e-05,
      "loss": 0.003,
      "step": 92450
    },
    {
      "epoch": 3.082,
      "grad_norm": 0.11492360383272171,
      "learning_rate": 3.07375e-05,
      "loss": 0.002,
      "step": 92460
    },
    {
      "epoch": 3.082333333333333,
      "grad_norm": 0.17235751450061798,
      "learning_rate": 3.073541666666667e-05,
      "loss": 0.0021,
      "step": 92470
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.2582794427871704,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0017,
      "step": 92480
    },
    {
      "epoch": 3.083,
      "grad_norm": 0.31571075320243835,
      "learning_rate": 3.073125e-05,
      "loss": 0.0029,
      "step": 92490
    },
    {
      "epoch": 3.0833333333333335,
      "grad_norm": 0.5146811604499817,
      "learning_rate": 3.072916666666667e-05,
      "loss": 0.002,
      "step": 92500
    },
    {
      "epoch": 3.083666666666667,
      "grad_norm": 0.2296934425830841,
      "learning_rate": 3.0727083333333336e-05,
      "loss": 0.0026,
      "step": 92510
    },
    {
      "epoch": 3.084,
      "grad_norm": 0.11235906928777695,
      "learning_rate": 3.0725e-05,
      "loss": 0.0031,
      "step": 92520
    },
    {
      "epoch": 3.0843333333333334,
      "grad_norm": 0.17248190939426422,
      "learning_rate": 3.072291666666667e-05,
      "loss": 0.002,
      "step": 92530
    },
    {
      "epoch": 3.0846666666666667,
      "grad_norm": 0.2584506571292877,
      "learning_rate": 3.072083333333334e-05,
      "loss": 0.0032,
      "step": 92540
    },
    {
      "epoch": 3.085,
      "grad_norm": 0.114954873919487,
      "learning_rate": 3.0718750000000005e-05,
      "loss": 0.0029,
      "step": 92550
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.1722017377614975,
      "learning_rate": 3.0716666666666663e-05,
      "loss": 0.0026,
      "step": 92560
    },
    {
      "epoch": 3.0856666666666666,
      "grad_norm": 0.25833746790885925,
      "learning_rate": 3.0714583333333336e-05,
      "loss": 0.0021,
      "step": 92570
    },
    {
      "epoch": 3.086,
      "grad_norm": 0.40571218729019165,
      "learning_rate": 3.07125e-05,
      "loss": 0.0023,
      "step": 92580
    },
    {
      "epoch": 3.086333333333333,
      "grad_norm": 0.17226067185401917,
      "learning_rate": 3.071041666666667e-05,
      "loss": 0.0015,
      "step": 92590
    },
    {
      "epoch": 3.086666666666667,
      "grad_norm": 0.17251388728618622,
      "learning_rate": 3.070833333333333e-05,
      "loss": 0.0023,
      "step": 92600
    },
    {
      "epoch": 3.087,
      "grad_norm": 0.14361390471458435,
      "learning_rate": 3.0706250000000004e-05,
      "loss": 0.0035,
      "step": 92610
    },
    {
      "epoch": 3.0873333333333335,
      "grad_norm": 0.02943205088376999,
      "learning_rate": 3.070416666666667e-05,
      "loss": 0.0016,
      "step": 92620
    },
    {
      "epoch": 3.087666666666667,
      "grad_norm": 0.08653563261032104,
      "learning_rate": 3.0702083333333335e-05,
      "loss": 0.0019,
      "step": 92630
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.4374867379665375,
      "learning_rate": 3.07e-05,
      "loss": 0.0051,
      "step": 92640
    },
    {
      "epoch": 3.0883333333333334,
      "grad_norm": 0.029243797063827515,
      "learning_rate": 3.069791666666667e-05,
      "loss": 0.0025,
      "step": 92650
    },
    {
      "epoch": 3.0886666666666667,
      "grad_norm": 0.08622980117797852,
      "learning_rate": 3.069583333333333e-05,
      "loss": 0.0036,
      "step": 92660
    },
    {
      "epoch": 3.089,
      "grad_norm": 0.02917560189962387,
      "learning_rate": 3.069375e-05,
      "loss": 0.0027,
      "step": 92670
    },
    {
      "epoch": 3.0893333333333333,
      "grad_norm": 0.028891095891594887,
      "learning_rate": 3.069166666666667e-05,
      "loss": 0.0022,
      "step": 92680
    },
    {
      "epoch": 3.0896666666666666,
      "grad_norm": 0.14372147619724274,
      "learning_rate": 3.0689583333333335e-05,
      "loss": 0.0026,
      "step": 92690
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.0862496867775917,
      "learning_rate": 3.06875e-05,
      "loss": 0.0027,
      "step": 92700
    },
    {
      "epoch": 3.090333333333333,
      "grad_norm": 0.1722603738307953,
      "learning_rate": 3.0685416666666666e-05,
      "loss": 0.0025,
      "step": 92710
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.14368771016597748,
      "learning_rate": 3.068333333333334e-05,
      "loss": 0.0026,
      "step": 92720
    },
    {
      "epoch": 3.091,
      "grad_norm": 0.4592295289039612,
      "learning_rate": 3.068125e-05,
      "loss": 0.0021,
      "step": 92730
    },
    {
      "epoch": 3.0913333333333335,
      "grad_norm": 0.5453073978424072,
      "learning_rate": 3.067916666666667e-05,
      "loss": 0.0019,
      "step": 92740
    },
    {
      "epoch": 3.091666666666667,
      "grad_norm": 0.459328830242157,
      "learning_rate": 3.0677083333333335e-05,
      "loss": 0.0025,
      "step": 92750
    },
    {
      "epoch": 3.092,
      "grad_norm": 0.4053017199039459,
      "learning_rate": 3.067500000000001e-05,
      "loss": 0.0023,
      "step": 92760
    },
    {
      "epoch": 3.0923333333333334,
      "grad_norm": 0.057763174176216125,
      "learning_rate": 3.0672916666666666e-05,
      "loss": 0.0021,
      "step": 92770
    },
    {
      "epoch": 3.0926666666666667,
      "grad_norm": 0.05774945393204689,
      "learning_rate": 3.067083333333334e-05,
      "loss": 0.0017,
      "step": 92780
    },
    {
      "epoch": 3.093,
      "grad_norm": 0.11486026644706726,
      "learning_rate": 3.0668750000000003e-05,
      "loss": 0.0022,
      "step": 92790
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.2871599495410919,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.002,
      "step": 92800
    },
    {
      "epoch": 3.0936666666666666,
      "grad_norm": 0.5168185830116272,
      "learning_rate": 3.0664583333333334e-05,
      "loss": 0.0019,
      "step": 92810
    },
    {
      "epoch": 3.094,
      "grad_norm": 0.1152496188879013,
      "learning_rate": 3.06625e-05,
      "loss": 0.0026,
      "step": 92820
    },
    {
      "epoch": 3.094333333333333,
      "grad_norm": 0.14360682666301727,
      "learning_rate": 3.066041666666667e-05,
      "loss": 0.0024,
      "step": 92830
    },
    {
      "epoch": 3.0946666666666665,
      "grad_norm": 0.5675702095031738,
      "learning_rate": 3.065833333333333e-05,
      "loss": 0.0019,
      "step": 92840
    },
    {
      "epoch": 3.095,
      "grad_norm": 0.05765325948596001,
      "learning_rate": 3.065625e-05,
      "loss": 0.002,
      "step": 92850
    },
    {
      "epoch": 3.0953333333333335,
      "grad_norm": 0.3443378210067749,
      "learning_rate": 3.065416666666667e-05,
      "loss": 0.003,
      "step": 92860
    },
    {
      "epoch": 3.095666666666667,
      "grad_norm": 0.07699038833379745,
      "learning_rate": 3.0652083333333334e-05,
      "loss": 0.0021,
      "step": 92870
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.3954986333847046,
      "learning_rate": 3.065e-05,
      "loss": 0.0018,
      "step": 92880
    },
    {
      "epoch": 3.0963333333333334,
      "grad_norm": 0.23008111119270325,
      "learning_rate": 3.064791666666667e-05,
      "loss": 0.002,
      "step": 92890
    },
    {
      "epoch": 3.0966666666666667,
      "grad_norm": 0.7692327499389648,
      "learning_rate": 3.064583333333334e-05,
      "loss": 0.002,
      "step": 92900
    },
    {
      "epoch": 3.097,
      "grad_norm": 0.16241711378097534,
      "learning_rate": 3.0643749999999996e-05,
      "loss": 0.0025,
      "step": 92910
    },
    {
      "epoch": 3.0973333333333333,
      "grad_norm": 0.11514773964881897,
      "learning_rate": 3.064166666666667e-05,
      "loss": 0.0024,
      "step": 92920
    },
    {
      "epoch": 3.0976666666666666,
      "grad_norm": 0.05805334821343422,
      "learning_rate": 3.0639583333333334e-05,
      "loss": 0.0018,
      "step": 92930
    },
    {
      "epoch": 3.098,
      "grad_norm": 0.3157522976398468,
      "learning_rate": 3.06375e-05,
      "loss": 0.002,
      "step": 92940
    },
    {
      "epoch": 3.098333333333333,
      "grad_norm": 0.4881611168384552,
      "learning_rate": 3.0635416666666665e-05,
      "loss": 0.0017,
      "step": 92950
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.22961267828941345,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0025,
      "step": 92960
    },
    {
      "epoch": 3.099,
      "grad_norm": 0.31592321395874023,
      "learning_rate": 3.063125e-05,
      "loss": 0.0018,
      "step": 92970
    },
    {
      "epoch": 3.0993333333333335,
      "grad_norm": 0.08652820438146591,
      "learning_rate": 3.062916666666667e-05,
      "loss": 0.0015,
      "step": 92980
    },
    {
      "epoch": 3.099666666666667,
      "grad_norm": 0.2579496204853058,
      "learning_rate": 3.0627083333333334e-05,
      "loss": 0.0022,
      "step": 92990
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.15515880286693573,
      "learning_rate": 3.0625000000000006e-05,
      "loss": 0.0024,
      "step": 93000
    },
    {
      "epoch": 3.1003333333333334,
      "grad_norm": 0.02906951680779457,
      "learning_rate": 3.0622916666666664e-05,
      "loss": 0.0025,
      "step": 93010
    },
    {
      "epoch": 3.1006666666666667,
      "grad_norm": 0.1726638525724411,
      "learning_rate": 3.062083333333334e-05,
      "loss": 0.0018,
      "step": 93020
    },
    {
      "epoch": 3.101,
      "grad_norm": 0.2296806424856186,
      "learning_rate": 3.061875e-05,
      "loss": 0.0027,
      "step": 93030
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.08656056970357895,
      "learning_rate": 3.061666666666667e-05,
      "loss": 0.0017,
      "step": 93040
    },
    {
      "epoch": 3.1016666666666666,
      "grad_norm": 0.22977282106876373,
      "learning_rate": 3.061458333333333e-05,
      "loss": 0.0021,
      "step": 93050
    },
    {
      "epoch": 3.102,
      "grad_norm": 0.17240090668201447,
      "learning_rate": 3.06125e-05,
      "loss": 0.0021,
      "step": 93060
    },
    {
      "epoch": 3.102333333333333,
      "grad_norm": 0.08655305951833725,
      "learning_rate": 3.061041666666667e-05,
      "loss": 0.002,
      "step": 93070
    },
    {
      "epoch": 3.1026666666666665,
      "grad_norm": 0.4460938572883606,
      "learning_rate": 3.0608333333333336e-05,
      "loss": 0.0025,
      "step": 93080
    },
    {
      "epoch": 3.103,
      "grad_norm": 0.006662370637059212,
      "learning_rate": 3.060625e-05,
      "loss": 0.0018,
      "step": 93090
    },
    {
      "epoch": 3.1033333333333335,
      "grad_norm": 0.006630155257880688,
      "learning_rate": 3.060416666666667e-05,
      "loss": 0.0014,
      "step": 93100
    },
    {
      "epoch": 3.103666666666667,
      "grad_norm": 0.14437612891197205,
      "learning_rate": 3.060208333333334e-05,
      "loss": 0.002,
      "step": 93110
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.6030700206756592,
      "learning_rate": 3.06e-05,
      "loss": 0.0019,
      "step": 93120
    },
    {
      "epoch": 3.1043333333333334,
      "grad_norm": 0.4879561960697174,
      "learning_rate": 3.059791666666667e-05,
      "loss": 0.0021,
      "step": 93130
    },
    {
      "epoch": 3.1046666666666667,
      "grad_norm": 0.6887227892875671,
      "learning_rate": 3.0595833333333336e-05,
      "loss": 0.0024,
      "step": 93140
    },
    {
      "epoch": 3.105,
      "grad_norm": 0.05768614634871483,
      "learning_rate": 3.059375e-05,
      "loss": 0.0027,
      "step": 93150
    },
    {
      "epoch": 3.1053333333333333,
      "grad_norm": 0.13597440719604492,
      "learning_rate": 3.059166666666667e-05,
      "loss": 0.0018,
      "step": 93160
    },
    {
      "epoch": 3.1056666666666666,
      "grad_norm": 0.05756144970655441,
      "learning_rate": 3.058958333333333e-05,
      "loss": 0.0016,
      "step": 93170
    },
    {
      "epoch": 3.106,
      "grad_norm": 0.05755850300192833,
      "learning_rate": 3.0587500000000005e-05,
      "loss": 0.002,
      "step": 93180
    },
    {
      "epoch": 3.106333333333333,
      "grad_norm": 0.029207367449998856,
      "learning_rate": 3.0585416666666664e-05,
      "loss": 0.0028,
      "step": 93190
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.17226064205169678,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.0019,
      "step": 93200
    },
    {
      "epoch": 3.107,
      "grad_norm": 0.029181091114878654,
      "learning_rate": 3.058125e-05,
      "loss": 0.0023,
      "step": 93210
    },
    {
      "epoch": 3.1073333333333335,
      "grad_norm": 0.029517561197280884,
      "learning_rate": 3.057916666666667e-05,
      "loss": 0.0017,
      "step": 93220
    },
    {
      "epoch": 3.107666666666667,
      "grad_norm": 0.4020400643348694,
      "learning_rate": 3.057708333333333e-05,
      "loss": 0.0026,
      "step": 93230
    },
    {
      "epoch": 3.108,
      "grad_norm": 0.4305911064147949,
      "learning_rate": 3.0575000000000005e-05,
      "loss": 0.0022,
      "step": 93240
    },
    {
      "epoch": 3.1083333333333334,
      "grad_norm": 0.17252658307552338,
      "learning_rate": 3.057291666666667e-05,
      "loss": 0.0016,
      "step": 93250
    },
    {
      "epoch": 3.1086666666666667,
      "grad_norm": 0.17219987511634827,
      "learning_rate": 3.0570833333333336e-05,
      "loss": 0.0013,
      "step": 93260
    },
    {
      "epoch": 3.109,
      "grad_norm": 0.5418418049812317,
      "learning_rate": 3.056875e-05,
      "loss": 0.0023,
      "step": 93270
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.5940113067626953,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0026,
      "step": 93280
    },
    {
      "epoch": 3.1096666666666666,
      "grad_norm": 0.6307979822158813,
      "learning_rate": 3.056458333333333e-05,
      "loss": 0.003,
      "step": 93290
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.3444647490978241,
      "learning_rate": 3.05625e-05,
      "loss": 0.0027,
      "step": 93300
    },
    {
      "epoch": 3.110333333333333,
      "grad_norm": 0.2298581302165985,
      "learning_rate": 3.056041666666667e-05,
      "loss": 0.0026,
      "step": 93310
    },
    {
      "epoch": 3.1106666666666665,
      "grad_norm": 0.05749283358454704,
      "learning_rate": 3.0558333333333335e-05,
      "loss": 0.0019,
      "step": 93320
    },
    {
      "epoch": 3.111,
      "grad_norm": 0.11518973857164383,
      "learning_rate": 3.055625e-05,
      "loss": 0.0022,
      "step": 93330
    },
    {
      "epoch": 3.1113333333333335,
      "grad_norm": 0.08606547862291336,
      "learning_rate": 3.0554166666666666e-05,
      "loss": 0.0017,
      "step": 93340
    },
    {
      "epoch": 3.111666666666667,
      "grad_norm": 0.1722046434879303,
      "learning_rate": 3.055208333333334e-05,
      "loss": 0.0031,
      "step": 93350
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.08606336265802383,
      "learning_rate": 3.0550000000000004e-05,
      "loss": 0.003,
      "step": 93360
    },
    {
      "epoch": 3.1123333333333334,
      "grad_norm": 0.45914405584335327,
      "learning_rate": 3.054791666666667e-05,
      "loss": 0.0013,
      "step": 93370
    },
    {
      "epoch": 3.1126666666666667,
      "grad_norm": 0.37340521812438965,
      "learning_rate": 3.0545833333333335e-05,
      "loss": 0.0029,
      "step": 93380
    },
    {
      "epoch": 3.113,
      "grad_norm": 0.02899944968521595,
      "learning_rate": 3.054375e-05,
      "loss": 0.0021,
      "step": 93390
    },
    {
      "epoch": 3.1133333333333333,
      "grad_norm": 0.17236195504665375,
      "learning_rate": 3.0541666666666666e-05,
      "loss": 0.0017,
      "step": 93400
    },
    {
      "epoch": 3.1136666666666666,
      "grad_norm": 0.3442721664905548,
      "learning_rate": 3.053958333333333e-05,
      "loss": 0.0025,
      "step": 93410
    },
    {
      "epoch": 3.114,
      "grad_norm": 0.11494376510381699,
      "learning_rate": 3.0537500000000004e-05,
      "loss": 0.0018,
      "step": 93420
    },
    {
      "epoch": 3.114333333333333,
      "grad_norm": 0.2588116526603699,
      "learning_rate": 3.053541666666667e-05,
      "loss": 0.0017,
      "step": 93430
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.11493811756372452,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0025,
      "step": 93440
    },
    {
      "epoch": 3.115,
      "grad_norm": 0.35072439908981323,
      "learning_rate": 3.053125e-05,
      "loss": 0.0022,
      "step": 93450
    },
    {
      "epoch": 3.1153333333333335,
      "grad_norm": 0.004811425693333149,
      "learning_rate": 3.052916666666667e-05,
      "loss": 0.0018,
      "step": 93460
    },
    {
      "epoch": 3.115666666666667,
      "grad_norm": 0.31555411219596863,
      "learning_rate": 3.052708333333333e-05,
      "loss": 0.0022,
      "step": 93470
    },
    {
      "epoch": 3.116,
      "grad_norm": 0.02913549914956093,
      "learning_rate": 3.0525e-05,
      "loss": 0.0018,
      "step": 93480
    },
    {
      "epoch": 3.1163333333333334,
      "grad_norm": 0.172283336520195,
      "learning_rate": 3.052291666666667e-05,
      "loss": 0.0018,
      "step": 93490
    },
    {
      "epoch": 3.1166666666666667,
      "grad_norm": 0.22957739233970642,
      "learning_rate": 3.0520833333333334e-05,
      "loss": 0.0014,
      "step": 93500
    },
    {
      "epoch": 3.117,
      "grad_norm": 0.14370214939117432,
      "learning_rate": 3.051875e-05,
      "loss": 0.002,
      "step": 93510
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.17220072448253632,
      "learning_rate": 3.0516666666666665e-05,
      "loss": 0.0023,
      "step": 93520
    },
    {
      "epoch": 3.1176666666666666,
      "grad_norm": 0.1436806470155716,
      "learning_rate": 3.0514583333333334e-05,
      "loss": 0.0023,
      "step": 93530
    },
    {
      "epoch": 3.118,
      "grad_norm": 0.2008841335773468,
      "learning_rate": 3.05125e-05,
      "loss": 0.0023,
      "step": 93540
    },
    {
      "epoch": 3.118333333333333,
      "grad_norm": 0.22944912314414978,
      "learning_rate": 3.051041666666667e-05,
      "loss": 0.0019,
      "step": 93550
    },
    {
      "epoch": 3.1186666666666665,
      "grad_norm": 0.22949369251728058,
      "learning_rate": 3.0508333333333334e-05,
      "loss": 0.0029,
      "step": 93560
    },
    {
      "epoch": 3.1189999999999998,
      "grad_norm": 0.08680172264575958,
      "learning_rate": 3.0506250000000003e-05,
      "loss": 0.0026,
      "step": 93570
    },
    {
      "epoch": 3.1193333333333335,
      "grad_norm": 0.14392681419849396,
      "learning_rate": 3.050416666666667e-05,
      "loss": 0.0021,
      "step": 93580
    },
    {
      "epoch": 3.119666666666667,
      "grad_norm": 0.029931960627436638,
      "learning_rate": 3.0502083333333337e-05,
      "loss": 0.0016,
      "step": 93590
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.17225424945354462,
      "learning_rate": 3.05e-05,
      "loss": 0.0025,
      "step": 93600
    },
    {
      "epoch": 3.1203333333333334,
      "grad_norm": 0.48629438877105713,
      "learning_rate": 3.049791666666667e-05,
      "loss": 0.002,
      "step": 93610
    },
    {
      "epoch": 3.1206666666666667,
      "grad_norm": 0.25835058093070984,
      "learning_rate": 3.0495833333333334e-05,
      "loss": 0.0017,
      "step": 93620
    },
    {
      "epoch": 3.121,
      "grad_norm": 0.08634251356124878,
      "learning_rate": 3.049375e-05,
      "loss": 0.0027,
      "step": 93630
    },
    {
      "epoch": 3.1213333333333333,
      "grad_norm": 0.28727811574935913,
      "learning_rate": 3.0491666666666668e-05,
      "loss": 0.0015,
      "step": 93640
    },
    {
      "epoch": 3.1216666666666666,
      "grad_norm": 0.003181514795869589,
      "learning_rate": 3.0489583333333334e-05,
      "loss": 0.0018,
      "step": 93650
    },
    {
      "epoch": 3.122,
      "grad_norm": 0.029842684045433998,
      "learning_rate": 3.0487500000000002e-05,
      "loss": 0.0037,
      "step": 93660
    },
    {
      "epoch": 3.122333333333333,
      "grad_norm": 0.3161003291606903,
      "learning_rate": 3.0485416666666668e-05,
      "loss": 0.0028,
      "step": 93670
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.5453290343284607,
      "learning_rate": 3.0483333333333337e-05,
      "loss": 0.0024,
      "step": 93680
    },
    {
      "epoch": 3.123,
      "grad_norm": 0.11510073393583298,
      "learning_rate": 3.048125e-05,
      "loss": 0.0021,
      "step": 93690
    },
    {
      "epoch": 3.1233333333333335,
      "grad_norm": 0.3732373118400574,
      "learning_rate": 3.047916666666667e-05,
      "loss": 0.0023,
      "step": 93700
    },
    {
      "epoch": 3.123666666666667,
      "grad_norm": 0.6604834198951721,
      "learning_rate": 3.0477083333333333e-05,
      "loss": 0.0026,
      "step": 93710
    },
    {
      "epoch": 3.124,
      "grad_norm": 0.1149483174085617,
      "learning_rate": 3.0475000000000002e-05,
      "loss": 0.0022,
      "step": 93720
    },
    {
      "epoch": 3.1243333333333334,
      "grad_norm": 0.6670731902122498,
      "learning_rate": 3.0472916666666668e-05,
      "loss": 0.0025,
      "step": 93730
    },
    {
      "epoch": 3.1246666666666667,
      "grad_norm": 0.057538583874702454,
      "learning_rate": 3.0470833333333337e-05,
      "loss": 0.0026,
      "step": 93740
    },
    {
      "epoch": 3.125,
      "grad_norm": 0.3731016516685486,
      "learning_rate": 3.0468750000000002e-05,
      "loss": 0.002,
      "step": 93750
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.2867431640625,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0026,
      "step": 93760
    },
    {
      "epoch": 3.1256666666666666,
      "grad_norm": 0.31583648920059204,
      "learning_rate": 3.0464583333333336e-05,
      "loss": 0.0039,
      "step": 93770
    },
    {
      "epoch": 3.126,
      "grad_norm": 0.006991254165768623,
      "learning_rate": 3.04625e-05,
      "loss": 0.0016,
      "step": 93780
    },
    {
      "epoch": 3.126333333333333,
      "grad_norm": 0.14365904033184052,
      "learning_rate": 3.046041666666667e-05,
      "loss": 0.0015,
      "step": 93790
    },
    {
      "epoch": 3.1266666666666665,
      "grad_norm": 0.005659392569214106,
      "learning_rate": 3.0458333333333333e-05,
      "loss": 0.0025,
      "step": 93800
    },
    {
      "epoch": 3.127,
      "grad_norm": 0.25837185978889465,
      "learning_rate": 3.045625e-05,
      "loss": 0.0021,
      "step": 93810
    },
    {
      "epoch": 3.1273333333333335,
      "grad_norm": 0.08615458011627197,
      "learning_rate": 3.0454166666666667e-05,
      "loss": 0.0029,
      "step": 93820
    },
    {
      "epoch": 3.127666666666667,
      "grad_norm": 0.02976459078490734,
      "learning_rate": 3.0452083333333336e-05,
      "loss": 0.0017,
      "step": 93830
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.02939615771174431,
      "learning_rate": 3.045e-05,
      "loss": 0.0023,
      "step": 93840
    },
    {
      "epoch": 3.1283333333333334,
      "grad_norm": 1.1185977458953857,
      "learning_rate": 3.044791666666667e-05,
      "loss": 0.002,
      "step": 93850
    },
    {
      "epoch": 3.1286666666666667,
      "grad_norm": 0.34464144706726074,
      "learning_rate": 3.0445833333333336e-05,
      "loss": 0.0033,
      "step": 93860
    },
    {
      "epoch": 3.129,
      "grad_norm": 0.25808051228523254,
      "learning_rate": 3.0443750000000005e-05,
      "loss": 0.003,
      "step": 93870
    },
    {
      "epoch": 3.1293333333333333,
      "grad_norm": 0.2584121823310852,
      "learning_rate": 3.0441666666666667e-05,
      "loss": 0.0033,
      "step": 93880
    },
    {
      "epoch": 3.1296666666666666,
      "grad_norm": 0.22967183589935303,
      "learning_rate": 3.0439583333333332e-05,
      "loss": 0.0022,
      "step": 93890
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.5323305130004883,
      "learning_rate": 3.04375e-05,
      "loss": 0.0027,
      "step": 93900
    },
    {
      "epoch": 3.130333333333333,
      "grad_norm": 0.14349530637264252,
      "learning_rate": 3.0435416666666667e-05,
      "loss": 0.0023,
      "step": 93910
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.05791294574737549,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.002,
      "step": 93920
    },
    {
      "epoch": 3.1310000000000002,
      "grad_norm": 0.25821512937545776,
      "learning_rate": 3.043125e-05,
      "loss": 0.002,
      "step": 93930
    },
    {
      "epoch": 3.1313333333333335,
      "grad_norm": 0.05786367878317833,
      "learning_rate": 3.042916666666667e-05,
      "loss": 0.0029,
      "step": 93940
    },
    {
      "epoch": 3.131666666666667,
      "grad_norm": 0.2869848906993866,
      "learning_rate": 3.0427083333333335e-05,
      "loss": 0.0017,
      "step": 93950
    },
    {
      "epoch": 3.132,
      "grad_norm": 0.08665067702531815,
      "learning_rate": 3.0425000000000004e-05,
      "loss": 0.0016,
      "step": 93960
    },
    {
      "epoch": 3.1323333333333334,
      "grad_norm": 0.2009858638048172,
      "learning_rate": 3.0422916666666666e-05,
      "loss": 0.0029,
      "step": 93970
    },
    {
      "epoch": 3.1326666666666667,
      "grad_norm": 0.08619517832994461,
      "learning_rate": 3.042083333333334e-05,
      "loss": 0.0023,
      "step": 93980
    },
    {
      "epoch": 3.133,
      "grad_norm": 0.05762995406985283,
      "learning_rate": 3.041875e-05,
      "loss": 0.0033,
      "step": 93990
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.05785197764635086,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.0029,
      "step": 94000
    },
    {
      "epoch": 3.1336666666666666,
      "grad_norm": 0.05789356306195259,
      "learning_rate": 3.0414583333333335e-05,
      "loss": 0.0018,
      "step": 94010
    },
    {
      "epoch": 3.134,
      "grad_norm": 0.22970879077911377,
      "learning_rate": 3.04125e-05,
      "loss": 0.0014,
      "step": 94020
    },
    {
      "epoch": 3.134333333333333,
      "grad_norm": 0.22946220636367798,
      "learning_rate": 3.041041666666667e-05,
      "loss": 0.0019,
      "step": 94030
    },
    {
      "epoch": 3.1346666666666665,
      "grad_norm": 0.17211434245109558,
      "learning_rate": 3.040833333333333e-05,
      "loss": 0.0019,
      "step": 94040
    },
    {
      "epoch": 3.135,
      "grad_norm": 0.34417232871055603,
      "learning_rate": 3.0406250000000004e-05,
      "loss": 0.003,
      "step": 94050
    },
    {
      "epoch": 3.1353333333333335,
      "grad_norm": 0.0871896743774414,
      "learning_rate": 3.0404166666666666e-05,
      "loss": 0.0011,
      "step": 94060
    },
    {
      "epoch": 3.135666666666667,
      "grad_norm": 0.029081817716360092,
      "learning_rate": 3.0402083333333338e-05,
      "loss": 0.0024,
      "step": 94070
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.2119119018316269,
      "learning_rate": 3.04e-05,
      "loss": 0.0024,
      "step": 94080
    },
    {
      "epoch": 3.1363333333333334,
      "grad_norm": 0.22959551215171814,
      "learning_rate": 3.039791666666667e-05,
      "loss": 0.0023,
      "step": 94090
    },
    {
      "epoch": 3.1366666666666667,
      "grad_norm": 0.3442980945110321,
      "learning_rate": 3.0395833333333335e-05,
      "loss": 0.0022,
      "step": 94100
    },
    {
      "epoch": 3.137,
      "grad_norm": 0.05736459419131279,
      "learning_rate": 3.0393750000000004e-05,
      "loss": 0.0026,
      "step": 94110
    },
    {
      "epoch": 3.1373333333333333,
      "grad_norm": 0.45921945571899414,
      "learning_rate": 3.039166666666667e-05,
      "loss": 0.0017,
      "step": 94120
    },
    {
      "epoch": 3.1376666666666666,
      "grad_norm": 0.20110449194908142,
      "learning_rate": 3.038958333333333e-05,
      "loss": 0.0019,
      "step": 94130
    },
    {
      "epoch": 3.138,
      "grad_norm": 0.058129314333200455,
      "learning_rate": 3.0387500000000003e-05,
      "loss": 0.002,
      "step": 94140
    },
    {
      "epoch": 3.138333333333333,
      "grad_norm": 0.22978363931179047,
      "learning_rate": 3.0385416666666666e-05,
      "loss": 0.002,
      "step": 94150
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.11529196053743362,
      "learning_rate": 3.0383333333333334e-05,
      "loss": 0.0021,
      "step": 94160
    },
    {
      "epoch": 3.1390000000000002,
      "grad_norm": 0.5985546708106995,
      "learning_rate": 3.038125e-05,
      "loss": 0.0026,
      "step": 94170
    },
    {
      "epoch": 3.1393333333333335,
      "grad_norm": 0.11533273011445999,
      "learning_rate": 3.037916666666667e-05,
      "loss": 0.0025,
      "step": 94180
    },
    {
      "epoch": 3.139666666666667,
      "grad_norm": 0.5375094413757324,
      "learning_rate": 3.0377083333333334e-05,
      "loss": 0.0026,
      "step": 94190
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.20085200667381287,
      "learning_rate": 3.0375000000000003e-05,
      "loss": 0.0018,
      "step": 94200
    },
    {
      "epoch": 3.1403333333333334,
      "grad_norm": 0.0289271492511034,
      "learning_rate": 3.037291666666667e-05,
      "loss": 0.0023,
      "step": 94210
    },
    {
      "epoch": 3.1406666666666667,
      "grad_norm": 0.02882777526974678,
      "learning_rate": 3.0370833333333337e-05,
      "loss": 0.0021,
      "step": 94220
    },
    {
      "epoch": 3.141,
      "grad_norm": 0.11509791016578674,
      "learning_rate": 3.0368750000000003e-05,
      "loss": 0.0023,
      "step": 94230
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.05741450563073158,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0019,
      "step": 94240
    },
    {
      "epoch": 3.1416666666666666,
      "grad_norm": 0.2010459303855896,
      "learning_rate": 3.0364583333333334e-05,
      "loss": 0.0015,
      "step": 94250
    },
    {
      "epoch": 3.142,
      "grad_norm": 0.5166783928871155,
      "learning_rate": 3.03625e-05,
      "loss": 0.0024,
      "step": 94260
    },
    {
      "epoch": 3.142333333333333,
      "grad_norm": 0.20832118391990662,
      "learning_rate": 3.0360416666666668e-05,
      "loss": 0.0022,
      "step": 94270
    },
    {
      "epoch": 3.1426666666666665,
      "grad_norm": 0.05759495869278908,
      "learning_rate": 3.0358333333333334e-05,
      "loss": 0.0024,
      "step": 94280
    },
    {
      "epoch": 3.143,
      "grad_norm": 0.34433478116989136,
      "learning_rate": 3.0356250000000003e-05,
      "loss": 0.0016,
      "step": 94290
    },
    {
      "epoch": 3.1433333333333335,
      "grad_norm": 0.6601662039756775,
      "learning_rate": 3.0354166666666668e-05,
      "loss": 0.0022,
      "step": 94300
    },
    {
      "epoch": 3.143666666666667,
      "grad_norm": 0.40266478061676025,
      "learning_rate": 3.0352083333333337e-05,
      "loss": 0.0016,
      "step": 94310
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.20127332210540771,
      "learning_rate": 3.035e-05,
      "loss": 0.0015,
      "step": 94320
    },
    {
      "epoch": 3.1443333333333334,
      "grad_norm": 0.05795067176222801,
      "learning_rate": 3.034791666666667e-05,
      "loss": 0.0024,
      "step": 94330
    },
    {
      "epoch": 3.1446666666666667,
      "grad_norm": 0.2582724988460541,
      "learning_rate": 3.0345833333333333e-05,
      "loss": 0.0019,
      "step": 94340
    },
    {
      "epoch": 3.145,
      "grad_norm": 0.17228180170059204,
      "learning_rate": 3.0343750000000006e-05,
      "loss": 0.0031,
      "step": 94350
    },
    {
      "epoch": 3.1453333333333333,
      "grad_norm": 0.08629819750785828,
      "learning_rate": 3.0341666666666668e-05,
      "loss": 0.0025,
      "step": 94360
    },
    {
      "epoch": 3.1456666666666666,
      "grad_norm": 0.3159630596637726,
      "learning_rate": 3.0339583333333333e-05,
      "loss": 0.0021,
      "step": 94370
    },
    {
      "epoch": 3.146,
      "grad_norm": 0.057828571647405624,
      "learning_rate": 3.0337500000000002e-05,
      "loss": 0.0014,
      "step": 94380
    },
    {
      "epoch": 3.146333333333333,
      "grad_norm": 0.11505212634801865,
      "learning_rate": 3.0335416666666668e-05,
      "loss": 0.0025,
      "step": 94390
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.029271259903907776,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0026,
      "step": 94400
    },
    {
      "epoch": 3.147,
      "grad_norm": 0.40182507038116455,
      "learning_rate": 3.033125e-05,
      "loss": 0.0023,
      "step": 94410
    },
    {
      "epoch": 3.1473333333333335,
      "grad_norm": 0.17257492244243622,
      "learning_rate": 3.032916666666667e-05,
      "loss": 0.0018,
      "step": 94420
    },
    {
      "epoch": 3.147666666666667,
      "grad_norm": 0.6358069777488708,
      "learning_rate": 3.0327083333333333e-05,
      "loss": 0.0031,
      "step": 94430
    },
    {
      "epoch": 3.148,
      "grad_norm": 0.08625036478042603,
      "learning_rate": 3.0325000000000002e-05,
      "loss": 0.0025,
      "step": 94440
    },
    {
      "epoch": 3.1483333333333334,
      "grad_norm": 0.11477383971214294,
      "learning_rate": 3.0322916666666667e-05,
      "loss": 0.0025,
      "step": 94450
    },
    {
      "epoch": 3.1486666666666667,
      "grad_norm": 0.2008732110261917,
      "learning_rate": 3.0320833333333336e-05,
      "loss": 0.0025,
      "step": 94460
    },
    {
      "epoch": 3.149,
      "grad_norm": 0.4877377450466156,
      "learning_rate": 3.0318750000000002e-05,
      "loss": 0.0017,
      "step": 94470
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.02887599542737007,
      "learning_rate": 3.0316666666666664e-05,
      "loss": 0.0019,
      "step": 94480
    },
    {
      "epoch": 3.1496666666666666,
      "grad_norm": 0.14368070662021637,
      "learning_rate": 3.0314583333333336e-05,
      "loss": 0.0024,
      "step": 94490
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.31556621193885803,
      "learning_rate": 3.0312499999999998e-05,
      "loss": 0.0023,
      "step": 94500
    },
    {
      "epoch": 3.150333333333333,
      "grad_norm": 0.17228104174137115,
      "learning_rate": 3.031041666666667e-05,
      "loss": 0.0024,
      "step": 94510
    },
    {
      "epoch": 3.1506666666666665,
      "grad_norm": 0.22969181835651398,
      "learning_rate": 3.0308333333333333e-05,
      "loss": 0.0017,
      "step": 94520
    },
    {
      "epoch": 3.151,
      "grad_norm": 0.34292009472846985,
      "learning_rate": 3.030625e-05,
      "loss": 0.0018,
      "step": 94530
    },
    {
      "epoch": 3.1513333333333335,
      "grad_norm": 0.02915875054895878,
      "learning_rate": 3.0304166666666667e-05,
      "loss": 0.0024,
      "step": 94540
    },
    {
      "epoch": 3.151666666666667,
      "grad_norm": 0.5899407863616943,
      "learning_rate": 3.0302083333333336e-05,
      "loss": 0.002,
      "step": 94550
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.3440382778644562,
      "learning_rate": 3.03e-05,
      "loss": 0.002,
      "step": 94560
    },
    {
      "epoch": 3.1523333333333334,
      "grad_norm": 0.21232293546199799,
      "learning_rate": 3.029791666666667e-05,
      "loss": 0.0019,
      "step": 94570
    },
    {
      "epoch": 3.1526666666666667,
      "grad_norm": 0.007793555501848459,
      "learning_rate": 3.0295833333333336e-05,
      "loss": 0.0016,
      "step": 94580
    },
    {
      "epoch": 3.153,
      "grad_norm": 0.25847747921943665,
      "learning_rate": 3.0293750000000005e-05,
      "loss": 0.0021,
      "step": 94590
    },
    {
      "epoch": 3.1533333333333333,
      "grad_norm": 0.1437039077281952,
      "learning_rate": 3.0291666666666667e-05,
      "loss": 0.0021,
      "step": 94600
    },
    {
      "epoch": 3.1536666666666666,
      "grad_norm": 0.3529101014137268,
      "learning_rate": 3.0289583333333332e-05,
      "loss": 0.0024,
      "step": 94610
    },
    {
      "epoch": 3.154,
      "grad_norm": 0.028874263167381287,
      "learning_rate": 3.02875e-05,
      "loss": 0.0019,
      "step": 94620
    },
    {
      "epoch": 3.154333333333333,
      "grad_norm": 0.00579660153016448,
      "learning_rate": 3.0285416666666666e-05,
      "loss": 0.0016,
      "step": 94630
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.17232291400432587,
      "learning_rate": 3.0283333333333335e-05,
      "loss": 0.0015,
      "step": 94640
    },
    {
      "epoch": 3.155,
      "grad_norm": 0.22970938682556152,
      "learning_rate": 3.028125e-05,
      "loss": 0.0019,
      "step": 94650
    },
    {
      "epoch": 3.155333333333333,
      "grad_norm": 0.5163601636886597,
      "learning_rate": 3.027916666666667e-05,
      "loss": 0.0025,
      "step": 94660
    },
    {
      "epoch": 3.155666666666667,
      "grad_norm": 0.36014172434806824,
      "learning_rate": 3.0277083333333335e-05,
      "loss": 0.0028,
      "step": 94670
    },
    {
      "epoch": 3.156,
      "grad_norm": 0.08712111413478851,
      "learning_rate": 3.0275000000000004e-05,
      "loss": 0.0017,
      "step": 94680
    },
    {
      "epoch": 3.1563333333333334,
      "grad_norm": 0.4306528866291046,
      "learning_rate": 3.0272916666666666e-05,
      "loss": 0.0021,
      "step": 94690
    },
    {
      "epoch": 3.1566666666666667,
      "grad_norm": 0.6276059150695801,
      "learning_rate": 3.027083333333334e-05,
      "loss": 0.0021,
      "step": 94700
    },
    {
      "epoch": 3.157,
      "grad_norm": 0.17213566601276398,
      "learning_rate": 3.026875e-05,
      "loss": 0.002,
      "step": 94710
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.2583361566066742,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0017,
      "step": 94720
    },
    {
      "epoch": 3.1576666666666666,
      "grad_norm": 0.14444029331207275,
      "learning_rate": 3.0264583333333335e-05,
      "loss": 0.0024,
      "step": 94730
    },
    {
      "epoch": 3.158,
      "grad_norm": 0.029530303552746773,
      "learning_rate": 3.02625e-05,
      "loss": 0.0028,
      "step": 94740
    },
    {
      "epoch": 3.158333333333333,
      "grad_norm": 0.31566956639289856,
      "learning_rate": 3.026041666666667e-05,
      "loss": 0.0019,
      "step": 94750
    },
    {
      "epoch": 3.1586666666666665,
      "grad_norm": 0.43047794699668884,
      "learning_rate": 3.025833333333333e-05,
      "loss": 0.0029,
      "step": 94760
    },
    {
      "epoch": 3.159,
      "grad_norm": 0.08603543788194656,
      "learning_rate": 3.0256250000000004e-05,
      "loss": 0.0023,
      "step": 94770
    },
    {
      "epoch": 3.1593333333333335,
      "grad_norm": 0.17254218459129333,
      "learning_rate": 3.0254166666666666e-05,
      "loss": 0.0029,
      "step": 94780
    },
    {
      "epoch": 3.159666666666667,
      "grad_norm": 0.0031629730947315693,
      "learning_rate": 3.0252083333333338e-05,
      "loss": 0.0032,
      "step": 94790
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.5147155523300171,
      "learning_rate": 3.025e-05,
      "loss": 0.0026,
      "step": 94800
    },
    {
      "epoch": 3.1603333333333334,
      "grad_norm": 0.2291969656944275,
      "learning_rate": 3.024791666666667e-05,
      "loss": 0.0031,
      "step": 94810
    },
    {
      "epoch": 3.1606666666666667,
      "grad_norm": 0.3153282105922699,
      "learning_rate": 3.0245833333333334e-05,
      "loss": 0.0025,
      "step": 94820
    },
    {
      "epoch": 3.161,
      "grad_norm": 0.34462034702301025,
      "learning_rate": 3.0243750000000003e-05,
      "loss": 0.002,
      "step": 94830
    },
    {
      "epoch": 3.1613333333333333,
      "grad_norm": 0.28671568632125854,
      "learning_rate": 3.024166666666667e-05,
      "loss": 0.0025,
      "step": 94840
    },
    {
      "epoch": 3.1616666666666666,
      "grad_norm": 0.11492303758859634,
      "learning_rate": 3.023958333333333e-05,
      "loss": 0.0022,
      "step": 94850
    },
    {
      "epoch": 3.162,
      "grad_norm": 0.26158446073532104,
      "learning_rate": 3.0237500000000003e-05,
      "loss": 0.0024,
      "step": 94860
    },
    {
      "epoch": 3.162333333333333,
      "grad_norm": 0.20208628475666046,
      "learning_rate": 3.0235416666666665e-05,
      "loss": 0.0016,
      "step": 94870
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.003732089651748538,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.0018,
      "step": 94880
    },
    {
      "epoch": 3.163,
      "grad_norm": 0.1436181217432022,
      "learning_rate": 3.023125e-05,
      "loss": 0.0028,
      "step": 94890
    },
    {
      "epoch": 3.163333333333333,
      "grad_norm": 0.02875439263880253,
      "learning_rate": 3.022916666666667e-05,
      "loss": 0.0018,
      "step": 94900
    },
    {
      "epoch": 3.163666666666667,
      "grad_norm": 0.037944238632917404,
      "learning_rate": 3.0227083333333334e-05,
      "loss": 0.0022,
      "step": 94910
    },
    {
      "epoch": 3.164,
      "grad_norm": 0.11508075147867203,
      "learning_rate": 3.0225000000000003e-05,
      "loss": 0.0027,
      "step": 94920
    },
    {
      "epoch": 3.1643333333333334,
      "grad_norm": 0.11522391438484192,
      "learning_rate": 3.022291666666667e-05,
      "loss": 0.0018,
      "step": 94930
    },
    {
      "epoch": 3.1646666666666667,
      "grad_norm": 0.029248245060443878,
      "learning_rate": 3.0220833333333337e-05,
      "loss": 0.0023,
      "step": 94940
    },
    {
      "epoch": 3.165,
      "grad_norm": 0.4019565284252167,
      "learning_rate": 3.0218750000000003e-05,
      "loss": 0.0019,
      "step": 94950
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.17236082255840302,
      "learning_rate": 3.0216666666666665e-05,
      "loss": 0.0026,
      "step": 94960
    },
    {
      "epoch": 3.1656666666666666,
      "grad_norm": 0.14367324113845825,
      "learning_rate": 3.0214583333333334e-05,
      "loss": 0.002,
      "step": 94970
    },
    {
      "epoch": 3.166,
      "grad_norm": 0.5605319142341614,
      "learning_rate": 3.02125e-05,
      "loss": 0.0027,
      "step": 94980
    },
    {
      "epoch": 3.166333333333333,
      "grad_norm": 0.3442407250404358,
      "learning_rate": 3.0210416666666668e-05,
      "loss": 0.0016,
      "step": 94990
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.2866673469543457,
      "learning_rate": 3.0208333333333334e-05,
      "loss": 0.0018,
      "step": 95000
    },
    {
      "epoch": 3.167,
      "grad_norm": 0.11512666940689087,
      "learning_rate": 3.0206250000000002e-05,
      "loss": 0.0014,
      "step": 95010
    },
    {
      "epoch": 3.1673333333333336,
      "grad_norm": 0.5295423865318298,
      "learning_rate": 3.0204166666666668e-05,
      "loss": 0.0023,
      "step": 95020
    },
    {
      "epoch": 3.167666666666667,
      "grad_norm": 0.14354924857616425,
      "learning_rate": 3.0202083333333337e-05,
      "loss": 0.002,
      "step": 95030
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.028955470770597458,
      "learning_rate": 3.02e-05,
      "loss": 0.0019,
      "step": 95040
    },
    {
      "epoch": 3.1683333333333334,
      "grad_norm": 0.25810447335243225,
      "learning_rate": 3.019791666666667e-05,
      "loss": 0.0025,
      "step": 95050
    },
    {
      "epoch": 3.1686666666666667,
      "grad_norm": 0.14346925914287567,
      "learning_rate": 3.0195833333333333e-05,
      "loss": 0.0012,
      "step": 95060
    },
    {
      "epoch": 3.169,
      "grad_norm": 0.2010909914970398,
      "learning_rate": 3.0193750000000005e-05,
      "loss": 0.0023,
      "step": 95070
    },
    {
      "epoch": 3.1693333333333333,
      "grad_norm": 0.2583175003528595,
      "learning_rate": 3.0191666666666668e-05,
      "loss": 0.0017,
      "step": 95080
    },
    {
      "epoch": 3.1696666666666666,
      "grad_norm": 0.17216934263706207,
      "learning_rate": 3.0189583333333333e-05,
      "loss": 0.0017,
      "step": 95090
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.029530124738812447,
      "learning_rate": 3.0187500000000002e-05,
      "loss": 0.0022,
      "step": 95100
    },
    {
      "epoch": 3.1703333333333332,
      "grad_norm": 0.08623870462179184,
      "learning_rate": 3.0185416666666667e-05,
      "loss": 0.0026,
      "step": 95110
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.18507231771945953,
      "learning_rate": 3.0183333333333336e-05,
      "loss": 0.0024,
      "step": 95120
    },
    {
      "epoch": 3.171,
      "grad_norm": 0.17243482172489166,
      "learning_rate": 3.018125e-05,
      "loss": 0.003,
      "step": 95130
    },
    {
      "epoch": 3.171333333333333,
      "grad_norm": 0.11536961793899536,
      "learning_rate": 3.017916666666667e-05,
      "loss": 0.0027,
      "step": 95140
    },
    {
      "epoch": 3.171666666666667,
      "grad_norm": 0.4874950349330902,
      "learning_rate": 3.0177083333333333e-05,
      "loss": 0.0024,
      "step": 95150
    },
    {
      "epoch": 3.172,
      "grad_norm": 0.029574304819107056,
      "learning_rate": 3.0175e-05,
      "loss": 0.002,
      "step": 95160
    },
    {
      "epoch": 3.1723333333333334,
      "grad_norm": 0.45903632044792175,
      "learning_rate": 3.0172916666666667e-05,
      "loss": 0.002,
      "step": 95170
    },
    {
      "epoch": 3.1726666666666667,
      "grad_norm": 0.02888565883040428,
      "learning_rate": 3.0170833333333336e-05,
      "loss": 0.0022,
      "step": 95180
    },
    {
      "epoch": 3.173,
      "grad_norm": 0.20111778378486633,
      "learning_rate": 3.016875e-05,
      "loss": 0.0019,
      "step": 95190
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.11469655483961105,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0017,
      "step": 95200
    },
    {
      "epoch": 3.1736666666666666,
      "grad_norm": 0.3728899359703064,
      "learning_rate": 3.0164583333333336e-05,
      "loss": 0.0017,
      "step": 95210
    },
    {
      "epoch": 3.174,
      "grad_norm": 0.05797779932618141,
      "learning_rate": 3.0162499999999998e-05,
      "loss": 0.002,
      "step": 95220
    },
    {
      "epoch": 3.1743333333333332,
      "grad_norm": 0.08693180233240128,
      "learning_rate": 3.016041666666667e-05,
      "loss": 0.0017,
      "step": 95230
    },
    {
      "epoch": 3.1746666666666665,
      "grad_norm": 0.029199225828051567,
      "learning_rate": 3.0158333333333332e-05,
      "loss": 0.0014,
      "step": 95240
    },
    {
      "epoch": 3.175,
      "grad_norm": 0.25843346118927,
      "learning_rate": 3.015625e-05,
      "loss": 0.002,
      "step": 95250
    },
    {
      "epoch": 3.1753333333333336,
      "grad_norm": 0.2837144136428833,
      "learning_rate": 3.0154166666666667e-05,
      "loss": 0.0023,
      "step": 95260
    },
    {
      "epoch": 3.175666666666667,
      "grad_norm": 0.08604014664888382,
      "learning_rate": 3.0152083333333336e-05,
      "loss": 0.0014,
      "step": 95270
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.029670916497707367,
      "learning_rate": 3.015e-05,
      "loss": 0.0023,
      "step": 95280
    },
    {
      "epoch": 3.1763333333333335,
      "grad_norm": 0.029045702889561653,
      "learning_rate": 3.014791666666667e-05,
      "loss": 0.0015,
      "step": 95290
    },
    {
      "epoch": 3.1766666666666667,
      "grad_norm": 0.2295519858598709,
      "learning_rate": 3.0145833333333335e-05,
      "loss": 0.0028,
      "step": 95300
    },
    {
      "epoch": 3.177,
      "grad_norm": 0.17197369039058685,
      "learning_rate": 3.0143750000000004e-05,
      "loss": 0.0017,
      "step": 95310
    },
    {
      "epoch": 3.1773333333333333,
      "grad_norm": 0.5833079814910889,
      "learning_rate": 3.014166666666667e-05,
      "loss": 0.0024,
      "step": 95320
    },
    {
      "epoch": 3.1776666666666666,
      "grad_norm": 0.2581826448440552,
      "learning_rate": 3.0139583333333332e-05,
      "loss": 0.0014,
      "step": 95330
    },
    {
      "epoch": 3.178,
      "grad_norm": 0.37307947874069214,
      "learning_rate": 3.01375e-05,
      "loss": 0.002,
      "step": 95340
    },
    {
      "epoch": 3.1783333333333332,
      "grad_norm": 0.11504603922367096,
      "learning_rate": 3.0135416666666666e-05,
      "loss": 0.003,
      "step": 95350
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.05754851549863815,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0036,
      "step": 95360
    },
    {
      "epoch": 3.179,
      "grad_norm": 0.4055038094520569,
      "learning_rate": 3.013125e-05,
      "loss": 0.0033,
      "step": 95370
    },
    {
      "epoch": 3.179333333333333,
      "grad_norm": 0.5739834904670715,
      "learning_rate": 3.012916666666667e-05,
      "loss": 0.0022,
      "step": 95380
    },
    {
      "epoch": 3.179666666666667,
      "grad_norm": 0.17226848006248474,
      "learning_rate": 3.0127083333333335e-05,
      "loss": 0.0026,
      "step": 95390
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.20075657963752747,
      "learning_rate": 3.0125000000000004e-05,
      "loss": 0.0025,
      "step": 95400
    },
    {
      "epoch": 3.1803333333333335,
      "grad_norm": 0.02916799858212471,
      "learning_rate": 3.0122916666666666e-05,
      "loss": 0.0025,
      "step": 95410
    },
    {
      "epoch": 3.1806666666666668,
      "grad_norm": 0.2008165568113327,
      "learning_rate": 3.0120833333333338e-05,
      "loss": 0.0023,
      "step": 95420
    },
    {
      "epoch": 3.181,
      "grad_norm": 0.37310972809791565,
      "learning_rate": 3.011875e-05,
      "loss": 0.0019,
      "step": 95430
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.22972451150417328,
      "learning_rate": 3.011666666666667e-05,
      "loss": 0.0013,
      "step": 95440
    },
    {
      "epoch": 3.1816666666666666,
      "grad_norm": 0.3728125989437103,
      "learning_rate": 3.0114583333333335e-05,
      "loss": 0.0023,
      "step": 95450
    },
    {
      "epoch": 3.182,
      "grad_norm": 0.2009044885635376,
      "learning_rate": 3.01125e-05,
      "loss": 0.0026,
      "step": 95460
    },
    {
      "epoch": 3.1823333333333332,
      "grad_norm": 0.057700593024492264,
      "learning_rate": 3.011041666666667e-05,
      "loss": 0.0026,
      "step": 95470
    },
    {
      "epoch": 3.1826666666666665,
      "grad_norm": 0.37259551882743835,
      "learning_rate": 3.0108333333333334e-05,
      "loss": 0.0019,
      "step": 95480
    },
    {
      "epoch": 3.183,
      "grad_norm": 0.17210237681865692,
      "learning_rate": 3.0106250000000003e-05,
      "loss": 0.0019,
      "step": 95490
    },
    {
      "epoch": 3.183333333333333,
      "grad_norm": 0.546497642993927,
      "learning_rate": 3.0104166666666665e-05,
      "loss": 0.0022,
      "step": 95500
    },
    {
      "epoch": 3.183666666666667,
      "grad_norm": 0.3443657457828522,
      "learning_rate": 3.0102083333333338e-05,
      "loss": 0.0025,
      "step": 95510
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.20077404379844666,
      "learning_rate": 3.01e-05,
      "loss": 0.0015,
      "step": 95520
    },
    {
      "epoch": 3.1843333333333335,
      "grad_norm": 0.1052960753440857,
      "learning_rate": 3.009791666666667e-05,
      "loss": 0.0019,
      "step": 95530
    },
    {
      "epoch": 3.1846666666666668,
      "grad_norm": 0.08612442016601562,
      "learning_rate": 3.0095833333333334e-05,
      "loss": 0.0029,
      "step": 95540
    },
    {
      "epoch": 3.185,
      "grad_norm": 0.40151098370552063,
      "learning_rate": 3.0093750000000003e-05,
      "loss": 0.002,
      "step": 95550
    },
    {
      "epoch": 3.1853333333333333,
      "grad_norm": 0.2009650021791458,
      "learning_rate": 3.009166666666667e-05,
      "loss": 0.0022,
      "step": 95560
    },
    {
      "epoch": 3.1856666666666666,
      "grad_norm": 0.28679201006889343,
      "learning_rate": 3.008958333333333e-05,
      "loss": 0.0023,
      "step": 95570
    },
    {
      "epoch": 3.186,
      "grad_norm": 0.7165901064872742,
      "learning_rate": 3.0087500000000003e-05,
      "loss": 0.0019,
      "step": 95580
    },
    {
      "epoch": 3.1863333333333332,
      "grad_norm": 0.010642933659255505,
      "learning_rate": 3.0085416666666665e-05,
      "loss": 0.0017,
      "step": 95590
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.14376544952392578,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.0016,
      "step": 95600
    },
    {
      "epoch": 3.187,
      "grad_norm": 0.08643822371959686,
      "learning_rate": 3.008125e-05,
      "loss": 0.0022,
      "step": 95610
    },
    {
      "epoch": 3.187333333333333,
      "grad_norm": 0.05757516622543335,
      "learning_rate": 3.0079166666666668e-05,
      "loss": 0.0019,
      "step": 95620
    },
    {
      "epoch": 3.187666666666667,
      "grad_norm": 0.11469683051109314,
      "learning_rate": 3.0077083333333334e-05,
      "loss": 0.0021,
      "step": 95630
    },
    {
      "epoch": 3.188,
      "grad_norm": 0.3154262602329254,
      "learning_rate": 3.0075000000000003e-05,
      "loss": 0.0028,
      "step": 95640
    },
    {
      "epoch": 3.1883333333333335,
      "grad_norm": 0.28670933842658997,
      "learning_rate": 3.0072916666666668e-05,
      "loss": 0.0015,
      "step": 95650
    },
    {
      "epoch": 3.1886666666666668,
      "grad_norm": 0.40108540654182434,
      "learning_rate": 3.0070833333333337e-05,
      "loss": 0.002,
      "step": 95660
    },
    {
      "epoch": 3.189,
      "grad_norm": 0.31574705243110657,
      "learning_rate": 3.0068750000000002e-05,
      "loss": 0.0018,
      "step": 95670
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.3443821370601654,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0023,
      "step": 95680
    },
    {
      "epoch": 3.1896666666666667,
      "grad_norm": 0.28704139590263367,
      "learning_rate": 3.0064583333333333e-05,
      "loss": 0.0031,
      "step": 95690
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.05799761041998863,
      "learning_rate": 3.00625e-05,
      "loss": 0.0019,
      "step": 95700
    },
    {
      "epoch": 3.1903333333333332,
      "grad_norm": 0.1721520572900772,
      "learning_rate": 3.0060416666666668e-05,
      "loss": 0.0021,
      "step": 95710
    },
    {
      "epoch": 3.1906666666666665,
      "grad_norm": 0.033466093242168427,
      "learning_rate": 3.0058333333333333e-05,
      "loss": 0.0021,
      "step": 95720
    },
    {
      "epoch": 3.191,
      "grad_norm": 0.008238617330789566,
      "learning_rate": 3.0056250000000002e-05,
      "loss": 0.0014,
      "step": 95730
    },
    {
      "epoch": 3.191333333333333,
      "grad_norm": 0.05792832374572754,
      "learning_rate": 3.0054166666666668e-05,
      "loss": 0.0023,
      "step": 95740
    },
    {
      "epoch": 3.191666666666667,
      "grad_norm": 0.200703963637352,
      "learning_rate": 3.0052083333333336e-05,
      "loss": 0.0023,
      "step": 95750
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.22979764640331268,
      "learning_rate": 3.0050000000000002e-05,
      "loss": 0.0023,
      "step": 95760
    },
    {
      "epoch": 3.1923333333333335,
      "grad_norm": 0.14351584017276764,
      "learning_rate": 3.004791666666667e-05,
      "loss": 0.0019,
      "step": 95770
    },
    {
      "epoch": 3.1926666666666668,
      "grad_norm": 0.029012618586421013,
      "learning_rate": 3.0045833333333333e-05,
      "loss": 0.0019,
      "step": 95780
    },
    {
      "epoch": 3.193,
      "grad_norm": 0.2582486569881439,
      "learning_rate": 3.0043750000000005e-05,
      "loss": 0.0021,
      "step": 95790
    },
    {
      "epoch": 3.1933333333333334,
      "grad_norm": 0.5449576377868652,
      "learning_rate": 3.0041666666666667e-05,
      "loss": 0.002,
      "step": 95800
    },
    {
      "epoch": 3.1936666666666667,
      "grad_norm": 0.2578994333744049,
      "learning_rate": 3.0039583333333333e-05,
      "loss": 0.0011,
      "step": 95810
    },
    {
      "epoch": 3.194,
      "grad_norm": 0.4016728103160858,
      "learning_rate": 3.00375e-05,
      "loss": 0.0025,
      "step": 95820
    },
    {
      "epoch": 3.1943333333333332,
      "grad_norm": 0.008054574020206928,
      "learning_rate": 3.0035416666666667e-05,
      "loss": 0.0021,
      "step": 95830
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.6583317518234253,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0014,
      "step": 95840
    },
    {
      "epoch": 3.195,
      "grad_norm": 0.08638042956590652,
      "learning_rate": 3.0031249999999998e-05,
      "loss": 0.0022,
      "step": 95850
    },
    {
      "epoch": 3.195333333333333,
      "grad_norm": 0.22933806478977203,
      "learning_rate": 3.002916666666667e-05,
      "loss": 0.0028,
      "step": 95860
    },
    {
      "epoch": 3.195666666666667,
      "grad_norm": 0.11486757546663284,
      "learning_rate": 3.0027083333333332e-05,
      "loss": 0.0024,
      "step": 95870
    },
    {
      "epoch": 3.196,
      "grad_norm": 0.28643590211868286,
      "learning_rate": 3.0025000000000005e-05,
      "loss": 0.0021,
      "step": 95880
    },
    {
      "epoch": 3.1963333333333335,
      "grad_norm": 0.14329808950424194,
      "learning_rate": 3.0022916666666667e-05,
      "loss": 0.0023,
      "step": 95890
    },
    {
      "epoch": 3.1966666666666668,
      "grad_norm": 0.08615892380475998,
      "learning_rate": 3.0020833333333336e-05,
      "loss": 0.0024,
      "step": 95900
    },
    {
      "epoch": 3.197,
      "grad_norm": 0.11458133906126022,
      "learning_rate": 3.001875e-05,
      "loss": 0.002,
      "step": 95910
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.029300527647137642,
      "learning_rate": 3.001666666666667e-05,
      "loss": 0.0024,
      "step": 95920
    },
    {
      "epoch": 3.1976666666666667,
      "grad_norm": 0.3154243528842926,
      "learning_rate": 3.0014583333333336e-05,
      "loss": 0.0029,
      "step": 95930
    },
    {
      "epoch": 3.198,
      "grad_norm": 0.05778239667415619,
      "learning_rate": 3.0012499999999998e-05,
      "loss": 0.0013,
      "step": 95940
    },
    {
      "epoch": 3.1983333333333333,
      "grad_norm": 0.31524842977523804,
      "learning_rate": 3.001041666666667e-05,
      "loss": 0.002,
      "step": 95950
    },
    {
      "epoch": 3.1986666666666665,
      "grad_norm": 0.22963665425777435,
      "learning_rate": 3.0008333333333332e-05,
      "loss": 0.0026,
      "step": 95960
    },
    {
      "epoch": 3.199,
      "grad_norm": 0.1150580495595932,
      "learning_rate": 3.000625e-05,
      "loss": 0.0021,
      "step": 95970
    },
    {
      "epoch": 3.199333333333333,
      "grad_norm": 0.1723286211490631,
      "learning_rate": 3.0004166666666666e-05,
      "loss": 0.0024,
      "step": 95980
    },
    {
      "epoch": 3.1996666666666664,
      "grad_norm": 0.3688901662826538,
      "learning_rate": 3.0002083333333335e-05,
      "loss": 0.0031,
      "step": 95990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.544403076171875,
      "learning_rate": 3e-05,
      "loss": 0.0029,
      "step": 96000
    },
    {
      "epoch": 3.2003333333333335,
      "grad_norm": 0.343908429145813,
      "learning_rate": 2.999791666666667e-05,
      "loss": 0.0019,
      "step": 96010
    },
    {
      "epoch": 3.2006666666666668,
      "grad_norm": 0.057895857840776443,
      "learning_rate": 2.9995833333333335e-05,
      "loss": 0.0023,
      "step": 96020
    },
    {
      "epoch": 3.201,
      "grad_norm": 0.14317038655281067,
      "learning_rate": 2.9993750000000004e-05,
      "loss": 0.0016,
      "step": 96030
    },
    {
      "epoch": 3.2013333333333334,
      "grad_norm": 0.37311258912086487,
      "learning_rate": 2.999166666666667e-05,
      "loss": 0.0035,
      "step": 96040
    },
    {
      "epoch": 3.2016666666666667,
      "grad_norm": 0.3154139816761017,
      "learning_rate": 2.998958333333333e-05,
      "loss": 0.0027,
      "step": 96050
    },
    {
      "epoch": 3.202,
      "grad_norm": 0.1722813993692398,
      "learning_rate": 2.99875e-05,
      "loss": 0.002,
      "step": 96060
    },
    {
      "epoch": 3.2023333333333333,
      "grad_norm": 0.34389400482177734,
      "learning_rate": 2.9985416666666666e-05,
      "loss": 0.0018,
      "step": 96070
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.11507818847894669,
      "learning_rate": 2.9983333333333335e-05,
      "loss": 0.0028,
      "step": 96080
    },
    {
      "epoch": 3.203,
      "grad_norm": 0.1438031941652298,
      "learning_rate": 2.998125e-05,
      "loss": 0.0027,
      "step": 96090
    },
    {
      "epoch": 3.203333333333333,
      "grad_norm": 0.08646893501281738,
      "learning_rate": 2.997916666666667e-05,
      "loss": 0.0021,
      "step": 96100
    },
    {
      "epoch": 3.203666666666667,
      "grad_norm": 0.007495857309550047,
      "learning_rate": 2.9977083333333335e-05,
      "loss": 0.0021,
      "step": 96110
    },
    {
      "epoch": 3.204,
      "grad_norm": 0.02896600216627121,
      "learning_rate": 2.9975000000000004e-05,
      "loss": 0.0016,
      "step": 96120
    },
    {
      "epoch": 3.2043333333333335,
      "grad_norm": 0.029386715963482857,
      "learning_rate": 2.9972916666666666e-05,
      "loss": 0.0023,
      "step": 96130
    },
    {
      "epoch": 3.2046666666666668,
      "grad_norm": 0.4134407341480255,
      "learning_rate": 2.9970833333333338e-05,
      "loss": 0.002,
      "step": 96140
    },
    {
      "epoch": 3.205,
      "grad_norm": 0.03151893615722656,
      "learning_rate": 2.996875e-05,
      "loss": 0.0024,
      "step": 96150
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.2293732911348343,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.0027,
      "step": 96160
    },
    {
      "epoch": 3.2056666666666667,
      "grad_norm": 0.00782647542655468,
      "learning_rate": 2.9964583333333334e-05,
      "loss": 0.0021,
      "step": 96170
    },
    {
      "epoch": 3.206,
      "grad_norm": 0.4588368237018585,
      "learning_rate": 2.99625e-05,
      "loss": 0.0024,
      "step": 96180
    },
    {
      "epoch": 3.2063333333333333,
      "grad_norm": 0.086211197078228,
      "learning_rate": 2.996041666666667e-05,
      "loss": 0.002,
      "step": 96190
    },
    {
      "epoch": 3.2066666666666666,
      "grad_norm": 0.08611008524894714,
      "learning_rate": 2.9958333333333334e-05,
      "loss": 0.0029,
      "step": 96200
    },
    {
      "epoch": 3.207,
      "grad_norm": 0.17217688262462616,
      "learning_rate": 2.9956250000000003e-05,
      "loss": 0.0019,
      "step": 96210
    },
    {
      "epoch": 3.207333333333333,
      "grad_norm": 0.17233215272426605,
      "learning_rate": 2.9954166666666665e-05,
      "loss": 0.0022,
      "step": 96220
    },
    {
      "epoch": 3.2076666666666664,
      "grad_norm": 0.028911733999848366,
      "learning_rate": 2.9952083333333337e-05,
      "loss": 0.002,
      "step": 96230
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.17213383316993713,
      "learning_rate": 2.995e-05,
      "loss": 0.0022,
      "step": 96240
    },
    {
      "epoch": 3.2083333333333335,
      "grad_norm": 0.05755823850631714,
      "learning_rate": 2.994791666666667e-05,
      "loss": 0.0036,
      "step": 96250
    },
    {
      "epoch": 3.208666666666667,
      "grad_norm": 0.14329078793525696,
      "learning_rate": 2.9945833333333334e-05,
      "loss": 0.0022,
      "step": 96260
    },
    {
      "epoch": 3.209,
      "grad_norm": 0.2578909397125244,
      "learning_rate": 2.9943750000000003e-05,
      "loss": 0.0027,
      "step": 96270
    },
    {
      "epoch": 3.2093333333333334,
      "grad_norm": 0.34489157795906067,
      "learning_rate": 2.9941666666666668e-05,
      "loss": 0.0019,
      "step": 96280
    },
    {
      "epoch": 3.2096666666666667,
      "grad_norm": 0.17193418741226196,
      "learning_rate": 2.9939583333333337e-05,
      "loss": 0.0021,
      "step": 96290
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.311032235622406,
      "learning_rate": 2.9937500000000003e-05,
      "loss": 0.0024,
      "step": 96300
    },
    {
      "epoch": 3.2103333333333333,
      "grad_norm": 0.7227115631103516,
      "learning_rate": 2.9935416666666665e-05,
      "loss": 0.0027,
      "step": 96310
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.17222267389297485,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0025,
      "step": 96320
    },
    {
      "epoch": 3.211,
      "grad_norm": 0.14330434799194336,
      "learning_rate": 2.993125e-05,
      "loss": 0.0028,
      "step": 96330
    },
    {
      "epoch": 3.211333333333333,
      "grad_norm": 0.028991740196943283,
      "learning_rate": 2.9929166666666668e-05,
      "loss": 0.0028,
      "step": 96340
    },
    {
      "epoch": 3.211666666666667,
      "grad_norm": 0.2582876682281494,
      "learning_rate": 2.9927083333333333e-05,
      "loss": 0.0021,
      "step": 96350
    },
    {
      "epoch": 3.212,
      "grad_norm": 0.6433188319206238,
      "learning_rate": 2.9925000000000002e-05,
      "loss": 0.0015,
      "step": 96360
    },
    {
      "epoch": 3.2123333333333335,
      "grad_norm": 0.029281625524163246,
      "learning_rate": 2.9922916666666668e-05,
      "loss": 0.0016,
      "step": 96370
    },
    {
      "epoch": 3.212666666666667,
      "grad_norm": 0.22937224805355072,
      "learning_rate": 2.9920833333333337e-05,
      "loss": 0.0018,
      "step": 96380
    },
    {
      "epoch": 3.213,
      "grad_norm": 0.31566423177719116,
      "learning_rate": 2.9918750000000002e-05,
      "loss": 0.0024,
      "step": 96390
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.7554304599761963,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.0034,
      "step": 96400
    },
    {
      "epoch": 3.2136666666666667,
      "grad_norm": 0.05080850049853325,
      "learning_rate": 2.9914583333333333e-05,
      "loss": 0.0019,
      "step": 96410
    },
    {
      "epoch": 3.214,
      "grad_norm": 0.11500225216150284,
      "learning_rate": 2.99125e-05,
      "loss": 0.0028,
      "step": 96420
    },
    {
      "epoch": 3.2143333333333333,
      "grad_norm": 1.0953724384307861,
      "learning_rate": 2.9910416666666668e-05,
      "loss": 0.0021,
      "step": 96430
    },
    {
      "epoch": 3.2146666666666666,
      "grad_norm": 0.48753219842910767,
      "learning_rate": 2.9908333333333333e-05,
      "loss": 0.0022,
      "step": 96440
    },
    {
      "epoch": 3.215,
      "grad_norm": 0.02997596189379692,
      "learning_rate": 2.9906250000000002e-05,
      "loss": 0.0018,
      "step": 96450
    },
    {
      "epoch": 3.215333333333333,
      "grad_norm": 0.40139099955558777,
      "learning_rate": 2.9904166666666667e-05,
      "loss": 0.0025,
      "step": 96460
    },
    {
      "epoch": 3.2156666666666665,
      "grad_norm": 0.3153579831123352,
      "learning_rate": 2.9902083333333336e-05,
      "loss": 0.0014,
      "step": 96470
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.02887740172445774,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0014,
      "step": 96480
    },
    {
      "epoch": 3.2163333333333335,
      "grad_norm": 0.14349524676799774,
      "learning_rate": 2.989791666666667e-05,
      "loss": 0.0028,
      "step": 96490
    },
    {
      "epoch": 3.216666666666667,
      "grad_norm": 0.2583010792732239,
      "learning_rate": 2.9895833333333333e-05,
      "loss": 0.0024,
      "step": 96500
    },
    {
      "epoch": 3.217,
      "grad_norm": 0.48570093512535095,
      "learning_rate": 2.9893750000000005e-05,
      "loss": 0.0035,
      "step": 96510
    },
    {
      "epoch": 3.2173333333333334,
      "grad_norm": 0.033689625561237335,
      "learning_rate": 2.9891666666666667e-05,
      "loss": 0.0019,
      "step": 96520
    },
    {
      "epoch": 3.2176666666666667,
      "grad_norm": 0.08662377297878265,
      "learning_rate": 2.9889583333333336e-05,
      "loss": 0.0023,
      "step": 96530
    },
    {
      "epoch": 3.218,
      "grad_norm": 0.05779409408569336,
      "learning_rate": 2.98875e-05,
      "loss": 0.0026,
      "step": 96540
    },
    {
      "epoch": 3.2183333333333333,
      "grad_norm": 0.08757825940847397,
      "learning_rate": 2.9885416666666667e-05,
      "loss": 0.0025,
      "step": 96550
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.20132656395435333,
      "learning_rate": 2.9883333333333336e-05,
      "loss": 0.0028,
      "step": 96560
    },
    {
      "epoch": 3.219,
      "grad_norm": 0.2301342934370041,
      "learning_rate": 2.9881249999999998e-05,
      "loss": 0.0018,
      "step": 96570
    },
    {
      "epoch": 3.219333333333333,
      "grad_norm": 0.200774148106575,
      "learning_rate": 2.987916666666667e-05,
      "loss": 0.0021,
      "step": 96580
    },
    {
      "epoch": 3.219666666666667,
      "grad_norm": 0.34416618943214417,
      "learning_rate": 2.9877083333333332e-05,
      "loss": 0.0022,
      "step": 96590
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.1433108150959015,
      "learning_rate": 2.9875000000000004e-05,
      "loss": 0.002,
      "step": 96600
    },
    {
      "epoch": 3.2203333333333335,
      "grad_norm": 0.31538912653923035,
      "learning_rate": 2.9872916666666667e-05,
      "loss": 0.0018,
      "step": 96610
    },
    {
      "epoch": 3.220666666666667,
      "grad_norm": 0.0583873987197876,
      "learning_rate": 2.9870833333333335e-05,
      "loss": 0.0033,
      "step": 96620
    },
    {
      "epoch": 3.221,
      "grad_norm": 0.22954551875591278,
      "learning_rate": 2.986875e-05,
      "loss": 0.0017,
      "step": 96630
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.3154251277446747,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0019,
      "step": 96640
    },
    {
      "epoch": 3.2216666666666667,
      "grad_norm": 0.17214345932006836,
      "learning_rate": 2.9864583333333335e-05,
      "loss": 0.0019,
      "step": 96650
    },
    {
      "epoch": 3.222,
      "grad_norm": 0.05743946135044098,
      "learning_rate": 2.9862499999999997e-05,
      "loss": 0.0019,
      "step": 96660
    },
    {
      "epoch": 3.2223333333333333,
      "grad_norm": 0.05769062787294388,
      "learning_rate": 2.986041666666667e-05,
      "loss": 0.0022,
      "step": 96670
    },
    {
      "epoch": 3.2226666666666666,
      "grad_norm": 0.22944167256355286,
      "learning_rate": 2.9858333333333332e-05,
      "loss": 0.0034,
      "step": 96680
    },
    {
      "epoch": 3.223,
      "grad_norm": 0.14398780465126038,
      "learning_rate": 2.985625e-05,
      "loss": 0.0023,
      "step": 96690
    },
    {
      "epoch": 3.223333333333333,
      "grad_norm": 0.2580733597278595,
      "learning_rate": 2.9854166666666666e-05,
      "loss": 0.0018,
      "step": 96700
    },
    {
      "epoch": 3.2236666666666665,
      "grad_norm": 0.0862521380186081,
      "learning_rate": 2.9852083333333335e-05,
      "loss": 0.002,
      "step": 96710
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.1434546262025833,
      "learning_rate": 2.985e-05,
      "loss": 0.0011,
      "step": 96720
    },
    {
      "epoch": 3.2243333333333335,
      "grad_norm": 0.3153868615627289,
      "learning_rate": 2.984791666666667e-05,
      "loss": 0.0019,
      "step": 96730
    },
    {
      "epoch": 3.224666666666667,
      "grad_norm": 0.3960762917995453,
      "learning_rate": 2.9845833333333335e-05,
      "loss": 0.0027,
      "step": 96740
    },
    {
      "epoch": 3.225,
      "grad_norm": 0.20065702497959137,
      "learning_rate": 2.9843750000000004e-05,
      "loss": 0.0015,
      "step": 96750
    },
    {
      "epoch": 3.2253333333333334,
      "grad_norm": 0.05738171190023422,
      "learning_rate": 2.984166666666667e-05,
      "loss": 0.0023,
      "step": 96760
    },
    {
      "epoch": 3.2256666666666667,
      "grad_norm": 0.14334428310394287,
      "learning_rate": 2.9839583333333338e-05,
      "loss": 0.0026,
      "step": 96770
    },
    {
      "epoch": 3.226,
      "grad_norm": 0.229641854763031,
      "learning_rate": 2.98375e-05,
      "loss": 0.0031,
      "step": 96780
    },
    {
      "epoch": 3.2263333333333333,
      "grad_norm": 0.20065104961395264,
      "learning_rate": 2.9835416666666666e-05,
      "loss": 0.0019,
      "step": 96790
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.11485853046178818,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0023,
      "step": 96800
    },
    {
      "epoch": 3.227,
      "grad_norm": 0.3443160057067871,
      "learning_rate": 2.983125e-05,
      "loss": 0.0026,
      "step": 96810
    },
    {
      "epoch": 3.227333333333333,
      "grad_norm": 0.08618615567684174,
      "learning_rate": 2.982916666666667e-05,
      "loss": 0.0023,
      "step": 96820
    },
    {
      "epoch": 3.2276666666666665,
      "grad_norm": 0.08600922673940659,
      "learning_rate": 2.9827083333333334e-05,
      "loss": 0.0024,
      "step": 96830
    },
    {
      "epoch": 3.228,
      "grad_norm": 0.09091334789991379,
      "learning_rate": 2.9825000000000003e-05,
      "loss": 0.0013,
      "step": 96840
    },
    {
      "epoch": 3.2283333333333335,
      "grad_norm": 0.11485974490642548,
      "learning_rate": 2.9822916666666665e-05,
      "loss": 0.003,
      "step": 96850
    },
    {
      "epoch": 3.228666666666667,
      "grad_norm": 0.14325854182243347,
      "learning_rate": 2.9820833333333338e-05,
      "loss": 0.0019,
      "step": 96860
    },
    {
      "epoch": 3.229,
      "grad_norm": 0.401399165391922,
      "learning_rate": 2.981875e-05,
      "loss": 0.003,
      "step": 96870
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.1147383525967598,
      "learning_rate": 2.9816666666666672e-05,
      "loss": 0.003,
      "step": 96880
    },
    {
      "epoch": 3.2296666666666667,
      "grad_norm": 0.43019112944602966,
      "learning_rate": 2.9814583333333334e-05,
      "loss": 0.0021,
      "step": 96890
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.1149667426943779,
      "learning_rate": 2.98125e-05,
      "loss": 0.0014,
      "step": 96900
    },
    {
      "epoch": 3.2303333333333333,
      "grad_norm": 0.029245657846331596,
      "learning_rate": 2.981041666666667e-05,
      "loss": 0.0019,
      "step": 96910
    },
    {
      "epoch": 3.2306666666666666,
      "grad_norm": 0.02890949510037899,
      "learning_rate": 2.9808333333333334e-05,
      "loss": 0.0027,
      "step": 96920
    },
    {
      "epoch": 3.231,
      "grad_norm": 0.029667170718312263,
      "learning_rate": 2.9806250000000003e-05,
      "loss": 0.0016,
      "step": 96930
    },
    {
      "epoch": 3.231333333333333,
      "grad_norm": 0.4588528871536255,
      "learning_rate": 2.9804166666666665e-05,
      "loss": 0.0032,
      "step": 96940
    },
    {
      "epoch": 3.2316666666666665,
      "grad_norm": 0.0030033751390874386,
      "learning_rate": 2.9802083333333337e-05,
      "loss": 0.0015,
      "step": 96950
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.12894368171691895,
      "learning_rate": 2.98e-05,
      "loss": 0.0029,
      "step": 96960
    },
    {
      "epoch": 3.2323333333333335,
      "grad_norm": 0.05808517709374428,
      "learning_rate": 2.9797916666666668e-05,
      "loss": 0.0025,
      "step": 96970
    },
    {
      "epoch": 3.232666666666667,
      "grad_norm": 0.4455706775188446,
      "learning_rate": 2.9795833333333334e-05,
      "loss": 0.0026,
      "step": 96980
    },
    {
      "epoch": 3.233,
      "grad_norm": 0.11484020948410034,
      "learning_rate": 2.9793750000000003e-05,
      "loss": 0.003,
      "step": 96990
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.2869211733341217,
      "learning_rate": 2.9791666666666668e-05,
      "loss": 0.0017,
      "step": 97000
    },
    {
      "epoch": 3.2336666666666667,
      "grad_norm": 0.2867492139339447,
      "learning_rate": 2.9789583333333337e-05,
      "loss": 0.0031,
      "step": 97010
    },
    {
      "epoch": 3.234,
      "grad_norm": 0.11477236449718475,
      "learning_rate": 2.9787500000000002e-05,
      "loss": 0.002,
      "step": 97020
    },
    {
      "epoch": 3.2343333333333333,
      "grad_norm": 0.28676265478134155,
      "learning_rate": 2.9785416666666664e-05,
      "loss": 0.0019,
      "step": 97030
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.3154356777667999,
      "learning_rate": 2.9783333333333337e-05,
      "loss": 0.0019,
      "step": 97040
    },
    {
      "epoch": 3.235,
      "grad_norm": 0.05806302651762962,
      "learning_rate": 2.978125e-05,
      "loss": 0.0027,
      "step": 97050
    },
    {
      "epoch": 3.235333333333333,
      "grad_norm": 0.37298819422721863,
      "learning_rate": 2.9779166666666668e-05,
      "loss": 0.0028,
      "step": 97060
    },
    {
      "epoch": 3.2356666666666665,
      "grad_norm": 0.14367623627185822,
      "learning_rate": 2.9777083333333333e-05,
      "loss": 0.0017,
      "step": 97070
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 0.22956335544586182,
      "learning_rate": 2.9775000000000002e-05,
      "loss": 0.0031,
      "step": 97080
    },
    {
      "epoch": 3.2363333333333335,
      "grad_norm": 0.34394824504852295,
      "learning_rate": 2.9772916666666668e-05,
      "loss": 0.0026,
      "step": 97090
    },
    {
      "epoch": 3.236666666666667,
      "grad_norm": 0.17218360304832458,
      "learning_rate": 2.9770833333333336e-05,
      "loss": 0.002,
      "step": 97100
    },
    {
      "epoch": 3.237,
      "grad_norm": 0.17206984758377075,
      "learning_rate": 2.9768750000000002e-05,
      "loss": 0.0028,
      "step": 97110
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.02969835139811039,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0022,
      "step": 97120
    },
    {
      "epoch": 3.2376666666666667,
      "grad_norm": 0.17268286645412445,
      "learning_rate": 2.9764583333333336e-05,
      "loss": 0.0024,
      "step": 97130
    },
    {
      "epoch": 3.238,
      "grad_norm": 0.28701871633529663,
      "learning_rate": 2.97625e-05,
      "loss": 0.0024,
      "step": 97140
    },
    {
      "epoch": 3.2383333333333333,
      "grad_norm": 0.08675303310155869,
      "learning_rate": 2.9760416666666667e-05,
      "loss": 0.0026,
      "step": 97150
    },
    {
      "epoch": 3.2386666666666666,
      "grad_norm": 0.17210599780082703,
      "learning_rate": 2.9758333333333333e-05,
      "loss": 0.0017,
      "step": 97160
    },
    {
      "epoch": 3.239,
      "grad_norm": 0.14349302649497986,
      "learning_rate": 2.975625e-05,
      "loss": 0.0026,
      "step": 97170
    },
    {
      "epoch": 3.239333333333333,
      "grad_norm": 0.5445075631141663,
      "learning_rate": 2.9754166666666667e-05,
      "loss": 0.0017,
      "step": 97180
    },
    {
      "epoch": 3.2396666666666665,
      "grad_norm": 0.5978448390960693,
      "learning_rate": 2.9752083333333336e-05,
      "loss": 0.0033,
      "step": 97190
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.030193427577614784,
      "learning_rate": 2.975e-05,
      "loss": 0.0027,
      "step": 97200
    },
    {
      "epoch": 3.2403333333333335,
      "grad_norm": 0.029207661747932434,
      "learning_rate": 2.974791666666667e-05,
      "loss": 0.0023,
      "step": 97210
    },
    {
      "epoch": 3.240666666666667,
      "grad_norm": 0.12178558856248856,
      "learning_rate": 2.9745833333333332e-05,
      "loss": 0.0023,
      "step": 97220
    },
    {
      "epoch": 3.241,
      "grad_norm": 0.2297646403312683,
      "learning_rate": 2.9743750000000005e-05,
      "loss": 0.0022,
      "step": 97230
    },
    {
      "epoch": 3.2413333333333334,
      "grad_norm": 0.14358985424041748,
      "learning_rate": 2.9741666666666667e-05,
      "loss": 0.003,
      "step": 97240
    },
    {
      "epoch": 3.2416666666666667,
      "grad_norm": 0.372866690158844,
      "learning_rate": 2.9739583333333336e-05,
      "loss": 0.0029,
      "step": 97250
    },
    {
      "epoch": 3.242,
      "grad_norm": 0.14321672916412354,
      "learning_rate": 2.97375e-05,
      "loss": 0.0025,
      "step": 97260
    },
    {
      "epoch": 3.2423333333333333,
      "grad_norm": 0.20072536170482635,
      "learning_rate": 2.9735416666666667e-05,
      "loss": 0.0017,
      "step": 97270
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.05757235363125801,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0025,
      "step": 97280
    },
    {
      "epoch": 3.243,
      "grad_norm": 0.20091916620731354,
      "learning_rate": 2.973125e-05,
      "loss": 0.0023,
      "step": 97290
    },
    {
      "epoch": 3.243333333333333,
      "grad_norm": 0.057637982070446014,
      "learning_rate": 2.972916666666667e-05,
      "loss": 0.0025,
      "step": 97300
    },
    {
      "epoch": 3.2436666666666665,
      "grad_norm": 0.1434619277715683,
      "learning_rate": 2.9727083333333332e-05,
      "loss": 0.0022,
      "step": 97310
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 0.22922827303409576,
      "learning_rate": 2.9725000000000004e-05,
      "loss": 0.0024,
      "step": 97320
    },
    {
      "epoch": 3.2443333333333335,
      "grad_norm": 0.1720784604549408,
      "learning_rate": 2.9722916666666666e-05,
      "loss": 0.0028,
      "step": 97330
    },
    {
      "epoch": 3.244666666666667,
      "grad_norm": 0.14333166182041168,
      "learning_rate": 2.9720833333333335e-05,
      "loss": 0.0022,
      "step": 97340
    },
    {
      "epoch": 3.245,
      "grad_norm": 0.02892133593559265,
      "learning_rate": 2.971875e-05,
      "loss": 0.0021,
      "step": 97350
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.3425496816635132,
      "learning_rate": 2.971666666666667e-05,
      "loss": 0.0036,
      "step": 97360
    },
    {
      "epoch": 3.2456666666666667,
      "grad_norm": 0.057528916746377945,
      "learning_rate": 2.9714583333333335e-05,
      "loss": 0.0028,
      "step": 97370
    },
    {
      "epoch": 3.246,
      "grad_norm": 0.373206228017807,
      "learning_rate": 2.9712499999999997e-05,
      "loss": 0.0025,
      "step": 97380
    },
    {
      "epoch": 3.2463333333333333,
      "grad_norm": 0.48747262358665466,
      "learning_rate": 2.971041666666667e-05,
      "loss": 0.0021,
      "step": 97390
    },
    {
      "epoch": 3.2466666666666666,
      "grad_norm": 0.1159510463476181,
      "learning_rate": 2.970833333333333e-05,
      "loss": 0.0027,
      "step": 97400
    },
    {
      "epoch": 3.247,
      "grad_norm": 0.14363764226436615,
      "learning_rate": 2.9706250000000004e-05,
      "loss": 0.0023,
      "step": 97410
    },
    {
      "epoch": 3.247333333333333,
      "grad_norm": 0.17220593988895416,
      "learning_rate": 2.9704166666666666e-05,
      "loss": 0.0023,
      "step": 97420
    },
    {
      "epoch": 3.2476666666666665,
      "grad_norm": 0.0573197565972805,
      "learning_rate": 2.9702083333333335e-05,
      "loss": 0.0025,
      "step": 97430
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.11517465859651566,
      "learning_rate": 2.97e-05,
      "loss": 0.0023,
      "step": 97440
    },
    {
      "epoch": 3.2483333333333335,
      "grad_norm": 0.08630939573049545,
      "learning_rate": 2.969791666666667e-05,
      "loss": 0.0016,
      "step": 97450
    },
    {
      "epoch": 3.248666666666667,
      "grad_norm": 0.2868787348270416,
      "learning_rate": 2.9695833333333335e-05,
      "loss": 0.0029,
      "step": 97460
    },
    {
      "epoch": 3.249,
      "grad_norm": 0.1718810498714447,
      "learning_rate": 2.9693750000000003e-05,
      "loss": 0.0016,
      "step": 97470
    },
    {
      "epoch": 3.2493333333333334,
      "grad_norm": 0.05759046971797943,
      "learning_rate": 2.969166666666667e-05,
      "loss": 0.0023,
      "step": 97480
    },
    {
      "epoch": 3.2496666666666667,
      "grad_norm": 0.2867670953273773,
      "learning_rate": 2.9689583333333338e-05,
      "loss": 0.0026,
      "step": 97490
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.14348271489143372,
      "learning_rate": 2.96875e-05,
      "loss": 0.0014,
      "step": 97500
    },
    {
      "epoch": 3.2503333333333333,
      "grad_norm": 0.2868844270706177,
      "learning_rate": 2.9685416666666665e-05,
      "loss": 0.0026,
      "step": 97510
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.20066571235656738,
      "learning_rate": 2.9683333333333334e-05,
      "loss": 0.0018,
      "step": 97520
    },
    {
      "epoch": 3.251,
      "grad_norm": 0.2867724895477295,
      "learning_rate": 2.968125e-05,
      "loss": 0.0026,
      "step": 97530
    },
    {
      "epoch": 3.251333333333333,
      "grad_norm": 0.05754522979259491,
      "learning_rate": 2.967916666666667e-05,
      "loss": 0.0024,
      "step": 97540
    },
    {
      "epoch": 3.2516666666666665,
      "grad_norm": 0.02921367436647415,
      "learning_rate": 2.9677083333333334e-05,
      "loss": 0.0019,
      "step": 97550
    },
    {
      "epoch": 3.252,
      "grad_norm": 0.23054251074790955,
      "learning_rate": 2.9675000000000003e-05,
      "loss": 0.0019,
      "step": 97560
    },
    {
      "epoch": 3.2523333333333335,
      "grad_norm": 0.22940808534622192,
      "learning_rate": 2.967291666666667e-05,
      "loss": 0.0027,
      "step": 97570
    },
    {
      "epoch": 3.252666666666667,
      "grad_norm": 0.05757586285471916,
      "learning_rate": 2.9670833333333337e-05,
      "loss": 0.0018,
      "step": 97580
    },
    {
      "epoch": 3.253,
      "grad_norm": 0.37286216020584106,
      "learning_rate": 2.966875e-05,
      "loss": 0.002,
      "step": 97590
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.1436118632555008,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0022,
      "step": 97600
    },
    {
      "epoch": 3.2536666666666667,
      "grad_norm": 0.20068976283073425,
      "learning_rate": 2.9664583333333334e-05,
      "loss": 0.0022,
      "step": 97610
    },
    {
      "epoch": 3.254,
      "grad_norm": 0.1722821146249771,
      "learning_rate": 2.9662500000000003e-05,
      "loss": 0.0016,
      "step": 97620
    },
    {
      "epoch": 3.2543333333333333,
      "grad_norm": 0.14376485347747803,
      "learning_rate": 2.9660416666666668e-05,
      "loss": 0.002,
      "step": 97630
    },
    {
      "epoch": 3.2546666666666666,
      "grad_norm": 0.11491133272647858,
      "learning_rate": 2.9658333333333334e-05,
      "loss": 0.0021,
      "step": 97640
    },
    {
      "epoch": 3.255,
      "grad_norm": 0.11477391421794891,
      "learning_rate": 2.9656250000000003e-05,
      "loss": 0.002,
      "step": 97650
    },
    {
      "epoch": 3.255333333333333,
      "grad_norm": 0.05792825296521187,
      "learning_rate": 2.9654166666666665e-05,
      "loss": 0.0026,
      "step": 97660
    },
    {
      "epoch": 3.2556666666666665,
      "grad_norm": 0.565081000328064,
      "learning_rate": 2.9652083333333337e-05,
      "loss": 0.0018,
      "step": 97670
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.14394491910934448,
      "learning_rate": 2.965e-05,
      "loss": 0.0018,
      "step": 97680
    },
    {
      "epoch": 3.2563333333333335,
      "grad_norm": 0.2006867527961731,
      "learning_rate": 2.964791666666667e-05,
      "loss": 0.0028,
      "step": 97690
    },
    {
      "epoch": 3.256666666666667,
      "grad_norm": 0.3157605528831482,
      "learning_rate": 2.9645833333333333e-05,
      "loss": 0.0024,
      "step": 97700
    },
    {
      "epoch": 3.257,
      "grad_norm": 0.0575997531414032,
      "learning_rate": 2.9643750000000002e-05,
      "loss": 0.0027,
      "step": 97710
    },
    {
      "epoch": 3.2573333333333334,
      "grad_norm": 0.0289427749812603,
      "learning_rate": 2.9641666666666668e-05,
      "loss": 0.0016,
      "step": 97720
    },
    {
      "epoch": 3.2576666666666667,
      "grad_norm": 0.05875321477651596,
      "learning_rate": 2.9639583333333337e-05,
      "loss": 0.0019,
      "step": 97730
    },
    {
      "epoch": 3.258,
      "grad_norm": 0.0870351642370224,
      "learning_rate": 2.9637500000000002e-05,
      "loss": 0.0017,
      "step": 97740
    },
    {
      "epoch": 3.2583333333333333,
      "grad_norm": 0.0865650326013565,
      "learning_rate": 2.9635416666666664e-05,
      "loss": 0.0026,
      "step": 97750
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.31561559438705444,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0024,
      "step": 97760
    },
    {
      "epoch": 3.259,
      "grad_norm": 0.34394383430480957,
      "learning_rate": 2.963125e-05,
      "loss": 0.0022,
      "step": 97770
    },
    {
      "epoch": 3.259333333333333,
      "grad_norm": 0.48769572377204895,
      "learning_rate": 2.9629166666666667e-05,
      "loss": 0.0013,
      "step": 97780
    },
    {
      "epoch": 3.2596666666666665,
      "grad_norm": 0.45861145853996277,
      "learning_rate": 2.9627083333333333e-05,
      "loss": 0.0029,
      "step": 97790
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.20683181285858154,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 0.0017,
      "step": 97800
    },
    {
      "epoch": 3.2603333333333335,
      "grad_norm": 0.17219053208827972,
      "learning_rate": 2.9622916666666667e-05,
      "loss": 0.0018,
      "step": 97810
    },
    {
      "epoch": 3.260666666666667,
      "grad_norm": 0.029285853728652,
      "learning_rate": 2.9620833333333336e-05,
      "loss": 0.0021,
      "step": 97820
    },
    {
      "epoch": 3.261,
      "grad_norm": 0.0295394416898489,
      "learning_rate": 2.961875e-05,
      "loss": 0.0026,
      "step": 97830
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.034426815807819366,
      "learning_rate": 2.961666666666667e-05,
      "loss": 0.0033,
      "step": 97840
    },
    {
      "epoch": 3.2616666666666667,
      "grad_norm": 0.01328770536929369,
      "learning_rate": 2.9614583333333336e-05,
      "loss": 0.0024,
      "step": 97850
    },
    {
      "epoch": 3.262,
      "grad_norm": 0.31532490253448486,
      "learning_rate": 2.9612500000000005e-05,
      "loss": 0.0023,
      "step": 97860
    },
    {
      "epoch": 3.2623333333333333,
      "grad_norm": 0.007659886498004198,
      "learning_rate": 2.9610416666666667e-05,
      "loss": 0.0026,
      "step": 97870
    },
    {
      "epoch": 3.2626666666666666,
      "grad_norm": 0.02934456244111061,
      "learning_rate": 2.9608333333333332e-05,
      "loss": 0.0024,
      "step": 97880
    },
    {
      "epoch": 3.263,
      "grad_norm": 0.5591272711753845,
      "learning_rate": 2.960625e-05,
      "loss": 0.0018,
      "step": 97890
    },
    {
      "epoch": 3.263333333333333,
      "grad_norm": 0.05727889761328697,
      "learning_rate": 2.9604166666666667e-05,
      "loss": 0.0016,
      "step": 97900
    },
    {
      "epoch": 3.2636666666666665,
      "grad_norm": 0.05764942243695259,
      "learning_rate": 2.9602083333333336e-05,
      "loss": 0.0024,
      "step": 97910
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.25805535912513733,
      "learning_rate": 2.96e-05,
      "loss": 0.0019,
      "step": 97920
    },
    {
      "epoch": 3.264333333333333,
      "grad_norm": 0.315483421087265,
      "learning_rate": 2.959791666666667e-05,
      "loss": 0.002,
      "step": 97930
    },
    {
      "epoch": 3.264666666666667,
      "grad_norm": 0.030738038942217827,
      "learning_rate": 2.9595833333333332e-05,
      "loss": 0.0017,
      "step": 97940
    },
    {
      "epoch": 3.265,
      "grad_norm": 0.004503923002630472,
      "learning_rate": 2.9593750000000004e-05,
      "loss": 0.0023,
      "step": 97950
    },
    {
      "epoch": 3.2653333333333334,
      "grad_norm": 0.0586242638528347,
      "learning_rate": 2.9591666666666667e-05,
      "loss": 0.0018,
      "step": 97960
    },
    {
      "epoch": 3.2656666666666667,
      "grad_norm": 0.028835706412792206,
      "learning_rate": 2.958958333333334e-05,
      "loss": 0.0018,
      "step": 97970
    },
    {
      "epoch": 3.266,
      "grad_norm": 0.11449471861124039,
      "learning_rate": 2.95875e-05,
      "loss": 0.002,
      "step": 97980
    },
    {
      "epoch": 3.2663333333333333,
      "grad_norm": 0.057975880801677704,
      "learning_rate": 2.9585416666666666e-05,
      "loss": 0.0017,
      "step": 97990
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.22954441606998444,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.0021,
      "step": 98000
    },
    {
      "epoch": 3.267,
      "grad_norm": 0.2577977776527405,
      "learning_rate": 2.958125e-05,
      "loss": 0.0019,
      "step": 98010
    },
    {
      "epoch": 3.267333333333333,
      "grad_norm": 0.08659998327493668,
      "learning_rate": 2.957916666666667e-05,
      "loss": 0.0019,
      "step": 98020
    },
    {
      "epoch": 3.2676666666666665,
      "grad_norm": 0.14339883625507355,
      "learning_rate": 2.9577083333333332e-05,
      "loss": 0.0014,
      "step": 98030
    },
    {
      "epoch": 3.268,
      "grad_norm": 0.25789889693260193,
      "learning_rate": 2.9575000000000004e-05,
      "loss": 0.003,
      "step": 98040
    },
    {
      "epoch": 3.2683333333333335,
      "grad_norm": 0.11534951627254486,
      "learning_rate": 2.9572916666666666e-05,
      "loss": 0.0019,
      "step": 98050
    },
    {
      "epoch": 3.268666666666667,
      "grad_norm": 0.14350956678390503,
      "learning_rate": 2.9570833333333335e-05,
      "loss": 0.0026,
      "step": 98060
    },
    {
      "epoch": 3.269,
      "grad_norm": 0.17250171303749084,
      "learning_rate": 2.956875e-05,
      "loss": 0.0018,
      "step": 98070
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.20056389272212982,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0026,
      "step": 98080
    },
    {
      "epoch": 3.2696666666666667,
      "grad_norm": 0.2294263243675232,
      "learning_rate": 2.9564583333333335e-05,
      "loss": 0.0028,
      "step": 98090
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.14351046085357666,
      "learning_rate": 2.9562500000000004e-05,
      "loss": 0.0014,
      "step": 98100
    },
    {
      "epoch": 3.2703333333333333,
      "grad_norm": 0.45883649587631226,
      "learning_rate": 2.956041666666667e-05,
      "loss": 0.0019,
      "step": 98110
    },
    {
      "epoch": 3.2706666666666666,
      "grad_norm": 0.08646071702241898,
      "learning_rate": 2.955833333333333e-05,
      "loss": 0.0014,
      "step": 98120
    },
    {
      "epoch": 3.271,
      "grad_norm": 0.3154379725456238,
      "learning_rate": 2.9556250000000004e-05,
      "loss": 0.0035,
      "step": 98130
    },
    {
      "epoch": 3.271333333333333,
      "grad_norm": 0.03062002919614315,
      "learning_rate": 2.9554166666666666e-05,
      "loss": 0.0018,
      "step": 98140
    },
    {
      "epoch": 3.2716666666666665,
      "grad_norm": 0.4014700949192047,
      "learning_rate": 2.9552083333333334e-05,
      "loss": 0.0023,
      "step": 98150
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.48743534088134766,
      "learning_rate": 2.955e-05,
      "loss": 0.0023,
      "step": 98160
    },
    {
      "epoch": 3.272333333333333,
      "grad_norm": 0.11521468311548233,
      "learning_rate": 2.954791666666667e-05,
      "loss": 0.0024,
      "step": 98170
    },
    {
      "epoch": 3.272666666666667,
      "grad_norm": 0.004993004258722067,
      "learning_rate": 2.9545833333333334e-05,
      "loss": 0.0024,
      "step": 98180
    },
    {
      "epoch": 3.273,
      "grad_norm": 0.37278151512145996,
      "learning_rate": 2.9543750000000003e-05,
      "loss": 0.0021,
      "step": 98190
    },
    {
      "epoch": 3.2733333333333334,
      "grad_norm": 0.057712264358997345,
      "learning_rate": 2.954166666666667e-05,
      "loss": 0.002,
      "step": 98200
    },
    {
      "epoch": 3.2736666666666667,
      "grad_norm": 0.432561993598938,
      "learning_rate": 2.9539583333333338e-05,
      "loss": 0.0013,
      "step": 98210
    },
    {
      "epoch": 3.274,
      "grad_norm": 0.22957579791545868,
      "learning_rate": 2.95375e-05,
      "loss": 0.0016,
      "step": 98220
    },
    {
      "epoch": 3.2743333333333333,
      "grad_norm": 0.05741144344210625,
      "learning_rate": 2.9535416666666665e-05,
      "loss": 0.0018,
      "step": 98230
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.3441622853279114,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0014,
      "step": 98240
    },
    {
      "epoch": 3.275,
      "grad_norm": 0.14364580810070038,
      "learning_rate": 2.953125e-05,
      "loss": 0.0021,
      "step": 98250
    },
    {
      "epoch": 3.275333333333333,
      "grad_norm": 0.08604662865400314,
      "learning_rate": 2.952916666666667e-05,
      "loss": 0.0017,
      "step": 98260
    },
    {
      "epoch": 3.2756666666666665,
      "grad_norm": 0.28673243522644043,
      "learning_rate": 2.9527083333333334e-05,
      "loss": 0.0021,
      "step": 98270
    },
    {
      "epoch": 3.276,
      "grad_norm": 0.17198927700519562,
      "learning_rate": 2.9525000000000003e-05,
      "loss": 0.0021,
      "step": 98280
    },
    {
      "epoch": 3.2763333333333335,
      "grad_norm": 0.1719307154417038,
      "learning_rate": 2.9522916666666668e-05,
      "loss": 0.0018,
      "step": 98290
    },
    {
      "epoch": 3.276666666666667,
      "grad_norm": 0.343923956155777,
      "learning_rate": 2.9520833333333337e-05,
      "loss": 0.0021,
      "step": 98300
    },
    {
      "epoch": 3.277,
      "grad_norm": 0.05751290172338486,
      "learning_rate": 2.951875e-05,
      "loss": 0.0021,
      "step": 98310
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.4014643430709839,
      "learning_rate": 2.951666666666667e-05,
      "loss": 0.0016,
      "step": 98320
    },
    {
      "epoch": 3.2776666666666667,
      "grad_norm": 0.14341312646865845,
      "learning_rate": 2.9514583333333334e-05,
      "loss": 0.0024,
      "step": 98330
    },
    {
      "epoch": 3.278,
      "grad_norm": 0.315363347530365,
      "learning_rate": 2.9512500000000002e-05,
      "loss": 0.0013,
      "step": 98340
    },
    {
      "epoch": 3.2783333333333333,
      "grad_norm": 0.2007761299610138,
      "learning_rate": 2.9510416666666668e-05,
      "loss": 0.0031,
      "step": 98350
    },
    {
      "epoch": 3.2786666666666666,
      "grad_norm": 0.2865186631679535,
      "learning_rate": 2.9508333333333333e-05,
      "loss": 0.0014,
      "step": 98360
    },
    {
      "epoch": 3.279,
      "grad_norm": 0.14360986649990082,
      "learning_rate": 2.9506250000000002e-05,
      "loss": 0.0028,
      "step": 98370
    },
    {
      "epoch": 3.279333333333333,
      "grad_norm": 0.20091763138771057,
      "learning_rate": 2.9504166666666664e-05,
      "loss": 0.0024,
      "step": 98380
    },
    {
      "epoch": 3.2796666666666665,
      "grad_norm": 0.3155590891838074,
      "learning_rate": 2.9502083333333337e-05,
      "loss": 0.0021,
      "step": 98390
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.25829973816871643,
      "learning_rate": 2.95e-05,
      "loss": 0.0023,
      "step": 98400
    },
    {
      "epoch": 3.280333333333333,
      "grad_norm": 0.5169493556022644,
      "learning_rate": 2.949791666666667e-05,
      "loss": 0.0034,
      "step": 98410
    },
    {
      "epoch": 3.280666666666667,
      "grad_norm": 0.11483225971460342,
      "learning_rate": 2.9495833333333333e-05,
      "loss": 0.0023,
      "step": 98420
    },
    {
      "epoch": 3.281,
      "grad_norm": 0.31514090299606323,
      "learning_rate": 2.9493750000000002e-05,
      "loss": 0.0017,
      "step": 98430
    },
    {
      "epoch": 3.2813333333333334,
      "grad_norm": 0.1433749496936798,
      "learning_rate": 2.9491666666666667e-05,
      "loss": 0.0016,
      "step": 98440
    },
    {
      "epoch": 3.2816666666666667,
      "grad_norm": 0.1721673160791397,
      "learning_rate": 2.9489583333333336e-05,
      "loss": 0.0023,
      "step": 98450
    },
    {
      "epoch": 3.282,
      "grad_norm": 0.3498471975326538,
      "learning_rate": 2.9487500000000002e-05,
      "loss": 0.0023,
      "step": 98460
    },
    {
      "epoch": 3.2823333333333333,
      "grad_norm": 0.05823906883597374,
      "learning_rate": 2.9485416666666664e-05,
      "loss": 0.0021,
      "step": 98470
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.08631081879138947,
      "learning_rate": 2.9483333333333336e-05,
      "loss": 0.0034,
      "step": 98480
    },
    {
      "epoch": 3.283,
      "grad_norm": 0.11478392779827118,
      "learning_rate": 2.948125e-05,
      "loss": 0.0014,
      "step": 98490
    },
    {
      "epoch": 3.283333333333333,
      "grad_norm": 0.17233389616012573,
      "learning_rate": 2.9479166666666667e-05,
      "loss": 0.0015,
      "step": 98500
    },
    {
      "epoch": 3.2836666666666665,
      "grad_norm": 0.25808480381965637,
      "learning_rate": 2.9477083333333333e-05,
      "loss": 0.002,
      "step": 98510
    },
    {
      "epoch": 3.284,
      "grad_norm": 0.057505812495946884,
      "learning_rate": 2.9475e-05,
      "loss": 0.0011,
      "step": 98520
    },
    {
      "epoch": 3.2843333333333335,
      "grad_norm": 0.17188777029514313,
      "learning_rate": 2.9472916666666667e-05,
      "loss": 0.002,
      "step": 98530
    },
    {
      "epoch": 3.284666666666667,
      "grad_norm": 0.4584869146347046,
      "learning_rate": 2.9470833333333336e-05,
      "loss": 0.002,
      "step": 98540
    },
    {
      "epoch": 3.285,
      "grad_norm": 0.22944743931293488,
      "learning_rate": 2.946875e-05,
      "loss": 0.0029,
      "step": 98550
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.23001983761787415,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0012,
      "step": 98560
    },
    {
      "epoch": 3.2856666666666667,
      "grad_norm": 0.20077423751354218,
      "learning_rate": 2.9464583333333336e-05,
      "loss": 0.0028,
      "step": 98570
    },
    {
      "epoch": 3.286,
      "grad_norm": 0.11475098133087158,
      "learning_rate": 2.9462500000000005e-05,
      "loss": 0.0027,
      "step": 98580
    },
    {
      "epoch": 3.2863333333333333,
      "grad_norm": 0.20100311934947968,
      "learning_rate": 2.9460416666666667e-05,
      "loss": 0.0029,
      "step": 98590
    },
    {
      "epoch": 3.2866666666666666,
      "grad_norm": 0.2580197751522064,
      "learning_rate": 2.9458333333333332e-05,
      "loss": 0.0015,
      "step": 98600
    },
    {
      "epoch": 3.287,
      "grad_norm": 0.14408057928085327,
      "learning_rate": 2.945625e-05,
      "loss": 0.0024,
      "step": 98610
    },
    {
      "epoch": 3.287333333333333,
      "grad_norm": 0.25793173909187317,
      "learning_rate": 2.9454166666666667e-05,
      "loss": 0.002,
      "step": 98620
    },
    {
      "epoch": 3.2876666666666665,
      "grad_norm": 0.08620762825012207,
      "learning_rate": 2.9452083333333335e-05,
      "loss": 0.0019,
      "step": 98630
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.057568807154893875,
      "learning_rate": 2.945e-05,
      "loss": 0.002,
      "step": 98640
    },
    {
      "epoch": 3.288333333333333,
      "grad_norm": 0.22940297424793243,
      "learning_rate": 2.944791666666667e-05,
      "loss": 0.0021,
      "step": 98650
    },
    {
      "epoch": 3.288666666666667,
      "grad_norm": 0.028936879709362984,
      "learning_rate": 2.9445833333333332e-05,
      "loss": 0.0023,
      "step": 98660
    },
    {
      "epoch": 3.289,
      "grad_norm": 0.8484060168266296,
      "learning_rate": 2.9443750000000004e-05,
      "loss": 0.0027,
      "step": 98670
    },
    {
      "epoch": 3.2893333333333334,
      "grad_norm": 0.007928562350571156,
      "learning_rate": 2.9441666666666666e-05,
      "loss": 0.0022,
      "step": 98680
    },
    {
      "epoch": 3.2896666666666667,
      "grad_norm": 0.17196263372898102,
      "learning_rate": 2.943958333333334e-05,
      "loss": 0.0018,
      "step": 98690
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.14334727823734283,
      "learning_rate": 2.94375e-05,
      "loss": 0.002,
      "step": 98700
    },
    {
      "epoch": 3.2903333333333333,
      "grad_norm": 0.14350250363349915,
      "learning_rate": 2.9435416666666666e-05,
      "loss": 0.0024,
      "step": 98710
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.2296430915594101,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0019,
      "step": 98720
    },
    {
      "epoch": 3.291,
      "grad_norm": 0.029525963589549065,
      "learning_rate": 2.943125e-05,
      "loss": 0.002,
      "step": 98730
    },
    {
      "epoch": 3.291333333333333,
      "grad_norm": 0.2525857090950012,
      "learning_rate": 2.942916666666667e-05,
      "loss": 0.0045,
      "step": 98740
    },
    {
      "epoch": 3.2916666666666665,
      "grad_norm": 0.02880449779331684,
      "learning_rate": 2.942708333333333e-05,
      "loss": 0.0021,
      "step": 98750
    },
    {
      "epoch": 3.292,
      "grad_norm": 0.2002391517162323,
      "learning_rate": 2.9425000000000004e-05,
      "loss": 0.0019,
      "step": 98760
    },
    {
      "epoch": 3.2923333333333336,
      "grad_norm": 0.14371298253536224,
      "learning_rate": 2.9422916666666666e-05,
      "loss": 0.0018,
      "step": 98770
    },
    {
      "epoch": 3.292666666666667,
      "grad_norm": 0.08586638420820236,
      "learning_rate": 2.9420833333333335e-05,
      "loss": 0.0022,
      "step": 98780
    },
    {
      "epoch": 3.293,
      "grad_norm": 0.010750895366072655,
      "learning_rate": 2.941875e-05,
      "loss": 0.0015,
      "step": 98790
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.28705742955207825,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.0027,
      "step": 98800
    },
    {
      "epoch": 3.2936666666666667,
      "grad_norm": 0.11502888798713684,
      "learning_rate": 2.9414583333333335e-05,
      "loss": 0.0016,
      "step": 98810
    },
    {
      "epoch": 3.294,
      "grad_norm": 0.0863647386431694,
      "learning_rate": 2.9412500000000003e-05,
      "loss": 0.0023,
      "step": 98820
    },
    {
      "epoch": 3.2943333333333333,
      "grad_norm": 0.2293359637260437,
      "learning_rate": 2.941041666666667e-05,
      "loss": 0.0028,
      "step": 98830
    },
    {
      "epoch": 3.2946666666666666,
      "grad_norm": 0.11571390926837921,
      "learning_rate": 2.940833333333333e-05,
      "loss": 0.0016,
      "step": 98840
    },
    {
      "epoch": 3.295,
      "grad_norm": 0.4516257643699646,
      "learning_rate": 2.9406250000000003e-05,
      "loss": 0.0024,
      "step": 98850
    },
    {
      "epoch": 3.2953333333333332,
      "grad_norm": 0.2291080206632614,
      "learning_rate": 2.9404166666666665e-05,
      "loss": 0.0035,
      "step": 98860
    },
    {
      "epoch": 3.2956666666666665,
      "grad_norm": 0.40116941928863525,
      "learning_rate": 2.9402083333333334e-05,
      "loss": 0.002,
      "step": 98870
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.5930219292640686,
      "learning_rate": 2.94e-05,
      "loss": 0.0015,
      "step": 98880
    },
    {
      "epoch": 3.296333333333333,
      "grad_norm": 0.22945809364318848,
      "learning_rate": 2.939791666666667e-05,
      "loss": 0.0029,
      "step": 98890
    },
    {
      "epoch": 3.296666666666667,
      "grad_norm": 0.0038361481856554747,
      "learning_rate": 2.9395833333333334e-05,
      "loss": 0.003,
      "step": 98900
    },
    {
      "epoch": 3.297,
      "grad_norm": 0.3440624177455902,
      "learning_rate": 2.9393750000000003e-05,
      "loss": 0.0019,
      "step": 98910
    },
    {
      "epoch": 3.2973333333333334,
      "grad_norm": 0.02971918135881424,
      "learning_rate": 2.939166666666667e-05,
      "loss": 0.0027,
      "step": 98920
    },
    {
      "epoch": 3.2976666666666667,
      "grad_norm": 0.11750777065753937,
      "learning_rate": 2.9389583333333337e-05,
      "loss": 0.0023,
      "step": 98930
    },
    {
      "epoch": 3.298,
      "grad_norm": 0.015448824502527714,
      "learning_rate": 2.9387500000000003e-05,
      "loss": 0.0033,
      "step": 98940
    },
    {
      "epoch": 3.2983333333333333,
      "grad_norm": 0.05737947300076485,
      "learning_rate": 2.938541666666667e-05,
      "loss": 0.002,
      "step": 98950
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.2007557451725006,
      "learning_rate": 2.9383333333333334e-05,
      "loss": 0.0017,
      "step": 98960
    },
    {
      "epoch": 3.299,
      "grad_norm": 0.45863139629364014,
      "learning_rate": 2.938125e-05,
      "loss": 0.0026,
      "step": 98970
    },
    {
      "epoch": 3.2993333333333332,
      "grad_norm": 0.6017336845397949,
      "learning_rate": 2.9379166666666668e-05,
      "loss": 0.0023,
      "step": 98980
    },
    {
      "epoch": 3.2996666666666665,
      "grad_norm": 0.25783246755599976,
      "learning_rate": 2.9377083333333334e-05,
      "loss": 0.0024,
      "step": 98990
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.2866479456424713,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 0.0015,
      "step": 99000
    },
    {
      "epoch": 3.3003333333333336,
      "grad_norm": 0.030819121748209,
      "learning_rate": 2.9372916666666668e-05,
      "loss": 0.0018,
      "step": 99010
    },
    {
      "epoch": 3.300666666666667,
      "grad_norm": 0.25801342725753784,
      "learning_rate": 2.9370833333333337e-05,
      "loss": 0.0011,
      "step": 99020
    },
    {
      "epoch": 3.301,
      "grad_norm": 0.02898411825299263,
      "learning_rate": 2.936875e-05,
      "loss": 0.0024,
      "step": 99030
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.31522080302238464,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.0033,
      "step": 99040
    },
    {
      "epoch": 3.3016666666666667,
      "grad_norm": 0.004879586864262819,
      "learning_rate": 2.9364583333333333e-05,
      "loss": 0.0028,
      "step": 99050
    },
    {
      "epoch": 3.302,
      "grad_norm": 0.02889508381485939,
      "learning_rate": 2.9362500000000002e-05,
      "loss": 0.0017,
      "step": 99060
    },
    {
      "epoch": 3.3023333333333333,
      "grad_norm": 0.17212653160095215,
      "learning_rate": 2.9360416666666668e-05,
      "loss": 0.0016,
      "step": 99070
    },
    {
      "epoch": 3.3026666666666666,
      "grad_norm": 0.14354294538497925,
      "learning_rate": 2.9358333333333333e-05,
      "loss": 0.0018,
      "step": 99080
    },
    {
      "epoch": 3.303,
      "grad_norm": 0.057609591633081436,
      "learning_rate": 2.9356250000000002e-05,
      "loss": 0.0016,
      "step": 99090
    },
    {
      "epoch": 3.3033333333333332,
      "grad_norm": 0.029372019693255424,
      "learning_rate": 2.9354166666666668e-05,
      "loss": 0.0023,
      "step": 99100
    },
    {
      "epoch": 3.3036666666666665,
      "grad_norm": 0.20073601603507996,
      "learning_rate": 2.9352083333333336e-05,
      "loss": 0.002,
      "step": 99110
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.05752575024962425,
      "learning_rate": 2.935e-05,
      "loss": 0.0024,
      "step": 99120
    },
    {
      "epoch": 3.304333333333333,
      "grad_norm": 0.08610924333333969,
      "learning_rate": 2.934791666666667e-05,
      "loss": 0.0017,
      "step": 99130
    },
    {
      "epoch": 3.304666666666667,
      "grad_norm": 0.4872714877128601,
      "learning_rate": 2.9345833333333333e-05,
      "loss": 0.003,
      "step": 99140
    },
    {
      "epoch": 3.305,
      "grad_norm": 0.4873778522014618,
      "learning_rate": 2.9343750000000002e-05,
      "loss": 0.0018,
      "step": 99150
    },
    {
      "epoch": 3.3053333333333335,
      "grad_norm": 0.029452906921505928,
      "learning_rate": 2.9341666666666667e-05,
      "loss": 0.0017,
      "step": 99160
    },
    {
      "epoch": 3.3056666666666668,
      "grad_norm": 0.17187215387821198,
      "learning_rate": 2.9339583333333336e-05,
      "loss": 0.0025,
      "step": 99170
    },
    {
      "epoch": 3.306,
      "grad_norm": 0.17251116037368774,
      "learning_rate": 2.93375e-05,
      "loss": 0.0026,
      "step": 99180
    },
    {
      "epoch": 3.3063333333333333,
      "grad_norm": 0.11489411443471909,
      "learning_rate": 2.933541666666667e-05,
      "loss": 0.0027,
      "step": 99190
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.40112897753715515,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0025,
      "step": 99200
    },
    {
      "epoch": 3.307,
      "grad_norm": 0.45843034982681274,
      "learning_rate": 2.9331249999999998e-05,
      "loss": 0.0019,
      "step": 99210
    },
    {
      "epoch": 3.3073333333333332,
      "grad_norm": 0.013076827861368656,
      "learning_rate": 2.932916666666667e-05,
      "loss": 0.0028,
      "step": 99220
    },
    {
      "epoch": 3.3076666666666665,
      "grad_norm": 0.057789504528045654,
      "learning_rate": 2.9327083333333332e-05,
      "loss": 0.0024,
      "step": 99230
    },
    {
      "epoch": 3.308,
      "grad_norm": 0.483310341835022,
      "learning_rate": 2.9325e-05,
      "loss": 0.0018,
      "step": 99240
    },
    {
      "epoch": 3.3083333333333336,
      "grad_norm": 0.013418538495898247,
      "learning_rate": 2.9322916666666667e-05,
      "loss": 0.0021,
      "step": 99250
    },
    {
      "epoch": 3.3086666666666664,
      "grad_norm": 0.004410120192915201,
      "learning_rate": 2.9320833333333336e-05,
      "loss": 0.0019,
      "step": 99260
    },
    {
      "epoch": 3.309,
      "grad_norm": 0.31570175290107727,
      "learning_rate": 2.931875e-05,
      "loss": 0.0021,
      "step": 99270
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.4663774371147156,
      "learning_rate": 2.931666666666667e-05,
      "loss": 0.0026,
      "step": 99280
    },
    {
      "epoch": 3.3096666666666668,
      "grad_norm": 0.25782448053359985,
      "learning_rate": 2.9314583333333336e-05,
      "loss": 0.0026,
      "step": 99290
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.3436155915260315,
      "learning_rate": 2.9312500000000004e-05,
      "loss": 0.0021,
      "step": 99300
    },
    {
      "epoch": 3.3103333333333333,
      "grad_norm": 0.31548550724983215,
      "learning_rate": 2.9310416666666666e-05,
      "loss": 0.0021,
      "step": 99310
    },
    {
      "epoch": 3.3106666666666666,
      "grad_norm": 0.11528444290161133,
      "learning_rate": 2.9308333333333332e-05,
      "loss": 0.0022,
      "step": 99320
    },
    {
      "epoch": 3.311,
      "grad_norm": 0.029179172590374947,
      "learning_rate": 2.930625e-05,
      "loss": 0.0017,
      "step": 99330
    },
    {
      "epoch": 3.3113333333333332,
      "grad_norm": 0.00718457018956542,
      "learning_rate": 2.9304166666666666e-05,
      "loss": 0.0027,
      "step": 99340
    },
    {
      "epoch": 3.3116666666666665,
      "grad_norm": 0.42999136447906494,
      "learning_rate": 2.9302083333333335e-05,
      "loss": 0.0023,
      "step": 99350
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.3726840615272522,
      "learning_rate": 2.93e-05,
      "loss": 0.0027,
      "step": 99360
    },
    {
      "epoch": 3.312333333333333,
      "grad_norm": 0.22934071719646454,
      "learning_rate": 2.929791666666667e-05,
      "loss": 0.0012,
      "step": 99370
    },
    {
      "epoch": 3.312666666666667,
      "grad_norm": 0.31521332263946533,
      "learning_rate": 2.9295833333333335e-05,
      "loss": 0.002,
      "step": 99380
    },
    {
      "epoch": 3.313,
      "grad_norm": 0.08599164336919785,
      "learning_rate": 2.9293750000000004e-05,
      "loss": 0.002,
      "step": 99390
    },
    {
      "epoch": 3.3133333333333335,
      "grad_norm": 0.004721696488559246,
      "learning_rate": 2.9291666666666666e-05,
      "loss": 0.0022,
      "step": 99400
    },
    {
      "epoch": 3.3136666666666668,
      "grad_norm": 0.2864406704902649,
      "learning_rate": 2.9289583333333338e-05,
      "loss": 0.0023,
      "step": 99410
    },
    {
      "epoch": 3.314,
      "grad_norm": 0.835666835308075,
      "learning_rate": 2.92875e-05,
      "loss": 0.0018,
      "step": 99420
    },
    {
      "epoch": 3.3143333333333334,
      "grad_norm": 0.28673291206359863,
      "learning_rate": 2.928541666666667e-05,
      "loss": 0.0017,
      "step": 99430
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.14340969920158386,
      "learning_rate": 2.9283333333333335e-05,
      "loss": 0.0027,
      "step": 99440
    },
    {
      "epoch": 3.315,
      "grad_norm": 0.1720208078622818,
      "learning_rate": 2.928125e-05,
      "loss": 0.0021,
      "step": 99450
    },
    {
      "epoch": 3.3153333333333332,
      "grad_norm": 0.08598219603300095,
      "learning_rate": 2.927916666666667e-05,
      "loss": 0.003,
      "step": 99460
    },
    {
      "epoch": 3.3156666666666665,
      "grad_norm": 0.11487991362810135,
      "learning_rate": 2.927708333333333e-05,
      "loss": 0.0041,
      "step": 99470
    },
    {
      "epoch": 3.316,
      "grad_norm": 0.22927898168563843,
      "learning_rate": 2.9275000000000003e-05,
      "loss": 0.0029,
      "step": 99480
    },
    {
      "epoch": 3.3163333333333336,
      "grad_norm": 0.0860920250415802,
      "learning_rate": 2.9272916666666666e-05,
      "loss": 0.0017,
      "step": 99490
    },
    {
      "epoch": 3.3166666666666664,
      "grad_norm": 0.37274569272994995,
      "learning_rate": 2.9270833333333338e-05,
      "loss": 0.0024,
      "step": 99500
    },
    {
      "epoch": 3.317,
      "grad_norm": 0.14335991442203522,
      "learning_rate": 2.926875e-05,
      "loss": 0.0016,
      "step": 99510
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.37245237827301025,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.002,
      "step": 99520
    },
    {
      "epoch": 3.3176666666666668,
      "grad_norm": 0.22947609424591064,
      "learning_rate": 2.9264583333333334e-05,
      "loss": 0.0025,
      "step": 99530
    },
    {
      "epoch": 3.318,
      "grad_norm": 0.11467685550451279,
      "learning_rate": 2.9262500000000003e-05,
      "loss": 0.0023,
      "step": 99540
    },
    {
      "epoch": 3.3183333333333334,
      "grad_norm": 0.2579261362552643,
      "learning_rate": 2.926041666666667e-05,
      "loss": 0.0018,
      "step": 99550
    },
    {
      "epoch": 3.3186666666666667,
      "grad_norm": 0.2579489052295685,
      "learning_rate": 2.925833333333333e-05,
      "loss": 0.0018,
      "step": 99560
    },
    {
      "epoch": 3.319,
      "grad_norm": 0.22658641636371613,
      "learning_rate": 2.9256250000000003e-05,
      "loss": 0.0026,
      "step": 99570
    },
    {
      "epoch": 3.3193333333333332,
      "grad_norm": 0.0861654132604599,
      "learning_rate": 2.9254166666666665e-05,
      "loss": 0.0023,
      "step": 99580
    },
    {
      "epoch": 3.3196666666666665,
      "grad_norm": 0.17204846441745758,
      "learning_rate": 2.9252083333333334e-05,
      "loss": 0.0022,
      "step": 99590
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.3440375030040741,
      "learning_rate": 2.925e-05,
      "loss": 0.0023,
      "step": 99600
    },
    {
      "epoch": 3.320333333333333,
      "grad_norm": 0.11475560814142227,
      "learning_rate": 2.924791666666667e-05,
      "loss": 0.0018,
      "step": 99610
    },
    {
      "epoch": 3.320666666666667,
      "grad_norm": 0.02904629521071911,
      "learning_rate": 2.9245833333333334e-05,
      "loss": 0.0025,
      "step": 99620
    },
    {
      "epoch": 3.321,
      "grad_norm": 0.25795668363571167,
      "learning_rate": 2.9243750000000003e-05,
      "loss": 0.0019,
      "step": 99630
    },
    {
      "epoch": 3.3213333333333335,
      "grad_norm": 0.34408098459243774,
      "learning_rate": 2.9241666666666668e-05,
      "loss": 0.0029,
      "step": 99640
    },
    {
      "epoch": 3.3216666666666668,
      "grad_norm": 0.9055910110473633,
      "learning_rate": 2.9239583333333337e-05,
      "loss": 0.002,
      "step": 99650
    },
    {
      "epoch": 3.322,
      "grad_norm": 0.08610374480485916,
      "learning_rate": 2.9237500000000003e-05,
      "loss": 0.0019,
      "step": 99660
    },
    {
      "epoch": 3.3223333333333334,
      "grad_norm": 0.20057521760463715,
      "learning_rate": 2.923541666666667e-05,
      "loss": 0.0025,
      "step": 99670
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.02945023775100708,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.0019,
      "step": 99680
    },
    {
      "epoch": 3.323,
      "grad_norm": 0.20074999332427979,
      "learning_rate": 2.923125e-05,
      "loss": 0.0016,
      "step": 99690
    },
    {
      "epoch": 3.3233333333333333,
      "grad_norm": 0.42993661761283875,
      "learning_rate": 2.9229166666666668e-05,
      "loss": 0.0026,
      "step": 99700
    },
    {
      "epoch": 3.3236666666666665,
      "grad_norm": 0.11503635346889496,
      "learning_rate": 2.9227083333333333e-05,
      "loss": 0.0025,
      "step": 99710
    },
    {
      "epoch": 3.324,
      "grad_norm": 0.22930137813091278,
      "learning_rate": 2.9225000000000002e-05,
      "loss": 0.0039,
      "step": 99720
    },
    {
      "epoch": 3.324333333333333,
      "grad_norm": 0.05759499967098236,
      "learning_rate": 2.9222916666666668e-05,
      "loss": 0.0023,
      "step": 99730
    },
    {
      "epoch": 3.3246666666666664,
      "grad_norm": 0.2005605548620224,
      "learning_rate": 2.9220833333333337e-05,
      "loss": 0.0033,
      "step": 99740
    },
    {
      "epoch": 3.325,
      "grad_norm": 0.08619413524866104,
      "learning_rate": 2.921875e-05,
      "loss": 0.0025,
      "step": 99750
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.10161969810724258,
      "learning_rate": 2.921666666666667e-05,
      "loss": 0.002,
      "step": 99760
    },
    {
      "epoch": 3.3256666666666668,
      "grad_norm": 0.3727290630340576,
      "learning_rate": 2.9214583333333333e-05,
      "loss": 0.0023,
      "step": 99770
    },
    {
      "epoch": 3.326,
      "grad_norm": 0.22913694381713867,
      "learning_rate": 2.9212500000000005e-05,
      "loss": 0.002,
      "step": 99780
    },
    {
      "epoch": 3.3263333333333334,
      "grad_norm": 0.5447206497192383,
      "learning_rate": 2.9210416666666667e-05,
      "loss": 0.0019,
      "step": 99790
    },
    {
      "epoch": 3.3266666666666667,
      "grad_norm": 0.31512245535850525,
      "learning_rate": 2.9208333333333333e-05,
      "loss": 0.0033,
      "step": 99800
    },
    {
      "epoch": 3.327,
      "grad_norm": 0.2294541597366333,
      "learning_rate": 2.9206250000000002e-05,
      "loss": 0.0025,
      "step": 99810
    },
    {
      "epoch": 3.3273333333333333,
      "grad_norm": 0.258079469203949,
      "learning_rate": 2.9204166666666667e-05,
      "loss": 0.003,
      "step": 99820
    },
    {
      "epoch": 3.3276666666666666,
      "grad_norm": 0.2581345736980438,
      "learning_rate": 2.9202083333333336e-05,
      "loss": 0.003,
      "step": 99830
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.057646360248327255,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0027,
      "step": 99840
    },
    {
      "epoch": 3.328333333333333,
      "grad_norm": 0.28655099868774414,
      "learning_rate": 2.919791666666667e-05,
      "loss": 0.0022,
      "step": 99850
    },
    {
      "epoch": 3.328666666666667,
      "grad_norm": 0.0575508214533329,
      "learning_rate": 2.9195833333333333e-05,
      "loss": 0.0021,
      "step": 99860
    },
    {
      "epoch": 3.329,
      "grad_norm": 0.4299865663051605,
      "learning_rate": 2.919375e-05,
      "loss": 0.0023,
      "step": 99870
    },
    {
      "epoch": 3.3293333333333335,
      "grad_norm": 0.005436642561107874,
      "learning_rate": 2.9191666666666667e-05,
      "loss": 0.0021,
      "step": 99880
    },
    {
      "epoch": 3.3296666666666668,
      "grad_norm": 0.20114865899085999,
      "learning_rate": 2.9189583333333336e-05,
      "loss": 0.0014,
      "step": 99890
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.08656267076730728,
      "learning_rate": 2.91875e-05,
      "loss": 0.0028,
      "step": 99900
    },
    {
      "epoch": 3.3303333333333334,
      "grad_norm": 0.2009214162826538,
      "learning_rate": 2.918541666666667e-05,
      "loss": 0.0018,
      "step": 99910
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.37320199608802795,
      "learning_rate": 2.9183333333333336e-05,
      "loss": 0.0027,
      "step": 99920
    },
    {
      "epoch": 3.331,
      "grad_norm": 0.11512976139783859,
      "learning_rate": 2.9181249999999998e-05,
      "loss": 0.0021,
      "step": 99930
    },
    {
      "epoch": 3.3313333333333333,
      "grad_norm": 0.11769682168960571,
      "learning_rate": 2.917916666666667e-05,
      "loss": 0.002,
      "step": 99940
    },
    {
      "epoch": 3.3316666666666666,
      "grad_norm": 0.1723005175590515,
      "learning_rate": 2.9177083333333332e-05,
      "loss": 0.0019,
      "step": 99950
    },
    {
      "epoch": 3.332,
      "grad_norm": 0.4584554433822632,
      "learning_rate": 2.9175e-05,
      "loss": 0.0031,
      "step": 99960
    },
    {
      "epoch": 3.332333333333333,
      "grad_norm": 0.40126898884773254,
      "learning_rate": 2.9172916666666667e-05,
      "loss": 0.0024,
      "step": 99970
    },
    {
      "epoch": 3.3326666666666664,
      "grad_norm": 0.029117954894900322,
      "learning_rate": 2.9170833333333335e-05,
      "loss": 0.0018,
      "step": 99980
    },
    {
      "epoch": 3.333,
      "grad_norm": 0.05815583094954491,
      "learning_rate": 2.916875e-05,
      "loss": 0.0021,
      "step": 99990
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.1147717610001564,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0028,
      "step": 100000
    },
    {
      "epoch": 3.333666666666667,
      "grad_norm": 0.004912534262984991,
      "learning_rate": 2.9164583333333335e-05,
      "loss": 0.0023,
      "step": 100010
    },
    {
      "epoch": 3.334,
      "grad_norm": 0.057420458644628525,
      "learning_rate": 2.9162500000000004e-05,
      "loss": 0.0023,
      "step": 100020
    },
    {
      "epoch": 3.3343333333333334,
      "grad_norm": 0.3726726770401001,
      "learning_rate": 2.9160416666666666e-05,
      "loss": 0.0032,
      "step": 100030
    },
    {
      "epoch": 3.3346666666666667,
      "grad_norm": 0.05736749619245529,
      "learning_rate": 2.915833333333334e-05,
      "loss": 0.0022,
      "step": 100040
    },
    {
      "epoch": 3.335,
      "grad_norm": 0.6915886402130127,
      "learning_rate": 2.915625e-05,
      "loss": 0.0018,
      "step": 100050
    },
    {
      "epoch": 3.3353333333333333,
      "grad_norm": 0.4299686551094055,
      "learning_rate": 2.9154166666666666e-05,
      "loss": 0.0031,
      "step": 100060
    },
    {
      "epoch": 3.3356666666666666,
      "grad_norm": 0.22936761379241943,
      "learning_rate": 2.9152083333333335e-05,
      "loss": 0.003,
      "step": 100070
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.17260539531707764,
      "learning_rate": 2.915e-05,
      "loss": 0.002,
      "step": 100080
    },
    {
      "epoch": 3.336333333333333,
      "grad_norm": 0.2865551710128784,
      "learning_rate": 2.914791666666667e-05,
      "loss": 0.0019,
      "step": 100090
    },
    {
      "epoch": 3.336666666666667,
      "grad_norm": 0.05739355459809303,
      "learning_rate": 2.9145833333333335e-05,
      "loss": 0.0025,
      "step": 100100
    },
    {
      "epoch": 3.337,
      "grad_norm": 0.45862141251564026,
      "learning_rate": 2.9143750000000004e-05,
      "loss": 0.0023,
      "step": 100110
    },
    {
      "epoch": 3.3373333333333335,
      "grad_norm": 0.3152235448360443,
      "learning_rate": 2.9141666666666666e-05,
      "loss": 0.0017,
      "step": 100120
    },
    {
      "epoch": 3.337666666666667,
      "grad_norm": 0.3439439833164215,
      "learning_rate": 2.9139583333333338e-05,
      "loss": 0.0026,
      "step": 100130
    },
    {
      "epoch": 3.338,
      "grad_norm": 0.20069555938243866,
      "learning_rate": 2.91375e-05,
      "loss": 0.002,
      "step": 100140
    },
    {
      "epoch": 3.3383333333333334,
      "grad_norm": 0.3439823091030121,
      "learning_rate": 2.913541666666667e-05,
      "loss": 0.002,
      "step": 100150
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.5397624969482422,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0031,
      "step": 100160
    },
    {
      "epoch": 3.339,
      "grad_norm": 0.286569207906723,
      "learning_rate": 2.913125e-05,
      "loss": 0.003,
      "step": 100170
    },
    {
      "epoch": 3.3393333333333333,
      "grad_norm": 0.2299518883228302,
      "learning_rate": 2.912916666666667e-05,
      "loss": 0.0018,
      "step": 100180
    },
    {
      "epoch": 3.3396666666666666,
      "grad_norm": 0.030448148027062416,
      "learning_rate": 2.912708333333333e-05,
      "loss": 0.0021,
      "step": 100190
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.4733944237232208,
      "learning_rate": 2.9125000000000003e-05,
      "loss": 0.0027,
      "step": 100200
    },
    {
      "epoch": 3.340333333333333,
      "grad_norm": 0.4297688603401184,
      "learning_rate": 2.9122916666666665e-05,
      "loss": 0.0033,
      "step": 100210
    },
    {
      "epoch": 3.3406666666666665,
      "grad_norm": 0.17232535779476166,
      "learning_rate": 2.9120833333333338e-05,
      "loss": 0.0031,
      "step": 100220
    },
    {
      "epoch": 3.341,
      "grad_norm": 0.2291317582130432,
      "learning_rate": 2.911875e-05,
      "loss": 0.002,
      "step": 100230
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.30038660764694214,
      "learning_rate": 2.911666666666667e-05,
      "loss": 0.0021,
      "step": 100240
    },
    {
      "epoch": 3.341666666666667,
      "grad_norm": 0.00642731599509716,
      "learning_rate": 2.9114583333333334e-05,
      "loss": 0.0018,
      "step": 100250
    },
    {
      "epoch": 3.342,
      "grad_norm": 0.05753781646490097,
      "learning_rate": 2.9112500000000003e-05,
      "loss": 0.0019,
      "step": 100260
    },
    {
      "epoch": 3.3423333333333334,
      "grad_norm": 0.05815626308321953,
      "learning_rate": 2.911041666666667e-05,
      "loss": 0.0042,
      "step": 100270
    },
    {
      "epoch": 3.3426666666666667,
      "grad_norm": 0.4013311564922333,
      "learning_rate": 2.9108333333333337e-05,
      "loss": 0.0019,
      "step": 100280
    },
    {
      "epoch": 3.343,
      "grad_norm": 0.2865142822265625,
      "learning_rate": 2.9106250000000003e-05,
      "loss": 0.0018,
      "step": 100290
    },
    {
      "epoch": 3.3433333333333333,
      "grad_norm": 0.20078067481517792,
      "learning_rate": 2.9104166666666665e-05,
      "loss": 0.0024,
      "step": 100300
    },
    {
      "epoch": 3.3436666666666666,
      "grad_norm": 0.37268489599227905,
      "learning_rate": 2.9102083333333334e-05,
      "loss": 0.0035,
      "step": 100310
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.05761958286166191,
      "learning_rate": 2.91e-05,
      "loss": 0.0016,
      "step": 100320
    },
    {
      "epoch": 3.344333333333333,
      "grad_norm": 0.2580672800540924,
      "learning_rate": 2.9097916666666668e-05,
      "loss": 0.0021,
      "step": 100330
    },
    {
      "epoch": 3.344666666666667,
      "grad_norm": 0.057604651898145676,
      "learning_rate": 2.9095833333333334e-05,
      "loss": 0.0027,
      "step": 100340
    },
    {
      "epoch": 3.3449999999999998,
      "grad_norm": 0.08639129996299744,
      "learning_rate": 2.9093750000000002e-05,
      "loss": 0.0022,
      "step": 100350
    },
    {
      "epoch": 3.3453333333333335,
      "grad_norm": 0.058829374611377716,
      "learning_rate": 2.9091666666666668e-05,
      "loss": 0.0029,
      "step": 100360
    },
    {
      "epoch": 3.345666666666667,
      "grad_norm": 0.11501296609640121,
      "learning_rate": 2.9089583333333337e-05,
      "loss": 0.0044,
      "step": 100370
    },
    {
      "epoch": 3.346,
      "grad_norm": 0.007547896821051836,
      "learning_rate": 2.9087500000000002e-05,
      "loss": 0.0025,
      "step": 100380
    },
    {
      "epoch": 3.3463333333333334,
      "grad_norm": 0.03157084062695503,
      "learning_rate": 2.908541666666667e-05,
      "loss": 0.0017,
      "step": 100390
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.20155069231987,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.002,
      "step": 100400
    },
    {
      "epoch": 3.347,
      "grad_norm": 0.2866424024105072,
      "learning_rate": 2.908125e-05,
      "loss": 0.003,
      "step": 100410
    },
    {
      "epoch": 3.3473333333333333,
      "grad_norm": 0.2296627014875412,
      "learning_rate": 2.9079166666666668e-05,
      "loss": 0.002,
      "step": 100420
    },
    {
      "epoch": 3.3476666666666666,
      "grad_norm": 0.028696691617369652,
      "learning_rate": 2.9077083333333333e-05,
      "loss": 0.0022,
      "step": 100430
    },
    {
      "epoch": 3.348,
      "grad_norm": 0.31525924801826477,
      "learning_rate": 2.9075000000000002e-05,
      "loss": 0.0014,
      "step": 100440
    },
    {
      "epoch": 3.348333333333333,
      "grad_norm": 0.115240179002285,
      "learning_rate": 2.9072916666666667e-05,
      "loss": 0.002,
      "step": 100450
    },
    {
      "epoch": 3.3486666666666665,
      "grad_norm": 0.08699202537536621,
      "learning_rate": 2.9070833333333336e-05,
      "loss": 0.0029,
      "step": 100460
    },
    {
      "epoch": 3.349,
      "grad_norm": 0.1720256507396698,
      "learning_rate": 2.9068750000000002e-05,
      "loss": 0.0015,
      "step": 100470
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.17184925079345703,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0029,
      "step": 100480
    },
    {
      "epoch": 3.349666666666667,
      "grad_norm": 0.0075095035135746,
      "learning_rate": 2.9064583333333333e-05,
      "loss": 0.0018,
      "step": 100490
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.40083611011505127,
      "learning_rate": 2.9062500000000005e-05,
      "loss": 0.0025,
      "step": 100500
    },
    {
      "epoch": 3.3503333333333334,
      "grad_norm": 0.25800174474716187,
      "learning_rate": 2.9060416666666667e-05,
      "loss": 0.0012,
      "step": 100510
    },
    {
      "epoch": 3.3506666666666667,
      "grad_norm": 0.22949400544166565,
      "learning_rate": 2.9058333333333336e-05,
      "loss": 0.0016,
      "step": 100520
    },
    {
      "epoch": 3.351,
      "grad_norm": 0.20067866146564484,
      "learning_rate": 2.905625e-05,
      "loss": 0.0021,
      "step": 100530
    },
    {
      "epoch": 3.3513333333333333,
      "grad_norm": 0.08662740886211395,
      "learning_rate": 2.9054166666666667e-05,
      "loss": 0.0025,
      "step": 100540
    },
    {
      "epoch": 3.3516666666666666,
      "grad_norm": 0.14337100088596344,
      "learning_rate": 2.9052083333333336e-05,
      "loss": 0.0024,
      "step": 100550
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.007458323612809181,
      "learning_rate": 2.9049999999999998e-05,
      "loss": 0.0021,
      "step": 100560
    },
    {
      "epoch": 3.352333333333333,
      "grad_norm": 0.25776657462120056,
      "learning_rate": 2.904791666666667e-05,
      "loss": 0.0018,
      "step": 100570
    },
    {
      "epoch": 3.352666666666667,
      "grad_norm": 0.28661733865737915,
      "learning_rate": 2.9045833333333332e-05,
      "loss": 0.0016,
      "step": 100580
    },
    {
      "epoch": 3.3529999999999998,
      "grad_norm": 0.20071129500865936,
      "learning_rate": 2.904375e-05,
      "loss": 0.0024,
      "step": 100590
    },
    {
      "epoch": 3.3533333333333335,
      "grad_norm": 0.02896244265139103,
      "learning_rate": 2.9041666666666667e-05,
      "loss": 0.0017,
      "step": 100600
    },
    {
      "epoch": 3.353666666666667,
      "grad_norm": 0.11473128944635391,
      "learning_rate": 2.9039583333333336e-05,
      "loss": 0.0023,
      "step": 100610
    },
    {
      "epoch": 3.354,
      "grad_norm": 0.17188704013824463,
      "learning_rate": 2.90375e-05,
      "loss": 0.002,
      "step": 100620
    },
    {
      "epoch": 3.3543333333333334,
      "grad_norm": 0.14354805648326874,
      "learning_rate": 2.903541666666667e-05,
      "loss": 0.0017,
      "step": 100630
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.16857489943504333,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0028,
      "step": 100640
    },
    {
      "epoch": 3.355,
      "grad_norm": 0.6684375405311584,
      "learning_rate": 2.9031249999999998e-05,
      "loss": 0.0017,
      "step": 100650
    },
    {
      "epoch": 3.3553333333333333,
      "grad_norm": 0.34393835067749023,
      "learning_rate": 2.902916666666667e-05,
      "loss": 0.0023,
      "step": 100660
    },
    {
      "epoch": 3.3556666666666666,
      "grad_norm": 0.8886404037475586,
      "learning_rate": 2.9027083333333332e-05,
      "loss": 0.0026,
      "step": 100670
    },
    {
      "epoch": 3.356,
      "grad_norm": 0.1721225380897522,
      "learning_rate": 2.9025e-05,
      "loss": 0.0019,
      "step": 100680
    },
    {
      "epoch": 3.356333333333333,
      "grad_norm": 0.22925284504890442,
      "learning_rate": 2.9022916666666666e-05,
      "loss": 0.0023,
      "step": 100690
    },
    {
      "epoch": 3.3566666666666665,
      "grad_norm": 0.057543154805898666,
      "learning_rate": 2.9020833333333335e-05,
      "loss": 0.0017,
      "step": 100700
    },
    {
      "epoch": 3.357,
      "grad_norm": 0.1431903839111328,
      "learning_rate": 2.901875e-05,
      "loss": 0.0026,
      "step": 100710
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.14345955848693848,
      "learning_rate": 2.901666666666667e-05,
      "loss": 0.0018,
      "step": 100720
    },
    {
      "epoch": 3.357666666666667,
      "grad_norm": 0.2293613851070404,
      "learning_rate": 2.9014583333333335e-05,
      "loss": 0.0022,
      "step": 100730
    },
    {
      "epoch": 3.358,
      "grad_norm": 0.3516470193862915,
      "learning_rate": 2.9012500000000004e-05,
      "loss": 0.0019,
      "step": 100740
    },
    {
      "epoch": 3.3583333333333334,
      "grad_norm": 0.057359471917152405,
      "learning_rate": 2.901041666666667e-05,
      "loss": 0.0025,
      "step": 100750
    },
    {
      "epoch": 3.3586666666666667,
      "grad_norm": 0.08617258071899414,
      "learning_rate": 2.9008333333333338e-05,
      "loss": 0.0023,
      "step": 100760
    },
    {
      "epoch": 3.359,
      "grad_norm": 0.17195259034633636,
      "learning_rate": 2.900625e-05,
      "loss": 0.0025,
      "step": 100770
    },
    {
      "epoch": 3.3593333333333333,
      "grad_norm": 0.7363782525062561,
      "learning_rate": 2.9004166666666666e-05,
      "loss": 0.0026,
      "step": 100780
    },
    {
      "epoch": 3.3596666666666666,
      "grad_norm": 0.03421890735626221,
      "learning_rate": 2.9002083333333335e-05,
      "loss": 0.0021,
      "step": 100790
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.08607295155525208,
      "learning_rate": 2.9e-05,
      "loss": 0.0023,
      "step": 100800
    },
    {
      "epoch": 3.360333333333333,
      "grad_norm": 0.2639865279197693,
      "learning_rate": 2.899791666666667e-05,
      "loss": 0.0027,
      "step": 100810
    },
    {
      "epoch": 3.360666666666667,
      "grad_norm": 0.02863534912467003,
      "learning_rate": 2.8995833333333335e-05,
      "loss": 0.0018,
      "step": 100820
    },
    {
      "epoch": 3.3609999999999998,
      "grad_norm": 0.516135573387146,
      "learning_rate": 2.8993750000000003e-05,
      "loss": 0.0023,
      "step": 100830
    },
    {
      "epoch": 3.3613333333333335,
      "grad_norm": 0.3156539797782898,
      "learning_rate": 2.8991666666666666e-05,
      "loss": 0.0019,
      "step": 100840
    },
    {
      "epoch": 3.361666666666667,
      "grad_norm": 0.03142625838518143,
      "learning_rate": 2.8989583333333338e-05,
      "loss": 0.002,
      "step": 100850
    },
    {
      "epoch": 3.362,
      "grad_norm": 0.0306351650506258,
      "learning_rate": 2.89875e-05,
      "loss": 0.0017,
      "step": 100860
    },
    {
      "epoch": 3.3623333333333334,
      "grad_norm": 0.05990190431475639,
      "learning_rate": 2.8985416666666672e-05,
      "loss": 0.0013,
      "step": 100870
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.0049707042053341866,
      "learning_rate": 2.8983333333333334e-05,
      "loss": 0.0024,
      "step": 100880
    },
    {
      "epoch": 3.363,
      "grad_norm": 0.058596592396497726,
      "learning_rate": 2.898125e-05,
      "loss": 0.0019,
      "step": 100890
    },
    {
      "epoch": 3.3633333333333333,
      "grad_norm": 0.166222482919693,
      "learning_rate": 2.897916666666667e-05,
      "loss": 0.0021,
      "step": 100900
    },
    {
      "epoch": 3.3636666666666666,
      "grad_norm": 0.210984468460083,
      "learning_rate": 2.8977083333333334e-05,
      "loss": 0.0019,
      "step": 100910
    },
    {
      "epoch": 3.364,
      "grad_norm": 0.40100544691085815,
      "learning_rate": 2.8975000000000003e-05,
      "loss": 0.0023,
      "step": 100920
    },
    {
      "epoch": 3.364333333333333,
      "grad_norm": 0.0863795354962349,
      "learning_rate": 2.8972916666666665e-05,
      "loss": 0.0023,
      "step": 100930
    },
    {
      "epoch": 3.3646666666666665,
      "grad_norm": 0.029610518366098404,
      "learning_rate": 2.8970833333333337e-05,
      "loss": 0.002,
      "step": 100940
    },
    {
      "epoch": 3.365,
      "grad_norm": 0.005825934931635857,
      "learning_rate": 2.896875e-05,
      "loss": 0.0015,
      "step": 100950
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.40116313099861145,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0025,
      "step": 100960
    },
    {
      "epoch": 3.365666666666667,
      "grad_norm": 0.22926419973373413,
      "learning_rate": 2.8964583333333334e-05,
      "loss": 0.0021,
      "step": 100970
    },
    {
      "epoch": 3.366,
      "grad_norm": 0.5729328989982605,
      "learning_rate": 2.8962500000000003e-05,
      "loss": 0.0026,
      "step": 100980
    },
    {
      "epoch": 3.3663333333333334,
      "grad_norm": 0.23146454989910126,
      "learning_rate": 2.8960416666666668e-05,
      "loss": 0.0025,
      "step": 100990
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.2864755392074585,
      "learning_rate": 2.8958333333333337e-05,
      "loss": 0.0018,
      "step": 101000
    },
    {
      "epoch": 3.367,
      "grad_norm": 0.05771113559603691,
      "learning_rate": 2.8956250000000002e-05,
      "loss": 0.0022,
      "step": 101010
    },
    {
      "epoch": 3.3673333333333333,
      "grad_norm": 0.11531857401132584,
      "learning_rate": 2.8954166666666665e-05,
      "loss": 0.002,
      "step": 101020
    },
    {
      "epoch": 3.3676666666666666,
      "grad_norm": 0.17106510698795319,
      "learning_rate": 2.8952083333333337e-05,
      "loss": 0.0029,
      "step": 101030
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.1717110276222229,
      "learning_rate": 2.895e-05,
      "loss": 0.0022,
      "step": 101040
    },
    {
      "epoch": 3.368333333333333,
      "grad_norm": 0.22938552498817444,
      "learning_rate": 2.8947916666666668e-05,
      "loss": 0.0021,
      "step": 101050
    },
    {
      "epoch": 3.3686666666666665,
      "grad_norm": 0.17208418250083923,
      "learning_rate": 2.8945833333333333e-05,
      "loss": 0.0024,
      "step": 101060
    },
    {
      "epoch": 3.3689999999999998,
      "grad_norm": 0.28692322969436646,
      "learning_rate": 2.8943750000000002e-05,
      "loss": 0.003,
      "step": 101070
    },
    {
      "epoch": 3.3693333333333335,
      "grad_norm": 0.2794401943683624,
      "learning_rate": 2.8941666666666668e-05,
      "loss": 0.0024,
      "step": 101080
    },
    {
      "epoch": 3.369666666666667,
      "grad_norm": 0.08595918864011765,
      "learning_rate": 2.8939583333333337e-05,
      "loss": 0.0022,
      "step": 101090
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.12249606102705002,
      "learning_rate": 2.8937500000000002e-05,
      "loss": 0.0019,
      "step": 101100
    },
    {
      "epoch": 3.3703333333333334,
      "grad_norm": 0.5495683550834656,
      "learning_rate": 2.893541666666667e-05,
      "loss": 0.0024,
      "step": 101110
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.17227791249752045,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0024,
      "step": 101120
    },
    {
      "epoch": 3.371,
      "grad_norm": 0.2870505452156067,
      "learning_rate": 2.893125e-05,
      "loss": 0.0018,
      "step": 101130
    },
    {
      "epoch": 3.3713333333333333,
      "grad_norm": 0.7481167316436768,
      "learning_rate": 2.8929166666666667e-05,
      "loss": 0.0028,
      "step": 101140
    },
    {
      "epoch": 3.3716666666666666,
      "grad_norm": 0.03579484298825264,
      "learning_rate": 2.8927083333333333e-05,
      "loss": 0.0018,
      "step": 101150
    },
    {
      "epoch": 3.372,
      "grad_norm": 0.11515297740697861,
      "learning_rate": 2.8925000000000002e-05,
      "loss": 0.0032,
      "step": 101160
    },
    {
      "epoch": 3.372333333333333,
      "grad_norm": 0.14344331622123718,
      "learning_rate": 2.8922916666666667e-05,
      "loss": 0.0014,
      "step": 101170
    },
    {
      "epoch": 3.3726666666666665,
      "grad_norm": 0.3726940453052521,
      "learning_rate": 2.8920833333333336e-05,
      "loss": 0.0021,
      "step": 101180
    },
    {
      "epoch": 3.373,
      "grad_norm": 0.02964896894991398,
      "learning_rate": 2.891875e-05,
      "loss": 0.0028,
      "step": 101190
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.02877877652645111,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.0022,
      "step": 101200
    },
    {
      "epoch": 3.373666666666667,
      "grad_norm": 0.14405034482479095,
      "learning_rate": 2.8914583333333333e-05,
      "loss": 0.0017,
      "step": 101210
    },
    {
      "epoch": 3.374,
      "grad_norm": 0.028890687972307205,
      "learning_rate": 2.8912500000000005e-05,
      "loss": 0.0019,
      "step": 101220
    },
    {
      "epoch": 3.3743333333333334,
      "grad_norm": 0.4298040270805359,
      "learning_rate": 2.8910416666666667e-05,
      "loss": 0.0015,
      "step": 101230
    },
    {
      "epoch": 3.3746666666666667,
      "grad_norm": 0.9609627723693848,
      "learning_rate": 2.8908333333333336e-05,
      "loss": 0.0021,
      "step": 101240
    },
    {
      "epoch": 3.375,
      "grad_norm": 0.8342006206512451,
      "learning_rate": 2.890625e-05,
      "loss": 0.0026,
      "step": 101250
    },
    {
      "epoch": 3.3753333333333333,
      "grad_norm": 0.31317880749702454,
      "learning_rate": 2.8904166666666667e-05,
      "loss": 0.0025,
      "step": 101260
    },
    {
      "epoch": 3.3756666666666666,
      "grad_norm": 0.762805163860321,
      "learning_rate": 2.8902083333333336e-05,
      "loss": 0.0019,
      "step": 101270
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.08612656593322754,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0022,
      "step": 101280
    },
    {
      "epoch": 3.376333333333333,
      "grad_norm": 1.0515825748443604,
      "learning_rate": 2.889791666666667e-05,
      "loss": 0.0026,
      "step": 101290
    },
    {
      "epoch": 3.3766666666666665,
      "grad_norm": 0.2866734564304352,
      "learning_rate": 2.8895833333333332e-05,
      "loss": 0.0025,
      "step": 101300
    },
    {
      "epoch": 3.377,
      "grad_norm": 0.028772598132491112,
      "learning_rate": 2.8893750000000004e-05,
      "loss": 0.0017,
      "step": 101310
    },
    {
      "epoch": 3.3773333333333335,
      "grad_norm": 0.20064350962638855,
      "learning_rate": 2.8891666666666666e-05,
      "loss": 0.0018,
      "step": 101320
    },
    {
      "epoch": 3.377666666666667,
      "grad_norm": 0.029394863173365593,
      "learning_rate": 2.8889583333333335e-05,
      "loss": 0.002,
      "step": 101330
    },
    {
      "epoch": 3.378,
      "grad_norm": 0.22939614951610565,
      "learning_rate": 2.88875e-05,
      "loss": 0.002,
      "step": 101340
    },
    {
      "epoch": 3.3783333333333334,
      "grad_norm": 0.029244989156723022,
      "learning_rate": 2.888541666666667e-05,
      "loss": 0.0027,
      "step": 101350
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.1719791740179062,
      "learning_rate": 2.8883333333333335e-05,
      "loss": 0.0018,
      "step": 101360
    },
    {
      "epoch": 3.379,
      "grad_norm": 0.057429876178503036,
      "learning_rate": 2.8881250000000004e-05,
      "loss": 0.0023,
      "step": 101370
    },
    {
      "epoch": 3.3793333333333333,
      "grad_norm": 0.1432969570159912,
      "learning_rate": 2.887916666666667e-05,
      "loss": 0.0017,
      "step": 101380
    },
    {
      "epoch": 3.3796666666666666,
      "grad_norm": 0.11463508009910583,
      "learning_rate": 2.887708333333333e-05,
      "loss": 0.0021,
      "step": 101390
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.45842498540878296,
      "learning_rate": 2.8875e-05,
      "loss": 0.002,
      "step": 101400
    },
    {
      "epoch": 3.380333333333333,
      "grad_norm": 0.2580110728740692,
      "learning_rate": 2.8872916666666666e-05,
      "loss": 0.0024,
      "step": 101410
    },
    {
      "epoch": 3.3806666666666665,
      "grad_norm": 0.20069251954555511,
      "learning_rate": 2.8870833333333335e-05,
      "loss": 0.0023,
      "step": 101420
    },
    {
      "epoch": 3.3810000000000002,
      "grad_norm": 0.08637383580207825,
      "learning_rate": 2.886875e-05,
      "loss": 0.0023,
      "step": 101430
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.1436787247657776,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0016,
      "step": 101440
    },
    {
      "epoch": 3.381666666666667,
      "grad_norm": 0.28667178750038147,
      "learning_rate": 2.8864583333333335e-05,
      "loss": 0.0014,
      "step": 101450
    },
    {
      "epoch": 3.382,
      "grad_norm": 0.08620379865169525,
      "learning_rate": 2.8862500000000004e-05,
      "loss": 0.0022,
      "step": 101460
    },
    {
      "epoch": 3.3823333333333334,
      "grad_norm": 0.005971306469291449,
      "learning_rate": 2.886041666666667e-05,
      "loss": 0.0021,
      "step": 101470
    },
    {
      "epoch": 3.3826666666666667,
      "grad_norm": 0.22931724786758423,
      "learning_rate": 2.8858333333333338e-05,
      "loss": 0.0027,
      "step": 101480
    },
    {
      "epoch": 3.383,
      "grad_norm": 0.08608020842075348,
      "learning_rate": 2.885625e-05,
      "loss": 0.0028,
      "step": 101490
    },
    {
      "epoch": 3.3833333333333333,
      "grad_norm": 0.3155227303504944,
      "learning_rate": 2.8854166666666666e-05,
      "loss": 0.0032,
      "step": 101500
    },
    {
      "epoch": 3.3836666666666666,
      "grad_norm": 0.14316971600055695,
      "learning_rate": 2.8852083333333334e-05,
      "loss": 0.0015,
      "step": 101510
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.3437470495700836,
      "learning_rate": 2.885e-05,
      "loss": 0.0035,
      "step": 101520
    },
    {
      "epoch": 3.384333333333333,
      "grad_norm": 0.08925458788871765,
      "learning_rate": 2.884791666666667e-05,
      "loss": 0.0026,
      "step": 101530
    },
    {
      "epoch": 3.3846666666666665,
      "grad_norm": 0.1437922865152359,
      "learning_rate": 2.8845833333333334e-05,
      "loss": 0.0016,
      "step": 101540
    },
    {
      "epoch": 3.385,
      "grad_norm": 0.086217001080513,
      "learning_rate": 2.8843750000000003e-05,
      "loss": 0.0022,
      "step": 101550
    },
    {
      "epoch": 3.3853333333333335,
      "grad_norm": 0.4299023747444153,
      "learning_rate": 2.8841666666666665e-05,
      "loss": 0.0018,
      "step": 101560
    },
    {
      "epoch": 3.385666666666667,
      "grad_norm": 0.17211031913757324,
      "learning_rate": 2.8839583333333338e-05,
      "loss": 0.0019,
      "step": 101570
    },
    {
      "epoch": 3.386,
      "grad_norm": 0.327595055103302,
      "learning_rate": 2.88375e-05,
      "loss": 0.0021,
      "step": 101580
    },
    {
      "epoch": 3.3863333333333334,
      "grad_norm": 0.31514430046081543,
      "learning_rate": 2.8835416666666672e-05,
      "loss": 0.0021,
      "step": 101590
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.05851253867149353,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0013,
      "step": 101600
    },
    {
      "epoch": 3.387,
      "grad_norm": 0.11477867513895035,
      "learning_rate": 2.8831250000000003e-05,
      "loss": 0.0027,
      "step": 101610
    },
    {
      "epoch": 3.3873333333333333,
      "grad_norm": 0.030205873772501945,
      "learning_rate": 2.882916666666667e-05,
      "loss": 0.0016,
      "step": 101620
    },
    {
      "epoch": 3.3876666666666666,
      "grad_norm": 0.2866644859313965,
      "learning_rate": 2.8827083333333334e-05,
      "loss": 0.0023,
      "step": 101630
    },
    {
      "epoch": 3.388,
      "grad_norm": 0.08607550710439682,
      "learning_rate": 2.8825000000000003e-05,
      "loss": 0.0028,
      "step": 101640
    },
    {
      "epoch": 3.388333333333333,
      "grad_norm": 0.2006390541791916,
      "learning_rate": 2.8822916666666665e-05,
      "loss": 0.0016,
      "step": 101650
    },
    {
      "epoch": 3.3886666666666665,
      "grad_norm": 0.37234067916870117,
      "learning_rate": 2.8820833333333337e-05,
      "loss": 0.0018,
      "step": 101660
    },
    {
      "epoch": 3.3890000000000002,
      "grad_norm": 0.3438984155654907,
      "learning_rate": 2.881875e-05,
      "loss": 0.0021,
      "step": 101670
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.057630542665719986,
      "learning_rate": 2.8816666666666668e-05,
      "loss": 0.0025,
      "step": 101680
    },
    {
      "epoch": 3.389666666666667,
      "grad_norm": 0.49436062574386597,
      "learning_rate": 2.8814583333333334e-05,
      "loss": 0.0023,
      "step": 101690
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.08641696721315384,
      "learning_rate": 2.8812500000000002e-05,
      "loss": 0.0018,
      "step": 101700
    },
    {
      "epoch": 3.3903333333333334,
      "grad_norm": 0.004224948585033417,
      "learning_rate": 2.8810416666666668e-05,
      "loss": 0.0022,
      "step": 101710
    },
    {
      "epoch": 3.3906666666666667,
      "grad_norm": 0.25804516673088074,
      "learning_rate": 2.8808333333333337e-05,
      "loss": 0.0026,
      "step": 101720
    },
    {
      "epoch": 3.391,
      "grad_norm": 0.20042136311531067,
      "learning_rate": 2.8806250000000002e-05,
      "loss": 0.0015,
      "step": 101730
    },
    {
      "epoch": 3.3913333333333333,
      "grad_norm": 0.0071236263029277325,
      "learning_rate": 2.8804166666666664e-05,
      "loss": 0.0021,
      "step": 101740
    },
    {
      "epoch": 3.3916666666666666,
      "grad_norm": 0.11629199236631393,
      "learning_rate": 2.8802083333333337e-05,
      "loss": 0.002,
      "step": 101750
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.34404510259628296,
      "learning_rate": 2.88e-05,
      "loss": 0.0022,
      "step": 101760
    },
    {
      "epoch": 3.392333333333333,
      "grad_norm": 0.22958794236183167,
      "learning_rate": 2.8797916666666668e-05,
      "loss": 0.0022,
      "step": 101770
    },
    {
      "epoch": 3.3926666666666665,
      "grad_norm": 0.2863704562187195,
      "learning_rate": 2.8795833333333333e-05,
      "loss": 0.0023,
      "step": 101780
    },
    {
      "epoch": 3.393,
      "grad_norm": 0.2005125731229782,
      "learning_rate": 2.8793750000000002e-05,
      "loss": 0.0031,
      "step": 101790
    },
    {
      "epoch": 3.3933333333333335,
      "grad_norm": 0.20094935595989227,
      "learning_rate": 2.8791666666666667e-05,
      "loss": 0.0016,
      "step": 101800
    },
    {
      "epoch": 3.393666666666667,
      "grad_norm": 0.0593160018324852,
      "learning_rate": 2.8789583333333336e-05,
      "loss": 0.0019,
      "step": 101810
    },
    {
      "epoch": 3.394,
      "grad_norm": 0.40123987197875977,
      "learning_rate": 2.8787500000000002e-05,
      "loss": 0.0021,
      "step": 101820
    },
    {
      "epoch": 3.3943333333333334,
      "grad_norm": 0.2864799201488495,
      "learning_rate": 2.878541666666667e-05,
      "loss": 0.0014,
      "step": 101830
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.00715843727812171,
      "learning_rate": 2.8783333333333333e-05,
      "loss": 0.0027,
      "step": 101840
    },
    {
      "epoch": 3.395,
      "grad_norm": 0.09462445974349976,
      "learning_rate": 2.8781250000000005e-05,
      "loss": 0.0027,
      "step": 101850
    },
    {
      "epoch": 3.3953333333333333,
      "grad_norm": 0.08644717186689377,
      "learning_rate": 2.8779166666666667e-05,
      "loss": 0.0018,
      "step": 101860
    },
    {
      "epoch": 3.3956666666666666,
      "grad_norm": 0.17194566130638123,
      "learning_rate": 2.8777083333333333e-05,
      "loss": 0.002,
      "step": 101870
    },
    {
      "epoch": 3.396,
      "grad_norm": 0.22936809062957764,
      "learning_rate": 2.8775e-05,
      "loss": 0.0019,
      "step": 101880
    },
    {
      "epoch": 3.396333333333333,
      "grad_norm": 0.05792620778083801,
      "learning_rate": 2.8772916666666667e-05,
      "loss": 0.0024,
      "step": 101890
    },
    {
      "epoch": 3.3966666666666665,
      "grad_norm": 0.17174649238586426,
      "learning_rate": 2.8770833333333336e-05,
      "loss": 0.0021,
      "step": 101900
    },
    {
      "epoch": 3.3970000000000002,
      "grad_norm": 0.08609690517187119,
      "learning_rate": 2.876875e-05,
      "loss": 0.0029,
      "step": 101910
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.1720094084739685,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.0014,
      "step": 101920
    },
    {
      "epoch": 3.397666666666667,
      "grad_norm": 0.08628135919570923,
      "learning_rate": 2.8764583333333332e-05,
      "loss": 0.0018,
      "step": 101930
    },
    {
      "epoch": 3.398,
      "grad_norm": 0.17182935774326324,
      "learning_rate": 2.8762500000000005e-05,
      "loss": 0.0015,
      "step": 101940
    },
    {
      "epoch": 3.3983333333333334,
      "grad_norm": 0.34342533349990845,
      "learning_rate": 2.8760416666666667e-05,
      "loss": 0.0019,
      "step": 101950
    },
    {
      "epoch": 3.3986666666666667,
      "grad_norm": 0.20055492222309113,
      "learning_rate": 2.8758333333333336e-05,
      "loss": 0.0018,
      "step": 101960
    },
    {
      "epoch": 3.399,
      "grad_norm": 0.25813427567481995,
      "learning_rate": 2.875625e-05,
      "loss": 0.002,
      "step": 101970
    },
    {
      "epoch": 3.3993333333333333,
      "grad_norm": 0.2868061363697052,
      "learning_rate": 2.8754166666666667e-05,
      "loss": 0.0014,
      "step": 101980
    },
    {
      "epoch": 3.3996666666666666,
      "grad_norm": 0.1148604229092598,
      "learning_rate": 2.8752083333333335e-05,
      "loss": 0.0026,
      "step": 101990
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.058740291744470596,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.002,
      "step": 102000
    },
    {
      "epoch": 3.400333333333333,
      "grad_norm": 0.28751495480537415,
      "learning_rate": 2.874791666666667e-05,
      "loss": 0.002,
      "step": 102010
    },
    {
      "epoch": 3.4006666666666665,
      "grad_norm": 0.017401086166501045,
      "learning_rate": 2.8745833333333332e-05,
      "loss": 0.0018,
      "step": 102020
    },
    {
      "epoch": 3.401,
      "grad_norm": 0.5453025698661804,
      "learning_rate": 2.8743750000000004e-05,
      "loss": 0.0034,
      "step": 102030
    },
    {
      "epoch": 3.4013333333333335,
      "grad_norm": 0.33711063861846924,
      "learning_rate": 2.8741666666666666e-05,
      "loss": 0.0029,
      "step": 102040
    },
    {
      "epoch": 3.401666666666667,
      "grad_norm": 0.14345543086528778,
      "learning_rate": 2.8739583333333335e-05,
      "loss": 0.002,
      "step": 102050
    },
    {
      "epoch": 3.402,
      "grad_norm": 0.22902388870716095,
      "learning_rate": 2.87375e-05,
      "loss": 0.0024,
      "step": 102060
    },
    {
      "epoch": 3.4023333333333334,
      "grad_norm": 0.003403598675504327,
      "learning_rate": 2.873541666666667e-05,
      "loss": 0.0026,
      "step": 102070
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.02883107028901577,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0028,
      "step": 102080
    },
    {
      "epoch": 3.403,
      "grad_norm": 0.20091180503368378,
      "learning_rate": 2.8731250000000004e-05,
      "loss": 0.0024,
      "step": 102090
    },
    {
      "epoch": 3.4033333333333333,
      "grad_norm": 0.14324334263801575,
      "learning_rate": 2.872916666666667e-05,
      "loss": 0.0026,
      "step": 102100
    },
    {
      "epoch": 3.4036666666666666,
      "grad_norm": 0.11443252116441727,
      "learning_rate": 2.872708333333333e-05,
      "loss": 0.0031,
      "step": 102110
    },
    {
      "epoch": 3.404,
      "grad_norm": 0.030587660148739815,
      "learning_rate": 2.8725e-05,
      "loss": 0.0028,
      "step": 102120
    },
    {
      "epoch": 3.404333333333333,
      "grad_norm": 0.02960405871272087,
      "learning_rate": 2.8722916666666666e-05,
      "loss": 0.0022,
      "step": 102130
    },
    {
      "epoch": 3.4046666666666665,
      "grad_norm": 0.08629559725522995,
      "learning_rate": 2.8720833333333335e-05,
      "loss": 0.0025,
      "step": 102140
    },
    {
      "epoch": 3.4050000000000002,
      "grad_norm": 0.03082301653921604,
      "learning_rate": 2.871875e-05,
      "loss": 0.0031,
      "step": 102150
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.11440388113260269,
      "learning_rate": 2.871666666666667e-05,
      "loss": 0.0021,
      "step": 102160
    },
    {
      "epoch": 3.405666666666667,
      "grad_norm": 0.031039658933877945,
      "learning_rate": 2.8714583333333334e-05,
      "loss": 0.0026,
      "step": 102170
    },
    {
      "epoch": 3.406,
      "grad_norm": 0.3152203857898712,
      "learning_rate": 2.8712500000000003e-05,
      "loss": 0.0022,
      "step": 102180
    },
    {
      "epoch": 3.4063333333333334,
      "grad_norm": 0.28668496012687683,
      "learning_rate": 2.871041666666667e-05,
      "loss": 0.0025,
      "step": 102190
    },
    {
      "epoch": 3.4066666666666667,
      "grad_norm": 0.20047622919082642,
      "learning_rate": 2.8708333333333338e-05,
      "loss": 0.0024,
      "step": 102200
    },
    {
      "epoch": 3.407,
      "grad_norm": 0.05759203061461449,
      "learning_rate": 2.870625e-05,
      "loss": 0.0017,
      "step": 102210
    },
    {
      "epoch": 3.4073333333333333,
      "grad_norm": 0.37231889367103577,
      "learning_rate": 2.8704166666666665e-05,
      "loss": 0.0029,
      "step": 102220
    },
    {
      "epoch": 3.4076666666666666,
      "grad_norm": 0.05840180441737175,
      "learning_rate": 2.8702083333333334e-05,
      "loss": 0.0023,
      "step": 102230
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.1716868132352829,
      "learning_rate": 2.87e-05,
      "loss": 0.0018,
      "step": 102240
    },
    {
      "epoch": 3.408333333333333,
      "grad_norm": 0.057772547006607056,
      "learning_rate": 2.869791666666667e-05,
      "loss": 0.0023,
      "step": 102250
    },
    {
      "epoch": 3.4086666666666665,
      "grad_norm": 0.02958107925951481,
      "learning_rate": 2.8695833333333334e-05,
      "loss": 0.0018,
      "step": 102260
    },
    {
      "epoch": 3.409,
      "grad_norm": 0.05808059498667717,
      "learning_rate": 2.8693750000000003e-05,
      "loss": 0.0027,
      "step": 102270
    },
    {
      "epoch": 3.4093333333333335,
      "grad_norm": 0.37257421016693115,
      "learning_rate": 2.869166666666667e-05,
      "loss": 0.0025,
      "step": 102280
    },
    {
      "epoch": 3.409666666666667,
      "grad_norm": 0.14338842034339905,
      "learning_rate": 2.8689583333333337e-05,
      "loss": 0.0027,
      "step": 102290
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.23172536492347717,
      "learning_rate": 2.86875e-05,
      "loss": 0.0023,
      "step": 102300
    },
    {
      "epoch": 3.4103333333333334,
      "grad_norm": 0.17211757600307465,
      "learning_rate": 2.868541666666667e-05,
      "loss": 0.0028,
      "step": 102310
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.14353887736797333,
      "learning_rate": 2.8683333333333334e-05,
      "loss": 0.0022,
      "step": 102320
    },
    {
      "epoch": 3.411,
      "grad_norm": 0.2579737901687622,
      "learning_rate": 2.8681250000000003e-05,
      "loss": 0.0018,
      "step": 102330
    },
    {
      "epoch": 3.4113333333333333,
      "grad_norm": 0.1718462109565735,
      "learning_rate": 2.8679166666666668e-05,
      "loss": 0.0023,
      "step": 102340
    },
    {
      "epoch": 3.4116666666666666,
      "grad_norm": 0.25799962878227234,
      "learning_rate": 2.8677083333333334e-05,
      "loss": 0.0029,
      "step": 102350
    },
    {
      "epoch": 3.412,
      "grad_norm": 0.3436836898326874,
      "learning_rate": 2.8675000000000002e-05,
      "loss": 0.002,
      "step": 102360
    },
    {
      "epoch": 3.412333333333333,
      "grad_norm": 0.2579249441623688,
      "learning_rate": 2.8672916666666665e-05,
      "loss": 0.0027,
      "step": 102370
    },
    {
      "epoch": 3.4126666666666665,
      "grad_norm": 0.1147296354174614,
      "learning_rate": 2.8670833333333337e-05,
      "loss": 0.0031,
      "step": 102380
    },
    {
      "epoch": 3.413,
      "grad_norm": 0.17225830256938934,
      "learning_rate": 2.866875e-05,
      "loss": 0.0025,
      "step": 102390
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.22925475239753723,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0018,
      "step": 102400
    },
    {
      "epoch": 3.413666666666667,
      "grad_norm": 0.006821304559707642,
      "learning_rate": 2.8664583333333333e-05,
      "loss": 0.0018,
      "step": 102410
    },
    {
      "epoch": 3.414,
      "grad_norm": 0.11488653719425201,
      "learning_rate": 2.8662500000000002e-05,
      "loss": 0.0018,
      "step": 102420
    },
    {
      "epoch": 3.4143333333333334,
      "grad_norm": 0.02913300320506096,
      "learning_rate": 2.8660416666666668e-05,
      "loss": 0.0036,
      "step": 102430
    },
    {
      "epoch": 3.4146666666666667,
      "grad_norm": 0.3437274992465973,
      "learning_rate": 2.8658333333333336e-05,
      "loss": 0.0021,
      "step": 102440
    },
    {
      "epoch": 3.415,
      "grad_norm": 0.11462756246328354,
      "learning_rate": 2.8656250000000002e-05,
      "loss": 0.0022,
      "step": 102450
    },
    {
      "epoch": 3.4153333333333333,
      "grad_norm": 0.20031031966209412,
      "learning_rate": 2.8654166666666664e-05,
      "loss": 0.0018,
      "step": 102460
    },
    {
      "epoch": 3.4156666666666666,
      "grad_norm": 0.458291620016098,
      "learning_rate": 2.8652083333333336e-05,
      "loss": 0.003,
      "step": 102470
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.029367592185735703,
      "learning_rate": 2.865e-05,
      "loss": 0.0031,
      "step": 102480
    },
    {
      "epoch": 3.416333333333333,
      "grad_norm": 0.11471922695636749,
      "learning_rate": 2.8647916666666667e-05,
      "loss": 0.0012,
      "step": 102490
    },
    {
      "epoch": 3.4166666666666665,
      "grad_norm": 0.0860782191157341,
      "learning_rate": 2.8645833333333333e-05,
      "loss": 0.0022,
      "step": 102500
    },
    {
      "epoch": 3.417,
      "grad_norm": 0.22910025715827942,
      "learning_rate": 2.864375e-05,
      "loss": 0.0023,
      "step": 102510
    },
    {
      "epoch": 3.4173333333333336,
      "grad_norm": 0.5064727067947388,
      "learning_rate": 2.8641666666666667e-05,
      "loss": 0.0028,
      "step": 102520
    },
    {
      "epoch": 3.417666666666667,
      "grad_norm": 0.1721293181180954,
      "learning_rate": 2.8639583333333336e-05,
      "loss": 0.0023,
      "step": 102530
    },
    {
      "epoch": 3.418,
      "grad_norm": 0.02898765727877617,
      "learning_rate": 2.86375e-05,
      "loss": 0.0018,
      "step": 102540
    },
    {
      "epoch": 3.4183333333333334,
      "grad_norm": 0.5729488134384155,
      "learning_rate": 2.863541666666667e-05,
      "loss": 0.0029,
      "step": 102550
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.22940392792224884,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.0039,
      "step": 102560
    },
    {
      "epoch": 3.419,
      "grad_norm": 0.0861944854259491,
      "learning_rate": 2.8631250000000005e-05,
      "loss": 0.0024,
      "step": 102570
    },
    {
      "epoch": 3.4193333333333333,
      "grad_norm": 0.11472614854574203,
      "learning_rate": 2.8629166666666667e-05,
      "loss": 0.0023,
      "step": 102580
    },
    {
      "epoch": 3.4196666666666666,
      "grad_norm": 0.11504647135734558,
      "learning_rate": 2.8627083333333332e-05,
      "loss": 0.0018,
      "step": 102590
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.05820037052035332,
      "learning_rate": 2.8625e-05,
      "loss": 0.0035,
      "step": 102600
    },
    {
      "epoch": 3.4203333333333332,
      "grad_norm": 0.42971810698509216,
      "learning_rate": 2.8622916666666667e-05,
      "loss": 0.0014,
      "step": 102610
    },
    {
      "epoch": 3.4206666666666665,
      "grad_norm": 0.031154917553067207,
      "learning_rate": 2.8620833333333336e-05,
      "loss": 0.0018,
      "step": 102620
    },
    {
      "epoch": 3.421,
      "grad_norm": 0.37243255972862244,
      "learning_rate": 2.861875e-05,
      "loss": 0.003,
      "step": 102630
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.28658756613731384,
      "learning_rate": 2.861666666666667e-05,
      "loss": 0.002,
      "step": 102640
    },
    {
      "epoch": 3.421666666666667,
      "grad_norm": 0.25770431756973267,
      "learning_rate": 2.8614583333333332e-05,
      "loss": 0.002,
      "step": 102650
    },
    {
      "epoch": 3.422,
      "grad_norm": 0.4584336578845978,
      "learning_rate": 2.8612500000000004e-05,
      "loss": 0.0017,
      "step": 102660
    },
    {
      "epoch": 3.4223333333333334,
      "grad_norm": 0.0067050522193312645,
      "learning_rate": 2.8610416666666666e-05,
      "loss": 0.0025,
      "step": 102670
    },
    {
      "epoch": 3.4226666666666667,
      "grad_norm": 0.029058780521154404,
      "learning_rate": 2.860833333333334e-05,
      "loss": 0.0025,
      "step": 102680
    },
    {
      "epoch": 3.423,
      "grad_norm": 0.20073099434375763,
      "learning_rate": 2.860625e-05,
      "loss": 0.0023,
      "step": 102690
    },
    {
      "epoch": 3.4233333333333333,
      "grad_norm": 0.4580753445625305,
      "learning_rate": 2.860416666666667e-05,
      "loss": 0.0019,
      "step": 102700
    },
    {
      "epoch": 3.4236666666666666,
      "grad_norm": 0.17199182510375977,
      "learning_rate": 2.8602083333333335e-05,
      "loss": 0.0026,
      "step": 102710
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.0578349344432354,
      "learning_rate": 2.86e-05,
      "loss": 0.0027,
      "step": 102720
    },
    {
      "epoch": 3.4243333333333332,
      "grad_norm": 0.02897716499865055,
      "learning_rate": 2.859791666666667e-05,
      "loss": 0.0019,
      "step": 102730
    },
    {
      "epoch": 3.4246666666666665,
      "grad_norm": 0.14348965883255005,
      "learning_rate": 2.859583333333333e-05,
      "loss": 0.0021,
      "step": 102740
    },
    {
      "epoch": 3.425,
      "grad_norm": 0.1147235631942749,
      "learning_rate": 2.8593750000000004e-05,
      "loss": 0.0022,
      "step": 102750
    },
    {
      "epoch": 3.4253333333333336,
      "grad_norm": 0.11477615684270859,
      "learning_rate": 2.8591666666666666e-05,
      "loss": 0.0013,
      "step": 102760
    },
    {
      "epoch": 3.425666666666667,
      "grad_norm": 0.08606784045696259,
      "learning_rate": 2.8589583333333335e-05,
      "loss": 0.0015,
      "step": 102770
    },
    {
      "epoch": 3.426,
      "grad_norm": 0.17217053472995758,
      "learning_rate": 2.85875e-05,
      "loss": 0.0023,
      "step": 102780
    },
    {
      "epoch": 3.4263333333333335,
      "grad_norm": 0.22922205924987793,
      "learning_rate": 2.858541666666667e-05,
      "loss": 0.0027,
      "step": 102790
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.14349780976772308,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.0029,
      "step": 102800
    },
    {
      "epoch": 3.427,
      "grad_norm": 0.057860467582941055,
      "learning_rate": 2.8581250000000004e-05,
      "loss": 0.0024,
      "step": 102810
    },
    {
      "epoch": 3.4273333333333333,
      "grad_norm": 0.2580776512622833,
      "learning_rate": 2.857916666666667e-05,
      "loss": 0.0031,
      "step": 102820
    },
    {
      "epoch": 3.4276666666666666,
      "grad_norm": 0.42018377780914307,
      "learning_rate": 2.857708333333333e-05,
      "loss": 0.002,
      "step": 102830
    },
    {
      "epoch": 3.428,
      "grad_norm": 0.2866189479827881,
      "learning_rate": 2.8575000000000003e-05,
      "loss": 0.0023,
      "step": 102840
    },
    {
      "epoch": 3.4283333333333332,
      "grad_norm": 0.057642027735710144,
      "learning_rate": 2.8572916666666665e-05,
      "loss": 0.0021,
      "step": 102850
    },
    {
      "epoch": 3.4286666666666665,
      "grad_norm": 0.004578446503728628,
      "learning_rate": 2.8570833333333334e-05,
      "loss": 0.0022,
      "step": 102860
    },
    {
      "epoch": 3.429,
      "grad_norm": 0.5443494319915771,
      "learning_rate": 2.856875e-05,
      "loss": 0.002,
      "step": 102870
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.17233212292194366,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0024,
      "step": 102880
    },
    {
      "epoch": 3.429666666666667,
      "grad_norm": 0.1146441251039505,
      "learning_rate": 2.8564583333333334e-05,
      "loss": 0.0022,
      "step": 102890
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.14330600202083588,
      "learning_rate": 2.8562500000000003e-05,
      "loss": 0.0022,
      "step": 102900
    },
    {
      "epoch": 3.4303333333333335,
      "grad_norm": 0.029840832576155663,
      "learning_rate": 2.856041666666667e-05,
      "loss": 0.002,
      "step": 102910
    },
    {
      "epoch": 3.4306666666666668,
      "grad_norm": 0.028691614046692848,
      "learning_rate": 2.8558333333333337e-05,
      "loss": 0.0023,
      "step": 102920
    },
    {
      "epoch": 3.431,
      "grad_norm": 0.20061653852462769,
      "learning_rate": 2.855625e-05,
      "loss": 0.0022,
      "step": 102930
    },
    {
      "epoch": 3.4313333333333333,
      "grad_norm": 0.003360521048307419,
      "learning_rate": 2.8554166666666672e-05,
      "loss": 0.0014,
      "step": 102940
    },
    {
      "epoch": 3.4316666666666666,
      "grad_norm": 0.34373828768730164,
      "learning_rate": 2.8552083333333334e-05,
      "loss": 0.0028,
      "step": 102950
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.0294183362275362,
      "learning_rate": 2.855e-05,
      "loss": 0.0018,
      "step": 102960
    },
    {
      "epoch": 3.4323333333333332,
      "grad_norm": 0.20038393139839172,
      "learning_rate": 2.8547916666666668e-05,
      "loss": 0.0018,
      "step": 102970
    },
    {
      "epoch": 3.4326666666666665,
      "grad_norm": 0.4012947380542755,
      "learning_rate": 2.8545833333333334e-05,
      "loss": 0.0017,
      "step": 102980
    },
    {
      "epoch": 3.433,
      "grad_norm": 0.5757794380187988,
      "learning_rate": 2.8543750000000003e-05,
      "loss": 0.0016,
      "step": 102990
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.004258931148797274,
      "learning_rate": 2.8541666666666668e-05,
      "loss": 0.0019,
      "step": 103000
    },
    {
      "epoch": 3.4336666666666664,
      "grad_norm": 0.11467970907688141,
      "learning_rate": 2.8539583333333337e-05,
      "loss": 0.0019,
      "step": 103010
    },
    {
      "epoch": 3.434,
      "grad_norm": 0.2864396572113037,
      "learning_rate": 2.85375e-05,
      "loss": 0.0018,
      "step": 103020
    },
    {
      "epoch": 3.4343333333333335,
      "grad_norm": 0.057454563677310944,
      "learning_rate": 2.853541666666667e-05,
      "loss": 0.0026,
      "step": 103030
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.20057499408721924,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0023,
      "step": 103040
    },
    {
      "epoch": 3.435,
      "grad_norm": 0.14393749833106995,
      "learning_rate": 2.8531250000000002e-05,
      "loss": 0.0015,
      "step": 103050
    },
    {
      "epoch": 3.4353333333333333,
      "grad_norm": 0.08655066788196564,
      "learning_rate": 2.8529166666666668e-05,
      "loss": 0.0029,
      "step": 103060
    },
    {
      "epoch": 3.4356666666666666,
      "grad_norm": 0.37237071990966797,
      "learning_rate": 2.8527083333333333e-05,
      "loss": 0.0024,
      "step": 103070
    },
    {
      "epoch": 3.436,
      "grad_norm": 0.1718377321958542,
      "learning_rate": 2.8525000000000002e-05,
      "loss": 0.0022,
      "step": 103080
    },
    {
      "epoch": 3.4363333333333332,
      "grad_norm": 0.2864520847797394,
      "learning_rate": 2.8522916666666664e-05,
      "loss": 0.0016,
      "step": 103090
    },
    {
      "epoch": 3.4366666666666665,
      "grad_norm": 0.14323483407497406,
      "learning_rate": 2.8520833333333337e-05,
      "loss": 0.0026,
      "step": 103100
    },
    {
      "epoch": 3.437,
      "grad_norm": 0.02890896238386631,
      "learning_rate": 2.851875e-05,
      "loss": 0.0025,
      "step": 103110
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.11463996022939682,
      "learning_rate": 2.851666666666667e-05,
      "loss": 0.0015,
      "step": 103120
    },
    {
      "epoch": 3.437666666666667,
      "grad_norm": 0.0576554499566555,
      "learning_rate": 2.8514583333333333e-05,
      "loss": 0.0023,
      "step": 103130
    },
    {
      "epoch": 3.438,
      "grad_norm": 0.08596599102020264,
      "learning_rate": 2.8512500000000002e-05,
      "loss": 0.0018,
      "step": 103140
    },
    {
      "epoch": 3.4383333333333335,
      "grad_norm": 0.14370889961719513,
      "learning_rate": 2.8510416666666667e-05,
      "loss": 0.0021,
      "step": 103150
    },
    {
      "epoch": 3.4386666666666668,
      "grad_norm": 0.2578032314777374,
      "learning_rate": 2.8508333333333336e-05,
      "loss": 0.0018,
      "step": 103160
    },
    {
      "epoch": 3.439,
      "grad_norm": 0.2866169214248657,
      "learning_rate": 2.8506250000000002e-05,
      "loss": 0.0021,
      "step": 103170
    },
    {
      "epoch": 3.4393333333333334,
      "grad_norm": 0.34356889128685,
      "learning_rate": 2.850416666666667e-05,
      "loss": 0.0024,
      "step": 103180
    },
    {
      "epoch": 3.4396666666666667,
      "grad_norm": 0.0037621567025780678,
      "learning_rate": 2.8502083333333336e-05,
      "loss": 0.002,
      "step": 103190
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.058029524981975555,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.002,
      "step": 103200
    },
    {
      "epoch": 3.4403333333333332,
      "grad_norm": 0.2865481376647949,
      "learning_rate": 2.8497916666666667e-05,
      "loss": 0.0015,
      "step": 103210
    },
    {
      "epoch": 3.4406666666666665,
      "grad_norm": 0.08611230552196503,
      "learning_rate": 2.8495833333333333e-05,
      "loss": 0.002,
      "step": 103220
    },
    {
      "epoch": 3.441,
      "grad_norm": 0.14323154091835022,
      "learning_rate": 2.849375e-05,
      "loss": 0.0031,
      "step": 103230
    },
    {
      "epoch": 3.4413333333333336,
      "grad_norm": 0.05778403580188751,
      "learning_rate": 2.8491666666666667e-05,
      "loss": 0.002,
      "step": 103240
    },
    {
      "epoch": 3.4416666666666664,
      "grad_norm": 0.05771176144480705,
      "learning_rate": 2.8489583333333336e-05,
      "loss": 0.0027,
      "step": 103250
    },
    {
      "epoch": 3.442,
      "grad_norm": 0.029083915054798126,
      "learning_rate": 2.84875e-05,
      "loss": 0.0015,
      "step": 103260
    },
    {
      "epoch": 3.4423333333333335,
      "grad_norm": 0.3436918556690216,
      "learning_rate": 2.848541666666667e-05,
      "loss": 0.0027,
      "step": 103270
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.31513482332229614,
      "learning_rate": 2.8483333333333336e-05,
      "loss": 0.0014,
      "step": 103280
    },
    {
      "epoch": 3.443,
      "grad_norm": 0.1719488501548767,
      "learning_rate": 2.8481250000000005e-05,
      "loss": 0.002,
      "step": 103290
    },
    {
      "epoch": 3.4433333333333334,
      "grad_norm": 0.11452531814575195,
      "learning_rate": 2.8479166666666667e-05,
      "loss": 0.0018,
      "step": 103300
    },
    {
      "epoch": 3.4436666666666667,
      "grad_norm": 0.029346171766519547,
      "learning_rate": 2.8477083333333332e-05,
      "loss": 0.0026,
      "step": 103310
    },
    {
      "epoch": 3.444,
      "grad_norm": 0.20046597719192505,
      "learning_rate": 2.8475e-05,
      "loss": 0.0022,
      "step": 103320
    },
    {
      "epoch": 3.4443333333333332,
      "grad_norm": 0.4988188147544861,
      "learning_rate": 2.8472916666666666e-05,
      "loss": 0.0035,
      "step": 103330
    },
    {
      "epoch": 3.4446666666666665,
      "grad_norm": 0.029980817809700966,
      "learning_rate": 2.8470833333333335e-05,
      "loss": 0.0064,
      "step": 103340
    },
    {
      "epoch": 3.445,
      "grad_norm": 0.2865155041217804,
      "learning_rate": 2.846875e-05,
      "loss": 0.0021,
      "step": 103350
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.028975091874599457,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0018,
      "step": 103360
    },
    {
      "epoch": 3.445666666666667,
      "grad_norm": 0.343557208776474,
      "learning_rate": 2.8464583333333332e-05,
      "loss": 0.0028,
      "step": 103370
    },
    {
      "epoch": 3.446,
      "grad_norm": 0.4523896872997284,
      "learning_rate": 2.8462500000000004e-05,
      "loss": 0.0033,
      "step": 103380
    },
    {
      "epoch": 3.4463333333333335,
      "grad_norm": 0.1146143227815628,
      "learning_rate": 2.8460416666666666e-05,
      "loss": 0.0032,
      "step": 103390
    },
    {
      "epoch": 3.4466666666666668,
      "grad_norm": 0.31499841809272766,
      "learning_rate": 2.845833333333334e-05,
      "loss": 0.0014,
      "step": 103400
    },
    {
      "epoch": 3.447,
      "grad_norm": 0.028890075162053108,
      "learning_rate": 2.845625e-05,
      "loss": 0.0024,
      "step": 103410
    },
    {
      "epoch": 3.4473333333333334,
      "grad_norm": 0.343515008687973,
      "learning_rate": 2.845416666666667e-05,
      "loss": 0.0029,
      "step": 103420
    },
    {
      "epoch": 3.4476666666666667,
      "grad_norm": 0.08585897088050842,
      "learning_rate": 2.8452083333333335e-05,
      "loss": 0.0019,
      "step": 103430
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.05750281363725662,
      "learning_rate": 2.845e-05,
      "loss": 0.0035,
      "step": 103440
    },
    {
      "epoch": 3.4483333333333333,
      "grad_norm": 0.2331322878599167,
      "learning_rate": 2.844791666666667e-05,
      "loss": 0.0015,
      "step": 103450
    },
    {
      "epoch": 3.4486666666666665,
      "grad_norm": 0.029700858518481255,
      "learning_rate": 2.844583333333333e-05,
      "loss": 0.0015,
      "step": 103460
    },
    {
      "epoch": 3.449,
      "grad_norm": 0.05793047323822975,
      "learning_rate": 2.8443750000000004e-05,
      "loss": 0.0012,
      "step": 103470
    },
    {
      "epoch": 3.449333333333333,
      "grad_norm": 0.057723239064216614,
      "learning_rate": 2.8441666666666666e-05,
      "loss": 0.0026,
      "step": 103480
    },
    {
      "epoch": 3.4496666666666664,
      "grad_norm": 0.05766846612095833,
      "learning_rate": 2.8439583333333335e-05,
      "loss": 0.0021,
      "step": 103490
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.17168265581130981,
      "learning_rate": 2.84375e-05,
      "loss": 0.0024,
      "step": 103500
    },
    {
      "epoch": 3.4503333333333335,
      "grad_norm": 0.08599525690078735,
      "learning_rate": 2.843541666666667e-05,
      "loss": 0.0021,
      "step": 103510
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.2007058560848236,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.0019,
      "step": 103520
    },
    {
      "epoch": 3.451,
      "grad_norm": 0.20027728378772736,
      "learning_rate": 2.8431250000000003e-05,
      "loss": 0.0017,
      "step": 103530
    },
    {
      "epoch": 3.4513333333333334,
      "grad_norm": 0.11549851298332214,
      "learning_rate": 2.842916666666667e-05,
      "loss": 0.0019,
      "step": 103540
    },
    {
      "epoch": 3.4516666666666667,
      "grad_norm": 0.3045763671398163,
      "learning_rate": 2.842708333333333e-05,
      "loss": 0.0024,
      "step": 103550
    },
    {
      "epoch": 3.452,
      "grad_norm": 0.11454745382070541,
      "learning_rate": 2.8425000000000003e-05,
      "loss": 0.002,
      "step": 103560
    },
    {
      "epoch": 3.4523333333333333,
      "grad_norm": 0.005521297920495272,
      "learning_rate": 2.8422916666666665e-05,
      "loss": 0.0015,
      "step": 103570
    },
    {
      "epoch": 3.4526666666666666,
      "grad_norm": 0.28628334403038025,
      "learning_rate": 2.8420833333333334e-05,
      "loss": 0.0022,
      "step": 103580
    },
    {
      "epoch": 3.453,
      "grad_norm": 0.17177951335906982,
      "learning_rate": 2.841875e-05,
      "loss": 0.0014,
      "step": 103590
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.814014732837677,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.0019,
      "step": 103600
    },
    {
      "epoch": 3.453666666666667,
      "grad_norm": 0.2003202587366104,
      "learning_rate": 2.8414583333333334e-05,
      "loss": 0.0028,
      "step": 103610
    },
    {
      "epoch": 3.454,
      "grad_norm": 0.0864788219332695,
      "learning_rate": 2.8412500000000003e-05,
      "loss": 0.0026,
      "step": 103620
    },
    {
      "epoch": 3.4543333333333335,
      "grad_norm": 0.41309264302253723,
      "learning_rate": 2.841041666666667e-05,
      "loss": 0.0019,
      "step": 103630
    },
    {
      "epoch": 3.4546666666666668,
      "grad_norm": 0.3722230792045593,
      "learning_rate": 2.8408333333333337e-05,
      "loss": 0.0017,
      "step": 103640
    },
    {
      "epoch": 3.455,
      "grad_norm": 0.1145211011171341,
      "learning_rate": 2.840625e-05,
      "loss": 0.0018,
      "step": 103650
    },
    {
      "epoch": 3.4553333333333334,
      "grad_norm": 0.11792641878128052,
      "learning_rate": 2.840416666666667e-05,
      "loss": 0.0026,
      "step": 103660
    },
    {
      "epoch": 3.4556666666666667,
      "grad_norm": 0.11504070460796356,
      "learning_rate": 2.8402083333333334e-05,
      "loss": 0.0036,
      "step": 103670
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.28453510999679565,
      "learning_rate": 2.84e-05,
      "loss": 0.0021,
      "step": 103680
    },
    {
      "epoch": 3.4563333333333333,
      "grad_norm": 0.14312471449375153,
      "learning_rate": 2.8397916666666668e-05,
      "loss": 0.0021,
      "step": 103690
    },
    {
      "epoch": 3.4566666666666666,
      "grad_norm": 0.20035985112190247,
      "learning_rate": 2.8395833333333333e-05,
      "loss": 0.0023,
      "step": 103700
    },
    {
      "epoch": 3.457,
      "grad_norm": 0.48706966638565063,
      "learning_rate": 2.8393750000000002e-05,
      "loss": 0.0021,
      "step": 103710
    },
    {
      "epoch": 3.457333333333333,
      "grad_norm": 0.17190194129943848,
      "learning_rate": 2.8391666666666668e-05,
      "loss": 0.0021,
      "step": 103720
    },
    {
      "epoch": 3.4576666666666664,
      "grad_norm": 0.20062130689620972,
      "learning_rate": 2.8389583333333337e-05,
      "loss": 0.0022,
      "step": 103730
    },
    {
      "epoch": 3.458,
      "grad_norm": 0.9561728835105896,
      "learning_rate": 2.83875e-05,
      "loss": 0.0026,
      "step": 103740
    },
    {
      "epoch": 3.4583333333333335,
      "grad_norm": 0.00517817959189415,
      "learning_rate": 2.838541666666667e-05,
      "loss": 0.003,
      "step": 103750
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.6299626231193542,
      "learning_rate": 2.8383333333333333e-05,
      "loss": 0.0016,
      "step": 103760
    },
    {
      "epoch": 3.459,
      "grad_norm": 0.45823243260383606,
      "learning_rate": 2.8381250000000002e-05,
      "loss": 0.0026,
      "step": 103770
    },
    {
      "epoch": 3.4593333333333334,
      "grad_norm": 0.05733180418610573,
      "learning_rate": 2.8379166666666668e-05,
      "loss": 0.0021,
      "step": 103780
    },
    {
      "epoch": 3.4596666666666667,
      "grad_norm": 0.3437909185886383,
      "learning_rate": 2.8377083333333336e-05,
      "loss": 0.0014,
      "step": 103790
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.057321906089782715,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 0.0028,
      "step": 103800
    },
    {
      "epoch": 3.4603333333333333,
      "grad_norm": 0.34414738416671753,
      "learning_rate": 2.8372916666666664e-05,
      "loss": 0.0025,
      "step": 103810
    },
    {
      "epoch": 3.4606666666666666,
      "grad_norm": 0.08612699806690216,
      "learning_rate": 2.8370833333333336e-05,
      "loss": 0.0018,
      "step": 103820
    },
    {
      "epoch": 3.461,
      "grad_norm": 0.31494536995887756,
      "learning_rate": 2.836875e-05,
      "loss": 0.0025,
      "step": 103830
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.14366832375526428,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.002,
      "step": 103840
    },
    {
      "epoch": 3.461666666666667,
      "grad_norm": 0.007768906652927399,
      "learning_rate": 2.8364583333333333e-05,
      "loss": 0.0029,
      "step": 103850
    },
    {
      "epoch": 3.462,
      "grad_norm": 0.057436831295490265,
      "learning_rate": 2.83625e-05,
      "loss": 0.0021,
      "step": 103860
    },
    {
      "epoch": 3.4623333333333335,
      "grad_norm": 0.5154262781143188,
      "learning_rate": 2.8360416666666667e-05,
      "loss": 0.0029,
      "step": 103870
    },
    {
      "epoch": 3.462666666666667,
      "grad_norm": 0.05756787210702896,
      "learning_rate": 2.8358333333333336e-05,
      "loss": 0.0021,
      "step": 103880
    },
    {
      "epoch": 3.463,
      "grad_norm": 0.11455598473548889,
      "learning_rate": 2.835625e-05,
      "loss": 0.0022,
      "step": 103890
    },
    {
      "epoch": 3.4633333333333334,
      "grad_norm": 0.22916217148303986,
      "learning_rate": 2.835416666666667e-05,
      "loss": 0.0022,
      "step": 103900
    },
    {
      "epoch": 3.4636666666666667,
      "grad_norm": 0.004656610079109669,
      "learning_rate": 2.8352083333333336e-05,
      "loss": 0.0017,
      "step": 103910
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.08682107925415039,
      "learning_rate": 2.8349999999999998e-05,
      "loss": 0.0025,
      "step": 103920
    },
    {
      "epoch": 3.4643333333333333,
      "grad_norm": 0.0044540343806147575,
      "learning_rate": 2.8347916666666667e-05,
      "loss": 0.0027,
      "step": 103930
    },
    {
      "epoch": 3.4646666666666666,
      "grad_norm": 0.0648048147559166,
      "learning_rate": 2.8345833333333332e-05,
      "loss": 0.0024,
      "step": 103940
    },
    {
      "epoch": 3.465,
      "grad_norm": 0.4301038980484009,
      "learning_rate": 2.834375e-05,
      "loss": 0.0027,
      "step": 103950
    },
    {
      "epoch": 3.465333333333333,
      "grad_norm": 0.08609800040721893,
      "learning_rate": 2.8341666666666667e-05,
      "loss": 0.0021,
      "step": 103960
    },
    {
      "epoch": 3.4656666666666665,
      "grad_norm": 0.2862721085548401,
      "learning_rate": 2.8339583333333336e-05,
      "loss": 0.0017,
      "step": 103970
    },
    {
      "epoch": 3.466,
      "grad_norm": 0.08598550409078598,
      "learning_rate": 2.83375e-05,
      "loss": 0.0027,
      "step": 103980
    },
    {
      "epoch": 3.4663333333333335,
      "grad_norm": 0.05731264129281044,
      "learning_rate": 2.833541666666667e-05,
      "loss": 0.0016,
      "step": 103990
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.08613000810146332,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0026,
      "step": 104000
    },
    {
      "epoch": 3.467,
      "grad_norm": 0.40102675557136536,
      "learning_rate": 2.8331250000000004e-05,
      "loss": 0.0019,
      "step": 104010
    },
    {
      "epoch": 3.4673333333333334,
      "grad_norm": 0.17183727025985718,
      "learning_rate": 2.8329166666666666e-05,
      "loss": 0.0033,
      "step": 104020
    },
    {
      "epoch": 3.4676666666666667,
      "grad_norm": 0.08630526065826416,
      "learning_rate": 2.832708333333334e-05,
      "loss": 0.0027,
      "step": 104030
    },
    {
      "epoch": 3.468,
      "grad_norm": 0.6515610218048096,
      "learning_rate": 2.8325e-05,
      "loss": 0.0022,
      "step": 104040
    },
    {
      "epoch": 3.4683333333333333,
      "grad_norm": 0.029000407084822655,
      "learning_rate": 2.8322916666666666e-05,
      "loss": 0.0025,
      "step": 104050
    },
    {
      "epoch": 3.4686666666666666,
      "grad_norm": 0.34362030029296875,
      "learning_rate": 2.8320833333333335e-05,
      "loss": 0.0028,
      "step": 104060
    },
    {
      "epoch": 3.469,
      "grad_norm": 0.22914747893810272,
      "learning_rate": 2.831875e-05,
      "loss": 0.0022,
      "step": 104070
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.005040548741817474,
      "learning_rate": 2.831666666666667e-05,
      "loss": 0.002,
      "step": 104080
    },
    {
      "epoch": 3.469666666666667,
      "grad_norm": 0.17292580008506775,
      "learning_rate": 2.8314583333333335e-05,
      "loss": 0.0025,
      "step": 104090
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 0.20053134858608246,
      "learning_rate": 2.8312500000000004e-05,
      "loss": 0.0033,
      "step": 104100
    },
    {
      "epoch": 3.4703333333333335,
      "grad_norm": 0.05771878361701965,
      "learning_rate": 2.8310416666666666e-05,
      "loss": 0.0023,
      "step": 104110
    },
    {
      "epoch": 3.470666666666667,
      "grad_norm": 0.029272237792611122,
      "learning_rate": 2.8308333333333338e-05,
      "loss": 0.0017,
      "step": 104120
    },
    {
      "epoch": 3.471,
      "grad_norm": 0.05827257037162781,
      "learning_rate": 2.830625e-05,
      "loss": 0.0027,
      "step": 104130
    },
    {
      "epoch": 3.4713333333333334,
      "grad_norm": 0.029869886115193367,
      "learning_rate": 2.830416666666667e-05,
      "loss": 0.0033,
      "step": 104140
    },
    {
      "epoch": 3.4716666666666667,
      "grad_norm": 0.05717005208134651,
      "learning_rate": 2.8302083333333335e-05,
      "loss": 0.0023,
      "step": 104150
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.14344309270381927,
      "learning_rate": 2.83e-05,
      "loss": 0.0018,
      "step": 104160
    },
    {
      "epoch": 3.4723333333333333,
      "grad_norm": 0.0862283781170845,
      "learning_rate": 2.829791666666667e-05,
      "loss": 0.002,
      "step": 104170
    },
    {
      "epoch": 3.4726666666666666,
      "grad_norm": 0.9756911993026733,
      "learning_rate": 2.829583333333333e-05,
      "loss": 0.0027,
      "step": 104180
    },
    {
      "epoch": 3.473,
      "grad_norm": 0.20133814215660095,
      "learning_rate": 2.8293750000000003e-05,
      "loss": 0.0029,
      "step": 104190
    },
    {
      "epoch": 3.473333333333333,
      "grad_norm": 0.22903287410736084,
      "learning_rate": 2.8291666666666665e-05,
      "loss": 0.0026,
      "step": 104200
    },
    {
      "epoch": 3.4736666666666665,
      "grad_norm": 0.08602751046419144,
      "learning_rate": 2.8289583333333334e-05,
      "loss": 0.0016,
      "step": 104210
    },
    {
      "epoch": 3.474,
      "grad_norm": 0.11468235403299332,
      "learning_rate": 2.82875e-05,
      "loss": 0.0014,
      "step": 104220
    },
    {
      "epoch": 3.4743333333333335,
      "grad_norm": 0.2864932119846344,
      "learning_rate": 2.828541666666667e-05,
      "loss": 0.0026,
      "step": 104230
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.1146962121129036,
      "learning_rate": 2.8283333333333334e-05,
      "loss": 0.0028,
      "step": 104240
    },
    {
      "epoch": 3.475,
      "grad_norm": 0.23819012939929962,
      "learning_rate": 2.8281250000000003e-05,
      "loss": 0.0024,
      "step": 104250
    },
    {
      "epoch": 3.4753333333333334,
      "grad_norm": 0.20071685314178467,
      "learning_rate": 2.827916666666667e-05,
      "loss": 0.0018,
      "step": 104260
    },
    {
      "epoch": 3.4756666666666667,
      "grad_norm": 0.17198865115642548,
      "learning_rate": 2.8277083333333337e-05,
      "loss": 0.0022,
      "step": 104270
    },
    {
      "epoch": 3.476,
      "grad_norm": 0.02906734310090542,
      "learning_rate": 2.8275000000000003e-05,
      "loss": 0.0023,
      "step": 104280
    },
    {
      "epoch": 3.4763333333333333,
      "grad_norm": 0.22921867668628693,
      "learning_rate": 2.8272916666666665e-05,
      "loss": 0.0025,
      "step": 104290
    },
    {
      "epoch": 3.4766666666666666,
      "grad_norm": 0.057862937450408936,
      "learning_rate": 2.8270833333333334e-05,
      "loss": 0.0018,
      "step": 104300
    },
    {
      "epoch": 3.477,
      "grad_norm": 0.0863499715924263,
      "learning_rate": 2.826875e-05,
      "loss": 0.0028,
      "step": 104310
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.05746396258473396,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0019,
      "step": 104320
    },
    {
      "epoch": 3.477666666666667,
      "grad_norm": 0.057196374982595444,
      "learning_rate": 2.8264583333333334e-05,
      "loss": 0.0014,
      "step": 104330
    },
    {
      "epoch": 3.4779999999999998,
      "grad_norm": 0.15832707285881042,
      "learning_rate": 2.8262500000000003e-05,
      "loss": 0.0027,
      "step": 104340
    },
    {
      "epoch": 3.4783333333333335,
      "grad_norm": 0.11463987082242966,
      "learning_rate": 2.8260416666666668e-05,
      "loss": 0.0023,
      "step": 104350
    },
    {
      "epoch": 3.478666666666667,
      "grad_norm": 0.20058071613311768,
      "learning_rate": 2.8258333333333337e-05,
      "loss": 0.0017,
      "step": 104360
    },
    {
      "epoch": 3.479,
      "grad_norm": 0.08695491403341293,
      "learning_rate": 2.8256250000000002e-05,
      "loss": 0.0018,
      "step": 104370
    },
    {
      "epoch": 3.4793333333333334,
      "grad_norm": 0.4868831932544708,
      "learning_rate": 2.825416666666667e-05,
      "loss": 0.0028,
      "step": 104380
    },
    {
      "epoch": 3.4796666666666667,
      "grad_norm": 0.42953985929489136,
      "learning_rate": 2.8252083333333333e-05,
      "loss": 0.0016,
      "step": 104390
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.2291804403066635,
      "learning_rate": 2.825e-05,
      "loss": 0.0024,
      "step": 104400
    },
    {
      "epoch": 3.4803333333333333,
      "grad_norm": 0.086600162088871,
      "learning_rate": 2.8247916666666668e-05,
      "loss": 0.0026,
      "step": 104410
    },
    {
      "epoch": 3.4806666666666666,
      "grad_norm": 0.00463749747723341,
      "learning_rate": 2.8245833333333333e-05,
      "loss": 0.0023,
      "step": 104420
    },
    {
      "epoch": 3.481,
      "grad_norm": 0.0291670523583889,
      "learning_rate": 2.8243750000000002e-05,
      "loss": 0.0018,
      "step": 104430
    },
    {
      "epoch": 3.481333333333333,
      "grad_norm": 0.08617087453603745,
      "learning_rate": 2.8241666666666668e-05,
      "loss": 0.0023,
      "step": 104440
    },
    {
      "epoch": 3.4816666666666665,
      "grad_norm": 0.4005359709262848,
      "learning_rate": 2.8239583333333336e-05,
      "loss": 0.0024,
      "step": 104450
    },
    {
      "epoch": 3.482,
      "grad_norm": 0.1719333827495575,
      "learning_rate": 2.82375e-05,
      "loss": 0.0023,
      "step": 104460
    },
    {
      "epoch": 3.4823333333333335,
      "grad_norm": 0.02870543859899044,
      "learning_rate": 2.823541666666667e-05,
      "loss": 0.0026,
      "step": 104470
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.14324742555618286,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0026,
      "step": 104480
    },
    {
      "epoch": 3.483,
      "grad_norm": 0.5154989361763,
      "learning_rate": 2.8231250000000005e-05,
      "loss": 0.0033,
      "step": 104490
    },
    {
      "epoch": 3.4833333333333334,
      "grad_norm": 0.11472392082214355,
      "learning_rate": 2.8229166666666667e-05,
      "loss": 0.0018,
      "step": 104500
    },
    {
      "epoch": 3.4836666666666667,
      "grad_norm": 0.08608847856521606,
      "learning_rate": 2.8227083333333336e-05,
      "loss": 0.0027,
      "step": 104510
    },
    {
      "epoch": 3.484,
      "grad_norm": 0.7178950905799866,
      "learning_rate": 2.8225e-05,
      "loss": 0.0028,
      "step": 104520
    },
    {
      "epoch": 3.4843333333333333,
      "grad_norm": 0.27939045429229736,
      "learning_rate": 2.8222916666666667e-05,
      "loss": 0.0025,
      "step": 104530
    },
    {
      "epoch": 3.4846666666666666,
      "grad_norm": 0.11445985734462738,
      "learning_rate": 2.8220833333333336e-05,
      "loss": 0.0037,
      "step": 104540
    },
    {
      "epoch": 3.485,
      "grad_norm": 0.4586032032966614,
      "learning_rate": 2.8218749999999998e-05,
      "loss": 0.0022,
      "step": 104550
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.08611957728862762,
      "learning_rate": 2.821666666666667e-05,
      "loss": 0.0016,
      "step": 104560
    },
    {
      "epoch": 3.485666666666667,
      "grad_norm": 0.25752195715904236,
      "learning_rate": 2.8214583333333332e-05,
      "loss": 0.0013,
      "step": 104570
    },
    {
      "epoch": 3.4859999999999998,
      "grad_norm": 0.43610525131225586,
      "learning_rate": 2.82125e-05,
      "loss": 0.0024,
      "step": 104580
    },
    {
      "epoch": 3.4863333333333335,
      "grad_norm": 0.31489279866218567,
      "learning_rate": 2.8210416666666667e-05,
      "loss": 0.0029,
      "step": 104590
    },
    {
      "epoch": 3.486666666666667,
      "grad_norm": 0.05769588425755501,
      "learning_rate": 2.8208333333333336e-05,
      "loss": 0.0018,
      "step": 104600
    },
    {
      "epoch": 3.487,
      "grad_norm": 0.14360760152339935,
      "learning_rate": 2.820625e-05,
      "loss": 0.002,
      "step": 104610
    },
    {
      "epoch": 3.4873333333333334,
      "grad_norm": 0.05740392208099365,
      "learning_rate": 2.820416666666667e-05,
      "loss": 0.0021,
      "step": 104620
    },
    {
      "epoch": 3.4876666666666667,
      "grad_norm": 0.002794243162497878,
      "learning_rate": 2.8202083333333336e-05,
      "loss": 0.0024,
      "step": 104630
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.17203781008720398,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0021,
      "step": 104640
    },
    {
      "epoch": 3.4883333333333333,
      "grad_norm": 0.22891241312026978,
      "learning_rate": 2.819791666666667e-05,
      "loss": 0.0018,
      "step": 104650
    },
    {
      "epoch": 3.4886666666666666,
      "grad_norm": 0.05743886157870293,
      "learning_rate": 2.8195833333333332e-05,
      "loss": 0.0029,
      "step": 104660
    },
    {
      "epoch": 3.489,
      "grad_norm": 0.005881444085389376,
      "learning_rate": 2.819375e-05,
      "loss": 0.0022,
      "step": 104670
    },
    {
      "epoch": 3.489333333333333,
      "grad_norm": 0.005040732212364674,
      "learning_rate": 2.8191666666666666e-05,
      "loss": 0.0021,
      "step": 104680
    },
    {
      "epoch": 3.4896666666666665,
      "grad_norm": 0.1429096758365631,
      "learning_rate": 2.8189583333333335e-05,
      "loss": 0.0023,
      "step": 104690
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.3149838447570801,
      "learning_rate": 2.81875e-05,
      "loss": 0.0025,
      "step": 104700
    },
    {
      "epoch": 3.4903333333333335,
      "grad_norm": 0.1719777137041092,
      "learning_rate": 2.818541666666667e-05,
      "loss": 0.0016,
      "step": 104710
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.1150040403008461,
      "learning_rate": 2.8183333333333335e-05,
      "loss": 0.002,
      "step": 104720
    },
    {
      "epoch": 3.491,
      "grad_norm": 0.17237046360969543,
      "learning_rate": 2.8181250000000004e-05,
      "loss": 0.0013,
      "step": 104730
    },
    {
      "epoch": 3.4913333333333334,
      "grad_norm": 0.45827022194862366,
      "learning_rate": 2.8179166666666666e-05,
      "loss": 0.0019,
      "step": 104740
    },
    {
      "epoch": 3.4916666666666667,
      "grad_norm": 0.057555779814720154,
      "learning_rate": 2.817708333333334e-05,
      "loss": 0.0018,
      "step": 104750
    },
    {
      "epoch": 3.492,
      "grad_norm": 0.11491061747074127,
      "learning_rate": 2.8175e-05,
      "loss": 0.0034,
      "step": 104760
    },
    {
      "epoch": 3.4923333333333333,
      "grad_norm": 0.7897899150848389,
      "learning_rate": 2.8172916666666666e-05,
      "loss": 0.0025,
      "step": 104770
    },
    {
      "epoch": 3.4926666666666666,
      "grad_norm": 0.0614626444876194,
      "learning_rate": 2.8170833333333335e-05,
      "loss": 0.0025,
      "step": 104780
    },
    {
      "epoch": 3.493,
      "grad_norm": 0.2473326027393341,
      "learning_rate": 2.816875e-05,
      "loss": 0.003,
      "step": 104790
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.0861939936876297,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.0021,
      "step": 104800
    },
    {
      "epoch": 3.4936666666666665,
      "grad_norm": 0.14326061308383942,
      "learning_rate": 2.8164583333333335e-05,
      "loss": 0.0019,
      "step": 104810
    },
    {
      "epoch": 3.4939999999999998,
      "grad_norm": 0.20050355792045593,
      "learning_rate": 2.8162500000000004e-05,
      "loss": 0.0022,
      "step": 104820
    },
    {
      "epoch": 3.4943333333333335,
      "grad_norm": 0.22908388078212738,
      "learning_rate": 2.8160416666666666e-05,
      "loss": 0.0019,
      "step": 104830
    },
    {
      "epoch": 3.494666666666667,
      "grad_norm": 0.24619926512241364,
      "learning_rate": 2.8158333333333338e-05,
      "loss": 0.0029,
      "step": 104840
    },
    {
      "epoch": 3.495,
      "grad_norm": 0.2321971356868744,
      "learning_rate": 2.815625e-05,
      "loss": 0.0025,
      "step": 104850
    },
    {
      "epoch": 3.4953333333333334,
      "grad_norm": 0.08622893691062927,
      "learning_rate": 2.815416666666667e-05,
      "loss": 0.0029,
      "step": 104860
    },
    {
      "epoch": 3.4956666666666667,
      "grad_norm": 0.1433953493833542,
      "learning_rate": 2.8152083333333334e-05,
      "loss": 0.0028,
      "step": 104870
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.5441623330116272,
      "learning_rate": 2.815e-05,
      "loss": 0.0016,
      "step": 104880
    },
    {
      "epoch": 3.4963333333333333,
      "grad_norm": 0.20048119127750397,
      "learning_rate": 2.814791666666667e-05,
      "loss": 0.0021,
      "step": 104890
    },
    {
      "epoch": 3.4966666666666666,
      "grad_norm": 0.030391506850719452,
      "learning_rate": 2.814583333333333e-05,
      "loss": 0.0026,
      "step": 104900
    },
    {
      "epoch": 3.497,
      "grad_norm": 0.37261223793029785,
      "learning_rate": 2.8143750000000003e-05,
      "loss": 0.0021,
      "step": 104910
    },
    {
      "epoch": 3.497333333333333,
      "grad_norm": 0.229160875082016,
      "learning_rate": 2.8141666666666665e-05,
      "loss": 0.0031,
      "step": 104920
    },
    {
      "epoch": 3.4976666666666665,
      "grad_norm": 0.029297787696123123,
      "learning_rate": 2.8139583333333337e-05,
      "loss": 0.0023,
      "step": 104930
    },
    {
      "epoch": 3.498,
      "grad_norm": 0.14314664900302887,
      "learning_rate": 2.81375e-05,
      "loss": 0.0025,
      "step": 104940
    },
    {
      "epoch": 3.4983333333333335,
      "grad_norm": 0.11458634585142136,
      "learning_rate": 2.813541666666667e-05,
      "loss": 0.0019,
      "step": 104950
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.46033531427383423,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0033,
      "step": 104960
    },
    {
      "epoch": 3.499,
      "grad_norm": 0.086732417345047,
      "learning_rate": 2.8131250000000003e-05,
      "loss": 0.0027,
      "step": 104970
    },
    {
      "epoch": 3.4993333333333334,
      "grad_norm": 0.7864964604377747,
      "learning_rate": 2.8129166666666668e-05,
      "loss": 0.0021,
      "step": 104980
    },
    {
      "epoch": 3.4996666666666667,
      "grad_norm": 0.029903694987297058,
      "learning_rate": 2.8127083333333337e-05,
      "loss": 0.0023,
      "step": 104990
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.05800113454461098,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 0.0027,
      "step": 105000
    },
    {
      "epoch": 3.5003333333333333,
      "grad_norm": 0.08570209890604019,
      "learning_rate": 2.8122916666666665e-05,
      "loss": 0.002,
      "step": 105010
    },
    {
      "epoch": 3.5006666666666666,
      "grad_norm": 0.08634704351425171,
      "learning_rate": 2.8120833333333334e-05,
      "loss": 0.0018,
      "step": 105020
    },
    {
      "epoch": 3.501,
      "grad_norm": 0.842426598072052,
      "learning_rate": 2.811875e-05,
      "loss": 0.0029,
      "step": 105030
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.18454618752002716,
      "learning_rate": 2.8116666666666668e-05,
      "loss": 0.003,
      "step": 105040
    },
    {
      "epoch": 3.501666666666667,
      "grad_norm": 0.14370322227478027,
      "learning_rate": 2.8114583333333333e-05,
      "loss": 0.0021,
      "step": 105050
    },
    {
      "epoch": 3.502,
      "grad_norm": 0.029077619314193726,
      "learning_rate": 2.8112500000000002e-05,
      "loss": 0.002,
      "step": 105060
    },
    {
      "epoch": 3.5023333333333335,
      "grad_norm": 0.17184759676456451,
      "learning_rate": 2.8110416666666668e-05,
      "loss": 0.0017,
      "step": 105070
    },
    {
      "epoch": 3.502666666666667,
      "grad_norm": 0.05782399699091911,
      "learning_rate": 2.8108333333333337e-05,
      "loss": 0.002,
      "step": 105080
    },
    {
      "epoch": 3.503,
      "grad_norm": 0.29921144247055054,
      "learning_rate": 2.8106250000000002e-05,
      "loss": 0.0023,
      "step": 105090
    },
    {
      "epoch": 3.5033333333333334,
      "grad_norm": 0.14305844902992249,
      "learning_rate": 2.810416666666667e-05,
      "loss": 0.0025,
      "step": 105100
    },
    {
      "epoch": 3.5036666666666667,
      "grad_norm": 1.120545744895935,
      "learning_rate": 2.8102083333333333e-05,
      "loss": 0.0022,
      "step": 105110
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.22926154732704163,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.0025,
      "step": 105120
    },
    {
      "epoch": 3.5043333333333333,
      "grad_norm": 0.17194226384162903,
      "learning_rate": 2.8097916666666667e-05,
      "loss": 0.0029,
      "step": 105130
    },
    {
      "epoch": 3.5046666666666666,
      "grad_norm": 0.02906990982592106,
      "learning_rate": 2.8095833333333333e-05,
      "loss": 0.0017,
      "step": 105140
    },
    {
      "epoch": 3.505,
      "grad_norm": 0.029416562989354134,
      "learning_rate": 2.8093750000000002e-05,
      "loss": 0.0021,
      "step": 105150
    },
    {
      "epoch": 3.505333333333333,
      "grad_norm": 0.006812975276261568,
      "learning_rate": 2.8091666666666667e-05,
      "loss": 0.0023,
      "step": 105160
    },
    {
      "epoch": 3.5056666666666665,
      "grad_norm": 0.05752599611878395,
      "learning_rate": 2.8089583333333336e-05,
      "loss": 0.002,
      "step": 105170
    },
    {
      "epoch": 3.5060000000000002,
      "grad_norm": 0.2581177353858948,
      "learning_rate": 2.80875e-05,
      "loss": 0.0022,
      "step": 105180
    },
    {
      "epoch": 3.506333333333333,
      "grad_norm": 0.3437308371067047,
      "learning_rate": 2.808541666666667e-05,
      "loss": 0.0022,
      "step": 105190
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.5263729691505432,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.0025,
      "step": 105200
    },
    {
      "epoch": 3.507,
      "grad_norm": 0.08782327175140381,
      "learning_rate": 2.8081250000000005e-05,
      "loss": 0.0018,
      "step": 105210
    },
    {
      "epoch": 3.5073333333333334,
      "grad_norm": 0.029621992260217667,
      "learning_rate": 2.8079166666666667e-05,
      "loss": 0.0019,
      "step": 105220
    },
    {
      "epoch": 3.5076666666666667,
      "grad_norm": 0.1720060557126999,
      "learning_rate": 2.8077083333333336e-05,
      "loss": 0.0024,
      "step": 105230
    },
    {
      "epoch": 3.508,
      "grad_norm": 0.2007954716682434,
      "learning_rate": 2.8075e-05,
      "loss": 0.0016,
      "step": 105240
    },
    {
      "epoch": 3.5083333333333333,
      "grad_norm": 0.0862003043293953,
      "learning_rate": 2.8072916666666667e-05,
      "loss": 0.0034,
      "step": 105250
    },
    {
      "epoch": 3.5086666666666666,
      "grad_norm": 0.11475742608308792,
      "learning_rate": 2.8070833333333336e-05,
      "loss": 0.0016,
      "step": 105260
    },
    {
      "epoch": 3.509,
      "grad_norm": 0.17170564830303192,
      "learning_rate": 2.8068749999999998e-05,
      "loss": 0.0025,
      "step": 105270
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.029564259573817253,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0027,
      "step": 105280
    },
    {
      "epoch": 3.509666666666667,
      "grad_norm": 0.31490111351013184,
      "learning_rate": 2.8064583333333332e-05,
      "loss": 0.0024,
      "step": 105290
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.14443561434745789,
      "learning_rate": 2.80625e-05,
      "loss": 0.0016,
      "step": 105300
    },
    {
      "epoch": 3.5103333333333335,
      "grad_norm": 0.01427003089338541,
      "learning_rate": 2.8060416666666667e-05,
      "loss": 0.0017,
      "step": 105310
    },
    {
      "epoch": 3.510666666666667,
      "grad_norm": 0.058305688202381134,
      "learning_rate": 2.8058333333333335e-05,
      "loss": 0.0022,
      "step": 105320
    },
    {
      "epoch": 3.511,
      "grad_norm": 0.0863446295261383,
      "learning_rate": 2.805625e-05,
      "loss": 0.0016,
      "step": 105330
    },
    {
      "epoch": 3.5113333333333334,
      "grad_norm": 0.3148462176322937,
      "learning_rate": 2.805416666666667e-05,
      "loss": 0.0025,
      "step": 105340
    },
    {
      "epoch": 3.5116666666666667,
      "grad_norm": 0.08580593764781952,
      "learning_rate": 2.8052083333333335e-05,
      "loss": 0.0023,
      "step": 105350
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.2862248718738556,
      "learning_rate": 2.8050000000000004e-05,
      "loss": 0.0015,
      "step": 105360
    },
    {
      "epoch": 3.5123333333333333,
      "grad_norm": 0.0573294535279274,
      "learning_rate": 2.804791666666667e-05,
      "loss": 0.0025,
      "step": 105370
    },
    {
      "epoch": 3.5126666666666666,
      "grad_norm": 0.4007050096988678,
      "learning_rate": 2.8045833333333332e-05,
      "loss": 0.0026,
      "step": 105380
    },
    {
      "epoch": 3.513,
      "grad_norm": 0.25776124000549316,
      "learning_rate": 2.804375e-05,
      "loss": 0.0037,
      "step": 105390
    },
    {
      "epoch": 3.513333333333333,
      "grad_norm": 0.029281694442033768,
      "learning_rate": 2.8041666666666666e-05,
      "loss": 0.0023,
      "step": 105400
    },
    {
      "epoch": 3.5136666666666665,
      "grad_norm": 0.3720237612724304,
      "learning_rate": 2.8039583333333335e-05,
      "loss": 0.0025,
      "step": 105410
    },
    {
      "epoch": 3.5140000000000002,
      "grad_norm": 0.17155112326145172,
      "learning_rate": 2.80375e-05,
      "loss": 0.0018,
      "step": 105420
    },
    {
      "epoch": 3.514333333333333,
      "grad_norm": 0.24045145511627197,
      "learning_rate": 2.803541666666667e-05,
      "loss": 0.0021,
      "step": 105430
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.05770556628704071,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0026,
      "step": 105440
    },
    {
      "epoch": 3.515,
      "grad_norm": 0.05752548202872276,
      "learning_rate": 2.8031250000000004e-05,
      "loss": 0.0022,
      "step": 105450
    },
    {
      "epoch": 3.5153333333333334,
      "grad_norm": 0.39195677638053894,
      "learning_rate": 2.8029166666666666e-05,
      "loss": 0.0021,
      "step": 105460
    },
    {
      "epoch": 3.5156666666666667,
      "grad_norm": 0.06024894490838051,
      "learning_rate": 2.8027083333333338e-05,
      "loss": 0.0025,
      "step": 105470
    },
    {
      "epoch": 3.516,
      "grad_norm": 0.05808686465024948,
      "learning_rate": 2.8025e-05,
      "loss": 0.0018,
      "step": 105480
    },
    {
      "epoch": 3.5163333333333333,
      "grad_norm": 0.0865730345249176,
      "learning_rate": 2.8022916666666666e-05,
      "loss": 0.0022,
      "step": 105490
    },
    {
      "epoch": 3.5166666666666666,
      "grad_norm": 0.05774051696062088,
      "learning_rate": 2.8020833333333335e-05,
      "loss": 0.0021,
      "step": 105500
    },
    {
      "epoch": 3.517,
      "grad_norm": 0.17181432247161865,
      "learning_rate": 2.801875e-05,
      "loss": 0.0022,
      "step": 105510
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.02886730246245861,
      "learning_rate": 2.801666666666667e-05,
      "loss": 0.0032,
      "step": 105520
    },
    {
      "epoch": 3.517666666666667,
      "grad_norm": 0.17207613587379456,
      "learning_rate": 2.8014583333333334e-05,
      "loss": 0.0024,
      "step": 105530
    },
    {
      "epoch": 3.518,
      "grad_norm": 0.5143613815307617,
      "learning_rate": 2.8012500000000003e-05,
      "loss": 0.0029,
      "step": 105540
    },
    {
      "epoch": 3.5183333333333335,
      "grad_norm": 0.2863401770591736,
      "learning_rate": 2.8010416666666665e-05,
      "loss": 0.0019,
      "step": 105550
    },
    {
      "epoch": 3.518666666666667,
      "grad_norm": 0.08617545664310455,
      "learning_rate": 2.8008333333333338e-05,
      "loss": 0.0029,
      "step": 105560
    },
    {
      "epoch": 3.519,
      "grad_norm": 0.02955189347267151,
      "learning_rate": 2.800625e-05,
      "loss": 0.0018,
      "step": 105570
    },
    {
      "epoch": 3.5193333333333334,
      "grad_norm": 0.25770649313926697,
      "learning_rate": 2.800416666666667e-05,
      "loss": 0.0022,
      "step": 105580
    },
    {
      "epoch": 3.5196666666666667,
      "grad_norm": 0.008782011456787586,
      "learning_rate": 2.8002083333333334e-05,
      "loss": 0.0022,
      "step": 105590
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.12895195186138153,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0025,
      "step": 105600
    },
    {
      "epoch": 3.5203333333333333,
      "grad_norm": 0.11442691087722778,
      "learning_rate": 2.799791666666667e-05,
      "loss": 0.0028,
      "step": 105610
    },
    {
      "epoch": 3.5206666666666666,
      "grad_norm": 0.11485667526721954,
      "learning_rate": 2.799583333333333e-05,
      "loss": 0.0023,
      "step": 105620
    },
    {
      "epoch": 3.521,
      "grad_norm": 0.08614064753055573,
      "learning_rate": 2.7993750000000003e-05,
      "loss": 0.0013,
      "step": 105630
    },
    {
      "epoch": 3.521333333333333,
      "grad_norm": 0.02909383922815323,
      "learning_rate": 2.7991666666666665e-05,
      "loss": 0.0023,
      "step": 105640
    },
    {
      "epoch": 3.5216666666666665,
      "grad_norm": 0.17196692526340485,
      "learning_rate": 2.7989583333333337e-05,
      "loss": 0.0017,
      "step": 105650
    },
    {
      "epoch": 3.5220000000000002,
      "grad_norm": 0.17176444828510284,
      "learning_rate": 2.79875e-05,
      "loss": 0.0026,
      "step": 105660
    },
    {
      "epoch": 3.522333333333333,
      "grad_norm": 0.2841493785381317,
      "learning_rate": 2.7985416666666668e-05,
      "loss": 0.0022,
      "step": 105670
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.04300733283162117,
      "learning_rate": 2.7983333333333334e-05,
      "loss": 0.0034,
      "step": 105680
    },
    {
      "epoch": 3.523,
      "grad_norm": 0.22910639643669128,
      "learning_rate": 2.7981250000000003e-05,
      "loss": 0.0025,
      "step": 105690
    },
    {
      "epoch": 3.5233333333333334,
      "grad_norm": 0.003660986665636301,
      "learning_rate": 2.7979166666666668e-05,
      "loss": 0.0026,
      "step": 105700
    },
    {
      "epoch": 3.5236666666666667,
      "grad_norm": 0.11465387791395187,
      "learning_rate": 2.7977083333333337e-05,
      "loss": 0.0016,
      "step": 105710
    },
    {
      "epoch": 3.524,
      "grad_norm": 0.28646165132522583,
      "learning_rate": 2.7975000000000002e-05,
      "loss": 0.0023,
      "step": 105720
    },
    {
      "epoch": 3.5243333333333333,
      "grad_norm": 0.08635637164115906,
      "learning_rate": 2.7972916666666664e-05,
      "loss": 0.002,
      "step": 105730
    },
    {
      "epoch": 3.5246666666666666,
      "grad_norm": 0.34353727102279663,
      "learning_rate": 2.7970833333333333e-05,
      "loss": 0.0019,
      "step": 105740
    },
    {
      "epoch": 3.525,
      "grad_norm": 0.4092404544353485,
      "learning_rate": 2.796875e-05,
      "loss": 0.0022,
      "step": 105750
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.029221436008810997,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.0025,
      "step": 105760
    },
    {
      "epoch": 3.5256666666666665,
      "grad_norm": 0.34361934661865234,
      "learning_rate": 2.7964583333333333e-05,
      "loss": 0.0016,
      "step": 105770
    },
    {
      "epoch": 3.526,
      "grad_norm": 0.1144944280385971,
      "learning_rate": 2.7962500000000002e-05,
      "loss": 0.0022,
      "step": 105780
    },
    {
      "epoch": 3.5263333333333335,
      "grad_norm": 0.47272300720214844,
      "learning_rate": 2.7960416666666668e-05,
      "loss": 0.0021,
      "step": 105790
    },
    {
      "epoch": 3.5266666666666664,
      "grad_norm": 0.11457303911447525,
      "learning_rate": 2.7958333333333336e-05,
      "loss": 0.0019,
      "step": 105800
    },
    {
      "epoch": 3.527,
      "grad_norm": 0.08607807755470276,
      "learning_rate": 2.7956250000000002e-05,
      "loss": 0.0035,
      "step": 105810
    },
    {
      "epoch": 3.5273333333333334,
      "grad_norm": 0.057509925216436386,
      "learning_rate": 2.795416666666667e-05,
      "loss": 0.0015,
      "step": 105820
    },
    {
      "epoch": 3.5276666666666667,
      "grad_norm": 0.08569049090147018,
      "learning_rate": 2.7952083333333333e-05,
      "loss": 0.0015,
      "step": 105830
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.005960152484476566,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 0.0028,
      "step": 105840
    },
    {
      "epoch": 3.5283333333333333,
      "grad_norm": 0.0036123902536928654,
      "learning_rate": 2.7947916666666667e-05,
      "loss": 0.0022,
      "step": 105850
    },
    {
      "epoch": 3.5286666666666666,
      "grad_norm": 0.28616783022880554,
      "learning_rate": 2.7945833333333333e-05,
      "loss": 0.0019,
      "step": 105860
    },
    {
      "epoch": 3.529,
      "grad_norm": 0.11528290063142776,
      "learning_rate": 2.794375e-05,
      "loss": 0.0019,
      "step": 105870
    },
    {
      "epoch": 3.529333333333333,
      "grad_norm": 0.02931886538863182,
      "learning_rate": 2.7941666666666667e-05,
      "loss": 0.0025,
      "step": 105880
    },
    {
      "epoch": 3.5296666666666665,
      "grad_norm": 0.11456786841154099,
      "learning_rate": 2.7939583333333336e-05,
      "loss": 0.0014,
      "step": 105890
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 0.17184898257255554,
      "learning_rate": 2.79375e-05,
      "loss": 0.0017,
      "step": 105900
    },
    {
      "epoch": 3.530333333333333,
      "grad_norm": 0.3146572411060333,
      "learning_rate": 2.793541666666667e-05,
      "loss": 0.0028,
      "step": 105910
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.24186575412750244,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0023,
      "step": 105920
    },
    {
      "epoch": 3.531,
      "grad_norm": 0.02960117720067501,
      "learning_rate": 2.7931250000000005e-05,
      "loss": 0.0028,
      "step": 105930
    },
    {
      "epoch": 3.5313333333333334,
      "grad_norm": 0.08649489283561707,
      "learning_rate": 2.7929166666666667e-05,
      "loss": 0.0031,
      "step": 105940
    },
    {
      "epoch": 3.5316666666666667,
      "grad_norm": 0.05755830183625221,
      "learning_rate": 2.7927083333333336e-05,
      "loss": 0.0019,
      "step": 105950
    },
    {
      "epoch": 3.532,
      "grad_norm": 0.31462928652763367,
      "learning_rate": 2.7925e-05,
      "loss": 0.0019,
      "step": 105960
    },
    {
      "epoch": 3.5323333333333333,
      "grad_norm": 0.20065926015377045,
      "learning_rate": 2.7922916666666667e-05,
      "loss": 0.0022,
      "step": 105970
    },
    {
      "epoch": 3.5326666666666666,
      "grad_norm": 0.257586270570755,
      "learning_rate": 2.7920833333333336e-05,
      "loss": 0.0021,
      "step": 105980
    },
    {
      "epoch": 3.533,
      "grad_norm": 0.2298995852470398,
      "learning_rate": 2.7918749999999998e-05,
      "loss": 0.0025,
      "step": 105990
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.1726016253232956,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.0019,
      "step": 106000
    },
    {
      "epoch": 3.5336666666666665,
      "grad_norm": 0.173007994890213,
      "learning_rate": 2.7914583333333332e-05,
      "loss": 0.002,
      "step": 106010
    },
    {
      "epoch": 3.534,
      "grad_norm": 0.5669304132461548,
      "learning_rate": 2.7912500000000004e-05,
      "loss": 0.0025,
      "step": 106020
    },
    {
      "epoch": 3.5343333333333335,
      "grad_norm": 0.22275839745998383,
      "learning_rate": 2.7910416666666666e-05,
      "loss": 0.0028,
      "step": 106030
    },
    {
      "epoch": 3.5346666666666664,
      "grad_norm": 0.029088666662573814,
      "learning_rate": 2.7908333333333335e-05,
      "loss": 0.0017,
      "step": 106040
    },
    {
      "epoch": 3.535,
      "grad_norm": 0.22916913032531738,
      "learning_rate": 2.790625e-05,
      "loss": 0.0016,
      "step": 106050
    },
    {
      "epoch": 3.5353333333333334,
      "grad_norm": 0.2003878802061081,
      "learning_rate": 2.790416666666667e-05,
      "loss": 0.0021,
      "step": 106060
    },
    {
      "epoch": 3.5356666666666667,
      "grad_norm": 0.17163293063640594,
      "learning_rate": 2.7902083333333335e-05,
      "loss": 0.0023,
      "step": 106070
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.11467099189758301,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0024,
      "step": 106080
    },
    {
      "epoch": 3.5363333333333333,
      "grad_norm": 0.22904276847839355,
      "learning_rate": 2.789791666666667e-05,
      "loss": 0.002,
      "step": 106090
    },
    {
      "epoch": 3.5366666666666666,
      "grad_norm": 0.006021601147949696,
      "learning_rate": 2.789583333333333e-05,
      "loss": 0.0016,
      "step": 106100
    },
    {
      "epoch": 3.537,
      "grad_norm": 0.20036305487155914,
      "learning_rate": 2.789375e-05,
      "loss": 0.0017,
      "step": 106110
    },
    {
      "epoch": 3.537333333333333,
      "grad_norm": 0.11463865637779236,
      "learning_rate": 2.7891666666666666e-05,
      "loss": 0.0031,
      "step": 106120
    },
    {
      "epoch": 3.5376666666666665,
      "grad_norm": 0.0863320529460907,
      "learning_rate": 2.7889583333333335e-05,
      "loss": 0.0027,
      "step": 106130
    },
    {
      "epoch": 3.5380000000000003,
      "grad_norm": 0.02894468791782856,
      "learning_rate": 2.78875e-05,
      "loss": 0.0016,
      "step": 106140
    },
    {
      "epoch": 3.538333333333333,
      "grad_norm": 0.029262475669384003,
      "learning_rate": 2.788541666666667e-05,
      "loss": 0.0018,
      "step": 106150
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.028799762949347496,
      "learning_rate": 2.7883333333333335e-05,
      "loss": 0.0027,
      "step": 106160
    },
    {
      "epoch": 3.539,
      "grad_norm": 0.02926482819020748,
      "learning_rate": 2.7881250000000003e-05,
      "loss": 0.0017,
      "step": 106170
    },
    {
      "epoch": 3.5393333333333334,
      "grad_norm": 0.057418640702962875,
      "learning_rate": 2.787916666666667e-05,
      "loss": 0.002,
      "step": 106180
    },
    {
      "epoch": 3.5396666666666667,
      "grad_norm": 0.4579199552536011,
      "learning_rate": 2.7877083333333338e-05,
      "loss": 0.0021,
      "step": 106190
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.057855695486068726,
      "learning_rate": 2.7875e-05,
      "loss": 0.0025,
      "step": 106200
    },
    {
      "epoch": 3.5403333333333333,
      "grad_norm": 0.08608607947826385,
      "learning_rate": 2.7872916666666665e-05,
      "loss": 0.0024,
      "step": 106210
    },
    {
      "epoch": 3.5406666666666666,
      "grad_norm": 0.20670536160469055,
      "learning_rate": 2.7870833333333334e-05,
      "loss": 0.0018,
      "step": 106220
    },
    {
      "epoch": 3.541,
      "grad_norm": 0.05737760290503502,
      "learning_rate": 2.786875e-05,
      "loss": 0.0028,
      "step": 106230
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.14327457547187805,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0015,
      "step": 106240
    },
    {
      "epoch": 3.5416666666666665,
      "grad_norm": 0.030095651745796204,
      "learning_rate": 2.7864583333333334e-05,
      "loss": 0.0022,
      "step": 106250
    },
    {
      "epoch": 3.542,
      "grad_norm": 0.22885070741176605,
      "learning_rate": 2.7862500000000003e-05,
      "loss": 0.0023,
      "step": 106260
    },
    {
      "epoch": 3.5423333333333336,
      "grad_norm": 0.05780976265668869,
      "learning_rate": 2.7860416666666665e-05,
      "loss": 0.0014,
      "step": 106270
    },
    {
      "epoch": 3.5426666666666664,
      "grad_norm": 0.08643826097249985,
      "learning_rate": 2.7858333333333337e-05,
      "loss": 0.002,
      "step": 106280
    },
    {
      "epoch": 3.543,
      "grad_norm": 0.05746707692742348,
      "learning_rate": 2.785625e-05,
      "loss": 0.0021,
      "step": 106290
    },
    {
      "epoch": 3.5433333333333334,
      "grad_norm": 0.2861585021018982,
      "learning_rate": 2.7854166666666672e-05,
      "loss": 0.0014,
      "step": 106300
    },
    {
      "epoch": 3.5436666666666667,
      "grad_norm": 0.11467306315898895,
      "learning_rate": 2.7852083333333334e-05,
      "loss": 0.0021,
      "step": 106310
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.28635600209236145,
      "learning_rate": 2.7850000000000003e-05,
      "loss": 0.002,
      "step": 106320
    },
    {
      "epoch": 3.5443333333333333,
      "grad_norm": 0.20048807561397552,
      "learning_rate": 2.7847916666666668e-05,
      "loss": 0.0029,
      "step": 106330
    },
    {
      "epoch": 3.5446666666666666,
      "grad_norm": 0.087380051612854,
      "learning_rate": 2.7845833333333334e-05,
      "loss": 0.0029,
      "step": 106340
    },
    {
      "epoch": 3.545,
      "grad_norm": 0.028932640329003334,
      "learning_rate": 2.7843750000000003e-05,
      "loss": 0.0028,
      "step": 106350
    },
    {
      "epoch": 3.5453333333333332,
      "grad_norm": 0.5438055396080017,
      "learning_rate": 2.7841666666666665e-05,
      "loss": 0.0022,
      "step": 106360
    },
    {
      "epoch": 3.5456666666666665,
      "grad_norm": 0.1718447059392929,
      "learning_rate": 2.7839583333333337e-05,
      "loss": 0.0039,
      "step": 106370
    },
    {
      "epoch": 3.5460000000000003,
      "grad_norm": 0.22905251383781433,
      "learning_rate": 2.78375e-05,
      "loss": 0.0024,
      "step": 106380
    },
    {
      "epoch": 3.546333333333333,
      "grad_norm": 0.007827794179320335,
      "learning_rate": 2.7835416666666668e-05,
      "loss": 0.0016,
      "step": 106390
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.3718741536140442,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0029,
      "step": 106400
    },
    {
      "epoch": 3.547,
      "grad_norm": 0.02894916944205761,
      "learning_rate": 2.7831250000000002e-05,
      "loss": 0.0026,
      "step": 106410
    },
    {
      "epoch": 3.5473333333333334,
      "grad_norm": 0.34362363815307617,
      "learning_rate": 2.7829166666666668e-05,
      "loss": 0.0016,
      "step": 106420
    },
    {
      "epoch": 3.5476666666666667,
      "grad_norm": 0.03295805677771568,
      "learning_rate": 2.7827083333333337e-05,
      "loss": 0.002,
      "step": 106430
    },
    {
      "epoch": 3.548,
      "grad_norm": 0.029187392443418503,
      "learning_rate": 2.7825000000000002e-05,
      "loss": 0.0026,
      "step": 106440
    },
    {
      "epoch": 3.5483333333333333,
      "grad_norm": 0.4295481741428375,
      "learning_rate": 2.782291666666667e-05,
      "loss": 0.0022,
      "step": 106450
    },
    {
      "epoch": 3.5486666666666666,
      "grad_norm": 0.4703972339630127,
      "learning_rate": 2.7820833333333336e-05,
      "loss": 0.0017,
      "step": 106460
    },
    {
      "epoch": 3.549,
      "grad_norm": 0.20076541602611542,
      "learning_rate": 2.781875e-05,
      "loss": 0.0015,
      "step": 106470
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.11635375022888184,
      "learning_rate": 2.7816666666666667e-05,
      "loss": 0.0029,
      "step": 106480
    },
    {
      "epoch": 3.5496666666666665,
      "grad_norm": 0.05790161341428757,
      "learning_rate": 2.7814583333333333e-05,
      "loss": 0.0024,
      "step": 106490
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.28616219758987427,
      "learning_rate": 2.7812500000000002e-05,
      "loss": 0.0018,
      "step": 106500
    },
    {
      "epoch": 3.5503333333333336,
      "grad_norm": 0.03282725438475609,
      "learning_rate": 2.7810416666666667e-05,
      "loss": 0.002,
      "step": 106510
    },
    {
      "epoch": 3.5506666666666664,
      "grad_norm": 0.14334408938884735,
      "learning_rate": 2.7808333333333336e-05,
      "loss": 0.0025,
      "step": 106520
    },
    {
      "epoch": 3.551,
      "grad_norm": 0.20049349963665009,
      "learning_rate": 2.780625e-05,
      "loss": 0.0023,
      "step": 106530
    },
    {
      "epoch": 3.5513333333333335,
      "grad_norm": 0.057438645511865616,
      "learning_rate": 2.780416666666667e-05,
      "loss": 0.0021,
      "step": 106540
    },
    {
      "epoch": 3.5516666666666667,
      "grad_norm": 0.400646448135376,
      "learning_rate": 2.7802083333333333e-05,
      "loss": 0.0021,
      "step": 106550
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.14347012341022491,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0034,
      "step": 106560
    },
    {
      "epoch": 3.5523333333333333,
      "grad_norm": 0.05775273218750954,
      "learning_rate": 2.7797916666666667e-05,
      "loss": 0.0022,
      "step": 106570
    },
    {
      "epoch": 3.5526666666666666,
      "grad_norm": 0.11456964910030365,
      "learning_rate": 2.7795833333333332e-05,
      "loss": 0.0024,
      "step": 106580
    },
    {
      "epoch": 3.553,
      "grad_norm": 0.03023993968963623,
      "learning_rate": 2.779375e-05,
      "loss": 0.0019,
      "step": 106590
    },
    {
      "epoch": 3.5533333333333332,
      "grad_norm": 0.08665040135383606,
      "learning_rate": 2.7791666666666667e-05,
      "loss": 0.002,
      "step": 106600
    },
    {
      "epoch": 3.5536666666666665,
      "grad_norm": 0.48678261041641235,
      "learning_rate": 2.7789583333333336e-05,
      "loss": 0.0023,
      "step": 106610
    },
    {
      "epoch": 3.5540000000000003,
      "grad_norm": 0.03210991621017456,
      "learning_rate": 2.77875e-05,
      "loss": 0.0014,
      "step": 106620
    },
    {
      "epoch": 3.554333333333333,
      "grad_norm": 0.3790445327758789,
      "learning_rate": 2.778541666666667e-05,
      "loss": 0.0018,
      "step": 106630
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.11473671346902847,
      "learning_rate": 2.7783333333333332e-05,
      "loss": 0.0017,
      "step": 106640
    },
    {
      "epoch": 3.555,
      "grad_norm": 0.14376632869243622,
      "learning_rate": 2.7781250000000004e-05,
      "loss": 0.0018,
      "step": 106650
    },
    {
      "epoch": 3.5553333333333335,
      "grad_norm": 0.14376235008239746,
      "learning_rate": 2.7779166666666667e-05,
      "loss": 0.0013,
      "step": 106660
    },
    {
      "epoch": 3.5556666666666668,
      "grad_norm": 0.05841963365674019,
      "learning_rate": 2.7777083333333335e-05,
      "loss": 0.003,
      "step": 106670
    },
    {
      "epoch": 3.556,
      "grad_norm": 0.09150221198797226,
      "learning_rate": 2.7775e-05,
      "loss": 0.0017,
      "step": 106680
    },
    {
      "epoch": 3.5563333333333333,
      "grad_norm": 0.057598236948251724,
      "learning_rate": 2.777291666666667e-05,
      "loss": 0.0021,
      "step": 106690
    },
    {
      "epoch": 3.5566666666666666,
      "grad_norm": 0.20024104416370392,
      "learning_rate": 2.7770833333333335e-05,
      "loss": 0.0029,
      "step": 106700
    },
    {
      "epoch": 3.557,
      "grad_norm": 0.058969881385564804,
      "learning_rate": 2.7768749999999997e-05,
      "loss": 0.0023,
      "step": 106710
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.258284330368042,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.0026,
      "step": 106720
    },
    {
      "epoch": 3.5576666666666665,
      "grad_norm": 0.11512909829616547,
      "learning_rate": 2.7764583333333332e-05,
      "loss": 0.0021,
      "step": 106730
    },
    {
      "epoch": 3.558,
      "grad_norm": 0.5412536263465881,
      "learning_rate": 2.7762500000000004e-05,
      "loss": 0.0018,
      "step": 106740
    },
    {
      "epoch": 3.5583333333333336,
      "grad_norm": 0.45793616771698,
      "learning_rate": 2.7760416666666666e-05,
      "loss": 0.0012,
      "step": 106750
    },
    {
      "epoch": 3.5586666666666664,
      "grad_norm": 0.08593279123306274,
      "learning_rate": 2.7758333333333335e-05,
      "loss": 0.002,
      "step": 106760
    },
    {
      "epoch": 3.559,
      "grad_norm": 0.05782607942819595,
      "learning_rate": 2.775625e-05,
      "loss": 0.0017,
      "step": 106770
    },
    {
      "epoch": 3.5593333333333335,
      "grad_norm": 0.22909758985042572,
      "learning_rate": 2.775416666666667e-05,
      "loss": 0.0014,
      "step": 106780
    },
    {
      "epoch": 3.5596666666666668,
      "grad_norm": 0.05764321610331535,
      "learning_rate": 2.7752083333333335e-05,
      "loss": 0.0017,
      "step": 106790
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.08636283874511719,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.0015,
      "step": 106800
    },
    {
      "epoch": 3.5603333333333333,
      "grad_norm": 0.14290300011634827,
      "learning_rate": 2.774791666666667e-05,
      "loss": 0.0034,
      "step": 106810
    },
    {
      "epoch": 3.5606666666666666,
      "grad_norm": 0.11446850746870041,
      "learning_rate": 2.774583333333333e-05,
      "loss": 0.0028,
      "step": 106820
    },
    {
      "epoch": 3.561,
      "grad_norm": 0.02921699546277523,
      "learning_rate": 2.774375e-05,
      "loss": 0.0021,
      "step": 106830
    },
    {
      "epoch": 3.5613333333333332,
      "grad_norm": 0.5612049698829651,
      "learning_rate": 2.7741666666666666e-05,
      "loss": 0.0029,
      "step": 106840
    },
    {
      "epoch": 3.5616666666666665,
      "grad_norm": 0.20042099058628082,
      "learning_rate": 2.7739583333333334e-05,
      "loss": 0.0032,
      "step": 106850
    },
    {
      "epoch": 3.5620000000000003,
      "grad_norm": 0.11436865478754044,
      "learning_rate": 2.77375e-05,
      "loss": 0.0016,
      "step": 106860
    },
    {
      "epoch": 3.562333333333333,
      "grad_norm": 0.28584352135658264,
      "learning_rate": 2.773541666666667e-05,
      "loss": 0.0023,
      "step": 106870
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 1.4880871772766113,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0026,
      "step": 106880
    },
    {
      "epoch": 3.5629999999999997,
      "grad_norm": 0.17175473272800446,
      "learning_rate": 2.7731250000000003e-05,
      "loss": 0.0025,
      "step": 106890
    },
    {
      "epoch": 3.5633333333333335,
      "grad_norm": 0.0574556402862072,
      "learning_rate": 2.772916666666667e-05,
      "loss": 0.0021,
      "step": 106900
    },
    {
      "epoch": 3.5636666666666668,
      "grad_norm": 0.03039545565843582,
      "learning_rate": 2.7727083333333338e-05,
      "loss": 0.0023,
      "step": 106910
    },
    {
      "epoch": 3.564,
      "grad_norm": 0.5152447819709778,
      "learning_rate": 2.7725e-05,
      "loss": 0.0038,
      "step": 106920
    },
    {
      "epoch": 3.5643333333333334,
      "grad_norm": 0.11454833298921585,
      "learning_rate": 2.7722916666666672e-05,
      "loss": 0.0018,
      "step": 106930
    },
    {
      "epoch": 3.5646666666666667,
      "grad_norm": 0.14300474524497986,
      "learning_rate": 2.7720833333333334e-05,
      "loss": 0.0017,
      "step": 106940
    },
    {
      "epoch": 3.565,
      "grad_norm": 0.08608926087617874,
      "learning_rate": 2.771875e-05,
      "loss": 0.003,
      "step": 106950
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.11444974690675735,
      "learning_rate": 2.771666666666667e-05,
      "loss": 0.0023,
      "step": 106960
    },
    {
      "epoch": 3.5656666666666665,
      "grad_norm": 0.02970641665160656,
      "learning_rate": 2.7714583333333334e-05,
      "loss": 0.0031,
      "step": 106970
    },
    {
      "epoch": 3.566,
      "grad_norm": 0.457779198884964,
      "learning_rate": 2.7712500000000003e-05,
      "loss": 0.0025,
      "step": 106980
    },
    {
      "epoch": 3.5663333333333336,
      "grad_norm": 0.028948616236448288,
      "learning_rate": 2.7710416666666665e-05,
      "loss": 0.0014,
      "step": 106990
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.0577574223279953,
      "learning_rate": 2.7708333333333337e-05,
      "loss": 0.0029,
      "step": 107000
    },
    {
      "epoch": 3.567,
      "grad_norm": 0.14334434270858765,
      "learning_rate": 2.770625e-05,
      "loss": 0.0026,
      "step": 107010
    },
    {
      "epoch": 3.5673333333333335,
      "grad_norm": 0.029194707050919533,
      "learning_rate": 2.770416666666667e-05,
      "loss": 0.0018,
      "step": 107020
    },
    {
      "epoch": 3.5676666666666668,
      "grad_norm": 0.3189801275730133,
      "learning_rate": 2.7702083333333334e-05,
      "loss": 0.0026,
      "step": 107030
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.007860119454562664,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.002,
      "step": 107040
    },
    {
      "epoch": 3.5683333333333334,
      "grad_norm": 0.286335289478302,
      "learning_rate": 2.7697916666666668e-05,
      "loss": 0.0027,
      "step": 107050
    },
    {
      "epoch": 3.5686666666666667,
      "grad_norm": 0.6868558526039124,
      "learning_rate": 2.7695833333333333e-05,
      "loss": 0.0018,
      "step": 107060
    },
    {
      "epoch": 3.569,
      "grad_norm": 0.20053665339946747,
      "learning_rate": 2.7693750000000002e-05,
      "loss": 0.0031,
      "step": 107070
    },
    {
      "epoch": 3.5693333333333332,
      "grad_norm": 0.17180874943733215,
      "learning_rate": 2.7691666666666664e-05,
      "loss": 0.0018,
      "step": 107080
    },
    {
      "epoch": 3.5696666666666665,
      "grad_norm": 0.22889447212219238,
      "learning_rate": 2.7689583333333337e-05,
      "loss": 0.0023,
      "step": 107090
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.11551965028047562,
      "learning_rate": 2.76875e-05,
      "loss": 0.0031,
      "step": 107100
    },
    {
      "epoch": 3.570333333333333,
      "grad_norm": 0.030137857422232628,
      "learning_rate": 2.7685416666666668e-05,
      "loss": 0.0029,
      "step": 107110
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.34392452239990234,
      "learning_rate": 2.7683333333333333e-05,
      "loss": 0.0022,
      "step": 107120
    },
    {
      "epoch": 3.5709999999999997,
      "grad_norm": 0.11474811285734177,
      "learning_rate": 2.7681250000000002e-05,
      "loss": 0.0019,
      "step": 107130
    },
    {
      "epoch": 3.5713333333333335,
      "grad_norm": 0.11454176157712936,
      "learning_rate": 2.7679166666666667e-05,
      "loss": 0.0019,
      "step": 107140
    },
    {
      "epoch": 3.5716666666666668,
      "grad_norm": 0.08626183122396469,
      "learning_rate": 2.7677083333333336e-05,
      "loss": 0.0026,
      "step": 107150
    },
    {
      "epoch": 3.572,
      "grad_norm": 0.0864439606666565,
      "learning_rate": 2.7675000000000002e-05,
      "loss": 0.0025,
      "step": 107160
    },
    {
      "epoch": 3.5723333333333334,
      "grad_norm": 0.09057210385799408,
      "learning_rate": 2.767291666666667e-05,
      "loss": 0.0022,
      "step": 107170
    },
    {
      "epoch": 3.5726666666666667,
      "grad_norm": 0.28622886538505554,
      "learning_rate": 2.7670833333333336e-05,
      "loss": 0.0034,
      "step": 107180
    },
    {
      "epoch": 3.573,
      "grad_norm": 0.22935751080513,
      "learning_rate": 2.7668749999999998e-05,
      "loss": 0.0029,
      "step": 107190
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.486457496881485,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0019,
      "step": 107200
    },
    {
      "epoch": 3.5736666666666665,
      "grad_norm": 0.08595868200063705,
      "learning_rate": 2.7664583333333333e-05,
      "loss": 0.0021,
      "step": 107210
    },
    {
      "epoch": 3.574,
      "grad_norm": 0.03019017167389393,
      "learning_rate": 2.76625e-05,
      "loss": 0.0019,
      "step": 107220
    },
    {
      "epoch": 3.5743333333333336,
      "grad_norm": 0.2574863135814667,
      "learning_rate": 2.7660416666666667e-05,
      "loss": 0.0029,
      "step": 107230
    },
    {
      "epoch": 3.5746666666666664,
      "grad_norm": 0.11465097218751907,
      "learning_rate": 2.7658333333333336e-05,
      "loss": 0.0018,
      "step": 107240
    },
    {
      "epoch": 3.575,
      "grad_norm": 0.11510668694972992,
      "learning_rate": 2.765625e-05,
      "loss": 0.0018,
      "step": 107250
    },
    {
      "epoch": 3.5753333333333335,
      "grad_norm": 0.08607704937458038,
      "learning_rate": 2.765416666666667e-05,
      "loss": 0.0024,
      "step": 107260
    },
    {
      "epoch": 3.5756666666666668,
      "grad_norm": 0.31703460216522217,
      "learning_rate": 2.7652083333333332e-05,
      "loss": 0.0017,
      "step": 107270
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.11473104357719421,
      "learning_rate": 2.7650000000000005e-05,
      "loss": 0.0022,
      "step": 107280
    },
    {
      "epoch": 3.5763333333333334,
      "grad_norm": 0.02891925722360611,
      "learning_rate": 2.7647916666666667e-05,
      "loss": 0.0028,
      "step": 107290
    },
    {
      "epoch": 3.5766666666666667,
      "grad_norm": 0.0291693527251482,
      "learning_rate": 2.7645833333333332e-05,
      "loss": 0.0022,
      "step": 107300
    },
    {
      "epoch": 3.577,
      "grad_norm": 0.17188724875450134,
      "learning_rate": 2.764375e-05,
      "loss": 0.0018,
      "step": 107310
    },
    {
      "epoch": 3.5773333333333333,
      "grad_norm": 0.2862459421157837,
      "learning_rate": 2.7641666666666667e-05,
      "loss": 0.002,
      "step": 107320
    },
    {
      "epoch": 3.5776666666666666,
      "grad_norm": 0.029648471623659134,
      "learning_rate": 2.7639583333333335e-05,
      "loss": 0.002,
      "step": 107330
    },
    {
      "epoch": 3.578,
      "grad_norm": 0.1145288422703743,
      "learning_rate": 2.76375e-05,
      "loss": 0.0012,
      "step": 107340
    },
    {
      "epoch": 3.578333333333333,
      "grad_norm": 0.029329944401979446,
      "learning_rate": 2.763541666666667e-05,
      "loss": 0.0036,
      "step": 107350
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.030301161110401154,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0019,
      "step": 107360
    },
    {
      "epoch": 3.5789999999999997,
      "grad_norm": 0.029941566288471222,
      "learning_rate": 2.7631250000000004e-05,
      "loss": 0.003,
      "step": 107370
    },
    {
      "epoch": 3.5793333333333335,
      "grad_norm": 0.02976694330573082,
      "learning_rate": 2.7629166666666666e-05,
      "loss": 0.0019,
      "step": 107380
    },
    {
      "epoch": 3.5796666666666668,
      "grad_norm": 1.0673326253890991,
      "learning_rate": 2.7627083333333335e-05,
      "loss": 0.0031,
      "step": 107390
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.4009009897708893,
      "learning_rate": 2.7625e-05,
      "loss": 0.0018,
      "step": 107400
    },
    {
      "epoch": 3.5803333333333334,
      "grad_norm": 0.029514750465750694,
      "learning_rate": 2.762291666666667e-05,
      "loss": 0.0026,
      "step": 107410
    },
    {
      "epoch": 3.5806666666666667,
      "grad_norm": 0.05755100026726723,
      "learning_rate": 2.7620833333333335e-05,
      "loss": 0.0025,
      "step": 107420
    },
    {
      "epoch": 3.581,
      "grad_norm": 0.08595488965511322,
      "learning_rate": 2.761875e-05,
      "loss": 0.0032,
      "step": 107430
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.28626394271850586,
      "learning_rate": 2.761666666666667e-05,
      "loss": 0.0023,
      "step": 107440
    },
    {
      "epoch": 3.5816666666666666,
      "grad_norm": 0.372053325176239,
      "learning_rate": 2.761458333333333e-05,
      "loss": 0.0021,
      "step": 107450
    },
    {
      "epoch": 3.582,
      "grad_norm": 0.34338077902793884,
      "learning_rate": 2.7612500000000004e-05,
      "loss": 0.0018,
      "step": 107460
    },
    {
      "epoch": 3.5823333333333336,
      "grad_norm": 0.3149433135986328,
      "learning_rate": 2.7610416666666666e-05,
      "loss": 0.0016,
      "step": 107470
    },
    {
      "epoch": 3.5826666666666664,
      "grad_norm": 0.057517021894454956,
      "learning_rate": 2.7608333333333335e-05,
      "loss": 0.0027,
      "step": 107480
    },
    {
      "epoch": 3.583,
      "grad_norm": 0.25757357478141785,
      "learning_rate": 2.760625e-05,
      "loss": 0.0023,
      "step": 107490
    },
    {
      "epoch": 3.5833333333333335,
      "grad_norm": 0.11465538293123245,
      "learning_rate": 2.760416666666667e-05,
      "loss": 0.0021,
      "step": 107500
    },
    {
      "epoch": 3.583666666666667,
      "grad_norm": 0.2860678732395172,
      "learning_rate": 2.7602083333333335e-05,
      "loss": 0.0023,
      "step": 107510
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.2290661334991455,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.003,
      "step": 107520
    },
    {
      "epoch": 3.5843333333333334,
      "grad_norm": 0.1431894600391388,
      "learning_rate": 2.759791666666667e-05,
      "loss": 0.0025,
      "step": 107530
    },
    {
      "epoch": 3.5846666666666667,
      "grad_norm": 0.20038114488124847,
      "learning_rate": 2.7595833333333338e-05,
      "loss": 0.002,
      "step": 107540
    },
    {
      "epoch": 3.585,
      "grad_norm": 0.11470214277505875,
      "learning_rate": 2.759375e-05,
      "loss": 0.0018,
      "step": 107550
    },
    {
      "epoch": 3.5853333333333333,
      "grad_norm": 0.28039178252220154,
      "learning_rate": 2.7591666666666665e-05,
      "loss": 0.0022,
      "step": 107560
    },
    {
      "epoch": 3.5856666666666666,
      "grad_norm": 0.08591111749410629,
      "learning_rate": 2.7589583333333334e-05,
      "loss": 0.0019,
      "step": 107570
    },
    {
      "epoch": 3.586,
      "grad_norm": 0.7017664313316345,
      "learning_rate": 2.75875e-05,
      "loss": 0.0016,
      "step": 107580
    },
    {
      "epoch": 3.586333333333333,
      "grad_norm": 0.11466097086668015,
      "learning_rate": 2.758541666666667e-05,
      "loss": 0.0024,
      "step": 107590
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.029330721125006676,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.0021,
      "step": 107600
    },
    {
      "epoch": 3.5869999999999997,
      "grad_norm": 0.14326351881027222,
      "learning_rate": 2.7581250000000003e-05,
      "loss": 0.0023,
      "step": 107610
    },
    {
      "epoch": 3.5873333333333335,
      "grad_norm": 0.1720936894416809,
      "learning_rate": 2.757916666666667e-05,
      "loss": 0.002,
      "step": 107620
    },
    {
      "epoch": 3.587666666666667,
      "grad_norm": 0.2288370132446289,
      "learning_rate": 2.7577083333333337e-05,
      "loss": 0.0026,
      "step": 107630
    },
    {
      "epoch": 3.588,
      "grad_norm": 0.08602505922317505,
      "learning_rate": 2.7575e-05,
      "loss": 0.0025,
      "step": 107640
    },
    {
      "epoch": 3.5883333333333334,
      "grad_norm": 0.6269169449806213,
      "learning_rate": 2.757291666666667e-05,
      "loss": 0.0024,
      "step": 107650
    },
    {
      "epoch": 3.5886666666666667,
      "grad_norm": 0.42070433497428894,
      "learning_rate": 2.7570833333333334e-05,
      "loss": 0.0024,
      "step": 107660
    },
    {
      "epoch": 3.589,
      "grad_norm": 0.5083613991737366,
      "learning_rate": 2.756875e-05,
      "loss": 0.0018,
      "step": 107670
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.17178668081760406,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0025,
      "step": 107680
    },
    {
      "epoch": 3.5896666666666666,
      "grad_norm": 0.07705255597829819,
      "learning_rate": 2.7564583333333334e-05,
      "loss": 0.0021,
      "step": 107690
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.029259486123919487,
      "learning_rate": 2.7562500000000002e-05,
      "loss": 0.0021,
      "step": 107700
    },
    {
      "epoch": 3.5903333333333336,
      "grad_norm": 0.20026928186416626,
      "learning_rate": 2.7560416666666668e-05,
      "loss": 0.0026,
      "step": 107710
    },
    {
      "epoch": 3.5906666666666665,
      "grad_norm": 0.009522016160190105,
      "learning_rate": 2.7558333333333337e-05,
      "loss": 0.0023,
      "step": 107720
    },
    {
      "epoch": 3.591,
      "grad_norm": 0.40085333585739136,
      "learning_rate": 2.755625e-05,
      "loss": 0.0018,
      "step": 107730
    },
    {
      "epoch": 3.5913333333333335,
      "grad_norm": 0.08605267107486725,
      "learning_rate": 2.755416666666667e-05,
      "loss": 0.002,
      "step": 107740
    },
    {
      "epoch": 3.591666666666667,
      "grad_norm": 0.354217529296875,
      "learning_rate": 2.7552083333333333e-05,
      "loss": 0.0025,
      "step": 107750
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.14335419237613678,
      "learning_rate": 2.7550000000000002e-05,
      "loss": 0.0018,
      "step": 107760
    },
    {
      "epoch": 3.5923333333333334,
      "grad_norm": 0.28606563806533813,
      "learning_rate": 2.7547916666666668e-05,
      "loss": 0.0016,
      "step": 107770
    },
    {
      "epoch": 3.5926666666666667,
      "grad_norm": 0.22911973297595978,
      "learning_rate": 2.7545833333333337e-05,
      "loss": 0.0017,
      "step": 107780
    },
    {
      "epoch": 3.593,
      "grad_norm": 0.030554840341210365,
      "learning_rate": 2.7543750000000002e-05,
      "loss": 0.0036,
      "step": 107790
    },
    {
      "epoch": 3.5933333333333333,
      "grad_norm": 0.08596171438694,
      "learning_rate": 2.7541666666666664e-05,
      "loss": 0.0014,
      "step": 107800
    },
    {
      "epoch": 3.5936666666666666,
      "grad_norm": 0.14305514097213745,
      "learning_rate": 2.7539583333333336e-05,
      "loss": 0.0019,
      "step": 107810
    },
    {
      "epoch": 3.594,
      "grad_norm": 0.22907958924770355,
      "learning_rate": 2.75375e-05,
      "loss": 0.0026,
      "step": 107820
    },
    {
      "epoch": 3.594333333333333,
      "grad_norm": 0.029432304203510284,
      "learning_rate": 2.753541666666667e-05,
      "loss": 0.002,
      "step": 107830
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.20096755027770996,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0024,
      "step": 107840
    },
    {
      "epoch": 3.5949999999999998,
      "grad_norm": 0.314744770526886,
      "learning_rate": 2.7531250000000002e-05,
      "loss": 0.0019,
      "step": 107850
    },
    {
      "epoch": 3.5953333333333335,
      "grad_norm": 0.030796200037002563,
      "learning_rate": 2.7529166666666667e-05,
      "loss": 0.0014,
      "step": 107860
    },
    {
      "epoch": 3.595666666666667,
      "grad_norm": 0.2002415806055069,
      "learning_rate": 2.7527083333333336e-05,
      "loss": 0.0024,
      "step": 107870
    },
    {
      "epoch": 3.596,
      "grad_norm": 0.007183223497122526,
      "learning_rate": 2.7525e-05,
      "loss": 0.0018,
      "step": 107880
    },
    {
      "epoch": 3.5963333333333334,
      "grad_norm": 0.005239898804575205,
      "learning_rate": 2.752291666666667e-05,
      "loss": 0.0025,
      "step": 107890
    },
    {
      "epoch": 3.5966666666666667,
      "grad_norm": 0.54612797498703,
      "learning_rate": 2.7520833333333336e-05,
      "loss": 0.0019,
      "step": 107900
    },
    {
      "epoch": 3.597,
      "grad_norm": 0.3733946979045868,
      "learning_rate": 2.7518749999999998e-05,
      "loss": 0.003,
      "step": 107910
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.007813925854861736,
      "learning_rate": 2.7516666666666667e-05,
      "loss": 0.0026,
      "step": 107920
    },
    {
      "epoch": 3.5976666666666666,
      "grad_norm": 0.11657144129276276,
      "learning_rate": 2.7514583333333332e-05,
      "loss": 0.0028,
      "step": 107930
    },
    {
      "epoch": 3.598,
      "grad_norm": 0.11486145108938217,
      "learning_rate": 2.75125e-05,
      "loss": 0.0017,
      "step": 107940
    },
    {
      "epoch": 3.5983333333333336,
      "grad_norm": 0.2580406963825226,
      "learning_rate": 2.7510416666666667e-05,
      "loss": 0.0019,
      "step": 107950
    },
    {
      "epoch": 3.5986666666666665,
      "grad_norm": 0.004496669862419367,
      "learning_rate": 2.7508333333333336e-05,
      "loss": 0.002,
      "step": 107960
    },
    {
      "epoch": 3.599,
      "grad_norm": 0.228973850607872,
      "learning_rate": 2.750625e-05,
      "loss": 0.0017,
      "step": 107970
    },
    {
      "epoch": 3.5993333333333335,
      "grad_norm": 0.171815887093544,
      "learning_rate": 2.750416666666667e-05,
      "loss": 0.002,
      "step": 107980
    },
    {
      "epoch": 3.599666666666667,
      "grad_norm": 0.14356139302253723,
      "learning_rate": 2.7502083333333335e-05,
      "loss": 0.0018,
      "step": 107990
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.17173658311367035,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0021,
      "step": 108000
    },
    {
      "epoch": 3.6003333333333334,
      "grad_norm": 0.22932063043117523,
      "learning_rate": 2.7497916666666666e-05,
      "loss": 0.002,
      "step": 108010
    },
    {
      "epoch": 3.6006666666666667,
      "grad_norm": 0.2860022485256195,
      "learning_rate": 2.749583333333334e-05,
      "loss": 0.0027,
      "step": 108020
    },
    {
      "epoch": 3.601,
      "grad_norm": 0.036176376044750214,
      "learning_rate": 2.749375e-05,
      "loss": 0.0024,
      "step": 108030
    },
    {
      "epoch": 3.6013333333333333,
      "grad_norm": 0.22886060178279877,
      "learning_rate": 2.7491666666666666e-05,
      "loss": 0.0017,
      "step": 108040
    },
    {
      "epoch": 3.6016666666666666,
      "grad_norm": 0.20016571879386902,
      "learning_rate": 2.7489583333333335e-05,
      "loss": 0.0022,
      "step": 108050
    },
    {
      "epoch": 3.602,
      "grad_norm": 0.05730845034122467,
      "learning_rate": 2.74875e-05,
      "loss": 0.002,
      "step": 108060
    },
    {
      "epoch": 3.602333333333333,
      "grad_norm": 0.14369645714759827,
      "learning_rate": 2.748541666666667e-05,
      "loss": 0.0023,
      "step": 108070
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.058104775846004486,
      "learning_rate": 2.748333333333333e-05,
      "loss": 0.0021,
      "step": 108080
    },
    {
      "epoch": 3.6029999999999998,
      "grad_norm": 0.1717565953731537,
      "learning_rate": 2.7481250000000004e-05,
      "loss": 0.0025,
      "step": 108090
    },
    {
      "epoch": 3.6033333333333335,
      "grad_norm": 0.22925911843776703,
      "learning_rate": 2.7479166666666666e-05,
      "loss": 0.0021,
      "step": 108100
    },
    {
      "epoch": 3.603666666666667,
      "grad_norm": 0.005381375551223755,
      "learning_rate": 2.7477083333333338e-05,
      "loss": 0.0015,
      "step": 108110
    },
    {
      "epoch": 3.604,
      "grad_norm": 0.03001464158296585,
      "learning_rate": 2.7475e-05,
      "loss": 0.0024,
      "step": 108120
    },
    {
      "epoch": 3.6043333333333334,
      "grad_norm": 0.5681284070014954,
      "learning_rate": 2.747291666666667e-05,
      "loss": 0.0021,
      "step": 108130
    },
    {
      "epoch": 3.6046666666666667,
      "grad_norm": 0.1719941347837448,
      "learning_rate": 2.7470833333333335e-05,
      "loss": 0.0027,
      "step": 108140
    },
    {
      "epoch": 3.605,
      "grad_norm": 0.20041276514530182,
      "learning_rate": 2.746875e-05,
      "loss": 0.0026,
      "step": 108150
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.029606500640511513,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0015,
      "step": 108160
    },
    {
      "epoch": 3.6056666666666666,
      "grad_norm": 0.14307014644145966,
      "learning_rate": 2.746458333333333e-05,
      "loss": 0.0027,
      "step": 108170
    },
    {
      "epoch": 3.606,
      "grad_norm": 0.25751426815986633,
      "learning_rate": 2.7462500000000003e-05,
      "loss": 0.0029,
      "step": 108180
    },
    {
      "epoch": 3.606333333333333,
      "grad_norm": 0.17164742946624756,
      "learning_rate": 2.7460416666666666e-05,
      "loss": 0.0026,
      "step": 108190
    },
    {
      "epoch": 3.6066666666666665,
      "grad_norm": 0.08591794967651367,
      "learning_rate": 2.7458333333333334e-05,
      "loss": 0.0029,
      "step": 108200
    },
    {
      "epoch": 3.607,
      "grad_norm": 0.203227236866951,
      "learning_rate": 2.745625e-05,
      "loss": 0.0024,
      "step": 108210
    },
    {
      "epoch": 3.607333333333333,
      "grad_norm": 0.028831806033849716,
      "learning_rate": 2.745416666666667e-05,
      "loss": 0.0024,
      "step": 108220
    },
    {
      "epoch": 3.607666666666667,
      "grad_norm": 0.3717082142829895,
      "learning_rate": 2.7452083333333334e-05,
      "loss": 0.0032,
      "step": 108230
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.08596891909837723,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 0.0022,
      "step": 108240
    },
    {
      "epoch": 3.6083333333333334,
      "grad_norm": 0.48618537187576294,
      "learning_rate": 2.744791666666667e-05,
      "loss": 0.0025,
      "step": 108250
    },
    {
      "epoch": 3.6086666666666667,
      "grad_norm": 0.05779669061303139,
      "learning_rate": 2.7445833333333338e-05,
      "loss": 0.0022,
      "step": 108260
    },
    {
      "epoch": 3.609,
      "grad_norm": 0.11466821283102036,
      "learning_rate": 2.7443750000000003e-05,
      "loss": 0.0021,
      "step": 108270
    },
    {
      "epoch": 3.6093333333333333,
      "grad_norm": 0.7044382095336914,
      "learning_rate": 2.7441666666666665e-05,
      "loss": 0.0024,
      "step": 108280
    },
    {
      "epoch": 3.6096666666666666,
      "grad_norm": 0.11511281132698059,
      "learning_rate": 2.7439583333333334e-05,
      "loss": 0.0027,
      "step": 108290
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.2861023545265198,
      "learning_rate": 2.74375e-05,
      "loss": 0.0019,
      "step": 108300
    },
    {
      "epoch": 3.610333333333333,
      "grad_norm": 0.009501147083938122,
      "learning_rate": 2.743541666666667e-05,
      "loss": 0.002,
      "step": 108310
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.11457832902669907,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0025,
      "step": 108320
    },
    {
      "epoch": 3.6109999999999998,
      "grad_norm": 0.457754522562027,
      "learning_rate": 2.7431250000000003e-05,
      "loss": 0.0025,
      "step": 108330
    },
    {
      "epoch": 3.6113333333333335,
      "grad_norm": 0.2000918686389923,
      "learning_rate": 2.7429166666666668e-05,
      "loss": 0.002,
      "step": 108340
    },
    {
      "epoch": 3.611666666666667,
      "grad_norm": 0.028864439576864243,
      "learning_rate": 2.7427083333333337e-05,
      "loss": 0.0019,
      "step": 108350
    },
    {
      "epoch": 3.612,
      "grad_norm": 0.3720190227031708,
      "learning_rate": 2.7425e-05,
      "loss": 0.0024,
      "step": 108360
    },
    {
      "epoch": 3.6123333333333334,
      "grad_norm": 0.1432723104953766,
      "learning_rate": 2.742291666666667e-05,
      "loss": 0.0024,
      "step": 108370
    },
    {
      "epoch": 3.6126666666666667,
      "grad_norm": 0.4292289614677429,
      "learning_rate": 2.7420833333333334e-05,
      "loss": 0.0022,
      "step": 108380
    },
    {
      "epoch": 3.613,
      "grad_norm": 0.05831466615200043,
      "learning_rate": 2.741875e-05,
      "loss": 0.0021,
      "step": 108390
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.257502943277359,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.0019,
      "step": 108400
    },
    {
      "epoch": 3.6136666666666666,
      "grad_norm": 0.02988859824836254,
      "learning_rate": 2.7414583333333333e-05,
      "loss": 0.0027,
      "step": 108410
    },
    {
      "epoch": 3.614,
      "grad_norm": 0.4005627930164337,
      "learning_rate": 2.7412500000000002e-05,
      "loss": 0.0028,
      "step": 108420
    },
    {
      "epoch": 3.614333333333333,
      "grad_norm": 0.006766095757484436,
      "learning_rate": 2.7410416666666668e-05,
      "loss": 0.0024,
      "step": 108430
    },
    {
      "epoch": 3.6146666666666665,
      "grad_norm": 0.08605553209781647,
      "learning_rate": 2.7408333333333337e-05,
      "loss": 0.0021,
      "step": 108440
    },
    {
      "epoch": 3.615,
      "grad_norm": 0.08651071786880493,
      "learning_rate": 2.740625e-05,
      "loss": 0.0019,
      "step": 108450
    },
    {
      "epoch": 3.615333333333333,
      "grad_norm": 0.37213048338890076,
      "learning_rate": 2.740416666666667e-05,
      "loss": 0.0021,
      "step": 108460
    },
    {
      "epoch": 3.615666666666667,
      "grad_norm": 0.22922378778457642,
      "learning_rate": 2.7402083333333333e-05,
      "loss": 0.0021,
      "step": 108470
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.34324926137924194,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0023,
      "step": 108480
    },
    {
      "epoch": 3.6163333333333334,
      "grad_norm": 0.3719213604927063,
      "learning_rate": 2.7397916666666667e-05,
      "loss": 0.0018,
      "step": 108490
    },
    {
      "epoch": 3.6166666666666667,
      "grad_norm": 0.17199011147022247,
      "learning_rate": 2.7395833333333336e-05,
      "loss": 0.0018,
      "step": 108500
    },
    {
      "epoch": 3.617,
      "grad_norm": 0.057366993278265,
      "learning_rate": 2.7393750000000002e-05,
      "loss": 0.0018,
      "step": 108510
    },
    {
      "epoch": 3.6173333333333333,
      "grad_norm": 0.02926008775830269,
      "learning_rate": 2.7391666666666664e-05,
      "loss": 0.0021,
      "step": 108520
    },
    {
      "epoch": 3.6176666666666666,
      "grad_norm": 0.2576413154602051,
      "learning_rate": 2.7389583333333336e-05,
      "loss": 0.0019,
      "step": 108530
    },
    {
      "epoch": 3.618,
      "grad_norm": 0.45832109451293945,
      "learning_rate": 2.7387499999999998e-05,
      "loss": 0.003,
      "step": 108540
    },
    {
      "epoch": 3.618333333333333,
      "grad_norm": 0.11497557908296585,
      "learning_rate": 2.738541666666667e-05,
      "loss": 0.0015,
      "step": 108550
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.20038214325904846,
      "learning_rate": 2.7383333333333333e-05,
      "loss": 0.0021,
      "step": 108560
    },
    {
      "epoch": 3.6189999999999998,
      "grad_norm": 0.11481386423110962,
      "learning_rate": 2.738125e-05,
      "loss": 0.002,
      "step": 108570
    },
    {
      "epoch": 3.6193333333333335,
      "grad_norm": 0.28620967268943787,
      "learning_rate": 2.7379166666666667e-05,
      "loss": 0.0019,
      "step": 108580
    },
    {
      "epoch": 3.619666666666667,
      "grad_norm": 0.14363832771778107,
      "learning_rate": 2.7377083333333336e-05,
      "loss": 0.0019,
      "step": 108590
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.08631116151809692,
      "learning_rate": 2.7375e-05,
      "loss": 0.0015,
      "step": 108600
    },
    {
      "epoch": 3.6203333333333334,
      "grad_norm": 0.28626716136932373,
      "learning_rate": 2.737291666666667e-05,
      "loss": 0.0032,
      "step": 108610
    },
    {
      "epoch": 3.6206666666666667,
      "grad_norm": 0.4289375841617584,
      "learning_rate": 2.7370833333333336e-05,
      "loss": 0.002,
      "step": 108620
    },
    {
      "epoch": 3.621,
      "grad_norm": 0.046517424285411835,
      "learning_rate": 2.7368749999999998e-05,
      "loss": 0.0019,
      "step": 108630
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.057520464062690735,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0029,
      "step": 108640
    },
    {
      "epoch": 3.6216666666666666,
      "grad_norm": 0.010274809785187244,
      "learning_rate": 2.7364583333333332e-05,
      "loss": 0.0023,
      "step": 108650
    },
    {
      "epoch": 3.622,
      "grad_norm": 0.08610120415687561,
      "learning_rate": 2.73625e-05,
      "loss": 0.0028,
      "step": 108660
    },
    {
      "epoch": 3.622333333333333,
      "grad_norm": 0.48624712228775024,
      "learning_rate": 2.7360416666666667e-05,
      "loss": 0.002,
      "step": 108670
    },
    {
      "epoch": 3.6226666666666665,
      "grad_norm": 0.20051319897174835,
      "learning_rate": 2.7358333333333335e-05,
      "loss": 0.0016,
      "step": 108680
    },
    {
      "epoch": 3.623,
      "grad_norm": 0.1719382107257843,
      "learning_rate": 2.735625e-05,
      "loss": 0.002,
      "step": 108690
    },
    {
      "epoch": 3.623333333333333,
      "grad_norm": 0.25747230648994446,
      "learning_rate": 2.735416666666667e-05,
      "loss": 0.003,
      "step": 108700
    },
    {
      "epoch": 3.623666666666667,
      "grad_norm": 0.14312991499900818,
      "learning_rate": 2.7352083333333335e-05,
      "loss": 0.0021,
      "step": 108710
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.02900426834821701,
      "learning_rate": 2.7350000000000004e-05,
      "loss": 0.002,
      "step": 108720
    },
    {
      "epoch": 3.6243333333333334,
      "grad_norm": 0.14323125779628754,
      "learning_rate": 2.7347916666666666e-05,
      "loss": 0.0022,
      "step": 108730
    },
    {
      "epoch": 3.6246666666666667,
      "grad_norm": 0.22913286089897156,
      "learning_rate": 2.734583333333334e-05,
      "loss": 0.002,
      "step": 108740
    },
    {
      "epoch": 3.625,
      "grad_norm": 0.2288941740989685,
      "learning_rate": 2.734375e-05,
      "loss": 0.0028,
      "step": 108750
    },
    {
      "epoch": 3.6253333333333333,
      "grad_norm": 0.11469676345586777,
      "learning_rate": 2.7341666666666666e-05,
      "loss": 0.0023,
      "step": 108760
    },
    {
      "epoch": 3.6256666666666666,
      "grad_norm": 0.3146072328090668,
      "learning_rate": 2.7339583333333335e-05,
      "loss": 0.0023,
      "step": 108770
    },
    {
      "epoch": 3.626,
      "grad_norm": 0.6518875360488892,
      "learning_rate": 2.73375e-05,
      "loss": 0.002,
      "step": 108780
    },
    {
      "epoch": 3.626333333333333,
      "grad_norm": 0.057574618607759476,
      "learning_rate": 2.733541666666667e-05,
      "loss": 0.0021,
      "step": 108790
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.057591356337070465,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0017,
      "step": 108800
    },
    {
      "epoch": 3.627,
      "grad_norm": 0.08602291345596313,
      "learning_rate": 2.7331250000000004e-05,
      "loss": 0.0024,
      "step": 108810
    },
    {
      "epoch": 3.6273333333333335,
      "grad_norm": 0.11452917009592056,
      "learning_rate": 2.7329166666666666e-05,
      "loss": 0.002,
      "step": 108820
    },
    {
      "epoch": 3.627666666666667,
      "grad_norm": 0.02908514067530632,
      "learning_rate": 2.7327083333333338e-05,
      "loss": 0.0026,
      "step": 108830
    },
    {
      "epoch": 3.628,
      "grad_norm": 0.08601534366607666,
      "learning_rate": 2.7325e-05,
      "loss": 0.0022,
      "step": 108840
    },
    {
      "epoch": 3.6283333333333334,
      "grad_norm": 0.2004266083240509,
      "learning_rate": 2.732291666666667e-05,
      "loss": 0.0023,
      "step": 108850
    },
    {
      "epoch": 3.6286666666666667,
      "grad_norm": 0.2574926018714905,
      "learning_rate": 2.7320833333333334e-05,
      "loss": 0.0026,
      "step": 108860
    },
    {
      "epoch": 3.629,
      "grad_norm": 0.4293065667152405,
      "learning_rate": 2.7318750000000003e-05,
      "loss": 0.0023,
      "step": 108870
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.009656990878283978,
      "learning_rate": 2.731666666666667e-05,
      "loss": 0.0018,
      "step": 108880
    },
    {
      "epoch": 3.6296666666666666,
      "grad_norm": 0.0861407220363617,
      "learning_rate": 2.731458333333333e-05,
      "loss": 0.0012,
      "step": 108890
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.6291574239730835,
      "learning_rate": 2.7312500000000003e-05,
      "loss": 0.0023,
      "step": 108900
    },
    {
      "epoch": 3.630333333333333,
      "grad_norm": 0.40073642134666443,
      "learning_rate": 2.7310416666666665e-05,
      "loss": 0.0023,
      "step": 108910
    },
    {
      "epoch": 3.6306666666666665,
      "grad_norm": 0.22911769151687622,
      "learning_rate": 2.7308333333333334e-05,
      "loss": 0.0013,
      "step": 108920
    },
    {
      "epoch": 3.6310000000000002,
      "grad_norm": 0.0033587198704481125,
      "learning_rate": 2.730625e-05,
      "loss": 0.0017,
      "step": 108930
    },
    {
      "epoch": 3.631333333333333,
      "grad_norm": 0.2863019108772278,
      "learning_rate": 2.730416666666667e-05,
      "loss": 0.002,
      "step": 108940
    },
    {
      "epoch": 3.631666666666667,
      "grad_norm": 0.05759206414222717,
      "learning_rate": 2.7302083333333334e-05,
      "loss": 0.002,
      "step": 108950
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.006312949117273092,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0041,
      "step": 108960
    },
    {
      "epoch": 3.6323333333333334,
      "grad_norm": 0.05773237347602844,
      "learning_rate": 2.729791666666667e-05,
      "loss": 0.0019,
      "step": 108970
    },
    {
      "epoch": 3.6326666666666667,
      "grad_norm": 0.2860592007637024,
      "learning_rate": 2.7295833333333337e-05,
      "loss": 0.0022,
      "step": 108980
    },
    {
      "epoch": 3.633,
      "grad_norm": 0.4005771577358246,
      "learning_rate": 2.7293750000000003e-05,
      "loss": 0.0013,
      "step": 108990
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.20038361847400665,
      "learning_rate": 2.7291666666666665e-05,
      "loss": 0.0022,
      "step": 109000
    },
    {
      "epoch": 3.6336666666666666,
      "grad_norm": 0.1430462747812271,
      "learning_rate": 2.7289583333333334e-05,
      "loss": 0.0022,
      "step": 109010
    },
    {
      "epoch": 3.634,
      "grad_norm": 0.057493310421705246,
      "learning_rate": 2.72875e-05,
      "loss": 0.0023,
      "step": 109020
    },
    {
      "epoch": 3.634333333333333,
      "grad_norm": 0.002860664390027523,
      "learning_rate": 2.7285416666666668e-05,
      "loss": 0.0018,
      "step": 109030
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.11474087834358215,
      "learning_rate": 2.7283333333333334e-05,
      "loss": 0.0026,
      "step": 109040
    },
    {
      "epoch": 3.635,
      "grad_norm": 0.11470133066177368,
      "learning_rate": 2.7281250000000002e-05,
      "loss": 0.0024,
      "step": 109050
    },
    {
      "epoch": 3.6353333333333335,
      "grad_norm": 0.08585793524980545,
      "learning_rate": 2.7279166666666668e-05,
      "loss": 0.0026,
      "step": 109060
    },
    {
      "epoch": 3.635666666666667,
      "grad_norm": 0.3434397876262665,
      "learning_rate": 2.7277083333333337e-05,
      "loss": 0.0018,
      "step": 109070
    },
    {
      "epoch": 3.636,
      "grad_norm": 0.21494917571544647,
      "learning_rate": 2.7275e-05,
      "loss": 0.0018,
      "step": 109080
    },
    {
      "epoch": 3.6363333333333334,
      "grad_norm": 0.4007546007633209,
      "learning_rate": 2.727291666666667e-05,
      "loss": 0.0011,
      "step": 109090
    },
    {
      "epoch": 3.6366666666666667,
      "grad_norm": 0.20037081837654114,
      "learning_rate": 2.7270833333333333e-05,
      "loss": 0.0016,
      "step": 109100
    },
    {
      "epoch": 3.637,
      "grad_norm": 0.22878845036029816,
      "learning_rate": 2.7268750000000006e-05,
      "loss": 0.0014,
      "step": 109110
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.08606602251529694,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0021,
      "step": 109120
    },
    {
      "epoch": 3.6376666666666666,
      "grad_norm": 0.17193138599395752,
      "learning_rate": 2.7264583333333333e-05,
      "loss": 0.002,
      "step": 109130
    },
    {
      "epoch": 3.638,
      "grad_norm": 0.171898752450943,
      "learning_rate": 2.7262500000000002e-05,
      "loss": 0.0018,
      "step": 109140
    },
    {
      "epoch": 3.638333333333333,
      "grad_norm": 0.05751969665288925,
      "learning_rate": 2.7260416666666667e-05,
      "loss": 0.0019,
      "step": 109150
    },
    {
      "epoch": 3.6386666666666665,
      "grad_norm": 0.2613046169281006,
      "learning_rate": 2.7258333333333336e-05,
      "loss": 0.0017,
      "step": 109160
    },
    {
      "epoch": 3.6390000000000002,
      "grad_norm": 0.3721461892127991,
      "learning_rate": 2.725625e-05,
      "loss": 0.0023,
      "step": 109170
    },
    {
      "epoch": 3.639333333333333,
      "grad_norm": 0.2861756980419159,
      "learning_rate": 2.725416666666667e-05,
      "loss": 0.0019,
      "step": 109180
    },
    {
      "epoch": 3.639666666666667,
      "grad_norm": 0.28607869148254395,
      "learning_rate": 2.7252083333333333e-05,
      "loss": 0.0021,
      "step": 109190
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.31474998593330383,
      "learning_rate": 2.725e-05,
      "loss": 0.0022,
      "step": 109200
    },
    {
      "epoch": 3.6403333333333334,
      "grad_norm": 0.20074564218521118,
      "learning_rate": 2.7247916666666667e-05,
      "loss": 0.0026,
      "step": 109210
    },
    {
      "epoch": 3.6406666666666667,
      "grad_norm": 0.20036204159259796,
      "learning_rate": 2.7245833333333336e-05,
      "loss": 0.0022,
      "step": 109220
    },
    {
      "epoch": 3.641,
      "grad_norm": 0.03345272317528725,
      "learning_rate": 2.724375e-05,
      "loss": 0.0025,
      "step": 109230
    },
    {
      "epoch": 3.6413333333333333,
      "grad_norm": 0.2865344285964966,
      "learning_rate": 2.7241666666666667e-05,
      "loss": 0.003,
      "step": 109240
    },
    {
      "epoch": 3.6416666666666666,
      "grad_norm": 0.171895831823349,
      "learning_rate": 2.7239583333333336e-05,
      "loss": 0.002,
      "step": 109250
    },
    {
      "epoch": 3.642,
      "grad_norm": 0.11467403918504715,
      "learning_rate": 2.7237499999999998e-05,
      "loss": 0.0017,
      "step": 109260
    },
    {
      "epoch": 3.642333333333333,
      "grad_norm": 0.08631599694490433,
      "learning_rate": 2.723541666666667e-05,
      "loss": 0.0021,
      "step": 109270
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.08637724071741104,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0024,
      "step": 109280
    },
    {
      "epoch": 3.643,
      "grad_norm": 0.34354615211486816,
      "learning_rate": 2.723125e-05,
      "loss": 0.0014,
      "step": 109290
    },
    {
      "epoch": 3.6433333333333335,
      "grad_norm": 0.0046993400901556015,
      "learning_rate": 2.7229166666666667e-05,
      "loss": 0.002,
      "step": 109300
    },
    {
      "epoch": 3.643666666666667,
      "grad_norm": 0.010745333507657051,
      "learning_rate": 2.7227083333333336e-05,
      "loss": 0.0017,
      "step": 109310
    },
    {
      "epoch": 3.644,
      "grad_norm": 0.3432263433933258,
      "learning_rate": 2.7225e-05,
      "loss": 0.0018,
      "step": 109320
    },
    {
      "epoch": 3.6443333333333334,
      "grad_norm": 0.08647827059030533,
      "learning_rate": 2.722291666666667e-05,
      "loss": 0.0016,
      "step": 109330
    },
    {
      "epoch": 3.6446666666666667,
      "grad_norm": 0.31472957134246826,
      "learning_rate": 2.7220833333333335e-05,
      "loss": 0.0018,
      "step": 109340
    },
    {
      "epoch": 3.645,
      "grad_norm": 0.4861772656440735,
      "learning_rate": 2.7218750000000004e-05,
      "loss": 0.0021,
      "step": 109350
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.1435101181268692,
      "learning_rate": 2.7216666666666666e-05,
      "loss": 0.0023,
      "step": 109360
    },
    {
      "epoch": 3.6456666666666666,
      "grad_norm": 0.11465587466955185,
      "learning_rate": 2.7214583333333332e-05,
      "loss": 0.0026,
      "step": 109370
    },
    {
      "epoch": 3.646,
      "grad_norm": 0.4464995265007019,
      "learning_rate": 2.72125e-05,
      "loss": 0.0023,
      "step": 109380
    },
    {
      "epoch": 3.646333333333333,
      "grad_norm": 0.005457320716232061,
      "learning_rate": 2.7210416666666666e-05,
      "loss": 0.003,
      "step": 109390
    },
    {
      "epoch": 3.6466666666666665,
      "grad_norm": 0.2862187623977661,
      "learning_rate": 2.7208333333333335e-05,
      "loss": 0.0024,
      "step": 109400
    },
    {
      "epoch": 3.6470000000000002,
      "grad_norm": 0.08601733297109604,
      "learning_rate": 2.720625e-05,
      "loss": 0.002,
      "step": 109410
    },
    {
      "epoch": 3.647333333333333,
      "grad_norm": 0.17156749963760376,
      "learning_rate": 2.720416666666667e-05,
      "loss": 0.0034,
      "step": 109420
    },
    {
      "epoch": 3.647666666666667,
      "grad_norm": 0.11452547460794449,
      "learning_rate": 2.7202083333333335e-05,
      "loss": 0.0025,
      "step": 109430
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.31479617953300476,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0026,
      "step": 109440
    },
    {
      "epoch": 3.6483333333333334,
      "grad_norm": 0.009645824320614338,
      "learning_rate": 2.7197916666666666e-05,
      "loss": 0.0024,
      "step": 109450
    },
    {
      "epoch": 3.6486666666666667,
      "grad_norm": 0.3719862699508667,
      "learning_rate": 2.7195833333333338e-05,
      "loss": 0.0021,
      "step": 109460
    },
    {
      "epoch": 3.649,
      "grad_norm": 0.28609713912010193,
      "learning_rate": 2.719375e-05,
      "loss": 0.0019,
      "step": 109470
    },
    {
      "epoch": 3.6493333333333333,
      "grad_norm": 0.08587977290153503,
      "learning_rate": 2.7191666666666666e-05,
      "loss": 0.0028,
      "step": 109480
    },
    {
      "epoch": 3.6496666666666666,
      "grad_norm": 0.08636613190174103,
      "learning_rate": 2.7189583333333335e-05,
      "loss": 0.0027,
      "step": 109490
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.05787499248981476,
      "learning_rate": 2.71875e-05,
      "loss": 0.0017,
      "step": 109500
    },
    {
      "epoch": 3.650333333333333,
      "grad_norm": 0.02884191833436489,
      "learning_rate": 2.718541666666667e-05,
      "loss": 0.0025,
      "step": 109510
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.17181794345378876,
      "learning_rate": 2.7183333333333335e-05,
      "loss": 0.0026,
      "step": 109520
    },
    {
      "epoch": 3.651,
      "grad_norm": 0.05712283030152321,
      "learning_rate": 2.7181250000000003e-05,
      "loss": 0.0018,
      "step": 109530
    },
    {
      "epoch": 3.6513333333333335,
      "grad_norm": 0.05716992914676666,
      "learning_rate": 2.7179166666666665e-05,
      "loss": 0.0023,
      "step": 109540
    },
    {
      "epoch": 3.6516666666666664,
      "grad_norm": 0.20040009915828705,
      "learning_rate": 2.7177083333333338e-05,
      "loss": 0.0017,
      "step": 109550
    },
    {
      "epoch": 3.652,
      "grad_norm": 0.02968513034284115,
      "learning_rate": 2.7175e-05,
      "loss": 0.0026,
      "step": 109560
    },
    {
      "epoch": 3.6523333333333334,
      "grad_norm": 0.008451500907540321,
      "learning_rate": 2.717291666666667e-05,
      "loss": 0.0017,
      "step": 109570
    },
    {
      "epoch": 3.6526666666666667,
      "grad_norm": 0.4578966796398163,
      "learning_rate": 2.7170833333333334e-05,
      "loss": 0.0024,
      "step": 109580
    },
    {
      "epoch": 3.653,
      "grad_norm": 0.2577946186065674,
      "learning_rate": 2.7168750000000003e-05,
      "loss": 0.0029,
      "step": 109590
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.11485575139522552,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0036,
      "step": 109600
    },
    {
      "epoch": 3.6536666666666666,
      "grad_norm": 0.20038598775863647,
      "learning_rate": 2.716458333333333e-05,
      "loss": 0.0021,
      "step": 109610
    },
    {
      "epoch": 3.654,
      "grad_norm": 0.005013142246752977,
      "learning_rate": 2.7162500000000003e-05,
      "loss": 0.0021,
      "step": 109620
    },
    {
      "epoch": 3.654333333333333,
      "grad_norm": 0.05838841199874878,
      "learning_rate": 2.7160416666666665e-05,
      "loss": 0.0028,
      "step": 109630
    },
    {
      "epoch": 3.6546666666666665,
      "grad_norm": 0.0858767107129097,
      "learning_rate": 2.7158333333333337e-05,
      "loss": 0.0014,
      "step": 109640
    },
    {
      "epoch": 3.6550000000000002,
      "grad_norm": 0.14341609179973602,
      "learning_rate": 2.715625e-05,
      "loss": 0.0016,
      "step": 109650
    },
    {
      "epoch": 3.655333333333333,
      "grad_norm": 0.3719739019870758,
      "learning_rate": 2.7154166666666668e-05,
      "loss": 0.0023,
      "step": 109660
    },
    {
      "epoch": 3.655666666666667,
      "grad_norm": 0.20037008821964264,
      "learning_rate": 2.7152083333333334e-05,
      "loss": 0.0024,
      "step": 109670
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.05744015797972679,
      "learning_rate": 2.7150000000000003e-05,
      "loss": 0.0025,
      "step": 109680
    },
    {
      "epoch": 3.6563333333333334,
      "grad_norm": 0.20005087554454803,
      "learning_rate": 2.7147916666666668e-05,
      "loss": 0.0022,
      "step": 109690
    },
    {
      "epoch": 3.6566666666666667,
      "grad_norm": 0.08609273284673691,
      "learning_rate": 2.7145833333333337e-05,
      "loss": 0.0026,
      "step": 109700
    },
    {
      "epoch": 3.657,
      "grad_norm": 0.5555488467216492,
      "learning_rate": 2.7143750000000002e-05,
      "loss": 0.0023,
      "step": 109710
    },
    {
      "epoch": 3.6573333333333333,
      "grad_norm": 0.17157407104969025,
      "learning_rate": 2.7141666666666665e-05,
      "loss": 0.0019,
      "step": 109720
    },
    {
      "epoch": 3.6576666666666666,
      "grad_norm": 0.010187395848333836,
      "learning_rate": 2.7139583333333333e-05,
      "loss": 0.0033,
      "step": 109730
    },
    {
      "epoch": 3.658,
      "grad_norm": 0.01184032391756773,
      "learning_rate": 2.71375e-05,
      "loss": 0.0016,
      "step": 109740
    },
    {
      "epoch": 3.658333333333333,
      "grad_norm": 0.549342930316925,
      "learning_rate": 2.7135416666666668e-05,
      "loss": 0.0027,
      "step": 109750
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.11472220718860626,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0023,
      "step": 109760
    },
    {
      "epoch": 3.659,
      "grad_norm": 0.11474751681089401,
      "learning_rate": 2.7131250000000002e-05,
      "loss": 0.0014,
      "step": 109770
    },
    {
      "epoch": 3.6593333333333335,
      "grad_norm": 0.14300872385501862,
      "learning_rate": 2.7129166666666668e-05,
      "loss": 0.0017,
      "step": 109780
    },
    {
      "epoch": 3.6596666666666664,
      "grad_norm": 0.02882503718137741,
      "learning_rate": 2.7127083333333337e-05,
      "loss": 0.0021,
      "step": 109790
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.3146841526031494,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 0.0019,
      "step": 109800
    },
    {
      "epoch": 3.6603333333333334,
      "grad_norm": 0.3430922031402588,
      "learning_rate": 2.712291666666667e-05,
      "loss": 0.0033,
      "step": 109810
    },
    {
      "epoch": 3.6606666666666667,
      "grad_norm": 0.1716080605983734,
      "learning_rate": 2.7120833333333333e-05,
      "loss": 0.0028,
      "step": 109820
    },
    {
      "epoch": 3.661,
      "grad_norm": 0.05745829641819,
      "learning_rate": 2.7118750000000005e-05,
      "loss": 0.0023,
      "step": 109830
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.02886858955025673,
      "learning_rate": 2.7116666666666667e-05,
      "loss": 0.0015,
      "step": 109840
    },
    {
      "epoch": 3.6616666666666666,
      "grad_norm": 0.03321651741862297,
      "learning_rate": 2.7114583333333333e-05,
      "loss": 0.0019,
      "step": 109850
    },
    {
      "epoch": 3.662,
      "grad_norm": 0.005101954564452171,
      "learning_rate": 2.7112500000000002e-05,
      "loss": 0.0022,
      "step": 109860
    },
    {
      "epoch": 3.662333333333333,
      "grad_norm": 0.057405050843954086,
      "learning_rate": 2.7110416666666667e-05,
      "loss": 0.0034,
      "step": 109870
    },
    {
      "epoch": 3.6626666666666665,
      "grad_norm": 0.3146674633026123,
      "learning_rate": 2.7108333333333336e-05,
      "loss": 0.0023,
      "step": 109880
    },
    {
      "epoch": 3.6630000000000003,
      "grad_norm": 0.05734992399811745,
      "learning_rate": 2.7106249999999998e-05,
      "loss": 0.0019,
      "step": 109890
    },
    {
      "epoch": 3.663333333333333,
      "grad_norm": 0.3519929051399231,
      "learning_rate": 2.710416666666667e-05,
      "loss": 0.0023,
      "step": 109900
    },
    {
      "epoch": 3.663666666666667,
      "grad_norm": 0.20025594532489777,
      "learning_rate": 2.7102083333333333e-05,
      "loss": 0.002,
      "step": 109910
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.20154665410518646,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0018,
      "step": 109920
    },
    {
      "epoch": 3.6643333333333334,
      "grad_norm": 0.1433449685573578,
      "learning_rate": 2.7097916666666667e-05,
      "loss": 0.002,
      "step": 109930
    },
    {
      "epoch": 3.6646666666666667,
      "grad_norm": 0.2573179304599762,
      "learning_rate": 2.7095833333333336e-05,
      "loss": 0.0023,
      "step": 109940
    },
    {
      "epoch": 3.665,
      "grad_norm": 0.2287832647562027,
      "learning_rate": 2.709375e-05,
      "loss": 0.0014,
      "step": 109950
    },
    {
      "epoch": 3.6653333333333333,
      "grad_norm": 0.002761990763247013,
      "learning_rate": 2.7091666666666667e-05,
      "loss": 0.0019,
      "step": 109960
    },
    {
      "epoch": 3.6656666666666666,
      "grad_norm": 0.3148247301578522,
      "learning_rate": 2.7089583333333336e-05,
      "loss": 0.0014,
      "step": 109970
    },
    {
      "epoch": 3.666,
      "grad_norm": 0.2000194936990738,
      "learning_rate": 2.7087499999999998e-05,
      "loss": 0.0029,
      "step": 109980
    },
    {
      "epoch": 3.666333333333333,
      "grad_norm": 0.17188696563243866,
      "learning_rate": 2.708541666666667e-05,
      "loss": 0.0021,
      "step": 109990
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.03295765817165375,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.0026,
      "step": 110000
    },
    {
      "epoch": 3.667,
      "grad_norm": 0.08637291938066483,
      "learning_rate": 2.708125e-05,
      "loss": 0.0025,
      "step": 110010
    },
    {
      "epoch": 3.6673333333333336,
      "grad_norm": 0.11438797414302826,
      "learning_rate": 2.7079166666666666e-05,
      "loss": 0.0014,
      "step": 110020
    },
    {
      "epoch": 3.6676666666666664,
      "grad_norm": 0.2001398205757141,
      "learning_rate": 2.7077083333333335e-05,
      "loss": 0.0025,
      "step": 110030
    },
    {
      "epoch": 3.668,
      "grad_norm": 0.086420439183712,
      "learning_rate": 2.7075e-05,
      "loss": 0.0016,
      "step": 110040
    },
    {
      "epoch": 3.6683333333333334,
      "grad_norm": 0.3641289174556732,
      "learning_rate": 2.707291666666667e-05,
      "loss": 0.0025,
      "step": 110050
    },
    {
      "epoch": 3.6686666666666667,
      "grad_norm": 0.257392555475235,
      "learning_rate": 2.7070833333333335e-05,
      "loss": 0.0026,
      "step": 110060
    },
    {
      "epoch": 3.669,
      "grad_norm": 0.14298276603221893,
      "learning_rate": 2.7068750000000004e-05,
      "loss": 0.0031,
      "step": 110070
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.31448128819465637,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0022,
      "step": 110080
    },
    {
      "epoch": 3.6696666666666666,
      "grad_norm": 0.029247449710965157,
      "learning_rate": 2.706458333333333e-05,
      "loss": 0.0021,
      "step": 110090
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.029418332502245903,
      "learning_rate": 2.70625e-05,
      "loss": 0.0025,
      "step": 110100
    },
    {
      "epoch": 3.6703333333333332,
      "grad_norm": 0.08612523227930069,
      "learning_rate": 2.7060416666666666e-05,
      "loss": 0.0023,
      "step": 110110
    },
    {
      "epoch": 3.6706666666666665,
      "grad_norm": 0.5456919074058533,
      "learning_rate": 2.7058333333333335e-05,
      "loss": 0.0026,
      "step": 110120
    },
    {
      "epoch": 3.6710000000000003,
      "grad_norm": 0.006479124538600445,
      "learning_rate": 2.705625e-05,
      "loss": 0.0023,
      "step": 110130
    },
    {
      "epoch": 3.671333333333333,
      "grad_norm": 0.11500091850757599,
      "learning_rate": 2.705416666666667e-05,
      "loss": 0.0014,
      "step": 110140
    },
    {
      "epoch": 3.671666666666667,
      "grad_norm": 0.25746989250183105,
      "learning_rate": 2.7052083333333335e-05,
      "loss": 0.002,
      "step": 110150
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.07453610748052597,
      "learning_rate": 2.7050000000000004e-05,
      "loss": 0.0023,
      "step": 110160
    },
    {
      "epoch": 3.6723333333333334,
      "grad_norm": 0.005482823122292757,
      "learning_rate": 2.7047916666666666e-05,
      "loss": 0.0017,
      "step": 110170
    },
    {
      "epoch": 3.6726666666666667,
      "grad_norm": 0.37240472435951233,
      "learning_rate": 2.7045833333333338e-05,
      "loss": 0.0012,
      "step": 110180
    },
    {
      "epoch": 3.673,
      "grad_norm": 0.11454760283231735,
      "learning_rate": 2.704375e-05,
      "loss": 0.0018,
      "step": 110190
    },
    {
      "epoch": 3.6733333333333333,
      "grad_norm": 0.42905688285827637,
      "learning_rate": 2.7041666666666672e-05,
      "loss": 0.0017,
      "step": 110200
    },
    {
      "epoch": 3.6736666666666666,
      "grad_norm": 0.08595885336399078,
      "learning_rate": 2.7039583333333334e-05,
      "loss": 0.002,
      "step": 110210
    },
    {
      "epoch": 3.674,
      "grad_norm": 0.22883667051792145,
      "learning_rate": 2.70375e-05,
      "loss": 0.0023,
      "step": 110220
    },
    {
      "epoch": 3.6743333333333332,
      "grad_norm": 0.11444754898548126,
      "learning_rate": 2.703541666666667e-05,
      "loss": 0.0027,
      "step": 110230
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.22882306575775146,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.0026,
      "step": 110240
    },
    {
      "epoch": 3.675,
      "grad_norm": 0.37186843156814575,
      "learning_rate": 2.7031250000000003e-05,
      "loss": 0.002,
      "step": 110250
    },
    {
      "epoch": 3.6753333333333336,
      "grad_norm": 0.08664891868829727,
      "learning_rate": 2.7029166666666665e-05,
      "loss": 0.0016,
      "step": 110260
    },
    {
      "epoch": 3.6756666666666664,
      "grad_norm": 0.3716795742511749,
      "learning_rate": 2.7027083333333337e-05,
      "loss": 0.0023,
      "step": 110270
    },
    {
      "epoch": 3.676,
      "grad_norm": 0.11461145430803299,
      "learning_rate": 2.7025e-05,
      "loss": 0.0021,
      "step": 110280
    },
    {
      "epoch": 3.6763333333333335,
      "grad_norm": 0.11500339955091476,
      "learning_rate": 2.702291666666667e-05,
      "loss": 0.0019,
      "step": 110290
    },
    {
      "epoch": 3.6766666666666667,
      "grad_norm": 0.0587322935461998,
      "learning_rate": 2.7020833333333334e-05,
      "loss": 0.0032,
      "step": 110300
    },
    {
      "epoch": 3.677,
      "grad_norm": 0.006446933839470148,
      "learning_rate": 2.7018750000000003e-05,
      "loss": 0.0021,
      "step": 110310
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.0575767382979393,
      "learning_rate": 2.701666666666667e-05,
      "loss": 0.0022,
      "step": 110320
    },
    {
      "epoch": 3.6776666666666666,
      "grad_norm": 0.2759518325328827,
      "learning_rate": 2.701458333333333e-05,
      "loss": 0.0029,
      "step": 110330
    },
    {
      "epoch": 3.678,
      "grad_norm": 0.0577203631401062,
      "learning_rate": 2.7012500000000003e-05,
      "loss": 0.0024,
      "step": 110340
    },
    {
      "epoch": 3.6783333333333332,
      "grad_norm": 0.17292921245098114,
      "learning_rate": 2.7010416666666665e-05,
      "loss": 0.0026,
      "step": 110350
    },
    {
      "epoch": 3.6786666666666665,
      "grad_norm": 0.4006485342979431,
      "learning_rate": 2.7008333333333337e-05,
      "loss": 0.0016,
      "step": 110360
    },
    {
      "epoch": 3.6790000000000003,
      "grad_norm": 0.25762322545051575,
      "learning_rate": 2.700625e-05,
      "loss": 0.002,
      "step": 110370
    },
    {
      "epoch": 3.679333333333333,
      "grad_norm": 0.0310369860380888,
      "learning_rate": 2.7004166666666668e-05,
      "loss": 0.0022,
      "step": 110380
    },
    {
      "epoch": 3.679666666666667,
      "grad_norm": 0.45760291814804077,
      "learning_rate": 2.7002083333333334e-05,
      "loss": 0.0019,
      "step": 110390
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.4860967993736267,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0022,
      "step": 110400
    },
    {
      "epoch": 3.6803333333333335,
      "grad_norm": 0.3145984411239624,
      "learning_rate": 2.6997916666666668e-05,
      "loss": 0.0021,
      "step": 110410
    },
    {
      "epoch": 3.6806666666666668,
      "grad_norm": 0.19859440624713898,
      "learning_rate": 2.6995833333333337e-05,
      "loss": 0.0019,
      "step": 110420
    },
    {
      "epoch": 3.681,
      "grad_norm": 0.1429058015346527,
      "learning_rate": 2.6993750000000002e-05,
      "loss": 0.002,
      "step": 110430
    },
    {
      "epoch": 3.6813333333333333,
      "grad_norm": 0.1469937413930893,
      "learning_rate": 2.699166666666667e-05,
      "loss": 0.0026,
      "step": 110440
    },
    {
      "epoch": 3.6816666666666666,
      "grad_norm": 0.5148658156394958,
      "learning_rate": 2.6989583333333333e-05,
      "loss": 0.0032,
      "step": 110450
    },
    {
      "epoch": 3.682,
      "grad_norm": 0.02919722907245159,
      "learning_rate": 2.69875e-05,
      "loss": 0.0025,
      "step": 110460
    },
    {
      "epoch": 3.6823333333333332,
      "grad_norm": 0.03050435334444046,
      "learning_rate": 2.6985416666666668e-05,
      "loss": 0.0021,
      "step": 110470
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.1718231588602066,
      "learning_rate": 2.6983333333333333e-05,
      "loss": 0.0024,
      "step": 110480
    },
    {
      "epoch": 3.683,
      "grad_norm": 0.2861041724681854,
      "learning_rate": 2.6981250000000002e-05,
      "loss": 0.0023,
      "step": 110490
    },
    {
      "epoch": 3.6833333333333336,
      "grad_norm": 0.11466608196496964,
      "learning_rate": 2.6979166666666667e-05,
      "loss": 0.0023,
      "step": 110500
    },
    {
      "epoch": 3.6836666666666664,
      "grad_norm": 0.03405224159359932,
      "learning_rate": 2.6977083333333336e-05,
      "loss": 0.0021,
      "step": 110510
    },
    {
      "epoch": 3.684,
      "grad_norm": 0.17240719497203827,
      "learning_rate": 2.6975000000000002e-05,
      "loss": 0.0021,
      "step": 110520
    },
    {
      "epoch": 3.6843333333333335,
      "grad_norm": 0.05814240500330925,
      "learning_rate": 2.697291666666667e-05,
      "loss": 0.0024,
      "step": 110530
    },
    {
      "epoch": 3.6846666666666668,
      "grad_norm": 0.9493618607521057,
      "learning_rate": 2.6970833333333333e-05,
      "loss": 0.0034,
      "step": 110540
    },
    {
      "epoch": 3.685,
      "grad_norm": 0.5892764329910278,
      "learning_rate": 2.6968750000000005e-05,
      "loss": 0.0027,
      "step": 110550
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.6772152781486511,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.0018,
      "step": 110560
    },
    {
      "epoch": 3.6856666666666666,
      "grad_norm": 0.0574311688542366,
      "learning_rate": 2.6964583333333333e-05,
      "loss": 0.0022,
      "step": 110570
    },
    {
      "epoch": 3.686,
      "grad_norm": 0.05812208727002144,
      "learning_rate": 2.69625e-05,
      "loss": 0.0034,
      "step": 110580
    },
    {
      "epoch": 3.6863333333333332,
      "grad_norm": 0.030522175133228302,
      "learning_rate": 2.6960416666666667e-05,
      "loss": 0.002,
      "step": 110590
    },
    {
      "epoch": 3.6866666666666665,
      "grad_norm": 0.004693527705967426,
      "learning_rate": 2.6958333333333336e-05,
      "loss": 0.0014,
      "step": 110600
    },
    {
      "epoch": 3.6870000000000003,
      "grad_norm": 0.08577720075845718,
      "learning_rate": 2.6956249999999998e-05,
      "loss": 0.0015,
      "step": 110610
    },
    {
      "epoch": 3.687333333333333,
      "grad_norm": 0.14320725202560425,
      "learning_rate": 2.695416666666667e-05,
      "loss": 0.0018,
      "step": 110620
    },
    {
      "epoch": 3.687666666666667,
      "grad_norm": 0.1721167117357254,
      "learning_rate": 2.6952083333333332e-05,
      "loss": 0.0025,
      "step": 110630
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.2574436068534851,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 0.0018,
      "step": 110640
    },
    {
      "epoch": 3.6883333333333335,
      "grad_norm": 0.08592789620161057,
      "learning_rate": 2.6947916666666667e-05,
      "loss": 0.0025,
      "step": 110650
    },
    {
      "epoch": 3.6886666666666668,
      "grad_norm": 0.2007235735654831,
      "learning_rate": 2.6945833333333336e-05,
      "loss": 0.0021,
      "step": 110660
    },
    {
      "epoch": 3.689,
      "grad_norm": 0.40065544843673706,
      "learning_rate": 2.694375e-05,
      "loss": 0.0024,
      "step": 110670
    },
    {
      "epoch": 3.6893333333333334,
      "grad_norm": 0.20032519102096558,
      "learning_rate": 2.694166666666667e-05,
      "loss": 0.0023,
      "step": 110680
    },
    {
      "epoch": 3.6896666666666667,
      "grad_norm": 0.17182877659797668,
      "learning_rate": 2.6939583333333335e-05,
      "loss": 0.0021,
      "step": 110690
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.14323443174362183,
      "learning_rate": 2.6937499999999997e-05,
      "loss": 0.0019,
      "step": 110700
    },
    {
      "epoch": 3.6903333333333332,
      "grad_norm": 0.05758551135659218,
      "learning_rate": 2.693541666666667e-05,
      "loss": 0.002,
      "step": 110710
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.3432907462120056,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0017,
      "step": 110720
    },
    {
      "epoch": 3.691,
      "grad_norm": 0.11509019881486893,
      "learning_rate": 2.693125e-05,
      "loss": 0.0037,
      "step": 110730
    },
    {
      "epoch": 3.6913333333333336,
      "grad_norm": 0.13482549786567688,
      "learning_rate": 2.6929166666666666e-05,
      "loss": 0.0027,
      "step": 110740
    },
    {
      "epoch": 3.6916666666666664,
      "grad_norm": 0.3719603419303894,
      "learning_rate": 2.6927083333333335e-05,
      "loss": 0.0018,
      "step": 110750
    },
    {
      "epoch": 3.692,
      "grad_norm": 0.7428842782974243,
      "learning_rate": 2.6925e-05,
      "loss": 0.0027,
      "step": 110760
    },
    {
      "epoch": 3.6923333333333335,
      "grad_norm": 0.17213881015777588,
      "learning_rate": 2.692291666666667e-05,
      "loss": 0.0025,
      "step": 110770
    },
    {
      "epoch": 3.6926666666666668,
      "grad_norm": 0.08623624593019485,
      "learning_rate": 2.6920833333333335e-05,
      "loss": 0.0025,
      "step": 110780
    },
    {
      "epoch": 3.693,
      "grad_norm": 0.22923463582992554,
      "learning_rate": 2.6918750000000004e-05,
      "loss": 0.0028,
      "step": 110790
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.1717279702425003,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.0022,
      "step": 110800
    },
    {
      "epoch": 3.6936666666666667,
      "grad_norm": 0.02905394323170185,
      "learning_rate": 2.691458333333333e-05,
      "loss": 0.0023,
      "step": 110810
    },
    {
      "epoch": 3.694,
      "grad_norm": 0.14305023849010468,
      "learning_rate": 2.69125e-05,
      "loss": 0.0021,
      "step": 110820
    },
    {
      "epoch": 3.6943333333333332,
      "grad_norm": 0.2002612054347992,
      "learning_rate": 2.6910416666666666e-05,
      "loss": 0.0022,
      "step": 110830
    },
    {
      "epoch": 3.6946666666666665,
      "grad_norm": 0.31486305594444275,
      "learning_rate": 2.6908333333333335e-05,
      "loss": 0.0016,
      "step": 110840
    },
    {
      "epoch": 3.695,
      "grad_norm": 0.05728742852807045,
      "learning_rate": 2.690625e-05,
      "loss": 0.002,
      "step": 110850
    },
    {
      "epoch": 3.695333333333333,
      "grad_norm": 0.5147929191589355,
      "learning_rate": 2.690416666666667e-05,
      "loss": 0.0021,
      "step": 110860
    },
    {
      "epoch": 3.695666666666667,
      "grad_norm": 0.030689328908920288,
      "learning_rate": 2.6902083333333334e-05,
      "loss": 0.0026,
      "step": 110870
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.14339464902877808,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.0028,
      "step": 110880
    },
    {
      "epoch": 3.6963333333333335,
      "grad_norm": 0.05749242380261421,
      "learning_rate": 2.6897916666666665e-05,
      "loss": 0.0026,
      "step": 110890
    },
    {
      "epoch": 3.6966666666666668,
      "grad_norm": 0.2575546205043793,
      "learning_rate": 2.6895833333333338e-05,
      "loss": 0.0024,
      "step": 110900
    },
    {
      "epoch": 3.697,
      "grad_norm": 0.026722358539700508,
      "learning_rate": 2.689375e-05,
      "loss": 0.0017,
      "step": 110910
    },
    {
      "epoch": 3.6973333333333334,
      "grad_norm": 0.17197461426258087,
      "learning_rate": 2.6891666666666672e-05,
      "loss": 0.0022,
      "step": 110920
    },
    {
      "epoch": 3.6976666666666667,
      "grad_norm": 0.34324878454208374,
      "learning_rate": 2.6889583333333334e-05,
      "loss": 0.0019,
      "step": 110930
    },
    {
      "epoch": 3.698,
      "grad_norm": 0.030116217210888863,
      "learning_rate": 2.68875e-05,
      "loss": 0.0029,
      "step": 110940
    },
    {
      "epoch": 3.6983333333333333,
      "grad_norm": 0.20011815428733826,
      "learning_rate": 2.688541666666667e-05,
      "loss": 0.0023,
      "step": 110950
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.05749781057238579,
      "learning_rate": 2.6883333333333334e-05,
      "loss": 0.0019,
      "step": 110960
    },
    {
      "epoch": 3.699,
      "grad_norm": 0.028804289177060127,
      "learning_rate": 2.6881250000000003e-05,
      "loss": 0.0019,
      "step": 110970
    },
    {
      "epoch": 3.6993333333333336,
      "grad_norm": 0.2575984001159668,
      "learning_rate": 2.6879166666666665e-05,
      "loss": 0.0028,
      "step": 110980
    },
    {
      "epoch": 3.6996666666666664,
      "grad_norm": 0.05728951841592789,
      "learning_rate": 2.6877083333333337e-05,
      "loss": 0.0018,
      "step": 110990
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.2575035095214844,
      "learning_rate": 2.6875e-05,
      "loss": 0.0023,
      "step": 111000
    },
    {
      "epoch": 3.7003333333333335,
      "grad_norm": 0.0041692424565553665,
      "learning_rate": 2.6872916666666668e-05,
      "loss": 0.0024,
      "step": 111010
    },
    {
      "epoch": 3.7006666666666668,
      "grad_norm": 0.17193642258644104,
      "learning_rate": 2.6870833333333334e-05,
      "loss": 0.0022,
      "step": 111020
    },
    {
      "epoch": 3.701,
      "grad_norm": 0.20012834668159485,
      "learning_rate": 2.6868750000000003e-05,
      "loss": 0.0017,
      "step": 111030
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.0293684471398592,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0019,
      "step": 111040
    },
    {
      "epoch": 3.7016666666666667,
      "grad_norm": 0.22882752120494843,
      "learning_rate": 2.6864583333333334e-05,
      "loss": 0.0028,
      "step": 111050
    },
    {
      "epoch": 3.702,
      "grad_norm": 0.05750775337219238,
      "learning_rate": 2.6862500000000002e-05,
      "loss": 0.002,
      "step": 111060
    },
    {
      "epoch": 3.7023333333333333,
      "grad_norm": 0.4004110097885132,
      "learning_rate": 2.6860416666666665e-05,
      "loss": 0.0027,
      "step": 111070
    },
    {
      "epoch": 3.7026666666666666,
      "grad_norm": 0.5435023903846741,
      "learning_rate": 2.6858333333333337e-05,
      "loss": 0.0026,
      "step": 111080
    },
    {
      "epoch": 3.703,
      "grad_norm": 0.7436069250106812,
      "learning_rate": 2.685625e-05,
      "loss": 0.0011,
      "step": 111090
    },
    {
      "epoch": 3.703333333333333,
      "grad_norm": 0.48644211888313293,
      "learning_rate": 2.6854166666666668e-05,
      "loss": 0.0031,
      "step": 111100
    },
    {
      "epoch": 3.703666666666667,
      "grad_norm": 0.7046042084693909,
      "learning_rate": 2.6852083333333333e-05,
      "loss": 0.0023,
      "step": 111110
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.058341365307569504,
      "learning_rate": 2.6850000000000002e-05,
      "loss": 0.0028,
      "step": 111120
    },
    {
      "epoch": 3.7043333333333335,
      "grad_norm": 0.00671260803937912,
      "learning_rate": 2.6847916666666668e-05,
      "loss": 0.0017,
      "step": 111130
    },
    {
      "epoch": 3.7046666666666668,
      "grad_norm": 0.08608676493167877,
      "learning_rate": 2.6845833333333336e-05,
      "loss": 0.0044,
      "step": 111140
    },
    {
      "epoch": 3.705,
      "grad_norm": 0.013017835095524788,
      "learning_rate": 2.6843750000000002e-05,
      "loss": 0.0023,
      "step": 111150
    },
    {
      "epoch": 3.7053333333333334,
      "grad_norm": 0.28879857063293457,
      "learning_rate": 2.684166666666667e-05,
      "loss": 0.0034,
      "step": 111160
    },
    {
      "epoch": 3.7056666666666667,
      "grad_norm": 0.09951654821634293,
      "learning_rate": 2.6839583333333333e-05,
      "loss": 0.0024,
      "step": 111170
    },
    {
      "epoch": 3.706,
      "grad_norm": 0.029866062104701996,
      "learning_rate": 2.68375e-05,
      "loss": 0.0026,
      "step": 111180
    },
    {
      "epoch": 3.7063333333333333,
      "grad_norm": 0.3656580150127411,
      "learning_rate": 2.6835416666666667e-05,
      "loss": 0.0026,
      "step": 111190
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.02874433621764183,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0022,
      "step": 111200
    },
    {
      "epoch": 3.707,
      "grad_norm": 0.28593710064888,
      "learning_rate": 2.683125e-05,
      "loss": 0.0024,
      "step": 111210
    },
    {
      "epoch": 3.7073333333333336,
      "grad_norm": 0.08851870894432068,
      "learning_rate": 2.6829166666666667e-05,
      "loss": 0.0038,
      "step": 111220
    },
    {
      "epoch": 3.7076666666666664,
      "grad_norm": 0.008807848207652569,
      "learning_rate": 2.6827083333333336e-05,
      "loss": 0.0032,
      "step": 111230
    },
    {
      "epoch": 3.708,
      "grad_norm": 0.11455927044153214,
      "learning_rate": 2.6825e-05,
      "loss": 0.0022,
      "step": 111240
    },
    {
      "epoch": 3.7083333333333335,
      "grad_norm": 0.28714054822921753,
      "learning_rate": 2.682291666666667e-05,
      "loss": 0.0022,
      "step": 111250
    },
    {
      "epoch": 3.708666666666667,
      "grad_norm": 0.14357151091098785,
      "learning_rate": 2.6820833333333332e-05,
      "loss": 0.0026,
      "step": 111260
    },
    {
      "epoch": 3.709,
      "grad_norm": 0.28805798292160034,
      "learning_rate": 2.6818750000000005e-05,
      "loss": 0.0019,
      "step": 111270
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.11492703855037689,
      "learning_rate": 2.6816666666666667e-05,
      "loss": 0.0021,
      "step": 111280
    },
    {
      "epoch": 3.7096666666666667,
      "grad_norm": 0.08574170619249344,
      "learning_rate": 2.6814583333333336e-05,
      "loss": 0.0016,
      "step": 111290
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.02990427240729332,
      "learning_rate": 2.68125e-05,
      "loss": 0.0015,
      "step": 111300
    },
    {
      "epoch": 3.7103333333333333,
      "grad_norm": 0.11488334834575653,
      "learning_rate": 2.6810416666666667e-05,
      "loss": 0.0021,
      "step": 111310
    },
    {
      "epoch": 3.7106666666666666,
      "grad_norm": 0.34336376190185547,
      "learning_rate": 2.6808333333333336e-05,
      "loss": 0.0017,
      "step": 111320
    },
    {
      "epoch": 3.711,
      "grad_norm": 0.02907629683613777,
      "learning_rate": 2.680625e-05,
      "loss": 0.0025,
      "step": 111330
    },
    {
      "epoch": 3.711333333333333,
      "grad_norm": 0.14312590658664703,
      "learning_rate": 2.680416666666667e-05,
      "loss": 0.003,
      "step": 111340
    },
    {
      "epoch": 3.711666666666667,
      "grad_norm": 0.20015205442905426,
      "learning_rate": 2.6802083333333332e-05,
      "loss": 0.0017,
      "step": 111350
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.05745241045951843,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0023,
      "step": 111360
    },
    {
      "epoch": 3.7123333333333335,
      "grad_norm": 0.030883457511663437,
      "learning_rate": 2.6797916666666666e-05,
      "loss": 0.0022,
      "step": 111370
    },
    {
      "epoch": 3.712666666666667,
      "grad_norm": 0.3721092641353607,
      "learning_rate": 2.6795833333333335e-05,
      "loss": 0.0024,
      "step": 111380
    },
    {
      "epoch": 3.713,
      "grad_norm": 0.20086877048015594,
      "learning_rate": 2.679375e-05,
      "loss": 0.002,
      "step": 111390
    },
    {
      "epoch": 3.7133333333333334,
      "grad_norm": 0.2576754689216614,
      "learning_rate": 2.679166666666667e-05,
      "loss": 0.0024,
      "step": 111400
    },
    {
      "epoch": 3.7136666666666667,
      "grad_norm": 0.28603503108024597,
      "learning_rate": 2.6789583333333335e-05,
      "loss": 0.0022,
      "step": 111410
    },
    {
      "epoch": 3.714,
      "grad_norm": 0.6942786574363708,
      "learning_rate": 2.6787499999999997e-05,
      "loss": 0.002,
      "step": 111420
    },
    {
      "epoch": 3.7143333333333333,
      "grad_norm": 0.03353734314441681,
      "learning_rate": 2.678541666666667e-05,
      "loss": 0.0015,
      "step": 111430
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.0575423389673233,
      "learning_rate": 2.678333333333333e-05,
      "loss": 0.0021,
      "step": 111440
    },
    {
      "epoch": 3.715,
      "grad_norm": 0.114474818110466,
      "learning_rate": 2.6781250000000004e-05,
      "loss": 0.002,
      "step": 111450
    },
    {
      "epoch": 3.7153333333333336,
      "grad_norm": 0.08579359948635101,
      "learning_rate": 2.6779166666666666e-05,
      "loss": 0.0032,
      "step": 111460
    },
    {
      "epoch": 3.7156666666666665,
      "grad_norm": 0.34325987100601196,
      "learning_rate": 2.6777083333333335e-05,
      "loss": 0.0018,
      "step": 111470
    },
    {
      "epoch": 3.716,
      "grad_norm": 0.029120828956365585,
      "learning_rate": 2.6775e-05,
      "loss": 0.0018,
      "step": 111480
    },
    {
      "epoch": 3.7163333333333335,
      "grad_norm": 0.20005539059638977,
      "learning_rate": 2.677291666666667e-05,
      "loss": 0.0027,
      "step": 111490
    },
    {
      "epoch": 3.716666666666667,
      "grad_norm": 0.08602384477853775,
      "learning_rate": 2.6770833333333335e-05,
      "loss": 0.0023,
      "step": 111500
    },
    {
      "epoch": 3.717,
      "grad_norm": 0.3146043121814728,
      "learning_rate": 2.6768750000000004e-05,
      "loss": 0.0018,
      "step": 111510
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.05743669345974922,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.0021,
      "step": 111520
    },
    {
      "epoch": 3.7176666666666667,
      "grad_norm": 0.3428003787994385,
      "learning_rate": 2.6764583333333338e-05,
      "loss": 0.0022,
      "step": 111530
    },
    {
      "epoch": 3.718,
      "grad_norm": 0.31454306840896606,
      "learning_rate": 2.67625e-05,
      "loss": 0.0025,
      "step": 111540
    },
    {
      "epoch": 3.7183333333333333,
      "grad_norm": 0.05781422555446625,
      "learning_rate": 2.6760416666666665e-05,
      "loss": 0.0019,
      "step": 111550
    },
    {
      "epoch": 3.7186666666666666,
      "grad_norm": 0.1714484691619873,
      "learning_rate": 2.6758333333333334e-05,
      "loss": 0.0024,
      "step": 111560
    },
    {
      "epoch": 3.719,
      "grad_norm": 0.029272381216287613,
      "learning_rate": 2.675625e-05,
      "loss": 0.0022,
      "step": 111570
    },
    {
      "epoch": 3.719333333333333,
      "grad_norm": 0.2857830822467804,
      "learning_rate": 2.675416666666667e-05,
      "loss": 0.0022,
      "step": 111580
    },
    {
      "epoch": 3.719666666666667,
      "grad_norm": 0.05787848308682442,
      "learning_rate": 2.6752083333333334e-05,
      "loss": 0.0024,
      "step": 111590
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.08582112938165665,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.0024,
      "step": 111600
    },
    {
      "epoch": 3.7203333333333335,
      "grad_norm": 0.25755369663238525,
      "learning_rate": 2.674791666666667e-05,
      "loss": 0.0024,
      "step": 111610
    },
    {
      "epoch": 3.720666666666667,
      "grad_norm": 0.02995673008263111,
      "learning_rate": 2.6745833333333337e-05,
      "loss": 0.0021,
      "step": 111620
    },
    {
      "epoch": 3.721,
      "grad_norm": 0.3717622756958008,
      "learning_rate": 2.674375e-05,
      "loss": 0.002,
      "step": 111630
    },
    {
      "epoch": 3.7213333333333334,
      "grad_norm": 0.1719646006822586,
      "learning_rate": 2.6741666666666672e-05,
      "loss": 0.0024,
      "step": 111640
    },
    {
      "epoch": 3.7216666666666667,
      "grad_norm": 0.1431792676448822,
      "learning_rate": 2.6739583333333334e-05,
      "loss": 0.0018,
      "step": 111650
    },
    {
      "epoch": 3.722,
      "grad_norm": 0.228941410779953,
      "learning_rate": 2.67375e-05,
      "loss": 0.0018,
      "step": 111660
    },
    {
      "epoch": 3.7223333333333333,
      "grad_norm": 0.029318958520889282,
      "learning_rate": 2.6735416666666668e-05,
      "loss": 0.0016,
      "step": 111670
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.003992313053458929,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0025,
      "step": 111680
    },
    {
      "epoch": 3.723,
      "grad_norm": 0.14295406639575958,
      "learning_rate": 2.6731250000000003e-05,
      "loss": 0.0019,
      "step": 111690
    },
    {
      "epoch": 3.7233333333333336,
      "grad_norm": 0.22904863953590393,
      "learning_rate": 2.6729166666666665e-05,
      "loss": 0.0022,
      "step": 111700
    },
    {
      "epoch": 3.7236666666666665,
      "grad_norm": 0.20043273270130157,
      "learning_rate": 2.6727083333333337e-05,
      "loss": 0.0017,
      "step": 111710
    },
    {
      "epoch": 3.724,
      "grad_norm": 0.029270850121974945,
      "learning_rate": 2.6725e-05,
      "loss": 0.0021,
      "step": 111720
    },
    {
      "epoch": 3.7243333333333335,
      "grad_norm": 0.28596094250679016,
      "learning_rate": 2.672291666666667e-05,
      "loss": 0.0018,
      "step": 111730
    },
    {
      "epoch": 3.724666666666667,
      "grad_norm": 0.17186221480369568,
      "learning_rate": 2.6720833333333333e-05,
      "loss": 0.0016,
      "step": 111740
    },
    {
      "epoch": 3.725,
      "grad_norm": 0.11446850746870041,
      "learning_rate": 2.6718750000000002e-05,
      "loss": 0.0028,
      "step": 111750
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.2289588749408722,
      "learning_rate": 2.6716666666666668e-05,
      "loss": 0.003,
      "step": 111760
    },
    {
      "epoch": 3.7256666666666667,
      "grad_norm": 0.05787404254078865,
      "learning_rate": 2.6714583333333337e-05,
      "loss": 0.0021,
      "step": 111770
    },
    {
      "epoch": 3.726,
      "grad_norm": 0.05741838365793228,
      "learning_rate": 2.6712500000000002e-05,
      "loss": 0.0024,
      "step": 111780
    },
    {
      "epoch": 3.7263333333333333,
      "grad_norm": 0.22873876988887787,
      "learning_rate": 2.6710416666666664e-05,
      "loss": 0.0023,
      "step": 111790
    },
    {
      "epoch": 3.7266666666666666,
      "grad_norm": 0.17150180041790009,
      "learning_rate": 2.6708333333333337e-05,
      "loss": 0.0026,
      "step": 111800
    },
    {
      "epoch": 3.727,
      "grad_norm": 0.28649842739105225,
      "learning_rate": 2.670625e-05,
      "loss": 0.0017,
      "step": 111810
    },
    {
      "epoch": 3.727333333333333,
      "grad_norm": 0.2004929482936859,
      "learning_rate": 2.6704166666666668e-05,
      "loss": 0.0026,
      "step": 111820
    },
    {
      "epoch": 3.727666666666667,
      "grad_norm": 0.12699852883815765,
      "learning_rate": 2.6702083333333333e-05,
      "loss": 0.0026,
      "step": 111830
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.028684310615062714,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0022,
      "step": 111840
    },
    {
      "epoch": 3.7283333333333335,
      "grad_norm": 0.28556573390960693,
      "learning_rate": 2.6697916666666667e-05,
      "loss": 0.0018,
      "step": 111850
    },
    {
      "epoch": 3.728666666666667,
      "grad_norm": 0.05739397183060646,
      "learning_rate": 2.6695833333333336e-05,
      "loss": 0.003,
      "step": 111860
    },
    {
      "epoch": 3.729,
      "grad_norm": 0.22874630987644196,
      "learning_rate": 2.6693750000000002e-05,
      "loss": 0.0012,
      "step": 111870
    },
    {
      "epoch": 3.7293333333333334,
      "grad_norm": 0.08647090196609497,
      "learning_rate": 2.669166666666667e-05,
      "loss": 0.002,
      "step": 111880
    },
    {
      "epoch": 3.7296666666666667,
      "grad_norm": 0.2001800686120987,
      "learning_rate": 2.6689583333333336e-05,
      "loss": 0.0027,
      "step": 111890
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.257656455039978,
      "learning_rate": 2.6687499999999998e-05,
      "loss": 0.0016,
      "step": 111900
    },
    {
      "epoch": 3.7303333333333333,
      "grad_norm": 0.05817199498414993,
      "learning_rate": 2.6685416666666667e-05,
      "loss": 0.0014,
      "step": 111910
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.0860842913389206,
      "learning_rate": 2.6683333333333333e-05,
      "loss": 0.0012,
      "step": 111920
    },
    {
      "epoch": 3.731,
      "grad_norm": 0.08580265194177628,
      "learning_rate": 2.668125e-05,
      "loss": 0.0023,
      "step": 111930
    },
    {
      "epoch": 3.731333333333333,
      "grad_norm": 0.14329826831817627,
      "learning_rate": 2.6679166666666667e-05,
      "loss": 0.0024,
      "step": 111940
    },
    {
      "epoch": 3.7316666666666665,
      "grad_norm": 0.005171001423150301,
      "learning_rate": 2.6677083333333336e-05,
      "loss": 0.0025,
      "step": 111950
    },
    {
      "epoch": 3.732,
      "grad_norm": 0.2568211853504181,
      "learning_rate": 2.6675e-05,
      "loss": 0.0025,
      "step": 111960
    },
    {
      "epoch": 3.732333333333333,
      "grad_norm": 0.11426705867052078,
      "learning_rate": 2.667291666666667e-05,
      "loss": 0.0024,
      "step": 111970
    },
    {
      "epoch": 3.732666666666667,
      "grad_norm": 0.057646315544843674,
      "learning_rate": 2.6670833333333332e-05,
      "loss": 0.0024,
      "step": 111980
    },
    {
      "epoch": 3.733,
      "grad_norm": 0.3660182058811188,
      "learning_rate": 2.6668750000000004e-05,
      "loss": 0.0018,
      "step": 111990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.003093322506174445,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0013,
      "step": 112000
    },
    {
      "epoch": 3.7336666666666667,
      "grad_norm": 0.3434106707572937,
      "learning_rate": 2.666458333333334e-05,
      "loss": 0.0022,
      "step": 112010
    },
    {
      "epoch": 3.734,
      "grad_norm": 0.08592883497476578,
      "learning_rate": 2.66625e-05,
      "loss": 0.0018,
      "step": 112020
    },
    {
      "epoch": 3.7343333333333333,
      "grad_norm": 0.45749667286872864,
      "learning_rate": 2.6660416666666666e-05,
      "loss": 0.0022,
      "step": 112030
    },
    {
      "epoch": 3.7346666666666666,
      "grad_norm": 0.1717052459716797,
      "learning_rate": 2.6658333333333335e-05,
      "loss": 0.0021,
      "step": 112040
    },
    {
      "epoch": 3.735,
      "grad_norm": 0.14888222515583038,
      "learning_rate": 2.665625e-05,
      "loss": 0.0019,
      "step": 112050
    },
    {
      "epoch": 3.735333333333333,
      "grad_norm": 0.11452766507863998,
      "learning_rate": 2.665416666666667e-05,
      "loss": 0.0015,
      "step": 112060
    },
    {
      "epoch": 3.735666666666667,
      "grad_norm": 0.11439035087823868,
      "learning_rate": 2.6652083333333332e-05,
      "loss": 0.0021,
      "step": 112070
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.22923092544078827,
      "learning_rate": 2.6650000000000004e-05,
      "loss": 0.0017,
      "step": 112080
    },
    {
      "epoch": 3.7363333333333335,
      "grad_norm": 0.11481747776269913,
      "learning_rate": 2.6647916666666666e-05,
      "loss": 0.0022,
      "step": 112090
    },
    {
      "epoch": 3.736666666666667,
      "grad_norm": 0.6388780474662781,
      "learning_rate": 2.6645833333333335e-05,
      "loss": 0.0024,
      "step": 112100
    },
    {
      "epoch": 3.737,
      "grad_norm": 0.08579655736684799,
      "learning_rate": 2.664375e-05,
      "loss": 0.0017,
      "step": 112110
    },
    {
      "epoch": 3.7373333333333334,
      "grad_norm": 0.049843836575746536,
      "learning_rate": 2.664166666666667e-05,
      "loss": 0.002,
      "step": 112120
    },
    {
      "epoch": 3.7376666666666667,
      "grad_norm": 0.1429988145828247,
      "learning_rate": 2.6639583333333335e-05,
      "loss": 0.0022,
      "step": 112130
    },
    {
      "epoch": 3.738,
      "grad_norm": 0.22858946025371552,
      "learning_rate": 2.6637499999999997e-05,
      "loss": 0.0019,
      "step": 112140
    },
    {
      "epoch": 3.7383333333333333,
      "grad_norm": 0.17882750928401947,
      "learning_rate": 2.663541666666667e-05,
      "loss": 0.0025,
      "step": 112150
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.08591720461845398,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.0023,
      "step": 112160
    },
    {
      "epoch": 3.739,
      "grad_norm": 0.19672463834285736,
      "learning_rate": 2.6631250000000004e-05,
      "loss": 0.0014,
      "step": 112170
    },
    {
      "epoch": 3.739333333333333,
      "grad_norm": 0.40109509229660034,
      "learning_rate": 2.6629166666666666e-05,
      "loss": 0.0025,
      "step": 112180
    },
    {
      "epoch": 3.7396666666666665,
      "grad_norm": 0.11502324044704437,
      "learning_rate": 2.6627083333333335e-05,
      "loss": 0.0013,
      "step": 112190
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.08679462969303131,
      "learning_rate": 2.6625e-05,
      "loss": 0.0023,
      "step": 112200
    },
    {
      "epoch": 3.740333333333333,
      "grad_norm": 0.08680222928524017,
      "learning_rate": 2.662291666666667e-05,
      "loss": 0.0019,
      "step": 112210
    },
    {
      "epoch": 3.740666666666667,
      "grad_norm": 0.08672405034303665,
      "learning_rate": 2.6620833333333334e-05,
      "loss": 0.0017,
      "step": 112220
    },
    {
      "epoch": 3.741,
      "grad_norm": 0.17148154973983765,
      "learning_rate": 2.6618750000000003e-05,
      "loss": 0.0018,
      "step": 112230
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.11449259519577026,
      "learning_rate": 2.661666666666667e-05,
      "loss": 0.0018,
      "step": 112240
    },
    {
      "epoch": 3.7416666666666667,
      "grad_norm": 0.005514020100235939,
      "learning_rate": 2.6614583333333338e-05,
      "loss": 0.0021,
      "step": 112250
    },
    {
      "epoch": 3.742,
      "grad_norm": 0.2778944969177246,
      "learning_rate": 2.66125e-05,
      "loss": 0.0023,
      "step": 112260
    },
    {
      "epoch": 3.7423333333333333,
      "grad_norm": 0.25731131434440613,
      "learning_rate": 2.6610416666666665e-05,
      "loss": 0.0017,
      "step": 112270
    },
    {
      "epoch": 3.7426666666666666,
      "grad_norm": 0.5047691464424133,
      "learning_rate": 2.6608333333333334e-05,
      "loss": 0.0022,
      "step": 112280
    },
    {
      "epoch": 3.743,
      "grad_norm": 0.058329805731773376,
      "learning_rate": 2.660625e-05,
      "loss": 0.0021,
      "step": 112290
    },
    {
      "epoch": 3.743333333333333,
      "grad_norm": 0.14318636059761047,
      "learning_rate": 2.660416666666667e-05,
      "loss": 0.002,
      "step": 112300
    },
    {
      "epoch": 3.743666666666667,
      "grad_norm": 0.43782156705856323,
      "learning_rate": 2.6602083333333334e-05,
      "loss": 0.0022,
      "step": 112310
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.24338173866271973,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0022,
      "step": 112320
    },
    {
      "epoch": 3.7443333333333335,
      "grad_norm": 0.004481033887714148,
      "learning_rate": 2.659791666666667e-05,
      "loss": 0.0017,
      "step": 112330
    },
    {
      "epoch": 3.744666666666667,
      "grad_norm": 0.0575968399643898,
      "learning_rate": 2.6595833333333337e-05,
      "loss": 0.0021,
      "step": 112340
    },
    {
      "epoch": 3.745,
      "grad_norm": 0.42901626229286194,
      "learning_rate": 2.659375e-05,
      "loss": 0.0014,
      "step": 112350
    },
    {
      "epoch": 3.7453333333333334,
      "grad_norm": 0.028759054839611053,
      "learning_rate": 2.659166666666667e-05,
      "loss": 0.0022,
      "step": 112360
    },
    {
      "epoch": 3.7456666666666667,
      "grad_norm": 0.3430396318435669,
      "learning_rate": 2.6589583333333334e-05,
      "loss": 0.002,
      "step": 112370
    },
    {
      "epoch": 3.746,
      "grad_norm": 0.37160632014274597,
      "learning_rate": 2.65875e-05,
      "loss": 0.0027,
      "step": 112380
    },
    {
      "epoch": 3.7463333333333333,
      "grad_norm": 0.17186777293682098,
      "learning_rate": 2.6585416666666668e-05,
      "loss": 0.0029,
      "step": 112390
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.029529280960559845,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.0018,
      "step": 112400
    },
    {
      "epoch": 3.747,
      "grad_norm": 0.2862870395183563,
      "learning_rate": 2.6581250000000002e-05,
      "loss": 0.0017,
      "step": 112410
    },
    {
      "epoch": 3.747333333333333,
      "grad_norm": 0.05744606629014015,
      "learning_rate": 2.6579166666666664e-05,
      "loss": 0.0028,
      "step": 112420
    },
    {
      "epoch": 3.7476666666666665,
      "grad_norm": 0.19136032462120056,
      "learning_rate": 2.6577083333333337e-05,
      "loss": 0.0023,
      "step": 112430
    },
    {
      "epoch": 3.748,
      "grad_norm": 0.11447408050298691,
      "learning_rate": 2.6575e-05,
      "loss": 0.0024,
      "step": 112440
    },
    {
      "epoch": 3.748333333333333,
      "grad_norm": 0.1717781275510788,
      "learning_rate": 2.657291666666667e-05,
      "loss": 0.0024,
      "step": 112450
    },
    {
      "epoch": 3.748666666666667,
      "grad_norm": 0.14026035368442535,
      "learning_rate": 2.6570833333333333e-05,
      "loss": 0.0025,
      "step": 112460
    },
    {
      "epoch": 3.749,
      "grad_norm": 0.28586992621421814,
      "learning_rate": 2.6568750000000002e-05,
      "loss": 0.0024,
      "step": 112470
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.05720686912536621,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0033,
      "step": 112480
    },
    {
      "epoch": 3.7496666666666667,
      "grad_norm": 0.5147066116333008,
      "learning_rate": 2.6564583333333336e-05,
      "loss": 0.0024,
      "step": 112490
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.2001088559627533,
      "learning_rate": 2.6562500000000002e-05,
      "loss": 0.0024,
      "step": 112500
    },
    {
      "epoch": 3.7503333333333333,
      "grad_norm": 0.2576187551021576,
      "learning_rate": 2.6560416666666664e-05,
      "loss": 0.0024,
      "step": 112510
    },
    {
      "epoch": 3.7506666666666666,
      "grad_norm": 0.08662064373493195,
      "learning_rate": 2.6558333333333336e-05,
      "loss": 0.0018,
      "step": 112520
    },
    {
      "epoch": 3.751,
      "grad_norm": 0.14295770227909088,
      "learning_rate": 2.655625e-05,
      "loss": 0.0027,
      "step": 112530
    },
    {
      "epoch": 3.751333333333333,
      "grad_norm": 0.17163927853107452,
      "learning_rate": 2.6554166666666667e-05,
      "loss": 0.0019,
      "step": 112540
    },
    {
      "epoch": 3.751666666666667,
      "grad_norm": 0.39997637271881104,
      "learning_rate": 2.6552083333333333e-05,
      "loss": 0.0027,
      "step": 112550
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.02998242899775505,
      "learning_rate": 2.655e-05,
      "loss": 0.0022,
      "step": 112560
    },
    {
      "epoch": 3.7523333333333335,
      "grad_norm": 0.14288672804832458,
      "learning_rate": 2.6547916666666667e-05,
      "loss": 0.0019,
      "step": 112570
    },
    {
      "epoch": 3.752666666666667,
      "grad_norm": 0.14327000081539154,
      "learning_rate": 2.6545833333333336e-05,
      "loss": 0.0024,
      "step": 112580
    },
    {
      "epoch": 3.753,
      "grad_norm": 0.058632317930459976,
      "learning_rate": 2.654375e-05,
      "loss": 0.0033,
      "step": 112590
    },
    {
      "epoch": 3.7533333333333334,
      "grad_norm": 0.17215679585933685,
      "learning_rate": 2.654166666666667e-05,
      "loss": 0.0018,
      "step": 112600
    },
    {
      "epoch": 3.7536666666666667,
      "grad_norm": 0.14428290724754333,
      "learning_rate": 2.6539583333333336e-05,
      "loss": 0.002,
      "step": 112610
    },
    {
      "epoch": 3.754,
      "grad_norm": 0.1713610589504242,
      "learning_rate": 2.6537500000000005e-05,
      "loss": 0.0027,
      "step": 112620
    },
    {
      "epoch": 3.7543333333333333,
      "grad_norm": 0.004692309070378542,
      "learning_rate": 2.6535416666666667e-05,
      "loss": 0.0016,
      "step": 112630
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.15091271698474884,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0025,
      "step": 112640
    },
    {
      "epoch": 3.755,
      "grad_norm": 0.20030662417411804,
      "learning_rate": 2.653125e-05,
      "loss": 0.0023,
      "step": 112650
    },
    {
      "epoch": 3.755333333333333,
      "grad_norm": 0.45760512351989746,
      "learning_rate": 2.6529166666666667e-05,
      "loss": 0.0027,
      "step": 112660
    },
    {
      "epoch": 3.7556666666666665,
      "grad_norm": 0.0061242468655109406,
      "learning_rate": 2.6527083333333336e-05,
      "loss": 0.0022,
      "step": 112670
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 0.05785253643989563,
      "learning_rate": 2.6525e-05,
      "loss": 0.0017,
      "step": 112680
    },
    {
      "epoch": 3.756333333333333,
      "grad_norm": 0.20060943067073822,
      "learning_rate": 2.652291666666667e-05,
      "loss": 0.0023,
      "step": 112690
    },
    {
      "epoch": 3.756666666666667,
      "grad_norm": 0.37158945202827454,
      "learning_rate": 2.6520833333333332e-05,
      "loss": 0.0018,
      "step": 112700
    },
    {
      "epoch": 3.757,
      "grad_norm": 0.041559848934412,
      "learning_rate": 2.6518750000000004e-05,
      "loss": 0.0017,
      "step": 112710
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.11419959366321564,
      "learning_rate": 2.6516666666666666e-05,
      "loss": 0.0019,
      "step": 112720
    },
    {
      "epoch": 3.7576666666666667,
      "grad_norm": 0.11437872797250748,
      "learning_rate": 2.651458333333334e-05,
      "loss": 0.0014,
      "step": 112730
    },
    {
      "epoch": 3.758,
      "grad_norm": 0.02891599014401436,
      "learning_rate": 2.65125e-05,
      "loss": 0.0035,
      "step": 112740
    },
    {
      "epoch": 3.7583333333333333,
      "grad_norm": 0.3015040457248688,
      "learning_rate": 2.6510416666666666e-05,
      "loss": 0.0032,
      "step": 112750
    },
    {
      "epoch": 3.7586666666666666,
      "grad_norm": 0.25731441378593445,
      "learning_rate": 2.6508333333333335e-05,
      "loss": 0.002,
      "step": 112760
    },
    {
      "epoch": 3.759,
      "grad_norm": 0.40025708079338074,
      "learning_rate": 2.650625e-05,
      "loss": 0.0022,
      "step": 112770
    },
    {
      "epoch": 3.759333333333333,
      "grad_norm": 0.2290402352809906,
      "learning_rate": 2.650416666666667e-05,
      "loss": 0.0029,
      "step": 112780
    },
    {
      "epoch": 3.759666666666667,
      "grad_norm": 0.20003172755241394,
      "learning_rate": 2.650208333333333e-05,
      "loss": 0.0023,
      "step": 112790
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.09513597190380096,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0016,
      "step": 112800
    },
    {
      "epoch": 3.7603333333333335,
      "grad_norm": 0.03518480062484741,
      "learning_rate": 2.6497916666666666e-05,
      "loss": 0.0041,
      "step": 112810
    },
    {
      "epoch": 3.760666666666667,
      "grad_norm": 0.2856796681880951,
      "learning_rate": 2.6495833333333335e-05,
      "loss": 0.0017,
      "step": 112820
    },
    {
      "epoch": 3.761,
      "grad_norm": 0.25713247060775757,
      "learning_rate": 2.649375e-05,
      "loss": 0.0025,
      "step": 112830
    },
    {
      "epoch": 3.7613333333333334,
      "grad_norm": 0.285999059677124,
      "learning_rate": 2.649166666666667e-05,
      "loss": 0.0015,
      "step": 112840
    },
    {
      "epoch": 3.7616666666666667,
      "grad_norm": 0.2004663646221161,
      "learning_rate": 2.6489583333333335e-05,
      "loss": 0.0023,
      "step": 112850
    },
    {
      "epoch": 3.762,
      "grad_norm": 0.057906072586774826,
      "learning_rate": 2.6487500000000003e-05,
      "loss": 0.0021,
      "step": 112860
    },
    {
      "epoch": 3.7623333333333333,
      "grad_norm": 0.31480491161346436,
      "learning_rate": 2.648541666666667e-05,
      "loss": 0.0025,
      "step": 112870
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.0694483295083046,
      "learning_rate": 2.648333333333333e-05,
      "loss": 0.0024,
      "step": 112880
    },
    {
      "epoch": 3.763,
      "grad_norm": 0.2372332364320755,
      "learning_rate": 2.6481250000000003e-05,
      "loss": 0.0015,
      "step": 112890
    },
    {
      "epoch": 3.763333333333333,
      "grad_norm": 0.08610530197620392,
      "learning_rate": 2.6479166666666665e-05,
      "loss": 0.0031,
      "step": 112900
    },
    {
      "epoch": 3.7636666666666665,
      "grad_norm": 0.40284720063209534,
      "learning_rate": 2.6477083333333334e-05,
      "loss": 0.0013,
      "step": 112910
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 0.057694293558597565,
      "learning_rate": 2.6475e-05,
      "loss": 0.0019,
      "step": 112920
    },
    {
      "epoch": 3.764333333333333,
      "grad_norm": 0.11473888158798218,
      "learning_rate": 2.647291666666667e-05,
      "loss": 0.0016,
      "step": 112930
    },
    {
      "epoch": 3.764666666666667,
      "grad_norm": 0.6287254691123962,
      "learning_rate": 2.6470833333333334e-05,
      "loss": 0.0015,
      "step": 112940
    },
    {
      "epoch": 3.765,
      "grad_norm": 0.25744712352752686,
      "learning_rate": 2.6468750000000003e-05,
      "loss": 0.0017,
      "step": 112950
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.029029402881860733,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0021,
      "step": 112960
    },
    {
      "epoch": 3.7656666666666667,
      "grad_norm": 0.057544220238924026,
      "learning_rate": 2.6464583333333337e-05,
      "loss": 0.0021,
      "step": 112970
    },
    {
      "epoch": 3.766,
      "grad_norm": 0.02918369136750698,
      "learning_rate": 2.6462500000000003e-05,
      "loss": 0.0023,
      "step": 112980
    },
    {
      "epoch": 3.7663333333333333,
      "grad_norm": 0.14310358464717865,
      "learning_rate": 2.6460416666666665e-05,
      "loss": 0.0019,
      "step": 112990
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.11456680297851562,
      "learning_rate": 2.6458333333333334e-05,
      "loss": 0.0018,
      "step": 113000
    },
    {
      "epoch": 3.767,
      "grad_norm": 0.11447253823280334,
      "learning_rate": 2.645625e-05,
      "loss": 0.0025,
      "step": 113010
    },
    {
      "epoch": 3.767333333333333,
      "grad_norm": 0.28648361563682556,
      "learning_rate": 2.6454166666666668e-05,
      "loss": 0.0018,
      "step": 113020
    },
    {
      "epoch": 3.767666666666667,
      "grad_norm": 0.0862053707242012,
      "learning_rate": 2.6452083333333334e-05,
      "loss": 0.0023,
      "step": 113030
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.004611658863723278,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 0.0018,
      "step": 113040
    },
    {
      "epoch": 3.7683333333333335,
      "grad_norm": 0.08587537705898285,
      "learning_rate": 2.6447916666666668e-05,
      "loss": 0.0021,
      "step": 113050
    },
    {
      "epoch": 3.768666666666667,
      "grad_norm": 0.1431836187839508,
      "learning_rate": 2.6445833333333337e-05,
      "loss": 0.0012,
      "step": 113060
    },
    {
      "epoch": 3.769,
      "grad_norm": 0.11478625982999802,
      "learning_rate": 2.644375e-05,
      "loss": 0.0022,
      "step": 113070
    },
    {
      "epoch": 3.7693333333333334,
      "grad_norm": 0.31523197889328003,
      "learning_rate": 2.644166666666667e-05,
      "loss": 0.0014,
      "step": 113080
    },
    {
      "epoch": 3.7696666666666667,
      "grad_norm": 0.08854622393846512,
      "learning_rate": 2.6439583333333333e-05,
      "loss": 0.0023,
      "step": 113090
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.030345967039465904,
      "learning_rate": 2.6437500000000002e-05,
      "loss": 0.0025,
      "step": 113100
    },
    {
      "epoch": 3.7703333333333333,
      "grad_norm": 0.14320063591003418,
      "learning_rate": 2.6435416666666668e-05,
      "loss": 0.0019,
      "step": 113110
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.19986629486083984,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.002,
      "step": 113120
    },
    {
      "epoch": 3.771,
      "grad_norm": 0.02870941534638405,
      "learning_rate": 2.6431250000000002e-05,
      "loss": 0.0024,
      "step": 113130
    },
    {
      "epoch": 3.771333333333333,
      "grad_norm": 0.029080526903271675,
      "learning_rate": 2.6429166666666668e-05,
      "loss": 0.0013,
      "step": 113140
    },
    {
      "epoch": 3.7716666666666665,
      "grad_norm": 0.029573820531368256,
      "learning_rate": 2.6427083333333336e-05,
      "loss": 0.0017,
      "step": 113150
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 0.08624694496393204,
      "learning_rate": 2.6425e-05,
      "loss": 0.0014,
      "step": 113160
    },
    {
      "epoch": 3.772333333333333,
      "grad_norm": 0.005889204330742359,
      "learning_rate": 2.642291666666667e-05,
      "loss": 0.003,
      "step": 113170
    },
    {
      "epoch": 3.772666666666667,
      "grad_norm": 0.22859464585781097,
      "learning_rate": 2.6420833333333333e-05,
      "loss": 0.0017,
      "step": 113180
    },
    {
      "epoch": 3.773,
      "grad_norm": 0.14353302121162415,
      "learning_rate": 2.6418750000000002e-05,
      "loss": 0.0029,
      "step": 113190
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.11549724638462067,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.002,
      "step": 113200
    },
    {
      "epoch": 3.7736666666666667,
      "grad_norm": 0.2287297248840332,
      "learning_rate": 2.6414583333333336e-05,
      "loss": 0.002,
      "step": 113210
    },
    {
      "epoch": 3.774,
      "grad_norm": 0.08604031801223755,
      "learning_rate": 2.64125e-05,
      "loss": 0.0014,
      "step": 113220
    },
    {
      "epoch": 3.7743333333333333,
      "grad_norm": 0.37157848477363586,
      "learning_rate": 2.6410416666666664e-05,
      "loss": 0.0017,
      "step": 113230
    },
    {
      "epoch": 3.7746666666666666,
      "grad_norm": 0.45736944675445557,
      "learning_rate": 2.6408333333333336e-05,
      "loss": 0.0016,
      "step": 113240
    },
    {
      "epoch": 3.775,
      "grad_norm": 0.1431986540555954,
      "learning_rate": 2.6406249999999998e-05,
      "loss": 0.0018,
      "step": 113250
    },
    {
      "epoch": 3.775333333333333,
      "grad_norm": 0.17182326316833496,
      "learning_rate": 2.640416666666667e-05,
      "loss": 0.0022,
      "step": 113260
    },
    {
      "epoch": 3.7756666666666665,
      "grad_norm": 0.1713820993900299,
      "learning_rate": 2.6402083333333332e-05,
      "loss": 0.0015,
      "step": 113270
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.3453111946582794,
      "learning_rate": 2.64e-05,
      "loss": 0.0013,
      "step": 113280
    },
    {
      "epoch": 3.7763333333333335,
      "grad_norm": 0.4858187139034271,
      "learning_rate": 2.6397916666666667e-05,
      "loss": 0.0014,
      "step": 113290
    },
    {
      "epoch": 3.7766666666666664,
      "grad_norm": 0.02921854704618454,
      "learning_rate": 2.6395833333333336e-05,
      "loss": 0.0019,
      "step": 113300
    },
    {
      "epoch": 3.777,
      "grad_norm": 0.25728216767311096,
      "learning_rate": 2.639375e-05,
      "loss": 0.0022,
      "step": 113310
    },
    {
      "epoch": 3.7773333333333334,
      "grad_norm": 0.37145596742630005,
      "learning_rate": 2.639166666666667e-05,
      "loss": 0.0013,
      "step": 113320
    },
    {
      "epoch": 3.7776666666666667,
      "grad_norm": 0.086037278175354,
      "learning_rate": 2.6389583333333336e-05,
      "loss": 0.0026,
      "step": 113330
    },
    {
      "epoch": 3.778,
      "grad_norm": 0.17165429890155792,
      "learning_rate": 2.6387500000000004e-05,
      "loss": 0.0029,
      "step": 113340
    },
    {
      "epoch": 3.7783333333333333,
      "grad_norm": 2.5904018878936768,
      "learning_rate": 2.6385416666666667e-05,
      "loss": 0.0028,
      "step": 113350
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 0.17144520580768585,
      "learning_rate": 2.6383333333333332e-05,
      "loss": 0.0021,
      "step": 113360
    },
    {
      "epoch": 3.779,
      "grad_norm": 0.11464249342679977,
      "learning_rate": 2.638125e-05,
      "loss": 0.0022,
      "step": 113370
    },
    {
      "epoch": 3.779333333333333,
      "grad_norm": 0.1445302963256836,
      "learning_rate": 2.6379166666666666e-05,
      "loss": 0.0019,
      "step": 113380
    },
    {
      "epoch": 3.7796666666666665,
      "grad_norm": 0.0580161027610302,
      "learning_rate": 2.6377083333333335e-05,
      "loss": 0.0016,
      "step": 113390
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.1433744728565216,
      "learning_rate": 2.6375e-05,
      "loss": 0.0027,
      "step": 113400
    },
    {
      "epoch": 3.780333333333333,
      "grad_norm": 0.031218208372592926,
      "learning_rate": 2.637291666666667e-05,
      "loss": 0.0022,
      "step": 113410
    },
    {
      "epoch": 3.780666666666667,
      "grad_norm": 0.1714795082807541,
      "learning_rate": 2.6370833333333335e-05,
      "loss": 0.0023,
      "step": 113420
    },
    {
      "epoch": 3.781,
      "grad_norm": 0.20001617074012756,
      "learning_rate": 2.6368750000000004e-05,
      "loss": 0.0024,
      "step": 113430
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.1715438812971115,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.0021,
      "step": 113440
    },
    {
      "epoch": 3.7816666666666667,
      "grad_norm": 0.17154626548290253,
      "learning_rate": 2.636458333333334e-05,
      "loss": 0.0031,
      "step": 113450
    },
    {
      "epoch": 3.782,
      "grad_norm": 0.11986686289310455,
      "learning_rate": 2.63625e-05,
      "loss": 0.0025,
      "step": 113460
    },
    {
      "epoch": 3.7823333333333333,
      "grad_norm": 0.029009805992245674,
      "learning_rate": 2.6360416666666666e-05,
      "loss": 0.0029,
      "step": 113470
    },
    {
      "epoch": 3.7826666666666666,
      "grad_norm": 0.30426862835884094,
      "learning_rate": 2.6358333333333335e-05,
      "loss": 0.0026,
      "step": 113480
    },
    {
      "epoch": 3.783,
      "grad_norm": 0.45744815468788147,
      "learning_rate": 2.635625e-05,
      "loss": 0.0016,
      "step": 113490
    },
    {
      "epoch": 3.783333333333333,
      "grad_norm": 0.08586376160383224,
      "learning_rate": 2.635416666666667e-05,
      "loss": 0.0023,
      "step": 113500
    },
    {
      "epoch": 3.7836666666666665,
      "grad_norm": 0.14290963113307953,
      "learning_rate": 2.635208333333333e-05,
      "loss": 0.0018,
      "step": 113510
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.20018167793750763,
      "learning_rate": 2.6350000000000004e-05,
      "loss": 0.0025,
      "step": 113520
    },
    {
      "epoch": 3.7843333333333335,
      "grad_norm": 0.08603750914335251,
      "learning_rate": 2.6347916666666666e-05,
      "loss": 0.0032,
      "step": 113530
    },
    {
      "epoch": 3.7846666666666664,
      "grad_norm": 0.2857728898525238,
      "learning_rate": 2.6345833333333338e-05,
      "loss": 0.0018,
      "step": 113540
    },
    {
      "epoch": 3.785,
      "grad_norm": 0.05733661353588104,
      "learning_rate": 2.634375e-05,
      "loss": 0.0016,
      "step": 113550
    },
    {
      "epoch": 3.7853333333333334,
      "grad_norm": 0.05770076811313629,
      "learning_rate": 2.634166666666667e-05,
      "loss": 0.0018,
      "step": 113560
    },
    {
      "epoch": 3.7856666666666667,
      "grad_norm": 0.08607272058725357,
      "learning_rate": 2.6339583333333334e-05,
      "loss": 0.0029,
      "step": 113570
    },
    {
      "epoch": 3.786,
      "grad_norm": 0.11443612724542618,
      "learning_rate": 2.6337500000000003e-05,
      "loss": 0.002,
      "step": 113580
    },
    {
      "epoch": 3.7863333333333333,
      "grad_norm": 0.006846193224191666,
      "learning_rate": 2.633541666666667e-05,
      "loss": 0.0029,
      "step": 113590
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.009538844227790833,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0017,
      "step": 113600
    },
    {
      "epoch": 3.787,
      "grad_norm": 0.059648677706718445,
      "learning_rate": 2.6331250000000003e-05,
      "loss": 0.0026,
      "step": 113610
    },
    {
      "epoch": 3.787333333333333,
      "grad_norm": 0.057407625019550323,
      "learning_rate": 2.6329166666666665e-05,
      "loss": 0.002,
      "step": 113620
    },
    {
      "epoch": 3.7876666666666665,
      "grad_norm": 0.1716793030500412,
      "learning_rate": 2.6327083333333334e-05,
      "loss": 0.0018,
      "step": 113630
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 0.14330622553825378,
      "learning_rate": 2.6325e-05,
      "loss": 0.0026,
      "step": 113640
    },
    {
      "epoch": 3.788333333333333,
      "grad_norm": 0.2460700124502182,
      "learning_rate": 2.632291666666667e-05,
      "loss": 0.0025,
      "step": 113650
    },
    {
      "epoch": 3.788666666666667,
      "grad_norm": 0.02958674170076847,
      "learning_rate": 2.6320833333333334e-05,
      "loss": 0.0022,
      "step": 113660
    },
    {
      "epoch": 3.789,
      "grad_norm": 0.11456113308668137,
      "learning_rate": 2.6318750000000003e-05,
      "loss": 0.0014,
      "step": 113670
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.01395111158490181,
      "learning_rate": 2.6316666666666668e-05,
      "loss": 0.0018,
      "step": 113680
    },
    {
      "epoch": 3.7896666666666667,
      "grad_norm": 0.029434554278850555,
      "learning_rate": 2.6314583333333337e-05,
      "loss": 0.0024,
      "step": 113690
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.14574375748634338,
      "learning_rate": 2.6312500000000003e-05,
      "loss": 0.0017,
      "step": 113700
    },
    {
      "epoch": 3.7903333333333333,
      "grad_norm": 0.057420551776885986,
      "learning_rate": 2.6310416666666665e-05,
      "loss": 0.0028,
      "step": 113710
    },
    {
      "epoch": 3.7906666666666666,
      "grad_norm": 0.1430969089269638,
      "learning_rate": 2.6308333333333334e-05,
      "loss": 0.0018,
      "step": 113720
    },
    {
      "epoch": 3.791,
      "grad_norm": 0.6396089792251587,
      "learning_rate": 2.630625e-05,
      "loss": 0.0022,
      "step": 113730
    },
    {
      "epoch": 3.791333333333333,
      "grad_norm": 0.17140233516693115,
      "learning_rate": 2.6304166666666668e-05,
      "loss": 0.0017,
      "step": 113740
    },
    {
      "epoch": 3.7916666666666665,
      "grad_norm": 0.02866436168551445,
      "learning_rate": 2.6302083333333333e-05,
      "loss": 0.0021,
      "step": 113750
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.11458969861268997,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0019,
      "step": 113760
    },
    {
      "epoch": 3.7923333333333336,
      "grad_norm": 0.12004631012678146,
      "learning_rate": 2.6297916666666668e-05,
      "loss": 0.0013,
      "step": 113770
    },
    {
      "epoch": 3.7926666666666664,
      "grad_norm": 0.11692094057798386,
      "learning_rate": 2.6295833333333337e-05,
      "loss": 0.0029,
      "step": 113780
    },
    {
      "epoch": 3.793,
      "grad_norm": 0.5472644567489624,
      "learning_rate": 2.629375e-05,
      "loss": 0.0031,
      "step": 113790
    },
    {
      "epoch": 3.7933333333333334,
      "grad_norm": 0.08639244735240936,
      "learning_rate": 2.629166666666667e-05,
      "loss": 0.0012,
      "step": 113800
    },
    {
      "epoch": 3.7936666666666667,
      "grad_norm": 0.05836087092757225,
      "learning_rate": 2.6289583333333333e-05,
      "loss": 0.0024,
      "step": 113810
    },
    {
      "epoch": 3.794,
      "grad_norm": 0.2287227362394333,
      "learning_rate": 2.6287500000000005e-05,
      "loss": 0.0027,
      "step": 113820
    },
    {
      "epoch": 3.7943333333333333,
      "grad_norm": 0.17150267958641052,
      "learning_rate": 2.6285416666666667e-05,
      "loss": 0.0018,
      "step": 113830
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.0859009325504303,
      "learning_rate": 2.6283333333333333e-05,
      "loss": 0.0015,
      "step": 113840
    },
    {
      "epoch": 3.795,
      "grad_norm": 0.05823686346411705,
      "learning_rate": 2.6281250000000002e-05,
      "loss": 0.0019,
      "step": 113850
    },
    {
      "epoch": 3.7953333333333332,
      "grad_norm": 0.14378716051578522,
      "learning_rate": 2.6279166666666667e-05,
      "loss": 0.0031,
      "step": 113860
    },
    {
      "epoch": 3.7956666666666665,
      "grad_norm": 0.029204007238149643,
      "learning_rate": 2.6277083333333336e-05,
      "loss": 0.0027,
      "step": 113870
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.05744786933064461,
      "learning_rate": 2.6275e-05,
      "loss": 0.0025,
      "step": 113880
    },
    {
      "epoch": 3.796333333333333,
      "grad_norm": 0.08588971197605133,
      "learning_rate": 2.627291666666667e-05,
      "loss": 0.0022,
      "step": 113890
    },
    {
      "epoch": 3.796666666666667,
      "grad_norm": 0.20028264820575714,
      "learning_rate": 2.6270833333333333e-05,
      "loss": 0.0035,
      "step": 113900
    },
    {
      "epoch": 3.797,
      "grad_norm": 0.22876271605491638,
      "learning_rate": 2.626875e-05,
      "loss": 0.003,
      "step": 113910
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.20001937448978424,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0031,
      "step": 113920
    },
    {
      "epoch": 3.7976666666666667,
      "grad_norm": 0.057596638798713684,
      "learning_rate": 2.6264583333333336e-05,
      "loss": 0.0027,
      "step": 113930
    },
    {
      "epoch": 3.798,
      "grad_norm": 0.2856440544128418,
      "learning_rate": 2.62625e-05,
      "loss": 0.0036,
      "step": 113940
    },
    {
      "epoch": 3.7983333333333333,
      "grad_norm": 0.6842835545539856,
      "learning_rate": 2.626041666666667e-05,
      "loss": 0.0021,
      "step": 113950
    },
    {
      "epoch": 3.7986666666666666,
      "grad_norm": 0.1436382234096527,
      "learning_rate": 2.6258333333333336e-05,
      "loss": 0.0018,
      "step": 113960
    },
    {
      "epoch": 3.799,
      "grad_norm": 0.11471648514270782,
      "learning_rate": 2.6256249999999998e-05,
      "loss": 0.0016,
      "step": 113970
    },
    {
      "epoch": 3.7993333333333332,
      "grad_norm": 0.1719619482755661,
      "learning_rate": 2.625416666666667e-05,
      "loss": 0.0015,
      "step": 113980
    },
    {
      "epoch": 3.7996666666666665,
      "grad_norm": 0.3146120011806488,
      "learning_rate": 2.6252083333333332e-05,
      "loss": 0.002,
      "step": 113990
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.11434169113636017,
      "learning_rate": 2.625e-05,
      "loss": 0.0023,
      "step": 114000
    },
    {
      "epoch": 3.8003333333333336,
      "grad_norm": 0.1712210476398468,
      "learning_rate": 2.6247916666666667e-05,
      "loss": 0.0014,
      "step": 114010
    },
    {
      "epoch": 3.8006666666666664,
      "grad_norm": 0.08587098866701126,
      "learning_rate": 2.6245833333333335e-05,
      "loss": 0.002,
      "step": 114020
    },
    {
      "epoch": 3.801,
      "grad_norm": 0.11428073793649673,
      "learning_rate": 2.624375e-05,
      "loss": 0.0037,
      "step": 114030
    },
    {
      "epoch": 3.8013333333333335,
      "grad_norm": 0.20007063448429108,
      "learning_rate": 2.624166666666667e-05,
      "loss": 0.0026,
      "step": 114040
    },
    {
      "epoch": 3.8016666666666667,
      "grad_norm": 0.5448744893074036,
      "learning_rate": 2.6239583333333335e-05,
      "loss": 0.0021,
      "step": 114050
    },
    {
      "epoch": 3.802,
      "grad_norm": 0.2571043372154236,
      "learning_rate": 2.6237500000000004e-05,
      "loss": 0.0018,
      "step": 114060
    },
    {
      "epoch": 3.8023333333333333,
      "grad_norm": 0.34292057156562805,
      "learning_rate": 2.6235416666666666e-05,
      "loss": 0.0021,
      "step": 114070
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.029597485437989235,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.0015,
      "step": 114080
    },
    {
      "epoch": 3.803,
      "grad_norm": 0.19997896254062653,
      "learning_rate": 2.623125e-05,
      "loss": 0.0016,
      "step": 114090
    },
    {
      "epoch": 3.8033333333333332,
      "grad_norm": 0.08587818592786789,
      "learning_rate": 2.6229166666666666e-05,
      "loss": 0.0018,
      "step": 114100
    },
    {
      "epoch": 3.8036666666666665,
      "grad_norm": 0.05757201090455055,
      "learning_rate": 2.6227083333333335e-05,
      "loss": 0.0022,
      "step": 114110
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 0.029144734144210815,
      "learning_rate": 2.6225e-05,
      "loss": 0.0023,
      "step": 114120
    },
    {
      "epoch": 3.804333333333333,
      "grad_norm": 0.08612043410539627,
      "learning_rate": 2.622291666666667e-05,
      "loss": 0.0023,
      "step": 114130
    },
    {
      "epoch": 3.804666666666667,
      "grad_norm": 0.005960342474281788,
      "learning_rate": 2.6220833333333335e-05,
      "loss": 0.0026,
      "step": 114140
    },
    {
      "epoch": 3.805,
      "grad_norm": 0.3714444041252136,
      "learning_rate": 2.6218750000000004e-05,
      "loss": 0.003,
      "step": 114150
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.14283421635627747,
      "learning_rate": 2.6216666666666666e-05,
      "loss": 0.002,
      "step": 114160
    },
    {
      "epoch": 3.8056666666666668,
      "grad_norm": 0.48561006784439087,
      "learning_rate": 2.6214583333333338e-05,
      "loss": 0.0015,
      "step": 114170
    },
    {
      "epoch": 3.806,
      "grad_norm": 0.22895517945289612,
      "learning_rate": 2.62125e-05,
      "loss": 0.0016,
      "step": 114180
    },
    {
      "epoch": 3.8063333333333333,
      "grad_norm": 0.057408418506383896,
      "learning_rate": 2.621041666666667e-05,
      "loss": 0.002,
      "step": 114190
    },
    {
      "epoch": 3.8066666666666666,
      "grad_norm": 0.31428083777427673,
      "learning_rate": 2.6208333333333335e-05,
      "loss": 0.0015,
      "step": 114200
    },
    {
      "epoch": 3.807,
      "grad_norm": 0.2573540508747101,
      "learning_rate": 2.620625e-05,
      "loss": 0.0032,
      "step": 114210
    },
    {
      "epoch": 3.8073333333333332,
      "grad_norm": 0.3145739734172821,
      "learning_rate": 2.620416666666667e-05,
      "loss": 0.0024,
      "step": 114220
    },
    {
      "epoch": 3.8076666666666665,
      "grad_norm": 0.22981195151805878,
      "learning_rate": 2.620208333333333e-05,
      "loss": 0.0019,
      "step": 114230
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.17177166044712067,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0023,
      "step": 114240
    },
    {
      "epoch": 3.8083333333333336,
      "grad_norm": 0.4597039818763733,
      "learning_rate": 2.6197916666666665e-05,
      "loss": 0.002,
      "step": 114250
    },
    {
      "epoch": 3.8086666666666664,
      "grad_norm": 0.11415482312440872,
      "learning_rate": 2.6195833333333338e-05,
      "loss": 0.0024,
      "step": 114260
    },
    {
      "epoch": 3.809,
      "grad_norm": 0.18261092901229858,
      "learning_rate": 2.619375e-05,
      "loss": 0.002,
      "step": 114270
    },
    {
      "epoch": 3.8093333333333335,
      "grad_norm": 0.14302892982959747,
      "learning_rate": 2.619166666666667e-05,
      "loss": 0.0031,
      "step": 114280
    },
    {
      "epoch": 3.8096666666666668,
      "grad_norm": 0.34288710355758667,
      "learning_rate": 2.6189583333333334e-05,
      "loss": 0.0017,
      "step": 114290
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.17141422629356384,
      "learning_rate": 2.6187500000000003e-05,
      "loss": 0.0016,
      "step": 114300
    },
    {
      "epoch": 3.8103333333333333,
      "grad_norm": 0.029310664162039757,
      "learning_rate": 2.618541666666667e-05,
      "loss": 0.0022,
      "step": 114310
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.2003028392791748,
      "learning_rate": 2.618333333333333e-05,
      "loss": 0.0024,
      "step": 114320
    },
    {
      "epoch": 3.811,
      "grad_norm": 0.2577105462551117,
      "learning_rate": 2.6181250000000003e-05,
      "loss": 0.0026,
      "step": 114330
    },
    {
      "epoch": 3.8113333333333332,
      "grad_norm": 0.17126594483852386,
      "learning_rate": 2.6179166666666665e-05,
      "loss": 0.0027,
      "step": 114340
    },
    {
      "epoch": 3.8116666666666665,
      "grad_norm": 0.3714599311351776,
      "learning_rate": 2.6177083333333334e-05,
      "loss": 0.0025,
      "step": 114350
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 1.3119468688964844,
      "learning_rate": 2.6175e-05,
      "loss": 0.0028,
      "step": 114360
    },
    {
      "epoch": 3.812333333333333,
      "grad_norm": 0.40015023946762085,
      "learning_rate": 2.6172916666666668e-05,
      "loss": 0.0031,
      "step": 114370
    },
    {
      "epoch": 3.812666666666667,
      "grad_norm": 0.3432087004184723,
      "learning_rate": 2.6170833333333334e-05,
      "loss": 0.0025,
      "step": 114380
    },
    {
      "epoch": 3.8129999999999997,
      "grad_norm": 0.08580189198255539,
      "learning_rate": 2.6168750000000003e-05,
      "loss": 0.002,
      "step": 114390
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.485952764749527,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0023,
      "step": 114400
    },
    {
      "epoch": 3.8136666666666668,
      "grad_norm": 0.0858149379491806,
      "learning_rate": 2.6164583333333337e-05,
      "loss": 0.0017,
      "step": 114410
    },
    {
      "epoch": 3.814,
      "grad_norm": 0.17219409346580505,
      "learning_rate": 2.6162500000000002e-05,
      "loss": 0.0029,
      "step": 114420
    },
    {
      "epoch": 3.8143333333333334,
      "grad_norm": 0.05811966955661774,
      "learning_rate": 2.616041666666667e-05,
      "loss": 0.002,
      "step": 114430
    },
    {
      "epoch": 3.8146666666666667,
      "grad_norm": 0.02947138249874115,
      "learning_rate": 2.6158333333333333e-05,
      "loss": 0.0018,
      "step": 114440
    },
    {
      "epoch": 3.815,
      "grad_norm": 0.06109379976987839,
      "learning_rate": 2.615625e-05,
      "loss": 0.0026,
      "step": 114450
    },
    {
      "epoch": 3.8153333333333332,
      "grad_norm": 0.23529528081417084,
      "learning_rate": 2.6154166666666668e-05,
      "loss": 0.0026,
      "step": 114460
    },
    {
      "epoch": 3.8156666666666665,
      "grad_norm": 0.17150728404521942,
      "learning_rate": 2.6152083333333333e-05,
      "loss": 0.0016,
      "step": 114470
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.14286920428276062,
      "learning_rate": 2.6150000000000002e-05,
      "loss": 0.0017,
      "step": 114480
    },
    {
      "epoch": 3.8163333333333336,
      "grad_norm": 0.057322777807712555,
      "learning_rate": 2.6147916666666668e-05,
      "loss": 0.002,
      "step": 114490
    },
    {
      "epoch": 3.8166666666666664,
      "grad_norm": 0.02925422601401806,
      "learning_rate": 2.6145833333333336e-05,
      "loss": 0.0015,
      "step": 114500
    },
    {
      "epoch": 3.817,
      "grad_norm": 0.1430252641439438,
      "learning_rate": 2.614375e-05,
      "loss": 0.0032,
      "step": 114510
    },
    {
      "epoch": 3.8173333333333335,
      "grad_norm": 0.11457400023937225,
      "learning_rate": 2.614166666666667e-05,
      "loss": 0.0025,
      "step": 114520
    },
    {
      "epoch": 3.8176666666666668,
      "grad_norm": 0.1428162157535553,
      "learning_rate": 2.6139583333333333e-05,
      "loss": 0.0017,
      "step": 114530
    },
    {
      "epoch": 3.818,
      "grad_norm": 0.02909841015934944,
      "learning_rate": 2.6137500000000005e-05,
      "loss": 0.0022,
      "step": 114540
    },
    {
      "epoch": 3.8183333333333334,
      "grad_norm": 0.200071781873703,
      "learning_rate": 2.6135416666666667e-05,
      "loss": 0.0019,
      "step": 114550
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.0858253538608551,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0023,
      "step": 114560
    },
    {
      "epoch": 3.819,
      "grad_norm": 0.028937628492712975,
      "learning_rate": 2.613125e-05,
      "loss": 0.0018,
      "step": 114570
    },
    {
      "epoch": 3.8193333333333332,
      "grad_norm": 0.14278677105903625,
      "learning_rate": 2.6129166666666667e-05,
      "loss": 0.0018,
      "step": 114580
    },
    {
      "epoch": 3.8196666666666665,
      "grad_norm": 0.381034255027771,
      "learning_rate": 2.6127083333333336e-05,
      "loss": 0.0015,
      "step": 114590
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.09186838567256927,
      "learning_rate": 2.6124999999999998e-05,
      "loss": 0.0029,
      "step": 114600
    },
    {
      "epoch": 3.820333333333333,
      "grad_norm": 0.393497109413147,
      "learning_rate": 2.612291666666667e-05,
      "loss": 0.0019,
      "step": 114610
    },
    {
      "epoch": 3.820666666666667,
      "grad_norm": 0.42864781618118286,
      "learning_rate": 2.6120833333333332e-05,
      "loss": 0.0019,
      "step": 114620
    },
    {
      "epoch": 3.8209999999999997,
      "grad_norm": 0.4853380620479584,
      "learning_rate": 2.611875e-05,
      "loss": 0.0028,
      "step": 114630
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.1999206244945526,
      "learning_rate": 2.6116666666666667e-05,
      "loss": 0.0028,
      "step": 114640
    },
    {
      "epoch": 3.8216666666666668,
      "grad_norm": 0.06753898411989212,
      "learning_rate": 2.6114583333333336e-05,
      "loss": 0.0027,
      "step": 114650
    },
    {
      "epoch": 3.822,
      "grad_norm": 0.06869860738515854,
      "learning_rate": 2.61125e-05,
      "loss": 0.0026,
      "step": 114660
    },
    {
      "epoch": 3.8223333333333334,
      "grad_norm": 0.14516030251979828,
      "learning_rate": 2.611041666666667e-05,
      "loss": 0.0013,
      "step": 114670
    },
    {
      "epoch": 3.8226666666666667,
      "grad_norm": 0.34286463260650635,
      "learning_rate": 2.6108333333333335e-05,
      "loss": 0.0023,
      "step": 114680
    },
    {
      "epoch": 3.823,
      "grad_norm": 0.14314484596252441,
      "learning_rate": 2.6106249999999998e-05,
      "loss": 0.0022,
      "step": 114690
    },
    {
      "epoch": 3.8233333333333333,
      "grad_norm": 0.28579241037368774,
      "learning_rate": 2.610416666666667e-05,
      "loss": 0.0019,
      "step": 114700
    },
    {
      "epoch": 3.8236666666666665,
      "grad_norm": 0.22894099354743958,
      "learning_rate": 2.6102083333333332e-05,
      "loss": 0.0022,
      "step": 114710
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.028974318876862526,
      "learning_rate": 2.61e-05,
      "loss": 0.0022,
      "step": 114720
    },
    {
      "epoch": 3.8243333333333336,
      "grad_norm": 0.02912355400621891,
      "learning_rate": 2.6097916666666666e-05,
      "loss": 0.0016,
      "step": 114730
    },
    {
      "epoch": 3.8246666666666664,
      "grad_norm": 0.030818931758403778,
      "learning_rate": 2.6095833333333335e-05,
      "loss": 0.0017,
      "step": 114740
    },
    {
      "epoch": 3.825,
      "grad_norm": 0.03103046491742134,
      "learning_rate": 2.609375e-05,
      "loss": 0.0019,
      "step": 114750
    },
    {
      "epoch": 3.8253333333333335,
      "grad_norm": 0.1143113449215889,
      "learning_rate": 2.609166666666667e-05,
      "loss": 0.0019,
      "step": 114760
    },
    {
      "epoch": 3.8256666666666668,
      "grad_norm": 0.28565290570259094,
      "learning_rate": 2.6089583333333335e-05,
      "loss": 0.0019,
      "step": 114770
    },
    {
      "epoch": 3.826,
      "grad_norm": 0.25704845786094666,
      "learning_rate": 2.6087500000000004e-05,
      "loss": 0.0014,
      "step": 114780
    },
    {
      "epoch": 3.8263333333333334,
      "grad_norm": 0.1474403440952301,
      "learning_rate": 2.608541666666667e-05,
      "loss": 0.0025,
      "step": 114790
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.31461992859840393,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.0017,
      "step": 114800
    },
    {
      "epoch": 3.827,
      "grad_norm": 0.3429555594921112,
      "learning_rate": 2.608125e-05,
      "loss": 0.002,
      "step": 114810
    },
    {
      "epoch": 3.8273333333333333,
      "grad_norm": 0.6568722128868103,
      "learning_rate": 2.6079166666666666e-05,
      "loss": 0.0023,
      "step": 114820
    },
    {
      "epoch": 3.8276666666666666,
      "grad_norm": 0.17140232026576996,
      "learning_rate": 2.6077083333333335e-05,
      "loss": 0.0027,
      "step": 114830
    },
    {
      "epoch": 3.828,
      "grad_norm": 0.05769357457756996,
      "learning_rate": 2.6075e-05,
      "loss": 0.0027,
      "step": 114840
    },
    {
      "epoch": 3.828333333333333,
      "grad_norm": 0.3672032058238983,
      "learning_rate": 2.607291666666667e-05,
      "loss": 0.0019,
      "step": 114850
    },
    {
      "epoch": 3.828666666666667,
      "grad_norm": 0.02918171137571335,
      "learning_rate": 2.6070833333333335e-05,
      "loss": 0.0016,
      "step": 114860
    },
    {
      "epoch": 3.8289999999999997,
      "grad_norm": 0.02882329374551773,
      "learning_rate": 2.6068750000000003e-05,
      "loss": 0.0022,
      "step": 114870
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.11412791907787323,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0022,
      "step": 114880
    },
    {
      "epoch": 3.8296666666666668,
      "grad_norm": 0.4859219193458557,
      "learning_rate": 2.6064583333333338e-05,
      "loss": 0.0019,
      "step": 114890
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.428628146648407,
      "learning_rate": 2.60625e-05,
      "loss": 0.0026,
      "step": 114900
    },
    {
      "epoch": 3.8303333333333334,
      "grad_norm": 0.08597585558891296,
      "learning_rate": 2.606041666666667e-05,
      "loss": 0.0027,
      "step": 114910
    },
    {
      "epoch": 3.8306666666666667,
      "grad_norm": 0.02966488152742386,
      "learning_rate": 2.6058333333333334e-05,
      "loss": 0.0028,
      "step": 114920
    },
    {
      "epoch": 3.831,
      "grad_norm": 0.2857871353626251,
      "learning_rate": 2.605625e-05,
      "loss": 0.0019,
      "step": 114930
    },
    {
      "epoch": 3.8313333333333333,
      "grad_norm": 0.005940305534750223,
      "learning_rate": 2.605416666666667e-05,
      "loss": 0.0018,
      "step": 114940
    },
    {
      "epoch": 3.8316666666666666,
      "grad_norm": 0.25786250829696655,
      "learning_rate": 2.6052083333333334e-05,
      "loss": 0.0024,
      "step": 114950
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.05777868255972862,
      "learning_rate": 2.6050000000000003e-05,
      "loss": 0.0021,
      "step": 114960
    },
    {
      "epoch": 3.8323333333333336,
      "grad_norm": 0.5999055504798889,
      "learning_rate": 2.6047916666666665e-05,
      "loss": 0.0024,
      "step": 114970
    },
    {
      "epoch": 3.8326666666666664,
      "grad_norm": 0.007107158657163382,
      "learning_rate": 2.6045833333333337e-05,
      "loss": 0.0021,
      "step": 114980
    },
    {
      "epoch": 3.833,
      "grad_norm": 0.25712502002716064,
      "learning_rate": 2.604375e-05,
      "loss": 0.0021,
      "step": 114990
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.14286252856254578,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.0024,
      "step": 115000
    },
    {
      "epoch": 3.833666666666667,
      "grad_norm": 0.1429632157087326,
      "learning_rate": 2.6039583333333334e-05,
      "loss": 0.002,
      "step": 115010
    },
    {
      "epoch": 3.834,
      "grad_norm": 0.11434736102819443,
      "learning_rate": 2.6037500000000003e-05,
      "loss": 0.0017,
      "step": 115020
    },
    {
      "epoch": 3.8343333333333334,
      "grad_norm": 0.3142342269420624,
      "learning_rate": 2.6035416666666668e-05,
      "loss": 0.0019,
      "step": 115030
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.3429247736930847,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.0015,
      "step": 115040
    },
    {
      "epoch": 3.835,
      "grad_norm": 0.012279183603823185,
      "learning_rate": 2.6031250000000003e-05,
      "loss": 0.0021,
      "step": 115050
    },
    {
      "epoch": 3.8353333333333333,
      "grad_norm": 0.08605770766735077,
      "learning_rate": 2.6029166666666665e-05,
      "loss": 0.0022,
      "step": 115060
    },
    {
      "epoch": 3.8356666666666666,
      "grad_norm": 0.20037230849266052,
      "learning_rate": 2.6027083333333337e-05,
      "loss": 0.0018,
      "step": 115070
    },
    {
      "epoch": 3.836,
      "grad_norm": 0.08615849167108536,
      "learning_rate": 2.6025e-05,
      "loss": 0.0018,
      "step": 115080
    },
    {
      "epoch": 3.836333333333333,
      "grad_norm": 0.3141912519931793,
      "learning_rate": 2.6022916666666668e-05,
      "loss": 0.0024,
      "step": 115090
    },
    {
      "epoch": 3.836666666666667,
      "grad_norm": 0.48563680052757263,
      "learning_rate": 2.6020833333333333e-05,
      "loss": 0.0016,
      "step": 115100
    },
    {
      "epoch": 3.8369999999999997,
      "grad_norm": 0.029248017817735672,
      "learning_rate": 2.6018750000000002e-05,
      "loss": 0.0023,
      "step": 115110
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.2288745790719986,
      "learning_rate": 2.6016666666666668e-05,
      "loss": 0.0023,
      "step": 115120
    },
    {
      "epoch": 3.837666666666667,
      "grad_norm": 0.17162449657917023,
      "learning_rate": 2.6014583333333337e-05,
      "loss": 0.0016,
      "step": 115130
    },
    {
      "epoch": 3.838,
      "grad_norm": 0.17192046344280243,
      "learning_rate": 2.6012500000000002e-05,
      "loss": 0.002,
      "step": 115140
    },
    {
      "epoch": 3.8383333333333334,
      "grad_norm": 0.03133698180317879,
      "learning_rate": 2.601041666666667e-05,
      "loss": 0.0014,
      "step": 115150
    },
    {
      "epoch": 3.8386666666666667,
      "grad_norm": 0.03224625810980797,
      "learning_rate": 2.6008333333333333e-05,
      "loss": 0.0021,
      "step": 115160
    },
    {
      "epoch": 3.839,
      "grad_norm": 0.01219636108726263,
      "learning_rate": 2.600625e-05,
      "loss": 0.0037,
      "step": 115170
    },
    {
      "epoch": 3.8393333333333333,
      "grad_norm": 0.09087371081113815,
      "learning_rate": 2.6004166666666667e-05,
      "loss": 0.0021,
      "step": 115180
    },
    {
      "epoch": 3.8396666666666666,
      "grad_norm": 0.03010847046971321,
      "learning_rate": 2.6002083333333333e-05,
      "loss": 0.0019,
      "step": 115190
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.2000254988670349,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0016,
      "step": 115200
    },
    {
      "epoch": 3.8403333333333336,
      "grad_norm": 0.20013704895973206,
      "learning_rate": 2.5997916666666667e-05,
      "loss": 0.0019,
      "step": 115210
    },
    {
      "epoch": 3.8406666666666665,
      "grad_norm": 0.25715211033821106,
      "learning_rate": 2.5995833333333336e-05,
      "loss": 0.0018,
      "step": 115220
    },
    {
      "epoch": 3.841,
      "grad_norm": 0.14287199079990387,
      "learning_rate": 2.599375e-05,
      "loss": 0.0024,
      "step": 115230
    },
    {
      "epoch": 3.8413333333333335,
      "grad_norm": 0.6503136157989502,
      "learning_rate": 2.599166666666667e-05,
      "loss": 0.0031,
      "step": 115240
    },
    {
      "epoch": 3.841666666666667,
      "grad_norm": 0.11427992582321167,
      "learning_rate": 2.5989583333333333e-05,
      "loss": 0.0027,
      "step": 115250
    },
    {
      "epoch": 3.842,
      "grad_norm": 0.11419258266687393,
      "learning_rate": 2.5987500000000005e-05,
      "loss": 0.003,
      "step": 115260
    },
    {
      "epoch": 3.8423333333333334,
      "grad_norm": 0.28571438789367676,
      "learning_rate": 2.5985416666666667e-05,
      "loss": 0.0033,
      "step": 115270
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.1434689611196518,
      "learning_rate": 2.5983333333333336e-05,
      "loss": 0.0014,
      "step": 115280
    },
    {
      "epoch": 3.843,
      "grad_norm": 0.4855213165283203,
      "learning_rate": 2.598125e-05,
      "loss": 0.0029,
      "step": 115290
    },
    {
      "epoch": 3.8433333333333333,
      "grad_norm": 0.23102128505706787,
      "learning_rate": 2.5979166666666667e-05,
      "loss": 0.0016,
      "step": 115300
    },
    {
      "epoch": 3.8436666666666666,
      "grad_norm": 0.22167660295963287,
      "learning_rate": 2.5977083333333336e-05,
      "loss": 0.0022,
      "step": 115310
    },
    {
      "epoch": 3.844,
      "grad_norm": 0.2842886447906494,
      "learning_rate": 2.5974999999999998e-05,
      "loss": 0.0014,
      "step": 115320
    },
    {
      "epoch": 3.844333333333333,
      "grad_norm": 0.02951185777783394,
      "learning_rate": 2.597291666666667e-05,
      "loss": 0.0015,
      "step": 115330
    },
    {
      "epoch": 3.844666666666667,
      "grad_norm": 0.029376549646258354,
      "learning_rate": 2.5970833333333332e-05,
      "loss": 0.0026,
      "step": 115340
    },
    {
      "epoch": 3.8449999999999998,
      "grad_norm": 0.029026629403233528,
      "learning_rate": 2.5968750000000004e-05,
      "loss": 0.0026,
      "step": 115350
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.3455764055252075,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.0021,
      "step": 115360
    },
    {
      "epoch": 3.845666666666667,
      "grad_norm": 0.005149433854967356,
      "learning_rate": 2.5964583333333335e-05,
      "loss": 0.0025,
      "step": 115370
    },
    {
      "epoch": 3.846,
      "grad_norm": 0.02917350083589554,
      "learning_rate": 2.59625e-05,
      "loss": 0.0022,
      "step": 115380
    },
    {
      "epoch": 3.8463333333333334,
      "grad_norm": 0.17156529426574707,
      "learning_rate": 2.596041666666667e-05,
      "loss": 0.0021,
      "step": 115390
    },
    {
      "epoch": 3.8466666666666667,
      "grad_norm": 0.714074969291687,
      "learning_rate": 2.5958333333333335e-05,
      "loss": 0.0029,
      "step": 115400
    },
    {
      "epoch": 3.847,
      "grad_norm": 0.48573607206344604,
      "learning_rate": 2.5956249999999997e-05,
      "loss": 0.0023,
      "step": 115410
    },
    {
      "epoch": 3.8473333333333333,
      "grad_norm": 0.25753453373908997,
      "learning_rate": 2.595416666666667e-05,
      "loss": 0.0033,
      "step": 115420
    },
    {
      "epoch": 3.8476666666666666,
      "grad_norm": 0.3713984787464142,
      "learning_rate": 2.5952083333333332e-05,
      "loss": 0.0018,
      "step": 115430
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.08635634183883667,
      "learning_rate": 2.595e-05,
      "loss": 0.0023,
      "step": 115440
    },
    {
      "epoch": 3.8483333333333336,
      "grad_norm": 0.14293955266475677,
      "learning_rate": 2.5947916666666666e-05,
      "loss": 0.0026,
      "step": 115450
    },
    {
      "epoch": 3.8486666666666665,
      "grad_norm": 0.37161415815353394,
      "learning_rate": 2.5945833333333335e-05,
      "loss": 0.0028,
      "step": 115460
    },
    {
      "epoch": 3.849,
      "grad_norm": 0.14284975826740265,
      "learning_rate": 2.594375e-05,
      "loss": 0.0022,
      "step": 115470
    },
    {
      "epoch": 3.8493333333333335,
      "grad_norm": 0.08645595610141754,
      "learning_rate": 2.594166666666667e-05,
      "loss": 0.0025,
      "step": 115480
    },
    {
      "epoch": 3.849666666666667,
      "grad_norm": 0.17213739454746246,
      "learning_rate": 2.5939583333333335e-05,
      "loss": 0.0019,
      "step": 115490
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.6002377271652222,
      "learning_rate": 2.5937500000000004e-05,
      "loss": 0.0021,
      "step": 115500
    },
    {
      "epoch": 3.8503333333333334,
      "grad_norm": 0.1999461054801941,
      "learning_rate": 2.593541666666667e-05,
      "loss": 0.0017,
      "step": 115510
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.40003228187561035,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.003,
      "step": 115520
    },
    {
      "epoch": 3.851,
      "grad_norm": 0.08613651990890503,
      "learning_rate": 2.593125e-05,
      "loss": 0.0029,
      "step": 115530
    },
    {
      "epoch": 3.8513333333333333,
      "grad_norm": 0.2006729543209076,
      "learning_rate": 2.5929166666666666e-05,
      "loss": 0.0014,
      "step": 115540
    },
    {
      "epoch": 3.8516666666666666,
      "grad_norm": 0.17147985100746155,
      "learning_rate": 2.5927083333333334e-05,
      "loss": 0.0012,
      "step": 115550
    },
    {
      "epoch": 3.852,
      "grad_norm": 0.42847731709480286,
      "learning_rate": 2.5925e-05,
      "loss": 0.0014,
      "step": 115560
    },
    {
      "epoch": 3.852333333333333,
      "grad_norm": 0.1716911941766739,
      "learning_rate": 2.592291666666667e-05,
      "loss": 0.0023,
      "step": 115570
    },
    {
      "epoch": 3.852666666666667,
      "grad_norm": 0.48558005690574646,
      "learning_rate": 2.5920833333333334e-05,
      "loss": 0.002,
      "step": 115580
    },
    {
      "epoch": 3.8529999999999998,
      "grad_norm": 0.08608807623386383,
      "learning_rate": 2.5918750000000003e-05,
      "loss": 0.0022,
      "step": 115590
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.25702565908432007,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.0019,
      "step": 115600
    },
    {
      "epoch": 3.853666666666667,
      "grad_norm": 0.20038853585720062,
      "learning_rate": 2.5914583333333338e-05,
      "loss": 0.0026,
      "step": 115610
    },
    {
      "epoch": 3.854,
      "grad_norm": 0.028898824006319046,
      "learning_rate": 2.59125e-05,
      "loss": 0.0026,
      "step": 115620
    },
    {
      "epoch": 3.8543333333333334,
      "grad_norm": 0.25750720500946045,
      "learning_rate": 2.5910416666666672e-05,
      "loss": 0.0021,
      "step": 115630
    },
    {
      "epoch": 3.8546666666666667,
      "grad_norm": 0.2004353106021881,
      "learning_rate": 2.5908333333333334e-05,
      "loss": 0.0015,
      "step": 115640
    },
    {
      "epoch": 3.855,
      "grad_norm": 0.34304043650627136,
      "learning_rate": 2.590625e-05,
      "loss": 0.003,
      "step": 115650
    },
    {
      "epoch": 3.8553333333333333,
      "grad_norm": 0.17156538367271423,
      "learning_rate": 2.590416666666667e-05,
      "loss": 0.0019,
      "step": 115660
    },
    {
      "epoch": 3.8556666666666666,
      "grad_norm": 0.03586818650364876,
      "learning_rate": 2.5902083333333334e-05,
      "loss": 0.0024,
      "step": 115670
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.08887416124343872,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0017,
      "step": 115680
    },
    {
      "epoch": 3.856333333333333,
      "grad_norm": 0.028884267434477806,
      "learning_rate": 2.5897916666666665e-05,
      "loss": 0.002,
      "step": 115690
    },
    {
      "epoch": 3.8566666666666665,
      "grad_norm": 0.4001263678073883,
      "learning_rate": 2.5895833333333337e-05,
      "loss": 0.0025,
      "step": 115700
    },
    {
      "epoch": 3.857,
      "grad_norm": 0.0322091244161129,
      "learning_rate": 2.589375e-05,
      "loss": 0.0026,
      "step": 115710
    },
    {
      "epoch": 3.857333333333333,
      "grad_norm": 0.2572607696056366,
      "learning_rate": 2.5891666666666668e-05,
      "loss": 0.0029,
      "step": 115720
    },
    {
      "epoch": 3.857666666666667,
      "grad_norm": 0.1430516391992569,
      "learning_rate": 2.5889583333333334e-05,
      "loss": 0.0034,
      "step": 115730
    },
    {
      "epoch": 3.858,
      "grad_norm": 0.08653230965137482,
      "learning_rate": 2.5887500000000002e-05,
      "loss": 0.0021,
      "step": 115740
    },
    {
      "epoch": 3.8583333333333334,
      "grad_norm": 0.22851945459842682,
      "learning_rate": 2.5885416666666668e-05,
      "loss": 0.002,
      "step": 115750
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.057715725153684616,
      "learning_rate": 2.5883333333333337e-05,
      "loss": 0.0024,
      "step": 115760
    },
    {
      "epoch": 3.859,
      "grad_norm": 0.03295088931918144,
      "learning_rate": 2.5881250000000002e-05,
      "loss": 0.002,
      "step": 115770
    },
    {
      "epoch": 3.8593333333333333,
      "grad_norm": 0.08648454397916794,
      "learning_rate": 2.5879166666666664e-05,
      "loss": 0.0014,
      "step": 115780
    },
    {
      "epoch": 3.8596666666666666,
      "grad_norm": 0.08607558161020279,
      "learning_rate": 2.5877083333333337e-05,
      "loss": 0.0024,
      "step": 115790
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.2764001190662384,
      "learning_rate": 2.5875e-05,
      "loss": 0.0022,
      "step": 115800
    },
    {
      "epoch": 3.860333333333333,
      "grad_norm": 0.08559081703424454,
      "learning_rate": 2.5872916666666668e-05,
      "loss": 0.0019,
      "step": 115810
    },
    {
      "epoch": 3.860666666666667,
      "grad_norm": 0.1148674413561821,
      "learning_rate": 2.5870833333333333e-05,
      "loss": 0.0021,
      "step": 115820
    },
    {
      "epoch": 3.8609999999999998,
      "grad_norm": 0.143257275223732,
      "learning_rate": 2.5868750000000002e-05,
      "loss": 0.0021,
      "step": 115830
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.6239986419677734,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0027,
      "step": 115840
    },
    {
      "epoch": 3.861666666666667,
      "grad_norm": 0.37137338519096375,
      "learning_rate": 2.5864583333333336e-05,
      "loss": 0.002,
      "step": 115850
    },
    {
      "epoch": 3.862,
      "grad_norm": 0.3713158965110779,
      "learning_rate": 2.5862500000000002e-05,
      "loss": 0.0028,
      "step": 115860
    },
    {
      "epoch": 3.8623333333333334,
      "grad_norm": 0.657558023929596,
      "learning_rate": 2.586041666666667e-05,
      "loss": 0.0027,
      "step": 115870
    },
    {
      "epoch": 3.8626666666666667,
      "grad_norm": 0.2200472503900528,
      "learning_rate": 2.5858333333333333e-05,
      "loss": 0.0031,
      "step": 115880
    },
    {
      "epoch": 3.863,
      "grad_norm": 0.30336570739746094,
      "learning_rate": 2.5856249999999998e-05,
      "loss": 0.0022,
      "step": 115890
    },
    {
      "epoch": 3.8633333333333333,
      "grad_norm": 0.029693718999624252,
      "learning_rate": 2.5854166666666667e-05,
      "loss": 0.0027,
      "step": 115900
    },
    {
      "epoch": 3.8636666666666666,
      "grad_norm": 0.028993604704737663,
      "learning_rate": 2.5852083333333333e-05,
      "loss": 0.0034,
      "step": 115910
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.46705564856529236,
      "learning_rate": 2.585e-05,
      "loss": 0.0016,
      "step": 115920
    },
    {
      "epoch": 3.864333333333333,
      "grad_norm": 0.20025669038295746,
      "learning_rate": 2.5847916666666667e-05,
      "loss": 0.0018,
      "step": 115930
    },
    {
      "epoch": 3.8646666666666665,
      "grad_norm": 0.08578044921159744,
      "learning_rate": 2.5845833333333336e-05,
      "loss": 0.0019,
      "step": 115940
    },
    {
      "epoch": 3.865,
      "grad_norm": 0.11473080515861511,
      "learning_rate": 2.584375e-05,
      "loss": 0.0021,
      "step": 115950
    },
    {
      "epoch": 3.865333333333333,
      "grad_norm": 0.24881994724273682,
      "learning_rate": 2.584166666666667e-05,
      "loss": 0.0022,
      "step": 115960
    },
    {
      "epoch": 3.865666666666667,
      "grad_norm": 0.20021885633468628,
      "learning_rate": 2.5839583333333332e-05,
      "loss": 0.0015,
      "step": 115970
    },
    {
      "epoch": 3.866,
      "grad_norm": 0.11456750333309174,
      "learning_rate": 2.5837500000000005e-05,
      "loss": 0.0032,
      "step": 115980
    },
    {
      "epoch": 3.8663333333333334,
      "grad_norm": 0.05730452015995979,
      "learning_rate": 2.5835416666666667e-05,
      "loss": 0.0023,
      "step": 115990
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.4000517427921295,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.0019,
      "step": 116000
    },
    {
      "epoch": 3.867,
      "grad_norm": 0.1742365062236786,
      "learning_rate": 2.583125e-05,
      "loss": 0.0019,
      "step": 116010
    },
    {
      "epoch": 3.8673333333333333,
      "grad_norm": 0.15895405411720276,
      "learning_rate": 2.5829166666666667e-05,
      "loss": 0.0027,
      "step": 116020
    },
    {
      "epoch": 3.8676666666666666,
      "grad_norm": 0.03380201384425163,
      "learning_rate": 2.5827083333333335e-05,
      "loss": 0.0025,
      "step": 116030
    },
    {
      "epoch": 3.868,
      "grad_norm": 0.2287878394126892,
      "learning_rate": 2.5824999999999998e-05,
      "loss": 0.0025,
      "step": 116040
    },
    {
      "epoch": 3.868333333333333,
      "grad_norm": 0.3714284598827362,
      "learning_rate": 2.582291666666667e-05,
      "loss": 0.0014,
      "step": 116050
    },
    {
      "epoch": 3.868666666666667,
      "grad_norm": 0.4283936023712158,
      "learning_rate": 2.5820833333333332e-05,
      "loss": 0.0024,
      "step": 116060
    },
    {
      "epoch": 3.8689999999999998,
      "grad_norm": 0.057541098445653915,
      "learning_rate": 2.5818750000000004e-05,
      "loss": 0.0013,
      "step": 116070
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.14310036599636078,
      "learning_rate": 2.5816666666666666e-05,
      "loss": 0.0025,
      "step": 116080
    },
    {
      "epoch": 3.869666666666667,
      "grad_norm": 0.02923717349767685,
      "learning_rate": 2.5814583333333335e-05,
      "loss": 0.0033,
      "step": 116090
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.0305729191750288,
      "learning_rate": 2.58125e-05,
      "loss": 0.0014,
      "step": 116100
    },
    {
      "epoch": 3.8703333333333334,
      "grad_norm": 0.05759581923484802,
      "learning_rate": 2.581041666666667e-05,
      "loss": 0.0025,
      "step": 116110
    },
    {
      "epoch": 3.8706666666666667,
      "grad_norm": 0.1143760159611702,
      "learning_rate": 2.5808333333333335e-05,
      "loss": 0.0027,
      "step": 116120
    },
    {
      "epoch": 3.871,
      "grad_norm": 0.3715895116329193,
      "learning_rate": 2.5806249999999997e-05,
      "loss": 0.0025,
      "step": 116130
    },
    {
      "epoch": 3.8713333333333333,
      "grad_norm": 0.08576484024524689,
      "learning_rate": 2.580416666666667e-05,
      "loss": 0.0027,
      "step": 116140
    },
    {
      "epoch": 3.8716666666666666,
      "grad_norm": 0.2286532074213028,
      "learning_rate": 2.580208333333333e-05,
      "loss": 0.0014,
      "step": 116150
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.25726497173309326,
      "learning_rate": 2.58e-05,
      "loss": 0.002,
      "step": 116160
    },
    {
      "epoch": 3.872333333333333,
      "grad_norm": 0.004720404278486967,
      "learning_rate": 2.5797916666666666e-05,
      "loss": 0.0022,
      "step": 116170
    },
    {
      "epoch": 3.8726666666666665,
      "grad_norm": 0.032949790358543396,
      "learning_rate": 2.5795833333333335e-05,
      "loss": 0.0027,
      "step": 116180
    },
    {
      "epoch": 3.873,
      "grad_norm": 0.1714145541191101,
      "learning_rate": 2.579375e-05,
      "loss": 0.002,
      "step": 116190
    },
    {
      "epoch": 3.873333333333333,
      "grad_norm": 0.20005355775356293,
      "learning_rate": 2.579166666666667e-05,
      "loss": 0.0026,
      "step": 116200
    },
    {
      "epoch": 3.873666666666667,
      "grad_norm": 0.4285460412502289,
      "learning_rate": 2.5789583333333335e-05,
      "loss": 0.0019,
      "step": 116210
    },
    {
      "epoch": 3.874,
      "grad_norm": 0.08574213832616806,
      "learning_rate": 2.5787500000000003e-05,
      "loss": 0.0024,
      "step": 116220
    },
    {
      "epoch": 3.8743333333333334,
      "grad_norm": 0.25763434171676636,
      "learning_rate": 2.578541666666667e-05,
      "loss": 0.0028,
      "step": 116230
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.38074764609336853,
      "learning_rate": 2.5783333333333338e-05,
      "loss": 0.0018,
      "step": 116240
    },
    {
      "epoch": 3.875,
      "grad_norm": 0.011869960464537144,
      "learning_rate": 2.578125e-05,
      "loss": 0.0019,
      "step": 116250
    },
    {
      "epoch": 3.8753333333333333,
      "grad_norm": 0.03113337978720665,
      "learning_rate": 2.5779166666666665e-05,
      "loss": 0.0025,
      "step": 116260
    },
    {
      "epoch": 3.8756666666666666,
      "grad_norm": 0.1428254395723343,
      "learning_rate": 2.5777083333333334e-05,
      "loss": 0.0025,
      "step": 116270
    },
    {
      "epoch": 3.876,
      "grad_norm": 0.14336873590946198,
      "learning_rate": 2.5775e-05,
      "loss": 0.0023,
      "step": 116280
    },
    {
      "epoch": 3.876333333333333,
      "grad_norm": 0.01282165851444006,
      "learning_rate": 2.577291666666667e-05,
      "loss": 0.0014,
      "step": 116290
    },
    {
      "epoch": 3.876666666666667,
      "grad_norm": 0.1166505217552185,
      "learning_rate": 2.5770833333333334e-05,
      "loss": 0.0016,
      "step": 116300
    },
    {
      "epoch": 3.877,
      "grad_norm": 0.05747135356068611,
      "learning_rate": 2.5768750000000003e-05,
      "loss": 0.0026,
      "step": 116310
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.05821872502565384,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.0022,
      "step": 116320
    },
    {
      "epoch": 3.877666666666667,
      "grad_norm": 0.45726829767227173,
      "learning_rate": 2.5764583333333337e-05,
      "loss": 0.0014,
      "step": 116330
    },
    {
      "epoch": 3.878,
      "grad_norm": 0.11420255899429321,
      "learning_rate": 2.57625e-05,
      "loss": 0.0016,
      "step": 116340
    },
    {
      "epoch": 3.8783333333333334,
      "grad_norm": 0.11445235460996628,
      "learning_rate": 2.576041666666667e-05,
      "loss": 0.0028,
      "step": 116350
    },
    {
      "epoch": 3.8786666666666667,
      "grad_norm": 0.17150907218456268,
      "learning_rate": 2.5758333333333334e-05,
      "loss": 0.0021,
      "step": 116360
    },
    {
      "epoch": 3.879,
      "grad_norm": 0.0857035368680954,
      "learning_rate": 2.5756250000000003e-05,
      "loss": 0.0014,
      "step": 116370
    },
    {
      "epoch": 3.8793333333333333,
      "grad_norm": 0.00443591084331274,
      "learning_rate": 2.5754166666666668e-05,
      "loss": 0.0032,
      "step": 116380
    },
    {
      "epoch": 3.8796666666666666,
      "grad_norm": 0.05974217504262924,
      "learning_rate": 2.5752083333333334e-05,
      "loss": 0.0023,
      "step": 116390
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.11439614742994308,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.0017,
      "step": 116400
    },
    {
      "epoch": 3.880333333333333,
      "grad_norm": 0.2290673702955246,
      "learning_rate": 2.5747916666666665e-05,
      "loss": 0.0025,
      "step": 116410
    },
    {
      "epoch": 3.8806666666666665,
      "grad_norm": 0.11531341075897217,
      "learning_rate": 2.5745833333333337e-05,
      "loss": 0.0015,
      "step": 116420
    },
    {
      "epoch": 3.8810000000000002,
      "grad_norm": 0.17206913232803345,
      "learning_rate": 2.574375e-05,
      "loss": 0.0017,
      "step": 116430
    },
    {
      "epoch": 3.881333333333333,
      "grad_norm": 0.3716311454772949,
      "learning_rate": 2.5741666666666668e-05,
      "loss": 0.0021,
      "step": 116440
    },
    {
      "epoch": 3.881666666666667,
      "grad_norm": 0.03153577446937561,
      "learning_rate": 2.5739583333333333e-05,
      "loss": 0.0023,
      "step": 116450
    },
    {
      "epoch": 3.882,
      "grad_norm": 0.05864395946264267,
      "learning_rate": 2.5737500000000002e-05,
      "loss": 0.0031,
      "step": 116460
    },
    {
      "epoch": 3.8823333333333334,
      "grad_norm": 0.3428792357444763,
      "learning_rate": 2.5735416666666668e-05,
      "loss": 0.0017,
      "step": 116470
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.1145157739520073,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.002,
      "step": 116480
    },
    {
      "epoch": 3.883,
      "grad_norm": 0.25725623965263367,
      "learning_rate": 2.5731250000000002e-05,
      "loss": 0.002,
      "step": 116490
    },
    {
      "epoch": 3.8833333333333333,
      "grad_norm": 0.19999182224273682,
      "learning_rate": 2.5729166666666664e-05,
      "loss": 0.0027,
      "step": 116500
    },
    {
      "epoch": 3.8836666666666666,
      "grad_norm": 0.32725411653518677,
      "learning_rate": 2.5727083333333336e-05,
      "loss": 0.0028,
      "step": 116510
    },
    {
      "epoch": 3.884,
      "grad_norm": 0.057426534593105316,
      "learning_rate": 2.5725e-05,
      "loss": 0.0023,
      "step": 116520
    },
    {
      "epoch": 3.884333333333333,
      "grad_norm": 0.08745653927326202,
      "learning_rate": 2.5722916666666667e-05,
      "loss": 0.0022,
      "step": 116530
    },
    {
      "epoch": 3.884666666666667,
      "grad_norm": 0.11529450863599777,
      "learning_rate": 2.5720833333333333e-05,
      "loss": 0.002,
      "step": 116540
    },
    {
      "epoch": 3.885,
      "grad_norm": 0.40024736523628235,
      "learning_rate": 2.5718750000000002e-05,
      "loss": 0.0023,
      "step": 116550
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.25736644864082336,
      "learning_rate": 2.5716666666666667e-05,
      "loss": 0.0029,
      "step": 116560
    },
    {
      "epoch": 3.885666666666667,
      "grad_norm": 0.5426192879676819,
      "learning_rate": 2.5714583333333336e-05,
      "loss": 0.0025,
      "step": 116570
    },
    {
      "epoch": 3.886,
      "grad_norm": 0.11853643506765366,
      "learning_rate": 2.57125e-05,
      "loss": 0.0022,
      "step": 116580
    },
    {
      "epoch": 3.8863333333333334,
      "grad_norm": 0.34267181158065796,
      "learning_rate": 2.571041666666667e-05,
      "loss": 0.0022,
      "step": 116590
    },
    {
      "epoch": 3.8866666666666667,
      "grad_norm": 0.3432413935661316,
      "learning_rate": 2.5708333333333336e-05,
      "loss": 0.0017,
      "step": 116600
    },
    {
      "epoch": 3.887,
      "grad_norm": 0.08614468574523926,
      "learning_rate": 2.5706250000000005e-05,
      "loss": 0.0026,
      "step": 116610
    },
    {
      "epoch": 3.8873333333333333,
      "grad_norm": 0.08588505536317825,
      "learning_rate": 2.5704166666666667e-05,
      "loss": 0.0017,
      "step": 116620
    },
    {
      "epoch": 3.8876666666666666,
      "grad_norm": 0.03680412843823433,
      "learning_rate": 2.5702083333333332e-05,
      "loss": 0.0027,
      "step": 116630
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.17133013904094696,
      "learning_rate": 2.57e-05,
      "loss": 0.0027,
      "step": 116640
    },
    {
      "epoch": 3.888333333333333,
      "grad_norm": 0.6569923758506775,
      "learning_rate": 2.5697916666666667e-05,
      "loss": 0.0018,
      "step": 116650
    },
    {
      "epoch": 3.8886666666666665,
      "grad_norm": 0.17157991230487823,
      "learning_rate": 2.5695833333333336e-05,
      "loss": 0.0015,
      "step": 116660
    },
    {
      "epoch": 3.8890000000000002,
      "grad_norm": 0.22855792939662933,
      "learning_rate": 2.569375e-05,
      "loss": 0.002,
      "step": 116670
    },
    {
      "epoch": 3.889333333333333,
      "grad_norm": 0.19997923076152802,
      "learning_rate": 2.569166666666667e-05,
      "loss": 0.002,
      "step": 116680
    },
    {
      "epoch": 3.889666666666667,
      "grad_norm": 0.23601359128952026,
      "learning_rate": 2.5689583333333332e-05,
      "loss": 0.0026,
      "step": 116690
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.2855193614959717,
      "learning_rate": 2.5687500000000004e-05,
      "loss": 0.0023,
      "step": 116700
    },
    {
      "epoch": 3.8903333333333334,
      "grad_norm": 0.08602236211299896,
      "learning_rate": 2.5685416666666666e-05,
      "loss": 0.0028,
      "step": 116710
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.3425823450088501,
      "learning_rate": 2.5683333333333335e-05,
      "loss": 0.0015,
      "step": 116720
    },
    {
      "epoch": 3.891,
      "grad_norm": 0.19476734101772308,
      "learning_rate": 2.568125e-05,
      "loss": 0.0019,
      "step": 116730
    },
    {
      "epoch": 3.8913333333333333,
      "grad_norm": 0.17166654765605927,
      "learning_rate": 2.5679166666666666e-05,
      "loss": 0.0024,
      "step": 116740
    },
    {
      "epoch": 3.8916666666666666,
      "grad_norm": 0.39969325065612793,
      "learning_rate": 2.5677083333333335e-05,
      "loss": 0.0013,
      "step": 116750
    },
    {
      "epoch": 3.892,
      "grad_norm": 0.678056538105011,
      "learning_rate": 2.5675e-05,
      "loss": 0.0017,
      "step": 116760
    },
    {
      "epoch": 3.892333333333333,
      "grad_norm": 0.3928452432155609,
      "learning_rate": 2.567291666666667e-05,
      "loss": 0.0023,
      "step": 116770
    },
    {
      "epoch": 3.892666666666667,
      "grad_norm": 0.2762468159198761,
      "learning_rate": 2.567083333333333e-05,
      "loss": 0.0017,
      "step": 116780
    },
    {
      "epoch": 3.893,
      "grad_norm": 0.31392720341682434,
      "learning_rate": 2.5668750000000004e-05,
      "loss": 0.0029,
      "step": 116790
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.08582094311714172,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0021,
      "step": 116800
    },
    {
      "epoch": 3.893666666666667,
      "grad_norm": 0.11469040811061859,
      "learning_rate": 2.5664583333333335e-05,
      "loss": 0.0018,
      "step": 116810
    },
    {
      "epoch": 3.894,
      "grad_norm": 0.030694708228111267,
      "learning_rate": 2.56625e-05,
      "loss": 0.0032,
      "step": 116820
    },
    {
      "epoch": 3.8943333333333334,
      "grad_norm": 0.08679798990488052,
      "learning_rate": 2.566041666666667e-05,
      "loss": 0.0019,
      "step": 116830
    },
    {
      "epoch": 3.8946666666666667,
      "grad_norm": 0.14360305666923523,
      "learning_rate": 2.5658333333333335e-05,
      "loss": 0.0021,
      "step": 116840
    },
    {
      "epoch": 3.895,
      "grad_norm": 0.2573021650314331,
      "learning_rate": 2.5656250000000004e-05,
      "loss": 0.0023,
      "step": 116850
    },
    {
      "epoch": 3.8953333333333333,
      "grad_norm": 0.2611537575721741,
      "learning_rate": 2.565416666666667e-05,
      "loss": 0.0025,
      "step": 116860
    },
    {
      "epoch": 3.8956666666666666,
      "grad_norm": 0.028762632980942726,
      "learning_rate": 2.565208333333333e-05,
      "loss": 0.0031,
      "step": 116870
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.02899399772286415,
      "learning_rate": 2.5650000000000003e-05,
      "loss": 0.0014,
      "step": 116880
    },
    {
      "epoch": 3.896333333333333,
      "grad_norm": 0.007075540255755186,
      "learning_rate": 2.5647916666666666e-05,
      "loss": 0.0013,
      "step": 116890
    },
    {
      "epoch": 3.8966666666666665,
      "grad_norm": 0.057247623801231384,
      "learning_rate": 2.5645833333333334e-05,
      "loss": 0.002,
      "step": 116900
    },
    {
      "epoch": 3.8970000000000002,
      "grad_norm": 0.171648770570755,
      "learning_rate": 2.564375e-05,
      "loss": 0.0017,
      "step": 116910
    },
    {
      "epoch": 3.897333333333333,
      "grad_norm": 0.17141135036945343,
      "learning_rate": 2.564166666666667e-05,
      "loss": 0.0017,
      "step": 116920
    },
    {
      "epoch": 3.897666666666667,
      "grad_norm": 0.14500746130943298,
      "learning_rate": 2.5639583333333334e-05,
      "loss": 0.0026,
      "step": 116930
    },
    {
      "epoch": 3.898,
      "grad_norm": 0.17373408377170563,
      "learning_rate": 2.5637500000000003e-05,
      "loss": 0.0027,
      "step": 116940
    },
    {
      "epoch": 3.8983333333333334,
      "grad_norm": 0.03193783015012741,
      "learning_rate": 2.563541666666667e-05,
      "loss": 0.0024,
      "step": 116950
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.3145902752876282,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0029,
      "step": 116960
    },
    {
      "epoch": 3.899,
      "grad_norm": 0.08633638173341751,
      "learning_rate": 2.563125e-05,
      "loss": 0.0027,
      "step": 116970
    },
    {
      "epoch": 3.8993333333333333,
      "grad_norm": 0.1715480536222458,
      "learning_rate": 2.5629166666666665e-05,
      "loss": 0.0023,
      "step": 116980
    },
    {
      "epoch": 3.8996666666666666,
      "grad_norm": 0.6348463296890259,
      "learning_rate": 2.5627083333333334e-05,
      "loss": 0.0025,
      "step": 116990
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.17161241173744202,
      "learning_rate": 2.5625e-05,
      "loss": 0.0016,
      "step": 117000
    },
    {
      "epoch": 3.900333333333333,
      "grad_norm": 0.029458671808242798,
      "learning_rate": 2.562291666666667e-05,
      "loss": 0.0028,
      "step": 117010
    },
    {
      "epoch": 3.9006666666666665,
      "grad_norm": 0.23331937193870544,
      "learning_rate": 2.5620833333333334e-05,
      "loss": 0.0027,
      "step": 117020
    },
    {
      "epoch": 3.901,
      "grad_norm": 0.17565296590328217,
      "learning_rate": 2.5618750000000003e-05,
      "loss": 0.0027,
      "step": 117030
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.22850334644317627,
      "learning_rate": 2.5616666666666668e-05,
      "loss": 0.0028,
      "step": 117040
    },
    {
      "epoch": 3.9016666666666664,
      "grad_norm": 0.05731607973575592,
      "learning_rate": 2.5614583333333337e-05,
      "loss": 0.0019,
      "step": 117050
    },
    {
      "epoch": 3.902,
      "grad_norm": 0.12610864639282227,
      "learning_rate": 2.56125e-05,
      "loss": 0.0018,
      "step": 117060
    },
    {
      "epoch": 3.9023333333333334,
      "grad_norm": 0.05734669789671898,
      "learning_rate": 2.561041666666667e-05,
      "loss": 0.0022,
      "step": 117070
    },
    {
      "epoch": 3.9026666666666667,
      "grad_norm": 0.47402071952819824,
      "learning_rate": 2.5608333333333334e-05,
      "loss": 0.0022,
      "step": 117080
    },
    {
      "epoch": 3.903,
      "grad_norm": 0.02974383905529976,
      "learning_rate": 2.5606250000000002e-05,
      "loss": 0.0013,
      "step": 117090
    },
    {
      "epoch": 3.9033333333333333,
      "grad_norm": 0.19991764426231384,
      "learning_rate": 2.5604166666666668e-05,
      "loss": 0.0025,
      "step": 117100
    },
    {
      "epoch": 3.9036666666666666,
      "grad_norm": 0.1426616907119751,
      "learning_rate": 2.5602083333333333e-05,
      "loss": 0.0018,
      "step": 117110
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.4283908009529114,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.002,
      "step": 117120
    },
    {
      "epoch": 3.904333333333333,
      "grad_norm": 0.3774086833000183,
      "learning_rate": 2.5597916666666664e-05,
      "loss": 0.0026,
      "step": 117130
    },
    {
      "epoch": 3.9046666666666665,
      "grad_norm": 0.08611048758029938,
      "learning_rate": 2.5595833333333337e-05,
      "loss": 0.0023,
      "step": 117140
    },
    {
      "epoch": 3.9050000000000002,
      "grad_norm": 0.1718582808971405,
      "learning_rate": 2.559375e-05,
      "loss": 0.0017,
      "step": 117150
    },
    {
      "epoch": 3.905333333333333,
      "grad_norm": 0.34320300817489624,
      "learning_rate": 2.559166666666667e-05,
      "loss": 0.0013,
      "step": 117160
    },
    {
      "epoch": 3.905666666666667,
      "grad_norm": 0.44633400440216064,
      "learning_rate": 2.5589583333333333e-05,
      "loss": 0.0018,
      "step": 117170
    },
    {
      "epoch": 3.906,
      "grad_norm": 0.42476359009742737,
      "learning_rate": 2.5587500000000002e-05,
      "loss": 0.0033,
      "step": 117180
    },
    {
      "epoch": 3.9063333333333334,
      "grad_norm": 0.4189941883087158,
      "learning_rate": 2.5585416666666667e-05,
      "loss": 0.0036,
      "step": 117190
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.14305037260055542,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.0023,
      "step": 117200
    },
    {
      "epoch": 3.907,
      "grad_norm": 0.1997583657503128,
      "learning_rate": 2.5581250000000002e-05,
      "loss": 0.0022,
      "step": 117210
    },
    {
      "epoch": 3.9073333333333333,
      "grad_norm": 0.22905071079730988,
      "learning_rate": 2.5579166666666664e-05,
      "loss": 0.0031,
      "step": 117220
    },
    {
      "epoch": 3.9076666666666666,
      "grad_norm": 0.17113856971263885,
      "learning_rate": 2.5577083333333336e-05,
      "loss": 0.0016,
      "step": 117230
    },
    {
      "epoch": 3.908,
      "grad_norm": 0.20098687708377838,
      "learning_rate": 2.5574999999999998e-05,
      "loss": 0.0012,
      "step": 117240
    },
    {
      "epoch": 3.908333333333333,
      "grad_norm": 0.22829915583133698,
      "learning_rate": 2.5572916666666667e-05,
      "loss": 0.0025,
      "step": 117250
    },
    {
      "epoch": 3.9086666666666665,
      "grad_norm": 0.029299434274435043,
      "learning_rate": 2.5570833333333333e-05,
      "loss": 0.0019,
      "step": 117260
    },
    {
      "epoch": 3.909,
      "grad_norm": 0.08582644164562225,
      "learning_rate": 2.556875e-05,
      "loss": 0.0025,
      "step": 117270
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.2857474684715271,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0026,
      "step": 117280
    },
    {
      "epoch": 3.9096666666666664,
      "grad_norm": 0.21482636034488678,
      "learning_rate": 2.5564583333333336e-05,
      "loss": 0.0029,
      "step": 117290
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.48571810126304626,
      "learning_rate": 2.55625e-05,
      "loss": 0.0017,
      "step": 117300
    },
    {
      "epoch": 3.9103333333333334,
      "grad_norm": 0.2857949137687683,
      "learning_rate": 2.556041666666667e-05,
      "loss": 0.0021,
      "step": 117310
    },
    {
      "epoch": 3.9106666666666667,
      "grad_norm": 0.1430201381444931,
      "learning_rate": 2.5558333333333336e-05,
      "loss": 0.0019,
      "step": 117320
    },
    {
      "epoch": 3.911,
      "grad_norm": 0.7142888307571411,
      "learning_rate": 2.5556250000000005e-05,
      "loss": 0.0031,
      "step": 117330
    },
    {
      "epoch": 3.9113333333333333,
      "grad_norm": 0.31483128666877747,
      "learning_rate": 2.5554166666666667e-05,
      "loss": 0.0016,
      "step": 117340
    },
    {
      "epoch": 3.9116666666666666,
      "grad_norm": 0.40010473132133484,
      "learning_rate": 2.5552083333333332e-05,
      "loss": 0.0017,
      "step": 117350
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.42828962206840515,
      "learning_rate": 2.555e-05,
      "loss": 0.0021,
      "step": 117360
    },
    {
      "epoch": 3.912333333333333,
      "grad_norm": 0.05995834991335869,
      "learning_rate": 2.5547916666666667e-05,
      "loss": 0.0031,
      "step": 117370
    },
    {
      "epoch": 3.9126666666666665,
      "grad_norm": 0.5546345710754395,
      "learning_rate": 2.5545833333333335e-05,
      "loss": 0.0024,
      "step": 117380
    },
    {
      "epoch": 3.9130000000000003,
      "grad_norm": 0.02901221439242363,
      "learning_rate": 2.554375e-05,
      "loss": 0.0014,
      "step": 117390
    },
    {
      "epoch": 3.913333333333333,
      "grad_norm": 0.17154614627361298,
      "learning_rate": 2.554166666666667e-05,
      "loss": 0.0017,
      "step": 117400
    },
    {
      "epoch": 3.913666666666667,
      "grad_norm": 0.014577953144907951,
      "learning_rate": 2.5539583333333332e-05,
      "loss": 0.0029,
      "step": 117410
    },
    {
      "epoch": 3.914,
      "grad_norm": 0.05855463445186615,
      "learning_rate": 2.5537500000000004e-05,
      "loss": 0.0027,
      "step": 117420
    },
    {
      "epoch": 3.9143333333333334,
      "grad_norm": 0.25768718123435974,
      "learning_rate": 2.5535416666666666e-05,
      "loss": 0.0018,
      "step": 117430
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.2567484378814697,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0022,
      "step": 117440
    },
    {
      "epoch": 3.915,
      "grad_norm": 0.02928593009710312,
      "learning_rate": 2.553125e-05,
      "loss": 0.0019,
      "step": 117450
    },
    {
      "epoch": 3.9153333333333333,
      "grad_norm": 0.05738207697868347,
      "learning_rate": 2.5529166666666666e-05,
      "loss": 0.0036,
      "step": 117460
    },
    {
      "epoch": 3.9156666666666666,
      "grad_norm": 0.4285098910331726,
      "learning_rate": 2.5527083333333335e-05,
      "loss": 0.002,
      "step": 117470
    },
    {
      "epoch": 3.916,
      "grad_norm": 0.2002723067998886,
      "learning_rate": 2.5525e-05,
      "loss": 0.0019,
      "step": 117480
    },
    {
      "epoch": 3.916333333333333,
      "grad_norm": 0.08700095117092133,
      "learning_rate": 2.552291666666667e-05,
      "loss": 0.0016,
      "step": 117490
    },
    {
      "epoch": 3.9166666666666665,
      "grad_norm": 0.3636268973350525,
      "learning_rate": 2.552083333333333e-05,
      "loss": 0.0028,
      "step": 117500
    },
    {
      "epoch": 3.917,
      "grad_norm": 0.029302243143320084,
      "learning_rate": 2.5518750000000004e-05,
      "loss": 0.0019,
      "step": 117510
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.2816259264945984,
      "learning_rate": 2.5516666666666666e-05,
      "loss": 0.0024,
      "step": 117520
    },
    {
      "epoch": 3.9176666666666664,
      "grad_norm": 0.0857112780213356,
      "learning_rate": 2.5514583333333335e-05,
      "loss": 0.0021,
      "step": 117530
    },
    {
      "epoch": 3.918,
      "grad_norm": 0.25733646750450134,
      "learning_rate": 2.55125e-05,
      "loss": 0.0024,
      "step": 117540
    },
    {
      "epoch": 3.9183333333333334,
      "grad_norm": 0.25817251205444336,
      "learning_rate": 2.551041666666667e-05,
      "loss": 0.0017,
      "step": 117550
    },
    {
      "epoch": 3.9186666666666667,
      "grad_norm": 0.4285750985145569,
      "learning_rate": 2.5508333333333334e-05,
      "loss": 0.0017,
      "step": 117560
    },
    {
      "epoch": 3.919,
      "grad_norm": 0.1428859978914261,
      "learning_rate": 2.5506250000000003e-05,
      "loss": 0.002,
      "step": 117570
    },
    {
      "epoch": 3.9193333333333333,
      "grad_norm": 0.3713497817516327,
      "learning_rate": 2.550416666666667e-05,
      "loss": 0.002,
      "step": 117580
    },
    {
      "epoch": 3.9196666666666666,
      "grad_norm": 0.19989852607250214,
      "learning_rate": 2.550208333333333e-05,
      "loss": 0.0016,
      "step": 117590
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.14280804991722107,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0029,
      "step": 117600
    },
    {
      "epoch": 3.9203333333333332,
      "grad_norm": 0.03205662593245506,
      "learning_rate": 2.5497916666666665e-05,
      "loss": 0.0017,
      "step": 117610
    },
    {
      "epoch": 3.9206666666666665,
      "grad_norm": 0.0674581304192543,
      "learning_rate": 2.5495833333333334e-05,
      "loss": 0.0021,
      "step": 117620
    },
    {
      "epoch": 3.9210000000000003,
      "grad_norm": 0.02859567478299141,
      "learning_rate": 2.549375e-05,
      "loss": 0.0022,
      "step": 117630
    },
    {
      "epoch": 3.921333333333333,
      "grad_norm": 0.14281919598579407,
      "learning_rate": 2.549166666666667e-05,
      "loss": 0.0019,
      "step": 117640
    },
    {
      "epoch": 3.921666666666667,
      "grad_norm": 0.1428459882736206,
      "learning_rate": 2.5489583333333334e-05,
      "loss": 0.0022,
      "step": 117650
    },
    {
      "epoch": 3.922,
      "grad_norm": 0.14284288883209229,
      "learning_rate": 2.5487500000000003e-05,
      "loss": 0.0022,
      "step": 117660
    },
    {
      "epoch": 3.9223333333333334,
      "grad_norm": 0.2856297791004181,
      "learning_rate": 2.548541666666667e-05,
      "loss": 0.0027,
      "step": 117670
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.05758645758032799,
      "learning_rate": 2.5483333333333337e-05,
      "loss": 0.0023,
      "step": 117680
    },
    {
      "epoch": 3.923,
      "grad_norm": 0.005804487969726324,
      "learning_rate": 2.548125e-05,
      "loss": 0.0021,
      "step": 117690
    },
    {
      "epoch": 3.9233333333333333,
      "grad_norm": 0.0576043426990509,
      "learning_rate": 2.547916666666667e-05,
      "loss": 0.0026,
      "step": 117700
    },
    {
      "epoch": 3.9236666666666666,
      "grad_norm": 0.05818726122379303,
      "learning_rate": 2.5477083333333334e-05,
      "loss": 0.0022,
      "step": 117710
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.2570771872997284,
      "learning_rate": 2.5475e-05,
      "loss": 0.002,
      "step": 117720
    },
    {
      "epoch": 3.9243333333333332,
      "grad_norm": 0.033320873975753784,
      "learning_rate": 2.5472916666666668e-05,
      "loss": 0.002,
      "step": 117730
    },
    {
      "epoch": 3.9246666666666665,
      "grad_norm": 0.14292006194591522,
      "learning_rate": 2.5470833333333334e-05,
      "loss": 0.0023,
      "step": 117740
    },
    {
      "epoch": 3.925,
      "grad_norm": 0.05743957310914993,
      "learning_rate": 2.5468750000000002e-05,
      "loss": 0.002,
      "step": 117750
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.057354170829057693,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0018,
      "step": 117760
    },
    {
      "epoch": 3.9256666666666664,
      "grad_norm": 0.05748452991247177,
      "learning_rate": 2.5464583333333337e-05,
      "loss": 0.0018,
      "step": 117770
    },
    {
      "epoch": 3.926,
      "grad_norm": 0.00338764488697052,
      "learning_rate": 2.54625e-05,
      "loss": 0.0016,
      "step": 117780
    },
    {
      "epoch": 3.9263333333333335,
      "grad_norm": 0.08593779057264328,
      "learning_rate": 2.546041666666667e-05,
      "loss": 0.0019,
      "step": 117790
    },
    {
      "epoch": 3.9266666666666667,
      "grad_norm": 0.6619055867195129,
      "learning_rate": 2.5458333333333333e-05,
      "loss": 0.0015,
      "step": 117800
    },
    {
      "epoch": 3.927,
      "grad_norm": 0.4626099169254303,
      "learning_rate": 2.5456250000000002e-05,
      "loss": 0.0028,
      "step": 117810
    },
    {
      "epoch": 3.9273333333333333,
      "grad_norm": 0.0572945699095726,
      "learning_rate": 2.5454166666666668e-05,
      "loss": 0.0017,
      "step": 117820
    },
    {
      "epoch": 3.9276666666666666,
      "grad_norm": 0.114258773624897,
      "learning_rate": 2.5452083333333333e-05,
      "loss": 0.0018,
      "step": 117830
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.26687386631965637,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 0.0015,
      "step": 117840
    },
    {
      "epoch": 3.9283333333333332,
      "grad_norm": 0.071668341755867,
      "learning_rate": 2.5447916666666664e-05,
      "loss": 0.0027,
      "step": 117850
    },
    {
      "epoch": 3.9286666666666665,
      "grad_norm": 0.32844647765159607,
      "learning_rate": 2.5445833333333336e-05,
      "loss": 0.0027,
      "step": 117860
    },
    {
      "epoch": 3.9290000000000003,
      "grad_norm": 0.20016734302043915,
      "learning_rate": 2.544375e-05,
      "loss": 0.0018,
      "step": 117870
    },
    {
      "epoch": 3.929333333333333,
      "grad_norm": 0.14283621311187744,
      "learning_rate": 2.544166666666667e-05,
      "loss": 0.0019,
      "step": 117880
    },
    {
      "epoch": 3.929666666666667,
      "grad_norm": 0.05912553891539574,
      "learning_rate": 2.5439583333333333e-05,
      "loss": 0.002,
      "step": 117890
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.11459191888570786,
      "learning_rate": 2.54375e-05,
      "loss": 0.0013,
      "step": 117900
    },
    {
      "epoch": 3.9303333333333335,
      "grad_norm": 0.029163455590605736,
      "learning_rate": 2.5435416666666667e-05,
      "loss": 0.0018,
      "step": 117910
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.31438499689102173,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0017,
      "step": 117920
    },
    {
      "epoch": 3.931,
      "grad_norm": 0.2858472168445587,
      "learning_rate": 2.543125e-05,
      "loss": 0.0021,
      "step": 117930
    },
    {
      "epoch": 3.9313333333333333,
      "grad_norm": 0.02924894541501999,
      "learning_rate": 2.542916666666667e-05,
      "loss": 0.0014,
      "step": 117940
    },
    {
      "epoch": 3.9316666666666666,
      "grad_norm": 0.08570890873670578,
      "learning_rate": 2.5427083333333336e-05,
      "loss": 0.0016,
      "step": 117950
    },
    {
      "epoch": 3.932,
      "grad_norm": 0.22845347225666046,
      "learning_rate": 2.5424999999999998e-05,
      "loss": 0.0024,
      "step": 117960
    },
    {
      "epoch": 3.9323333333333332,
      "grad_norm": 0.14297659695148468,
      "learning_rate": 2.5422916666666667e-05,
      "loss": 0.0024,
      "step": 117970
    },
    {
      "epoch": 3.9326666666666665,
      "grad_norm": 0.2286359816789627,
      "learning_rate": 2.5420833333333332e-05,
      "loss": 0.0027,
      "step": 117980
    },
    {
      "epoch": 3.933,
      "grad_norm": 0.2005368322134018,
      "learning_rate": 2.541875e-05,
      "loss": 0.003,
      "step": 117990
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.6289527416229248,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.0013,
      "step": 118000
    },
    {
      "epoch": 3.9336666666666664,
      "grad_norm": 0.28554049134254456,
      "learning_rate": 2.5414583333333336e-05,
      "loss": 0.0018,
      "step": 118010
    },
    {
      "epoch": 3.934,
      "grad_norm": 0.08721593022346497,
      "learning_rate": 2.54125e-05,
      "loss": 0.0029,
      "step": 118020
    },
    {
      "epoch": 3.9343333333333335,
      "grad_norm": 0.2856338620185852,
      "learning_rate": 2.541041666666667e-05,
      "loss": 0.0017,
      "step": 118030
    },
    {
      "epoch": 3.9346666666666668,
      "grad_norm": 0.5145337581634521,
      "learning_rate": 2.5408333333333335e-05,
      "loss": 0.002,
      "step": 118040
    },
    {
      "epoch": 3.935,
      "grad_norm": 0.20014514029026031,
      "learning_rate": 2.5406250000000004e-05,
      "loss": 0.0019,
      "step": 118050
    },
    {
      "epoch": 3.9353333333333333,
      "grad_norm": 0.5520675182342529,
      "learning_rate": 2.5404166666666666e-05,
      "loss": 0.0018,
      "step": 118060
    },
    {
      "epoch": 3.9356666666666666,
      "grad_norm": 0.11545200645923615,
      "learning_rate": 2.5402083333333332e-05,
      "loss": 0.0029,
      "step": 118070
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.17138490080833435,
      "learning_rate": 2.54e-05,
      "loss": 0.0022,
      "step": 118080
    },
    {
      "epoch": 3.9363333333333332,
      "grad_norm": 0.6596298813819885,
      "learning_rate": 2.5397916666666666e-05,
      "loss": 0.0017,
      "step": 118090
    },
    {
      "epoch": 3.9366666666666665,
      "grad_norm": 0.4286850392818451,
      "learning_rate": 2.5395833333333335e-05,
      "loss": 0.0027,
      "step": 118100
    },
    {
      "epoch": 3.9370000000000003,
      "grad_norm": 0.20016218721866608,
      "learning_rate": 2.539375e-05,
      "loss": 0.0018,
      "step": 118110
    },
    {
      "epoch": 3.937333333333333,
      "grad_norm": 0.19990581274032593,
      "learning_rate": 2.539166666666667e-05,
      "loss": 0.0022,
      "step": 118120
    },
    {
      "epoch": 3.937666666666667,
      "grad_norm": 0.08619802445173264,
      "learning_rate": 2.5389583333333335e-05,
      "loss": 0.0026,
      "step": 118130
    },
    {
      "epoch": 3.9379999999999997,
      "grad_norm": 0.008683612570166588,
      "learning_rate": 2.5387500000000004e-05,
      "loss": 0.0027,
      "step": 118140
    },
    {
      "epoch": 3.9383333333333335,
      "grad_norm": 0.22871218621730804,
      "learning_rate": 2.5385416666666666e-05,
      "loss": 0.0025,
      "step": 118150
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.08583641797304153,
      "learning_rate": 2.5383333333333338e-05,
      "loss": 0.0023,
      "step": 118160
    },
    {
      "epoch": 3.939,
      "grad_norm": 0.200204536318779,
      "learning_rate": 2.538125e-05,
      "loss": 0.0029,
      "step": 118170
    },
    {
      "epoch": 3.9393333333333334,
      "grad_norm": 0.34275588393211365,
      "learning_rate": 2.537916666666667e-05,
      "loss": 0.0026,
      "step": 118180
    },
    {
      "epoch": 3.9396666666666667,
      "grad_norm": 0.0036200955510139465,
      "learning_rate": 2.5377083333333335e-05,
      "loss": 0.0018,
      "step": 118190
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.01586940325796604,
      "learning_rate": 2.5375e-05,
      "loss": 0.0029,
      "step": 118200
    },
    {
      "epoch": 3.9403333333333332,
      "grad_norm": 0.2855199873447418,
      "learning_rate": 2.537291666666667e-05,
      "loss": 0.0021,
      "step": 118210
    },
    {
      "epoch": 3.9406666666666665,
      "grad_norm": 0.2571503818035126,
      "learning_rate": 2.537083333333333e-05,
      "loss": 0.0016,
      "step": 118220
    },
    {
      "epoch": 3.941,
      "grad_norm": 0.02880178950726986,
      "learning_rate": 2.5368750000000003e-05,
      "loss": 0.003,
      "step": 118230
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.2120383232831955,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0019,
      "step": 118240
    },
    {
      "epoch": 3.9416666666666664,
      "grad_norm": 0.4855349361896515,
      "learning_rate": 2.5364583333333334e-05,
      "loss": 0.002,
      "step": 118250
    },
    {
      "epoch": 3.942,
      "grad_norm": 0.08611143380403519,
      "learning_rate": 2.53625e-05,
      "loss": 0.0021,
      "step": 118260
    },
    {
      "epoch": 3.9423333333333335,
      "grad_norm": 0.14295653998851776,
      "learning_rate": 2.536041666666667e-05,
      "loss": 0.0017,
      "step": 118270
    },
    {
      "epoch": 3.9426666666666668,
      "grad_norm": 0.11462733894586563,
      "learning_rate": 2.5358333333333334e-05,
      "loss": 0.0017,
      "step": 118280
    },
    {
      "epoch": 3.943,
      "grad_norm": 0.1671803593635559,
      "learning_rate": 2.5356250000000003e-05,
      "loss": 0.0033,
      "step": 118290
    },
    {
      "epoch": 3.9433333333333334,
      "grad_norm": 0.06765541434288025,
      "learning_rate": 2.535416666666667e-05,
      "loss": 0.002,
      "step": 118300
    },
    {
      "epoch": 3.9436666666666667,
      "grad_norm": 0.057692985981702805,
      "learning_rate": 2.535208333333333e-05,
      "loss": 0.0019,
      "step": 118310
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.22870714962482452,
      "learning_rate": 2.5350000000000003e-05,
      "loss": 0.0018,
      "step": 118320
    },
    {
      "epoch": 3.9443333333333332,
      "grad_norm": 0.05886387825012207,
      "learning_rate": 2.5347916666666665e-05,
      "loss": 0.0026,
      "step": 118330
    },
    {
      "epoch": 3.9446666666666665,
      "grad_norm": 0.17145447432994843,
      "learning_rate": 2.5345833333333334e-05,
      "loss": 0.0014,
      "step": 118340
    },
    {
      "epoch": 3.945,
      "grad_norm": 0.05863649770617485,
      "learning_rate": 2.534375e-05,
      "loss": 0.0028,
      "step": 118350
    },
    {
      "epoch": 3.945333333333333,
      "grad_norm": 0.14319677650928497,
      "learning_rate": 2.5341666666666668e-05,
      "loss": 0.0028,
      "step": 118360
    },
    {
      "epoch": 3.945666666666667,
      "grad_norm": 0.31437361240386963,
      "learning_rate": 2.5339583333333334e-05,
      "loss": 0.0025,
      "step": 118370
    },
    {
      "epoch": 3.9459999999999997,
      "grad_norm": 0.457086980342865,
      "learning_rate": 2.5337500000000003e-05,
      "loss": 0.0034,
      "step": 118380
    },
    {
      "epoch": 3.9463333333333335,
      "grad_norm": 0.3304649293422699,
      "learning_rate": 2.5335416666666668e-05,
      "loss": 0.0024,
      "step": 118390
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.3142163157463074,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0018,
      "step": 118400
    },
    {
      "epoch": 3.947,
      "grad_norm": 0.057732995599508286,
      "learning_rate": 2.5331250000000002e-05,
      "loss": 0.0021,
      "step": 118410
    },
    {
      "epoch": 3.9473333333333334,
      "grad_norm": 0.17159169912338257,
      "learning_rate": 2.532916666666667e-05,
      "loss": 0.0027,
      "step": 118420
    },
    {
      "epoch": 3.9476666666666667,
      "grad_norm": 0.0326615534722805,
      "learning_rate": 2.5327083333333333e-05,
      "loss": 0.0028,
      "step": 118430
    },
    {
      "epoch": 3.948,
      "grad_norm": 0.31420984864234924,
      "learning_rate": 2.5325e-05,
      "loss": 0.0022,
      "step": 118440
    },
    {
      "epoch": 3.9483333333333333,
      "grad_norm": 0.031081004068255424,
      "learning_rate": 2.5322916666666668e-05,
      "loss": 0.0026,
      "step": 118450
    },
    {
      "epoch": 3.9486666666666665,
      "grad_norm": 0.3430647850036621,
      "learning_rate": 2.5320833333333333e-05,
      "loss": 0.0023,
      "step": 118460
    },
    {
      "epoch": 3.949,
      "grad_norm": 0.029580894857645035,
      "learning_rate": 2.5318750000000002e-05,
      "loss": 0.0023,
      "step": 118470
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.22876524925231934,
      "learning_rate": 2.5316666666666668e-05,
      "loss": 0.0017,
      "step": 118480
    },
    {
      "epoch": 3.9496666666666664,
      "grad_norm": 0.14291447401046753,
      "learning_rate": 2.5314583333333337e-05,
      "loss": 0.0018,
      "step": 118490
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.1429099589586258,
      "learning_rate": 2.53125e-05,
      "loss": 0.002,
      "step": 118500
    },
    {
      "epoch": 3.9503333333333335,
      "grad_norm": 0.17378906905651093,
      "learning_rate": 2.531041666666667e-05,
      "loss": 0.0018,
      "step": 118510
    },
    {
      "epoch": 3.9506666666666668,
      "grad_norm": 0.2582351863384247,
      "learning_rate": 2.5308333333333333e-05,
      "loss": 0.0023,
      "step": 118520
    },
    {
      "epoch": 3.951,
      "grad_norm": 0.17216815054416656,
      "learning_rate": 2.5306250000000005e-05,
      "loss": 0.0031,
      "step": 118530
    },
    {
      "epoch": 3.9513333333333334,
      "grad_norm": 0.057404886931180954,
      "learning_rate": 2.5304166666666667e-05,
      "loss": 0.0025,
      "step": 118540
    },
    {
      "epoch": 3.9516666666666667,
      "grad_norm": 0.08608292043209076,
      "learning_rate": 2.5302083333333333e-05,
      "loss": 0.0026,
      "step": 118550
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.20054414868354797,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.0026,
      "step": 118560
    },
    {
      "epoch": 3.9523333333333333,
      "grad_norm": 0.11487861722707748,
      "learning_rate": 2.5297916666666667e-05,
      "loss": 0.0025,
      "step": 118570
    },
    {
      "epoch": 3.9526666666666666,
      "grad_norm": 0.057253751903772354,
      "learning_rate": 2.5295833333333336e-05,
      "loss": 0.0019,
      "step": 118580
    },
    {
      "epoch": 3.953,
      "grad_norm": 0.058558300137519836,
      "learning_rate": 2.5293749999999998e-05,
      "loss": 0.0016,
      "step": 118590
    },
    {
      "epoch": 3.953333333333333,
      "grad_norm": 0.25710105895996094,
      "learning_rate": 2.529166666666667e-05,
      "loss": 0.0018,
      "step": 118600
    },
    {
      "epoch": 3.953666666666667,
      "grad_norm": 0.09759195894002914,
      "learning_rate": 2.5289583333333333e-05,
      "loss": 0.0019,
      "step": 118610
    },
    {
      "epoch": 3.9539999999999997,
      "grad_norm": 0.19984111189842224,
      "learning_rate": 2.52875e-05,
      "loss": 0.0019,
      "step": 118620
    },
    {
      "epoch": 3.9543333333333335,
      "grad_norm": 0.11400657147169113,
      "learning_rate": 2.5285416666666667e-05,
      "loss": 0.0022,
      "step": 118630
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.08599076420068741,
      "learning_rate": 2.5283333333333336e-05,
      "loss": 0.0022,
      "step": 118640
    },
    {
      "epoch": 3.955,
      "grad_norm": 0.28600725531578064,
      "learning_rate": 2.528125e-05,
      "loss": 0.0018,
      "step": 118650
    },
    {
      "epoch": 3.9553333333333334,
      "grad_norm": 0.3997853696346283,
      "learning_rate": 2.527916666666667e-05,
      "loss": 0.002,
      "step": 118660
    },
    {
      "epoch": 3.9556666666666667,
      "grad_norm": 0.11426012963056564,
      "learning_rate": 2.5277083333333336e-05,
      "loss": 0.0017,
      "step": 118670
    },
    {
      "epoch": 3.956,
      "grad_norm": 0.28565430641174316,
      "learning_rate": 2.5274999999999998e-05,
      "loss": 0.0019,
      "step": 118680
    },
    {
      "epoch": 3.9563333333333333,
      "grad_norm": 0.14346900582313538,
      "learning_rate": 2.527291666666667e-05,
      "loss": 0.0032,
      "step": 118690
    },
    {
      "epoch": 3.9566666666666666,
      "grad_norm": 0.058741264045238495,
      "learning_rate": 2.5270833333333332e-05,
      "loss": 0.0021,
      "step": 118700
    },
    {
      "epoch": 3.957,
      "grad_norm": 0.015524236485362053,
      "learning_rate": 2.526875e-05,
      "loss": 0.0017,
      "step": 118710
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.19983035326004028,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0034,
      "step": 118720
    },
    {
      "epoch": 3.9576666666666664,
      "grad_norm": 0.05944814905524254,
      "learning_rate": 2.5264583333333335e-05,
      "loss": 0.002,
      "step": 118730
    },
    {
      "epoch": 3.958,
      "grad_norm": 0.17159700393676758,
      "learning_rate": 2.52625e-05,
      "loss": 0.0023,
      "step": 118740
    },
    {
      "epoch": 3.9583333333333335,
      "grad_norm": 0.036889735609292984,
      "learning_rate": 2.526041666666667e-05,
      "loss": 0.0019,
      "step": 118750
    },
    {
      "epoch": 3.958666666666667,
      "grad_norm": 0.17161297798156738,
      "learning_rate": 2.5258333333333335e-05,
      "loss": 0.0022,
      "step": 118760
    },
    {
      "epoch": 3.959,
      "grad_norm": 0.1144532561302185,
      "learning_rate": 2.5256250000000004e-05,
      "loss": 0.0028,
      "step": 118770
    },
    {
      "epoch": 3.9593333333333334,
      "grad_norm": 0.3712111711502075,
      "learning_rate": 2.5254166666666666e-05,
      "loss": 0.0024,
      "step": 118780
    },
    {
      "epoch": 3.9596666666666667,
      "grad_norm": 0.22855232656002045,
      "learning_rate": 2.525208333333334e-05,
      "loss": 0.0016,
      "step": 118790
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.006437128409743309,
      "learning_rate": 2.525e-05,
      "loss": 0.0016,
      "step": 118800
    },
    {
      "epoch": 3.9603333333333333,
      "grad_norm": 0.14335997402668,
      "learning_rate": 2.5247916666666666e-05,
      "loss": 0.0014,
      "step": 118810
    },
    {
      "epoch": 3.9606666666666666,
      "grad_norm": 0.3999340832233429,
      "learning_rate": 2.5245833333333335e-05,
      "loss": 0.0016,
      "step": 118820
    },
    {
      "epoch": 3.961,
      "grad_norm": 0.25761905312538147,
      "learning_rate": 2.524375e-05,
      "loss": 0.002,
      "step": 118830
    },
    {
      "epoch": 3.961333333333333,
      "grad_norm": 0.6458293795585632,
      "learning_rate": 2.524166666666667e-05,
      "loss": 0.0023,
      "step": 118840
    },
    {
      "epoch": 3.961666666666667,
      "grad_norm": 0.14303690195083618,
      "learning_rate": 2.5239583333333335e-05,
      "loss": 0.002,
      "step": 118850
    },
    {
      "epoch": 3.9619999999999997,
      "grad_norm": 0.05732547491788864,
      "learning_rate": 2.5237500000000004e-05,
      "loss": 0.0023,
      "step": 118860
    },
    {
      "epoch": 3.9623333333333335,
      "grad_norm": 0.0589749701321125,
      "learning_rate": 2.5235416666666666e-05,
      "loss": 0.0024,
      "step": 118870
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.45713356137275696,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0016,
      "step": 118880
    },
    {
      "epoch": 3.963,
      "grad_norm": 0.5109884738922119,
      "learning_rate": 2.523125e-05,
      "loss": 0.0023,
      "step": 118890
    },
    {
      "epoch": 3.9633333333333334,
      "grad_norm": 0.2571507692337036,
      "learning_rate": 2.522916666666667e-05,
      "loss": 0.0021,
      "step": 118900
    },
    {
      "epoch": 3.9636666666666667,
      "grad_norm": 0.03275492042303085,
      "learning_rate": 2.5227083333333334e-05,
      "loss": 0.0029,
      "step": 118910
    },
    {
      "epoch": 3.964,
      "grad_norm": 0.11503253132104874,
      "learning_rate": 2.5225e-05,
      "loss": 0.0022,
      "step": 118920
    },
    {
      "epoch": 3.9643333333333333,
      "grad_norm": 0.004423745907843113,
      "learning_rate": 2.522291666666667e-05,
      "loss": 0.002,
      "step": 118930
    },
    {
      "epoch": 3.9646666666666666,
      "grad_norm": 0.22887541353702545,
      "learning_rate": 2.522083333333333e-05,
      "loss": 0.0016,
      "step": 118940
    },
    {
      "epoch": 3.965,
      "grad_norm": 0.2855451703071594,
      "learning_rate": 2.5218750000000003e-05,
      "loss": 0.0018,
      "step": 118950
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.11425919085741043,
      "learning_rate": 2.5216666666666665e-05,
      "loss": 0.0023,
      "step": 118960
    },
    {
      "epoch": 3.9656666666666665,
      "grad_norm": 0.057108864188194275,
      "learning_rate": 2.5214583333333337e-05,
      "loss": 0.0023,
      "step": 118970
    },
    {
      "epoch": 3.966,
      "grad_norm": 0.5515263676643372,
      "learning_rate": 2.52125e-05,
      "loss": 0.0026,
      "step": 118980
    },
    {
      "epoch": 3.9663333333333335,
      "grad_norm": 0.11465778946876526,
      "learning_rate": 2.521041666666667e-05,
      "loss": 0.0014,
      "step": 118990
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.22826382517814636,
      "learning_rate": 2.5208333333333334e-05,
      "loss": 0.0021,
      "step": 119000
    },
    {
      "epoch": 3.967,
      "grad_norm": 0.31472209095954895,
      "learning_rate": 2.5206250000000003e-05,
      "loss": 0.0022,
      "step": 119010
    },
    {
      "epoch": 3.9673333333333334,
      "grad_norm": 0.260573148727417,
      "learning_rate": 2.520416666666667e-05,
      "loss": 0.0019,
      "step": 119020
    },
    {
      "epoch": 3.9676666666666667,
      "grad_norm": 0.08594518899917603,
      "learning_rate": 2.5202083333333337e-05,
      "loss": 0.0015,
      "step": 119030
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.19991327822208405,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0023,
      "step": 119040
    },
    {
      "epoch": 3.9683333333333333,
      "grad_norm": 0.02946869097650051,
      "learning_rate": 2.5197916666666665e-05,
      "loss": 0.0017,
      "step": 119050
    },
    {
      "epoch": 3.9686666666666666,
      "grad_norm": 0.007364440709352493,
      "learning_rate": 2.5195833333333334e-05,
      "loss": 0.0021,
      "step": 119060
    },
    {
      "epoch": 3.969,
      "grad_norm": 0.25723201036453247,
      "learning_rate": 2.519375e-05,
      "loss": 0.0029,
      "step": 119070
    },
    {
      "epoch": 3.969333333333333,
      "grad_norm": 0.31437650322914124,
      "learning_rate": 2.5191666666666668e-05,
      "loss": 0.0016,
      "step": 119080
    },
    {
      "epoch": 3.969666666666667,
      "grad_norm": 0.01680411957204342,
      "learning_rate": 2.5189583333333333e-05,
      "loss": 0.002,
      "step": 119090
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 0.05863364413380623,
      "learning_rate": 2.5187500000000002e-05,
      "loss": 0.0016,
      "step": 119100
    },
    {
      "epoch": 3.9703333333333335,
      "grad_norm": 0.11579925566911697,
      "learning_rate": 2.5185416666666668e-05,
      "loss": 0.0025,
      "step": 119110
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.057391826063394547,
      "learning_rate": 2.5183333333333337e-05,
      "loss": 0.0022,
      "step": 119120
    },
    {
      "epoch": 3.971,
      "grad_norm": 0.19980593025684357,
      "learning_rate": 2.5181250000000002e-05,
      "loss": 0.0013,
      "step": 119130
    },
    {
      "epoch": 3.9713333333333334,
      "grad_norm": 0.029568837955594063,
      "learning_rate": 2.517916666666667e-05,
      "loss": 0.0022,
      "step": 119140
    },
    {
      "epoch": 3.9716666666666667,
      "grad_norm": 0.399911493062973,
      "learning_rate": 2.5177083333333333e-05,
      "loss": 0.0021,
      "step": 119150
    },
    {
      "epoch": 3.972,
      "grad_norm": 0.7579723000526428,
      "learning_rate": 2.5175e-05,
      "loss": 0.0022,
      "step": 119160
    },
    {
      "epoch": 3.9723333333333333,
      "grad_norm": 0.34266918897628784,
      "learning_rate": 2.5172916666666668e-05,
      "loss": 0.002,
      "step": 119170
    },
    {
      "epoch": 3.9726666666666666,
      "grad_norm": 0.47690996527671814,
      "learning_rate": 2.5170833333333333e-05,
      "loss": 0.0013,
      "step": 119180
    },
    {
      "epoch": 3.973,
      "grad_norm": 0.20018091797828674,
      "learning_rate": 2.5168750000000002e-05,
      "loss": 0.0018,
      "step": 119190
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.45692795515060425,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0024,
      "step": 119200
    },
    {
      "epoch": 3.9736666666666665,
      "grad_norm": 0.17140686511993408,
      "learning_rate": 2.5164583333333336e-05,
      "loss": 0.0017,
      "step": 119210
    },
    {
      "epoch": 3.974,
      "grad_norm": 0.14313079416751862,
      "learning_rate": 2.51625e-05,
      "loss": 0.003,
      "step": 119220
    },
    {
      "epoch": 3.9743333333333335,
      "grad_norm": 0.34300386905670166,
      "learning_rate": 2.516041666666667e-05,
      "loss": 0.0028,
      "step": 119230
    },
    {
      "epoch": 3.974666666666667,
      "grad_norm": 0.11433364450931549,
      "learning_rate": 2.5158333333333333e-05,
      "loss": 0.0026,
      "step": 119240
    },
    {
      "epoch": 3.975,
      "grad_norm": 0.31462347507476807,
      "learning_rate": 2.5156250000000005e-05,
      "loss": 0.0018,
      "step": 119250
    },
    {
      "epoch": 3.9753333333333334,
      "grad_norm": 0.14374254643917084,
      "learning_rate": 2.5154166666666667e-05,
      "loss": 0.0029,
      "step": 119260
    },
    {
      "epoch": 3.9756666666666667,
      "grad_norm": 0.11435777693986893,
      "learning_rate": 2.5152083333333336e-05,
      "loss": 0.0025,
      "step": 119270
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.5100142955780029,
      "learning_rate": 2.515e-05,
      "loss": 0.0024,
      "step": 119280
    },
    {
      "epoch": 3.9763333333333333,
      "grad_norm": 0.02915332280099392,
      "learning_rate": 2.5147916666666667e-05,
      "loss": 0.0025,
      "step": 119290
    },
    {
      "epoch": 3.9766666666666666,
      "grad_norm": 0.029416929930448532,
      "learning_rate": 2.5145833333333336e-05,
      "loss": 0.0017,
      "step": 119300
    },
    {
      "epoch": 3.977,
      "grad_norm": 0.029224233701825142,
      "learning_rate": 2.5143749999999998e-05,
      "loss": 0.0026,
      "step": 119310
    },
    {
      "epoch": 3.977333333333333,
      "grad_norm": 0.028696861118078232,
      "learning_rate": 2.514166666666667e-05,
      "loss": 0.0031,
      "step": 119320
    },
    {
      "epoch": 3.977666666666667,
      "grad_norm": 0.1146995946764946,
      "learning_rate": 2.5139583333333332e-05,
      "loss": 0.0024,
      "step": 119330
    },
    {
      "epoch": 3.9779999999999998,
      "grad_norm": 0.8047499656677246,
      "learning_rate": 2.51375e-05,
      "loss": 0.0036,
      "step": 119340
    },
    {
      "epoch": 3.9783333333333335,
      "grad_norm": 0.0744192898273468,
      "learning_rate": 2.5135416666666667e-05,
      "loss": 0.002,
      "step": 119350
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.08721266686916351,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0029,
      "step": 119360
    },
    {
      "epoch": 3.979,
      "grad_norm": 0.005799269303679466,
      "learning_rate": 2.513125e-05,
      "loss": 0.0022,
      "step": 119370
    },
    {
      "epoch": 3.9793333333333334,
      "grad_norm": 0.0034796742256730795,
      "learning_rate": 2.512916666666667e-05,
      "loss": 0.0025,
      "step": 119380
    },
    {
      "epoch": 3.9796666666666667,
      "grad_norm": 0.45741546154022217,
      "learning_rate": 2.5127083333333335e-05,
      "loss": 0.0034,
      "step": 119390
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.004623137880116701,
      "learning_rate": 2.5124999999999997e-05,
      "loss": 0.0021,
      "step": 119400
    },
    {
      "epoch": 3.9803333333333333,
      "grad_norm": 0.08585108816623688,
      "learning_rate": 2.512291666666667e-05,
      "loss": 0.0017,
      "step": 119410
    },
    {
      "epoch": 3.9806666666666666,
      "grad_norm": 0.14282166957855225,
      "learning_rate": 2.5120833333333332e-05,
      "loss": 0.002,
      "step": 119420
    },
    {
      "epoch": 3.981,
      "grad_norm": 0.028659669682383537,
      "learning_rate": 2.511875e-05,
      "loss": 0.0024,
      "step": 119430
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.34262487292289734,
      "learning_rate": 2.5116666666666666e-05,
      "loss": 0.0023,
      "step": 119440
    },
    {
      "epoch": 3.9816666666666665,
      "grad_norm": 0.08569782227277756,
      "learning_rate": 2.5114583333333335e-05,
      "loss": 0.0024,
      "step": 119450
    },
    {
      "epoch": 3.982,
      "grad_norm": 0.05774135887622833,
      "learning_rate": 2.51125e-05,
      "loss": 0.0026,
      "step": 119460
    },
    {
      "epoch": 3.982333333333333,
      "grad_norm": 0.25769808888435364,
      "learning_rate": 2.511041666666667e-05,
      "loss": 0.0017,
      "step": 119470
    },
    {
      "epoch": 3.982666666666667,
      "grad_norm": 0.007187863811850548,
      "learning_rate": 2.5108333333333335e-05,
      "loss": 0.0015,
      "step": 119480
    },
    {
      "epoch": 3.983,
      "grad_norm": 0.2857910394668579,
      "learning_rate": 2.5106250000000004e-05,
      "loss": 0.002,
      "step": 119490
    },
    {
      "epoch": 3.9833333333333334,
      "grad_norm": 0.12346328794956207,
      "learning_rate": 2.5104166666666666e-05,
      "loss": 0.0024,
      "step": 119500
    },
    {
      "epoch": 3.9836666666666667,
      "grad_norm": 0.35992276668548584,
      "learning_rate": 2.5102083333333338e-05,
      "loss": 0.0024,
      "step": 119510
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.1720012128353119,
      "learning_rate": 2.51e-05,
      "loss": 0.0021,
      "step": 119520
    },
    {
      "epoch": 3.9843333333333333,
      "grad_norm": 0.19318018853664398,
      "learning_rate": 2.5097916666666666e-05,
      "loss": 0.0017,
      "step": 119530
    },
    {
      "epoch": 3.9846666666666666,
      "grad_norm": 0.24462570250034332,
      "learning_rate": 2.5095833333333335e-05,
      "loss": 0.0023,
      "step": 119540
    },
    {
      "epoch": 3.985,
      "grad_norm": 0.8415114879608154,
      "learning_rate": 2.509375e-05,
      "loss": 0.0022,
      "step": 119550
    },
    {
      "epoch": 3.985333333333333,
      "grad_norm": 0.2572915554046631,
      "learning_rate": 2.509166666666667e-05,
      "loss": 0.0026,
      "step": 119560
    },
    {
      "epoch": 3.985666666666667,
      "grad_norm": 0.11616046726703644,
      "learning_rate": 2.5089583333333334e-05,
      "loss": 0.0011,
      "step": 119570
    },
    {
      "epoch": 3.9859999999999998,
      "grad_norm": 0.03373218700289726,
      "learning_rate": 2.5087500000000003e-05,
      "loss": 0.0036,
      "step": 119580
    },
    {
      "epoch": 3.9863333333333335,
      "grad_norm": 0.20059722661972046,
      "learning_rate": 2.5085416666666665e-05,
      "loss": 0.0027,
      "step": 119590
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.17138808965682983,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.0019,
      "step": 119600
    },
    {
      "epoch": 3.987,
      "grad_norm": 0.11420798301696777,
      "learning_rate": 2.508125e-05,
      "loss": 0.0018,
      "step": 119610
    },
    {
      "epoch": 3.9873333333333334,
      "grad_norm": 0.05978855863213539,
      "learning_rate": 2.507916666666667e-05,
      "loss": 0.0021,
      "step": 119620
    },
    {
      "epoch": 3.9876666666666667,
      "grad_norm": 0.3144470751285553,
      "learning_rate": 2.5077083333333334e-05,
      "loss": 0.0022,
      "step": 119630
    },
    {
      "epoch": 3.988,
      "grad_norm": 0.08743620663881302,
      "learning_rate": 2.5075e-05,
      "loss": 0.0023,
      "step": 119640
    },
    {
      "epoch": 3.9883333333333333,
      "grad_norm": 0.17122113704681396,
      "learning_rate": 2.507291666666667e-05,
      "loss": 0.0023,
      "step": 119650
    },
    {
      "epoch": 3.9886666666666666,
      "grad_norm": 0.2570546567440033,
      "learning_rate": 2.507083333333333e-05,
      "loss": 0.002,
      "step": 119660
    },
    {
      "epoch": 3.989,
      "grad_norm": 0.3429203927516937,
      "learning_rate": 2.5068750000000003e-05,
      "loss": 0.0022,
      "step": 119670
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.20066331326961517,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.002,
      "step": 119680
    },
    {
      "epoch": 3.9896666666666665,
      "grad_norm": 0.251311719417572,
      "learning_rate": 2.5064583333333337e-05,
      "loss": 0.0025,
      "step": 119690
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.029290886595845222,
      "learning_rate": 2.50625e-05,
      "loss": 0.0024,
      "step": 119700
    },
    {
      "epoch": 3.990333333333333,
      "grad_norm": 0.11464077234268188,
      "learning_rate": 2.5060416666666668e-05,
      "loss": 0.0036,
      "step": 119710
    },
    {
      "epoch": 3.990666666666667,
      "grad_norm": 0.11568747460842133,
      "learning_rate": 2.5058333333333334e-05,
      "loss": 0.0021,
      "step": 119720
    },
    {
      "epoch": 3.991,
      "grad_norm": 0.49934592843055725,
      "learning_rate": 2.5056250000000003e-05,
      "loss": 0.0014,
      "step": 119730
    },
    {
      "epoch": 3.9913333333333334,
      "grad_norm": 0.08584866672754288,
      "learning_rate": 2.5054166666666668e-05,
      "loss": 0.0028,
      "step": 119740
    },
    {
      "epoch": 3.9916666666666667,
      "grad_norm": 0.3140721917152405,
      "learning_rate": 2.5052083333333337e-05,
      "loss": 0.003,
      "step": 119750
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.1712988018989563,
      "learning_rate": 2.5050000000000002e-05,
      "loss": 0.0026,
      "step": 119760
    },
    {
      "epoch": 3.9923333333333333,
      "grad_norm": 0.004762730561196804,
      "learning_rate": 2.5047916666666665e-05,
      "loss": 0.0017,
      "step": 119770
    },
    {
      "epoch": 3.9926666666666666,
      "grad_norm": 0.004496393259614706,
      "learning_rate": 2.5045833333333333e-05,
      "loss": 0.0021,
      "step": 119780
    },
    {
      "epoch": 3.993,
      "grad_norm": 0.5711564421653748,
      "learning_rate": 2.504375e-05,
      "loss": 0.0022,
      "step": 119790
    },
    {
      "epoch": 3.993333333333333,
      "grad_norm": 0.1714148372411728,
      "learning_rate": 2.5041666666666668e-05,
      "loss": 0.0016,
      "step": 119800
    },
    {
      "epoch": 3.993666666666667,
      "grad_norm": 0.22892382740974426,
      "learning_rate": 2.5039583333333333e-05,
      "loss": 0.0018,
      "step": 119810
    },
    {
      "epoch": 3.9939999999999998,
      "grad_norm": 0.5995661020278931,
      "learning_rate": 2.5037500000000002e-05,
      "loss": 0.0021,
      "step": 119820
    },
    {
      "epoch": 3.9943333333333335,
      "grad_norm": 0.5996853709220886,
      "learning_rate": 2.5035416666666668e-05,
      "loss": 0.0023,
      "step": 119830
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.3983790874481201,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.0016,
      "step": 119840
    },
    {
      "epoch": 3.995,
      "grad_norm": 0.0893765464425087,
      "learning_rate": 2.5031250000000002e-05,
      "loss": 0.0021,
      "step": 119850
    },
    {
      "epoch": 3.9953333333333334,
      "grad_norm": 0.25888752937316895,
      "learning_rate": 2.502916666666667e-05,
      "loss": 0.0029,
      "step": 119860
    },
    {
      "epoch": 3.9956666666666667,
      "grad_norm": 0.08757540583610535,
      "learning_rate": 2.5027083333333333e-05,
      "loss": 0.0026,
      "step": 119870
    },
    {
      "epoch": 3.996,
      "grad_norm": 0.19995112717151642,
      "learning_rate": 2.5025e-05,
      "loss": 0.0018,
      "step": 119880
    },
    {
      "epoch": 3.9963333333333333,
      "grad_norm": 0.3712391257286072,
      "learning_rate": 2.5022916666666667e-05,
      "loss": 0.0014,
      "step": 119890
    },
    {
      "epoch": 3.9966666666666666,
      "grad_norm": 0.05768387019634247,
      "learning_rate": 2.5020833333333333e-05,
      "loss": 0.0018,
      "step": 119900
    },
    {
      "epoch": 3.997,
      "grad_norm": 0.03531811758875847,
      "learning_rate": 2.501875e-05,
      "loss": 0.0027,
      "step": 119910
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.004263525363057852,
      "learning_rate": 2.5016666666666667e-05,
      "loss": 0.0023,
      "step": 119920
    },
    {
      "epoch": 3.9976666666666665,
      "grad_norm": 0.11111374199390411,
      "learning_rate": 2.5014583333333336e-05,
      "loss": 0.0013,
      "step": 119930
    },
    {
      "epoch": 3.998,
      "grad_norm": 0.2009923756122589,
      "learning_rate": 2.50125e-05,
      "loss": 0.0015,
      "step": 119940
    },
    {
      "epoch": 3.998333333333333,
      "grad_norm": 0.08618167787790298,
      "learning_rate": 2.501041666666667e-05,
      "loss": 0.002,
      "step": 119950
    },
    {
      "epoch": 3.998666666666667,
      "grad_norm": 0.22900785505771637,
      "learning_rate": 2.5008333333333332e-05,
      "loss": 0.002,
      "step": 119960
    },
    {
      "epoch": 3.999,
      "grad_norm": 0.05844660848379135,
      "learning_rate": 2.5006250000000005e-05,
      "loss": 0.0024,
      "step": 119970
    },
    {
      "epoch": 3.9993333333333334,
      "grad_norm": 0.17172294855117798,
      "learning_rate": 2.5004166666666667e-05,
      "loss": 0.0017,
      "step": 119980
    },
    {
      "epoch": 3.9996666666666667,
      "grad_norm": 0.28649795055389404,
      "learning_rate": 2.5002083333333336e-05,
      "loss": 0.0028,
      "step": 119990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.0575038343667984,
      "learning_rate": 2.5e-05,
      "loss": 0.0032,
      "step": 120000
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.0021700120996683836,
      "eval_runtime": 131.7685,
      "eval_samples_per_second": 1517.814,
      "eval_steps_per_second": 37.945,
      "step": 120000
    },
    {
      "epoch": 4.000333333333334,
      "grad_norm": 0.14311973750591278,
      "learning_rate": 2.4997916666666667e-05,
      "loss": 0.0014,
      "step": 120010
    },
    {
      "epoch": 4.000666666666667,
      "grad_norm": 0.7194696068763733,
      "learning_rate": 2.4995833333333336e-05,
      "loss": 0.0017,
      "step": 120020
    },
    {
      "epoch": 4.001,
      "grad_norm": 0.6768826246261597,
      "learning_rate": 2.499375e-05,
      "loss": 0.0028,
      "step": 120030
    },
    {
      "epoch": 4.001333333333333,
      "grad_norm": 0.006506660487502813,
      "learning_rate": 2.499166666666667e-05,
      "loss": 0.0019,
      "step": 120040
    },
    {
      "epoch": 4.001666666666667,
      "grad_norm": 0.22844423353672028,
      "learning_rate": 2.4989583333333335e-05,
      "loss": 0.0017,
      "step": 120050
    },
    {
      "epoch": 4.002,
      "grad_norm": 0.059157826006412506,
      "learning_rate": 2.49875e-05,
      "loss": 0.0017,
      "step": 120060
    },
    {
      "epoch": 4.0023333333333335,
      "grad_norm": 0.19999556243419647,
      "learning_rate": 2.4985416666666666e-05,
      "loss": 0.0022,
      "step": 120070
    },
    {
      "epoch": 4.002666666666666,
      "grad_norm": 0.14273694157600403,
      "learning_rate": 2.4983333333333335e-05,
      "loss": 0.0025,
      "step": 120080
    },
    {
      "epoch": 4.003,
      "grad_norm": 0.1433762013912201,
      "learning_rate": 2.498125e-05,
      "loss": 0.0023,
      "step": 120090
    },
    {
      "epoch": 4.003333333333333,
      "grad_norm": 0.05749765783548355,
      "learning_rate": 2.4979166666666666e-05,
      "loss": 0.0033,
      "step": 120100
    },
    {
      "epoch": 4.003666666666667,
      "grad_norm": 0.029227981343865395,
      "learning_rate": 2.4977083333333335e-05,
      "loss": 0.0018,
      "step": 120110
    },
    {
      "epoch": 4.004,
      "grad_norm": 0.006986635737121105,
      "learning_rate": 2.4975e-05,
      "loss": 0.0018,
      "step": 120120
    },
    {
      "epoch": 4.004333333333333,
      "grad_norm": 0.3589118719100952,
      "learning_rate": 2.497291666666667e-05,
      "loss": 0.002,
      "step": 120130
    },
    {
      "epoch": 4.004666666666667,
      "grad_norm": 0.11558894068002701,
      "learning_rate": 2.4970833333333335e-05,
      "loss": 0.0028,
      "step": 120140
    },
    {
      "epoch": 4.005,
      "grad_norm": 0.28622332215309143,
      "learning_rate": 2.496875e-05,
      "loss": 0.0027,
      "step": 120150
    },
    {
      "epoch": 4.005333333333334,
      "grad_norm": 0.11451423913240433,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0021,
      "step": 120160
    },
    {
      "epoch": 4.0056666666666665,
      "grad_norm": 0.029562795534729958,
      "learning_rate": 2.4964583333333335e-05,
      "loss": 0.0018,
      "step": 120170
    },
    {
      "epoch": 4.006,
      "grad_norm": 0.05767911300063133,
      "learning_rate": 2.4962500000000004e-05,
      "loss": 0.0035,
      "step": 120180
    },
    {
      "epoch": 4.006333333333333,
      "grad_norm": 0.2572314441204071,
      "learning_rate": 2.4960416666666666e-05,
      "loss": 0.0033,
      "step": 120190
    },
    {
      "epoch": 4.006666666666667,
      "grad_norm": 0.12134525179862976,
      "learning_rate": 2.4958333333333335e-05,
      "loss": 0.0028,
      "step": 120200
    },
    {
      "epoch": 4.007,
      "grad_norm": 0.08569809049367905,
      "learning_rate": 2.495625e-05,
      "loss": 0.0026,
      "step": 120210
    },
    {
      "epoch": 4.007333333333333,
      "grad_norm": 0.14272212982177734,
      "learning_rate": 2.495416666666667e-05,
      "loss": 0.0016,
      "step": 120220
    },
    {
      "epoch": 4.007666666666666,
      "grad_norm": 0.1999947428703308,
      "learning_rate": 2.4952083333333334e-05,
      "loss": 0.0024,
      "step": 120230
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.36336931586265564,
      "learning_rate": 2.495e-05,
      "loss": 0.0022,
      "step": 120240
    },
    {
      "epoch": 4.008333333333334,
      "grad_norm": 0.31402409076690674,
      "learning_rate": 2.494791666666667e-05,
      "loss": 0.0028,
      "step": 120250
    },
    {
      "epoch": 4.008666666666667,
      "grad_norm": 0.17158520221710205,
      "learning_rate": 2.4945833333333334e-05,
      "loss": 0.0028,
      "step": 120260
    },
    {
      "epoch": 4.009,
      "grad_norm": 0.25687235593795776,
      "learning_rate": 2.4943750000000003e-05,
      "loss": 0.0021,
      "step": 120270
    },
    {
      "epoch": 4.009333333333333,
      "grad_norm": 0.31408262252807617,
      "learning_rate": 2.494166666666667e-05,
      "loss": 0.0019,
      "step": 120280
    },
    {
      "epoch": 4.009666666666667,
      "grad_norm": 0.4423372745513916,
      "learning_rate": 2.4939583333333334e-05,
      "loss": 0.002,
      "step": 120290
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.34308716654777527,
      "learning_rate": 2.4937500000000003e-05,
      "loss": 0.0019,
      "step": 120300
    },
    {
      "epoch": 4.0103333333333335,
      "grad_norm": 0.0858018696308136,
      "learning_rate": 2.4935416666666665e-05,
      "loss": 0.0019,
      "step": 120310
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.28617990016937256,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0029,
      "step": 120320
    },
    {
      "epoch": 4.011,
      "grad_norm": 0.08829759806394577,
      "learning_rate": 2.493125e-05,
      "loss": 0.0024,
      "step": 120330
    },
    {
      "epoch": 4.011333333333333,
      "grad_norm": 0.2575148940086365,
      "learning_rate": 2.492916666666667e-05,
      "loss": 0.0024,
      "step": 120340
    },
    {
      "epoch": 4.011666666666667,
      "grad_norm": 0.13505053520202637,
      "learning_rate": 2.4927083333333334e-05,
      "loss": 0.0023,
      "step": 120350
    },
    {
      "epoch": 4.012,
      "grad_norm": 0.17173291742801666,
      "learning_rate": 2.4925000000000003e-05,
      "loss": 0.0016,
      "step": 120360
    },
    {
      "epoch": 4.012333333333333,
      "grad_norm": 0.25711801648139954,
      "learning_rate": 2.4922916666666668e-05,
      "loss": 0.0032,
      "step": 120370
    },
    {
      "epoch": 4.012666666666667,
      "grad_norm": 0.0858391523361206,
      "learning_rate": 2.4920833333333334e-05,
      "loss": 0.0025,
      "step": 120380
    },
    {
      "epoch": 4.013,
      "grad_norm": 0.1714768409729004,
      "learning_rate": 2.4918750000000003e-05,
      "loss": 0.0021,
      "step": 120390
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.05918974056839943,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.0027,
      "step": 120400
    },
    {
      "epoch": 4.0136666666666665,
      "grad_norm": 0.015676366165280342,
      "learning_rate": 2.4914583333333337e-05,
      "loss": 0.0018,
      "step": 120410
    },
    {
      "epoch": 4.014,
      "grad_norm": 0.1154218390583992,
      "learning_rate": 2.4912500000000002e-05,
      "loss": 0.0037,
      "step": 120420
    },
    {
      "epoch": 4.014333333333333,
      "grad_norm": 0.1727685034275055,
      "learning_rate": 2.4910416666666668e-05,
      "loss": 0.0038,
      "step": 120430
    },
    {
      "epoch": 4.014666666666667,
      "grad_norm": 0.02965586446225643,
      "learning_rate": 2.4908333333333333e-05,
      "loss": 0.0017,
      "step": 120440
    },
    {
      "epoch": 4.015,
      "grad_norm": 0.4860004484653473,
      "learning_rate": 2.490625e-05,
      "loss": 0.0017,
      "step": 120450
    },
    {
      "epoch": 4.015333333333333,
      "grad_norm": 0.1426662802696228,
      "learning_rate": 2.4904166666666668e-05,
      "loss": 0.0023,
      "step": 120460
    },
    {
      "epoch": 4.015666666666666,
      "grad_norm": 0.06044869124889374,
      "learning_rate": 2.4902083333333333e-05,
      "loss": 0.0026,
      "step": 120470
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.03036106377840042,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0018,
      "step": 120480
    },
    {
      "epoch": 4.016333333333334,
      "grad_norm": 0.17856889963150024,
      "learning_rate": 2.4897916666666668e-05,
      "loss": 0.0026,
      "step": 120490
    },
    {
      "epoch": 4.016666666666667,
      "grad_norm": 0.42818185687065125,
      "learning_rate": 2.4895833333333337e-05,
      "loss": 0.0023,
      "step": 120500
    },
    {
      "epoch": 4.017,
      "grad_norm": 0.3997693955898285,
      "learning_rate": 2.4893750000000002e-05,
      "loss": 0.0019,
      "step": 120510
    },
    {
      "epoch": 4.017333333333333,
      "grad_norm": 0.2571098804473877,
      "learning_rate": 2.4891666666666667e-05,
      "loss": 0.0024,
      "step": 120520
    },
    {
      "epoch": 4.017666666666667,
      "grad_norm": 0.004120352678000927,
      "learning_rate": 2.4889583333333336e-05,
      "loss": 0.0022,
      "step": 120530
    },
    {
      "epoch": 4.018,
      "grad_norm": 0.17253811657428741,
      "learning_rate": 2.4887500000000002e-05,
      "loss": 0.002,
      "step": 120540
    },
    {
      "epoch": 4.0183333333333335,
      "grad_norm": 0.11535815894603729,
      "learning_rate": 2.4885416666666667e-05,
      "loss": 0.0024,
      "step": 120550
    },
    {
      "epoch": 4.018666666666666,
      "grad_norm": 0.14329467713832855,
      "learning_rate": 2.4883333333333333e-05,
      "loss": 0.0019,
      "step": 120560
    },
    {
      "epoch": 4.019,
      "grad_norm": 0.42914870381355286,
      "learning_rate": 2.4881250000000002e-05,
      "loss": 0.0019,
      "step": 120570
    },
    {
      "epoch": 4.019333333333333,
      "grad_norm": 0.1426849365234375,
      "learning_rate": 2.4879166666666667e-05,
      "loss": 0.0023,
      "step": 120580
    },
    {
      "epoch": 4.019666666666667,
      "grad_norm": 0.17180569469928741,
      "learning_rate": 2.4877083333333333e-05,
      "loss": 0.002,
      "step": 120590
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.05722954869270325,
      "learning_rate": 2.4875e-05,
      "loss": 0.0018,
      "step": 120600
    },
    {
      "epoch": 4.020333333333333,
      "grad_norm": 0.14652879536151886,
      "learning_rate": 2.4872916666666667e-05,
      "loss": 0.0024,
      "step": 120610
    },
    {
      "epoch": 4.020666666666667,
      "grad_norm": 0.1716485321521759,
      "learning_rate": 2.4870833333333336e-05,
      "loss": 0.0032,
      "step": 120620
    },
    {
      "epoch": 4.021,
      "grad_norm": 0.1941973865032196,
      "learning_rate": 2.486875e-05,
      "loss": 0.0016,
      "step": 120630
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.19960935413837433,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0013,
      "step": 120640
    },
    {
      "epoch": 4.0216666666666665,
      "grad_norm": 0.44491007924079895,
      "learning_rate": 2.4864583333333336e-05,
      "loss": 0.0017,
      "step": 120650
    },
    {
      "epoch": 4.022,
      "grad_norm": 0.030825594440102577,
      "learning_rate": 2.48625e-05,
      "loss": 0.0015,
      "step": 120660
    },
    {
      "epoch": 4.022333333333333,
      "grad_norm": 0.057195644825696945,
      "learning_rate": 2.4860416666666667e-05,
      "loss": 0.0017,
      "step": 120670
    },
    {
      "epoch": 4.022666666666667,
      "grad_norm": 0.057708803564310074,
      "learning_rate": 2.4858333333333332e-05,
      "loss": 0.0024,
      "step": 120680
    },
    {
      "epoch": 4.023,
      "grad_norm": 0.5425974130630493,
      "learning_rate": 2.485625e-05,
      "loss": 0.003,
      "step": 120690
    },
    {
      "epoch": 4.023333333333333,
      "grad_norm": 0.5770266056060791,
      "learning_rate": 2.4854166666666667e-05,
      "loss": 0.0021,
      "step": 120700
    },
    {
      "epoch": 4.023666666666666,
      "grad_norm": 0.1715405434370041,
      "learning_rate": 2.4852083333333335e-05,
      "loss": 0.002,
      "step": 120710
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.19968074560165405,
      "learning_rate": 2.485e-05,
      "loss": 0.0024,
      "step": 120720
    },
    {
      "epoch": 4.024333333333334,
      "grad_norm": 0.020391589030623436,
      "learning_rate": 2.4847916666666666e-05,
      "loss": 0.0023,
      "step": 120730
    },
    {
      "epoch": 4.024666666666667,
      "grad_norm": 0.11523953080177307,
      "learning_rate": 2.4845833333333335e-05,
      "loss": 0.0017,
      "step": 120740
    },
    {
      "epoch": 4.025,
      "grad_norm": 0.08596252650022507,
      "learning_rate": 2.484375e-05,
      "loss": 0.0022,
      "step": 120750
    },
    {
      "epoch": 4.025333333333333,
      "grad_norm": 0.029422536492347717,
      "learning_rate": 2.484166666666667e-05,
      "loss": 0.0021,
      "step": 120760
    },
    {
      "epoch": 4.025666666666667,
      "grad_norm": 0.030129708349704742,
      "learning_rate": 2.4839583333333335e-05,
      "loss": 0.003,
      "step": 120770
    },
    {
      "epoch": 4.026,
      "grad_norm": 0.11634383350610733,
      "learning_rate": 2.4837500000000004e-05,
      "loss": 0.0034,
      "step": 120780
    },
    {
      "epoch": 4.0263333333333335,
      "grad_norm": 0.4814419448375702,
      "learning_rate": 2.4835416666666666e-05,
      "loss": 0.0025,
      "step": 120790
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.009821100160479546,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0025,
      "step": 120800
    },
    {
      "epoch": 4.027,
      "grad_norm": 0.34266841411590576,
      "learning_rate": 2.483125e-05,
      "loss": 0.0024,
      "step": 120810
    },
    {
      "epoch": 4.027333333333333,
      "grad_norm": 0.14316608011722565,
      "learning_rate": 2.4829166666666666e-05,
      "loss": 0.0033,
      "step": 120820
    },
    {
      "epoch": 4.027666666666667,
      "grad_norm": 0.4000855088233948,
      "learning_rate": 2.4827083333333335e-05,
      "loss": 0.0026,
      "step": 120830
    },
    {
      "epoch": 4.028,
      "grad_norm": 0.11561673134565353,
      "learning_rate": 2.4825e-05,
      "loss": 0.0025,
      "step": 120840
    },
    {
      "epoch": 4.028333333333333,
      "grad_norm": 0.4584469795227051,
      "learning_rate": 2.482291666666667e-05,
      "loss": 0.002,
      "step": 120850
    },
    {
      "epoch": 4.028666666666667,
      "grad_norm": 0.17252232134342194,
      "learning_rate": 2.4820833333333335e-05,
      "loss": 0.0024,
      "step": 120860
    },
    {
      "epoch": 4.029,
      "grad_norm": 0.5086439847946167,
      "learning_rate": 2.481875e-05,
      "loss": 0.0027,
      "step": 120870
    },
    {
      "epoch": 4.029333333333334,
      "grad_norm": 0.23601499199867249,
      "learning_rate": 2.481666666666667e-05,
      "loss": 0.0024,
      "step": 120880
    },
    {
      "epoch": 4.0296666666666665,
      "grad_norm": 0.008982456289231777,
      "learning_rate": 2.4814583333333335e-05,
      "loss": 0.002,
      "step": 120890
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.1431279331445694,
      "learning_rate": 2.4812500000000003e-05,
      "loss": 0.0016,
      "step": 120900
    },
    {
      "epoch": 4.030333333333333,
      "grad_norm": 0.14294686913490295,
      "learning_rate": 2.4810416666666666e-05,
      "loss": 0.002,
      "step": 120910
    },
    {
      "epoch": 4.030666666666667,
      "grad_norm": 0.05756915733218193,
      "learning_rate": 2.4808333333333334e-05,
      "loss": 0.0025,
      "step": 120920
    },
    {
      "epoch": 4.031,
      "grad_norm": 0.05776486173272133,
      "learning_rate": 2.480625e-05,
      "loss": 0.0013,
      "step": 120930
    },
    {
      "epoch": 4.031333333333333,
      "grad_norm": 0.2128581404685974,
      "learning_rate": 2.480416666666667e-05,
      "loss": 0.0019,
      "step": 120940
    },
    {
      "epoch": 4.031666666666666,
      "grad_norm": 0.4772210717201233,
      "learning_rate": 2.4802083333333334e-05,
      "loss": 0.002,
      "step": 120950
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.17133189737796783,
      "learning_rate": 2.48e-05,
      "loss": 0.0021,
      "step": 120960
    },
    {
      "epoch": 4.032333333333334,
      "grad_norm": 0.14785262942314148,
      "learning_rate": 2.479791666666667e-05,
      "loss": 0.0022,
      "step": 120970
    },
    {
      "epoch": 4.032666666666667,
      "grad_norm": 0.029230719432234764,
      "learning_rate": 2.4795833333333334e-05,
      "loss": 0.0021,
      "step": 120980
    },
    {
      "epoch": 4.033,
      "grad_norm": 0.08582369983196259,
      "learning_rate": 2.4793750000000003e-05,
      "loss": 0.002,
      "step": 120990
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 0.20012307167053223,
      "learning_rate": 2.479166666666667e-05,
      "loss": 0.0023,
      "step": 121000
    },
    {
      "epoch": 4.033666666666667,
      "grad_norm": 0.5711483955383301,
      "learning_rate": 2.4789583333333334e-05,
      "loss": 0.0017,
      "step": 121010
    },
    {
      "epoch": 4.034,
      "grad_norm": 0.06020671874284744,
      "learning_rate": 2.4787500000000003e-05,
      "loss": 0.0016,
      "step": 121020
    },
    {
      "epoch": 4.0343333333333335,
      "grad_norm": 0.12084692716598511,
      "learning_rate": 2.4785416666666665e-05,
      "loss": 0.0017,
      "step": 121030
    },
    {
      "epoch": 4.034666666666666,
      "grad_norm": 0.11433979868888855,
      "learning_rate": 2.4783333333333334e-05,
      "loss": 0.0022,
      "step": 121040
    },
    {
      "epoch": 4.035,
      "grad_norm": 0.17195917665958405,
      "learning_rate": 2.478125e-05,
      "loss": 0.002,
      "step": 121050
    },
    {
      "epoch": 4.035333333333333,
      "grad_norm": 0.1147100180387497,
      "learning_rate": 2.4779166666666668e-05,
      "loss": 0.0017,
      "step": 121060
    },
    {
      "epoch": 4.035666666666667,
      "grad_norm": 0.39985746145248413,
      "learning_rate": 2.4777083333333334e-05,
      "loss": 0.0029,
      "step": 121070
    },
    {
      "epoch": 4.036,
      "grad_norm": 0.17160791158676147,
      "learning_rate": 2.4775000000000003e-05,
      "loss": 0.0015,
      "step": 121080
    },
    {
      "epoch": 4.036333333333333,
      "grad_norm": 0.1712430864572525,
      "learning_rate": 2.4772916666666668e-05,
      "loss": 0.0028,
      "step": 121090
    },
    {
      "epoch": 4.036666666666667,
      "grad_norm": 0.1998055875301361,
      "learning_rate": 2.4770833333333333e-05,
      "loss": 0.0026,
      "step": 121100
    },
    {
      "epoch": 4.037,
      "grad_norm": 0.11878693848848343,
      "learning_rate": 2.4768750000000002e-05,
      "loss": 0.0018,
      "step": 121110
    },
    {
      "epoch": 4.037333333333334,
      "grad_norm": 0.09114760905504227,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0022,
      "step": 121120
    },
    {
      "epoch": 4.0376666666666665,
      "grad_norm": 0.6714730858802795,
      "learning_rate": 2.4764583333333337e-05,
      "loss": 0.002,
      "step": 121130
    },
    {
      "epoch": 4.038,
      "grad_norm": 0.029435664415359497,
      "learning_rate": 2.4762500000000002e-05,
      "loss": 0.0022,
      "step": 121140
    },
    {
      "epoch": 4.038333333333333,
      "grad_norm": 0.17141199111938477,
      "learning_rate": 2.4760416666666668e-05,
      "loss": 0.0023,
      "step": 121150
    },
    {
      "epoch": 4.038666666666667,
      "grad_norm": 0.400624543428421,
      "learning_rate": 2.4758333333333333e-05,
      "loss": 0.0029,
      "step": 121160
    },
    {
      "epoch": 4.039,
      "grad_norm": 0.11445722728967667,
      "learning_rate": 2.475625e-05,
      "loss": 0.0023,
      "step": 121170
    },
    {
      "epoch": 4.039333333333333,
      "grad_norm": 0.008997301571071148,
      "learning_rate": 2.4754166666666668e-05,
      "loss": 0.0025,
      "step": 121180
    },
    {
      "epoch": 4.039666666666666,
      "grad_norm": 0.17128725349903107,
      "learning_rate": 2.4752083333333333e-05,
      "loss": 0.0019,
      "step": 121190
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.05717188119888306,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.0022,
      "step": 121200
    },
    {
      "epoch": 4.040333333333333,
      "grad_norm": 0.4281773865222931,
      "learning_rate": 2.4747916666666667e-05,
      "loss": 0.0018,
      "step": 121210
    },
    {
      "epoch": 4.040666666666667,
      "grad_norm": 0.0858888328075409,
      "learning_rate": 2.4745833333333336e-05,
      "loss": 0.0019,
      "step": 121220
    },
    {
      "epoch": 4.041,
      "grad_norm": 0.1140051931142807,
      "learning_rate": 2.4743750000000002e-05,
      "loss": 0.0031,
      "step": 121230
    },
    {
      "epoch": 4.041333333333333,
      "grad_norm": 0.14301352202892303,
      "learning_rate": 2.4741666666666667e-05,
      "loss": 0.0022,
      "step": 121240
    },
    {
      "epoch": 4.041666666666667,
      "grad_norm": 0.670020341873169,
      "learning_rate": 2.4739583333333336e-05,
      "loss": 0.002,
      "step": 121250
    },
    {
      "epoch": 4.042,
      "grad_norm": 0.31429362297058105,
      "learning_rate": 2.47375e-05,
      "loss": 0.0021,
      "step": 121260
    },
    {
      "epoch": 4.042333333333334,
      "grad_norm": 0.0048555065877735615,
      "learning_rate": 2.473541666666667e-05,
      "loss": 0.0028,
      "step": 121270
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.057520169764757156,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0023,
      "step": 121280
    },
    {
      "epoch": 4.043,
      "grad_norm": 0.08597613126039505,
      "learning_rate": 2.473125e-05,
      "loss": 0.0017,
      "step": 121290
    },
    {
      "epoch": 4.043333333333333,
      "grad_norm": 0.058007244020700455,
      "learning_rate": 2.4729166666666667e-05,
      "loss": 0.0016,
      "step": 121300
    },
    {
      "epoch": 4.043666666666667,
      "grad_norm": 0.011101813986897469,
      "learning_rate": 2.4727083333333332e-05,
      "loss": 0.0021,
      "step": 121310
    },
    {
      "epoch": 4.044,
      "grad_norm": 0.19039040803909302,
      "learning_rate": 2.4725e-05,
      "loss": 0.0038,
      "step": 121320
    },
    {
      "epoch": 4.044333333333333,
      "grad_norm": 0.028844546526670456,
      "learning_rate": 2.4722916666666667e-05,
      "loss": 0.0022,
      "step": 121330
    },
    {
      "epoch": 4.044666666666667,
      "grad_norm": 0.4851446747779846,
      "learning_rate": 2.4720833333333336e-05,
      "loss": 0.002,
      "step": 121340
    },
    {
      "epoch": 4.045,
      "grad_norm": 0.1426216959953308,
      "learning_rate": 2.471875e-05,
      "loss": 0.0024,
      "step": 121350
    },
    {
      "epoch": 4.045333333333334,
      "grad_norm": 0.11443359404802322,
      "learning_rate": 2.471666666666667e-05,
      "loss": 0.0018,
      "step": 121360
    },
    {
      "epoch": 4.0456666666666665,
      "grad_norm": 0.2570236623287201,
      "learning_rate": 2.4714583333333335e-05,
      "loss": 0.0021,
      "step": 121370
    },
    {
      "epoch": 4.046,
      "grad_norm": 0.09528204798698425,
      "learning_rate": 2.47125e-05,
      "loss": 0.0018,
      "step": 121380
    },
    {
      "epoch": 4.046333333333333,
      "grad_norm": 0.02904842048883438,
      "learning_rate": 2.471041666666667e-05,
      "loss": 0.0015,
      "step": 121390
    },
    {
      "epoch": 4.046666666666667,
      "grad_norm": 0.3140440881252289,
      "learning_rate": 2.4708333333333332e-05,
      "loss": 0.0025,
      "step": 121400
    },
    {
      "epoch": 4.047,
      "grad_norm": 0.14267446100711823,
      "learning_rate": 2.470625e-05,
      "loss": 0.0022,
      "step": 121410
    },
    {
      "epoch": 4.0473333333333334,
      "grad_norm": 0.1712731420993805,
      "learning_rate": 2.4704166666666666e-05,
      "loss": 0.0025,
      "step": 121420
    },
    {
      "epoch": 4.047666666666666,
      "grad_norm": 0.39968422055244446,
      "learning_rate": 2.4702083333333335e-05,
      "loss": 0.0022,
      "step": 121430
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.08580838143825531,
      "learning_rate": 2.47e-05,
      "loss": 0.0019,
      "step": 121440
    },
    {
      "epoch": 4.048333333333333,
      "grad_norm": 0.3995307981967926,
      "learning_rate": 2.4697916666666666e-05,
      "loss": 0.0022,
      "step": 121450
    },
    {
      "epoch": 4.048666666666667,
      "grad_norm": 0.11471005529165268,
      "learning_rate": 2.4695833333333335e-05,
      "loss": 0.0017,
      "step": 121460
    },
    {
      "epoch": 4.049,
      "grad_norm": 0.20012758672237396,
      "learning_rate": 2.469375e-05,
      "loss": 0.0021,
      "step": 121470
    },
    {
      "epoch": 4.049333333333333,
      "grad_norm": 0.31395038962364197,
      "learning_rate": 2.469166666666667e-05,
      "loss": 0.002,
      "step": 121480
    },
    {
      "epoch": 4.049666666666667,
      "grad_norm": 0.20153473317623138,
      "learning_rate": 2.4689583333333335e-05,
      "loss": 0.003,
      "step": 121490
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.05868711322546005,
      "learning_rate": 2.4687500000000004e-05,
      "loss": 0.0024,
      "step": 121500
    },
    {
      "epoch": 4.050333333333334,
      "grad_norm": 0.060444604605436325,
      "learning_rate": 2.468541666666667e-05,
      "loss": 0.0021,
      "step": 121510
    },
    {
      "epoch": 4.050666666666666,
      "grad_norm": 0.25727713108062744,
      "learning_rate": 2.4683333333333335e-05,
      "loss": 0.0022,
      "step": 121520
    },
    {
      "epoch": 4.051,
      "grad_norm": 0.3137274980545044,
      "learning_rate": 2.468125e-05,
      "loss": 0.003,
      "step": 121530
    },
    {
      "epoch": 4.051333333333333,
      "grad_norm": 0.3714321255683899,
      "learning_rate": 2.4679166666666666e-05,
      "loss": 0.0022,
      "step": 121540
    },
    {
      "epoch": 4.051666666666667,
      "grad_norm": 0.17124630510807037,
      "learning_rate": 2.4677083333333335e-05,
      "loss": 0.0023,
      "step": 121550
    },
    {
      "epoch": 4.052,
      "grad_norm": 0.6844916939735413,
      "learning_rate": 2.4675e-05,
      "loss": 0.0027,
      "step": 121560
    },
    {
      "epoch": 4.052333333333333,
      "grad_norm": 0.31421959400177,
      "learning_rate": 2.467291666666667e-05,
      "loss": 0.0026,
      "step": 121570
    },
    {
      "epoch": 4.052666666666667,
      "grad_norm": 0.0185167845338583,
      "learning_rate": 2.4670833333333334e-05,
      "loss": 0.0021,
      "step": 121580
    },
    {
      "epoch": 4.053,
      "grad_norm": 0.014237483032047749,
      "learning_rate": 2.466875e-05,
      "loss": 0.0019,
      "step": 121590
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.11462966352701187,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0019,
      "step": 121600
    },
    {
      "epoch": 4.0536666666666665,
      "grad_norm": 0.04962244629859924,
      "learning_rate": 2.4664583333333334e-05,
      "loss": 0.0021,
      "step": 121610
    },
    {
      "epoch": 4.054,
      "grad_norm": 0.0880286917090416,
      "learning_rate": 2.4662500000000003e-05,
      "loss": 0.0031,
      "step": 121620
    },
    {
      "epoch": 4.054333333333333,
      "grad_norm": 0.3141520023345947,
      "learning_rate": 2.466041666666667e-05,
      "loss": 0.0021,
      "step": 121630
    },
    {
      "epoch": 4.054666666666667,
      "grad_norm": 0.05797174200415611,
      "learning_rate": 2.4658333333333334e-05,
      "loss": 0.0018,
      "step": 121640
    },
    {
      "epoch": 4.055,
      "grad_norm": 0.2856188416481018,
      "learning_rate": 2.465625e-05,
      "loss": 0.0042,
      "step": 121650
    },
    {
      "epoch": 4.0553333333333335,
      "grad_norm": 0.17144055664539337,
      "learning_rate": 2.465416666666667e-05,
      "loss": 0.0015,
      "step": 121660
    },
    {
      "epoch": 4.055666666666666,
      "grad_norm": 0.05714074894785881,
      "learning_rate": 2.4652083333333334e-05,
      "loss": 0.0019,
      "step": 121670
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.2569066286087036,
      "learning_rate": 2.465e-05,
      "loss": 0.003,
      "step": 121680
    },
    {
      "epoch": 4.056333333333333,
      "grad_norm": 0.3710557818412781,
      "learning_rate": 2.464791666666667e-05,
      "loss": 0.0027,
      "step": 121690
    },
    {
      "epoch": 4.056666666666667,
      "grad_norm": 0.22811006009578705,
      "learning_rate": 2.4645833333333334e-05,
      "loss": 0.0021,
      "step": 121700
    },
    {
      "epoch": 4.057,
      "grad_norm": 0.2003083974123001,
      "learning_rate": 2.4643750000000003e-05,
      "loss": 0.0036,
      "step": 121710
    },
    {
      "epoch": 4.057333333333333,
      "grad_norm": 0.5245497226715088,
      "learning_rate": 2.4641666666666668e-05,
      "loss": 0.0023,
      "step": 121720
    },
    {
      "epoch": 4.057666666666667,
      "grad_norm": 0.02864358015358448,
      "learning_rate": 2.4639583333333334e-05,
      "loss": 0.0015,
      "step": 121730
    },
    {
      "epoch": 4.058,
      "grad_norm": 0.08692528307437897,
      "learning_rate": 2.4637500000000003e-05,
      "loss": 0.0019,
      "step": 121740
    },
    {
      "epoch": 4.058333333333334,
      "grad_norm": 0.08575046807527542,
      "learning_rate": 2.4635416666666668e-05,
      "loss": 0.0025,
      "step": 121750
    },
    {
      "epoch": 4.058666666666666,
      "grad_norm": 0.6567317843437195,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.0023,
      "step": 121760
    },
    {
      "epoch": 4.059,
      "grad_norm": 0.08598262071609497,
      "learning_rate": 2.463125e-05,
      "loss": 0.0024,
      "step": 121770
    },
    {
      "epoch": 4.059333333333333,
      "grad_norm": 0.14309220016002655,
      "learning_rate": 2.4629166666666668e-05,
      "loss": 0.0017,
      "step": 121780
    },
    {
      "epoch": 4.059666666666667,
      "grad_norm": 0.08578382432460785,
      "learning_rate": 2.4627083333333333e-05,
      "loss": 0.0026,
      "step": 121790
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.42845097184181213,
      "learning_rate": 2.4625000000000002e-05,
      "loss": 0.0027,
      "step": 121800
    },
    {
      "epoch": 4.060333333333333,
      "grad_norm": 0.25705763697624207,
      "learning_rate": 2.4622916666666668e-05,
      "loss": 0.0029,
      "step": 121810
    },
    {
      "epoch": 4.060666666666667,
      "grad_norm": 0.28532907366752625,
      "learning_rate": 2.4620833333333333e-05,
      "loss": 0.0031,
      "step": 121820
    },
    {
      "epoch": 4.061,
      "grad_norm": 0.22841782867908478,
      "learning_rate": 2.4618750000000002e-05,
      "loss": 0.0022,
      "step": 121830
    },
    {
      "epoch": 4.061333333333334,
      "grad_norm": 0.00595315033569932,
      "learning_rate": 2.4616666666666668e-05,
      "loss": 0.0022,
      "step": 121840
    },
    {
      "epoch": 4.0616666666666665,
      "grad_norm": 0.05840742588043213,
      "learning_rate": 2.4614583333333336e-05,
      "loss": 0.0019,
      "step": 121850
    },
    {
      "epoch": 4.062,
      "grad_norm": 0.20008215308189392,
      "learning_rate": 2.4612500000000002e-05,
      "loss": 0.0023,
      "step": 121860
    },
    {
      "epoch": 4.062333333333333,
      "grad_norm": 0.3424680531024933,
      "learning_rate": 2.4610416666666667e-05,
      "loss": 0.0016,
      "step": 121870
    },
    {
      "epoch": 4.062666666666667,
      "grad_norm": 0.14289745688438416,
      "learning_rate": 2.4608333333333333e-05,
      "loss": 0.002,
      "step": 121880
    },
    {
      "epoch": 4.063,
      "grad_norm": 0.1428610384464264,
      "learning_rate": 2.4606250000000002e-05,
      "loss": 0.0022,
      "step": 121890
    },
    {
      "epoch": 4.0633333333333335,
      "grad_norm": 0.2571858763694763,
      "learning_rate": 2.4604166666666667e-05,
      "loss": 0.0023,
      "step": 121900
    },
    {
      "epoch": 4.063666666666666,
      "grad_norm": 0.1710488200187683,
      "learning_rate": 2.4602083333333333e-05,
      "loss": 0.0023,
      "step": 121910
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.06053513288497925,
      "learning_rate": 2.46e-05,
      "loss": 0.0027,
      "step": 121920
    },
    {
      "epoch": 4.064333333333333,
      "grad_norm": 0.20038184523582458,
      "learning_rate": 2.4597916666666667e-05,
      "loss": 0.0021,
      "step": 121930
    },
    {
      "epoch": 4.064666666666667,
      "grad_norm": 0.3145313262939453,
      "learning_rate": 2.4595833333333336e-05,
      "loss": 0.0023,
      "step": 121940
    },
    {
      "epoch": 4.065,
      "grad_norm": 0.399318128824234,
      "learning_rate": 2.459375e-05,
      "loss": 0.0021,
      "step": 121950
    },
    {
      "epoch": 4.065333333333333,
      "grad_norm": 0.14276222884655,
      "learning_rate": 2.4591666666666667e-05,
      "loss": 0.0022,
      "step": 121960
    },
    {
      "epoch": 4.065666666666667,
      "grad_norm": 0.08622373640537262,
      "learning_rate": 2.4589583333333336e-05,
      "loss": 0.002,
      "step": 121970
    },
    {
      "epoch": 4.066,
      "grad_norm": 0.20010823011398315,
      "learning_rate": 2.45875e-05,
      "loss": 0.0018,
      "step": 121980
    },
    {
      "epoch": 4.066333333333334,
      "grad_norm": 0.456818550825119,
      "learning_rate": 2.458541666666667e-05,
      "loss": 0.0019,
      "step": 121990
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.22825190424919128,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.0029,
      "step": 122000
    },
    {
      "epoch": 4.067,
      "grad_norm": 0.11435773223638535,
      "learning_rate": 2.458125e-05,
      "loss": 0.0024,
      "step": 122010
    },
    {
      "epoch": 4.067333333333333,
      "grad_norm": 0.2569921910762787,
      "learning_rate": 2.4579166666666667e-05,
      "loss": 0.0023,
      "step": 122020
    },
    {
      "epoch": 4.067666666666667,
      "grad_norm": 0.11444501578807831,
      "learning_rate": 2.4577083333333336e-05,
      "loss": 0.0015,
      "step": 122030
    },
    {
      "epoch": 4.068,
      "grad_norm": 0.11432409286499023,
      "learning_rate": 2.4575e-05,
      "loss": 0.0019,
      "step": 122040
    },
    {
      "epoch": 4.068333333333333,
      "grad_norm": 0.11778197437524796,
      "learning_rate": 2.4572916666666667e-05,
      "loss": 0.0023,
      "step": 122050
    },
    {
      "epoch": 4.068666666666667,
      "grad_norm": 0.08591233193874359,
      "learning_rate": 2.4570833333333335e-05,
      "loss": 0.0022,
      "step": 122060
    },
    {
      "epoch": 4.069,
      "grad_norm": 0.3429334759712219,
      "learning_rate": 2.456875e-05,
      "loss": 0.0014,
      "step": 122070
    },
    {
      "epoch": 4.069333333333334,
      "grad_norm": 0.17157602310180664,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.002,
      "step": 122080
    },
    {
      "epoch": 4.0696666666666665,
      "grad_norm": 0.17120516300201416,
      "learning_rate": 2.4564583333333335e-05,
      "loss": 0.0017,
      "step": 122090
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.43202322721481323,
      "learning_rate": 2.45625e-05,
      "loss": 0.0019,
      "step": 122100
    },
    {
      "epoch": 4.070333333333333,
      "grad_norm": 0.028624147176742554,
      "learning_rate": 2.456041666666667e-05,
      "loss": 0.0021,
      "step": 122110
    },
    {
      "epoch": 4.070666666666667,
      "grad_norm": 0.11478228867053986,
      "learning_rate": 2.4558333333333332e-05,
      "loss": 0.0019,
      "step": 122120
    },
    {
      "epoch": 4.071,
      "grad_norm": 0.2576960623264313,
      "learning_rate": 2.455625e-05,
      "loss": 0.0017,
      "step": 122130
    },
    {
      "epoch": 4.0713333333333335,
      "grad_norm": 0.05831165984272957,
      "learning_rate": 2.4554166666666666e-05,
      "loss": 0.0016,
      "step": 122140
    },
    {
      "epoch": 4.071666666666666,
      "grad_norm": 0.45689815282821655,
      "learning_rate": 2.4552083333333335e-05,
      "loss": 0.0018,
      "step": 122150
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.25721389055252075,
      "learning_rate": 2.455e-05,
      "loss": 0.0023,
      "step": 122160
    },
    {
      "epoch": 4.072333333333333,
      "grad_norm": 0.00867220014333725,
      "learning_rate": 2.454791666666667e-05,
      "loss": 0.0016,
      "step": 122170
    },
    {
      "epoch": 4.072666666666667,
      "grad_norm": 0.20004184544086456,
      "learning_rate": 2.4545833333333335e-05,
      "loss": 0.0031,
      "step": 122180
    },
    {
      "epoch": 4.073,
      "grad_norm": 0.08567845821380615,
      "learning_rate": 2.454375e-05,
      "loss": 0.0029,
      "step": 122190
    },
    {
      "epoch": 4.073333333333333,
      "grad_norm": 0.5091806650161743,
      "learning_rate": 2.454166666666667e-05,
      "loss": 0.0017,
      "step": 122200
    },
    {
      "epoch": 4.073666666666667,
      "grad_norm": 0.08611609786748886,
      "learning_rate": 2.4539583333333335e-05,
      "loss": 0.0021,
      "step": 122210
    },
    {
      "epoch": 4.074,
      "grad_norm": 0.20008482038974762,
      "learning_rate": 2.4537500000000004e-05,
      "loss": 0.0016,
      "step": 122220
    },
    {
      "epoch": 4.074333333333334,
      "grad_norm": 0.19994738698005676,
      "learning_rate": 2.453541666666667e-05,
      "loss": 0.002,
      "step": 122230
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.1715046912431717,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0048,
      "step": 122240
    },
    {
      "epoch": 4.075,
      "grad_norm": 0.028815776109695435,
      "learning_rate": 2.453125e-05,
      "loss": 0.0026,
      "step": 122250
    },
    {
      "epoch": 4.075333333333333,
      "grad_norm": 0.05730632320046425,
      "learning_rate": 2.4529166666666665e-05,
      "loss": 0.002,
      "step": 122260
    },
    {
      "epoch": 4.075666666666667,
      "grad_norm": 0.057276614010334015,
      "learning_rate": 2.4527083333333334e-05,
      "loss": 0.0023,
      "step": 122270
    },
    {
      "epoch": 4.076,
      "grad_norm": 0.3711034953594208,
      "learning_rate": 2.4525e-05,
      "loss": 0.002,
      "step": 122280
    },
    {
      "epoch": 4.076333333333333,
      "grad_norm": 0.17139701545238495,
      "learning_rate": 2.452291666666667e-05,
      "loss": 0.002,
      "step": 122290
    },
    {
      "epoch": 4.076666666666666,
      "grad_norm": 0.5420904159545898,
      "learning_rate": 2.4520833333333334e-05,
      "loss": 0.002,
      "step": 122300
    },
    {
      "epoch": 4.077,
      "grad_norm": 0.19956813752651215,
      "learning_rate": 2.4518750000000003e-05,
      "loss": 0.0026,
      "step": 122310
    },
    {
      "epoch": 4.077333333333334,
      "grad_norm": 0.5421324968338013,
      "learning_rate": 2.451666666666667e-05,
      "loss": 0.0022,
      "step": 122320
    },
    {
      "epoch": 4.0776666666666666,
      "grad_norm": 0.42853567004203796,
      "learning_rate": 2.4514583333333334e-05,
      "loss": 0.0022,
      "step": 122330
    },
    {
      "epoch": 4.078,
      "grad_norm": 0.22827844321727753,
      "learning_rate": 2.4512500000000003e-05,
      "loss": 0.0016,
      "step": 122340
    },
    {
      "epoch": 4.078333333333333,
      "grad_norm": 0.4825129806995392,
      "learning_rate": 2.451041666666667e-05,
      "loss": 0.0013,
      "step": 122350
    },
    {
      "epoch": 4.078666666666667,
      "grad_norm": 0.3140272796154022,
      "learning_rate": 2.4508333333333334e-05,
      "loss": 0.0023,
      "step": 122360
    },
    {
      "epoch": 4.079,
      "grad_norm": 0.1715586632490158,
      "learning_rate": 2.450625e-05,
      "loss": 0.0022,
      "step": 122370
    },
    {
      "epoch": 4.0793333333333335,
      "grad_norm": 0.0075364201329648495,
      "learning_rate": 2.4504166666666668e-05,
      "loss": 0.0023,
      "step": 122380
    },
    {
      "epoch": 4.079666666666666,
      "grad_norm": 0.37077364325523376,
      "learning_rate": 2.4502083333333334e-05,
      "loss": 0.0021,
      "step": 122390
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.1191212609410286,
      "learning_rate": 2.45e-05,
      "loss": 0.003,
      "step": 122400
    },
    {
      "epoch": 4.080333333333333,
      "grad_norm": 0.0294730793684721,
      "learning_rate": 2.4497916666666668e-05,
      "loss": 0.0018,
      "step": 122410
    },
    {
      "epoch": 4.080666666666667,
      "grad_norm": 0.28572607040405273,
      "learning_rate": 2.4495833333333334e-05,
      "loss": 0.0024,
      "step": 122420
    },
    {
      "epoch": 4.081,
      "grad_norm": 0.17127856612205505,
      "learning_rate": 2.4493750000000002e-05,
      "loss": 0.0018,
      "step": 122430
    },
    {
      "epoch": 4.081333333333333,
      "grad_norm": 0.28539764881134033,
      "learning_rate": 2.4491666666666668e-05,
      "loss": 0.0025,
      "step": 122440
    },
    {
      "epoch": 4.081666666666667,
      "grad_norm": 0.14291758835315704,
      "learning_rate": 2.4489583333333337e-05,
      "loss": 0.0024,
      "step": 122450
    },
    {
      "epoch": 4.082,
      "grad_norm": 0.39970308542251587,
      "learning_rate": 2.4487500000000002e-05,
      "loss": 0.0023,
      "step": 122460
    },
    {
      "epoch": 4.082333333333334,
      "grad_norm": 0.14403051137924194,
      "learning_rate": 2.4485416666666668e-05,
      "loss": 0.0016,
      "step": 122470
    },
    {
      "epoch": 4.082666666666666,
      "grad_norm": 0.007460713852196932,
      "learning_rate": 2.4483333333333333e-05,
      "loss": 0.0018,
      "step": 122480
    },
    {
      "epoch": 4.083,
      "grad_norm": 0.08554235845804214,
      "learning_rate": 2.448125e-05,
      "loss": 0.0015,
      "step": 122490
    },
    {
      "epoch": 4.083333333333333,
      "grad_norm": 0.37079986929893494,
      "learning_rate": 2.4479166666666668e-05,
      "loss": 0.0016,
      "step": 122500
    },
    {
      "epoch": 4.083666666666667,
      "grad_norm": 0.22835738956928253,
      "learning_rate": 2.4477083333333333e-05,
      "loss": 0.0018,
      "step": 122510
    },
    {
      "epoch": 4.084,
      "grad_norm": 0.11434927582740784,
      "learning_rate": 2.4475000000000002e-05,
      "loss": 0.0025,
      "step": 122520
    },
    {
      "epoch": 4.084333333333333,
      "grad_norm": 0.6308436393737793,
      "learning_rate": 2.4472916666666667e-05,
      "loss": 0.0029,
      "step": 122530
    },
    {
      "epoch": 4.084666666666667,
      "grad_norm": 0.17146611213684082,
      "learning_rate": 2.4470833333333333e-05,
      "loss": 0.0018,
      "step": 122540
    },
    {
      "epoch": 4.085,
      "grad_norm": 0.08565139025449753,
      "learning_rate": 2.4468750000000002e-05,
      "loss": 0.0017,
      "step": 122550
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.3869166672229767,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0028,
      "step": 122560
    },
    {
      "epoch": 4.085666666666667,
      "grad_norm": 0.20027606189250946,
      "learning_rate": 2.4464583333333336e-05,
      "loss": 0.0014,
      "step": 122570
    },
    {
      "epoch": 4.086,
      "grad_norm": 0.14290082454681396,
      "learning_rate": 2.44625e-05,
      "loss": 0.0026,
      "step": 122580
    },
    {
      "epoch": 4.086333333333333,
      "grad_norm": 0.4280976355075836,
      "learning_rate": 2.446041666666667e-05,
      "loss": 0.0021,
      "step": 122590
    },
    {
      "epoch": 4.086666666666667,
      "grad_norm": 0.22857484221458435,
      "learning_rate": 2.4458333333333336e-05,
      "loss": 0.0028,
      "step": 122600
    },
    {
      "epoch": 4.087,
      "grad_norm": 0.19996137917041779,
      "learning_rate": 2.445625e-05,
      "loss": 0.0025,
      "step": 122610
    },
    {
      "epoch": 4.0873333333333335,
      "grad_norm": 0.2384953647851944,
      "learning_rate": 2.4454166666666667e-05,
      "loss": 0.0024,
      "step": 122620
    },
    {
      "epoch": 4.087666666666666,
      "grad_norm": 0.22840331494808197,
      "learning_rate": 2.4452083333333333e-05,
      "loss": 0.0014,
      "step": 122630
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.1716717779636383,
      "learning_rate": 2.445e-05,
      "loss": 0.002,
      "step": 122640
    },
    {
      "epoch": 4.088333333333333,
      "grad_norm": 0.1146165207028389,
      "learning_rate": 2.4447916666666667e-05,
      "loss": 0.0016,
      "step": 122650
    },
    {
      "epoch": 4.088666666666667,
      "grad_norm": 0.05811905860900879,
      "learning_rate": 2.4445833333333336e-05,
      "loss": 0.0018,
      "step": 122660
    },
    {
      "epoch": 4.089,
      "grad_norm": 0.20043498277664185,
      "learning_rate": 2.444375e-05,
      "loss": 0.0023,
      "step": 122670
    },
    {
      "epoch": 4.089333333333333,
      "grad_norm": 0.11413221061229706,
      "learning_rate": 2.4441666666666667e-05,
      "loss": 0.0026,
      "step": 122680
    },
    {
      "epoch": 4.089666666666667,
      "grad_norm": 0.3995216190814972,
      "learning_rate": 2.4439583333333336e-05,
      "loss": 0.0022,
      "step": 122690
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.31403592228889465,
      "learning_rate": 2.44375e-05,
      "loss": 0.0026,
      "step": 122700
    },
    {
      "epoch": 4.090333333333334,
      "grad_norm": 0.17279547452926636,
      "learning_rate": 2.443541666666667e-05,
      "loss": 0.0032,
      "step": 122710
    },
    {
      "epoch": 4.0906666666666665,
      "grad_norm": 0.3425198793411255,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0019,
      "step": 122720
    },
    {
      "epoch": 4.091,
      "grad_norm": 0.25722381472587585,
      "learning_rate": 2.443125e-05,
      "loss": 0.0016,
      "step": 122730
    },
    {
      "epoch": 4.091333333333333,
      "grad_norm": 0.22886264324188232,
      "learning_rate": 2.4429166666666666e-05,
      "loss": 0.0015,
      "step": 122740
    },
    {
      "epoch": 4.091666666666667,
      "grad_norm": 0.39976856112480164,
      "learning_rate": 2.4427083333333335e-05,
      "loss": 0.0016,
      "step": 122750
    },
    {
      "epoch": 4.092,
      "grad_norm": 0.17129948735237122,
      "learning_rate": 2.4425e-05,
      "loss": 0.0024,
      "step": 122760
    },
    {
      "epoch": 4.092333333333333,
      "grad_norm": 0.551433801651001,
      "learning_rate": 2.4422916666666666e-05,
      "loss": 0.0018,
      "step": 122770
    },
    {
      "epoch": 4.092666666666666,
      "grad_norm": 0.11463312804698944,
      "learning_rate": 2.4420833333333335e-05,
      "loss": 0.0015,
      "step": 122780
    },
    {
      "epoch": 4.093,
      "grad_norm": 0.029359908774495125,
      "learning_rate": 2.441875e-05,
      "loss": 0.0024,
      "step": 122790
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.1144305095076561,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.0021,
      "step": 122800
    },
    {
      "epoch": 4.093666666666667,
      "grad_norm": 0.2006252557039261,
      "learning_rate": 2.4414583333333335e-05,
      "loss": 0.0018,
      "step": 122810
    },
    {
      "epoch": 4.094,
      "grad_norm": 0.15361474454402924,
      "learning_rate": 2.44125e-05,
      "loss": 0.0032,
      "step": 122820
    },
    {
      "epoch": 4.094333333333333,
      "grad_norm": 0.11490368098020554,
      "learning_rate": 2.441041666666667e-05,
      "loss": 0.0013,
      "step": 122830
    },
    {
      "epoch": 4.094666666666667,
      "grad_norm": 0.22825276851654053,
      "learning_rate": 2.4408333333333335e-05,
      "loss": 0.002,
      "step": 122840
    },
    {
      "epoch": 4.095,
      "grad_norm": 0.14284250140190125,
      "learning_rate": 2.440625e-05,
      "loss": 0.0018,
      "step": 122850
    },
    {
      "epoch": 4.0953333333333335,
      "grad_norm": 0.08594677597284317,
      "learning_rate": 2.4404166666666666e-05,
      "loss": 0.0026,
      "step": 122860
    },
    {
      "epoch": 4.095666666666666,
      "grad_norm": 0.028658295050263405,
      "learning_rate": 2.4402083333333335e-05,
      "loss": 0.0025,
      "step": 122870
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.5792730450630188,
      "learning_rate": 2.44e-05,
      "loss": 0.002,
      "step": 122880
    },
    {
      "epoch": 4.096333333333333,
      "grad_norm": 0.31255730986595154,
      "learning_rate": 2.439791666666667e-05,
      "loss": 0.0021,
      "step": 122890
    },
    {
      "epoch": 4.096666666666667,
      "grad_norm": 0.021114936098456383,
      "learning_rate": 2.4395833333333335e-05,
      "loss": 0.0017,
      "step": 122900
    },
    {
      "epoch": 4.097,
      "grad_norm": 0.08629202097654343,
      "learning_rate": 2.439375e-05,
      "loss": 0.0023,
      "step": 122910
    },
    {
      "epoch": 4.097333333333333,
      "grad_norm": 0.3143928349018097,
      "learning_rate": 2.439166666666667e-05,
      "loss": 0.0019,
      "step": 122920
    },
    {
      "epoch": 4.097666666666667,
      "grad_norm": 0.06005999073386192,
      "learning_rate": 2.4389583333333334e-05,
      "loss": 0.0026,
      "step": 122930
    },
    {
      "epoch": 4.098,
      "grad_norm": 0.25788864493370056,
      "learning_rate": 2.4387500000000003e-05,
      "loss": 0.0027,
      "step": 122940
    },
    {
      "epoch": 4.098333333333334,
      "grad_norm": 0.3996236324310303,
      "learning_rate": 2.438541666666667e-05,
      "loss": 0.002,
      "step": 122950
    },
    {
      "epoch": 4.0986666666666665,
      "grad_norm": 0.08676157146692276,
      "learning_rate": 2.4383333333333334e-05,
      "loss": 0.0015,
      "step": 122960
    },
    {
      "epoch": 4.099,
      "grad_norm": 0.2876286506652832,
      "learning_rate": 2.438125e-05,
      "loss": 0.0017,
      "step": 122970
    },
    {
      "epoch": 4.099333333333333,
      "grad_norm": 0.0297672301530838,
      "learning_rate": 2.4379166666666665e-05,
      "loss": 0.0018,
      "step": 122980
    },
    {
      "epoch": 4.099666666666667,
      "grad_norm": 0.028998687863349915,
      "learning_rate": 2.4377083333333334e-05,
      "loss": 0.0019,
      "step": 122990
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.029037248343229294,
      "learning_rate": 2.4375e-05,
      "loss": 0.0018,
      "step": 123000
    },
    {
      "epoch": 4.100333333333333,
      "grad_norm": 0.31387606263160706,
      "learning_rate": 2.437291666666667e-05,
      "loss": 0.0027,
      "step": 123010
    },
    {
      "epoch": 4.100666666666666,
      "grad_norm": 0.22841061651706696,
      "learning_rate": 2.4370833333333334e-05,
      "loss": 0.0029,
      "step": 123020
    },
    {
      "epoch": 4.101,
      "grad_norm": 0.0998016744852066,
      "learning_rate": 2.4368750000000003e-05,
      "loss": 0.0029,
      "step": 123030
    },
    {
      "epoch": 4.101333333333334,
      "grad_norm": 0.11461123079061508,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.0015,
      "step": 123040
    },
    {
      "epoch": 4.101666666666667,
      "grad_norm": 0.2291184514760971,
      "learning_rate": 2.4364583333333334e-05,
      "loss": 0.0022,
      "step": 123050
    },
    {
      "epoch": 4.102,
      "grad_norm": 0.1231742724776268,
      "learning_rate": 2.4362500000000003e-05,
      "loss": 0.0022,
      "step": 123060
    },
    {
      "epoch": 4.102333333333333,
      "grad_norm": 0.313884973526001,
      "learning_rate": 2.4360416666666668e-05,
      "loss": 0.0027,
      "step": 123070
    },
    {
      "epoch": 4.102666666666667,
      "grad_norm": 0.05788211151957512,
      "learning_rate": 2.4358333333333337e-05,
      "loss": 0.0017,
      "step": 123080
    },
    {
      "epoch": 4.103,
      "grad_norm": 0.03181587532162666,
      "learning_rate": 2.435625e-05,
      "loss": 0.0022,
      "step": 123090
    },
    {
      "epoch": 4.1033333333333335,
      "grad_norm": 0.14294245839118958,
      "learning_rate": 2.4354166666666668e-05,
      "loss": 0.0021,
      "step": 123100
    },
    {
      "epoch": 4.103666666666666,
      "grad_norm": 0.5601041316986084,
      "learning_rate": 2.4352083333333333e-05,
      "loss": 0.0019,
      "step": 123110
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.14502720534801483,
      "learning_rate": 2.435e-05,
      "loss": 0.0019,
      "step": 123120
    },
    {
      "epoch": 4.104333333333333,
      "grad_norm": 0.17165833711624146,
      "learning_rate": 2.4347916666666668e-05,
      "loss": 0.0012,
      "step": 123130
    },
    {
      "epoch": 4.104666666666667,
      "grad_norm": 0.030045056715607643,
      "learning_rate": 2.4345833333333333e-05,
      "loss": 0.0023,
      "step": 123140
    },
    {
      "epoch": 4.105,
      "grad_norm": 0.029845578595995903,
      "learning_rate": 2.4343750000000002e-05,
      "loss": 0.0024,
      "step": 123150
    },
    {
      "epoch": 4.105333333333333,
      "grad_norm": 0.11550146341323853,
      "learning_rate": 2.4341666666666668e-05,
      "loss": 0.0021,
      "step": 123160
    },
    {
      "epoch": 4.105666666666667,
      "grad_norm": 0.22818997502326965,
      "learning_rate": 2.4339583333333337e-05,
      "loss": 0.0022,
      "step": 123170
    },
    {
      "epoch": 4.106,
      "grad_norm": 0.19993843138217926,
      "learning_rate": 2.4337500000000002e-05,
      "loss": 0.002,
      "step": 123180
    },
    {
      "epoch": 4.106333333333334,
      "grad_norm": 0.3430425524711609,
      "learning_rate": 2.4335416666666668e-05,
      "loss": 0.002,
      "step": 123190
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.17127077281475067,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0014,
      "step": 123200
    },
    {
      "epoch": 4.107,
      "grad_norm": 0.0576019361615181,
      "learning_rate": 2.433125e-05,
      "loss": 0.0018,
      "step": 123210
    },
    {
      "epoch": 4.107333333333333,
      "grad_norm": 0.2573464810848236,
      "learning_rate": 2.4329166666666667e-05,
      "loss": 0.0015,
      "step": 123220
    },
    {
      "epoch": 4.107666666666667,
      "grad_norm": 0.1727297157049179,
      "learning_rate": 2.4327083333333333e-05,
      "loss": 0.0025,
      "step": 123230
    },
    {
      "epoch": 4.108,
      "grad_norm": 0.029012981802225113,
      "learning_rate": 2.4325000000000002e-05,
      "loss": 0.0017,
      "step": 123240
    },
    {
      "epoch": 4.108333333333333,
      "grad_norm": 0.2569427490234375,
      "learning_rate": 2.4322916666666667e-05,
      "loss": 0.0031,
      "step": 123250
    },
    {
      "epoch": 4.108666666666666,
      "grad_norm": 0.17137156426906586,
      "learning_rate": 2.4320833333333333e-05,
      "loss": 0.0021,
      "step": 123260
    },
    {
      "epoch": 4.109,
      "grad_norm": 0.05715237557888031,
      "learning_rate": 2.431875e-05,
      "loss": 0.0025,
      "step": 123270
    },
    {
      "epoch": 4.109333333333334,
      "grad_norm": 0.17148110270500183,
      "learning_rate": 2.4316666666666667e-05,
      "loss": 0.0022,
      "step": 123280
    },
    {
      "epoch": 4.109666666666667,
      "grad_norm": 0.029186557978391647,
      "learning_rate": 2.4314583333333336e-05,
      "loss": 0.0029,
      "step": 123290
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.007208539638668299,
      "learning_rate": 2.43125e-05,
      "loss": 0.0023,
      "step": 123300
    },
    {
      "epoch": 4.110333333333333,
      "grad_norm": 0.1528065800666809,
      "learning_rate": 2.431041666666667e-05,
      "loss": 0.002,
      "step": 123310
    },
    {
      "epoch": 4.110666666666667,
      "grad_norm": 0.17055992782115936,
      "learning_rate": 2.4308333333333336e-05,
      "loss": 0.0014,
      "step": 123320
    },
    {
      "epoch": 4.111,
      "grad_norm": 0.08603456616401672,
      "learning_rate": 2.430625e-05,
      "loss": 0.0017,
      "step": 123330
    },
    {
      "epoch": 4.1113333333333335,
      "grad_norm": 0.22825369238853455,
      "learning_rate": 2.4304166666666667e-05,
      "loss": 0.002,
      "step": 123340
    },
    {
      "epoch": 4.111666666666666,
      "grad_norm": 0.3157177269458771,
      "learning_rate": 2.4302083333333332e-05,
      "loss": 0.0029,
      "step": 123350
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.030208580195903778,
      "learning_rate": 2.43e-05,
      "loss": 0.0023,
      "step": 123360
    },
    {
      "epoch": 4.112333333333333,
      "grad_norm": 0.25707924365997314,
      "learning_rate": 2.4297916666666667e-05,
      "loss": 0.0023,
      "step": 123370
    },
    {
      "epoch": 4.112666666666667,
      "grad_norm": 0.08653668314218521,
      "learning_rate": 2.4295833333333335e-05,
      "loss": 0.0024,
      "step": 123380
    },
    {
      "epoch": 4.113,
      "grad_norm": 0.25730475783348083,
      "learning_rate": 2.429375e-05,
      "loss": 0.0023,
      "step": 123390
    },
    {
      "epoch": 4.113333333333333,
      "grad_norm": 0.16356270015239716,
      "learning_rate": 2.4291666666666666e-05,
      "loss": 0.0021,
      "step": 123400
    },
    {
      "epoch": 4.113666666666667,
      "grad_norm": 0.028834199532866478,
      "learning_rate": 2.4289583333333335e-05,
      "loss": 0.0023,
      "step": 123410
    },
    {
      "epoch": 4.114,
      "grad_norm": 0.19296202063560486,
      "learning_rate": 2.42875e-05,
      "loss": 0.0021,
      "step": 123420
    },
    {
      "epoch": 4.114333333333334,
      "grad_norm": 0.28573063015937805,
      "learning_rate": 2.428541666666667e-05,
      "loss": 0.0032,
      "step": 123430
    },
    {
      "epoch": 4.1146666666666665,
      "grad_norm": 0.17202213406562805,
      "learning_rate": 2.4283333333333335e-05,
      "loss": 0.0022,
      "step": 123440
    },
    {
      "epoch": 4.115,
      "grad_norm": 0.11421599984169006,
      "learning_rate": 2.428125e-05,
      "loss": 0.0014,
      "step": 123450
    },
    {
      "epoch": 4.115333333333333,
      "grad_norm": 0.05737622082233429,
      "learning_rate": 2.4279166666666666e-05,
      "loss": 0.0019,
      "step": 123460
    },
    {
      "epoch": 4.115666666666667,
      "grad_norm": 0.2569632828235626,
      "learning_rate": 2.4277083333333335e-05,
      "loss": 0.0022,
      "step": 123470
    },
    {
      "epoch": 4.116,
      "grad_norm": 0.11431802064180374,
      "learning_rate": 2.4275e-05,
      "loss": 0.0019,
      "step": 123480
    },
    {
      "epoch": 4.116333333333333,
      "grad_norm": 0.05788714066147804,
      "learning_rate": 2.4272916666666666e-05,
      "loss": 0.0031,
      "step": 123490
    },
    {
      "epoch": 4.116666666666666,
      "grad_norm": 0.2288796752691269,
      "learning_rate": 2.4270833333333335e-05,
      "loss": 0.0024,
      "step": 123500
    },
    {
      "epoch": 4.117,
      "grad_norm": 0.05816400423645973,
      "learning_rate": 2.426875e-05,
      "loss": 0.0029,
      "step": 123510
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.17211954295635223,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0014,
      "step": 123520
    },
    {
      "epoch": 4.117666666666667,
      "grad_norm": 0.22814032435417175,
      "learning_rate": 2.4264583333333335e-05,
      "loss": 0.002,
      "step": 123530
    },
    {
      "epoch": 4.118,
      "grad_norm": 0.04927608743309975,
      "learning_rate": 2.42625e-05,
      "loss": 0.0015,
      "step": 123540
    },
    {
      "epoch": 4.118333333333333,
      "grad_norm": 0.2648002505302429,
      "learning_rate": 2.426041666666667e-05,
      "loss": 0.0024,
      "step": 123550
    },
    {
      "epoch": 4.118666666666667,
      "grad_norm": 0.11613287776708603,
      "learning_rate": 2.4258333333333335e-05,
      "loss": 0.002,
      "step": 123560
    },
    {
      "epoch": 4.119,
      "grad_norm": 0.029034126549959183,
      "learning_rate": 2.425625e-05,
      "loss": 0.0019,
      "step": 123570
    },
    {
      "epoch": 4.1193333333333335,
      "grad_norm": 0.38959965109825134,
      "learning_rate": 2.4254166666666666e-05,
      "loss": 0.0028,
      "step": 123580
    },
    {
      "epoch": 4.119666666666666,
      "grad_norm": 0.14318525791168213,
      "learning_rate": 2.4252083333333334e-05,
      "loss": 0.0022,
      "step": 123590
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.02912941761314869,
      "learning_rate": 2.425e-05,
      "loss": 0.0016,
      "step": 123600
    },
    {
      "epoch": 4.120333333333333,
      "grad_norm": 0.480240136384964,
      "learning_rate": 2.424791666666667e-05,
      "loss": 0.0016,
      "step": 123610
    },
    {
      "epoch": 4.120666666666667,
      "grad_norm": 0.17133451998233795,
      "learning_rate": 2.4245833333333334e-05,
      "loss": 0.0021,
      "step": 123620
    },
    {
      "epoch": 4.121,
      "grad_norm": 0.5294936299324036,
      "learning_rate": 2.424375e-05,
      "loss": 0.0016,
      "step": 123630
    },
    {
      "epoch": 4.121333333333333,
      "grad_norm": 0.007702148985117674,
      "learning_rate": 2.424166666666667e-05,
      "loss": 0.0018,
      "step": 123640
    },
    {
      "epoch": 4.121666666666667,
      "grad_norm": 0.057436782866716385,
      "learning_rate": 2.4239583333333334e-05,
      "loss": 0.0014,
      "step": 123650
    },
    {
      "epoch": 4.122,
      "grad_norm": 0.11491959542036057,
      "learning_rate": 2.4237500000000003e-05,
      "loss": 0.002,
      "step": 123660
    },
    {
      "epoch": 4.122333333333334,
      "grad_norm": 0.1996488720178604,
      "learning_rate": 2.423541666666667e-05,
      "loss": 0.0019,
      "step": 123670
    },
    {
      "epoch": 4.1226666666666665,
      "grad_norm": 0.37134453654289246,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0029,
      "step": 123680
    },
    {
      "epoch": 4.123,
      "grad_norm": 0.526923656463623,
      "learning_rate": 2.423125e-05,
      "loss": 0.0024,
      "step": 123690
    },
    {
      "epoch": 4.123333333333333,
      "grad_norm": 0.08595246821641922,
      "learning_rate": 2.422916666666667e-05,
      "loss": 0.0021,
      "step": 123700
    },
    {
      "epoch": 4.123666666666667,
      "grad_norm": 0.730476975440979,
      "learning_rate": 2.4227083333333334e-05,
      "loss": 0.0017,
      "step": 123710
    },
    {
      "epoch": 4.124,
      "grad_norm": 0.029042856767773628,
      "learning_rate": 2.4225e-05,
      "loss": 0.003,
      "step": 123720
    },
    {
      "epoch": 4.124333333333333,
      "grad_norm": 0.08601222187280655,
      "learning_rate": 2.4222916666666668e-05,
      "loss": 0.0017,
      "step": 123730
    },
    {
      "epoch": 4.124666666666666,
      "grad_norm": 0.11523494869470596,
      "learning_rate": 2.4220833333333334e-05,
      "loss": 0.0016,
      "step": 123740
    },
    {
      "epoch": 4.125,
      "grad_norm": 0.11403549462556839,
      "learning_rate": 2.4218750000000003e-05,
      "loss": 0.0021,
      "step": 123750
    },
    {
      "epoch": 4.125333333333334,
      "grad_norm": 0.3424389958381653,
      "learning_rate": 2.4216666666666668e-05,
      "loss": 0.0018,
      "step": 123760
    },
    {
      "epoch": 4.125666666666667,
      "grad_norm": 0.46349337697029114,
      "learning_rate": 2.4214583333333334e-05,
      "loss": 0.002,
      "step": 123770
    },
    {
      "epoch": 4.126,
      "grad_norm": 0.11615730822086334,
      "learning_rate": 2.4212500000000002e-05,
      "loss": 0.0019,
      "step": 123780
    },
    {
      "epoch": 4.126333333333333,
      "grad_norm": 0.20087619125843048,
      "learning_rate": 2.4210416666666668e-05,
      "loss": 0.0021,
      "step": 123790
    },
    {
      "epoch": 4.126666666666667,
      "grad_norm": 0.25719162821769714,
      "learning_rate": 2.4208333333333337e-05,
      "loss": 0.0018,
      "step": 123800
    },
    {
      "epoch": 4.127,
      "grad_norm": 0.0334136001765728,
      "learning_rate": 2.4206250000000002e-05,
      "loss": 0.0024,
      "step": 123810
    },
    {
      "epoch": 4.1273333333333335,
      "grad_norm": 0.11431384831666946,
      "learning_rate": 2.4204166666666668e-05,
      "loss": 0.002,
      "step": 123820
    },
    {
      "epoch": 4.127666666666666,
      "grad_norm": 0.3656642436981201,
      "learning_rate": 2.4202083333333333e-05,
      "loss": 0.0024,
      "step": 123830
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.14281024038791656,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0014,
      "step": 123840
    },
    {
      "epoch": 4.128333333333333,
      "grad_norm": 0.3299202024936676,
      "learning_rate": 2.4197916666666668e-05,
      "loss": 0.0031,
      "step": 123850
    },
    {
      "epoch": 4.128666666666667,
      "grad_norm": 0.19998909533023834,
      "learning_rate": 2.4195833333333333e-05,
      "loss": 0.0012,
      "step": 123860
    },
    {
      "epoch": 4.129,
      "grad_norm": 0.17140039801597595,
      "learning_rate": 2.4193750000000002e-05,
      "loss": 0.0015,
      "step": 123870
    },
    {
      "epoch": 4.129333333333333,
      "grad_norm": 0.17129357159137726,
      "learning_rate": 2.4191666666666667e-05,
      "loss": 0.0017,
      "step": 123880
    },
    {
      "epoch": 4.129666666666667,
      "grad_norm": 0.19990918040275574,
      "learning_rate": 2.4189583333333336e-05,
      "loss": 0.0018,
      "step": 123890
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.2865634262561798,
      "learning_rate": 2.4187500000000002e-05,
      "loss": 0.002,
      "step": 123900
    },
    {
      "epoch": 4.130333333333334,
      "grad_norm": 0.02926328405737877,
      "learning_rate": 2.4185416666666667e-05,
      "loss": 0.0018,
      "step": 123910
    },
    {
      "epoch": 4.1306666666666665,
      "grad_norm": 0.17134208977222443,
      "learning_rate": 2.4183333333333336e-05,
      "loss": 0.0013,
      "step": 123920
    },
    {
      "epoch": 4.131,
      "grad_norm": 0.1719423532485962,
      "learning_rate": 2.418125e-05,
      "loss": 0.0023,
      "step": 123930
    },
    {
      "epoch": 4.131333333333333,
      "grad_norm": 0.3139880895614624,
      "learning_rate": 2.4179166666666667e-05,
      "loss": 0.0025,
      "step": 123940
    },
    {
      "epoch": 4.131666666666667,
      "grad_norm": 0.34245967864990234,
      "learning_rate": 2.4177083333333333e-05,
      "loss": 0.0034,
      "step": 123950
    },
    {
      "epoch": 4.132,
      "grad_norm": 0.025420911610126495,
      "learning_rate": 2.4175e-05,
      "loss": 0.0024,
      "step": 123960
    },
    {
      "epoch": 4.132333333333333,
      "grad_norm": 0.11426929384469986,
      "learning_rate": 2.4172916666666667e-05,
      "loss": 0.0025,
      "step": 123970
    },
    {
      "epoch": 4.132666666666666,
      "grad_norm": 0.006840931251645088,
      "learning_rate": 2.4170833333333336e-05,
      "loss": 0.0021,
      "step": 123980
    },
    {
      "epoch": 4.133,
      "grad_norm": 0.34256574511528015,
      "learning_rate": 2.416875e-05,
      "loss": 0.0019,
      "step": 123990
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.1425117403268814,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0015,
      "step": 124000
    },
    {
      "epoch": 4.133666666666667,
      "grad_norm": 0.016087254509329796,
      "learning_rate": 2.4164583333333336e-05,
      "loss": 0.0021,
      "step": 124010
    },
    {
      "epoch": 4.134,
      "grad_norm": 0.14399421215057373,
      "learning_rate": 2.41625e-05,
      "loss": 0.0023,
      "step": 124020
    },
    {
      "epoch": 4.134333333333333,
      "grad_norm": 0.2884821593761444,
      "learning_rate": 2.416041666666667e-05,
      "loss": 0.0016,
      "step": 124030
    },
    {
      "epoch": 4.134666666666667,
      "grad_norm": 0.05796409025788307,
      "learning_rate": 2.4158333333333336e-05,
      "loss": 0.0024,
      "step": 124040
    },
    {
      "epoch": 4.135,
      "grad_norm": 0.086049385368824,
      "learning_rate": 2.415625e-05,
      "loss": 0.002,
      "step": 124050
    },
    {
      "epoch": 4.1353333333333335,
      "grad_norm": 0.486732542514801,
      "learning_rate": 2.4154166666666667e-05,
      "loss": 0.0018,
      "step": 124060
    },
    {
      "epoch": 4.135666666666666,
      "grad_norm": 0.14305530488491058,
      "learning_rate": 2.4152083333333332e-05,
      "loss": 0.0025,
      "step": 124070
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.2858079969882965,
      "learning_rate": 2.415e-05,
      "loss": 0.0031,
      "step": 124080
    },
    {
      "epoch": 4.136333333333333,
      "grad_norm": 0.14350800216197968,
      "learning_rate": 2.4147916666666666e-05,
      "loss": 0.0025,
      "step": 124090
    },
    {
      "epoch": 4.136666666666667,
      "grad_norm": 0.037120647728443146,
      "learning_rate": 2.4145833333333335e-05,
      "loss": 0.002,
      "step": 124100
    },
    {
      "epoch": 4.1370000000000005,
      "grad_norm": 0.19992931187152863,
      "learning_rate": 2.414375e-05,
      "loss": 0.0021,
      "step": 124110
    },
    {
      "epoch": 4.137333333333333,
      "grad_norm": 0.17109249532222748,
      "learning_rate": 2.414166666666667e-05,
      "loss": 0.002,
      "step": 124120
    },
    {
      "epoch": 4.137666666666667,
      "grad_norm": 0.0660843551158905,
      "learning_rate": 2.4139583333333335e-05,
      "loss": 0.0015,
      "step": 124130
    },
    {
      "epoch": 4.138,
      "grad_norm": 0.006743134465068579,
      "learning_rate": 2.41375e-05,
      "loss": 0.0026,
      "step": 124140
    },
    {
      "epoch": 4.138333333333334,
      "grad_norm": 0.25721386075019836,
      "learning_rate": 2.413541666666667e-05,
      "loss": 0.002,
      "step": 124150
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.2021799087524414,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0016,
      "step": 124160
    },
    {
      "epoch": 4.139,
      "grad_norm": 0.17202627658843994,
      "learning_rate": 2.4131250000000004e-05,
      "loss": 0.0019,
      "step": 124170
    },
    {
      "epoch": 4.139333333333333,
      "grad_norm": 0.4280441701412201,
      "learning_rate": 2.4129166666666666e-05,
      "loss": 0.002,
      "step": 124180
    },
    {
      "epoch": 4.139666666666667,
      "grad_norm": 0.1143113225698471,
      "learning_rate": 2.4127083333333335e-05,
      "loss": 0.0018,
      "step": 124190
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.1713315099477768,
      "learning_rate": 2.4125e-05,
      "loss": 0.0031,
      "step": 124200
    },
    {
      "epoch": 4.140333333333333,
      "grad_norm": 0.05736541375517845,
      "learning_rate": 2.4122916666666666e-05,
      "loss": 0.0019,
      "step": 124210
    },
    {
      "epoch": 4.140666666666666,
      "grad_norm": 0.028827963396906853,
      "learning_rate": 2.4120833333333335e-05,
      "loss": 0.0015,
      "step": 124220
    },
    {
      "epoch": 4.141,
      "grad_norm": 0.143944650888443,
      "learning_rate": 2.411875e-05,
      "loss": 0.0027,
      "step": 124230
    },
    {
      "epoch": 4.141333333333334,
      "grad_norm": 0.22819074988365173,
      "learning_rate": 2.411666666666667e-05,
      "loss": 0.0022,
      "step": 124240
    },
    {
      "epoch": 4.141666666666667,
      "grad_norm": 0.08587481826543808,
      "learning_rate": 2.4114583333333334e-05,
      "loss": 0.0026,
      "step": 124250
    },
    {
      "epoch": 4.142,
      "grad_norm": 0.1452426314353943,
      "learning_rate": 2.4112500000000003e-05,
      "loss": 0.0018,
      "step": 124260
    },
    {
      "epoch": 4.142333333333333,
      "grad_norm": 0.39952611923217773,
      "learning_rate": 2.411041666666667e-05,
      "loss": 0.002,
      "step": 124270
    },
    {
      "epoch": 4.142666666666667,
      "grad_norm": 0.07333602756261826,
      "learning_rate": 2.4108333333333334e-05,
      "loss": 0.0032,
      "step": 124280
    },
    {
      "epoch": 4.143,
      "grad_norm": 0.1724899858236313,
      "learning_rate": 2.4106250000000003e-05,
      "loss": 0.0023,
      "step": 124290
    },
    {
      "epoch": 4.1433333333333335,
      "grad_norm": 0.22835004329681396,
      "learning_rate": 2.4104166666666665e-05,
      "loss": 0.0018,
      "step": 124300
    },
    {
      "epoch": 4.143666666666666,
      "grad_norm": 0.3999550938606262,
      "learning_rate": 2.4102083333333334e-05,
      "loss": 0.0015,
      "step": 124310
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.20033101737499237,
      "learning_rate": 2.41e-05,
      "loss": 0.0021,
      "step": 124320
    },
    {
      "epoch": 4.144333333333333,
      "grad_norm": 0.43067049980163574,
      "learning_rate": 2.409791666666667e-05,
      "loss": 0.002,
      "step": 124330
    },
    {
      "epoch": 4.144666666666667,
      "grad_norm": 0.39961501955986023,
      "learning_rate": 2.4095833333333334e-05,
      "loss": 0.002,
      "step": 124340
    },
    {
      "epoch": 4.145,
      "grad_norm": 0.3426710069179535,
      "learning_rate": 2.409375e-05,
      "loss": 0.0019,
      "step": 124350
    },
    {
      "epoch": 4.145333333333333,
      "grad_norm": 0.028777481988072395,
      "learning_rate": 2.409166666666667e-05,
      "loss": 0.0016,
      "step": 124360
    },
    {
      "epoch": 4.145666666666667,
      "grad_norm": 0.31412625312805176,
      "learning_rate": 2.4089583333333334e-05,
      "loss": 0.0024,
      "step": 124370
    },
    {
      "epoch": 4.146,
      "grad_norm": 0.028817225247621536,
      "learning_rate": 2.4087500000000003e-05,
      "loss": 0.002,
      "step": 124380
    },
    {
      "epoch": 4.146333333333334,
      "grad_norm": 0.37058183550834656,
      "learning_rate": 2.4085416666666668e-05,
      "loss": 0.0029,
      "step": 124390
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.1798117309808731,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.0026,
      "step": 124400
    },
    {
      "epoch": 4.147,
      "grad_norm": 0.03121887892484665,
      "learning_rate": 2.4081250000000003e-05,
      "loss": 0.0021,
      "step": 124410
    },
    {
      "epoch": 4.147333333333333,
      "grad_norm": 0.3468744158744812,
      "learning_rate": 2.4079166666666668e-05,
      "loss": 0.0026,
      "step": 124420
    },
    {
      "epoch": 4.147666666666667,
      "grad_norm": 0.4563351273536682,
      "learning_rate": 2.4077083333333334e-05,
      "loss": 0.0019,
      "step": 124430
    },
    {
      "epoch": 4.148,
      "grad_norm": 0.40926697850227356,
      "learning_rate": 2.4075e-05,
      "loss": 0.0021,
      "step": 124440
    },
    {
      "epoch": 4.148333333333333,
      "grad_norm": 0.05781325697898865,
      "learning_rate": 2.4072916666666668e-05,
      "loss": 0.0021,
      "step": 124450
    },
    {
      "epoch": 4.148666666666666,
      "grad_norm": 0.030155692249536514,
      "learning_rate": 2.4070833333333333e-05,
      "loss": 0.0021,
      "step": 124460
    },
    {
      "epoch": 4.149,
      "grad_norm": 0.2854827642440796,
      "learning_rate": 2.4068750000000002e-05,
      "loss": 0.0016,
      "step": 124470
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.14267079532146454,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0022,
      "step": 124480
    },
    {
      "epoch": 4.149666666666667,
      "grad_norm": 0.028858399018645287,
      "learning_rate": 2.4064583333333333e-05,
      "loss": 0.0021,
      "step": 124490
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.08610942959785461,
      "learning_rate": 2.4062500000000002e-05,
      "loss": 0.0023,
      "step": 124500
    },
    {
      "epoch": 4.150333333333333,
      "grad_norm": 0.029379621148109436,
      "learning_rate": 2.4060416666666668e-05,
      "loss": 0.0015,
      "step": 124510
    },
    {
      "epoch": 4.150666666666667,
      "grad_norm": 0.05759238079190254,
      "learning_rate": 2.4058333333333336e-05,
      "loss": 0.0019,
      "step": 124520
    },
    {
      "epoch": 4.151,
      "grad_norm": 0.035498298704624176,
      "learning_rate": 2.4056250000000002e-05,
      "loss": 0.0023,
      "step": 124530
    },
    {
      "epoch": 4.1513333333333335,
      "grad_norm": 0.034728165715932846,
      "learning_rate": 2.4054166666666667e-05,
      "loss": 0.0027,
      "step": 124540
    },
    {
      "epoch": 4.151666666666666,
      "grad_norm": 0.030251428484916687,
      "learning_rate": 2.4052083333333333e-05,
      "loss": 0.0024,
      "step": 124550
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.11451907455921173,
      "learning_rate": 2.4050000000000002e-05,
      "loss": 0.0018,
      "step": 124560
    },
    {
      "epoch": 4.152333333333333,
      "grad_norm": 0.4015694856643677,
      "learning_rate": 2.4047916666666667e-05,
      "loss": 0.002,
      "step": 124570
    },
    {
      "epoch": 4.152666666666667,
      "grad_norm": 0.22792668640613556,
      "learning_rate": 2.4045833333333333e-05,
      "loss": 0.0024,
      "step": 124580
    },
    {
      "epoch": 4.153,
      "grad_norm": 0.2574198246002197,
      "learning_rate": 2.404375e-05,
      "loss": 0.0021,
      "step": 124590
    },
    {
      "epoch": 4.153333333333333,
      "grad_norm": 0.2568826973438263,
      "learning_rate": 2.4041666666666667e-05,
      "loss": 0.0027,
      "step": 124600
    },
    {
      "epoch": 4.153666666666667,
      "grad_norm": 0.14499902725219727,
      "learning_rate": 2.4039583333333336e-05,
      "loss": 0.0027,
      "step": 124610
    },
    {
      "epoch": 4.154,
      "grad_norm": 0.0625731572508812,
      "learning_rate": 2.40375e-05,
      "loss": 0.0021,
      "step": 124620
    },
    {
      "epoch": 4.154333333333334,
      "grad_norm": 0.20034167170524597,
      "learning_rate": 2.4035416666666667e-05,
      "loss": 0.0019,
      "step": 124630
    },
    {
      "epoch": 4.1546666666666665,
      "grad_norm": 0.1140839010477066,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0018,
      "step": 124640
    },
    {
      "epoch": 4.155,
      "grad_norm": 0.5962987542152405,
      "learning_rate": 2.403125e-05,
      "loss": 0.0021,
      "step": 124650
    },
    {
      "epoch": 4.155333333333333,
      "grad_norm": 0.4566909968852997,
      "learning_rate": 2.4029166666666667e-05,
      "loss": 0.0023,
      "step": 124660
    },
    {
      "epoch": 4.155666666666667,
      "grad_norm": 0.008167974650859833,
      "learning_rate": 2.4027083333333332e-05,
      "loss": 0.0017,
      "step": 124670
    },
    {
      "epoch": 4.156,
      "grad_norm": 0.5709590315818787,
      "learning_rate": 2.4025e-05,
      "loss": 0.003,
      "step": 124680
    },
    {
      "epoch": 4.156333333333333,
      "grad_norm": 0.29284563660621643,
      "learning_rate": 2.4022916666666667e-05,
      "loss": 0.0024,
      "step": 124690
    },
    {
      "epoch": 4.156666666666666,
      "grad_norm": 0.19979262351989746,
      "learning_rate": 2.4020833333333336e-05,
      "loss": 0.0014,
      "step": 124700
    },
    {
      "epoch": 4.157,
      "grad_norm": 0.28567618131637573,
      "learning_rate": 2.401875e-05,
      "loss": 0.0019,
      "step": 124710
    },
    {
      "epoch": 4.157333333333334,
      "grad_norm": 0.003485725726932287,
      "learning_rate": 2.4016666666666667e-05,
      "loss": 0.0023,
      "step": 124720
    },
    {
      "epoch": 4.157666666666667,
      "grad_norm": 0.028909659013152122,
      "learning_rate": 2.4014583333333335e-05,
      "loss": 0.0016,
      "step": 124730
    },
    {
      "epoch": 4.158,
      "grad_norm": 0.19950911402702332,
      "learning_rate": 2.40125e-05,
      "loss": 0.002,
      "step": 124740
    },
    {
      "epoch": 4.158333333333333,
      "grad_norm": 0.17164470255374908,
      "learning_rate": 2.401041666666667e-05,
      "loss": 0.0023,
      "step": 124750
    },
    {
      "epoch": 4.158666666666667,
      "grad_norm": 0.02948063611984253,
      "learning_rate": 2.4008333333333335e-05,
      "loss": 0.0027,
      "step": 124760
    },
    {
      "epoch": 4.159,
      "grad_norm": 0.22866329550743103,
      "learning_rate": 2.400625e-05,
      "loss": 0.0015,
      "step": 124770
    },
    {
      "epoch": 4.1593333333333335,
      "grad_norm": 0.11434820294380188,
      "learning_rate": 2.4004166666666666e-05,
      "loss": 0.0016,
      "step": 124780
    },
    {
      "epoch": 4.159666666666666,
      "grad_norm": 0.25650787353515625,
      "learning_rate": 2.4002083333333332e-05,
      "loss": 0.0021,
      "step": 124790
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.28559380769729614,
      "learning_rate": 2.4e-05,
      "loss": 0.0024,
      "step": 124800
    },
    {
      "epoch": 4.160333333333333,
      "grad_norm": 0.08671528100967407,
      "learning_rate": 2.3997916666666666e-05,
      "loss": 0.0016,
      "step": 124810
    },
    {
      "epoch": 4.160666666666667,
      "grad_norm": 0.22861838340759277,
      "learning_rate": 2.3995833333333335e-05,
      "loss": 0.0031,
      "step": 124820
    },
    {
      "epoch": 4.161,
      "grad_norm": 0.05713896080851555,
      "learning_rate": 2.399375e-05,
      "loss": 0.0021,
      "step": 124830
    },
    {
      "epoch": 4.161333333333333,
      "grad_norm": 0.03176766261458397,
      "learning_rate": 2.399166666666667e-05,
      "loss": 0.0018,
      "step": 124840
    },
    {
      "epoch": 4.161666666666667,
      "grad_norm": 0.010343051515519619,
      "learning_rate": 2.3989583333333335e-05,
      "loss": 0.0023,
      "step": 124850
    },
    {
      "epoch": 4.162,
      "grad_norm": 0.20164906978607178,
      "learning_rate": 2.39875e-05,
      "loss": 0.003,
      "step": 124860
    },
    {
      "epoch": 4.162333333333334,
      "grad_norm": 0.057399798184633255,
      "learning_rate": 2.398541666666667e-05,
      "loss": 0.0015,
      "step": 124870
    },
    {
      "epoch": 4.1626666666666665,
      "grad_norm": 0.006639156024903059,
      "learning_rate": 2.3983333333333335e-05,
      "loss": 0.0015,
      "step": 124880
    },
    {
      "epoch": 4.163,
      "grad_norm": 0.009705533273518085,
      "learning_rate": 2.3981250000000004e-05,
      "loss": 0.0021,
      "step": 124890
    },
    {
      "epoch": 4.163333333333333,
      "grad_norm": 0.03111252747476101,
      "learning_rate": 2.3979166666666666e-05,
      "loss": 0.002,
      "step": 124900
    },
    {
      "epoch": 4.163666666666667,
      "grad_norm": 0.49355801939964294,
      "learning_rate": 2.3977083333333335e-05,
      "loss": 0.0024,
      "step": 124910
    },
    {
      "epoch": 4.164,
      "grad_norm": 0.1996600329875946,
      "learning_rate": 2.3975e-05,
      "loss": 0.0026,
      "step": 124920
    },
    {
      "epoch": 4.164333333333333,
      "grad_norm": 0.08608902245759964,
      "learning_rate": 2.3972916666666665e-05,
      "loss": 0.0017,
      "step": 124930
    },
    {
      "epoch": 4.164666666666666,
      "grad_norm": 0.47752845287323,
      "learning_rate": 2.3970833333333334e-05,
      "loss": 0.0017,
      "step": 124940
    },
    {
      "epoch": 4.165,
      "grad_norm": 0.20006732642650604,
      "learning_rate": 2.396875e-05,
      "loss": 0.0023,
      "step": 124950
    },
    {
      "epoch": 4.165333333333333,
      "grad_norm": 0.14444510638713837,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0025,
      "step": 124960
    },
    {
      "epoch": 4.165666666666667,
      "grad_norm": 0.39913374185562134,
      "learning_rate": 2.3964583333333334e-05,
      "loss": 0.0021,
      "step": 124970
    },
    {
      "epoch": 4.166,
      "grad_norm": 0.2283061146736145,
      "learning_rate": 2.3962500000000003e-05,
      "loss": 0.0023,
      "step": 124980
    },
    {
      "epoch": 4.166333333333333,
      "grad_norm": 0.0881265327334404,
      "learning_rate": 2.396041666666667e-05,
      "loss": 0.0022,
      "step": 124990
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 0.06198965758085251,
      "learning_rate": 2.3958333333333334e-05,
      "loss": 0.0035,
      "step": 125000
    },
    {
      "epoch": 4.167,
      "grad_norm": 0.1713109165430069,
      "learning_rate": 2.3956250000000003e-05,
      "loss": 0.0024,
      "step": 125010
    },
    {
      "epoch": 4.167333333333334,
      "grad_norm": 0.41234397888183594,
      "learning_rate": 2.395416666666667e-05,
      "loss": 0.0031,
      "step": 125020
    },
    {
      "epoch": 4.167666666666666,
      "grad_norm": 0.08677542209625244,
      "learning_rate": 2.3952083333333334e-05,
      "loss": 0.0017,
      "step": 125030
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.17130006849765778,
      "learning_rate": 2.395e-05,
      "loss": 0.0019,
      "step": 125040
    },
    {
      "epoch": 4.168333333333333,
      "grad_norm": 0.1423867642879486,
      "learning_rate": 2.3947916666666668e-05,
      "loss": 0.0021,
      "step": 125050
    },
    {
      "epoch": 4.168666666666667,
      "grad_norm": 0.20920027792453766,
      "learning_rate": 2.3945833333333334e-05,
      "loss": 0.0012,
      "step": 125060
    },
    {
      "epoch": 4.169,
      "grad_norm": 0.11432896554470062,
      "learning_rate": 2.394375e-05,
      "loss": 0.0015,
      "step": 125070
    },
    {
      "epoch": 4.169333333333333,
      "grad_norm": 0.11440643668174744,
      "learning_rate": 2.3941666666666668e-05,
      "loss": 0.003,
      "step": 125080
    },
    {
      "epoch": 4.169666666666667,
      "grad_norm": 0.14277160167694092,
      "learning_rate": 2.3939583333333334e-05,
      "loss": 0.0018,
      "step": 125090
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.08618412911891937,
      "learning_rate": 2.3937500000000002e-05,
      "loss": 0.0017,
      "step": 125100
    },
    {
      "epoch": 4.170333333333334,
      "grad_norm": 0.08600163459777832,
      "learning_rate": 2.3935416666666668e-05,
      "loss": 0.0019,
      "step": 125110
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.1716948002576828,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0033,
      "step": 125120
    },
    {
      "epoch": 4.171,
      "grad_norm": 0.08672180026769638,
      "learning_rate": 2.3931250000000002e-05,
      "loss": 0.0029,
      "step": 125130
    },
    {
      "epoch": 4.171333333333333,
      "grad_norm": 0.05916542932391167,
      "learning_rate": 2.3929166666666668e-05,
      "loss": 0.0014,
      "step": 125140
    },
    {
      "epoch": 4.171666666666667,
      "grad_norm": 0.3991418182849884,
      "learning_rate": 2.3927083333333333e-05,
      "loss": 0.0023,
      "step": 125150
    },
    {
      "epoch": 4.172,
      "grad_norm": 0.37154069542884827,
      "learning_rate": 2.3925e-05,
      "loss": 0.003,
      "step": 125160
    },
    {
      "epoch": 4.1723333333333334,
      "grad_norm": 0.08578511327505112,
      "learning_rate": 2.3922916666666668e-05,
      "loss": 0.0023,
      "step": 125170
    },
    {
      "epoch": 4.172666666666666,
      "grad_norm": 0.035577546805143356,
      "learning_rate": 2.3920833333333333e-05,
      "loss": 0.0026,
      "step": 125180
    },
    {
      "epoch": 4.173,
      "grad_norm": 0.038687147200107574,
      "learning_rate": 2.3918750000000002e-05,
      "loss": 0.0031,
      "step": 125190
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.05792054906487465,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.002,
      "step": 125200
    },
    {
      "epoch": 4.173666666666667,
      "grad_norm": 0.05853159725666046,
      "learning_rate": 2.3914583333333333e-05,
      "loss": 0.0017,
      "step": 125210
    },
    {
      "epoch": 4.174,
      "grad_norm": 0.11500368267297745,
      "learning_rate": 2.3912500000000002e-05,
      "loss": 0.0016,
      "step": 125220
    },
    {
      "epoch": 4.174333333333333,
      "grad_norm": 0.31387361884117126,
      "learning_rate": 2.3910416666666667e-05,
      "loss": 0.0022,
      "step": 125230
    },
    {
      "epoch": 4.174666666666667,
      "grad_norm": 0.2858373820781708,
      "learning_rate": 2.3908333333333336e-05,
      "loss": 0.0019,
      "step": 125240
    },
    {
      "epoch": 4.175,
      "grad_norm": 0.2285756915807724,
      "learning_rate": 2.3906250000000002e-05,
      "loss": 0.002,
      "step": 125250
    },
    {
      "epoch": 4.175333333333334,
      "grad_norm": 0.20065034925937653,
      "learning_rate": 2.390416666666667e-05,
      "loss": 0.0028,
      "step": 125260
    },
    {
      "epoch": 4.175666666666666,
      "grad_norm": 0.044714052230119705,
      "learning_rate": 2.3902083333333333e-05,
      "loss": 0.0027,
      "step": 125270
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.008422831073403358,
      "learning_rate": 2.39e-05,
      "loss": 0.0021,
      "step": 125280
    },
    {
      "epoch": 4.176333333333333,
      "grad_norm": 0.0631013885140419,
      "learning_rate": 2.3897916666666667e-05,
      "loss": 0.0026,
      "step": 125290
    },
    {
      "epoch": 4.176666666666667,
      "grad_norm": 0.31486955285072327,
      "learning_rate": 2.3895833333333333e-05,
      "loss": 0.0018,
      "step": 125300
    },
    {
      "epoch": 4.177,
      "grad_norm": 0.1714463233947754,
      "learning_rate": 2.389375e-05,
      "loss": 0.0022,
      "step": 125310
    },
    {
      "epoch": 4.177333333333333,
      "grad_norm": 0.19966204464435577,
      "learning_rate": 2.3891666666666667e-05,
      "loss": 0.0018,
      "step": 125320
    },
    {
      "epoch": 4.177666666666667,
      "grad_norm": 0.03006216697394848,
      "learning_rate": 2.3889583333333336e-05,
      "loss": 0.0018,
      "step": 125330
    },
    {
      "epoch": 4.178,
      "grad_norm": 0.3138073980808258,
      "learning_rate": 2.38875e-05,
      "loss": 0.0016,
      "step": 125340
    },
    {
      "epoch": 4.178333333333334,
      "grad_norm": 0.4716222584247589,
      "learning_rate": 2.3885416666666667e-05,
      "loss": 0.0024,
      "step": 125350
    },
    {
      "epoch": 4.1786666666666665,
      "grad_norm": 0.14268508553504944,
      "learning_rate": 2.3883333333333336e-05,
      "loss": 0.0016,
      "step": 125360
    },
    {
      "epoch": 4.179,
      "grad_norm": 0.06458892673254013,
      "learning_rate": 2.388125e-05,
      "loss": 0.0027,
      "step": 125370
    },
    {
      "epoch": 4.179333333333333,
      "grad_norm": 0.1714596003293991,
      "learning_rate": 2.387916666666667e-05,
      "loss": 0.0026,
      "step": 125380
    },
    {
      "epoch": 4.179666666666667,
      "grad_norm": 0.14290253818035126,
      "learning_rate": 2.3877083333333332e-05,
      "loss": 0.0018,
      "step": 125390
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.22845230996608734,
      "learning_rate": 2.3875e-05,
      "loss": 0.0022,
      "step": 125400
    },
    {
      "epoch": 4.1803333333333335,
      "grad_norm": 0.17118225991725922,
      "learning_rate": 2.3872916666666666e-05,
      "loss": 0.0014,
      "step": 125410
    },
    {
      "epoch": 4.180666666666666,
      "grad_norm": 0.032849349081516266,
      "learning_rate": 2.3870833333333335e-05,
      "loss": 0.0019,
      "step": 125420
    },
    {
      "epoch": 4.181,
      "grad_norm": 0.2855098247528076,
      "learning_rate": 2.386875e-05,
      "loss": 0.0022,
      "step": 125430
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.0855337381362915,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0029,
      "step": 125440
    },
    {
      "epoch": 4.181666666666667,
      "grad_norm": 0.11423216760158539,
      "learning_rate": 2.3864583333333335e-05,
      "loss": 0.0021,
      "step": 125450
    },
    {
      "epoch": 4.182,
      "grad_norm": 0.030009865760803223,
      "learning_rate": 2.38625e-05,
      "loss": 0.0025,
      "step": 125460
    },
    {
      "epoch": 4.182333333333333,
      "grad_norm": 0.08699171245098114,
      "learning_rate": 2.386041666666667e-05,
      "loss": 0.0019,
      "step": 125470
    },
    {
      "epoch": 4.182666666666667,
      "grad_norm": 0.031134409829974174,
      "learning_rate": 2.3858333333333335e-05,
      "loss": 0.0017,
      "step": 125480
    },
    {
      "epoch": 4.183,
      "grad_norm": 0.08671792596578598,
      "learning_rate": 2.3856250000000004e-05,
      "loss": 0.0016,
      "step": 125490
    },
    {
      "epoch": 4.183333333333334,
      "grad_norm": 0.1722891926765442,
      "learning_rate": 2.385416666666667e-05,
      "loss": 0.0014,
      "step": 125500
    },
    {
      "epoch": 4.183666666666666,
      "grad_norm": 0.19980362057685852,
      "learning_rate": 2.3852083333333335e-05,
      "loss": 0.0016,
      "step": 125510
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.22850137948989868,
      "learning_rate": 2.385e-05,
      "loss": 0.0017,
      "step": 125520
    },
    {
      "epoch": 4.184333333333333,
      "grad_norm": 0.20014271140098572,
      "learning_rate": 2.3847916666666666e-05,
      "loss": 0.0019,
      "step": 125530
    },
    {
      "epoch": 4.184666666666667,
      "grad_norm": 0.11388396471738815,
      "learning_rate": 2.3845833333333335e-05,
      "loss": 0.0015,
      "step": 125540
    },
    {
      "epoch": 4.185,
      "grad_norm": 0.0381770096719265,
      "learning_rate": 2.384375e-05,
      "loss": 0.0015,
      "step": 125550
    },
    {
      "epoch": 4.185333333333333,
      "grad_norm": 0.5018165111541748,
      "learning_rate": 2.384166666666667e-05,
      "loss": 0.0029,
      "step": 125560
    },
    {
      "epoch": 4.185666666666667,
      "grad_norm": 0.799938976764679,
      "learning_rate": 2.3839583333333335e-05,
      "loss": 0.002,
      "step": 125570
    },
    {
      "epoch": 4.186,
      "grad_norm": 0.2855101227760315,
      "learning_rate": 2.38375e-05,
      "loss": 0.0016,
      "step": 125580
    },
    {
      "epoch": 4.186333333333334,
      "grad_norm": 0.31388580799102783,
      "learning_rate": 2.383541666666667e-05,
      "loss": 0.0029,
      "step": 125590
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.14273889362812042,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.0011,
      "step": 125600
    },
    {
      "epoch": 4.187,
      "grad_norm": 0.1720840036869049,
      "learning_rate": 2.3831250000000003e-05,
      "loss": 0.0023,
      "step": 125610
    },
    {
      "epoch": 4.187333333333333,
      "grad_norm": 0.2754528224468231,
      "learning_rate": 2.382916666666667e-05,
      "loss": 0.0034,
      "step": 125620
    },
    {
      "epoch": 4.187666666666667,
      "grad_norm": 0.1440986543893814,
      "learning_rate": 2.3827083333333334e-05,
      "loss": 0.0022,
      "step": 125630
    },
    {
      "epoch": 4.188,
      "grad_norm": 0.05768910422921181,
      "learning_rate": 2.3825e-05,
      "loss": 0.0019,
      "step": 125640
    },
    {
      "epoch": 4.1883333333333335,
      "grad_norm": 0.14287136495113373,
      "learning_rate": 2.382291666666667e-05,
      "loss": 0.0016,
      "step": 125650
    },
    {
      "epoch": 4.188666666666666,
      "grad_norm": 0.1432272493839264,
      "learning_rate": 2.3820833333333334e-05,
      "loss": 0.0023,
      "step": 125660
    },
    {
      "epoch": 4.189,
      "grad_norm": 0.07746414840221405,
      "learning_rate": 2.381875e-05,
      "loss": 0.002,
      "step": 125670
    },
    {
      "epoch": 4.189333333333333,
      "grad_norm": 0.08605135977268219,
      "learning_rate": 2.381666666666667e-05,
      "loss": 0.0018,
      "step": 125680
    },
    {
      "epoch": 4.189666666666667,
      "grad_norm": 0.05763435736298561,
      "learning_rate": 2.3814583333333334e-05,
      "loss": 0.0015,
      "step": 125690
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.5421507358551025,
      "learning_rate": 2.3812500000000003e-05,
      "loss": 0.0016,
      "step": 125700
    },
    {
      "epoch": 4.190333333333333,
      "grad_norm": 0.029830513522028923,
      "learning_rate": 2.381041666666667e-05,
      "loss": 0.0023,
      "step": 125710
    },
    {
      "epoch": 4.190666666666667,
      "grad_norm": 0.058256302028894424,
      "learning_rate": 2.3808333333333334e-05,
      "loss": 0.0025,
      "step": 125720
    },
    {
      "epoch": 4.191,
      "grad_norm": 0.17176194489002228,
      "learning_rate": 2.3806250000000003e-05,
      "loss": 0.0014,
      "step": 125730
    },
    {
      "epoch": 4.191333333333334,
      "grad_norm": 0.06274467706680298,
      "learning_rate": 2.3804166666666668e-05,
      "loss": 0.0017,
      "step": 125740
    },
    {
      "epoch": 4.191666666666666,
      "grad_norm": 0.11451061815023422,
      "learning_rate": 2.3802083333333334e-05,
      "loss": 0.0026,
      "step": 125750
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.17155908048152924,
      "learning_rate": 2.38e-05,
      "loss": 0.0019,
      "step": 125760
    },
    {
      "epoch": 4.192333333333333,
      "grad_norm": 0.19982847571372986,
      "learning_rate": 2.3797916666666668e-05,
      "loss": 0.002,
      "step": 125770
    },
    {
      "epoch": 4.192666666666667,
      "grad_norm": 0.05863475799560547,
      "learning_rate": 2.3795833333333333e-05,
      "loss": 0.0018,
      "step": 125780
    },
    {
      "epoch": 4.193,
      "grad_norm": 0.0857635959982872,
      "learning_rate": 2.3793750000000002e-05,
      "loss": 0.0014,
      "step": 125790
    },
    {
      "epoch": 4.193333333333333,
      "grad_norm": 0.030024761334061623,
      "learning_rate": 2.3791666666666668e-05,
      "loss": 0.0017,
      "step": 125800
    },
    {
      "epoch": 4.193666666666667,
      "grad_norm": 0.08697516471147537,
      "learning_rate": 2.3789583333333333e-05,
      "loss": 0.0014,
      "step": 125810
    },
    {
      "epoch": 4.194,
      "grad_norm": 0.05766129493713379,
      "learning_rate": 2.3787500000000002e-05,
      "loss": 0.0014,
      "step": 125820
    },
    {
      "epoch": 4.194333333333334,
      "grad_norm": 0.14303451776504517,
      "learning_rate": 2.3785416666666668e-05,
      "loss": 0.0015,
      "step": 125830
    },
    {
      "epoch": 4.1946666666666665,
      "grad_norm": 0.11414358019828796,
      "learning_rate": 2.3783333333333337e-05,
      "loss": 0.003,
      "step": 125840
    },
    {
      "epoch": 4.195,
      "grad_norm": 0.22888873517513275,
      "learning_rate": 2.3781250000000002e-05,
      "loss": 0.0023,
      "step": 125850
    },
    {
      "epoch": 4.195333333333333,
      "grad_norm": 0.20020191371440887,
      "learning_rate": 2.3779166666666668e-05,
      "loss": 0.0022,
      "step": 125860
    },
    {
      "epoch": 4.195666666666667,
      "grad_norm": 0.007792642805725336,
      "learning_rate": 2.3777083333333333e-05,
      "loss": 0.0016,
      "step": 125870
    },
    {
      "epoch": 4.196,
      "grad_norm": 0.42842575907707214,
      "learning_rate": 2.3775e-05,
      "loss": 0.0018,
      "step": 125880
    },
    {
      "epoch": 4.1963333333333335,
      "grad_norm": 0.5424574613571167,
      "learning_rate": 2.3772916666666667e-05,
      "loss": 0.0015,
      "step": 125890
    },
    {
      "epoch": 4.196666666666666,
      "grad_norm": 0.3138830065727234,
      "learning_rate": 2.3770833333333333e-05,
      "loss": 0.0021,
      "step": 125900
    },
    {
      "epoch": 4.197,
      "grad_norm": 0.2283446043729782,
      "learning_rate": 2.3768750000000002e-05,
      "loss": 0.0016,
      "step": 125910
    },
    {
      "epoch": 4.197333333333333,
      "grad_norm": 0.0859009250998497,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0025,
      "step": 125920
    },
    {
      "epoch": 4.197666666666667,
      "grad_norm": 0.0609203465282917,
      "learning_rate": 2.3764583333333336e-05,
      "loss": 0.0017,
      "step": 125930
    },
    {
      "epoch": 4.198,
      "grad_norm": 0.08591319620609283,
      "learning_rate": 2.37625e-05,
      "loss": 0.0019,
      "step": 125940
    },
    {
      "epoch": 4.198333333333333,
      "grad_norm": 0.11496113240718842,
      "learning_rate": 2.3760416666666667e-05,
      "loss": 0.0019,
      "step": 125950
    },
    {
      "epoch": 4.198666666666667,
      "grad_norm": 0.14787167310714722,
      "learning_rate": 2.3758333333333336e-05,
      "loss": 0.0031,
      "step": 125960
    },
    {
      "epoch": 4.199,
      "grad_norm": 0.029285039752721786,
      "learning_rate": 2.375625e-05,
      "loss": 0.0021,
      "step": 125970
    },
    {
      "epoch": 4.199333333333334,
      "grad_norm": 0.08598868548870087,
      "learning_rate": 2.375416666666667e-05,
      "loss": 0.0018,
      "step": 125980
    },
    {
      "epoch": 4.199666666666666,
      "grad_norm": 0.22854796051979065,
      "learning_rate": 2.3752083333333332e-05,
      "loss": 0.0028,
      "step": 125990
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.11438485980033875,
      "learning_rate": 2.375e-05,
      "loss": 0.0018,
      "step": 126000
    },
    {
      "epoch": 4.200333333333333,
      "grad_norm": 0.06169462203979492,
      "learning_rate": 2.3747916666666667e-05,
      "loss": 0.0017,
      "step": 126010
    },
    {
      "epoch": 4.200666666666667,
      "grad_norm": 0.17122766375541687,
      "learning_rate": 2.3745833333333332e-05,
      "loss": 0.0018,
      "step": 126020
    },
    {
      "epoch": 4.201,
      "grad_norm": 0.4329228699207306,
      "learning_rate": 2.374375e-05,
      "loss": 0.003,
      "step": 126030
    },
    {
      "epoch": 4.201333333333333,
      "grad_norm": 0.19992060959339142,
      "learning_rate": 2.3741666666666667e-05,
      "loss": 0.0024,
      "step": 126040
    },
    {
      "epoch": 4.201666666666666,
      "grad_norm": 0.5655635595321655,
      "learning_rate": 2.3739583333333336e-05,
      "loss": 0.003,
      "step": 126050
    },
    {
      "epoch": 4.202,
      "grad_norm": 0.009494435973465443,
      "learning_rate": 2.37375e-05,
      "loss": 0.0024,
      "step": 126060
    },
    {
      "epoch": 4.202333333333334,
      "grad_norm": 0.058723319321870804,
      "learning_rate": 2.373541666666667e-05,
      "loss": 0.0026,
      "step": 126070
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.17145368456840515,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0017,
      "step": 126080
    },
    {
      "epoch": 4.203,
      "grad_norm": 0.057438503950834274,
      "learning_rate": 2.373125e-05,
      "loss": 0.0025,
      "step": 126090
    },
    {
      "epoch": 4.203333333333333,
      "grad_norm": 0.11418922245502472,
      "learning_rate": 2.372916666666667e-05,
      "loss": 0.0018,
      "step": 126100
    },
    {
      "epoch": 4.203666666666667,
      "grad_norm": 0.08581175655126572,
      "learning_rate": 2.3727083333333332e-05,
      "loss": 0.0017,
      "step": 126110
    },
    {
      "epoch": 4.204,
      "grad_norm": 0.05801359564065933,
      "learning_rate": 2.3725e-05,
      "loss": 0.0017,
      "step": 126120
    },
    {
      "epoch": 4.2043333333333335,
      "grad_norm": 0.11403219401836395,
      "learning_rate": 2.3722916666666666e-05,
      "loss": 0.0026,
      "step": 126130
    },
    {
      "epoch": 4.204666666666666,
      "grad_norm": 0.19986480474472046,
      "learning_rate": 2.3720833333333335e-05,
      "loss": 0.0027,
      "step": 126140
    },
    {
      "epoch": 4.205,
      "grad_norm": 0.1436549723148346,
      "learning_rate": 2.371875e-05,
      "loss": 0.0018,
      "step": 126150
    },
    {
      "epoch": 4.205333333333333,
      "grad_norm": 0.2289896458387375,
      "learning_rate": 2.3716666666666666e-05,
      "loss": 0.0018,
      "step": 126160
    },
    {
      "epoch": 4.205666666666667,
      "grad_norm": 0.08612702786922455,
      "learning_rate": 2.3714583333333335e-05,
      "loss": 0.0019,
      "step": 126170
    },
    {
      "epoch": 4.206,
      "grad_norm": 0.427252858877182,
      "learning_rate": 2.37125e-05,
      "loss": 0.002,
      "step": 126180
    },
    {
      "epoch": 4.206333333333333,
      "grad_norm": 0.34394586086273193,
      "learning_rate": 2.371041666666667e-05,
      "loss": 0.0021,
      "step": 126190
    },
    {
      "epoch": 4.206666666666667,
      "grad_norm": 0.029665682464838028,
      "learning_rate": 2.3708333333333335e-05,
      "loss": 0.0028,
      "step": 126200
    },
    {
      "epoch": 4.207,
      "grad_norm": 0.057118941098451614,
      "learning_rate": 2.3706250000000004e-05,
      "loss": 0.0022,
      "step": 126210
    },
    {
      "epoch": 4.207333333333334,
      "grad_norm": 0.006992911454290152,
      "learning_rate": 2.370416666666667e-05,
      "loss": 0.0022,
      "step": 126220
    },
    {
      "epoch": 4.207666666666666,
      "grad_norm": 0.31436291337013245,
      "learning_rate": 2.3702083333333335e-05,
      "loss": 0.0031,
      "step": 126230
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.05676918476819992,
      "learning_rate": 2.37e-05,
      "loss": 0.0016,
      "step": 126240
    },
    {
      "epoch": 4.208333333333333,
      "grad_norm": 0.2861465811729431,
      "learning_rate": 2.3697916666666666e-05,
      "loss": 0.0025,
      "step": 126250
    },
    {
      "epoch": 4.208666666666667,
      "grad_norm": 0.4644469916820526,
      "learning_rate": 2.3695833333333334e-05,
      "loss": 0.0027,
      "step": 126260
    },
    {
      "epoch": 4.209,
      "grad_norm": 0.07413879781961441,
      "learning_rate": 2.369375e-05,
      "loss": 0.0027,
      "step": 126270
    },
    {
      "epoch": 4.209333333333333,
      "grad_norm": 0.03227788209915161,
      "learning_rate": 2.369166666666667e-05,
      "loss": 0.0025,
      "step": 126280
    },
    {
      "epoch": 4.209666666666667,
      "grad_norm": 0.006415925454348326,
      "learning_rate": 2.3689583333333334e-05,
      "loss": 0.0028,
      "step": 126290
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.03050023876130581,
      "learning_rate": 2.36875e-05,
      "loss": 0.0021,
      "step": 126300
    },
    {
      "epoch": 4.210333333333334,
      "grad_norm": 0.3421662747859955,
      "learning_rate": 2.368541666666667e-05,
      "loss": 0.0031,
      "step": 126310
    },
    {
      "epoch": 4.210666666666667,
      "grad_norm": 0.1161157637834549,
      "learning_rate": 2.3683333333333334e-05,
      "loss": 0.002,
      "step": 126320
    },
    {
      "epoch": 4.211,
      "grad_norm": 0.028694787994027138,
      "learning_rate": 2.3681250000000003e-05,
      "loss": 0.0016,
      "step": 126330
    },
    {
      "epoch": 4.211333333333333,
      "grad_norm": 0.34247490763664246,
      "learning_rate": 2.367916666666667e-05,
      "loss": 0.0027,
      "step": 126340
    },
    {
      "epoch": 4.211666666666667,
      "grad_norm": 0.2002089023590088,
      "learning_rate": 2.3677083333333337e-05,
      "loss": 0.0018,
      "step": 126350
    },
    {
      "epoch": 4.212,
      "grad_norm": 0.08599326014518738,
      "learning_rate": 2.3675e-05,
      "loss": 0.0027,
      "step": 126360
    },
    {
      "epoch": 4.2123333333333335,
      "grad_norm": 0.11591248214244843,
      "learning_rate": 2.367291666666667e-05,
      "loss": 0.0029,
      "step": 126370
    },
    {
      "epoch": 4.212666666666666,
      "grad_norm": 0.058563753962516785,
      "learning_rate": 2.3670833333333334e-05,
      "loss": 0.0024,
      "step": 126380
    },
    {
      "epoch": 4.213,
      "grad_norm": 0.05732492357492447,
      "learning_rate": 2.366875e-05,
      "loss": 0.0022,
      "step": 126390
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.1789635419845581,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0027,
      "step": 126400
    },
    {
      "epoch": 4.213666666666667,
      "grad_norm": 0.03440862149000168,
      "learning_rate": 2.3664583333333334e-05,
      "loss": 0.0024,
      "step": 126410
    },
    {
      "epoch": 4.214,
      "grad_norm": 0.17337802052497864,
      "learning_rate": 2.3662500000000003e-05,
      "loss": 0.0029,
      "step": 126420
    },
    {
      "epoch": 4.214333333333333,
      "grad_norm": 0.20034322142601013,
      "learning_rate": 2.3660416666666668e-05,
      "loss": 0.0022,
      "step": 126430
    },
    {
      "epoch": 4.214666666666667,
      "grad_norm": 0.34244242310523987,
      "learning_rate": 2.3658333333333334e-05,
      "loss": 0.0016,
      "step": 126440
    },
    {
      "epoch": 4.215,
      "grad_norm": 0.2857661545276642,
      "learning_rate": 2.3656250000000002e-05,
      "loss": 0.0017,
      "step": 126450
    },
    {
      "epoch": 4.215333333333334,
      "grad_norm": 0.08630751073360443,
      "learning_rate": 2.3654166666666668e-05,
      "loss": 0.0016,
      "step": 126460
    },
    {
      "epoch": 4.2156666666666665,
      "grad_norm": 0.08585378527641296,
      "learning_rate": 2.3652083333333337e-05,
      "loss": 0.0026,
      "step": 126470
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.20201389491558075,
      "learning_rate": 2.365e-05,
      "loss": 0.0017,
      "step": 126480
    },
    {
      "epoch": 4.216333333333333,
      "grad_norm": 0.11410322040319443,
      "learning_rate": 2.3647916666666668e-05,
      "loss": 0.0027,
      "step": 126490
    },
    {
      "epoch": 4.216666666666667,
      "grad_norm": 0.25844547152519226,
      "learning_rate": 2.3645833333333333e-05,
      "loss": 0.0021,
      "step": 126500
    },
    {
      "epoch": 4.217,
      "grad_norm": 0.45680880546569824,
      "learning_rate": 2.3643750000000002e-05,
      "loss": 0.002,
      "step": 126510
    },
    {
      "epoch": 4.217333333333333,
      "grad_norm": 0.6601148843765259,
      "learning_rate": 2.3641666666666668e-05,
      "loss": 0.003,
      "step": 126520
    },
    {
      "epoch": 4.217666666666666,
      "grad_norm": 0.0858321264386177,
      "learning_rate": 2.3639583333333333e-05,
      "loss": 0.0021,
      "step": 126530
    },
    {
      "epoch": 4.218,
      "grad_norm": 0.06358038634061813,
      "learning_rate": 2.3637500000000002e-05,
      "loss": 0.0026,
      "step": 126540
    },
    {
      "epoch": 4.218333333333334,
      "grad_norm": 0.2031046450138092,
      "learning_rate": 2.3635416666666667e-05,
      "loss": 0.002,
      "step": 126550
    },
    {
      "epoch": 4.218666666666667,
      "grad_norm": 0.1428280621767044,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0026,
      "step": 126560
    },
    {
      "epoch": 4.219,
      "grad_norm": 0.1713382452726364,
      "learning_rate": 2.3631250000000002e-05,
      "loss": 0.0031,
      "step": 126570
    },
    {
      "epoch": 4.219333333333333,
      "grad_norm": 0.25719019770622253,
      "learning_rate": 2.3629166666666667e-05,
      "loss": 0.0017,
      "step": 126580
    },
    {
      "epoch": 4.219666666666667,
      "grad_norm": 0.42820850014686584,
      "learning_rate": 2.3627083333333336e-05,
      "loss": 0.0023,
      "step": 126590
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.08611857891082764,
      "learning_rate": 2.3624999999999998e-05,
      "loss": 0.0019,
      "step": 126600
    },
    {
      "epoch": 4.2203333333333335,
      "grad_norm": 0.0857013389468193,
      "learning_rate": 2.3622916666666667e-05,
      "loss": 0.0014,
      "step": 126610
    },
    {
      "epoch": 4.220666666666666,
      "grad_norm": 0.0860355943441391,
      "learning_rate": 2.3620833333333333e-05,
      "loss": 0.0019,
      "step": 126620
    },
    {
      "epoch": 4.221,
      "grad_norm": 0.3141880929470062,
      "learning_rate": 2.361875e-05,
      "loss": 0.002,
      "step": 126630
    },
    {
      "epoch": 4.221333333333333,
      "grad_norm": 0.4280144274234772,
      "learning_rate": 2.3616666666666667e-05,
      "loss": 0.0023,
      "step": 126640
    },
    {
      "epoch": 4.221666666666667,
      "grad_norm": 0.34195637702941895,
      "learning_rate": 2.3614583333333336e-05,
      "loss": 0.0028,
      "step": 126650
    },
    {
      "epoch": 4.222,
      "grad_norm": 0.22846809029579163,
      "learning_rate": 2.36125e-05,
      "loss": 0.002,
      "step": 126660
    },
    {
      "epoch": 4.222333333333333,
      "grad_norm": 0.05719036981463432,
      "learning_rate": 2.3610416666666667e-05,
      "loss": 0.002,
      "step": 126670
    },
    {
      "epoch": 4.222666666666667,
      "grad_norm": 0.02914934977889061,
      "learning_rate": 2.3608333333333336e-05,
      "loss": 0.0022,
      "step": 126680
    },
    {
      "epoch": 4.223,
      "grad_norm": 0.010367922484874725,
      "learning_rate": 2.360625e-05,
      "loss": 0.0022,
      "step": 126690
    },
    {
      "epoch": 4.223333333333334,
      "grad_norm": 0.06280308216810226,
      "learning_rate": 2.360416666666667e-05,
      "loss": 0.003,
      "step": 126700
    },
    {
      "epoch": 4.2236666666666665,
      "grad_norm": 0.43289655447006226,
      "learning_rate": 2.3602083333333336e-05,
      "loss": 0.0019,
      "step": 126710
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.22836002707481384,
      "learning_rate": 2.36e-05,
      "loss": 0.0021,
      "step": 126720
    },
    {
      "epoch": 4.224333333333333,
      "grad_norm": 0.14236663281917572,
      "learning_rate": 2.3597916666666667e-05,
      "loss": 0.0016,
      "step": 126730
    },
    {
      "epoch": 4.224666666666667,
      "grad_norm": 0.08597593009471893,
      "learning_rate": 2.3595833333333332e-05,
      "loss": 0.0023,
      "step": 126740
    },
    {
      "epoch": 4.225,
      "grad_norm": 0.029348790645599365,
      "learning_rate": 2.359375e-05,
      "loss": 0.0025,
      "step": 126750
    },
    {
      "epoch": 4.225333333333333,
      "grad_norm": 0.3139743506908417,
      "learning_rate": 2.3591666666666666e-05,
      "loss": 0.0019,
      "step": 126760
    },
    {
      "epoch": 4.225666666666666,
      "grad_norm": 0.3705873489379883,
      "learning_rate": 2.3589583333333335e-05,
      "loss": 0.0017,
      "step": 126770
    },
    {
      "epoch": 4.226,
      "grad_norm": 0.039920974522829056,
      "learning_rate": 2.35875e-05,
      "loss": 0.0025,
      "step": 126780
    },
    {
      "epoch": 4.226333333333334,
      "grad_norm": 0.0036291605792939663,
      "learning_rate": 2.358541666666667e-05,
      "loss": 0.002,
      "step": 126790
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.08800382167100906,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.0015,
      "step": 126800
    },
    {
      "epoch": 4.227,
      "grad_norm": 0.19948376715183258,
      "learning_rate": 2.358125e-05,
      "loss": 0.0022,
      "step": 126810
    },
    {
      "epoch": 4.227333333333333,
      "grad_norm": 0.22840307652950287,
      "learning_rate": 2.357916666666667e-05,
      "loss": 0.0013,
      "step": 126820
    },
    {
      "epoch": 4.227666666666667,
      "grad_norm": 0.2569500803947449,
      "learning_rate": 2.3577083333333335e-05,
      "loss": 0.0023,
      "step": 126830
    },
    {
      "epoch": 4.228,
      "grad_norm": 0.14243824779987335,
      "learning_rate": 2.3575e-05,
      "loss": 0.0016,
      "step": 126840
    },
    {
      "epoch": 4.2283333333333335,
      "grad_norm": 0.17307762801647186,
      "learning_rate": 2.3572916666666666e-05,
      "loss": 0.0026,
      "step": 126850
    },
    {
      "epoch": 4.228666666666666,
      "grad_norm": 0.3994792401790619,
      "learning_rate": 2.3570833333333335e-05,
      "loss": 0.0016,
      "step": 126860
    },
    {
      "epoch": 4.229,
      "grad_norm": 0.02867858111858368,
      "learning_rate": 2.356875e-05,
      "loss": 0.0015,
      "step": 126870
    },
    {
      "epoch": 4.229333333333333,
      "grad_norm": 0.006162385456264019,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0022,
      "step": 126880
    },
    {
      "epoch": 4.229666666666667,
      "grad_norm": 0.029310541227459908,
      "learning_rate": 2.3564583333333335e-05,
      "loss": 0.0032,
      "step": 126890
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.05746443569660187,
      "learning_rate": 2.35625e-05,
      "loss": 0.0022,
      "step": 126900
    },
    {
      "epoch": 4.230333333333333,
      "grad_norm": 0.02889869548380375,
      "learning_rate": 2.356041666666667e-05,
      "loss": 0.0016,
      "step": 126910
    },
    {
      "epoch": 4.230666666666667,
      "grad_norm": 0.057912129908800125,
      "learning_rate": 2.3558333333333334e-05,
      "loss": 0.0015,
      "step": 126920
    },
    {
      "epoch": 4.231,
      "grad_norm": 0.14588843286037445,
      "learning_rate": 2.3556250000000003e-05,
      "loss": 0.0021,
      "step": 126930
    },
    {
      "epoch": 4.231333333333334,
      "grad_norm": 0.257053941488266,
      "learning_rate": 2.355416666666667e-05,
      "loss": 0.002,
      "step": 126940
    },
    {
      "epoch": 4.2316666666666665,
      "grad_norm": 0.2000410109758377,
      "learning_rate": 2.3552083333333334e-05,
      "loss": 0.0021,
      "step": 126950
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.02987433783710003,
      "learning_rate": 2.355e-05,
      "loss": 0.003,
      "step": 126960
    },
    {
      "epoch": 4.232333333333333,
      "grad_norm": 0.029400251805782318,
      "learning_rate": 2.3547916666666665e-05,
      "loss": 0.0026,
      "step": 126970
    },
    {
      "epoch": 4.232666666666667,
      "grad_norm": 0.17115463316440582,
      "learning_rate": 2.3545833333333334e-05,
      "loss": 0.0025,
      "step": 126980
    },
    {
      "epoch": 4.233,
      "grad_norm": 0.19965779781341553,
      "learning_rate": 2.354375e-05,
      "loss": 0.0014,
      "step": 126990
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 0.0074583315290510654,
      "learning_rate": 2.354166666666667e-05,
      "loss": 0.0025,
      "step": 127000
    },
    {
      "epoch": 4.233666666666666,
      "grad_norm": 0.14277705550193787,
      "learning_rate": 2.3539583333333334e-05,
      "loss": 0.0016,
      "step": 127010
    },
    {
      "epoch": 4.234,
      "grad_norm": 0.059898048639297485,
      "learning_rate": 2.35375e-05,
      "loss": 0.0029,
      "step": 127020
    },
    {
      "epoch": 4.234333333333334,
      "grad_norm": 0.19984547793865204,
      "learning_rate": 2.353541666666667e-05,
      "loss": 0.0026,
      "step": 127030
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.25692424178123474,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0018,
      "step": 127040
    },
    {
      "epoch": 4.235,
      "grad_norm": 0.1713155061006546,
      "learning_rate": 2.3531250000000003e-05,
      "loss": 0.0024,
      "step": 127050
    },
    {
      "epoch": 4.235333333333333,
      "grad_norm": 0.05739642679691315,
      "learning_rate": 2.3529166666666668e-05,
      "loss": 0.0026,
      "step": 127060
    },
    {
      "epoch": 4.235666666666667,
      "grad_norm": 0.34243088960647583,
      "learning_rate": 2.3527083333333337e-05,
      "loss": 0.002,
      "step": 127070
    },
    {
      "epoch": 4.236,
      "grad_norm": 0.4849097728729248,
      "learning_rate": 2.3525e-05,
      "loss": 0.0021,
      "step": 127080
    },
    {
      "epoch": 4.2363333333333335,
      "grad_norm": 0.20014195144176483,
      "learning_rate": 2.3522916666666668e-05,
      "loss": 0.003,
      "step": 127090
    },
    {
      "epoch": 4.236666666666666,
      "grad_norm": 0.11438640207052231,
      "learning_rate": 2.3520833333333334e-05,
      "loss": 0.0014,
      "step": 127100
    },
    {
      "epoch": 4.237,
      "grad_norm": 0.0333438515663147,
      "learning_rate": 2.351875e-05,
      "loss": 0.0018,
      "step": 127110
    },
    {
      "epoch": 4.237333333333333,
      "grad_norm": 0.11458007246255875,
      "learning_rate": 2.3516666666666668e-05,
      "loss": 0.0027,
      "step": 127120
    },
    {
      "epoch": 4.237666666666667,
      "grad_norm": 0.45719432830810547,
      "learning_rate": 2.3514583333333333e-05,
      "loss": 0.0024,
      "step": 127130
    },
    {
      "epoch": 4.2379999999999995,
      "grad_norm": 0.19944578409194946,
      "learning_rate": 2.3512500000000002e-05,
      "loss": 0.002,
      "step": 127140
    },
    {
      "epoch": 4.238333333333333,
      "grad_norm": 0.17134331166744232,
      "learning_rate": 2.3510416666666668e-05,
      "loss": 0.0018,
      "step": 127150
    },
    {
      "epoch": 4.238666666666667,
      "grad_norm": 0.0859467163681984,
      "learning_rate": 2.3508333333333337e-05,
      "loss": 0.0031,
      "step": 127160
    },
    {
      "epoch": 4.239,
      "grad_norm": 0.5022223591804504,
      "learning_rate": 2.3506250000000002e-05,
      "loss": 0.0022,
      "step": 127170
    },
    {
      "epoch": 4.239333333333334,
      "grad_norm": 0.508352518081665,
      "learning_rate": 2.3504166666666668e-05,
      "loss": 0.003,
      "step": 127180
    },
    {
      "epoch": 4.2396666666666665,
      "grad_norm": 0.1996953934431076,
      "learning_rate": 2.3502083333333337e-05,
      "loss": 0.0027,
      "step": 127190
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.02987230010330677,
      "learning_rate": 2.35e-05,
      "loss": 0.0019,
      "step": 127200
    },
    {
      "epoch": 4.240333333333333,
      "grad_norm": 0.34248286485671997,
      "learning_rate": 2.3497916666666667e-05,
      "loss": 0.002,
      "step": 127210
    },
    {
      "epoch": 4.240666666666667,
      "grad_norm": 0.22843532264232635,
      "learning_rate": 2.3495833333333333e-05,
      "loss": 0.0015,
      "step": 127220
    },
    {
      "epoch": 4.241,
      "grad_norm": 0.17124730348587036,
      "learning_rate": 2.3493750000000002e-05,
      "loss": 0.0022,
      "step": 127230
    },
    {
      "epoch": 4.241333333333333,
      "grad_norm": 0.05813625827431679,
      "learning_rate": 2.3491666666666667e-05,
      "loss": 0.0018,
      "step": 127240
    },
    {
      "epoch": 4.241666666666666,
      "grad_norm": 0.0858166292309761,
      "learning_rate": 2.3489583333333333e-05,
      "loss": 0.0017,
      "step": 127250
    },
    {
      "epoch": 4.242,
      "grad_norm": 0.20009002089500427,
      "learning_rate": 2.3487500000000002e-05,
      "loss": 0.0023,
      "step": 127260
    },
    {
      "epoch": 4.242333333333334,
      "grad_norm": 0.7631351351737976,
      "learning_rate": 2.3485416666666667e-05,
      "loss": 0.0024,
      "step": 127270
    },
    {
      "epoch": 4.242666666666667,
      "grad_norm": 0.3148365616798401,
      "learning_rate": 2.3483333333333336e-05,
      "loss": 0.0026,
      "step": 127280
    },
    {
      "epoch": 4.243,
      "grad_norm": 0.011359126307070255,
      "learning_rate": 2.348125e-05,
      "loss": 0.0019,
      "step": 127290
    },
    {
      "epoch": 4.243333333333333,
      "grad_norm": 0.1712765395641327,
      "learning_rate": 2.347916666666667e-05,
      "loss": 0.0014,
      "step": 127300
    },
    {
      "epoch": 4.243666666666667,
      "grad_norm": 0.22798709571361542,
      "learning_rate": 2.3477083333333336e-05,
      "loss": 0.0017,
      "step": 127310
    },
    {
      "epoch": 4.244,
      "grad_norm": 0.3711598217487335,
      "learning_rate": 2.3475e-05,
      "loss": 0.0025,
      "step": 127320
    },
    {
      "epoch": 4.2443333333333335,
      "grad_norm": 0.39924356341362,
      "learning_rate": 2.3472916666666667e-05,
      "loss": 0.0027,
      "step": 127330
    },
    {
      "epoch": 4.244666666666666,
      "grad_norm": 0.058163031935691833,
      "learning_rate": 2.3470833333333332e-05,
      "loss": 0.0022,
      "step": 127340
    },
    {
      "epoch": 4.245,
      "grad_norm": 0.07549229264259338,
      "learning_rate": 2.346875e-05,
      "loss": 0.0025,
      "step": 127350
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.11447451263666153,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0022,
      "step": 127360
    },
    {
      "epoch": 4.245666666666667,
      "grad_norm": 0.28534430265426636,
      "learning_rate": 2.3464583333333336e-05,
      "loss": 0.0022,
      "step": 127370
    },
    {
      "epoch": 4.246,
      "grad_norm": 0.1999967396259308,
      "learning_rate": 2.34625e-05,
      "loss": 0.0025,
      "step": 127380
    },
    {
      "epoch": 4.246333333333333,
      "grad_norm": 0.37109556794166565,
      "learning_rate": 2.3460416666666667e-05,
      "loss": 0.0026,
      "step": 127390
    },
    {
      "epoch": 4.246666666666667,
      "grad_norm": 0.6939538717269897,
      "learning_rate": 2.3458333333333335e-05,
      "loss": 0.0021,
      "step": 127400
    },
    {
      "epoch": 4.247,
      "grad_norm": 0.22948728501796722,
      "learning_rate": 2.345625e-05,
      "loss": 0.0019,
      "step": 127410
    },
    {
      "epoch": 4.247333333333334,
      "grad_norm": 0.3412631154060364,
      "learning_rate": 2.345416666666667e-05,
      "loss": 0.0017,
      "step": 127420
    },
    {
      "epoch": 4.2476666666666665,
      "grad_norm": 0.1712622344493866,
      "learning_rate": 2.3452083333333335e-05,
      "loss": 0.0026,
      "step": 127430
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.11446060985326767,
      "learning_rate": 2.345e-05,
      "loss": 0.0017,
      "step": 127440
    },
    {
      "epoch": 4.248333333333333,
      "grad_norm": 0.20677649974822998,
      "learning_rate": 2.3447916666666666e-05,
      "loss": 0.0026,
      "step": 127450
    },
    {
      "epoch": 4.248666666666667,
      "grad_norm": 0.3433752954006195,
      "learning_rate": 2.3445833333333335e-05,
      "loss": 0.0022,
      "step": 127460
    },
    {
      "epoch": 4.249,
      "grad_norm": 0.2854859530925751,
      "learning_rate": 2.344375e-05,
      "loss": 0.0028,
      "step": 127470
    },
    {
      "epoch": 4.249333333333333,
      "grad_norm": 0.399689644575119,
      "learning_rate": 2.3441666666666666e-05,
      "loss": 0.0019,
      "step": 127480
    },
    {
      "epoch": 4.249666666666666,
      "grad_norm": 0.02990679070353508,
      "learning_rate": 2.3439583333333335e-05,
      "loss": 0.0026,
      "step": 127490
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.17222703993320465,
      "learning_rate": 2.34375e-05,
      "loss": 0.0021,
      "step": 127500
    },
    {
      "epoch": 4.250333333333334,
      "grad_norm": 0.19999438524246216,
      "learning_rate": 2.343541666666667e-05,
      "loss": 0.0015,
      "step": 127510
    },
    {
      "epoch": 4.250666666666667,
      "grad_norm": 0.08556079864501953,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.0018,
      "step": 127520
    },
    {
      "epoch": 4.251,
      "grad_norm": 0.011391707696020603,
      "learning_rate": 2.343125e-05,
      "loss": 0.0024,
      "step": 127530
    },
    {
      "epoch": 4.251333333333333,
      "grad_norm": 0.20489928126335144,
      "learning_rate": 2.342916666666667e-05,
      "loss": 0.0022,
      "step": 127540
    },
    {
      "epoch": 4.251666666666667,
      "grad_norm": 0.11474258452653885,
      "learning_rate": 2.3427083333333335e-05,
      "loss": 0.0015,
      "step": 127550
    },
    {
      "epoch": 4.252,
      "grad_norm": 0.14286433160305023,
      "learning_rate": 2.3425000000000004e-05,
      "loss": 0.0028,
      "step": 127560
    },
    {
      "epoch": 4.2523333333333335,
      "grad_norm": 0.08961456269025803,
      "learning_rate": 2.3422916666666666e-05,
      "loss": 0.0022,
      "step": 127570
    },
    {
      "epoch": 4.252666666666666,
      "grad_norm": 0.4853459894657135,
      "learning_rate": 2.3420833333333335e-05,
      "loss": 0.0027,
      "step": 127580
    },
    {
      "epoch": 4.253,
      "grad_norm": 0.05257173627614975,
      "learning_rate": 2.341875e-05,
      "loss": 0.0013,
      "step": 127590
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.1141844317317009,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.0022,
      "step": 127600
    },
    {
      "epoch": 4.253666666666667,
      "grad_norm": 0.11442717164754868,
      "learning_rate": 2.3414583333333334e-05,
      "loss": 0.0021,
      "step": 127610
    },
    {
      "epoch": 4.254,
      "grad_norm": 0.1713973432779312,
      "learning_rate": 2.34125e-05,
      "loss": 0.0023,
      "step": 127620
    },
    {
      "epoch": 4.254333333333333,
      "grad_norm": 0.3149920403957367,
      "learning_rate": 2.341041666666667e-05,
      "loss": 0.0021,
      "step": 127630
    },
    {
      "epoch": 4.254666666666667,
      "grad_norm": 0.29056042432785034,
      "learning_rate": 2.3408333333333334e-05,
      "loss": 0.0023,
      "step": 127640
    },
    {
      "epoch": 4.255,
      "grad_norm": 0.200022354722023,
      "learning_rate": 2.3406250000000003e-05,
      "loss": 0.0018,
      "step": 127650
    },
    {
      "epoch": 4.255333333333334,
      "grad_norm": 0.37085020542144775,
      "learning_rate": 2.340416666666667e-05,
      "loss": 0.0016,
      "step": 127660
    },
    {
      "epoch": 4.2556666666666665,
      "grad_norm": 0.5191182494163513,
      "learning_rate": 2.3402083333333334e-05,
      "loss": 0.0026,
      "step": 127670
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.09224285930395126,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0025,
      "step": 127680
    },
    {
      "epoch": 4.256333333333333,
      "grad_norm": 0.11428775638341904,
      "learning_rate": 2.3397916666666665e-05,
      "loss": 0.0028,
      "step": 127690
    },
    {
      "epoch": 4.256666666666667,
      "grad_norm": 0.5706205368041992,
      "learning_rate": 2.3395833333333334e-05,
      "loss": 0.002,
      "step": 127700
    },
    {
      "epoch": 4.257,
      "grad_norm": 0.17135420441627502,
      "learning_rate": 2.339375e-05,
      "loss": 0.0029,
      "step": 127710
    },
    {
      "epoch": 4.257333333333333,
      "grad_norm": 0.029200613498687744,
      "learning_rate": 2.3391666666666668e-05,
      "loss": 0.0016,
      "step": 127720
    },
    {
      "epoch": 4.257666666666666,
      "grad_norm": 0.11409865319728851,
      "learning_rate": 2.3389583333333334e-05,
      "loss": 0.0017,
      "step": 127730
    },
    {
      "epoch": 4.258,
      "grad_norm": 0.03012380190193653,
      "learning_rate": 2.3387500000000003e-05,
      "loss": 0.0017,
      "step": 127740
    },
    {
      "epoch": 4.258333333333334,
      "grad_norm": 0.08564382046461105,
      "learning_rate": 2.3385416666666668e-05,
      "loss": 0.0021,
      "step": 127750
    },
    {
      "epoch": 4.258666666666667,
      "grad_norm": 0.3711622655391693,
      "learning_rate": 2.3383333333333334e-05,
      "loss": 0.0017,
      "step": 127760
    },
    {
      "epoch": 4.259,
      "grad_norm": 0.004440980963408947,
      "learning_rate": 2.3381250000000003e-05,
      "loss": 0.0026,
      "step": 127770
    },
    {
      "epoch": 4.259333333333333,
      "grad_norm": 0.11402085423469543,
      "learning_rate": 2.3379166666666668e-05,
      "loss": 0.0017,
      "step": 127780
    },
    {
      "epoch": 4.259666666666667,
      "grad_norm": 0.2517675459384918,
      "learning_rate": 2.3377083333333337e-05,
      "loss": 0.0024,
      "step": 127790
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.11490678787231445,
      "learning_rate": 2.3375000000000002e-05,
      "loss": 0.0016,
      "step": 127800
    },
    {
      "epoch": 4.2603333333333335,
      "grad_norm": 0.11555028706789017,
      "learning_rate": 2.3372916666666668e-05,
      "loss": 0.0019,
      "step": 127810
    },
    {
      "epoch": 4.260666666666666,
      "grad_norm": 0.05788403004407883,
      "learning_rate": 2.3370833333333333e-05,
      "loss": 0.0017,
      "step": 127820
    },
    {
      "epoch": 4.261,
      "grad_norm": 0.17327280342578888,
      "learning_rate": 2.336875e-05,
      "loss": 0.0026,
      "step": 127830
    },
    {
      "epoch": 4.261333333333333,
      "grad_norm": 0.11442704498767853,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0018,
      "step": 127840
    },
    {
      "epoch": 4.261666666666667,
      "grad_norm": 0.05775992572307587,
      "learning_rate": 2.3364583333333333e-05,
      "loss": 0.002,
      "step": 127850
    },
    {
      "epoch": 4.2620000000000005,
      "grad_norm": 0.17136558890342712,
      "learning_rate": 2.3362500000000002e-05,
      "loss": 0.0023,
      "step": 127860
    },
    {
      "epoch": 4.262333333333333,
      "grad_norm": 0.3997199535369873,
      "learning_rate": 2.3360416666666668e-05,
      "loss": 0.0026,
      "step": 127870
    },
    {
      "epoch": 4.262666666666667,
      "grad_norm": 0.45629245042800903,
      "learning_rate": 2.3358333333333336e-05,
      "loss": 0.0026,
      "step": 127880
    },
    {
      "epoch": 4.263,
      "grad_norm": 0.20032109320163727,
      "learning_rate": 2.3356250000000002e-05,
      "loss": 0.0026,
      "step": 127890
    },
    {
      "epoch": 4.263333333333334,
      "grad_norm": 0.08554572612047195,
      "learning_rate": 2.3354166666666667e-05,
      "loss": 0.0023,
      "step": 127900
    },
    {
      "epoch": 4.2636666666666665,
      "grad_norm": 0.2284201830625534,
      "learning_rate": 2.3352083333333336e-05,
      "loss": 0.0023,
      "step": 127910
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.0055931201204657555,
      "learning_rate": 2.3350000000000002e-05,
      "loss": 0.0017,
      "step": 127920
    },
    {
      "epoch": 4.264333333333333,
      "grad_norm": 0.05864088609814644,
      "learning_rate": 2.3347916666666667e-05,
      "loss": 0.0017,
      "step": 127930
    },
    {
      "epoch": 4.264666666666667,
      "grad_norm": 0.0036676335148513317,
      "learning_rate": 2.3345833333333333e-05,
      "loss": 0.0018,
      "step": 127940
    },
    {
      "epoch": 4.265,
      "grad_norm": 0.15323691070079803,
      "learning_rate": 2.334375e-05,
      "loss": 0.0025,
      "step": 127950
    },
    {
      "epoch": 4.265333333333333,
      "grad_norm": 0.041151273995637894,
      "learning_rate": 2.3341666666666667e-05,
      "loss": 0.0022,
      "step": 127960
    },
    {
      "epoch": 4.265666666666666,
      "grad_norm": 0.17104396224021912,
      "learning_rate": 2.3339583333333333e-05,
      "loss": 0.0019,
      "step": 127970
    },
    {
      "epoch": 4.266,
      "grad_norm": 0.030336374416947365,
      "learning_rate": 2.33375e-05,
      "loss": 0.002,
      "step": 127980
    },
    {
      "epoch": 4.266333333333334,
      "grad_norm": 0.3138287365436554,
      "learning_rate": 2.3335416666666667e-05,
      "loss": 0.0018,
      "step": 127990
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.5705559253692627,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0016,
      "step": 128000
    },
    {
      "epoch": 4.267,
      "grad_norm": 0.17107653617858887,
      "learning_rate": 2.333125e-05,
      "loss": 0.0013,
      "step": 128010
    },
    {
      "epoch": 4.267333333333333,
      "grad_norm": 0.05711326748132706,
      "learning_rate": 2.332916666666667e-05,
      "loss": 0.0015,
      "step": 128020
    },
    {
      "epoch": 4.267666666666667,
      "grad_norm": 0.2015351802110672,
      "learning_rate": 2.3327083333333336e-05,
      "loss": 0.0029,
      "step": 128030
    },
    {
      "epoch": 4.268,
      "grad_norm": 0.029408929869532585,
      "learning_rate": 2.3325e-05,
      "loss": 0.0022,
      "step": 128040
    },
    {
      "epoch": 4.2683333333333335,
      "grad_norm": 0.1434917449951172,
      "learning_rate": 2.3322916666666667e-05,
      "loss": 0.0016,
      "step": 128050
    },
    {
      "epoch": 4.268666666666666,
      "grad_norm": 0.11835389584302902,
      "learning_rate": 2.3320833333333332e-05,
      "loss": 0.002,
      "step": 128060
    },
    {
      "epoch": 4.269,
      "grad_norm": 0.08599156141281128,
      "learning_rate": 2.331875e-05,
      "loss": 0.002,
      "step": 128070
    },
    {
      "epoch": 4.269333333333333,
      "grad_norm": 0.17318809032440186,
      "learning_rate": 2.3316666666666666e-05,
      "loss": 0.0021,
      "step": 128080
    },
    {
      "epoch": 4.269666666666667,
      "grad_norm": 0.09442342817783356,
      "learning_rate": 2.3314583333333335e-05,
      "loss": 0.0018,
      "step": 128090
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.17163626849651337,
      "learning_rate": 2.33125e-05,
      "loss": 0.0023,
      "step": 128100
    },
    {
      "epoch": 4.270333333333333,
      "grad_norm": 0.02937045320868492,
      "learning_rate": 2.3310416666666666e-05,
      "loss": 0.0024,
      "step": 128110
    },
    {
      "epoch": 4.270666666666667,
      "grad_norm": 0.4850045442581177,
      "learning_rate": 2.3308333333333335e-05,
      "loss": 0.002,
      "step": 128120
    },
    {
      "epoch": 4.271,
      "grad_norm": 0.091288723051548,
      "learning_rate": 2.330625e-05,
      "loss": 0.0021,
      "step": 128130
    },
    {
      "epoch": 4.271333333333334,
      "grad_norm": 0.08605068176984787,
      "learning_rate": 2.330416666666667e-05,
      "loss": 0.0027,
      "step": 128140
    },
    {
      "epoch": 4.2716666666666665,
      "grad_norm": 0.029281634837388992,
      "learning_rate": 2.3302083333333335e-05,
      "loss": 0.0019,
      "step": 128150
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.4566894471645355,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0032,
      "step": 128160
    },
    {
      "epoch": 4.272333333333333,
      "grad_norm": 0.2001355141401291,
      "learning_rate": 2.3297916666666666e-05,
      "loss": 0.002,
      "step": 128170
    },
    {
      "epoch": 4.272666666666667,
      "grad_norm": 0.11442101746797562,
      "learning_rate": 2.3295833333333335e-05,
      "loss": 0.0033,
      "step": 128180
    },
    {
      "epoch": 4.273,
      "grad_norm": 0.05715244263410568,
      "learning_rate": 2.329375e-05,
      "loss": 0.0019,
      "step": 128190
    },
    {
      "epoch": 4.273333333333333,
      "grad_norm": 0.2869175672531128,
      "learning_rate": 2.3291666666666666e-05,
      "loss": 0.0029,
      "step": 128200
    },
    {
      "epoch": 4.273666666666666,
      "grad_norm": 0.02984613925218582,
      "learning_rate": 2.3289583333333335e-05,
      "loss": 0.0019,
      "step": 128210
    },
    {
      "epoch": 4.274,
      "grad_norm": 0.028688041493296623,
      "learning_rate": 2.32875e-05,
      "loss": 0.0019,
      "step": 128220
    },
    {
      "epoch": 4.274333333333333,
      "grad_norm": 0.00813103187829256,
      "learning_rate": 2.328541666666667e-05,
      "loss": 0.0022,
      "step": 128230
    },
    {
      "epoch": 4.274666666666667,
      "grad_norm": 0.007308471016585827,
      "learning_rate": 2.3283333333333335e-05,
      "loss": 0.0016,
      "step": 128240
    },
    {
      "epoch": 4.275,
      "grad_norm": 0.17480579018592834,
      "learning_rate": 2.328125e-05,
      "loss": 0.0018,
      "step": 128250
    },
    {
      "epoch": 4.275333333333333,
      "grad_norm": 0.11572021245956421,
      "learning_rate": 2.327916666666667e-05,
      "loss": 0.0013,
      "step": 128260
    },
    {
      "epoch": 4.275666666666667,
      "grad_norm": 0.34217679500579834,
      "learning_rate": 2.3277083333333334e-05,
      "loss": 0.0019,
      "step": 128270
    },
    {
      "epoch": 4.276,
      "grad_norm": 0.14467722177505493,
      "learning_rate": 2.3275000000000003e-05,
      "loss": 0.0026,
      "step": 128280
    },
    {
      "epoch": 4.2763333333333335,
      "grad_norm": 0.1999349594116211,
      "learning_rate": 2.3272916666666665e-05,
      "loss": 0.0025,
      "step": 128290
    },
    {
      "epoch": 4.276666666666666,
      "grad_norm": 0.028993884101510048,
      "learning_rate": 2.3270833333333334e-05,
      "loss": 0.0023,
      "step": 128300
    },
    {
      "epoch": 4.277,
      "grad_norm": 0.3144386112689972,
      "learning_rate": 2.326875e-05,
      "loss": 0.0023,
      "step": 128310
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.05726184695959091,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0018,
      "step": 128320
    },
    {
      "epoch": 4.277666666666667,
      "grad_norm": 0.05791105329990387,
      "learning_rate": 2.3264583333333334e-05,
      "loss": 0.0013,
      "step": 128330
    },
    {
      "epoch": 4.2780000000000005,
      "grad_norm": 0.03188144043087959,
      "learning_rate": 2.32625e-05,
      "loss": 0.002,
      "step": 128340
    },
    {
      "epoch": 4.278333333333333,
      "grad_norm": 0.20252132415771484,
      "learning_rate": 2.326041666666667e-05,
      "loss": 0.003,
      "step": 128350
    },
    {
      "epoch": 4.278666666666667,
      "grad_norm": 0.057486433535814285,
      "learning_rate": 2.3258333333333334e-05,
      "loss": 0.0017,
      "step": 128360
    },
    {
      "epoch": 4.279,
      "grad_norm": 0.2852206230163574,
      "learning_rate": 2.3256250000000003e-05,
      "loss": 0.0022,
      "step": 128370
    },
    {
      "epoch": 4.279333333333334,
      "grad_norm": 0.1427241712808609,
      "learning_rate": 2.325416666666667e-05,
      "loss": 0.0019,
      "step": 128380
    },
    {
      "epoch": 4.2796666666666665,
      "grad_norm": 0.17137809097766876,
      "learning_rate": 2.3252083333333334e-05,
      "loss": 0.0027,
      "step": 128390
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.3705984652042389,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.0019,
      "step": 128400
    },
    {
      "epoch": 4.280333333333333,
      "grad_norm": 0.28545942902565,
      "learning_rate": 2.3247916666666665e-05,
      "loss": 0.0015,
      "step": 128410
    },
    {
      "epoch": 4.280666666666667,
      "grad_norm": 0.28544414043426514,
      "learning_rate": 2.3245833333333334e-05,
      "loss": 0.0029,
      "step": 128420
    },
    {
      "epoch": 4.281,
      "grad_norm": 0.3137344717979431,
      "learning_rate": 2.324375e-05,
      "loss": 0.0026,
      "step": 128430
    },
    {
      "epoch": 4.281333333333333,
      "grad_norm": 0.17307105660438538,
      "learning_rate": 2.3241666666666668e-05,
      "loss": 0.0025,
      "step": 128440
    },
    {
      "epoch": 4.281666666666666,
      "grad_norm": 0.3423897325992584,
      "learning_rate": 2.3239583333333334e-05,
      "loss": 0.002,
      "step": 128450
    },
    {
      "epoch": 4.282,
      "grad_norm": 0.17108294367790222,
      "learning_rate": 2.3237500000000002e-05,
      "loss": 0.0018,
      "step": 128460
    },
    {
      "epoch": 4.282333333333334,
      "grad_norm": 0.11447147279977798,
      "learning_rate": 2.3235416666666668e-05,
      "loss": 0.0015,
      "step": 128470
    },
    {
      "epoch": 4.282666666666667,
      "grad_norm": 0.555581271648407,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.0037,
      "step": 128480
    },
    {
      "epoch": 4.283,
      "grad_norm": 0.2282167226076126,
      "learning_rate": 2.3231250000000002e-05,
      "loss": 0.0018,
      "step": 128490
    },
    {
      "epoch": 4.283333333333333,
      "grad_norm": 0.2850821316242218,
      "learning_rate": 2.3229166666666668e-05,
      "loss": 0.0017,
      "step": 128500
    },
    {
      "epoch": 4.283666666666667,
      "grad_norm": 0.029028162360191345,
      "learning_rate": 2.3227083333333337e-05,
      "loss": 0.002,
      "step": 128510
    },
    {
      "epoch": 4.284,
      "grad_norm": 0.14463554322719574,
      "learning_rate": 2.3225000000000002e-05,
      "loss": 0.0021,
      "step": 128520
    },
    {
      "epoch": 4.2843333333333335,
      "grad_norm": 0.1178046390414238,
      "learning_rate": 2.3222916666666668e-05,
      "loss": 0.0016,
      "step": 128530
    },
    {
      "epoch": 4.284666666666666,
      "grad_norm": 0.25665590167045593,
      "learning_rate": 2.3220833333333333e-05,
      "loss": 0.0031,
      "step": 128540
    },
    {
      "epoch": 4.285,
      "grad_norm": 0.2852378785610199,
      "learning_rate": 2.321875e-05,
      "loss": 0.0021,
      "step": 128550
    },
    {
      "epoch": 4.285333333333333,
      "grad_norm": 0.17124800384044647,
      "learning_rate": 2.3216666666666667e-05,
      "loss": 0.0023,
      "step": 128560
    },
    {
      "epoch": 4.285666666666667,
      "grad_norm": 0.05775332823395729,
      "learning_rate": 2.3214583333333333e-05,
      "loss": 0.002,
      "step": 128570
    },
    {
      "epoch": 4.286,
      "grad_norm": 0.2570117115974426,
      "learning_rate": 2.3212500000000002e-05,
      "loss": 0.0025,
      "step": 128580
    },
    {
      "epoch": 4.286333333333333,
      "grad_norm": 0.02873115800321102,
      "learning_rate": 2.3210416666666667e-05,
      "loss": 0.0033,
      "step": 128590
    },
    {
      "epoch": 4.286666666666667,
      "grad_norm": 0.3149455189704895,
      "learning_rate": 2.3208333333333336e-05,
      "loss": 0.0024,
      "step": 128600
    },
    {
      "epoch": 4.287,
      "grad_norm": 0.39936670660972595,
      "learning_rate": 2.320625e-05,
      "loss": 0.0028,
      "step": 128610
    },
    {
      "epoch": 4.287333333333334,
      "grad_norm": 0.131282240152359,
      "learning_rate": 2.3204166666666667e-05,
      "loss": 0.0025,
      "step": 128620
    },
    {
      "epoch": 4.2876666666666665,
      "grad_norm": 0.0859849601984024,
      "learning_rate": 2.3202083333333336e-05,
      "loss": 0.0017,
      "step": 128630
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.008670364506542683,
      "learning_rate": 2.32e-05,
      "loss": 0.0012,
      "step": 128640
    },
    {
      "epoch": 4.288333333333333,
      "grad_norm": 0.31373459100723267,
      "learning_rate": 2.3197916666666667e-05,
      "loss": 0.0023,
      "step": 128650
    },
    {
      "epoch": 4.288666666666667,
      "grad_norm": 0.11456180363893509,
      "learning_rate": 2.3195833333333332e-05,
      "loss": 0.0017,
      "step": 128660
    },
    {
      "epoch": 4.289,
      "grad_norm": 0.11434105038642883,
      "learning_rate": 2.319375e-05,
      "loss": 0.0017,
      "step": 128670
    },
    {
      "epoch": 4.289333333333333,
      "grad_norm": 0.19988173246383667,
      "learning_rate": 2.3191666666666667e-05,
      "loss": 0.0014,
      "step": 128680
    },
    {
      "epoch": 4.289666666666666,
      "grad_norm": 0.2568538188934326,
      "learning_rate": 2.3189583333333332e-05,
      "loss": 0.0018,
      "step": 128690
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.17420299351215363,
      "learning_rate": 2.31875e-05,
      "loss": 0.002,
      "step": 128700
    },
    {
      "epoch": 4.290333333333333,
      "grad_norm": 0.1467103362083435,
      "learning_rate": 2.3185416666666667e-05,
      "loss": 0.0027,
      "step": 128710
    },
    {
      "epoch": 4.290666666666667,
      "grad_norm": 0.19980627298355103,
      "learning_rate": 2.3183333333333336e-05,
      "loss": 0.0021,
      "step": 128720
    },
    {
      "epoch": 4.291,
      "grad_norm": 0.11417431384325027,
      "learning_rate": 2.318125e-05,
      "loss": 0.0017,
      "step": 128730
    },
    {
      "epoch": 4.291333333333333,
      "grad_norm": 0.05777834728360176,
      "learning_rate": 2.317916666666667e-05,
      "loss": 0.002,
      "step": 128740
    },
    {
      "epoch": 4.291666666666667,
      "grad_norm": 0.17116011679172516,
      "learning_rate": 2.3177083333333335e-05,
      "loss": 0.0026,
      "step": 128750
    },
    {
      "epoch": 4.292,
      "grad_norm": 0.11453192681074142,
      "learning_rate": 2.3175e-05,
      "loss": 0.0019,
      "step": 128760
    },
    {
      "epoch": 4.292333333333334,
      "grad_norm": 0.2287243753671646,
      "learning_rate": 2.317291666666667e-05,
      "loss": 0.0016,
      "step": 128770
    },
    {
      "epoch": 4.292666666666666,
      "grad_norm": 0.23025687038898468,
      "learning_rate": 2.3170833333333332e-05,
      "loss": 0.0033,
      "step": 128780
    },
    {
      "epoch": 4.293,
      "grad_norm": 0.11448448151350021,
      "learning_rate": 2.316875e-05,
      "loss": 0.0021,
      "step": 128790
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.11746688932180405,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0027,
      "step": 128800
    },
    {
      "epoch": 4.293666666666667,
      "grad_norm": 0.2284730225801468,
      "learning_rate": 2.3164583333333335e-05,
      "loss": 0.0022,
      "step": 128810
    },
    {
      "epoch": 4.294,
      "grad_norm": 0.029766399413347244,
      "learning_rate": 2.31625e-05,
      "loss": 0.002,
      "step": 128820
    },
    {
      "epoch": 4.294333333333333,
      "grad_norm": 0.028730524703860283,
      "learning_rate": 2.316041666666667e-05,
      "loss": 0.0022,
      "step": 128830
    },
    {
      "epoch": 4.294666666666667,
      "grad_norm": 0.39940571784973145,
      "learning_rate": 2.3158333333333335e-05,
      "loss": 0.0023,
      "step": 128840
    },
    {
      "epoch": 4.295,
      "grad_norm": 0.1315910965204239,
      "learning_rate": 2.315625e-05,
      "loss": 0.0019,
      "step": 128850
    },
    {
      "epoch": 4.295333333333334,
      "grad_norm": 0.14279726147651672,
      "learning_rate": 2.315416666666667e-05,
      "loss": 0.0021,
      "step": 128860
    },
    {
      "epoch": 4.2956666666666665,
      "grad_norm": 0.11476363241672516,
      "learning_rate": 2.3152083333333335e-05,
      "loss": 0.0017,
      "step": 128870
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.08731479197740555,
      "learning_rate": 2.3150000000000004e-05,
      "loss": 0.0021,
      "step": 128880
    },
    {
      "epoch": 4.296333333333333,
      "grad_norm": 0.029687505215406418,
      "learning_rate": 2.314791666666667e-05,
      "loss": 0.002,
      "step": 128890
    },
    {
      "epoch": 4.296666666666667,
      "grad_norm": 0.06438853591680527,
      "learning_rate": 2.3145833333333335e-05,
      "loss": 0.0017,
      "step": 128900
    },
    {
      "epoch": 4.297,
      "grad_norm": 0.11418874561786652,
      "learning_rate": 2.314375e-05,
      "loss": 0.0016,
      "step": 128910
    },
    {
      "epoch": 4.2973333333333334,
      "grad_norm": 0.08656921982765198,
      "learning_rate": 2.3141666666666666e-05,
      "loss": 0.0019,
      "step": 128920
    },
    {
      "epoch": 4.297666666666666,
      "grad_norm": 0.05725211650133133,
      "learning_rate": 2.3139583333333334e-05,
      "loss": 0.0026,
      "step": 128930
    },
    {
      "epoch": 4.298,
      "grad_norm": 0.030558615922927856,
      "learning_rate": 2.31375e-05,
      "loss": 0.0022,
      "step": 128940
    },
    {
      "epoch": 4.298333333333334,
      "grad_norm": 0.05183984339237213,
      "learning_rate": 2.313541666666667e-05,
      "loss": 0.0018,
      "step": 128950
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.11396574229001999,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0022,
      "step": 128960
    },
    {
      "epoch": 4.299,
      "grad_norm": 0.2286185473203659,
      "learning_rate": 2.3131250000000003e-05,
      "loss": 0.002,
      "step": 128970
    },
    {
      "epoch": 4.299333333333333,
      "grad_norm": 0.008052438497543335,
      "learning_rate": 2.312916666666667e-05,
      "loss": 0.0024,
      "step": 128980
    },
    {
      "epoch": 4.299666666666667,
      "grad_norm": 0.17142793536186218,
      "learning_rate": 2.3127083333333334e-05,
      "loss": 0.0013,
      "step": 128990
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.3421193063259125,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 0.0021,
      "step": 129000
    },
    {
      "epoch": 4.300333333333334,
      "grad_norm": 0.20012003183364868,
      "learning_rate": 2.312291666666667e-05,
      "loss": 0.0019,
      "step": 129010
    },
    {
      "epoch": 4.300666666666666,
      "grad_norm": 0.4202110469341278,
      "learning_rate": 2.3120833333333334e-05,
      "loss": 0.0025,
      "step": 129020
    },
    {
      "epoch": 4.301,
      "grad_norm": 0.029554255306720734,
      "learning_rate": 2.311875e-05,
      "loss": 0.0019,
      "step": 129030
    },
    {
      "epoch": 4.301333333333333,
      "grad_norm": 0.22866977751255035,
      "learning_rate": 2.311666666666667e-05,
      "loss": 0.0021,
      "step": 129040
    },
    {
      "epoch": 4.301666666666667,
      "grad_norm": 0.058114346116781235,
      "learning_rate": 2.3114583333333334e-05,
      "loss": 0.002,
      "step": 129050
    },
    {
      "epoch": 4.302,
      "grad_norm": 0.38880008459091187,
      "learning_rate": 2.31125e-05,
      "loss": 0.0027,
      "step": 129060
    },
    {
      "epoch": 4.302333333333333,
      "grad_norm": 0.7010537981987,
      "learning_rate": 2.3110416666666668e-05,
      "loss": 0.0023,
      "step": 129070
    },
    {
      "epoch": 4.302666666666667,
      "grad_norm": 0.08941203355789185,
      "learning_rate": 2.3108333333333334e-05,
      "loss": 0.0022,
      "step": 129080
    },
    {
      "epoch": 4.303,
      "grad_norm": 0.2304091900587082,
      "learning_rate": 2.3106250000000003e-05,
      "loss": 0.0016,
      "step": 129090
    },
    {
      "epoch": 4.303333333333334,
      "grad_norm": 0.11426039785146713,
      "learning_rate": 2.3104166666666668e-05,
      "loss": 0.002,
      "step": 129100
    },
    {
      "epoch": 4.3036666666666665,
      "grad_norm": 0.1996830552816391,
      "learning_rate": 2.3102083333333337e-05,
      "loss": 0.0025,
      "step": 129110
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.11460646986961365,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0018,
      "step": 129120
    },
    {
      "epoch": 4.304333333333333,
      "grad_norm": 0.1714015156030655,
      "learning_rate": 2.3097916666666668e-05,
      "loss": 0.0021,
      "step": 129130
    },
    {
      "epoch": 4.304666666666667,
      "grad_norm": 0.05802503973245621,
      "learning_rate": 2.3095833333333333e-05,
      "loss": 0.0018,
      "step": 129140
    },
    {
      "epoch": 4.305,
      "grad_norm": 0.0410970114171505,
      "learning_rate": 2.309375e-05,
      "loss": 0.0015,
      "step": 129150
    },
    {
      "epoch": 4.3053333333333335,
      "grad_norm": 0.08633532375097275,
      "learning_rate": 2.3091666666666668e-05,
      "loss": 0.0015,
      "step": 129160
    },
    {
      "epoch": 4.305666666666666,
      "grad_norm": 0.31424641609191895,
      "learning_rate": 2.3089583333333333e-05,
      "loss": 0.0022,
      "step": 129170
    },
    {
      "epoch": 4.306,
      "grad_norm": 0.03261253982782364,
      "learning_rate": 2.3087500000000002e-05,
      "loss": 0.0023,
      "step": 129180
    },
    {
      "epoch": 4.306333333333333,
      "grad_norm": 0.057519763708114624,
      "learning_rate": 2.3085416666666668e-05,
      "loss": 0.0023,
      "step": 129190
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.14262309670448303,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.0022,
      "step": 129200
    },
    {
      "epoch": 4.307,
      "grad_norm": 0.06842634826898575,
      "learning_rate": 2.3081250000000002e-05,
      "loss": 0.0015,
      "step": 129210
    },
    {
      "epoch": 4.307333333333333,
      "grad_norm": 0.23015999794006348,
      "learning_rate": 2.3079166666666667e-05,
      "loss": 0.0025,
      "step": 129220
    },
    {
      "epoch": 4.307666666666667,
      "grad_norm": 0.057600654661655426,
      "learning_rate": 2.3077083333333336e-05,
      "loss": 0.0018,
      "step": 129230
    },
    {
      "epoch": 4.308,
      "grad_norm": 0.17107495665550232,
      "learning_rate": 2.3075000000000002e-05,
      "loss": 0.0017,
      "step": 129240
    },
    {
      "epoch": 4.308333333333334,
      "grad_norm": 0.20394709706306458,
      "learning_rate": 2.307291666666667e-05,
      "loss": 0.0018,
      "step": 129250
    },
    {
      "epoch": 4.308666666666666,
      "grad_norm": 0.4532689154148102,
      "learning_rate": 2.3070833333333333e-05,
      "loss": 0.0034,
      "step": 129260
    },
    {
      "epoch": 4.309,
      "grad_norm": 0.057999953627586365,
      "learning_rate": 2.306875e-05,
      "loss": 0.002,
      "step": 129270
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.0860266461968422,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0015,
      "step": 129280
    },
    {
      "epoch": 4.309666666666667,
      "grad_norm": 0.11445903033018112,
      "learning_rate": 2.3064583333333333e-05,
      "loss": 0.002,
      "step": 129290
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.029941551387310028,
      "learning_rate": 2.30625e-05,
      "loss": 0.0021,
      "step": 129300
    },
    {
      "epoch": 4.310333333333333,
      "grad_norm": 0.004033731296658516,
      "learning_rate": 2.3060416666666667e-05,
      "loss": 0.0014,
      "step": 129310
    },
    {
      "epoch": 4.310666666666666,
      "grad_norm": 0.23069684207439423,
      "learning_rate": 2.3058333333333336e-05,
      "loss": 0.0017,
      "step": 129320
    },
    {
      "epoch": 4.311,
      "grad_norm": 0.17106346786022186,
      "learning_rate": 2.305625e-05,
      "loss": 0.0027,
      "step": 129330
    },
    {
      "epoch": 4.311333333333334,
      "grad_norm": 0.028765235096216202,
      "learning_rate": 2.3054166666666667e-05,
      "loss": 0.0024,
      "step": 129340
    },
    {
      "epoch": 4.3116666666666665,
      "grad_norm": 0.09440555423498154,
      "learning_rate": 2.3052083333333336e-05,
      "loss": 0.0017,
      "step": 129350
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.3137744069099426,
      "learning_rate": 2.305e-05,
      "loss": 0.0018,
      "step": 129360
    },
    {
      "epoch": 4.312333333333333,
      "grad_norm": 0.14336901903152466,
      "learning_rate": 2.304791666666667e-05,
      "loss": 0.0021,
      "step": 129370
    },
    {
      "epoch": 4.312666666666667,
      "grad_norm": 0.08630818128585815,
      "learning_rate": 2.3045833333333332e-05,
      "loss": 0.002,
      "step": 129380
    },
    {
      "epoch": 4.313,
      "grad_norm": 0.11476987600326538,
      "learning_rate": 2.304375e-05,
      "loss": 0.0028,
      "step": 129390
    },
    {
      "epoch": 4.3133333333333335,
      "grad_norm": 0.1144448071718216,
      "learning_rate": 2.3041666666666667e-05,
      "loss": 0.0019,
      "step": 129400
    },
    {
      "epoch": 4.313666666666666,
      "grad_norm": 0.2855827510356903,
      "learning_rate": 2.3039583333333335e-05,
      "loss": 0.0018,
      "step": 129410
    },
    {
      "epoch": 4.314,
      "grad_norm": 0.31342044472694397,
      "learning_rate": 2.30375e-05,
      "loss": 0.0016,
      "step": 129420
    },
    {
      "epoch": 4.314333333333334,
      "grad_norm": 0.057119641453027725,
      "learning_rate": 2.3035416666666666e-05,
      "loss": 0.003,
      "step": 129430
    },
    {
      "epoch": 4.314666666666667,
      "grad_norm": 0.17316526174545288,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0012,
      "step": 129440
    },
    {
      "epoch": 4.315,
      "grad_norm": 0.6277798414230347,
      "learning_rate": 2.303125e-05,
      "loss": 0.0023,
      "step": 129450
    },
    {
      "epoch": 4.315333333333333,
      "grad_norm": 0.6291650533676147,
      "learning_rate": 2.302916666666667e-05,
      "loss": 0.0023,
      "step": 129460
    },
    {
      "epoch": 4.315666666666667,
      "grad_norm": 0.2002747505903244,
      "learning_rate": 2.3027083333333335e-05,
      "loss": 0.0023,
      "step": 129470
    },
    {
      "epoch": 4.316,
      "grad_norm": 0.5429272055625916,
      "learning_rate": 2.3025e-05,
      "loss": 0.0018,
      "step": 129480
    },
    {
      "epoch": 4.316333333333334,
      "grad_norm": 0.09656105935573578,
      "learning_rate": 2.302291666666667e-05,
      "loss": 0.0017,
      "step": 129490
    },
    {
      "epoch": 4.316666666666666,
      "grad_norm": 0.19966481626033783,
      "learning_rate": 2.302083333333333e-05,
      "loss": 0.0016,
      "step": 129500
    },
    {
      "epoch": 4.317,
      "grad_norm": 0.058412835001945496,
      "learning_rate": 2.301875e-05,
      "loss": 0.0016,
      "step": 129510
    },
    {
      "epoch": 4.317333333333333,
      "grad_norm": 0.11410306394100189,
      "learning_rate": 2.3016666666666666e-05,
      "loss": 0.0017,
      "step": 129520
    },
    {
      "epoch": 4.317666666666667,
      "grad_norm": 0.17158332467079163,
      "learning_rate": 2.3014583333333335e-05,
      "loss": 0.0015,
      "step": 129530
    },
    {
      "epoch": 4.318,
      "grad_norm": 0.2565053105354309,
      "learning_rate": 2.30125e-05,
      "loss": 0.0028,
      "step": 129540
    },
    {
      "epoch": 4.318333333333333,
      "grad_norm": 0.2573228180408478,
      "learning_rate": 2.301041666666667e-05,
      "loss": 0.0028,
      "step": 129550
    },
    {
      "epoch": 4.318666666666667,
      "grad_norm": 0.17598918080329895,
      "learning_rate": 2.3008333333333335e-05,
      "loss": 0.0035,
      "step": 129560
    },
    {
      "epoch": 4.319,
      "grad_norm": 0.17299427092075348,
      "learning_rate": 2.300625e-05,
      "loss": 0.0015,
      "step": 129570
    },
    {
      "epoch": 4.319333333333334,
      "grad_norm": 0.08627448976039886,
      "learning_rate": 2.300416666666667e-05,
      "loss": 0.0023,
      "step": 129580
    },
    {
      "epoch": 4.3196666666666665,
      "grad_norm": 0.05745118856430054,
      "learning_rate": 2.3002083333333335e-05,
      "loss": 0.0026,
      "step": 129590
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.03012690506875515,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0028,
      "step": 129600
    },
    {
      "epoch": 4.320333333333333,
      "grad_norm": 0.02899378351867199,
      "learning_rate": 2.299791666666667e-05,
      "loss": 0.0016,
      "step": 129610
    },
    {
      "epoch": 4.320666666666667,
      "grad_norm": 0.11513353884220123,
      "learning_rate": 2.2995833333333334e-05,
      "loss": 0.0022,
      "step": 129620
    },
    {
      "epoch": 4.321,
      "grad_norm": 0.14748887717723846,
      "learning_rate": 2.299375e-05,
      "loss": 0.0021,
      "step": 129630
    },
    {
      "epoch": 4.3213333333333335,
      "grad_norm": 0.17115645110607147,
      "learning_rate": 2.2991666666666665e-05,
      "loss": 0.002,
      "step": 129640
    },
    {
      "epoch": 4.321666666666666,
      "grad_norm": 0.11419812589883804,
      "learning_rate": 2.2989583333333334e-05,
      "loss": 0.0026,
      "step": 129650
    },
    {
      "epoch": 4.322,
      "grad_norm": 0.14520347118377686,
      "learning_rate": 2.29875e-05,
      "loss": 0.002,
      "step": 129660
    },
    {
      "epoch": 4.322333333333333,
      "grad_norm": 0.6655236482620239,
      "learning_rate": 2.298541666666667e-05,
      "loss": 0.0025,
      "step": 129670
    },
    {
      "epoch": 4.322666666666667,
      "grad_norm": 0.3141327500343323,
      "learning_rate": 2.2983333333333334e-05,
      "loss": 0.002,
      "step": 129680
    },
    {
      "epoch": 4.323,
      "grad_norm": 0.057915642857551575,
      "learning_rate": 2.2981250000000003e-05,
      "loss": 0.0018,
      "step": 129690
    },
    {
      "epoch": 4.323333333333333,
      "grad_norm": 0.4000706076622009,
      "learning_rate": 2.297916666666667e-05,
      "loss": 0.0019,
      "step": 129700
    },
    {
      "epoch": 4.323666666666667,
      "grad_norm": 0.11414118111133575,
      "learning_rate": 2.2977083333333334e-05,
      "loss": 0.0018,
      "step": 129710
    },
    {
      "epoch": 4.324,
      "grad_norm": 0.22779616713523865,
      "learning_rate": 2.2975000000000003e-05,
      "loss": 0.0018,
      "step": 129720
    },
    {
      "epoch": 4.324333333333334,
      "grad_norm": 0.08556780964136124,
      "learning_rate": 2.2972916666666668e-05,
      "loss": 0.0021,
      "step": 129730
    },
    {
      "epoch": 4.324666666666666,
      "grad_norm": 0.059494420886039734,
      "learning_rate": 2.2970833333333334e-05,
      "loss": 0.0019,
      "step": 129740
    },
    {
      "epoch": 4.325,
      "grad_norm": 0.14233484864234924,
      "learning_rate": 2.296875e-05,
      "loss": 0.0021,
      "step": 129750
    },
    {
      "epoch": 4.325333333333333,
      "grad_norm": 0.11406442523002625,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.0029,
      "step": 129760
    },
    {
      "epoch": 4.325666666666667,
      "grad_norm": 0.2878158390522003,
      "learning_rate": 2.2964583333333334e-05,
      "loss": 0.0019,
      "step": 129770
    },
    {
      "epoch": 4.326,
      "grad_norm": 0.030384328216314316,
      "learning_rate": 2.29625e-05,
      "loss": 0.0016,
      "step": 129780
    },
    {
      "epoch": 4.326333333333333,
      "grad_norm": 0.23335786163806915,
      "learning_rate": 2.2960416666666668e-05,
      "loss": 0.0019,
      "step": 129790
    },
    {
      "epoch": 4.326666666666666,
      "grad_norm": 0.11444945633411407,
      "learning_rate": 2.2958333333333333e-05,
      "loss": 0.0017,
      "step": 129800
    },
    {
      "epoch": 4.327,
      "grad_norm": 0.5990118980407715,
      "learning_rate": 2.2956250000000002e-05,
      "loss": 0.0025,
      "step": 129810
    },
    {
      "epoch": 4.327333333333334,
      "grad_norm": 0.3136173486709595,
      "learning_rate": 2.2954166666666668e-05,
      "loss": 0.0026,
      "step": 129820
    },
    {
      "epoch": 4.3276666666666666,
      "grad_norm": 0.25857070088386536,
      "learning_rate": 2.2952083333333337e-05,
      "loss": 0.0025,
      "step": 129830
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.1629582643508911,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 0.0022,
      "step": 129840
    },
    {
      "epoch": 4.328333333333333,
      "grad_norm": 0.3620778024196625,
      "learning_rate": 2.2947916666666668e-05,
      "loss": 0.0024,
      "step": 129850
    },
    {
      "epoch": 4.328666666666667,
      "grad_norm": 0.057532452046871185,
      "learning_rate": 2.2945833333333333e-05,
      "loss": 0.0016,
      "step": 129860
    },
    {
      "epoch": 4.329,
      "grad_norm": 0.02954886294901371,
      "learning_rate": 2.294375e-05,
      "loss": 0.0014,
      "step": 129870
    },
    {
      "epoch": 4.3293333333333335,
      "grad_norm": 0.05754857137799263,
      "learning_rate": 2.2941666666666668e-05,
      "loss": 0.0019,
      "step": 129880
    },
    {
      "epoch": 4.329666666666666,
      "grad_norm": 0.1714203953742981,
      "learning_rate": 2.2939583333333333e-05,
      "loss": 0.0021,
      "step": 129890
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.19963406026363373,
      "learning_rate": 2.2937500000000002e-05,
      "loss": 0.0026,
      "step": 129900
    },
    {
      "epoch": 4.330333333333333,
      "grad_norm": 0.3421977460384369,
      "learning_rate": 2.2935416666666667e-05,
      "loss": 0.0016,
      "step": 129910
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 0.08695530146360397,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0022,
      "step": 129920
    },
    {
      "epoch": 4.331,
      "grad_norm": 0.14272908866405487,
      "learning_rate": 2.2931250000000002e-05,
      "loss": 0.0032,
      "step": 129930
    },
    {
      "epoch": 4.331333333333333,
      "grad_norm": 0.19967521727085114,
      "learning_rate": 2.2929166666666667e-05,
      "loss": 0.0018,
      "step": 129940
    },
    {
      "epoch": 4.331666666666667,
      "grad_norm": 0.5704878568649292,
      "learning_rate": 2.2927083333333336e-05,
      "loss": 0.0023,
      "step": 129950
    },
    {
      "epoch": 4.332,
      "grad_norm": 0.6023796200752258,
      "learning_rate": 2.2925e-05,
      "loss": 0.0017,
      "step": 129960
    },
    {
      "epoch": 4.332333333333334,
      "grad_norm": 0.28503283858299255,
      "learning_rate": 2.292291666666667e-05,
      "loss": 0.0016,
      "step": 129970
    },
    {
      "epoch": 4.332666666666666,
      "grad_norm": 0.19976834952831268,
      "learning_rate": 2.2920833333333333e-05,
      "loss": 0.0016,
      "step": 129980
    },
    {
      "epoch": 4.333,
      "grad_norm": 0.089076928794384,
      "learning_rate": 2.291875e-05,
      "loss": 0.0017,
      "step": 129990
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.06051688641309738,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.0027,
      "step": 130000
    },
    {
      "epoch": 4.333666666666667,
      "grad_norm": 0.28515011072158813,
      "learning_rate": 2.2914583333333332e-05,
      "loss": 0.0016,
      "step": 130010
    },
    {
      "epoch": 4.334,
      "grad_norm": 0.3996088206768036,
      "learning_rate": 2.29125e-05,
      "loss": 0.0025,
      "step": 130020
    },
    {
      "epoch": 4.334333333333333,
      "grad_norm": 0.17331203818321228,
      "learning_rate": 2.2910416666666667e-05,
      "loss": 0.002,
      "step": 130030
    },
    {
      "epoch": 4.334666666666667,
      "grad_norm": 0.17117953300476074,
      "learning_rate": 2.2908333333333336e-05,
      "loss": 0.0025,
      "step": 130040
    },
    {
      "epoch": 4.335,
      "grad_norm": 0.14454974234104156,
      "learning_rate": 2.290625e-05,
      "loss": 0.0028,
      "step": 130050
    },
    {
      "epoch": 4.335333333333334,
      "grad_norm": 0.14322194457054138,
      "learning_rate": 2.2904166666666667e-05,
      "loss": 0.0041,
      "step": 130060
    },
    {
      "epoch": 4.335666666666667,
      "grad_norm": 0.029438314959406853,
      "learning_rate": 2.2902083333333335e-05,
      "loss": 0.0017,
      "step": 130070
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.25665923953056335,
      "learning_rate": 2.29e-05,
      "loss": 0.0018,
      "step": 130080
    },
    {
      "epoch": 4.336333333333333,
      "grad_norm": 0.316127747297287,
      "learning_rate": 2.289791666666667e-05,
      "loss": 0.0031,
      "step": 130090
    },
    {
      "epoch": 4.336666666666667,
      "grad_norm": 0.06459684669971466,
      "learning_rate": 2.2895833333333335e-05,
      "loss": 0.0022,
      "step": 130100
    },
    {
      "epoch": 4.337,
      "grad_norm": 0.08807028830051422,
      "learning_rate": 2.289375e-05,
      "loss": 0.0028,
      "step": 130110
    },
    {
      "epoch": 4.3373333333333335,
      "grad_norm": 0.11421188712120056,
      "learning_rate": 2.2891666666666666e-05,
      "loss": 0.0027,
      "step": 130120
    },
    {
      "epoch": 4.337666666666666,
      "grad_norm": 0.058084581047296524,
      "learning_rate": 2.2889583333333335e-05,
      "loss": 0.0016,
      "step": 130130
    },
    {
      "epoch": 4.338,
      "grad_norm": 0.5422189235687256,
      "learning_rate": 2.28875e-05,
      "loss": 0.0017,
      "step": 130140
    },
    {
      "epoch": 4.338333333333333,
      "grad_norm": 0.1711849421262741,
      "learning_rate": 2.2885416666666666e-05,
      "loss": 0.0026,
      "step": 130150
    },
    {
      "epoch": 4.338666666666667,
      "grad_norm": 0.5483781695365906,
      "learning_rate": 2.2883333333333335e-05,
      "loss": 0.0015,
      "step": 130160
    },
    {
      "epoch": 4.339,
      "grad_norm": 0.17113789916038513,
      "learning_rate": 2.288125e-05,
      "loss": 0.0022,
      "step": 130170
    },
    {
      "epoch": 4.339333333333333,
      "grad_norm": 0.029668215662240982,
      "learning_rate": 2.287916666666667e-05,
      "loss": 0.0021,
      "step": 130180
    },
    {
      "epoch": 4.339666666666667,
      "grad_norm": 0.1424262970685959,
      "learning_rate": 2.2877083333333335e-05,
      "loss": 0.0023,
      "step": 130190
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.3990395963191986,
      "learning_rate": 2.2875e-05,
      "loss": 0.0018,
      "step": 130200
    },
    {
      "epoch": 4.340333333333334,
      "grad_norm": 0.05792407691478729,
      "learning_rate": 2.287291666666667e-05,
      "loss": 0.0017,
      "step": 130210
    },
    {
      "epoch": 4.3406666666666665,
      "grad_norm": 0.1728816032409668,
      "learning_rate": 2.2870833333333335e-05,
      "loss": 0.0024,
      "step": 130220
    },
    {
      "epoch": 4.341,
      "grad_norm": 0.3138720691204071,
      "learning_rate": 2.286875e-05,
      "loss": 0.002,
      "step": 130230
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.14277975261211395,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0025,
      "step": 130240
    },
    {
      "epoch": 4.341666666666667,
      "grad_norm": 0.08618032932281494,
      "learning_rate": 2.2864583333333335e-05,
      "loss": 0.0018,
      "step": 130250
    },
    {
      "epoch": 4.342,
      "grad_norm": 0.5137040019035339,
      "learning_rate": 2.28625e-05,
      "loss": 0.002,
      "step": 130260
    },
    {
      "epoch": 4.342333333333333,
      "grad_norm": 0.08593472838401794,
      "learning_rate": 2.286041666666667e-05,
      "loss": 0.0019,
      "step": 130270
    },
    {
      "epoch": 4.342666666666666,
      "grad_norm": 0.2571231424808502,
      "learning_rate": 2.2858333333333334e-05,
      "loss": 0.0019,
      "step": 130280
    },
    {
      "epoch": 4.343,
      "grad_norm": 0.2008277326822281,
      "learning_rate": 2.285625e-05,
      "loss": 0.0023,
      "step": 130290
    },
    {
      "epoch": 4.343333333333334,
      "grad_norm": 0.1714688390493393,
      "learning_rate": 2.285416666666667e-05,
      "loss": 0.0037,
      "step": 130300
    },
    {
      "epoch": 4.343666666666667,
      "grad_norm": 0.0865999311208725,
      "learning_rate": 2.2852083333333334e-05,
      "loss": 0.0017,
      "step": 130310
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.08596105128526688,
      "learning_rate": 2.2850000000000003e-05,
      "loss": 0.0023,
      "step": 130320
    },
    {
      "epoch": 4.344333333333333,
      "grad_norm": 0.08579416573047638,
      "learning_rate": 2.284791666666667e-05,
      "loss": 0.0022,
      "step": 130330
    },
    {
      "epoch": 4.344666666666667,
      "grad_norm": 0.19954253733158112,
      "learning_rate": 2.2845833333333334e-05,
      "loss": 0.0022,
      "step": 130340
    },
    {
      "epoch": 4.345,
      "grad_norm": 0.22845833003520966,
      "learning_rate": 2.284375e-05,
      "loss": 0.0024,
      "step": 130350
    },
    {
      "epoch": 4.3453333333333335,
      "grad_norm": 0.39974427223205566,
      "learning_rate": 2.2841666666666665e-05,
      "loss": 0.0021,
      "step": 130360
    },
    {
      "epoch": 4.345666666666666,
      "grad_norm": 0.3444150984287262,
      "learning_rate": 2.2839583333333334e-05,
      "loss": 0.002,
      "step": 130370
    },
    {
      "epoch": 4.346,
      "grad_norm": 0.4567152261734009,
      "learning_rate": 2.28375e-05,
      "loss": 0.0015,
      "step": 130380
    },
    {
      "epoch": 4.346333333333333,
      "grad_norm": 0.010278668254613876,
      "learning_rate": 2.283541666666667e-05,
      "loss": 0.0021,
      "step": 130390
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.09020750224590302,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0024,
      "step": 130400
    },
    {
      "epoch": 4.3469999999999995,
      "grad_norm": 0.028941281139850616,
      "learning_rate": 2.2831250000000003e-05,
      "loss": 0.0034,
      "step": 130410
    },
    {
      "epoch": 4.347333333333333,
      "grad_norm": 0.22824627161026,
      "learning_rate": 2.2829166666666668e-05,
      "loss": 0.0014,
      "step": 130420
    },
    {
      "epoch": 4.347666666666667,
      "grad_norm": 0.2280576080083847,
      "learning_rate": 2.2827083333333334e-05,
      "loss": 0.0018,
      "step": 130430
    },
    {
      "epoch": 4.348,
      "grad_norm": 0.028930259868502617,
      "learning_rate": 2.2825000000000003e-05,
      "loss": 0.0028,
      "step": 130440
    },
    {
      "epoch": 4.348333333333334,
      "grad_norm": 0.691542387008667,
      "learning_rate": 2.2822916666666668e-05,
      "loss": 0.0017,
      "step": 130450
    },
    {
      "epoch": 4.3486666666666665,
      "grad_norm": 0.22867290675640106,
      "learning_rate": 2.2820833333333337e-05,
      "loss": 0.002,
      "step": 130460
    },
    {
      "epoch": 4.349,
      "grad_norm": 0.08598262071609497,
      "learning_rate": 2.281875e-05,
      "loss": 0.0026,
      "step": 130470
    },
    {
      "epoch": 4.349333333333333,
      "grad_norm": 0.2602865993976593,
      "learning_rate": 2.2816666666666668e-05,
      "loss": 0.0017,
      "step": 130480
    },
    {
      "epoch": 4.349666666666667,
      "grad_norm": 0.11421120911836624,
      "learning_rate": 2.2814583333333333e-05,
      "loss": 0.0017,
      "step": 130490
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.2567228376865387,
      "learning_rate": 2.28125e-05,
      "loss": 0.0019,
      "step": 130500
    },
    {
      "epoch": 4.350333333333333,
      "grad_norm": 0.28562936186790466,
      "learning_rate": 2.2810416666666668e-05,
      "loss": 0.0019,
      "step": 130510
    },
    {
      "epoch": 4.350666666666667,
      "grad_norm": 0.02941945753991604,
      "learning_rate": 2.2808333333333333e-05,
      "loss": 0.0019,
      "step": 130520
    },
    {
      "epoch": 4.351,
      "grad_norm": 0.14277371764183044,
      "learning_rate": 2.2806250000000002e-05,
      "loss": 0.0023,
      "step": 130530
    },
    {
      "epoch": 4.351333333333334,
      "grad_norm": 0.029089296236634254,
      "learning_rate": 2.2804166666666668e-05,
      "loss": 0.0016,
      "step": 130540
    },
    {
      "epoch": 4.351666666666667,
      "grad_norm": 0.08629477769136429,
      "learning_rate": 2.2802083333333336e-05,
      "loss": 0.0016,
      "step": 130550
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.0859539732336998,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.002,
      "step": 130560
    },
    {
      "epoch": 4.352333333333333,
      "grad_norm": 0.008382774889469147,
      "learning_rate": 2.2797916666666667e-05,
      "loss": 0.0018,
      "step": 130570
    },
    {
      "epoch": 4.352666666666667,
      "grad_norm": 0.17152047157287598,
      "learning_rate": 2.2795833333333336e-05,
      "loss": 0.0016,
      "step": 130580
    },
    {
      "epoch": 4.353,
      "grad_norm": 0.230805903673172,
      "learning_rate": 2.279375e-05,
      "loss": 0.0018,
      "step": 130590
    },
    {
      "epoch": 4.3533333333333335,
      "grad_norm": 0.22829480469226837,
      "learning_rate": 2.2791666666666667e-05,
      "loss": 0.0011,
      "step": 130600
    },
    {
      "epoch": 4.353666666666666,
      "grad_norm": 0.08589193224906921,
      "learning_rate": 2.2789583333333333e-05,
      "loss": 0.0019,
      "step": 130610
    },
    {
      "epoch": 4.354,
      "grad_norm": 0.08586562424898148,
      "learning_rate": 2.27875e-05,
      "loss": 0.0025,
      "step": 130620
    },
    {
      "epoch": 4.354333333333333,
      "grad_norm": 0.08636040985584259,
      "learning_rate": 2.2785416666666667e-05,
      "loss": 0.0022,
      "step": 130630
    },
    {
      "epoch": 4.354666666666667,
      "grad_norm": 0.31647413969039917,
      "learning_rate": 2.2783333333333336e-05,
      "loss": 0.0024,
      "step": 130640
    },
    {
      "epoch": 4.355,
      "grad_norm": 0.11426514387130737,
      "learning_rate": 2.278125e-05,
      "loss": 0.0023,
      "step": 130650
    },
    {
      "epoch": 4.355333333333333,
      "grad_norm": 0.46889257431030273,
      "learning_rate": 2.2779166666666667e-05,
      "loss": 0.0021,
      "step": 130660
    },
    {
      "epoch": 4.355666666666667,
      "grad_norm": 0.14251665771007538,
      "learning_rate": 2.2777083333333336e-05,
      "loss": 0.0023,
      "step": 130670
    },
    {
      "epoch": 4.356,
      "grad_norm": 0.39999470114707947,
      "learning_rate": 2.2775e-05,
      "loss": 0.0019,
      "step": 130680
    },
    {
      "epoch": 4.356333333333334,
      "grad_norm": 0.1998879313468933,
      "learning_rate": 2.277291666666667e-05,
      "loss": 0.0029,
      "step": 130690
    },
    {
      "epoch": 4.3566666666666665,
      "grad_norm": 0.22845353186130524,
      "learning_rate": 2.2770833333333336e-05,
      "loss": 0.0016,
      "step": 130700
    },
    {
      "epoch": 4.357,
      "grad_norm": 0.029304375872015953,
      "learning_rate": 2.276875e-05,
      "loss": 0.0017,
      "step": 130710
    },
    {
      "epoch": 4.357333333333333,
      "grad_norm": 0.17113761603832245,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.0026,
      "step": 130720
    },
    {
      "epoch": 4.357666666666667,
      "grad_norm": 0.1992192417383194,
      "learning_rate": 2.2764583333333332e-05,
      "loss": 0.0012,
      "step": 130730
    },
    {
      "epoch": 4.358,
      "grad_norm": 0.2870592772960663,
      "learning_rate": 2.27625e-05,
      "loss": 0.0017,
      "step": 130740
    },
    {
      "epoch": 4.358333333333333,
      "grad_norm": 0.14254571497440338,
      "learning_rate": 2.2760416666666667e-05,
      "loss": 0.0017,
      "step": 130750
    },
    {
      "epoch": 4.358666666666666,
      "grad_norm": 0.02883969061076641,
      "learning_rate": 2.2758333333333335e-05,
      "loss": 0.0022,
      "step": 130760
    },
    {
      "epoch": 4.359,
      "grad_norm": 0.25886356830596924,
      "learning_rate": 2.275625e-05,
      "loss": 0.0016,
      "step": 130770
    },
    {
      "epoch": 4.359333333333334,
      "grad_norm": 0.31565436720848083,
      "learning_rate": 2.275416666666667e-05,
      "loss": 0.0021,
      "step": 130780
    },
    {
      "epoch": 4.359666666666667,
      "grad_norm": 0.08590524643659592,
      "learning_rate": 2.2752083333333335e-05,
      "loss": 0.0017,
      "step": 130790
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.40136247873306274,
      "learning_rate": 2.275e-05,
      "loss": 0.0021,
      "step": 130800
    },
    {
      "epoch": 4.360333333333333,
      "grad_norm": 0.6660610437393188,
      "learning_rate": 2.274791666666667e-05,
      "loss": 0.0019,
      "step": 130810
    },
    {
      "epoch": 4.360666666666667,
      "grad_norm": 0.02909925952553749,
      "learning_rate": 2.2745833333333335e-05,
      "loss": 0.0021,
      "step": 130820
    },
    {
      "epoch": 4.361,
      "grad_norm": 0.14310479164123535,
      "learning_rate": 2.274375e-05,
      "loss": 0.0021,
      "step": 130830
    },
    {
      "epoch": 4.3613333333333335,
      "grad_norm": 0.09637406468391418,
      "learning_rate": 2.2741666666666666e-05,
      "loss": 0.0023,
      "step": 130840
    },
    {
      "epoch": 4.361666666666666,
      "grad_norm": 0.007093771360814571,
      "learning_rate": 2.2739583333333335e-05,
      "loss": 0.0023,
      "step": 130850
    },
    {
      "epoch": 4.362,
      "grad_norm": 0.0861622765660286,
      "learning_rate": 2.27375e-05,
      "loss": 0.0016,
      "step": 130860
    },
    {
      "epoch": 4.362333333333333,
      "grad_norm": 0.14245299994945526,
      "learning_rate": 2.2735416666666666e-05,
      "loss": 0.002,
      "step": 130870
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.0857711061835289,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0015,
      "step": 130880
    },
    {
      "epoch": 4.3629999999999995,
      "grad_norm": 0.005469038151204586,
      "learning_rate": 2.273125e-05,
      "loss": 0.0019,
      "step": 130890
    },
    {
      "epoch": 4.363333333333333,
      "grad_norm": 0.5419607758522034,
      "learning_rate": 2.272916666666667e-05,
      "loss": 0.0028,
      "step": 130900
    },
    {
      "epoch": 4.363666666666667,
      "grad_norm": 0.4850600063800812,
      "learning_rate": 2.2727083333333335e-05,
      "loss": 0.0016,
      "step": 130910
    },
    {
      "epoch": 4.364,
      "grad_norm": 0.02975640818476677,
      "learning_rate": 2.2725000000000003e-05,
      "loss": 0.0016,
      "step": 130920
    },
    {
      "epoch": 4.364333333333334,
      "grad_norm": 0.05754167214035988,
      "learning_rate": 2.272291666666667e-05,
      "loss": 0.0013,
      "step": 130930
    },
    {
      "epoch": 4.3646666666666665,
      "grad_norm": 0.08590281754732132,
      "learning_rate": 2.2720833333333334e-05,
      "loss": 0.0016,
      "step": 130940
    },
    {
      "epoch": 4.365,
      "grad_norm": 0.2854824662208557,
      "learning_rate": 2.271875e-05,
      "loss": 0.0024,
      "step": 130950
    },
    {
      "epoch": 4.365333333333333,
      "grad_norm": 0.11496618390083313,
      "learning_rate": 2.2716666666666665e-05,
      "loss": 0.0027,
      "step": 130960
    },
    {
      "epoch": 4.365666666666667,
      "grad_norm": 0.199944868683815,
      "learning_rate": 2.2714583333333334e-05,
      "loss": 0.0016,
      "step": 130970
    },
    {
      "epoch": 4.366,
      "grad_norm": 0.08319155871868134,
      "learning_rate": 2.27125e-05,
      "loss": 0.002,
      "step": 130980
    },
    {
      "epoch": 4.366333333333333,
      "grad_norm": 0.23643232882022858,
      "learning_rate": 2.271041666666667e-05,
      "loss": 0.0026,
      "step": 130990
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 0.2911693751811981,
      "learning_rate": 2.2708333333333334e-05,
      "loss": 0.0018,
      "step": 131000
    },
    {
      "epoch": 4.367,
      "grad_norm": 0.14696301519870758,
      "learning_rate": 2.270625e-05,
      "loss": 0.002,
      "step": 131010
    },
    {
      "epoch": 4.367333333333334,
      "grad_norm": 0.2283153533935547,
      "learning_rate": 2.270416666666667e-05,
      "loss": 0.0024,
      "step": 131020
    },
    {
      "epoch": 4.367666666666667,
      "grad_norm": 0.1713019460439682,
      "learning_rate": 2.2702083333333334e-05,
      "loss": 0.0024,
      "step": 131030
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.08603020012378693,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0023,
      "step": 131040
    },
    {
      "epoch": 4.368333333333333,
      "grad_norm": 0.058959923684597015,
      "learning_rate": 2.269791666666667e-05,
      "loss": 0.0015,
      "step": 131050
    },
    {
      "epoch": 4.368666666666667,
      "grad_norm": 0.28708815574645996,
      "learning_rate": 2.2695833333333337e-05,
      "loss": 0.0018,
      "step": 131060
    },
    {
      "epoch": 4.369,
      "grad_norm": 0.3421595096588135,
      "learning_rate": 2.269375e-05,
      "loss": 0.0022,
      "step": 131070
    },
    {
      "epoch": 4.3693333333333335,
      "grad_norm": 0.1999039351940155,
      "learning_rate": 2.2691666666666668e-05,
      "loss": 0.0018,
      "step": 131080
    },
    {
      "epoch": 4.369666666666666,
      "grad_norm": 0.1426023244857788,
      "learning_rate": 2.2689583333333334e-05,
      "loss": 0.0028,
      "step": 131090
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.3421539068222046,
      "learning_rate": 2.26875e-05,
      "loss": 0.0024,
      "step": 131100
    },
    {
      "epoch": 4.370333333333333,
      "grad_norm": 0.030558662489056587,
      "learning_rate": 2.2685416666666668e-05,
      "loss": 0.0014,
      "step": 131110
    },
    {
      "epoch": 4.370666666666667,
      "grad_norm": 0.007292399648576975,
      "learning_rate": 2.2683333333333334e-05,
      "loss": 0.0015,
      "step": 131120
    },
    {
      "epoch": 4.371,
      "grad_norm": 0.0582067146897316,
      "learning_rate": 2.2681250000000002e-05,
      "loss": 0.0015,
      "step": 131130
    },
    {
      "epoch": 4.371333333333333,
      "grad_norm": 0.029300471767783165,
      "learning_rate": 2.2679166666666668e-05,
      "loss": 0.002,
      "step": 131140
    },
    {
      "epoch": 4.371666666666667,
      "grad_norm": 0.029873445630073547,
      "learning_rate": 2.2677083333333333e-05,
      "loss": 0.0025,
      "step": 131150
    },
    {
      "epoch": 4.372,
      "grad_norm": 0.11988817155361176,
      "learning_rate": 2.2675000000000002e-05,
      "loss": 0.0019,
      "step": 131160
    },
    {
      "epoch": 4.372333333333334,
      "grad_norm": 0.14298900961875916,
      "learning_rate": 2.2672916666666668e-05,
      "loss": 0.0025,
      "step": 131170
    },
    {
      "epoch": 4.3726666666666665,
      "grad_norm": 0.2565881609916687,
      "learning_rate": 2.2670833333333337e-05,
      "loss": 0.0017,
      "step": 131180
    },
    {
      "epoch": 4.373,
      "grad_norm": 0.007299001328647137,
      "learning_rate": 2.266875e-05,
      "loss": 0.0018,
      "step": 131190
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.02947426401078701,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0016,
      "step": 131200
    },
    {
      "epoch": 4.373666666666667,
      "grad_norm": 0.17097565531730652,
      "learning_rate": 2.2664583333333333e-05,
      "loss": 0.0015,
      "step": 131210
    },
    {
      "epoch": 4.374,
      "grad_norm": 0.2569640874862671,
      "learning_rate": 2.2662500000000002e-05,
      "loss": 0.002,
      "step": 131220
    },
    {
      "epoch": 4.374333333333333,
      "grad_norm": 0.05715136229991913,
      "learning_rate": 2.2660416666666667e-05,
      "loss": 0.0018,
      "step": 131230
    },
    {
      "epoch": 4.374666666666666,
      "grad_norm": 0.2568081021308899,
      "learning_rate": 2.2658333333333333e-05,
      "loss": 0.0022,
      "step": 131240
    },
    {
      "epoch": 4.375,
      "grad_norm": 0.287934273481369,
      "learning_rate": 2.2656250000000002e-05,
      "loss": 0.0017,
      "step": 131250
    },
    {
      "epoch": 4.375333333333334,
      "grad_norm": 0.5135379433631897,
      "learning_rate": 2.2654166666666667e-05,
      "loss": 0.0018,
      "step": 131260
    },
    {
      "epoch": 4.375666666666667,
      "grad_norm": 0.19966080784797668,
      "learning_rate": 2.2652083333333336e-05,
      "loss": 0.0017,
      "step": 131270
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.3708967864513397,
      "learning_rate": 2.265e-05,
      "loss": 0.0013,
      "step": 131280
    },
    {
      "epoch": 4.376333333333333,
      "grad_norm": 0.22836574912071228,
      "learning_rate": 2.2647916666666667e-05,
      "loss": 0.0012,
      "step": 131290
    },
    {
      "epoch": 4.376666666666667,
      "grad_norm": 0.05726645886898041,
      "learning_rate": 2.2645833333333336e-05,
      "loss": 0.0022,
      "step": 131300
    },
    {
      "epoch": 4.377,
      "grad_norm": 0.14367754757404327,
      "learning_rate": 2.264375e-05,
      "loss": 0.0016,
      "step": 131310
    },
    {
      "epoch": 4.3773333333333335,
      "grad_norm": 0.1141793504357338,
      "learning_rate": 2.2641666666666667e-05,
      "loss": 0.0017,
      "step": 131320
    },
    {
      "epoch": 4.377666666666666,
      "grad_norm": 0.5993490219116211,
      "learning_rate": 2.2639583333333332e-05,
      "loss": 0.0017,
      "step": 131330
    },
    {
      "epoch": 4.378,
      "grad_norm": 0.6513375639915466,
      "learning_rate": 2.26375e-05,
      "loss": 0.0024,
      "step": 131340
    },
    {
      "epoch": 4.378333333333333,
      "grad_norm": 0.086520254611969,
      "learning_rate": 2.2635416666666667e-05,
      "loss": 0.0017,
      "step": 131350
    },
    {
      "epoch": 4.378666666666667,
      "grad_norm": 0.17155979573726654,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.0018,
      "step": 131360
    },
    {
      "epoch": 4.379,
      "grad_norm": 0.2569173574447632,
      "learning_rate": 2.263125e-05,
      "loss": 0.0015,
      "step": 131370
    },
    {
      "epoch": 4.379333333333333,
      "grad_norm": 0.1733788549900055,
      "learning_rate": 2.2629166666666667e-05,
      "loss": 0.002,
      "step": 131380
    },
    {
      "epoch": 4.379666666666667,
      "grad_norm": 0.14661316573619843,
      "learning_rate": 2.2627083333333336e-05,
      "loss": 0.002,
      "step": 131390
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.11479400098323822,
      "learning_rate": 2.2625e-05,
      "loss": 0.0016,
      "step": 131400
    },
    {
      "epoch": 4.380333333333334,
      "grad_norm": 0.08759921789169312,
      "learning_rate": 2.262291666666667e-05,
      "loss": 0.0022,
      "step": 131410
    },
    {
      "epoch": 4.3806666666666665,
      "grad_norm": 0.02944651059806347,
      "learning_rate": 2.2620833333333335e-05,
      "loss": 0.002,
      "step": 131420
    },
    {
      "epoch": 4.381,
      "grad_norm": 0.02950061298906803,
      "learning_rate": 2.261875e-05,
      "loss": 0.0036,
      "step": 131430
    },
    {
      "epoch": 4.381333333333333,
      "grad_norm": 0.15006259083747864,
      "learning_rate": 2.2616666666666666e-05,
      "loss": 0.0027,
      "step": 131440
    },
    {
      "epoch": 4.381666666666667,
      "grad_norm": 0.42797723412513733,
      "learning_rate": 2.2614583333333332e-05,
      "loss": 0.002,
      "step": 131450
    },
    {
      "epoch": 4.382,
      "grad_norm": 0.05769364908337593,
      "learning_rate": 2.26125e-05,
      "loss": 0.0016,
      "step": 131460
    },
    {
      "epoch": 4.382333333333333,
      "grad_norm": 0.2286674976348877,
      "learning_rate": 2.2610416666666666e-05,
      "loss": 0.0015,
      "step": 131470
    },
    {
      "epoch": 4.382666666666666,
      "grad_norm": 0.3136328160762787,
      "learning_rate": 2.2608333333333335e-05,
      "loss": 0.0023,
      "step": 131480
    },
    {
      "epoch": 4.383,
      "grad_norm": 0.014563722535967827,
      "learning_rate": 2.260625e-05,
      "loss": 0.0025,
      "step": 131490
    },
    {
      "epoch": 4.383333333333334,
      "grad_norm": 0.11442511528730392,
      "learning_rate": 2.260416666666667e-05,
      "loss": 0.0016,
      "step": 131500
    },
    {
      "epoch": 4.383666666666667,
      "grad_norm": 0.14270129799842834,
      "learning_rate": 2.2602083333333335e-05,
      "loss": 0.002,
      "step": 131510
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.09024443477392197,
      "learning_rate": 2.26e-05,
      "loss": 0.0019,
      "step": 131520
    },
    {
      "epoch": 4.384333333333333,
      "grad_norm": 0.21166718006134033,
      "learning_rate": 2.259791666666667e-05,
      "loss": 0.0018,
      "step": 131530
    },
    {
      "epoch": 4.384666666666667,
      "grad_norm": 0.37104254961013794,
      "learning_rate": 2.2595833333333335e-05,
      "loss": 0.0027,
      "step": 131540
    },
    {
      "epoch": 4.385,
      "grad_norm": 0.02930028736591339,
      "learning_rate": 2.2593750000000004e-05,
      "loss": 0.002,
      "step": 131550
    },
    {
      "epoch": 4.3853333333333335,
      "grad_norm": 0.22786779701709747,
      "learning_rate": 2.2591666666666666e-05,
      "loss": 0.0016,
      "step": 131560
    },
    {
      "epoch": 4.385666666666666,
      "grad_norm": 0.11426068842411041,
      "learning_rate": 2.2589583333333335e-05,
      "loss": 0.0014,
      "step": 131570
    },
    {
      "epoch": 4.386,
      "grad_norm": 0.5136607885360718,
      "learning_rate": 2.25875e-05,
      "loss": 0.0019,
      "step": 131580
    },
    {
      "epoch": 4.386333333333333,
      "grad_norm": 0.08593544363975525,
      "learning_rate": 2.2585416666666666e-05,
      "loss": 0.0018,
      "step": 131590
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.006449432112276554,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.0024,
      "step": 131600
    },
    {
      "epoch": 4.3870000000000005,
      "grad_norm": 0.05731881409883499,
      "learning_rate": 2.258125e-05,
      "loss": 0.0018,
      "step": 131610
    },
    {
      "epoch": 4.387333333333333,
      "grad_norm": 0.03579416871070862,
      "learning_rate": 2.257916666666667e-05,
      "loss": 0.0031,
      "step": 131620
    },
    {
      "epoch": 4.387666666666667,
      "grad_norm": 0.2570357322692871,
      "learning_rate": 2.2577083333333334e-05,
      "loss": 0.0018,
      "step": 131630
    },
    {
      "epoch": 4.388,
      "grad_norm": 0.28499194979667664,
      "learning_rate": 2.2575000000000003e-05,
      "loss": 0.0019,
      "step": 131640
    },
    {
      "epoch": 4.388333333333334,
      "grad_norm": 0.2850789427757263,
      "learning_rate": 2.257291666666667e-05,
      "loss": 0.002,
      "step": 131650
    },
    {
      "epoch": 4.3886666666666665,
      "grad_norm": 0.08566530793905258,
      "learning_rate": 2.2570833333333334e-05,
      "loss": 0.0012,
      "step": 131660
    },
    {
      "epoch": 4.389,
      "grad_norm": 0.23061887919902802,
      "learning_rate": 2.2568750000000003e-05,
      "loss": 0.003,
      "step": 131670
    },
    {
      "epoch": 4.389333333333333,
      "grad_norm": 0.17146986722946167,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0019,
      "step": 131680
    },
    {
      "epoch": 4.389666666666667,
      "grad_norm": 0.14302608370780945,
      "learning_rate": 2.2564583333333334e-05,
      "loss": 0.0035,
      "step": 131690
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.1476646363735199,
      "learning_rate": 2.25625e-05,
      "loss": 0.0014,
      "step": 131700
    },
    {
      "epoch": 4.390333333333333,
      "grad_norm": 0.17149870097637177,
      "learning_rate": 2.256041666666667e-05,
      "loss": 0.0029,
      "step": 131710
    },
    {
      "epoch": 4.390666666666666,
      "grad_norm": 0.17166656255722046,
      "learning_rate": 2.2558333333333334e-05,
      "loss": 0.0027,
      "step": 131720
    },
    {
      "epoch": 4.391,
      "grad_norm": 0.19959424436092377,
      "learning_rate": 2.255625e-05,
      "loss": 0.0015,
      "step": 131730
    },
    {
      "epoch": 4.391333333333334,
      "grad_norm": 0.14278805255889893,
      "learning_rate": 2.2554166666666668e-05,
      "loss": 0.0015,
      "step": 131740
    },
    {
      "epoch": 4.391666666666667,
      "grad_norm": 0.22837641835212708,
      "learning_rate": 2.2552083333333334e-05,
      "loss": 0.0022,
      "step": 131750
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.14275440573692322,
      "learning_rate": 2.2550000000000003e-05,
      "loss": 0.0024,
      "step": 131760
    },
    {
      "epoch": 4.392333333333333,
      "grad_norm": 0.028820425271987915,
      "learning_rate": 2.2547916666666668e-05,
      "loss": 0.0025,
      "step": 131770
    },
    {
      "epoch": 4.392666666666667,
      "grad_norm": 0.17307530343532562,
      "learning_rate": 2.2545833333333337e-05,
      "loss": 0.002,
      "step": 131780
    },
    {
      "epoch": 4.393,
      "grad_norm": 0.28675734996795654,
      "learning_rate": 2.2543750000000002e-05,
      "loss": 0.0028,
      "step": 131790
    },
    {
      "epoch": 4.3933333333333335,
      "grad_norm": 0.058148343116045,
      "learning_rate": 2.2541666666666668e-05,
      "loss": 0.0027,
      "step": 131800
    },
    {
      "epoch": 4.393666666666666,
      "grad_norm": 0.11457662284374237,
      "learning_rate": 2.2539583333333333e-05,
      "loss": 0.002,
      "step": 131810
    },
    {
      "epoch": 4.394,
      "grad_norm": 0.051277194172143936,
      "learning_rate": 2.25375e-05,
      "loss": 0.0037,
      "step": 131820
    },
    {
      "epoch": 4.394333333333333,
      "grad_norm": 0.2603929340839386,
      "learning_rate": 2.2535416666666668e-05,
      "loss": 0.003,
      "step": 131830
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.02916891500353813,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0031,
      "step": 131840
    },
    {
      "epoch": 4.395,
      "grad_norm": 0.052375465631484985,
      "learning_rate": 2.2531250000000002e-05,
      "loss": 0.002,
      "step": 131850
    },
    {
      "epoch": 4.395333333333333,
      "grad_norm": 0.3135104179382324,
      "learning_rate": 2.2529166666666668e-05,
      "loss": 0.0015,
      "step": 131860
    },
    {
      "epoch": 4.395666666666667,
      "grad_norm": 0.02998354844748974,
      "learning_rate": 2.2527083333333333e-05,
      "loss": 0.0026,
      "step": 131870
    },
    {
      "epoch": 4.396,
      "grad_norm": 0.08600874990224838,
      "learning_rate": 2.2525000000000002e-05,
      "loss": 0.0022,
      "step": 131880
    },
    {
      "epoch": 4.396333333333334,
      "grad_norm": 0.19971972703933716,
      "learning_rate": 2.2522916666666668e-05,
      "loss": 0.0022,
      "step": 131890
    },
    {
      "epoch": 4.3966666666666665,
      "grad_norm": 0.19946295022964478,
      "learning_rate": 2.2520833333333336e-05,
      "loss": 0.0014,
      "step": 131900
    },
    {
      "epoch": 4.397,
      "grad_norm": 0.1144038662314415,
      "learning_rate": 2.2518750000000002e-05,
      "loss": 0.0016,
      "step": 131910
    },
    {
      "epoch": 4.397333333333333,
      "grad_norm": 0.19970789551734924,
      "learning_rate": 2.2516666666666667e-05,
      "loss": 0.0015,
      "step": 131920
    },
    {
      "epoch": 4.397666666666667,
      "grad_norm": 0.1715674251317978,
      "learning_rate": 2.2514583333333333e-05,
      "loss": 0.0016,
      "step": 131930
    },
    {
      "epoch": 4.398,
      "grad_norm": 0.057597000151872635,
      "learning_rate": 2.2512500000000002e-05,
      "loss": 0.0023,
      "step": 131940
    },
    {
      "epoch": 4.398333333333333,
      "grad_norm": 0.08639311045408249,
      "learning_rate": 2.2510416666666667e-05,
      "loss": 0.0018,
      "step": 131950
    },
    {
      "epoch": 4.398666666666666,
      "grad_norm": 0.2851978838443756,
      "learning_rate": 2.2508333333333333e-05,
      "loss": 0.0023,
      "step": 131960
    },
    {
      "epoch": 4.399,
      "grad_norm": 0.3994499444961548,
      "learning_rate": 2.250625e-05,
      "loss": 0.0021,
      "step": 131970
    },
    {
      "epoch": 4.399333333333333,
      "grad_norm": 0.428276926279068,
      "learning_rate": 2.2504166666666667e-05,
      "loss": 0.002,
      "step": 131980
    },
    {
      "epoch": 4.399666666666667,
      "grad_norm": 0.05702805519104004,
      "learning_rate": 2.2502083333333336e-05,
      "loss": 0.0025,
      "step": 131990
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.0859505832195282,
      "learning_rate": 2.25e-05,
      "loss": 0.0024,
      "step": 132000
    },
    {
      "epoch": 4.400333333333333,
      "grad_norm": 0.1877771019935608,
      "learning_rate": 2.2497916666666667e-05,
      "loss": 0.0023,
      "step": 132010
    },
    {
      "epoch": 4.400666666666667,
      "grad_norm": 0.11995575577020645,
      "learning_rate": 2.2495833333333336e-05,
      "loss": 0.0021,
      "step": 132020
    },
    {
      "epoch": 4.401,
      "grad_norm": 0.11447189003229141,
      "learning_rate": 2.249375e-05,
      "loss": 0.0022,
      "step": 132030
    },
    {
      "epoch": 4.4013333333333335,
      "grad_norm": 0.08599832653999329,
      "learning_rate": 2.2491666666666667e-05,
      "loss": 0.0022,
      "step": 132040
    },
    {
      "epoch": 4.401666666666666,
      "grad_norm": 0.2645547091960907,
      "learning_rate": 2.2489583333333332e-05,
      "loss": 0.0019,
      "step": 132050
    },
    {
      "epoch": 4.402,
      "grad_norm": 0.45650967955589294,
      "learning_rate": 2.24875e-05,
      "loss": 0.0022,
      "step": 132060
    },
    {
      "epoch": 4.402333333333333,
      "grad_norm": 0.25945156812667847,
      "learning_rate": 2.2485416666666667e-05,
      "loss": 0.0023,
      "step": 132070
    },
    {
      "epoch": 4.402666666666667,
      "grad_norm": 0.3719208836555481,
      "learning_rate": 2.2483333333333335e-05,
      "loss": 0.002,
      "step": 132080
    },
    {
      "epoch": 4.4030000000000005,
      "grad_norm": 0.02953086979687214,
      "learning_rate": 2.248125e-05,
      "loss": 0.0018,
      "step": 132090
    },
    {
      "epoch": 4.403333333333333,
      "grad_norm": 0.029619241133332253,
      "learning_rate": 2.2479166666666666e-05,
      "loss": 0.0027,
      "step": 132100
    },
    {
      "epoch": 4.403666666666667,
      "grad_norm": 0.22805123031139374,
      "learning_rate": 2.2477083333333335e-05,
      "loss": 0.0027,
      "step": 132110
    },
    {
      "epoch": 4.404,
      "grad_norm": 0.6207937002182007,
      "learning_rate": 2.2475e-05,
      "loss": 0.0025,
      "step": 132120
    },
    {
      "epoch": 4.404333333333334,
      "grad_norm": 0.5134952664375305,
      "learning_rate": 2.247291666666667e-05,
      "loss": 0.0029,
      "step": 132130
    },
    {
      "epoch": 4.4046666666666665,
      "grad_norm": 0.6976594924926758,
      "learning_rate": 2.2470833333333335e-05,
      "loss": 0.0021,
      "step": 132140
    },
    {
      "epoch": 4.405,
      "grad_norm": 0.34236788749694824,
      "learning_rate": 2.246875e-05,
      "loss": 0.002,
      "step": 132150
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.1717306673526764,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0026,
      "step": 132160
    },
    {
      "epoch": 4.405666666666667,
      "grad_norm": 0.14378084242343903,
      "learning_rate": 2.246458333333333e-05,
      "loss": 0.0023,
      "step": 132170
    },
    {
      "epoch": 4.406,
      "grad_norm": 0.2567894160747528,
      "learning_rate": 2.24625e-05,
      "loss": 0.0026,
      "step": 132180
    },
    {
      "epoch": 4.406333333333333,
      "grad_norm": 0.3138059973716736,
      "learning_rate": 2.2460416666666666e-05,
      "loss": 0.0021,
      "step": 132190
    },
    {
      "epoch": 4.406666666666666,
      "grad_norm": 0.27613261342048645,
      "learning_rate": 2.2458333333333335e-05,
      "loss": 0.002,
      "step": 132200
    },
    {
      "epoch": 4.407,
      "grad_norm": 0.2566749155521393,
      "learning_rate": 2.245625e-05,
      "loss": 0.0028,
      "step": 132210
    },
    {
      "epoch": 4.407333333333334,
      "grad_norm": 0.006691244430840015,
      "learning_rate": 2.245416666666667e-05,
      "loss": 0.0028,
      "step": 132220
    },
    {
      "epoch": 4.407666666666667,
      "grad_norm": 0.2854807376861572,
      "learning_rate": 2.2452083333333335e-05,
      "loss": 0.0018,
      "step": 132230
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.03425974026322365,
      "learning_rate": 2.245e-05,
      "loss": 0.002,
      "step": 132240
    },
    {
      "epoch": 4.408333333333333,
      "grad_norm": 0.22807209193706512,
      "learning_rate": 2.244791666666667e-05,
      "loss": 0.0018,
      "step": 132250
    },
    {
      "epoch": 4.408666666666667,
      "grad_norm": 0.05266188830137253,
      "learning_rate": 2.2445833333333335e-05,
      "loss": 0.0029,
      "step": 132260
    },
    {
      "epoch": 4.409,
      "grad_norm": 0.1431644856929779,
      "learning_rate": 2.2443750000000003e-05,
      "loss": 0.0025,
      "step": 132270
    },
    {
      "epoch": 4.4093333333333335,
      "grad_norm": 0.03379962593317032,
      "learning_rate": 2.2441666666666666e-05,
      "loss": 0.0023,
      "step": 132280
    },
    {
      "epoch": 4.409666666666666,
      "grad_norm": 0.04116976633667946,
      "learning_rate": 2.2439583333333334e-05,
      "loss": 0.0018,
      "step": 132290
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.4254499077796936,
      "learning_rate": 2.24375e-05,
      "loss": 0.0029,
      "step": 132300
    },
    {
      "epoch": 4.410333333333333,
      "grad_norm": 0.22899207472801208,
      "learning_rate": 2.243541666666667e-05,
      "loss": 0.0016,
      "step": 132310
    },
    {
      "epoch": 4.410666666666667,
      "grad_norm": 0.5640835165977478,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.0033,
      "step": 132320
    },
    {
      "epoch": 4.411,
      "grad_norm": 0.31574735045433044,
      "learning_rate": 2.243125e-05,
      "loss": 0.002,
      "step": 132330
    },
    {
      "epoch": 4.411333333333333,
      "grad_norm": 0.19971555471420288,
      "learning_rate": 2.242916666666667e-05,
      "loss": 0.0014,
      "step": 132340
    },
    {
      "epoch": 4.411666666666667,
      "grad_norm": 0.3706415593624115,
      "learning_rate": 2.2427083333333334e-05,
      "loss": 0.0019,
      "step": 132350
    },
    {
      "epoch": 4.412,
      "grad_norm": 0.17084991931915283,
      "learning_rate": 2.2425000000000003e-05,
      "loss": 0.0018,
      "step": 132360
    },
    {
      "epoch": 4.412333333333334,
      "grad_norm": 0.1759360134601593,
      "learning_rate": 2.242291666666667e-05,
      "loss": 0.002,
      "step": 132370
    },
    {
      "epoch": 4.4126666666666665,
      "grad_norm": 0.08618423342704773,
      "learning_rate": 2.2420833333333334e-05,
      "loss": 0.0022,
      "step": 132380
    },
    {
      "epoch": 4.413,
      "grad_norm": 0.08549893647432327,
      "learning_rate": 2.2418750000000003e-05,
      "loss": 0.0019,
      "step": 132390
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.029543841257691383,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.0023,
      "step": 132400
    },
    {
      "epoch": 4.413666666666667,
      "grad_norm": 0.40271836519241333,
      "learning_rate": 2.2414583333333334e-05,
      "loss": 0.0019,
      "step": 132410
    },
    {
      "epoch": 4.414,
      "grad_norm": 0.20021095871925354,
      "learning_rate": 2.24125e-05,
      "loss": 0.0018,
      "step": 132420
    },
    {
      "epoch": 4.414333333333333,
      "grad_norm": 0.11804737150669098,
      "learning_rate": 2.2410416666666668e-05,
      "loss": 0.0027,
      "step": 132430
    },
    {
      "epoch": 4.414666666666666,
      "grad_norm": 0.43888670206069946,
      "learning_rate": 2.2408333333333334e-05,
      "loss": 0.0017,
      "step": 132440
    },
    {
      "epoch": 4.415,
      "grad_norm": 0.22799910604953766,
      "learning_rate": 2.2406250000000003e-05,
      "loss": 0.0023,
      "step": 132450
    },
    {
      "epoch": 4.415333333333333,
      "grad_norm": 0.03126005455851555,
      "learning_rate": 2.2404166666666668e-05,
      "loss": 0.0021,
      "step": 132460
    },
    {
      "epoch": 4.415666666666667,
      "grad_norm": 0.3422139883041382,
      "learning_rate": 2.2402083333333333e-05,
      "loss": 0.0018,
      "step": 132470
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.2004779726266861,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0014,
      "step": 132480
    },
    {
      "epoch": 4.416333333333333,
      "grad_norm": 0.04978242143988609,
      "learning_rate": 2.2397916666666668e-05,
      "loss": 0.002,
      "step": 132490
    },
    {
      "epoch": 4.416666666666667,
      "grad_norm": 0.057575616985559464,
      "learning_rate": 2.2395833333333337e-05,
      "loss": 0.0015,
      "step": 132500
    },
    {
      "epoch": 4.417,
      "grad_norm": 0.11396455019712448,
      "learning_rate": 2.2393750000000002e-05,
      "loss": 0.0018,
      "step": 132510
    },
    {
      "epoch": 4.417333333333334,
      "grad_norm": 0.42783427238464355,
      "learning_rate": 2.2391666666666668e-05,
      "loss": 0.0014,
      "step": 132520
    },
    {
      "epoch": 4.417666666666666,
      "grad_norm": 0.34289035201072693,
      "learning_rate": 2.2389583333333333e-05,
      "loss": 0.0015,
      "step": 132530
    },
    {
      "epoch": 4.418,
      "grad_norm": 0.8274301886558533,
      "learning_rate": 2.23875e-05,
      "loss": 0.0014,
      "step": 132540
    },
    {
      "epoch": 4.418333333333333,
      "grad_norm": 0.34299734234809875,
      "learning_rate": 2.2385416666666668e-05,
      "loss": 0.0029,
      "step": 132550
    },
    {
      "epoch": 4.418666666666667,
      "grad_norm": 0.1999237835407257,
      "learning_rate": 2.2383333333333333e-05,
      "loss": 0.0015,
      "step": 132560
    },
    {
      "epoch": 4.419,
      "grad_norm": 0.08611904829740524,
      "learning_rate": 2.2381250000000002e-05,
      "loss": 0.0021,
      "step": 132570
    },
    {
      "epoch": 4.419333333333333,
      "grad_norm": 0.02956850826740265,
      "learning_rate": 2.2379166666666667e-05,
      "loss": 0.0027,
      "step": 132580
    },
    {
      "epoch": 4.419666666666667,
      "grad_norm": 0.05798143520951271,
      "learning_rate": 2.2377083333333336e-05,
      "loss": 0.0019,
      "step": 132590
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.2279711365699768,
      "learning_rate": 2.2375000000000002e-05,
      "loss": 0.0015,
      "step": 132600
    },
    {
      "epoch": 4.420333333333334,
      "grad_norm": 0.08219662308692932,
      "learning_rate": 2.2372916666666667e-05,
      "loss": 0.0023,
      "step": 132610
    },
    {
      "epoch": 4.4206666666666665,
      "grad_norm": 0.13969388604164124,
      "learning_rate": 2.2370833333333336e-05,
      "loss": 0.0018,
      "step": 132620
    },
    {
      "epoch": 4.421,
      "grad_norm": 0.11425922811031342,
      "learning_rate": 2.236875e-05,
      "loss": 0.0023,
      "step": 132630
    },
    {
      "epoch": 4.421333333333333,
      "grad_norm": 0.05716158077120781,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.0025,
      "step": 132640
    },
    {
      "epoch": 4.421666666666667,
      "grad_norm": 0.20052912831306458,
      "learning_rate": 2.2364583333333333e-05,
      "loss": 0.0028,
      "step": 132650
    },
    {
      "epoch": 4.422,
      "grad_norm": 0.31351238489151,
      "learning_rate": 2.23625e-05,
      "loss": 0.0035,
      "step": 132660
    },
    {
      "epoch": 4.4223333333333334,
      "grad_norm": 0.19996599853038788,
      "learning_rate": 2.2360416666666667e-05,
      "loss": 0.0018,
      "step": 132670
    },
    {
      "epoch": 4.422666666666666,
      "grad_norm": 0.11425507068634033,
      "learning_rate": 2.2358333333333332e-05,
      "loss": 0.0015,
      "step": 132680
    },
    {
      "epoch": 4.423,
      "grad_norm": 0.14944486320018768,
      "learning_rate": 2.235625e-05,
      "loss": 0.0037,
      "step": 132690
    },
    {
      "epoch": 4.423333333333334,
      "grad_norm": 0.029366519302129745,
      "learning_rate": 2.2354166666666667e-05,
      "loss": 0.0015,
      "step": 132700
    },
    {
      "epoch": 4.423666666666667,
      "grad_norm": 0.31706681847572327,
      "learning_rate": 2.2352083333333336e-05,
      "loss": 0.0022,
      "step": 132710
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.08601255714893341,
      "learning_rate": 2.235e-05,
      "loss": 0.0022,
      "step": 132720
    },
    {
      "epoch": 4.424333333333333,
      "grad_norm": 0.5132216215133667,
      "learning_rate": 2.234791666666667e-05,
      "loss": 0.0017,
      "step": 132730
    },
    {
      "epoch": 4.424666666666667,
      "grad_norm": 0.1711839884519577,
      "learning_rate": 2.2345833333333336e-05,
      "loss": 0.0018,
      "step": 132740
    },
    {
      "epoch": 4.425,
      "grad_norm": 0.08705708384513855,
      "learning_rate": 2.234375e-05,
      "loss": 0.0018,
      "step": 132750
    },
    {
      "epoch": 4.425333333333334,
      "grad_norm": 0.14266878366470337,
      "learning_rate": 2.234166666666667e-05,
      "loss": 0.0023,
      "step": 132760
    },
    {
      "epoch": 4.425666666666666,
      "grad_norm": 0.371446818113327,
      "learning_rate": 2.2339583333333332e-05,
      "loss": 0.0018,
      "step": 132770
    },
    {
      "epoch": 4.426,
      "grad_norm": 0.0640481486916542,
      "learning_rate": 2.23375e-05,
      "loss": 0.0019,
      "step": 132780
    },
    {
      "epoch": 4.426333333333333,
      "grad_norm": 0.14777372777462006,
      "learning_rate": 2.2335416666666666e-05,
      "loss": 0.002,
      "step": 132790
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.08696980029344559,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.002,
      "step": 132800
    },
    {
      "epoch": 4.427,
      "grad_norm": 0.20045392215251923,
      "learning_rate": 2.233125e-05,
      "loss": 0.0015,
      "step": 132810
    },
    {
      "epoch": 4.427333333333333,
      "grad_norm": 0.11577790230512619,
      "learning_rate": 2.2329166666666666e-05,
      "loss": 0.0025,
      "step": 132820
    },
    {
      "epoch": 4.427666666666667,
      "grad_norm": 0.42854374647140503,
      "learning_rate": 2.2327083333333335e-05,
      "loss": 0.0014,
      "step": 132830
    },
    {
      "epoch": 4.428,
      "grad_norm": 0.5422930717468262,
      "learning_rate": 2.2325e-05,
      "loss": 0.0021,
      "step": 132840
    },
    {
      "epoch": 4.428333333333334,
      "grad_norm": 0.009227637201547623,
      "learning_rate": 2.232291666666667e-05,
      "loss": 0.003,
      "step": 132850
    },
    {
      "epoch": 4.4286666666666665,
      "grad_norm": 0.2850174903869629,
      "learning_rate": 2.2320833333333335e-05,
      "loss": 0.0012,
      "step": 132860
    },
    {
      "epoch": 4.429,
      "grad_norm": 0.2581823170185089,
      "learning_rate": 2.2318750000000004e-05,
      "loss": 0.002,
      "step": 132870
    },
    {
      "epoch": 4.429333333333333,
      "grad_norm": 0.029903659597039223,
      "learning_rate": 2.231666666666667e-05,
      "loss": 0.0015,
      "step": 132880
    },
    {
      "epoch": 4.429666666666667,
      "grad_norm": 0.14382565021514893,
      "learning_rate": 2.2314583333333335e-05,
      "loss": 0.0018,
      "step": 132890
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.14365951716899872,
      "learning_rate": 2.23125e-05,
      "loss": 0.0024,
      "step": 132900
    },
    {
      "epoch": 4.4303333333333335,
      "grad_norm": 0.14263762533664703,
      "learning_rate": 2.2310416666666666e-05,
      "loss": 0.0014,
      "step": 132910
    },
    {
      "epoch": 4.430666666666666,
      "grad_norm": 0.1995663344860077,
      "learning_rate": 2.2308333333333335e-05,
      "loss": 0.002,
      "step": 132920
    },
    {
      "epoch": 4.431,
      "grad_norm": 0.114437535405159,
      "learning_rate": 2.230625e-05,
      "loss": 0.0019,
      "step": 132930
    },
    {
      "epoch": 4.431333333333333,
      "grad_norm": 0.22856806218624115,
      "learning_rate": 2.230416666666667e-05,
      "loss": 0.0018,
      "step": 132940
    },
    {
      "epoch": 4.431666666666667,
      "grad_norm": 0.44912752509117126,
      "learning_rate": 2.2302083333333334e-05,
      "loss": 0.0021,
      "step": 132950
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.40535253286361694,
      "learning_rate": 2.23e-05,
      "loss": 0.002,
      "step": 132960
    },
    {
      "epoch": 4.432333333333333,
      "grad_norm": 0.2569981813430786,
      "learning_rate": 2.229791666666667e-05,
      "loss": 0.0024,
      "step": 132970
    },
    {
      "epoch": 4.432666666666667,
      "grad_norm": 0.09577016532421112,
      "learning_rate": 2.2295833333333334e-05,
      "loss": 0.0018,
      "step": 132980
    },
    {
      "epoch": 4.433,
      "grad_norm": 0.09034540504217148,
      "learning_rate": 2.2293750000000003e-05,
      "loss": 0.0019,
      "step": 132990
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 0.05747056379914284,
      "learning_rate": 2.229166666666667e-05,
      "loss": 0.0025,
      "step": 133000
    },
    {
      "epoch": 4.433666666666666,
      "grad_norm": 0.429495245218277,
      "learning_rate": 2.2289583333333334e-05,
      "loss": 0.0025,
      "step": 133010
    },
    {
      "epoch": 4.434,
      "grad_norm": 0.006184238474816084,
      "learning_rate": 2.22875e-05,
      "loss": 0.0023,
      "step": 133020
    },
    {
      "epoch": 4.434333333333333,
      "grad_norm": 0.28527650237083435,
      "learning_rate": 2.228541666666667e-05,
      "loss": 0.0033,
      "step": 133030
    },
    {
      "epoch": 4.434666666666667,
      "grad_norm": 0.46211713552474976,
      "learning_rate": 2.2283333333333334e-05,
      "loss": 0.0023,
      "step": 133040
    },
    {
      "epoch": 4.435,
      "grad_norm": 0.5419961810112,
      "learning_rate": 2.228125e-05,
      "loss": 0.0017,
      "step": 133050
    },
    {
      "epoch": 4.435333333333333,
      "grad_norm": 0.058445386588573456,
      "learning_rate": 2.227916666666667e-05,
      "loss": 0.0019,
      "step": 133060
    },
    {
      "epoch": 4.435666666666666,
      "grad_norm": 0.06830333173274994,
      "learning_rate": 2.2277083333333334e-05,
      "loss": 0.0032,
      "step": 133070
    },
    {
      "epoch": 4.436,
      "grad_norm": 0.029349420219659805,
      "learning_rate": 2.2275000000000003e-05,
      "loss": 0.0019,
      "step": 133080
    },
    {
      "epoch": 4.436333333333334,
      "grad_norm": 0.0857134461402893,
      "learning_rate": 2.2272916666666668e-05,
      "loss": 0.0016,
      "step": 133090
    },
    {
      "epoch": 4.4366666666666665,
      "grad_norm": 0.18028004467487335,
      "learning_rate": 2.2270833333333334e-05,
      "loss": 0.0014,
      "step": 133100
    },
    {
      "epoch": 4.437,
      "grad_norm": 0.1998482644557953,
      "learning_rate": 2.2268750000000003e-05,
      "loss": 0.003,
      "step": 133110
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.34326013922691345,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0019,
      "step": 133120
    },
    {
      "epoch": 4.437666666666667,
      "grad_norm": 0.11421291530132294,
      "learning_rate": 2.2264583333333334e-05,
      "loss": 0.0024,
      "step": 133130
    },
    {
      "epoch": 4.438,
      "grad_norm": 0.05754843354225159,
      "learning_rate": 2.22625e-05,
      "loss": 0.002,
      "step": 133140
    },
    {
      "epoch": 4.4383333333333335,
      "grad_norm": 0.14255604147911072,
      "learning_rate": 2.2260416666666668e-05,
      "loss": 0.0022,
      "step": 133150
    },
    {
      "epoch": 4.438666666666666,
      "grad_norm": 0.1718873530626297,
      "learning_rate": 2.2258333333333333e-05,
      "loss": 0.002,
      "step": 133160
    },
    {
      "epoch": 4.439,
      "grad_norm": 0.5347909331321716,
      "learning_rate": 2.2256250000000002e-05,
      "loss": 0.0012,
      "step": 133170
    },
    {
      "epoch": 4.439333333333334,
      "grad_norm": 0.1544385701417923,
      "learning_rate": 2.2254166666666668e-05,
      "loss": 0.0017,
      "step": 133180
    },
    {
      "epoch": 4.439666666666667,
      "grad_norm": 0.5704799294471741,
      "learning_rate": 2.2252083333333333e-05,
      "loss": 0.0018,
      "step": 133190
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.3992636799812317,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.0017,
      "step": 133200
    },
    {
      "epoch": 4.440333333333333,
      "grad_norm": 0.03508392721414566,
      "learning_rate": 2.2247916666666668e-05,
      "loss": 0.0028,
      "step": 133210
    },
    {
      "epoch": 4.440666666666667,
      "grad_norm": 0.11458923667669296,
      "learning_rate": 2.2245833333333336e-05,
      "loss": 0.0019,
      "step": 133220
    },
    {
      "epoch": 4.441,
      "grad_norm": 0.0857563391327858,
      "learning_rate": 2.2243750000000002e-05,
      "loss": 0.0022,
      "step": 133230
    },
    {
      "epoch": 4.441333333333334,
      "grad_norm": 0.08681398630142212,
      "learning_rate": 2.2241666666666667e-05,
      "loss": 0.0035,
      "step": 133240
    },
    {
      "epoch": 4.441666666666666,
      "grad_norm": 0.06163487210869789,
      "learning_rate": 2.2239583333333333e-05,
      "loss": 0.0013,
      "step": 133250
    },
    {
      "epoch": 4.442,
      "grad_norm": 0.22919882833957672,
      "learning_rate": 2.22375e-05,
      "loss": 0.0036,
      "step": 133260
    },
    {
      "epoch": 4.442333333333333,
      "grad_norm": 0.20251914858818054,
      "learning_rate": 2.2235416666666667e-05,
      "loss": 0.0021,
      "step": 133270
    },
    {
      "epoch": 4.442666666666667,
      "grad_norm": 0.005119505804032087,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0021,
      "step": 133280
    },
    {
      "epoch": 4.443,
      "grad_norm": 0.7167067527770996,
      "learning_rate": 2.223125e-05,
      "loss": 0.0023,
      "step": 133290
    },
    {
      "epoch": 4.443333333333333,
      "grad_norm": 0.1709957718849182,
      "learning_rate": 2.2229166666666667e-05,
      "loss": 0.0018,
      "step": 133300
    },
    {
      "epoch": 4.443666666666667,
      "grad_norm": 0.28536853194236755,
      "learning_rate": 2.2227083333333336e-05,
      "loss": 0.002,
      "step": 133310
    },
    {
      "epoch": 4.444,
      "grad_norm": 0.22843997180461884,
      "learning_rate": 2.2225e-05,
      "loss": 0.0022,
      "step": 133320
    },
    {
      "epoch": 4.444333333333334,
      "grad_norm": 0.14300493896007538,
      "learning_rate": 2.2222916666666667e-05,
      "loss": 0.0029,
      "step": 133330
    },
    {
      "epoch": 4.4446666666666665,
      "grad_norm": 0.22806969285011292,
      "learning_rate": 2.2220833333333336e-05,
      "loss": 0.0015,
      "step": 133340
    },
    {
      "epoch": 4.445,
      "grad_norm": 0.08674002438783646,
      "learning_rate": 2.221875e-05,
      "loss": 0.0023,
      "step": 133350
    },
    {
      "epoch": 4.445333333333333,
      "grad_norm": 0.3978906273841858,
      "learning_rate": 2.221666666666667e-05,
      "loss": 0.0016,
      "step": 133360
    },
    {
      "epoch": 4.445666666666667,
      "grad_norm": 0.11411760002374649,
      "learning_rate": 2.2214583333333332e-05,
      "loss": 0.0018,
      "step": 133370
    },
    {
      "epoch": 4.446,
      "grad_norm": 0.171232670545578,
      "learning_rate": 2.22125e-05,
      "loss": 0.0023,
      "step": 133380
    },
    {
      "epoch": 4.4463333333333335,
      "grad_norm": 0.14346492290496826,
      "learning_rate": 2.2210416666666667e-05,
      "loss": 0.0031,
      "step": 133390
    },
    {
      "epoch": 4.446666666666666,
      "grad_norm": 0.057468876242637634,
      "learning_rate": 2.2208333333333332e-05,
      "loss": 0.0016,
      "step": 133400
    },
    {
      "epoch": 4.447,
      "grad_norm": 0.23174728453159332,
      "learning_rate": 2.220625e-05,
      "loss": 0.0019,
      "step": 133410
    },
    {
      "epoch": 4.447333333333333,
      "grad_norm": 0.08862891793251038,
      "learning_rate": 2.2204166666666667e-05,
      "loss": 0.0027,
      "step": 133420
    },
    {
      "epoch": 4.447666666666667,
      "grad_norm": 0.08622536063194275,
      "learning_rate": 2.2202083333333335e-05,
      "loss": 0.0018,
      "step": 133430
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.17162977159023285,
      "learning_rate": 2.22e-05,
      "loss": 0.0019,
      "step": 133440
    },
    {
      "epoch": 4.448333333333333,
      "grad_norm": 0.030853331089019775,
      "learning_rate": 2.219791666666667e-05,
      "loss": 0.0013,
      "step": 133450
    },
    {
      "epoch": 4.448666666666667,
      "grad_norm": 0.4564252197742462,
      "learning_rate": 2.2195833333333335e-05,
      "loss": 0.0013,
      "step": 133460
    },
    {
      "epoch": 4.449,
      "grad_norm": 0.1840839385986328,
      "learning_rate": 2.219375e-05,
      "loss": 0.0026,
      "step": 133470
    },
    {
      "epoch": 4.449333333333334,
      "grad_norm": 0.03302164003252983,
      "learning_rate": 2.219166666666667e-05,
      "loss": 0.0023,
      "step": 133480
    },
    {
      "epoch": 4.449666666666666,
      "grad_norm": 0.10514429956674576,
      "learning_rate": 2.2189583333333332e-05,
      "loss": 0.0018,
      "step": 133490
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.029443154111504555,
      "learning_rate": 2.21875e-05,
      "loss": 0.0022,
      "step": 133500
    },
    {
      "epoch": 4.450333333333333,
      "grad_norm": 0.4783829152584076,
      "learning_rate": 2.2185416666666666e-05,
      "loss": 0.0021,
      "step": 133510
    },
    {
      "epoch": 4.450666666666667,
      "grad_norm": 0.1998354196548462,
      "learning_rate": 2.2183333333333335e-05,
      "loss": 0.0024,
      "step": 133520
    },
    {
      "epoch": 4.451,
      "grad_norm": 0.12467402964830399,
      "learning_rate": 2.218125e-05,
      "loss": 0.0016,
      "step": 133530
    },
    {
      "epoch": 4.451333333333333,
      "grad_norm": 0.2856382131576538,
      "learning_rate": 2.2179166666666666e-05,
      "loss": 0.0018,
      "step": 133540
    },
    {
      "epoch": 4.451666666666666,
      "grad_norm": 0.25702765583992004,
      "learning_rate": 2.2177083333333335e-05,
      "loss": 0.0023,
      "step": 133550
    },
    {
      "epoch": 4.452,
      "grad_norm": 0.08606881648302078,
      "learning_rate": 2.2175e-05,
      "loss": 0.002,
      "step": 133560
    },
    {
      "epoch": 4.452333333333334,
      "grad_norm": 0.1139385774731636,
      "learning_rate": 2.217291666666667e-05,
      "loss": 0.0017,
      "step": 133570
    },
    {
      "epoch": 4.4526666666666666,
      "grad_norm": 0.5134071707725525,
      "learning_rate": 2.2170833333333335e-05,
      "loss": 0.0025,
      "step": 133580
    },
    {
      "epoch": 4.453,
      "grad_norm": 0.1713181883096695,
      "learning_rate": 2.2168750000000004e-05,
      "loss": 0.0015,
      "step": 133590
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.11447147279977798,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0021,
      "step": 133600
    },
    {
      "epoch": 4.453666666666667,
      "grad_norm": 0.05792282894253731,
      "learning_rate": 2.2164583333333335e-05,
      "loss": 0.0023,
      "step": 133610
    },
    {
      "epoch": 4.454,
      "grad_norm": 0.43016302585601807,
      "learning_rate": 2.21625e-05,
      "loss": 0.0018,
      "step": 133620
    },
    {
      "epoch": 4.4543333333333335,
      "grad_norm": 0.2567349076271057,
      "learning_rate": 2.2160416666666665e-05,
      "loss": 0.0029,
      "step": 133630
    },
    {
      "epoch": 4.454666666666666,
      "grad_norm": 0.14259664714336395,
      "learning_rate": 2.2158333333333334e-05,
      "loss": 0.002,
      "step": 133640
    },
    {
      "epoch": 4.455,
      "grad_norm": 0.14323072135448456,
      "learning_rate": 2.215625e-05,
      "loss": 0.002,
      "step": 133650
    },
    {
      "epoch": 4.455333333333333,
      "grad_norm": 0.14243055880069733,
      "learning_rate": 2.215416666666667e-05,
      "loss": 0.0036,
      "step": 133660
    },
    {
      "epoch": 4.455666666666667,
      "grad_norm": 0.05227759853005409,
      "learning_rate": 2.2152083333333334e-05,
      "loss": 0.0025,
      "step": 133670
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.2856394946575165,
      "learning_rate": 2.215e-05,
      "loss": 0.0018,
      "step": 133680
    },
    {
      "epoch": 4.456333333333333,
      "grad_norm": 0.14295998215675354,
      "learning_rate": 2.214791666666667e-05,
      "loss": 0.0015,
      "step": 133690
    },
    {
      "epoch": 4.456666666666667,
      "grad_norm": 0.11447753012180328,
      "learning_rate": 2.2145833333333334e-05,
      "loss": 0.0018,
      "step": 133700
    },
    {
      "epoch": 4.457,
      "grad_norm": 0.006814619060605764,
      "learning_rate": 2.2143750000000003e-05,
      "loss": 0.0011,
      "step": 133710
    },
    {
      "epoch": 4.457333333333334,
      "grad_norm": 0.2853466868400574,
      "learning_rate": 2.214166666666667e-05,
      "loss": 0.0024,
      "step": 133720
    },
    {
      "epoch": 4.457666666666666,
      "grad_norm": 0.08680328726768494,
      "learning_rate": 2.2139583333333334e-05,
      "loss": 0.0019,
      "step": 133730
    },
    {
      "epoch": 4.458,
      "grad_norm": 0.14277972280979156,
      "learning_rate": 2.21375e-05,
      "loss": 0.0022,
      "step": 133740
    },
    {
      "epoch": 4.458333333333333,
      "grad_norm": 0.13299134373664856,
      "learning_rate": 2.2135416666666668e-05,
      "loss": 0.0024,
      "step": 133750
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.03487833961844444,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0014,
      "step": 133760
    },
    {
      "epoch": 4.459,
      "grad_norm": 0.34226563572883606,
      "learning_rate": 2.213125e-05,
      "loss": 0.0019,
      "step": 133770
    },
    {
      "epoch": 4.459333333333333,
      "grad_norm": 0.3421066105365753,
      "learning_rate": 2.2129166666666668e-05,
      "loss": 0.0014,
      "step": 133780
    },
    {
      "epoch": 4.459666666666667,
      "grad_norm": 0.1996990591287613,
      "learning_rate": 2.2127083333333334e-05,
      "loss": 0.0021,
      "step": 133790
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.2279653400182724,
      "learning_rate": 2.2125000000000002e-05,
      "loss": 0.0024,
      "step": 133800
    },
    {
      "epoch": 4.460333333333334,
      "grad_norm": 0.42595958709716797,
      "learning_rate": 2.2122916666666668e-05,
      "loss": 0.0026,
      "step": 133810
    },
    {
      "epoch": 4.460666666666667,
      "grad_norm": 0.6567200422286987,
      "learning_rate": 2.2120833333333333e-05,
      "loss": 0.0015,
      "step": 133820
    },
    {
      "epoch": 4.461,
      "grad_norm": 0.47553080320358276,
      "learning_rate": 2.2118750000000002e-05,
      "loss": 0.0019,
      "step": 133830
    },
    {
      "epoch": 4.461333333333333,
      "grad_norm": 0.3425203263759613,
      "learning_rate": 2.2116666666666668e-05,
      "loss": 0.002,
      "step": 133840
    },
    {
      "epoch": 4.461666666666667,
      "grad_norm": 0.11803685128688812,
      "learning_rate": 2.2114583333333337e-05,
      "loss": 0.0017,
      "step": 133850
    },
    {
      "epoch": 4.462,
      "grad_norm": 0.5627071261405945,
      "learning_rate": 2.21125e-05,
      "loss": 0.0018,
      "step": 133860
    },
    {
      "epoch": 4.4623333333333335,
      "grad_norm": 0.14291417598724365,
      "learning_rate": 2.2110416666666668e-05,
      "loss": 0.002,
      "step": 133870
    },
    {
      "epoch": 4.462666666666666,
      "grad_norm": 0.0725414827466011,
      "learning_rate": 2.2108333333333333e-05,
      "loss": 0.0025,
      "step": 133880
    },
    {
      "epoch": 4.463,
      "grad_norm": 0.05050639808177948,
      "learning_rate": 2.2106250000000002e-05,
      "loss": 0.003,
      "step": 133890
    },
    {
      "epoch": 4.463333333333333,
      "grad_norm": 0.10944999754428864,
      "learning_rate": 2.2104166666666667e-05,
      "loss": 0.0026,
      "step": 133900
    },
    {
      "epoch": 4.463666666666667,
      "grad_norm": 0.2021806687116623,
      "learning_rate": 2.2102083333333333e-05,
      "loss": 0.0014,
      "step": 133910
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.08572614192962646,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0023,
      "step": 133920
    },
    {
      "epoch": 4.464333333333333,
      "grad_norm": 0.2282550036907196,
      "learning_rate": 2.2097916666666667e-05,
      "loss": 0.0022,
      "step": 133930
    },
    {
      "epoch": 4.464666666666667,
      "grad_norm": 0.057360634207725525,
      "learning_rate": 2.2095833333333336e-05,
      "loss": 0.0024,
      "step": 133940
    },
    {
      "epoch": 4.465,
      "grad_norm": 0.22829127311706543,
      "learning_rate": 2.2093750000000002e-05,
      "loss": 0.0022,
      "step": 133950
    },
    {
      "epoch": 4.465333333333334,
      "grad_norm": 0.11404436826705933,
      "learning_rate": 2.2091666666666667e-05,
      "loss": 0.0021,
      "step": 133960
    },
    {
      "epoch": 4.4656666666666665,
      "grad_norm": 0.4880269169807434,
      "learning_rate": 2.2089583333333336e-05,
      "loss": 0.003,
      "step": 133970
    },
    {
      "epoch": 4.466,
      "grad_norm": 0.005796407349407673,
      "learning_rate": 2.2087499999999998e-05,
      "loss": 0.0015,
      "step": 133980
    },
    {
      "epoch": 4.466333333333333,
      "grad_norm": 0.17163121700286865,
      "learning_rate": 2.2085416666666667e-05,
      "loss": 0.0026,
      "step": 133990
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.09679712355136871,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.0023,
      "step": 134000
    },
    {
      "epoch": 4.467,
      "grad_norm": 0.12745539844036102,
      "learning_rate": 2.208125e-05,
      "loss": 0.0021,
      "step": 134010
    },
    {
      "epoch": 4.467333333333333,
      "grad_norm": 0.05739045888185501,
      "learning_rate": 2.2079166666666667e-05,
      "loss": 0.002,
      "step": 134020
    },
    {
      "epoch": 4.467666666666666,
      "grad_norm": 0.11430243402719498,
      "learning_rate": 2.2077083333333336e-05,
      "loss": 0.0015,
      "step": 134030
    },
    {
      "epoch": 4.468,
      "grad_norm": 0.32392409443855286,
      "learning_rate": 2.2075e-05,
      "loss": 0.002,
      "step": 134040
    },
    {
      "epoch": 4.468333333333334,
      "grad_norm": 0.14310722053050995,
      "learning_rate": 2.2072916666666667e-05,
      "loss": 0.0023,
      "step": 134050
    },
    {
      "epoch": 4.468666666666667,
      "grad_norm": 0.1434532105922699,
      "learning_rate": 2.2070833333333336e-05,
      "loss": 0.0021,
      "step": 134060
    },
    {
      "epoch": 4.469,
      "grad_norm": 0.11437731981277466,
      "learning_rate": 2.206875e-05,
      "loss": 0.0016,
      "step": 134070
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.6277791857719421,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0031,
      "step": 134080
    },
    {
      "epoch": 4.469666666666667,
      "grad_norm": 0.6217372417449951,
      "learning_rate": 2.2064583333333335e-05,
      "loss": 0.0022,
      "step": 134090
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.31407004594802856,
      "learning_rate": 2.20625e-05,
      "loss": 0.0029,
      "step": 134100
    },
    {
      "epoch": 4.4703333333333335,
      "grad_norm": 0.08561158180236816,
      "learning_rate": 2.2060416666666666e-05,
      "loss": 0.0017,
      "step": 134110
    },
    {
      "epoch": 4.470666666666666,
      "grad_norm": 0.11506877094507217,
      "learning_rate": 2.2058333333333335e-05,
      "loss": 0.0024,
      "step": 134120
    },
    {
      "epoch": 4.471,
      "grad_norm": 0.08607758581638336,
      "learning_rate": 2.205625e-05,
      "loss": 0.0021,
      "step": 134130
    },
    {
      "epoch": 4.471333333333333,
      "grad_norm": 0.142689511179924,
      "learning_rate": 2.2054166666666666e-05,
      "loss": 0.0032,
      "step": 134140
    },
    {
      "epoch": 4.471666666666667,
      "grad_norm": 0.22836045920848846,
      "learning_rate": 2.2052083333333335e-05,
      "loss": 0.0014,
      "step": 134150
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.08573032170534134,
      "learning_rate": 2.205e-05,
      "loss": 0.0021,
      "step": 134160
    },
    {
      "epoch": 4.472333333333333,
      "grad_norm": 0.14255984127521515,
      "learning_rate": 2.204791666666667e-05,
      "loss": 0.0024,
      "step": 134170
    },
    {
      "epoch": 4.472666666666667,
      "grad_norm": 0.22813211381435394,
      "learning_rate": 2.2045833333333335e-05,
      "loss": 0.0027,
      "step": 134180
    },
    {
      "epoch": 4.473,
      "grad_norm": 0.11424944549798965,
      "learning_rate": 2.204375e-05,
      "loss": 0.0026,
      "step": 134190
    },
    {
      "epoch": 4.473333333333334,
      "grad_norm": 0.17154569923877716,
      "learning_rate": 2.204166666666667e-05,
      "loss": 0.002,
      "step": 134200
    },
    {
      "epoch": 4.4736666666666665,
      "grad_norm": 0.5414446592330933,
      "learning_rate": 2.2039583333333335e-05,
      "loss": 0.002,
      "step": 134210
    },
    {
      "epoch": 4.474,
      "grad_norm": 0.17320884764194489,
      "learning_rate": 2.20375e-05,
      "loss": 0.0022,
      "step": 134220
    },
    {
      "epoch": 4.474333333333333,
      "grad_norm": 0.2055606245994568,
      "learning_rate": 2.2035416666666666e-05,
      "loss": 0.0015,
      "step": 134230
    },
    {
      "epoch": 4.474666666666667,
      "grad_norm": 0.11447561532258987,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0023,
      "step": 134240
    },
    {
      "epoch": 4.475,
      "grad_norm": 0.17254629731178284,
      "learning_rate": 2.203125e-05,
      "loss": 0.0029,
      "step": 134250
    },
    {
      "epoch": 4.475333333333333,
      "grad_norm": 0.1417745053768158,
      "learning_rate": 2.202916666666667e-05,
      "loss": 0.0022,
      "step": 134260
    },
    {
      "epoch": 4.475666666666667,
      "grad_norm": 0.3463972210884094,
      "learning_rate": 2.2027083333333335e-05,
      "loss": 0.0026,
      "step": 134270
    },
    {
      "epoch": 4.476,
      "grad_norm": 0.14298222959041595,
      "learning_rate": 2.2025e-05,
      "loss": 0.0022,
      "step": 134280
    },
    {
      "epoch": 4.476333333333334,
      "grad_norm": 0.17143353819847107,
      "learning_rate": 2.202291666666667e-05,
      "loss": 0.0014,
      "step": 134290
    },
    {
      "epoch": 4.476666666666667,
      "grad_norm": 0.3997455835342407,
      "learning_rate": 2.2020833333333334e-05,
      "loss": 0.0022,
      "step": 134300
    },
    {
      "epoch": 4.477,
      "grad_norm": 0.19961698353290558,
      "learning_rate": 2.2018750000000003e-05,
      "loss": 0.0018,
      "step": 134310
    },
    {
      "epoch": 4.477333333333333,
      "grad_norm": 0.2851729989051819,
      "learning_rate": 2.201666666666667e-05,
      "loss": 0.0016,
      "step": 134320
    },
    {
      "epoch": 4.477666666666667,
      "grad_norm": 0.2569349408149719,
      "learning_rate": 2.2014583333333334e-05,
      "loss": 0.0019,
      "step": 134330
    },
    {
      "epoch": 4.478,
      "grad_norm": 0.14310947060585022,
      "learning_rate": 2.20125e-05,
      "loss": 0.002,
      "step": 134340
    },
    {
      "epoch": 4.4783333333333335,
      "grad_norm": 0.17883680760860443,
      "learning_rate": 2.2010416666666665e-05,
      "loss": 0.0023,
      "step": 134350
    },
    {
      "epoch": 4.478666666666666,
      "grad_norm": 0.14443041384220123,
      "learning_rate": 2.2008333333333334e-05,
      "loss": 0.0016,
      "step": 134360
    },
    {
      "epoch": 4.479,
      "grad_norm": 0.08572208136320114,
      "learning_rate": 2.200625e-05,
      "loss": 0.0028,
      "step": 134370
    },
    {
      "epoch": 4.479333333333333,
      "grad_norm": 0.11094913631677628,
      "learning_rate": 2.200416666666667e-05,
      "loss": 0.002,
      "step": 134380
    },
    {
      "epoch": 4.479666666666667,
      "grad_norm": 0.06510312110185623,
      "learning_rate": 2.2002083333333334e-05,
      "loss": 0.0026,
      "step": 134390
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.1475568264722824,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0018,
      "step": 134400
    },
    {
      "epoch": 4.480333333333333,
      "grad_norm": 0.08589663356542587,
      "learning_rate": 2.1997916666666668e-05,
      "loss": 0.0014,
      "step": 134410
    },
    {
      "epoch": 4.480666666666667,
      "grad_norm": 0.1432034969329834,
      "learning_rate": 2.1995833333333334e-05,
      "loss": 0.0013,
      "step": 134420
    },
    {
      "epoch": 4.481,
      "grad_norm": 0.3136157691478729,
      "learning_rate": 2.1993750000000003e-05,
      "loss": 0.0017,
      "step": 134430
    },
    {
      "epoch": 4.481333333333334,
      "grad_norm": 0.3430216312408447,
      "learning_rate": 2.1991666666666668e-05,
      "loss": 0.0018,
      "step": 134440
    },
    {
      "epoch": 4.4816666666666665,
      "grad_norm": 0.31356891989707947,
      "learning_rate": 2.1989583333333337e-05,
      "loss": 0.0021,
      "step": 134450
    },
    {
      "epoch": 4.482,
      "grad_norm": 0.11413200944662094,
      "learning_rate": 2.19875e-05,
      "loss": 0.0017,
      "step": 134460
    },
    {
      "epoch": 4.482333333333333,
      "grad_norm": 0.0579378604888916,
      "learning_rate": 2.1985416666666668e-05,
      "loss": 0.0016,
      "step": 134470
    },
    {
      "epoch": 4.482666666666667,
      "grad_norm": 0.17156900465488434,
      "learning_rate": 2.1983333333333333e-05,
      "loss": 0.0026,
      "step": 134480
    },
    {
      "epoch": 4.483,
      "grad_norm": 0.057413000613451004,
      "learning_rate": 2.198125e-05,
      "loss": 0.002,
      "step": 134490
    },
    {
      "epoch": 4.483333333333333,
      "grad_norm": 0.008673916570842266,
      "learning_rate": 2.1979166666666668e-05,
      "loss": 0.0022,
      "step": 134500
    },
    {
      "epoch": 4.483666666666666,
      "grad_norm": 0.20175188779830933,
      "learning_rate": 2.1977083333333333e-05,
      "loss": 0.0024,
      "step": 134510
    },
    {
      "epoch": 4.484,
      "grad_norm": 0.3992196023464203,
      "learning_rate": 2.1975000000000002e-05,
      "loss": 0.0017,
      "step": 134520
    },
    {
      "epoch": 4.484333333333334,
      "grad_norm": 0.227909654378891,
      "learning_rate": 2.1972916666666668e-05,
      "loss": 0.0019,
      "step": 134530
    },
    {
      "epoch": 4.484666666666667,
      "grad_norm": 0.05847848579287529,
      "learning_rate": 2.1970833333333337e-05,
      "loss": 0.002,
      "step": 134540
    },
    {
      "epoch": 4.485,
      "grad_norm": 0.0860828310251236,
      "learning_rate": 2.1968750000000002e-05,
      "loss": 0.0014,
      "step": 134550
    },
    {
      "epoch": 4.485333333333333,
      "grad_norm": 0.08603190630674362,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.0017,
      "step": 134560
    },
    {
      "epoch": 4.485666666666667,
      "grad_norm": 0.3423347473144531,
      "learning_rate": 2.1964583333333336e-05,
      "loss": 0.0015,
      "step": 134570
    },
    {
      "epoch": 4.486,
      "grad_norm": 0.5418093204498291,
      "learning_rate": 2.19625e-05,
      "loss": 0.0025,
      "step": 134580
    },
    {
      "epoch": 4.4863333333333335,
      "grad_norm": 0.11434712260961533,
      "learning_rate": 2.1960416666666667e-05,
      "loss": 0.0025,
      "step": 134590
    },
    {
      "epoch": 4.486666666666666,
      "grad_norm": 0.0858142301440239,
      "learning_rate": 2.1958333333333333e-05,
      "loss": 0.0015,
      "step": 134600
    },
    {
      "epoch": 4.487,
      "grad_norm": 0.05767039954662323,
      "learning_rate": 2.1956250000000002e-05,
      "loss": 0.0022,
      "step": 134610
    },
    {
      "epoch": 4.487333333333333,
      "grad_norm": 0.005856417119503021,
      "learning_rate": 2.1954166666666667e-05,
      "loss": 0.0018,
      "step": 134620
    },
    {
      "epoch": 4.487666666666667,
      "grad_norm": 0.14777307212352753,
      "learning_rate": 2.1952083333333333e-05,
      "loss": 0.0018,
      "step": 134630
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.08479882031679153,
      "learning_rate": 2.195e-05,
      "loss": 0.0015,
      "step": 134640
    },
    {
      "epoch": 4.488333333333333,
      "grad_norm": 0.030184047296643257,
      "learning_rate": 2.1947916666666667e-05,
      "loss": 0.0027,
      "step": 134650
    },
    {
      "epoch": 4.488666666666667,
      "grad_norm": 0.057673219591379166,
      "learning_rate": 2.1945833333333336e-05,
      "loss": 0.0021,
      "step": 134660
    },
    {
      "epoch": 4.489,
      "grad_norm": 0.171190544962883,
      "learning_rate": 2.194375e-05,
      "loss": 0.0017,
      "step": 134670
    },
    {
      "epoch": 4.489333333333334,
      "grad_norm": 0.11457964032888412,
      "learning_rate": 2.194166666666667e-05,
      "loss": 0.002,
      "step": 134680
    },
    {
      "epoch": 4.4896666666666665,
      "grad_norm": 0.09749767929315567,
      "learning_rate": 2.1939583333333336e-05,
      "loss": 0.0022,
      "step": 134690
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.03016282618045807,
      "learning_rate": 2.19375e-05,
      "loss": 0.0024,
      "step": 134700
    },
    {
      "epoch": 4.490333333333333,
      "grad_norm": 0.5135351419448853,
      "learning_rate": 2.1935416666666667e-05,
      "loss": 0.0018,
      "step": 134710
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.20018072426319122,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0017,
      "step": 134720
    },
    {
      "epoch": 4.491,
      "grad_norm": 0.14248645305633545,
      "learning_rate": 2.193125e-05,
      "loss": 0.0017,
      "step": 134730
    },
    {
      "epoch": 4.491333333333333,
      "grad_norm": 0.25666335225105286,
      "learning_rate": 2.1929166666666667e-05,
      "loss": 0.0016,
      "step": 134740
    },
    {
      "epoch": 4.491666666666666,
      "grad_norm": 0.013063253834843636,
      "learning_rate": 2.1927083333333336e-05,
      "loss": 0.0021,
      "step": 134750
    },
    {
      "epoch": 4.492,
      "grad_norm": 0.14253287017345428,
      "learning_rate": 2.1925e-05,
      "loss": 0.0016,
      "step": 134760
    },
    {
      "epoch": 4.492333333333334,
      "grad_norm": 0.029568340629339218,
      "learning_rate": 2.1922916666666666e-05,
      "loss": 0.0021,
      "step": 134770
    },
    {
      "epoch": 4.492666666666667,
      "grad_norm": 0.31816366314888,
      "learning_rate": 2.1920833333333335e-05,
      "loss": 0.0028,
      "step": 134780
    },
    {
      "epoch": 4.493,
      "grad_norm": 0.10477641224861145,
      "learning_rate": 2.191875e-05,
      "loss": 0.0023,
      "step": 134790
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.1173933893442154,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.0021,
      "step": 134800
    },
    {
      "epoch": 4.493666666666667,
      "grad_norm": 0.05743610858917236,
      "learning_rate": 2.1914583333333335e-05,
      "loss": 0.0012,
      "step": 134810
    },
    {
      "epoch": 4.494,
      "grad_norm": 0.1220659539103508,
      "learning_rate": 2.19125e-05,
      "loss": 0.0028,
      "step": 134820
    },
    {
      "epoch": 4.4943333333333335,
      "grad_norm": 0.31673601269721985,
      "learning_rate": 2.1910416666666666e-05,
      "loss": 0.0033,
      "step": 134830
    },
    {
      "epoch": 4.494666666666666,
      "grad_norm": 0.25705575942993164,
      "learning_rate": 2.1908333333333335e-05,
      "loss": 0.002,
      "step": 134840
    },
    {
      "epoch": 4.495,
      "grad_norm": 0.05716956406831741,
      "learning_rate": 2.190625e-05,
      "loss": 0.0016,
      "step": 134850
    },
    {
      "epoch": 4.495333333333333,
      "grad_norm": 0.22869901359081268,
      "learning_rate": 2.1904166666666666e-05,
      "loss": 0.0014,
      "step": 134860
    },
    {
      "epoch": 4.495666666666667,
      "grad_norm": 0.31384527683258057,
      "learning_rate": 2.1902083333333335e-05,
      "loss": 0.0022,
      "step": 134870
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.030818110331892967,
      "learning_rate": 2.19e-05,
      "loss": 0.0019,
      "step": 134880
    },
    {
      "epoch": 4.496333333333333,
      "grad_norm": 0.031362149864435196,
      "learning_rate": 2.189791666666667e-05,
      "loss": 0.0017,
      "step": 134890
    },
    {
      "epoch": 4.496666666666667,
      "grad_norm": 0.11460213363170624,
      "learning_rate": 2.1895833333333335e-05,
      "loss": 0.0038,
      "step": 134900
    },
    {
      "epoch": 4.497,
      "grad_norm": 0.42953765392303467,
      "learning_rate": 2.189375e-05,
      "loss": 0.0024,
      "step": 134910
    },
    {
      "epoch": 4.497333333333334,
      "grad_norm": 0.17150650918483734,
      "learning_rate": 2.189166666666667e-05,
      "loss": 0.0023,
      "step": 134920
    },
    {
      "epoch": 4.4976666666666665,
      "grad_norm": 0.2679750323295593,
      "learning_rate": 2.1889583333333335e-05,
      "loss": 0.0032,
      "step": 134930
    },
    {
      "epoch": 4.498,
      "grad_norm": 0.11429872363805771,
      "learning_rate": 2.18875e-05,
      "loss": 0.0023,
      "step": 134940
    },
    {
      "epoch": 4.498333333333333,
      "grad_norm": 0.3136258125305176,
      "learning_rate": 2.1885416666666666e-05,
      "loss": 0.0023,
      "step": 134950
    },
    {
      "epoch": 4.498666666666667,
      "grad_norm": 0.05756482854485512,
      "learning_rate": 2.1883333333333334e-05,
      "loss": 0.002,
      "step": 134960
    },
    {
      "epoch": 4.499,
      "grad_norm": 0.17129381000995636,
      "learning_rate": 2.188125e-05,
      "loss": 0.0022,
      "step": 134970
    },
    {
      "epoch": 4.499333333333333,
      "grad_norm": 0.28540295362472534,
      "learning_rate": 2.187916666666667e-05,
      "loss": 0.0018,
      "step": 134980
    },
    {
      "epoch": 4.499666666666666,
      "grad_norm": 0.2569146454334259,
      "learning_rate": 2.1877083333333334e-05,
      "loss": 0.0022,
      "step": 134990
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.2332133799791336,
      "learning_rate": 2.1875e-05,
      "loss": 0.0024,
      "step": 135000
    },
    {
      "epoch": 4.500333333333334,
      "grad_norm": 0.030532537028193474,
      "learning_rate": 2.187291666666667e-05,
      "loss": 0.0018,
      "step": 135010
    },
    {
      "epoch": 4.500666666666667,
      "grad_norm": 0.08590517938137054,
      "learning_rate": 2.1870833333333334e-05,
      "loss": 0.0019,
      "step": 135020
    },
    {
      "epoch": 4.501,
      "grad_norm": 0.13141536712646484,
      "learning_rate": 2.1868750000000003e-05,
      "loss": 0.002,
      "step": 135030
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.05575723573565483,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0023,
      "step": 135040
    },
    {
      "epoch": 4.501666666666667,
      "grad_norm": 0.22840256989002228,
      "learning_rate": 2.1864583333333334e-05,
      "loss": 0.0022,
      "step": 135050
    },
    {
      "epoch": 4.502,
      "grad_norm": 0.03213278204202652,
      "learning_rate": 2.1862500000000003e-05,
      "loss": 0.0026,
      "step": 135060
    },
    {
      "epoch": 4.5023333333333335,
      "grad_norm": 0.11434482038021088,
      "learning_rate": 2.1860416666666665e-05,
      "loss": 0.0019,
      "step": 135070
    },
    {
      "epoch": 4.502666666666666,
      "grad_norm": 0.34274670481681824,
      "learning_rate": 2.1858333333333334e-05,
      "loss": 0.0017,
      "step": 135080
    },
    {
      "epoch": 4.503,
      "grad_norm": 0.0891355499625206,
      "learning_rate": 2.185625e-05,
      "loss": 0.0021,
      "step": 135090
    },
    {
      "epoch": 4.503333333333333,
      "grad_norm": 0.1711057722568512,
      "learning_rate": 2.1854166666666668e-05,
      "loss": 0.0019,
      "step": 135100
    },
    {
      "epoch": 4.503666666666667,
      "grad_norm": 0.03007700853049755,
      "learning_rate": 2.1852083333333334e-05,
      "loss": 0.0017,
      "step": 135110
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.057580627501010895,
      "learning_rate": 2.1850000000000003e-05,
      "loss": 0.0031,
      "step": 135120
    },
    {
      "epoch": 4.504333333333333,
      "grad_norm": 0.3122883439064026,
      "learning_rate": 2.1847916666666668e-05,
      "loss": 0.002,
      "step": 135130
    },
    {
      "epoch": 4.504666666666667,
      "grad_norm": 0.08605292439460754,
      "learning_rate": 2.1845833333333334e-05,
      "loss": 0.0027,
      "step": 135140
    },
    {
      "epoch": 4.505,
      "grad_norm": 0.16214291751384735,
      "learning_rate": 2.1843750000000002e-05,
      "loss": 0.0019,
      "step": 135150
    },
    {
      "epoch": 4.505333333333334,
      "grad_norm": 0.22898726165294647,
      "learning_rate": 2.1841666666666668e-05,
      "loss": 0.0022,
      "step": 135160
    },
    {
      "epoch": 4.5056666666666665,
      "grad_norm": 0.2858222424983978,
      "learning_rate": 2.1839583333333337e-05,
      "loss": 0.0023,
      "step": 135170
    },
    {
      "epoch": 4.506,
      "grad_norm": 0.08600393682718277,
      "learning_rate": 2.1837500000000002e-05,
      "loss": 0.0018,
      "step": 135180
    },
    {
      "epoch": 4.506333333333333,
      "grad_norm": 0.5024675726890564,
      "learning_rate": 2.1835416666666668e-05,
      "loss": 0.0022,
      "step": 135190
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.029501236975193024,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0032,
      "step": 135200
    },
    {
      "epoch": 4.507,
      "grad_norm": 0.17632904648780823,
      "learning_rate": 2.183125e-05,
      "loss": 0.0024,
      "step": 135210
    },
    {
      "epoch": 4.507333333333333,
      "grad_norm": 0.07971249520778656,
      "learning_rate": 2.1829166666666668e-05,
      "loss": 0.0023,
      "step": 135220
    },
    {
      "epoch": 4.507666666666667,
      "grad_norm": 0.011391821317374706,
      "learning_rate": 2.1827083333333333e-05,
      "loss": 0.0019,
      "step": 135230
    },
    {
      "epoch": 4.508,
      "grad_norm": 0.051392994821071625,
      "learning_rate": 2.1825000000000002e-05,
      "loss": 0.002,
      "step": 135240
    },
    {
      "epoch": 4.508333333333333,
      "grad_norm": 0.12617844343185425,
      "learning_rate": 2.1822916666666667e-05,
      "loss": 0.0025,
      "step": 135250
    },
    {
      "epoch": 4.508666666666667,
      "grad_norm": 0.00650303578004241,
      "learning_rate": 2.1820833333333336e-05,
      "loss": 0.0015,
      "step": 135260
    },
    {
      "epoch": 4.509,
      "grad_norm": 0.22836630046367645,
      "learning_rate": 2.1818750000000002e-05,
      "loss": 0.0023,
      "step": 135270
    },
    {
      "epoch": 4.509333333333333,
      "grad_norm": 0.011821244843304157,
      "learning_rate": 2.1816666666666667e-05,
      "loss": 0.0014,
      "step": 135280
    },
    {
      "epoch": 4.509666666666667,
      "grad_norm": 0.3162579834461212,
      "learning_rate": 2.1814583333333336e-05,
      "loss": 0.0018,
      "step": 135290
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.46017324924468994,
      "learning_rate": 2.18125e-05,
      "loss": 0.0026,
      "step": 135300
    },
    {
      "epoch": 4.5103333333333335,
      "grad_norm": 0.14291658997535706,
      "learning_rate": 2.1810416666666667e-05,
      "loss": 0.0019,
      "step": 135310
    },
    {
      "epoch": 4.510666666666666,
      "grad_norm": 0.08573833853006363,
      "learning_rate": 2.1808333333333333e-05,
      "loss": 0.0022,
      "step": 135320
    },
    {
      "epoch": 4.511,
      "grad_norm": 0.031009165570139885,
      "learning_rate": 2.180625e-05,
      "loss": 0.0017,
      "step": 135330
    },
    {
      "epoch": 4.511333333333333,
      "grad_norm": 0.08573465794324875,
      "learning_rate": 2.1804166666666667e-05,
      "loss": 0.0028,
      "step": 135340
    },
    {
      "epoch": 4.511666666666667,
      "grad_norm": 0.08601922541856766,
      "learning_rate": 2.1802083333333332e-05,
      "loss": 0.0022,
      "step": 135350
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.029440084472298622,
      "learning_rate": 2.18e-05,
      "loss": 0.0014,
      "step": 135360
    },
    {
      "epoch": 4.512333333333333,
      "grad_norm": 0.2281583994626999,
      "learning_rate": 2.1797916666666667e-05,
      "loss": 0.0023,
      "step": 135370
    },
    {
      "epoch": 4.512666666666667,
      "grad_norm": 0.029217898845672607,
      "learning_rate": 2.1795833333333336e-05,
      "loss": 0.0029,
      "step": 135380
    },
    {
      "epoch": 4.513,
      "grad_norm": 0.08588548004627228,
      "learning_rate": 2.179375e-05,
      "loss": 0.0023,
      "step": 135390
    },
    {
      "epoch": 4.513333333333334,
      "grad_norm": 0.25714555382728577,
      "learning_rate": 2.179166666666667e-05,
      "loss": 0.0021,
      "step": 135400
    },
    {
      "epoch": 4.5136666666666665,
      "grad_norm": 0.2567932903766632,
      "learning_rate": 2.1789583333333336e-05,
      "loss": 0.0026,
      "step": 135410
    },
    {
      "epoch": 4.514,
      "grad_norm": 0.032943543046712875,
      "learning_rate": 2.17875e-05,
      "loss": 0.0023,
      "step": 135420
    },
    {
      "epoch": 4.514333333333333,
      "grad_norm": 0.34243345260620117,
      "learning_rate": 2.1785416666666667e-05,
      "loss": 0.0026,
      "step": 135430
    },
    {
      "epoch": 4.514666666666667,
      "grad_norm": 0.2009141743183136,
      "learning_rate": 2.1783333333333332e-05,
      "loss": 0.0026,
      "step": 135440
    },
    {
      "epoch": 4.515,
      "grad_norm": 0.05827806144952774,
      "learning_rate": 2.178125e-05,
      "loss": 0.0018,
      "step": 135450
    },
    {
      "epoch": 4.515333333333333,
      "grad_norm": 0.22838194668293,
      "learning_rate": 2.1779166666666666e-05,
      "loss": 0.0017,
      "step": 135460
    },
    {
      "epoch": 4.515666666666666,
      "grad_norm": 0.09059485048055649,
      "learning_rate": 2.1777083333333335e-05,
      "loss": 0.0011,
      "step": 135470
    },
    {
      "epoch": 4.516,
      "grad_norm": 0.31599539518356323,
      "learning_rate": 2.1775e-05,
      "loss": 0.0015,
      "step": 135480
    },
    {
      "epoch": 4.516333333333334,
      "grad_norm": 0.7118245363235474,
      "learning_rate": 2.1772916666666666e-05,
      "loss": 0.0028,
      "step": 135490
    },
    {
      "epoch": 4.516666666666667,
      "grad_norm": 0.07711583375930786,
      "learning_rate": 2.1770833333333335e-05,
      "loss": 0.002,
      "step": 135500
    },
    {
      "epoch": 4.517,
      "grad_norm": 0.07069815695285797,
      "learning_rate": 2.176875e-05,
      "loss": 0.0025,
      "step": 135510
    },
    {
      "epoch": 4.517333333333333,
      "grad_norm": 0.2855704426765442,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0017,
      "step": 135520
    },
    {
      "epoch": 4.517666666666667,
      "grad_norm": 0.17111371457576752,
      "learning_rate": 2.1764583333333335e-05,
      "loss": 0.0013,
      "step": 135530
    },
    {
      "epoch": 4.518,
      "grad_norm": 0.17107802629470825,
      "learning_rate": 2.1762500000000004e-05,
      "loss": 0.0026,
      "step": 135540
    },
    {
      "epoch": 4.5183333333333335,
      "grad_norm": 0.17088980972766876,
      "learning_rate": 2.1760416666666666e-05,
      "loss": 0.0019,
      "step": 135550
    },
    {
      "epoch": 4.518666666666666,
      "grad_norm": 0.22838737070560455,
      "learning_rate": 2.1758333333333335e-05,
      "loss": 0.0018,
      "step": 135560
    },
    {
      "epoch": 4.519,
      "grad_norm": 0.28529006242752075,
      "learning_rate": 2.175625e-05,
      "loss": 0.0018,
      "step": 135570
    },
    {
      "epoch": 4.519333333333333,
      "grad_norm": 0.22846192121505737,
      "learning_rate": 2.1754166666666666e-05,
      "loss": 0.0031,
      "step": 135580
    },
    {
      "epoch": 4.519666666666667,
      "grad_norm": 0.02896573208272457,
      "learning_rate": 2.1752083333333335e-05,
      "loss": 0.0027,
      "step": 135590
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.174209862947464,
      "learning_rate": 2.175e-05,
      "loss": 0.0024,
      "step": 135600
    },
    {
      "epoch": 4.520333333333333,
      "grad_norm": 0.14264734089374542,
      "learning_rate": 2.174791666666667e-05,
      "loss": 0.0024,
      "step": 135610
    },
    {
      "epoch": 4.520666666666667,
      "grad_norm": 0.49668338894844055,
      "learning_rate": 2.1745833333333334e-05,
      "loss": 0.0022,
      "step": 135620
    },
    {
      "epoch": 4.521,
      "grad_norm": 0.14295001327991486,
      "learning_rate": 2.174375e-05,
      "loss": 0.0021,
      "step": 135630
    },
    {
      "epoch": 4.521333333333334,
      "grad_norm": 0.3152907192707062,
      "learning_rate": 2.174166666666667e-05,
      "loss": 0.0026,
      "step": 135640
    },
    {
      "epoch": 4.5216666666666665,
      "grad_norm": 0.3186410963535309,
      "learning_rate": 2.1739583333333334e-05,
      "loss": 0.0024,
      "step": 135650
    },
    {
      "epoch": 4.522,
      "grad_norm": 0.1431812345981598,
      "learning_rate": 2.1737500000000003e-05,
      "loss": 0.0018,
      "step": 135660
    },
    {
      "epoch": 4.522333333333333,
      "grad_norm": 0.1430305391550064,
      "learning_rate": 2.1735416666666665e-05,
      "loss": 0.0026,
      "step": 135670
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.1723390370607376,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0018,
      "step": 135680
    },
    {
      "epoch": 4.523,
      "grad_norm": 0.15480555593967438,
      "learning_rate": 2.173125e-05,
      "loss": 0.0019,
      "step": 135690
    },
    {
      "epoch": 4.523333333333333,
      "grad_norm": 0.14311738312244415,
      "learning_rate": 2.172916666666667e-05,
      "loss": 0.0025,
      "step": 135700
    },
    {
      "epoch": 4.523666666666666,
      "grad_norm": 0.08585469424724579,
      "learning_rate": 2.1727083333333334e-05,
      "loss": 0.0019,
      "step": 135710
    },
    {
      "epoch": 4.524,
      "grad_norm": 0.1356065273284912,
      "learning_rate": 2.1725e-05,
      "loss": 0.002,
      "step": 135720
    },
    {
      "epoch": 4.524333333333333,
      "grad_norm": 0.057990189641714096,
      "learning_rate": 2.172291666666667e-05,
      "loss": 0.0014,
      "step": 135730
    },
    {
      "epoch": 4.524666666666667,
      "grad_norm": 0.17649325728416443,
      "learning_rate": 2.1720833333333334e-05,
      "loss": 0.0023,
      "step": 135740
    },
    {
      "epoch": 4.525,
      "grad_norm": 0.0500536784529686,
      "learning_rate": 2.1718750000000003e-05,
      "loss": 0.0023,
      "step": 135750
    },
    {
      "epoch": 4.525333333333333,
      "grad_norm": 0.08569903671741486,
      "learning_rate": 2.1716666666666668e-05,
      "loss": 0.0015,
      "step": 135760
    },
    {
      "epoch": 4.525666666666667,
      "grad_norm": 0.2571312189102173,
      "learning_rate": 2.1714583333333334e-05,
      "loss": 0.0019,
      "step": 135770
    },
    {
      "epoch": 4.526,
      "grad_norm": 0.1723019778728485,
      "learning_rate": 2.1712500000000003e-05,
      "loss": 0.0018,
      "step": 135780
    },
    {
      "epoch": 4.5263333333333335,
      "grad_norm": 0.1711704283952713,
      "learning_rate": 2.1710416666666668e-05,
      "loss": 0.0025,
      "step": 135790
    },
    {
      "epoch": 4.526666666666666,
      "grad_norm": 0.06689043343067169,
      "learning_rate": 2.1708333333333334e-05,
      "loss": 0.0013,
      "step": 135800
    },
    {
      "epoch": 4.527,
      "grad_norm": 0.22847434878349304,
      "learning_rate": 2.170625e-05,
      "loss": 0.0019,
      "step": 135810
    },
    {
      "epoch": 4.527333333333333,
      "grad_norm": 0.17181046307086945,
      "learning_rate": 2.1704166666666668e-05,
      "loss": 0.0019,
      "step": 135820
    },
    {
      "epoch": 4.527666666666667,
      "grad_norm": 0.11461497843265533,
      "learning_rate": 2.1702083333333333e-05,
      "loss": 0.0016,
      "step": 135830
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.17211930453777313,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.0022,
      "step": 135840
    },
    {
      "epoch": 4.528333333333333,
      "grad_norm": 0.257203072309494,
      "learning_rate": 2.1697916666666668e-05,
      "loss": 0.0018,
      "step": 135850
    },
    {
      "epoch": 4.528666666666666,
      "grad_norm": 0.2281746119260788,
      "learning_rate": 2.1695833333333333e-05,
      "loss": 0.0016,
      "step": 135860
    },
    {
      "epoch": 4.529,
      "grad_norm": 0.06868556886911392,
      "learning_rate": 2.1693750000000002e-05,
      "loss": 0.0018,
      "step": 135870
    },
    {
      "epoch": 4.529333333333334,
      "grad_norm": 0.14336520433425903,
      "learning_rate": 2.1691666666666668e-05,
      "loss": 0.0021,
      "step": 135880
    },
    {
      "epoch": 4.5296666666666665,
      "grad_norm": 0.15034984052181244,
      "learning_rate": 2.1689583333333337e-05,
      "loss": 0.0018,
      "step": 135890
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.3143964111804962,
      "learning_rate": 2.1687500000000002e-05,
      "loss": 0.0014,
      "step": 135900
    },
    {
      "epoch": 4.530333333333333,
      "grad_norm": 0.17156179249286652,
      "learning_rate": 2.1685416666666667e-05,
      "loss": 0.0023,
      "step": 135910
    },
    {
      "epoch": 4.530666666666667,
      "grad_norm": 0.05735160410404205,
      "learning_rate": 2.1683333333333333e-05,
      "loss": 0.0024,
      "step": 135920
    },
    {
      "epoch": 4.531,
      "grad_norm": 0.3421991765499115,
      "learning_rate": 2.1681250000000002e-05,
      "loss": 0.0013,
      "step": 135930
    },
    {
      "epoch": 4.531333333333333,
      "grad_norm": 0.05746663361787796,
      "learning_rate": 2.1679166666666667e-05,
      "loss": 0.0015,
      "step": 135940
    },
    {
      "epoch": 4.531666666666666,
      "grad_norm": 0.030818521976470947,
      "learning_rate": 2.1677083333333333e-05,
      "loss": 0.0023,
      "step": 135950
    },
    {
      "epoch": 4.532,
      "grad_norm": 0.5144869089126587,
      "learning_rate": 2.1675e-05,
      "loss": 0.0017,
      "step": 135960
    },
    {
      "epoch": 4.532333333333334,
      "grad_norm": 0.22799095511436462,
      "learning_rate": 2.1672916666666667e-05,
      "loss": 0.0017,
      "step": 135970
    },
    {
      "epoch": 4.532666666666667,
      "grad_norm": 0.3710123598575592,
      "learning_rate": 2.1670833333333336e-05,
      "loss": 0.0026,
      "step": 135980
    },
    {
      "epoch": 4.533,
      "grad_norm": 0.11506287753582001,
      "learning_rate": 2.166875e-05,
      "loss": 0.0021,
      "step": 135990
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.058984071016311646,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0023,
      "step": 136000
    },
    {
      "epoch": 4.533666666666667,
      "grad_norm": 0.042534954845905304,
      "learning_rate": 2.1664583333333336e-05,
      "loss": 0.0016,
      "step": 136010
    },
    {
      "epoch": 4.534,
      "grad_norm": 0.17138072848320007,
      "learning_rate": 2.16625e-05,
      "loss": 0.0022,
      "step": 136020
    },
    {
      "epoch": 4.5343333333333335,
      "grad_norm": 0.17218929529190063,
      "learning_rate": 2.1660416666666667e-05,
      "loss": 0.0015,
      "step": 136030
    },
    {
      "epoch": 4.534666666666666,
      "grad_norm": 0.14389336109161377,
      "learning_rate": 2.1658333333333332e-05,
      "loss": 0.0018,
      "step": 136040
    },
    {
      "epoch": 4.535,
      "grad_norm": 0.256956547498703,
      "learning_rate": 2.165625e-05,
      "loss": 0.0017,
      "step": 136050
    },
    {
      "epoch": 4.535333333333333,
      "grad_norm": 0.10333321243524551,
      "learning_rate": 2.1654166666666667e-05,
      "loss": 0.0025,
      "step": 136060
    },
    {
      "epoch": 4.535666666666667,
      "grad_norm": 0.14775216579437256,
      "learning_rate": 2.1652083333333336e-05,
      "loss": 0.0019,
      "step": 136070
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.17156797647476196,
      "learning_rate": 2.165e-05,
      "loss": 0.0021,
      "step": 136080
    },
    {
      "epoch": 4.536333333333333,
      "grad_norm": 0.03267183154821396,
      "learning_rate": 2.1647916666666667e-05,
      "loss": 0.0012,
      "step": 136090
    },
    {
      "epoch": 4.536666666666667,
      "grad_norm": 0.03645727038383484,
      "learning_rate": 2.1645833333333335e-05,
      "loss": 0.0028,
      "step": 136100
    },
    {
      "epoch": 4.537,
      "grad_norm": 0.1716070920228958,
      "learning_rate": 2.164375e-05,
      "loss": 0.0025,
      "step": 136110
    },
    {
      "epoch": 4.537333333333334,
      "grad_norm": 0.2317899465560913,
      "learning_rate": 2.164166666666667e-05,
      "loss": 0.0027,
      "step": 136120
    },
    {
      "epoch": 4.5376666666666665,
      "grad_norm": 0.06190779432654381,
      "learning_rate": 2.1639583333333335e-05,
      "loss": 0.0036,
      "step": 136130
    },
    {
      "epoch": 4.538,
      "grad_norm": 0.17171861231327057,
      "learning_rate": 2.16375e-05,
      "loss": 0.0025,
      "step": 136140
    },
    {
      "epoch": 4.538333333333333,
      "grad_norm": 0.5647129416465759,
      "learning_rate": 2.1635416666666666e-05,
      "loss": 0.0028,
      "step": 136150
    },
    {
      "epoch": 4.538666666666667,
      "grad_norm": 0.37131741642951965,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0013,
      "step": 136160
    },
    {
      "epoch": 4.539,
      "grad_norm": 0.14290851354599,
      "learning_rate": 2.163125e-05,
      "loss": 0.0027,
      "step": 136170
    },
    {
      "epoch": 4.539333333333333,
      "grad_norm": 0.2577102780342102,
      "learning_rate": 2.1629166666666666e-05,
      "loss": 0.002,
      "step": 136180
    },
    {
      "epoch": 4.539666666666666,
      "grad_norm": 0.4006442129611969,
      "learning_rate": 2.1627083333333335e-05,
      "loss": 0.0019,
      "step": 136190
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.45724743604660034,
      "learning_rate": 2.1625e-05,
      "loss": 0.0021,
      "step": 136200
    },
    {
      "epoch": 4.540333333333333,
      "grad_norm": 0.08646160364151001,
      "learning_rate": 2.162291666666667e-05,
      "loss": 0.0016,
      "step": 136210
    },
    {
      "epoch": 4.540666666666667,
      "grad_norm": 0.1711874157190323,
      "learning_rate": 2.1620833333333335e-05,
      "loss": 0.0028,
      "step": 136220
    },
    {
      "epoch": 4.541,
      "grad_norm": 0.4418283700942993,
      "learning_rate": 2.161875e-05,
      "loss": 0.0016,
      "step": 136230
    },
    {
      "epoch": 4.541333333333333,
      "grad_norm": 0.5147077441215515,
      "learning_rate": 2.161666666666667e-05,
      "loss": 0.0018,
      "step": 136240
    },
    {
      "epoch": 4.541666666666667,
      "grad_norm": 0.1143098995089531,
      "learning_rate": 2.1614583333333335e-05,
      "loss": 0.0016,
      "step": 136250
    },
    {
      "epoch": 4.542,
      "grad_norm": 0.031149515882134438,
      "learning_rate": 2.1612500000000004e-05,
      "loss": 0.0023,
      "step": 136260
    },
    {
      "epoch": 4.542333333333334,
      "grad_norm": 0.43010464310646057,
      "learning_rate": 2.161041666666667e-05,
      "loss": 0.0021,
      "step": 136270
    },
    {
      "epoch": 4.542666666666666,
      "grad_norm": 0.012229847721755505,
      "learning_rate": 2.1608333333333335e-05,
      "loss": 0.0032,
      "step": 136280
    },
    {
      "epoch": 4.543,
      "grad_norm": 0.11195933073759079,
      "learning_rate": 2.160625e-05,
      "loss": 0.002,
      "step": 136290
    },
    {
      "epoch": 4.543333333333333,
      "grad_norm": 0.4191334545612335,
      "learning_rate": 2.1604166666666666e-05,
      "loss": 0.0019,
      "step": 136300
    },
    {
      "epoch": 4.543666666666667,
      "grad_norm": 0.05171492323279381,
      "learning_rate": 2.1602083333333334e-05,
      "loss": 0.0022,
      "step": 136310
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.08008795976638794,
      "learning_rate": 2.16e-05,
      "loss": 0.0025,
      "step": 136320
    },
    {
      "epoch": 4.544333333333333,
      "grad_norm": 0.2346017211675644,
      "learning_rate": 2.159791666666667e-05,
      "loss": 0.0015,
      "step": 136330
    },
    {
      "epoch": 4.544666666666666,
      "grad_norm": 0.029624268412590027,
      "learning_rate": 2.1595833333333334e-05,
      "loss": 0.0019,
      "step": 136340
    },
    {
      "epoch": 4.545,
      "grad_norm": 0.11499397456645966,
      "learning_rate": 2.1593750000000003e-05,
      "loss": 0.0018,
      "step": 136350
    },
    {
      "epoch": 4.545333333333334,
      "grad_norm": 0.6450849771499634,
      "learning_rate": 2.159166666666667e-05,
      "loss": 0.002,
      "step": 136360
    },
    {
      "epoch": 4.5456666666666665,
      "grad_norm": 0.05772744491696358,
      "learning_rate": 2.1589583333333334e-05,
      "loss": 0.0014,
      "step": 136370
    },
    {
      "epoch": 4.546,
      "grad_norm": 0.1714620590209961,
      "learning_rate": 2.1587500000000003e-05,
      "loss": 0.002,
      "step": 136380
    },
    {
      "epoch": 4.546333333333333,
      "grad_norm": 0.057411376386880875,
      "learning_rate": 2.158541666666667e-05,
      "loss": 0.0019,
      "step": 136390
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.12505418062210083,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.0027,
      "step": 136400
    },
    {
      "epoch": 4.547,
      "grad_norm": 0.01084861345589161,
      "learning_rate": 2.158125e-05,
      "loss": 0.0021,
      "step": 136410
    },
    {
      "epoch": 4.5473333333333334,
      "grad_norm": 0.01770874671638012,
      "learning_rate": 2.1579166666666668e-05,
      "loss": 0.0017,
      "step": 136420
    },
    {
      "epoch": 4.547666666666666,
      "grad_norm": 0.4514602720737457,
      "learning_rate": 2.1577083333333334e-05,
      "loss": 0.0017,
      "step": 136430
    },
    {
      "epoch": 4.548,
      "grad_norm": 0.20012657344341278,
      "learning_rate": 2.1575e-05,
      "loss": 0.0018,
      "step": 136440
    },
    {
      "epoch": 4.548333333333334,
      "grad_norm": 0.20526602864265442,
      "learning_rate": 2.1572916666666668e-05,
      "loss": 0.002,
      "step": 136450
    },
    {
      "epoch": 4.548666666666667,
      "grad_norm": 0.17122548818588257,
      "learning_rate": 2.1570833333333334e-05,
      "loss": 0.0015,
      "step": 136460
    },
    {
      "epoch": 4.549,
      "grad_norm": 0.14303432404994965,
      "learning_rate": 2.1568750000000002e-05,
      "loss": 0.0011,
      "step": 136470
    },
    {
      "epoch": 4.549333333333333,
      "grad_norm": 0.07791315019130707,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.0018,
      "step": 136480
    },
    {
      "epoch": 4.549666666666667,
      "grad_norm": 0.358005166053772,
      "learning_rate": 2.1564583333333337e-05,
      "loss": 0.002,
      "step": 136490
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.029679112136363983,
      "learning_rate": 2.1562500000000002e-05,
      "loss": 0.0022,
      "step": 136500
    },
    {
      "epoch": 4.550333333333334,
      "grad_norm": 0.03246544301509857,
      "learning_rate": 2.1560416666666668e-05,
      "loss": 0.0018,
      "step": 136510
    },
    {
      "epoch": 4.550666666666666,
      "grad_norm": 0.17116719484329224,
      "learning_rate": 2.1558333333333333e-05,
      "loss": 0.0021,
      "step": 136520
    },
    {
      "epoch": 4.551,
      "grad_norm": 0.1499975174665451,
      "learning_rate": 2.155625e-05,
      "loss": 0.0034,
      "step": 136530
    },
    {
      "epoch": 4.551333333333333,
      "grad_norm": 0.17302078008651733,
      "learning_rate": 2.1554166666666668e-05,
      "loss": 0.0021,
      "step": 136540
    },
    {
      "epoch": 4.551666666666667,
      "grad_norm": 0.2283385992050171,
      "learning_rate": 2.1552083333333333e-05,
      "loss": 0.0019,
      "step": 136550
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.03019791655242443,
      "learning_rate": 2.1550000000000002e-05,
      "loss": 0.0019,
      "step": 136560
    },
    {
      "epoch": 4.552333333333333,
      "grad_norm": 0.45679059624671936,
      "learning_rate": 2.1547916666666668e-05,
      "loss": 0.0016,
      "step": 136570
    },
    {
      "epoch": 4.552666666666667,
      "grad_norm": 0.11431436985731125,
      "learning_rate": 2.1545833333333333e-05,
      "loss": 0.0023,
      "step": 136580
    },
    {
      "epoch": 4.553,
      "grad_norm": 0.11481310427188873,
      "learning_rate": 2.1543750000000002e-05,
      "loss": 0.0025,
      "step": 136590
    },
    {
      "epoch": 4.553333333333334,
      "grad_norm": 0.02925277128815651,
      "learning_rate": 2.1541666666666667e-05,
      "loss": 0.0022,
      "step": 136600
    },
    {
      "epoch": 4.5536666666666665,
      "grad_norm": 0.04939683899283409,
      "learning_rate": 2.1539583333333336e-05,
      "loss": 0.0023,
      "step": 136610
    },
    {
      "epoch": 4.554,
      "grad_norm": 0.20005479454994202,
      "learning_rate": 2.1537500000000002e-05,
      "loss": 0.002,
      "step": 136620
    },
    {
      "epoch": 4.554333333333333,
      "grad_norm": 0.1453746110200882,
      "learning_rate": 2.153541666666667e-05,
      "loss": 0.0019,
      "step": 136630
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.14304675161838531,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0021,
      "step": 136640
    },
    {
      "epoch": 4.555,
      "grad_norm": 0.08648251742124557,
      "learning_rate": 2.153125e-05,
      "loss": 0.0023,
      "step": 136650
    },
    {
      "epoch": 4.5553333333333335,
      "grad_norm": 0.34210315346717834,
      "learning_rate": 2.1529166666666667e-05,
      "loss": 0.0013,
      "step": 136660
    },
    {
      "epoch": 4.555666666666666,
      "grad_norm": 0.19976934790611267,
      "learning_rate": 2.1527083333333333e-05,
      "loss": 0.0019,
      "step": 136670
    },
    {
      "epoch": 4.556,
      "grad_norm": 0.030294815078377724,
      "learning_rate": 2.1525e-05,
      "loss": 0.002,
      "step": 136680
    },
    {
      "epoch": 4.556333333333333,
      "grad_norm": 0.09566647559404373,
      "learning_rate": 2.1522916666666667e-05,
      "loss": 0.0024,
      "step": 136690
    },
    {
      "epoch": 4.556666666666667,
      "grad_norm": 0.05732043460011482,
      "learning_rate": 2.1520833333333336e-05,
      "loss": 0.0021,
      "step": 136700
    },
    {
      "epoch": 4.557,
      "grad_norm": 0.31407850980758667,
      "learning_rate": 2.151875e-05,
      "loss": 0.0023,
      "step": 136710
    },
    {
      "epoch": 4.557333333333333,
      "grad_norm": 0.29561468958854675,
      "learning_rate": 2.1516666666666667e-05,
      "loss": 0.0019,
      "step": 136720
    },
    {
      "epoch": 4.557666666666667,
      "grad_norm": 0.1437368243932724,
      "learning_rate": 2.1514583333333336e-05,
      "loss": 0.0018,
      "step": 136730
    },
    {
      "epoch": 4.558,
      "grad_norm": 0.05784352123737335,
      "learning_rate": 2.15125e-05,
      "loss": 0.0019,
      "step": 136740
    },
    {
      "epoch": 4.558333333333334,
      "grad_norm": 0.11429383605718613,
      "learning_rate": 2.151041666666667e-05,
      "loss": 0.002,
      "step": 136750
    },
    {
      "epoch": 4.558666666666666,
      "grad_norm": 0.3141591250896454,
      "learning_rate": 2.1508333333333332e-05,
      "loss": 0.0028,
      "step": 136760
    },
    {
      "epoch": 4.559,
      "grad_norm": 0.20371049642562866,
      "learning_rate": 2.150625e-05,
      "loss": 0.0026,
      "step": 136770
    },
    {
      "epoch": 4.559333333333333,
      "grad_norm": 0.009595992974936962,
      "learning_rate": 2.1504166666666666e-05,
      "loss": 0.0014,
      "step": 136780
    },
    {
      "epoch": 4.559666666666667,
      "grad_norm": 0.22865606844425201,
      "learning_rate": 2.1502083333333335e-05,
      "loss": 0.0016,
      "step": 136790
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.08613620698451996,
      "learning_rate": 2.15e-05,
      "loss": 0.0024,
      "step": 136800
    },
    {
      "epoch": 4.560333333333333,
      "grad_norm": 0.05738576129078865,
      "learning_rate": 2.1497916666666666e-05,
      "loss": 0.0021,
      "step": 136810
    },
    {
      "epoch": 4.560666666666666,
      "grad_norm": 0.010467945598065853,
      "learning_rate": 2.1495833333333335e-05,
      "loss": 0.0021,
      "step": 136820
    },
    {
      "epoch": 4.561,
      "grad_norm": 0.22811727225780487,
      "learning_rate": 2.149375e-05,
      "loss": 0.0017,
      "step": 136830
    },
    {
      "epoch": 4.561333333333334,
      "grad_norm": 0.11382482200860977,
      "learning_rate": 2.149166666666667e-05,
      "loss": 0.0021,
      "step": 136840
    },
    {
      "epoch": 4.5616666666666665,
      "grad_norm": 0.5437473058700562,
      "learning_rate": 2.1489583333333335e-05,
      "loss": 0.0016,
      "step": 136850
    },
    {
      "epoch": 4.562,
      "grad_norm": 0.5420528054237366,
      "learning_rate": 2.14875e-05,
      "loss": 0.0027,
      "step": 136860
    },
    {
      "epoch": 4.562333333333333,
      "grad_norm": 0.20750382542610168,
      "learning_rate": 2.148541666666667e-05,
      "loss": 0.0018,
      "step": 136870
    },
    {
      "epoch": 4.562666666666667,
      "grad_norm": 0.14270782470703125,
      "learning_rate": 2.148333333333333e-05,
      "loss": 0.0012,
      "step": 136880
    },
    {
      "epoch": 4.563,
      "grad_norm": 0.4564644694328308,
      "learning_rate": 2.148125e-05,
      "loss": 0.0019,
      "step": 136890
    },
    {
      "epoch": 4.5633333333333335,
      "grad_norm": 0.47003185749053955,
      "learning_rate": 2.1479166666666666e-05,
      "loss": 0.0021,
      "step": 136900
    },
    {
      "epoch": 4.563666666666666,
      "grad_norm": 0.315734326839447,
      "learning_rate": 2.1477083333333335e-05,
      "loss": 0.002,
      "step": 136910
    },
    {
      "epoch": 4.564,
      "grad_norm": 0.2281995713710785,
      "learning_rate": 2.1475e-05,
      "loss": 0.0022,
      "step": 136920
    },
    {
      "epoch": 4.564333333333334,
      "grad_norm": 0.031498417258262634,
      "learning_rate": 2.147291666666667e-05,
      "loss": 0.0022,
      "step": 136930
    },
    {
      "epoch": 4.564666666666667,
      "grad_norm": 0.22921663522720337,
      "learning_rate": 2.1470833333333335e-05,
      "loss": 0.0044,
      "step": 136940
    },
    {
      "epoch": 4.5649999999999995,
      "grad_norm": 0.05994421988725662,
      "learning_rate": 2.146875e-05,
      "loss": 0.0033,
      "step": 136950
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.059116899967193604,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0027,
      "step": 136960
    },
    {
      "epoch": 4.565666666666667,
      "grad_norm": 0.14282944798469543,
      "learning_rate": 2.1464583333333334e-05,
      "loss": 0.0019,
      "step": 136970
    },
    {
      "epoch": 4.566,
      "grad_norm": 0.14300867915153503,
      "learning_rate": 2.1462500000000003e-05,
      "loss": 0.0019,
      "step": 136980
    },
    {
      "epoch": 4.566333333333334,
      "grad_norm": 0.014427346177399158,
      "learning_rate": 2.146041666666667e-05,
      "loss": 0.0016,
      "step": 136990
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 0.17347173392772675,
      "learning_rate": 2.1458333333333334e-05,
      "loss": 0.0023,
      "step": 137000
    },
    {
      "epoch": 4.567,
      "grad_norm": 0.029043134301900864,
      "learning_rate": 2.145625e-05,
      "loss": 0.0019,
      "step": 137010
    },
    {
      "epoch": 4.567333333333333,
      "grad_norm": 0.1999153196811676,
      "learning_rate": 2.1454166666666665e-05,
      "loss": 0.0024,
      "step": 137020
    },
    {
      "epoch": 4.567666666666667,
      "grad_norm": 0.28546932339668274,
      "learning_rate": 2.1452083333333334e-05,
      "loss": 0.002,
      "step": 137030
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.05051364749670029,
      "learning_rate": 2.145e-05,
      "loss": 0.0024,
      "step": 137040
    },
    {
      "epoch": 4.568333333333333,
      "grad_norm": 0.11225736886262894,
      "learning_rate": 2.144791666666667e-05,
      "loss": 0.002,
      "step": 137050
    },
    {
      "epoch": 4.568666666666667,
      "grad_norm": 0.48675084114074707,
      "learning_rate": 2.1445833333333334e-05,
      "loss": 0.0023,
      "step": 137060
    },
    {
      "epoch": 4.569,
      "grad_norm": 0.006759874988347292,
      "learning_rate": 2.1443750000000003e-05,
      "loss": 0.0021,
      "step": 137070
    },
    {
      "epoch": 4.569333333333334,
      "grad_norm": 0.02960442192852497,
      "learning_rate": 2.144166666666667e-05,
      "loss": 0.003,
      "step": 137080
    },
    {
      "epoch": 4.5696666666666665,
      "grad_norm": 0.030023228377103806,
      "learning_rate": 2.1439583333333334e-05,
      "loss": 0.0024,
      "step": 137090
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.3142280578613281,
      "learning_rate": 2.1437500000000003e-05,
      "loss": 0.0028,
      "step": 137100
    },
    {
      "epoch": 4.570333333333333,
      "grad_norm": 0.23340702056884766,
      "learning_rate": 2.1435416666666668e-05,
      "loss": 0.0022,
      "step": 137110
    },
    {
      "epoch": 4.570666666666667,
      "grad_norm": 0.16037359833717346,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.0027,
      "step": 137120
    },
    {
      "epoch": 4.571,
      "grad_norm": 0.11724753677845001,
      "learning_rate": 2.143125e-05,
      "loss": 0.0025,
      "step": 137130
    },
    {
      "epoch": 4.5713333333333335,
      "grad_norm": 0.14320601522922516,
      "learning_rate": 2.1429166666666668e-05,
      "loss": 0.0019,
      "step": 137140
    },
    {
      "epoch": 4.571666666666666,
      "grad_norm": 0.14283354580402374,
      "learning_rate": 2.1427083333333334e-05,
      "loss": 0.0014,
      "step": 137150
    },
    {
      "epoch": 4.572,
      "grad_norm": 0.37114936113357544,
      "learning_rate": 2.1425e-05,
      "loss": 0.0014,
      "step": 137160
    },
    {
      "epoch": 4.572333333333333,
      "grad_norm": 0.14260733127593994,
      "learning_rate": 2.1422916666666668e-05,
      "loss": 0.0028,
      "step": 137170
    },
    {
      "epoch": 4.572666666666667,
      "grad_norm": 0.11424962431192398,
      "learning_rate": 2.1420833333333333e-05,
      "loss": 0.0019,
      "step": 137180
    },
    {
      "epoch": 4.573,
      "grad_norm": 0.059970226138830185,
      "learning_rate": 2.1418750000000002e-05,
      "loss": 0.0019,
      "step": 137190
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.37194669246673584,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.0023,
      "step": 137200
    },
    {
      "epoch": 4.573666666666667,
      "grad_norm": 0.031081540510058403,
      "learning_rate": 2.1414583333333337e-05,
      "loss": 0.0021,
      "step": 137210
    },
    {
      "epoch": 4.574,
      "grad_norm": 0.11465270817279816,
      "learning_rate": 2.1412500000000002e-05,
      "loss": 0.0017,
      "step": 137220
    },
    {
      "epoch": 4.574333333333334,
      "grad_norm": 0.08571159094572067,
      "learning_rate": 2.1410416666666668e-05,
      "loss": 0.002,
      "step": 137230
    },
    {
      "epoch": 4.574666666666666,
      "grad_norm": 0.14734402298927307,
      "learning_rate": 2.1408333333333333e-05,
      "loss": 0.0017,
      "step": 137240
    },
    {
      "epoch": 4.575,
      "grad_norm": 0.7424136996269226,
      "learning_rate": 2.140625e-05,
      "loss": 0.0026,
      "step": 137250
    },
    {
      "epoch": 4.575333333333333,
      "grad_norm": 0.42807620763778687,
      "learning_rate": 2.1404166666666667e-05,
      "loss": 0.0013,
      "step": 137260
    },
    {
      "epoch": 4.575666666666667,
      "grad_norm": 0.23089949786663055,
      "learning_rate": 2.1402083333333333e-05,
      "loss": 0.0017,
      "step": 137270
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.0065303221344947815,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0026,
      "step": 137280
    },
    {
      "epoch": 4.576333333333333,
      "grad_norm": 0.42778581380844116,
      "learning_rate": 2.1397916666666667e-05,
      "loss": 0.0033,
      "step": 137290
    },
    {
      "epoch": 4.576666666666666,
      "grad_norm": 0.031157519668340683,
      "learning_rate": 2.1395833333333333e-05,
      "loss": 0.0015,
      "step": 137300
    },
    {
      "epoch": 4.577,
      "grad_norm": 0.08574318885803223,
      "learning_rate": 2.139375e-05,
      "loss": 0.0016,
      "step": 137310
    },
    {
      "epoch": 4.577333333333334,
      "grad_norm": 0.1284799426794052,
      "learning_rate": 2.1391666666666667e-05,
      "loss": 0.0015,
      "step": 137320
    },
    {
      "epoch": 4.5776666666666666,
      "grad_norm": 0.0861099511384964,
      "learning_rate": 2.1389583333333336e-05,
      "loss": 0.0025,
      "step": 137330
    },
    {
      "epoch": 4.578,
      "grad_norm": 0.21045641601085663,
      "learning_rate": 2.13875e-05,
      "loss": 0.0031,
      "step": 137340
    },
    {
      "epoch": 4.578333333333333,
      "grad_norm": 0.31353646516799927,
      "learning_rate": 2.138541666666667e-05,
      "loss": 0.0021,
      "step": 137350
    },
    {
      "epoch": 4.578666666666667,
      "grad_norm": 0.3426937162876129,
      "learning_rate": 2.1383333333333332e-05,
      "loss": 0.0019,
      "step": 137360
    },
    {
      "epoch": 4.579,
      "grad_norm": 0.007304398342967033,
      "learning_rate": 2.138125e-05,
      "loss": 0.0028,
      "step": 137370
    },
    {
      "epoch": 4.5793333333333335,
      "grad_norm": 0.030057823285460472,
      "learning_rate": 2.1379166666666667e-05,
      "loss": 0.002,
      "step": 137380
    },
    {
      "epoch": 4.579666666666666,
      "grad_norm": 0.07641039043664932,
      "learning_rate": 2.1377083333333332e-05,
      "loss": 0.0027,
      "step": 137390
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.5708755254745483,
      "learning_rate": 2.1375e-05,
      "loss": 0.0024,
      "step": 137400
    },
    {
      "epoch": 4.580333333333334,
      "grad_norm": 0.1715628057718277,
      "learning_rate": 2.1372916666666667e-05,
      "loss": 0.0026,
      "step": 137410
    },
    {
      "epoch": 4.580666666666667,
      "grad_norm": 0.03158096224069595,
      "learning_rate": 2.1370833333333336e-05,
      "loss": 0.0013,
      "step": 137420
    },
    {
      "epoch": 4.5809999999999995,
      "grad_norm": 0.2068513184785843,
      "learning_rate": 2.136875e-05,
      "loss": 0.0025,
      "step": 137430
    },
    {
      "epoch": 4.581333333333333,
      "grad_norm": 0.06254184991121292,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.002,
      "step": 137440
    },
    {
      "epoch": 4.581666666666667,
      "grad_norm": 0.40113821625709534,
      "learning_rate": 2.1364583333333335e-05,
      "loss": 0.0013,
      "step": 137450
    },
    {
      "epoch": 4.582,
      "grad_norm": 0.10288862884044647,
      "learning_rate": 2.13625e-05,
      "loss": 0.0023,
      "step": 137460
    },
    {
      "epoch": 4.582333333333334,
      "grad_norm": 0.08702430129051208,
      "learning_rate": 2.136041666666667e-05,
      "loss": 0.0027,
      "step": 137470
    },
    {
      "epoch": 4.582666666666666,
      "grad_norm": 0.14301232993602753,
      "learning_rate": 2.1358333333333332e-05,
      "loss": 0.0023,
      "step": 137480
    },
    {
      "epoch": 4.583,
      "grad_norm": 0.20194344222545624,
      "learning_rate": 2.135625e-05,
      "loss": 0.002,
      "step": 137490
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 0.11420576274394989,
      "learning_rate": 2.1354166666666666e-05,
      "loss": 0.0025,
      "step": 137500
    },
    {
      "epoch": 4.583666666666667,
      "grad_norm": 0.09873820096254349,
      "learning_rate": 2.1352083333333335e-05,
      "loss": 0.0029,
      "step": 137510
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.1807333081960678,
      "learning_rate": 2.135e-05,
      "loss": 0.0021,
      "step": 137520
    },
    {
      "epoch": 4.584333333333333,
      "grad_norm": 0.10320112109184265,
      "learning_rate": 2.1347916666666666e-05,
      "loss": 0.0019,
      "step": 137530
    },
    {
      "epoch": 4.584666666666667,
      "grad_norm": 0.09255614876747131,
      "learning_rate": 2.1345833333333335e-05,
      "loss": 0.0019,
      "step": 137540
    },
    {
      "epoch": 4.585,
      "grad_norm": 0.11505249887704849,
      "learning_rate": 2.134375e-05,
      "loss": 0.0025,
      "step": 137550
    },
    {
      "epoch": 4.585333333333334,
      "grad_norm": 0.14245909452438354,
      "learning_rate": 2.134166666666667e-05,
      "loss": 0.002,
      "step": 137560
    },
    {
      "epoch": 4.585666666666667,
      "grad_norm": 0.05877874046564102,
      "learning_rate": 2.1339583333333335e-05,
      "loss": 0.0013,
      "step": 137570
    },
    {
      "epoch": 4.586,
      "grad_norm": 0.17136842012405396,
      "learning_rate": 2.13375e-05,
      "loss": 0.0022,
      "step": 137580
    },
    {
      "epoch": 4.586333333333333,
      "grad_norm": 0.08586633205413818,
      "learning_rate": 2.133541666666667e-05,
      "loss": 0.0016,
      "step": 137590
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.2577352523803711,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0015,
      "step": 137600
    },
    {
      "epoch": 4.587,
      "grad_norm": 0.3141516149044037,
      "learning_rate": 2.133125e-05,
      "loss": 0.0019,
      "step": 137610
    },
    {
      "epoch": 4.5873333333333335,
      "grad_norm": 0.15541397035121918,
      "learning_rate": 2.1329166666666666e-05,
      "loss": 0.0014,
      "step": 137620
    },
    {
      "epoch": 4.587666666666666,
      "grad_norm": 0.11440423876047134,
      "learning_rate": 2.1327083333333334e-05,
      "loss": 0.002,
      "step": 137630
    },
    {
      "epoch": 4.588,
      "grad_norm": 0.11500692367553711,
      "learning_rate": 2.1325e-05,
      "loss": 0.0019,
      "step": 137640
    },
    {
      "epoch": 4.588333333333333,
      "grad_norm": 0.14301367104053497,
      "learning_rate": 2.132291666666667e-05,
      "loss": 0.0026,
      "step": 137650
    },
    {
      "epoch": 4.588666666666667,
      "grad_norm": 0.20512373745441437,
      "learning_rate": 2.1320833333333334e-05,
      "loss": 0.002,
      "step": 137660
    },
    {
      "epoch": 4.589,
      "grad_norm": 0.08571735769510269,
      "learning_rate": 2.131875e-05,
      "loss": 0.0015,
      "step": 137670
    },
    {
      "epoch": 4.589333333333333,
      "grad_norm": 0.14271049201488495,
      "learning_rate": 2.131666666666667e-05,
      "loss": 0.0017,
      "step": 137680
    },
    {
      "epoch": 4.589666666666667,
      "grad_norm": 0.22860173881053925,
      "learning_rate": 2.1314583333333334e-05,
      "loss": 0.002,
      "step": 137690
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.31441155076026917,
      "learning_rate": 2.1312500000000003e-05,
      "loss": 0.0018,
      "step": 137700
    },
    {
      "epoch": 4.590333333333334,
      "grad_norm": 0.057392895221710205,
      "learning_rate": 2.131041666666667e-05,
      "loss": 0.0014,
      "step": 137710
    },
    {
      "epoch": 4.5906666666666665,
      "grad_norm": 0.19972077012062073,
      "learning_rate": 2.1308333333333334e-05,
      "loss": 0.002,
      "step": 137720
    },
    {
      "epoch": 4.591,
      "grad_norm": 0.644403874874115,
      "learning_rate": 2.130625e-05,
      "loss": 0.0026,
      "step": 137730
    },
    {
      "epoch": 4.591333333333333,
      "grad_norm": 0.2855750620365143,
      "learning_rate": 2.130416666666667e-05,
      "loss": 0.0021,
      "step": 137740
    },
    {
      "epoch": 4.591666666666667,
      "grad_norm": 0.41946879029273987,
      "learning_rate": 2.1302083333333334e-05,
      "loss": 0.0023,
      "step": 137750
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.2615784704685211,
      "learning_rate": 2.13e-05,
      "loss": 0.0025,
      "step": 137760
    },
    {
      "epoch": 4.592333333333333,
      "grad_norm": 0.2889149785041809,
      "learning_rate": 2.1297916666666668e-05,
      "loss": 0.0017,
      "step": 137770
    },
    {
      "epoch": 4.592666666666666,
      "grad_norm": 0.14325034618377686,
      "learning_rate": 2.1295833333333334e-05,
      "loss": 0.0023,
      "step": 137780
    },
    {
      "epoch": 4.593,
      "grad_norm": 0.1010010838508606,
      "learning_rate": 2.1293750000000003e-05,
      "loss": 0.0022,
      "step": 137790
    },
    {
      "epoch": 4.593333333333334,
      "grad_norm": 0.08576235920190811,
      "learning_rate": 2.1291666666666668e-05,
      "loss": 0.0022,
      "step": 137800
    },
    {
      "epoch": 4.593666666666667,
      "grad_norm": 0.20299933850765228,
      "learning_rate": 2.1289583333333334e-05,
      "loss": 0.0021,
      "step": 137810
    },
    {
      "epoch": 4.594,
      "grad_norm": 0.05746477097272873,
      "learning_rate": 2.1287500000000002e-05,
      "loss": 0.0034,
      "step": 137820
    },
    {
      "epoch": 4.594333333333333,
      "grad_norm": 0.28510063886642456,
      "learning_rate": 2.1285416666666668e-05,
      "loss": 0.0023,
      "step": 137830
    },
    {
      "epoch": 4.594666666666667,
      "grad_norm": 0.2571706175804138,
      "learning_rate": 2.1283333333333337e-05,
      "loss": 0.0015,
      "step": 137840
    },
    {
      "epoch": 4.595,
      "grad_norm": 0.057756371796131134,
      "learning_rate": 2.128125e-05,
      "loss": 0.0018,
      "step": 137850
    },
    {
      "epoch": 4.5953333333333335,
      "grad_norm": 0.3138357996940613,
      "learning_rate": 2.1279166666666668e-05,
      "loss": 0.0019,
      "step": 137860
    },
    {
      "epoch": 4.595666666666666,
      "grad_norm": 0.23755483329296112,
      "learning_rate": 2.1277083333333333e-05,
      "loss": 0.0026,
      "step": 137870
    },
    {
      "epoch": 4.596,
      "grad_norm": 0.2284138798713684,
      "learning_rate": 2.1275000000000002e-05,
      "loss": 0.0015,
      "step": 137880
    },
    {
      "epoch": 4.596333333333334,
      "grad_norm": 0.008446881547570229,
      "learning_rate": 2.1272916666666668e-05,
      "loss": 0.0013,
      "step": 137890
    },
    {
      "epoch": 4.596666666666667,
      "grad_norm": 0.1522923856973648,
      "learning_rate": 2.1270833333333333e-05,
      "loss": 0.0023,
      "step": 137900
    },
    {
      "epoch": 4.5969999999999995,
      "grad_norm": 0.37085506319999695,
      "learning_rate": 2.1268750000000002e-05,
      "loss": 0.0021,
      "step": 137910
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.08593922853469849,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0017,
      "step": 137920
    },
    {
      "epoch": 4.597666666666667,
      "grad_norm": 0.08601005375385284,
      "learning_rate": 2.1264583333333336e-05,
      "loss": 0.0025,
      "step": 137930
    },
    {
      "epoch": 4.598,
      "grad_norm": 0.28568536043167114,
      "learning_rate": 2.1262500000000002e-05,
      "loss": 0.0024,
      "step": 137940
    },
    {
      "epoch": 4.598333333333334,
      "grad_norm": 0.2571425437927246,
      "learning_rate": 2.1260416666666667e-05,
      "loss": 0.0019,
      "step": 137950
    },
    {
      "epoch": 4.5986666666666665,
      "grad_norm": 0.14384888112545013,
      "learning_rate": 2.1258333333333336e-05,
      "loss": 0.0016,
      "step": 137960
    },
    {
      "epoch": 4.599,
      "grad_norm": 0.09996237605810165,
      "learning_rate": 2.1256249999999998e-05,
      "loss": 0.0014,
      "step": 137970
    },
    {
      "epoch": 4.599333333333333,
      "grad_norm": 0.257301926612854,
      "learning_rate": 2.1254166666666667e-05,
      "loss": 0.0019,
      "step": 137980
    },
    {
      "epoch": 4.599666666666667,
      "grad_norm": 0.14246611297130585,
      "learning_rate": 2.1252083333333333e-05,
      "loss": 0.0018,
      "step": 137990
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.19952329993247986,
      "learning_rate": 2.125e-05,
      "loss": 0.0023,
      "step": 138000
    },
    {
      "epoch": 4.600333333333333,
      "grad_norm": 0.2000340223312378,
      "learning_rate": 2.1247916666666667e-05,
      "loss": 0.0022,
      "step": 138010
    },
    {
      "epoch": 4.600666666666667,
      "grad_norm": 0.40552666783332825,
      "learning_rate": 2.1245833333333336e-05,
      "loss": 0.0027,
      "step": 138020
    },
    {
      "epoch": 4.601,
      "grad_norm": 0.11539354175329208,
      "learning_rate": 2.124375e-05,
      "loss": 0.0015,
      "step": 138030
    },
    {
      "epoch": 4.601333333333334,
      "grad_norm": 0.25701385736465454,
      "learning_rate": 2.1241666666666667e-05,
      "loss": 0.0026,
      "step": 138040
    },
    {
      "epoch": 4.601666666666667,
      "grad_norm": 0.37120309472084045,
      "learning_rate": 2.1239583333333336e-05,
      "loss": 0.0019,
      "step": 138050
    },
    {
      "epoch": 4.602,
      "grad_norm": 0.31395381689071655,
      "learning_rate": 2.12375e-05,
      "loss": 0.0018,
      "step": 138060
    },
    {
      "epoch": 4.602333333333333,
      "grad_norm": 0.14294269680976868,
      "learning_rate": 2.123541666666667e-05,
      "loss": 0.0016,
      "step": 138070
    },
    {
      "epoch": 4.602666666666667,
      "grad_norm": 0.17140915989875793,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0017,
      "step": 138080
    },
    {
      "epoch": 4.603,
      "grad_norm": 0.05806261673569679,
      "learning_rate": 2.123125e-05,
      "loss": 0.0017,
      "step": 138090
    },
    {
      "epoch": 4.6033333333333335,
      "grad_norm": 0.3148164749145508,
      "learning_rate": 2.1229166666666667e-05,
      "loss": 0.002,
      "step": 138100
    },
    {
      "epoch": 4.603666666666666,
      "grad_norm": 0.19988535344600677,
      "learning_rate": 2.1227083333333332e-05,
      "loss": 0.002,
      "step": 138110
    },
    {
      "epoch": 4.604,
      "grad_norm": 0.14265157282352448,
      "learning_rate": 2.1225e-05,
      "loss": 0.0023,
      "step": 138120
    },
    {
      "epoch": 4.604333333333333,
      "grad_norm": 0.29407089948654175,
      "learning_rate": 2.1222916666666666e-05,
      "loss": 0.0021,
      "step": 138130
    },
    {
      "epoch": 4.604666666666667,
      "grad_norm": 0.05739756301045418,
      "learning_rate": 2.1220833333333335e-05,
      "loss": 0.0018,
      "step": 138140
    },
    {
      "epoch": 4.605,
      "grad_norm": 0.45199957489967346,
      "learning_rate": 2.121875e-05,
      "loss": 0.0016,
      "step": 138150
    },
    {
      "epoch": 4.605333333333333,
      "grad_norm": 0.10905773937702179,
      "learning_rate": 2.121666666666667e-05,
      "loss": 0.0017,
      "step": 138160
    },
    {
      "epoch": 4.605666666666667,
      "grad_norm": 0.02991233393549919,
      "learning_rate": 2.1214583333333335e-05,
      "loss": 0.0016,
      "step": 138170
    },
    {
      "epoch": 4.606,
      "grad_norm": 0.11442474275827408,
      "learning_rate": 2.12125e-05,
      "loss": 0.003,
      "step": 138180
    },
    {
      "epoch": 4.606333333333334,
      "grad_norm": 0.22983038425445557,
      "learning_rate": 2.121041666666667e-05,
      "loss": 0.0013,
      "step": 138190
    },
    {
      "epoch": 4.6066666666666665,
      "grad_norm": 0.2280072718858719,
      "learning_rate": 2.1208333333333335e-05,
      "loss": 0.0017,
      "step": 138200
    },
    {
      "epoch": 4.607,
      "grad_norm": 0.11442845314741135,
      "learning_rate": 2.120625e-05,
      "loss": 0.0018,
      "step": 138210
    },
    {
      "epoch": 4.607333333333333,
      "grad_norm": 0.11452214419841766,
      "learning_rate": 2.1204166666666666e-05,
      "loss": 0.0021,
      "step": 138220
    },
    {
      "epoch": 4.607666666666667,
      "grad_norm": 0.11495908349752426,
      "learning_rate": 2.1202083333333335e-05,
      "loss": 0.0021,
      "step": 138230
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.2574698030948639,
      "learning_rate": 2.12e-05,
      "loss": 0.0026,
      "step": 138240
    },
    {
      "epoch": 4.608333333333333,
      "grad_norm": 0.08794081956148148,
      "learning_rate": 2.1197916666666666e-05,
      "loss": 0.0017,
      "step": 138250
    },
    {
      "epoch": 4.608666666666666,
      "grad_norm": 0.05817676708102226,
      "learning_rate": 2.1195833333333335e-05,
      "loss": 0.002,
      "step": 138260
    },
    {
      "epoch": 4.609,
      "grad_norm": 0.13872502744197845,
      "learning_rate": 2.119375e-05,
      "loss": 0.0016,
      "step": 138270
    },
    {
      "epoch": 4.609333333333334,
      "grad_norm": 0.16024266183376312,
      "learning_rate": 2.119166666666667e-05,
      "loss": 0.0022,
      "step": 138280
    },
    {
      "epoch": 4.609666666666667,
      "grad_norm": 0.04372521489858627,
      "learning_rate": 2.1189583333333335e-05,
      "loss": 0.0013,
      "step": 138290
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.22896531224250793,
      "learning_rate": 2.1187500000000003e-05,
      "loss": 0.0015,
      "step": 138300
    },
    {
      "epoch": 4.610333333333333,
      "grad_norm": 0.11440889537334442,
      "learning_rate": 2.118541666666667e-05,
      "loss": 0.0012,
      "step": 138310
    },
    {
      "epoch": 4.610666666666667,
      "grad_norm": 0.485046923160553,
      "learning_rate": 2.1183333333333334e-05,
      "loss": 0.002,
      "step": 138320
    },
    {
      "epoch": 4.611,
      "grad_norm": 0.00732113141566515,
      "learning_rate": 2.118125e-05,
      "loss": 0.002,
      "step": 138330
    },
    {
      "epoch": 4.6113333333333335,
      "grad_norm": 0.08987598866224289,
      "learning_rate": 2.1179166666666665e-05,
      "loss": 0.0023,
      "step": 138340
    },
    {
      "epoch": 4.611666666666666,
      "grad_norm": 0.005705962888896465,
      "learning_rate": 2.1177083333333334e-05,
      "loss": 0.0016,
      "step": 138350
    },
    {
      "epoch": 4.612,
      "grad_norm": 0.029424427077174187,
      "learning_rate": 2.1175e-05,
      "loss": 0.0024,
      "step": 138360
    },
    {
      "epoch": 4.612333333333333,
      "grad_norm": 0.029873868450522423,
      "learning_rate": 2.117291666666667e-05,
      "loss": 0.0016,
      "step": 138370
    },
    {
      "epoch": 4.612666666666667,
      "grad_norm": 0.031291358172893524,
      "learning_rate": 2.1170833333333334e-05,
      "loss": 0.0018,
      "step": 138380
    },
    {
      "epoch": 4.6129999999999995,
      "grad_norm": 0.1520247906446457,
      "learning_rate": 2.116875e-05,
      "loss": 0.0017,
      "step": 138390
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.03036283515393734,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.0014,
      "step": 138400
    },
    {
      "epoch": 4.613666666666667,
      "grad_norm": 0.07631663978099823,
      "learning_rate": 2.1164583333333334e-05,
      "loss": 0.0024,
      "step": 138410
    },
    {
      "epoch": 4.614,
      "grad_norm": 0.22914113104343414,
      "learning_rate": 2.1162500000000003e-05,
      "loss": 0.0023,
      "step": 138420
    },
    {
      "epoch": 4.614333333333334,
      "grad_norm": 0.8291670083999634,
      "learning_rate": 2.1160416666666668e-05,
      "loss": 0.0016,
      "step": 138430
    },
    {
      "epoch": 4.6146666666666665,
      "grad_norm": 0.09354297071695328,
      "learning_rate": 2.1158333333333337e-05,
      "loss": 0.0023,
      "step": 138440
    },
    {
      "epoch": 4.615,
      "grad_norm": 0.3502325415611267,
      "learning_rate": 2.115625e-05,
      "loss": 0.0015,
      "step": 138450
    },
    {
      "epoch": 4.615333333333333,
      "grad_norm": 0.05730995535850525,
      "learning_rate": 2.1154166666666668e-05,
      "loss": 0.0019,
      "step": 138460
    },
    {
      "epoch": 4.615666666666667,
      "grad_norm": 0.17155052721500397,
      "learning_rate": 2.1152083333333334e-05,
      "loss": 0.0021,
      "step": 138470
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.11424709111452103,
      "learning_rate": 2.115e-05,
      "loss": 0.0017,
      "step": 138480
    },
    {
      "epoch": 4.616333333333333,
      "grad_norm": 0.14291825890541077,
      "learning_rate": 2.1147916666666668e-05,
      "loss": 0.0019,
      "step": 138490
    },
    {
      "epoch": 4.616666666666667,
      "grad_norm": 0.10388790816068649,
      "learning_rate": 2.1145833333333333e-05,
      "loss": 0.0023,
      "step": 138500
    },
    {
      "epoch": 4.617,
      "grad_norm": 0.17190824449062347,
      "learning_rate": 2.1143750000000002e-05,
      "loss": 0.0017,
      "step": 138510
    },
    {
      "epoch": 4.617333333333333,
      "grad_norm": 0.05897156149148941,
      "learning_rate": 2.1141666666666668e-05,
      "loss": 0.0021,
      "step": 138520
    },
    {
      "epoch": 4.617666666666667,
      "grad_norm": 0.11411114037036896,
      "learning_rate": 2.1139583333333333e-05,
      "loss": 0.0021,
      "step": 138530
    },
    {
      "epoch": 4.618,
      "grad_norm": 0.0595083124935627,
      "learning_rate": 2.1137500000000002e-05,
      "loss": 0.0021,
      "step": 138540
    },
    {
      "epoch": 4.618333333333333,
      "grad_norm": 0.2144077718257904,
      "learning_rate": 2.1135416666666668e-05,
      "loss": 0.0028,
      "step": 138550
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.08663634955883026,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0015,
      "step": 138560
    },
    {
      "epoch": 4.619,
      "grad_norm": 0.20103730261325836,
      "learning_rate": 2.113125e-05,
      "loss": 0.0026,
      "step": 138570
    },
    {
      "epoch": 4.6193333333333335,
      "grad_norm": 0.0575285479426384,
      "learning_rate": 2.1129166666666668e-05,
      "loss": 0.0022,
      "step": 138580
    },
    {
      "epoch": 4.619666666666666,
      "grad_norm": 0.3422747552394867,
      "learning_rate": 2.1127083333333333e-05,
      "loss": 0.0021,
      "step": 138590
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.3145076632499695,
      "learning_rate": 2.1125000000000002e-05,
      "loss": 0.0012,
      "step": 138600
    },
    {
      "epoch": 4.620333333333333,
      "grad_norm": 0.08586857467889786,
      "learning_rate": 2.1122916666666667e-05,
      "loss": 0.0016,
      "step": 138610
    },
    {
      "epoch": 4.620666666666667,
      "grad_norm": 0.30540886521339417,
      "learning_rate": 2.1120833333333333e-05,
      "loss": 0.0021,
      "step": 138620
    },
    {
      "epoch": 4.621,
      "grad_norm": 0.17243990302085876,
      "learning_rate": 2.1118750000000002e-05,
      "loss": 0.0018,
      "step": 138630
    },
    {
      "epoch": 4.621333333333333,
      "grad_norm": 0.14337430894374847,
      "learning_rate": 2.1116666666666667e-05,
      "loss": 0.0027,
      "step": 138640
    },
    {
      "epoch": 4.621666666666667,
      "grad_norm": 0.030468259006738663,
      "learning_rate": 2.1114583333333336e-05,
      "loss": 0.0015,
      "step": 138650
    },
    {
      "epoch": 4.622,
      "grad_norm": 0.3999079465866089,
      "learning_rate": 2.11125e-05,
      "loss": 0.002,
      "step": 138660
    },
    {
      "epoch": 4.622333333333334,
      "grad_norm": 0.2002079039812088,
      "learning_rate": 2.1110416666666667e-05,
      "loss": 0.0015,
      "step": 138670
    },
    {
      "epoch": 4.6226666666666665,
      "grad_norm": 0.5144649744033813,
      "learning_rate": 2.1108333333333336e-05,
      "loss": 0.0016,
      "step": 138680
    },
    {
      "epoch": 4.623,
      "grad_norm": 0.00815661158412695,
      "learning_rate": 2.1106249999999998e-05,
      "loss": 0.0018,
      "step": 138690
    },
    {
      "epoch": 4.623333333333333,
      "grad_norm": 0.11585691571235657,
      "learning_rate": 2.1104166666666667e-05,
      "loss": 0.0023,
      "step": 138700
    },
    {
      "epoch": 4.623666666666667,
      "grad_norm": 0.5145883560180664,
      "learning_rate": 2.1102083333333332e-05,
      "loss": 0.0016,
      "step": 138710
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.008172223344445229,
      "learning_rate": 2.11e-05,
      "loss": 0.002,
      "step": 138720
    },
    {
      "epoch": 4.624333333333333,
      "grad_norm": 0.085709348320961,
      "learning_rate": 2.1097916666666667e-05,
      "loss": 0.0015,
      "step": 138730
    },
    {
      "epoch": 4.624666666666666,
      "grad_norm": 0.3143894672393799,
      "learning_rate": 2.1095833333333336e-05,
      "loss": 0.0016,
      "step": 138740
    },
    {
      "epoch": 4.625,
      "grad_norm": 0.2365216314792633,
      "learning_rate": 2.109375e-05,
      "loss": 0.0018,
      "step": 138750
    },
    {
      "epoch": 4.625333333333334,
      "grad_norm": 0.28571346402168274,
      "learning_rate": 2.1091666666666667e-05,
      "loss": 0.002,
      "step": 138760
    },
    {
      "epoch": 4.625666666666667,
      "grad_norm": 0.030908828601241112,
      "learning_rate": 2.1089583333333335e-05,
      "loss": 0.0021,
      "step": 138770
    },
    {
      "epoch": 4.626,
      "grad_norm": 0.08601614832878113,
      "learning_rate": 2.10875e-05,
      "loss": 0.0017,
      "step": 138780
    },
    {
      "epoch": 4.626333333333333,
      "grad_norm": 0.09723106026649475,
      "learning_rate": 2.108541666666667e-05,
      "loss": 0.0019,
      "step": 138790
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.34797579050064087,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.0025,
      "step": 138800
    },
    {
      "epoch": 4.627,
      "grad_norm": 0.007342128548771143,
      "learning_rate": 2.108125e-05,
      "loss": 0.0019,
      "step": 138810
    },
    {
      "epoch": 4.6273333333333335,
      "grad_norm": 0.20105154812335968,
      "learning_rate": 2.1079166666666666e-05,
      "loss": 0.0019,
      "step": 138820
    },
    {
      "epoch": 4.627666666666666,
      "grad_norm": 0.26519376039505005,
      "learning_rate": 2.1077083333333332e-05,
      "loss": 0.0019,
      "step": 138830
    },
    {
      "epoch": 4.628,
      "grad_norm": 0.03104390762746334,
      "learning_rate": 2.1075e-05,
      "loss": 0.0017,
      "step": 138840
    },
    {
      "epoch": 4.628333333333333,
      "grad_norm": 0.34255021810531616,
      "learning_rate": 2.1072916666666666e-05,
      "loss": 0.0021,
      "step": 138850
    },
    {
      "epoch": 4.628666666666667,
      "grad_norm": 0.20236387848854065,
      "learning_rate": 2.1070833333333335e-05,
      "loss": 0.0024,
      "step": 138860
    },
    {
      "epoch": 4.629,
      "grad_norm": 0.39961329102516174,
      "learning_rate": 2.106875e-05,
      "loss": 0.0017,
      "step": 138870
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.25735029578208923,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.001,
      "step": 138880
    },
    {
      "epoch": 4.629666666666667,
      "grad_norm": 0.3195602595806122,
      "learning_rate": 2.1064583333333335e-05,
      "loss": 0.0026,
      "step": 138890
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.07028014957904816,
      "learning_rate": 2.10625e-05,
      "loss": 0.0016,
      "step": 138900
    },
    {
      "epoch": 4.630333333333334,
      "grad_norm": 0.4627071022987366,
      "learning_rate": 2.106041666666667e-05,
      "loss": 0.0014,
      "step": 138910
    },
    {
      "epoch": 4.6306666666666665,
      "grad_norm": 0.0867869183421135,
      "learning_rate": 2.1058333333333335e-05,
      "loss": 0.0013,
      "step": 138920
    },
    {
      "epoch": 4.631,
      "grad_norm": 0.37118297815322876,
      "learning_rate": 2.1056250000000004e-05,
      "loss": 0.0021,
      "step": 138930
    },
    {
      "epoch": 4.631333333333333,
      "grad_norm": 0.25599250197410583,
      "learning_rate": 2.1054166666666666e-05,
      "loss": 0.0026,
      "step": 138940
    },
    {
      "epoch": 4.631666666666667,
      "grad_norm": 0.37167707085609436,
      "learning_rate": 2.1052083333333335e-05,
      "loss": 0.0014,
      "step": 138950
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.22860920429229736,
      "learning_rate": 2.105e-05,
      "loss": 0.0015,
      "step": 138960
    },
    {
      "epoch": 4.632333333333333,
      "grad_norm": 0.029347937554121017,
      "learning_rate": 2.1047916666666666e-05,
      "loss": 0.0018,
      "step": 138970
    },
    {
      "epoch": 4.632666666666667,
      "grad_norm": 0.14285941421985626,
      "learning_rate": 2.1045833333333334e-05,
      "loss": 0.0017,
      "step": 138980
    },
    {
      "epoch": 4.633,
      "grad_norm": 0.17158226668834686,
      "learning_rate": 2.104375e-05,
      "loss": 0.0014,
      "step": 138990
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 0.08665870130062103,
      "learning_rate": 2.104166666666667e-05,
      "loss": 0.002,
      "step": 139000
    },
    {
      "epoch": 4.633666666666667,
      "grad_norm": 0.05853695422410965,
      "learning_rate": 2.1039583333333334e-05,
      "loss": 0.0027,
      "step": 139010
    },
    {
      "epoch": 4.634,
      "grad_norm": 0.11441567540168762,
      "learning_rate": 2.1037500000000003e-05,
      "loss": 0.0016,
      "step": 139020
    },
    {
      "epoch": 4.634333333333333,
      "grad_norm": 0.4074834883213043,
      "learning_rate": 2.103541666666667e-05,
      "loss": 0.0018,
      "step": 139030
    },
    {
      "epoch": 4.634666666666667,
      "grad_norm": 1.0834077596664429,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0019,
      "step": 139040
    },
    {
      "epoch": 4.635,
      "grad_norm": 0.030054526403546333,
      "learning_rate": 2.1031250000000003e-05,
      "loss": 0.0012,
      "step": 139050
    },
    {
      "epoch": 4.6353333333333335,
      "grad_norm": 0.1996709555387497,
      "learning_rate": 2.1029166666666665e-05,
      "loss": 0.0014,
      "step": 139060
    },
    {
      "epoch": 4.635666666666666,
      "grad_norm": 0.28599533438682556,
      "learning_rate": 2.1027083333333334e-05,
      "loss": 0.0021,
      "step": 139070
    },
    {
      "epoch": 4.636,
      "grad_norm": 0.2038239985704422,
      "learning_rate": 2.1025e-05,
      "loss": 0.0034,
      "step": 139080
    },
    {
      "epoch": 4.636333333333333,
      "grad_norm": 0.4319874048233032,
      "learning_rate": 2.102291666666667e-05,
      "loss": 0.002,
      "step": 139090
    },
    {
      "epoch": 4.636666666666667,
      "grad_norm": 0.2282925248146057,
      "learning_rate": 2.1020833333333334e-05,
      "loss": 0.0016,
      "step": 139100
    },
    {
      "epoch": 4.6370000000000005,
      "grad_norm": 0.009808591566979885,
      "learning_rate": 2.101875e-05,
      "loss": 0.0016,
      "step": 139110
    },
    {
      "epoch": 4.637333333333333,
      "grad_norm": 0.28628212213516235,
      "learning_rate": 2.1016666666666668e-05,
      "loss": 0.0021,
      "step": 139120
    },
    {
      "epoch": 4.637666666666667,
      "grad_norm": 0.14322331547737122,
      "learning_rate": 2.1014583333333334e-05,
      "loss": 0.0024,
      "step": 139130
    },
    {
      "epoch": 4.638,
      "grad_norm": 0.17148005962371826,
      "learning_rate": 2.1012500000000003e-05,
      "loss": 0.002,
      "step": 139140
    },
    {
      "epoch": 4.638333333333334,
      "grad_norm": 0.3715392053127289,
      "learning_rate": 2.1010416666666668e-05,
      "loss": 0.0023,
      "step": 139150
    },
    {
      "epoch": 4.6386666666666665,
      "grad_norm": 0.029149604961276054,
      "learning_rate": 2.1008333333333337e-05,
      "loss": 0.0021,
      "step": 139160
    },
    {
      "epoch": 4.639,
      "grad_norm": 0.2569774389266968,
      "learning_rate": 2.1006250000000002e-05,
      "loss": 0.0022,
      "step": 139170
    },
    {
      "epoch": 4.639333333333333,
      "grad_norm": 0.346579909324646,
      "learning_rate": 2.1004166666666668e-05,
      "loss": 0.0024,
      "step": 139180
    },
    {
      "epoch": 4.639666666666667,
      "grad_norm": 0.11448242515325546,
      "learning_rate": 2.1002083333333333e-05,
      "loss": 0.0018,
      "step": 139190
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.23072709143161774,
      "learning_rate": 2.1e-05,
      "loss": 0.0016,
      "step": 139200
    },
    {
      "epoch": 4.640333333333333,
      "grad_norm": 0.02990972250699997,
      "learning_rate": 2.0997916666666668e-05,
      "loss": 0.0016,
      "step": 139210
    },
    {
      "epoch": 4.640666666666666,
      "grad_norm": 0.17146600782871246,
      "learning_rate": 2.0995833333333333e-05,
      "loss": 0.0023,
      "step": 139220
    },
    {
      "epoch": 4.641,
      "grad_norm": 0.057597238570451736,
      "learning_rate": 2.0993750000000002e-05,
      "loss": 0.0021,
      "step": 139230
    },
    {
      "epoch": 4.641333333333334,
      "grad_norm": 0.17808854579925537,
      "learning_rate": 2.0991666666666668e-05,
      "loss": 0.0017,
      "step": 139240
    },
    {
      "epoch": 4.641666666666667,
      "grad_norm": 0.029104311019182205,
      "learning_rate": 2.0989583333333333e-05,
      "loss": 0.0016,
      "step": 139250
    },
    {
      "epoch": 4.642,
      "grad_norm": 0.2002556174993515,
      "learning_rate": 2.0987500000000002e-05,
      "loss": 0.0018,
      "step": 139260
    },
    {
      "epoch": 4.642333333333333,
      "grad_norm": 0.39970651268959045,
      "learning_rate": 2.0985416666666667e-05,
      "loss": 0.0018,
      "step": 139270
    },
    {
      "epoch": 4.642666666666667,
      "grad_norm": 0.3016083538532257,
      "learning_rate": 2.0983333333333336e-05,
      "loss": 0.0019,
      "step": 139280
    },
    {
      "epoch": 4.643,
      "grad_norm": 0.17230094969272614,
      "learning_rate": 2.0981250000000002e-05,
      "loss": 0.0022,
      "step": 139290
    },
    {
      "epoch": 4.6433333333333335,
      "grad_norm": 0.08539626002311707,
      "learning_rate": 2.0979166666666667e-05,
      "loss": 0.003,
      "step": 139300
    },
    {
      "epoch": 4.643666666666666,
      "grad_norm": 0.17125290632247925,
      "learning_rate": 2.0977083333333333e-05,
      "loss": 0.0015,
      "step": 139310
    },
    {
      "epoch": 4.644,
      "grad_norm": 0.2568526864051819,
      "learning_rate": 2.0975e-05,
      "loss": 0.0019,
      "step": 139320
    },
    {
      "epoch": 4.644333333333333,
      "grad_norm": 0.3139045834541321,
      "learning_rate": 2.0972916666666667e-05,
      "loss": 0.0013,
      "step": 139330
    },
    {
      "epoch": 4.644666666666667,
      "grad_norm": 0.37169310450553894,
      "learning_rate": 2.0970833333333333e-05,
      "loss": 0.002,
      "step": 139340
    },
    {
      "epoch": 4.645,
      "grad_norm": 0.45709139108657837,
      "learning_rate": 2.096875e-05,
      "loss": 0.0016,
      "step": 139350
    },
    {
      "epoch": 4.645333333333333,
      "grad_norm": 0.1714228093624115,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.003,
      "step": 139360
    },
    {
      "epoch": 4.645666666666667,
      "grad_norm": 0.017241759225726128,
      "learning_rate": 2.0964583333333336e-05,
      "loss": 0.0024,
      "step": 139370
    },
    {
      "epoch": 4.646,
      "grad_norm": 0.13307037949562073,
      "learning_rate": 2.09625e-05,
      "loss": 0.0024,
      "step": 139380
    },
    {
      "epoch": 4.646333333333334,
      "grad_norm": 0.08309335261583328,
      "learning_rate": 2.0960416666666667e-05,
      "loss": 0.0022,
      "step": 139390
    },
    {
      "epoch": 4.6466666666666665,
      "grad_norm": 0.1715804785490036,
      "learning_rate": 2.0958333333333336e-05,
      "loss": 0.0017,
      "step": 139400
    },
    {
      "epoch": 4.647,
      "grad_norm": 0.029413584619760513,
      "learning_rate": 2.095625e-05,
      "loss": 0.0019,
      "step": 139410
    },
    {
      "epoch": 4.647333333333333,
      "grad_norm": 0.1716860681772232,
      "learning_rate": 2.0954166666666667e-05,
      "loss": 0.0019,
      "step": 139420
    },
    {
      "epoch": 4.647666666666667,
      "grad_norm": 0.14289449155330658,
      "learning_rate": 2.0952083333333332e-05,
      "loss": 0.0023,
      "step": 139430
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.20060737431049347,
      "learning_rate": 2.095e-05,
      "loss": 0.0016,
      "step": 139440
    },
    {
      "epoch": 4.648333333333333,
      "grad_norm": 0.028856651857495308,
      "learning_rate": 2.0947916666666666e-05,
      "loss": 0.0016,
      "step": 139450
    },
    {
      "epoch": 4.648666666666666,
      "grad_norm": 0.24599601328372955,
      "learning_rate": 2.0945833333333335e-05,
      "loss": 0.0021,
      "step": 139460
    },
    {
      "epoch": 4.649,
      "grad_norm": 0.2853010594844818,
      "learning_rate": 2.094375e-05,
      "loss": 0.002,
      "step": 139470
    },
    {
      "epoch": 4.649333333333333,
      "grad_norm": 0.12751494348049164,
      "learning_rate": 2.0941666666666666e-05,
      "loss": 0.0019,
      "step": 139480
    },
    {
      "epoch": 4.649666666666667,
      "grad_norm": 0.5591602921485901,
      "learning_rate": 2.0939583333333335e-05,
      "loss": 0.002,
      "step": 139490
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.058111630380153656,
      "learning_rate": 2.09375e-05,
      "loss": 0.0014,
      "step": 139500
    },
    {
      "epoch": 4.650333333333333,
      "grad_norm": 0.0307024959474802,
      "learning_rate": 2.093541666666667e-05,
      "loss": 0.0021,
      "step": 139510
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.28569868206977844,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0017,
      "step": 139520
    },
    {
      "epoch": 4.651,
      "grad_norm": 0.3653801381587982,
      "learning_rate": 2.093125e-05,
      "loss": 0.0019,
      "step": 139530
    },
    {
      "epoch": 4.6513333333333335,
      "grad_norm": 0.22863775491714478,
      "learning_rate": 2.0929166666666666e-05,
      "loss": 0.003,
      "step": 139540
    },
    {
      "epoch": 4.651666666666666,
      "grad_norm": 0.05863912031054497,
      "learning_rate": 2.0927083333333335e-05,
      "loss": 0.0022,
      "step": 139550
    },
    {
      "epoch": 4.652,
      "grad_norm": 0.08810868859291077,
      "learning_rate": 2.0925e-05,
      "loss": 0.0016,
      "step": 139560
    },
    {
      "epoch": 4.652333333333333,
      "grad_norm": 0.011974262073636055,
      "learning_rate": 2.0922916666666666e-05,
      "loss": 0.0012,
      "step": 139570
    },
    {
      "epoch": 4.652666666666667,
      "grad_norm": 0.2858477532863617,
      "learning_rate": 2.0920833333333335e-05,
      "loss": 0.0022,
      "step": 139580
    },
    {
      "epoch": 4.6530000000000005,
      "grad_norm": 0.360206663608551,
      "learning_rate": 2.091875e-05,
      "loss": 0.0022,
      "step": 139590
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.016241295263171196,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.0017,
      "step": 139600
    },
    {
      "epoch": 4.653666666666666,
      "grad_norm": 0.030880896374583244,
      "learning_rate": 2.0914583333333335e-05,
      "loss": 0.0022,
      "step": 139610
    },
    {
      "epoch": 4.654,
      "grad_norm": 0.11463763564825058,
      "learning_rate": 2.09125e-05,
      "loss": 0.0027,
      "step": 139620
    },
    {
      "epoch": 4.654333333333334,
      "grad_norm": 0.034090638160705566,
      "learning_rate": 2.091041666666667e-05,
      "loss": 0.0023,
      "step": 139630
    },
    {
      "epoch": 4.6546666666666665,
      "grad_norm": 0.11294135451316833,
      "learning_rate": 2.0908333333333334e-05,
      "loss": 0.0017,
      "step": 139640
    },
    {
      "epoch": 4.655,
      "grad_norm": 0.007615123875439167,
      "learning_rate": 2.0906250000000003e-05,
      "loss": 0.0018,
      "step": 139650
    },
    {
      "epoch": 4.655333333333333,
      "grad_norm": 0.14368829131126404,
      "learning_rate": 2.0904166666666665e-05,
      "loss": 0.0017,
      "step": 139660
    },
    {
      "epoch": 4.655666666666667,
      "grad_norm": 0.08695291727781296,
      "learning_rate": 2.0902083333333334e-05,
      "loss": 0.0018,
      "step": 139670
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.2571706175804138,
      "learning_rate": 2.09e-05,
      "loss": 0.002,
      "step": 139680
    },
    {
      "epoch": 4.656333333333333,
      "grad_norm": 0.11732002347707748,
      "learning_rate": 2.089791666666667e-05,
      "loss": 0.0016,
      "step": 139690
    },
    {
      "epoch": 4.656666666666666,
      "grad_norm": 0.25728240609169006,
      "learning_rate": 2.0895833333333334e-05,
      "loss": 0.0022,
      "step": 139700
    },
    {
      "epoch": 4.657,
      "grad_norm": 0.14293070137500763,
      "learning_rate": 2.089375e-05,
      "loss": 0.0018,
      "step": 139710
    },
    {
      "epoch": 4.657333333333334,
      "grad_norm": 0.28542113304138184,
      "learning_rate": 2.089166666666667e-05,
      "loss": 0.0025,
      "step": 139720
    },
    {
      "epoch": 4.657666666666667,
      "grad_norm": 0.6282792091369629,
      "learning_rate": 2.0889583333333334e-05,
      "loss": 0.0018,
      "step": 139730
    },
    {
      "epoch": 4.658,
      "grad_norm": 0.11533837020397186,
      "learning_rate": 2.0887500000000003e-05,
      "loss": 0.0017,
      "step": 139740
    },
    {
      "epoch": 4.658333333333333,
      "grad_norm": 0.0865933746099472,
      "learning_rate": 2.088541666666667e-05,
      "loss": 0.0021,
      "step": 139750
    },
    {
      "epoch": 4.658666666666667,
      "grad_norm": 0.35479333996772766,
      "learning_rate": 2.0883333333333334e-05,
      "loss": 0.0023,
      "step": 139760
    },
    {
      "epoch": 4.659,
      "grad_norm": 0.257430762052536,
      "learning_rate": 2.0881250000000003e-05,
      "loss": 0.002,
      "step": 139770
    },
    {
      "epoch": 4.6593333333333335,
      "grad_norm": 0.06075635924935341,
      "learning_rate": 2.0879166666666665e-05,
      "loss": 0.0019,
      "step": 139780
    },
    {
      "epoch": 4.659666666666666,
      "grad_norm": 0.02873801253736019,
      "learning_rate": 2.0877083333333334e-05,
      "loss": 0.0022,
      "step": 139790
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.11710791289806366,
      "learning_rate": 2.0875e-05,
      "loss": 0.0015,
      "step": 139800
    },
    {
      "epoch": 4.660333333333333,
      "grad_norm": 0.08639747649431229,
      "learning_rate": 2.0872916666666668e-05,
      "loss": 0.002,
      "step": 139810
    },
    {
      "epoch": 4.660666666666667,
      "grad_norm": 0.02953258529305458,
      "learning_rate": 2.0870833333333334e-05,
      "loss": 0.0019,
      "step": 139820
    },
    {
      "epoch": 4.661,
      "grad_norm": 0.2450055629014969,
      "learning_rate": 2.0868750000000002e-05,
      "loss": 0.0029,
      "step": 139830
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.08658889681100845,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.002,
      "step": 139840
    },
    {
      "epoch": 4.661666666666667,
      "grad_norm": 0.22158165276050568,
      "learning_rate": 2.0864583333333333e-05,
      "loss": 0.0013,
      "step": 139850
    },
    {
      "epoch": 4.662,
      "grad_norm": 0.3419722616672516,
      "learning_rate": 2.0862500000000002e-05,
      "loss": 0.003,
      "step": 139860
    },
    {
      "epoch": 4.662333333333334,
      "grad_norm": 0.11515572667121887,
      "learning_rate": 2.0860416666666668e-05,
      "loss": 0.0018,
      "step": 139870
    },
    {
      "epoch": 4.6626666666666665,
      "grad_norm": 0.31512683629989624,
      "learning_rate": 2.0858333333333337e-05,
      "loss": 0.0023,
      "step": 139880
    },
    {
      "epoch": 4.663,
      "grad_norm": 0.19969382882118225,
      "learning_rate": 2.0856250000000002e-05,
      "loss": 0.0014,
      "step": 139890
    },
    {
      "epoch": 4.663333333333333,
      "grad_norm": 0.17737390100955963,
      "learning_rate": 2.0854166666666668e-05,
      "loss": 0.0021,
      "step": 139900
    },
    {
      "epoch": 4.663666666666667,
      "grad_norm": 0.25704705715179443,
      "learning_rate": 2.0852083333333333e-05,
      "loss": 0.0023,
      "step": 139910
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.31772810220718384,
      "learning_rate": 2.085e-05,
      "loss": 0.0018,
      "step": 139920
    },
    {
      "epoch": 4.664333333333333,
      "grad_norm": 0.11926937848329544,
      "learning_rate": 2.0847916666666667e-05,
      "loss": 0.002,
      "step": 139930
    },
    {
      "epoch": 4.664666666666666,
      "grad_norm": 0.031505268067121506,
      "learning_rate": 2.0845833333333333e-05,
      "loss": 0.0023,
      "step": 139940
    },
    {
      "epoch": 4.665,
      "grad_norm": 0.17235733568668365,
      "learning_rate": 2.0843750000000002e-05,
      "loss": 0.0014,
      "step": 139950
    },
    {
      "epoch": 4.665333333333333,
      "grad_norm": 0.20042133331298828,
      "learning_rate": 2.0841666666666667e-05,
      "loss": 0.0016,
      "step": 139960
    },
    {
      "epoch": 4.665666666666667,
      "grad_norm": 0.22890420258045197,
      "learning_rate": 2.0839583333333336e-05,
      "loss": 0.002,
      "step": 139970
    },
    {
      "epoch": 4.666,
      "grad_norm": 0.2834285497665405,
      "learning_rate": 2.08375e-05,
      "loss": 0.0021,
      "step": 139980
    },
    {
      "epoch": 4.666333333333333,
      "grad_norm": 0.17248974740505219,
      "learning_rate": 2.0835416666666667e-05,
      "loss": 0.0016,
      "step": 139990
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.08616253733634949,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.002,
      "step": 140000
    },
    {
      "epoch": 4.667,
      "grad_norm": 0.010279743000864983,
      "learning_rate": 2.083125e-05,
      "loss": 0.003,
      "step": 140010
    },
    {
      "epoch": 4.667333333333334,
      "grad_norm": 0.14386369287967682,
      "learning_rate": 2.082916666666667e-05,
      "loss": 0.0019,
      "step": 140020
    },
    {
      "epoch": 4.667666666666666,
      "grad_norm": 0.17115052044391632,
      "learning_rate": 2.0827083333333332e-05,
      "loss": 0.0024,
      "step": 140030
    },
    {
      "epoch": 4.668,
      "grad_norm": 0.34274986386299133,
      "learning_rate": 2.0825e-05,
      "loss": 0.002,
      "step": 140040
    },
    {
      "epoch": 4.668333333333333,
      "grad_norm": 0.005765893030911684,
      "learning_rate": 2.0822916666666667e-05,
      "loss": 0.002,
      "step": 140050
    },
    {
      "epoch": 4.668666666666667,
      "grad_norm": 0.17167270183563232,
      "learning_rate": 2.0820833333333332e-05,
      "loss": 0.003,
      "step": 140060
    },
    {
      "epoch": 4.6690000000000005,
      "grad_norm": 0.3711383044719696,
      "learning_rate": 2.081875e-05,
      "loss": 0.0018,
      "step": 140070
    },
    {
      "epoch": 4.669333333333333,
      "grad_norm": 0.23595453798770905,
      "learning_rate": 2.0816666666666667e-05,
      "loss": 0.0023,
      "step": 140080
    },
    {
      "epoch": 4.669666666666666,
      "grad_norm": 0.44006621837615967,
      "learning_rate": 2.0814583333333336e-05,
      "loss": 0.0026,
      "step": 140090
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.005084656644612551,
      "learning_rate": 2.08125e-05,
      "loss": 0.0018,
      "step": 140100
    },
    {
      "epoch": 4.670333333333334,
      "grad_norm": 0.09219015389680862,
      "learning_rate": 2.081041666666667e-05,
      "loss": 0.0025,
      "step": 140110
    },
    {
      "epoch": 4.6706666666666665,
      "grad_norm": 0.059510692954063416,
      "learning_rate": 2.0808333333333335e-05,
      "loss": 0.0024,
      "step": 140120
    },
    {
      "epoch": 4.671,
      "grad_norm": 0.10275890678167343,
      "learning_rate": 2.080625e-05,
      "loss": 0.0022,
      "step": 140130
    },
    {
      "epoch": 4.671333333333333,
      "grad_norm": 0.20055118203163147,
      "learning_rate": 2.080416666666667e-05,
      "loss": 0.0014,
      "step": 140140
    },
    {
      "epoch": 4.671666666666667,
      "grad_norm": 0.11528125405311584,
      "learning_rate": 2.0802083333333332e-05,
      "loss": 0.0015,
      "step": 140150
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.007273806259036064,
      "learning_rate": 2.08e-05,
      "loss": 0.0023,
      "step": 140160
    },
    {
      "epoch": 4.6723333333333334,
      "grad_norm": 0.11264064162969589,
      "learning_rate": 2.0797916666666666e-05,
      "loss": 0.0022,
      "step": 140170
    },
    {
      "epoch": 4.672666666666666,
      "grad_norm": 0.1431860625743866,
      "learning_rate": 2.0795833333333335e-05,
      "loss": 0.0015,
      "step": 140180
    },
    {
      "epoch": 4.673,
      "grad_norm": 0.22827377915382385,
      "learning_rate": 2.079375e-05,
      "loss": 0.0022,
      "step": 140190
    },
    {
      "epoch": 4.673333333333334,
      "grad_norm": 0.1431729793548584,
      "learning_rate": 2.0791666666666666e-05,
      "loss": 0.0023,
      "step": 140200
    },
    {
      "epoch": 4.673666666666667,
      "grad_norm": 0.3428938090801239,
      "learning_rate": 2.0789583333333335e-05,
      "loss": 0.0019,
      "step": 140210
    },
    {
      "epoch": 4.674,
      "grad_norm": 0.17190858721733093,
      "learning_rate": 2.07875e-05,
      "loss": 0.002,
      "step": 140220
    },
    {
      "epoch": 4.674333333333333,
      "grad_norm": 0.3141656816005707,
      "learning_rate": 2.078541666666667e-05,
      "loss": 0.0017,
      "step": 140230
    },
    {
      "epoch": 4.674666666666667,
      "grad_norm": 0.3144458532333374,
      "learning_rate": 2.0783333333333335e-05,
      "loss": 0.0019,
      "step": 140240
    },
    {
      "epoch": 4.675,
      "grad_norm": 0.11472555994987488,
      "learning_rate": 2.0781250000000004e-05,
      "loss": 0.0025,
      "step": 140250
    },
    {
      "epoch": 4.675333333333334,
      "grad_norm": 0.02948114462196827,
      "learning_rate": 2.077916666666667e-05,
      "loss": 0.0013,
      "step": 140260
    },
    {
      "epoch": 4.675666666666666,
      "grad_norm": 0.007849515415728092,
      "learning_rate": 2.0777083333333335e-05,
      "loss": 0.0018,
      "step": 140270
    },
    {
      "epoch": 4.676,
      "grad_norm": 0.031089086085557938,
      "learning_rate": 2.0775e-05,
      "loss": 0.0019,
      "step": 140280
    },
    {
      "epoch": 4.676333333333333,
      "grad_norm": 0.01226840354502201,
      "learning_rate": 2.0772916666666666e-05,
      "loss": 0.0025,
      "step": 140290
    },
    {
      "epoch": 4.676666666666667,
      "grad_norm": 0.19987276196479797,
      "learning_rate": 2.0770833333333335e-05,
      "loss": 0.0018,
      "step": 140300
    },
    {
      "epoch": 4.677,
      "grad_norm": 0.14364755153656006,
      "learning_rate": 2.076875e-05,
      "loss": 0.0022,
      "step": 140310
    },
    {
      "epoch": 4.677333333333333,
      "grad_norm": 0.05722270533442497,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.0021,
      "step": 140320
    },
    {
      "epoch": 4.677666666666667,
      "grad_norm": 0.14558617770671844,
      "learning_rate": 2.0764583333333334e-05,
      "loss": 0.0022,
      "step": 140330
    },
    {
      "epoch": 4.678,
      "grad_norm": 0.14978185296058655,
      "learning_rate": 2.07625e-05,
      "loss": 0.0019,
      "step": 140340
    },
    {
      "epoch": 4.678333333333334,
      "grad_norm": 0.042788103222846985,
      "learning_rate": 2.076041666666667e-05,
      "loss": 0.0023,
      "step": 140350
    },
    {
      "epoch": 4.6786666666666665,
      "grad_norm": 0.05754292756319046,
      "learning_rate": 2.0758333333333334e-05,
      "loss": 0.0012,
      "step": 140360
    },
    {
      "epoch": 4.679,
      "grad_norm": 0.4320793151855469,
      "learning_rate": 2.0756250000000003e-05,
      "loss": 0.0027,
      "step": 140370
    },
    {
      "epoch": 4.679333333333333,
      "grad_norm": 0.1716737598180771,
      "learning_rate": 2.075416666666667e-05,
      "loss": 0.0031,
      "step": 140380
    },
    {
      "epoch": 4.679666666666667,
      "grad_norm": 0.25703370571136475,
      "learning_rate": 2.0752083333333334e-05,
      "loss": 0.0018,
      "step": 140390
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.14279618859291077,
      "learning_rate": 2.075e-05,
      "loss": 0.0024,
      "step": 140400
    },
    {
      "epoch": 4.6803333333333335,
      "grad_norm": 0.03253718093037605,
      "learning_rate": 2.074791666666667e-05,
      "loss": 0.002,
      "step": 140410
    },
    {
      "epoch": 4.680666666666666,
      "grad_norm": 0.26811501383781433,
      "learning_rate": 2.0745833333333334e-05,
      "loss": 0.0016,
      "step": 140420
    },
    {
      "epoch": 4.681,
      "grad_norm": 0.08591987937688828,
      "learning_rate": 2.074375e-05,
      "loss": 0.002,
      "step": 140430
    },
    {
      "epoch": 4.681333333333333,
      "grad_norm": 0.2587137222290039,
      "learning_rate": 2.0741666666666668e-05,
      "loss": 0.002,
      "step": 140440
    },
    {
      "epoch": 4.681666666666667,
      "grad_norm": 0.03118290938436985,
      "learning_rate": 2.0739583333333334e-05,
      "loss": 0.0015,
      "step": 140450
    },
    {
      "epoch": 4.682,
      "grad_norm": 0.12068015336990356,
      "learning_rate": 2.0737500000000003e-05,
      "loss": 0.0026,
      "step": 140460
    },
    {
      "epoch": 4.682333333333333,
      "grad_norm": 0.20061573386192322,
      "learning_rate": 2.0735416666666668e-05,
      "loss": 0.0018,
      "step": 140470
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.31500160694122314,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0022,
      "step": 140480
    },
    {
      "epoch": 4.683,
      "grad_norm": 0.09844934940338135,
      "learning_rate": 2.0731250000000002e-05,
      "loss": 0.002,
      "step": 140490
    },
    {
      "epoch": 4.683333333333334,
      "grad_norm": 0.05805527791380882,
      "learning_rate": 2.0729166666666668e-05,
      "loss": 0.002,
      "step": 140500
    },
    {
      "epoch": 4.683666666666666,
      "grad_norm": 0.1261742264032364,
      "learning_rate": 2.0727083333333333e-05,
      "loss": 0.0016,
      "step": 140510
    },
    {
      "epoch": 4.684,
      "grad_norm": 0.19985589385032654,
      "learning_rate": 2.0725e-05,
      "loss": 0.0023,
      "step": 140520
    },
    {
      "epoch": 4.684333333333333,
      "grad_norm": 0.08606178313493729,
      "learning_rate": 2.0722916666666668e-05,
      "loss": 0.003,
      "step": 140530
    },
    {
      "epoch": 4.684666666666667,
      "grad_norm": 0.25746649503707886,
      "learning_rate": 2.0720833333333333e-05,
      "loss": 0.0019,
      "step": 140540
    },
    {
      "epoch": 4.6850000000000005,
      "grad_norm": 0.11483727395534515,
      "learning_rate": 2.0718750000000002e-05,
      "loss": 0.0013,
      "step": 140550
    },
    {
      "epoch": 4.685333333333333,
      "grad_norm": 0.4350457787513733,
      "learning_rate": 2.0716666666666668e-05,
      "loss": 0.0021,
      "step": 140560
    },
    {
      "epoch": 4.685666666666666,
      "grad_norm": 0.2286481112241745,
      "learning_rate": 2.0714583333333333e-05,
      "loss": 0.0013,
      "step": 140570
    },
    {
      "epoch": 4.686,
      "grad_norm": 0.4282377362251282,
      "learning_rate": 2.0712500000000002e-05,
      "loss": 0.0019,
      "step": 140580
    },
    {
      "epoch": 4.686333333333334,
      "grad_norm": 0.2289951741695404,
      "learning_rate": 2.0710416666666668e-05,
      "loss": 0.0019,
      "step": 140590
    },
    {
      "epoch": 4.6866666666666665,
      "grad_norm": 0.28591543436050415,
      "learning_rate": 2.0708333333333336e-05,
      "loss": 0.0016,
      "step": 140600
    },
    {
      "epoch": 4.687,
      "grad_norm": 0.14374230802059174,
      "learning_rate": 2.0706250000000002e-05,
      "loss": 0.0018,
      "step": 140610
    },
    {
      "epoch": 4.687333333333333,
      "grad_norm": 0.14328673481941223,
      "learning_rate": 2.0704166666666667e-05,
      "loss": 0.0019,
      "step": 140620
    },
    {
      "epoch": 4.687666666666667,
      "grad_norm": 0.24894987046718597,
      "learning_rate": 2.0702083333333333e-05,
      "loss": 0.0022,
      "step": 140630
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.3801596164703369,
      "learning_rate": 2.07e-05,
      "loss": 0.0022,
      "step": 140640
    },
    {
      "epoch": 4.6883333333333335,
      "grad_norm": 0.4453375041484833,
      "learning_rate": 2.0697916666666667e-05,
      "loss": 0.002,
      "step": 140650
    },
    {
      "epoch": 4.688666666666666,
      "grad_norm": 0.22875338792800903,
      "learning_rate": 2.0695833333333333e-05,
      "loss": 0.0028,
      "step": 140660
    },
    {
      "epoch": 4.689,
      "grad_norm": 0.007759666070342064,
      "learning_rate": 2.069375e-05,
      "loss": 0.0026,
      "step": 140670
    },
    {
      "epoch": 4.689333333333334,
      "grad_norm": 0.22853797674179077,
      "learning_rate": 2.0691666666666667e-05,
      "loss": 0.0022,
      "step": 140680
    },
    {
      "epoch": 4.689666666666667,
      "grad_norm": 0.3162432014942169,
      "learning_rate": 2.0689583333333336e-05,
      "loss": 0.0021,
      "step": 140690
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 0.2571486234664917,
      "learning_rate": 2.06875e-05,
      "loss": 0.0018,
      "step": 140700
    },
    {
      "epoch": 4.690333333333333,
      "grad_norm": 0.355929434299469,
      "learning_rate": 2.0685416666666667e-05,
      "loss": 0.0025,
      "step": 140710
    },
    {
      "epoch": 4.690666666666667,
      "grad_norm": 0.11441739648580551,
      "learning_rate": 2.0683333333333336e-05,
      "loss": 0.0022,
      "step": 140720
    },
    {
      "epoch": 4.691,
      "grad_norm": 0.14315946400165558,
      "learning_rate": 2.068125e-05,
      "loss": 0.0018,
      "step": 140730
    },
    {
      "epoch": 4.691333333333334,
      "grad_norm": 0.17175443470478058,
      "learning_rate": 2.067916666666667e-05,
      "loss": 0.0019,
      "step": 140740
    },
    {
      "epoch": 4.691666666666666,
      "grad_norm": 0.2860143482685089,
      "learning_rate": 2.0677083333333332e-05,
      "loss": 0.0017,
      "step": 140750
    },
    {
      "epoch": 4.692,
      "grad_norm": 0.1452820748090744,
      "learning_rate": 2.0675e-05,
      "loss": 0.0022,
      "step": 140760
    },
    {
      "epoch": 4.692333333333333,
      "grad_norm": 0.3637308180332184,
      "learning_rate": 2.0672916666666667e-05,
      "loss": 0.0013,
      "step": 140770
    },
    {
      "epoch": 4.692666666666667,
      "grad_norm": 0.2000332772731781,
      "learning_rate": 2.0670833333333332e-05,
      "loss": 0.0016,
      "step": 140780
    },
    {
      "epoch": 4.693,
      "grad_norm": 0.22828608751296997,
      "learning_rate": 2.066875e-05,
      "loss": 0.0013,
      "step": 140790
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.14338566362857819,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0017,
      "step": 140800
    },
    {
      "epoch": 4.693666666666667,
      "grad_norm": 0.057191383093595505,
      "learning_rate": 2.0664583333333335e-05,
      "loss": 0.0028,
      "step": 140810
    },
    {
      "epoch": 4.694,
      "grad_norm": 0.31434816122055054,
      "learning_rate": 2.06625e-05,
      "loss": 0.0014,
      "step": 140820
    },
    {
      "epoch": 4.694333333333334,
      "grad_norm": 0.34289148449897766,
      "learning_rate": 2.066041666666667e-05,
      "loss": 0.0014,
      "step": 140830
    },
    {
      "epoch": 4.6946666666666665,
      "grad_norm": 0.13388563692569733,
      "learning_rate": 2.0658333333333335e-05,
      "loss": 0.0025,
      "step": 140840
    },
    {
      "epoch": 4.695,
      "grad_norm": 0.1529342234134674,
      "learning_rate": 2.065625e-05,
      "loss": 0.0021,
      "step": 140850
    },
    {
      "epoch": 4.695333333333333,
      "grad_norm": 0.11475463211536407,
      "learning_rate": 2.065416666666667e-05,
      "loss": 0.0028,
      "step": 140860
    },
    {
      "epoch": 4.695666666666667,
      "grad_norm": 0.029064370319247246,
      "learning_rate": 2.065208333333333e-05,
      "loss": 0.0015,
      "step": 140870
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.20012734830379486,
      "learning_rate": 2.065e-05,
      "loss": 0.0028,
      "step": 140880
    },
    {
      "epoch": 4.6963333333333335,
      "grad_norm": 0.22912688553333282,
      "learning_rate": 2.0647916666666666e-05,
      "loss": 0.0026,
      "step": 140890
    },
    {
      "epoch": 4.696666666666666,
      "grad_norm": 0.08505955338478088,
      "learning_rate": 2.0645833333333335e-05,
      "loss": 0.0023,
      "step": 140900
    },
    {
      "epoch": 4.697,
      "grad_norm": 0.030048396438360214,
      "learning_rate": 2.064375e-05,
      "loss": 0.0019,
      "step": 140910
    },
    {
      "epoch": 4.697333333333333,
      "grad_norm": 0.19996638596057892,
      "learning_rate": 2.0641666666666666e-05,
      "loss": 0.0022,
      "step": 140920
    },
    {
      "epoch": 4.697666666666667,
      "grad_norm": 0.24689550697803497,
      "learning_rate": 2.0639583333333335e-05,
      "loss": 0.0024,
      "step": 140930
    },
    {
      "epoch": 4.698,
      "grad_norm": 0.16521179676055908,
      "learning_rate": 2.06375e-05,
      "loss": 0.0021,
      "step": 140940
    },
    {
      "epoch": 4.698333333333333,
      "grad_norm": 0.057593245059251785,
      "learning_rate": 2.063541666666667e-05,
      "loss": 0.0018,
      "step": 140950
    },
    {
      "epoch": 4.698666666666667,
      "grad_norm": 0.17152561247348785,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.0013,
      "step": 140960
    },
    {
      "epoch": 4.699,
      "grad_norm": 0.08620407432317734,
      "learning_rate": 2.0631250000000003e-05,
      "loss": 0.0018,
      "step": 140970
    },
    {
      "epoch": 4.699333333333334,
      "grad_norm": 0.42016953229904175,
      "learning_rate": 2.062916666666667e-05,
      "loss": 0.0022,
      "step": 140980
    },
    {
      "epoch": 4.699666666666666,
      "grad_norm": 0.12255912274122238,
      "learning_rate": 2.0627083333333334e-05,
      "loss": 0.002,
      "step": 140990
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.23467805981636047,
      "learning_rate": 2.0625e-05,
      "loss": 0.0014,
      "step": 141000
    },
    {
      "epoch": 4.700333333333333,
      "grad_norm": 0.3485977053642273,
      "learning_rate": 2.0622916666666665e-05,
      "loss": 0.0025,
      "step": 141010
    },
    {
      "epoch": 4.700666666666667,
      "grad_norm": 0.22919060289859772,
      "learning_rate": 2.0620833333333334e-05,
      "loss": 0.0021,
      "step": 141020
    },
    {
      "epoch": 4.701,
      "grad_norm": 0.143715500831604,
      "learning_rate": 2.061875e-05,
      "loss": 0.0025,
      "step": 141030
    },
    {
      "epoch": 4.701333333333333,
      "grad_norm": 0.11424632370471954,
      "learning_rate": 2.061666666666667e-05,
      "loss": 0.0016,
      "step": 141040
    },
    {
      "epoch": 4.701666666666666,
      "grad_norm": 0.08586782962083817,
      "learning_rate": 2.0614583333333334e-05,
      "loss": 0.0022,
      "step": 141050
    },
    {
      "epoch": 4.702,
      "grad_norm": 0.029212873429059982,
      "learning_rate": 2.06125e-05,
      "loss": 0.002,
      "step": 141060
    },
    {
      "epoch": 4.702333333333334,
      "grad_norm": 0.3427658975124359,
      "learning_rate": 2.061041666666667e-05,
      "loss": 0.0023,
      "step": 141070
    },
    {
      "epoch": 4.7026666666666666,
      "grad_norm": 0.11439461261034012,
      "learning_rate": 2.0608333333333334e-05,
      "loss": 0.0016,
      "step": 141080
    },
    {
      "epoch": 4.703,
      "grad_norm": 0.1999807059764862,
      "learning_rate": 2.0606250000000003e-05,
      "loss": 0.002,
      "step": 141090
    },
    {
      "epoch": 4.703333333333333,
      "grad_norm": 0.11488994210958481,
      "learning_rate": 2.060416666666667e-05,
      "loss": 0.0023,
      "step": 141100
    },
    {
      "epoch": 4.703666666666667,
      "grad_norm": 0.3475921154022217,
      "learning_rate": 2.0602083333333334e-05,
      "loss": 0.0024,
      "step": 141110
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.13944284617900848,
      "learning_rate": 2.06e-05,
      "loss": 0.0016,
      "step": 141120
    },
    {
      "epoch": 4.7043333333333335,
      "grad_norm": 0.12510138750076294,
      "learning_rate": 2.0597916666666668e-05,
      "loss": 0.0026,
      "step": 141130
    },
    {
      "epoch": 4.704666666666666,
      "grad_norm": 0.25061342120170593,
      "learning_rate": 2.0595833333333334e-05,
      "loss": 0.0025,
      "step": 141140
    },
    {
      "epoch": 4.705,
      "grad_norm": 0.17138360440731049,
      "learning_rate": 2.059375e-05,
      "loss": 0.0019,
      "step": 141150
    },
    {
      "epoch": 4.705333333333334,
      "grad_norm": 0.03361013904213905,
      "learning_rate": 2.0591666666666668e-05,
      "loss": 0.0011,
      "step": 141160
    },
    {
      "epoch": 4.705666666666667,
      "grad_norm": 0.05925774201750755,
      "learning_rate": 2.0589583333333333e-05,
      "loss": 0.0028,
      "step": 141170
    },
    {
      "epoch": 4.7059999999999995,
      "grad_norm": 0.257636696100235,
      "learning_rate": 2.0587500000000002e-05,
      "loss": 0.0022,
      "step": 141180
    },
    {
      "epoch": 4.706333333333333,
      "grad_norm": 0.314087837934494,
      "learning_rate": 2.0585416666666668e-05,
      "loss": 0.0014,
      "step": 141190
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.14334946870803833,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.0026,
      "step": 141200
    },
    {
      "epoch": 4.707,
      "grad_norm": 0.6278288960456848,
      "learning_rate": 2.0581250000000002e-05,
      "loss": 0.0028,
      "step": 141210
    },
    {
      "epoch": 4.707333333333334,
      "grad_norm": 0.1885516494512558,
      "learning_rate": 2.0579166666666668e-05,
      "loss": 0.0019,
      "step": 141220
    },
    {
      "epoch": 4.707666666666666,
      "grad_norm": 0.009568259119987488,
      "learning_rate": 2.0577083333333333e-05,
      "loss": 0.0015,
      "step": 141230
    },
    {
      "epoch": 4.708,
      "grad_norm": 0.5095067024230957,
      "learning_rate": 2.0575e-05,
      "loss": 0.0015,
      "step": 141240
    },
    {
      "epoch": 4.708333333333333,
      "grad_norm": 0.08616214990615845,
      "learning_rate": 2.0572916666666668e-05,
      "loss": 0.0012,
      "step": 141250
    },
    {
      "epoch": 4.708666666666667,
      "grad_norm": 0.2999045252799988,
      "learning_rate": 2.0570833333333333e-05,
      "loss": 0.0034,
      "step": 141260
    },
    {
      "epoch": 4.709,
      "grad_norm": 0.17169712483882904,
      "learning_rate": 2.0568750000000002e-05,
      "loss": 0.0022,
      "step": 141270
    },
    {
      "epoch": 4.709333333333333,
      "grad_norm": 0.34315669536590576,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0015,
      "step": 141280
    },
    {
      "epoch": 4.709666666666667,
      "grad_norm": 0.1152413934469223,
      "learning_rate": 2.0564583333333333e-05,
      "loss": 0.0014,
      "step": 141290
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.2574763298034668,
      "learning_rate": 2.0562500000000002e-05,
      "loss": 0.0013,
      "step": 141300
    },
    {
      "epoch": 4.710333333333334,
      "grad_norm": 0.058563876897096634,
      "learning_rate": 2.0560416666666667e-05,
      "loss": 0.0021,
      "step": 141310
    },
    {
      "epoch": 4.710666666666667,
      "grad_norm": 0.1433960646390915,
      "learning_rate": 2.0558333333333336e-05,
      "loss": 0.0012,
      "step": 141320
    },
    {
      "epoch": 4.711,
      "grad_norm": 0.22903607785701752,
      "learning_rate": 2.055625e-05,
      "loss": 0.0019,
      "step": 141330
    },
    {
      "epoch": 4.711333333333333,
      "grad_norm": 0.05770497024059296,
      "learning_rate": 2.055416666666667e-05,
      "loss": 0.0014,
      "step": 141340
    },
    {
      "epoch": 4.711666666666667,
      "grad_norm": 0.4170903265476227,
      "learning_rate": 2.0552083333333336e-05,
      "loss": 0.0012,
      "step": 141350
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.2856920659542084,
      "learning_rate": 2.055e-05,
      "loss": 0.0016,
      "step": 141360
    },
    {
      "epoch": 4.7123333333333335,
      "grad_norm": 0.08887021988630295,
      "learning_rate": 2.0547916666666667e-05,
      "loss": 0.0026,
      "step": 141370
    },
    {
      "epoch": 4.712666666666666,
      "grad_norm": 0.34319618344306946,
      "learning_rate": 2.0545833333333332e-05,
      "loss": 0.0026,
      "step": 141380
    },
    {
      "epoch": 4.713,
      "grad_norm": 0.09281589090824127,
      "learning_rate": 2.054375e-05,
      "loss": 0.0017,
      "step": 141390
    },
    {
      "epoch": 4.713333333333333,
      "grad_norm": 0.1431460976600647,
      "learning_rate": 2.0541666666666667e-05,
      "loss": 0.0026,
      "step": 141400
    },
    {
      "epoch": 4.713666666666667,
      "grad_norm": 0.2858494222164154,
      "learning_rate": 2.0539583333333336e-05,
      "loss": 0.0017,
      "step": 141410
    },
    {
      "epoch": 4.714,
      "grad_norm": 0.2574070990085602,
      "learning_rate": 2.05375e-05,
      "loss": 0.0014,
      "step": 141420
    },
    {
      "epoch": 4.714333333333333,
      "grad_norm": 0.029444178566336632,
      "learning_rate": 2.0535416666666667e-05,
      "loss": 0.0017,
      "step": 141430
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.09270015358924866,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0019,
      "step": 141440
    },
    {
      "epoch": 4.715,
      "grad_norm": 0.3427853584289551,
      "learning_rate": 2.053125e-05,
      "loss": 0.0018,
      "step": 141450
    },
    {
      "epoch": 4.715333333333334,
      "grad_norm": 0.17228682339191437,
      "learning_rate": 2.052916666666667e-05,
      "loss": 0.0024,
      "step": 141460
    },
    {
      "epoch": 4.7156666666666665,
      "grad_norm": 0.07316401600837708,
      "learning_rate": 2.0527083333333335e-05,
      "loss": 0.003,
      "step": 141470
    },
    {
      "epoch": 4.716,
      "grad_norm": 0.24559395015239716,
      "learning_rate": 2.0525e-05,
      "loss": 0.0023,
      "step": 141480
    },
    {
      "epoch": 4.716333333333333,
      "grad_norm": 0.17137248814105988,
      "learning_rate": 2.0522916666666666e-05,
      "loss": 0.0023,
      "step": 141490
    },
    {
      "epoch": 4.716666666666667,
      "grad_norm": 0.2016884684562683,
      "learning_rate": 2.0520833333333335e-05,
      "loss": 0.0013,
      "step": 141500
    },
    {
      "epoch": 4.717,
      "grad_norm": 0.14309938251972198,
      "learning_rate": 2.051875e-05,
      "loss": 0.0012,
      "step": 141510
    },
    {
      "epoch": 4.717333333333333,
      "grad_norm": 0.033390846103429794,
      "learning_rate": 2.0516666666666666e-05,
      "loss": 0.0018,
      "step": 141520
    },
    {
      "epoch": 4.717666666666666,
      "grad_norm": 0.2859499752521515,
      "learning_rate": 2.0514583333333335e-05,
      "loss": 0.0018,
      "step": 141530
    },
    {
      "epoch": 4.718,
      "grad_norm": 0.045079946517944336,
      "learning_rate": 2.05125e-05,
      "loss": 0.0017,
      "step": 141540
    },
    {
      "epoch": 4.718333333333334,
      "grad_norm": 0.08663444966077805,
      "learning_rate": 2.051041666666667e-05,
      "loss": 0.0016,
      "step": 141550
    },
    {
      "epoch": 4.718666666666667,
      "grad_norm": 0.45357805490493774,
      "learning_rate": 2.0508333333333335e-05,
      "loss": 0.0016,
      "step": 141560
    },
    {
      "epoch": 4.719,
      "grad_norm": 0.028939571231603622,
      "learning_rate": 2.050625e-05,
      "loss": 0.0013,
      "step": 141570
    },
    {
      "epoch": 4.719333333333333,
      "grad_norm": 0.08670612424612045,
      "learning_rate": 2.050416666666667e-05,
      "loss": 0.0019,
      "step": 141580
    },
    {
      "epoch": 4.719666666666667,
      "grad_norm": 0.05751933157444,
      "learning_rate": 2.0502083333333335e-05,
      "loss": 0.0027,
      "step": 141590
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.22960373759269714,
      "learning_rate": 2.05e-05,
      "loss": 0.0019,
      "step": 141600
    },
    {
      "epoch": 4.7203333333333335,
      "grad_norm": 0.0858013853430748,
      "learning_rate": 2.0497916666666666e-05,
      "loss": 0.0019,
      "step": 141610
    },
    {
      "epoch": 4.720666666666666,
      "grad_norm": 0.19979903101921082,
      "learning_rate": 2.0495833333333335e-05,
      "loss": 0.0016,
      "step": 141620
    },
    {
      "epoch": 4.721,
      "grad_norm": 0.28575190901756287,
      "learning_rate": 2.049375e-05,
      "loss": 0.0023,
      "step": 141630
    },
    {
      "epoch": 4.721333333333334,
      "grad_norm": 0.05807845667004585,
      "learning_rate": 2.049166666666667e-05,
      "loss": 0.002,
      "step": 141640
    },
    {
      "epoch": 4.721666666666667,
      "grad_norm": 0.17170670628547668,
      "learning_rate": 2.0489583333333334e-05,
      "loss": 0.0016,
      "step": 141650
    },
    {
      "epoch": 4.7219999999999995,
      "grad_norm": 0.31961438059806824,
      "learning_rate": 2.04875e-05,
      "loss": 0.0021,
      "step": 141660
    },
    {
      "epoch": 4.722333333333333,
      "grad_norm": 0.26557573676109314,
      "learning_rate": 2.048541666666667e-05,
      "loss": 0.0024,
      "step": 141670
    },
    {
      "epoch": 4.722666666666667,
      "grad_norm": 0.07593207061290741,
      "learning_rate": 2.0483333333333334e-05,
      "loss": 0.0025,
      "step": 141680
    },
    {
      "epoch": 4.723,
      "grad_norm": 0.31452426314353943,
      "learning_rate": 2.0481250000000003e-05,
      "loss": 0.0019,
      "step": 141690
    },
    {
      "epoch": 4.723333333333334,
      "grad_norm": 0.029940268024802208,
      "learning_rate": 2.047916666666667e-05,
      "loss": 0.0016,
      "step": 141700
    },
    {
      "epoch": 4.7236666666666665,
      "grad_norm": 0.059164904057979584,
      "learning_rate": 2.0477083333333334e-05,
      "loss": 0.0027,
      "step": 141710
    },
    {
      "epoch": 4.724,
      "grad_norm": 0.058235809206962585,
      "learning_rate": 2.0475e-05,
      "loss": 0.0023,
      "step": 141720
    },
    {
      "epoch": 4.724333333333333,
      "grad_norm": 0.029965410009026527,
      "learning_rate": 2.0472916666666665e-05,
      "loss": 0.0015,
      "step": 141730
    },
    {
      "epoch": 4.724666666666667,
      "grad_norm": 0.25708675384521484,
      "learning_rate": 2.0470833333333334e-05,
      "loss": 0.0018,
      "step": 141740
    },
    {
      "epoch": 4.725,
      "grad_norm": 0.3432026505470276,
      "learning_rate": 2.046875e-05,
      "loss": 0.0018,
      "step": 141750
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.17181949317455292,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.002,
      "step": 141760
    },
    {
      "epoch": 4.725666666666667,
      "grad_norm": 0.31706076860427856,
      "learning_rate": 2.0464583333333334e-05,
      "loss": 0.0021,
      "step": 141770
    },
    {
      "epoch": 4.726,
      "grad_norm": 0.08609968423843384,
      "learning_rate": 2.0462500000000003e-05,
      "loss": 0.002,
      "step": 141780
    },
    {
      "epoch": 4.726333333333334,
      "grad_norm": 0.1156366840004921,
      "learning_rate": 2.0460416666666668e-05,
      "loss": 0.0016,
      "step": 141790
    },
    {
      "epoch": 4.726666666666667,
      "grad_norm": 0.2855823040008545,
      "learning_rate": 2.0458333333333334e-05,
      "loss": 0.0021,
      "step": 141800
    },
    {
      "epoch": 4.727,
      "grad_norm": 0.486817330121994,
      "learning_rate": 2.0456250000000003e-05,
      "loss": 0.0021,
      "step": 141810
    },
    {
      "epoch": 4.727333333333333,
      "grad_norm": 0.25887322425842285,
      "learning_rate": 2.0454166666666668e-05,
      "loss": 0.0015,
      "step": 141820
    },
    {
      "epoch": 4.727666666666667,
      "grad_norm": 0.2863992750644684,
      "learning_rate": 2.0452083333333337e-05,
      "loss": 0.002,
      "step": 141830
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.201047882437706,
      "learning_rate": 2.045e-05,
      "loss": 0.002,
      "step": 141840
    },
    {
      "epoch": 4.7283333333333335,
      "grad_norm": 0.0909198597073555,
      "learning_rate": 2.0447916666666668e-05,
      "loss": 0.0026,
      "step": 141850
    },
    {
      "epoch": 4.728666666666666,
      "grad_norm": 0.057635460048913956,
      "learning_rate": 2.0445833333333333e-05,
      "loss": 0.0024,
      "step": 141860
    },
    {
      "epoch": 4.729,
      "grad_norm": 0.058658141642808914,
      "learning_rate": 2.044375e-05,
      "loss": 0.002,
      "step": 141870
    },
    {
      "epoch": 4.729333333333333,
      "grad_norm": 0.17235809564590454,
      "learning_rate": 2.0441666666666668e-05,
      "loss": 0.0013,
      "step": 141880
    },
    {
      "epoch": 4.729666666666667,
      "grad_norm": 0.32458555698394775,
      "learning_rate": 2.0439583333333333e-05,
      "loss": 0.0021,
      "step": 141890
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.24389903247356415,
      "learning_rate": 2.0437500000000002e-05,
      "loss": 0.0015,
      "step": 141900
    },
    {
      "epoch": 4.730333333333333,
      "grad_norm": 0.15238343179225922,
      "learning_rate": 2.0435416666666668e-05,
      "loss": 0.0014,
      "step": 141910
    },
    {
      "epoch": 4.730666666666667,
      "grad_norm": 0.14313867688179016,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.002,
      "step": 141920
    },
    {
      "epoch": 4.731,
      "grad_norm": 0.1152990534901619,
      "learning_rate": 2.0431250000000002e-05,
      "loss": 0.0018,
      "step": 141930
    },
    {
      "epoch": 4.731333333333334,
      "grad_norm": 0.2597387135028839,
      "learning_rate": 2.0429166666666667e-05,
      "loss": 0.0016,
      "step": 141940
    },
    {
      "epoch": 4.7316666666666665,
      "grad_norm": 0.31097105145454407,
      "learning_rate": 2.0427083333333336e-05,
      "loss": 0.0024,
      "step": 141950
    },
    {
      "epoch": 4.732,
      "grad_norm": 0.20099517703056335,
      "learning_rate": 2.0425e-05,
      "loss": 0.002,
      "step": 141960
    },
    {
      "epoch": 4.732333333333333,
      "grad_norm": 0.4960283935070038,
      "learning_rate": 2.0422916666666667e-05,
      "loss": 0.0026,
      "step": 141970
    },
    {
      "epoch": 4.732666666666667,
      "grad_norm": 0.4816019833087921,
      "learning_rate": 2.0420833333333333e-05,
      "loss": 0.0023,
      "step": 141980
    },
    {
      "epoch": 4.733,
      "grad_norm": 0.2281516194343567,
      "learning_rate": 2.041875e-05,
      "loss": 0.002,
      "step": 141990
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.058095213025808334,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.0012,
      "step": 142000
    },
    {
      "epoch": 4.733666666666666,
      "grad_norm": 0.1716040074825287,
      "learning_rate": 2.0414583333333333e-05,
      "loss": 0.0016,
      "step": 142010
    },
    {
      "epoch": 4.734,
      "grad_norm": 0.2855992615222931,
      "learning_rate": 2.04125e-05,
      "loss": 0.0017,
      "step": 142020
    },
    {
      "epoch": 4.734333333333334,
      "grad_norm": 0.25704655051231384,
      "learning_rate": 2.0410416666666667e-05,
      "loss": 0.0017,
      "step": 142030
    },
    {
      "epoch": 4.734666666666667,
      "grad_norm": 0.14346864819526672,
      "learning_rate": 2.0408333333333336e-05,
      "loss": 0.0026,
      "step": 142040
    },
    {
      "epoch": 4.735,
      "grad_norm": 0.1145515888929367,
      "learning_rate": 2.040625e-05,
      "loss": 0.0016,
      "step": 142050
    },
    {
      "epoch": 4.735333333333333,
      "grad_norm": 0.3433840572834015,
      "learning_rate": 2.040416666666667e-05,
      "loss": 0.0025,
      "step": 142060
    },
    {
      "epoch": 4.735666666666667,
      "grad_norm": 0.08557885885238647,
      "learning_rate": 2.0402083333333336e-05,
      "loss": 0.0017,
      "step": 142070
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.010997367091476917,
      "learning_rate": 2.04e-05,
      "loss": 0.0034,
      "step": 142080
    },
    {
      "epoch": 4.7363333333333335,
      "grad_norm": 0.057528797537088394,
      "learning_rate": 2.0397916666666667e-05,
      "loss": 0.0015,
      "step": 142090
    },
    {
      "epoch": 4.736666666666666,
      "grad_norm": 0.14367979764938354,
      "learning_rate": 2.0395833333333332e-05,
      "loss": 0.0015,
      "step": 142100
    },
    {
      "epoch": 4.737,
      "grad_norm": 0.28582078218460083,
      "learning_rate": 2.039375e-05,
      "loss": 0.0013,
      "step": 142110
    },
    {
      "epoch": 4.737333333333333,
      "grad_norm": 0.48555874824523926,
      "learning_rate": 2.0391666666666667e-05,
      "loss": 0.0017,
      "step": 142120
    },
    {
      "epoch": 4.737666666666667,
      "grad_norm": 0.11440793424844742,
      "learning_rate": 2.0389583333333335e-05,
      "loss": 0.0011,
      "step": 142130
    },
    {
      "epoch": 4.7379999999999995,
      "grad_norm": 0.05986538901925087,
      "learning_rate": 2.03875e-05,
      "loss": 0.0014,
      "step": 142140
    },
    {
      "epoch": 4.738333333333333,
      "grad_norm": 0.13649111986160278,
      "learning_rate": 2.0385416666666666e-05,
      "loss": 0.0016,
      "step": 142150
    },
    {
      "epoch": 4.738666666666667,
      "grad_norm": 0.057207632809877396,
      "learning_rate": 2.0383333333333335e-05,
      "loss": 0.0018,
      "step": 142160
    },
    {
      "epoch": 4.739,
      "grad_norm": 0.05773535743355751,
      "learning_rate": 2.038125e-05,
      "loss": 0.0016,
      "step": 142170
    },
    {
      "epoch": 4.739333333333334,
      "grad_norm": 0.4718567132949829,
      "learning_rate": 2.037916666666667e-05,
      "loss": 0.0022,
      "step": 142180
    },
    {
      "epoch": 4.7396666666666665,
      "grad_norm": 0.031128956004977226,
      "learning_rate": 2.0377083333333335e-05,
      "loss": 0.0017,
      "step": 142190
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.006373880431056023,
      "learning_rate": 2.0375e-05,
      "loss": 0.0024,
      "step": 142200
    },
    {
      "epoch": 4.740333333333333,
      "grad_norm": 0.11457055807113647,
      "learning_rate": 2.0372916666666666e-05,
      "loss": 0.0017,
      "step": 142210
    },
    {
      "epoch": 4.740666666666667,
      "grad_norm": 0.17186875641345978,
      "learning_rate": 2.0370833333333335e-05,
      "loss": 0.0015,
      "step": 142220
    },
    {
      "epoch": 4.741,
      "grad_norm": 0.4728055000305176,
      "learning_rate": 2.036875e-05,
      "loss": 0.0025,
      "step": 142230
    },
    {
      "epoch": 4.741333333333333,
      "grad_norm": 0.128483846783638,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0022,
      "step": 142240
    },
    {
      "epoch": 4.741666666666667,
      "grad_norm": 0.2287903130054474,
      "learning_rate": 2.0364583333333335e-05,
      "loss": 0.0023,
      "step": 142250
    },
    {
      "epoch": 4.742,
      "grad_norm": 0.42938682436943054,
      "learning_rate": 2.03625e-05,
      "loss": 0.0018,
      "step": 142260
    },
    {
      "epoch": 4.742333333333333,
      "grad_norm": 0.029955299571156502,
      "learning_rate": 2.036041666666667e-05,
      "loss": 0.0014,
      "step": 142270
    },
    {
      "epoch": 4.742666666666667,
      "grad_norm": 0.3430011570453644,
      "learning_rate": 2.0358333333333335e-05,
      "loss": 0.0016,
      "step": 142280
    },
    {
      "epoch": 4.743,
      "grad_norm": 0.03155339136719704,
      "learning_rate": 2.035625e-05,
      "loss": 0.0018,
      "step": 142290
    },
    {
      "epoch": 4.743333333333333,
      "grad_norm": 0.1449955403804779,
      "learning_rate": 2.035416666666667e-05,
      "loss": 0.0021,
      "step": 142300
    },
    {
      "epoch": 4.743666666666667,
      "grad_norm": 0.1157321184873581,
      "learning_rate": 2.0352083333333334e-05,
      "loss": 0.0018,
      "step": 142310
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.22874918580055237,
      "learning_rate": 2.035e-05,
      "loss": 0.0014,
      "step": 142320
    },
    {
      "epoch": 4.7443333333333335,
      "grad_norm": 0.25738829374313354,
      "learning_rate": 2.0347916666666665e-05,
      "loss": 0.0016,
      "step": 142330
    },
    {
      "epoch": 4.744666666666666,
      "grad_norm": 0.029161561280488968,
      "learning_rate": 2.0345833333333334e-05,
      "loss": 0.0019,
      "step": 142340
    },
    {
      "epoch": 4.745,
      "grad_norm": 0.032933056354522705,
      "learning_rate": 2.034375e-05,
      "loss": 0.0017,
      "step": 142350
    },
    {
      "epoch": 4.745333333333333,
      "grad_norm": 0.11530027538537979,
      "learning_rate": 2.034166666666667e-05,
      "loss": 0.0015,
      "step": 142360
    },
    {
      "epoch": 4.745666666666667,
      "grad_norm": 0.11477909237146378,
      "learning_rate": 2.0339583333333334e-05,
      "loss": 0.0021,
      "step": 142370
    },
    {
      "epoch": 4.746,
      "grad_norm": 0.05945117026567459,
      "learning_rate": 2.03375e-05,
      "loss": 0.0029,
      "step": 142380
    },
    {
      "epoch": 4.746333333333333,
      "grad_norm": 0.17278990149497986,
      "learning_rate": 2.033541666666667e-05,
      "loss": 0.0019,
      "step": 142390
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.12296360731124878,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0017,
      "step": 142400
    },
    {
      "epoch": 4.747,
      "grad_norm": 0.3524530827999115,
      "learning_rate": 2.0331250000000003e-05,
      "loss": 0.0014,
      "step": 142410
    },
    {
      "epoch": 4.747333333333334,
      "grad_norm": 0.029735291376709938,
      "learning_rate": 2.032916666666667e-05,
      "loss": 0.0014,
      "step": 142420
    },
    {
      "epoch": 4.7476666666666665,
      "grad_norm": 0.08582005649805069,
      "learning_rate": 2.0327083333333334e-05,
      "loss": 0.0016,
      "step": 142430
    },
    {
      "epoch": 4.748,
      "grad_norm": 0.2002108097076416,
      "learning_rate": 2.0325e-05,
      "loss": 0.0021,
      "step": 142440
    },
    {
      "epoch": 4.748333333333333,
      "grad_norm": 0.08609796315431595,
      "learning_rate": 2.0322916666666665e-05,
      "loss": 0.0017,
      "step": 142450
    },
    {
      "epoch": 4.748666666666667,
      "grad_norm": 0.20028631389141083,
      "learning_rate": 2.0320833333333334e-05,
      "loss": 0.0025,
      "step": 142460
    },
    {
      "epoch": 4.749,
      "grad_norm": 0.1434832662343979,
      "learning_rate": 2.031875e-05,
      "loss": 0.0029,
      "step": 142470
    },
    {
      "epoch": 4.749333333333333,
      "grad_norm": 0.08643465489149094,
      "learning_rate": 2.0316666666666668e-05,
      "loss": 0.0017,
      "step": 142480
    },
    {
      "epoch": 4.749666666666666,
      "grad_norm": 0.3428553342819214,
      "learning_rate": 2.0314583333333334e-05,
      "loss": 0.002,
      "step": 142490
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.17133018374443054,
      "learning_rate": 2.0312500000000002e-05,
      "loss": 0.0016,
      "step": 142500
    },
    {
      "epoch": 4.750333333333334,
      "grad_norm": 0.08629366010427475,
      "learning_rate": 2.0310416666666668e-05,
      "loss": 0.002,
      "step": 142510
    },
    {
      "epoch": 4.750666666666667,
      "grad_norm": 0.31407204270362854,
      "learning_rate": 2.0308333333333333e-05,
      "loss": 0.0027,
      "step": 142520
    },
    {
      "epoch": 4.751,
      "grad_norm": 0.05793742462992668,
      "learning_rate": 2.0306250000000002e-05,
      "loss": 0.0017,
      "step": 142530
    },
    {
      "epoch": 4.751333333333333,
      "grad_norm": 0.08603191375732422,
      "learning_rate": 2.0304166666666668e-05,
      "loss": 0.0013,
      "step": 142540
    },
    {
      "epoch": 4.751666666666667,
      "grad_norm": 0.3147127032279968,
      "learning_rate": 2.0302083333333337e-05,
      "loss": 0.0015,
      "step": 142550
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.02907615154981613,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0021,
      "step": 142560
    },
    {
      "epoch": 4.7523333333333335,
      "grad_norm": 0.09240370243787766,
      "learning_rate": 2.0297916666666668e-05,
      "loss": 0.0016,
      "step": 142570
    },
    {
      "epoch": 4.752666666666666,
      "grad_norm": 0.08643534034490585,
      "learning_rate": 2.0295833333333333e-05,
      "loss": 0.0029,
      "step": 142580
    },
    {
      "epoch": 4.753,
      "grad_norm": 0.05739334225654602,
      "learning_rate": 2.029375e-05,
      "loss": 0.0013,
      "step": 142590
    },
    {
      "epoch": 4.753333333333333,
      "grad_norm": 0.03203313797712326,
      "learning_rate": 2.0291666666666667e-05,
      "loss": 0.0015,
      "step": 142600
    },
    {
      "epoch": 4.753666666666667,
      "grad_norm": 0.11512899398803711,
      "learning_rate": 2.0289583333333333e-05,
      "loss": 0.0025,
      "step": 142610
    },
    {
      "epoch": 4.754,
      "grad_norm": 0.33930331468582153,
      "learning_rate": 2.0287500000000002e-05,
      "loss": 0.002,
      "step": 142620
    },
    {
      "epoch": 4.754333333333333,
      "grad_norm": 0.20000194013118744,
      "learning_rate": 2.0285416666666667e-05,
      "loss": 0.0015,
      "step": 142630
    },
    {
      "epoch": 4.754666666666667,
      "grad_norm": 0.19986893236637115,
      "learning_rate": 2.0283333333333336e-05,
      "loss": 0.0011,
      "step": 142640
    },
    {
      "epoch": 4.755,
      "grad_norm": 0.31434303522109985,
      "learning_rate": 2.0281250000000002e-05,
      "loss": 0.0017,
      "step": 142650
    },
    {
      "epoch": 4.755333333333334,
      "grad_norm": 0.11730273813009262,
      "learning_rate": 2.0279166666666667e-05,
      "loss": 0.0017,
      "step": 142660
    },
    {
      "epoch": 4.7556666666666665,
      "grad_norm": 0.4285911023616791,
      "learning_rate": 2.0277083333333336e-05,
      "loss": 0.0012,
      "step": 142670
    },
    {
      "epoch": 4.756,
      "grad_norm": 0.07061869651079178,
      "learning_rate": 2.0275e-05,
      "loss": 0.002,
      "step": 142680
    },
    {
      "epoch": 4.756333333333333,
      "grad_norm": 0.3432130217552185,
      "learning_rate": 2.0272916666666667e-05,
      "loss": 0.0013,
      "step": 142690
    },
    {
      "epoch": 4.756666666666667,
      "grad_norm": 0.22116586565971375,
      "learning_rate": 2.0270833333333333e-05,
      "loss": 0.002,
      "step": 142700
    },
    {
      "epoch": 4.757,
      "grad_norm": 0.22821681201457977,
      "learning_rate": 2.026875e-05,
      "loss": 0.0026,
      "step": 142710
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.591366171836853,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0021,
      "step": 142720
    },
    {
      "epoch": 4.757666666666667,
      "grad_norm": 0.4590524137020111,
      "learning_rate": 2.0264583333333332e-05,
      "loss": 0.0018,
      "step": 142730
    },
    {
      "epoch": 4.758,
      "grad_norm": 0.11462786793708801,
      "learning_rate": 2.02625e-05,
      "loss": 0.0014,
      "step": 142740
    },
    {
      "epoch": 4.758333333333333,
      "grad_norm": 0.2572689652442932,
      "learning_rate": 2.0260416666666667e-05,
      "loss": 0.0014,
      "step": 142750
    },
    {
      "epoch": 4.758666666666667,
      "grad_norm": 0.2607627809047699,
      "learning_rate": 2.0258333333333336e-05,
      "loss": 0.001,
      "step": 142760
    },
    {
      "epoch": 4.759,
      "grad_norm": 0.03014732524752617,
      "learning_rate": 2.025625e-05,
      "loss": 0.0021,
      "step": 142770
    },
    {
      "epoch": 4.759333333333333,
      "grad_norm": 0.05854378640651703,
      "learning_rate": 2.025416666666667e-05,
      "loss": 0.0021,
      "step": 142780
    },
    {
      "epoch": 4.759666666666667,
      "grad_norm": 0.13426999747753143,
      "learning_rate": 2.0252083333333335e-05,
      "loss": 0.002,
      "step": 142790
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.22887398302555084,
      "learning_rate": 2.025e-05,
      "loss": 0.0015,
      "step": 142800
    },
    {
      "epoch": 4.7603333333333335,
      "grad_norm": 0.05745067074894905,
      "learning_rate": 2.0247916666666666e-05,
      "loss": 0.0024,
      "step": 142810
    },
    {
      "epoch": 4.760666666666666,
      "grad_norm": 0.40043407678604126,
      "learning_rate": 2.0245833333333332e-05,
      "loss": 0.0026,
      "step": 142820
    },
    {
      "epoch": 4.761,
      "grad_norm": 0.02941165305674076,
      "learning_rate": 2.024375e-05,
      "loss": 0.0014,
      "step": 142830
    },
    {
      "epoch": 4.761333333333333,
      "grad_norm": 0.17237354815006256,
      "learning_rate": 2.0241666666666666e-05,
      "loss": 0.002,
      "step": 142840
    },
    {
      "epoch": 4.761666666666667,
      "grad_norm": 0.2729288935661316,
      "learning_rate": 2.0239583333333335e-05,
      "loss": 0.0019,
      "step": 142850
    },
    {
      "epoch": 4.7620000000000005,
      "grad_norm": 0.030143853276968002,
      "learning_rate": 2.02375e-05,
      "loss": 0.0021,
      "step": 142860
    },
    {
      "epoch": 4.762333333333333,
      "grad_norm": 0.25259217619895935,
      "learning_rate": 2.0235416666666666e-05,
      "loss": 0.0027,
      "step": 142870
    },
    {
      "epoch": 4.762666666666667,
      "grad_norm": 0.030204249545931816,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.0018,
      "step": 142880
    },
    {
      "epoch": 4.763,
      "grad_norm": 0.23052026331424713,
      "learning_rate": 2.023125e-05,
      "loss": 0.0016,
      "step": 142890
    },
    {
      "epoch": 4.763333333333334,
      "grad_norm": 0.031087208539247513,
      "learning_rate": 2.022916666666667e-05,
      "loss": 0.0013,
      "step": 142900
    },
    {
      "epoch": 4.7636666666666665,
      "grad_norm": 0.11460191756486893,
      "learning_rate": 2.0227083333333335e-05,
      "loss": 0.0015,
      "step": 142910
    },
    {
      "epoch": 4.764,
      "grad_norm": 0.03237400949001312,
      "learning_rate": 2.0225000000000004e-05,
      "loss": 0.0025,
      "step": 142920
    },
    {
      "epoch": 4.764333333333333,
      "grad_norm": 0.3065699338912964,
      "learning_rate": 2.0222916666666666e-05,
      "loss": 0.0023,
      "step": 142930
    },
    {
      "epoch": 4.764666666666667,
      "grad_norm": 0.18171696364879608,
      "learning_rate": 2.0220833333333335e-05,
      "loss": 0.0021,
      "step": 142940
    },
    {
      "epoch": 4.765,
      "grad_norm": 0.12459689378738403,
      "learning_rate": 2.021875e-05,
      "loss": 0.0019,
      "step": 142950
    },
    {
      "epoch": 4.765333333333333,
      "grad_norm": 0.1858067661523819,
      "learning_rate": 2.0216666666666666e-05,
      "loss": 0.0022,
      "step": 142960
    },
    {
      "epoch": 4.765666666666666,
      "grad_norm": 0.2286146730184555,
      "learning_rate": 2.0214583333333335e-05,
      "loss": 0.0019,
      "step": 142970
    },
    {
      "epoch": 4.766,
      "grad_norm": 0.22857984900474548,
      "learning_rate": 2.02125e-05,
      "loss": 0.0015,
      "step": 142980
    },
    {
      "epoch": 4.766333333333334,
      "grad_norm": 0.2572784125804901,
      "learning_rate": 2.021041666666667e-05,
      "loss": 0.0033,
      "step": 142990
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 0.4166118800640106,
      "learning_rate": 2.0208333333333334e-05,
      "loss": 0.0022,
      "step": 143000
    },
    {
      "epoch": 4.767,
      "grad_norm": 0.25747057795524597,
      "learning_rate": 2.0206250000000003e-05,
      "loss": 0.0023,
      "step": 143010
    },
    {
      "epoch": 4.767333333333333,
      "grad_norm": 0.20053955912590027,
      "learning_rate": 2.020416666666667e-05,
      "loss": 0.0035,
      "step": 143020
    },
    {
      "epoch": 4.767666666666667,
      "grad_norm": 0.4649919271469116,
      "learning_rate": 2.0202083333333334e-05,
      "loss": 0.0018,
      "step": 143030
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.278383731842041,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0024,
      "step": 143040
    },
    {
      "epoch": 4.7683333333333335,
      "grad_norm": 0.49442771077156067,
      "learning_rate": 2.0197916666666665e-05,
      "loss": 0.0017,
      "step": 143050
    },
    {
      "epoch": 4.768666666666666,
      "grad_norm": 0.11435599625110626,
      "learning_rate": 2.0195833333333334e-05,
      "loss": 0.0018,
      "step": 143060
    },
    {
      "epoch": 4.769,
      "grad_norm": 0.31432560086250305,
      "learning_rate": 2.019375e-05,
      "loss": 0.0019,
      "step": 143070
    },
    {
      "epoch": 4.769333333333333,
      "grad_norm": 0.14282016456127167,
      "learning_rate": 2.019166666666667e-05,
      "loss": 0.0017,
      "step": 143080
    },
    {
      "epoch": 4.769666666666667,
      "grad_norm": 0.14329470694065094,
      "learning_rate": 2.0189583333333334e-05,
      "loss": 0.0017,
      "step": 143090
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.14353695511817932,
      "learning_rate": 2.01875e-05,
      "loss": 0.0017,
      "step": 143100
    },
    {
      "epoch": 4.770333333333333,
      "grad_norm": 0.19488543272018433,
      "learning_rate": 2.0185416666666668e-05,
      "loss": 0.002,
      "step": 143110
    },
    {
      "epoch": 4.770666666666667,
      "grad_norm": 0.17334522306919098,
      "learning_rate": 2.0183333333333334e-05,
      "loss": 0.0025,
      "step": 143120
    },
    {
      "epoch": 4.771,
      "grad_norm": 0.6016767621040344,
      "learning_rate": 2.0181250000000003e-05,
      "loss": 0.0017,
      "step": 143130
    },
    {
      "epoch": 4.771333333333334,
      "grad_norm": 0.34365659952163696,
      "learning_rate": 2.0179166666666668e-05,
      "loss": 0.0021,
      "step": 143140
    },
    {
      "epoch": 4.7716666666666665,
      "grad_norm": 0.17064830660820007,
      "learning_rate": 2.0177083333333337e-05,
      "loss": 0.0022,
      "step": 143150
    },
    {
      "epoch": 4.772,
      "grad_norm": 0.14263929426670074,
      "learning_rate": 2.0175000000000003e-05,
      "loss": 0.0023,
      "step": 143160
    },
    {
      "epoch": 4.772333333333333,
      "grad_norm": 0.08595110476016998,
      "learning_rate": 2.0172916666666668e-05,
      "loss": 0.002,
      "step": 143170
    },
    {
      "epoch": 4.772666666666667,
      "grad_norm": 0.011933764442801476,
      "learning_rate": 2.0170833333333333e-05,
      "loss": 0.0019,
      "step": 143180
    },
    {
      "epoch": 4.773,
      "grad_norm": 0.004565714858472347,
      "learning_rate": 2.016875e-05,
      "loss": 0.002,
      "step": 143190
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.6060150861740112,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0013,
      "step": 143200
    },
    {
      "epoch": 4.773666666666666,
      "grad_norm": 0.11436997354030609,
      "learning_rate": 2.0164583333333333e-05,
      "loss": 0.0021,
      "step": 143210
    },
    {
      "epoch": 4.774,
      "grad_norm": 0.20048266649246216,
      "learning_rate": 2.0162500000000002e-05,
      "loss": 0.0021,
      "step": 143220
    },
    {
      "epoch": 4.774333333333333,
      "grad_norm": 0.3150334060192108,
      "learning_rate": 2.0160416666666668e-05,
      "loss": 0.0023,
      "step": 143230
    },
    {
      "epoch": 4.774666666666667,
      "grad_norm": 0.17161981761455536,
      "learning_rate": 2.0158333333333333e-05,
      "loss": 0.0023,
      "step": 143240
    },
    {
      "epoch": 4.775,
      "grad_norm": 0.08645281195640564,
      "learning_rate": 2.0156250000000002e-05,
      "loss": 0.0016,
      "step": 143250
    },
    {
      "epoch": 4.775333333333333,
      "grad_norm": 0.029372965916991234,
      "learning_rate": 2.0154166666666668e-05,
      "loss": 0.0025,
      "step": 143260
    },
    {
      "epoch": 4.775666666666667,
      "grad_norm": 0.11459449678659439,
      "learning_rate": 2.0152083333333336e-05,
      "loss": 0.0021,
      "step": 143270
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.22883173823356628,
      "learning_rate": 2.0150000000000002e-05,
      "loss": 0.0015,
      "step": 143280
    },
    {
      "epoch": 4.7763333333333335,
      "grad_norm": 0.22938689589500427,
      "learning_rate": 2.0147916666666667e-05,
      "loss": 0.0024,
      "step": 143290
    },
    {
      "epoch": 4.776666666666666,
      "grad_norm": 0.006383499596267939,
      "learning_rate": 2.0145833333333333e-05,
      "loss": 0.0026,
      "step": 143300
    },
    {
      "epoch": 4.777,
      "grad_norm": 0.05925055965781212,
      "learning_rate": 2.0143750000000002e-05,
      "loss": 0.0016,
      "step": 143310
    },
    {
      "epoch": 4.777333333333333,
      "grad_norm": 0.03175244480371475,
      "learning_rate": 2.0141666666666667e-05,
      "loss": 0.0017,
      "step": 143320
    },
    {
      "epoch": 4.777666666666667,
      "grad_norm": 0.08732166141271591,
      "learning_rate": 2.0139583333333333e-05,
      "loss": 0.0016,
      "step": 143330
    },
    {
      "epoch": 4.7780000000000005,
      "grad_norm": 0.27353227138519287,
      "learning_rate": 2.01375e-05,
      "loss": 0.0022,
      "step": 143340
    },
    {
      "epoch": 4.778333333333333,
      "grad_norm": 0.09058529883623123,
      "learning_rate": 2.0135416666666667e-05,
      "loss": 0.0018,
      "step": 143350
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.5030078291893005,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0019,
      "step": 143360
    },
    {
      "epoch": 4.779,
      "grad_norm": 0.11445604264736176,
      "learning_rate": 2.013125e-05,
      "loss": 0.0019,
      "step": 143370
    },
    {
      "epoch": 4.779333333333334,
      "grad_norm": 0.08644264191389084,
      "learning_rate": 2.0129166666666667e-05,
      "loss": 0.002,
      "step": 143380
    },
    {
      "epoch": 4.7796666666666665,
      "grad_norm": 0.1715587079524994,
      "learning_rate": 2.0127083333333336e-05,
      "loss": 0.0017,
      "step": 143390
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.059214476495981216,
      "learning_rate": 2.0125e-05,
      "loss": 0.0019,
      "step": 143400
    },
    {
      "epoch": 4.780333333333333,
      "grad_norm": 0.29276683926582336,
      "learning_rate": 2.0122916666666667e-05,
      "loss": 0.0025,
      "step": 143410
    },
    {
      "epoch": 4.780666666666667,
      "grad_norm": 0.01029240619391203,
      "learning_rate": 2.0120833333333332e-05,
      "loss": 0.0014,
      "step": 143420
    },
    {
      "epoch": 4.781,
      "grad_norm": 0.00692260405048728,
      "learning_rate": 2.011875e-05,
      "loss": 0.0026,
      "step": 143430
    },
    {
      "epoch": 4.781333333333333,
      "grad_norm": 0.029840022325515747,
      "learning_rate": 2.0116666666666667e-05,
      "loss": 0.0018,
      "step": 143440
    },
    {
      "epoch": 4.781666666666666,
      "grad_norm": 0.39684543013572693,
      "learning_rate": 2.0114583333333335e-05,
      "loss": 0.0015,
      "step": 143450
    },
    {
      "epoch": 4.782,
      "grad_norm": 0.061760690063238144,
      "learning_rate": 2.01125e-05,
      "loss": 0.0027,
      "step": 143460
    },
    {
      "epoch": 4.782333333333334,
      "grad_norm": 0.37188810110092163,
      "learning_rate": 2.0110416666666666e-05,
      "loss": 0.0021,
      "step": 143470
    },
    {
      "epoch": 4.782666666666667,
      "grad_norm": 0.5816333889961243,
      "learning_rate": 2.0108333333333335e-05,
      "loss": 0.0023,
      "step": 143480
    },
    {
      "epoch": 4.783,
      "grad_norm": 0.39067205786705017,
      "learning_rate": 2.010625e-05,
      "loss": 0.0016,
      "step": 143490
    },
    {
      "epoch": 4.783333333333333,
      "grad_norm": 0.14302264153957367,
      "learning_rate": 2.010416666666667e-05,
      "loss": 0.0022,
      "step": 143500
    },
    {
      "epoch": 4.783666666666667,
      "grad_norm": 0.11498565971851349,
      "learning_rate": 2.0102083333333335e-05,
      "loss": 0.0018,
      "step": 143510
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.009406929835677147,
      "learning_rate": 2.01e-05,
      "loss": 0.0019,
      "step": 143520
    },
    {
      "epoch": 4.7843333333333335,
      "grad_norm": 0.05755415931344032,
      "learning_rate": 2.0097916666666666e-05,
      "loss": 0.0016,
      "step": 143530
    },
    {
      "epoch": 4.784666666666666,
      "grad_norm": 0.6029982566833496,
      "learning_rate": 2.009583333333333e-05,
      "loss": 0.0019,
      "step": 143540
    },
    {
      "epoch": 4.785,
      "grad_norm": 0.057454489171504974,
      "learning_rate": 2.009375e-05,
      "loss": 0.0014,
      "step": 143550
    },
    {
      "epoch": 4.785333333333333,
      "grad_norm": 0.030366038903594017,
      "learning_rate": 2.0091666666666666e-05,
      "loss": 0.0014,
      "step": 143560
    },
    {
      "epoch": 4.785666666666667,
      "grad_norm": 0.1152474656701088,
      "learning_rate": 2.0089583333333335e-05,
      "loss": 0.0023,
      "step": 143570
    },
    {
      "epoch": 4.786,
      "grad_norm": 0.1717451810836792,
      "learning_rate": 2.00875e-05,
      "loss": 0.0018,
      "step": 143580
    },
    {
      "epoch": 4.786333333333333,
      "grad_norm": 0.11451476067304611,
      "learning_rate": 2.008541666666667e-05,
      "loss": 0.0017,
      "step": 143590
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.35770368576049805,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.0022,
      "step": 143600
    },
    {
      "epoch": 4.787,
      "grad_norm": 0.0861564353108406,
      "learning_rate": 2.008125e-05,
      "loss": 0.0021,
      "step": 143610
    },
    {
      "epoch": 4.787333333333334,
      "grad_norm": 0.08570899069309235,
      "learning_rate": 2.007916666666667e-05,
      "loss": 0.0022,
      "step": 143620
    },
    {
      "epoch": 4.7876666666666665,
      "grad_norm": 0.11466238647699356,
      "learning_rate": 2.0077083333333335e-05,
      "loss": 0.0022,
      "step": 143630
    },
    {
      "epoch": 4.788,
      "grad_norm": 0.030974814668297768,
      "learning_rate": 2.0075000000000003e-05,
      "loss": 0.002,
      "step": 143640
    },
    {
      "epoch": 4.788333333333333,
      "grad_norm": 0.28558778762817383,
      "learning_rate": 2.0072916666666666e-05,
      "loss": 0.0021,
      "step": 143650
    },
    {
      "epoch": 4.788666666666667,
      "grad_norm": 0.557978630065918,
      "learning_rate": 2.0070833333333334e-05,
      "loss": 0.0019,
      "step": 143660
    },
    {
      "epoch": 4.789,
      "grad_norm": 0.3714372515678406,
      "learning_rate": 2.006875e-05,
      "loss": 0.0015,
      "step": 143670
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 0.15871594846248627,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0027,
      "step": 143680
    },
    {
      "epoch": 4.789666666666666,
      "grad_norm": 0.34318751096725464,
      "learning_rate": 2.0064583333333334e-05,
      "loss": 0.0013,
      "step": 143690
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.17256926000118256,
      "learning_rate": 2.00625e-05,
      "loss": 0.0014,
      "step": 143700
    },
    {
      "epoch": 4.790333333333333,
      "grad_norm": 0.05881946533918381,
      "learning_rate": 2.006041666666667e-05,
      "loss": 0.0012,
      "step": 143710
    },
    {
      "epoch": 4.790666666666667,
      "grad_norm": 0.3143160939216614,
      "learning_rate": 2.0058333333333334e-05,
      "loss": 0.0015,
      "step": 143720
    },
    {
      "epoch": 4.791,
      "grad_norm": 0.057384513318538666,
      "learning_rate": 2.0056250000000003e-05,
      "loss": 0.0017,
      "step": 143730
    },
    {
      "epoch": 4.791333333333333,
      "grad_norm": 0.4002087712287903,
      "learning_rate": 2.005416666666667e-05,
      "loss": 0.0017,
      "step": 143740
    },
    {
      "epoch": 4.791666666666667,
      "grad_norm": 0.17113834619522095,
      "learning_rate": 2.0052083333333334e-05,
      "loss": 0.0013,
      "step": 143750
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.11606443673372269,
      "learning_rate": 2.0050000000000003e-05,
      "loss": 0.0013,
      "step": 143760
    },
    {
      "epoch": 4.792333333333334,
      "grad_norm": 0.08673559874296188,
      "learning_rate": 2.004791666666667e-05,
      "loss": 0.0024,
      "step": 143770
    },
    {
      "epoch": 4.792666666666666,
      "grad_norm": 0.08680862188339233,
      "learning_rate": 2.0045833333333334e-05,
      "loss": 0.0017,
      "step": 143780
    },
    {
      "epoch": 4.793,
      "grad_norm": 0.20014849305152893,
      "learning_rate": 2.004375e-05,
      "loss": 0.0015,
      "step": 143790
    },
    {
      "epoch": 4.793333333333333,
      "grad_norm": 0.22874630987644196,
      "learning_rate": 2.0041666666666668e-05,
      "loss": 0.0015,
      "step": 143800
    },
    {
      "epoch": 4.793666666666667,
      "grad_norm": 0.05705063417553902,
      "learning_rate": 2.0039583333333334e-05,
      "loss": 0.0021,
      "step": 143810
    },
    {
      "epoch": 4.7940000000000005,
      "grad_norm": 0.0861998125910759,
      "learning_rate": 2.00375e-05,
      "loss": 0.0019,
      "step": 143820
    },
    {
      "epoch": 4.794333333333333,
      "grad_norm": 0.22916744649410248,
      "learning_rate": 2.0035416666666668e-05,
      "loss": 0.0026,
      "step": 143830
    },
    {
      "epoch": 4.794666666666666,
      "grad_norm": 0.20011277496814728,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0023,
      "step": 143840
    },
    {
      "epoch": 4.795,
      "grad_norm": 0.3716978132724762,
      "learning_rate": 2.0031250000000002e-05,
      "loss": 0.002,
      "step": 143850
    },
    {
      "epoch": 4.795333333333334,
      "grad_norm": 0.1153862476348877,
      "learning_rate": 2.0029166666666668e-05,
      "loss": 0.0018,
      "step": 143860
    },
    {
      "epoch": 4.7956666666666665,
      "grad_norm": 0.05723046511411667,
      "learning_rate": 2.0027083333333337e-05,
      "loss": 0.0015,
      "step": 143870
    },
    {
      "epoch": 4.796,
      "grad_norm": 0.02916382998228073,
      "learning_rate": 2.0025000000000002e-05,
      "loss": 0.0021,
      "step": 143880
    },
    {
      "epoch": 4.796333333333333,
      "grad_norm": 0.05959748849272728,
      "learning_rate": 2.0022916666666668e-05,
      "loss": 0.0015,
      "step": 143890
    },
    {
      "epoch": 4.796666666666667,
      "grad_norm": 0.23112428188323975,
      "learning_rate": 2.0020833333333333e-05,
      "loss": 0.0019,
      "step": 143900
    },
    {
      "epoch": 4.797,
      "grad_norm": 0.3426952660083771,
      "learning_rate": 2.001875e-05,
      "loss": 0.0018,
      "step": 143910
    },
    {
      "epoch": 4.7973333333333334,
      "grad_norm": 0.1585492491722107,
      "learning_rate": 2.0016666666666668e-05,
      "loss": 0.0014,
      "step": 143920
    },
    {
      "epoch": 4.797666666666666,
      "grad_norm": 0.08591526746749878,
      "learning_rate": 2.0014583333333333e-05,
      "loss": 0.0017,
      "step": 143930
    },
    {
      "epoch": 4.798,
      "grad_norm": 0.11213502287864685,
      "learning_rate": 2.0012500000000002e-05,
      "loss": 0.0018,
      "step": 143940
    },
    {
      "epoch": 4.798333333333334,
      "grad_norm": 0.2289164513349533,
      "learning_rate": 2.0010416666666667e-05,
      "loss": 0.0024,
      "step": 143950
    },
    {
      "epoch": 4.798666666666667,
      "grad_norm": 0.3716995418071747,
      "learning_rate": 2.0008333333333333e-05,
      "loss": 0.0016,
      "step": 143960
    },
    {
      "epoch": 4.799,
      "grad_norm": 0.11437956243753433,
      "learning_rate": 2.0006250000000002e-05,
      "loss": 0.0015,
      "step": 143970
    },
    {
      "epoch": 4.799333333333333,
      "grad_norm": 0.11458610743284225,
      "learning_rate": 2.0004166666666667e-05,
      "loss": 0.0014,
      "step": 143980
    },
    {
      "epoch": 4.799666666666667,
      "grad_norm": 0.1716647446155548,
      "learning_rate": 2.0002083333333336e-05,
      "loss": 0.0015,
      "step": 143990
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.34306612610816956,
      "learning_rate": 2e-05,
      "loss": 0.0013,
      "step": 144000
    },
    {
      "epoch": 4.800333333333334,
      "grad_norm": 0.033554885536432266,
      "learning_rate": 1.999791666666667e-05,
      "loss": 0.0024,
      "step": 144010
    },
    {
      "epoch": 4.800666666666666,
      "grad_norm": 0.25749877095222473,
      "learning_rate": 1.9995833333333333e-05,
      "loss": 0.0018,
      "step": 144020
    },
    {
      "epoch": 4.801,
      "grad_norm": 0.007391499821096659,
      "learning_rate": 1.999375e-05,
      "loss": 0.0022,
      "step": 144030
    },
    {
      "epoch": 4.801333333333333,
      "grad_norm": 0.3908493220806122,
      "learning_rate": 1.9991666666666667e-05,
      "loss": 0.0025,
      "step": 144040
    },
    {
      "epoch": 4.801666666666667,
      "grad_norm": 0.37152692675590515,
      "learning_rate": 1.9989583333333332e-05,
      "loss": 0.0022,
      "step": 144050
    },
    {
      "epoch": 4.802,
      "grad_norm": 0.059433646500110626,
      "learning_rate": 1.99875e-05,
      "loss": 0.0016,
      "step": 144060
    },
    {
      "epoch": 4.802333333333333,
      "grad_norm": 0.4000560939311981,
      "learning_rate": 1.9985416666666667e-05,
      "loss": 0.0021,
      "step": 144070
    },
    {
      "epoch": 4.802666666666667,
      "grad_norm": 0.2857055962085724,
      "learning_rate": 1.9983333333333336e-05,
      "loss": 0.0015,
      "step": 144080
    },
    {
      "epoch": 4.803,
      "grad_norm": 0.2006169557571411,
      "learning_rate": 1.998125e-05,
      "loss": 0.0018,
      "step": 144090
    },
    {
      "epoch": 4.803333333333334,
      "grad_norm": 0.051692914217710495,
      "learning_rate": 1.9979166666666667e-05,
      "loss": 0.0014,
      "step": 144100
    },
    {
      "epoch": 4.8036666666666665,
      "grad_norm": 0.22884303331375122,
      "learning_rate": 1.9977083333333336e-05,
      "loss": 0.0027,
      "step": 144110
    },
    {
      "epoch": 4.804,
      "grad_norm": 0.2609642446041107,
      "learning_rate": 1.9975e-05,
      "loss": 0.0023,
      "step": 144120
    },
    {
      "epoch": 4.804333333333333,
      "grad_norm": 0.14304035902023315,
      "learning_rate": 1.997291666666667e-05,
      "loss": 0.0023,
      "step": 144130
    },
    {
      "epoch": 4.804666666666667,
      "grad_norm": 0.08598850667476654,
      "learning_rate": 1.9970833333333332e-05,
      "loss": 0.0017,
      "step": 144140
    },
    {
      "epoch": 4.805,
      "grad_norm": 0.0057246508076786995,
      "learning_rate": 1.996875e-05,
      "loss": 0.0015,
      "step": 144150
    },
    {
      "epoch": 4.8053333333333335,
      "grad_norm": 0.20018257200717926,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0017,
      "step": 144160
    },
    {
      "epoch": 4.805666666666666,
      "grad_norm": 0.1999330073595047,
      "learning_rate": 1.9964583333333335e-05,
      "loss": 0.0015,
      "step": 144170
    },
    {
      "epoch": 4.806,
      "grad_norm": 0.2855525314807892,
      "learning_rate": 1.99625e-05,
      "loss": 0.0013,
      "step": 144180
    },
    {
      "epoch": 4.806333333333333,
      "grad_norm": 0.2905574440956116,
      "learning_rate": 1.9960416666666666e-05,
      "loss": 0.0018,
      "step": 144190
    },
    {
      "epoch": 4.806666666666667,
      "grad_norm": 0.26648786664009094,
      "learning_rate": 1.9958333333333335e-05,
      "loss": 0.0032,
      "step": 144200
    },
    {
      "epoch": 4.807,
      "grad_norm": 0.2754739820957184,
      "learning_rate": 1.995625e-05,
      "loss": 0.0017,
      "step": 144210
    },
    {
      "epoch": 4.807333333333333,
      "grad_norm": 0.03331095352768898,
      "learning_rate": 1.995416666666667e-05,
      "loss": 0.0029,
      "step": 144220
    },
    {
      "epoch": 4.807666666666667,
      "grad_norm": 0.11476872116327286,
      "learning_rate": 1.9952083333333335e-05,
      "loss": 0.0016,
      "step": 144230
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.11471537500619888,
      "learning_rate": 1.995e-05,
      "loss": 0.0018,
      "step": 144240
    },
    {
      "epoch": 4.808333333333334,
      "grad_norm": 0.007953877560794353,
      "learning_rate": 1.994791666666667e-05,
      "loss": 0.0019,
      "step": 144250
    },
    {
      "epoch": 4.808666666666666,
      "grad_norm": 0.5149679183959961,
      "learning_rate": 1.994583333333333e-05,
      "loss": 0.0027,
      "step": 144260
    },
    {
      "epoch": 4.809,
      "grad_norm": 0.22994987666606903,
      "learning_rate": 1.994375e-05,
      "loss": 0.0017,
      "step": 144270
    },
    {
      "epoch": 4.809333333333333,
      "grad_norm": 0.502242386341095,
      "learning_rate": 1.9941666666666666e-05,
      "loss": 0.002,
      "step": 144280
    },
    {
      "epoch": 4.809666666666667,
      "grad_norm": 0.31493210792541504,
      "learning_rate": 1.9939583333333335e-05,
      "loss": 0.0015,
      "step": 144290
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 0.2290785014629364,
      "learning_rate": 1.99375e-05,
      "loss": 0.0012,
      "step": 144300
    },
    {
      "epoch": 4.810333333333333,
      "grad_norm": 0.1997007131576538,
      "learning_rate": 1.993541666666667e-05,
      "loss": 0.0016,
      "step": 144310
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.029319820925593376,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0024,
      "step": 144320
    },
    {
      "epoch": 4.811,
      "grad_norm": 0.0305593591183424,
      "learning_rate": 1.993125e-05,
      "loss": 0.0018,
      "step": 144330
    },
    {
      "epoch": 4.811333333333334,
      "grad_norm": 0.4281749725341797,
      "learning_rate": 1.992916666666667e-05,
      "loss": 0.002,
      "step": 144340
    },
    {
      "epoch": 4.8116666666666665,
      "grad_norm": 0.031952302902936935,
      "learning_rate": 1.9927083333333334e-05,
      "loss": 0.0027,
      "step": 144350
    },
    {
      "epoch": 4.812,
      "grad_norm": 0.22892236709594727,
      "learning_rate": 1.9925000000000003e-05,
      "loss": 0.0018,
      "step": 144360
    },
    {
      "epoch": 4.812333333333333,
      "grad_norm": 0.2638965845108032,
      "learning_rate": 1.992291666666667e-05,
      "loss": 0.0017,
      "step": 144370
    },
    {
      "epoch": 4.812666666666667,
      "grad_norm": 0.18007490038871765,
      "learning_rate": 1.9920833333333334e-05,
      "loss": 0.0016,
      "step": 144380
    },
    {
      "epoch": 4.813,
      "grad_norm": 0.08583058416843414,
      "learning_rate": 1.991875e-05,
      "loss": 0.0019,
      "step": 144390
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.20548734068870544,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.0015,
      "step": 144400
    },
    {
      "epoch": 4.813666666666666,
      "grad_norm": 0.4697771370410919,
      "learning_rate": 1.9914583333333334e-05,
      "loss": 0.0019,
      "step": 144410
    },
    {
      "epoch": 4.814,
      "grad_norm": 0.4283137023448944,
      "learning_rate": 1.99125e-05,
      "loss": 0.0017,
      "step": 144420
    },
    {
      "epoch": 4.814333333333334,
      "grad_norm": 0.6002416014671326,
      "learning_rate": 1.991041666666667e-05,
      "loss": 0.0015,
      "step": 144430
    },
    {
      "epoch": 4.814666666666667,
      "grad_norm": 0.18995442986488342,
      "learning_rate": 1.9908333333333334e-05,
      "loss": 0.0013,
      "step": 144440
    },
    {
      "epoch": 4.8149999999999995,
      "grad_norm": 0.11430355161428452,
      "learning_rate": 1.9906250000000003e-05,
      "loss": 0.0019,
      "step": 144450
    },
    {
      "epoch": 4.815333333333333,
      "grad_norm": 0.2858635187149048,
      "learning_rate": 1.9904166666666668e-05,
      "loss": 0.0025,
      "step": 144460
    },
    {
      "epoch": 4.815666666666667,
      "grad_norm": 0.14272695779800415,
      "learning_rate": 1.9902083333333334e-05,
      "loss": 0.0021,
      "step": 144470
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.17158928513526917,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0018,
      "step": 144480
    },
    {
      "epoch": 4.816333333333334,
      "grad_norm": 0.029401931911706924,
      "learning_rate": 1.9897916666666668e-05,
      "loss": 0.002,
      "step": 144490
    },
    {
      "epoch": 4.816666666666666,
      "grad_norm": 0.3148016929626465,
      "learning_rate": 1.9895833333333334e-05,
      "loss": 0.003,
      "step": 144500
    },
    {
      "epoch": 4.817,
      "grad_norm": 0.11457005143165588,
      "learning_rate": 1.989375e-05,
      "loss": 0.0011,
      "step": 144510
    },
    {
      "epoch": 4.817333333333333,
      "grad_norm": 0.11522433161735535,
      "learning_rate": 1.9891666666666668e-05,
      "loss": 0.0018,
      "step": 144520
    },
    {
      "epoch": 4.817666666666667,
      "grad_norm": 0.17298725247383118,
      "learning_rate": 1.9889583333333333e-05,
      "loss": 0.0018,
      "step": 144530
    },
    {
      "epoch": 4.818,
      "grad_norm": 0.08645856380462646,
      "learning_rate": 1.98875e-05,
      "loss": 0.0013,
      "step": 144540
    },
    {
      "epoch": 4.818333333333333,
      "grad_norm": 0.19988566637039185,
      "learning_rate": 1.9885416666666668e-05,
      "loss": 0.0021,
      "step": 144550
    },
    {
      "epoch": 4.818666666666667,
      "grad_norm": 0.08632493019104004,
      "learning_rate": 1.9883333333333333e-05,
      "loss": 0.0018,
      "step": 144560
    },
    {
      "epoch": 4.819,
      "grad_norm": 0.37156298756599426,
      "learning_rate": 1.9881250000000002e-05,
      "loss": 0.0021,
      "step": 144570
    },
    {
      "epoch": 4.819333333333334,
      "grad_norm": 0.3956312835216522,
      "learning_rate": 1.9879166666666668e-05,
      "loss": 0.0019,
      "step": 144580
    },
    {
      "epoch": 4.8196666666666665,
      "grad_norm": 0.37205323576927185,
      "learning_rate": 1.9877083333333336e-05,
      "loss": 0.0022,
      "step": 144590
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.11453820019960403,
      "learning_rate": 1.9875000000000002e-05,
      "loss": 0.0016,
      "step": 144600
    },
    {
      "epoch": 4.820333333333333,
      "grad_norm": 0.1145508661866188,
      "learning_rate": 1.9872916666666667e-05,
      "loss": 0.0023,
      "step": 144610
    },
    {
      "epoch": 4.820666666666667,
      "grad_norm": 0.44381794333457947,
      "learning_rate": 1.9870833333333333e-05,
      "loss": 0.0024,
      "step": 144620
    },
    {
      "epoch": 4.821,
      "grad_norm": 0.0078032249584794044,
      "learning_rate": 1.986875e-05,
      "loss": 0.0021,
      "step": 144630
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.28907856345176697,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0029,
      "step": 144640
    },
    {
      "epoch": 4.821666666666666,
      "grad_norm": 0.02963365614414215,
      "learning_rate": 1.9864583333333333e-05,
      "loss": 0.0026,
      "step": 144650
    },
    {
      "epoch": 4.822,
      "grad_norm": 0.11482319980859756,
      "learning_rate": 1.98625e-05,
      "loss": 0.0018,
      "step": 144660
    },
    {
      "epoch": 4.822333333333333,
      "grad_norm": 0.028711536899209023,
      "learning_rate": 1.9860416666666667e-05,
      "loss": 0.0017,
      "step": 144670
    },
    {
      "epoch": 4.822666666666667,
      "grad_norm": 0.05808582156896591,
      "learning_rate": 1.9858333333333333e-05,
      "loss": 0.0021,
      "step": 144680
    },
    {
      "epoch": 4.823,
      "grad_norm": 0.22867977619171143,
      "learning_rate": 1.985625e-05,
      "loss": 0.0016,
      "step": 144690
    },
    {
      "epoch": 4.823333333333333,
      "grad_norm": 0.1588389277458191,
      "learning_rate": 1.9854166666666667e-05,
      "loss": 0.0024,
      "step": 144700
    },
    {
      "epoch": 4.823666666666667,
      "grad_norm": 0.5432556867599487,
      "learning_rate": 1.9852083333333336e-05,
      "loss": 0.0015,
      "step": 144710
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.05744755640625954,
      "learning_rate": 1.985e-05,
      "loss": 0.003,
      "step": 144720
    },
    {
      "epoch": 4.824333333333334,
      "grad_norm": 0.2001458704471588,
      "learning_rate": 1.984791666666667e-05,
      "loss": 0.0034,
      "step": 144730
    },
    {
      "epoch": 4.824666666666666,
      "grad_norm": 0.11790530383586884,
      "learning_rate": 1.9845833333333332e-05,
      "loss": 0.0018,
      "step": 144740
    },
    {
      "epoch": 4.825,
      "grad_norm": 0.25761595368385315,
      "learning_rate": 1.984375e-05,
      "loss": 0.0015,
      "step": 144750
    },
    {
      "epoch": 4.825333333333333,
      "grad_norm": 0.14322079718112946,
      "learning_rate": 1.9841666666666667e-05,
      "loss": 0.0018,
      "step": 144760
    },
    {
      "epoch": 4.825666666666667,
      "grad_norm": 0.20028340816497803,
      "learning_rate": 1.9839583333333332e-05,
      "loss": 0.0022,
      "step": 144770
    },
    {
      "epoch": 4.826,
      "grad_norm": 0.057495541870594025,
      "learning_rate": 1.98375e-05,
      "loss": 0.003,
      "step": 144780
    },
    {
      "epoch": 4.826333333333333,
      "grad_norm": 0.17165473103523254,
      "learning_rate": 1.9835416666666667e-05,
      "loss": 0.0037,
      "step": 144790
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.43089237809181213,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0015,
      "step": 144800
    },
    {
      "epoch": 4.827,
      "grad_norm": 0.05717340111732483,
      "learning_rate": 1.983125e-05,
      "loss": 0.0024,
      "step": 144810
    },
    {
      "epoch": 4.827333333333334,
      "grad_norm": 0.2637040317058563,
      "learning_rate": 1.982916666666667e-05,
      "loss": 0.0017,
      "step": 144820
    },
    {
      "epoch": 4.8276666666666666,
      "grad_norm": 0.05754745751619339,
      "learning_rate": 1.9827083333333335e-05,
      "loss": 0.0014,
      "step": 144830
    },
    {
      "epoch": 4.828,
      "grad_norm": 0.3432275652885437,
      "learning_rate": 1.9825e-05,
      "loss": 0.0021,
      "step": 144840
    },
    {
      "epoch": 4.828333333333333,
      "grad_norm": 0.3477918803691864,
      "learning_rate": 1.982291666666667e-05,
      "loss": 0.0017,
      "step": 144850
    },
    {
      "epoch": 4.828666666666667,
      "grad_norm": 0.031036578118801117,
      "learning_rate": 1.9820833333333332e-05,
      "loss": 0.0023,
      "step": 144860
    },
    {
      "epoch": 4.829,
      "grad_norm": 0.029555674642324448,
      "learning_rate": 1.981875e-05,
      "loss": 0.0015,
      "step": 144870
    },
    {
      "epoch": 4.8293333333333335,
      "grad_norm": 0.25692933797836304,
      "learning_rate": 1.9816666666666666e-05,
      "loss": 0.0015,
      "step": 144880
    },
    {
      "epoch": 4.829666666666666,
      "grad_norm": 0.25015291571617126,
      "learning_rate": 1.9814583333333335e-05,
      "loss": 0.0027,
      "step": 144890
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.14292126893997192,
      "learning_rate": 1.98125e-05,
      "loss": 0.0027,
      "step": 144900
    },
    {
      "epoch": 4.830333333333334,
      "grad_norm": 0.05714089050889015,
      "learning_rate": 1.9810416666666666e-05,
      "loss": 0.0019,
      "step": 144910
    },
    {
      "epoch": 4.830666666666667,
      "grad_norm": 0.05738510936498642,
      "learning_rate": 1.9808333333333335e-05,
      "loss": 0.002,
      "step": 144920
    },
    {
      "epoch": 4.8309999999999995,
      "grad_norm": 0.2861449122428894,
      "learning_rate": 1.980625e-05,
      "loss": 0.0023,
      "step": 144930
    },
    {
      "epoch": 4.831333333333333,
      "grad_norm": 0.1175578311085701,
      "learning_rate": 1.980416666666667e-05,
      "loss": 0.0014,
      "step": 144940
    },
    {
      "epoch": 4.831666666666667,
      "grad_norm": 0.05797327309846878,
      "learning_rate": 1.9802083333333335e-05,
      "loss": 0.0015,
      "step": 144950
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.08648819476366043,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0024,
      "step": 144960
    },
    {
      "epoch": 4.832333333333334,
      "grad_norm": 0.14292192459106445,
      "learning_rate": 1.979791666666667e-05,
      "loss": 0.002,
      "step": 144970
    },
    {
      "epoch": 4.832666666666666,
      "grad_norm": 0.20086872577667236,
      "learning_rate": 1.9795833333333335e-05,
      "loss": 0.0014,
      "step": 144980
    },
    {
      "epoch": 4.833,
      "grad_norm": 0.2861684560775757,
      "learning_rate": 1.979375e-05,
      "loss": 0.0025,
      "step": 144990
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 0.5144866704940796,
      "learning_rate": 1.9791666666666665e-05,
      "loss": 0.0017,
      "step": 145000
    },
    {
      "epoch": 4.833666666666667,
      "grad_norm": 0.11829148232936859,
      "learning_rate": 1.9789583333333334e-05,
      "loss": 0.0018,
      "step": 145010
    },
    {
      "epoch": 4.834,
      "grad_norm": 0.11433619260787964,
      "learning_rate": 1.97875e-05,
      "loss": 0.0038,
      "step": 145020
    },
    {
      "epoch": 4.834333333333333,
      "grad_norm": 0.2861480712890625,
      "learning_rate": 1.978541666666667e-05,
      "loss": 0.0017,
      "step": 145030
    },
    {
      "epoch": 4.834666666666667,
      "grad_norm": 0.030155856162309647,
      "learning_rate": 1.9783333333333334e-05,
      "loss": 0.0013,
      "step": 145040
    },
    {
      "epoch": 4.835,
      "grad_norm": 0.38509422540664673,
      "learning_rate": 1.978125e-05,
      "loss": 0.0018,
      "step": 145050
    },
    {
      "epoch": 4.835333333333334,
      "grad_norm": 0.085906982421875,
      "learning_rate": 1.977916666666667e-05,
      "loss": 0.0026,
      "step": 145060
    },
    {
      "epoch": 4.835666666666667,
      "grad_norm": 0.22930856049060822,
      "learning_rate": 1.9777083333333334e-05,
      "loss": 0.0025,
      "step": 145070
    },
    {
      "epoch": 4.836,
      "grad_norm": 0.029408983886241913,
      "learning_rate": 1.9775000000000003e-05,
      "loss": 0.0016,
      "step": 145080
    },
    {
      "epoch": 4.836333333333333,
      "grad_norm": 0.22829364240169525,
      "learning_rate": 1.977291666666667e-05,
      "loss": 0.0023,
      "step": 145090
    },
    {
      "epoch": 4.836666666666667,
      "grad_norm": 0.2011362761259079,
      "learning_rate": 1.9770833333333337e-05,
      "loss": 0.0039,
      "step": 145100
    },
    {
      "epoch": 4.837,
      "grad_norm": 0.24318502843379974,
      "learning_rate": 1.976875e-05,
      "loss": 0.002,
      "step": 145110
    },
    {
      "epoch": 4.8373333333333335,
      "grad_norm": 0.0859086811542511,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0015,
      "step": 145120
    },
    {
      "epoch": 4.837666666666666,
      "grad_norm": 0.01161686610430479,
      "learning_rate": 1.9764583333333334e-05,
      "loss": 0.0017,
      "step": 145130
    },
    {
      "epoch": 4.838,
      "grad_norm": 0.3151434063911438,
      "learning_rate": 1.97625e-05,
      "loss": 0.0021,
      "step": 145140
    },
    {
      "epoch": 4.838333333333333,
      "grad_norm": 0.35243988037109375,
      "learning_rate": 1.9760416666666668e-05,
      "loss": 0.0017,
      "step": 145150
    },
    {
      "epoch": 4.838666666666667,
      "grad_norm": 0.5717931389808655,
      "learning_rate": 1.9758333333333334e-05,
      "loss": 0.0021,
      "step": 145160
    },
    {
      "epoch": 4.839,
      "grad_norm": 0.4574381113052368,
      "learning_rate": 1.9756250000000002e-05,
      "loss": 0.0016,
      "step": 145170
    },
    {
      "epoch": 4.839333333333333,
      "grad_norm": 0.550044059753418,
      "learning_rate": 1.9754166666666668e-05,
      "loss": 0.0022,
      "step": 145180
    },
    {
      "epoch": 4.839666666666667,
      "grad_norm": 0.22978413105010986,
      "learning_rate": 1.9752083333333333e-05,
      "loss": 0.0023,
      "step": 145190
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.13897556066513062,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.0019,
      "step": 145200
    },
    {
      "epoch": 4.840333333333334,
      "grad_norm": 0.2575216293334961,
      "learning_rate": 1.9747916666666668e-05,
      "loss": 0.0022,
      "step": 145210
    },
    {
      "epoch": 4.8406666666666665,
      "grad_norm": 0.2005273699760437,
      "learning_rate": 1.9745833333333337e-05,
      "loss": 0.0022,
      "step": 145220
    },
    {
      "epoch": 4.841,
      "grad_norm": 0.25746601819992065,
      "learning_rate": 1.974375e-05,
      "loss": 0.0019,
      "step": 145230
    },
    {
      "epoch": 4.841333333333333,
      "grad_norm": 0.06996983289718628,
      "learning_rate": 1.9741666666666668e-05,
      "loss": 0.0025,
      "step": 145240
    },
    {
      "epoch": 4.841666666666667,
      "grad_norm": 0.11464475840330124,
      "learning_rate": 1.9739583333333333e-05,
      "loss": 0.0019,
      "step": 145250
    },
    {
      "epoch": 4.842,
      "grad_norm": 0.034251995384693146,
      "learning_rate": 1.9737500000000002e-05,
      "loss": 0.0023,
      "step": 145260
    },
    {
      "epoch": 4.842333333333333,
      "grad_norm": 0.0573359914124012,
      "learning_rate": 1.9735416666666668e-05,
      "loss": 0.0026,
      "step": 145270
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.14338184893131256,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0014,
      "step": 145280
    },
    {
      "epoch": 4.843,
      "grad_norm": 0.12422327697277069,
      "learning_rate": 1.9731250000000002e-05,
      "loss": 0.0027,
      "step": 145290
    },
    {
      "epoch": 4.843333333333334,
      "grad_norm": 0.05754522234201431,
      "learning_rate": 1.9729166666666667e-05,
      "loss": 0.0018,
      "step": 145300
    },
    {
      "epoch": 4.843666666666667,
      "grad_norm": 0.3433603048324585,
      "learning_rate": 1.9727083333333336e-05,
      "loss": 0.0017,
      "step": 145310
    },
    {
      "epoch": 4.844,
      "grad_norm": 0.1999831348657608,
      "learning_rate": 1.9725000000000002e-05,
      "loss": 0.0021,
      "step": 145320
    },
    {
      "epoch": 4.844333333333333,
      "grad_norm": 0.2012200653553009,
      "learning_rate": 1.9722916666666667e-05,
      "loss": 0.0022,
      "step": 145330
    },
    {
      "epoch": 4.844666666666667,
      "grad_norm": 0.12198030948638916,
      "learning_rate": 1.9720833333333336e-05,
      "loss": 0.0023,
      "step": 145340
    },
    {
      "epoch": 4.845,
      "grad_norm": 0.17626197636127472,
      "learning_rate": 1.9718749999999998e-05,
      "loss": 0.0021,
      "step": 145350
    },
    {
      "epoch": 4.8453333333333335,
      "grad_norm": 0.4570379853248596,
      "learning_rate": 1.9716666666666667e-05,
      "loss": 0.0024,
      "step": 145360
    },
    {
      "epoch": 4.845666666666666,
      "grad_norm": 0.16660164296627045,
      "learning_rate": 1.9714583333333333e-05,
      "loss": 0.003,
      "step": 145370
    },
    {
      "epoch": 4.846,
      "grad_norm": 0.1718631535768509,
      "learning_rate": 1.97125e-05,
      "loss": 0.0015,
      "step": 145380
    },
    {
      "epoch": 4.846333333333334,
      "grad_norm": 0.48933449387550354,
      "learning_rate": 1.9710416666666667e-05,
      "loss": 0.0018,
      "step": 145390
    },
    {
      "epoch": 4.846666666666667,
      "grad_norm": 0.08605486899614334,
      "learning_rate": 1.9708333333333336e-05,
      "loss": 0.0023,
      "step": 145400
    },
    {
      "epoch": 4.8469999999999995,
      "grad_norm": 0.030856918543577194,
      "learning_rate": 1.970625e-05,
      "loss": 0.0021,
      "step": 145410
    },
    {
      "epoch": 4.847333333333333,
      "grad_norm": 0.1427931934595108,
      "learning_rate": 1.9704166666666667e-05,
      "loss": 0.0025,
      "step": 145420
    },
    {
      "epoch": 4.847666666666667,
      "grad_norm": 0.27242934703826904,
      "learning_rate": 1.9702083333333336e-05,
      "loss": 0.0017,
      "step": 145430
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.11489780992269516,
      "learning_rate": 1.97e-05,
      "loss": 0.0018,
      "step": 145440
    },
    {
      "epoch": 4.848333333333334,
      "grad_norm": 0.11495975404977798,
      "learning_rate": 1.969791666666667e-05,
      "loss": 0.0019,
      "step": 145450
    },
    {
      "epoch": 4.8486666666666665,
      "grad_norm": 0.11456835269927979,
      "learning_rate": 1.9695833333333335e-05,
      "loss": 0.0014,
      "step": 145460
    },
    {
      "epoch": 4.849,
      "grad_norm": 0.3429710268974304,
      "learning_rate": 1.969375e-05,
      "loss": 0.0022,
      "step": 145470
    },
    {
      "epoch": 4.849333333333333,
      "grad_norm": 0.027481330558657646,
      "learning_rate": 1.9691666666666666e-05,
      "loss": 0.0017,
      "step": 145480
    },
    {
      "epoch": 4.849666666666667,
      "grad_norm": 0.03638610616326332,
      "learning_rate": 1.9689583333333332e-05,
      "loss": 0.0021,
      "step": 145490
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.4003320634365082,
      "learning_rate": 1.96875e-05,
      "loss": 0.0016,
      "step": 145500
    },
    {
      "epoch": 4.850333333333333,
      "grad_norm": 0.11460459232330322,
      "learning_rate": 1.9685416666666666e-05,
      "loss": 0.0016,
      "step": 145510
    },
    {
      "epoch": 4.850666666666667,
      "grad_norm": 0.08569975197315216,
      "learning_rate": 1.9683333333333335e-05,
      "loss": 0.0016,
      "step": 145520
    },
    {
      "epoch": 4.851,
      "grad_norm": 0.2281576246023178,
      "learning_rate": 1.968125e-05,
      "loss": 0.0018,
      "step": 145530
    },
    {
      "epoch": 4.851333333333334,
      "grad_norm": 0.13147099316120148,
      "learning_rate": 1.967916666666667e-05,
      "loss": 0.0023,
      "step": 145540
    },
    {
      "epoch": 4.851666666666667,
      "grad_norm": 0.17168089747428894,
      "learning_rate": 1.9677083333333335e-05,
      "loss": 0.0015,
      "step": 145550
    },
    {
      "epoch": 4.852,
      "grad_norm": 0.058724235743284225,
      "learning_rate": 1.9675e-05,
      "loss": 0.0016,
      "step": 145560
    },
    {
      "epoch": 4.852333333333333,
      "grad_norm": 0.08704453706741333,
      "learning_rate": 1.967291666666667e-05,
      "loss": 0.0017,
      "step": 145570
    },
    {
      "epoch": 4.852666666666667,
      "grad_norm": 0.429150253534317,
      "learning_rate": 1.9670833333333335e-05,
      "loss": 0.0016,
      "step": 145580
    },
    {
      "epoch": 4.853,
      "grad_norm": 0.08792208880186081,
      "learning_rate": 1.966875e-05,
      "loss": 0.0021,
      "step": 145590
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.19996511936187744,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0025,
      "step": 145600
    },
    {
      "epoch": 4.853666666666666,
      "grad_norm": 0.004766973666846752,
      "learning_rate": 1.9664583333333335e-05,
      "loss": 0.0018,
      "step": 145610
    },
    {
      "epoch": 4.854,
      "grad_norm": 0.22872373461723328,
      "learning_rate": 1.96625e-05,
      "loss": 0.0018,
      "step": 145620
    },
    {
      "epoch": 4.854333333333333,
      "grad_norm": 0.3479550778865814,
      "learning_rate": 1.9660416666666666e-05,
      "loss": 0.0029,
      "step": 145630
    },
    {
      "epoch": 4.854666666666667,
      "grad_norm": 0.05816879868507385,
      "learning_rate": 1.9658333333333335e-05,
      "loss": 0.0022,
      "step": 145640
    },
    {
      "epoch": 4.855,
      "grad_norm": 0.2859216630458832,
      "learning_rate": 1.965625e-05,
      "loss": 0.0018,
      "step": 145650
    },
    {
      "epoch": 4.855333333333333,
      "grad_norm": 0.17182129621505737,
      "learning_rate": 1.965416666666667e-05,
      "loss": 0.0021,
      "step": 145660
    },
    {
      "epoch": 4.855666666666667,
      "grad_norm": 0.21003109216690063,
      "learning_rate": 1.9652083333333334e-05,
      "loss": 0.0027,
      "step": 145670
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.19748352468013763,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 0.0022,
      "step": 145680
    },
    {
      "epoch": 4.856333333333334,
      "grad_norm": 0.05289096757769585,
      "learning_rate": 1.964791666666667e-05,
      "loss": 0.0019,
      "step": 145690
    },
    {
      "epoch": 4.8566666666666665,
      "grad_norm": 0.27714651823043823,
      "learning_rate": 1.9645833333333334e-05,
      "loss": 0.0038,
      "step": 145700
    },
    {
      "epoch": 4.857,
      "grad_norm": 0.5914759039878845,
      "learning_rate": 1.964375e-05,
      "loss": 0.0018,
      "step": 145710
    },
    {
      "epoch": 4.857333333333333,
      "grad_norm": 0.5141157507896423,
      "learning_rate": 1.9641666666666665e-05,
      "loss": 0.0014,
      "step": 145720
    },
    {
      "epoch": 4.857666666666667,
      "grad_norm": 0.685823917388916,
      "learning_rate": 1.9639583333333334e-05,
      "loss": 0.0032,
      "step": 145730
    },
    {
      "epoch": 4.858,
      "grad_norm": 0.37174415588378906,
      "learning_rate": 1.96375e-05,
      "loss": 0.0029,
      "step": 145740
    },
    {
      "epoch": 4.858333333333333,
      "grad_norm": 0.20024587213993073,
      "learning_rate": 1.963541666666667e-05,
      "loss": 0.0011,
      "step": 145750
    },
    {
      "epoch": 4.858666666666666,
      "grad_norm": 0.0861809179186821,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.002,
      "step": 145760
    },
    {
      "epoch": 4.859,
      "grad_norm": 0.2290104627609253,
      "learning_rate": 1.963125e-05,
      "loss": 0.0022,
      "step": 145770
    },
    {
      "epoch": 4.859333333333334,
      "grad_norm": 0.15065157413482666,
      "learning_rate": 1.962916666666667e-05,
      "loss": 0.0015,
      "step": 145780
    },
    {
      "epoch": 4.859666666666667,
      "grad_norm": 0.06042718514800072,
      "learning_rate": 1.9627083333333334e-05,
      "loss": 0.0017,
      "step": 145790
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.2300151288509369,
      "learning_rate": 1.9625000000000003e-05,
      "loss": 0.0024,
      "step": 145800
    },
    {
      "epoch": 4.860333333333333,
      "grad_norm": 0.00637290021404624,
      "learning_rate": 1.9622916666666668e-05,
      "loss": 0.002,
      "step": 145810
    },
    {
      "epoch": 4.860666666666667,
      "grad_norm": 0.08666858822107315,
      "learning_rate": 1.9620833333333337e-05,
      "loss": 0.0019,
      "step": 145820
    },
    {
      "epoch": 4.861,
      "grad_norm": 0.030482955276966095,
      "learning_rate": 1.961875e-05,
      "loss": 0.0018,
      "step": 145830
    },
    {
      "epoch": 4.8613333333333335,
      "grad_norm": 0.3431503176689148,
      "learning_rate": 1.9616666666666668e-05,
      "loss": 0.0016,
      "step": 145840
    },
    {
      "epoch": 4.861666666666666,
      "grad_norm": 0.08637425303459167,
      "learning_rate": 1.9614583333333334e-05,
      "loss": 0.0017,
      "step": 145850
    },
    {
      "epoch": 4.862,
      "grad_norm": 0.4960106909275055,
      "learning_rate": 1.96125e-05,
      "loss": 0.0025,
      "step": 145860
    },
    {
      "epoch": 4.862333333333333,
      "grad_norm": 0.1399272233247757,
      "learning_rate": 1.9610416666666668e-05,
      "loss": 0.0017,
      "step": 145870
    },
    {
      "epoch": 4.862666666666667,
      "grad_norm": 0.1248888373374939,
      "learning_rate": 1.9608333333333333e-05,
      "loss": 0.0016,
      "step": 145880
    },
    {
      "epoch": 4.8629999999999995,
      "grad_norm": 0.08593279123306274,
      "learning_rate": 1.9606250000000002e-05,
      "loss": 0.0024,
      "step": 145890
    },
    {
      "epoch": 4.863333333333333,
      "grad_norm": 0.11500223726034164,
      "learning_rate": 1.9604166666666668e-05,
      "loss": 0.0024,
      "step": 145900
    },
    {
      "epoch": 4.863666666666667,
      "grad_norm": 0.4291478395462036,
      "learning_rate": 1.9602083333333333e-05,
      "loss": 0.0018,
      "step": 145910
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.08697774261236191,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0017,
      "step": 145920
    },
    {
      "epoch": 4.864333333333334,
      "grad_norm": 0.2001381516456604,
      "learning_rate": 1.9597916666666668e-05,
      "loss": 0.0018,
      "step": 145930
    },
    {
      "epoch": 4.8646666666666665,
      "grad_norm": 0.17141777276992798,
      "learning_rate": 1.9595833333333336e-05,
      "loss": 0.0026,
      "step": 145940
    },
    {
      "epoch": 4.865,
      "grad_norm": 0.00973771046847105,
      "learning_rate": 1.959375e-05,
      "loss": 0.0017,
      "step": 145950
    },
    {
      "epoch": 4.865333333333333,
      "grad_norm": 0.20028901100158691,
      "learning_rate": 1.9591666666666667e-05,
      "loss": 0.0016,
      "step": 145960
    },
    {
      "epoch": 4.865666666666667,
      "grad_norm": 0.40009549260139465,
      "learning_rate": 1.9589583333333333e-05,
      "loss": 0.0023,
      "step": 145970
    },
    {
      "epoch": 4.866,
      "grad_norm": 0.1472291648387909,
      "learning_rate": 1.9587500000000002e-05,
      "loss": 0.0019,
      "step": 145980
    },
    {
      "epoch": 4.866333333333333,
      "grad_norm": 0.03139699995517731,
      "learning_rate": 1.9585416666666667e-05,
      "loss": 0.0017,
      "step": 145990
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.28434357047080994,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0026,
      "step": 146000
    },
    {
      "epoch": 4.867,
      "grad_norm": 0.40042197704315186,
      "learning_rate": 1.958125e-05,
      "loss": 0.0023,
      "step": 146010
    },
    {
      "epoch": 4.867333333333333,
      "grad_norm": 0.05488171428442001,
      "learning_rate": 1.9579166666666667e-05,
      "loss": 0.0014,
      "step": 146020
    },
    {
      "epoch": 4.867666666666667,
      "grad_norm": 0.17185238003730774,
      "learning_rate": 1.9577083333333336e-05,
      "loss": 0.0016,
      "step": 146030
    },
    {
      "epoch": 4.868,
      "grad_norm": 0.14337390661239624,
      "learning_rate": 1.9575e-05,
      "loss": 0.0015,
      "step": 146040
    },
    {
      "epoch": 4.868333333333333,
      "grad_norm": 0.25715598464012146,
      "learning_rate": 1.9572916666666667e-05,
      "loss": 0.0014,
      "step": 146050
    },
    {
      "epoch": 4.868666666666667,
      "grad_norm": 0.31495803594589233,
      "learning_rate": 1.9570833333333336e-05,
      "loss": 0.0016,
      "step": 146060
    },
    {
      "epoch": 4.869,
      "grad_norm": 0.3430963456630707,
      "learning_rate": 1.9568749999999998e-05,
      "loss": 0.0019,
      "step": 146070
    },
    {
      "epoch": 4.8693333333333335,
      "grad_norm": 0.11488354951143265,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.003,
      "step": 146080
    },
    {
      "epoch": 4.869666666666666,
      "grad_norm": 0.008611672557890415,
      "learning_rate": 1.9564583333333332e-05,
      "loss": 0.0016,
      "step": 146090
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.129001647233963,
      "learning_rate": 1.95625e-05,
      "loss": 0.0014,
      "step": 146100
    },
    {
      "epoch": 4.870333333333333,
      "grad_norm": 0.8629432320594788,
      "learning_rate": 1.9560416666666667e-05,
      "loss": 0.0018,
      "step": 146110
    },
    {
      "epoch": 4.870666666666667,
      "grad_norm": 0.32005617022514343,
      "learning_rate": 1.9558333333333336e-05,
      "loss": 0.0015,
      "step": 146120
    },
    {
      "epoch": 4.871,
      "grad_norm": 0.3144557774066925,
      "learning_rate": 1.955625e-05,
      "loss": 0.0018,
      "step": 146130
    },
    {
      "epoch": 4.871333333333333,
      "grad_norm": 0.05868084728717804,
      "learning_rate": 1.9554166666666667e-05,
      "loss": 0.0023,
      "step": 146140
    },
    {
      "epoch": 4.871666666666667,
      "grad_norm": 0.031370021402835846,
      "learning_rate": 1.9552083333333335e-05,
      "loss": 0.0017,
      "step": 146150
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.22901439666748047,
      "learning_rate": 1.955e-05,
      "loss": 0.002,
      "step": 146160
    },
    {
      "epoch": 4.872333333333334,
      "grad_norm": 0.058271247893571854,
      "learning_rate": 1.954791666666667e-05,
      "loss": 0.0011,
      "step": 146170
    },
    {
      "epoch": 4.8726666666666665,
      "grad_norm": 0.030955424532294273,
      "learning_rate": 1.9545833333333335e-05,
      "loss": 0.0019,
      "step": 146180
    },
    {
      "epoch": 4.873,
      "grad_norm": 0.28566014766693115,
      "learning_rate": 1.954375e-05,
      "loss": 0.0031,
      "step": 146190
    },
    {
      "epoch": 4.873333333333333,
      "grad_norm": 0.07322077453136444,
      "learning_rate": 1.9541666666666666e-05,
      "loss": 0.0019,
      "step": 146200
    },
    {
      "epoch": 4.873666666666667,
      "grad_norm": 0.07834675163030624,
      "learning_rate": 1.953958333333333e-05,
      "loss": 0.0025,
      "step": 146210
    },
    {
      "epoch": 4.874,
      "grad_norm": 0.5046044588088989,
      "learning_rate": 1.95375e-05,
      "loss": 0.0017,
      "step": 146220
    },
    {
      "epoch": 4.874333333333333,
      "grad_norm": 0.3148444592952728,
      "learning_rate": 1.9535416666666666e-05,
      "loss": 0.0023,
      "step": 146230
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.36534085869789124,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0023,
      "step": 146240
    },
    {
      "epoch": 4.875,
      "grad_norm": 0.14348599314689636,
      "learning_rate": 1.953125e-05,
      "loss": 0.0014,
      "step": 146250
    },
    {
      "epoch": 4.875333333333334,
      "grad_norm": 0.02943027764558792,
      "learning_rate": 1.952916666666667e-05,
      "loss": 0.0017,
      "step": 146260
    },
    {
      "epoch": 4.875666666666667,
      "grad_norm": 0.2288958728313446,
      "learning_rate": 1.9527083333333335e-05,
      "loss": 0.0018,
      "step": 146270
    },
    {
      "epoch": 4.876,
      "grad_norm": 0.013262322172522545,
      "learning_rate": 1.9525e-05,
      "loss": 0.0017,
      "step": 146280
    },
    {
      "epoch": 4.876333333333333,
      "grad_norm": 0.05791128799319267,
      "learning_rate": 1.952291666666667e-05,
      "loss": 0.0019,
      "step": 146290
    },
    {
      "epoch": 4.876666666666667,
      "grad_norm": 0.029477978125214577,
      "learning_rate": 1.9520833333333335e-05,
      "loss": 0.0016,
      "step": 146300
    },
    {
      "epoch": 4.877,
      "grad_norm": 0.22844645380973816,
      "learning_rate": 1.9518750000000003e-05,
      "loss": 0.002,
      "step": 146310
    },
    {
      "epoch": 4.8773333333333335,
      "grad_norm": 0.08661312609910965,
      "learning_rate": 1.9516666666666666e-05,
      "loss": 0.0021,
      "step": 146320
    },
    {
      "epoch": 4.877666666666666,
      "grad_norm": 0.11456821858882904,
      "learning_rate": 1.9514583333333334e-05,
      "loss": 0.0026,
      "step": 146330
    },
    {
      "epoch": 4.878,
      "grad_norm": 0.2292603850364685,
      "learning_rate": 1.95125e-05,
      "loss": 0.0022,
      "step": 146340
    },
    {
      "epoch": 4.878333333333333,
      "grad_norm": 0.2863711714744568,
      "learning_rate": 1.9510416666666665e-05,
      "loss": 0.0018,
      "step": 146350
    },
    {
      "epoch": 4.878666666666667,
      "grad_norm": 0.3730275332927704,
      "learning_rate": 1.9508333333333334e-05,
      "loss": 0.0015,
      "step": 146360
    },
    {
      "epoch": 4.879,
      "grad_norm": 0.3145356774330139,
      "learning_rate": 1.950625e-05,
      "loss": 0.002,
      "step": 146370
    },
    {
      "epoch": 4.879333333333333,
      "grad_norm": 0.028998028486967087,
      "learning_rate": 1.950416666666667e-05,
      "loss": 0.0013,
      "step": 146380
    },
    {
      "epoch": 4.879666666666667,
      "grad_norm": 0.11552447825670242,
      "learning_rate": 1.9502083333333334e-05,
      "loss": 0.0024,
      "step": 146390
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.12386929243803024,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.0016,
      "step": 146400
    },
    {
      "epoch": 4.880333333333334,
      "grad_norm": 0.14289672672748566,
      "learning_rate": 1.949791666666667e-05,
      "loss": 0.0015,
      "step": 146410
    },
    {
      "epoch": 4.8806666666666665,
      "grad_norm": 0.20087000727653503,
      "learning_rate": 1.9495833333333334e-05,
      "loss": 0.0013,
      "step": 146420
    },
    {
      "epoch": 4.881,
      "grad_norm": 0.11431385576725006,
      "learning_rate": 1.9493750000000003e-05,
      "loss": 0.0018,
      "step": 146430
    },
    {
      "epoch": 4.881333333333333,
      "grad_norm": 0.1719026267528534,
      "learning_rate": 1.9491666666666665e-05,
      "loss": 0.0017,
      "step": 146440
    },
    {
      "epoch": 4.881666666666667,
      "grad_norm": 0.057312268763780594,
      "learning_rate": 1.9489583333333334e-05,
      "loss": 0.0024,
      "step": 146450
    },
    {
      "epoch": 4.882,
      "grad_norm": 0.28610342741012573,
      "learning_rate": 1.94875e-05,
      "loss": 0.0016,
      "step": 146460
    },
    {
      "epoch": 4.882333333333333,
      "grad_norm": 0.1425943225622177,
      "learning_rate": 1.9485416666666668e-05,
      "loss": 0.0021,
      "step": 146470
    },
    {
      "epoch": 4.882666666666667,
      "grad_norm": 0.5756084322929382,
      "learning_rate": 1.9483333333333334e-05,
      "loss": 0.0022,
      "step": 146480
    },
    {
      "epoch": 4.883,
      "grad_norm": 0.3429597318172455,
      "learning_rate": 1.9481250000000003e-05,
      "loss": 0.0016,
      "step": 146490
    },
    {
      "epoch": 4.883333333333333,
      "grad_norm": 0.1716119945049286,
      "learning_rate": 1.9479166666666668e-05,
      "loss": 0.0024,
      "step": 146500
    },
    {
      "epoch": 4.883666666666667,
      "grad_norm": 0.016663679853081703,
      "learning_rate": 1.9477083333333334e-05,
      "loss": 0.0018,
      "step": 146510
    },
    {
      "epoch": 4.884,
      "grad_norm": 0.2569405734539032,
      "learning_rate": 1.9475000000000002e-05,
      "loss": 0.0021,
      "step": 146520
    },
    {
      "epoch": 4.884333333333333,
      "grad_norm": 0.0074712843634188175,
      "learning_rate": 1.9472916666666668e-05,
      "loss": 0.0018,
      "step": 146530
    },
    {
      "epoch": 4.884666666666667,
      "grad_norm": 0.1890386939048767,
      "learning_rate": 1.9470833333333337e-05,
      "loss": 0.0026,
      "step": 146540
    },
    {
      "epoch": 4.885,
      "grad_norm": 0.25721797347068787,
      "learning_rate": 1.9468750000000002e-05,
      "loss": 0.0022,
      "step": 146550
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.17172583937644958,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0028,
      "step": 146560
    },
    {
      "epoch": 4.885666666666666,
      "grad_norm": 0.057333361357450485,
      "learning_rate": 1.9464583333333333e-05,
      "loss": 0.0016,
      "step": 146570
    },
    {
      "epoch": 4.886,
      "grad_norm": 0.17177899181842804,
      "learning_rate": 1.94625e-05,
      "loss": 0.0023,
      "step": 146580
    },
    {
      "epoch": 4.886333333333333,
      "grad_norm": 0.02933674305677414,
      "learning_rate": 1.9460416666666668e-05,
      "loss": 0.0021,
      "step": 146590
    },
    {
      "epoch": 4.886666666666667,
      "grad_norm": 0.3143427073955536,
      "learning_rate": 1.9458333333333333e-05,
      "loss": 0.003,
      "step": 146600
    },
    {
      "epoch": 4.8870000000000005,
      "grad_norm": 0.029968926683068275,
      "learning_rate": 1.9456250000000002e-05,
      "loss": 0.0023,
      "step": 146610
    },
    {
      "epoch": 4.887333333333333,
      "grad_norm": 0.11555617302656174,
      "learning_rate": 1.9454166666666667e-05,
      "loss": 0.0019,
      "step": 146620
    },
    {
      "epoch": 4.887666666666667,
      "grad_norm": 0.46988755464553833,
      "learning_rate": 1.9452083333333336e-05,
      "loss": 0.0027,
      "step": 146630
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.06807960569858551,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 0.0019,
      "step": 146640
    },
    {
      "epoch": 4.888333333333334,
      "grad_norm": 0.20025716722011566,
      "learning_rate": 1.9447916666666667e-05,
      "loss": 0.002,
      "step": 146650
    },
    {
      "epoch": 4.8886666666666665,
      "grad_norm": 0.058245714753866196,
      "learning_rate": 1.9445833333333336e-05,
      "loss": 0.0017,
      "step": 146660
    },
    {
      "epoch": 4.889,
      "grad_norm": 0.03077191859483719,
      "learning_rate": 1.944375e-05,
      "loss": 0.0013,
      "step": 146670
    },
    {
      "epoch": 4.889333333333333,
      "grad_norm": 0.08724852651357651,
      "learning_rate": 1.9441666666666667e-05,
      "loss": 0.0017,
      "step": 146680
    },
    {
      "epoch": 4.889666666666667,
      "grad_norm": 0.2730301320552826,
      "learning_rate": 1.9439583333333333e-05,
      "loss": 0.0017,
      "step": 146690
    },
    {
      "epoch": 4.89,
      "grad_norm": 0.05847153812646866,
      "learning_rate": 1.94375e-05,
      "loss": 0.0012,
      "step": 146700
    },
    {
      "epoch": 4.890333333333333,
      "grad_norm": 0.14343728125095367,
      "learning_rate": 1.9435416666666667e-05,
      "loss": 0.0014,
      "step": 146710
    },
    {
      "epoch": 4.890666666666666,
      "grad_norm": 0.28638792037963867,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0018,
      "step": 146720
    },
    {
      "epoch": 4.891,
      "grad_norm": 0.6004382371902466,
      "learning_rate": 1.943125e-05,
      "loss": 0.0022,
      "step": 146730
    },
    {
      "epoch": 4.891333333333334,
      "grad_norm": 0.2576214075088501,
      "learning_rate": 1.9429166666666667e-05,
      "loss": 0.0021,
      "step": 146740
    },
    {
      "epoch": 4.891666666666667,
      "grad_norm": 0.02867606095969677,
      "learning_rate": 1.9427083333333336e-05,
      "loss": 0.002,
      "step": 146750
    },
    {
      "epoch": 4.892,
      "grad_norm": 0.2572280168533325,
      "learning_rate": 1.9425e-05,
      "loss": 0.0017,
      "step": 146760
    },
    {
      "epoch": 4.892333333333333,
      "grad_norm": 0.1432848423719406,
      "learning_rate": 1.942291666666667e-05,
      "loss": 0.0014,
      "step": 146770
    },
    {
      "epoch": 4.892666666666667,
      "grad_norm": 0.2644696533679962,
      "learning_rate": 1.9420833333333336e-05,
      "loss": 0.002,
      "step": 146780
    },
    {
      "epoch": 4.893,
      "grad_norm": 0.11497452855110168,
      "learning_rate": 1.941875e-05,
      "loss": 0.0027,
      "step": 146790
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.11537454277276993,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.002,
      "step": 146800
    },
    {
      "epoch": 4.893666666666666,
      "grad_norm": 0.11647555977106094,
      "learning_rate": 1.9414583333333332e-05,
      "loss": 0.0032,
      "step": 146810
    },
    {
      "epoch": 4.894,
      "grad_norm": 0.03675324097275734,
      "learning_rate": 1.94125e-05,
      "loss": 0.0016,
      "step": 146820
    },
    {
      "epoch": 4.894333333333333,
      "grad_norm": 0.14367543160915375,
      "learning_rate": 1.9410416666666666e-05,
      "loss": 0.0013,
      "step": 146830
    },
    {
      "epoch": 4.894666666666667,
      "grad_norm": 0.20012526214122772,
      "learning_rate": 1.9408333333333335e-05,
      "loss": 0.0016,
      "step": 146840
    },
    {
      "epoch": 4.895,
      "grad_norm": 0.14328502118587494,
      "learning_rate": 1.940625e-05,
      "loss": 0.0016,
      "step": 146850
    },
    {
      "epoch": 4.895333333333333,
      "grad_norm": 0.08647752553224564,
      "learning_rate": 1.9404166666666666e-05,
      "loss": 0.0016,
      "step": 146860
    },
    {
      "epoch": 4.895666666666667,
      "grad_norm": 0.08665484935045242,
      "learning_rate": 1.9402083333333335e-05,
      "loss": 0.0026,
      "step": 146870
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.11482558399438858,
      "learning_rate": 1.94e-05,
      "loss": 0.0017,
      "step": 146880
    },
    {
      "epoch": 4.896333333333334,
      "grad_norm": 0.11450905352830887,
      "learning_rate": 1.939791666666667e-05,
      "loss": 0.0021,
      "step": 146890
    },
    {
      "epoch": 4.8966666666666665,
      "grad_norm": 0.085881307721138,
      "learning_rate": 1.9395833333333335e-05,
      "loss": 0.0024,
      "step": 146900
    },
    {
      "epoch": 4.897,
      "grad_norm": 0.14308607578277588,
      "learning_rate": 1.9393750000000004e-05,
      "loss": 0.0024,
      "step": 146910
    },
    {
      "epoch": 4.897333333333333,
      "grad_norm": 0.05779535323381424,
      "learning_rate": 1.9391666666666666e-05,
      "loss": 0.0019,
      "step": 146920
    },
    {
      "epoch": 4.897666666666667,
      "grad_norm": 0.33994486927986145,
      "learning_rate": 1.9389583333333335e-05,
      "loss": 0.0024,
      "step": 146930
    },
    {
      "epoch": 4.898,
      "grad_norm": 0.08564870059490204,
      "learning_rate": 1.93875e-05,
      "loss": 0.0017,
      "step": 146940
    },
    {
      "epoch": 4.898333333333333,
      "grad_norm": 0.2856557071208954,
      "learning_rate": 1.9385416666666666e-05,
      "loss": 0.0019,
      "step": 146950
    },
    {
      "epoch": 4.898666666666666,
      "grad_norm": 0.22893089056015015,
      "learning_rate": 1.9383333333333335e-05,
      "loss": 0.0026,
      "step": 146960
    },
    {
      "epoch": 4.899,
      "grad_norm": 0.11435813456773758,
      "learning_rate": 1.938125e-05,
      "loss": 0.0012,
      "step": 146970
    },
    {
      "epoch": 4.899333333333333,
      "grad_norm": 0.17179331183433533,
      "learning_rate": 1.937916666666667e-05,
      "loss": 0.0021,
      "step": 146980
    },
    {
      "epoch": 4.899666666666667,
      "grad_norm": 0.4294944107532501,
      "learning_rate": 1.9377083333333335e-05,
      "loss": 0.0019,
      "step": 146990
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.20054124295711517,
      "learning_rate": 1.9375e-05,
      "loss": 0.0019,
      "step": 147000
    },
    {
      "epoch": 4.900333333333333,
      "grad_norm": 0.17133204638957977,
      "learning_rate": 1.937291666666667e-05,
      "loss": 0.0023,
      "step": 147010
    },
    {
      "epoch": 4.900666666666667,
      "grad_norm": 0.01582333818078041,
      "learning_rate": 1.9370833333333334e-05,
      "loss": 0.0025,
      "step": 147020
    },
    {
      "epoch": 4.901,
      "grad_norm": 0.3359149098396301,
      "learning_rate": 1.9368750000000003e-05,
      "loss": 0.0017,
      "step": 147030
    },
    {
      "epoch": 4.9013333333333335,
      "grad_norm": 0.22999057173728943,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.0023,
      "step": 147040
    },
    {
      "epoch": 4.901666666666666,
      "grad_norm": 0.22842882573604584,
      "learning_rate": 1.9364583333333334e-05,
      "loss": 0.0015,
      "step": 147050
    },
    {
      "epoch": 4.902,
      "grad_norm": 0.14340625703334808,
      "learning_rate": 1.93625e-05,
      "loss": 0.0015,
      "step": 147060
    },
    {
      "epoch": 4.902333333333333,
      "grad_norm": 0.06047862395644188,
      "learning_rate": 1.936041666666667e-05,
      "loss": 0.0022,
      "step": 147070
    },
    {
      "epoch": 4.902666666666667,
      "grad_norm": 0.28539639711380005,
      "learning_rate": 1.9358333333333334e-05,
      "loss": 0.0021,
      "step": 147080
    },
    {
      "epoch": 4.9030000000000005,
      "grad_norm": 0.032431621104478836,
      "learning_rate": 1.935625e-05,
      "loss": 0.0018,
      "step": 147090
    },
    {
      "epoch": 4.903333333333333,
      "grad_norm": 0.11434734612703323,
      "learning_rate": 1.935416666666667e-05,
      "loss": 0.0021,
      "step": 147100
    },
    {
      "epoch": 4.903666666666666,
      "grad_norm": 0.15457428991794586,
      "learning_rate": 1.9352083333333334e-05,
      "loss": 0.0015,
      "step": 147110
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.08591299504041672,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 0.0021,
      "step": 147120
    },
    {
      "epoch": 4.904333333333334,
      "grad_norm": 0.20066756010055542,
      "learning_rate": 1.9347916666666668e-05,
      "loss": 0.0022,
      "step": 147130
    },
    {
      "epoch": 4.9046666666666665,
      "grad_norm": 0.37350523471832275,
      "learning_rate": 1.9345833333333334e-05,
      "loss": 0.0015,
      "step": 147140
    },
    {
      "epoch": 4.905,
      "grad_norm": 0.14348599314689636,
      "learning_rate": 1.9343750000000003e-05,
      "loss": 0.0019,
      "step": 147150
    },
    {
      "epoch": 4.905333333333333,
      "grad_norm": 0.25765755772590637,
      "learning_rate": 1.9341666666666665e-05,
      "loss": 0.0024,
      "step": 147160
    },
    {
      "epoch": 4.905666666666667,
      "grad_norm": 0.1429920196533203,
      "learning_rate": 1.9339583333333334e-05,
      "loss": 0.0015,
      "step": 147170
    },
    {
      "epoch": 4.906,
      "grad_norm": 0.1714286506175995,
      "learning_rate": 1.93375e-05,
      "loss": 0.0018,
      "step": 147180
    },
    {
      "epoch": 4.906333333333333,
      "grad_norm": 0.08624891936779022,
      "learning_rate": 1.9335416666666668e-05,
      "loss": 0.002,
      "step": 147190
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.4288844168186188,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0026,
      "step": 147200
    },
    {
      "epoch": 4.907,
      "grad_norm": 0.00997638888657093,
      "learning_rate": 1.9331250000000002e-05,
      "loss": 0.0015,
      "step": 147210
    },
    {
      "epoch": 4.907333333333334,
      "grad_norm": 0.4287809431552887,
      "learning_rate": 1.9329166666666668e-05,
      "loss": 0.0021,
      "step": 147220
    },
    {
      "epoch": 4.907666666666667,
      "grad_norm": 0.06801722943782806,
      "learning_rate": 1.9327083333333333e-05,
      "loss": 0.0015,
      "step": 147230
    },
    {
      "epoch": 4.908,
      "grad_norm": 0.18307143449783325,
      "learning_rate": 1.9325000000000002e-05,
      "loss": 0.0022,
      "step": 147240
    },
    {
      "epoch": 4.908333333333333,
      "grad_norm": 0.1732001155614853,
      "learning_rate": 1.9322916666666668e-05,
      "loss": 0.0026,
      "step": 147250
    },
    {
      "epoch": 4.908666666666667,
      "grad_norm": 0.0863146260380745,
      "learning_rate": 1.9320833333333337e-05,
      "loss": 0.0018,
      "step": 147260
    },
    {
      "epoch": 4.909,
      "grad_norm": 0.37125831842422485,
      "learning_rate": 1.9318750000000002e-05,
      "loss": 0.0016,
      "step": 147270
    },
    {
      "epoch": 4.9093333333333335,
      "grad_norm": 0.11423110961914062,
      "learning_rate": 1.9316666666666668e-05,
      "loss": 0.0026,
      "step": 147280
    },
    {
      "epoch": 4.909666666666666,
      "grad_norm": 0.0574653223156929,
      "learning_rate": 1.9314583333333333e-05,
      "loss": 0.0018,
      "step": 147290
    },
    {
      "epoch": 4.91,
      "grad_norm": 0.4568718671798706,
      "learning_rate": 1.93125e-05,
      "loss": 0.0015,
      "step": 147300
    },
    {
      "epoch": 4.910333333333333,
      "grad_norm": 0.3717789947986603,
      "learning_rate": 1.9310416666666667e-05,
      "loss": 0.0015,
      "step": 147310
    },
    {
      "epoch": 4.910666666666667,
      "grad_norm": 0.20037682354450226,
      "learning_rate": 1.9308333333333333e-05,
      "loss": 0.0017,
      "step": 147320
    },
    {
      "epoch": 4.911,
      "grad_norm": 0.19909007847309113,
      "learning_rate": 1.9306250000000002e-05,
      "loss": 0.0024,
      "step": 147330
    },
    {
      "epoch": 4.911333333333333,
      "grad_norm": 0.02996780350804329,
      "learning_rate": 1.9304166666666667e-05,
      "loss": 0.0015,
      "step": 147340
    },
    {
      "epoch": 4.911666666666667,
      "grad_norm": 0.20015724003314972,
      "learning_rate": 1.9302083333333336e-05,
      "loss": 0.0015,
      "step": 147350
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.05925079435110092,
      "learning_rate": 1.93e-05,
      "loss": 0.0014,
      "step": 147360
    },
    {
      "epoch": 4.912333333333334,
      "grad_norm": 0.08641212433576584,
      "learning_rate": 1.9297916666666667e-05,
      "loss": 0.0014,
      "step": 147370
    },
    {
      "epoch": 4.9126666666666665,
      "grad_norm": 0.0294489748775959,
      "learning_rate": 1.9295833333333336e-05,
      "loss": 0.0022,
      "step": 147380
    },
    {
      "epoch": 4.913,
      "grad_norm": 0.19675853848457336,
      "learning_rate": 1.929375e-05,
      "loss": 0.0015,
      "step": 147390
    },
    {
      "epoch": 4.913333333333333,
      "grad_norm": 0.0580914281308651,
      "learning_rate": 1.9291666666666667e-05,
      "loss": 0.0014,
      "step": 147400
    },
    {
      "epoch": 4.913666666666667,
      "grad_norm": 0.20707917213439941,
      "learning_rate": 1.9289583333333332e-05,
      "loss": 0.0019,
      "step": 147410
    },
    {
      "epoch": 4.914,
      "grad_norm": 0.11473405361175537,
      "learning_rate": 1.92875e-05,
      "loss": 0.0024,
      "step": 147420
    },
    {
      "epoch": 4.914333333333333,
      "grad_norm": 0.057836953550577164,
      "learning_rate": 1.9285416666666667e-05,
      "loss": 0.0024,
      "step": 147430
    },
    {
      "epoch": 4.914666666666666,
      "grad_norm": 0.1437477320432663,
      "learning_rate": 1.9283333333333332e-05,
      "loss": 0.0018,
      "step": 147440
    },
    {
      "epoch": 4.915,
      "grad_norm": 0.016138598322868347,
      "learning_rate": 1.928125e-05,
      "loss": 0.0012,
      "step": 147450
    },
    {
      "epoch": 4.915333333333333,
      "grad_norm": 0.11459963023662567,
      "learning_rate": 1.9279166666666667e-05,
      "loss": 0.0025,
      "step": 147460
    },
    {
      "epoch": 4.915666666666667,
      "grad_norm": 0.40053269267082214,
      "learning_rate": 1.9277083333333335e-05,
      "loss": 0.0014,
      "step": 147470
    },
    {
      "epoch": 4.916,
      "grad_norm": 0.20019377768039703,
      "learning_rate": 1.9275e-05,
      "loss": 0.002,
      "step": 147480
    },
    {
      "epoch": 4.916333333333333,
      "grad_norm": 0.05845033377408981,
      "learning_rate": 1.927291666666667e-05,
      "loss": 0.0015,
      "step": 147490
    },
    {
      "epoch": 4.916666666666667,
      "grad_norm": 0.1714346557855606,
      "learning_rate": 1.9270833333333335e-05,
      "loss": 0.0019,
      "step": 147500
    },
    {
      "epoch": 4.917,
      "grad_norm": 0.5454829335212708,
      "learning_rate": 1.926875e-05,
      "loss": 0.0023,
      "step": 147510
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 0.4647783637046814,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0024,
      "step": 147520
    },
    {
      "epoch": 4.917666666666666,
      "grad_norm": 0.4410420060157776,
      "learning_rate": 1.9264583333333332e-05,
      "loss": 0.0022,
      "step": 147530
    },
    {
      "epoch": 4.918,
      "grad_norm": 0.3151099979877472,
      "learning_rate": 1.92625e-05,
      "loss": 0.0017,
      "step": 147540
    },
    {
      "epoch": 4.918333333333333,
      "grad_norm": 0.25774165987968445,
      "learning_rate": 1.9260416666666666e-05,
      "loss": 0.0022,
      "step": 147550
    },
    {
      "epoch": 4.918666666666667,
      "grad_norm": 0.17638058960437775,
      "learning_rate": 1.9258333333333335e-05,
      "loss": 0.0015,
      "step": 147560
    },
    {
      "epoch": 4.9190000000000005,
      "grad_norm": 0.22849401831626892,
      "learning_rate": 1.925625e-05,
      "loss": 0.0024,
      "step": 147570
    },
    {
      "epoch": 4.919333333333333,
      "grad_norm": 0.48566052317619324,
      "learning_rate": 1.9254166666666666e-05,
      "loss": 0.0016,
      "step": 147580
    },
    {
      "epoch": 4.919666666666666,
      "grad_norm": 0.17202287912368774,
      "learning_rate": 1.9252083333333335e-05,
      "loss": 0.0013,
      "step": 147590
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.371615469455719,
      "learning_rate": 1.925e-05,
      "loss": 0.0023,
      "step": 147600
    },
    {
      "epoch": 4.920333333333334,
      "grad_norm": 0.1433558464050293,
      "learning_rate": 1.924791666666667e-05,
      "loss": 0.0025,
      "step": 147610
    },
    {
      "epoch": 4.9206666666666665,
      "grad_norm": 0.1751536726951599,
      "learning_rate": 1.9245833333333335e-05,
      "loss": 0.002,
      "step": 147620
    },
    {
      "epoch": 4.921,
      "grad_norm": 0.08583298325538635,
      "learning_rate": 1.9243750000000004e-05,
      "loss": 0.0014,
      "step": 147630
    },
    {
      "epoch": 4.921333333333333,
      "grad_norm": 0.31412798166275024,
      "learning_rate": 1.924166666666667e-05,
      "loss": 0.0017,
      "step": 147640
    },
    {
      "epoch": 4.921666666666667,
      "grad_norm": 0.02963622845709324,
      "learning_rate": 1.9239583333333335e-05,
      "loss": 0.0031,
      "step": 147650
    },
    {
      "epoch": 4.922,
      "grad_norm": 0.41331955790519714,
      "learning_rate": 1.92375e-05,
      "loss": 0.002,
      "step": 147660
    },
    {
      "epoch": 4.9223333333333334,
      "grad_norm": 0.059776123613119125,
      "learning_rate": 1.9235416666666666e-05,
      "loss": 0.0017,
      "step": 147670
    },
    {
      "epoch": 4.922666666666666,
      "grad_norm": 0.03232412412762642,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0017,
      "step": 147680
    },
    {
      "epoch": 4.923,
      "grad_norm": 0.22897285223007202,
      "learning_rate": 1.923125e-05,
      "loss": 0.0017,
      "step": 147690
    },
    {
      "epoch": 4.923333333333334,
      "grad_norm": 0.2612420916557312,
      "learning_rate": 1.922916666666667e-05,
      "loss": 0.0025,
      "step": 147700
    },
    {
      "epoch": 4.923666666666667,
      "grad_norm": 0.5718045234680176,
      "learning_rate": 1.9227083333333334e-05,
      "loss": 0.0025,
      "step": 147710
    },
    {
      "epoch": 4.924,
      "grad_norm": 0.11518067121505737,
      "learning_rate": 1.9225e-05,
      "loss": 0.0017,
      "step": 147720
    },
    {
      "epoch": 4.924333333333333,
      "grad_norm": 0.2580503523349762,
      "learning_rate": 1.922291666666667e-05,
      "loss": 0.0016,
      "step": 147730
    },
    {
      "epoch": 4.924666666666667,
      "grad_norm": 0.08627405762672424,
      "learning_rate": 1.9220833333333334e-05,
      "loss": 0.0024,
      "step": 147740
    },
    {
      "epoch": 4.925,
      "grad_norm": 0.2899218201637268,
      "learning_rate": 1.9218750000000003e-05,
      "loss": 0.0019,
      "step": 147750
    },
    {
      "epoch": 4.925333333333334,
      "grad_norm": 0.03103707544505596,
      "learning_rate": 1.921666666666667e-05,
      "loss": 0.0015,
      "step": 147760
    },
    {
      "epoch": 4.925666666666666,
      "grad_norm": 0.3721717596054077,
      "learning_rate": 1.9214583333333334e-05,
      "loss": 0.0019,
      "step": 147770
    },
    {
      "epoch": 4.926,
      "grad_norm": 0.02985883504152298,
      "learning_rate": 1.92125e-05,
      "loss": 0.0014,
      "step": 147780
    },
    {
      "epoch": 4.926333333333333,
      "grad_norm": 0.1137346401810646,
      "learning_rate": 1.921041666666667e-05,
      "loss": 0.0024,
      "step": 147790
    },
    {
      "epoch": 4.926666666666667,
      "grad_norm": 0.08591755479574203,
      "learning_rate": 1.9208333333333334e-05,
      "loss": 0.0014,
      "step": 147800
    },
    {
      "epoch": 4.927,
      "grad_norm": 0.11425688117742538,
      "learning_rate": 1.920625e-05,
      "loss": 0.0019,
      "step": 147810
    },
    {
      "epoch": 4.927333333333333,
      "grad_norm": 0.3714084029197693,
      "learning_rate": 1.9204166666666668e-05,
      "loss": 0.0015,
      "step": 147820
    },
    {
      "epoch": 4.927666666666667,
      "grad_norm": 0.1432984620332718,
      "learning_rate": 1.9202083333333334e-05,
      "loss": 0.0028,
      "step": 147830
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.0861063152551651,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0022,
      "step": 147840
    },
    {
      "epoch": 4.928333333333334,
      "grad_norm": 0.28628018498420715,
      "learning_rate": 1.9197916666666668e-05,
      "loss": 0.0013,
      "step": 147850
    },
    {
      "epoch": 4.9286666666666665,
      "grad_norm": 0.21808373928070068,
      "learning_rate": 1.9195833333333333e-05,
      "loss": 0.0015,
      "step": 147860
    },
    {
      "epoch": 4.929,
      "grad_norm": 0.03606919199228287,
      "learning_rate": 1.9193750000000002e-05,
      "loss": 0.0025,
      "step": 147870
    },
    {
      "epoch": 4.929333333333333,
      "grad_norm": 0.3143264353275299,
      "learning_rate": 1.9191666666666668e-05,
      "loss": 0.0021,
      "step": 147880
    },
    {
      "epoch": 4.929666666666667,
      "grad_norm": 0.4002523720264435,
      "learning_rate": 1.9189583333333333e-05,
      "loss": 0.0014,
      "step": 147890
    },
    {
      "epoch": 4.93,
      "grad_norm": 0.3146960437297821,
      "learning_rate": 1.91875e-05,
      "loss": 0.0018,
      "step": 147900
    },
    {
      "epoch": 4.9303333333333335,
      "grad_norm": 0.3147788643836975,
      "learning_rate": 1.9185416666666668e-05,
      "loss": 0.002,
      "step": 147910
    },
    {
      "epoch": 4.930666666666666,
      "grad_norm": 0.058896880596876144,
      "learning_rate": 1.9183333333333333e-05,
      "loss": 0.0017,
      "step": 147920
    },
    {
      "epoch": 4.931,
      "grad_norm": 0.14281921088695526,
      "learning_rate": 1.9181250000000002e-05,
      "loss": 0.0027,
      "step": 147930
    },
    {
      "epoch": 4.931333333333333,
      "grad_norm": 0.08698923885822296,
      "learning_rate": 1.9179166666666668e-05,
      "loss": 0.0018,
      "step": 147940
    },
    {
      "epoch": 4.931666666666667,
      "grad_norm": 0.4487856924533844,
      "learning_rate": 1.9177083333333333e-05,
      "loss": 0.002,
      "step": 147950
    },
    {
      "epoch": 4.932,
      "grad_norm": 0.14370162785053253,
      "learning_rate": 1.9175000000000002e-05,
      "loss": 0.0026,
      "step": 147960
    },
    {
      "epoch": 4.932333333333333,
      "grad_norm": 0.31453147530555725,
      "learning_rate": 1.9172916666666667e-05,
      "loss": 0.0019,
      "step": 147970
    },
    {
      "epoch": 4.932666666666667,
      "grad_norm": 0.37163639068603516,
      "learning_rate": 1.9170833333333336e-05,
      "loss": 0.0013,
      "step": 147980
    },
    {
      "epoch": 4.933,
      "grad_norm": 0.14245492219924927,
      "learning_rate": 1.9168750000000002e-05,
      "loss": 0.0017,
      "step": 147990
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.05792961269617081,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0021,
      "step": 148000
    },
    {
      "epoch": 4.933666666666666,
      "grad_norm": 0.057708267122507095,
      "learning_rate": 1.9164583333333333e-05,
      "loss": 0.0015,
      "step": 148010
    },
    {
      "epoch": 4.934,
      "grad_norm": 0.48619306087493896,
      "learning_rate": 1.9162499999999998e-05,
      "loss": 0.0019,
      "step": 148020
    },
    {
      "epoch": 4.934333333333333,
      "grad_norm": 0.22966724634170532,
      "learning_rate": 1.9160416666666667e-05,
      "loss": 0.002,
      "step": 148030
    },
    {
      "epoch": 4.934666666666667,
      "grad_norm": 0.03164820000529289,
      "learning_rate": 1.9158333333333333e-05,
      "loss": 0.002,
      "step": 148040
    },
    {
      "epoch": 4.9350000000000005,
      "grad_norm": 0.28628113865852356,
      "learning_rate": 1.915625e-05,
      "loss": 0.0015,
      "step": 148050
    },
    {
      "epoch": 4.935333333333333,
      "grad_norm": 0.28603148460388184,
      "learning_rate": 1.9154166666666667e-05,
      "loss": 0.0027,
      "step": 148060
    },
    {
      "epoch": 4.935666666666666,
      "grad_norm": 0.25763601064682007,
      "learning_rate": 1.9152083333333336e-05,
      "loss": 0.0021,
      "step": 148070
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.3142690360546112,
      "learning_rate": 1.915e-05,
      "loss": 0.0027,
      "step": 148080
    },
    {
      "epoch": 4.936333333333334,
      "grad_norm": 0.17298007011413574,
      "learning_rate": 1.9147916666666667e-05,
      "loss": 0.0014,
      "step": 148090
    },
    {
      "epoch": 4.9366666666666665,
      "grad_norm": 0.08611449599266052,
      "learning_rate": 1.9145833333333336e-05,
      "loss": 0.002,
      "step": 148100
    },
    {
      "epoch": 4.937,
      "grad_norm": 0.11472457647323608,
      "learning_rate": 1.914375e-05,
      "loss": 0.0015,
      "step": 148110
    },
    {
      "epoch": 4.937333333333333,
      "grad_norm": 0.1147841140627861,
      "learning_rate": 1.914166666666667e-05,
      "loss": 0.0016,
      "step": 148120
    },
    {
      "epoch": 4.937666666666667,
      "grad_norm": 0.2293253093957901,
      "learning_rate": 1.9139583333333332e-05,
      "loss": 0.0019,
      "step": 148130
    },
    {
      "epoch": 4.938,
      "grad_norm": 0.40008312463760376,
      "learning_rate": 1.91375e-05,
      "loss": 0.0015,
      "step": 148140
    },
    {
      "epoch": 4.9383333333333335,
      "grad_norm": 0.1142493486404419,
      "learning_rate": 1.9135416666666666e-05,
      "loss": 0.0018,
      "step": 148150
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.17190511524677277,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0014,
      "step": 148160
    },
    {
      "epoch": 4.939,
      "grad_norm": 0.10274391621351242,
      "learning_rate": 1.913125e-05,
      "loss": 0.0018,
      "step": 148170
    },
    {
      "epoch": 4.939333333333334,
      "grad_norm": 0.14369069039821625,
      "learning_rate": 1.9129166666666666e-05,
      "loss": 0.0019,
      "step": 148180
    },
    {
      "epoch": 4.939666666666667,
      "grad_norm": 0.14298498630523682,
      "learning_rate": 1.9127083333333335e-05,
      "loss": 0.0019,
      "step": 148190
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.030481694266200066,
      "learning_rate": 1.9125e-05,
      "loss": 0.0016,
      "step": 148200
    },
    {
      "epoch": 4.940333333333333,
      "grad_norm": 0.06127581000328064,
      "learning_rate": 1.912291666666667e-05,
      "loss": 0.0021,
      "step": 148210
    },
    {
      "epoch": 4.940666666666667,
      "grad_norm": 0.08638361096382141,
      "learning_rate": 1.9120833333333335e-05,
      "loss": 0.0014,
      "step": 148220
    },
    {
      "epoch": 4.941,
      "grad_norm": 0.11460225284099579,
      "learning_rate": 1.911875e-05,
      "loss": 0.0024,
      "step": 148230
    },
    {
      "epoch": 4.941333333333334,
      "grad_norm": 0.14272168278694153,
      "learning_rate": 1.911666666666667e-05,
      "loss": 0.0015,
      "step": 148240
    },
    {
      "epoch": 4.941666666666666,
      "grad_norm": 0.17210747301578522,
      "learning_rate": 1.911458333333333e-05,
      "loss": 0.0014,
      "step": 148250
    },
    {
      "epoch": 4.942,
      "grad_norm": 0.08653362095355988,
      "learning_rate": 1.91125e-05,
      "loss": 0.0025,
      "step": 148260
    },
    {
      "epoch": 4.942333333333333,
      "grad_norm": 0.28630220890045166,
      "learning_rate": 1.9110416666666666e-05,
      "loss": 0.0021,
      "step": 148270
    },
    {
      "epoch": 4.942666666666667,
      "grad_norm": 0.11713508516550064,
      "learning_rate": 1.9108333333333335e-05,
      "loss": 0.0019,
      "step": 148280
    },
    {
      "epoch": 4.943,
      "grad_norm": 0.31622350215911865,
      "learning_rate": 1.910625e-05,
      "loss": 0.0025,
      "step": 148290
    },
    {
      "epoch": 4.943333333333333,
      "grad_norm": 0.14331693947315216,
      "learning_rate": 1.910416666666667e-05,
      "loss": 0.0024,
      "step": 148300
    },
    {
      "epoch": 4.943666666666667,
      "grad_norm": 0.43944424390792847,
      "learning_rate": 1.9102083333333335e-05,
      "loss": 0.0017,
      "step": 148310
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.029746536165475845,
      "learning_rate": 1.91e-05,
      "loss": 0.0017,
      "step": 148320
    },
    {
      "epoch": 4.944333333333334,
      "grad_norm": 0.3430496156215668,
      "learning_rate": 1.909791666666667e-05,
      "loss": 0.0024,
      "step": 148330
    },
    {
      "epoch": 4.9446666666666665,
      "grad_norm": 0.14296776056289673,
      "learning_rate": 1.9095833333333334e-05,
      "loss": 0.002,
      "step": 148340
    },
    {
      "epoch": 4.945,
      "grad_norm": 0.4288506805896759,
      "learning_rate": 1.9093750000000003e-05,
      "loss": 0.0022,
      "step": 148350
    },
    {
      "epoch": 4.945333333333333,
      "grad_norm": 0.08670103549957275,
      "learning_rate": 1.909166666666667e-05,
      "loss": 0.0016,
      "step": 148360
    },
    {
      "epoch": 4.945666666666667,
      "grad_norm": 0.42832592129707336,
      "learning_rate": 1.9089583333333334e-05,
      "loss": 0.0025,
      "step": 148370
    },
    {
      "epoch": 4.946,
      "grad_norm": 0.2861235439777374,
      "learning_rate": 1.90875e-05,
      "loss": 0.0018,
      "step": 148380
    },
    {
      "epoch": 4.9463333333333335,
      "grad_norm": 0.2574337422847748,
      "learning_rate": 1.9085416666666665e-05,
      "loss": 0.0022,
      "step": 148390
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.057586535811424255,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.0017,
      "step": 148400
    },
    {
      "epoch": 4.947,
      "grad_norm": 0.058519281446933746,
      "learning_rate": 1.908125e-05,
      "loss": 0.0016,
      "step": 148410
    },
    {
      "epoch": 4.947333333333333,
      "grad_norm": 0.17273949086666107,
      "learning_rate": 1.907916666666667e-05,
      "loss": 0.0021,
      "step": 148420
    },
    {
      "epoch": 4.947666666666667,
      "grad_norm": 0.057690080255270004,
      "learning_rate": 1.9077083333333334e-05,
      "loss": 0.0015,
      "step": 148430
    },
    {
      "epoch": 4.948,
      "grad_norm": 0.028908126056194305,
      "learning_rate": 1.9075000000000003e-05,
      "loss": 0.0016,
      "step": 148440
    },
    {
      "epoch": 4.948333333333333,
      "grad_norm": 0.17141328752040863,
      "learning_rate": 1.907291666666667e-05,
      "loss": 0.0019,
      "step": 148450
    },
    {
      "epoch": 4.948666666666667,
      "grad_norm": 0.14393223822116852,
      "learning_rate": 1.9070833333333334e-05,
      "loss": 0.0016,
      "step": 148460
    },
    {
      "epoch": 4.949,
      "grad_norm": 0.029099054634571075,
      "learning_rate": 1.9068750000000003e-05,
      "loss": 0.0013,
      "step": 148470
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.4285694658756256,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.003,
      "step": 148480
    },
    {
      "epoch": 4.949666666666666,
      "grad_norm": 0.08680311590433121,
      "learning_rate": 1.9064583333333334e-05,
      "loss": 0.0021,
      "step": 148490
    },
    {
      "epoch": 4.95,
      "grad_norm": 0.2004234343767166,
      "learning_rate": 1.90625e-05,
      "loss": 0.0017,
      "step": 148500
    },
    {
      "epoch": 4.950333333333333,
      "grad_norm": 0.029049811884760857,
      "learning_rate": 1.9060416666666668e-05,
      "loss": 0.0016,
      "step": 148510
    },
    {
      "epoch": 4.950666666666667,
      "grad_norm": 0.0578857846558094,
      "learning_rate": 1.9058333333333334e-05,
      "loss": 0.0023,
      "step": 148520
    },
    {
      "epoch": 4.951,
      "grad_norm": 0.04415151849389076,
      "learning_rate": 1.905625e-05,
      "loss": 0.001,
      "step": 148530
    },
    {
      "epoch": 4.951333333333333,
      "grad_norm": 0.22923053801059723,
      "learning_rate": 1.9054166666666668e-05,
      "loss": 0.0021,
      "step": 148540
    },
    {
      "epoch": 4.951666666666666,
      "grad_norm": 0.05881105735898018,
      "learning_rate": 1.9052083333333333e-05,
      "loss": 0.0018,
      "step": 148550
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.06625756621360779,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 0.0014,
      "step": 148560
    },
    {
      "epoch": 4.952333333333334,
      "grad_norm": 0.11486044526100159,
      "learning_rate": 1.9047916666666668e-05,
      "loss": 0.0016,
      "step": 148570
    },
    {
      "epoch": 4.9526666666666666,
      "grad_norm": 0.033329084515571594,
      "learning_rate": 1.9045833333333337e-05,
      "loss": 0.0013,
      "step": 148580
    },
    {
      "epoch": 4.953,
      "grad_norm": 0.5436131954193115,
      "learning_rate": 1.9043750000000002e-05,
      "loss": 0.0024,
      "step": 148590
    },
    {
      "epoch": 4.953333333333333,
      "grad_norm": 0.17170217633247375,
      "learning_rate": 1.9041666666666668e-05,
      "loss": 0.0017,
      "step": 148600
    },
    {
      "epoch": 4.953666666666667,
      "grad_norm": 0.514747142791748,
      "learning_rate": 1.9039583333333333e-05,
      "loss": 0.0021,
      "step": 148610
    },
    {
      "epoch": 4.954,
      "grad_norm": 0.17241428792476654,
      "learning_rate": 1.90375e-05,
      "loss": 0.0014,
      "step": 148620
    },
    {
      "epoch": 4.9543333333333335,
      "grad_norm": 0.38823452591896057,
      "learning_rate": 1.9035416666666667e-05,
      "loss": 0.002,
      "step": 148630
    },
    {
      "epoch": 4.954666666666666,
      "grad_norm": 0.20155662298202515,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0021,
      "step": 148640
    },
    {
      "epoch": 4.955,
      "grad_norm": 0.2990223467350006,
      "learning_rate": 1.9031250000000002e-05,
      "loss": 0.0015,
      "step": 148650
    },
    {
      "epoch": 4.955333333333334,
      "grad_norm": 0.029961593449115753,
      "learning_rate": 1.9029166666666667e-05,
      "loss": 0.0014,
      "step": 148660
    },
    {
      "epoch": 4.955666666666667,
      "grad_norm": 0.06011507660150528,
      "learning_rate": 1.9027083333333333e-05,
      "loss": 0.0017,
      "step": 148670
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 0.14315906167030334,
      "learning_rate": 1.9025e-05,
      "loss": 0.0011,
      "step": 148680
    },
    {
      "epoch": 4.956333333333333,
      "grad_norm": 0.2872892916202545,
      "learning_rate": 1.9022916666666667e-05,
      "loss": 0.0022,
      "step": 148690
    },
    {
      "epoch": 4.956666666666667,
      "grad_norm": 0.4003221392631531,
      "learning_rate": 1.9020833333333336e-05,
      "loss": 0.0015,
      "step": 148700
    },
    {
      "epoch": 4.957,
      "grad_norm": 0.23540538549423218,
      "learning_rate": 1.901875e-05,
      "loss": 0.0013,
      "step": 148710
    },
    {
      "epoch": 4.957333333333334,
      "grad_norm": 0.008325104601681232,
      "learning_rate": 1.901666666666667e-05,
      "loss": 0.0017,
      "step": 148720
    },
    {
      "epoch": 4.957666666666666,
      "grad_norm": 0.22992469370365143,
      "learning_rate": 1.9014583333333332e-05,
      "loss": 0.0013,
      "step": 148730
    },
    {
      "epoch": 4.958,
      "grad_norm": 0.1719924509525299,
      "learning_rate": 1.90125e-05,
      "loss": 0.0023,
      "step": 148740
    },
    {
      "epoch": 4.958333333333333,
      "grad_norm": 0.153015598654747,
      "learning_rate": 1.9010416666666667e-05,
      "loss": 0.0022,
      "step": 148750
    },
    {
      "epoch": 4.958666666666667,
      "grad_norm": 0.360294371843338,
      "learning_rate": 1.9008333333333332e-05,
      "loss": 0.0017,
      "step": 148760
    },
    {
      "epoch": 4.959,
      "grad_norm": 0.010770515538752079,
      "learning_rate": 1.900625e-05,
      "loss": 0.0017,
      "step": 148770
    },
    {
      "epoch": 4.959333333333333,
      "grad_norm": 0.1425490528345108,
      "learning_rate": 1.9004166666666667e-05,
      "loss": 0.0024,
      "step": 148780
    },
    {
      "epoch": 4.959666666666667,
      "grad_norm": 0.14292964339256287,
      "learning_rate": 1.9002083333333336e-05,
      "loss": 0.0021,
      "step": 148790
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.14293362200260162,
      "learning_rate": 1.9e-05,
      "loss": 0.0025,
      "step": 148800
    },
    {
      "epoch": 4.960333333333334,
      "grad_norm": 0.08694858103990555,
      "learning_rate": 1.8997916666666667e-05,
      "loss": 0.0017,
      "step": 148810
    },
    {
      "epoch": 4.960666666666667,
      "grad_norm": 0.14261317253112793,
      "learning_rate": 1.8995833333333335e-05,
      "loss": 0.0021,
      "step": 148820
    },
    {
      "epoch": 4.961,
      "grad_norm": 0.318886935710907,
      "learning_rate": 1.899375e-05,
      "loss": 0.0015,
      "step": 148830
    },
    {
      "epoch": 4.961333333333333,
      "grad_norm": 0.28572574257850647,
      "learning_rate": 1.899166666666667e-05,
      "loss": 0.0021,
      "step": 148840
    },
    {
      "epoch": 4.961666666666667,
      "grad_norm": 0.18698497116565704,
      "learning_rate": 1.8989583333333335e-05,
      "loss": 0.0024,
      "step": 148850
    },
    {
      "epoch": 4.962,
      "grad_norm": 0.11485599726438522,
      "learning_rate": 1.89875e-05,
      "loss": 0.0019,
      "step": 148860
    },
    {
      "epoch": 4.9623333333333335,
      "grad_norm": 0.4092344045639038,
      "learning_rate": 1.8985416666666666e-05,
      "loss": 0.0023,
      "step": 148870
    },
    {
      "epoch": 4.962666666666666,
      "grad_norm": 0.058163486421108246,
      "learning_rate": 1.8983333333333335e-05,
      "loss": 0.0019,
      "step": 148880
    },
    {
      "epoch": 4.963,
      "grad_norm": 0.08706822246313095,
      "learning_rate": 1.898125e-05,
      "loss": 0.0018,
      "step": 148890
    },
    {
      "epoch": 4.963333333333333,
      "grad_norm": 0.11456801742315292,
      "learning_rate": 1.8979166666666666e-05,
      "loss": 0.0022,
      "step": 148900
    },
    {
      "epoch": 4.963666666666667,
      "grad_norm": 0.11477441340684891,
      "learning_rate": 1.8977083333333335e-05,
      "loss": 0.0019,
      "step": 148910
    },
    {
      "epoch": 4.964,
      "grad_norm": 0.11460530757904053,
      "learning_rate": 1.8975e-05,
      "loss": 0.0028,
      "step": 148920
    },
    {
      "epoch": 4.964333333333333,
      "grad_norm": 0.14322476089000702,
      "learning_rate": 1.897291666666667e-05,
      "loss": 0.0028,
      "step": 148930
    },
    {
      "epoch": 4.964666666666667,
      "grad_norm": 0.17712587118148804,
      "learning_rate": 1.8970833333333335e-05,
      "loss": 0.0024,
      "step": 148940
    },
    {
      "epoch": 4.965,
      "grad_norm": 0.08577857911586761,
      "learning_rate": 1.896875e-05,
      "loss": 0.0018,
      "step": 148950
    },
    {
      "epoch": 4.965333333333334,
      "grad_norm": 0.030063239857554436,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0016,
      "step": 148960
    },
    {
      "epoch": 4.9656666666666665,
      "grad_norm": 0.20163260400295258,
      "learning_rate": 1.8964583333333335e-05,
      "loss": 0.0019,
      "step": 148970
    },
    {
      "epoch": 4.966,
      "grad_norm": 0.17234264314174652,
      "learning_rate": 1.89625e-05,
      "loss": 0.0019,
      "step": 148980
    },
    {
      "epoch": 4.966333333333333,
      "grad_norm": 0.030998002737760544,
      "learning_rate": 1.8960416666666666e-05,
      "loss": 0.002,
      "step": 148990
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 0.4185301661491394,
      "learning_rate": 1.8958333333333334e-05,
      "loss": 0.0019,
      "step": 149000
    },
    {
      "epoch": 4.967,
      "grad_norm": 0.20036576688289642,
      "learning_rate": 1.895625e-05,
      "loss": 0.0018,
      "step": 149010
    },
    {
      "epoch": 4.967333333333333,
      "grad_norm": 0.2289799004793167,
      "learning_rate": 1.895416666666667e-05,
      "loss": 0.0013,
      "step": 149020
    },
    {
      "epoch": 4.967666666666666,
      "grad_norm": 0.22881227731704712,
      "learning_rate": 1.8952083333333334e-05,
      "loss": 0.003,
      "step": 149030
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.2859703004360199,
      "learning_rate": 1.895e-05,
      "loss": 0.0013,
      "step": 149040
    },
    {
      "epoch": 4.968333333333334,
      "grad_norm": 0.25690481066703796,
      "learning_rate": 1.894791666666667e-05,
      "loss": 0.0019,
      "step": 149050
    },
    {
      "epoch": 4.968666666666667,
      "grad_norm": 0.3713604211807251,
      "learning_rate": 1.8945833333333334e-05,
      "loss": 0.0018,
      "step": 149060
    },
    {
      "epoch": 4.969,
      "grad_norm": 0.11464177817106247,
      "learning_rate": 1.8943750000000003e-05,
      "loss": 0.0017,
      "step": 149070
    },
    {
      "epoch": 4.969333333333333,
      "grad_norm": 0.371610552072525,
      "learning_rate": 1.894166666666667e-05,
      "loss": 0.0019,
      "step": 149080
    },
    {
      "epoch": 4.969666666666667,
      "grad_norm": 0.029423357918858528,
      "learning_rate": 1.8939583333333334e-05,
      "loss": 0.0018,
      "step": 149090
    },
    {
      "epoch": 4.97,
      "grad_norm": 0.14278154075145721,
      "learning_rate": 1.89375e-05,
      "loss": 0.0019,
      "step": 149100
    },
    {
      "epoch": 4.9703333333333335,
      "grad_norm": 0.057254742830991745,
      "learning_rate": 1.8935416666666665e-05,
      "loss": 0.0015,
      "step": 149110
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.05844457074999809,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0016,
      "step": 149120
    },
    {
      "epoch": 4.971,
      "grad_norm": 0.3412208557128906,
      "learning_rate": 1.893125e-05,
      "loss": 0.0027,
      "step": 149130
    },
    {
      "epoch": 4.971333333333334,
      "grad_norm": 0.17660853266716003,
      "learning_rate": 1.8929166666666668e-05,
      "loss": 0.0013,
      "step": 149140
    },
    {
      "epoch": 4.971666666666667,
      "grad_norm": 0.34458908438682556,
      "learning_rate": 1.8927083333333334e-05,
      "loss": 0.0019,
      "step": 149150
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 0.5355309247970581,
      "learning_rate": 1.8925000000000003e-05,
      "loss": 0.0026,
      "step": 149160
    },
    {
      "epoch": 4.972333333333333,
      "grad_norm": 0.11538942903280258,
      "learning_rate": 1.8922916666666668e-05,
      "loss": 0.0021,
      "step": 149170
    },
    {
      "epoch": 4.972666666666667,
      "grad_norm": 0.2582378089427948,
      "learning_rate": 1.8920833333333334e-05,
      "loss": 0.0013,
      "step": 149180
    },
    {
      "epoch": 4.973,
      "grad_norm": 0.11498997360467911,
      "learning_rate": 1.8918750000000002e-05,
      "loss": 0.0018,
      "step": 149190
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.228539377450943,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.0016,
      "step": 149200
    },
    {
      "epoch": 4.9736666666666665,
      "grad_norm": 0.28601527214050293,
      "learning_rate": 1.8914583333333337e-05,
      "loss": 0.0021,
      "step": 149210
    },
    {
      "epoch": 4.974,
      "grad_norm": 0.20050613582134247,
      "learning_rate": 1.89125e-05,
      "loss": 0.0014,
      "step": 149220
    },
    {
      "epoch": 4.974333333333333,
      "grad_norm": 0.031050635501742363,
      "learning_rate": 1.8910416666666668e-05,
      "loss": 0.0017,
      "step": 149230
    },
    {
      "epoch": 4.974666666666667,
      "grad_norm": 0.11454357951879501,
      "learning_rate": 1.8908333333333333e-05,
      "loss": 0.0016,
      "step": 149240
    },
    {
      "epoch": 4.975,
      "grad_norm": 0.030264003202319145,
      "learning_rate": 1.890625e-05,
      "loss": 0.0022,
      "step": 149250
    },
    {
      "epoch": 4.975333333333333,
      "grad_norm": 0.31478068232536316,
      "learning_rate": 1.8904166666666668e-05,
      "loss": 0.0015,
      "step": 149260
    },
    {
      "epoch": 4.975666666666667,
      "grad_norm": 0.1721537560224533,
      "learning_rate": 1.8902083333333333e-05,
      "loss": 0.0015,
      "step": 149270
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.05785415694117546,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.002,
      "step": 149280
    },
    {
      "epoch": 4.976333333333334,
      "grad_norm": 0.18473288416862488,
      "learning_rate": 1.8897916666666667e-05,
      "loss": 0.0021,
      "step": 149290
    },
    {
      "epoch": 4.976666666666667,
      "grad_norm": 0.01297272089868784,
      "learning_rate": 1.8895833333333336e-05,
      "loss": 0.0018,
      "step": 149300
    },
    {
      "epoch": 4.977,
      "grad_norm": 0.05719425529241562,
      "learning_rate": 1.8893750000000002e-05,
      "loss": 0.0017,
      "step": 149310
    },
    {
      "epoch": 4.977333333333333,
      "grad_norm": 0.058273009955883026,
      "learning_rate": 1.8891666666666667e-05,
      "loss": 0.0025,
      "step": 149320
    },
    {
      "epoch": 4.977666666666667,
      "grad_norm": 0.19973580539226532,
      "learning_rate": 1.8889583333333336e-05,
      "loss": 0.0024,
      "step": 149330
    },
    {
      "epoch": 4.978,
      "grad_norm": 0.2579290568828583,
      "learning_rate": 1.88875e-05,
      "loss": 0.002,
      "step": 149340
    },
    {
      "epoch": 4.9783333333333335,
      "grad_norm": 0.5094255805015564,
      "learning_rate": 1.8885416666666667e-05,
      "loss": 0.0023,
      "step": 149350
    },
    {
      "epoch": 4.978666666666666,
      "grad_norm": 0.20193888247013092,
      "learning_rate": 1.8883333333333333e-05,
      "loss": 0.0018,
      "step": 149360
    },
    {
      "epoch": 4.979,
      "grad_norm": 0.05754593387246132,
      "learning_rate": 1.888125e-05,
      "loss": 0.002,
      "step": 149370
    },
    {
      "epoch": 4.979333333333333,
      "grad_norm": 0.057639509439468384,
      "learning_rate": 1.8879166666666667e-05,
      "loss": 0.0028,
      "step": 149380
    },
    {
      "epoch": 4.979666666666667,
      "grad_norm": 0.11512096971273422,
      "learning_rate": 1.8877083333333333e-05,
      "loss": 0.002,
      "step": 149390
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.08620673418045044,
      "learning_rate": 1.8875e-05,
      "loss": 0.0019,
      "step": 149400
    },
    {
      "epoch": 4.980333333333333,
      "grad_norm": 0.4291585385799408,
      "learning_rate": 1.8872916666666667e-05,
      "loss": 0.0024,
      "step": 149410
    },
    {
      "epoch": 4.980666666666667,
      "grad_norm": 0.02918456308543682,
      "learning_rate": 1.8870833333333336e-05,
      "loss": 0.0019,
      "step": 149420
    },
    {
      "epoch": 4.981,
      "grad_norm": 0.008790471591055393,
      "learning_rate": 1.886875e-05,
      "loss": 0.0013,
      "step": 149430
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.05775788798928261,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0013,
      "step": 149440
    },
    {
      "epoch": 4.9816666666666665,
      "grad_norm": 0.6843652129173279,
      "learning_rate": 1.8864583333333336e-05,
      "loss": 0.002,
      "step": 149450
    },
    {
      "epoch": 4.982,
      "grad_norm": 0.14309880137443542,
      "learning_rate": 1.88625e-05,
      "loss": 0.0011,
      "step": 149460
    },
    {
      "epoch": 4.982333333333333,
      "grad_norm": 0.007887118496000767,
      "learning_rate": 1.8860416666666667e-05,
      "loss": 0.0018,
      "step": 149470
    },
    {
      "epoch": 4.982666666666667,
      "grad_norm": 0.08616283535957336,
      "learning_rate": 1.8858333333333332e-05,
      "loss": 0.0015,
      "step": 149480
    },
    {
      "epoch": 4.983,
      "grad_norm": 0.08639762550592422,
      "learning_rate": 1.885625e-05,
      "loss": 0.0014,
      "step": 149490
    },
    {
      "epoch": 4.983333333333333,
      "grad_norm": 0.1435006707906723,
      "learning_rate": 1.8854166666666666e-05,
      "loss": 0.0012,
      "step": 149500
    },
    {
      "epoch": 4.983666666666666,
      "grad_norm": 0.030742013826966286,
      "learning_rate": 1.8852083333333335e-05,
      "loss": 0.0021,
      "step": 149510
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.08641251176595688,
      "learning_rate": 1.885e-05,
      "loss": 0.0019,
      "step": 149520
    },
    {
      "epoch": 4.984333333333334,
      "grad_norm": 0.14333835244178772,
      "learning_rate": 1.8847916666666666e-05,
      "loss": 0.0022,
      "step": 149530
    },
    {
      "epoch": 4.984666666666667,
      "grad_norm": 0.272604763507843,
      "learning_rate": 1.8845833333333335e-05,
      "loss": 0.0019,
      "step": 149540
    },
    {
      "epoch": 4.985,
      "grad_norm": 0.11465083807706833,
      "learning_rate": 1.884375e-05,
      "loss": 0.0024,
      "step": 149550
    },
    {
      "epoch": 4.985333333333333,
      "grad_norm": 0.45744457840919495,
      "learning_rate": 1.884166666666667e-05,
      "loss": 0.0023,
      "step": 149560
    },
    {
      "epoch": 4.985666666666667,
      "grad_norm": 0.1144644245505333,
      "learning_rate": 1.8839583333333335e-05,
      "loss": 0.0017,
      "step": 149570
    },
    {
      "epoch": 4.986,
      "grad_norm": 0.31416574120521545,
      "learning_rate": 1.88375e-05,
      "loss": 0.0026,
      "step": 149580
    },
    {
      "epoch": 4.9863333333333335,
      "grad_norm": 0.1714925467967987,
      "learning_rate": 1.8835416666666666e-05,
      "loss": 0.0022,
      "step": 149590
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.19994492828845978,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0014,
      "step": 149600
    },
    {
      "epoch": 4.987,
      "grad_norm": 0.14359208941459656,
      "learning_rate": 1.883125e-05,
      "loss": 0.002,
      "step": 149610
    },
    {
      "epoch": 4.987333333333333,
      "grad_norm": 0.09641878306865692,
      "learning_rate": 1.8829166666666666e-05,
      "loss": 0.0022,
      "step": 149620
    },
    {
      "epoch": 4.987666666666667,
      "grad_norm": 0.2558421790599823,
      "learning_rate": 1.8827083333333335e-05,
      "loss": 0.0017,
      "step": 149630
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 0.006439007353037596,
      "learning_rate": 1.8825e-05,
      "loss": 0.002,
      "step": 149640
    },
    {
      "epoch": 4.988333333333333,
      "grad_norm": 0.15097948908805847,
      "learning_rate": 1.882291666666667e-05,
      "loss": 0.002,
      "step": 149650
    },
    {
      "epoch": 4.988666666666667,
      "grad_norm": 0.14332763850688934,
      "learning_rate": 1.8820833333333335e-05,
      "loss": 0.0019,
      "step": 149660
    },
    {
      "epoch": 4.989,
      "grad_norm": 0.4416796863079071,
      "learning_rate": 1.881875e-05,
      "loss": 0.0015,
      "step": 149670
    },
    {
      "epoch": 4.989333333333334,
      "grad_norm": 0.0858193039894104,
      "learning_rate": 1.881666666666667e-05,
      "loss": 0.0018,
      "step": 149680
    },
    {
      "epoch": 4.9896666666666665,
      "grad_norm": 0.2856796383857727,
      "learning_rate": 1.8814583333333334e-05,
      "loss": 0.0018,
      "step": 149690
    },
    {
      "epoch": 4.99,
      "grad_norm": 0.143681600689888,
      "learning_rate": 1.88125e-05,
      "loss": 0.0021,
      "step": 149700
    },
    {
      "epoch": 4.990333333333333,
      "grad_norm": 0.22888964414596558,
      "learning_rate": 1.8810416666666665e-05,
      "loss": 0.0017,
      "step": 149710
    },
    {
      "epoch": 4.990666666666667,
      "grad_norm": 0.2576204836368561,
      "learning_rate": 1.8808333333333334e-05,
      "loss": 0.0024,
      "step": 149720
    },
    {
      "epoch": 4.991,
      "grad_norm": 0.11472681164741516,
      "learning_rate": 1.880625e-05,
      "loss": 0.0021,
      "step": 149730
    },
    {
      "epoch": 4.991333333333333,
      "grad_norm": 0.285735547542572,
      "learning_rate": 1.880416666666667e-05,
      "loss": 0.0016,
      "step": 149740
    },
    {
      "epoch": 4.991666666666667,
      "grad_norm": 0.25779518485069275,
      "learning_rate": 1.8802083333333334e-05,
      "loss": 0.0023,
      "step": 149750
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.08629079908132553,
      "learning_rate": 1.88e-05,
      "loss": 0.002,
      "step": 149760
    },
    {
      "epoch": 4.992333333333333,
      "grad_norm": 0.17212612926959991,
      "learning_rate": 1.879791666666667e-05,
      "loss": 0.0019,
      "step": 149770
    },
    {
      "epoch": 4.992666666666667,
      "grad_norm": 0.2283250093460083,
      "learning_rate": 1.8795833333333334e-05,
      "loss": 0.0015,
      "step": 149780
    },
    {
      "epoch": 4.993,
      "grad_norm": 0.014513805508613586,
      "learning_rate": 1.8793750000000003e-05,
      "loss": 0.0019,
      "step": 149790
    },
    {
      "epoch": 4.993333333333333,
      "grad_norm": 0.14427538216114044,
      "learning_rate": 1.8791666666666668e-05,
      "loss": 0.0023,
      "step": 149800
    },
    {
      "epoch": 4.993666666666667,
      "grad_norm": 0.20024950802326202,
      "learning_rate": 1.8789583333333334e-05,
      "loss": 0.0017,
      "step": 149810
    },
    {
      "epoch": 4.994,
      "grad_norm": 0.11448255181312561,
      "learning_rate": 1.87875e-05,
      "loss": 0.0019,
      "step": 149820
    },
    {
      "epoch": 4.9943333333333335,
      "grad_norm": 0.19989155232906342,
      "learning_rate": 1.8785416666666665e-05,
      "loss": 0.0015,
      "step": 149830
    },
    {
      "epoch": 4.994666666666666,
      "grad_norm": 0.2914851903915405,
      "learning_rate": 1.8783333333333334e-05,
      "loss": 0.0023,
      "step": 149840
    },
    {
      "epoch": 4.995,
      "grad_norm": 0.2863803505897522,
      "learning_rate": 1.878125e-05,
      "loss": 0.0021,
      "step": 149850
    },
    {
      "epoch": 4.995333333333333,
      "grad_norm": 0.3716038167476654,
      "learning_rate": 1.8779166666666668e-05,
      "loss": 0.0021,
      "step": 149860
    },
    {
      "epoch": 4.995666666666667,
      "grad_norm": 0.08677245676517487,
      "learning_rate": 1.8777083333333333e-05,
      "loss": 0.0019,
      "step": 149870
    },
    {
      "epoch": 4.996,
      "grad_norm": 0.11519642919301987,
      "learning_rate": 1.8775000000000002e-05,
      "loss": 0.0017,
      "step": 149880
    },
    {
      "epoch": 4.996333333333333,
      "grad_norm": 0.14358007907867432,
      "learning_rate": 1.8772916666666668e-05,
      "loss": 0.0016,
      "step": 149890
    },
    {
      "epoch": 4.996666666666667,
      "grad_norm": 0.20076484978199005,
      "learning_rate": 1.8770833333333333e-05,
      "loss": 0.0016,
      "step": 149900
    },
    {
      "epoch": 4.997,
      "grad_norm": 0.05937850847840309,
      "learning_rate": 1.8768750000000002e-05,
      "loss": 0.002,
      "step": 149910
    },
    {
      "epoch": 4.997333333333334,
      "grad_norm": 0.09129400551319122,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.0018,
      "step": 149920
    },
    {
      "epoch": 4.9976666666666665,
      "grad_norm": 0.05920061096549034,
      "learning_rate": 1.8764583333333337e-05,
      "loss": 0.0019,
      "step": 149930
    },
    {
      "epoch": 4.998,
      "grad_norm": 0.08657712489366531,
      "learning_rate": 1.87625e-05,
      "loss": 0.0022,
      "step": 149940
    },
    {
      "epoch": 4.998333333333333,
      "grad_norm": 0.08599445223808289,
      "learning_rate": 1.8760416666666668e-05,
      "loss": 0.0028,
      "step": 149950
    },
    {
      "epoch": 4.998666666666667,
      "grad_norm": 0.029767503961920738,
      "learning_rate": 1.8758333333333333e-05,
      "loss": 0.0015,
      "step": 149960
    },
    {
      "epoch": 4.999,
      "grad_norm": 0.14373338222503662,
      "learning_rate": 1.8756250000000002e-05,
      "loss": 0.0016,
      "step": 149970
    },
    {
      "epoch": 4.999333333333333,
      "grad_norm": 0.17235517501831055,
      "learning_rate": 1.8754166666666667e-05,
      "loss": 0.0018,
      "step": 149980
    },
    {
      "epoch": 4.999666666666666,
      "grad_norm": 0.5226857662200928,
      "learning_rate": 1.8752083333333333e-05,
      "loss": 0.0017,
      "step": 149990
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.1159883514046669,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.0017,
      "step": 150000
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.001862390898168087,
      "eval_runtime": 130.1655,
      "eval_samples_per_second": 1536.506,
      "eval_steps_per_second": 38.413,
      "step": 150000
    },
    {
      "epoch": 5.000333333333334,
      "grad_norm": 0.25742974877357483,
      "learning_rate": 1.8747916666666667e-05,
      "loss": 0.0014,
      "step": 150010
    },
    {
      "epoch": 5.000666666666667,
      "grad_norm": 0.05738663300871849,
      "learning_rate": 1.8745833333333336e-05,
      "loss": 0.0015,
      "step": 150020
    },
    {
      "epoch": 5.001,
      "grad_norm": 0.42844581604003906,
      "learning_rate": 1.874375e-05,
      "loss": 0.0015,
      "step": 150030
    },
    {
      "epoch": 5.001333333333333,
      "grad_norm": 0.036751482635736465,
      "learning_rate": 1.8741666666666667e-05,
      "loss": 0.0023,
      "step": 150040
    },
    {
      "epoch": 5.001666666666667,
      "grad_norm": 0.2010982483625412,
      "learning_rate": 1.8739583333333336e-05,
      "loss": 0.0025,
      "step": 150050
    },
    {
      "epoch": 5.002,
      "grad_norm": 0.3495792746543884,
      "learning_rate": 1.87375e-05,
      "loss": 0.0016,
      "step": 150060
    },
    {
      "epoch": 5.0023333333333335,
      "grad_norm": 0.05753815174102783,
      "learning_rate": 1.8735416666666667e-05,
      "loss": 0.0017,
      "step": 150070
    },
    {
      "epoch": 5.002666666666666,
      "grad_norm": 0.16243354976177216,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0013,
      "step": 150080
    },
    {
      "epoch": 5.003,
      "grad_norm": 0.25997793674468994,
      "learning_rate": 1.873125e-05,
      "loss": 0.0022,
      "step": 150090
    },
    {
      "epoch": 5.003333333333333,
      "grad_norm": 0.029589515179395676,
      "learning_rate": 1.8729166666666667e-05,
      "loss": 0.0023,
      "step": 150100
    },
    {
      "epoch": 5.003666666666667,
      "grad_norm": 0.17231030762195587,
      "learning_rate": 1.8727083333333336e-05,
      "loss": 0.0019,
      "step": 150110
    },
    {
      "epoch": 5.004,
      "grad_norm": 0.4577082693576813,
      "learning_rate": 1.8725e-05,
      "loss": 0.0021,
      "step": 150120
    },
    {
      "epoch": 5.004333333333333,
      "grad_norm": 0.0581083707511425,
      "learning_rate": 1.8722916666666667e-05,
      "loss": 0.0018,
      "step": 150130
    },
    {
      "epoch": 5.004666666666667,
      "grad_norm": 0.3432168960571289,
      "learning_rate": 1.8720833333333335e-05,
      "loss": 0.0018,
      "step": 150140
    },
    {
      "epoch": 5.005,
      "grad_norm": 0.11447025090456009,
      "learning_rate": 1.871875e-05,
      "loss": 0.0017,
      "step": 150150
    },
    {
      "epoch": 5.005333333333334,
      "grad_norm": 0.3160206377506256,
      "learning_rate": 1.871666666666667e-05,
      "loss": 0.0021,
      "step": 150160
    },
    {
      "epoch": 5.0056666666666665,
      "grad_norm": 0.7539482116699219,
      "learning_rate": 1.8714583333333335e-05,
      "loss": 0.0019,
      "step": 150170
    },
    {
      "epoch": 5.006,
      "grad_norm": 0.058902084827423096,
      "learning_rate": 1.87125e-05,
      "loss": 0.002,
      "step": 150180
    },
    {
      "epoch": 5.006333333333333,
      "grad_norm": 0.08650717884302139,
      "learning_rate": 1.8710416666666666e-05,
      "loss": 0.0014,
      "step": 150190
    },
    {
      "epoch": 5.006666666666667,
      "grad_norm": 0.1764499843120575,
      "learning_rate": 1.8708333333333332e-05,
      "loss": 0.0016,
      "step": 150200
    },
    {
      "epoch": 5.007,
      "grad_norm": 0.08655819296836853,
      "learning_rate": 1.870625e-05,
      "loss": 0.0022,
      "step": 150210
    },
    {
      "epoch": 5.007333333333333,
      "grad_norm": 0.0864570364356041,
      "learning_rate": 1.8704166666666666e-05,
      "loss": 0.0018,
      "step": 150220
    },
    {
      "epoch": 5.007666666666666,
      "grad_norm": 0.17171956598758698,
      "learning_rate": 1.8702083333333335e-05,
      "loss": 0.0015,
      "step": 150230
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.02953403815627098,
      "learning_rate": 1.87e-05,
      "loss": 0.0022,
      "step": 150240
    },
    {
      "epoch": 5.008333333333334,
      "grad_norm": 0.4862026274204254,
      "learning_rate": 1.869791666666667e-05,
      "loss": 0.0026,
      "step": 150250
    },
    {
      "epoch": 5.008666666666667,
      "grad_norm": 0.20006977021694183,
      "learning_rate": 1.8695833333333335e-05,
      "loss": 0.0022,
      "step": 150260
    },
    {
      "epoch": 5.009,
      "grad_norm": 0.3425401747226715,
      "learning_rate": 1.869375e-05,
      "loss": 0.0014,
      "step": 150270
    },
    {
      "epoch": 5.009333333333333,
      "grad_norm": 0.28532832860946655,
      "learning_rate": 1.869166666666667e-05,
      "loss": 0.0035,
      "step": 150280
    },
    {
      "epoch": 5.009666666666667,
      "grad_norm": 0.25706130266189575,
      "learning_rate": 1.8689583333333335e-05,
      "loss": 0.0027,
      "step": 150290
    },
    {
      "epoch": 5.01,
      "grad_norm": 0.31433242559432983,
      "learning_rate": 1.8687500000000004e-05,
      "loss": 0.0012,
      "step": 150300
    },
    {
      "epoch": 5.0103333333333335,
      "grad_norm": 0.3542454242706299,
      "learning_rate": 1.8685416666666666e-05,
      "loss": 0.0016,
      "step": 150310
    },
    {
      "epoch": 5.010666666666666,
      "grad_norm": 0.40058633685112,
      "learning_rate": 1.8683333333333335e-05,
      "loss": 0.0022,
      "step": 150320
    },
    {
      "epoch": 5.011,
      "grad_norm": 0.20004881918430328,
      "learning_rate": 1.868125e-05,
      "loss": 0.0014,
      "step": 150330
    },
    {
      "epoch": 5.011333333333333,
      "grad_norm": 0.28533169627189636,
      "learning_rate": 1.8679166666666666e-05,
      "loss": 0.0022,
      "step": 150340
    },
    {
      "epoch": 5.011666666666667,
      "grad_norm": 0.030643537640571594,
      "learning_rate": 1.8677083333333334e-05,
      "loss": 0.002,
      "step": 150350
    },
    {
      "epoch": 5.012,
      "grad_norm": 0.029815232381224632,
      "learning_rate": 1.8675e-05,
      "loss": 0.0018,
      "step": 150360
    },
    {
      "epoch": 5.012333333333333,
      "grad_norm": 0.05853992700576782,
      "learning_rate": 1.867291666666667e-05,
      "loss": 0.0026,
      "step": 150370
    },
    {
      "epoch": 5.012666666666667,
      "grad_norm": 0.43384838104248047,
      "learning_rate": 1.8670833333333334e-05,
      "loss": 0.0014,
      "step": 150380
    },
    {
      "epoch": 5.013,
      "grad_norm": 0.14320863783359528,
      "learning_rate": 1.8668750000000003e-05,
      "loss": 0.0016,
      "step": 150390
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 0.17181342840194702,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.002,
      "step": 150400
    },
    {
      "epoch": 5.0136666666666665,
      "grad_norm": 0.03090609237551689,
      "learning_rate": 1.8664583333333334e-05,
      "loss": 0.0014,
      "step": 150410
    },
    {
      "epoch": 5.014,
      "grad_norm": 0.14693956077098846,
      "learning_rate": 1.8662500000000003e-05,
      "loss": 0.0018,
      "step": 150420
    },
    {
      "epoch": 5.014333333333333,
      "grad_norm": 0.4762256145477295,
      "learning_rate": 1.8660416666666665e-05,
      "loss": 0.0022,
      "step": 150430
    },
    {
      "epoch": 5.014666666666667,
      "grad_norm": 0.25744545459747314,
      "learning_rate": 1.8658333333333334e-05,
      "loss": 0.0014,
      "step": 150440
    },
    {
      "epoch": 5.015,
      "grad_norm": 0.40062570571899414,
      "learning_rate": 1.865625e-05,
      "loss": 0.0021,
      "step": 150450
    },
    {
      "epoch": 5.015333333333333,
      "grad_norm": 0.05908317491412163,
      "learning_rate": 1.865416666666667e-05,
      "loss": 0.0016,
      "step": 150460
    },
    {
      "epoch": 5.015666666666666,
      "grad_norm": 0.08643246442079544,
      "learning_rate": 1.8652083333333334e-05,
      "loss": 0.0015,
      "step": 150470
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.11504550278186798,
      "learning_rate": 1.865e-05,
      "loss": 0.0022,
      "step": 150480
    },
    {
      "epoch": 5.016333333333334,
      "grad_norm": 0.3438238799571991,
      "learning_rate": 1.8647916666666668e-05,
      "loss": 0.0018,
      "step": 150490
    },
    {
      "epoch": 5.016666666666667,
      "grad_norm": 0.17178037762641907,
      "learning_rate": 1.8645833333333334e-05,
      "loss": 0.0018,
      "step": 150500
    },
    {
      "epoch": 5.017,
      "grad_norm": 0.14293156564235687,
      "learning_rate": 1.8643750000000003e-05,
      "loss": 0.0031,
      "step": 150510
    },
    {
      "epoch": 5.017333333333333,
      "grad_norm": 0.314454585313797,
      "learning_rate": 1.8641666666666668e-05,
      "loss": 0.0018,
      "step": 150520
    },
    {
      "epoch": 5.017666666666667,
      "grad_norm": 0.314172625541687,
      "learning_rate": 1.8639583333333337e-05,
      "loss": 0.0024,
      "step": 150530
    },
    {
      "epoch": 5.018,
      "grad_norm": 0.2290997952222824,
      "learning_rate": 1.8637500000000002e-05,
      "loss": 0.0014,
      "step": 150540
    },
    {
      "epoch": 5.0183333333333335,
      "grad_norm": 0.11439334601163864,
      "learning_rate": 1.8635416666666668e-05,
      "loss": 0.0016,
      "step": 150550
    },
    {
      "epoch": 5.018666666666666,
      "grad_norm": 0.0587371364235878,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.0017,
      "step": 150560
    },
    {
      "epoch": 5.019,
      "grad_norm": 0.4317725598812103,
      "learning_rate": 1.863125e-05,
      "loss": 0.0017,
      "step": 150570
    },
    {
      "epoch": 5.019333333333333,
      "grad_norm": 0.22860629856586456,
      "learning_rate": 1.8629166666666668e-05,
      "loss": 0.0018,
      "step": 150580
    },
    {
      "epoch": 5.019666666666667,
      "grad_norm": 0.05794231593608856,
      "learning_rate": 1.8627083333333333e-05,
      "loss": 0.0015,
      "step": 150590
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.05881734564900398,
      "learning_rate": 1.8625000000000002e-05,
      "loss": 0.0017,
      "step": 150600
    },
    {
      "epoch": 5.020333333333333,
      "grad_norm": 0.257515549659729,
      "learning_rate": 1.8622916666666668e-05,
      "loss": 0.0031,
      "step": 150610
    },
    {
      "epoch": 5.020666666666667,
      "grad_norm": 0.20030975341796875,
      "learning_rate": 1.8620833333333333e-05,
      "loss": 0.0019,
      "step": 150620
    },
    {
      "epoch": 5.021,
      "grad_norm": 0.05827883258461952,
      "learning_rate": 1.8618750000000002e-05,
      "loss": 0.0015,
      "step": 150630
    },
    {
      "epoch": 5.021333333333334,
      "grad_norm": 0.3429115414619446,
      "learning_rate": 1.8616666666666667e-05,
      "loss": 0.002,
      "step": 150640
    },
    {
      "epoch": 5.0216666666666665,
      "grad_norm": 0.5144523978233337,
      "learning_rate": 1.8614583333333336e-05,
      "loss": 0.0014,
      "step": 150650
    },
    {
      "epoch": 5.022,
      "grad_norm": 0.3144502639770508,
      "learning_rate": 1.8612500000000002e-05,
      "loss": 0.0017,
      "step": 150660
    },
    {
      "epoch": 5.022333333333333,
      "grad_norm": 0.2866433262825012,
      "learning_rate": 1.8610416666666667e-05,
      "loss": 0.0016,
      "step": 150670
    },
    {
      "epoch": 5.022666666666667,
      "grad_norm": 0.03151405602693558,
      "learning_rate": 1.8608333333333333e-05,
      "loss": 0.0014,
      "step": 150680
    },
    {
      "epoch": 5.023,
      "grad_norm": 0.22845308482646942,
      "learning_rate": 1.860625e-05,
      "loss": 0.0021,
      "step": 150690
    },
    {
      "epoch": 5.023333333333333,
      "grad_norm": 0.5868114829063416,
      "learning_rate": 1.8604166666666667e-05,
      "loss": 0.0019,
      "step": 150700
    },
    {
      "epoch": 5.023666666666666,
      "grad_norm": 0.029529672116041183,
      "learning_rate": 1.8602083333333333e-05,
      "loss": 0.0015,
      "step": 150710
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.02958289533853531,
      "learning_rate": 1.86e-05,
      "loss": 0.0019,
      "step": 150720
    },
    {
      "epoch": 5.024333333333334,
      "grad_norm": 0.20012418925762177,
      "learning_rate": 1.8597916666666667e-05,
      "loss": 0.0018,
      "step": 150730
    },
    {
      "epoch": 5.024666666666667,
      "grad_norm": 0.030375929549336433,
      "learning_rate": 1.8595833333333336e-05,
      "loss": 0.0027,
      "step": 150740
    },
    {
      "epoch": 5.025,
      "grad_norm": 0.030366942286491394,
      "learning_rate": 1.859375e-05,
      "loss": 0.002,
      "step": 150750
    },
    {
      "epoch": 5.025333333333333,
      "grad_norm": 0.0863925963640213,
      "learning_rate": 1.8591666666666667e-05,
      "loss": 0.0026,
      "step": 150760
    },
    {
      "epoch": 5.025666666666667,
      "grad_norm": 0.25933367013931274,
      "learning_rate": 1.8589583333333336e-05,
      "loss": 0.0017,
      "step": 150770
    },
    {
      "epoch": 5.026,
      "grad_norm": 0.5716502070426941,
      "learning_rate": 1.85875e-05,
      "loss": 0.0016,
      "step": 150780
    },
    {
      "epoch": 5.0263333333333335,
      "grad_norm": 0.19320939481258392,
      "learning_rate": 1.8585416666666667e-05,
      "loss": 0.0016,
      "step": 150790
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 0.03185514733195305,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.0016,
      "step": 150800
    },
    {
      "epoch": 5.027,
      "grad_norm": 0.1142667755484581,
      "learning_rate": 1.858125e-05,
      "loss": 0.0019,
      "step": 150810
    },
    {
      "epoch": 5.027333333333333,
      "grad_norm": 0.1435905247926712,
      "learning_rate": 1.8579166666666667e-05,
      "loss": 0.0016,
      "step": 150820
    },
    {
      "epoch": 5.027666666666667,
      "grad_norm": 0.03048621118068695,
      "learning_rate": 1.8577083333333335e-05,
      "loss": 0.0025,
      "step": 150830
    },
    {
      "epoch": 5.028,
      "grad_norm": 0.05808757618069649,
      "learning_rate": 1.8575e-05,
      "loss": 0.0014,
      "step": 150840
    },
    {
      "epoch": 5.028333333333333,
      "grad_norm": 0.1156390830874443,
      "learning_rate": 1.8572916666666666e-05,
      "loss": 0.0017,
      "step": 150850
    },
    {
      "epoch": 5.028666666666667,
      "grad_norm": 0.03462807089090347,
      "learning_rate": 1.8570833333333335e-05,
      "loss": 0.0021,
      "step": 150860
    },
    {
      "epoch": 5.029,
      "grad_norm": 0.14321863651275635,
      "learning_rate": 1.856875e-05,
      "loss": 0.0018,
      "step": 150870
    },
    {
      "epoch": 5.029333333333334,
      "grad_norm": 0.14271488785743713,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0017,
      "step": 150880
    },
    {
      "epoch": 5.0296666666666665,
      "grad_norm": 0.14384406805038452,
      "learning_rate": 1.8564583333333335e-05,
      "loss": 0.0026,
      "step": 150890
    },
    {
      "epoch": 5.03,
      "grad_norm": 0.19974936544895172,
      "learning_rate": 1.85625e-05,
      "loss": 0.0013,
      "step": 150900
    },
    {
      "epoch": 5.030333333333333,
      "grad_norm": 0.4841114580631256,
      "learning_rate": 1.8560416666666666e-05,
      "loss": 0.0026,
      "step": 150910
    },
    {
      "epoch": 5.030666666666667,
      "grad_norm": 0.14296278357505798,
      "learning_rate": 1.855833333333333e-05,
      "loss": 0.0015,
      "step": 150920
    },
    {
      "epoch": 5.031,
      "grad_norm": 0.031491365283727646,
      "learning_rate": 1.855625e-05,
      "loss": 0.0021,
      "step": 150930
    },
    {
      "epoch": 5.031333333333333,
      "grad_norm": 0.1714441180229187,
      "learning_rate": 1.8554166666666666e-05,
      "loss": 0.0014,
      "step": 150940
    },
    {
      "epoch": 5.031666666666666,
      "grad_norm": 0.09608887135982513,
      "learning_rate": 1.8552083333333335e-05,
      "loss": 0.0028,
      "step": 150950
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.22883550822734833,
      "learning_rate": 1.855e-05,
      "loss": 0.0017,
      "step": 150960
    },
    {
      "epoch": 5.032333333333334,
      "grad_norm": 0.2019737958908081,
      "learning_rate": 1.854791666666667e-05,
      "loss": 0.0015,
      "step": 150970
    },
    {
      "epoch": 5.032666666666667,
      "grad_norm": 0.1432352215051651,
      "learning_rate": 1.8545833333333335e-05,
      "loss": 0.002,
      "step": 150980
    },
    {
      "epoch": 5.033,
      "grad_norm": 0.29197296500205994,
      "learning_rate": 1.854375e-05,
      "loss": 0.0011,
      "step": 150990
    },
    {
      "epoch": 5.033333333333333,
      "grad_norm": 0.12070979177951813,
      "learning_rate": 1.854166666666667e-05,
      "loss": 0.0024,
      "step": 151000
    },
    {
      "epoch": 5.033666666666667,
      "grad_norm": 0.31439825892448425,
      "learning_rate": 1.8539583333333334e-05,
      "loss": 0.002,
      "step": 151010
    },
    {
      "epoch": 5.034,
      "grad_norm": 0.24531081318855286,
      "learning_rate": 1.8537500000000003e-05,
      "loss": 0.0014,
      "step": 151020
    },
    {
      "epoch": 5.0343333333333335,
      "grad_norm": 0.1552211344242096,
      "learning_rate": 1.8535416666666665e-05,
      "loss": 0.0017,
      "step": 151030
    },
    {
      "epoch": 5.034666666666666,
      "grad_norm": 0.22857719659805298,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0017,
      "step": 151040
    },
    {
      "epoch": 5.035,
      "grad_norm": 0.0982125774025917,
      "learning_rate": 1.853125e-05,
      "loss": 0.002,
      "step": 151050
    },
    {
      "epoch": 5.035333333333333,
      "grad_norm": 0.1395207643508911,
      "learning_rate": 1.8529166666666665e-05,
      "loss": 0.0026,
      "step": 151060
    },
    {
      "epoch": 5.035666666666667,
      "grad_norm": 0.08563803881406784,
      "learning_rate": 1.8527083333333334e-05,
      "loss": 0.0013,
      "step": 151070
    },
    {
      "epoch": 5.036,
      "grad_norm": 0.08709748089313507,
      "learning_rate": 1.8525e-05,
      "loss": 0.0024,
      "step": 151080
    },
    {
      "epoch": 5.036333333333333,
      "grad_norm": 0.14363932609558105,
      "learning_rate": 1.852291666666667e-05,
      "loss": 0.0018,
      "step": 151090
    },
    {
      "epoch": 5.036666666666667,
      "grad_norm": 0.030992742627859116,
      "learning_rate": 1.8520833333333334e-05,
      "loss": 0.0014,
      "step": 151100
    },
    {
      "epoch": 5.037,
      "grad_norm": 0.08627115190029144,
      "learning_rate": 1.8518750000000003e-05,
      "loss": 0.0023,
      "step": 151110
    },
    {
      "epoch": 5.037333333333334,
      "grad_norm": 0.14338266849517822,
      "learning_rate": 1.851666666666667e-05,
      "loss": 0.0017,
      "step": 151120
    },
    {
      "epoch": 5.0376666666666665,
      "grad_norm": 0.2650715708732605,
      "learning_rate": 1.8514583333333334e-05,
      "loss": 0.0016,
      "step": 151130
    },
    {
      "epoch": 5.038,
      "grad_norm": 0.2793710231781006,
      "learning_rate": 1.8512500000000003e-05,
      "loss": 0.0033,
      "step": 151140
    },
    {
      "epoch": 5.038333333333333,
      "grad_norm": 0.2860512435436249,
      "learning_rate": 1.8510416666666665e-05,
      "loss": 0.0019,
      "step": 151150
    },
    {
      "epoch": 5.038666666666667,
      "grad_norm": 0.2573036849498749,
      "learning_rate": 1.8508333333333334e-05,
      "loss": 0.0019,
      "step": 151160
    },
    {
      "epoch": 5.039,
      "grad_norm": 0.14323502779006958,
      "learning_rate": 1.850625e-05,
      "loss": 0.0018,
      "step": 151170
    },
    {
      "epoch": 5.039333333333333,
      "grad_norm": 0.061875201761722565,
      "learning_rate": 1.8504166666666668e-05,
      "loss": 0.0023,
      "step": 151180
    },
    {
      "epoch": 5.039666666666666,
      "grad_norm": 0.25737109780311584,
      "learning_rate": 1.8502083333333334e-05,
      "loss": 0.0015,
      "step": 151190
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.2004825621843338,
      "learning_rate": 1.85e-05,
      "loss": 0.0027,
      "step": 151200
    },
    {
      "epoch": 5.040333333333333,
      "grad_norm": 0.08677610009908676,
      "learning_rate": 1.8497916666666668e-05,
      "loss": 0.0011,
      "step": 151210
    },
    {
      "epoch": 5.040666666666667,
      "grad_norm": 0.011550243012607098,
      "learning_rate": 1.8495833333333333e-05,
      "loss": 0.0018,
      "step": 151220
    },
    {
      "epoch": 5.041,
      "grad_norm": 0.172019824385643,
      "learning_rate": 1.8493750000000002e-05,
      "loss": 0.0021,
      "step": 151230
    },
    {
      "epoch": 5.041333333333333,
      "grad_norm": 0.22847676277160645,
      "learning_rate": 1.8491666666666668e-05,
      "loss": 0.0017,
      "step": 151240
    },
    {
      "epoch": 5.041666666666667,
      "grad_norm": 0.1721390187740326,
      "learning_rate": 1.8489583333333337e-05,
      "loss": 0.0024,
      "step": 151250
    },
    {
      "epoch": 5.042,
      "grad_norm": 0.19244131445884705,
      "learning_rate": 1.8487500000000002e-05,
      "loss": 0.0016,
      "step": 151260
    },
    {
      "epoch": 5.042333333333334,
      "grad_norm": 0.14087530970573425,
      "learning_rate": 1.8485416666666668e-05,
      "loss": 0.0017,
      "step": 151270
    },
    {
      "epoch": 5.042666666666666,
      "grad_norm": 0.25772106647491455,
      "learning_rate": 1.8483333333333333e-05,
      "loss": 0.0024,
      "step": 151280
    },
    {
      "epoch": 5.043,
      "grad_norm": 0.2578312158584595,
      "learning_rate": 1.848125e-05,
      "loss": 0.002,
      "step": 151290
    },
    {
      "epoch": 5.043333333333333,
      "grad_norm": 0.11516650766134262,
      "learning_rate": 1.8479166666666667e-05,
      "loss": 0.0016,
      "step": 151300
    },
    {
      "epoch": 5.043666666666667,
      "grad_norm": 0.004921815358102322,
      "learning_rate": 1.8477083333333333e-05,
      "loss": 0.002,
      "step": 151310
    },
    {
      "epoch": 5.044,
      "grad_norm": 0.05764377862215042,
      "learning_rate": 1.8475000000000002e-05,
      "loss": 0.0015,
      "step": 151320
    },
    {
      "epoch": 5.044333333333333,
      "grad_norm": 0.4856809377670288,
      "learning_rate": 1.8472916666666667e-05,
      "loss": 0.0019,
      "step": 151330
    },
    {
      "epoch": 5.044666666666667,
      "grad_norm": 0.22885467112064362,
      "learning_rate": 1.8470833333333333e-05,
      "loss": 0.0018,
      "step": 151340
    },
    {
      "epoch": 5.045,
      "grad_norm": 0.17113997042179108,
      "learning_rate": 1.846875e-05,
      "loss": 0.0014,
      "step": 151350
    },
    {
      "epoch": 5.045333333333334,
      "grad_norm": 0.23007842898368835,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0017,
      "step": 151360
    },
    {
      "epoch": 5.0456666666666665,
      "grad_norm": 0.06102082133293152,
      "learning_rate": 1.8464583333333336e-05,
      "loss": 0.0019,
      "step": 151370
    },
    {
      "epoch": 5.046,
      "grad_norm": 0.19486954808235168,
      "learning_rate": 1.84625e-05,
      "loss": 0.002,
      "step": 151380
    },
    {
      "epoch": 5.046333333333333,
      "grad_norm": 0.11500512063503265,
      "learning_rate": 1.846041666666667e-05,
      "loss": 0.0013,
      "step": 151390
    },
    {
      "epoch": 5.046666666666667,
      "grad_norm": 0.4286074936389923,
      "learning_rate": 1.8458333333333333e-05,
      "loss": 0.0018,
      "step": 151400
    },
    {
      "epoch": 5.047,
      "grad_norm": 0.08752983808517456,
      "learning_rate": 1.845625e-05,
      "loss": 0.0017,
      "step": 151410
    },
    {
      "epoch": 5.0473333333333334,
      "grad_norm": 0.031304243952035904,
      "learning_rate": 1.8454166666666667e-05,
      "loss": 0.0018,
      "step": 151420
    },
    {
      "epoch": 5.047666666666666,
      "grad_norm": 0.1431063413619995,
      "learning_rate": 1.8452083333333332e-05,
      "loss": 0.0012,
      "step": 151430
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.029809975996613503,
      "learning_rate": 1.845e-05,
      "loss": 0.0018,
      "step": 151440
    },
    {
      "epoch": 5.048333333333333,
      "grad_norm": 0.1422886997461319,
      "learning_rate": 1.8447916666666667e-05,
      "loss": 0.0015,
      "step": 151450
    },
    {
      "epoch": 5.048666666666667,
      "grad_norm": 0.3144548535346985,
      "learning_rate": 1.8445833333333336e-05,
      "loss": 0.0022,
      "step": 151460
    },
    {
      "epoch": 5.049,
      "grad_norm": 0.1149287223815918,
      "learning_rate": 1.844375e-05,
      "loss": 0.0024,
      "step": 151470
    },
    {
      "epoch": 5.049333333333333,
      "grad_norm": 0.08604548871517181,
      "learning_rate": 1.8441666666666667e-05,
      "loss": 0.0017,
      "step": 151480
    },
    {
      "epoch": 5.049666666666667,
      "grad_norm": 0.08706234395503998,
      "learning_rate": 1.8439583333333335e-05,
      "loss": 0.0019,
      "step": 151490
    },
    {
      "epoch": 5.05,
      "grad_norm": 0.14289115369319916,
      "learning_rate": 1.84375e-05,
      "loss": 0.0024,
      "step": 151500
    },
    {
      "epoch": 5.050333333333334,
      "grad_norm": 0.14318600296974182,
      "learning_rate": 1.843541666666667e-05,
      "loss": 0.0019,
      "step": 151510
    },
    {
      "epoch": 5.050666666666666,
      "grad_norm": 0.5810418128967285,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.0019,
      "step": 151520
    },
    {
      "epoch": 5.051,
      "grad_norm": 0.3428117632865906,
      "learning_rate": 1.843125e-05,
      "loss": 0.0021,
      "step": 151530
    },
    {
      "epoch": 5.051333333333333,
      "grad_norm": 0.02998543716967106,
      "learning_rate": 1.8429166666666666e-05,
      "loss": 0.0026,
      "step": 151540
    },
    {
      "epoch": 5.051666666666667,
      "grad_norm": 0.03062925487756729,
      "learning_rate": 1.8427083333333335e-05,
      "loss": 0.0024,
      "step": 151550
    },
    {
      "epoch": 5.052,
      "grad_norm": 0.08564748615026474,
      "learning_rate": 1.8425e-05,
      "loss": 0.0014,
      "step": 151560
    },
    {
      "epoch": 5.052333333333333,
      "grad_norm": 0.03001425787806511,
      "learning_rate": 1.8422916666666666e-05,
      "loss": 0.0019,
      "step": 151570
    },
    {
      "epoch": 5.052666666666667,
      "grad_norm": 0.16061700880527496,
      "learning_rate": 1.8420833333333335e-05,
      "loss": 0.0017,
      "step": 151580
    },
    {
      "epoch": 5.053,
      "grad_norm": 0.05751503258943558,
      "learning_rate": 1.841875e-05,
      "loss": 0.0026,
      "step": 151590
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.14367930591106415,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.0019,
      "step": 151600
    },
    {
      "epoch": 5.0536666666666665,
      "grad_norm": 0.11747343093156815,
      "learning_rate": 1.8414583333333335e-05,
      "loss": 0.0018,
      "step": 151610
    },
    {
      "epoch": 5.054,
      "grad_norm": 0.029235176742076874,
      "learning_rate": 1.84125e-05,
      "loss": 0.0021,
      "step": 151620
    },
    {
      "epoch": 5.054333333333333,
      "grad_norm": 0.030720410868525505,
      "learning_rate": 1.841041666666667e-05,
      "loss": 0.0014,
      "step": 151630
    },
    {
      "epoch": 5.054666666666667,
      "grad_norm": 0.14451836049556732,
      "learning_rate": 1.8408333333333335e-05,
      "loss": 0.0019,
      "step": 151640
    },
    {
      "epoch": 5.055,
      "grad_norm": 0.14304551482200623,
      "learning_rate": 1.840625e-05,
      "loss": 0.0025,
      "step": 151650
    },
    {
      "epoch": 5.0553333333333335,
      "grad_norm": 0.2573789060115814,
      "learning_rate": 1.8404166666666666e-05,
      "loss": 0.0017,
      "step": 151660
    },
    {
      "epoch": 5.055666666666666,
      "grad_norm": 0.11495833098888397,
      "learning_rate": 1.8402083333333335e-05,
      "loss": 0.0022,
      "step": 151670
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.0859561637043953,
      "learning_rate": 1.84e-05,
      "loss": 0.002,
      "step": 151680
    },
    {
      "epoch": 5.056333333333333,
      "grad_norm": 0.5825515985488892,
      "learning_rate": 1.839791666666667e-05,
      "loss": 0.0022,
      "step": 151690
    },
    {
      "epoch": 5.056666666666667,
      "grad_norm": 0.39978402853012085,
      "learning_rate": 1.8395833333333334e-05,
      "loss": 0.0019,
      "step": 151700
    },
    {
      "epoch": 5.057,
      "grad_norm": 0.14318238198757172,
      "learning_rate": 1.839375e-05,
      "loss": 0.0014,
      "step": 151710
    },
    {
      "epoch": 5.057333333333333,
      "grad_norm": 0.1721879243850708,
      "learning_rate": 1.839166666666667e-05,
      "loss": 0.0013,
      "step": 151720
    },
    {
      "epoch": 5.057666666666667,
      "grad_norm": 0.030510326847434044,
      "learning_rate": 1.8389583333333334e-05,
      "loss": 0.0025,
      "step": 151730
    },
    {
      "epoch": 5.058,
      "grad_norm": 0.029478851705789566,
      "learning_rate": 1.8387500000000003e-05,
      "loss": 0.0016,
      "step": 151740
    },
    {
      "epoch": 5.058333333333334,
      "grad_norm": 0.19996267557144165,
      "learning_rate": 1.838541666666667e-05,
      "loss": 0.0018,
      "step": 151750
    },
    {
      "epoch": 5.058666666666666,
      "grad_norm": 0.3141533434391022,
      "learning_rate": 1.8383333333333334e-05,
      "loss": 0.0019,
      "step": 151760
    },
    {
      "epoch": 5.059,
      "grad_norm": 0.2566295564174652,
      "learning_rate": 1.838125e-05,
      "loss": 0.0019,
      "step": 151770
    },
    {
      "epoch": 5.059333333333333,
      "grad_norm": 0.031978823244571686,
      "learning_rate": 1.837916666666667e-05,
      "loss": 0.0015,
      "step": 151780
    },
    {
      "epoch": 5.059666666666667,
      "grad_norm": 0.004941293504089117,
      "learning_rate": 1.8377083333333334e-05,
      "loss": 0.0017,
      "step": 151790
    },
    {
      "epoch": 5.06,
      "grad_norm": 0.057741694152355194,
      "learning_rate": 1.8375e-05,
      "loss": 0.0023,
      "step": 151800
    },
    {
      "epoch": 5.060333333333333,
      "grad_norm": 0.24318990111351013,
      "learning_rate": 1.8372916666666668e-05,
      "loss": 0.0022,
      "step": 151810
    },
    {
      "epoch": 5.060666666666667,
      "grad_norm": 0.22949306666851044,
      "learning_rate": 1.8370833333333334e-05,
      "loss": 0.0014,
      "step": 151820
    },
    {
      "epoch": 5.061,
      "grad_norm": 0.1154940128326416,
      "learning_rate": 1.8368750000000003e-05,
      "loss": 0.0016,
      "step": 151830
    },
    {
      "epoch": 5.061333333333334,
      "grad_norm": 0.030082035809755325,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.0018,
      "step": 151840
    },
    {
      "epoch": 5.0616666666666665,
      "grad_norm": 0.37087535858154297,
      "learning_rate": 1.8364583333333334e-05,
      "loss": 0.0019,
      "step": 151850
    },
    {
      "epoch": 5.062,
      "grad_norm": 0.02972070686519146,
      "learning_rate": 1.8362500000000002e-05,
      "loss": 0.0011,
      "step": 151860
    },
    {
      "epoch": 5.062333333333333,
      "grad_norm": 0.1998918056488037,
      "learning_rate": 1.8360416666666668e-05,
      "loss": 0.0025,
      "step": 151870
    },
    {
      "epoch": 5.062666666666667,
      "grad_norm": 0.009428652003407478,
      "learning_rate": 1.8358333333333333e-05,
      "loss": 0.0018,
      "step": 151880
    },
    {
      "epoch": 5.063,
      "grad_norm": 0.08575315028429031,
      "learning_rate": 1.835625e-05,
      "loss": 0.0021,
      "step": 151890
    },
    {
      "epoch": 5.0633333333333335,
      "grad_norm": 0.05868963152170181,
      "learning_rate": 1.8354166666666668e-05,
      "loss": 0.0016,
      "step": 151900
    },
    {
      "epoch": 5.063666666666666,
      "grad_norm": 0.47199907898902893,
      "learning_rate": 1.8352083333333333e-05,
      "loss": 0.0014,
      "step": 151910
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.012856715358793736,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 0.0014,
      "step": 151920
    },
    {
      "epoch": 5.064333333333333,
      "grad_norm": 0.20284177362918854,
      "learning_rate": 1.8347916666666668e-05,
      "loss": 0.0019,
      "step": 151930
    },
    {
      "epoch": 5.064666666666667,
      "grad_norm": 0.060879938304424286,
      "learning_rate": 1.8345833333333333e-05,
      "loss": 0.0017,
      "step": 151940
    },
    {
      "epoch": 5.065,
      "grad_norm": 0.21423259377479553,
      "learning_rate": 1.8343750000000002e-05,
      "loss": 0.0017,
      "step": 151950
    },
    {
      "epoch": 5.065333333333333,
      "grad_norm": 0.08623770624399185,
      "learning_rate": 1.8341666666666668e-05,
      "loss": 0.0023,
      "step": 151960
    },
    {
      "epoch": 5.065666666666667,
      "grad_norm": 0.07657837867736816,
      "learning_rate": 1.8339583333333336e-05,
      "loss": 0.0017,
      "step": 151970
    },
    {
      "epoch": 5.066,
      "grad_norm": 0.1433725357055664,
      "learning_rate": 1.8337500000000002e-05,
      "loss": 0.0016,
      "step": 151980
    },
    {
      "epoch": 5.066333333333334,
      "grad_norm": 0.05761533975601196,
      "learning_rate": 1.8335416666666667e-05,
      "loss": 0.0018,
      "step": 151990
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.058018215000629425,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0017,
      "step": 152000
    },
    {
      "epoch": 5.067,
      "grad_norm": 0.2572113573551178,
      "learning_rate": 1.833125e-05,
      "loss": 0.0014,
      "step": 152010
    },
    {
      "epoch": 5.067333333333333,
      "grad_norm": 0.5145657658576965,
      "learning_rate": 1.8329166666666667e-05,
      "loss": 0.0025,
      "step": 152020
    },
    {
      "epoch": 5.067666666666667,
      "grad_norm": 0.48963308334350586,
      "learning_rate": 1.8327083333333333e-05,
      "loss": 0.0016,
      "step": 152030
    },
    {
      "epoch": 5.068,
      "grad_norm": 0.40651658177375793,
      "learning_rate": 1.8325e-05,
      "loss": 0.0019,
      "step": 152040
    },
    {
      "epoch": 5.068333333333333,
      "grad_norm": 0.08608080446720123,
      "learning_rate": 1.8322916666666667e-05,
      "loss": 0.0021,
      "step": 152050
    },
    {
      "epoch": 5.068666666666667,
      "grad_norm": 0.02916773594915867,
      "learning_rate": 1.8320833333333336e-05,
      "loss": 0.0013,
      "step": 152060
    },
    {
      "epoch": 5.069,
      "grad_norm": 0.12655355036258698,
      "learning_rate": 1.831875e-05,
      "loss": 0.002,
      "step": 152070
    },
    {
      "epoch": 5.069333333333334,
      "grad_norm": 0.029879597947001457,
      "learning_rate": 1.8316666666666667e-05,
      "loss": 0.0029,
      "step": 152080
    },
    {
      "epoch": 5.0696666666666665,
      "grad_norm": 0.14336684346199036,
      "learning_rate": 1.8314583333333336e-05,
      "loss": 0.0021,
      "step": 152090
    },
    {
      "epoch": 5.07,
      "grad_norm": 0.11480913311243057,
      "learning_rate": 1.83125e-05,
      "loss": 0.0016,
      "step": 152100
    },
    {
      "epoch": 5.070333333333333,
      "grad_norm": 0.08627010136842728,
      "learning_rate": 1.831041666666667e-05,
      "loss": 0.0019,
      "step": 152110
    },
    {
      "epoch": 5.070666666666667,
      "grad_norm": 0.6244053244590759,
      "learning_rate": 1.8308333333333332e-05,
      "loss": 0.0023,
      "step": 152120
    },
    {
      "epoch": 5.071,
      "grad_norm": 0.08606284856796265,
      "learning_rate": 1.830625e-05,
      "loss": 0.0016,
      "step": 152130
    },
    {
      "epoch": 5.0713333333333335,
      "grad_norm": 0.029341572895646095,
      "learning_rate": 1.8304166666666667e-05,
      "loss": 0.0019,
      "step": 152140
    },
    {
      "epoch": 5.071666666666666,
      "grad_norm": 0.030493581667542458,
      "learning_rate": 1.8302083333333332e-05,
      "loss": 0.0028,
      "step": 152150
    },
    {
      "epoch": 5.072,
      "grad_norm": 0.22829902172088623,
      "learning_rate": 1.83e-05,
      "loss": 0.0019,
      "step": 152160
    },
    {
      "epoch": 5.072333333333333,
      "grad_norm": 0.38491666316986084,
      "learning_rate": 1.8297916666666666e-05,
      "loss": 0.0025,
      "step": 152170
    },
    {
      "epoch": 5.072666666666667,
      "grad_norm": 0.057900942862033844,
      "learning_rate": 1.8295833333333335e-05,
      "loss": 0.0015,
      "step": 152180
    },
    {
      "epoch": 5.073,
      "grad_norm": 0.057610053569078445,
      "learning_rate": 1.829375e-05,
      "loss": 0.0022,
      "step": 152190
    },
    {
      "epoch": 5.073333333333333,
      "grad_norm": 0.5829513072967529,
      "learning_rate": 1.829166666666667e-05,
      "loss": 0.0012,
      "step": 152200
    },
    {
      "epoch": 5.073666666666667,
      "grad_norm": 0.5427873134613037,
      "learning_rate": 1.8289583333333335e-05,
      "loss": 0.0016,
      "step": 152210
    },
    {
      "epoch": 5.074,
      "grad_norm": 0.2173987329006195,
      "learning_rate": 1.82875e-05,
      "loss": 0.002,
      "step": 152220
    },
    {
      "epoch": 5.074333333333334,
      "grad_norm": 0.20074500143527985,
      "learning_rate": 1.828541666666667e-05,
      "loss": 0.0023,
      "step": 152230
    },
    {
      "epoch": 5.074666666666666,
      "grad_norm": 0.11533673107624054,
      "learning_rate": 1.828333333333333e-05,
      "loss": 0.0017,
      "step": 152240
    },
    {
      "epoch": 5.075,
      "grad_norm": 0.45774510502815247,
      "learning_rate": 1.828125e-05,
      "loss": 0.0023,
      "step": 152250
    },
    {
      "epoch": 5.075333333333333,
      "grad_norm": 0.34297406673431396,
      "learning_rate": 1.8279166666666666e-05,
      "loss": 0.0013,
      "step": 152260
    },
    {
      "epoch": 5.075666666666667,
      "grad_norm": 0.005898200906813145,
      "learning_rate": 1.8277083333333335e-05,
      "loss": 0.0019,
      "step": 152270
    },
    {
      "epoch": 5.076,
      "grad_norm": 0.1996765285730362,
      "learning_rate": 1.8275e-05,
      "loss": 0.0019,
      "step": 152280
    },
    {
      "epoch": 5.076333333333333,
      "grad_norm": 0.4003208875656128,
      "learning_rate": 1.8272916666666666e-05,
      "loss": 0.0014,
      "step": 152290
    },
    {
      "epoch": 5.076666666666666,
      "grad_norm": 0.10815244913101196,
      "learning_rate": 1.8270833333333335e-05,
      "loss": 0.0028,
      "step": 152300
    },
    {
      "epoch": 5.077,
      "grad_norm": 0.08628661930561066,
      "learning_rate": 1.826875e-05,
      "loss": 0.002,
      "step": 152310
    },
    {
      "epoch": 5.077333333333334,
      "grad_norm": 0.37111717462539673,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.002,
      "step": 152320
    },
    {
      "epoch": 5.0776666666666666,
      "grad_norm": 0.011295248754322529,
      "learning_rate": 1.8264583333333335e-05,
      "loss": 0.0018,
      "step": 152330
    },
    {
      "epoch": 5.078,
      "grad_norm": 0.030146680772304535,
      "learning_rate": 1.8262500000000003e-05,
      "loss": 0.0018,
      "step": 152340
    },
    {
      "epoch": 5.078333333333333,
      "grad_norm": 0.2575664222240448,
      "learning_rate": 1.826041666666667e-05,
      "loss": 0.0018,
      "step": 152350
    },
    {
      "epoch": 5.078666666666667,
      "grad_norm": 0.05910777300596237,
      "learning_rate": 1.8258333333333334e-05,
      "loss": 0.0018,
      "step": 152360
    },
    {
      "epoch": 5.079,
      "grad_norm": 0.008055119775235653,
      "learning_rate": 1.825625e-05,
      "loss": 0.0017,
      "step": 152370
    },
    {
      "epoch": 5.0793333333333335,
      "grad_norm": 0.05731536075472832,
      "learning_rate": 1.8254166666666665e-05,
      "loss": 0.0026,
      "step": 152380
    },
    {
      "epoch": 5.079666666666666,
      "grad_norm": 0.3139268159866333,
      "learning_rate": 1.8252083333333334e-05,
      "loss": 0.0017,
      "step": 152390
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.4856398105621338,
      "learning_rate": 1.825e-05,
      "loss": 0.0017,
      "step": 152400
    },
    {
      "epoch": 5.080333333333333,
      "grad_norm": 0.05764229968190193,
      "learning_rate": 1.824791666666667e-05,
      "loss": 0.0016,
      "step": 152410
    },
    {
      "epoch": 5.080666666666667,
      "grad_norm": 0.37184247374534607,
      "learning_rate": 1.8245833333333334e-05,
      "loss": 0.0019,
      "step": 152420
    },
    {
      "epoch": 5.081,
      "grad_norm": 0.5435037016868591,
      "learning_rate": 1.824375e-05,
      "loss": 0.002,
      "step": 152430
    },
    {
      "epoch": 5.081333333333333,
      "grad_norm": 0.2857387959957123,
      "learning_rate": 1.824166666666667e-05,
      "loss": 0.0017,
      "step": 152440
    },
    {
      "epoch": 5.081666666666667,
      "grad_norm": 0.015244082547724247,
      "learning_rate": 1.8239583333333334e-05,
      "loss": 0.0014,
      "step": 152450
    },
    {
      "epoch": 5.082,
      "grad_norm": 0.2304430902004242,
      "learning_rate": 1.8237500000000003e-05,
      "loss": 0.0021,
      "step": 152460
    },
    {
      "epoch": 5.082333333333334,
      "grad_norm": 0.20076680183410645,
      "learning_rate": 1.823541666666667e-05,
      "loss": 0.0021,
      "step": 152470
    },
    {
      "epoch": 5.082666666666666,
      "grad_norm": 0.37154191732406616,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.0015,
      "step": 152480
    },
    {
      "epoch": 5.083,
      "grad_norm": 0.14333848655223846,
      "learning_rate": 1.823125e-05,
      "loss": 0.0021,
      "step": 152490
    },
    {
      "epoch": 5.083333333333333,
      "grad_norm": 0.172388955950737,
      "learning_rate": 1.8229166666666668e-05,
      "loss": 0.0015,
      "step": 152500
    },
    {
      "epoch": 5.083666666666667,
      "grad_norm": 0.1716451346874237,
      "learning_rate": 1.8227083333333334e-05,
      "loss": 0.0018,
      "step": 152510
    },
    {
      "epoch": 5.084,
      "grad_norm": 0.05721503496170044,
      "learning_rate": 1.8225e-05,
      "loss": 0.0019,
      "step": 152520
    },
    {
      "epoch": 5.084333333333333,
      "grad_norm": 0.22892674803733826,
      "learning_rate": 1.8222916666666668e-05,
      "loss": 0.0018,
      "step": 152530
    },
    {
      "epoch": 5.084666666666667,
      "grad_norm": 0.057686012238264084,
      "learning_rate": 1.8220833333333334e-05,
      "loss": 0.0019,
      "step": 152540
    },
    {
      "epoch": 5.085,
      "grad_norm": 0.27822253108024597,
      "learning_rate": 1.8218750000000002e-05,
      "loss": 0.0016,
      "step": 152550
    },
    {
      "epoch": 5.085333333333334,
      "grad_norm": 0.03023425117135048,
      "learning_rate": 1.8216666666666668e-05,
      "loss": 0.0014,
      "step": 152560
    },
    {
      "epoch": 5.085666666666667,
      "grad_norm": 0.2577766180038452,
      "learning_rate": 1.8214583333333333e-05,
      "loss": 0.0018,
      "step": 152570
    },
    {
      "epoch": 5.086,
      "grad_norm": 0.17151349782943726,
      "learning_rate": 1.8212500000000002e-05,
      "loss": 0.0014,
      "step": 152580
    },
    {
      "epoch": 5.086333333333333,
      "grad_norm": 0.171672061085701,
      "learning_rate": 1.8210416666666668e-05,
      "loss": 0.0016,
      "step": 152590
    },
    {
      "epoch": 5.086666666666667,
      "grad_norm": 0.1752026081085205,
      "learning_rate": 1.8208333333333337e-05,
      "loss": 0.0015,
      "step": 152600
    },
    {
      "epoch": 5.087,
      "grad_norm": 0.009305180981755257,
      "learning_rate": 1.820625e-05,
      "loss": 0.0022,
      "step": 152610
    },
    {
      "epoch": 5.0873333333333335,
      "grad_norm": 0.22892236709594727,
      "learning_rate": 1.8204166666666668e-05,
      "loss": 0.0022,
      "step": 152620
    },
    {
      "epoch": 5.087666666666666,
      "grad_norm": 0.17207899689674377,
      "learning_rate": 1.8202083333333333e-05,
      "loss": 0.0023,
      "step": 152630
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.28624284267425537,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0012,
      "step": 152640
    },
    {
      "epoch": 5.088333333333333,
      "grad_norm": 0.14315083622932434,
      "learning_rate": 1.8197916666666667e-05,
      "loss": 0.0016,
      "step": 152650
    },
    {
      "epoch": 5.088666666666667,
      "grad_norm": 0.2593502700328827,
      "learning_rate": 1.8195833333333333e-05,
      "loss": 0.0018,
      "step": 152660
    },
    {
      "epoch": 5.089,
      "grad_norm": 0.02951221726834774,
      "learning_rate": 1.8193750000000002e-05,
      "loss": 0.0021,
      "step": 152670
    },
    {
      "epoch": 5.089333333333333,
      "grad_norm": 0.029378654435276985,
      "learning_rate": 1.8191666666666667e-05,
      "loss": 0.0021,
      "step": 152680
    },
    {
      "epoch": 5.089666666666667,
      "grad_norm": 0.2857871949672699,
      "learning_rate": 1.8189583333333336e-05,
      "loss": 0.0027,
      "step": 152690
    },
    {
      "epoch": 5.09,
      "grad_norm": 0.2575361132621765,
      "learning_rate": 1.81875e-05,
      "loss": 0.0014,
      "step": 152700
    },
    {
      "epoch": 5.090333333333334,
      "grad_norm": 0.3718241751194,
      "learning_rate": 1.8185416666666667e-05,
      "loss": 0.0026,
      "step": 152710
    },
    {
      "epoch": 5.0906666666666665,
      "grad_norm": 0.02954162284731865,
      "learning_rate": 1.8183333333333336e-05,
      "loss": 0.0018,
      "step": 152720
    },
    {
      "epoch": 5.091,
      "grad_norm": 0.057584695518016815,
      "learning_rate": 1.8181249999999998e-05,
      "loss": 0.0027,
      "step": 152730
    },
    {
      "epoch": 5.091333333333333,
      "grad_norm": 0.029099561274051666,
      "learning_rate": 1.8179166666666667e-05,
      "loss": 0.0027,
      "step": 152740
    },
    {
      "epoch": 5.091666666666667,
      "grad_norm": 0.009658765979111195,
      "learning_rate": 1.8177083333333332e-05,
      "loss": 0.0015,
      "step": 152750
    },
    {
      "epoch": 5.092,
      "grad_norm": 0.08571550995111465,
      "learning_rate": 1.8175e-05,
      "loss": 0.0015,
      "step": 152760
    },
    {
      "epoch": 5.092333333333333,
      "grad_norm": 0.005378516390919685,
      "learning_rate": 1.8172916666666667e-05,
      "loss": 0.0018,
      "step": 152770
    },
    {
      "epoch": 5.092666666666666,
      "grad_norm": 0.44548627734184265,
      "learning_rate": 1.8170833333333336e-05,
      "loss": 0.0016,
      "step": 152780
    },
    {
      "epoch": 5.093,
      "grad_norm": 0.03088652901351452,
      "learning_rate": 1.816875e-05,
      "loss": 0.0025,
      "step": 152790
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 0.4287349283695221,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0015,
      "step": 152800
    },
    {
      "epoch": 5.093666666666667,
      "grad_norm": 0.14291758835315704,
      "learning_rate": 1.8164583333333336e-05,
      "loss": 0.0014,
      "step": 152810
    },
    {
      "epoch": 5.094,
      "grad_norm": 0.27691850066185,
      "learning_rate": 1.81625e-05,
      "loss": 0.0022,
      "step": 152820
    },
    {
      "epoch": 5.094333333333333,
      "grad_norm": 0.08577238023281097,
      "learning_rate": 1.816041666666667e-05,
      "loss": 0.0018,
      "step": 152830
    },
    {
      "epoch": 5.094666666666667,
      "grad_norm": 0.14195296168327332,
      "learning_rate": 1.8158333333333335e-05,
      "loss": 0.0025,
      "step": 152840
    },
    {
      "epoch": 5.095,
      "grad_norm": 0.08656378835439682,
      "learning_rate": 1.815625e-05,
      "loss": 0.0024,
      "step": 152850
    },
    {
      "epoch": 5.0953333333333335,
      "grad_norm": 0.2573489844799042,
      "learning_rate": 1.8154166666666666e-05,
      "loss": 0.0015,
      "step": 152860
    },
    {
      "epoch": 5.095666666666666,
      "grad_norm": 0.0590897873044014,
      "learning_rate": 1.8152083333333332e-05,
      "loss": 0.0025,
      "step": 152870
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.25672203302383423,
      "learning_rate": 1.815e-05,
      "loss": 0.0015,
      "step": 152880
    },
    {
      "epoch": 5.096333333333333,
      "grad_norm": 0.2857849597930908,
      "learning_rate": 1.8147916666666666e-05,
      "loss": 0.0016,
      "step": 152890
    },
    {
      "epoch": 5.096666666666667,
      "grad_norm": 0.5401073694229126,
      "learning_rate": 1.8145833333333335e-05,
      "loss": 0.0016,
      "step": 152900
    },
    {
      "epoch": 5.097,
      "grad_norm": 0.08580239117145538,
      "learning_rate": 1.814375e-05,
      "loss": 0.0019,
      "step": 152910
    },
    {
      "epoch": 5.097333333333333,
      "grad_norm": 0.3427061140537262,
      "learning_rate": 1.814166666666667e-05,
      "loss": 0.002,
      "step": 152920
    },
    {
      "epoch": 5.097666666666667,
      "grad_norm": 0.00798773393034935,
      "learning_rate": 1.8139583333333335e-05,
      "loss": 0.0016,
      "step": 152930
    },
    {
      "epoch": 5.098,
      "grad_norm": 0.14284148812294006,
      "learning_rate": 1.81375e-05,
      "loss": 0.0014,
      "step": 152940
    },
    {
      "epoch": 5.098333333333334,
      "grad_norm": 0.14321795105934143,
      "learning_rate": 1.813541666666667e-05,
      "loss": 0.0019,
      "step": 152950
    },
    {
      "epoch": 5.0986666666666665,
      "grad_norm": 0.0295144934207201,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0025,
      "step": 152960
    },
    {
      "epoch": 5.099,
      "grad_norm": 0.5155735015869141,
      "learning_rate": 1.813125e-05,
      "loss": 0.003,
      "step": 152970
    },
    {
      "epoch": 5.099333333333333,
      "grad_norm": 0.2857016324996948,
      "learning_rate": 1.8129166666666666e-05,
      "loss": 0.002,
      "step": 152980
    },
    {
      "epoch": 5.099666666666667,
      "grad_norm": 0.2572931945323944,
      "learning_rate": 1.8127083333333335e-05,
      "loss": 0.0016,
      "step": 152990
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.11480201035737991,
      "learning_rate": 1.8125e-05,
      "loss": 0.0013,
      "step": 153000
    },
    {
      "epoch": 5.100333333333333,
      "grad_norm": 0.058604806661605835,
      "learning_rate": 1.8122916666666666e-05,
      "loss": 0.002,
      "step": 153010
    },
    {
      "epoch": 5.100666666666666,
      "grad_norm": 0.05791621655225754,
      "learning_rate": 1.8120833333333334e-05,
      "loss": 0.0024,
      "step": 153020
    },
    {
      "epoch": 5.101,
      "grad_norm": 0.2860817611217499,
      "learning_rate": 1.811875e-05,
      "loss": 0.0019,
      "step": 153030
    },
    {
      "epoch": 5.101333333333334,
      "grad_norm": 0.142994225025177,
      "learning_rate": 1.811666666666667e-05,
      "loss": 0.0022,
      "step": 153040
    },
    {
      "epoch": 5.101666666666667,
      "grad_norm": 0.08574756234884262,
      "learning_rate": 1.8114583333333334e-05,
      "loss": 0.0023,
      "step": 153050
    },
    {
      "epoch": 5.102,
      "grad_norm": 0.1436973661184311,
      "learning_rate": 1.8112500000000003e-05,
      "loss": 0.0021,
      "step": 153060
    },
    {
      "epoch": 5.102333333333333,
      "grad_norm": 0.26279938220977783,
      "learning_rate": 1.811041666666667e-05,
      "loss": 0.002,
      "step": 153070
    },
    {
      "epoch": 5.102666666666667,
      "grad_norm": 0.20840680599212646,
      "learning_rate": 1.8108333333333334e-05,
      "loss": 0.0017,
      "step": 153080
    },
    {
      "epoch": 5.103,
      "grad_norm": 0.48503872752189636,
      "learning_rate": 1.810625e-05,
      "loss": 0.0027,
      "step": 153090
    },
    {
      "epoch": 5.1033333333333335,
      "grad_norm": 0.2221846580505371,
      "learning_rate": 1.8104166666666665e-05,
      "loss": 0.0016,
      "step": 153100
    },
    {
      "epoch": 5.103666666666666,
      "grad_norm": 0.28564465045928955,
      "learning_rate": 1.8102083333333334e-05,
      "loss": 0.0016,
      "step": 153110
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.17126882076263428,
      "learning_rate": 1.81e-05,
      "loss": 0.0019,
      "step": 153120
    },
    {
      "epoch": 5.104333333333333,
      "grad_norm": 0.32941970229148865,
      "learning_rate": 1.809791666666667e-05,
      "loss": 0.0022,
      "step": 153130
    },
    {
      "epoch": 5.104666666666667,
      "grad_norm": 0.20013855397701263,
      "learning_rate": 1.8095833333333334e-05,
      "loss": 0.0019,
      "step": 153140
    },
    {
      "epoch": 5.105,
      "grad_norm": 0.2291092574596405,
      "learning_rate": 1.809375e-05,
      "loss": 0.0015,
      "step": 153150
    },
    {
      "epoch": 5.105333333333333,
      "grad_norm": 0.17168600857257843,
      "learning_rate": 1.8091666666666668e-05,
      "loss": 0.0012,
      "step": 153160
    },
    {
      "epoch": 5.105666666666667,
      "grad_norm": 0.14315913617610931,
      "learning_rate": 1.8089583333333334e-05,
      "loss": 0.0014,
      "step": 153170
    },
    {
      "epoch": 5.106,
      "grad_norm": 0.11446575820446014,
      "learning_rate": 1.8087500000000003e-05,
      "loss": 0.0016,
      "step": 153180
    },
    {
      "epoch": 5.106333333333334,
      "grad_norm": 0.11575984209775925,
      "learning_rate": 1.8085416666666668e-05,
      "loss": 0.0025,
      "step": 153190
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 0.5076659917831421,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.0018,
      "step": 153200
    },
    {
      "epoch": 5.107,
      "grad_norm": 0.37141454219818115,
      "learning_rate": 1.808125e-05,
      "loss": 0.0021,
      "step": 153210
    },
    {
      "epoch": 5.107333333333333,
      "grad_norm": 0.029254646971821785,
      "learning_rate": 1.8079166666666668e-05,
      "loss": 0.0022,
      "step": 153220
    },
    {
      "epoch": 5.107666666666667,
      "grad_norm": 0.1716456115245819,
      "learning_rate": 1.8077083333333333e-05,
      "loss": 0.0024,
      "step": 153230
    },
    {
      "epoch": 5.108,
      "grad_norm": 0.2567625343799591,
      "learning_rate": 1.8075e-05,
      "loss": 0.002,
      "step": 153240
    },
    {
      "epoch": 5.108333333333333,
      "grad_norm": 0.5339997410774231,
      "learning_rate": 1.8072916666666668e-05,
      "loss": 0.0013,
      "step": 153250
    },
    {
      "epoch": 5.108666666666666,
      "grad_norm": 0.14288371801376343,
      "learning_rate": 1.8070833333333333e-05,
      "loss": 0.0016,
      "step": 153260
    },
    {
      "epoch": 5.109,
      "grad_norm": 0.014772854745388031,
      "learning_rate": 1.8068750000000002e-05,
      "loss": 0.0025,
      "step": 153270
    },
    {
      "epoch": 5.109333333333334,
      "grad_norm": 0.5992525815963745,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0025,
      "step": 153280
    },
    {
      "epoch": 5.109666666666667,
      "grad_norm": 0.030561568215489388,
      "learning_rate": 1.8064583333333333e-05,
      "loss": 0.0019,
      "step": 153290
    },
    {
      "epoch": 5.11,
      "grad_norm": 0.0071182856336236,
      "learning_rate": 1.8062500000000002e-05,
      "loss": 0.0024,
      "step": 153300
    },
    {
      "epoch": 5.110333333333333,
      "grad_norm": 0.05757966637611389,
      "learning_rate": 1.8060416666666667e-05,
      "loss": 0.0017,
      "step": 153310
    },
    {
      "epoch": 5.110666666666667,
      "grad_norm": 0.08653029054403305,
      "learning_rate": 1.8058333333333336e-05,
      "loss": 0.0017,
      "step": 153320
    },
    {
      "epoch": 5.111,
      "grad_norm": 0.3827509880065918,
      "learning_rate": 1.805625e-05,
      "loss": 0.0015,
      "step": 153330
    },
    {
      "epoch": 5.1113333333333335,
      "grad_norm": 0.2857835292816162,
      "learning_rate": 1.8054166666666667e-05,
      "loss": 0.0014,
      "step": 153340
    },
    {
      "epoch": 5.111666666666666,
      "grad_norm": 0.22861681878566742,
      "learning_rate": 1.8052083333333333e-05,
      "loss": 0.0025,
      "step": 153350
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.14298997819423676,
      "learning_rate": 1.805e-05,
      "loss": 0.0019,
      "step": 153360
    },
    {
      "epoch": 5.112333333333333,
      "grad_norm": 0.17136330902576447,
      "learning_rate": 1.8047916666666667e-05,
      "loss": 0.002,
      "step": 153370
    },
    {
      "epoch": 5.112666666666667,
      "grad_norm": 0.14813241362571716,
      "learning_rate": 1.8045833333333333e-05,
      "loss": 0.0015,
      "step": 153380
    },
    {
      "epoch": 5.113,
      "grad_norm": 0.14319062232971191,
      "learning_rate": 1.804375e-05,
      "loss": 0.0016,
      "step": 153390
    },
    {
      "epoch": 5.113333333333333,
      "grad_norm": 0.05801740661263466,
      "learning_rate": 1.8041666666666667e-05,
      "loss": 0.0018,
      "step": 153400
    },
    {
      "epoch": 5.113666666666667,
      "grad_norm": 0.3147687017917633,
      "learning_rate": 1.8039583333333336e-05,
      "loss": 0.0013,
      "step": 153410
    },
    {
      "epoch": 5.114,
      "grad_norm": 0.34311118721961975,
      "learning_rate": 1.80375e-05,
      "loss": 0.0013,
      "step": 153420
    },
    {
      "epoch": 5.114333333333334,
      "grad_norm": 0.08603997528553009,
      "learning_rate": 1.8035416666666667e-05,
      "loss": 0.0016,
      "step": 153430
    },
    {
      "epoch": 5.1146666666666665,
      "grad_norm": 0.22859810292720795,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0014,
      "step": 153440
    },
    {
      "epoch": 5.115,
      "grad_norm": 0.0577811673283577,
      "learning_rate": 1.803125e-05,
      "loss": 0.0019,
      "step": 153450
    },
    {
      "epoch": 5.115333333333333,
      "grad_norm": 0.08635623008012772,
      "learning_rate": 1.8029166666666667e-05,
      "loss": 0.0026,
      "step": 153460
    },
    {
      "epoch": 5.115666666666667,
      "grad_norm": 0.6125531792640686,
      "learning_rate": 1.8027083333333332e-05,
      "loss": 0.0029,
      "step": 153470
    },
    {
      "epoch": 5.116,
      "grad_norm": 0.057656560093164444,
      "learning_rate": 1.8025e-05,
      "loss": 0.0013,
      "step": 153480
    },
    {
      "epoch": 5.116333333333333,
      "grad_norm": 0.29550468921661377,
      "learning_rate": 1.8022916666666667e-05,
      "loss": 0.0027,
      "step": 153490
    },
    {
      "epoch": 5.116666666666666,
      "grad_norm": 0.2851732075214386,
      "learning_rate": 1.8020833333333335e-05,
      "loss": 0.0025,
      "step": 153500
    },
    {
      "epoch": 5.117,
      "grad_norm": 0.34264442324638367,
      "learning_rate": 1.801875e-05,
      "loss": 0.0019,
      "step": 153510
    },
    {
      "epoch": 5.117333333333334,
      "grad_norm": 0.08644196391105652,
      "learning_rate": 1.8016666666666666e-05,
      "loss": 0.0016,
      "step": 153520
    },
    {
      "epoch": 5.117666666666667,
      "grad_norm": 0.3377176523208618,
      "learning_rate": 1.8014583333333335e-05,
      "loss": 0.0016,
      "step": 153530
    },
    {
      "epoch": 5.118,
      "grad_norm": 0.11547397822141647,
      "learning_rate": 1.80125e-05,
      "loss": 0.0018,
      "step": 153540
    },
    {
      "epoch": 5.118333333333333,
      "grad_norm": 0.17165562510490417,
      "learning_rate": 1.801041666666667e-05,
      "loss": 0.002,
      "step": 153550
    },
    {
      "epoch": 5.118666666666667,
      "grad_norm": 0.05878133699297905,
      "learning_rate": 1.8008333333333335e-05,
      "loss": 0.0013,
      "step": 153560
    },
    {
      "epoch": 5.119,
      "grad_norm": 0.03218306973576546,
      "learning_rate": 1.800625e-05,
      "loss": 0.0021,
      "step": 153570
    },
    {
      "epoch": 5.1193333333333335,
      "grad_norm": 0.2853441536426544,
      "learning_rate": 1.8004166666666666e-05,
      "loss": 0.002,
      "step": 153580
    },
    {
      "epoch": 5.119666666666666,
      "grad_norm": 0.29935967922210693,
      "learning_rate": 1.8002083333333335e-05,
      "loss": 0.0012,
      "step": 153590
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.11475475132465363,
      "learning_rate": 1.8e-05,
      "loss": 0.0011,
      "step": 153600
    },
    {
      "epoch": 5.120333333333333,
      "grad_norm": 0.10336810350418091,
      "learning_rate": 1.7997916666666666e-05,
      "loss": 0.0016,
      "step": 153610
    },
    {
      "epoch": 5.120666666666667,
      "grad_norm": 0.08996570110321045,
      "learning_rate": 1.7995833333333335e-05,
      "loss": 0.0023,
      "step": 153620
    },
    {
      "epoch": 5.121,
      "grad_norm": 0.14309382438659668,
      "learning_rate": 1.799375e-05,
      "loss": 0.0016,
      "step": 153630
    },
    {
      "epoch": 5.121333333333333,
      "grad_norm": 0.2005002349615097,
      "learning_rate": 1.799166666666667e-05,
      "loss": 0.0014,
      "step": 153640
    },
    {
      "epoch": 5.121666666666667,
      "grad_norm": 0.28549692034721375,
      "learning_rate": 1.7989583333333335e-05,
      "loss": 0.0014,
      "step": 153650
    },
    {
      "epoch": 5.122,
      "grad_norm": 0.1715664267539978,
      "learning_rate": 1.79875e-05,
      "loss": 0.0023,
      "step": 153660
    },
    {
      "epoch": 5.122333333333334,
      "grad_norm": 0.19995367527008057,
      "learning_rate": 1.798541666666667e-05,
      "loss": 0.0022,
      "step": 153670
    },
    {
      "epoch": 5.1226666666666665,
      "grad_norm": 0.14382269978523254,
      "learning_rate": 1.7983333333333335e-05,
      "loss": 0.0024,
      "step": 153680
    },
    {
      "epoch": 5.123,
      "grad_norm": 0.08669724315404892,
      "learning_rate": 1.798125e-05,
      "loss": 0.002,
      "step": 153690
    },
    {
      "epoch": 5.123333333333333,
      "grad_norm": 0.17161521315574646,
      "learning_rate": 1.7979166666666665e-05,
      "loss": 0.0016,
      "step": 153700
    },
    {
      "epoch": 5.123666666666667,
      "grad_norm": 0.05792646110057831,
      "learning_rate": 1.7977083333333334e-05,
      "loss": 0.0022,
      "step": 153710
    },
    {
      "epoch": 5.124,
      "grad_norm": 0.11401677876710892,
      "learning_rate": 1.7975e-05,
      "loss": 0.0015,
      "step": 153720
    },
    {
      "epoch": 5.124333333333333,
      "grad_norm": 0.1430276483297348,
      "learning_rate": 1.797291666666667e-05,
      "loss": 0.0012,
      "step": 153730
    },
    {
      "epoch": 5.124666666666666,
      "grad_norm": 0.2010091245174408,
      "learning_rate": 1.7970833333333334e-05,
      "loss": 0.0014,
      "step": 153740
    },
    {
      "epoch": 5.125,
      "grad_norm": 0.572187066078186,
      "learning_rate": 1.796875e-05,
      "loss": 0.0017,
      "step": 153750
    },
    {
      "epoch": 5.125333333333334,
      "grad_norm": 0.30571362376213074,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.002,
      "step": 153760
    },
    {
      "epoch": 5.125666666666667,
      "grad_norm": 0.22863078117370605,
      "learning_rate": 1.7964583333333334e-05,
      "loss": 0.0024,
      "step": 153770
    },
    {
      "epoch": 5.126,
      "grad_norm": 0.1717444360256195,
      "learning_rate": 1.7962500000000003e-05,
      "loss": 0.0017,
      "step": 153780
    },
    {
      "epoch": 5.126333333333333,
      "grad_norm": 0.06299396604299545,
      "learning_rate": 1.796041666666667e-05,
      "loss": 0.0017,
      "step": 153790
    },
    {
      "epoch": 5.126666666666667,
      "grad_norm": 0.22864557802677155,
      "learning_rate": 1.7958333333333334e-05,
      "loss": 0.0023,
      "step": 153800
    },
    {
      "epoch": 5.127,
      "grad_norm": 0.42867156863212585,
      "learning_rate": 1.7956250000000003e-05,
      "loss": 0.0012,
      "step": 153810
    },
    {
      "epoch": 5.1273333333333335,
      "grad_norm": 0.5401101112365723,
      "learning_rate": 1.7954166666666665e-05,
      "loss": 0.0023,
      "step": 153820
    },
    {
      "epoch": 5.127666666666666,
      "grad_norm": 0.372003436088562,
      "learning_rate": 1.7952083333333334e-05,
      "loss": 0.002,
      "step": 153830
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.14292146265506744,
      "learning_rate": 1.795e-05,
      "loss": 0.0018,
      "step": 153840
    },
    {
      "epoch": 5.128333333333333,
      "grad_norm": 0.006579756736755371,
      "learning_rate": 1.7947916666666668e-05,
      "loss": 0.0022,
      "step": 153850
    },
    {
      "epoch": 5.128666666666667,
      "grad_norm": 0.02915174886584282,
      "learning_rate": 1.7945833333333334e-05,
      "loss": 0.0026,
      "step": 153860
    },
    {
      "epoch": 5.129,
      "grad_norm": 0.11513040959835052,
      "learning_rate": 1.7943750000000002e-05,
      "loss": 0.0016,
      "step": 153870
    },
    {
      "epoch": 5.129333333333333,
      "grad_norm": 0.35806724429130554,
      "learning_rate": 1.7941666666666668e-05,
      "loss": 0.0026,
      "step": 153880
    },
    {
      "epoch": 5.129666666666667,
      "grad_norm": 0.08634790778160095,
      "learning_rate": 1.7939583333333333e-05,
      "loss": 0.0018,
      "step": 153890
    },
    {
      "epoch": 5.13,
      "grad_norm": 0.6200411319732666,
      "learning_rate": 1.7937500000000002e-05,
      "loss": 0.0023,
      "step": 153900
    },
    {
      "epoch": 5.130333333333334,
      "grad_norm": 0.011150510050356388,
      "learning_rate": 1.7935416666666668e-05,
      "loss": 0.0022,
      "step": 153910
    },
    {
      "epoch": 5.1306666666666665,
      "grad_norm": 0.03998393937945366,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0027,
      "step": 153920
    },
    {
      "epoch": 5.131,
      "grad_norm": 0.11411377042531967,
      "learning_rate": 1.7931250000000002e-05,
      "loss": 0.002,
      "step": 153930
    },
    {
      "epoch": 5.131333333333333,
      "grad_norm": 0.029652461409568787,
      "learning_rate": 1.7929166666666668e-05,
      "loss": 0.0018,
      "step": 153940
    },
    {
      "epoch": 5.131666666666667,
      "grad_norm": 0.2570449709892273,
      "learning_rate": 1.7927083333333333e-05,
      "loss": 0.002,
      "step": 153950
    },
    {
      "epoch": 5.132,
      "grad_norm": 0.0335901640355587,
      "learning_rate": 1.7925e-05,
      "loss": 0.0016,
      "step": 153960
    },
    {
      "epoch": 5.132333333333333,
      "grad_norm": 0.057481370866298676,
      "learning_rate": 1.7922916666666668e-05,
      "loss": 0.0017,
      "step": 153970
    },
    {
      "epoch": 5.132666666666666,
      "grad_norm": 0.15923528373241425,
      "learning_rate": 1.7920833333333333e-05,
      "loss": 0.0015,
      "step": 153980
    },
    {
      "epoch": 5.133,
      "grad_norm": 0.0572105310857296,
      "learning_rate": 1.7918750000000002e-05,
      "loss": 0.0021,
      "step": 153990
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.2287418693304062,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.0012,
      "step": 154000
    },
    {
      "epoch": 5.133666666666667,
      "grad_norm": 0.4380106031894684,
      "learning_rate": 1.7914583333333336e-05,
      "loss": 0.0028,
      "step": 154010
    },
    {
      "epoch": 5.134,
      "grad_norm": 0.10718647390604019,
      "learning_rate": 1.7912500000000002e-05,
      "loss": 0.0027,
      "step": 154020
    },
    {
      "epoch": 5.134333333333333,
      "grad_norm": 0.11469563841819763,
      "learning_rate": 1.7910416666666667e-05,
      "loss": 0.0013,
      "step": 154030
    },
    {
      "epoch": 5.134666666666667,
      "grad_norm": 0.1714916080236435,
      "learning_rate": 1.7908333333333336e-05,
      "loss": 0.0016,
      "step": 154040
    },
    {
      "epoch": 5.135,
      "grad_norm": 0.29759252071380615,
      "learning_rate": 1.790625e-05,
      "loss": 0.0017,
      "step": 154050
    },
    {
      "epoch": 5.1353333333333335,
      "grad_norm": 0.45677024126052856,
      "learning_rate": 1.7904166666666667e-05,
      "loss": 0.003,
      "step": 154060
    },
    {
      "epoch": 5.135666666666666,
      "grad_norm": 0.008998318575322628,
      "learning_rate": 1.7902083333333333e-05,
      "loss": 0.0027,
      "step": 154070
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.19235238432884216,
      "learning_rate": 1.79e-05,
      "loss": 0.0014,
      "step": 154080
    },
    {
      "epoch": 5.136333333333333,
      "grad_norm": 0.20202825963497162,
      "learning_rate": 1.7897916666666667e-05,
      "loss": 0.0014,
      "step": 154090
    },
    {
      "epoch": 5.136666666666667,
      "grad_norm": 0.3992910385131836,
      "learning_rate": 1.7895833333333332e-05,
      "loss": 0.0017,
      "step": 154100
    },
    {
      "epoch": 5.1370000000000005,
      "grad_norm": 0.20029112696647644,
      "learning_rate": 1.789375e-05,
      "loss": 0.0016,
      "step": 154110
    },
    {
      "epoch": 5.137333333333333,
      "grad_norm": 0.31999555230140686,
      "learning_rate": 1.7891666666666667e-05,
      "loss": 0.0017,
      "step": 154120
    },
    {
      "epoch": 5.137666666666667,
      "grad_norm": 0.2571595311164856,
      "learning_rate": 1.7889583333333336e-05,
      "loss": 0.0021,
      "step": 154130
    },
    {
      "epoch": 5.138,
      "grad_norm": 0.40014922618865967,
      "learning_rate": 1.78875e-05,
      "loss": 0.0016,
      "step": 154140
    },
    {
      "epoch": 5.138333333333334,
      "grad_norm": 0.08642534911632538,
      "learning_rate": 1.788541666666667e-05,
      "loss": 0.0022,
      "step": 154150
    },
    {
      "epoch": 5.1386666666666665,
      "grad_norm": 0.029670605435967445,
      "learning_rate": 1.7883333333333335e-05,
      "loss": 0.0016,
      "step": 154160
    },
    {
      "epoch": 5.139,
      "grad_norm": 0.34244292974472046,
      "learning_rate": 1.788125e-05,
      "loss": 0.0011,
      "step": 154170
    },
    {
      "epoch": 5.139333333333333,
      "grad_norm": 0.3716581463813782,
      "learning_rate": 1.7879166666666666e-05,
      "loss": 0.0014,
      "step": 154180
    },
    {
      "epoch": 5.139666666666667,
      "grad_norm": 0.25686249136924744,
      "learning_rate": 1.7877083333333332e-05,
      "loss": 0.002,
      "step": 154190
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.20017510652542114,
      "learning_rate": 1.7875e-05,
      "loss": 0.0013,
      "step": 154200
    },
    {
      "epoch": 5.140333333333333,
      "grad_norm": 0.4171769320964813,
      "learning_rate": 1.7872916666666666e-05,
      "loss": 0.0019,
      "step": 154210
    },
    {
      "epoch": 5.140666666666666,
      "grad_norm": 0.31492993235588074,
      "learning_rate": 1.7870833333333335e-05,
      "loss": 0.0018,
      "step": 154220
    },
    {
      "epoch": 5.141,
      "grad_norm": 0.08675670623779297,
      "learning_rate": 1.786875e-05,
      "loss": 0.0012,
      "step": 154230
    },
    {
      "epoch": 5.141333333333334,
      "grad_norm": 0.02979588694870472,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.002,
      "step": 154240
    },
    {
      "epoch": 5.141666666666667,
      "grad_norm": 0.22911490499973297,
      "learning_rate": 1.7864583333333335e-05,
      "loss": 0.0013,
      "step": 154250
    },
    {
      "epoch": 5.142,
      "grad_norm": 0.17153897881507874,
      "learning_rate": 1.78625e-05,
      "loss": 0.0017,
      "step": 154260
    },
    {
      "epoch": 5.142333333333333,
      "grad_norm": 0.17179372906684875,
      "learning_rate": 1.786041666666667e-05,
      "loss": 0.0017,
      "step": 154270
    },
    {
      "epoch": 5.142666666666667,
      "grad_norm": 0.0578765794634819,
      "learning_rate": 1.7858333333333335e-05,
      "loss": 0.0021,
      "step": 154280
    },
    {
      "epoch": 5.143,
      "grad_norm": 0.05747004598379135,
      "learning_rate": 1.7856250000000004e-05,
      "loss": 0.0021,
      "step": 154290
    },
    {
      "epoch": 5.1433333333333335,
      "grad_norm": 0.19980810582637787,
      "learning_rate": 1.7854166666666666e-05,
      "loss": 0.0014,
      "step": 154300
    },
    {
      "epoch": 5.143666666666666,
      "grad_norm": 0.45822080969810486,
      "learning_rate": 1.7852083333333335e-05,
      "loss": 0.0012,
      "step": 154310
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.42829829454421997,
      "learning_rate": 1.785e-05,
      "loss": 0.0014,
      "step": 154320
    },
    {
      "epoch": 5.144333333333333,
      "grad_norm": 0.22880589962005615,
      "learning_rate": 1.7847916666666666e-05,
      "loss": 0.0014,
      "step": 154330
    },
    {
      "epoch": 5.144666666666667,
      "grad_norm": 0.1145409345626831,
      "learning_rate": 1.7845833333333335e-05,
      "loss": 0.0018,
      "step": 154340
    },
    {
      "epoch": 5.145,
      "grad_norm": 0.08612499386072159,
      "learning_rate": 1.784375e-05,
      "loss": 0.0014,
      "step": 154350
    },
    {
      "epoch": 5.145333333333333,
      "grad_norm": 0.1324809491634369,
      "learning_rate": 1.784166666666667e-05,
      "loss": 0.0015,
      "step": 154360
    },
    {
      "epoch": 5.145666666666667,
      "grad_norm": 0.2285284847021103,
      "learning_rate": 1.7839583333333334e-05,
      "loss": 0.0022,
      "step": 154370
    },
    {
      "epoch": 5.146,
      "grad_norm": 0.05756264179944992,
      "learning_rate": 1.78375e-05,
      "loss": 0.0018,
      "step": 154380
    },
    {
      "epoch": 5.146333333333334,
      "grad_norm": 0.8092803955078125,
      "learning_rate": 1.783541666666667e-05,
      "loss": 0.0017,
      "step": 154390
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 0.08594156801700592,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.002,
      "step": 154400
    },
    {
      "epoch": 5.147,
      "grad_norm": 0.22973011434078217,
      "learning_rate": 1.7831250000000003e-05,
      "loss": 0.0016,
      "step": 154410
    },
    {
      "epoch": 5.147333333333333,
      "grad_norm": 0.1758703738451004,
      "learning_rate": 1.7829166666666665e-05,
      "loss": 0.002,
      "step": 154420
    },
    {
      "epoch": 5.147666666666667,
      "grad_norm": 0.030979149043560028,
      "learning_rate": 1.7827083333333334e-05,
      "loss": 0.0017,
      "step": 154430
    },
    {
      "epoch": 5.148,
      "grad_norm": 0.11432372778654099,
      "learning_rate": 1.7825e-05,
      "loss": 0.0017,
      "step": 154440
    },
    {
      "epoch": 5.148333333333333,
      "grad_norm": 0.057606253772974014,
      "learning_rate": 1.782291666666667e-05,
      "loss": 0.0013,
      "step": 154450
    },
    {
      "epoch": 5.148666666666666,
      "grad_norm": 0.11436184495687485,
      "learning_rate": 1.7820833333333334e-05,
      "loss": 0.002,
      "step": 154460
    },
    {
      "epoch": 5.149,
      "grad_norm": 0.11645181477069855,
      "learning_rate": 1.781875e-05,
      "loss": 0.0022,
      "step": 154470
    },
    {
      "epoch": 5.149333333333334,
      "grad_norm": 0.37165525555610657,
      "learning_rate": 1.781666666666667e-05,
      "loss": 0.0016,
      "step": 154480
    },
    {
      "epoch": 5.149666666666667,
      "grad_norm": 0.401745468378067,
      "learning_rate": 1.7814583333333334e-05,
      "loss": 0.0017,
      "step": 154490
    },
    {
      "epoch": 5.15,
      "grad_norm": 0.10506176203489304,
      "learning_rate": 1.7812500000000003e-05,
      "loss": 0.0022,
      "step": 154500
    },
    {
      "epoch": 5.150333333333333,
      "grad_norm": 0.11423860490322113,
      "learning_rate": 1.7810416666666668e-05,
      "loss": 0.0013,
      "step": 154510
    },
    {
      "epoch": 5.150666666666667,
      "grad_norm": 0.029045933857560158,
      "learning_rate": 1.7808333333333334e-05,
      "loss": 0.0024,
      "step": 154520
    },
    {
      "epoch": 5.151,
      "grad_norm": 0.11517585813999176,
      "learning_rate": 1.7806250000000003e-05,
      "loss": 0.0019,
      "step": 154530
    },
    {
      "epoch": 5.1513333333333335,
      "grad_norm": 0.19985215365886688,
      "learning_rate": 1.7804166666666665e-05,
      "loss": 0.0018,
      "step": 154540
    },
    {
      "epoch": 5.151666666666666,
      "grad_norm": 0.20012912154197693,
      "learning_rate": 1.7802083333333333e-05,
      "loss": 0.0021,
      "step": 154550
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.47499722242355347,
      "learning_rate": 1.78e-05,
      "loss": 0.0031,
      "step": 154560
    },
    {
      "epoch": 5.152333333333333,
      "grad_norm": 0.3999958336353302,
      "learning_rate": 1.7797916666666668e-05,
      "loss": 0.0016,
      "step": 154570
    },
    {
      "epoch": 5.152666666666667,
      "grad_norm": 0.012902899645268917,
      "learning_rate": 1.7795833333333333e-05,
      "loss": 0.0017,
      "step": 154580
    },
    {
      "epoch": 5.153,
      "grad_norm": 0.22909210622310638,
      "learning_rate": 1.7793750000000002e-05,
      "loss": 0.0021,
      "step": 154590
    },
    {
      "epoch": 5.153333333333333,
      "grad_norm": 0.058716412633657455,
      "learning_rate": 1.7791666666666668e-05,
      "loss": 0.0021,
      "step": 154600
    },
    {
      "epoch": 5.153666666666667,
      "grad_norm": 0.14296169579029083,
      "learning_rate": 1.7789583333333333e-05,
      "loss": 0.0019,
      "step": 154610
    },
    {
      "epoch": 5.154,
      "grad_norm": 0.3719852864742279,
      "learning_rate": 1.7787500000000002e-05,
      "loss": 0.002,
      "step": 154620
    },
    {
      "epoch": 5.154333333333334,
      "grad_norm": 0.05863283574581146,
      "learning_rate": 1.7785416666666668e-05,
      "loss": 0.0021,
      "step": 154630
    },
    {
      "epoch": 5.1546666666666665,
      "grad_norm": 0.057428888976573944,
      "learning_rate": 1.7783333333333336e-05,
      "loss": 0.0019,
      "step": 154640
    },
    {
      "epoch": 5.155,
      "grad_norm": 0.23603928089141846,
      "learning_rate": 1.7781250000000002e-05,
      "loss": 0.0019,
      "step": 154650
    },
    {
      "epoch": 5.155333333333333,
      "grad_norm": 0.3184932768344879,
      "learning_rate": 1.7779166666666667e-05,
      "loss": 0.0022,
      "step": 154660
    },
    {
      "epoch": 5.155666666666667,
      "grad_norm": 0.46333810687065125,
      "learning_rate": 1.7777083333333333e-05,
      "loss": 0.002,
      "step": 154670
    },
    {
      "epoch": 5.156,
      "grad_norm": 0.08649937808513641,
      "learning_rate": 1.7775e-05,
      "loss": 0.0014,
      "step": 154680
    },
    {
      "epoch": 5.156333333333333,
      "grad_norm": 0.20018073916435242,
      "learning_rate": 1.7772916666666667e-05,
      "loss": 0.002,
      "step": 154690
    },
    {
      "epoch": 5.156666666666666,
      "grad_norm": 0.0859893336892128,
      "learning_rate": 1.7770833333333333e-05,
      "loss": 0.0022,
      "step": 154700
    },
    {
      "epoch": 5.157,
      "grad_norm": 0.11442112177610397,
      "learning_rate": 1.776875e-05,
      "loss": 0.0014,
      "step": 154710
    },
    {
      "epoch": 5.157333333333334,
      "grad_norm": 0.3995206356048584,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0033,
      "step": 154720
    },
    {
      "epoch": 5.157666666666667,
      "grad_norm": 0.11417701840400696,
      "learning_rate": 1.7764583333333336e-05,
      "loss": 0.002,
      "step": 154730
    },
    {
      "epoch": 5.158,
      "grad_norm": 0.31480300426483154,
      "learning_rate": 1.77625e-05,
      "loss": 0.0015,
      "step": 154740
    },
    {
      "epoch": 5.158333333333333,
      "grad_norm": 0.33874693512916565,
      "learning_rate": 1.7760416666666667e-05,
      "loss": 0.0014,
      "step": 154750
    },
    {
      "epoch": 5.158666666666667,
      "grad_norm": 0.14301085472106934,
      "learning_rate": 1.7758333333333336e-05,
      "loss": 0.0022,
      "step": 154760
    },
    {
      "epoch": 5.159,
      "grad_norm": 0.031947772949934006,
      "learning_rate": 1.775625e-05,
      "loss": 0.0017,
      "step": 154770
    },
    {
      "epoch": 5.1593333333333335,
      "grad_norm": 0.2285872846841812,
      "learning_rate": 1.7754166666666667e-05,
      "loss": 0.0015,
      "step": 154780
    },
    {
      "epoch": 5.159666666666666,
      "grad_norm": 0.12120848894119263,
      "learning_rate": 1.7752083333333332e-05,
      "loss": 0.0017,
      "step": 154790
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.11562551558017731,
      "learning_rate": 1.775e-05,
      "loss": 0.0017,
      "step": 154800
    },
    {
      "epoch": 5.160333333333333,
      "grad_norm": 0.22890932857990265,
      "learning_rate": 1.7747916666666667e-05,
      "loss": 0.0014,
      "step": 154810
    },
    {
      "epoch": 5.160666666666667,
      "grad_norm": 0.08572767674922943,
      "learning_rate": 1.7745833333333332e-05,
      "loss": 0.0019,
      "step": 154820
    },
    {
      "epoch": 5.161,
      "grad_norm": 0.11480501294136047,
      "learning_rate": 1.774375e-05,
      "loss": 0.0017,
      "step": 154830
    },
    {
      "epoch": 5.161333333333333,
      "grad_norm": 0.08605285733938217,
      "learning_rate": 1.7741666666666666e-05,
      "loss": 0.002,
      "step": 154840
    },
    {
      "epoch": 5.161666666666667,
      "grad_norm": 0.17176327109336853,
      "learning_rate": 1.7739583333333335e-05,
      "loss": 0.002,
      "step": 154850
    },
    {
      "epoch": 5.162,
      "grad_norm": 0.3429218530654907,
      "learning_rate": 1.77375e-05,
      "loss": 0.0019,
      "step": 154860
    },
    {
      "epoch": 5.162333333333334,
      "grad_norm": 0.28599798679351807,
      "learning_rate": 1.773541666666667e-05,
      "loss": 0.0019,
      "step": 154870
    },
    {
      "epoch": 5.1626666666666665,
      "grad_norm": 0.20036233961582184,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0015,
      "step": 154880
    },
    {
      "epoch": 5.163,
      "grad_norm": 0.14338240027427673,
      "learning_rate": 1.773125e-05,
      "loss": 0.0024,
      "step": 154890
    },
    {
      "epoch": 5.163333333333333,
      "grad_norm": 0.1805993616580963,
      "learning_rate": 1.7729166666666666e-05,
      "loss": 0.0019,
      "step": 154900
    },
    {
      "epoch": 5.163666666666667,
      "grad_norm": 0.1145031750202179,
      "learning_rate": 1.772708333333333e-05,
      "loss": 0.0019,
      "step": 154910
    },
    {
      "epoch": 5.164,
      "grad_norm": 0.31873607635498047,
      "learning_rate": 1.7725e-05,
      "loss": 0.0024,
      "step": 154920
    },
    {
      "epoch": 5.164333333333333,
      "grad_norm": 0.3145628273487091,
      "learning_rate": 1.7722916666666666e-05,
      "loss": 0.0026,
      "step": 154930
    },
    {
      "epoch": 5.164666666666666,
      "grad_norm": 0.42875000834465027,
      "learning_rate": 1.7720833333333335e-05,
      "loss": 0.0016,
      "step": 154940
    },
    {
      "epoch": 5.165,
      "grad_norm": 0.37238943576812744,
      "learning_rate": 1.771875e-05,
      "loss": 0.0017,
      "step": 154950
    },
    {
      "epoch": 5.165333333333333,
      "grad_norm": 0.3257741630077362,
      "learning_rate": 1.7716666666666666e-05,
      "loss": 0.0021,
      "step": 154960
    },
    {
      "epoch": 5.165666666666667,
      "grad_norm": 0.031347472220659256,
      "learning_rate": 1.7714583333333335e-05,
      "loss": 0.0025,
      "step": 154970
    },
    {
      "epoch": 5.166,
      "grad_norm": 0.05839620158076286,
      "learning_rate": 1.77125e-05,
      "loss": 0.0015,
      "step": 154980
    },
    {
      "epoch": 5.166333333333333,
      "grad_norm": 0.21338848769664764,
      "learning_rate": 1.771041666666667e-05,
      "loss": 0.0013,
      "step": 154990
    },
    {
      "epoch": 5.166666666666667,
      "grad_norm": 0.34367263317108154,
      "learning_rate": 1.7708333333333335e-05,
      "loss": 0.0017,
      "step": 155000
    },
    {
      "epoch": 5.167,
      "grad_norm": 0.08651349693536758,
      "learning_rate": 1.7706250000000003e-05,
      "loss": 0.0017,
      "step": 155010
    },
    {
      "epoch": 5.167333333333334,
      "grad_norm": 0.17231711745262146,
      "learning_rate": 1.770416666666667e-05,
      "loss": 0.0015,
      "step": 155020
    },
    {
      "epoch": 5.167666666666666,
      "grad_norm": 0.2002413123846054,
      "learning_rate": 1.7702083333333334e-05,
      "loss": 0.002,
      "step": 155030
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.22833874821662903,
      "learning_rate": 1.77e-05,
      "loss": 0.002,
      "step": 155040
    },
    {
      "epoch": 5.168333333333333,
      "grad_norm": 0.029345467686653137,
      "learning_rate": 1.7697916666666665e-05,
      "loss": 0.0018,
      "step": 155050
    },
    {
      "epoch": 5.168666666666667,
      "grad_norm": 0.08634626120328903,
      "learning_rate": 1.7695833333333334e-05,
      "loss": 0.0013,
      "step": 155060
    },
    {
      "epoch": 5.169,
      "grad_norm": 0.08594873547554016,
      "learning_rate": 1.769375e-05,
      "loss": 0.002,
      "step": 155070
    },
    {
      "epoch": 5.169333333333333,
      "grad_norm": 0.22880662977695465,
      "learning_rate": 1.769166666666667e-05,
      "loss": 0.0021,
      "step": 155080
    },
    {
      "epoch": 5.169666666666667,
      "grad_norm": 0.2861613929271698,
      "learning_rate": 1.7689583333333334e-05,
      "loss": 0.0016,
      "step": 155090
    },
    {
      "epoch": 5.17,
      "grad_norm": 0.25766485929489136,
      "learning_rate": 1.76875e-05,
      "loss": 0.0019,
      "step": 155100
    },
    {
      "epoch": 5.170333333333334,
      "grad_norm": 0.2004307210445404,
      "learning_rate": 1.768541666666667e-05,
      "loss": 0.0023,
      "step": 155110
    },
    {
      "epoch": 5.1706666666666665,
      "grad_norm": 0.28686851263046265,
      "learning_rate": 1.7683333333333334e-05,
      "loss": 0.0015,
      "step": 155120
    },
    {
      "epoch": 5.171,
      "grad_norm": 0.37195804715156555,
      "learning_rate": 1.7681250000000003e-05,
      "loss": 0.002,
      "step": 155130
    },
    {
      "epoch": 5.171333333333333,
      "grad_norm": 0.44256848096847534,
      "learning_rate": 1.767916666666667e-05,
      "loss": 0.0017,
      "step": 155140
    },
    {
      "epoch": 5.171666666666667,
      "grad_norm": 0.11453799903392792,
      "learning_rate": 1.7677083333333334e-05,
      "loss": 0.0025,
      "step": 155150
    },
    {
      "epoch": 5.172,
      "grad_norm": 0.06011524796485901,
      "learning_rate": 1.7675e-05,
      "loss": 0.0015,
      "step": 155160
    },
    {
      "epoch": 5.1723333333333334,
      "grad_norm": 0.22975975275039673,
      "learning_rate": 1.7672916666666668e-05,
      "loss": 0.0013,
      "step": 155170
    },
    {
      "epoch": 5.172666666666666,
      "grad_norm": 0.11472618579864502,
      "learning_rate": 1.7670833333333334e-05,
      "loss": 0.0022,
      "step": 155180
    },
    {
      "epoch": 5.173,
      "grad_norm": 0.05843834578990936,
      "learning_rate": 1.766875e-05,
      "loss": 0.0019,
      "step": 155190
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.5362032055854797,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0029,
      "step": 155200
    },
    {
      "epoch": 5.173666666666667,
      "grad_norm": 0.2285376936197281,
      "learning_rate": 1.7664583333333334e-05,
      "loss": 0.0022,
      "step": 155210
    },
    {
      "epoch": 5.174,
      "grad_norm": 0.14453110098838806,
      "learning_rate": 1.7662500000000002e-05,
      "loss": 0.0021,
      "step": 155220
    },
    {
      "epoch": 5.174333333333333,
      "grad_norm": 0.08643073588609695,
      "learning_rate": 1.7660416666666668e-05,
      "loss": 0.0024,
      "step": 155230
    },
    {
      "epoch": 5.174666666666667,
      "grad_norm": 0.22898899018764496,
      "learning_rate": 1.7658333333333333e-05,
      "loss": 0.0024,
      "step": 155240
    },
    {
      "epoch": 5.175,
      "grad_norm": 0.08862172067165375,
      "learning_rate": 1.7656250000000002e-05,
      "loss": 0.002,
      "step": 155250
    },
    {
      "epoch": 5.175333333333334,
      "grad_norm": 0.20026226341724396,
      "learning_rate": 1.7654166666666668e-05,
      "loss": 0.0013,
      "step": 155260
    },
    {
      "epoch": 5.175666666666666,
      "grad_norm": 0.08689232170581818,
      "learning_rate": 1.7652083333333333e-05,
      "loss": 0.0014,
      "step": 155270
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.38322728872299194,
      "learning_rate": 1.765e-05,
      "loss": 0.0015,
      "step": 155280
    },
    {
      "epoch": 5.176333333333333,
      "grad_norm": 0.4573632776737213,
      "learning_rate": 1.7647916666666668e-05,
      "loss": 0.0019,
      "step": 155290
    },
    {
      "epoch": 5.176666666666667,
      "grad_norm": 0.6909772753715515,
      "learning_rate": 1.7645833333333333e-05,
      "loss": 0.003,
      "step": 155300
    },
    {
      "epoch": 5.177,
      "grad_norm": 0.007072084117680788,
      "learning_rate": 1.7643750000000002e-05,
      "loss": 0.0028,
      "step": 155310
    },
    {
      "epoch": 5.177333333333333,
      "grad_norm": 0.030420321971178055,
      "learning_rate": 1.7641666666666667e-05,
      "loss": 0.0019,
      "step": 155320
    },
    {
      "epoch": 5.177666666666667,
      "grad_norm": 0.08828455209732056,
      "learning_rate": 1.7639583333333333e-05,
      "loss": 0.0021,
      "step": 155330
    },
    {
      "epoch": 5.178,
      "grad_norm": 0.37167927622795105,
      "learning_rate": 1.7637500000000002e-05,
      "loss": 0.0015,
      "step": 155340
    },
    {
      "epoch": 5.178333333333334,
      "grad_norm": 0.14291280508041382,
      "learning_rate": 1.7635416666666667e-05,
      "loss": 0.0017,
      "step": 155350
    },
    {
      "epoch": 5.1786666666666665,
      "grad_norm": 0.11509164422750473,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0017,
      "step": 155360
    },
    {
      "epoch": 5.179,
      "grad_norm": 0.0575290247797966,
      "learning_rate": 1.763125e-05,
      "loss": 0.002,
      "step": 155370
    },
    {
      "epoch": 5.179333333333333,
      "grad_norm": 0.2004295289516449,
      "learning_rate": 1.7629166666666667e-05,
      "loss": 0.0017,
      "step": 155380
    },
    {
      "epoch": 5.179666666666667,
      "grad_norm": 0.4576296806335449,
      "learning_rate": 1.7627083333333333e-05,
      "loss": 0.0026,
      "step": 155390
    },
    {
      "epoch": 5.18,
      "grad_norm": 0.14304442703723907,
      "learning_rate": 1.7625e-05,
      "loss": 0.0014,
      "step": 155400
    },
    {
      "epoch": 5.1803333333333335,
      "grad_norm": 0.08657652139663696,
      "learning_rate": 1.7622916666666667e-05,
      "loss": 0.0015,
      "step": 155410
    },
    {
      "epoch": 5.180666666666666,
      "grad_norm": 0.058151453733444214,
      "learning_rate": 1.7620833333333332e-05,
      "loss": 0.0016,
      "step": 155420
    },
    {
      "epoch": 5.181,
      "grad_norm": 0.05794784426689148,
      "learning_rate": 1.761875e-05,
      "loss": 0.0017,
      "step": 155430
    },
    {
      "epoch": 5.181333333333333,
      "grad_norm": 0.00753824831917882,
      "learning_rate": 1.7616666666666667e-05,
      "loss": 0.0015,
      "step": 155440
    },
    {
      "epoch": 5.181666666666667,
      "grad_norm": 0.31484124064445496,
      "learning_rate": 1.7614583333333336e-05,
      "loss": 0.0023,
      "step": 155450
    },
    {
      "epoch": 5.182,
      "grad_norm": 0.22851069271564484,
      "learning_rate": 1.76125e-05,
      "loss": 0.0018,
      "step": 155460
    },
    {
      "epoch": 5.182333333333333,
      "grad_norm": 0.06000278890132904,
      "learning_rate": 1.7610416666666667e-05,
      "loss": 0.0021,
      "step": 155470
    },
    {
      "epoch": 5.182666666666667,
      "grad_norm": 0.007754033897072077,
      "learning_rate": 1.7608333333333336e-05,
      "loss": 0.0015,
      "step": 155480
    },
    {
      "epoch": 5.183,
      "grad_norm": 0.48641788959503174,
      "learning_rate": 1.760625e-05,
      "loss": 0.0013,
      "step": 155490
    },
    {
      "epoch": 5.183333333333334,
      "grad_norm": 0.2569013237953186,
      "learning_rate": 1.760416666666667e-05,
      "loss": 0.0026,
      "step": 155500
    },
    {
      "epoch": 5.183666666666666,
      "grad_norm": 0.43015971779823303,
      "learning_rate": 1.7602083333333332e-05,
      "loss": 0.0023,
      "step": 155510
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.08608155697584152,
      "learning_rate": 1.76e-05,
      "loss": 0.0023,
      "step": 155520
    },
    {
      "epoch": 5.184333333333333,
      "grad_norm": 0.0574009083211422,
      "learning_rate": 1.7597916666666666e-05,
      "loss": 0.0022,
      "step": 155530
    },
    {
      "epoch": 5.184666666666667,
      "grad_norm": 0.1998952478170395,
      "learning_rate": 1.7595833333333335e-05,
      "loss": 0.0029,
      "step": 155540
    },
    {
      "epoch": 5.185,
      "grad_norm": 0.11516023427248001,
      "learning_rate": 1.759375e-05,
      "loss": 0.0014,
      "step": 155550
    },
    {
      "epoch": 5.185333333333333,
      "grad_norm": 0.1438157856464386,
      "learning_rate": 1.7591666666666666e-05,
      "loss": 0.0017,
      "step": 155560
    },
    {
      "epoch": 5.185666666666667,
      "grad_norm": 0.14214754104614258,
      "learning_rate": 1.7589583333333335e-05,
      "loss": 0.0021,
      "step": 155570
    },
    {
      "epoch": 5.186,
      "grad_norm": 0.25818100571632385,
      "learning_rate": 1.75875e-05,
      "loss": 0.0011,
      "step": 155580
    },
    {
      "epoch": 5.186333333333334,
      "grad_norm": 0.40004682540893555,
      "learning_rate": 1.758541666666667e-05,
      "loss": 0.0015,
      "step": 155590
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 0.0861908420920372,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.0018,
      "step": 155600
    },
    {
      "epoch": 5.187,
      "grad_norm": 0.029352229088544846,
      "learning_rate": 1.758125e-05,
      "loss": 0.0017,
      "step": 155610
    },
    {
      "epoch": 5.187333333333333,
      "grad_norm": 0.14325885474681854,
      "learning_rate": 1.757916666666667e-05,
      "loss": 0.0024,
      "step": 155620
    },
    {
      "epoch": 5.187666666666667,
      "grad_norm": 0.08624601364135742,
      "learning_rate": 1.757708333333333e-05,
      "loss": 0.0019,
      "step": 155630
    },
    {
      "epoch": 5.188,
      "grad_norm": 0.11462104320526123,
      "learning_rate": 1.7575e-05,
      "loss": 0.0022,
      "step": 155640
    },
    {
      "epoch": 5.1883333333333335,
      "grad_norm": 0.010172748006880283,
      "learning_rate": 1.7572916666666666e-05,
      "loss": 0.0013,
      "step": 155650
    },
    {
      "epoch": 5.188666666666666,
      "grad_norm": 0.2578738033771515,
      "learning_rate": 1.7570833333333335e-05,
      "loss": 0.0022,
      "step": 155660
    },
    {
      "epoch": 5.189,
      "grad_norm": 0.08628369122743607,
      "learning_rate": 1.756875e-05,
      "loss": 0.0021,
      "step": 155670
    },
    {
      "epoch": 5.189333333333333,
      "grad_norm": 0.14254769682884216,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.0021,
      "step": 155680
    },
    {
      "epoch": 5.189666666666667,
      "grad_norm": 0.030604656785726547,
      "learning_rate": 1.7564583333333334e-05,
      "loss": 0.0018,
      "step": 155690
    },
    {
      "epoch": 5.19,
      "grad_norm": 0.31795838475227356,
      "learning_rate": 1.75625e-05,
      "loss": 0.0019,
      "step": 155700
    },
    {
      "epoch": 5.190333333333333,
      "grad_norm": 0.058449048548936844,
      "learning_rate": 1.756041666666667e-05,
      "loss": 0.0014,
      "step": 155710
    },
    {
      "epoch": 5.190666666666667,
      "grad_norm": 0.20191121101379395,
      "learning_rate": 1.7558333333333334e-05,
      "loss": 0.002,
      "step": 155720
    },
    {
      "epoch": 5.191,
      "grad_norm": 0.3427293598651886,
      "learning_rate": 1.7556250000000003e-05,
      "loss": 0.0018,
      "step": 155730
    },
    {
      "epoch": 5.191333333333334,
      "grad_norm": 0.32367271184921265,
      "learning_rate": 1.755416666666667e-05,
      "loss": 0.0012,
      "step": 155740
    },
    {
      "epoch": 5.191666666666666,
      "grad_norm": 0.32223689556121826,
      "learning_rate": 1.7552083333333334e-05,
      "loss": 0.0026,
      "step": 155750
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.012833046726882458,
      "learning_rate": 1.755e-05,
      "loss": 0.0015,
      "step": 155760
    },
    {
      "epoch": 5.192333333333333,
      "grad_norm": 0.22893773019313812,
      "learning_rate": 1.7547916666666665e-05,
      "loss": 0.0026,
      "step": 155770
    },
    {
      "epoch": 5.192666666666667,
      "grad_norm": 0.0858825072646141,
      "learning_rate": 1.7545833333333334e-05,
      "loss": 0.0012,
      "step": 155780
    },
    {
      "epoch": 5.193,
      "grad_norm": 0.17310400307178497,
      "learning_rate": 1.754375e-05,
      "loss": 0.0021,
      "step": 155790
    },
    {
      "epoch": 5.193333333333333,
      "grad_norm": 0.40001171827316284,
      "learning_rate": 1.754166666666667e-05,
      "loss": 0.0015,
      "step": 155800
    },
    {
      "epoch": 5.193666666666667,
      "grad_norm": 0.22895970940589905,
      "learning_rate": 1.7539583333333334e-05,
      "loss": 0.0017,
      "step": 155810
    },
    {
      "epoch": 5.194,
      "grad_norm": 0.14333587884902954,
      "learning_rate": 1.7537500000000003e-05,
      "loss": 0.0014,
      "step": 155820
    },
    {
      "epoch": 5.194333333333334,
      "grad_norm": 0.14330312609672546,
      "learning_rate": 1.7535416666666668e-05,
      "loss": 0.0016,
      "step": 155830
    },
    {
      "epoch": 5.1946666666666665,
      "grad_norm": 0.14062751829624176,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0015,
      "step": 155840
    },
    {
      "epoch": 5.195,
      "grad_norm": 0.008722064085304737,
      "learning_rate": 1.7531250000000003e-05,
      "loss": 0.0014,
      "step": 155850
    },
    {
      "epoch": 5.195333333333333,
      "grad_norm": 0.512814462184906,
      "learning_rate": 1.7529166666666668e-05,
      "loss": 0.0013,
      "step": 155860
    },
    {
      "epoch": 5.195666666666667,
      "grad_norm": 0.2333415448665619,
      "learning_rate": 1.7527083333333334e-05,
      "loss": 0.0024,
      "step": 155870
    },
    {
      "epoch": 5.196,
      "grad_norm": 0.3197647035121918,
      "learning_rate": 1.7525e-05,
      "loss": 0.0016,
      "step": 155880
    },
    {
      "epoch": 5.1963333333333335,
      "grad_norm": 0.004237838089466095,
      "learning_rate": 1.7522916666666668e-05,
      "loss": 0.0016,
      "step": 155890
    },
    {
      "epoch": 5.196666666666666,
      "grad_norm": 0.1816694736480713,
      "learning_rate": 1.7520833333333333e-05,
      "loss": 0.0022,
      "step": 155900
    },
    {
      "epoch": 5.197,
      "grad_norm": 0.4338873326778412,
      "learning_rate": 1.751875e-05,
      "loss": 0.0015,
      "step": 155910
    },
    {
      "epoch": 5.197333333333333,
      "grad_norm": 0.38650962710380554,
      "learning_rate": 1.7516666666666668e-05,
      "loss": 0.0016,
      "step": 155920
    },
    {
      "epoch": 5.197666666666667,
      "grad_norm": 0.036979202181100845,
      "learning_rate": 1.7514583333333333e-05,
      "loss": 0.0016,
      "step": 155930
    },
    {
      "epoch": 5.198,
      "grad_norm": 0.31545114517211914,
      "learning_rate": 1.7512500000000002e-05,
      "loss": 0.0028,
      "step": 155940
    },
    {
      "epoch": 5.198333333333333,
      "grad_norm": 0.08716674894094467,
      "learning_rate": 1.7510416666666668e-05,
      "loss": 0.0026,
      "step": 155950
    },
    {
      "epoch": 5.198666666666667,
      "grad_norm": 0.257671982049942,
      "learning_rate": 1.7508333333333337e-05,
      "loss": 0.0016,
      "step": 155960
    },
    {
      "epoch": 5.199,
      "grad_norm": 0.400330126285553,
      "learning_rate": 1.7506250000000002e-05,
      "loss": 0.0016,
      "step": 155970
    },
    {
      "epoch": 5.199333333333334,
      "grad_norm": 0.4852244257926941,
      "learning_rate": 1.7504166666666667e-05,
      "loss": 0.002,
      "step": 155980
    },
    {
      "epoch": 5.199666666666666,
      "grad_norm": 0.11459442973136902,
      "learning_rate": 1.7502083333333333e-05,
      "loss": 0.0017,
      "step": 155990
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.2006739377975464,
      "learning_rate": 1.75e-05,
      "loss": 0.0021,
      "step": 156000
    },
    {
      "epoch": 5.200333333333333,
      "grad_norm": 0.27469730377197266,
      "learning_rate": 1.7497916666666667e-05,
      "loss": 0.0021,
      "step": 156010
    },
    {
      "epoch": 5.200666666666667,
      "grad_norm": 0.25879162549972534,
      "learning_rate": 1.7495833333333333e-05,
      "loss": 0.0023,
      "step": 156020
    },
    {
      "epoch": 5.201,
      "grad_norm": 0.48646676540374756,
      "learning_rate": 1.7493750000000002e-05,
      "loss": 0.0018,
      "step": 156030
    },
    {
      "epoch": 5.201333333333333,
      "grad_norm": 0.11421750485897064,
      "learning_rate": 1.7491666666666667e-05,
      "loss": 0.0019,
      "step": 156040
    },
    {
      "epoch": 5.201666666666666,
      "grad_norm": 0.3595421314239502,
      "learning_rate": 1.7489583333333333e-05,
      "loss": 0.0021,
      "step": 156050
    },
    {
      "epoch": 5.202,
      "grad_norm": 0.11470922827720642,
      "learning_rate": 1.74875e-05,
      "loss": 0.0032,
      "step": 156060
    },
    {
      "epoch": 5.202333333333334,
      "grad_norm": 0.11569439619779587,
      "learning_rate": 1.7485416666666667e-05,
      "loss": 0.0016,
      "step": 156070
    },
    {
      "epoch": 5.2026666666666666,
      "grad_norm": 0.17188423871994019,
      "learning_rate": 1.7483333333333336e-05,
      "loss": 0.0022,
      "step": 156080
    },
    {
      "epoch": 5.203,
      "grad_norm": 0.19981105625629425,
      "learning_rate": 1.748125e-05,
      "loss": 0.0013,
      "step": 156090
    },
    {
      "epoch": 5.203333333333333,
      "grad_norm": 0.2611265778541565,
      "learning_rate": 1.747916666666667e-05,
      "loss": 0.0022,
      "step": 156100
    },
    {
      "epoch": 5.203666666666667,
      "grad_norm": 0.17180828750133514,
      "learning_rate": 1.7477083333333332e-05,
      "loss": 0.0011,
      "step": 156110
    },
    {
      "epoch": 5.204,
      "grad_norm": 0.05782565847039223,
      "learning_rate": 1.7475e-05,
      "loss": 0.0016,
      "step": 156120
    },
    {
      "epoch": 5.2043333333333335,
      "grad_norm": 0.007924661040306091,
      "learning_rate": 1.7472916666666667e-05,
      "loss": 0.0013,
      "step": 156130
    },
    {
      "epoch": 5.204666666666666,
      "grad_norm": 0.05804520472884178,
      "learning_rate": 1.7470833333333332e-05,
      "loss": 0.0019,
      "step": 156140
    },
    {
      "epoch": 5.205,
      "grad_norm": 0.6683840751647949,
      "learning_rate": 1.746875e-05,
      "loss": 0.002,
      "step": 156150
    },
    {
      "epoch": 5.205333333333333,
      "grad_norm": 0.11530651897192001,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0027,
      "step": 156160
    },
    {
      "epoch": 5.205666666666667,
      "grad_norm": 0.25718623399734497,
      "learning_rate": 1.7464583333333335e-05,
      "loss": 0.0025,
      "step": 156170
    },
    {
      "epoch": 5.206,
      "grad_norm": 0.03285038843750954,
      "learning_rate": 1.74625e-05,
      "loss": 0.002,
      "step": 156180
    },
    {
      "epoch": 5.206333333333333,
      "grad_norm": 0.013183803297579288,
      "learning_rate": 1.7460416666666666e-05,
      "loss": 0.0019,
      "step": 156190
    },
    {
      "epoch": 5.206666666666667,
      "grad_norm": 0.03175780177116394,
      "learning_rate": 1.7458333333333335e-05,
      "loss": 0.0026,
      "step": 156200
    },
    {
      "epoch": 5.207,
      "grad_norm": 0.11504748463630676,
      "learning_rate": 1.745625e-05,
      "loss": 0.0018,
      "step": 156210
    },
    {
      "epoch": 5.207333333333334,
      "grad_norm": 0.2861434817314148,
      "learning_rate": 1.745416666666667e-05,
      "loss": 0.0031,
      "step": 156220
    },
    {
      "epoch": 5.207666666666666,
      "grad_norm": 0.2858833074569702,
      "learning_rate": 1.7452083333333332e-05,
      "loss": 0.0019,
      "step": 156230
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.21616050601005554,
      "learning_rate": 1.745e-05,
      "loss": 0.0022,
      "step": 156240
    },
    {
      "epoch": 5.208333333333333,
      "grad_norm": 0.22867323458194733,
      "learning_rate": 1.7447916666666666e-05,
      "loss": 0.0017,
      "step": 156250
    },
    {
      "epoch": 5.208666666666667,
      "grad_norm": 0.03131626918911934,
      "learning_rate": 1.7445833333333335e-05,
      "loss": 0.003,
      "step": 156260
    },
    {
      "epoch": 5.209,
      "grad_norm": 0.08631189912557602,
      "learning_rate": 1.744375e-05,
      "loss": 0.0027,
      "step": 156270
    },
    {
      "epoch": 5.209333333333333,
      "grad_norm": 0.05711580440402031,
      "learning_rate": 1.7441666666666666e-05,
      "loss": 0.0024,
      "step": 156280
    },
    {
      "epoch": 5.209666666666667,
      "grad_norm": 0.4002198576927185,
      "learning_rate": 1.7439583333333335e-05,
      "loss": 0.0026,
      "step": 156290
    },
    {
      "epoch": 5.21,
      "grad_norm": 0.17173315584659576,
      "learning_rate": 1.74375e-05,
      "loss": 0.0019,
      "step": 156300
    },
    {
      "epoch": 5.210333333333334,
      "grad_norm": 0.031056636944413185,
      "learning_rate": 1.743541666666667e-05,
      "loss": 0.0019,
      "step": 156310
    },
    {
      "epoch": 5.210666666666667,
      "grad_norm": 0.05905262380838394,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0014,
      "step": 156320
    },
    {
      "epoch": 5.211,
      "grad_norm": 0.0578140988945961,
      "learning_rate": 1.743125e-05,
      "loss": 0.0017,
      "step": 156330
    },
    {
      "epoch": 5.211333333333333,
      "grad_norm": 0.18409420549869537,
      "learning_rate": 1.742916666666667e-05,
      "loss": 0.0024,
      "step": 156340
    },
    {
      "epoch": 5.211666666666667,
      "grad_norm": 0.20065735280513763,
      "learning_rate": 1.7427083333333335e-05,
      "loss": 0.0022,
      "step": 156350
    },
    {
      "epoch": 5.212,
      "grad_norm": 0.45685139298439026,
      "learning_rate": 1.7425e-05,
      "loss": 0.0017,
      "step": 156360
    },
    {
      "epoch": 5.2123333333333335,
      "grad_norm": 0.17156046628952026,
      "learning_rate": 1.7422916666666666e-05,
      "loss": 0.0021,
      "step": 156370
    },
    {
      "epoch": 5.212666666666666,
      "grad_norm": 0.37896835803985596,
      "learning_rate": 1.7420833333333334e-05,
      "loss": 0.002,
      "step": 156380
    },
    {
      "epoch": 5.213,
      "grad_norm": 0.03010861761868,
      "learning_rate": 1.741875e-05,
      "loss": 0.0023,
      "step": 156390
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 0.28595593571662903,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.0019,
      "step": 156400
    },
    {
      "epoch": 5.213666666666667,
      "grad_norm": 0.6341903209686279,
      "learning_rate": 1.7414583333333334e-05,
      "loss": 0.0016,
      "step": 156410
    },
    {
      "epoch": 5.214,
      "grad_norm": 0.2861676812171936,
      "learning_rate": 1.74125e-05,
      "loss": 0.0037,
      "step": 156420
    },
    {
      "epoch": 5.214333333333333,
      "grad_norm": 0.22931236028671265,
      "learning_rate": 1.741041666666667e-05,
      "loss": 0.0013,
      "step": 156430
    },
    {
      "epoch": 5.214666666666667,
      "grad_norm": 0.12911804020404816,
      "learning_rate": 1.7408333333333334e-05,
      "loss": 0.0022,
      "step": 156440
    },
    {
      "epoch": 5.215,
      "grad_norm": 0.030296240001916885,
      "learning_rate": 1.7406250000000003e-05,
      "loss": 0.0017,
      "step": 156450
    },
    {
      "epoch": 5.215333333333334,
      "grad_norm": 0.1145676001906395,
      "learning_rate": 1.740416666666667e-05,
      "loss": 0.0018,
      "step": 156460
    },
    {
      "epoch": 5.2156666666666665,
      "grad_norm": 0.20005713403224945,
      "learning_rate": 1.7402083333333334e-05,
      "loss": 0.0019,
      "step": 156470
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.14343586564064026,
      "learning_rate": 1.74e-05,
      "loss": 0.0019,
      "step": 156480
    },
    {
      "epoch": 5.216333333333333,
      "grad_norm": 0.031223246827721596,
      "learning_rate": 1.7397916666666665e-05,
      "loss": 0.0028,
      "step": 156490
    },
    {
      "epoch": 5.216666666666667,
      "grad_norm": 0.25699537992477417,
      "learning_rate": 1.7395833333333334e-05,
      "loss": 0.0022,
      "step": 156500
    },
    {
      "epoch": 5.217,
      "grad_norm": 0.22740793228149414,
      "learning_rate": 1.739375e-05,
      "loss": 0.0024,
      "step": 156510
    },
    {
      "epoch": 5.217333333333333,
      "grad_norm": 0.6942052245140076,
      "learning_rate": 1.7391666666666668e-05,
      "loss": 0.0024,
      "step": 156520
    },
    {
      "epoch": 5.217666666666666,
      "grad_norm": 0.35751327872276306,
      "learning_rate": 1.7389583333333334e-05,
      "loss": 0.0021,
      "step": 156530
    },
    {
      "epoch": 5.218,
      "grad_norm": 0.008378377184271812,
      "learning_rate": 1.7387500000000003e-05,
      "loss": 0.0023,
      "step": 156540
    },
    {
      "epoch": 5.218333333333334,
      "grad_norm": 0.3144865930080414,
      "learning_rate": 1.7385416666666668e-05,
      "loss": 0.002,
      "step": 156550
    },
    {
      "epoch": 5.218666666666667,
      "grad_norm": 0.1718803495168686,
      "learning_rate": 1.7383333333333333e-05,
      "loss": 0.0015,
      "step": 156560
    },
    {
      "epoch": 5.219,
      "grad_norm": 0.11499954760074615,
      "learning_rate": 1.7381250000000002e-05,
      "loss": 0.0022,
      "step": 156570
    },
    {
      "epoch": 5.219333333333333,
      "grad_norm": 0.22889289259910583,
      "learning_rate": 1.7379166666666668e-05,
      "loss": 0.0025,
      "step": 156580
    },
    {
      "epoch": 5.219666666666667,
      "grad_norm": 0.6392954587936401,
      "learning_rate": 1.7377083333333337e-05,
      "loss": 0.0014,
      "step": 156590
    },
    {
      "epoch": 5.22,
      "grad_norm": 0.04869113862514496,
      "learning_rate": 1.7375e-05,
      "loss": 0.003,
      "step": 156600
    },
    {
      "epoch": 5.2203333333333335,
      "grad_norm": 0.43736982345581055,
      "learning_rate": 1.7372916666666668e-05,
      "loss": 0.0021,
      "step": 156610
    },
    {
      "epoch": 5.220666666666666,
      "grad_norm": 0.11544852703809738,
      "learning_rate": 1.7370833333333333e-05,
      "loss": 0.0019,
      "step": 156620
    },
    {
      "epoch": 5.221,
      "grad_norm": 0.22923146188259125,
      "learning_rate": 1.736875e-05,
      "loss": 0.0013,
      "step": 156630
    },
    {
      "epoch": 5.221333333333333,
      "grad_norm": 0.22939187288284302,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0018,
      "step": 156640
    },
    {
      "epoch": 5.221666666666667,
      "grad_norm": 0.33034154772758484,
      "learning_rate": 1.7364583333333333e-05,
      "loss": 0.0024,
      "step": 156650
    },
    {
      "epoch": 5.222,
      "grad_norm": 0.058307938277721405,
      "learning_rate": 1.7362500000000002e-05,
      "loss": 0.0014,
      "step": 156660
    },
    {
      "epoch": 5.222333333333333,
      "grad_norm": 0.14368288218975067,
      "learning_rate": 1.7360416666666667e-05,
      "loss": 0.0017,
      "step": 156670
    },
    {
      "epoch": 5.222666666666667,
      "grad_norm": 0.00993611291050911,
      "learning_rate": 1.7358333333333336e-05,
      "loss": 0.0021,
      "step": 156680
    },
    {
      "epoch": 5.223,
      "grad_norm": 0.22882170975208282,
      "learning_rate": 1.7356250000000002e-05,
      "loss": 0.0026,
      "step": 156690
    },
    {
      "epoch": 5.223333333333334,
      "grad_norm": 0.006907918024808168,
      "learning_rate": 1.7354166666666667e-05,
      "loss": 0.0017,
      "step": 156700
    },
    {
      "epoch": 5.2236666666666665,
      "grad_norm": 0.07958556711673737,
      "learning_rate": 1.7352083333333336e-05,
      "loss": 0.002,
      "step": 156710
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.11434668302536011,
      "learning_rate": 1.7349999999999998e-05,
      "loss": 0.0019,
      "step": 156720
    },
    {
      "epoch": 5.224333333333333,
      "grad_norm": 0.22879517078399658,
      "learning_rate": 1.7347916666666667e-05,
      "loss": 0.0019,
      "step": 156730
    },
    {
      "epoch": 5.224666666666667,
      "grad_norm": 0.031072385609149933,
      "learning_rate": 1.7345833333333333e-05,
      "loss": 0.0014,
      "step": 156740
    },
    {
      "epoch": 5.225,
      "grad_norm": 0.14282704889774323,
      "learning_rate": 1.734375e-05,
      "loss": 0.0022,
      "step": 156750
    },
    {
      "epoch": 5.225333333333333,
      "grad_norm": 0.4174094796180725,
      "learning_rate": 1.7341666666666667e-05,
      "loss": 0.0015,
      "step": 156760
    },
    {
      "epoch": 5.225666666666666,
      "grad_norm": 0.18337148427963257,
      "learning_rate": 1.7339583333333332e-05,
      "loss": 0.0027,
      "step": 156770
    },
    {
      "epoch": 5.226,
      "grad_norm": 0.2860791087150574,
      "learning_rate": 1.73375e-05,
      "loss": 0.002,
      "step": 156780
    },
    {
      "epoch": 5.226333333333334,
      "grad_norm": 0.11534406244754791,
      "learning_rate": 1.7335416666666667e-05,
      "loss": 0.0022,
      "step": 156790
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.22823937237262726,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0017,
      "step": 156800
    },
    {
      "epoch": 5.227,
      "grad_norm": 0.3430649936199188,
      "learning_rate": 1.733125e-05,
      "loss": 0.0018,
      "step": 156810
    },
    {
      "epoch": 5.227333333333333,
      "grad_norm": 0.19136206805706024,
      "learning_rate": 1.732916666666667e-05,
      "loss": 0.0019,
      "step": 156820
    },
    {
      "epoch": 5.227666666666667,
      "grad_norm": 0.05808189883828163,
      "learning_rate": 1.7327083333333336e-05,
      "loss": 0.0014,
      "step": 156830
    },
    {
      "epoch": 5.228,
      "grad_norm": 0.11446493119001389,
      "learning_rate": 1.7325e-05,
      "loss": 0.0023,
      "step": 156840
    },
    {
      "epoch": 5.2283333333333335,
      "grad_norm": 0.03469887375831604,
      "learning_rate": 1.7322916666666666e-05,
      "loss": 0.0016,
      "step": 156850
    },
    {
      "epoch": 5.228666666666666,
      "grad_norm": 0.05817236751317978,
      "learning_rate": 1.7320833333333332e-05,
      "loss": 0.002,
      "step": 156860
    },
    {
      "epoch": 5.229,
      "grad_norm": 0.030183231458067894,
      "learning_rate": 1.731875e-05,
      "loss": 0.0023,
      "step": 156870
    },
    {
      "epoch": 5.229333333333333,
      "grad_norm": 0.5429303050041199,
      "learning_rate": 1.7316666666666666e-05,
      "loss": 0.0018,
      "step": 156880
    },
    {
      "epoch": 5.229666666666667,
      "grad_norm": 0.05775435268878937,
      "learning_rate": 1.7314583333333335e-05,
      "loss": 0.0018,
      "step": 156890
    },
    {
      "epoch": 5.23,
      "grad_norm": 0.05734582990407944,
      "learning_rate": 1.73125e-05,
      "loss": 0.0014,
      "step": 156900
    },
    {
      "epoch": 5.230333333333333,
      "grad_norm": 0.11452583968639374,
      "learning_rate": 1.7310416666666666e-05,
      "loss": 0.002,
      "step": 156910
    },
    {
      "epoch": 5.230666666666667,
      "grad_norm": 0.25776979327201843,
      "learning_rate": 1.7308333333333335e-05,
      "loss": 0.002,
      "step": 156920
    },
    {
      "epoch": 5.231,
      "grad_norm": 0.31434816122055054,
      "learning_rate": 1.730625e-05,
      "loss": 0.0016,
      "step": 156930
    },
    {
      "epoch": 5.231333333333334,
      "grad_norm": 0.14297893643379211,
      "learning_rate": 1.730416666666667e-05,
      "loss": 0.0023,
      "step": 156940
    },
    {
      "epoch": 5.2316666666666665,
      "grad_norm": 0.2860898971557617,
      "learning_rate": 1.7302083333333335e-05,
      "loss": 0.0027,
      "step": 156950
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.008655942976474762,
      "learning_rate": 1.73e-05,
      "loss": 0.0018,
      "step": 156960
    },
    {
      "epoch": 5.232333333333333,
      "grad_norm": 0.3151581287384033,
      "learning_rate": 1.7297916666666666e-05,
      "loss": 0.0016,
      "step": 156970
    },
    {
      "epoch": 5.232666666666667,
      "grad_norm": 0.11456813663244247,
      "learning_rate": 1.7295833333333335e-05,
      "loss": 0.0019,
      "step": 156980
    },
    {
      "epoch": 5.233,
      "grad_norm": 0.32342997193336487,
      "learning_rate": 1.729375e-05,
      "loss": 0.0022,
      "step": 156990
    },
    {
      "epoch": 5.233333333333333,
      "grad_norm": 0.25723108649253845,
      "learning_rate": 1.7291666666666666e-05,
      "loss": 0.0014,
      "step": 157000
    },
    {
      "epoch": 5.233666666666666,
      "grad_norm": 0.08580339699983597,
      "learning_rate": 1.7289583333333335e-05,
      "loss": 0.0023,
      "step": 157010
    },
    {
      "epoch": 5.234,
      "grad_norm": 0.17198936641216278,
      "learning_rate": 1.72875e-05,
      "loss": 0.0018,
      "step": 157020
    },
    {
      "epoch": 5.234333333333334,
      "grad_norm": 0.14304566383361816,
      "learning_rate": 1.728541666666667e-05,
      "loss": 0.0025,
      "step": 157030
    },
    {
      "epoch": 5.234666666666667,
      "grad_norm": 0.0678442120552063,
      "learning_rate": 1.7283333333333334e-05,
      "loss": 0.0015,
      "step": 157040
    },
    {
      "epoch": 5.235,
      "grad_norm": 0.058424804359674454,
      "learning_rate": 1.728125e-05,
      "loss": 0.0021,
      "step": 157050
    },
    {
      "epoch": 5.235333333333333,
      "grad_norm": 0.14324656128883362,
      "learning_rate": 1.727916666666667e-05,
      "loss": 0.0018,
      "step": 157060
    },
    {
      "epoch": 5.235666666666667,
      "grad_norm": 0.057997532188892365,
      "learning_rate": 1.7277083333333334e-05,
      "loss": 0.0017,
      "step": 157070
    },
    {
      "epoch": 5.236,
      "grad_norm": 0.17160120606422424,
      "learning_rate": 1.7275e-05,
      "loss": 0.0029,
      "step": 157080
    },
    {
      "epoch": 5.2363333333333335,
      "grad_norm": 0.08572111278772354,
      "learning_rate": 1.7272916666666665e-05,
      "loss": 0.0017,
      "step": 157090
    },
    {
      "epoch": 5.236666666666666,
      "grad_norm": 0.08593906462192535,
      "learning_rate": 1.7270833333333334e-05,
      "loss": 0.0016,
      "step": 157100
    },
    {
      "epoch": 5.237,
      "grad_norm": 0.14333540201187134,
      "learning_rate": 1.726875e-05,
      "loss": 0.0021,
      "step": 157110
    },
    {
      "epoch": 5.237333333333333,
      "grad_norm": 0.14253148436546326,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0019,
      "step": 157120
    },
    {
      "epoch": 5.237666666666667,
      "grad_norm": 0.257158100605011,
      "learning_rate": 1.7264583333333334e-05,
      "loss": 0.0018,
      "step": 157130
    },
    {
      "epoch": 5.2379999999999995,
      "grad_norm": 0.20018494129180908,
      "learning_rate": 1.72625e-05,
      "loss": 0.0017,
      "step": 157140
    },
    {
      "epoch": 5.238333333333333,
      "grad_norm": 0.14356563985347748,
      "learning_rate": 1.726041666666667e-05,
      "loss": 0.002,
      "step": 157150
    },
    {
      "epoch": 5.238666666666667,
      "grad_norm": 0.17184075713157654,
      "learning_rate": 1.7258333333333334e-05,
      "loss": 0.0013,
      "step": 157160
    },
    {
      "epoch": 5.239,
      "grad_norm": 0.11343128979206085,
      "learning_rate": 1.7256250000000003e-05,
      "loss": 0.0014,
      "step": 157170
    },
    {
      "epoch": 5.239333333333334,
      "grad_norm": 0.3185758590698242,
      "learning_rate": 1.7254166666666668e-05,
      "loss": 0.0022,
      "step": 157180
    },
    {
      "epoch": 5.2396666666666665,
      "grad_norm": 0.2569192051887512,
      "learning_rate": 1.7252083333333337e-05,
      "loss": 0.0014,
      "step": 157190
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.28708600997924805,
      "learning_rate": 1.725e-05,
      "loss": 0.0016,
      "step": 157200
    },
    {
      "epoch": 5.240333333333333,
      "grad_norm": 0.42820048332214355,
      "learning_rate": 1.7247916666666668e-05,
      "loss": 0.0015,
      "step": 157210
    },
    {
      "epoch": 5.240666666666667,
      "grad_norm": 0.2886766493320465,
      "learning_rate": 1.7245833333333334e-05,
      "loss": 0.0019,
      "step": 157220
    },
    {
      "epoch": 5.241,
      "grad_norm": 0.057410988956689835,
      "learning_rate": 1.724375e-05,
      "loss": 0.0022,
      "step": 157230
    },
    {
      "epoch": 5.241333333333333,
      "grad_norm": 0.2571254372596741,
      "learning_rate": 1.7241666666666668e-05,
      "loss": 0.0021,
      "step": 157240
    },
    {
      "epoch": 5.241666666666666,
      "grad_norm": 0.14396712183952332,
      "learning_rate": 1.7239583333333333e-05,
      "loss": 0.0021,
      "step": 157250
    },
    {
      "epoch": 5.242,
      "grad_norm": 0.515036940574646,
      "learning_rate": 1.7237500000000002e-05,
      "loss": 0.0017,
      "step": 157260
    },
    {
      "epoch": 5.242333333333334,
      "grad_norm": 0.6284030675888062,
      "learning_rate": 1.7235416666666668e-05,
      "loss": 0.0023,
      "step": 157270
    },
    {
      "epoch": 5.242666666666667,
      "grad_norm": 0.314208984375,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0019,
      "step": 157280
    },
    {
      "epoch": 5.243,
      "grad_norm": 0.2573142647743225,
      "learning_rate": 1.7231250000000002e-05,
      "loss": 0.0014,
      "step": 157290
    },
    {
      "epoch": 5.243333333333333,
      "grad_norm": 0.1475379765033722,
      "learning_rate": 1.7229166666666668e-05,
      "loss": 0.0012,
      "step": 157300
    },
    {
      "epoch": 5.243666666666667,
      "grad_norm": 0.22931250929832458,
      "learning_rate": 1.7227083333333336e-05,
      "loss": 0.002,
      "step": 157310
    },
    {
      "epoch": 5.244,
      "grad_norm": 0.17185741662979126,
      "learning_rate": 1.7225e-05,
      "loss": 0.0019,
      "step": 157320
    },
    {
      "epoch": 5.2443333333333335,
      "grad_norm": 0.08597906678915024,
      "learning_rate": 1.7222916666666667e-05,
      "loss": 0.0018,
      "step": 157330
    },
    {
      "epoch": 5.244666666666666,
      "grad_norm": 0.20005957782268524,
      "learning_rate": 1.7220833333333333e-05,
      "loss": 0.0028,
      "step": 157340
    },
    {
      "epoch": 5.245,
      "grad_norm": 0.5476558804512024,
      "learning_rate": 1.7218750000000002e-05,
      "loss": 0.0016,
      "step": 157350
    },
    {
      "epoch": 5.245333333333333,
      "grad_norm": 0.2550320327281952,
      "learning_rate": 1.7216666666666667e-05,
      "loss": 0.0018,
      "step": 157360
    },
    {
      "epoch": 5.245666666666667,
      "grad_norm": 0.514907717704773,
      "learning_rate": 1.7214583333333333e-05,
      "loss": 0.0014,
      "step": 157370
    },
    {
      "epoch": 5.246,
      "grad_norm": 0.010040750727057457,
      "learning_rate": 1.72125e-05,
      "loss": 0.0015,
      "step": 157380
    },
    {
      "epoch": 5.246333333333333,
      "grad_norm": 0.11469488590955734,
      "learning_rate": 1.7210416666666667e-05,
      "loss": 0.0015,
      "step": 157390
    },
    {
      "epoch": 5.246666666666667,
      "grad_norm": 0.20035651326179504,
      "learning_rate": 1.7208333333333336e-05,
      "loss": 0.0021,
      "step": 157400
    },
    {
      "epoch": 5.247,
      "grad_norm": 0.20005309581756592,
      "learning_rate": 1.720625e-05,
      "loss": 0.0014,
      "step": 157410
    },
    {
      "epoch": 5.247333333333334,
      "grad_norm": 0.08664782345294952,
      "learning_rate": 1.7204166666666667e-05,
      "loss": 0.0022,
      "step": 157420
    },
    {
      "epoch": 5.2476666666666665,
      "grad_norm": 0.028975704684853554,
      "learning_rate": 1.7202083333333336e-05,
      "loss": 0.0019,
      "step": 157430
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.114623062312603,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0022,
      "step": 157440
    },
    {
      "epoch": 5.248333333333333,
      "grad_norm": 0.008942903950810432,
      "learning_rate": 1.7197916666666667e-05,
      "loss": 0.002,
      "step": 157450
    },
    {
      "epoch": 5.248666666666667,
      "grad_norm": 0.3718060255050659,
      "learning_rate": 1.7195833333333332e-05,
      "loss": 0.0018,
      "step": 157460
    },
    {
      "epoch": 5.249,
      "grad_norm": 0.20063601434230804,
      "learning_rate": 1.719375e-05,
      "loss": 0.0022,
      "step": 157470
    },
    {
      "epoch": 5.249333333333333,
      "grad_norm": 0.030913744121789932,
      "learning_rate": 1.7191666666666667e-05,
      "loss": 0.0011,
      "step": 157480
    },
    {
      "epoch": 5.249666666666666,
      "grad_norm": 0.20036427676677704,
      "learning_rate": 1.7189583333333336e-05,
      "loss": 0.0023,
      "step": 157490
    },
    {
      "epoch": 5.25,
      "grad_norm": 0.03158950433135033,
      "learning_rate": 1.71875e-05,
      "loss": 0.0024,
      "step": 157500
    },
    {
      "epoch": 5.250333333333334,
      "grad_norm": 0.08570128679275513,
      "learning_rate": 1.7185416666666667e-05,
      "loss": 0.0019,
      "step": 157510
    },
    {
      "epoch": 5.250666666666667,
      "grad_norm": 0.2617813050746918,
      "learning_rate": 1.7183333333333335e-05,
      "loss": 0.0018,
      "step": 157520
    },
    {
      "epoch": 5.251,
      "grad_norm": 0.05737827718257904,
      "learning_rate": 1.718125e-05,
      "loss": 0.0016,
      "step": 157530
    },
    {
      "epoch": 5.251333333333333,
      "grad_norm": 0.29422253370285034,
      "learning_rate": 1.717916666666667e-05,
      "loss": 0.0014,
      "step": 157540
    },
    {
      "epoch": 5.251666666666667,
      "grad_norm": 0.0317428857088089,
      "learning_rate": 1.7177083333333335e-05,
      "loss": 0.0021,
      "step": 157550
    },
    {
      "epoch": 5.252,
      "grad_norm": 0.17205578088760376,
      "learning_rate": 1.7175e-05,
      "loss": 0.0022,
      "step": 157560
    },
    {
      "epoch": 5.2523333333333335,
      "grad_norm": 0.02939513511955738,
      "learning_rate": 1.7172916666666666e-05,
      "loss": 0.0017,
      "step": 157570
    },
    {
      "epoch": 5.252666666666666,
      "grad_norm": 0.00468419399112463,
      "learning_rate": 1.7170833333333332e-05,
      "loss": 0.0015,
      "step": 157580
    },
    {
      "epoch": 5.253,
      "grad_norm": 0.08279415220022202,
      "learning_rate": 1.716875e-05,
      "loss": 0.0021,
      "step": 157590
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 0.006617773324251175,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.002,
      "step": 157600
    },
    {
      "epoch": 5.253666666666667,
      "grad_norm": 0.14457643032073975,
      "learning_rate": 1.7164583333333335e-05,
      "loss": 0.0016,
      "step": 157610
    },
    {
      "epoch": 5.254,
      "grad_norm": 0.42876774072647095,
      "learning_rate": 1.71625e-05,
      "loss": 0.0022,
      "step": 157620
    },
    {
      "epoch": 5.254333333333333,
      "grad_norm": 0.2576722204685211,
      "learning_rate": 1.716041666666667e-05,
      "loss": 0.0012,
      "step": 157630
    },
    {
      "epoch": 5.254666666666667,
      "grad_norm": 0.14311322569847107,
      "learning_rate": 1.7158333333333335e-05,
      "loss": 0.0017,
      "step": 157640
    },
    {
      "epoch": 5.255,
      "grad_norm": 0.17199161648750305,
      "learning_rate": 1.715625e-05,
      "loss": 0.0018,
      "step": 157650
    },
    {
      "epoch": 5.255333333333334,
      "grad_norm": 0.08565518260002136,
      "learning_rate": 1.715416666666667e-05,
      "loss": 0.0018,
      "step": 157660
    },
    {
      "epoch": 5.2556666666666665,
      "grad_norm": 0.143244668841362,
      "learning_rate": 1.7152083333333335e-05,
      "loss": 0.003,
      "step": 157670
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.11527352035045624,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.0019,
      "step": 157680
    },
    {
      "epoch": 5.256333333333333,
      "grad_norm": 0.4293094277381897,
      "learning_rate": 1.7147916666666666e-05,
      "loss": 0.0014,
      "step": 157690
    },
    {
      "epoch": 5.256666666666667,
      "grad_norm": 0.02934473566710949,
      "learning_rate": 1.7145833333333334e-05,
      "loss": 0.0019,
      "step": 157700
    },
    {
      "epoch": 5.257,
      "grad_norm": 0.057367026805877686,
      "learning_rate": 1.714375e-05,
      "loss": 0.0022,
      "step": 157710
    },
    {
      "epoch": 5.257333333333333,
      "grad_norm": 0.1145956739783287,
      "learning_rate": 1.7141666666666665e-05,
      "loss": 0.0016,
      "step": 157720
    },
    {
      "epoch": 5.257666666666666,
      "grad_norm": 0.029395904392004013,
      "learning_rate": 1.7139583333333334e-05,
      "loss": 0.0014,
      "step": 157730
    },
    {
      "epoch": 5.258,
      "grad_norm": 0.11441640555858612,
      "learning_rate": 1.71375e-05,
      "loss": 0.0019,
      "step": 157740
    },
    {
      "epoch": 5.258333333333334,
      "grad_norm": 0.22933126986026764,
      "learning_rate": 1.713541666666667e-05,
      "loss": 0.0017,
      "step": 157750
    },
    {
      "epoch": 5.258666666666667,
      "grad_norm": 0.45700937509536743,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0018,
      "step": 157760
    },
    {
      "epoch": 5.259,
      "grad_norm": 0.4001607298851013,
      "learning_rate": 1.7131250000000003e-05,
      "loss": 0.0015,
      "step": 157770
    },
    {
      "epoch": 5.259333333333333,
      "grad_norm": 0.22893382608890533,
      "learning_rate": 1.712916666666667e-05,
      "loss": 0.0017,
      "step": 157780
    },
    {
      "epoch": 5.259666666666667,
      "grad_norm": 0.029344771057367325,
      "learning_rate": 1.7127083333333334e-05,
      "loss": 0.0017,
      "step": 157790
    },
    {
      "epoch": 5.26,
      "grad_norm": 0.22819563746452332,
      "learning_rate": 1.7125000000000003e-05,
      "loss": 0.0015,
      "step": 157800
    },
    {
      "epoch": 5.2603333333333335,
      "grad_norm": 0.08589763194322586,
      "learning_rate": 1.7122916666666665e-05,
      "loss": 0.0019,
      "step": 157810
    },
    {
      "epoch": 5.260666666666666,
      "grad_norm": 0.03194289281964302,
      "learning_rate": 1.7120833333333334e-05,
      "loss": 0.0024,
      "step": 157820
    },
    {
      "epoch": 5.261,
      "grad_norm": 0.11472134292125702,
      "learning_rate": 1.711875e-05,
      "loss": 0.002,
      "step": 157830
    },
    {
      "epoch": 5.261333333333333,
      "grad_norm": 0.4567301869392395,
      "learning_rate": 1.7116666666666668e-05,
      "loss": 0.0014,
      "step": 157840
    },
    {
      "epoch": 5.261666666666667,
      "grad_norm": 0.11519740521907806,
      "learning_rate": 1.7114583333333334e-05,
      "loss": 0.0015,
      "step": 157850
    },
    {
      "epoch": 5.2620000000000005,
      "grad_norm": 0.17205876111984253,
      "learning_rate": 1.71125e-05,
      "loss": 0.0018,
      "step": 157860
    },
    {
      "epoch": 5.262333333333333,
      "grad_norm": 0.2853228449821472,
      "learning_rate": 1.7110416666666668e-05,
      "loss": 0.0015,
      "step": 157870
    },
    {
      "epoch": 5.262666666666667,
      "grad_norm": 0.06252798438072205,
      "learning_rate": 1.7108333333333334e-05,
      "loss": 0.0018,
      "step": 157880
    },
    {
      "epoch": 5.263,
      "grad_norm": 0.11432291567325592,
      "learning_rate": 1.7106250000000002e-05,
      "loss": 0.0012,
      "step": 157890
    },
    {
      "epoch": 5.263333333333334,
      "grad_norm": 0.14275269210338593,
      "learning_rate": 1.7104166666666668e-05,
      "loss": 0.0017,
      "step": 157900
    },
    {
      "epoch": 5.2636666666666665,
      "grad_norm": 0.31403446197509766,
      "learning_rate": 1.7102083333333337e-05,
      "loss": 0.0016,
      "step": 157910
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.28649595379829407,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.002,
      "step": 157920
    },
    {
      "epoch": 5.264333333333333,
      "grad_norm": 0.15106314420700073,
      "learning_rate": 1.7097916666666668e-05,
      "loss": 0.0018,
      "step": 157930
    },
    {
      "epoch": 5.264666666666667,
      "grad_norm": 0.4566795527935028,
      "learning_rate": 1.7095833333333333e-05,
      "loss": 0.0019,
      "step": 157940
    },
    {
      "epoch": 5.265,
      "grad_norm": 0.07784927636384964,
      "learning_rate": 1.709375e-05,
      "loss": 0.0014,
      "step": 157950
    },
    {
      "epoch": 5.265333333333333,
      "grad_norm": 0.28581905364990234,
      "learning_rate": 1.7091666666666668e-05,
      "loss": 0.0018,
      "step": 157960
    },
    {
      "epoch": 5.265666666666666,
      "grad_norm": 0.08639118075370789,
      "learning_rate": 1.7089583333333333e-05,
      "loss": 0.0019,
      "step": 157970
    },
    {
      "epoch": 5.266,
      "grad_norm": 0.011450785212218761,
      "learning_rate": 1.7087500000000002e-05,
      "loss": 0.002,
      "step": 157980
    },
    {
      "epoch": 5.266333333333334,
      "grad_norm": 0.2865532636642456,
      "learning_rate": 1.7085416666666667e-05,
      "loss": 0.0017,
      "step": 157990
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.11514464020729065,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.0018,
      "step": 158000
    },
    {
      "epoch": 5.267,
      "grad_norm": 0.08609849214553833,
      "learning_rate": 1.7081250000000002e-05,
      "loss": 0.0017,
      "step": 158010
    },
    {
      "epoch": 5.267333333333333,
      "grad_norm": 0.0866413414478302,
      "learning_rate": 1.7079166666666667e-05,
      "loss": 0.002,
      "step": 158020
    },
    {
      "epoch": 5.267666666666667,
      "grad_norm": 0.1716579794883728,
      "learning_rate": 1.7077083333333336e-05,
      "loss": 0.0025,
      "step": 158030
    },
    {
      "epoch": 5.268,
      "grad_norm": 0.2288038432598114,
      "learning_rate": 1.7075e-05,
      "loss": 0.0017,
      "step": 158040
    },
    {
      "epoch": 5.2683333333333335,
      "grad_norm": 0.2289697229862213,
      "learning_rate": 1.7072916666666667e-05,
      "loss": 0.0017,
      "step": 158050
    },
    {
      "epoch": 5.268666666666666,
      "grad_norm": 0.030414754524827003,
      "learning_rate": 1.7070833333333333e-05,
      "loss": 0.0013,
      "step": 158060
    },
    {
      "epoch": 5.269,
      "grad_norm": 0.05741757154464722,
      "learning_rate": 1.706875e-05,
      "loss": 0.0019,
      "step": 158070
    },
    {
      "epoch": 5.269333333333333,
      "grad_norm": 0.02962173894047737,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.0022,
      "step": 158080
    },
    {
      "epoch": 5.269666666666667,
      "grad_norm": 0.11587858200073242,
      "learning_rate": 1.7064583333333333e-05,
      "loss": 0.0021,
      "step": 158090
    },
    {
      "epoch": 5.27,
      "grad_norm": 0.36062681674957275,
      "learning_rate": 1.70625e-05,
      "loss": 0.0016,
      "step": 158100
    },
    {
      "epoch": 5.270333333333333,
      "grad_norm": 0.3866141140460968,
      "learning_rate": 1.7060416666666667e-05,
      "loss": 0.0024,
      "step": 158110
    },
    {
      "epoch": 5.270666666666667,
      "grad_norm": 0.28569623827934265,
      "learning_rate": 1.7058333333333336e-05,
      "loss": 0.0019,
      "step": 158120
    },
    {
      "epoch": 5.271,
      "grad_norm": 0.03226212412118912,
      "learning_rate": 1.705625e-05,
      "loss": 0.0024,
      "step": 158130
    },
    {
      "epoch": 5.271333333333334,
      "grad_norm": 0.05741450935602188,
      "learning_rate": 1.7054166666666667e-05,
      "loss": 0.0019,
      "step": 158140
    },
    {
      "epoch": 5.2716666666666665,
      "grad_norm": 0.2583138644695282,
      "learning_rate": 1.7052083333333336e-05,
      "loss": 0.0016,
      "step": 158150
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.1151512935757637,
      "learning_rate": 1.705e-05,
      "loss": 0.0014,
      "step": 158160
    },
    {
      "epoch": 5.272333333333333,
      "grad_norm": 0.20056752860546112,
      "learning_rate": 1.7047916666666667e-05,
      "loss": 0.0012,
      "step": 158170
    },
    {
      "epoch": 5.272666666666667,
      "grad_norm": 0.05823826417326927,
      "learning_rate": 1.7045833333333332e-05,
      "loss": 0.0015,
      "step": 158180
    },
    {
      "epoch": 5.273,
      "grad_norm": 0.08637623488903046,
      "learning_rate": 1.704375e-05,
      "loss": 0.0018,
      "step": 158190
    },
    {
      "epoch": 5.273333333333333,
      "grad_norm": 0.14394117891788483,
      "learning_rate": 1.7041666666666666e-05,
      "loss": 0.0018,
      "step": 158200
    },
    {
      "epoch": 5.273666666666666,
      "grad_norm": 0.4105788767337799,
      "learning_rate": 1.7039583333333335e-05,
      "loss": 0.0023,
      "step": 158210
    },
    {
      "epoch": 5.274,
      "grad_norm": 0.03424978628754616,
      "learning_rate": 1.70375e-05,
      "loss": 0.0014,
      "step": 158220
    },
    {
      "epoch": 5.274333333333333,
      "grad_norm": 0.3426358103752136,
      "learning_rate": 1.7035416666666666e-05,
      "loss": 0.0015,
      "step": 158230
    },
    {
      "epoch": 5.274666666666667,
      "grad_norm": 0.029042234644293785,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0018,
      "step": 158240
    },
    {
      "epoch": 5.275,
      "grad_norm": 0.14400918781757355,
      "learning_rate": 1.703125e-05,
      "loss": 0.0025,
      "step": 158250
    },
    {
      "epoch": 5.275333333333333,
      "grad_norm": 0.48577699065208435,
      "learning_rate": 1.702916666666667e-05,
      "loss": 0.0016,
      "step": 158260
    },
    {
      "epoch": 5.275666666666667,
      "grad_norm": 0.08631055802106857,
      "learning_rate": 1.7027083333333335e-05,
      "loss": 0.0024,
      "step": 158270
    },
    {
      "epoch": 5.276,
      "grad_norm": 0.27669206261634827,
      "learning_rate": 1.7025e-05,
      "loss": 0.0029,
      "step": 158280
    },
    {
      "epoch": 5.2763333333333335,
      "grad_norm": 0.03126107528805733,
      "learning_rate": 1.7022916666666666e-05,
      "loss": 0.002,
      "step": 158290
    },
    {
      "epoch": 5.276666666666666,
      "grad_norm": 0.11451861262321472,
      "learning_rate": 1.702083333333333e-05,
      "loss": 0.0016,
      "step": 158300
    },
    {
      "epoch": 5.277,
      "grad_norm": 0.31397196650505066,
      "learning_rate": 1.701875e-05,
      "loss": 0.0018,
      "step": 158310
    },
    {
      "epoch": 5.277333333333333,
      "grad_norm": 0.470520555973053,
      "learning_rate": 1.7016666666666666e-05,
      "loss": 0.0016,
      "step": 158320
    },
    {
      "epoch": 5.277666666666667,
      "grad_norm": 0.11480654776096344,
      "learning_rate": 1.7014583333333335e-05,
      "loss": 0.0017,
      "step": 158330
    },
    {
      "epoch": 5.2780000000000005,
      "grad_norm": 0.25677788257598877,
      "learning_rate": 1.70125e-05,
      "loss": 0.0018,
      "step": 158340
    },
    {
      "epoch": 5.278333333333333,
      "grad_norm": 0.2588374614715576,
      "learning_rate": 1.701041666666667e-05,
      "loss": 0.0017,
      "step": 158350
    },
    {
      "epoch": 5.278666666666667,
      "grad_norm": 0.01689213141798973,
      "learning_rate": 1.7008333333333335e-05,
      "loss": 0.0018,
      "step": 158360
    },
    {
      "epoch": 5.279,
      "grad_norm": 0.17139551043510437,
      "learning_rate": 1.700625e-05,
      "loss": 0.0015,
      "step": 158370
    },
    {
      "epoch": 5.279333333333334,
      "grad_norm": 0.17146748304367065,
      "learning_rate": 1.700416666666667e-05,
      "loss": 0.0017,
      "step": 158380
    },
    {
      "epoch": 5.2796666666666665,
      "grad_norm": 0.5755935311317444,
      "learning_rate": 1.7002083333333334e-05,
      "loss": 0.0017,
      "step": 158390
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.20045150816440582,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0017,
      "step": 158400
    },
    {
      "epoch": 5.280333333333333,
      "grad_norm": 0.09504012018442154,
      "learning_rate": 1.6997916666666665e-05,
      "loss": 0.0013,
      "step": 158410
    },
    {
      "epoch": 5.280666666666667,
      "grad_norm": 0.17187869548797607,
      "learning_rate": 1.6995833333333334e-05,
      "loss": 0.0016,
      "step": 158420
    },
    {
      "epoch": 5.281,
      "grad_norm": 0.22852563858032227,
      "learning_rate": 1.699375e-05,
      "loss": 0.0015,
      "step": 158430
    },
    {
      "epoch": 5.281333333333333,
      "grad_norm": 0.4279055893421173,
      "learning_rate": 1.6991666666666665e-05,
      "loss": 0.0021,
      "step": 158440
    },
    {
      "epoch": 5.281666666666666,
      "grad_norm": 0.11496119201183319,
      "learning_rate": 1.6989583333333334e-05,
      "loss": 0.0016,
      "step": 158450
    },
    {
      "epoch": 5.282,
      "grad_norm": 0.1431315690279007,
      "learning_rate": 1.69875e-05,
      "loss": 0.0024,
      "step": 158460
    },
    {
      "epoch": 5.282333333333334,
      "grad_norm": 0.14308519661426544,
      "learning_rate": 1.698541666666667e-05,
      "loss": 0.0016,
      "step": 158470
    },
    {
      "epoch": 5.282666666666667,
      "grad_norm": 0.18113955855369568,
      "learning_rate": 1.6983333333333334e-05,
      "loss": 0.0018,
      "step": 158480
    },
    {
      "epoch": 5.283,
      "grad_norm": 0.25734272599220276,
      "learning_rate": 1.6981250000000003e-05,
      "loss": 0.0015,
      "step": 158490
    },
    {
      "epoch": 5.283333333333333,
      "grad_norm": 0.3606494069099426,
      "learning_rate": 1.6979166666666668e-05,
      "loss": 0.0022,
      "step": 158500
    },
    {
      "epoch": 5.283666666666667,
      "grad_norm": 0.029855992645025253,
      "learning_rate": 1.6977083333333334e-05,
      "loss": 0.0011,
      "step": 158510
    },
    {
      "epoch": 5.284,
      "grad_norm": 0.22902880609035492,
      "learning_rate": 1.6975000000000003e-05,
      "loss": 0.0015,
      "step": 158520
    },
    {
      "epoch": 5.2843333333333335,
      "grad_norm": 0.030283629894256592,
      "learning_rate": 1.6972916666666665e-05,
      "loss": 0.0014,
      "step": 158530
    },
    {
      "epoch": 5.284666666666666,
      "grad_norm": 0.030635090544819832,
      "learning_rate": 1.6970833333333334e-05,
      "loss": 0.0016,
      "step": 158540
    },
    {
      "epoch": 5.285,
      "grad_norm": 0.11467264592647552,
      "learning_rate": 1.696875e-05,
      "loss": 0.002,
      "step": 158550
    },
    {
      "epoch": 5.285333333333333,
      "grad_norm": 0.11623170226812363,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0016,
      "step": 158560
    },
    {
      "epoch": 5.285666666666667,
      "grad_norm": 0.1433364748954773,
      "learning_rate": 1.6964583333333333e-05,
      "loss": 0.0022,
      "step": 158570
    },
    {
      "epoch": 5.286,
      "grad_norm": 0.2569590210914612,
      "learning_rate": 1.69625e-05,
      "loss": 0.0015,
      "step": 158580
    },
    {
      "epoch": 5.286333333333333,
      "grad_norm": 0.3427707552909851,
      "learning_rate": 1.6960416666666668e-05,
      "loss": 0.0013,
      "step": 158590
    },
    {
      "epoch": 5.286666666666667,
      "grad_norm": 0.2680426239967346,
      "learning_rate": 1.6958333333333333e-05,
      "loss": 0.0014,
      "step": 158600
    },
    {
      "epoch": 5.287,
      "grad_norm": 0.0312846340239048,
      "learning_rate": 1.6956250000000002e-05,
      "loss": 0.0025,
      "step": 158610
    },
    {
      "epoch": 5.287333333333334,
      "grad_norm": 0.19999665021896362,
      "learning_rate": 1.6954166666666668e-05,
      "loss": 0.0021,
      "step": 158620
    },
    {
      "epoch": 5.2876666666666665,
      "grad_norm": 0.20020240545272827,
      "learning_rate": 1.6952083333333337e-05,
      "loss": 0.0022,
      "step": 158630
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.11457380652427673,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.0016,
      "step": 158640
    },
    {
      "epoch": 5.288333333333333,
      "grad_norm": 0.1440894454717636,
      "learning_rate": 1.6947916666666668e-05,
      "loss": 0.0018,
      "step": 158650
    },
    {
      "epoch": 5.288666666666667,
      "grad_norm": 0.08628822863101959,
      "learning_rate": 1.6945833333333333e-05,
      "loss": 0.003,
      "step": 158660
    },
    {
      "epoch": 5.289,
      "grad_norm": 0.20130054652690887,
      "learning_rate": 1.694375e-05,
      "loss": 0.0019,
      "step": 158670
    },
    {
      "epoch": 5.289333333333333,
      "grad_norm": 0.1436462551355362,
      "learning_rate": 1.6941666666666667e-05,
      "loss": 0.0019,
      "step": 158680
    },
    {
      "epoch": 5.289666666666666,
      "grad_norm": 0.24828824400901794,
      "learning_rate": 1.6939583333333333e-05,
      "loss": 0.0017,
      "step": 158690
    },
    {
      "epoch": 5.29,
      "grad_norm": 0.20017409324645996,
      "learning_rate": 1.6937500000000002e-05,
      "loss": 0.0015,
      "step": 158700
    },
    {
      "epoch": 5.290333333333333,
      "grad_norm": 0.36779534816741943,
      "learning_rate": 1.6935416666666667e-05,
      "loss": 0.0021,
      "step": 158710
    },
    {
      "epoch": 5.290666666666667,
      "grad_norm": 0.2811954617500305,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0013,
      "step": 158720
    },
    {
      "epoch": 5.291,
      "grad_norm": 0.2370036393404007,
      "learning_rate": 1.693125e-05,
      "loss": 0.0019,
      "step": 158730
    },
    {
      "epoch": 5.291333333333333,
      "grad_norm": 0.22869783639907837,
      "learning_rate": 1.6929166666666667e-05,
      "loss": 0.0018,
      "step": 158740
    },
    {
      "epoch": 5.291666666666667,
      "grad_norm": 0.08801532536745071,
      "learning_rate": 1.6927083333333336e-05,
      "loss": 0.0015,
      "step": 158750
    },
    {
      "epoch": 5.292,
      "grad_norm": 0.2575933635234833,
      "learning_rate": 1.6925e-05,
      "loss": 0.0018,
      "step": 158760
    },
    {
      "epoch": 5.292333333333334,
      "grad_norm": 0.14244575798511505,
      "learning_rate": 1.692291666666667e-05,
      "loss": 0.0015,
      "step": 158770
    },
    {
      "epoch": 5.292666666666666,
      "grad_norm": 0.11517639458179474,
      "learning_rate": 1.6920833333333332e-05,
      "loss": 0.0014,
      "step": 158780
    },
    {
      "epoch": 5.293,
      "grad_norm": 0.17204539477825165,
      "learning_rate": 1.691875e-05,
      "loss": 0.002,
      "step": 158790
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 0.2548338770866394,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.0017,
      "step": 158800
    },
    {
      "epoch": 5.293666666666667,
      "grad_norm": 0.4000011086463928,
      "learning_rate": 1.6914583333333332e-05,
      "loss": 0.0016,
      "step": 158810
    },
    {
      "epoch": 5.294,
      "grad_norm": 0.08625292032957077,
      "learning_rate": 1.69125e-05,
      "loss": 0.0019,
      "step": 158820
    },
    {
      "epoch": 5.294333333333333,
      "grad_norm": 0.2861365079879761,
      "learning_rate": 1.6910416666666667e-05,
      "loss": 0.0016,
      "step": 158830
    },
    {
      "epoch": 5.294666666666667,
      "grad_norm": 0.06862211972475052,
      "learning_rate": 1.6908333333333335e-05,
      "loss": 0.0017,
      "step": 158840
    },
    {
      "epoch": 5.295,
      "grad_norm": 0.20051226019859314,
      "learning_rate": 1.690625e-05,
      "loss": 0.0015,
      "step": 158850
    },
    {
      "epoch": 5.295333333333334,
      "grad_norm": 0.03204868733882904,
      "learning_rate": 1.6904166666666666e-05,
      "loss": 0.0022,
      "step": 158860
    },
    {
      "epoch": 5.2956666666666665,
      "grad_norm": 0.17140603065490723,
      "learning_rate": 1.6902083333333335e-05,
      "loss": 0.0018,
      "step": 158870
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.14293906092643738,
      "learning_rate": 1.69e-05,
      "loss": 0.0022,
      "step": 158880
    },
    {
      "epoch": 5.296333333333333,
      "grad_norm": 0.08607136458158493,
      "learning_rate": 1.689791666666667e-05,
      "loss": 0.0019,
      "step": 158890
    },
    {
      "epoch": 5.296666666666667,
      "grad_norm": 0.057739753276109695,
      "learning_rate": 1.6895833333333332e-05,
      "loss": 0.0019,
      "step": 158900
    },
    {
      "epoch": 5.297,
      "grad_norm": 0.11456198990345001,
      "learning_rate": 1.689375e-05,
      "loss": 0.0016,
      "step": 158910
    },
    {
      "epoch": 5.2973333333333334,
      "grad_norm": 0.28618526458740234,
      "learning_rate": 1.6891666666666666e-05,
      "loss": 0.0021,
      "step": 158920
    },
    {
      "epoch": 5.297666666666666,
      "grad_norm": 0.485582172870636,
      "learning_rate": 1.6889583333333335e-05,
      "loss": 0.002,
      "step": 158930
    },
    {
      "epoch": 5.298,
      "grad_norm": 0.45708686113357544,
      "learning_rate": 1.68875e-05,
      "loss": 0.0014,
      "step": 158940
    },
    {
      "epoch": 5.298333333333334,
      "grad_norm": 0.17171591520309448,
      "learning_rate": 1.6885416666666666e-05,
      "loss": 0.0014,
      "step": 158950
    },
    {
      "epoch": 5.298666666666667,
      "grad_norm": 0.1722692996263504,
      "learning_rate": 1.6883333333333335e-05,
      "loss": 0.0016,
      "step": 158960
    },
    {
      "epoch": 5.299,
      "grad_norm": 0.14685608446598053,
      "learning_rate": 1.688125e-05,
      "loss": 0.0018,
      "step": 158970
    },
    {
      "epoch": 5.299333333333333,
      "grad_norm": 0.20012043416500092,
      "learning_rate": 1.687916666666667e-05,
      "loss": 0.0023,
      "step": 158980
    },
    {
      "epoch": 5.299666666666667,
      "grad_norm": 0.31448858976364136,
      "learning_rate": 1.6877083333333335e-05,
      "loss": 0.0019,
      "step": 158990
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.08683596551418304,
      "learning_rate": 1.6875000000000004e-05,
      "loss": 0.0024,
      "step": 159000
    },
    {
      "epoch": 5.300333333333334,
      "grad_norm": 0.4568888545036316,
      "learning_rate": 1.687291666666667e-05,
      "loss": 0.0014,
      "step": 159010
    },
    {
      "epoch": 5.300666666666666,
      "grad_norm": 0.25688186287879944,
      "learning_rate": 1.6870833333333335e-05,
      "loss": 0.0025,
      "step": 159020
    },
    {
      "epoch": 5.301,
      "grad_norm": 0.6561137437820435,
      "learning_rate": 1.686875e-05,
      "loss": 0.0016,
      "step": 159030
    },
    {
      "epoch": 5.301333333333333,
      "grad_norm": 0.27241936326026917,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0015,
      "step": 159040
    },
    {
      "epoch": 5.301666666666667,
      "grad_norm": 0.057973913848400116,
      "learning_rate": 1.6864583333333334e-05,
      "loss": 0.0014,
      "step": 159050
    },
    {
      "epoch": 5.302,
      "grad_norm": 0.2579442262649536,
      "learning_rate": 1.68625e-05,
      "loss": 0.0014,
      "step": 159060
    },
    {
      "epoch": 5.302333333333333,
      "grad_norm": 0.057943195104599,
      "learning_rate": 1.686041666666667e-05,
      "loss": 0.0014,
      "step": 159070
    },
    {
      "epoch": 5.302666666666667,
      "grad_norm": 0.2006155103445053,
      "learning_rate": 1.6858333333333334e-05,
      "loss": 0.0019,
      "step": 159080
    },
    {
      "epoch": 5.303,
      "grad_norm": 0.20212256908416748,
      "learning_rate": 1.685625e-05,
      "loss": 0.0017,
      "step": 159090
    },
    {
      "epoch": 5.303333333333334,
      "grad_norm": 0.009338312782347202,
      "learning_rate": 1.685416666666667e-05,
      "loss": 0.0014,
      "step": 159100
    },
    {
      "epoch": 5.3036666666666665,
      "grad_norm": 0.28549501299858093,
      "learning_rate": 1.6852083333333334e-05,
      "loss": 0.0019,
      "step": 159110
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.20105400681495667,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 0.0019,
      "step": 159120
    },
    {
      "epoch": 5.304333333333333,
      "grad_norm": 0.26953327655792236,
      "learning_rate": 1.684791666666667e-05,
      "loss": 0.0028,
      "step": 159130
    },
    {
      "epoch": 5.304666666666667,
      "grad_norm": 0.31625092029571533,
      "learning_rate": 1.6845833333333334e-05,
      "loss": 0.0022,
      "step": 159140
    },
    {
      "epoch": 5.305,
      "grad_norm": 0.49048659205436707,
      "learning_rate": 1.684375e-05,
      "loss": 0.0028,
      "step": 159150
    },
    {
      "epoch": 5.3053333333333335,
      "grad_norm": 0.457290917634964,
      "learning_rate": 1.684166666666667e-05,
      "loss": 0.0021,
      "step": 159160
    },
    {
      "epoch": 5.305666666666666,
      "grad_norm": 0.11450500786304474,
      "learning_rate": 1.6839583333333334e-05,
      "loss": 0.0015,
      "step": 159170
    },
    {
      "epoch": 5.306,
      "grad_norm": 0.14389397203922272,
      "learning_rate": 1.68375e-05,
      "loss": 0.0025,
      "step": 159180
    },
    {
      "epoch": 5.306333333333333,
      "grad_norm": 0.14335094392299652,
      "learning_rate": 1.6835416666666668e-05,
      "loss": 0.0022,
      "step": 159190
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 0.6594722867012024,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0018,
      "step": 159200
    },
    {
      "epoch": 5.307,
      "grad_norm": 0.343149334192276,
      "learning_rate": 1.6831250000000003e-05,
      "loss": 0.0014,
      "step": 159210
    },
    {
      "epoch": 5.307333333333333,
      "grad_norm": 0.1437637209892273,
      "learning_rate": 1.6829166666666668e-05,
      "loss": 0.0017,
      "step": 159220
    },
    {
      "epoch": 5.307666666666667,
      "grad_norm": 0.2536647617816925,
      "learning_rate": 1.6827083333333334e-05,
      "loss": 0.0018,
      "step": 159230
    },
    {
      "epoch": 5.308,
      "grad_norm": 0.02999754808843136,
      "learning_rate": 1.6825000000000002e-05,
      "loss": 0.0023,
      "step": 159240
    },
    {
      "epoch": 5.308333333333334,
      "grad_norm": 0.06023142486810684,
      "learning_rate": 1.6822916666666668e-05,
      "loss": 0.0014,
      "step": 159250
    },
    {
      "epoch": 5.308666666666666,
      "grad_norm": 0.3147010803222656,
      "learning_rate": 1.6820833333333333e-05,
      "loss": 0.0018,
      "step": 159260
    },
    {
      "epoch": 5.309,
      "grad_norm": 0.009220941923558712,
      "learning_rate": 1.681875e-05,
      "loss": 0.0012,
      "step": 159270
    },
    {
      "epoch": 5.309333333333333,
      "grad_norm": 0.343408465385437,
      "learning_rate": 1.6816666666666668e-05,
      "loss": 0.002,
      "step": 159280
    },
    {
      "epoch": 5.309666666666667,
      "grad_norm": 0.05751819536089897,
      "learning_rate": 1.6814583333333333e-05,
      "loss": 0.0014,
      "step": 159290
    },
    {
      "epoch": 5.31,
      "grad_norm": 0.17143936455249786,
      "learning_rate": 1.6812500000000002e-05,
      "loss": 0.0019,
      "step": 159300
    },
    {
      "epoch": 5.310333333333333,
      "grad_norm": 0.05854734778404236,
      "learning_rate": 1.6810416666666668e-05,
      "loss": 0.0013,
      "step": 159310
    },
    {
      "epoch": 5.310666666666666,
      "grad_norm": 0.08600453287363052,
      "learning_rate": 1.6808333333333333e-05,
      "loss": 0.0013,
      "step": 159320
    },
    {
      "epoch": 5.311,
      "grad_norm": 0.11447633802890778,
      "learning_rate": 1.6806250000000002e-05,
      "loss": 0.0026,
      "step": 159330
    },
    {
      "epoch": 5.311333333333334,
      "grad_norm": 0.4290105104446411,
      "learning_rate": 1.6804166666666667e-05,
      "loss": 0.0017,
      "step": 159340
    },
    {
      "epoch": 5.3116666666666665,
      "grad_norm": 0.7743244767189026,
      "learning_rate": 1.6802083333333336e-05,
      "loss": 0.0021,
      "step": 159350
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.2287210375070572,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.002,
      "step": 159360
    },
    {
      "epoch": 5.312333333333333,
      "grad_norm": 0.4300801157951355,
      "learning_rate": 1.6797916666666667e-05,
      "loss": 0.0015,
      "step": 159370
    },
    {
      "epoch": 5.312666666666667,
      "grad_norm": 0.14336107671260834,
      "learning_rate": 1.6795833333333333e-05,
      "loss": 0.0016,
      "step": 159380
    },
    {
      "epoch": 5.313,
      "grad_norm": 0.1717863380908966,
      "learning_rate": 1.6793749999999998e-05,
      "loss": 0.0027,
      "step": 159390
    },
    {
      "epoch": 5.3133333333333335,
      "grad_norm": 0.2884986102581024,
      "learning_rate": 1.6791666666666667e-05,
      "loss": 0.0014,
      "step": 159400
    },
    {
      "epoch": 5.313666666666666,
      "grad_norm": 0.08767968416213989,
      "learning_rate": 1.6789583333333333e-05,
      "loss": 0.0022,
      "step": 159410
    },
    {
      "epoch": 5.314,
      "grad_norm": 0.3723157048225403,
      "learning_rate": 1.67875e-05,
      "loss": 0.0015,
      "step": 159420
    },
    {
      "epoch": 5.314333333333334,
      "grad_norm": 0.1435444951057434,
      "learning_rate": 1.6785416666666667e-05,
      "loss": 0.0019,
      "step": 159430
    },
    {
      "epoch": 5.314666666666667,
      "grad_norm": 0.34337037801742554,
      "learning_rate": 1.6783333333333336e-05,
      "loss": 0.0015,
      "step": 159440
    },
    {
      "epoch": 5.315,
      "grad_norm": 0.14305564761161804,
      "learning_rate": 1.678125e-05,
      "loss": 0.0022,
      "step": 159450
    },
    {
      "epoch": 5.315333333333333,
      "grad_norm": 0.015413153916597366,
      "learning_rate": 1.6779166666666667e-05,
      "loss": 0.0022,
      "step": 159460
    },
    {
      "epoch": 5.315666666666667,
      "grad_norm": 0.06405846774578094,
      "learning_rate": 1.6777083333333336e-05,
      "loss": 0.0022,
      "step": 159470
    },
    {
      "epoch": 5.316,
      "grad_norm": 0.1714911162853241,
      "learning_rate": 1.6775e-05,
      "loss": 0.0016,
      "step": 159480
    },
    {
      "epoch": 5.316333333333334,
      "grad_norm": 0.03193235769867897,
      "learning_rate": 1.677291666666667e-05,
      "loss": 0.0021,
      "step": 159490
    },
    {
      "epoch": 5.316666666666666,
      "grad_norm": 0.5149873495101929,
      "learning_rate": 1.6770833333333332e-05,
      "loss": 0.0014,
      "step": 159500
    },
    {
      "epoch": 5.317,
      "grad_norm": 0.4265396296977997,
      "learning_rate": 1.676875e-05,
      "loss": 0.0013,
      "step": 159510
    },
    {
      "epoch": 5.317333333333333,
      "grad_norm": 0.5150321125984192,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0017,
      "step": 159520
    },
    {
      "epoch": 5.317666666666667,
      "grad_norm": 0.17166757583618164,
      "learning_rate": 1.6764583333333332e-05,
      "loss": 0.0019,
      "step": 159530
    },
    {
      "epoch": 5.318,
      "grad_norm": 0.2865912616252899,
      "learning_rate": 1.67625e-05,
      "loss": 0.0018,
      "step": 159540
    },
    {
      "epoch": 5.318333333333333,
      "grad_norm": 0.11197233200073242,
      "learning_rate": 1.6760416666666666e-05,
      "loss": 0.0019,
      "step": 159550
    },
    {
      "epoch": 5.318666666666667,
      "grad_norm": 0.11456484347581863,
      "learning_rate": 1.6758333333333335e-05,
      "loss": 0.0017,
      "step": 159560
    },
    {
      "epoch": 5.319,
      "grad_norm": 0.1429634392261505,
      "learning_rate": 1.675625e-05,
      "loss": 0.0018,
      "step": 159570
    },
    {
      "epoch": 5.319333333333334,
      "grad_norm": 0.23022520542144775,
      "learning_rate": 1.675416666666667e-05,
      "loss": 0.0016,
      "step": 159580
    },
    {
      "epoch": 5.3196666666666665,
      "grad_norm": 0.05755620822310448,
      "learning_rate": 1.6752083333333335e-05,
      "loss": 0.0022,
      "step": 159590
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.34311527013778687,
      "learning_rate": 1.675e-05,
      "loss": 0.0016,
      "step": 159600
    },
    {
      "epoch": 5.320333333333333,
      "grad_norm": 0.114984892308712,
      "learning_rate": 1.674791666666667e-05,
      "loss": 0.0022,
      "step": 159610
    },
    {
      "epoch": 5.320666666666667,
      "grad_norm": 0.11483051627874374,
      "learning_rate": 1.674583333333333e-05,
      "loss": 0.0013,
      "step": 159620
    },
    {
      "epoch": 5.321,
      "grad_norm": 0.11489765346050262,
      "learning_rate": 1.674375e-05,
      "loss": 0.0018,
      "step": 159630
    },
    {
      "epoch": 5.3213333333333335,
      "grad_norm": 0.05813480541110039,
      "learning_rate": 1.6741666666666666e-05,
      "loss": 0.0016,
      "step": 159640
    },
    {
      "epoch": 5.321666666666666,
      "grad_norm": 0.31479117274284363,
      "learning_rate": 1.6739583333333335e-05,
      "loss": 0.0015,
      "step": 159650
    },
    {
      "epoch": 5.322,
      "grad_norm": 0.4005904793739319,
      "learning_rate": 1.67375e-05,
      "loss": 0.002,
      "step": 159660
    },
    {
      "epoch": 5.322333333333333,
      "grad_norm": 0.02963707409799099,
      "learning_rate": 1.6735416666666666e-05,
      "loss": 0.0018,
      "step": 159670
    },
    {
      "epoch": 5.322666666666667,
      "grad_norm": 0.08620265871286392,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0018,
      "step": 159680
    },
    {
      "epoch": 5.323,
      "grad_norm": 0.286133736371994,
      "learning_rate": 1.673125e-05,
      "loss": 0.0019,
      "step": 159690
    },
    {
      "epoch": 5.323333333333333,
      "grad_norm": 0.2856559157371521,
      "learning_rate": 1.672916666666667e-05,
      "loss": 0.0016,
      "step": 159700
    },
    {
      "epoch": 5.323666666666667,
      "grad_norm": 0.05737048014998436,
      "learning_rate": 1.6727083333333334e-05,
      "loss": 0.0025,
      "step": 159710
    },
    {
      "epoch": 5.324,
      "grad_norm": 0.11447600275278091,
      "learning_rate": 1.6725000000000003e-05,
      "loss": 0.0019,
      "step": 159720
    },
    {
      "epoch": 5.324333333333334,
      "grad_norm": 0.2014777660369873,
      "learning_rate": 1.672291666666667e-05,
      "loss": 0.0018,
      "step": 159730
    },
    {
      "epoch": 5.324666666666666,
      "grad_norm": 0.11469148099422455,
      "learning_rate": 1.6720833333333334e-05,
      "loss": 0.0013,
      "step": 159740
    },
    {
      "epoch": 5.325,
      "grad_norm": 0.03335385024547577,
      "learning_rate": 1.671875e-05,
      "loss": 0.0019,
      "step": 159750
    },
    {
      "epoch": 5.325333333333333,
      "grad_norm": 0.28620097041130066,
      "learning_rate": 1.6716666666666665e-05,
      "loss": 0.0021,
      "step": 159760
    },
    {
      "epoch": 5.325666666666667,
      "grad_norm": 0.28572216629981995,
      "learning_rate": 1.6714583333333334e-05,
      "loss": 0.0022,
      "step": 159770
    },
    {
      "epoch": 5.326,
      "grad_norm": 0.46740031242370605,
      "learning_rate": 1.67125e-05,
      "loss": 0.0015,
      "step": 159780
    },
    {
      "epoch": 5.326333333333333,
      "grad_norm": 0.11610443890094757,
      "learning_rate": 1.671041666666667e-05,
      "loss": 0.0017,
      "step": 159790
    },
    {
      "epoch": 5.326666666666666,
      "grad_norm": 0.2606220245361328,
      "learning_rate": 1.6708333333333334e-05,
      "loss": 0.0021,
      "step": 159800
    },
    {
      "epoch": 5.327,
      "grad_norm": 0.11435233056545258,
      "learning_rate": 1.670625e-05,
      "loss": 0.0012,
      "step": 159810
    },
    {
      "epoch": 5.327333333333334,
      "grad_norm": 0.2592945694923401,
      "learning_rate": 1.670416666666667e-05,
      "loss": 0.0025,
      "step": 159820
    },
    {
      "epoch": 5.3276666666666666,
      "grad_norm": 0.4003635048866272,
      "learning_rate": 1.6702083333333334e-05,
      "loss": 0.0023,
      "step": 159830
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.02983584627509117,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0014,
      "step": 159840
    },
    {
      "epoch": 5.328333333333333,
      "grad_norm": 0.05774182453751564,
      "learning_rate": 1.6697916666666668e-05,
      "loss": 0.0015,
      "step": 159850
    },
    {
      "epoch": 5.328666666666667,
      "grad_norm": 0.0891847163438797,
      "learning_rate": 1.6695833333333334e-05,
      "loss": 0.0022,
      "step": 159860
    },
    {
      "epoch": 5.329,
      "grad_norm": 0.08637256175279617,
      "learning_rate": 1.669375e-05,
      "loss": 0.0023,
      "step": 159870
    },
    {
      "epoch": 5.3293333333333335,
      "grad_norm": 0.029894446954131126,
      "learning_rate": 1.6691666666666668e-05,
      "loss": 0.0015,
      "step": 159880
    },
    {
      "epoch": 5.329666666666666,
      "grad_norm": 0.7426486015319824,
      "learning_rate": 1.6689583333333334e-05,
      "loss": 0.0016,
      "step": 159890
    },
    {
      "epoch": 5.33,
      "grad_norm": 0.22964631021022797,
      "learning_rate": 1.66875e-05,
      "loss": 0.0012,
      "step": 159900
    },
    {
      "epoch": 5.330333333333333,
      "grad_norm": 0.17206038534641266,
      "learning_rate": 1.6685416666666668e-05,
      "loss": 0.0018,
      "step": 159910
    },
    {
      "epoch": 5.330666666666667,
      "grad_norm": 0.01239432580769062,
      "learning_rate": 1.6683333333333333e-05,
      "loss": 0.0019,
      "step": 159920
    },
    {
      "epoch": 5.331,
      "grad_norm": 0.20085826516151428,
      "learning_rate": 1.6681250000000002e-05,
      "loss": 0.0016,
      "step": 159930
    },
    {
      "epoch": 5.331333333333333,
      "grad_norm": 0.1879902184009552,
      "learning_rate": 1.6679166666666668e-05,
      "loss": 0.0018,
      "step": 159940
    },
    {
      "epoch": 5.331666666666667,
      "grad_norm": 0.1428016722202301,
      "learning_rate": 1.6677083333333333e-05,
      "loss": 0.0014,
      "step": 159950
    },
    {
      "epoch": 5.332,
      "grad_norm": 0.34306856989860535,
      "learning_rate": 1.6675000000000002e-05,
      "loss": 0.0016,
      "step": 159960
    },
    {
      "epoch": 5.332333333333334,
      "grad_norm": 0.48551425337791443,
      "learning_rate": 1.6672916666666668e-05,
      "loss": 0.002,
      "step": 159970
    },
    {
      "epoch": 5.332666666666666,
      "grad_norm": 0.21460095047950745,
      "learning_rate": 1.6670833333333333e-05,
      "loss": 0.0019,
      "step": 159980
    },
    {
      "epoch": 5.333,
      "grad_norm": 0.1616142839193344,
      "learning_rate": 1.666875e-05,
      "loss": 0.0016,
      "step": 159990
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.08604437857866287,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.002,
      "step": 160000
    },
    {
      "epoch": 5.333666666666667,
      "grad_norm": 0.05743332579731941,
      "learning_rate": 1.6664583333333333e-05,
      "loss": 0.0024,
      "step": 160010
    },
    {
      "epoch": 5.334,
      "grad_norm": 0.008773580193519592,
      "learning_rate": 1.6662500000000002e-05,
      "loss": 0.0015,
      "step": 160020
    },
    {
      "epoch": 5.334333333333333,
      "grad_norm": 0.3169565498828888,
      "learning_rate": 1.6660416666666667e-05,
      "loss": 0.0014,
      "step": 160030
    },
    {
      "epoch": 5.334666666666667,
      "grad_norm": 0.2852306663990021,
      "learning_rate": 1.6658333333333333e-05,
      "loss": 0.0015,
      "step": 160040
    },
    {
      "epoch": 5.335,
      "grad_norm": 0.2005765438079834,
      "learning_rate": 1.665625e-05,
      "loss": 0.0014,
      "step": 160050
    },
    {
      "epoch": 5.335333333333334,
      "grad_norm": 0.45771345496177673,
      "learning_rate": 1.6654166666666667e-05,
      "loss": 0.0012,
      "step": 160060
    },
    {
      "epoch": 5.335666666666667,
      "grad_norm": 0.3147085905075073,
      "learning_rate": 1.6652083333333336e-05,
      "loss": 0.0019,
      "step": 160070
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.029694799333810806,
      "learning_rate": 1.665e-05,
      "loss": 0.0017,
      "step": 160080
    },
    {
      "epoch": 5.336333333333333,
      "grad_norm": 0.17618556320667267,
      "learning_rate": 1.6647916666666667e-05,
      "loss": 0.0018,
      "step": 160090
    },
    {
      "epoch": 5.336666666666667,
      "grad_norm": 0.14661839604377747,
      "learning_rate": 1.6645833333333336e-05,
      "loss": 0.0027,
      "step": 160100
    },
    {
      "epoch": 5.337,
      "grad_norm": 0.29991260170936584,
      "learning_rate": 1.6643749999999998e-05,
      "loss": 0.0024,
      "step": 160110
    },
    {
      "epoch": 5.3373333333333335,
      "grad_norm": 0.04154584929347038,
      "learning_rate": 1.6641666666666667e-05,
      "loss": 0.0017,
      "step": 160120
    },
    {
      "epoch": 5.337666666666666,
      "grad_norm": 0.05802663415670395,
      "learning_rate": 1.6639583333333332e-05,
      "loss": 0.0018,
      "step": 160130
    },
    {
      "epoch": 5.338,
      "grad_norm": 0.45747846364974976,
      "learning_rate": 1.66375e-05,
      "loss": 0.0017,
      "step": 160140
    },
    {
      "epoch": 5.338333333333333,
      "grad_norm": 0.41527485847473145,
      "learning_rate": 1.6635416666666667e-05,
      "loss": 0.0016,
      "step": 160150
    },
    {
      "epoch": 5.338666666666667,
      "grad_norm": 0.0587635412812233,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0025,
      "step": 160160
    },
    {
      "epoch": 5.339,
      "grad_norm": 0.342833936214447,
      "learning_rate": 1.663125e-05,
      "loss": 0.002,
      "step": 160170
    },
    {
      "epoch": 5.339333333333333,
      "grad_norm": 0.02902899868786335,
      "learning_rate": 1.6629166666666667e-05,
      "loss": 0.0013,
      "step": 160180
    },
    {
      "epoch": 5.339666666666667,
      "grad_norm": 0.2488028109073639,
      "learning_rate": 1.6627083333333335e-05,
      "loss": 0.0017,
      "step": 160190
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.20340436697006226,
      "learning_rate": 1.6625e-05,
      "loss": 0.0023,
      "step": 160200
    },
    {
      "epoch": 5.340333333333334,
      "grad_norm": 0.04408351704478264,
      "learning_rate": 1.662291666666667e-05,
      "loss": 0.0018,
      "step": 160210
    },
    {
      "epoch": 5.3406666666666665,
      "grad_norm": 0.05787569284439087,
      "learning_rate": 1.6620833333333335e-05,
      "loss": 0.0016,
      "step": 160220
    },
    {
      "epoch": 5.341,
      "grad_norm": 0.17095310986042023,
      "learning_rate": 1.661875e-05,
      "loss": 0.0019,
      "step": 160230
    },
    {
      "epoch": 5.341333333333333,
      "grad_norm": 0.11508584022521973,
      "learning_rate": 1.6616666666666666e-05,
      "loss": 0.0017,
      "step": 160240
    },
    {
      "epoch": 5.341666666666667,
      "grad_norm": 0.19994337856769562,
      "learning_rate": 1.6614583333333332e-05,
      "loss": 0.0011,
      "step": 160250
    },
    {
      "epoch": 5.342,
      "grad_norm": 0.3147504925727844,
      "learning_rate": 1.66125e-05,
      "loss": 0.0023,
      "step": 160260
    },
    {
      "epoch": 5.342333333333333,
      "grad_norm": 0.20015335083007812,
      "learning_rate": 1.6610416666666666e-05,
      "loss": 0.0026,
      "step": 160270
    },
    {
      "epoch": 5.342666666666666,
      "grad_norm": 0.25772610306739807,
      "learning_rate": 1.6608333333333335e-05,
      "loss": 0.002,
      "step": 160280
    },
    {
      "epoch": 5.343,
      "grad_norm": 0.5302503705024719,
      "learning_rate": 1.660625e-05,
      "loss": 0.0021,
      "step": 160290
    },
    {
      "epoch": 5.343333333333334,
      "grad_norm": 0.0295731071382761,
      "learning_rate": 1.660416666666667e-05,
      "loss": 0.0016,
      "step": 160300
    },
    {
      "epoch": 5.343666666666667,
      "grad_norm": 0.11434721946716309,
      "learning_rate": 1.6602083333333335e-05,
      "loss": 0.0018,
      "step": 160310
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.030166001990437508,
      "learning_rate": 1.66e-05,
      "loss": 0.0017,
      "step": 160320
    },
    {
      "epoch": 5.344333333333333,
      "grad_norm": 0.2285546213388443,
      "learning_rate": 1.659791666666667e-05,
      "loss": 0.0018,
      "step": 160330
    },
    {
      "epoch": 5.344666666666667,
      "grad_norm": 0.08583921194076538,
      "learning_rate": 1.6595833333333335e-05,
      "loss": 0.0016,
      "step": 160340
    },
    {
      "epoch": 5.345,
      "grad_norm": 0.14420485496520996,
      "learning_rate": 1.659375e-05,
      "loss": 0.0021,
      "step": 160350
    },
    {
      "epoch": 5.3453333333333335,
      "grad_norm": 0.14622829854488373,
      "learning_rate": 1.6591666666666666e-05,
      "loss": 0.002,
      "step": 160360
    },
    {
      "epoch": 5.345666666666666,
      "grad_norm": 0.49838775396347046,
      "learning_rate": 1.6589583333333335e-05,
      "loss": 0.0015,
      "step": 160370
    },
    {
      "epoch": 5.346,
      "grad_norm": 0.030533259734511375,
      "learning_rate": 1.65875e-05,
      "loss": 0.0019,
      "step": 160380
    },
    {
      "epoch": 5.346333333333333,
      "grad_norm": 0.1913021206855774,
      "learning_rate": 1.6585416666666665e-05,
      "loss": 0.0024,
      "step": 160390
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 0.0574994832277298,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.0018,
      "step": 160400
    },
    {
      "epoch": 5.3469999999999995,
      "grad_norm": 0.4632106125354767,
      "learning_rate": 1.658125e-05,
      "loss": 0.0014,
      "step": 160410
    },
    {
      "epoch": 5.347333333333333,
      "grad_norm": 0.456999808549881,
      "learning_rate": 1.657916666666667e-05,
      "loss": 0.002,
      "step": 160420
    },
    {
      "epoch": 5.347666666666667,
      "grad_norm": 0.20436887443065643,
      "learning_rate": 1.6577083333333334e-05,
      "loss": 0.0016,
      "step": 160430
    },
    {
      "epoch": 5.348,
      "grad_norm": 0.007921477779746056,
      "learning_rate": 1.6575000000000003e-05,
      "loss": 0.0022,
      "step": 160440
    },
    {
      "epoch": 5.348333333333334,
      "grad_norm": 0.05722200125455856,
      "learning_rate": 1.657291666666667e-05,
      "loss": 0.0014,
      "step": 160450
    },
    {
      "epoch": 5.3486666666666665,
      "grad_norm": 0.058304086327552795,
      "learning_rate": 1.6570833333333334e-05,
      "loss": 0.0018,
      "step": 160460
    },
    {
      "epoch": 5.349,
      "grad_norm": 0.5014835596084595,
      "learning_rate": 1.656875e-05,
      "loss": 0.0019,
      "step": 160470
    },
    {
      "epoch": 5.349333333333333,
      "grad_norm": 0.08565746992826462,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0019,
      "step": 160480
    },
    {
      "epoch": 5.349666666666667,
      "grad_norm": 0.1945265829563141,
      "learning_rate": 1.6564583333333334e-05,
      "loss": 0.0022,
      "step": 160490
    },
    {
      "epoch": 5.35,
      "grad_norm": 0.029170917347073555,
      "learning_rate": 1.65625e-05,
      "loss": 0.002,
      "step": 160500
    },
    {
      "epoch": 5.350333333333333,
      "grad_norm": 0.37125131487846375,
      "learning_rate": 1.6560416666666668e-05,
      "loss": 0.0013,
      "step": 160510
    },
    {
      "epoch": 5.350666666666667,
      "grad_norm": 0.05798296630382538,
      "learning_rate": 1.6558333333333334e-05,
      "loss": 0.0015,
      "step": 160520
    },
    {
      "epoch": 5.351,
      "grad_norm": 0.25720834732055664,
      "learning_rate": 1.655625e-05,
      "loss": 0.0016,
      "step": 160530
    },
    {
      "epoch": 5.351333333333334,
      "grad_norm": 0.34252816438674927,
      "learning_rate": 1.6554166666666668e-05,
      "loss": 0.0018,
      "step": 160540
    },
    {
      "epoch": 5.351666666666667,
      "grad_norm": 0.17136335372924805,
      "learning_rate": 1.6552083333333334e-05,
      "loss": 0.0017,
      "step": 160550
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.1146794930100441,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 0.0018,
      "step": 160560
    },
    {
      "epoch": 5.352333333333333,
      "grad_norm": 0.08600083738565445,
      "learning_rate": 1.6547916666666668e-05,
      "loss": 0.0018,
      "step": 160570
    },
    {
      "epoch": 5.352666666666667,
      "grad_norm": 0.25779494643211365,
      "learning_rate": 1.6545833333333337e-05,
      "loss": 0.0023,
      "step": 160580
    },
    {
      "epoch": 5.353,
      "grad_norm": 0.22916056215763092,
      "learning_rate": 1.654375e-05,
      "loss": 0.002,
      "step": 160590
    },
    {
      "epoch": 5.3533333333333335,
      "grad_norm": 0.05734176188707352,
      "learning_rate": 1.6541666666666668e-05,
      "loss": 0.0022,
      "step": 160600
    },
    {
      "epoch": 5.353666666666666,
      "grad_norm": 0.08582902699708939,
      "learning_rate": 1.6539583333333333e-05,
      "loss": 0.0029,
      "step": 160610
    },
    {
      "epoch": 5.354,
      "grad_norm": 0.1430376023054123,
      "learning_rate": 1.65375e-05,
      "loss": 0.0014,
      "step": 160620
    },
    {
      "epoch": 5.354333333333333,
      "grad_norm": 0.17105576395988464,
      "learning_rate": 1.6535416666666668e-05,
      "loss": 0.0017,
      "step": 160630
    },
    {
      "epoch": 5.354666666666667,
      "grad_norm": 0.08622294664382935,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0013,
      "step": 160640
    },
    {
      "epoch": 5.355,
      "grad_norm": 0.2788126468658447,
      "learning_rate": 1.6531250000000002e-05,
      "loss": 0.0019,
      "step": 160650
    },
    {
      "epoch": 5.355333333333333,
      "grad_norm": 0.17136989533901215,
      "learning_rate": 1.6529166666666668e-05,
      "loss": 0.0023,
      "step": 160660
    },
    {
      "epoch": 5.355666666666667,
      "grad_norm": 0.17147424817085266,
      "learning_rate": 1.6527083333333336e-05,
      "loss": 0.0013,
      "step": 160670
    },
    {
      "epoch": 5.356,
      "grad_norm": 0.17144660651683807,
      "learning_rate": 1.6525000000000002e-05,
      "loss": 0.0017,
      "step": 160680
    },
    {
      "epoch": 5.356333333333334,
      "grad_norm": 0.030077431350946426,
      "learning_rate": 1.6522916666666667e-05,
      "loss": 0.0022,
      "step": 160690
    },
    {
      "epoch": 5.3566666666666665,
      "grad_norm": 0.08592668175697327,
      "learning_rate": 1.6520833333333336e-05,
      "loss": 0.0024,
      "step": 160700
    },
    {
      "epoch": 5.357,
      "grad_norm": 0.35577189922332764,
      "learning_rate": 1.651875e-05,
      "loss": 0.0016,
      "step": 160710
    },
    {
      "epoch": 5.357333333333333,
      "grad_norm": 0.0299299955368042,
      "learning_rate": 1.6516666666666667e-05,
      "loss": 0.0015,
      "step": 160720
    },
    {
      "epoch": 5.357666666666667,
      "grad_norm": 0.06025712192058563,
      "learning_rate": 1.6514583333333333e-05,
      "loss": 0.0014,
      "step": 160730
    },
    {
      "epoch": 5.358,
      "grad_norm": 0.1147327870130539,
      "learning_rate": 1.65125e-05,
      "loss": 0.0018,
      "step": 160740
    },
    {
      "epoch": 5.358333333333333,
      "grad_norm": 0.029369041323661804,
      "learning_rate": 1.6510416666666667e-05,
      "loss": 0.0013,
      "step": 160750
    },
    {
      "epoch": 5.358666666666666,
      "grad_norm": 0.14343830943107605,
      "learning_rate": 1.6508333333333333e-05,
      "loss": 0.0016,
      "step": 160760
    },
    {
      "epoch": 5.359,
      "grad_norm": 0.16908049583435059,
      "learning_rate": 1.650625e-05,
      "loss": 0.0018,
      "step": 160770
    },
    {
      "epoch": 5.359333333333334,
      "grad_norm": 0.17247554659843445,
      "learning_rate": 1.6504166666666667e-05,
      "loss": 0.0018,
      "step": 160780
    },
    {
      "epoch": 5.359666666666667,
      "grad_norm": 0.08021587878465652,
      "learning_rate": 1.6502083333333336e-05,
      "loss": 0.0019,
      "step": 160790
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.08629584312438965,
      "learning_rate": 1.65e-05,
      "loss": 0.0035,
      "step": 160800
    },
    {
      "epoch": 5.360333333333333,
      "grad_norm": 0.1433192938566208,
      "learning_rate": 1.649791666666667e-05,
      "loss": 0.0022,
      "step": 160810
    },
    {
      "epoch": 5.360666666666667,
      "grad_norm": 0.11431167274713516,
      "learning_rate": 1.6495833333333336e-05,
      "loss": 0.0016,
      "step": 160820
    },
    {
      "epoch": 5.361,
      "grad_norm": 0.11513670533895493,
      "learning_rate": 1.649375e-05,
      "loss": 0.0019,
      "step": 160830
    },
    {
      "epoch": 5.3613333333333335,
      "grad_norm": 0.3435494601726532,
      "learning_rate": 1.6491666666666667e-05,
      "loss": 0.0021,
      "step": 160840
    },
    {
      "epoch": 5.361666666666666,
      "grad_norm": 0.4422111213207245,
      "learning_rate": 1.6489583333333332e-05,
      "loss": 0.0021,
      "step": 160850
    },
    {
      "epoch": 5.362,
      "grad_norm": 0.05836694315075874,
      "learning_rate": 1.64875e-05,
      "loss": 0.0023,
      "step": 160860
    },
    {
      "epoch": 5.362333333333333,
      "grad_norm": 0.6011901497840881,
      "learning_rate": 1.6485416666666666e-05,
      "loss": 0.0026,
      "step": 160870
    },
    {
      "epoch": 5.362666666666667,
      "grad_norm": 0.25806373357772827,
      "learning_rate": 1.6483333333333335e-05,
      "loss": 0.002,
      "step": 160880
    },
    {
      "epoch": 5.3629999999999995,
      "grad_norm": 0.3150424063205719,
      "learning_rate": 1.648125e-05,
      "loss": 0.0016,
      "step": 160890
    },
    {
      "epoch": 5.363333333333333,
      "grad_norm": 0.08648413419723511,
      "learning_rate": 1.6479166666666666e-05,
      "loss": 0.0019,
      "step": 160900
    },
    {
      "epoch": 5.363666666666667,
      "grad_norm": 0.20109114050865173,
      "learning_rate": 1.6477083333333335e-05,
      "loss": 0.0017,
      "step": 160910
    },
    {
      "epoch": 5.364,
      "grad_norm": 0.11447875946760178,
      "learning_rate": 1.6475e-05,
      "loss": 0.0025,
      "step": 160920
    },
    {
      "epoch": 5.364333333333334,
      "grad_norm": 0.11533863097429276,
      "learning_rate": 1.647291666666667e-05,
      "loss": 0.0013,
      "step": 160930
    },
    {
      "epoch": 5.3646666666666665,
      "grad_norm": 0.3141874074935913,
      "learning_rate": 1.6470833333333335e-05,
      "loss": 0.0027,
      "step": 160940
    },
    {
      "epoch": 5.365,
      "grad_norm": 0.08658719062805176,
      "learning_rate": 1.646875e-05,
      "loss": 0.0018,
      "step": 160950
    },
    {
      "epoch": 5.365333333333333,
      "grad_norm": 0.4425804913043976,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0022,
      "step": 160960
    },
    {
      "epoch": 5.365666666666667,
      "grad_norm": 0.093913733959198,
      "learning_rate": 1.6464583333333335e-05,
      "loss": 0.0018,
      "step": 160970
    },
    {
      "epoch": 5.366,
      "grad_norm": 0.0579240657389164,
      "learning_rate": 1.64625e-05,
      "loss": 0.0019,
      "step": 160980
    },
    {
      "epoch": 5.366333333333333,
      "grad_norm": 0.20531052350997925,
      "learning_rate": 1.6460416666666666e-05,
      "loss": 0.0016,
      "step": 160990
    },
    {
      "epoch": 5.366666666666666,
      "grad_norm": 0.3570408523082733,
      "learning_rate": 1.6458333333333335e-05,
      "loss": 0.0018,
      "step": 161000
    },
    {
      "epoch": 5.367,
      "grad_norm": 0.23083962500095367,
      "learning_rate": 1.645625e-05,
      "loss": 0.0015,
      "step": 161010
    },
    {
      "epoch": 5.367333333333334,
      "grad_norm": 0.22939081490039825,
      "learning_rate": 1.645416666666667e-05,
      "loss": 0.0031,
      "step": 161020
    },
    {
      "epoch": 5.367666666666667,
      "grad_norm": 0.058677561581134796,
      "learning_rate": 1.6452083333333335e-05,
      "loss": 0.002,
      "step": 161030
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.05773060396313667,
      "learning_rate": 1.645e-05,
      "loss": 0.0019,
      "step": 161040
    },
    {
      "epoch": 5.368333333333333,
      "grad_norm": 0.17176654934883118,
      "learning_rate": 1.644791666666667e-05,
      "loss": 0.0019,
      "step": 161050
    },
    {
      "epoch": 5.368666666666667,
      "grad_norm": 0.09920260310173035,
      "learning_rate": 1.6445833333333334e-05,
      "loss": 0.0016,
      "step": 161060
    },
    {
      "epoch": 5.369,
      "grad_norm": 0.05782253295183182,
      "learning_rate": 1.644375e-05,
      "loss": 0.0013,
      "step": 161070
    },
    {
      "epoch": 5.3693333333333335,
      "grad_norm": 0.2572062909603119,
      "learning_rate": 1.6441666666666665e-05,
      "loss": 0.0018,
      "step": 161080
    },
    {
      "epoch": 5.369666666666666,
      "grad_norm": 0.37157949805259705,
      "learning_rate": 1.6439583333333334e-05,
      "loss": 0.0023,
      "step": 161090
    },
    {
      "epoch": 5.37,
      "grad_norm": 0.39888623356819153,
      "learning_rate": 1.64375e-05,
      "loss": 0.0021,
      "step": 161100
    },
    {
      "epoch": 5.370333333333333,
      "grad_norm": 0.014167670160531998,
      "learning_rate": 1.643541666666667e-05,
      "loss": 0.0021,
      "step": 161110
    },
    {
      "epoch": 5.370666666666667,
      "grad_norm": 0.11556800454854965,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.0016,
      "step": 161120
    },
    {
      "epoch": 5.371,
      "grad_norm": 0.33342787623405457,
      "learning_rate": 1.643125e-05,
      "loss": 0.0022,
      "step": 161130
    },
    {
      "epoch": 5.371333333333333,
      "grad_norm": 0.05762666463851929,
      "learning_rate": 1.642916666666667e-05,
      "loss": 0.002,
      "step": 161140
    },
    {
      "epoch": 5.371666666666667,
      "grad_norm": 0.14326100051403046,
      "learning_rate": 1.6427083333333334e-05,
      "loss": 0.0022,
      "step": 161150
    },
    {
      "epoch": 5.372,
      "grad_norm": 0.1802944540977478,
      "learning_rate": 1.6425000000000003e-05,
      "loss": 0.0019,
      "step": 161160
    },
    {
      "epoch": 5.372333333333334,
      "grad_norm": 0.08586159348487854,
      "learning_rate": 1.642291666666667e-05,
      "loss": 0.0017,
      "step": 161170
    },
    {
      "epoch": 5.3726666666666665,
      "grad_norm": 0.03296622633934021,
      "learning_rate": 1.6420833333333334e-05,
      "loss": 0.0018,
      "step": 161180
    },
    {
      "epoch": 5.373,
      "grad_norm": 0.14346382021903992,
      "learning_rate": 1.641875e-05,
      "loss": 0.0015,
      "step": 161190
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 0.09421929717063904,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.0016,
      "step": 161200
    },
    {
      "epoch": 5.373666666666667,
      "grad_norm": 0.03053073026239872,
      "learning_rate": 1.6414583333333334e-05,
      "loss": 0.0026,
      "step": 161210
    },
    {
      "epoch": 5.374,
      "grad_norm": 0.5516787767410278,
      "learning_rate": 1.64125e-05,
      "loss": 0.0022,
      "step": 161220
    },
    {
      "epoch": 5.374333333333333,
      "grad_norm": 0.11480212211608887,
      "learning_rate": 1.6410416666666668e-05,
      "loss": 0.0018,
      "step": 161230
    },
    {
      "epoch": 5.374666666666666,
      "grad_norm": 0.08595798164606094,
      "learning_rate": 1.6408333333333333e-05,
      "loss": 0.0019,
      "step": 161240
    },
    {
      "epoch": 5.375,
      "grad_norm": 0.257155179977417,
      "learning_rate": 1.6406250000000002e-05,
      "loss": 0.002,
      "step": 161250
    },
    {
      "epoch": 5.375333333333334,
      "grad_norm": 0.42854541540145874,
      "learning_rate": 1.6404166666666668e-05,
      "loss": 0.0014,
      "step": 161260
    },
    {
      "epoch": 5.375666666666667,
      "grad_norm": 0.05833685025572777,
      "learning_rate": 1.6402083333333333e-05,
      "loss": 0.0023,
      "step": 161270
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.05876605585217476,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0016,
      "step": 161280
    },
    {
      "epoch": 5.376333333333333,
      "grad_norm": 0.5458992719650269,
      "learning_rate": 1.6397916666666668e-05,
      "loss": 0.0018,
      "step": 161290
    },
    {
      "epoch": 5.376666666666667,
      "grad_norm": 0.057419516146183014,
      "learning_rate": 1.6395833333333337e-05,
      "loss": 0.0018,
      "step": 161300
    },
    {
      "epoch": 5.377,
      "grad_norm": 0.3145224153995514,
      "learning_rate": 1.6393750000000002e-05,
      "loss": 0.0019,
      "step": 161310
    },
    {
      "epoch": 5.3773333333333335,
      "grad_norm": 0.22907300293445587,
      "learning_rate": 1.6391666666666668e-05,
      "loss": 0.0016,
      "step": 161320
    },
    {
      "epoch": 5.377666666666666,
      "grad_norm": 0.3427486717700958,
      "learning_rate": 1.6389583333333333e-05,
      "loss": 0.0021,
      "step": 161330
    },
    {
      "epoch": 5.378,
      "grad_norm": 0.028983453288674355,
      "learning_rate": 1.63875e-05,
      "loss": 0.0021,
      "step": 161340
    },
    {
      "epoch": 5.378333333333333,
      "grad_norm": 0.5423392057418823,
      "learning_rate": 1.6385416666666667e-05,
      "loss": 0.0025,
      "step": 161350
    },
    {
      "epoch": 5.378666666666667,
      "grad_norm": 0.2856961190700531,
      "learning_rate": 1.6383333333333333e-05,
      "loss": 0.0016,
      "step": 161360
    },
    {
      "epoch": 5.379,
      "grad_norm": 0.48616811633110046,
      "learning_rate": 1.6381250000000002e-05,
      "loss": 0.0017,
      "step": 161370
    },
    {
      "epoch": 5.379333333333333,
      "grad_norm": 0.5427926778793335,
      "learning_rate": 1.6379166666666667e-05,
      "loss": 0.0018,
      "step": 161380
    },
    {
      "epoch": 5.379666666666667,
      "grad_norm": 0.25706765055656433,
      "learning_rate": 1.6377083333333336e-05,
      "loss": 0.0017,
      "step": 161390
    },
    {
      "epoch": 5.38,
      "grad_norm": 0.38459768891334534,
      "learning_rate": 1.6375e-05,
      "loss": 0.0027,
      "step": 161400
    },
    {
      "epoch": 5.380333333333334,
      "grad_norm": 0.5357019901275635,
      "learning_rate": 1.6372916666666667e-05,
      "loss": 0.0015,
      "step": 161410
    },
    {
      "epoch": 5.3806666666666665,
      "grad_norm": 0.03209870681166649,
      "learning_rate": 1.6370833333333336e-05,
      "loss": 0.0017,
      "step": 161420
    },
    {
      "epoch": 5.381,
      "grad_norm": 0.008136829361319542,
      "learning_rate": 1.636875e-05,
      "loss": 0.0016,
      "step": 161430
    },
    {
      "epoch": 5.381333333333333,
      "grad_norm": 0.08603423833847046,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0013,
      "step": 161440
    },
    {
      "epoch": 5.381666666666667,
      "grad_norm": 0.20053774118423462,
      "learning_rate": 1.6364583333333332e-05,
      "loss": 0.0019,
      "step": 161450
    },
    {
      "epoch": 5.382,
      "grad_norm": 0.05892437696456909,
      "learning_rate": 1.63625e-05,
      "loss": 0.0014,
      "step": 161460
    },
    {
      "epoch": 5.382333333333333,
      "grad_norm": 0.3147728741168976,
      "learning_rate": 1.6360416666666667e-05,
      "loss": 0.0021,
      "step": 161470
    },
    {
      "epoch": 5.382666666666666,
      "grad_norm": 0.10673307627439499,
      "learning_rate": 1.6358333333333332e-05,
      "loss": 0.0015,
      "step": 161480
    },
    {
      "epoch": 5.383,
      "grad_norm": 0.2993316352367401,
      "learning_rate": 1.635625e-05,
      "loss": 0.0022,
      "step": 161490
    },
    {
      "epoch": 5.383333333333334,
      "grad_norm": 0.12412187457084656,
      "learning_rate": 1.6354166666666667e-05,
      "loss": 0.0017,
      "step": 161500
    },
    {
      "epoch": 5.383666666666667,
      "grad_norm": 0.17174012959003448,
      "learning_rate": 1.6352083333333336e-05,
      "loss": 0.0019,
      "step": 161510
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.2000158429145813,
      "learning_rate": 1.635e-05,
      "loss": 0.0018,
      "step": 161520
    },
    {
      "epoch": 5.384333333333333,
      "grad_norm": 0.010081383399665356,
      "learning_rate": 1.634791666666667e-05,
      "loss": 0.0019,
      "step": 161530
    },
    {
      "epoch": 5.384666666666667,
      "grad_norm": 0.029385779052972794,
      "learning_rate": 1.6345833333333335e-05,
      "loss": 0.002,
      "step": 161540
    },
    {
      "epoch": 5.385,
      "grad_norm": 0.3720477521419525,
      "learning_rate": 1.634375e-05,
      "loss": 0.0016,
      "step": 161550
    },
    {
      "epoch": 5.3853333333333335,
      "grad_norm": 0.22923597693443298,
      "learning_rate": 1.6341666666666666e-05,
      "loss": 0.002,
      "step": 161560
    },
    {
      "epoch": 5.385666666666666,
      "grad_norm": 0.07434909790754318,
      "learning_rate": 1.6339583333333332e-05,
      "loss": 0.0019,
      "step": 161570
    },
    {
      "epoch": 5.386,
      "grad_norm": 0.22870807349681854,
      "learning_rate": 1.63375e-05,
      "loss": 0.0021,
      "step": 161580
    },
    {
      "epoch": 5.386333333333333,
      "grad_norm": 0.02915658988058567,
      "learning_rate": 1.6335416666666666e-05,
      "loss": 0.0017,
      "step": 161590
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.11475858092308044,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0015,
      "step": 161600
    },
    {
      "epoch": 5.3870000000000005,
      "grad_norm": 0.1428343802690506,
      "learning_rate": 1.633125e-05,
      "loss": 0.0019,
      "step": 161610
    },
    {
      "epoch": 5.387333333333333,
      "grad_norm": 0.1715199053287506,
      "learning_rate": 1.6329166666666666e-05,
      "loss": 0.002,
      "step": 161620
    },
    {
      "epoch": 5.387666666666667,
      "grad_norm": 0.14259997010231018,
      "learning_rate": 1.6327083333333335e-05,
      "loss": 0.0015,
      "step": 161630
    },
    {
      "epoch": 5.388,
      "grad_norm": 0.05733514204621315,
      "learning_rate": 1.6325e-05,
      "loss": 0.0016,
      "step": 161640
    },
    {
      "epoch": 5.388333333333334,
      "grad_norm": 0.285383403301239,
      "learning_rate": 1.632291666666667e-05,
      "loss": 0.0023,
      "step": 161650
    },
    {
      "epoch": 5.3886666666666665,
      "grad_norm": 0.25694048404693604,
      "learning_rate": 1.6320833333333335e-05,
      "loss": 0.0023,
      "step": 161660
    },
    {
      "epoch": 5.389,
      "grad_norm": 0.14339861273765564,
      "learning_rate": 1.6318750000000004e-05,
      "loss": 0.0012,
      "step": 161670
    },
    {
      "epoch": 5.389333333333333,
      "grad_norm": 0.34328094124794006,
      "learning_rate": 1.6316666666666666e-05,
      "loss": 0.0016,
      "step": 161680
    },
    {
      "epoch": 5.389666666666667,
      "grad_norm": 0.09157691150903702,
      "learning_rate": 1.6314583333333335e-05,
      "loss": 0.0015,
      "step": 161690
    },
    {
      "epoch": 5.39,
      "grad_norm": 0.10431377589702606,
      "learning_rate": 1.63125e-05,
      "loss": 0.0015,
      "step": 161700
    },
    {
      "epoch": 5.390333333333333,
      "grad_norm": 0.08630068600177765,
      "learning_rate": 1.6310416666666666e-05,
      "loss": 0.0019,
      "step": 161710
    },
    {
      "epoch": 5.390666666666666,
      "grad_norm": 0.11434738337993622,
      "learning_rate": 1.6308333333333334e-05,
      "loss": 0.0018,
      "step": 161720
    },
    {
      "epoch": 5.391,
      "grad_norm": 0.1718909740447998,
      "learning_rate": 1.630625e-05,
      "loss": 0.0017,
      "step": 161730
    },
    {
      "epoch": 5.391333333333334,
      "grad_norm": 0.08611766993999481,
      "learning_rate": 1.630416666666667e-05,
      "loss": 0.0013,
      "step": 161740
    },
    {
      "epoch": 5.391666666666667,
      "grad_norm": 0.14408136904239655,
      "learning_rate": 1.6302083333333334e-05,
      "loss": 0.002,
      "step": 161750
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.1712997555732727,
      "learning_rate": 1.63e-05,
      "loss": 0.0015,
      "step": 161760
    },
    {
      "epoch": 5.392333333333333,
      "grad_norm": 0.08573275059461594,
      "learning_rate": 1.629791666666667e-05,
      "loss": 0.0018,
      "step": 161770
    },
    {
      "epoch": 5.392666666666667,
      "grad_norm": 0.2572634220123291,
      "learning_rate": 1.6295833333333334e-05,
      "loss": 0.0014,
      "step": 161780
    },
    {
      "epoch": 5.393,
      "grad_norm": 0.2286527156829834,
      "learning_rate": 1.6293750000000003e-05,
      "loss": 0.0018,
      "step": 161790
    },
    {
      "epoch": 5.3933333333333335,
      "grad_norm": 0.08586433529853821,
      "learning_rate": 1.6291666666666665e-05,
      "loss": 0.0017,
      "step": 161800
    },
    {
      "epoch": 5.393666666666666,
      "grad_norm": 0.12624315917491913,
      "learning_rate": 1.6289583333333334e-05,
      "loss": 0.0016,
      "step": 161810
    },
    {
      "epoch": 5.394,
      "grad_norm": 0.17155922949314117,
      "learning_rate": 1.62875e-05,
      "loss": 0.0018,
      "step": 161820
    },
    {
      "epoch": 5.394333333333333,
      "grad_norm": 0.13941539824008942,
      "learning_rate": 1.628541666666667e-05,
      "loss": 0.0015,
      "step": 161830
    },
    {
      "epoch": 5.394666666666667,
      "grad_norm": 0.11616324633359909,
      "learning_rate": 1.6283333333333334e-05,
      "loss": 0.0028,
      "step": 161840
    },
    {
      "epoch": 5.395,
      "grad_norm": 0.006466045044362545,
      "learning_rate": 1.628125e-05,
      "loss": 0.0023,
      "step": 161850
    },
    {
      "epoch": 5.395333333333333,
      "grad_norm": 0.008771510794758797,
      "learning_rate": 1.6279166666666668e-05,
      "loss": 0.0013,
      "step": 161860
    },
    {
      "epoch": 5.395666666666667,
      "grad_norm": 0.22770501673221588,
      "learning_rate": 1.6277083333333334e-05,
      "loss": 0.0018,
      "step": 161870
    },
    {
      "epoch": 5.396,
      "grad_norm": 0.08681610226631165,
      "learning_rate": 1.6275000000000003e-05,
      "loss": 0.002,
      "step": 161880
    },
    {
      "epoch": 5.396333333333334,
      "grad_norm": 0.3141394257545471,
      "learning_rate": 1.6272916666666668e-05,
      "loss": 0.0017,
      "step": 161890
    },
    {
      "epoch": 5.3966666666666665,
      "grad_norm": 0.3151872456073761,
      "learning_rate": 1.6270833333333334e-05,
      "loss": 0.0016,
      "step": 161900
    },
    {
      "epoch": 5.397,
      "grad_norm": 0.20103588700294495,
      "learning_rate": 1.6268750000000002e-05,
      "loss": 0.0021,
      "step": 161910
    },
    {
      "epoch": 5.397333333333333,
      "grad_norm": 0.2569924592971802,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0027,
      "step": 161920
    },
    {
      "epoch": 5.397666666666667,
      "grad_norm": 0.2568551301956177,
      "learning_rate": 1.6264583333333333e-05,
      "loss": 0.002,
      "step": 161930
    },
    {
      "epoch": 5.398,
      "grad_norm": 0.14302818477153778,
      "learning_rate": 1.62625e-05,
      "loss": 0.0023,
      "step": 161940
    },
    {
      "epoch": 5.398333333333333,
      "grad_norm": 0.20042504370212555,
      "learning_rate": 1.6260416666666668e-05,
      "loss": 0.0018,
      "step": 161950
    },
    {
      "epoch": 5.398666666666666,
      "grad_norm": 0.1428382843732834,
      "learning_rate": 1.6258333333333333e-05,
      "loss": 0.0015,
      "step": 161960
    },
    {
      "epoch": 5.399,
      "grad_norm": 0.3445768654346466,
      "learning_rate": 1.6256250000000002e-05,
      "loss": 0.0019,
      "step": 161970
    },
    {
      "epoch": 5.399333333333333,
      "grad_norm": 0.14292310178279877,
      "learning_rate": 1.6254166666666668e-05,
      "loss": 0.002,
      "step": 161980
    },
    {
      "epoch": 5.399666666666667,
      "grad_norm": 0.08751364797353745,
      "learning_rate": 1.6252083333333333e-05,
      "loss": 0.0015,
      "step": 161990
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.11437741667032242,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.0014,
      "step": 162000
    },
    {
      "epoch": 5.400333333333333,
      "grad_norm": 0.14520564675331116,
      "learning_rate": 1.6247916666666667e-05,
      "loss": 0.0022,
      "step": 162010
    },
    {
      "epoch": 5.400666666666667,
      "grad_norm": 0.11425145715475082,
      "learning_rate": 1.6245833333333336e-05,
      "loss": 0.0013,
      "step": 162020
    },
    {
      "epoch": 5.401,
      "grad_norm": 0.13126525282859802,
      "learning_rate": 1.6243750000000002e-05,
      "loss": 0.0022,
      "step": 162030
    },
    {
      "epoch": 5.4013333333333335,
      "grad_norm": 0.03016340732574463,
      "learning_rate": 1.6241666666666667e-05,
      "loss": 0.0025,
      "step": 162040
    },
    {
      "epoch": 5.401666666666666,
      "grad_norm": 0.49304625391960144,
      "learning_rate": 1.6239583333333333e-05,
      "loss": 0.002,
      "step": 162050
    },
    {
      "epoch": 5.402,
      "grad_norm": 0.3712794780731201,
      "learning_rate": 1.6237499999999998e-05,
      "loss": 0.0015,
      "step": 162060
    },
    {
      "epoch": 5.402333333333333,
      "grad_norm": 0.5716302394866943,
      "learning_rate": 1.6235416666666667e-05,
      "loss": 0.0018,
      "step": 162070
    },
    {
      "epoch": 5.402666666666667,
      "grad_norm": 0.17182445526123047,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.002,
      "step": 162080
    },
    {
      "epoch": 5.4030000000000005,
      "grad_norm": 0.28594380617141724,
      "learning_rate": 1.623125e-05,
      "loss": 0.0019,
      "step": 162090
    },
    {
      "epoch": 5.403333333333333,
      "grad_norm": 0.200611874461174,
      "learning_rate": 1.6229166666666667e-05,
      "loss": 0.0019,
      "step": 162100
    },
    {
      "epoch": 5.403666666666667,
      "grad_norm": 0.05772280693054199,
      "learning_rate": 1.6227083333333336e-05,
      "loss": 0.0013,
      "step": 162110
    },
    {
      "epoch": 5.404,
      "grad_norm": 0.2289503514766693,
      "learning_rate": 1.6225e-05,
      "loss": 0.0022,
      "step": 162120
    },
    {
      "epoch": 5.404333333333334,
      "grad_norm": 0.08606360852718353,
      "learning_rate": 1.6222916666666667e-05,
      "loss": 0.002,
      "step": 162130
    },
    {
      "epoch": 5.4046666666666665,
      "grad_norm": 0.030490823090076447,
      "learning_rate": 1.6220833333333336e-05,
      "loss": 0.0016,
      "step": 162140
    },
    {
      "epoch": 5.405,
      "grad_norm": 0.11441653221845627,
      "learning_rate": 1.621875e-05,
      "loss": 0.002,
      "step": 162150
    },
    {
      "epoch": 5.405333333333333,
      "grad_norm": 0.3432047367095947,
      "learning_rate": 1.6216666666666667e-05,
      "loss": 0.0019,
      "step": 162160
    },
    {
      "epoch": 5.405666666666667,
      "grad_norm": 0.25735461711883545,
      "learning_rate": 1.6214583333333332e-05,
      "loss": 0.0029,
      "step": 162170
    },
    {
      "epoch": 5.406,
      "grad_norm": 0.14297370612621307,
      "learning_rate": 1.62125e-05,
      "loss": 0.0012,
      "step": 162180
    },
    {
      "epoch": 5.406333333333333,
      "grad_norm": 0.1436973512172699,
      "learning_rate": 1.6210416666666667e-05,
      "loss": 0.0019,
      "step": 162190
    },
    {
      "epoch": 5.406666666666666,
      "grad_norm": 0.3212577998638153,
      "learning_rate": 1.6208333333333332e-05,
      "loss": 0.0022,
      "step": 162200
    },
    {
      "epoch": 5.407,
      "grad_norm": 0.3459317982196808,
      "learning_rate": 1.620625e-05,
      "loss": 0.002,
      "step": 162210
    },
    {
      "epoch": 5.407333333333334,
      "grad_norm": 0.029353804886341095,
      "learning_rate": 1.6204166666666666e-05,
      "loss": 0.0015,
      "step": 162220
    },
    {
      "epoch": 5.407666666666667,
      "grad_norm": 0.14250262081623077,
      "learning_rate": 1.6202083333333335e-05,
      "loss": 0.0015,
      "step": 162230
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.14312762022018433,
      "learning_rate": 1.62e-05,
      "loss": 0.0016,
      "step": 162240
    },
    {
      "epoch": 5.408333333333333,
      "grad_norm": 0.7055667638778687,
      "learning_rate": 1.619791666666667e-05,
      "loss": 0.0022,
      "step": 162250
    },
    {
      "epoch": 5.408666666666667,
      "grad_norm": 0.19103528559207916,
      "learning_rate": 1.6195833333333335e-05,
      "loss": 0.0018,
      "step": 162260
    },
    {
      "epoch": 5.409,
      "grad_norm": 0.18069525063037872,
      "learning_rate": 1.619375e-05,
      "loss": 0.0015,
      "step": 162270
    },
    {
      "epoch": 5.4093333333333335,
      "grad_norm": 0.016914069652557373,
      "learning_rate": 1.6191666666666666e-05,
      "loss": 0.0033,
      "step": 162280
    },
    {
      "epoch": 5.409666666666666,
      "grad_norm": 0.06325159966945648,
      "learning_rate": 1.618958333333333e-05,
      "loss": 0.0029,
      "step": 162290
    },
    {
      "epoch": 5.41,
      "grad_norm": 0.029535984620451927,
      "learning_rate": 1.61875e-05,
      "loss": 0.0016,
      "step": 162300
    },
    {
      "epoch": 5.410333333333333,
      "grad_norm": 0.17163409292697906,
      "learning_rate": 1.6185416666666666e-05,
      "loss": 0.0018,
      "step": 162310
    },
    {
      "epoch": 5.410666666666667,
      "grad_norm": 0.17239947617053986,
      "learning_rate": 1.6183333333333335e-05,
      "loss": 0.0014,
      "step": 162320
    },
    {
      "epoch": 5.411,
      "grad_norm": 0.1317594349384308,
      "learning_rate": 1.618125e-05,
      "loss": 0.0031,
      "step": 162330
    },
    {
      "epoch": 5.411333333333333,
      "grad_norm": 0.03196489065885544,
      "learning_rate": 1.6179166666666666e-05,
      "loss": 0.0021,
      "step": 162340
    },
    {
      "epoch": 5.411666666666667,
      "grad_norm": 0.25754067301750183,
      "learning_rate": 1.6177083333333335e-05,
      "loss": 0.0018,
      "step": 162350
    },
    {
      "epoch": 5.412,
      "grad_norm": 0.08669796586036682,
      "learning_rate": 1.6175e-05,
      "loss": 0.0013,
      "step": 162360
    },
    {
      "epoch": 5.412333333333334,
      "grad_norm": 0.17355948686599731,
      "learning_rate": 1.617291666666667e-05,
      "loss": 0.0017,
      "step": 162370
    },
    {
      "epoch": 5.4126666666666665,
      "grad_norm": 0.17154039442539215,
      "learning_rate": 1.6170833333333335e-05,
      "loss": 0.0019,
      "step": 162380
    },
    {
      "epoch": 5.413,
      "grad_norm": 0.14294108748435974,
      "learning_rate": 1.6168750000000003e-05,
      "loss": 0.0023,
      "step": 162390
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 0.22934599220752716,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0021,
      "step": 162400
    },
    {
      "epoch": 5.413666666666667,
      "grad_norm": 0.11456504464149475,
      "learning_rate": 1.6164583333333334e-05,
      "loss": 0.002,
      "step": 162410
    },
    {
      "epoch": 5.414,
      "grad_norm": 0.11465397477149963,
      "learning_rate": 1.61625e-05,
      "loss": 0.002,
      "step": 162420
    },
    {
      "epoch": 5.414333333333333,
      "grad_norm": 0.03215593472123146,
      "learning_rate": 1.6160416666666665e-05,
      "loss": 0.0023,
      "step": 162430
    },
    {
      "epoch": 5.414666666666666,
      "grad_norm": 0.05879543349146843,
      "learning_rate": 1.6158333333333334e-05,
      "loss": 0.0018,
      "step": 162440
    },
    {
      "epoch": 5.415,
      "grad_norm": 0.2005092352628708,
      "learning_rate": 1.615625e-05,
      "loss": 0.0018,
      "step": 162450
    },
    {
      "epoch": 5.415333333333333,
      "grad_norm": 0.05829184129834175,
      "learning_rate": 1.615416666666667e-05,
      "loss": 0.0019,
      "step": 162460
    },
    {
      "epoch": 5.415666666666667,
      "grad_norm": 0.03063715063035488,
      "learning_rate": 1.6152083333333334e-05,
      "loss": 0.0018,
      "step": 162470
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.25016865134239197,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.0026,
      "step": 162480
    },
    {
      "epoch": 5.416333333333333,
      "grad_norm": 0.14303302764892578,
      "learning_rate": 1.614791666666667e-05,
      "loss": 0.0018,
      "step": 162490
    },
    {
      "epoch": 5.416666666666667,
      "grad_norm": 0.3561820983886719,
      "learning_rate": 1.6145833333333334e-05,
      "loss": 0.0022,
      "step": 162500
    },
    {
      "epoch": 5.417,
      "grad_norm": 0.08691836148500443,
      "learning_rate": 1.6143750000000003e-05,
      "loss": 0.0022,
      "step": 162510
    },
    {
      "epoch": 5.417333333333334,
      "grad_norm": 0.16171720623970032,
      "learning_rate": 1.6141666666666668e-05,
      "loss": 0.0019,
      "step": 162520
    },
    {
      "epoch": 5.417666666666666,
      "grad_norm": 0.02957562543451786,
      "learning_rate": 1.6139583333333334e-05,
      "loss": 0.0022,
      "step": 162530
    },
    {
      "epoch": 5.418,
      "grad_norm": 0.2296149879693985,
      "learning_rate": 1.61375e-05,
      "loss": 0.0018,
      "step": 162540
    },
    {
      "epoch": 5.418333333333333,
      "grad_norm": 0.03571520000696182,
      "learning_rate": 1.6135416666666668e-05,
      "loss": 0.0016,
      "step": 162550
    },
    {
      "epoch": 5.418666666666667,
      "grad_norm": 0.14291396737098694,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.002,
      "step": 162560
    },
    {
      "epoch": 5.419,
      "grad_norm": 0.28561678528785706,
      "learning_rate": 1.613125e-05,
      "loss": 0.002,
      "step": 162570
    },
    {
      "epoch": 5.419333333333333,
      "grad_norm": 0.07126735150814056,
      "learning_rate": 1.6129166666666668e-05,
      "loss": 0.0017,
      "step": 162580
    },
    {
      "epoch": 5.419666666666667,
      "grad_norm": 0.11460663378238678,
      "learning_rate": 1.6127083333333333e-05,
      "loss": 0.0017,
      "step": 162590
    },
    {
      "epoch": 5.42,
      "grad_norm": 0.4007248878479004,
      "learning_rate": 1.6125000000000002e-05,
      "loss": 0.0024,
      "step": 162600
    },
    {
      "epoch": 5.420333333333334,
      "grad_norm": 0.057849399745464325,
      "learning_rate": 1.6122916666666668e-05,
      "loss": 0.0029,
      "step": 162610
    },
    {
      "epoch": 5.4206666666666665,
      "grad_norm": 0.08692982792854309,
      "learning_rate": 1.6120833333333337e-05,
      "loss": 0.0021,
      "step": 162620
    },
    {
      "epoch": 5.421,
      "grad_norm": 0.17202652990818024,
      "learning_rate": 1.6118750000000002e-05,
      "loss": 0.002,
      "step": 162630
    },
    {
      "epoch": 5.421333333333333,
      "grad_norm": 0.22863218188285828,
      "learning_rate": 1.6116666666666668e-05,
      "loss": 0.0016,
      "step": 162640
    },
    {
      "epoch": 5.421666666666667,
      "grad_norm": 0.20026926696300507,
      "learning_rate": 1.6114583333333333e-05,
      "loss": 0.002,
      "step": 162650
    },
    {
      "epoch": 5.422,
      "grad_norm": 0.033309247344732285,
      "learning_rate": 1.61125e-05,
      "loss": 0.0022,
      "step": 162660
    },
    {
      "epoch": 5.4223333333333334,
      "grad_norm": 0.1161186620593071,
      "learning_rate": 1.6110416666666667e-05,
      "loss": 0.002,
      "step": 162670
    },
    {
      "epoch": 5.422666666666666,
      "grad_norm": 0.25729313492774963,
      "learning_rate": 1.6108333333333333e-05,
      "loss": 0.0011,
      "step": 162680
    },
    {
      "epoch": 5.423,
      "grad_norm": 0.11456625908613205,
      "learning_rate": 1.6106250000000002e-05,
      "loss": 0.0022,
      "step": 162690
    },
    {
      "epoch": 5.423333333333334,
      "grad_norm": 0.17160381376743317,
      "learning_rate": 1.6104166666666667e-05,
      "loss": 0.0024,
      "step": 162700
    },
    {
      "epoch": 5.423666666666667,
      "grad_norm": 0.14292894303798676,
      "learning_rate": 1.6102083333333333e-05,
      "loss": 0.002,
      "step": 162710
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.1717647761106491,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0013,
      "step": 162720
    },
    {
      "epoch": 5.424333333333333,
      "grad_norm": 0.11801803857088089,
      "learning_rate": 1.6097916666666667e-05,
      "loss": 0.0017,
      "step": 162730
    },
    {
      "epoch": 5.424666666666667,
      "grad_norm": 0.05005304142832756,
      "learning_rate": 1.6095833333333336e-05,
      "loss": 0.0017,
      "step": 162740
    },
    {
      "epoch": 5.425,
      "grad_norm": 0.08617783337831497,
      "learning_rate": 1.609375e-05,
      "loss": 0.002,
      "step": 162750
    },
    {
      "epoch": 5.425333333333334,
      "grad_norm": 0.2574233412742615,
      "learning_rate": 1.609166666666667e-05,
      "loss": 0.0019,
      "step": 162760
    },
    {
      "epoch": 5.425666666666666,
      "grad_norm": 0.42109495401382446,
      "learning_rate": 1.6089583333333333e-05,
      "loss": 0.0019,
      "step": 162770
    },
    {
      "epoch": 5.426,
      "grad_norm": 0.19382990896701813,
      "learning_rate": 1.60875e-05,
      "loss": 0.0025,
      "step": 162780
    },
    {
      "epoch": 5.426333333333333,
      "grad_norm": 0.3146657943725586,
      "learning_rate": 1.6085416666666667e-05,
      "loss": 0.0021,
      "step": 162790
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.4650893211364746,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.0014,
      "step": 162800
    },
    {
      "epoch": 5.427,
      "grad_norm": 0.20059193670749664,
      "learning_rate": 1.608125e-05,
      "loss": 0.0015,
      "step": 162810
    },
    {
      "epoch": 5.427333333333333,
      "grad_norm": 0.23051251471042633,
      "learning_rate": 1.6079166666666667e-05,
      "loss": 0.0026,
      "step": 162820
    },
    {
      "epoch": 5.427666666666667,
      "grad_norm": 0.22901615500450134,
      "learning_rate": 1.6077083333333336e-05,
      "loss": 0.0016,
      "step": 162830
    },
    {
      "epoch": 5.428,
      "grad_norm": 0.22301538288593292,
      "learning_rate": 1.6075e-05,
      "loss": 0.0022,
      "step": 162840
    },
    {
      "epoch": 5.428333333333334,
      "grad_norm": 0.030489854514598846,
      "learning_rate": 1.6072916666666667e-05,
      "loss": 0.0016,
      "step": 162850
    },
    {
      "epoch": 5.4286666666666665,
      "grad_norm": 0.028889745473861694,
      "learning_rate": 1.6070833333333335e-05,
      "loss": 0.0014,
      "step": 162860
    },
    {
      "epoch": 5.429,
      "grad_norm": 0.05936986953020096,
      "learning_rate": 1.606875e-05,
      "loss": 0.0022,
      "step": 162870
    },
    {
      "epoch": 5.429333333333333,
      "grad_norm": 0.25378701090812683,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.002,
      "step": 162880
    },
    {
      "epoch": 5.429666666666667,
      "grad_norm": 0.6164028644561768,
      "learning_rate": 1.6064583333333332e-05,
      "loss": 0.0019,
      "step": 162890
    },
    {
      "epoch": 5.43,
      "grad_norm": 0.08576848357915878,
      "learning_rate": 1.60625e-05,
      "loss": 0.0027,
      "step": 162900
    },
    {
      "epoch": 5.4303333333333335,
      "grad_norm": 0.08590056002140045,
      "learning_rate": 1.6060416666666666e-05,
      "loss": 0.0018,
      "step": 162910
    },
    {
      "epoch": 5.430666666666666,
      "grad_norm": 0.6161752343177795,
      "learning_rate": 1.6058333333333335e-05,
      "loss": 0.0023,
      "step": 162920
    },
    {
      "epoch": 5.431,
      "grad_norm": 0.05838436260819435,
      "learning_rate": 1.605625e-05,
      "loss": 0.0018,
      "step": 162930
    },
    {
      "epoch": 5.431333333333333,
      "grad_norm": 0.2052372843027115,
      "learning_rate": 1.6054166666666666e-05,
      "loss": 0.0016,
      "step": 162940
    },
    {
      "epoch": 5.431666666666667,
      "grad_norm": 0.03086911141872406,
      "learning_rate": 1.6052083333333335e-05,
      "loss": 0.0015,
      "step": 162950
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.29379981756210327,
      "learning_rate": 1.605e-05,
      "loss": 0.0014,
      "step": 162960
    },
    {
      "epoch": 5.432333333333333,
      "grad_norm": 0.1430780589580536,
      "learning_rate": 1.604791666666667e-05,
      "loss": 0.0038,
      "step": 162970
    },
    {
      "epoch": 5.432666666666667,
      "grad_norm": 0.14326341450214386,
      "learning_rate": 1.6045833333333335e-05,
      "loss": 0.0015,
      "step": 162980
    },
    {
      "epoch": 5.433,
      "grad_norm": 0.03200260177254677,
      "learning_rate": 1.604375e-05,
      "loss": 0.0025,
      "step": 162990
    },
    {
      "epoch": 5.433333333333334,
      "grad_norm": 0.0861533060669899,
      "learning_rate": 1.604166666666667e-05,
      "loss": 0.0014,
      "step": 163000
    },
    {
      "epoch": 5.433666666666666,
      "grad_norm": 0.20658113062381744,
      "learning_rate": 1.603958333333333e-05,
      "loss": 0.0021,
      "step": 163010
    },
    {
      "epoch": 5.434,
      "grad_norm": 0.3426109254360199,
      "learning_rate": 1.60375e-05,
      "loss": 0.0019,
      "step": 163020
    },
    {
      "epoch": 5.434333333333333,
      "grad_norm": 0.2576565742492676,
      "learning_rate": 1.6035416666666666e-05,
      "loss": 0.0015,
      "step": 163030
    },
    {
      "epoch": 5.434666666666667,
      "grad_norm": 0.03119361773133278,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.0021,
      "step": 163040
    },
    {
      "epoch": 5.435,
      "grad_norm": 0.11471031606197357,
      "learning_rate": 1.603125e-05,
      "loss": 0.0017,
      "step": 163050
    },
    {
      "epoch": 5.435333333333333,
      "grad_norm": 0.010774597525596619,
      "learning_rate": 1.602916666666667e-05,
      "loss": 0.0013,
      "step": 163060
    },
    {
      "epoch": 5.435666666666666,
      "grad_norm": 0.03234805166721344,
      "learning_rate": 1.6027083333333334e-05,
      "loss": 0.0019,
      "step": 163070
    },
    {
      "epoch": 5.436,
      "grad_norm": 0.19957685470581055,
      "learning_rate": 1.6025e-05,
      "loss": 0.0017,
      "step": 163080
    },
    {
      "epoch": 5.436333333333334,
      "grad_norm": 0.05843663960695267,
      "learning_rate": 1.602291666666667e-05,
      "loss": 0.0023,
      "step": 163090
    },
    {
      "epoch": 5.4366666666666665,
      "grad_norm": 0.033233754336833954,
      "learning_rate": 1.6020833333333334e-05,
      "loss": 0.0014,
      "step": 163100
    },
    {
      "epoch": 5.437,
      "grad_norm": 0.24389444291591644,
      "learning_rate": 1.6018750000000003e-05,
      "loss": 0.0025,
      "step": 163110
    },
    {
      "epoch": 5.437333333333333,
      "grad_norm": 0.0578889325261116,
      "learning_rate": 1.601666666666667e-05,
      "loss": 0.0015,
      "step": 163120
    },
    {
      "epoch": 5.437666666666667,
      "grad_norm": 0.2021847516298294,
      "learning_rate": 1.6014583333333334e-05,
      "loss": 0.0018,
      "step": 163130
    },
    {
      "epoch": 5.438,
      "grad_norm": 0.2870844304561615,
      "learning_rate": 1.60125e-05,
      "loss": 0.0022,
      "step": 163140
    },
    {
      "epoch": 5.4383333333333335,
      "grad_norm": 0.3431135416030884,
      "learning_rate": 1.6010416666666665e-05,
      "loss": 0.0017,
      "step": 163150
    },
    {
      "epoch": 5.438666666666666,
      "grad_norm": 0.20064377784729004,
      "learning_rate": 1.6008333333333334e-05,
      "loss": 0.0018,
      "step": 163160
    },
    {
      "epoch": 5.439,
      "grad_norm": 0.2281191498041153,
      "learning_rate": 1.600625e-05,
      "loss": 0.0017,
      "step": 163170
    },
    {
      "epoch": 5.439333333333334,
      "grad_norm": 0.057758577167987823,
      "learning_rate": 1.6004166666666668e-05,
      "loss": 0.0013,
      "step": 163180
    },
    {
      "epoch": 5.439666666666667,
      "grad_norm": 0.4005142152309418,
      "learning_rate": 1.6002083333333334e-05,
      "loss": 0.0017,
      "step": 163190
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.15457390248775482,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0018,
      "step": 163200
    },
    {
      "epoch": 5.440333333333333,
      "grad_norm": 0.1439714878797531,
      "learning_rate": 1.5997916666666668e-05,
      "loss": 0.0023,
      "step": 163210
    },
    {
      "epoch": 5.440666666666667,
      "grad_norm": 0.05797195807099342,
      "learning_rate": 1.5995833333333334e-05,
      "loss": 0.0024,
      "step": 163220
    },
    {
      "epoch": 5.441,
      "grad_norm": 0.0058944677002727985,
      "learning_rate": 1.5993750000000003e-05,
      "loss": 0.0016,
      "step": 163230
    },
    {
      "epoch": 5.441333333333334,
      "grad_norm": 0.057478372007608414,
      "learning_rate": 1.5991666666666668e-05,
      "loss": 0.0014,
      "step": 163240
    },
    {
      "epoch": 5.441666666666666,
      "grad_norm": 0.05880173668265343,
      "learning_rate": 1.5989583333333333e-05,
      "loss": 0.0017,
      "step": 163250
    },
    {
      "epoch": 5.442,
      "grad_norm": 0.3712785243988037,
      "learning_rate": 1.59875e-05,
      "loss": 0.0016,
      "step": 163260
    },
    {
      "epoch": 5.442333333333333,
      "grad_norm": 0.0572885125875473,
      "learning_rate": 1.5985416666666668e-05,
      "loss": 0.0019,
      "step": 163270
    },
    {
      "epoch": 5.442666666666667,
      "grad_norm": 0.03056974522769451,
      "learning_rate": 1.5983333333333333e-05,
      "loss": 0.0021,
      "step": 163280
    },
    {
      "epoch": 5.443,
      "grad_norm": 0.11495853215456009,
      "learning_rate": 1.598125e-05,
      "loss": 0.0013,
      "step": 163290
    },
    {
      "epoch": 5.443333333333333,
      "grad_norm": 0.22815287113189697,
      "learning_rate": 1.5979166666666668e-05,
      "loss": 0.0019,
      "step": 163300
    },
    {
      "epoch": 5.443666666666667,
      "grad_norm": 0.11431342363357544,
      "learning_rate": 1.5977083333333333e-05,
      "loss": 0.0016,
      "step": 163310
    },
    {
      "epoch": 5.444,
      "grad_norm": 0.05772176757454872,
      "learning_rate": 1.5975000000000002e-05,
      "loss": 0.0019,
      "step": 163320
    },
    {
      "epoch": 5.444333333333334,
      "grad_norm": 0.25709134340286255,
      "learning_rate": 1.5972916666666668e-05,
      "loss": 0.0023,
      "step": 163330
    },
    {
      "epoch": 5.4446666666666665,
      "grad_norm": 0.25731799006462097,
      "learning_rate": 1.5970833333333336e-05,
      "loss": 0.0013,
      "step": 163340
    },
    {
      "epoch": 5.445,
      "grad_norm": 0.05924421548843384,
      "learning_rate": 1.5968750000000002e-05,
      "loss": 0.0018,
      "step": 163350
    },
    {
      "epoch": 5.445333333333333,
      "grad_norm": 0.23210875689983368,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0016,
      "step": 163360
    },
    {
      "epoch": 5.445666666666667,
      "grad_norm": 0.22859136760234833,
      "learning_rate": 1.5964583333333333e-05,
      "loss": 0.0024,
      "step": 163370
    },
    {
      "epoch": 5.446,
      "grad_norm": 0.20024780929088593,
      "learning_rate": 1.59625e-05,
      "loss": 0.0015,
      "step": 163380
    },
    {
      "epoch": 5.4463333333333335,
      "grad_norm": 0.20014822483062744,
      "learning_rate": 1.5960416666666667e-05,
      "loss": 0.0017,
      "step": 163390
    },
    {
      "epoch": 5.446666666666666,
      "grad_norm": 0.4004761874675751,
      "learning_rate": 1.5958333333333333e-05,
      "loss": 0.0024,
      "step": 163400
    },
    {
      "epoch": 5.447,
      "grad_norm": 0.31427404284477234,
      "learning_rate": 1.595625e-05,
      "loss": 0.0015,
      "step": 163410
    },
    {
      "epoch": 5.447333333333333,
      "grad_norm": 0.13218176364898682,
      "learning_rate": 1.5954166666666667e-05,
      "loss": 0.0021,
      "step": 163420
    },
    {
      "epoch": 5.447666666666667,
      "grad_norm": 0.3996639549732208,
      "learning_rate": 1.5952083333333333e-05,
      "loss": 0.0014,
      "step": 163430
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.02943604625761509,
      "learning_rate": 1.595e-05,
      "loss": 0.0012,
      "step": 163440
    },
    {
      "epoch": 5.448333333333333,
      "grad_norm": 0.05720582231879234,
      "learning_rate": 1.5947916666666667e-05,
      "loss": 0.0012,
      "step": 163450
    },
    {
      "epoch": 5.448666666666667,
      "grad_norm": 0.5138047933578491,
      "learning_rate": 1.5945833333333336e-05,
      "loss": 0.0018,
      "step": 163460
    },
    {
      "epoch": 5.449,
      "grad_norm": 0.05753781273961067,
      "learning_rate": 1.594375e-05,
      "loss": 0.002,
      "step": 163470
    },
    {
      "epoch": 5.449333333333334,
      "grad_norm": 0.010086003690958023,
      "learning_rate": 1.594166666666667e-05,
      "loss": 0.0014,
      "step": 163480
    },
    {
      "epoch": 5.449666666666666,
      "grad_norm": 0.34268811345100403,
      "learning_rate": 1.5939583333333332e-05,
      "loss": 0.0016,
      "step": 163490
    },
    {
      "epoch": 5.45,
      "grad_norm": 0.25772824883461,
      "learning_rate": 1.59375e-05,
      "loss": 0.0022,
      "step": 163500
    },
    {
      "epoch": 5.450333333333333,
      "grad_norm": 0.31432750821113586,
      "learning_rate": 1.5935416666666667e-05,
      "loss": 0.0025,
      "step": 163510
    },
    {
      "epoch": 5.450666666666667,
      "grad_norm": 0.2057345062494278,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0022,
      "step": 163520
    },
    {
      "epoch": 5.451,
      "grad_norm": 0.37146151065826416,
      "learning_rate": 1.593125e-05,
      "loss": 0.0026,
      "step": 163530
    },
    {
      "epoch": 5.451333333333333,
      "grad_norm": 0.2575201988220215,
      "learning_rate": 1.5929166666666666e-05,
      "loss": 0.0017,
      "step": 163540
    },
    {
      "epoch": 5.451666666666666,
      "grad_norm": 0.45651283860206604,
      "learning_rate": 1.5927083333333335e-05,
      "loss": 0.0019,
      "step": 163550
    },
    {
      "epoch": 5.452,
      "grad_norm": 0.08575516194105148,
      "learning_rate": 1.5925e-05,
      "loss": 0.0025,
      "step": 163560
    },
    {
      "epoch": 5.452333333333334,
      "grad_norm": 0.16910997033119202,
      "learning_rate": 1.5922916666666666e-05,
      "loss": 0.0023,
      "step": 163570
    },
    {
      "epoch": 5.4526666666666666,
      "grad_norm": 0.029747964814305305,
      "learning_rate": 1.5920833333333335e-05,
      "loss": 0.0022,
      "step": 163580
    },
    {
      "epoch": 5.453,
      "grad_norm": 0.11512366682291031,
      "learning_rate": 1.591875e-05,
      "loss": 0.0015,
      "step": 163590
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.11478295177221298,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.0016,
      "step": 163600
    },
    {
      "epoch": 5.453666666666667,
      "grad_norm": 0.2872108817100525,
      "learning_rate": 1.591458333333333e-05,
      "loss": 0.0021,
      "step": 163610
    },
    {
      "epoch": 5.454,
      "grad_norm": 0.1714392453432083,
      "learning_rate": 1.59125e-05,
      "loss": 0.0014,
      "step": 163620
    },
    {
      "epoch": 5.4543333333333335,
      "grad_norm": 0.17109903693199158,
      "learning_rate": 1.5910416666666666e-05,
      "loss": 0.0015,
      "step": 163630
    },
    {
      "epoch": 5.454666666666666,
      "grad_norm": 0.34626591205596924,
      "learning_rate": 1.5908333333333335e-05,
      "loss": 0.0017,
      "step": 163640
    },
    {
      "epoch": 5.455,
      "grad_norm": 0.2575526535511017,
      "learning_rate": 1.590625e-05,
      "loss": 0.0016,
      "step": 163650
    },
    {
      "epoch": 5.455333333333333,
      "grad_norm": 0.030086146667599678,
      "learning_rate": 1.5904166666666666e-05,
      "loss": 0.0018,
      "step": 163660
    },
    {
      "epoch": 5.455666666666667,
      "grad_norm": 0.6320561766624451,
      "learning_rate": 1.5902083333333335e-05,
      "loss": 0.0019,
      "step": 163670
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.45682722330093384,
      "learning_rate": 1.59e-05,
      "loss": 0.002,
      "step": 163680
    },
    {
      "epoch": 5.456333333333333,
      "grad_norm": 0.05766644328832626,
      "learning_rate": 1.589791666666667e-05,
      "loss": 0.0018,
      "step": 163690
    },
    {
      "epoch": 5.456666666666667,
      "grad_norm": 0.5425101518630981,
      "learning_rate": 1.5895833333333335e-05,
      "loss": 0.0017,
      "step": 163700
    },
    {
      "epoch": 5.457,
      "grad_norm": 0.23746263980865479,
      "learning_rate": 1.589375e-05,
      "loss": 0.0015,
      "step": 163710
    },
    {
      "epoch": 5.457333333333334,
      "grad_norm": 0.1125008761882782,
      "learning_rate": 1.589166666666667e-05,
      "loss": 0.0027,
      "step": 163720
    },
    {
      "epoch": 5.457666666666666,
      "grad_norm": 0.25476980209350586,
      "learning_rate": 1.588958333333333e-05,
      "loss": 0.0018,
      "step": 163730
    },
    {
      "epoch": 5.458,
      "grad_norm": 0.270233690738678,
      "learning_rate": 1.58875e-05,
      "loss": 0.0024,
      "step": 163740
    },
    {
      "epoch": 5.458333333333333,
      "grad_norm": 0.05784269794821739,
      "learning_rate": 1.5885416666666665e-05,
      "loss": 0.0023,
      "step": 163750
    },
    {
      "epoch": 5.458666666666667,
      "grad_norm": 0.20511791110038757,
      "learning_rate": 1.5883333333333334e-05,
      "loss": 0.002,
      "step": 163760
    },
    {
      "epoch": 5.459,
      "grad_norm": 0.012859024107456207,
      "learning_rate": 1.588125e-05,
      "loss": 0.0026,
      "step": 163770
    },
    {
      "epoch": 5.459333333333333,
      "grad_norm": 0.8078082203865051,
      "learning_rate": 1.587916666666667e-05,
      "loss": 0.0017,
      "step": 163780
    },
    {
      "epoch": 5.459666666666667,
      "grad_norm": 0.08555876463651657,
      "learning_rate": 1.5877083333333334e-05,
      "loss": 0.0016,
      "step": 163790
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.1148553192615509,
      "learning_rate": 1.5875e-05,
      "loss": 0.0014,
      "step": 163800
    },
    {
      "epoch": 5.460333333333334,
      "grad_norm": 0.14418983459472656,
      "learning_rate": 1.587291666666667e-05,
      "loss": 0.0019,
      "step": 163810
    },
    {
      "epoch": 5.460666666666667,
      "grad_norm": 0.6249834299087524,
      "learning_rate": 1.5870833333333334e-05,
      "loss": 0.0017,
      "step": 163820
    },
    {
      "epoch": 5.461,
      "grad_norm": 0.1142430230975151,
      "learning_rate": 1.5868750000000003e-05,
      "loss": 0.002,
      "step": 163830
    },
    {
      "epoch": 5.461333333333333,
      "grad_norm": 0.28612470626831055,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0021,
      "step": 163840
    },
    {
      "epoch": 5.461666666666667,
      "grad_norm": 0.1172531470656395,
      "learning_rate": 1.5864583333333334e-05,
      "loss": 0.0016,
      "step": 163850
    },
    {
      "epoch": 5.462,
      "grad_norm": 0.5908797383308411,
      "learning_rate": 1.58625e-05,
      "loss": 0.0017,
      "step": 163860
    },
    {
      "epoch": 5.4623333333333335,
      "grad_norm": 0.11430826783180237,
      "learning_rate": 1.5860416666666665e-05,
      "loss": 0.0027,
      "step": 163870
    },
    {
      "epoch": 5.462666666666666,
      "grad_norm": 0.11456868797540665,
      "learning_rate": 1.5858333333333334e-05,
      "loss": 0.0022,
      "step": 163880
    },
    {
      "epoch": 5.463,
      "grad_norm": 0.03000083938241005,
      "learning_rate": 1.585625e-05,
      "loss": 0.0022,
      "step": 163890
    },
    {
      "epoch": 5.463333333333333,
      "grad_norm": 0.057539377361536026,
      "learning_rate": 1.5854166666666668e-05,
      "loss": 0.0026,
      "step": 163900
    },
    {
      "epoch": 5.463666666666667,
      "grad_norm": 0.05767129361629486,
      "learning_rate": 1.5852083333333334e-05,
      "loss": 0.0023,
      "step": 163910
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.28575989603996277,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.0017,
      "step": 163920
    },
    {
      "epoch": 5.464333333333333,
      "grad_norm": 0.47142115235328674,
      "learning_rate": 1.5847916666666668e-05,
      "loss": 0.0016,
      "step": 163930
    },
    {
      "epoch": 5.464666666666667,
      "grad_norm": 0.1720440685749054,
      "learning_rate": 1.5845833333333333e-05,
      "loss": 0.002,
      "step": 163940
    },
    {
      "epoch": 5.465,
      "grad_norm": 0.3714759349822998,
      "learning_rate": 1.5843750000000002e-05,
      "loss": 0.0014,
      "step": 163950
    },
    {
      "epoch": 5.465333333333334,
      "grad_norm": 0.17166990041732788,
      "learning_rate": 1.5841666666666668e-05,
      "loss": 0.0015,
      "step": 163960
    },
    {
      "epoch": 5.4656666666666665,
      "grad_norm": 0.37197551131248474,
      "learning_rate": 1.5839583333333337e-05,
      "loss": 0.0018,
      "step": 163970
    },
    {
      "epoch": 5.466,
      "grad_norm": 0.23797911405563354,
      "learning_rate": 1.58375e-05,
      "loss": 0.0021,
      "step": 163980
    },
    {
      "epoch": 5.466333333333333,
      "grad_norm": 0.2574576139450073,
      "learning_rate": 1.5835416666666668e-05,
      "loss": 0.0013,
      "step": 163990
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.14268603920936584,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.0021,
      "step": 164000
    },
    {
      "epoch": 5.467,
      "grad_norm": 0.05819305032491684,
      "learning_rate": 1.583125e-05,
      "loss": 0.0022,
      "step": 164010
    },
    {
      "epoch": 5.467333333333333,
      "grad_norm": 0.09369657188653946,
      "learning_rate": 1.5829166666666667e-05,
      "loss": 0.002,
      "step": 164020
    },
    {
      "epoch": 5.467666666666666,
      "grad_norm": 0.11475987732410431,
      "learning_rate": 1.5827083333333333e-05,
      "loss": 0.0021,
      "step": 164030
    },
    {
      "epoch": 5.468,
      "grad_norm": 0.08626631647348404,
      "learning_rate": 1.5825000000000002e-05,
      "loss": 0.0022,
      "step": 164040
    },
    {
      "epoch": 5.468333333333334,
      "grad_norm": 0.05763963982462883,
      "learning_rate": 1.5822916666666667e-05,
      "loss": 0.0013,
      "step": 164050
    },
    {
      "epoch": 5.468666666666667,
      "grad_norm": 0.2574124336242676,
      "learning_rate": 1.5820833333333336e-05,
      "loss": 0.0027,
      "step": 164060
    },
    {
      "epoch": 5.469,
      "grad_norm": 0.05734528973698616,
      "learning_rate": 1.581875e-05,
      "loss": 0.0015,
      "step": 164070
    },
    {
      "epoch": 5.469333333333333,
      "grad_norm": 0.14405883848667145,
      "learning_rate": 1.5816666666666667e-05,
      "loss": 0.0022,
      "step": 164080
    },
    {
      "epoch": 5.469666666666667,
      "grad_norm": 0.2096712589263916,
      "learning_rate": 1.5814583333333336e-05,
      "loss": 0.0018,
      "step": 164090
    },
    {
      "epoch": 5.47,
      "grad_norm": 0.11538773030042648,
      "learning_rate": 1.5812499999999998e-05,
      "loss": 0.0014,
      "step": 164100
    },
    {
      "epoch": 5.4703333333333335,
      "grad_norm": 0.23037299513816833,
      "learning_rate": 1.5810416666666667e-05,
      "loss": 0.0015,
      "step": 164110
    },
    {
      "epoch": 5.470666666666666,
      "grad_norm": 0.229215607047081,
      "learning_rate": 1.5808333333333332e-05,
      "loss": 0.0015,
      "step": 164120
    },
    {
      "epoch": 5.471,
      "grad_norm": 0.19986283779144287,
      "learning_rate": 1.580625e-05,
      "loss": 0.0014,
      "step": 164130
    },
    {
      "epoch": 5.471333333333333,
      "grad_norm": 0.13553065061569214,
      "learning_rate": 1.5804166666666667e-05,
      "loss": 0.002,
      "step": 164140
    },
    {
      "epoch": 5.471666666666667,
      "grad_norm": 0.17713232338428497,
      "learning_rate": 1.5802083333333336e-05,
      "loss": 0.0015,
      "step": 164150
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.06709623336791992,
      "learning_rate": 1.58e-05,
      "loss": 0.0023,
      "step": 164160
    },
    {
      "epoch": 5.472333333333333,
      "grad_norm": 0.2001643031835556,
      "learning_rate": 1.5797916666666667e-05,
      "loss": 0.0016,
      "step": 164170
    },
    {
      "epoch": 5.472666666666667,
      "grad_norm": 0.2002888023853302,
      "learning_rate": 1.5795833333333336e-05,
      "loss": 0.0022,
      "step": 164180
    },
    {
      "epoch": 5.473,
      "grad_norm": 0.03310466185212135,
      "learning_rate": 1.579375e-05,
      "loss": 0.0018,
      "step": 164190
    },
    {
      "epoch": 5.473333333333334,
      "grad_norm": 0.08590927720069885,
      "learning_rate": 1.579166666666667e-05,
      "loss": 0.0025,
      "step": 164200
    },
    {
      "epoch": 5.4736666666666665,
      "grad_norm": 0.46942993998527527,
      "learning_rate": 1.5789583333333335e-05,
      "loss": 0.0019,
      "step": 164210
    },
    {
      "epoch": 5.474,
      "grad_norm": 0.25748541951179504,
      "learning_rate": 1.57875e-05,
      "loss": 0.0019,
      "step": 164220
    },
    {
      "epoch": 5.474333333333333,
      "grad_norm": 0.3149547874927521,
      "learning_rate": 1.5785416666666666e-05,
      "loss": 0.0013,
      "step": 164230
    },
    {
      "epoch": 5.474666666666667,
      "grad_norm": 0.22820110619068146,
      "learning_rate": 1.5783333333333332e-05,
      "loss": 0.0023,
      "step": 164240
    },
    {
      "epoch": 5.475,
      "grad_norm": 0.14313596487045288,
      "learning_rate": 1.578125e-05,
      "loss": 0.0025,
      "step": 164250
    },
    {
      "epoch": 5.475333333333333,
      "grad_norm": 0.2855241894721985,
      "learning_rate": 1.5779166666666666e-05,
      "loss": 0.0019,
      "step": 164260
    },
    {
      "epoch": 5.475666666666667,
      "grad_norm": 0.11938313394784927,
      "learning_rate": 1.5777083333333335e-05,
      "loss": 0.0019,
      "step": 164270
    },
    {
      "epoch": 5.476,
      "grad_norm": 0.47414430975914,
      "learning_rate": 1.5775e-05,
      "loss": 0.0018,
      "step": 164280
    },
    {
      "epoch": 5.476333333333334,
      "grad_norm": 0.1723514348268509,
      "learning_rate": 1.577291666666667e-05,
      "loss": 0.0021,
      "step": 164290
    },
    {
      "epoch": 5.476666666666667,
      "grad_norm": 0.5712631940841675,
      "learning_rate": 1.5770833333333335e-05,
      "loss": 0.0024,
      "step": 164300
    },
    {
      "epoch": 5.477,
      "grad_norm": 0.029321489855647087,
      "learning_rate": 1.576875e-05,
      "loss": 0.0023,
      "step": 164310
    },
    {
      "epoch": 5.477333333333333,
      "grad_norm": 0.14276579022407532,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0021,
      "step": 164320
    },
    {
      "epoch": 5.477666666666667,
      "grad_norm": 0.19989946484565735,
      "learning_rate": 1.5764583333333335e-05,
      "loss": 0.0018,
      "step": 164330
    },
    {
      "epoch": 5.478,
      "grad_norm": 0.5373282432556152,
      "learning_rate": 1.57625e-05,
      "loss": 0.0018,
      "step": 164340
    },
    {
      "epoch": 5.4783333333333335,
      "grad_norm": 0.008754346519708633,
      "learning_rate": 1.5760416666666666e-05,
      "loss": 0.0021,
      "step": 164350
    },
    {
      "epoch": 5.478666666666666,
      "grad_norm": 0.20025601983070374,
      "learning_rate": 1.5758333333333335e-05,
      "loss": 0.0019,
      "step": 164360
    },
    {
      "epoch": 5.479,
      "grad_norm": 0.009884360246360302,
      "learning_rate": 1.575625e-05,
      "loss": 0.0014,
      "step": 164370
    },
    {
      "epoch": 5.479333333333333,
      "grad_norm": 0.11739494651556015,
      "learning_rate": 1.5754166666666666e-05,
      "loss": 0.0027,
      "step": 164380
    },
    {
      "epoch": 5.479666666666667,
      "grad_norm": 0.10082942992448807,
      "learning_rate": 1.5752083333333334e-05,
      "loss": 0.0013,
      "step": 164390
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.17339538037776947,
      "learning_rate": 1.575e-05,
      "loss": 0.0013,
      "step": 164400
    },
    {
      "epoch": 5.480333333333333,
      "grad_norm": 0.0868670791387558,
      "learning_rate": 1.574791666666667e-05,
      "loss": 0.0017,
      "step": 164410
    },
    {
      "epoch": 5.480666666666667,
      "grad_norm": 0.058019738644361496,
      "learning_rate": 1.5745833333333334e-05,
      "loss": 0.0024,
      "step": 164420
    },
    {
      "epoch": 5.481,
      "grad_norm": 0.20003727078437805,
      "learning_rate": 1.5743750000000003e-05,
      "loss": 0.0014,
      "step": 164430
    },
    {
      "epoch": 5.481333333333334,
      "grad_norm": 0.057725585997104645,
      "learning_rate": 1.574166666666667e-05,
      "loss": 0.0017,
      "step": 164440
    },
    {
      "epoch": 5.4816666666666665,
      "grad_norm": 0.1431109607219696,
      "learning_rate": 1.5739583333333334e-05,
      "loss": 0.0016,
      "step": 164450
    },
    {
      "epoch": 5.482,
      "grad_norm": 0.14321869611740112,
      "learning_rate": 1.57375e-05,
      "loss": 0.0017,
      "step": 164460
    },
    {
      "epoch": 5.482333333333333,
      "grad_norm": 0.17205515503883362,
      "learning_rate": 1.5735416666666665e-05,
      "loss": 0.0016,
      "step": 164470
    },
    {
      "epoch": 5.482666666666667,
      "grad_norm": 0.030901245772838593,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0018,
      "step": 164480
    },
    {
      "epoch": 5.483,
      "grad_norm": 0.029228106141090393,
      "learning_rate": 1.573125e-05,
      "loss": 0.0021,
      "step": 164490
    },
    {
      "epoch": 5.483333333333333,
      "grad_norm": 0.0861668512225151,
      "learning_rate": 1.572916666666667e-05,
      "loss": 0.0014,
      "step": 164500
    },
    {
      "epoch": 5.483666666666666,
      "grad_norm": 0.19530746340751648,
      "learning_rate": 1.5727083333333334e-05,
      "loss": 0.0024,
      "step": 164510
    },
    {
      "epoch": 5.484,
      "grad_norm": 0.030149253085255623,
      "learning_rate": 1.5725e-05,
      "loss": 0.0019,
      "step": 164520
    },
    {
      "epoch": 5.484333333333334,
      "grad_norm": 0.23119783401489258,
      "learning_rate": 1.5722916666666668e-05,
      "loss": 0.0015,
      "step": 164530
    },
    {
      "epoch": 5.484666666666667,
      "grad_norm": 0.008606956340372562,
      "learning_rate": 1.5720833333333334e-05,
      "loss": 0.0014,
      "step": 164540
    },
    {
      "epoch": 5.485,
      "grad_norm": 0.05769123509526253,
      "learning_rate": 1.5718750000000003e-05,
      "loss": 0.0019,
      "step": 164550
    },
    {
      "epoch": 5.485333333333333,
      "grad_norm": 0.0330534502863884,
      "learning_rate": 1.5716666666666668e-05,
      "loss": 0.0013,
      "step": 164560
    },
    {
      "epoch": 5.485666666666667,
      "grad_norm": 0.5142263174057007,
      "learning_rate": 1.5714583333333337e-05,
      "loss": 0.0018,
      "step": 164570
    },
    {
      "epoch": 5.486,
      "grad_norm": 0.3096802234649658,
      "learning_rate": 1.57125e-05,
      "loss": 0.0017,
      "step": 164580
    },
    {
      "epoch": 5.4863333333333335,
      "grad_norm": 0.3426729440689087,
      "learning_rate": 1.5710416666666668e-05,
      "loss": 0.002,
      "step": 164590
    },
    {
      "epoch": 5.486666666666666,
      "grad_norm": 0.2575247287750244,
      "learning_rate": 1.5708333333333333e-05,
      "loss": 0.0016,
      "step": 164600
    },
    {
      "epoch": 5.487,
      "grad_norm": 0.08655637502670288,
      "learning_rate": 1.570625e-05,
      "loss": 0.002,
      "step": 164610
    },
    {
      "epoch": 5.487333333333333,
      "grad_norm": 0.1861080676317215,
      "learning_rate": 1.5704166666666668e-05,
      "loss": 0.0022,
      "step": 164620
    },
    {
      "epoch": 5.487666666666667,
      "grad_norm": 0.20080451667308807,
      "learning_rate": 1.5702083333333333e-05,
      "loss": 0.0021,
      "step": 164630
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.29063892364501953,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.002,
      "step": 164640
    },
    {
      "epoch": 5.488333333333333,
      "grad_norm": 0.7429555654525757,
      "learning_rate": 1.5697916666666668e-05,
      "loss": 0.0025,
      "step": 164650
    },
    {
      "epoch": 5.488666666666667,
      "grad_norm": 0.08588939160108566,
      "learning_rate": 1.5695833333333333e-05,
      "loss": 0.0021,
      "step": 164660
    },
    {
      "epoch": 5.489,
      "grad_norm": 0.008974936790764332,
      "learning_rate": 1.5693750000000002e-05,
      "loss": 0.0013,
      "step": 164670
    },
    {
      "epoch": 5.489333333333334,
      "grad_norm": 0.33736443519592285,
      "learning_rate": 1.5691666666666667e-05,
      "loss": 0.0024,
      "step": 164680
    },
    {
      "epoch": 5.4896666666666665,
      "grad_norm": 0.2574526071548462,
      "learning_rate": 1.5689583333333336e-05,
      "loss": 0.0025,
      "step": 164690
    },
    {
      "epoch": 5.49,
      "grad_norm": 0.17179669439792633,
      "learning_rate": 1.56875e-05,
      "loss": 0.003,
      "step": 164700
    },
    {
      "epoch": 5.490333333333333,
      "grad_norm": 0.07394184917211533,
      "learning_rate": 1.5685416666666667e-05,
      "loss": 0.0013,
      "step": 164710
    },
    {
      "epoch": 5.490666666666667,
      "grad_norm": 0.11470741778612137,
      "learning_rate": 1.5683333333333333e-05,
      "loss": 0.0017,
      "step": 164720
    },
    {
      "epoch": 5.491,
      "grad_norm": 0.11481182277202606,
      "learning_rate": 1.568125e-05,
      "loss": 0.0013,
      "step": 164730
    },
    {
      "epoch": 5.491333333333333,
      "grad_norm": 0.008528662845492363,
      "learning_rate": 1.5679166666666667e-05,
      "loss": 0.0023,
      "step": 164740
    },
    {
      "epoch": 5.491666666666666,
      "grad_norm": 0.17224079370498657,
      "learning_rate": 1.5677083333333333e-05,
      "loss": 0.0015,
      "step": 164750
    },
    {
      "epoch": 5.492,
      "grad_norm": 0.02905627340078354,
      "learning_rate": 1.5675e-05,
      "loss": 0.0019,
      "step": 164760
    },
    {
      "epoch": 5.492333333333334,
      "grad_norm": 0.22855478525161743,
      "learning_rate": 1.5672916666666667e-05,
      "loss": 0.002,
      "step": 164770
    },
    {
      "epoch": 5.492666666666667,
      "grad_norm": 0.05925554409623146,
      "learning_rate": 1.5670833333333336e-05,
      "loss": 0.0013,
      "step": 164780
    },
    {
      "epoch": 5.493,
      "grad_norm": 0.11428326368331909,
      "learning_rate": 1.566875e-05,
      "loss": 0.0019,
      "step": 164790
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.28567540645599365,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0016,
      "step": 164800
    },
    {
      "epoch": 5.493666666666667,
      "grad_norm": 0.20099711418151855,
      "learning_rate": 1.5664583333333336e-05,
      "loss": 0.0013,
      "step": 164810
    },
    {
      "epoch": 5.494,
      "grad_norm": 0.05734258145093918,
      "learning_rate": 1.5662499999999998e-05,
      "loss": 0.0021,
      "step": 164820
    },
    {
      "epoch": 5.4943333333333335,
      "grad_norm": 0.1438186764717102,
      "learning_rate": 1.5660416666666667e-05,
      "loss": 0.0017,
      "step": 164830
    },
    {
      "epoch": 5.494666666666666,
      "grad_norm": 0.11527697741985321,
      "learning_rate": 1.5658333333333332e-05,
      "loss": 0.002,
      "step": 164840
    },
    {
      "epoch": 5.495,
      "grad_norm": 0.3777313828468323,
      "learning_rate": 1.565625e-05,
      "loss": 0.0021,
      "step": 164850
    },
    {
      "epoch": 5.495333333333333,
      "grad_norm": 0.472660094499588,
      "learning_rate": 1.5654166666666667e-05,
      "loss": 0.0014,
      "step": 164860
    },
    {
      "epoch": 5.495666666666667,
      "grad_norm": 0.17115122079849243,
      "learning_rate": 1.5652083333333335e-05,
      "loss": 0.0014,
      "step": 164870
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.5142897963523865,
      "learning_rate": 1.565e-05,
      "loss": 0.002,
      "step": 164880
    },
    {
      "epoch": 5.496333333333333,
      "grad_norm": 0.19962459802627563,
      "learning_rate": 1.5647916666666666e-05,
      "loss": 0.0017,
      "step": 164890
    },
    {
      "epoch": 5.496666666666667,
      "grad_norm": 0.05868799611926079,
      "learning_rate": 1.5645833333333335e-05,
      "loss": 0.0011,
      "step": 164900
    },
    {
      "epoch": 5.497,
      "grad_norm": 0.22899366915225983,
      "learning_rate": 1.564375e-05,
      "loss": 0.0018,
      "step": 164910
    },
    {
      "epoch": 5.497333333333334,
      "grad_norm": 0.291706919670105,
      "learning_rate": 1.564166666666667e-05,
      "loss": 0.0016,
      "step": 164920
    },
    {
      "epoch": 5.4976666666666665,
      "grad_norm": 0.0580267533659935,
      "learning_rate": 1.5639583333333335e-05,
      "loss": 0.0022,
      "step": 164930
    },
    {
      "epoch": 5.498,
      "grad_norm": 0.200355663895607,
      "learning_rate": 1.56375e-05,
      "loss": 0.0012,
      "step": 164940
    },
    {
      "epoch": 5.498333333333333,
      "grad_norm": 0.20035403966903687,
      "learning_rate": 1.5635416666666666e-05,
      "loss": 0.002,
      "step": 164950
    },
    {
      "epoch": 5.498666666666667,
      "grad_norm": 0.22885674238204956,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0019,
      "step": 164960
    },
    {
      "epoch": 5.499,
      "grad_norm": 0.08605848997831345,
      "learning_rate": 1.563125e-05,
      "loss": 0.0015,
      "step": 164970
    },
    {
      "epoch": 5.499333333333333,
      "grad_norm": 0.3146182894706726,
      "learning_rate": 1.5629166666666666e-05,
      "loss": 0.0018,
      "step": 164980
    },
    {
      "epoch": 5.499666666666666,
      "grad_norm": 0.1430937647819519,
      "learning_rate": 1.5627083333333335e-05,
      "loss": 0.0024,
      "step": 164990
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.034365713596343994,
      "learning_rate": 1.5625e-05,
      "loss": 0.0024,
      "step": 165000
    },
    {
      "epoch": 5.500333333333334,
      "grad_norm": 0.35606586933135986,
      "learning_rate": 1.562291666666667e-05,
      "loss": 0.0026,
      "step": 165010
    },
    {
      "epoch": 5.500666666666667,
      "grad_norm": 0.14865291118621826,
      "learning_rate": 1.5620833333333335e-05,
      "loss": 0.002,
      "step": 165020
    },
    {
      "epoch": 5.501,
      "grad_norm": 0.08585743606090546,
      "learning_rate": 1.561875e-05,
      "loss": 0.0016,
      "step": 165030
    },
    {
      "epoch": 5.501333333333333,
      "grad_norm": 0.22893787920475006,
      "learning_rate": 1.561666666666667e-05,
      "loss": 0.002,
      "step": 165040
    },
    {
      "epoch": 5.501666666666667,
      "grad_norm": 0.11431188881397247,
      "learning_rate": 1.5614583333333335e-05,
      "loss": 0.0022,
      "step": 165050
    },
    {
      "epoch": 5.502,
      "grad_norm": 0.14332357048988342,
      "learning_rate": 1.5612500000000003e-05,
      "loss": 0.002,
      "step": 165060
    },
    {
      "epoch": 5.5023333333333335,
      "grad_norm": 0.1715249866247177,
      "learning_rate": 1.5610416666666666e-05,
      "loss": 0.002,
      "step": 165070
    },
    {
      "epoch": 5.502666666666666,
      "grad_norm": 0.2005520910024643,
      "learning_rate": 1.5608333333333334e-05,
      "loss": 0.0019,
      "step": 165080
    },
    {
      "epoch": 5.503,
      "grad_norm": 0.14311762154102325,
      "learning_rate": 1.560625e-05,
      "loss": 0.0012,
      "step": 165090
    },
    {
      "epoch": 5.503333333333333,
      "grad_norm": 0.05740706995129585,
      "learning_rate": 1.5604166666666665e-05,
      "loss": 0.0015,
      "step": 165100
    },
    {
      "epoch": 5.503666666666667,
      "grad_norm": 0.08685291558504105,
      "learning_rate": 1.5602083333333334e-05,
      "loss": 0.0014,
      "step": 165110
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.08628465980291367,
      "learning_rate": 1.56e-05,
      "loss": 0.0013,
      "step": 165120
    },
    {
      "epoch": 5.504333333333333,
      "grad_norm": 0.11434615403413773,
      "learning_rate": 1.559791666666667e-05,
      "loss": 0.0017,
      "step": 165130
    },
    {
      "epoch": 5.504666666666667,
      "grad_norm": 0.20015668869018555,
      "learning_rate": 1.5595833333333334e-05,
      "loss": 0.0015,
      "step": 165140
    },
    {
      "epoch": 5.505,
      "grad_norm": 0.05756787955760956,
      "learning_rate": 1.5593750000000003e-05,
      "loss": 0.0025,
      "step": 165150
    },
    {
      "epoch": 5.505333333333334,
      "grad_norm": 0.03154532611370087,
      "learning_rate": 1.559166666666667e-05,
      "loss": 0.0017,
      "step": 165160
    },
    {
      "epoch": 5.5056666666666665,
      "grad_norm": 0.1364724189043045,
      "learning_rate": 1.5589583333333334e-05,
      "loss": 0.0024,
      "step": 165170
    },
    {
      "epoch": 5.506,
      "grad_norm": 0.6863999366760254,
      "learning_rate": 1.5587500000000003e-05,
      "loss": 0.0021,
      "step": 165180
    },
    {
      "epoch": 5.506333333333333,
      "grad_norm": 0.059135451912879944,
      "learning_rate": 1.5585416666666665e-05,
      "loss": 0.0017,
      "step": 165190
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 0.08672430366277695,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.0018,
      "step": 165200
    },
    {
      "epoch": 5.507,
      "grad_norm": 0.1427288055419922,
      "learning_rate": 1.558125e-05,
      "loss": 0.0032,
      "step": 165210
    },
    {
      "epoch": 5.507333333333333,
      "grad_norm": 0.034348271787166595,
      "learning_rate": 1.5579166666666668e-05,
      "loss": 0.0014,
      "step": 165220
    },
    {
      "epoch": 5.507666666666667,
      "grad_norm": 0.05759340897202492,
      "learning_rate": 1.5577083333333334e-05,
      "loss": 0.0016,
      "step": 165230
    },
    {
      "epoch": 5.508,
      "grad_norm": 0.3713679313659668,
      "learning_rate": 1.5575e-05,
      "loss": 0.0016,
      "step": 165240
    },
    {
      "epoch": 5.508333333333333,
      "grad_norm": 0.11434134840965271,
      "learning_rate": 1.5572916666666668e-05,
      "loss": 0.0016,
      "step": 165250
    },
    {
      "epoch": 5.508666666666667,
      "grad_norm": 0.11446598172187805,
      "learning_rate": 1.5570833333333333e-05,
      "loss": 0.003,
      "step": 165260
    },
    {
      "epoch": 5.509,
      "grad_norm": 0.6003467440605164,
      "learning_rate": 1.5568750000000002e-05,
      "loss": 0.0016,
      "step": 165270
    },
    {
      "epoch": 5.509333333333333,
      "grad_norm": 0.5038488507270813,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0018,
      "step": 165280
    },
    {
      "epoch": 5.509666666666667,
      "grad_norm": 0.25729838013648987,
      "learning_rate": 1.5564583333333337e-05,
      "loss": 0.0018,
      "step": 165290
    },
    {
      "epoch": 5.51,
      "grad_norm": 0.3141571581363678,
      "learning_rate": 1.5562500000000002e-05,
      "loss": 0.0021,
      "step": 165300
    },
    {
      "epoch": 5.5103333333333335,
      "grad_norm": 0.17227105796337128,
      "learning_rate": 1.5560416666666668e-05,
      "loss": 0.0025,
      "step": 165310
    },
    {
      "epoch": 5.510666666666666,
      "grad_norm": 0.5426889657974243,
      "learning_rate": 1.5558333333333333e-05,
      "loss": 0.0023,
      "step": 165320
    },
    {
      "epoch": 5.511,
      "grad_norm": 0.24170838296413422,
      "learning_rate": 1.555625e-05,
      "loss": 0.0019,
      "step": 165330
    },
    {
      "epoch": 5.511333333333333,
      "grad_norm": 0.17156128585338593,
      "learning_rate": 1.5554166666666668e-05,
      "loss": 0.0014,
      "step": 165340
    },
    {
      "epoch": 5.511666666666667,
      "grad_norm": 0.007594318129122257,
      "learning_rate": 1.5552083333333333e-05,
      "loss": 0.0018,
      "step": 165350
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.22845478355884552,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 0.0016,
      "step": 165360
    },
    {
      "epoch": 5.512333333333333,
      "grad_norm": 0.11451228708028793,
      "learning_rate": 1.5547916666666667e-05,
      "loss": 0.0017,
      "step": 165370
    },
    {
      "epoch": 5.512666666666667,
      "grad_norm": 0.009258381091058254,
      "learning_rate": 1.5545833333333333e-05,
      "loss": 0.0018,
      "step": 165380
    },
    {
      "epoch": 5.513,
      "grad_norm": 0.3716091811656952,
      "learning_rate": 1.5543750000000002e-05,
      "loss": 0.0019,
      "step": 165390
    },
    {
      "epoch": 5.513333333333334,
      "grad_norm": 0.2567152678966522,
      "learning_rate": 1.5541666666666667e-05,
      "loss": 0.0015,
      "step": 165400
    },
    {
      "epoch": 5.5136666666666665,
      "grad_norm": 0.11464165896177292,
      "learning_rate": 1.5539583333333336e-05,
      "loss": 0.0016,
      "step": 165410
    },
    {
      "epoch": 5.514,
      "grad_norm": 0.2654073238372803,
      "learning_rate": 1.55375e-05,
      "loss": 0.0013,
      "step": 165420
    },
    {
      "epoch": 5.514333333333333,
      "grad_norm": 0.05777278542518616,
      "learning_rate": 1.5535416666666667e-05,
      "loss": 0.0018,
      "step": 165430
    },
    {
      "epoch": 5.514666666666667,
      "grad_norm": 0.4869098961353302,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0021,
      "step": 165440
    },
    {
      "epoch": 5.515,
      "grad_norm": 0.06239785626530647,
      "learning_rate": 1.553125e-05,
      "loss": 0.0023,
      "step": 165450
    },
    {
      "epoch": 5.515333333333333,
      "grad_norm": 0.043813556432724,
      "learning_rate": 1.5529166666666667e-05,
      "loss": 0.0019,
      "step": 165460
    },
    {
      "epoch": 5.515666666666666,
      "grad_norm": 0.14313718676567078,
      "learning_rate": 1.5527083333333332e-05,
      "loss": 0.0019,
      "step": 165470
    },
    {
      "epoch": 5.516,
      "grad_norm": 0.3712216913700104,
      "learning_rate": 1.5525e-05,
      "loss": 0.0024,
      "step": 165480
    },
    {
      "epoch": 5.516333333333334,
      "grad_norm": 0.11442691832780838,
      "learning_rate": 1.5522916666666667e-05,
      "loss": 0.0015,
      "step": 165490
    },
    {
      "epoch": 5.516666666666667,
      "grad_norm": 0.20003923773765564,
      "learning_rate": 1.5520833333333336e-05,
      "loss": 0.0018,
      "step": 165500
    },
    {
      "epoch": 5.517,
      "grad_norm": 0.029384197667241096,
      "learning_rate": 1.551875e-05,
      "loss": 0.0017,
      "step": 165510
    },
    {
      "epoch": 5.517333333333333,
      "grad_norm": 0.2567816972732544,
      "learning_rate": 1.5516666666666667e-05,
      "loss": 0.0028,
      "step": 165520
    },
    {
      "epoch": 5.517666666666667,
      "grad_norm": 0.03214631229639053,
      "learning_rate": 1.5514583333333335e-05,
      "loss": 0.0016,
      "step": 165530
    },
    {
      "epoch": 5.518,
      "grad_norm": 0.057331670075654984,
      "learning_rate": 1.55125e-05,
      "loss": 0.002,
      "step": 165540
    },
    {
      "epoch": 5.5183333333333335,
      "grad_norm": 0.14283767342567444,
      "learning_rate": 1.5510416666666666e-05,
      "loss": 0.0021,
      "step": 165550
    },
    {
      "epoch": 5.518666666666666,
      "grad_norm": 0.059943340718746185,
      "learning_rate": 1.5508333333333332e-05,
      "loss": 0.0019,
      "step": 165560
    },
    {
      "epoch": 5.519,
      "grad_norm": 0.0074526905082166195,
      "learning_rate": 1.550625e-05,
      "loss": 0.0028,
      "step": 165570
    },
    {
      "epoch": 5.519333333333333,
      "grad_norm": 0.1742551177740097,
      "learning_rate": 1.5504166666666666e-05,
      "loss": 0.0015,
      "step": 165580
    },
    {
      "epoch": 5.519666666666667,
      "grad_norm": 0.1470215916633606,
      "learning_rate": 1.5502083333333335e-05,
      "loss": 0.0028,
      "step": 165590
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.11461615562438965,
      "learning_rate": 1.55e-05,
      "loss": 0.0015,
      "step": 165600
    },
    {
      "epoch": 5.520333333333333,
      "grad_norm": 0.11481646448373795,
      "learning_rate": 1.5497916666666666e-05,
      "loss": 0.0018,
      "step": 165610
    },
    {
      "epoch": 5.520666666666667,
      "grad_norm": 0.2851196825504303,
      "learning_rate": 1.5495833333333335e-05,
      "loss": 0.0013,
      "step": 165620
    },
    {
      "epoch": 5.521,
      "grad_norm": 0.09796377271413803,
      "learning_rate": 1.549375e-05,
      "loss": 0.0022,
      "step": 165630
    },
    {
      "epoch": 5.521333333333334,
      "grad_norm": 0.22888636589050293,
      "learning_rate": 1.549166666666667e-05,
      "loss": 0.0012,
      "step": 165640
    },
    {
      "epoch": 5.5216666666666665,
      "grad_norm": 0.28580063581466675,
      "learning_rate": 1.5489583333333335e-05,
      "loss": 0.0015,
      "step": 165650
    },
    {
      "epoch": 5.522,
      "grad_norm": 0.011843960732221603,
      "learning_rate": 1.54875e-05,
      "loss": 0.0022,
      "step": 165660
    },
    {
      "epoch": 5.522333333333333,
      "grad_norm": 0.05761164799332619,
      "learning_rate": 1.5485416666666666e-05,
      "loss": 0.002,
      "step": 165670
    },
    {
      "epoch": 5.522666666666667,
      "grad_norm": 0.4717189371585846,
      "learning_rate": 1.548333333333333e-05,
      "loss": 0.0022,
      "step": 165680
    },
    {
      "epoch": 5.523,
      "grad_norm": 0.28627198934555054,
      "learning_rate": 1.548125e-05,
      "loss": 0.0021,
      "step": 165690
    },
    {
      "epoch": 5.523333333333333,
      "grad_norm": 0.008454076014459133,
      "learning_rate": 1.5479166666666666e-05,
      "loss": 0.0016,
      "step": 165700
    },
    {
      "epoch": 5.523666666666666,
      "grad_norm": 0.08990649878978729,
      "learning_rate": 1.5477083333333335e-05,
      "loss": 0.0017,
      "step": 165710
    },
    {
      "epoch": 5.524,
      "grad_norm": 0.007848645560443401,
      "learning_rate": 1.5475e-05,
      "loss": 0.0026,
      "step": 165720
    },
    {
      "epoch": 5.524333333333333,
      "grad_norm": 0.22932809591293335,
      "learning_rate": 1.547291666666667e-05,
      "loss": 0.0033,
      "step": 165730
    },
    {
      "epoch": 5.524666666666667,
      "grad_norm": 0.14297984540462494,
      "learning_rate": 1.5470833333333334e-05,
      "loss": 0.0018,
      "step": 165740
    },
    {
      "epoch": 5.525,
      "grad_norm": 0.05775582045316696,
      "learning_rate": 1.546875e-05,
      "loss": 0.0024,
      "step": 165750
    },
    {
      "epoch": 5.525333333333333,
      "grad_norm": 0.011265152134001255,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0021,
      "step": 165760
    },
    {
      "epoch": 5.525666666666667,
      "grad_norm": 0.37193140387535095,
      "learning_rate": 1.5464583333333334e-05,
      "loss": 0.002,
      "step": 165770
    },
    {
      "epoch": 5.526,
      "grad_norm": 0.14329518377780914,
      "learning_rate": 1.5462500000000003e-05,
      "loss": 0.0017,
      "step": 165780
    },
    {
      "epoch": 5.5263333333333335,
      "grad_norm": 0.1161046251654625,
      "learning_rate": 1.5460416666666665e-05,
      "loss": 0.0019,
      "step": 165790
    },
    {
      "epoch": 5.526666666666666,
      "grad_norm": 0.0865313932299614,
      "learning_rate": 1.5458333333333334e-05,
      "loss": 0.0014,
      "step": 165800
    },
    {
      "epoch": 5.527,
      "grad_norm": 0.3713952600955963,
      "learning_rate": 1.545625e-05,
      "loss": 0.0029,
      "step": 165810
    },
    {
      "epoch": 5.527333333333333,
      "grad_norm": 0.030282679945230484,
      "learning_rate": 1.545416666666667e-05,
      "loss": 0.0036,
      "step": 165820
    },
    {
      "epoch": 5.527666666666667,
      "grad_norm": 0.014425192028284073,
      "learning_rate": 1.5452083333333334e-05,
      "loss": 0.0023,
      "step": 165830
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.20027662813663483,
      "learning_rate": 1.545e-05,
      "loss": 0.0014,
      "step": 165840
    },
    {
      "epoch": 5.528333333333333,
      "grad_norm": 0.05748741701245308,
      "learning_rate": 1.544791666666667e-05,
      "loss": 0.0014,
      "step": 165850
    },
    {
      "epoch": 5.528666666666666,
      "grad_norm": 0.1442989557981491,
      "learning_rate": 1.5445833333333334e-05,
      "loss": 0.0026,
      "step": 165860
    },
    {
      "epoch": 5.529,
      "grad_norm": 0.14414043724536896,
      "learning_rate": 1.5443750000000003e-05,
      "loss": 0.0016,
      "step": 165870
    },
    {
      "epoch": 5.529333333333334,
      "grad_norm": 0.11404497176408768,
      "learning_rate": 1.5441666666666668e-05,
      "loss": 0.0021,
      "step": 165880
    },
    {
      "epoch": 5.5296666666666665,
      "grad_norm": 0.17232678830623627,
      "learning_rate": 1.5439583333333334e-05,
      "loss": 0.0025,
      "step": 165890
    },
    {
      "epoch": 5.53,
      "grad_norm": 0.17180223762989044,
      "learning_rate": 1.5437500000000003e-05,
      "loss": 0.0017,
      "step": 165900
    },
    {
      "epoch": 5.530333333333333,
      "grad_norm": 0.1426272839307785,
      "learning_rate": 1.5435416666666665e-05,
      "loss": 0.0025,
      "step": 165910
    },
    {
      "epoch": 5.530666666666667,
      "grad_norm": 0.05714068561792374,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.0017,
      "step": 165920
    },
    {
      "epoch": 5.531,
      "grad_norm": 0.40025594830513,
      "learning_rate": 1.543125e-05,
      "loss": 0.0021,
      "step": 165930
    },
    {
      "epoch": 5.531333333333333,
      "grad_norm": 0.143221914768219,
      "learning_rate": 1.5429166666666668e-05,
      "loss": 0.0018,
      "step": 165940
    },
    {
      "epoch": 5.531666666666666,
      "grad_norm": 0.19963456690311432,
      "learning_rate": 1.5427083333333333e-05,
      "loss": 0.0013,
      "step": 165950
    },
    {
      "epoch": 5.532,
      "grad_norm": 0.029144776985049248,
      "learning_rate": 1.5425000000000002e-05,
      "loss": 0.0012,
      "step": 165960
    },
    {
      "epoch": 5.532333333333334,
      "grad_norm": 0.1441374272108078,
      "learning_rate": 1.5422916666666668e-05,
      "loss": 0.0024,
      "step": 165970
    },
    {
      "epoch": 5.532666666666667,
      "grad_norm": 0.3147040903568268,
      "learning_rate": 1.5420833333333333e-05,
      "loss": 0.0014,
      "step": 165980
    },
    {
      "epoch": 5.533,
      "grad_norm": 0.40378338098526,
      "learning_rate": 1.5418750000000002e-05,
      "loss": 0.0022,
      "step": 165990
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.2858738601207733,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.0015,
      "step": 166000
    },
    {
      "epoch": 5.533666666666667,
      "grad_norm": 0.08575381338596344,
      "learning_rate": 1.5414583333333336e-05,
      "loss": 0.0019,
      "step": 166010
    },
    {
      "epoch": 5.534,
      "grad_norm": 0.0050020404160022736,
      "learning_rate": 1.5412500000000002e-05,
      "loss": 0.0017,
      "step": 166020
    },
    {
      "epoch": 5.5343333333333335,
      "grad_norm": 0.09945350140333176,
      "learning_rate": 1.5410416666666667e-05,
      "loss": 0.0017,
      "step": 166030
    },
    {
      "epoch": 5.534666666666666,
      "grad_norm": 0.3428545296192169,
      "learning_rate": 1.5408333333333333e-05,
      "loss": 0.0022,
      "step": 166040
    },
    {
      "epoch": 5.535,
      "grad_norm": 0.17131935060024261,
      "learning_rate": 1.540625e-05,
      "loss": 0.0013,
      "step": 166050
    },
    {
      "epoch": 5.535333333333333,
      "grad_norm": 0.1926274448633194,
      "learning_rate": 1.5404166666666667e-05,
      "loss": 0.0024,
      "step": 166060
    },
    {
      "epoch": 5.535666666666667,
      "grad_norm": 0.11509988456964493,
      "learning_rate": 1.5402083333333333e-05,
      "loss": 0.002,
      "step": 166070
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.2702026963233948,
      "learning_rate": 1.54e-05,
      "loss": 0.0021,
      "step": 166080
    },
    {
      "epoch": 5.536333333333333,
      "grad_norm": 0.05802929773926735,
      "learning_rate": 1.5397916666666667e-05,
      "loss": 0.0024,
      "step": 166090
    },
    {
      "epoch": 5.536666666666667,
      "grad_norm": 0.4286072850227356,
      "learning_rate": 1.5395833333333336e-05,
      "loss": 0.0018,
      "step": 166100
    },
    {
      "epoch": 5.537,
      "grad_norm": 0.058399856090545654,
      "learning_rate": 1.539375e-05,
      "loss": 0.0021,
      "step": 166110
    },
    {
      "epoch": 5.537333333333334,
      "grad_norm": 0.45422473549842834,
      "learning_rate": 1.5391666666666667e-05,
      "loss": 0.002,
      "step": 166120
    },
    {
      "epoch": 5.5376666666666665,
      "grad_norm": 0.3126124441623688,
      "learning_rate": 1.5389583333333336e-05,
      "loss": 0.002,
      "step": 166130
    },
    {
      "epoch": 5.538,
      "grad_norm": 0.5427055954933167,
      "learning_rate": 1.53875e-05,
      "loss": 0.0014,
      "step": 166140
    },
    {
      "epoch": 5.538333333333333,
      "grad_norm": 0.09109169244766235,
      "learning_rate": 1.5385416666666667e-05,
      "loss": 0.0017,
      "step": 166150
    },
    {
      "epoch": 5.538666666666667,
      "grad_norm": 0.14289025962352753,
      "learning_rate": 1.5383333333333332e-05,
      "loss": 0.0021,
      "step": 166160
    },
    {
      "epoch": 5.539,
      "grad_norm": 0.11473377048969269,
      "learning_rate": 1.538125e-05,
      "loss": 0.0014,
      "step": 166170
    },
    {
      "epoch": 5.539333333333333,
      "grad_norm": 0.3262621760368347,
      "learning_rate": 1.5379166666666667e-05,
      "loss": 0.0021,
      "step": 166180
    },
    {
      "epoch": 5.539666666666666,
      "grad_norm": 0.3141360878944397,
      "learning_rate": 1.5377083333333332e-05,
      "loss": 0.0017,
      "step": 166190
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.28572142124176025,
      "learning_rate": 1.5375e-05,
      "loss": 0.0013,
      "step": 166200
    },
    {
      "epoch": 5.540333333333333,
      "grad_norm": 0.05790349096059799,
      "learning_rate": 1.5372916666666667e-05,
      "loss": 0.0015,
      "step": 166210
    },
    {
      "epoch": 5.540666666666667,
      "grad_norm": 0.17158924043178558,
      "learning_rate": 1.5370833333333335e-05,
      "loss": 0.0016,
      "step": 166220
    },
    {
      "epoch": 5.541,
      "grad_norm": 0.05842215567827225,
      "learning_rate": 1.536875e-05,
      "loss": 0.0023,
      "step": 166230
    },
    {
      "epoch": 5.541333333333333,
      "grad_norm": 0.3426539897918701,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0021,
      "step": 166240
    },
    {
      "epoch": 5.541666666666667,
      "grad_norm": 0.30476057529449463,
      "learning_rate": 1.5364583333333335e-05,
      "loss": 0.0024,
      "step": 166250
    },
    {
      "epoch": 5.542,
      "grad_norm": 0.058131296187639236,
      "learning_rate": 1.53625e-05,
      "loss": 0.0014,
      "step": 166260
    },
    {
      "epoch": 5.542333333333334,
      "grad_norm": 0.11442980170249939,
      "learning_rate": 1.536041666666667e-05,
      "loss": 0.0012,
      "step": 166270
    },
    {
      "epoch": 5.542666666666666,
      "grad_norm": 0.25738051533699036,
      "learning_rate": 1.5358333333333332e-05,
      "loss": 0.0026,
      "step": 166280
    },
    {
      "epoch": 5.543,
      "grad_norm": 0.25704580545425415,
      "learning_rate": 1.535625e-05,
      "loss": 0.0015,
      "step": 166290
    },
    {
      "epoch": 5.543333333333333,
      "grad_norm": 0.15936952829360962,
      "learning_rate": 1.5354166666666666e-05,
      "loss": 0.0018,
      "step": 166300
    },
    {
      "epoch": 5.543666666666667,
      "grad_norm": 0.34310275316238403,
      "learning_rate": 1.5352083333333335e-05,
      "loss": 0.002,
      "step": 166310
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.2283128947019577,
      "learning_rate": 1.535e-05,
      "loss": 0.0024,
      "step": 166320
    },
    {
      "epoch": 5.544333333333333,
      "grad_norm": 0.17140288650989532,
      "learning_rate": 1.5347916666666666e-05,
      "loss": 0.0015,
      "step": 166330
    },
    {
      "epoch": 5.544666666666666,
      "grad_norm": 0.2570439577102661,
      "learning_rate": 1.5345833333333335e-05,
      "loss": 0.0019,
      "step": 166340
    },
    {
      "epoch": 5.545,
      "grad_norm": 0.14421860873699188,
      "learning_rate": 1.534375e-05,
      "loss": 0.0015,
      "step": 166350
    },
    {
      "epoch": 5.545333333333334,
      "grad_norm": 0.11446753144264221,
      "learning_rate": 1.534166666666667e-05,
      "loss": 0.0015,
      "step": 166360
    },
    {
      "epoch": 5.5456666666666665,
      "grad_norm": 0.08633191138505936,
      "learning_rate": 1.5339583333333335e-05,
      "loss": 0.0018,
      "step": 166370
    },
    {
      "epoch": 5.546,
      "grad_norm": 0.26150277256965637,
      "learning_rate": 1.5337500000000004e-05,
      "loss": 0.0024,
      "step": 166380
    },
    {
      "epoch": 5.546333333333333,
      "grad_norm": 0.08659873902797699,
      "learning_rate": 1.533541666666667e-05,
      "loss": 0.0018,
      "step": 166390
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.11412902176380157,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0015,
      "step": 166400
    },
    {
      "epoch": 5.547,
      "grad_norm": 0.029960842803120613,
      "learning_rate": 1.533125e-05,
      "loss": 0.0015,
      "step": 166410
    },
    {
      "epoch": 5.5473333333333334,
      "grad_norm": 0.11570773273706436,
      "learning_rate": 1.5329166666666665e-05,
      "loss": 0.002,
      "step": 166420
    },
    {
      "epoch": 5.547666666666666,
      "grad_norm": 0.057624053210020065,
      "learning_rate": 1.5327083333333334e-05,
      "loss": 0.0016,
      "step": 166430
    },
    {
      "epoch": 5.548,
      "grad_norm": 0.3436434268951416,
      "learning_rate": 1.5325e-05,
      "loss": 0.0021,
      "step": 166440
    },
    {
      "epoch": 5.548333333333334,
      "grad_norm": 0.171998530626297,
      "learning_rate": 1.532291666666667e-05,
      "loss": 0.002,
      "step": 166450
    },
    {
      "epoch": 5.548666666666667,
      "grad_norm": 0.20144647359848022,
      "learning_rate": 1.5320833333333334e-05,
      "loss": 0.0016,
      "step": 166460
    },
    {
      "epoch": 5.549,
      "grad_norm": 0.4267071783542633,
      "learning_rate": 1.531875e-05,
      "loss": 0.0025,
      "step": 166470
    },
    {
      "epoch": 5.549333333333333,
      "grad_norm": 0.05944451689720154,
      "learning_rate": 1.531666666666667e-05,
      "loss": 0.0018,
      "step": 166480
    },
    {
      "epoch": 5.549666666666667,
      "grad_norm": 0.1182025596499443,
      "learning_rate": 1.5314583333333334e-05,
      "loss": 0.0021,
      "step": 166490
    },
    {
      "epoch": 5.55,
      "grad_norm": 0.04556410759687424,
      "learning_rate": 1.5312500000000003e-05,
      "loss": 0.0015,
      "step": 166500
    },
    {
      "epoch": 5.550333333333334,
      "grad_norm": 0.2858641743659973,
      "learning_rate": 1.531041666666667e-05,
      "loss": 0.0016,
      "step": 166510
    },
    {
      "epoch": 5.550666666666666,
      "grad_norm": 0.2861054837703705,
      "learning_rate": 1.5308333333333334e-05,
      "loss": 0.0026,
      "step": 166520
    },
    {
      "epoch": 5.551,
      "grad_norm": 0.031032972037792206,
      "learning_rate": 1.530625e-05,
      "loss": 0.0017,
      "step": 166530
    },
    {
      "epoch": 5.551333333333333,
      "grad_norm": 0.2255229949951172,
      "learning_rate": 1.5304166666666668e-05,
      "loss": 0.0019,
      "step": 166540
    },
    {
      "epoch": 5.551666666666667,
      "grad_norm": 0.2283189594745636,
      "learning_rate": 1.5302083333333334e-05,
      "loss": 0.002,
      "step": 166550
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.17150644958019257,
      "learning_rate": 1.53e-05,
      "loss": 0.0022,
      "step": 166560
    },
    {
      "epoch": 5.552333333333333,
      "grad_norm": 0.17156386375427246,
      "learning_rate": 1.5297916666666668e-05,
      "loss": 0.0019,
      "step": 166570
    },
    {
      "epoch": 5.552666666666667,
      "grad_norm": 0.05777231976389885,
      "learning_rate": 1.5295833333333334e-05,
      "loss": 0.0018,
      "step": 166580
    },
    {
      "epoch": 5.553,
      "grad_norm": 0.1429244577884674,
      "learning_rate": 1.5293750000000002e-05,
      "loss": 0.002,
      "step": 166590
    },
    {
      "epoch": 5.553333333333334,
      "grad_norm": 0.5370088815689087,
      "learning_rate": 1.5291666666666668e-05,
      "loss": 0.0018,
      "step": 166600
    },
    {
      "epoch": 5.5536666666666665,
      "grad_norm": 0.08599531650543213,
      "learning_rate": 1.5289583333333333e-05,
      "loss": 0.0013,
      "step": 166610
    },
    {
      "epoch": 5.554,
      "grad_norm": 0.05970541015267372,
      "learning_rate": 1.5287500000000002e-05,
      "loss": 0.0019,
      "step": 166620
    },
    {
      "epoch": 5.554333333333333,
      "grad_norm": 0.14645273983478546,
      "learning_rate": 1.5285416666666668e-05,
      "loss": 0.0022,
      "step": 166630
    },
    {
      "epoch": 5.554666666666667,
      "grad_norm": 0.1718188226222992,
      "learning_rate": 1.5283333333333333e-05,
      "loss": 0.0018,
      "step": 166640
    },
    {
      "epoch": 5.555,
      "grad_norm": 0.08569791167974472,
      "learning_rate": 1.528125e-05,
      "loss": 0.0024,
      "step": 166650
    },
    {
      "epoch": 5.5553333333333335,
      "grad_norm": 0.1714824140071869,
      "learning_rate": 1.5279166666666668e-05,
      "loss": 0.0015,
      "step": 166660
    },
    {
      "epoch": 5.555666666666666,
      "grad_norm": 0.20046326518058777,
      "learning_rate": 1.5277083333333333e-05,
      "loss": 0.0022,
      "step": 166670
    },
    {
      "epoch": 5.556,
      "grad_norm": 0.1999121755361557,
      "learning_rate": 1.5275000000000002e-05,
      "loss": 0.0014,
      "step": 166680
    },
    {
      "epoch": 5.556333333333333,
      "grad_norm": 0.2008548378944397,
      "learning_rate": 1.5272916666666667e-05,
      "loss": 0.0021,
      "step": 166690
    },
    {
      "epoch": 5.556666666666667,
      "grad_norm": 0.3427998125553131,
      "learning_rate": 1.5270833333333333e-05,
      "loss": 0.0016,
      "step": 166700
    },
    {
      "epoch": 5.557,
      "grad_norm": 0.029071887955069542,
      "learning_rate": 1.5268750000000002e-05,
      "loss": 0.0018,
      "step": 166710
    },
    {
      "epoch": 5.557333333333333,
      "grad_norm": 0.22881439328193665,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0019,
      "step": 166720
    },
    {
      "epoch": 5.557666666666667,
      "grad_norm": 0.08654942363500595,
      "learning_rate": 1.5264583333333336e-05,
      "loss": 0.0018,
      "step": 166730
    },
    {
      "epoch": 5.558,
      "grad_norm": 0.17222686111927032,
      "learning_rate": 1.52625e-05,
      "loss": 0.0015,
      "step": 166740
    },
    {
      "epoch": 5.558333333333334,
      "grad_norm": 0.11253009736537933,
      "learning_rate": 1.5260416666666667e-05,
      "loss": 0.0017,
      "step": 166750
    },
    {
      "epoch": 5.558666666666666,
      "grad_norm": 0.17167243361473083,
      "learning_rate": 1.5258333333333333e-05,
      "loss": 0.0017,
      "step": 166760
    },
    {
      "epoch": 5.559,
      "grad_norm": 0.44466346502304077,
      "learning_rate": 1.525625e-05,
      "loss": 0.0019,
      "step": 166770
    },
    {
      "epoch": 5.559333333333333,
      "grad_norm": 0.19949859380722046,
      "learning_rate": 1.5254166666666667e-05,
      "loss": 0.0019,
      "step": 166780
    },
    {
      "epoch": 5.559666666666667,
      "grad_norm": 0.28577524423599243,
      "learning_rate": 1.5252083333333334e-05,
      "loss": 0.0017,
      "step": 166790
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.31469711661338806,
      "learning_rate": 1.525e-05,
      "loss": 0.0021,
      "step": 166800
    },
    {
      "epoch": 5.560333333333333,
      "grad_norm": 0.11429603397846222,
      "learning_rate": 1.5247916666666667e-05,
      "loss": 0.0013,
      "step": 166810
    },
    {
      "epoch": 5.560666666666666,
      "grad_norm": 0.010841327719390392,
      "learning_rate": 1.5245833333333334e-05,
      "loss": 0.0016,
      "step": 166820
    },
    {
      "epoch": 5.561,
      "grad_norm": 0.05737539380788803,
      "learning_rate": 1.5243750000000001e-05,
      "loss": 0.0016,
      "step": 166830
    },
    {
      "epoch": 5.561333333333334,
      "grad_norm": 0.031678806990385056,
      "learning_rate": 1.5241666666666668e-05,
      "loss": 0.0017,
      "step": 166840
    },
    {
      "epoch": 5.5616666666666665,
      "grad_norm": 0.05781467258930206,
      "learning_rate": 1.5239583333333336e-05,
      "loss": 0.0019,
      "step": 166850
    },
    {
      "epoch": 5.562,
      "grad_norm": 0.006893620826303959,
      "learning_rate": 1.5237500000000001e-05,
      "loss": 0.0017,
      "step": 166860
    },
    {
      "epoch": 5.562333333333333,
      "grad_norm": 0.05792706459760666,
      "learning_rate": 1.5235416666666668e-05,
      "loss": 0.0021,
      "step": 166870
    },
    {
      "epoch": 5.562666666666667,
      "grad_norm": 0.058000482618808746,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0028,
      "step": 166880
    },
    {
      "epoch": 5.563,
      "grad_norm": 0.057859499007463455,
      "learning_rate": 1.523125e-05,
      "loss": 0.0012,
      "step": 166890
    },
    {
      "epoch": 5.5633333333333335,
      "grad_norm": 0.4268375635147095,
      "learning_rate": 1.5229166666666666e-05,
      "loss": 0.003,
      "step": 166900
    },
    {
      "epoch": 5.563666666666666,
      "grad_norm": 0.17266908288002014,
      "learning_rate": 1.5227083333333334e-05,
      "loss": 0.0027,
      "step": 166910
    },
    {
      "epoch": 5.564,
      "grad_norm": 0.22874173521995544,
      "learning_rate": 1.5225e-05,
      "loss": 0.0018,
      "step": 166920
    },
    {
      "epoch": 5.564333333333334,
      "grad_norm": 0.09838440269231796,
      "learning_rate": 1.5222916666666668e-05,
      "loss": 0.0023,
      "step": 166930
    },
    {
      "epoch": 5.564666666666667,
      "grad_norm": 0.11426752060651779,
      "learning_rate": 1.5220833333333333e-05,
      "loss": 0.0018,
      "step": 166940
    },
    {
      "epoch": 5.5649999999999995,
      "grad_norm": 0.2290060818195343,
      "learning_rate": 1.521875e-05,
      "loss": 0.0024,
      "step": 166950
    },
    {
      "epoch": 5.565333333333333,
      "grad_norm": 0.08558440953493118,
      "learning_rate": 1.5216666666666668e-05,
      "loss": 0.0017,
      "step": 166960
    },
    {
      "epoch": 5.565666666666667,
      "grad_norm": 0.2574613094329834,
      "learning_rate": 1.5214583333333335e-05,
      "loss": 0.0018,
      "step": 166970
    },
    {
      "epoch": 5.566,
      "grad_norm": 0.14254817366600037,
      "learning_rate": 1.5212500000000002e-05,
      "loss": 0.0015,
      "step": 166980
    },
    {
      "epoch": 5.566333333333334,
      "grad_norm": 0.34312382340431213,
      "learning_rate": 1.521041666666667e-05,
      "loss": 0.0019,
      "step": 166990
    },
    {
      "epoch": 5.566666666666666,
      "grad_norm": 0.3429517447948456,
      "learning_rate": 1.5208333333333333e-05,
      "loss": 0.0018,
      "step": 167000
    },
    {
      "epoch": 5.567,
      "grad_norm": 0.08616498857736588,
      "learning_rate": 1.520625e-05,
      "loss": 0.0016,
      "step": 167010
    },
    {
      "epoch": 5.567333333333333,
      "grad_norm": 0.1719471663236618,
      "learning_rate": 1.5204166666666666e-05,
      "loss": 0.0015,
      "step": 167020
    },
    {
      "epoch": 5.567666666666667,
      "grad_norm": 0.011788158677518368,
      "learning_rate": 1.5202083333333333e-05,
      "loss": 0.0017,
      "step": 167030
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.11481045931577682,
      "learning_rate": 1.52e-05,
      "loss": 0.0013,
      "step": 167040
    },
    {
      "epoch": 5.568333333333333,
      "grad_norm": 0.22890055179595947,
      "learning_rate": 1.5197916666666667e-05,
      "loss": 0.0014,
      "step": 167050
    },
    {
      "epoch": 5.568666666666667,
      "grad_norm": 0.14300642907619476,
      "learning_rate": 1.5195833333333335e-05,
      "loss": 0.0014,
      "step": 167060
    },
    {
      "epoch": 5.569,
      "grad_norm": 0.08657358586788177,
      "learning_rate": 1.5193750000000002e-05,
      "loss": 0.0023,
      "step": 167070
    },
    {
      "epoch": 5.569333333333334,
      "grad_norm": 0.22847342491149902,
      "learning_rate": 1.5191666666666667e-05,
      "loss": 0.0021,
      "step": 167080
    },
    {
      "epoch": 5.5696666666666665,
      "grad_norm": 0.14305976033210754,
      "learning_rate": 1.5189583333333334e-05,
      "loss": 0.0024,
      "step": 167090
    },
    {
      "epoch": 5.57,
      "grad_norm": 0.11435135453939438,
      "learning_rate": 1.5187500000000002e-05,
      "loss": 0.002,
      "step": 167100
    },
    {
      "epoch": 5.570333333333333,
      "grad_norm": 0.1430109143257141,
      "learning_rate": 1.5185416666666669e-05,
      "loss": 0.0019,
      "step": 167110
    },
    {
      "epoch": 5.570666666666667,
      "grad_norm": 0.22870253026485443,
      "learning_rate": 1.5183333333333333e-05,
      "loss": 0.0024,
      "step": 167120
    },
    {
      "epoch": 5.571,
      "grad_norm": 0.11450628936290741,
      "learning_rate": 1.518125e-05,
      "loss": 0.0016,
      "step": 167130
    },
    {
      "epoch": 5.5713333333333335,
      "grad_norm": 0.05971930921077728,
      "learning_rate": 1.5179166666666667e-05,
      "loss": 0.0014,
      "step": 167140
    },
    {
      "epoch": 5.571666666666666,
      "grad_norm": 0.14286302030086517,
      "learning_rate": 1.5177083333333334e-05,
      "loss": 0.0014,
      "step": 167150
    },
    {
      "epoch": 5.572,
      "grad_norm": 0.006497849710285664,
      "learning_rate": 1.5175e-05,
      "loss": 0.0015,
      "step": 167160
    },
    {
      "epoch": 5.572333333333333,
      "grad_norm": 0.17146439850330353,
      "learning_rate": 1.5172916666666667e-05,
      "loss": 0.0019,
      "step": 167170
    },
    {
      "epoch": 5.572666666666667,
      "grad_norm": 0.05753127858042717,
      "learning_rate": 1.5170833333333334e-05,
      "loss": 0.0015,
      "step": 167180
    },
    {
      "epoch": 5.573,
      "grad_norm": 0.08656302839517593,
      "learning_rate": 1.5168750000000001e-05,
      "loss": 0.0013,
      "step": 167190
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 0.2968943417072296,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0027,
      "step": 167200
    },
    {
      "epoch": 5.573666666666667,
      "grad_norm": 0.08612317591905594,
      "learning_rate": 1.5164583333333335e-05,
      "loss": 0.0019,
      "step": 167210
    },
    {
      "epoch": 5.574,
      "grad_norm": 0.2857668101787567,
      "learning_rate": 1.5162500000000001e-05,
      "loss": 0.0019,
      "step": 167220
    },
    {
      "epoch": 5.574333333333334,
      "grad_norm": 0.14271202683448792,
      "learning_rate": 1.5160416666666668e-05,
      "loss": 0.0018,
      "step": 167230
    },
    {
      "epoch": 5.574666666666666,
      "grad_norm": 0.20053458213806152,
      "learning_rate": 1.5158333333333332e-05,
      "loss": 0.0017,
      "step": 167240
    },
    {
      "epoch": 5.575,
      "grad_norm": 0.20056575536727905,
      "learning_rate": 1.5156249999999999e-05,
      "loss": 0.0014,
      "step": 167250
    },
    {
      "epoch": 5.575333333333333,
      "grad_norm": 0.11514239013195038,
      "learning_rate": 1.5154166666666666e-05,
      "loss": 0.0012,
      "step": 167260
    },
    {
      "epoch": 5.575666666666667,
      "grad_norm": 0.42822346091270447,
      "learning_rate": 1.5152083333333333e-05,
      "loss": 0.0013,
      "step": 167270
    },
    {
      "epoch": 5.576,
      "grad_norm": 0.20044855773448944,
      "learning_rate": 1.515e-05,
      "loss": 0.0015,
      "step": 167280
    },
    {
      "epoch": 5.576333333333333,
      "grad_norm": 0.08610466122627258,
      "learning_rate": 1.5147916666666668e-05,
      "loss": 0.0018,
      "step": 167290
    },
    {
      "epoch": 5.576666666666666,
      "grad_norm": 0.3145218789577484,
      "learning_rate": 1.5145833333333333e-05,
      "loss": 0.0018,
      "step": 167300
    },
    {
      "epoch": 5.577,
      "grad_norm": 0.11389824748039246,
      "learning_rate": 1.514375e-05,
      "loss": 0.0017,
      "step": 167310
    },
    {
      "epoch": 5.577333333333334,
      "grad_norm": 0.3476087152957916,
      "learning_rate": 1.5141666666666668e-05,
      "loss": 0.0015,
      "step": 167320
    },
    {
      "epoch": 5.5776666666666666,
      "grad_norm": 0.00957182515412569,
      "learning_rate": 1.5139583333333335e-05,
      "loss": 0.0017,
      "step": 167330
    },
    {
      "epoch": 5.578,
      "grad_norm": 0.20017899572849274,
      "learning_rate": 1.5137500000000002e-05,
      "loss": 0.0013,
      "step": 167340
    },
    {
      "epoch": 5.578333333333333,
      "grad_norm": 0.257310152053833,
      "learning_rate": 1.513541666666667e-05,
      "loss": 0.0021,
      "step": 167350
    },
    {
      "epoch": 5.578666666666667,
      "grad_norm": 0.14347624778747559,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0016,
      "step": 167360
    },
    {
      "epoch": 5.579,
      "grad_norm": 0.11460448056459427,
      "learning_rate": 1.513125e-05,
      "loss": 0.0017,
      "step": 167370
    },
    {
      "epoch": 5.5793333333333335,
      "grad_norm": 0.02919449657201767,
      "learning_rate": 1.5129166666666666e-05,
      "loss": 0.0018,
      "step": 167380
    },
    {
      "epoch": 5.579666666666666,
      "grad_norm": 0.4578663408756256,
      "learning_rate": 1.5127083333333333e-05,
      "loss": 0.0014,
      "step": 167390
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.3433643579483032,
      "learning_rate": 1.5125e-05,
      "loss": 0.0026,
      "step": 167400
    },
    {
      "epoch": 5.580333333333334,
      "grad_norm": 0.2002371847629547,
      "learning_rate": 1.5122916666666667e-05,
      "loss": 0.0012,
      "step": 167410
    },
    {
      "epoch": 5.580666666666667,
      "grad_norm": 0.3429751992225647,
      "learning_rate": 1.5120833333333334e-05,
      "loss": 0.0015,
      "step": 167420
    },
    {
      "epoch": 5.5809999999999995,
      "grad_norm": 0.1430296003818512,
      "learning_rate": 1.5118750000000002e-05,
      "loss": 0.0019,
      "step": 167430
    },
    {
      "epoch": 5.581333333333333,
      "grad_norm": 0.17183303833007812,
      "learning_rate": 1.5116666666666667e-05,
      "loss": 0.0013,
      "step": 167440
    },
    {
      "epoch": 5.581666666666667,
      "grad_norm": 0.28575506806373596,
      "learning_rate": 1.5114583333333334e-05,
      "loss": 0.0017,
      "step": 167450
    },
    {
      "epoch": 5.582,
      "grad_norm": 0.2859724760055542,
      "learning_rate": 1.5112500000000001e-05,
      "loss": 0.0017,
      "step": 167460
    },
    {
      "epoch": 5.582333333333334,
      "grad_norm": 0.3141374886035919,
      "learning_rate": 1.5110416666666669e-05,
      "loss": 0.0025,
      "step": 167470
    },
    {
      "epoch": 5.582666666666666,
      "grad_norm": 0.08694317191839218,
      "learning_rate": 1.5108333333333332e-05,
      "loss": 0.0015,
      "step": 167480
    },
    {
      "epoch": 5.583,
      "grad_norm": 0.1429361253976822,
      "learning_rate": 1.510625e-05,
      "loss": 0.0014,
      "step": 167490
    },
    {
      "epoch": 5.583333333333333,
      "grad_norm": 0.06755304336547852,
      "learning_rate": 1.5104166666666667e-05,
      "loss": 0.0018,
      "step": 167500
    },
    {
      "epoch": 5.583666666666667,
      "grad_norm": 0.6860823035240173,
      "learning_rate": 1.5102083333333334e-05,
      "loss": 0.0015,
      "step": 167510
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.1426636278629303,
      "learning_rate": 1.51e-05,
      "loss": 0.0024,
      "step": 167520
    },
    {
      "epoch": 5.584333333333333,
      "grad_norm": 0.17105168104171753,
      "learning_rate": 1.5097916666666667e-05,
      "loss": 0.0018,
      "step": 167530
    },
    {
      "epoch": 5.584666666666667,
      "grad_norm": 0.17150536179542542,
      "learning_rate": 1.5095833333333334e-05,
      "loss": 0.0016,
      "step": 167540
    },
    {
      "epoch": 5.585,
      "grad_norm": 0.008249904029071331,
      "learning_rate": 1.5093750000000001e-05,
      "loss": 0.0024,
      "step": 167550
    },
    {
      "epoch": 5.585333333333334,
      "grad_norm": 0.17208954691886902,
      "learning_rate": 1.5091666666666668e-05,
      "loss": 0.0025,
      "step": 167560
    },
    {
      "epoch": 5.585666666666667,
      "grad_norm": 0.14348860085010529,
      "learning_rate": 1.5089583333333335e-05,
      "loss": 0.0016,
      "step": 167570
    },
    {
      "epoch": 5.586,
      "grad_norm": 0.14325843751430511,
      "learning_rate": 1.50875e-05,
      "loss": 0.0015,
      "step": 167580
    },
    {
      "epoch": 5.586333333333333,
      "grad_norm": 0.0581258200109005,
      "learning_rate": 1.5085416666666668e-05,
      "loss": 0.0014,
      "step": 167590
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 0.006008956115692854,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.0014,
      "step": 167600
    },
    {
      "epoch": 5.587,
      "grad_norm": 0.22834020853042603,
      "learning_rate": 1.5081249999999999e-05,
      "loss": 0.0022,
      "step": 167610
    },
    {
      "epoch": 5.5873333333333335,
      "grad_norm": 0.5183027386665344,
      "learning_rate": 1.5079166666666666e-05,
      "loss": 0.0023,
      "step": 167620
    },
    {
      "epoch": 5.587666666666666,
      "grad_norm": 0.05741037800908089,
      "learning_rate": 1.5077083333333333e-05,
      "loss": 0.0023,
      "step": 167630
    },
    {
      "epoch": 5.588,
      "grad_norm": 0.2626377046108246,
      "learning_rate": 1.5075e-05,
      "loss": 0.0028,
      "step": 167640
    },
    {
      "epoch": 5.588333333333333,
      "grad_norm": 0.2574317455291748,
      "learning_rate": 1.5072916666666668e-05,
      "loss": 0.0019,
      "step": 167650
    },
    {
      "epoch": 5.588666666666667,
      "grad_norm": 0.3147173523902893,
      "learning_rate": 1.5070833333333335e-05,
      "loss": 0.0016,
      "step": 167660
    },
    {
      "epoch": 5.589,
      "grad_norm": 0.19971446692943573,
      "learning_rate": 1.506875e-05,
      "loss": 0.0014,
      "step": 167670
    },
    {
      "epoch": 5.589333333333333,
      "grad_norm": 0.3146664798259735,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0017,
      "step": 167680
    },
    {
      "epoch": 5.589666666666667,
      "grad_norm": 0.08685258030891418,
      "learning_rate": 1.5064583333333335e-05,
      "loss": 0.0019,
      "step": 167690
    },
    {
      "epoch": 5.59,
      "grad_norm": 0.25746506452560425,
      "learning_rate": 1.5062500000000002e-05,
      "loss": 0.002,
      "step": 167700
    },
    {
      "epoch": 5.590333333333334,
      "grad_norm": 0.02927529253065586,
      "learning_rate": 1.5060416666666669e-05,
      "loss": 0.002,
      "step": 167710
    },
    {
      "epoch": 5.5906666666666665,
      "grad_norm": 0.11556392163038254,
      "learning_rate": 1.5058333333333335e-05,
      "loss": 0.0014,
      "step": 167720
    },
    {
      "epoch": 5.591,
      "grad_norm": 0.20019479095935822,
      "learning_rate": 1.505625e-05,
      "loss": 0.0013,
      "step": 167730
    },
    {
      "epoch": 5.591333333333333,
      "grad_norm": 0.2849004864692688,
      "learning_rate": 1.5054166666666667e-05,
      "loss": 0.0018,
      "step": 167740
    },
    {
      "epoch": 5.591666666666667,
      "grad_norm": 0.25008049607276917,
      "learning_rate": 1.5052083333333333e-05,
      "loss": 0.0025,
      "step": 167750
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.20024950802326202,
      "learning_rate": 1.505e-05,
      "loss": 0.0015,
      "step": 167760
    },
    {
      "epoch": 5.592333333333333,
      "grad_norm": 0.05882120132446289,
      "learning_rate": 1.5047916666666667e-05,
      "loss": 0.0026,
      "step": 167770
    },
    {
      "epoch": 5.592666666666666,
      "grad_norm": 0.18207433819770813,
      "learning_rate": 1.5045833333333334e-05,
      "loss": 0.0017,
      "step": 167780
    },
    {
      "epoch": 5.593,
      "grad_norm": 0.2578052580356598,
      "learning_rate": 1.5043750000000001e-05,
      "loss": 0.0018,
      "step": 167790
    },
    {
      "epoch": 5.593333333333334,
      "grad_norm": 0.29179102182388306,
      "learning_rate": 1.5041666666666669e-05,
      "loss": 0.0012,
      "step": 167800
    },
    {
      "epoch": 5.593666666666667,
      "grad_norm": 0.08665995299816132,
      "learning_rate": 1.5039583333333334e-05,
      "loss": 0.0014,
      "step": 167810
    },
    {
      "epoch": 5.594,
      "grad_norm": 0.22857341170310974,
      "learning_rate": 1.5037500000000001e-05,
      "loss": 0.0022,
      "step": 167820
    },
    {
      "epoch": 5.594333333333333,
      "grad_norm": 0.1438307911157608,
      "learning_rate": 1.5035416666666668e-05,
      "loss": 0.0013,
      "step": 167830
    },
    {
      "epoch": 5.594666666666667,
      "grad_norm": 0.36425238847732544,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0021,
      "step": 167840
    },
    {
      "epoch": 5.595,
      "grad_norm": 0.057540714740753174,
      "learning_rate": 1.503125e-05,
      "loss": 0.0015,
      "step": 167850
    },
    {
      "epoch": 5.5953333333333335,
      "grad_norm": 0.40248769521713257,
      "learning_rate": 1.5029166666666667e-05,
      "loss": 0.0017,
      "step": 167860
    },
    {
      "epoch": 5.595666666666666,
      "grad_norm": 0.08600781857967377,
      "learning_rate": 1.5027083333333334e-05,
      "loss": 0.0012,
      "step": 167870
    },
    {
      "epoch": 5.596,
      "grad_norm": 0.11533581465482712,
      "learning_rate": 1.5025000000000001e-05,
      "loss": 0.0028,
      "step": 167880
    },
    {
      "epoch": 5.596333333333334,
      "grad_norm": 0.28516480326652527,
      "learning_rate": 1.5022916666666666e-05,
      "loss": 0.0017,
      "step": 167890
    },
    {
      "epoch": 5.596666666666667,
      "grad_norm": 0.25719940662384033,
      "learning_rate": 1.5020833333333334e-05,
      "loss": 0.0014,
      "step": 167900
    },
    {
      "epoch": 5.5969999999999995,
      "grad_norm": 0.03452041372656822,
      "learning_rate": 1.501875e-05,
      "loss": 0.0022,
      "step": 167910
    },
    {
      "epoch": 5.597333333333333,
      "grad_norm": 0.0854838490486145,
      "learning_rate": 1.5016666666666668e-05,
      "loss": 0.0017,
      "step": 167920
    },
    {
      "epoch": 5.597666666666667,
      "grad_norm": 0.1426393687725067,
      "learning_rate": 1.5014583333333335e-05,
      "loss": 0.0014,
      "step": 167930
    },
    {
      "epoch": 5.598,
      "grad_norm": 0.22828294336795807,
      "learning_rate": 1.5012500000000002e-05,
      "loss": 0.0014,
      "step": 167940
    },
    {
      "epoch": 5.598333333333334,
      "grad_norm": 0.08595225214958191,
      "learning_rate": 1.5010416666666668e-05,
      "loss": 0.0015,
      "step": 167950
    },
    {
      "epoch": 5.5986666666666665,
      "grad_norm": 0.3919703960418701,
      "learning_rate": 1.5008333333333335e-05,
      "loss": 0.002,
      "step": 167960
    },
    {
      "epoch": 5.599,
      "grad_norm": 0.14350564777851105,
      "learning_rate": 1.5006249999999999e-05,
      "loss": 0.0015,
      "step": 167970
    },
    {
      "epoch": 5.599333333333333,
      "grad_norm": 0.2576724886894226,
      "learning_rate": 1.5004166666666666e-05,
      "loss": 0.0013,
      "step": 167980
    },
    {
      "epoch": 5.599666666666667,
      "grad_norm": 0.31417199969291687,
      "learning_rate": 1.5002083333333333e-05,
      "loss": 0.0013,
      "step": 167990
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.31484928727149963,
      "learning_rate": 1.5e-05,
      "loss": 0.0014,
      "step": 168000
    },
    {
      "epoch": 5.600333333333333,
      "grad_norm": 0.14297501742839813,
      "learning_rate": 1.4997916666666668e-05,
      "loss": 0.0014,
      "step": 168010
    },
    {
      "epoch": 5.600666666666667,
      "grad_norm": 0.007390469312667847,
      "learning_rate": 1.4995833333333335e-05,
      "loss": 0.0014,
      "step": 168020
    },
    {
      "epoch": 5.601,
      "grad_norm": 0.7155271768569946,
      "learning_rate": 1.499375e-05,
      "loss": 0.0018,
      "step": 168030
    },
    {
      "epoch": 5.601333333333334,
      "grad_norm": 0.05865911766886711,
      "learning_rate": 1.4991666666666667e-05,
      "loss": 0.0017,
      "step": 168040
    },
    {
      "epoch": 5.601666666666667,
      "grad_norm": 0.3141849637031555,
      "learning_rate": 1.4989583333333335e-05,
      "loss": 0.0021,
      "step": 168050
    },
    {
      "epoch": 5.602,
      "grad_norm": 0.5236965417861938,
      "learning_rate": 1.4987500000000002e-05,
      "loss": 0.0024,
      "step": 168060
    },
    {
      "epoch": 5.602333333333333,
      "grad_norm": 0.08657379448413849,
      "learning_rate": 1.4985416666666669e-05,
      "loss": 0.0018,
      "step": 168070
    },
    {
      "epoch": 5.602666666666667,
      "grad_norm": 0.16817203164100647,
      "learning_rate": 1.4983333333333336e-05,
      "loss": 0.0016,
      "step": 168080
    },
    {
      "epoch": 5.603,
      "grad_norm": 0.17201825976371765,
      "learning_rate": 1.498125e-05,
      "loss": 0.0024,
      "step": 168090
    },
    {
      "epoch": 5.6033333333333335,
      "grad_norm": 0.14388202130794525,
      "learning_rate": 1.4979166666666667e-05,
      "loss": 0.0015,
      "step": 168100
    },
    {
      "epoch": 5.603666666666666,
      "grad_norm": 0.058351464569568634,
      "learning_rate": 1.4977083333333333e-05,
      "loss": 0.0014,
      "step": 168110
    },
    {
      "epoch": 5.604,
      "grad_norm": 0.029305625706911087,
      "learning_rate": 1.4975e-05,
      "loss": 0.002,
      "step": 168120
    },
    {
      "epoch": 5.604333333333333,
      "grad_norm": 0.34797725081443787,
      "learning_rate": 1.4972916666666667e-05,
      "loss": 0.0013,
      "step": 168130
    },
    {
      "epoch": 5.604666666666667,
      "grad_norm": 0.5145382881164551,
      "learning_rate": 1.4970833333333334e-05,
      "loss": 0.0012,
      "step": 168140
    },
    {
      "epoch": 5.605,
      "grad_norm": 0.14325393736362457,
      "learning_rate": 1.4968750000000001e-05,
      "loss": 0.0022,
      "step": 168150
    },
    {
      "epoch": 5.605333333333333,
      "grad_norm": 0.4568178355693817,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.002,
      "step": 168160
    },
    {
      "epoch": 5.605666666666667,
      "grad_norm": 0.029359938576817513,
      "learning_rate": 1.4964583333333334e-05,
      "loss": 0.0012,
      "step": 168170
    },
    {
      "epoch": 5.606,
      "grad_norm": 0.05986007675528526,
      "learning_rate": 1.4962500000000001e-05,
      "loss": 0.0019,
      "step": 168180
    },
    {
      "epoch": 5.606333333333334,
      "grad_norm": 0.17207984626293182,
      "learning_rate": 1.4960416666666668e-05,
      "loss": 0.0016,
      "step": 168190
    },
    {
      "epoch": 5.6066666666666665,
      "grad_norm": 0.1150059923529625,
      "learning_rate": 1.4958333333333336e-05,
      "loss": 0.0016,
      "step": 168200
    },
    {
      "epoch": 5.607,
      "grad_norm": 0.14332285523414612,
      "learning_rate": 1.495625e-05,
      "loss": 0.0017,
      "step": 168210
    },
    {
      "epoch": 5.607333333333333,
      "grad_norm": 0.040507182478904724,
      "learning_rate": 1.4954166666666666e-05,
      "loss": 0.0015,
      "step": 168220
    },
    {
      "epoch": 5.607666666666667,
      "grad_norm": 0.5429803729057312,
      "learning_rate": 1.4952083333333334e-05,
      "loss": 0.0016,
      "step": 168230
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.05914967879652977,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 0.0021,
      "step": 168240
    },
    {
      "epoch": 5.608333333333333,
      "grad_norm": 0.03168574720621109,
      "learning_rate": 1.4947916666666666e-05,
      "loss": 0.0016,
      "step": 168250
    },
    {
      "epoch": 5.608666666666666,
      "grad_norm": 0.2577767074108124,
      "learning_rate": 1.4945833333333334e-05,
      "loss": 0.0021,
      "step": 168260
    },
    {
      "epoch": 5.609,
      "grad_norm": 0.08592376857995987,
      "learning_rate": 1.494375e-05,
      "loss": 0.0018,
      "step": 168270
    },
    {
      "epoch": 5.609333333333334,
      "grad_norm": 0.11484114080667496,
      "learning_rate": 1.4941666666666668e-05,
      "loss": 0.0016,
      "step": 168280
    },
    {
      "epoch": 5.609666666666667,
      "grad_norm": 0.34300774335861206,
      "learning_rate": 1.4939583333333335e-05,
      "loss": 0.0015,
      "step": 168290
    },
    {
      "epoch": 5.61,
      "grad_norm": 0.257607638835907,
      "learning_rate": 1.4937500000000002e-05,
      "loss": 0.0021,
      "step": 168300
    },
    {
      "epoch": 5.610333333333333,
      "grad_norm": 0.1715141236782074,
      "learning_rate": 1.4935416666666668e-05,
      "loss": 0.0015,
      "step": 168310
    },
    {
      "epoch": 5.610666666666667,
      "grad_norm": 0.007893556728959084,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0017,
      "step": 168320
    },
    {
      "epoch": 5.611,
      "grad_norm": 0.05759265646338463,
      "learning_rate": 1.4931249999999999e-05,
      "loss": 0.0017,
      "step": 168330
    },
    {
      "epoch": 5.6113333333333335,
      "grad_norm": 0.029897218570113182,
      "learning_rate": 1.4929166666666666e-05,
      "loss": 0.0021,
      "step": 168340
    },
    {
      "epoch": 5.611666666666666,
      "grad_norm": 0.4284009635448456,
      "learning_rate": 1.4927083333333333e-05,
      "loss": 0.0015,
      "step": 168350
    },
    {
      "epoch": 5.612,
      "grad_norm": 0.10212812572717667,
      "learning_rate": 1.4925e-05,
      "loss": 0.0019,
      "step": 168360
    },
    {
      "epoch": 5.612333333333333,
      "grad_norm": 0.11227839440107346,
      "learning_rate": 1.4922916666666667e-05,
      "loss": 0.0016,
      "step": 168370
    },
    {
      "epoch": 5.612666666666667,
      "grad_norm": 0.3146730065345764,
      "learning_rate": 1.4920833333333335e-05,
      "loss": 0.0022,
      "step": 168380
    },
    {
      "epoch": 5.6129999999999995,
      "grad_norm": 0.13258758187294006,
      "learning_rate": 1.491875e-05,
      "loss": 0.0016,
      "step": 168390
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 0.08610247075557709,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.0017,
      "step": 168400
    },
    {
      "epoch": 5.613666666666667,
      "grad_norm": 0.05835571885108948,
      "learning_rate": 1.4914583333333334e-05,
      "loss": 0.002,
      "step": 168410
    },
    {
      "epoch": 5.614,
      "grad_norm": 0.05939885973930359,
      "learning_rate": 1.4912500000000002e-05,
      "loss": 0.0014,
      "step": 168420
    },
    {
      "epoch": 5.614333333333334,
      "grad_norm": 0.08591371774673462,
      "learning_rate": 1.4910416666666669e-05,
      "loss": 0.0017,
      "step": 168430
    },
    {
      "epoch": 5.6146666666666665,
      "grad_norm": 0.30671927332878113,
      "learning_rate": 1.4908333333333336e-05,
      "loss": 0.0022,
      "step": 168440
    },
    {
      "epoch": 5.615,
      "grad_norm": 0.28582048416137695,
      "learning_rate": 1.490625e-05,
      "loss": 0.0013,
      "step": 168450
    },
    {
      "epoch": 5.615333333333333,
      "grad_norm": 0.2774195373058319,
      "learning_rate": 1.4904166666666667e-05,
      "loss": 0.0023,
      "step": 168460
    },
    {
      "epoch": 5.615666666666667,
      "grad_norm": 0.3341697156429291,
      "learning_rate": 1.4902083333333332e-05,
      "loss": 0.0015,
      "step": 168470
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.25698524713516235,
      "learning_rate": 1.49e-05,
      "loss": 0.0023,
      "step": 168480
    },
    {
      "epoch": 5.616333333333333,
      "grad_norm": 0.14273466169834137,
      "learning_rate": 1.4897916666666667e-05,
      "loss": 0.0016,
      "step": 168490
    },
    {
      "epoch": 5.616666666666667,
      "grad_norm": 0.14328442513942719,
      "learning_rate": 1.4895833333333334e-05,
      "loss": 0.0019,
      "step": 168500
    },
    {
      "epoch": 5.617,
      "grad_norm": 0.05756382644176483,
      "learning_rate": 1.4893750000000001e-05,
      "loss": 0.0021,
      "step": 168510
    },
    {
      "epoch": 5.617333333333333,
      "grad_norm": 0.14280934631824493,
      "learning_rate": 1.4891666666666668e-05,
      "loss": 0.0016,
      "step": 168520
    },
    {
      "epoch": 5.617666666666667,
      "grad_norm": 0.029342995956540108,
      "learning_rate": 1.4889583333333334e-05,
      "loss": 0.0015,
      "step": 168530
    },
    {
      "epoch": 5.618,
      "grad_norm": 0.03375660255551338,
      "learning_rate": 1.4887500000000001e-05,
      "loss": 0.0015,
      "step": 168540
    },
    {
      "epoch": 5.618333333333333,
      "grad_norm": 0.08705787360668182,
      "learning_rate": 1.4885416666666668e-05,
      "loss": 0.0013,
      "step": 168550
    },
    {
      "epoch": 5.618666666666667,
      "grad_norm": 0.26329487562179565,
      "learning_rate": 1.4883333333333335e-05,
      "loss": 0.0018,
      "step": 168560
    },
    {
      "epoch": 5.619,
      "grad_norm": 0.05838926136493683,
      "learning_rate": 1.488125e-05,
      "loss": 0.002,
      "step": 168570
    },
    {
      "epoch": 5.6193333333333335,
      "grad_norm": 0.11402864009141922,
      "learning_rate": 1.4879166666666666e-05,
      "loss": 0.0027,
      "step": 168580
    },
    {
      "epoch": 5.619666666666666,
      "grad_norm": 0.05766420438885689,
      "learning_rate": 1.4877083333333334e-05,
      "loss": 0.0018,
      "step": 168590
    },
    {
      "epoch": 5.62,
      "grad_norm": 0.384541779756546,
      "learning_rate": 1.4875e-05,
      "loss": 0.0022,
      "step": 168600
    },
    {
      "epoch": 5.620333333333333,
      "grad_norm": 0.08573119342327118,
      "learning_rate": 1.4872916666666666e-05,
      "loss": 0.0028,
      "step": 168610
    },
    {
      "epoch": 5.620666666666667,
      "grad_norm": 0.11493214219808578,
      "learning_rate": 1.4870833333333333e-05,
      "loss": 0.0012,
      "step": 168620
    },
    {
      "epoch": 5.621,
      "grad_norm": 0.14291752874851227,
      "learning_rate": 1.486875e-05,
      "loss": 0.0016,
      "step": 168630
    },
    {
      "epoch": 5.621333333333333,
      "grad_norm": 0.08632005751132965,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0016,
      "step": 168640
    },
    {
      "epoch": 5.621666666666667,
      "grad_norm": 0.20499806106090546,
      "learning_rate": 1.4864583333333335e-05,
      "loss": 0.0027,
      "step": 168650
    },
    {
      "epoch": 5.622,
      "grad_norm": 0.23635877668857574,
      "learning_rate": 1.4862500000000002e-05,
      "loss": 0.0025,
      "step": 168660
    },
    {
      "epoch": 5.622333333333334,
      "grad_norm": 0.3423216938972473,
      "learning_rate": 1.4860416666666668e-05,
      "loss": 0.0018,
      "step": 168670
    },
    {
      "epoch": 5.6226666666666665,
      "grad_norm": 0.08605234324932098,
      "learning_rate": 1.4858333333333335e-05,
      "loss": 0.0023,
      "step": 168680
    },
    {
      "epoch": 5.623,
      "grad_norm": 0.029077520594000816,
      "learning_rate": 1.4856249999999999e-05,
      "loss": 0.0014,
      "step": 168690
    },
    {
      "epoch": 5.623333333333333,
      "grad_norm": 0.59991455078125,
      "learning_rate": 1.4854166666666666e-05,
      "loss": 0.0016,
      "step": 168700
    },
    {
      "epoch": 5.623666666666667,
      "grad_norm": 0.39996537566185,
      "learning_rate": 1.4852083333333333e-05,
      "loss": 0.002,
      "step": 168710
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.03093099594116211,
      "learning_rate": 1.485e-05,
      "loss": 0.0016,
      "step": 168720
    },
    {
      "epoch": 5.624333333333333,
      "grad_norm": 0.05880843847990036,
      "learning_rate": 1.4847916666666667e-05,
      "loss": 0.0018,
      "step": 168730
    },
    {
      "epoch": 5.624666666666666,
      "grad_norm": 0.1327805072069168,
      "learning_rate": 1.4845833333333334e-05,
      "loss": 0.0018,
      "step": 168740
    },
    {
      "epoch": 5.625,
      "grad_norm": 0.08602061867713928,
      "learning_rate": 1.484375e-05,
      "loss": 0.0016,
      "step": 168750
    },
    {
      "epoch": 5.625333333333334,
      "grad_norm": 0.17250360548496246,
      "learning_rate": 1.4841666666666667e-05,
      "loss": 0.0022,
      "step": 168760
    },
    {
      "epoch": 5.625666666666667,
      "grad_norm": 0.19980260729789734,
      "learning_rate": 1.4839583333333334e-05,
      "loss": 0.0015,
      "step": 168770
    },
    {
      "epoch": 5.626,
      "grad_norm": 0.05767592415213585,
      "learning_rate": 1.4837500000000002e-05,
      "loss": 0.0017,
      "step": 168780
    },
    {
      "epoch": 5.626333333333333,
      "grad_norm": 0.21574299037456512,
      "learning_rate": 1.4835416666666669e-05,
      "loss": 0.0021,
      "step": 168790
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 0.14964301884174347,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0019,
      "step": 168800
    },
    {
      "epoch": 5.627,
      "grad_norm": 0.032292041927576065,
      "learning_rate": 1.4831250000000001e-05,
      "loss": 0.0018,
      "step": 168810
    },
    {
      "epoch": 5.6273333333333335,
      "grad_norm": 0.05739911273121834,
      "learning_rate": 1.4829166666666667e-05,
      "loss": 0.0019,
      "step": 168820
    },
    {
      "epoch": 5.627666666666666,
      "grad_norm": 0.32310429215431213,
      "learning_rate": 1.4827083333333332e-05,
      "loss": 0.0029,
      "step": 168830
    },
    {
      "epoch": 5.628,
      "grad_norm": 0.05768536776304245,
      "learning_rate": 1.4825e-05,
      "loss": 0.0022,
      "step": 168840
    },
    {
      "epoch": 5.628333333333333,
      "grad_norm": 0.005157172679901123,
      "learning_rate": 1.4822916666666667e-05,
      "loss": 0.0021,
      "step": 168850
    },
    {
      "epoch": 5.628666666666667,
      "grad_norm": 0.029757928103208542,
      "learning_rate": 1.4820833333333334e-05,
      "loss": 0.0012,
      "step": 168860
    },
    {
      "epoch": 5.629,
      "grad_norm": 0.029038654640316963,
      "learning_rate": 1.4818750000000001e-05,
      "loss": 0.0017,
      "step": 168870
    },
    {
      "epoch": 5.629333333333333,
      "grad_norm": 0.38410574197769165,
      "learning_rate": 1.4816666666666668e-05,
      "loss": 0.0018,
      "step": 168880
    },
    {
      "epoch": 5.629666666666667,
      "grad_norm": 0.12426290661096573,
      "learning_rate": 1.4814583333333334e-05,
      "loss": 0.0018,
      "step": 168890
    },
    {
      "epoch": 5.63,
      "grad_norm": 0.01344729121774435,
      "learning_rate": 1.4812500000000001e-05,
      "loss": 0.0014,
      "step": 168900
    },
    {
      "epoch": 5.630333333333334,
      "grad_norm": 0.45215609669685364,
      "learning_rate": 1.4810416666666668e-05,
      "loss": 0.0022,
      "step": 168910
    },
    {
      "epoch": 5.6306666666666665,
      "grad_norm": 0.37902435660362244,
      "learning_rate": 1.4808333333333335e-05,
      "loss": 0.0016,
      "step": 168920
    },
    {
      "epoch": 5.631,
      "grad_norm": 0.2285737246274948,
      "learning_rate": 1.4806250000000002e-05,
      "loss": 0.0015,
      "step": 168930
    },
    {
      "epoch": 5.631333333333333,
      "grad_norm": 0.31418800354003906,
      "learning_rate": 1.4804166666666666e-05,
      "loss": 0.0014,
      "step": 168940
    },
    {
      "epoch": 5.631666666666667,
      "grad_norm": 0.22818821668624878,
      "learning_rate": 1.4802083333333333e-05,
      "loss": 0.0017,
      "step": 168950
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.2653310000896454,
      "learning_rate": 1.48e-05,
      "loss": 0.0016,
      "step": 168960
    },
    {
      "epoch": 5.632333333333333,
      "grad_norm": 0.11493787169456482,
      "learning_rate": 1.4797916666666666e-05,
      "loss": 0.002,
      "step": 168970
    },
    {
      "epoch": 5.632666666666667,
      "grad_norm": 0.11445935815572739,
      "learning_rate": 1.4795833333333333e-05,
      "loss": 0.0022,
      "step": 168980
    },
    {
      "epoch": 5.633,
      "grad_norm": 0.2572685778141022,
      "learning_rate": 1.479375e-05,
      "loss": 0.0015,
      "step": 168990
    },
    {
      "epoch": 5.633333333333333,
      "grad_norm": 0.39979293942451477,
      "learning_rate": 1.4791666666666668e-05,
      "loss": 0.0012,
      "step": 169000
    },
    {
      "epoch": 5.633666666666667,
      "grad_norm": 0.08604355156421661,
      "learning_rate": 1.4789583333333335e-05,
      "loss": 0.0014,
      "step": 169010
    },
    {
      "epoch": 5.634,
      "grad_norm": 0.057840801775455475,
      "learning_rate": 1.4787500000000002e-05,
      "loss": 0.0019,
      "step": 169020
    },
    {
      "epoch": 5.634333333333333,
      "grad_norm": 0.14298716187477112,
      "learning_rate": 1.4785416666666667e-05,
      "loss": 0.0021,
      "step": 169030
    },
    {
      "epoch": 5.634666666666667,
      "grad_norm": 0.2572334408760071,
      "learning_rate": 1.4783333333333335e-05,
      "loss": 0.0017,
      "step": 169040
    },
    {
      "epoch": 5.635,
      "grad_norm": 0.4134449362754822,
      "learning_rate": 1.4781250000000002e-05,
      "loss": 0.0027,
      "step": 169050
    },
    {
      "epoch": 5.6353333333333335,
      "grad_norm": 0.17186538875102997,
      "learning_rate": 1.4779166666666666e-05,
      "loss": 0.0018,
      "step": 169060
    },
    {
      "epoch": 5.635666666666666,
      "grad_norm": 0.057563506066799164,
      "learning_rate": 1.4777083333333333e-05,
      "loss": 0.0018,
      "step": 169070
    },
    {
      "epoch": 5.636,
      "grad_norm": 0.6625228524208069,
      "learning_rate": 1.4775e-05,
      "loss": 0.002,
      "step": 169080
    },
    {
      "epoch": 5.636333333333333,
      "grad_norm": 0.1719183772802353,
      "learning_rate": 1.4772916666666667e-05,
      "loss": 0.0019,
      "step": 169090
    },
    {
      "epoch": 5.636666666666667,
      "grad_norm": 0.1439511477947235,
      "learning_rate": 1.4770833333333334e-05,
      "loss": 0.0028,
      "step": 169100
    },
    {
      "epoch": 5.6370000000000005,
      "grad_norm": 0.22854071855545044,
      "learning_rate": 1.476875e-05,
      "loss": 0.0015,
      "step": 169110
    },
    {
      "epoch": 5.637333333333333,
      "grad_norm": 0.25674986839294434,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.0026,
      "step": 169120
    },
    {
      "epoch": 5.637666666666667,
      "grad_norm": 0.314886212348938,
      "learning_rate": 1.4764583333333334e-05,
      "loss": 0.0016,
      "step": 169130
    },
    {
      "epoch": 5.638,
      "grad_norm": 0.11483445763587952,
      "learning_rate": 1.4762500000000001e-05,
      "loss": 0.003,
      "step": 169140
    },
    {
      "epoch": 5.638333333333334,
      "grad_norm": 0.22844985127449036,
      "learning_rate": 1.4760416666666669e-05,
      "loss": 0.0015,
      "step": 169150
    },
    {
      "epoch": 5.6386666666666665,
      "grad_norm": 0.08615913987159729,
      "learning_rate": 1.4758333333333336e-05,
      "loss": 0.0021,
      "step": 169160
    },
    {
      "epoch": 5.639,
      "grad_norm": 0.05744781717658043,
      "learning_rate": 1.4756250000000001e-05,
      "loss": 0.0016,
      "step": 169170
    },
    {
      "epoch": 5.639333333333333,
      "grad_norm": 0.11480224132537842,
      "learning_rate": 1.4754166666666667e-05,
      "loss": 0.0017,
      "step": 169180
    },
    {
      "epoch": 5.639666666666667,
      "grad_norm": 0.2566402852535248,
      "learning_rate": 1.4752083333333332e-05,
      "loss": 0.0021,
      "step": 169190
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.030614782124757767,
      "learning_rate": 1.475e-05,
      "loss": 0.0019,
      "step": 169200
    },
    {
      "epoch": 5.640333333333333,
      "grad_norm": 0.05725089833140373,
      "learning_rate": 1.4747916666666667e-05,
      "loss": 0.0015,
      "step": 169210
    },
    {
      "epoch": 5.640666666666666,
      "grad_norm": 0.01568921096622944,
      "learning_rate": 1.4745833333333334e-05,
      "loss": 0.0016,
      "step": 169220
    },
    {
      "epoch": 5.641,
      "grad_norm": 0.0864534080028534,
      "learning_rate": 1.4743750000000001e-05,
      "loss": 0.0013,
      "step": 169230
    },
    {
      "epoch": 5.641333333333334,
      "grad_norm": 0.3427192270755768,
      "learning_rate": 1.4741666666666668e-05,
      "loss": 0.0015,
      "step": 169240
    },
    {
      "epoch": 5.641666666666667,
      "grad_norm": 0.058825574815273285,
      "learning_rate": 1.4739583333333334e-05,
      "loss": 0.0014,
      "step": 169250
    },
    {
      "epoch": 5.642,
      "grad_norm": 0.08651169389486313,
      "learning_rate": 1.47375e-05,
      "loss": 0.0013,
      "step": 169260
    },
    {
      "epoch": 5.642333333333333,
      "grad_norm": 0.06225054711103439,
      "learning_rate": 1.4735416666666668e-05,
      "loss": 0.0016,
      "step": 169270
    },
    {
      "epoch": 5.642666666666667,
      "grad_norm": 0.00760261295363307,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0021,
      "step": 169280
    },
    {
      "epoch": 5.643,
      "grad_norm": 0.14318448305130005,
      "learning_rate": 1.4731250000000002e-05,
      "loss": 0.0014,
      "step": 169290
    },
    {
      "epoch": 5.6433333333333335,
      "grad_norm": 0.25736260414123535,
      "learning_rate": 1.4729166666666666e-05,
      "loss": 0.0014,
      "step": 169300
    },
    {
      "epoch": 5.643666666666666,
      "grad_norm": 0.3422257602214813,
      "learning_rate": 1.4727083333333333e-05,
      "loss": 0.0012,
      "step": 169310
    },
    {
      "epoch": 5.644,
      "grad_norm": 0.11415699869394302,
      "learning_rate": 1.4725e-05,
      "loss": 0.0018,
      "step": 169320
    },
    {
      "epoch": 5.644333333333333,
      "grad_norm": 0.11527581512928009,
      "learning_rate": 1.4722916666666666e-05,
      "loss": 0.0017,
      "step": 169330
    },
    {
      "epoch": 5.644666666666667,
      "grad_norm": 0.22217360138893127,
      "learning_rate": 1.4720833333333333e-05,
      "loss": 0.0018,
      "step": 169340
    },
    {
      "epoch": 5.645,
      "grad_norm": 0.029647862538695335,
      "learning_rate": 1.471875e-05,
      "loss": 0.0014,
      "step": 169350
    },
    {
      "epoch": 5.645333333333333,
      "grad_norm": 0.2569873034954071,
      "learning_rate": 1.4716666666666668e-05,
      "loss": 0.0013,
      "step": 169360
    },
    {
      "epoch": 5.645666666666667,
      "grad_norm": 0.3711768388748169,
      "learning_rate": 1.4714583333333335e-05,
      "loss": 0.0021,
      "step": 169370
    },
    {
      "epoch": 5.646,
      "grad_norm": 0.08665168285369873,
      "learning_rate": 1.4712500000000002e-05,
      "loss": 0.0019,
      "step": 169380
    },
    {
      "epoch": 5.646333333333334,
      "grad_norm": 0.42403149604797363,
      "learning_rate": 1.4710416666666667e-05,
      "loss": 0.0015,
      "step": 169390
    },
    {
      "epoch": 5.6466666666666665,
      "grad_norm": 0.029802104458212852,
      "learning_rate": 1.4708333333333335e-05,
      "loss": 0.0018,
      "step": 169400
    },
    {
      "epoch": 5.647,
      "grad_norm": 0.11728455871343613,
      "learning_rate": 1.4706250000000002e-05,
      "loss": 0.0016,
      "step": 169410
    },
    {
      "epoch": 5.647333333333333,
      "grad_norm": 0.0583912692964077,
      "learning_rate": 1.4704166666666666e-05,
      "loss": 0.0028,
      "step": 169420
    },
    {
      "epoch": 5.647666666666667,
      "grad_norm": 0.03029400296509266,
      "learning_rate": 1.4702083333333333e-05,
      "loss": 0.0014,
      "step": 169430
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.03397347778081894,
      "learning_rate": 1.47e-05,
      "loss": 0.0017,
      "step": 169440
    },
    {
      "epoch": 5.648333333333333,
      "grad_norm": 0.05856902152299881,
      "learning_rate": 1.4697916666666667e-05,
      "loss": 0.0015,
      "step": 169450
    },
    {
      "epoch": 5.648666666666666,
      "grad_norm": 0.08601555973291397,
      "learning_rate": 1.4695833333333334e-05,
      "loss": 0.0018,
      "step": 169460
    },
    {
      "epoch": 5.649,
      "grad_norm": 0.11517218500375748,
      "learning_rate": 1.4693750000000001e-05,
      "loss": 0.0017,
      "step": 169470
    },
    {
      "epoch": 5.649333333333333,
      "grad_norm": 0.05933089181780815,
      "learning_rate": 1.4691666666666667e-05,
      "loss": 0.0027,
      "step": 169480
    },
    {
      "epoch": 5.649666666666667,
      "grad_norm": 0.11425137519836426,
      "learning_rate": 1.4689583333333334e-05,
      "loss": 0.0019,
      "step": 169490
    },
    {
      "epoch": 5.65,
      "grad_norm": 0.22947444021701813,
      "learning_rate": 1.4687500000000001e-05,
      "loss": 0.0015,
      "step": 169500
    },
    {
      "epoch": 5.650333333333333,
      "grad_norm": 0.20025880634784698,
      "learning_rate": 1.4685416666666668e-05,
      "loss": 0.002,
      "step": 169510
    },
    {
      "epoch": 5.650666666666667,
      "grad_norm": 0.1805909276008606,
      "learning_rate": 1.4683333333333336e-05,
      "loss": 0.0017,
      "step": 169520
    },
    {
      "epoch": 5.651,
      "grad_norm": 0.3712020814418793,
      "learning_rate": 1.4681250000000001e-05,
      "loss": 0.0022,
      "step": 169530
    },
    {
      "epoch": 5.6513333333333335,
      "grad_norm": 0.14284229278564453,
      "learning_rate": 1.4679166666666667e-05,
      "loss": 0.0021,
      "step": 169540
    },
    {
      "epoch": 5.651666666666666,
      "grad_norm": 0.6967455744743347,
      "learning_rate": 1.4677083333333334e-05,
      "loss": 0.0016,
      "step": 169550
    },
    {
      "epoch": 5.652,
      "grad_norm": 0.2285882532596588,
      "learning_rate": 1.4675e-05,
      "loss": 0.0023,
      "step": 169560
    },
    {
      "epoch": 5.652333333333333,
      "grad_norm": 0.17158855497837067,
      "learning_rate": 1.4672916666666666e-05,
      "loss": 0.0023,
      "step": 169570
    },
    {
      "epoch": 5.652666666666667,
      "grad_norm": 0.35075604915618896,
      "learning_rate": 1.4670833333333334e-05,
      "loss": 0.0018,
      "step": 169580
    },
    {
      "epoch": 5.6530000000000005,
      "grad_norm": 0.03002958372235298,
      "learning_rate": 1.466875e-05,
      "loss": 0.0019,
      "step": 169590
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.11673977971076965,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0017,
      "step": 169600
    },
    {
      "epoch": 5.653666666666666,
      "grad_norm": 0.2004951387643814,
      "learning_rate": 1.4664583333333335e-05,
      "loss": 0.0012,
      "step": 169610
    },
    {
      "epoch": 5.654,
      "grad_norm": 0.3710169494152069,
      "learning_rate": 1.46625e-05,
      "loss": 0.0018,
      "step": 169620
    },
    {
      "epoch": 5.654333333333334,
      "grad_norm": 0.17099629342556,
      "learning_rate": 1.4660416666666668e-05,
      "loss": 0.0014,
      "step": 169630
    },
    {
      "epoch": 5.6546666666666665,
      "grad_norm": 0.05836993083357811,
      "learning_rate": 1.4658333333333335e-05,
      "loss": 0.0021,
      "step": 169640
    },
    {
      "epoch": 5.655,
      "grad_norm": 0.25684866309165955,
      "learning_rate": 1.4656250000000002e-05,
      "loss": 0.0017,
      "step": 169650
    },
    {
      "epoch": 5.655333333333333,
      "grad_norm": 0.08651834726333618,
      "learning_rate": 1.4654166666666666e-05,
      "loss": 0.0012,
      "step": 169660
    },
    {
      "epoch": 5.655666666666667,
      "grad_norm": 0.2572840750217438,
      "learning_rate": 1.4652083333333333e-05,
      "loss": 0.0017,
      "step": 169670
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.1997453272342682,
      "learning_rate": 1.465e-05,
      "loss": 0.0015,
      "step": 169680
    },
    {
      "epoch": 5.656333333333333,
      "grad_norm": 0.1709941178560257,
      "learning_rate": 1.4647916666666668e-05,
      "loss": 0.0018,
      "step": 169690
    },
    {
      "epoch": 5.656666666666666,
      "grad_norm": 0.05756533890962601,
      "learning_rate": 1.4645833333333333e-05,
      "loss": 0.0017,
      "step": 169700
    },
    {
      "epoch": 5.657,
      "grad_norm": 0.31447240710258484,
      "learning_rate": 1.464375e-05,
      "loss": 0.0029,
      "step": 169710
    },
    {
      "epoch": 5.657333333333334,
      "grad_norm": 0.12092235684394836,
      "learning_rate": 1.4641666666666667e-05,
      "loss": 0.002,
      "step": 169720
    },
    {
      "epoch": 5.657666666666667,
      "grad_norm": 0.1714366376399994,
      "learning_rate": 1.4639583333333335e-05,
      "loss": 0.0024,
      "step": 169730
    },
    {
      "epoch": 5.658,
      "grad_norm": 0.017474057152867317,
      "learning_rate": 1.4637500000000002e-05,
      "loss": 0.0016,
      "step": 169740
    },
    {
      "epoch": 5.658333333333333,
      "grad_norm": 0.2002129703760147,
      "learning_rate": 1.4635416666666669e-05,
      "loss": 0.0019,
      "step": 169750
    },
    {
      "epoch": 5.658666666666667,
      "grad_norm": 0.6802012920379639,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0028,
      "step": 169760
    },
    {
      "epoch": 5.659,
      "grad_norm": 0.2336287498474121,
      "learning_rate": 1.4631250000000002e-05,
      "loss": 0.0018,
      "step": 169770
    },
    {
      "epoch": 5.6593333333333335,
      "grad_norm": 0.17551325261592865,
      "learning_rate": 1.4629166666666665e-05,
      "loss": 0.0014,
      "step": 169780
    },
    {
      "epoch": 5.659666666666666,
      "grad_norm": 0.22872991859912872,
      "learning_rate": 1.4627083333333333e-05,
      "loss": 0.0016,
      "step": 169790
    },
    {
      "epoch": 5.66,
      "grad_norm": 0.5156658291816711,
      "learning_rate": 1.4625e-05,
      "loss": 0.0017,
      "step": 169800
    },
    {
      "epoch": 5.660333333333333,
      "grad_norm": 0.11453185975551605,
      "learning_rate": 1.4622916666666667e-05,
      "loss": 0.0019,
      "step": 169810
    },
    {
      "epoch": 5.660666666666667,
      "grad_norm": 0.31448104977607727,
      "learning_rate": 1.4620833333333334e-05,
      "loss": 0.0016,
      "step": 169820
    },
    {
      "epoch": 5.661,
      "grad_norm": 0.14275582134723663,
      "learning_rate": 1.4618750000000001e-05,
      "loss": 0.002,
      "step": 169830
    },
    {
      "epoch": 5.661333333333333,
      "grad_norm": 0.057420965284109116,
      "learning_rate": 1.4616666666666667e-05,
      "loss": 0.0019,
      "step": 169840
    },
    {
      "epoch": 5.661666666666667,
      "grad_norm": 0.3987123370170593,
      "learning_rate": 1.4614583333333334e-05,
      "loss": 0.0019,
      "step": 169850
    },
    {
      "epoch": 5.662,
      "grad_norm": 0.20018979907035828,
      "learning_rate": 1.4612500000000001e-05,
      "loss": 0.002,
      "step": 169860
    },
    {
      "epoch": 5.662333333333334,
      "grad_norm": 0.11599992215633392,
      "learning_rate": 1.4610416666666668e-05,
      "loss": 0.0017,
      "step": 169870
    },
    {
      "epoch": 5.6626666666666665,
      "grad_norm": 0.23817317187786102,
      "learning_rate": 1.4608333333333335e-05,
      "loss": 0.0023,
      "step": 169880
    },
    {
      "epoch": 5.663,
      "grad_norm": 0.8397963047027588,
      "learning_rate": 1.4606250000000003e-05,
      "loss": 0.002,
      "step": 169890
    },
    {
      "epoch": 5.663333333333333,
      "grad_norm": 0.22401733696460724,
      "learning_rate": 1.4604166666666666e-05,
      "loss": 0.0018,
      "step": 169900
    },
    {
      "epoch": 5.663666666666667,
      "grad_norm": 0.5997434854507446,
      "learning_rate": 1.4602083333333334e-05,
      "loss": 0.0022,
      "step": 169910
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.11474111676216125,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0012,
      "step": 169920
    },
    {
      "epoch": 5.664333333333333,
      "grad_norm": 0.03217432275414467,
      "learning_rate": 1.4597916666666666e-05,
      "loss": 0.0018,
      "step": 169930
    },
    {
      "epoch": 5.664666666666666,
      "grad_norm": 0.2860240340232849,
      "learning_rate": 1.4595833333333333e-05,
      "loss": 0.0019,
      "step": 169940
    },
    {
      "epoch": 5.665,
      "grad_norm": 0.4266054630279541,
      "learning_rate": 1.459375e-05,
      "loss": 0.0021,
      "step": 169950
    },
    {
      "epoch": 5.665333333333333,
      "grad_norm": 0.11449219286441803,
      "learning_rate": 1.4591666666666668e-05,
      "loss": 0.0016,
      "step": 169960
    },
    {
      "epoch": 5.665666666666667,
      "grad_norm": 0.2576102018356323,
      "learning_rate": 1.4589583333333335e-05,
      "loss": 0.0017,
      "step": 169970
    },
    {
      "epoch": 5.666,
      "grad_norm": 0.13747189939022064,
      "learning_rate": 1.45875e-05,
      "loss": 0.0024,
      "step": 169980
    },
    {
      "epoch": 5.666333333333333,
      "grad_norm": 0.11468590050935745,
      "learning_rate": 1.4585416666666668e-05,
      "loss": 0.0024,
      "step": 169990
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.08575531095266342,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.0016,
      "step": 170000
    },
    {
      "epoch": 5.667,
      "grad_norm": 0.030353713780641556,
      "learning_rate": 1.4581250000000002e-05,
      "loss": 0.0019,
      "step": 170010
    },
    {
      "epoch": 5.667333333333334,
      "grad_norm": 0.14306700229644775,
      "learning_rate": 1.457916666666667e-05,
      "loss": 0.0013,
      "step": 170020
    },
    {
      "epoch": 5.667666666666666,
      "grad_norm": 0.11420458555221558,
      "learning_rate": 1.4577083333333333e-05,
      "loss": 0.0019,
      "step": 170030
    },
    {
      "epoch": 5.668,
      "grad_norm": 0.11422695964574814,
      "learning_rate": 1.4575e-05,
      "loss": 0.0023,
      "step": 170040
    },
    {
      "epoch": 5.668333333333333,
      "grad_norm": 0.2266528457403183,
      "learning_rate": 1.4572916666666667e-05,
      "loss": 0.0025,
      "step": 170050
    },
    {
      "epoch": 5.668666666666667,
      "grad_norm": 0.031146414577960968,
      "learning_rate": 1.4570833333333333e-05,
      "loss": 0.0018,
      "step": 170060
    },
    {
      "epoch": 5.6690000000000005,
      "grad_norm": 0.05831466615200043,
      "learning_rate": 1.456875e-05,
      "loss": 0.0013,
      "step": 170070
    },
    {
      "epoch": 5.669333333333333,
      "grad_norm": 0.057573020458221436,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0013,
      "step": 170080
    },
    {
      "epoch": 5.669666666666666,
      "grad_norm": 0.006439915392547846,
      "learning_rate": 1.4564583333333334e-05,
      "loss": 0.0021,
      "step": 170090
    },
    {
      "epoch": 5.67,
      "grad_norm": 0.013830482959747314,
      "learning_rate": 1.4562500000000002e-05,
      "loss": 0.0023,
      "step": 170100
    },
    {
      "epoch": 5.670333333333334,
      "grad_norm": 0.22830627858638763,
      "learning_rate": 1.4560416666666669e-05,
      "loss": 0.0017,
      "step": 170110
    },
    {
      "epoch": 5.6706666666666665,
      "grad_norm": 0.08576178550720215,
      "learning_rate": 1.4558333333333334e-05,
      "loss": 0.0019,
      "step": 170120
    },
    {
      "epoch": 5.671,
      "grad_norm": 0.08615660667419434,
      "learning_rate": 1.4556250000000001e-05,
      "loss": 0.0013,
      "step": 170130
    },
    {
      "epoch": 5.671333333333333,
      "grad_norm": 0.2672252655029297,
      "learning_rate": 1.4554166666666669e-05,
      "loss": 0.002,
      "step": 170140
    },
    {
      "epoch": 5.671666666666667,
      "grad_norm": 0.09880321472883224,
      "learning_rate": 1.4552083333333332e-05,
      "loss": 0.0018,
      "step": 170150
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.03037947230041027,
      "learning_rate": 1.455e-05,
      "loss": 0.0019,
      "step": 170160
    },
    {
      "epoch": 5.6723333333333334,
      "grad_norm": 0.20518676936626434,
      "learning_rate": 1.4547916666666667e-05,
      "loss": 0.0014,
      "step": 170170
    },
    {
      "epoch": 5.672666666666666,
      "grad_norm": 0.11484912782907486,
      "learning_rate": 1.4545833333333334e-05,
      "loss": 0.0019,
      "step": 170180
    },
    {
      "epoch": 5.673,
      "grad_norm": 0.007947290316224098,
      "learning_rate": 1.4543750000000001e-05,
      "loss": 0.0014,
      "step": 170190
    },
    {
      "epoch": 5.673333333333334,
      "grad_norm": 0.1427386999130249,
      "learning_rate": 1.4541666666666667e-05,
      "loss": 0.0015,
      "step": 170200
    },
    {
      "epoch": 5.673666666666667,
      "grad_norm": 0.285381555557251,
      "learning_rate": 1.4539583333333334e-05,
      "loss": 0.0016,
      "step": 170210
    },
    {
      "epoch": 5.674,
      "grad_norm": 0.20080234110355377,
      "learning_rate": 1.4537500000000001e-05,
      "loss": 0.0017,
      "step": 170220
    },
    {
      "epoch": 5.674333333333333,
      "grad_norm": 0.11434817314147949,
      "learning_rate": 1.4535416666666668e-05,
      "loss": 0.0016,
      "step": 170230
    },
    {
      "epoch": 5.674666666666667,
      "grad_norm": 0.2792978286743164,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0017,
      "step": 170240
    },
    {
      "epoch": 5.675,
      "grad_norm": 0.14277243614196777,
      "learning_rate": 1.4531250000000003e-05,
      "loss": 0.0012,
      "step": 170250
    },
    {
      "epoch": 5.675333333333334,
      "grad_norm": 0.30372658371925354,
      "learning_rate": 1.4529166666666668e-05,
      "loss": 0.0018,
      "step": 170260
    },
    {
      "epoch": 5.675666666666666,
      "grad_norm": 0.09411739557981491,
      "learning_rate": 1.4527083333333334e-05,
      "loss": 0.0021,
      "step": 170270
    },
    {
      "epoch": 5.676,
      "grad_norm": 0.25764691829681396,
      "learning_rate": 1.4524999999999999e-05,
      "loss": 0.0021,
      "step": 170280
    },
    {
      "epoch": 5.676333333333333,
      "grad_norm": 0.11463900655508041,
      "learning_rate": 1.4522916666666666e-05,
      "loss": 0.0019,
      "step": 170290
    },
    {
      "epoch": 5.676666666666667,
      "grad_norm": 0.14299145340919495,
      "learning_rate": 1.4520833333333333e-05,
      "loss": 0.0017,
      "step": 170300
    },
    {
      "epoch": 5.677,
      "grad_norm": 0.1430044174194336,
      "learning_rate": 1.451875e-05,
      "loss": 0.0022,
      "step": 170310
    },
    {
      "epoch": 5.677333333333333,
      "grad_norm": 0.25755512714385986,
      "learning_rate": 1.4516666666666668e-05,
      "loss": 0.0015,
      "step": 170320
    },
    {
      "epoch": 5.677666666666667,
      "grad_norm": 0.009748690761625767,
      "learning_rate": 1.4514583333333335e-05,
      "loss": 0.0015,
      "step": 170330
    },
    {
      "epoch": 5.678,
      "grad_norm": 0.057771921157836914,
      "learning_rate": 1.45125e-05,
      "loss": 0.0017,
      "step": 170340
    },
    {
      "epoch": 5.678333333333334,
      "grad_norm": 0.09869364649057388,
      "learning_rate": 1.4510416666666668e-05,
      "loss": 0.0028,
      "step": 170350
    },
    {
      "epoch": 5.6786666666666665,
      "grad_norm": 0.062084928154945374,
      "learning_rate": 1.4508333333333335e-05,
      "loss": 0.0015,
      "step": 170360
    },
    {
      "epoch": 5.679,
      "grad_norm": 0.5169761776924133,
      "learning_rate": 1.4506250000000002e-05,
      "loss": 0.0014,
      "step": 170370
    },
    {
      "epoch": 5.679333333333333,
      "grad_norm": 0.17145231366157532,
      "learning_rate": 1.4504166666666669e-05,
      "loss": 0.0013,
      "step": 170380
    },
    {
      "epoch": 5.679666666666667,
      "grad_norm": 0.11501538753509521,
      "learning_rate": 1.4502083333333333e-05,
      "loss": 0.0012,
      "step": 170390
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.08567298203706741,
      "learning_rate": 1.45e-05,
      "loss": 0.0021,
      "step": 170400
    },
    {
      "epoch": 5.6803333333333335,
      "grad_norm": 0.08547084778547287,
      "learning_rate": 1.4497916666666667e-05,
      "loss": 0.0016,
      "step": 170410
    },
    {
      "epoch": 5.680666666666666,
      "grad_norm": 0.05785233527421951,
      "learning_rate": 1.4495833333333333e-05,
      "loss": 0.001,
      "step": 170420
    },
    {
      "epoch": 5.681,
      "grad_norm": 0.3150666356086731,
      "learning_rate": 1.449375e-05,
      "loss": 0.0015,
      "step": 170430
    },
    {
      "epoch": 5.681333333333333,
      "grad_norm": 0.11464279890060425,
      "learning_rate": 1.4491666666666667e-05,
      "loss": 0.0015,
      "step": 170440
    },
    {
      "epoch": 5.681666666666667,
      "grad_norm": 0.14306825399398804,
      "learning_rate": 1.4489583333333334e-05,
      "loss": 0.0021,
      "step": 170450
    },
    {
      "epoch": 5.682,
      "grad_norm": 0.25713837146759033,
      "learning_rate": 1.4487500000000001e-05,
      "loss": 0.0018,
      "step": 170460
    },
    {
      "epoch": 5.682333333333333,
      "grad_norm": 0.11452098935842514,
      "learning_rate": 1.4485416666666669e-05,
      "loss": 0.0018,
      "step": 170470
    },
    {
      "epoch": 5.682666666666667,
      "grad_norm": 0.08654513955116272,
      "learning_rate": 1.4483333333333334e-05,
      "loss": 0.0017,
      "step": 170480
    },
    {
      "epoch": 5.683,
      "grad_norm": 0.11483839154243469,
      "learning_rate": 1.4481250000000001e-05,
      "loss": 0.0024,
      "step": 170490
    },
    {
      "epoch": 5.683333333333334,
      "grad_norm": 0.08619540184736252,
      "learning_rate": 1.4479166666666669e-05,
      "loss": 0.0019,
      "step": 170500
    },
    {
      "epoch": 5.683666666666666,
      "grad_norm": 0.0572977140545845,
      "learning_rate": 1.4477083333333332e-05,
      "loss": 0.0026,
      "step": 170510
    },
    {
      "epoch": 5.684,
      "grad_norm": 0.05777370184659958,
      "learning_rate": 1.4475e-05,
      "loss": 0.0012,
      "step": 170520
    },
    {
      "epoch": 5.684333333333333,
      "grad_norm": 0.08656129240989685,
      "learning_rate": 1.4472916666666667e-05,
      "loss": 0.002,
      "step": 170530
    },
    {
      "epoch": 5.684666666666667,
      "grad_norm": 0.2857306897640228,
      "learning_rate": 1.4470833333333334e-05,
      "loss": 0.0021,
      "step": 170540
    },
    {
      "epoch": 5.6850000000000005,
      "grad_norm": 0.1427365243434906,
      "learning_rate": 1.4468750000000001e-05,
      "loss": 0.0015,
      "step": 170550
    },
    {
      "epoch": 5.685333333333333,
      "grad_norm": 0.058245301246643066,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0019,
      "step": 170560
    },
    {
      "epoch": 5.685666666666666,
      "grad_norm": 0.04103950411081314,
      "learning_rate": 1.4464583333333334e-05,
      "loss": 0.0022,
      "step": 170570
    },
    {
      "epoch": 5.686,
      "grad_norm": 0.11537288874387741,
      "learning_rate": 1.4462500000000001e-05,
      "loss": 0.0016,
      "step": 170580
    },
    {
      "epoch": 5.686333333333334,
      "grad_norm": 0.42845624685287476,
      "learning_rate": 1.4460416666666668e-05,
      "loss": 0.0014,
      "step": 170590
    },
    {
      "epoch": 5.6866666666666665,
      "grad_norm": 0.008462850004434586,
      "learning_rate": 1.4458333333333335e-05,
      "loss": 0.0019,
      "step": 170600
    },
    {
      "epoch": 5.687,
      "grad_norm": 0.14080274105072021,
      "learning_rate": 1.4456250000000002e-05,
      "loss": 0.0017,
      "step": 170610
    },
    {
      "epoch": 5.687333333333333,
      "grad_norm": 0.18296225368976593,
      "learning_rate": 1.4454166666666668e-05,
      "loss": 0.0016,
      "step": 170620
    },
    {
      "epoch": 5.687666666666667,
      "grad_norm": 0.030841553583741188,
      "learning_rate": 1.4452083333333333e-05,
      "loss": 0.0017,
      "step": 170630
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.05874839797616005,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 0.0016,
      "step": 170640
    },
    {
      "epoch": 5.6883333333333335,
      "grad_norm": 0.2558242082595825,
      "learning_rate": 1.4447916666666666e-05,
      "loss": 0.0017,
      "step": 170650
    },
    {
      "epoch": 5.688666666666666,
      "grad_norm": 0.22235806286334991,
      "learning_rate": 1.4445833333333333e-05,
      "loss": 0.0024,
      "step": 170660
    },
    {
      "epoch": 5.689,
      "grad_norm": 0.17210353910923004,
      "learning_rate": 1.444375e-05,
      "loss": 0.0016,
      "step": 170670
    },
    {
      "epoch": 5.689333333333334,
      "grad_norm": 0.08599842339754105,
      "learning_rate": 1.4441666666666668e-05,
      "loss": 0.0021,
      "step": 170680
    },
    {
      "epoch": 5.689666666666667,
      "grad_norm": 0.1709873527288437,
      "learning_rate": 1.4439583333333335e-05,
      "loss": 0.0027,
      "step": 170690
    },
    {
      "epoch": 5.6899999999999995,
      "grad_norm": 0.38460099697113037,
      "learning_rate": 1.44375e-05,
      "loss": 0.0017,
      "step": 170700
    },
    {
      "epoch": 5.690333333333333,
      "grad_norm": 0.08561621606349945,
      "learning_rate": 1.4435416666666667e-05,
      "loss": 0.0022,
      "step": 170710
    },
    {
      "epoch": 5.690666666666667,
      "grad_norm": 0.0582638680934906,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0013,
      "step": 170720
    },
    {
      "epoch": 5.691,
      "grad_norm": 0.4279923141002655,
      "learning_rate": 1.4431250000000002e-05,
      "loss": 0.0013,
      "step": 170730
    },
    {
      "epoch": 5.691333333333334,
      "grad_norm": 0.2569590210914612,
      "learning_rate": 1.4429166666666669e-05,
      "loss": 0.0018,
      "step": 170740
    },
    {
      "epoch": 5.691666666666666,
      "grad_norm": 0.19992908835411072,
      "learning_rate": 1.4427083333333333e-05,
      "loss": 0.0029,
      "step": 170750
    },
    {
      "epoch": 5.692,
      "grad_norm": 0.14258217811584473,
      "learning_rate": 1.4425e-05,
      "loss": 0.0018,
      "step": 170760
    },
    {
      "epoch": 5.692333333333333,
      "grad_norm": 0.17161835730075836,
      "learning_rate": 1.4422916666666667e-05,
      "loss": 0.0021,
      "step": 170770
    },
    {
      "epoch": 5.692666666666667,
      "grad_norm": 0.08709323406219482,
      "learning_rate": 1.4420833333333333e-05,
      "loss": 0.0015,
      "step": 170780
    },
    {
      "epoch": 5.693,
      "grad_norm": 0.02947629615664482,
      "learning_rate": 1.441875e-05,
      "loss": 0.002,
      "step": 170790
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 0.11513781547546387,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.0023,
      "step": 170800
    },
    {
      "epoch": 5.693666666666667,
      "grad_norm": 0.22889064252376556,
      "learning_rate": 1.4414583333333334e-05,
      "loss": 0.0018,
      "step": 170810
    },
    {
      "epoch": 5.694,
      "grad_norm": 0.029765300452709198,
      "learning_rate": 1.4412500000000001e-05,
      "loss": 0.0027,
      "step": 170820
    },
    {
      "epoch": 5.694333333333334,
      "grad_norm": 0.20044724643230438,
      "learning_rate": 1.4410416666666669e-05,
      "loss": 0.0017,
      "step": 170830
    },
    {
      "epoch": 5.6946666666666665,
      "grad_norm": 0.029194405302405357,
      "learning_rate": 1.4408333333333334e-05,
      "loss": 0.0018,
      "step": 170840
    },
    {
      "epoch": 5.695,
      "grad_norm": 0.37105491757392883,
      "learning_rate": 1.4406250000000001e-05,
      "loss": 0.0018,
      "step": 170850
    },
    {
      "epoch": 5.695333333333333,
      "grad_norm": 0.14343656599521637,
      "learning_rate": 1.4404166666666668e-05,
      "loss": 0.0016,
      "step": 170860
    },
    {
      "epoch": 5.695666666666667,
      "grad_norm": 0.12249037623405457,
      "learning_rate": 1.4402083333333332e-05,
      "loss": 0.0016,
      "step": 170870
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.05765761435031891,
      "learning_rate": 1.44e-05,
      "loss": 0.0022,
      "step": 170880
    },
    {
      "epoch": 5.6963333333333335,
      "grad_norm": 0.17870305478572845,
      "learning_rate": 1.4397916666666667e-05,
      "loss": 0.0025,
      "step": 170890
    },
    {
      "epoch": 5.696666666666666,
      "grad_norm": 0.5582485198974609,
      "learning_rate": 1.4395833333333334e-05,
      "loss": 0.0022,
      "step": 170900
    },
    {
      "epoch": 5.697,
      "grad_norm": 0.029883727431297302,
      "learning_rate": 1.4393750000000001e-05,
      "loss": 0.0016,
      "step": 170910
    },
    {
      "epoch": 5.697333333333333,
      "grad_norm": 0.03365268185734749,
      "learning_rate": 1.4391666666666666e-05,
      "loss": 0.0017,
      "step": 170920
    },
    {
      "epoch": 5.697666666666667,
      "grad_norm": 0.31422045826911926,
      "learning_rate": 1.4389583333333334e-05,
      "loss": 0.0012,
      "step": 170930
    },
    {
      "epoch": 5.698,
      "grad_norm": 0.14269109070301056,
      "learning_rate": 1.43875e-05,
      "loss": 0.0017,
      "step": 170940
    },
    {
      "epoch": 5.698333333333333,
      "grad_norm": 0.11505170166492462,
      "learning_rate": 1.4385416666666668e-05,
      "loss": 0.0015,
      "step": 170950
    },
    {
      "epoch": 5.698666666666667,
      "grad_norm": 0.11425697058439255,
      "learning_rate": 1.4383333333333335e-05,
      "loss": 0.0014,
      "step": 170960
    },
    {
      "epoch": 5.699,
      "grad_norm": 0.11514589935541153,
      "learning_rate": 1.4381250000000002e-05,
      "loss": 0.0019,
      "step": 170970
    },
    {
      "epoch": 5.699333333333334,
      "grad_norm": 0.009877758100628853,
      "learning_rate": 1.4379166666666668e-05,
      "loss": 0.0013,
      "step": 170980
    },
    {
      "epoch": 5.699666666666666,
      "grad_norm": 0.08597402274608612,
      "learning_rate": 1.4377083333333333e-05,
      "loss": 0.0021,
      "step": 170990
    },
    {
      "epoch": 5.7,
      "grad_norm": 0.006095990538597107,
      "learning_rate": 1.4374999999999999e-05,
      "loss": 0.0019,
      "step": 171000
    },
    {
      "epoch": 5.700333333333333,
      "grad_norm": 0.17152808606624603,
      "learning_rate": 1.4372916666666666e-05,
      "loss": 0.0015,
      "step": 171010
    },
    {
      "epoch": 5.700666666666667,
      "grad_norm": 0.012341084890067577,
      "learning_rate": 1.4370833333333333e-05,
      "loss": 0.0015,
      "step": 171020
    },
    {
      "epoch": 5.701,
      "grad_norm": 0.20731808245182037,
      "learning_rate": 1.436875e-05,
      "loss": 0.0025,
      "step": 171030
    },
    {
      "epoch": 5.701333333333333,
      "grad_norm": 0.08683940023183823,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0017,
      "step": 171040
    },
    {
      "epoch": 5.701666666666666,
      "grad_norm": 0.0865551084280014,
      "learning_rate": 1.4364583333333335e-05,
      "loss": 0.0017,
      "step": 171050
    },
    {
      "epoch": 5.702,
      "grad_norm": 0.19980597496032715,
      "learning_rate": 1.43625e-05,
      "loss": 0.0024,
      "step": 171060
    },
    {
      "epoch": 5.702333333333334,
      "grad_norm": 0.17178426682949066,
      "learning_rate": 1.4360416666666667e-05,
      "loss": 0.0029,
      "step": 171070
    },
    {
      "epoch": 5.7026666666666666,
      "grad_norm": 0.08632107079029083,
      "learning_rate": 1.4358333333333334e-05,
      "loss": 0.0016,
      "step": 171080
    },
    {
      "epoch": 5.703,
      "grad_norm": 0.20054499804973602,
      "learning_rate": 1.4356250000000002e-05,
      "loss": 0.0025,
      "step": 171090
    },
    {
      "epoch": 5.703333333333333,
      "grad_norm": 0.1910647451877594,
      "learning_rate": 1.4354166666666669e-05,
      "loss": 0.0026,
      "step": 171100
    },
    {
      "epoch": 5.703666666666667,
      "grad_norm": 0.4733877182006836,
      "learning_rate": 1.4352083333333333e-05,
      "loss": 0.0017,
      "step": 171110
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.1433088183403015,
      "learning_rate": 1.435e-05,
      "loss": 0.002,
      "step": 171120
    },
    {
      "epoch": 5.7043333333333335,
      "grad_norm": 0.30671900510787964,
      "learning_rate": 1.4347916666666667e-05,
      "loss": 0.0017,
      "step": 171130
    },
    {
      "epoch": 5.704666666666666,
      "grad_norm": 0.11452183127403259,
      "learning_rate": 1.4345833333333334e-05,
      "loss": 0.0019,
      "step": 171140
    },
    {
      "epoch": 5.705,
      "grad_norm": 0.17206613719463348,
      "learning_rate": 1.434375e-05,
      "loss": 0.0024,
      "step": 171150
    },
    {
      "epoch": 5.705333333333334,
      "grad_norm": 0.2575840651988983,
      "learning_rate": 1.4341666666666667e-05,
      "loss": 0.002,
      "step": 171160
    },
    {
      "epoch": 5.705666666666667,
      "grad_norm": 0.5080611109733582,
      "learning_rate": 1.4339583333333334e-05,
      "loss": 0.0013,
      "step": 171170
    },
    {
      "epoch": 5.7059999999999995,
      "grad_norm": 0.08757118135690689,
      "learning_rate": 1.4337500000000001e-05,
      "loss": 0.0014,
      "step": 171180
    },
    {
      "epoch": 5.706333333333333,
      "grad_norm": 0.11446230858564377,
      "learning_rate": 1.4335416666666668e-05,
      "loss": 0.002,
      "step": 171190
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.03407657891511917,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0013,
      "step": 171200
    },
    {
      "epoch": 5.707,
      "grad_norm": 0.2287454754114151,
      "learning_rate": 1.4331250000000001e-05,
      "loss": 0.0021,
      "step": 171210
    },
    {
      "epoch": 5.707333333333334,
      "grad_norm": 0.2497655153274536,
      "learning_rate": 1.4329166666666668e-05,
      "loss": 0.0023,
      "step": 171220
    },
    {
      "epoch": 5.707666666666666,
      "grad_norm": 0.022130152210593224,
      "learning_rate": 1.4327083333333332e-05,
      "loss": 0.0016,
      "step": 171230
    },
    {
      "epoch": 5.708,
      "grad_norm": 0.008494251407682896,
      "learning_rate": 1.4325e-05,
      "loss": 0.0016,
      "step": 171240
    },
    {
      "epoch": 5.708333333333333,
      "grad_norm": 0.34307861328125,
      "learning_rate": 1.4322916666666666e-05,
      "loss": 0.0016,
      "step": 171250
    },
    {
      "epoch": 5.708666666666667,
      "grad_norm": 0.3429652154445648,
      "learning_rate": 1.4320833333333334e-05,
      "loss": 0.0012,
      "step": 171260
    },
    {
      "epoch": 5.709,
      "grad_norm": 0.19979995489120483,
      "learning_rate": 1.431875e-05,
      "loss": 0.0016,
      "step": 171270
    },
    {
      "epoch": 5.709333333333333,
      "grad_norm": 0.028892695903778076,
      "learning_rate": 1.4316666666666668e-05,
      "loss": 0.002,
      "step": 171280
    },
    {
      "epoch": 5.709666666666667,
      "grad_norm": 0.0577632412314415,
      "learning_rate": 1.4314583333333333e-05,
      "loss": 0.0016,
      "step": 171290
    },
    {
      "epoch": 5.71,
      "grad_norm": 0.3141043782234192,
      "learning_rate": 1.43125e-05,
      "loss": 0.0015,
      "step": 171300
    },
    {
      "epoch": 5.710333333333334,
      "grad_norm": 0.029501471668481827,
      "learning_rate": 1.4310416666666668e-05,
      "loss": 0.0018,
      "step": 171310
    },
    {
      "epoch": 5.710666666666667,
      "grad_norm": 0.11450038105249405,
      "learning_rate": 1.4308333333333335e-05,
      "loss": 0.0021,
      "step": 171320
    },
    {
      "epoch": 5.711,
      "grad_norm": 0.030085911974310875,
      "learning_rate": 1.4306250000000002e-05,
      "loss": 0.0014,
      "step": 171330
    },
    {
      "epoch": 5.711333333333333,
      "grad_norm": 0.19393977522850037,
      "learning_rate": 1.430416666666667e-05,
      "loss": 0.0018,
      "step": 171340
    },
    {
      "epoch": 5.711666666666667,
      "grad_norm": 0.22914081811904907,
      "learning_rate": 1.4302083333333335e-05,
      "loss": 0.0022,
      "step": 171350
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.172231525182724,
      "learning_rate": 1.43e-05,
      "loss": 0.0014,
      "step": 171360
    },
    {
      "epoch": 5.7123333333333335,
      "grad_norm": 0.5429276823997498,
      "learning_rate": 1.4297916666666666e-05,
      "loss": 0.0016,
      "step": 171370
    },
    {
      "epoch": 5.712666666666666,
      "grad_norm": 0.11461193859577179,
      "learning_rate": 1.4295833333333333e-05,
      "loss": 0.0019,
      "step": 171380
    },
    {
      "epoch": 5.713,
      "grad_norm": 0.057576071470975876,
      "learning_rate": 1.429375e-05,
      "loss": 0.0022,
      "step": 171390
    },
    {
      "epoch": 5.713333333333333,
      "grad_norm": 0.05985499918460846,
      "learning_rate": 1.4291666666666667e-05,
      "loss": 0.0017,
      "step": 171400
    },
    {
      "epoch": 5.713666666666667,
      "grad_norm": 0.1725839227437973,
      "learning_rate": 1.4289583333333335e-05,
      "loss": 0.0018,
      "step": 171410
    },
    {
      "epoch": 5.714,
      "grad_norm": 0.14267699420452118,
      "learning_rate": 1.4287500000000002e-05,
      "loss": 0.0021,
      "step": 171420
    },
    {
      "epoch": 5.714333333333333,
      "grad_norm": 0.057572267949581146,
      "learning_rate": 1.4285416666666667e-05,
      "loss": 0.0019,
      "step": 171430
    },
    {
      "epoch": 5.714666666666667,
      "grad_norm": 0.1999397575855255,
      "learning_rate": 1.4283333333333334e-05,
      "loss": 0.0013,
      "step": 171440
    },
    {
      "epoch": 5.715,
      "grad_norm": 0.4042009115219116,
      "learning_rate": 1.4281250000000002e-05,
      "loss": 0.0023,
      "step": 171450
    },
    {
      "epoch": 5.715333333333334,
      "grad_norm": 0.2003031224012375,
      "learning_rate": 1.4279166666666669e-05,
      "loss": 0.002,
      "step": 171460
    },
    {
      "epoch": 5.7156666666666665,
      "grad_norm": 0.057992469519376755,
      "learning_rate": 1.4277083333333336e-05,
      "loss": 0.0019,
      "step": 171470
    },
    {
      "epoch": 5.716,
      "grad_norm": 0.22864621877670288,
      "learning_rate": 1.4275e-05,
      "loss": 0.0012,
      "step": 171480
    },
    {
      "epoch": 5.716333333333333,
      "grad_norm": 0.3428335189819336,
      "learning_rate": 1.4272916666666667e-05,
      "loss": 0.0024,
      "step": 171490
    },
    {
      "epoch": 5.716666666666667,
      "grad_norm": 0.22146379947662354,
      "learning_rate": 1.4270833333333334e-05,
      "loss": 0.0023,
      "step": 171500
    },
    {
      "epoch": 5.717,
      "grad_norm": 0.08702288568019867,
      "learning_rate": 1.426875e-05,
      "loss": 0.0017,
      "step": 171510
    },
    {
      "epoch": 5.717333333333333,
      "grad_norm": 0.30408090353012085,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.002,
      "step": 171520
    },
    {
      "epoch": 5.717666666666666,
      "grad_norm": 0.009339374490082264,
      "learning_rate": 1.4264583333333334e-05,
      "loss": 0.0024,
      "step": 171530
    },
    {
      "epoch": 5.718,
      "grad_norm": 0.02936452068388462,
      "learning_rate": 1.4262500000000001e-05,
      "loss": 0.0016,
      "step": 171540
    },
    {
      "epoch": 5.718333333333334,
      "grad_norm": 0.08589798957109451,
      "learning_rate": 1.4260416666666668e-05,
      "loss": 0.0016,
      "step": 171550
    },
    {
      "epoch": 5.718666666666667,
      "grad_norm": 0.14314725995063782,
      "learning_rate": 1.4258333333333335e-05,
      "loss": 0.0013,
      "step": 171560
    },
    {
      "epoch": 5.719,
      "grad_norm": 0.08608632534742355,
      "learning_rate": 1.4256250000000001e-05,
      "loss": 0.0016,
      "step": 171570
    },
    {
      "epoch": 5.719333333333333,
      "grad_norm": 0.047293901443481445,
      "learning_rate": 1.4254166666666668e-05,
      "loss": 0.0025,
      "step": 171580
    },
    {
      "epoch": 5.719666666666667,
      "grad_norm": 0.11504165083169937,
      "learning_rate": 1.4252083333333335e-05,
      "loss": 0.0019,
      "step": 171590
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.3147999048233032,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.0017,
      "step": 171600
    },
    {
      "epoch": 5.7203333333333335,
      "grad_norm": 0.31418049335479736,
      "learning_rate": 1.4247916666666666e-05,
      "loss": 0.0019,
      "step": 171610
    },
    {
      "epoch": 5.720666666666666,
      "grad_norm": 0.029383817687630653,
      "learning_rate": 1.4245833333333333e-05,
      "loss": 0.0018,
      "step": 171620
    },
    {
      "epoch": 5.721,
      "grad_norm": 0.17196951806545258,
      "learning_rate": 1.424375e-05,
      "loss": 0.0027,
      "step": 171630
    },
    {
      "epoch": 5.721333333333334,
      "grad_norm": 0.029557978734374046,
      "learning_rate": 1.4241666666666668e-05,
      "loss": 0.0016,
      "step": 171640
    },
    {
      "epoch": 5.721666666666667,
      "grad_norm": 0.22894980013370514,
      "learning_rate": 1.4239583333333333e-05,
      "loss": 0.0016,
      "step": 171650
    },
    {
      "epoch": 5.7219999999999995,
      "grad_norm": 0.11424508690834045,
      "learning_rate": 1.42375e-05,
      "loss": 0.0018,
      "step": 171660
    },
    {
      "epoch": 5.722333333333333,
      "grad_norm": 0.057366419583559036,
      "learning_rate": 1.4235416666666668e-05,
      "loss": 0.0015,
      "step": 171670
    },
    {
      "epoch": 5.722666666666667,
      "grad_norm": 0.2008851170539856,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.002,
      "step": 171680
    },
    {
      "epoch": 5.723,
      "grad_norm": 0.11404318362474442,
      "learning_rate": 1.4231250000000002e-05,
      "loss": 0.0024,
      "step": 171690
    },
    {
      "epoch": 5.723333333333334,
      "grad_norm": 0.11537527292966843,
      "learning_rate": 1.422916666666667e-05,
      "loss": 0.0018,
      "step": 171700
    },
    {
      "epoch": 5.7236666666666665,
      "grad_norm": 0.05931490287184715,
      "learning_rate": 1.4227083333333335e-05,
      "loss": 0.0016,
      "step": 171710
    },
    {
      "epoch": 5.724,
      "grad_norm": 0.400473952293396,
      "learning_rate": 1.4225e-05,
      "loss": 0.0018,
      "step": 171720
    },
    {
      "epoch": 5.724333333333333,
      "grad_norm": 0.010254847817122936,
      "learning_rate": 1.4222916666666666e-05,
      "loss": 0.0017,
      "step": 171730
    },
    {
      "epoch": 5.724666666666667,
      "grad_norm": 0.17148305475711823,
      "learning_rate": 1.4220833333333333e-05,
      "loss": 0.0021,
      "step": 171740
    },
    {
      "epoch": 5.725,
      "grad_norm": 0.283563494682312,
      "learning_rate": 1.421875e-05,
      "loss": 0.0029,
      "step": 171750
    },
    {
      "epoch": 5.725333333333333,
      "grad_norm": 0.08584635704755783,
      "learning_rate": 1.4216666666666667e-05,
      "loss": 0.0015,
      "step": 171760
    },
    {
      "epoch": 5.725666666666667,
      "grad_norm": 0.20029829442501068,
      "learning_rate": 1.4214583333333334e-05,
      "loss": 0.0024,
      "step": 171770
    },
    {
      "epoch": 5.726,
      "grad_norm": 0.31481999158859253,
      "learning_rate": 1.4212500000000002e-05,
      "loss": 0.0023,
      "step": 171780
    },
    {
      "epoch": 5.726333333333334,
      "grad_norm": 0.14349906146526337,
      "learning_rate": 1.4210416666666667e-05,
      "loss": 0.0018,
      "step": 171790
    },
    {
      "epoch": 5.726666666666667,
      "grad_norm": 0.22885958850383759,
      "learning_rate": 1.4208333333333334e-05,
      "loss": 0.0025,
      "step": 171800
    },
    {
      "epoch": 5.727,
      "grad_norm": 0.10322371125221252,
      "learning_rate": 1.4206250000000001e-05,
      "loss": 0.0017,
      "step": 171810
    },
    {
      "epoch": 5.727333333333333,
      "grad_norm": 0.08640950173139572,
      "learning_rate": 1.4204166666666669e-05,
      "loss": 0.0023,
      "step": 171820
    },
    {
      "epoch": 5.727666666666667,
      "grad_norm": 0.059823840856552124,
      "learning_rate": 1.4202083333333336e-05,
      "loss": 0.0015,
      "step": 171830
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.1716848760843277,
      "learning_rate": 1.42e-05,
      "loss": 0.0017,
      "step": 171840
    },
    {
      "epoch": 5.7283333333333335,
      "grad_norm": 0.3423973321914673,
      "learning_rate": 1.4197916666666667e-05,
      "loss": 0.0017,
      "step": 171850
    },
    {
      "epoch": 5.728666666666666,
      "grad_norm": 0.4851148724555969,
      "learning_rate": 1.4195833333333334e-05,
      "loss": 0.0015,
      "step": 171860
    },
    {
      "epoch": 5.729,
      "grad_norm": 0.127974733710289,
      "learning_rate": 1.419375e-05,
      "loss": 0.0018,
      "step": 171870
    },
    {
      "epoch": 5.729333333333333,
      "grad_norm": 0.08663547039031982,
      "learning_rate": 1.4191666666666667e-05,
      "loss": 0.0024,
      "step": 171880
    },
    {
      "epoch": 5.729666666666667,
      "grad_norm": 0.1144668310880661,
      "learning_rate": 1.4189583333333334e-05,
      "loss": 0.0014,
      "step": 171890
    },
    {
      "epoch": 5.73,
      "grad_norm": 0.3145151734352112,
      "learning_rate": 1.4187500000000001e-05,
      "loss": 0.0018,
      "step": 171900
    },
    {
      "epoch": 5.730333333333333,
      "grad_norm": 0.03402981907129288,
      "learning_rate": 1.4185416666666668e-05,
      "loss": 0.0016,
      "step": 171910
    },
    {
      "epoch": 5.730666666666667,
      "grad_norm": 0.34297242760658264,
      "learning_rate": 1.4183333333333335e-05,
      "loss": 0.0019,
      "step": 171920
    },
    {
      "epoch": 5.731,
      "grad_norm": 0.14368100464344025,
      "learning_rate": 1.418125e-05,
      "loss": 0.0014,
      "step": 171930
    },
    {
      "epoch": 5.731333333333334,
      "grad_norm": 0.029008809477090836,
      "learning_rate": 1.4179166666666668e-05,
      "loss": 0.0015,
      "step": 171940
    },
    {
      "epoch": 5.7316666666666665,
      "grad_norm": 0.14314089715480804,
      "learning_rate": 1.4177083333333335e-05,
      "loss": 0.0023,
      "step": 171950
    },
    {
      "epoch": 5.732,
      "grad_norm": 0.17167292535305023,
      "learning_rate": 1.4174999999999999e-05,
      "loss": 0.0026,
      "step": 171960
    },
    {
      "epoch": 5.732333333333333,
      "grad_norm": 0.2855897843837738,
      "learning_rate": 1.4172916666666666e-05,
      "loss": 0.0021,
      "step": 171970
    },
    {
      "epoch": 5.732666666666667,
      "grad_norm": 0.1718544065952301,
      "learning_rate": 1.4170833333333333e-05,
      "loss": 0.0016,
      "step": 171980
    },
    {
      "epoch": 5.733,
      "grad_norm": 0.05810084566473961,
      "learning_rate": 1.416875e-05,
      "loss": 0.002,
      "step": 171990
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.08619409799575806,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0011,
      "step": 172000
    },
    {
      "epoch": 5.733666666666666,
      "grad_norm": 0.4002218544483185,
      "learning_rate": 1.4164583333333333e-05,
      "loss": 0.0017,
      "step": 172010
    },
    {
      "epoch": 5.734,
      "grad_norm": 0.3432898223400116,
      "learning_rate": 1.41625e-05,
      "loss": 0.0019,
      "step": 172020
    },
    {
      "epoch": 5.734333333333334,
      "grad_norm": 0.06700243055820465,
      "learning_rate": 1.4160416666666668e-05,
      "loss": 0.0028,
      "step": 172030
    },
    {
      "epoch": 5.734666666666667,
      "grad_norm": 0.14293888211250305,
      "learning_rate": 1.4158333333333335e-05,
      "loss": 0.0027,
      "step": 172040
    },
    {
      "epoch": 5.735,
      "grad_norm": 0.014912689104676247,
      "learning_rate": 1.4156250000000002e-05,
      "loss": 0.0016,
      "step": 172050
    },
    {
      "epoch": 5.735333333333333,
      "grad_norm": 0.060165125876665115,
      "learning_rate": 1.4154166666666669e-05,
      "loss": 0.0027,
      "step": 172060
    },
    {
      "epoch": 5.735666666666667,
      "grad_norm": 0.011107907630503178,
      "learning_rate": 1.4152083333333335e-05,
      "loss": 0.0013,
      "step": 172070
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.08692418783903122,
      "learning_rate": 1.415e-05,
      "loss": 0.0024,
      "step": 172080
    },
    {
      "epoch": 5.7363333333333335,
      "grad_norm": 0.17140428721904755,
      "learning_rate": 1.4147916666666666e-05,
      "loss": 0.0014,
      "step": 172090
    },
    {
      "epoch": 5.736666666666666,
      "grad_norm": 0.3714667856693268,
      "learning_rate": 1.4145833333333333e-05,
      "loss": 0.0013,
      "step": 172100
    },
    {
      "epoch": 5.737,
      "grad_norm": 0.02984989434480667,
      "learning_rate": 1.414375e-05,
      "loss": 0.0028,
      "step": 172110
    },
    {
      "epoch": 5.737333333333333,
      "grad_norm": 0.08653469383716583,
      "learning_rate": 1.4141666666666667e-05,
      "loss": 0.0014,
      "step": 172120
    },
    {
      "epoch": 5.737666666666667,
      "grad_norm": 0.4282819628715515,
      "learning_rate": 1.4139583333333334e-05,
      "loss": 0.0015,
      "step": 172130
    },
    {
      "epoch": 5.7379999999999995,
      "grad_norm": 0.22841064631938934,
      "learning_rate": 1.4137500000000001e-05,
      "loss": 0.0016,
      "step": 172140
    },
    {
      "epoch": 5.738333333333333,
      "grad_norm": 0.08579153567552567,
      "learning_rate": 1.4135416666666667e-05,
      "loss": 0.0023,
      "step": 172150
    },
    {
      "epoch": 5.738666666666667,
      "grad_norm": 0.08590556681156158,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0019,
      "step": 172160
    },
    {
      "epoch": 5.739,
      "grad_norm": 0.14323659241199493,
      "learning_rate": 1.4131250000000001e-05,
      "loss": 0.0014,
      "step": 172170
    },
    {
      "epoch": 5.739333333333334,
      "grad_norm": 0.013984779827296734,
      "learning_rate": 1.4129166666666668e-05,
      "loss": 0.0018,
      "step": 172180
    },
    {
      "epoch": 5.7396666666666665,
      "grad_norm": 0.057464562356472015,
      "learning_rate": 1.4127083333333336e-05,
      "loss": 0.0016,
      "step": 172190
    },
    {
      "epoch": 5.74,
      "grad_norm": 0.20933067798614502,
      "learning_rate": 1.4125e-05,
      "loss": 0.0014,
      "step": 172200
    },
    {
      "epoch": 5.740333333333333,
      "grad_norm": 0.4436767101287842,
      "learning_rate": 1.4122916666666667e-05,
      "loss": 0.0016,
      "step": 172210
    },
    {
      "epoch": 5.740666666666667,
      "grad_norm": 0.3421906530857086,
      "learning_rate": 1.4120833333333334e-05,
      "loss": 0.002,
      "step": 172220
    },
    {
      "epoch": 5.741,
      "grad_norm": 0.0857030525803566,
      "learning_rate": 1.411875e-05,
      "loss": 0.0016,
      "step": 172230
    },
    {
      "epoch": 5.741333333333333,
      "grad_norm": 0.08551016449928284,
      "learning_rate": 1.4116666666666666e-05,
      "loss": 0.0012,
      "step": 172240
    },
    {
      "epoch": 5.741666666666667,
      "grad_norm": 0.22868436574935913,
      "learning_rate": 1.4114583333333334e-05,
      "loss": 0.0018,
      "step": 172250
    },
    {
      "epoch": 5.742,
      "grad_norm": 0.4860459864139557,
      "learning_rate": 1.41125e-05,
      "loss": 0.0025,
      "step": 172260
    },
    {
      "epoch": 5.742333333333333,
      "grad_norm": 0.1141941174864769,
      "learning_rate": 1.4110416666666668e-05,
      "loss": 0.0013,
      "step": 172270
    },
    {
      "epoch": 5.742666666666667,
      "grad_norm": 0.3153468072414398,
      "learning_rate": 1.4108333333333335e-05,
      "loss": 0.0015,
      "step": 172280
    },
    {
      "epoch": 5.743,
      "grad_norm": 0.14368538558483124,
      "learning_rate": 1.410625e-05,
      "loss": 0.0016,
      "step": 172290
    },
    {
      "epoch": 5.743333333333333,
      "grad_norm": 0.25726377964019775,
      "learning_rate": 1.4104166666666668e-05,
      "loss": 0.002,
      "step": 172300
    },
    {
      "epoch": 5.743666666666667,
      "grad_norm": 0.4681622087955475,
      "learning_rate": 1.4102083333333335e-05,
      "loss": 0.0014,
      "step": 172310
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.2852975130081177,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0016,
      "step": 172320
    },
    {
      "epoch": 5.7443333333333335,
      "grad_norm": 0.14264161884784698,
      "learning_rate": 1.4097916666666666e-05,
      "loss": 0.0018,
      "step": 172330
    },
    {
      "epoch": 5.744666666666666,
      "grad_norm": 0.005913226865231991,
      "learning_rate": 1.4095833333333333e-05,
      "loss": 0.0019,
      "step": 172340
    },
    {
      "epoch": 5.745,
      "grad_norm": 0.08581823855638504,
      "learning_rate": 1.409375e-05,
      "loss": 0.0025,
      "step": 172350
    },
    {
      "epoch": 5.745333333333333,
      "grad_norm": 0.11551213264465332,
      "learning_rate": 1.4091666666666668e-05,
      "loss": 0.0017,
      "step": 172360
    },
    {
      "epoch": 5.745666666666667,
      "grad_norm": 0.14260688424110413,
      "learning_rate": 1.4089583333333333e-05,
      "loss": 0.0025,
      "step": 172370
    },
    {
      "epoch": 5.746,
      "grad_norm": 0.201120987534523,
      "learning_rate": 1.40875e-05,
      "loss": 0.0016,
      "step": 172380
    },
    {
      "epoch": 5.746333333333333,
      "grad_norm": 0.31416165828704834,
      "learning_rate": 1.4085416666666667e-05,
      "loss": 0.0019,
      "step": 172390
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.11527958512306213,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.0024,
      "step": 172400
    },
    {
      "epoch": 5.747,
      "grad_norm": 0.09828298538923264,
      "learning_rate": 1.4081250000000002e-05,
      "loss": 0.0015,
      "step": 172410
    },
    {
      "epoch": 5.747333333333334,
      "grad_norm": 0.37958040833473206,
      "learning_rate": 1.4079166666666669e-05,
      "loss": 0.0016,
      "step": 172420
    },
    {
      "epoch": 5.7476666666666665,
      "grad_norm": 0.35904550552368164,
      "learning_rate": 1.4077083333333334e-05,
      "loss": 0.0021,
      "step": 172430
    },
    {
      "epoch": 5.748,
      "grad_norm": 0.029729807749390602,
      "learning_rate": 1.4075e-05,
      "loss": 0.0015,
      "step": 172440
    },
    {
      "epoch": 5.748333333333333,
      "grad_norm": 0.11461399495601654,
      "learning_rate": 1.4072916666666665e-05,
      "loss": 0.0019,
      "step": 172450
    },
    {
      "epoch": 5.748666666666667,
      "grad_norm": 0.08615177124738693,
      "learning_rate": 1.4070833333333333e-05,
      "loss": 0.0019,
      "step": 172460
    },
    {
      "epoch": 5.749,
      "grad_norm": 0.3346233069896698,
      "learning_rate": 1.406875e-05,
      "loss": 0.0014,
      "step": 172470
    },
    {
      "epoch": 5.749333333333333,
      "grad_norm": 0.2314523309469223,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0022,
      "step": 172480
    },
    {
      "epoch": 5.749666666666666,
      "grad_norm": 0.20007964968681335,
      "learning_rate": 1.4064583333333334e-05,
      "loss": 0.0018,
      "step": 172490
    },
    {
      "epoch": 5.75,
      "grad_norm": 0.17179426550865173,
      "learning_rate": 1.4062500000000001e-05,
      "loss": 0.0021,
      "step": 172500
    },
    {
      "epoch": 5.750333333333334,
      "grad_norm": 0.14389927685260773,
      "learning_rate": 1.4060416666666667e-05,
      "loss": 0.002,
      "step": 172510
    },
    {
      "epoch": 5.750666666666667,
      "grad_norm": 0.05822372063994408,
      "learning_rate": 1.4058333333333334e-05,
      "loss": 0.0014,
      "step": 172520
    },
    {
      "epoch": 5.751,
      "grad_norm": 0.20043987035751343,
      "learning_rate": 1.4056250000000001e-05,
      "loss": 0.0016,
      "step": 172530
    },
    {
      "epoch": 5.751333333333333,
      "grad_norm": 0.05809103325009346,
      "learning_rate": 1.4054166666666668e-05,
      "loss": 0.0016,
      "step": 172540
    },
    {
      "epoch": 5.751666666666667,
      "grad_norm": 0.033294010907411575,
      "learning_rate": 1.4052083333333336e-05,
      "loss": 0.0021,
      "step": 172550
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.4855290651321411,
      "learning_rate": 1.4050000000000003e-05,
      "loss": 0.0018,
      "step": 172560
    },
    {
      "epoch": 5.7523333333333335,
      "grad_norm": 0.058319345116615295,
      "learning_rate": 1.4047916666666666e-05,
      "loss": 0.0012,
      "step": 172570
    },
    {
      "epoch": 5.752666666666666,
      "grad_norm": 0.33695513010025024,
      "learning_rate": 1.4045833333333334e-05,
      "loss": 0.0017,
      "step": 172580
    },
    {
      "epoch": 5.753,
      "grad_norm": 0.3139531910419464,
      "learning_rate": 1.404375e-05,
      "loss": 0.0017,
      "step": 172590
    },
    {
      "epoch": 5.753333333333333,
      "grad_norm": 0.34394291043281555,
      "learning_rate": 1.4041666666666666e-05,
      "loss": 0.0014,
      "step": 172600
    },
    {
      "epoch": 5.753666666666667,
      "grad_norm": 0.03173509240150452,
      "learning_rate": 1.4039583333333334e-05,
      "loss": 0.0018,
      "step": 172610
    },
    {
      "epoch": 5.754,
      "grad_norm": 0.14343665540218353,
      "learning_rate": 1.40375e-05,
      "loss": 0.0025,
      "step": 172620
    },
    {
      "epoch": 5.754333333333333,
      "grad_norm": 0.029913321137428284,
      "learning_rate": 1.4035416666666668e-05,
      "loss": 0.0015,
      "step": 172630
    },
    {
      "epoch": 5.754666666666667,
      "grad_norm": 0.5994779467582703,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0017,
      "step": 172640
    },
    {
      "epoch": 5.755,
      "grad_norm": 0.4563862383365631,
      "learning_rate": 1.403125e-05,
      "loss": 0.0023,
      "step": 172650
    },
    {
      "epoch": 5.755333333333334,
      "grad_norm": 0.23620367050170898,
      "learning_rate": 1.4029166666666668e-05,
      "loss": 0.0019,
      "step": 172660
    },
    {
      "epoch": 5.7556666666666665,
      "grad_norm": 0.009372532367706299,
      "learning_rate": 1.4027083333333335e-05,
      "loss": 0.0021,
      "step": 172670
    },
    {
      "epoch": 5.756,
      "grad_norm": 0.22875460982322693,
      "learning_rate": 1.4025000000000002e-05,
      "loss": 0.0017,
      "step": 172680
    },
    {
      "epoch": 5.756333333333333,
      "grad_norm": 0.2565126419067383,
      "learning_rate": 1.4022916666666666e-05,
      "loss": 0.002,
      "step": 172690
    },
    {
      "epoch": 5.756666666666667,
      "grad_norm": 0.11499647796154022,
      "learning_rate": 1.4020833333333333e-05,
      "loss": 0.0013,
      "step": 172700
    },
    {
      "epoch": 5.757,
      "grad_norm": 0.05784812197089195,
      "learning_rate": 1.401875e-05,
      "loss": 0.0018,
      "step": 172710
    },
    {
      "epoch": 5.757333333333333,
      "grad_norm": 0.14356252551078796,
      "learning_rate": 1.4016666666666667e-05,
      "loss": 0.0014,
      "step": 172720
    },
    {
      "epoch": 5.757666666666667,
      "grad_norm": 0.25761210918426514,
      "learning_rate": 1.4014583333333333e-05,
      "loss": 0.002,
      "step": 172730
    },
    {
      "epoch": 5.758,
      "grad_norm": 0.030393701046705246,
      "learning_rate": 1.40125e-05,
      "loss": 0.0017,
      "step": 172740
    },
    {
      "epoch": 5.758333333333333,
      "grad_norm": 0.03158200532197952,
      "learning_rate": 1.4010416666666667e-05,
      "loss": 0.0022,
      "step": 172750
    },
    {
      "epoch": 5.758666666666667,
      "grad_norm": 0.0418957881629467,
      "learning_rate": 1.4008333333333334e-05,
      "loss": 0.0026,
      "step": 172760
    },
    {
      "epoch": 5.759,
      "grad_norm": 0.21618518233299255,
      "learning_rate": 1.4006250000000002e-05,
      "loss": 0.0013,
      "step": 172770
    },
    {
      "epoch": 5.759333333333333,
      "grad_norm": 0.2571084201335907,
      "learning_rate": 1.4004166666666669e-05,
      "loss": 0.0017,
      "step": 172780
    },
    {
      "epoch": 5.759666666666667,
      "grad_norm": 0.11459825932979584,
      "learning_rate": 1.4002083333333334e-05,
      "loss": 0.0014,
      "step": 172790
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.22962509095668793,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0018,
      "step": 172800
    },
    {
      "epoch": 5.7603333333333335,
      "grad_norm": 0.2004304826259613,
      "learning_rate": 1.3997916666666665e-05,
      "loss": 0.0017,
      "step": 172810
    },
    {
      "epoch": 5.760666666666666,
      "grad_norm": 0.029481196776032448,
      "learning_rate": 1.3995833333333332e-05,
      "loss": 0.0013,
      "step": 172820
    },
    {
      "epoch": 5.761,
      "grad_norm": 0.05785144492983818,
      "learning_rate": 1.399375e-05,
      "loss": 0.0029,
      "step": 172830
    },
    {
      "epoch": 5.761333333333333,
      "grad_norm": 0.03029620461165905,
      "learning_rate": 1.3991666666666667e-05,
      "loss": 0.0024,
      "step": 172840
    },
    {
      "epoch": 5.761666666666667,
      "grad_norm": 0.40024352073669434,
      "learning_rate": 1.3989583333333334e-05,
      "loss": 0.0016,
      "step": 172850
    },
    {
      "epoch": 5.7620000000000005,
      "grad_norm": 0.11417492479085922,
      "learning_rate": 1.3987500000000001e-05,
      "loss": 0.0022,
      "step": 172860
    },
    {
      "epoch": 5.762333333333333,
      "grad_norm": 0.11437296867370605,
      "learning_rate": 1.3985416666666667e-05,
      "loss": 0.002,
      "step": 172870
    },
    {
      "epoch": 5.762666666666667,
      "grad_norm": 0.4408462345600128,
      "learning_rate": 1.3983333333333334e-05,
      "loss": 0.0017,
      "step": 172880
    },
    {
      "epoch": 5.763,
      "grad_norm": 0.20027337968349457,
      "learning_rate": 1.3981250000000001e-05,
      "loss": 0.0014,
      "step": 172890
    },
    {
      "epoch": 5.763333333333334,
      "grad_norm": 0.5908534526824951,
      "learning_rate": 1.3979166666666668e-05,
      "loss": 0.002,
      "step": 172900
    },
    {
      "epoch": 5.7636666666666665,
      "grad_norm": 0.08586297184228897,
      "learning_rate": 1.3977083333333335e-05,
      "loss": 0.0021,
      "step": 172910
    },
    {
      "epoch": 5.764,
      "grad_norm": 0.05730521306395531,
      "learning_rate": 1.3975000000000003e-05,
      "loss": 0.0022,
      "step": 172920
    },
    {
      "epoch": 5.764333333333333,
      "grad_norm": 0.030318867415189743,
      "learning_rate": 1.3972916666666666e-05,
      "loss": 0.0015,
      "step": 172930
    },
    {
      "epoch": 5.764666666666667,
      "grad_norm": 0.08582289516925812,
      "learning_rate": 1.3970833333333334e-05,
      "loss": 0.0028,
      "step": 172940
    },
    {
      "epoch": 5.765,
      "grad_norm": 0.2140037715435028,
      "learning_rate": 1.396875e-05,
      "loss": 0.0017,
      "step": 172950
    },
    {
      "epoch": 5.765333333333333,
      "grad_norm": 0.23034638166427612,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.0026,
      "step": 172960
    },
    {
      "epoch": 5.765666666666666,
      "grad_norm": 0.08686501532793045,
      "learning_rate": 1.3964583333333333e-05,
      "loss": 0.002,
      "step": 172970
    },
    {
      "epoch": 5.766,
      "grad_norm": 0.2964749336242676,
      "learning_rate": 1.39625e-05,
      "loss": 0.0017,
      "step": 172980
    },
    {
      "epoch": 5.766333333333334,
      "grad_norm": 0.11439832299947739,
      "learning_rate": 1.3960416666666668e-05,
      "loss": 0.0014,
      "step": 172990
    },
    {
      "epoch": 5.766666666666667,
      "grad_norm": 0.3620067238807678,
      "learning_rate": 1.3958333333333335e-05,
      "loss": 0.0019,
      "step": 173000
    },
    {
      "epoch": 5.767,
      "grad_norm": 0.010090498253703117,
      "learning_rate": 1.3956250000000002e-05,
      "loss": 0.002,
      "step": 173010
    },
    {
      "epoch": 5.767333333333333,
      "grad_norm": 0.2002752125263214,
      "learning_rate": 1.3954166666666668e-05,
      "loss": 0.0015,
      "step": 173020
    },
    {
      "epoch": 5.767666666666667,
      "grad_norm": 0.46121323108673096,
      "learning_rate": 1.3952083333333335e-05,
      "loss": 0.0018,
      "step": 173030
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.37099531292915344,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 0.002,
      "step": 173040
    },
    {
      "epoch": 5.7683333333333335,
      "grad_norm": 0.11508940905332565,
      "learning_rate": 1.3947916666666666e-05,
      "loss": 0.0023,
      "step": 173050
    },
    {
      "epoch": 5.768666666666666,
      "grad_norm": 0.3087998926639557,
      "learning_rate": 1.3945833333333333e-05,
      "loss": 0.0022,
      "step": 173060
    },
    {
      "epoch": 5.769,
      "grad_norm": 0.42813190817832947,
      "learning_rate": 1.394375e-05,
      "loss": 0.0018,
      "step": 173070
    },
    {
      "epoch": 5.769333333333333,
      "grad_norm": 0.114316925406456,
      "learning_rate": 1.3941666666666667e-05,
      "loss": 0.0015,
      "step": 173080
    },
    {
      "epoch": 5.769666666666667,
      "grad_norm": 0.2190578430891037,
      "learning_rate": 1.3939583333333334e-05,
      "loss": 0.002,
      "step": 173090
    },
    {
      "epoch": 5.77,
      "grad_norm": 0.34574365615844727,
      "learning_rate": 1.39375e-05,
      "loss": 0.002,
      "step": 173100
    },
    {
      "epoch": 5.770333333333333,
      "grad_norm": 0.17174550890922546,
      "learning_rate": 1.3935416666666667e-05,
      "loss": 0.0018,
      "step": 173110
    },
    {
      "epoch": 5.770666666666667,
      "grad_norm": 0.05836528539657593,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0015,
      "step": 173120
    },
    {
      "epoch": 5.771,
      "grad_norm": 0.5101287961006165,
      "learning_rate": 1.3931250000000002e-05,
      "loss": 0.0018,
      "step": 173130
    },
    {
      "epoch": 5.771333333333334,
      "grad_norm": 0.21355028450489044,
      "learning_rate": 1.3929166666666669e-05,
      "loss": 0.0022,
      "step": 173140
    },
    {
      "epoch": 5.7716666666666665,
      "grad_norm": 0.1720142811536789,
      "learning_rate": 1.3927083333333336e-05,
      "loss": 0.0014,
      "step": 173150
    },
    {
      "epoch": 5.772,
      "grad_norm": 0.25799134373664856,
      "learning_rate": 1.3925000000000001e-05,
      "loss": 0.0012,
      "step": 173160
    },
    {
      "epoch": 5.772333333333333,
      "grad_norm": 0.17182031273841858,
      "learning_rate": 1.3922916666666667e-05,
      "loss": 0.0015,
      "step": 173170
    },
    {
      "epoch": 5.772666666666667,
      "grad_norm": 0.1434718519449234,
      "learning_rate": 1.3920833333333332e-05,
      "loss": 0.002,
      "step": 173180
    },
    {
      "epoch": 5.773,
      "grad_norm": 0.14284472167491913,
      "learning_rate": 1.391875e-05,
      "loss": 0.0015,
      "step": 173190
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 0.4496692419052124,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.0022,
      "step": 173200
    },
    {
      "epoch": 5.773666666666666,
      "grad_norm": 0.4193164110183716,
      "learning_rate": 1.3914583333333334e-05,
      "loss": 0.0023,
      "step": 173210
    },
    {
      "epoch": 5.774,
      "grad_norm": 0.05765870213508606,
      "learning_rate": 1.3912500000000001e-05,
      "loss": 0.0022,
      "step": 173220
    },
    {
      "epoch": 5.774333333333333,
      "grad_norm": 0.2858971953392029,
      "learning_rate": 1.3910416666666668e-05,
      "loss": 0.0016,
      "step": 173230
    },
    {
      "epoch": 5.774666666666667,
      "grad_norm": 0.35585036873817444,
      "learning_rate": 1.3908333333333334e-05,
      "loss": 0.0016,
      "step": 173240
    },
    {
      "epoch": 5.775,
      "grad_norm": 0.11436227709054947,
      "learning_rate": 1.3906250000000001e-05,
      "loss": 0.0015,
      "step": 173250
    },
    {
      "epoch": 5.775333333333333,
      "grad_norm": 0.0687728300690651,
      "learning_rate": 1.3904166666666668e-05,
      "loss": 0.0022,
      "step": 173260
    },
    {
      "epoch": 5.775666666666667,
      "grad_norm": 0.034395162016153336,
      "learning_rate": 1.3902083333333335e-05,
      "loss": 0.0025,
      "step": 173270
    },
    {
      "epoch": 5.776,
      "grad_norm": 0.15439613163471222,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0019,
      "step": 173280
    },
    {
      "epoch": 5.7763333333333335,
      "grad_norm": 0.05735728144645691,
      "learning_rate": 1.3897916666666666e-05,
      "loss": 0.0014,
      "step": 173290
    },
    {
      "epoch": 5.776666666666666,
      "grad_norm": 0.0579424723982811,
      "learning_rate": 1.3895833333333333e-05,
      "loss": 0.002,
      "step": 173300
    },
    {
      "epoch": 5.777,
      "grad_norm": 0.0574507862329483,
      "learning_rate": 1.389375e-05,
      "loss": 0.0016,
      "step": 173310
    },
    {
      "epoch": 5.777333333333333,
      "grad_norm": 0.058429643511772156,
      "learning_rate": 1.3891666666666666e-05,
      "loss": 0.0018,
      "step": 173320
    },
    {
      "epoch": 5.777666666666667,
      "grad_norm": 0.1147274300456047,
      "learning_rate": 1.3889583333333333e-05,
      "loss": 0.0021,
      "step": 173330
    },
    {
      "epoch": 5.7780000000000005,
      "grad_norm": 0.11443164199590683,
      "learning_rate": 1.38875e-05,
      "loss": 0.0016,
      "step": 173340
    },
    {
      "epoch": 5.778333333333333,
      "grad_norm": 0.11435748636722565,
      "learning_rate": 1.3885416666666668e-05,
      "loss": 0.0013,
      "step": 173350
    },
    {
      "epoch": 5.778666666666666,
      "grad_norm": 0.20003938674926758,
      "learning_rate": 1.3883333333333335e-05,
      "loss": 0.002,
      "step": 173360
    },
    {
      "epoch": 5.779,
      "grad_norm": 0.2008686661720276,
      "learning_rate": 1.3881250000000002e-05,
      "loss": 0.0016,
      "step": 173370
    },
    {
      "epoch": 5.779333333333334,
      "grad_norm": 0.029545417055487633,
      "learning_rate": 1.3879166666666667e-05,
      "loss": 0.002,
      "step": 173380
    },
    {
      "epoch": 5.7796666666666665,
      "grad_norm": 0.08712344616651535,
      "learning_rate": 1.3877083333333335e-05,
      "loss": 0.0015,
      "step": 173390
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.08586553484201431,
      "learning_rate": 1.3875000000000002e-05,
      "loss": 0.0022,
      "step": 173400
    },
    {
      "epoch": 5.780333333333333,
      "grad_norm": 0.22875697910785675,
      "learning_rate": 1.3872916666666666e-05,
      "loss": 0.0025,
      "step": 173410
    },
    {
      "epoch": 5.780666666666667,
      "grad_norm": 0.1717902421951294,
      "learning_rate": 1.3870833333333333e-05,
      "loss": 0.0013,
      "step": 173420
    },
    {
      "epoch": 5.781,
      "grad_norm": 0.4369875192642212,
      "learning_rate": 1.386875e-05,
      "loss": 0.0017,
      "step": 173430
    },
    {
      "epoch": 5.781333333333333,
      "grad_norm": 0.25673195719718933,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0016,
      "step": 173440
    },
    {
      "epoch": 5.781666666666666,
      "grad_norm": 0.2286810427904129,
      "learning_rate": 1.3864583333333334e-05,
      "loss": 0.0013,
      "step": 173450
    },
    {
      "epoch": 5.782,
      "grad_norm": 0.34275445342063904,
      "learning_rate": 1.38625e-05,
      "loss": 0.0026,
      "step": 173460
    },
    {
      "epoch": 5.782333333333334,
      "grad_norm": 0.1429966688156128,
      "learning_rate": 1.3860416666666667e-05,
      "loss": 0.0018,
      "step": 173470
    },
    {
      "epoch": 5.782666666666667,
      "grad_norm": 0.2286863476037979,
      "learning_rate": 1.3858333333333334e-05,
      "loss": 0.0015,
      "step": 173480
    },
    {
      "epoch": 5.783,
      "grad_norm": 0.032772548496723175,
      "learning_rate": 1.3856250000000001e-05,
      "loss": 0.0014,
      "step": 173490
    },
    {
      "epoch": 5.783333333333333,
      "grad_norm": 0.17191936075687408,
      "learning_rate": 1.3854166666666669e-05,
      "loss": 0.0018,
      "step": 173500
    },
    {
      "epoch": 5.783666666666667,
      "grad_norm": 0.1710170954465866,
      "learning_rate": 1.3852083333333336e-05,
      "loss": 0.0021,
      "step": 173510
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.17556168138980865,
      "learning_rate": 1.3850000000000001e-05,
      "loss": 0.0017,
      "step": 173520
    },
    {
      "epoch": 5.7843333333333335,
      "grad_norm": 0.27318739891052246,
      "learning_rate": 1.3847916666666667e-05,
      "loss": 0.0023,
      "step": 173530
    },
    {
      "epoch": 5.784666666666666,
      "grad_norm": 0.5311362147331238,
      "learning_rate": 1.3845833333333332e-05,
      "loss": 0.0024,
      "step": 173540
    },
    {
      "epoch": 5.785,
      "grad_norm": 0.2284274846315384,
      "learning_rate": 1.384375e-05,
      "loss": 0.0018,
      "step": 173550
    },
    {
      "epoch": 5.785333333333333,
      "grad_norm": 0.2570670545101166,
      "learning_rate": 1.3841666666666667e-05,
      "loss": 0.0015,
      "step": 173560
    },
    {
      "epoch": 5.785666666666667,
      "grad_norm": 0.029549645259976387,
      "learning_rate": 1.3839583333333334e-05,
      "loss": 0.0018,
      "step": 173570
    },
    {
      "epoch": 5.786,
      "grad_norm": 0.1431366354227066,
      "learning_rate": 1.3837500000000001e-05,
      "loss": 0.0016,
      "step": 173580
    },
    {
      "epoch": 5.786333333333333,
      "grad_norm": 0.1455659568309784,
      "learning_rate": 1.3835416666666668e-05,
      "loss": 0.0013,
      "step": 173590
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 0.1710217446088791,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.0015,
      "step": 173600
    },
    {
      "epoch": 5.787,
      "grad_norm": 0.28619450330734253,
      "learning_rate": 1.383125e-05,
      "loss": 0.0014,
      "step": 173610
    },
    {
      "epoch": 5.787333333333334,
      "grad_norm": 0.17102544009685516,
      "learning_rate": 1.3829166666666668e-05,
      "loss": 0.0015,
      "step": 173620
    },
    {
      "epoch": 5.7876666666666665,
      "grad_norm": 0.48561832308769226,
      "learning_rate": 1.3827083333333335e-05,
      "loss": 0.0022,
      "step": 173630
    },
    {
      "epoch": 5.788,
      "grad_norm": 0.1715073436498642,
      "learning_rate": 1.3825000000000002e-05,
      "loss": 0.002,
      "step": 173640
    },
    {
      "epoch": 5.788333333333333,
      "grad_norm": 0.05795922130346298,
      "learning_rate": 1.3822916666666666e-05,
      "loss": 0.0014,
      "step": 173650
    },
    {
      "epoch": 5.788666666666667,
      "grad_norm": 0.27578631043434143,
      "learning_rate": 1.3820833333333333e-05,
      "loss": 0.0013,
      "step": 173660
    },
    {
      "epoch": 5.789,
      "grad_norm": 0.3426392674446106,
      "learning_rate": 1.381875e-05,
      "loss": 0.0011,
      "step": 173670
    },
    {
      "epoch": 5.789333333333333,
      "grad_norm": 0.4573635160923004,
      "learning_rate": 1.3816666666666666e-05,
      "loss": 0.0015,
      "step": 173680
    },
    {
      "epoch": 5.789666666666666,
      "grad_norm": 0.14383170008659363,
      "learning_rate": 1.3814583333333333e-05,
      "loss": 0.0023,
      "step": 173690
    },
    {
      "epoch": 5.79,
      "grad_norm": 0.20031829178333282,
      "learning_rate": 1.38125e-05,
      "loss": 0.0018,
      "step": 173700
    },
    {
      "epoch": 5.790333333333333,
      "grad_norm": 0.4917786717414856,
      "learning_rate": 1.3810416666666667e-05,
      "loss": 0.0018,
      "step": 173710
    },
    {
      "epoch": 5.790666666666667,
      "grad_norm": 0.1714901626110077,
      "learning_rate": 1.3808333333333335e-05,
      "loss": 0.002,
      "step": 173720
    },
    {
      "epoch": 5.791,
      "grad_norm": 0.28509026765823364,
      "learning_rate": 1.3806250000000002e-05,
      "loss": 0.0011,
      "step": 173730
    },
    {
      "epoch": 5.791333333333333,
      "grad_norm": 0.0582195520401001,
      "learning_rate": 1.3804166666666667e-05,
      "loss": 0.0015,
      "step": 173740
    },
    {
      "epoch": 5.791666666666667,
      "grad_norm": 0.08653116226196289,
      "learning_rate": 1.3802083333333335e-05,
      "loss": 0.0019,
      "step": 173750
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.39452534914016724,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0019,
      "step": 173760
    },
    {
      "epoch": 5.792333333333334,
      "grad_norm": 0.17132402956485748,
      "learning_rate": 1.3797916666666669e-05,
      "loss": 0.0022,
      "step": 173770
    },
    {
      "epoch": 5.792666666666666,
      "grad_norm": 0.5422696471214294,
      "learning_rate": 1.3795833333333333e-05,
      "loss": 0.0013,
      "step": 173780
    },
    {
      "epoch": 5.793,
      "grad_norm": 0.39981764554977417,
      "learning_rate": 1.379375e-05,
      "loss": 0.0016,
      "step": 173790
    },
    {
      "epoch": 5.793333333333333,
      "grad_norm": 0.1453654021024704,
      "learning_rate": 1.3791666666666667e-05,
      "loss": 0.0017,
      "step": 173800
    },
    {
      "epoch": 5.793666666666667,
      "grad_norm": 0.22855405509471893,
      "learning_rate": 1.3789583333333334e-05,
      "loss": 0.0023,
      "step": 173810
    },
    {
      "epoch": 5.7940000000000005,
      "grad_norm": 0.171509250998497,
      "learning_rate": 1.37875e-05,
      "loss": 0.0017,
      "step": 173820
    },
    {
      "epoch": 5.794333333333333,
      "grad_norm": 0.17147952318191528,
      "learning_rate": 1.3785416666666667e-05,
      "loss": 0.0017,
      "step": 173830
    },
    {
      "epoch": 5.794666666666666,
      "grad_norm": 0.16825492680072784,
      "learning_rate": 1.3783333333333334e-05,
      "loss": 0.0014,
      "step": 173840
    },
    {
      "epoch": 5.795,
      "grad_norm": 0.1151042953133583,
      "learning_rate": 1.3781250000000001e-05,
      "loss": 0.0021,
      "step": 173850
    },
    {
      "epoch": 5.795333333333334,
      "grad_norm": 0.029497463256120682,
      "learning_rate": 1.3779166666666668e-05,
      "loss": 0.0019,
      "step": 173860
    },
    {
      "epoch": 5.7956666666666665,
      "grad_norm": 0.14281414449214935,
      "learning_rate": 1.3777083333333336e-05,
      "loss": 0.0019,
      "step": 173870
    },
    {
      "epoch": 5.796,
      "grad_norm": 0.057436950504779816,
      "learning_rate": 1.3775000000000001e-05,
      "loss": 0.0019,
      "step": 173880
    },
    {
      "epoch": 5.796333333333333,
      "grad_norm": 0.1719195693731308,
      "learning_rate": 1.3772916666666668e-05,
      "loss": 0.0014,
      "step": 173890
    },
    {
      "epoch": 5.796666666666667,
      "grad_norm": 0.08174654096364975,
      "learning_rate": 1.3770833333333332e-05,
      "loss": 0.0016,
      "step": 173900
    },
    {
      "epoch": 5.797,
      "grad_norm": 0.25764229893684387,
      "learning_rate": 1.376875e-05,
      "loss": 0.0013,
      "step": 173910
    },
    {
      "epoch": 5.7973333333333334,
      "grad_norm": 0.31378334760665894,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.0018,
      "step": 173920
    },
    {
      "epoch": 5.797666666666666,
      "grad_norm": 0.2574211657047272,
      "learning_rate": 1.3764583333333334e-05,
      "loss": 0.0017,
      "step": 173930
    },
    {
      "epoch": 5.798,
      "grad_norm": 0.030191518366336823,
      "learning_rate": 1.37625e-05,
      "loss": 0.0019,
      "step": 173940
    },
    {
      "epoch": 5.798333333333334,
      "grad_norm": 0.14283496141433716,
      "learning_rate": 1.3760416666666668e-05,
      "loss": 0.0021,
      "step": 173950
    },
    {
      "epoch": 5.798666666666667,
      "grad_norm": 0.4603782296180725,
      "learning_rate": 1.3758333333333333e-05,
      "loss": 0.0024,
      "step": 173960
    },
    {
      "epoch": 5.799,
      "grad_norm": 0.00950760766863823,
      "learning_rate": 1.375625e-05,
      "loss": 0.0025,
      "step": 173970
    },
    {
      "epoch": 5.799333333333333,
      "grad_norm": 0.11524075269699097,
      "learning_rate": 1.3754166666666668e-05,
      "loss": 0.0015,
      "step": 173980
    },
    {
      "epoch": 5.799666666666667,
      "grad_norm": 0.031642936170101166,
      "learning_rate": 1.3752083333333335e-05,
      "loss": 0.0015,
      "step": 173990
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.39982596039772034,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.0017,
      "step": 174000
    },
    {
      "epoch": 5.800333333333334,
      "grad_norm": 0.1717061996459961,
      "learning_rate": 1.374791666666667e-05,
      "loss": 0.0019,
      "step": 174010
    },
    {
      "epoch": 5.800666666666666,
      "grad_norm": 0.03066161461174488,
      "learning_rate": 1.3745833333333333e-05,
      "loss": 0.0023,
      "step": 174020
    },
    {
      "epoch": 5.801,
      "grad_norm": 0.11493033170700073,
      "learning_rate": 1.374375e-05,
      "loss": 0.0022,
      "step": 174030
    },
    {
      "epoch": 5.801333333333333,
      "grad_norm": 0.11466671526432037,
      "learning_rate": 1.3741666666666666e-05,
      "loss": 0.0015,
      "step": 174040
    },
    {
      "epoch": 5.801666666666667,
      "grad_norm": 0.4636322855949402,
      "learning_rate": 1.3739583333333333e-05,
      "loss": 0.0013,
      "step": 174050
    },
    {
      "epoch": 5.802,
      "grad_norm": 0.31473830342292786,
      "learning_rate": 1.37375e-05,
      "loss": 0.0017,
      "step": 174060
    },
    {
      "epoch": 5.802333333333333,
      "grad_norm": 0.2572799026966095,
      "learning_rate": 1.3735416666666667e-05,
      "loss": 0.0015,
      "step": 174070
    },
    {
      "epoch": 5.802666666666667,
      "grad_norm": 0.030333029106259346,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0018,
      "step": 174080
    },
    {
      "epoch": 5.803,
      "grad_norm": 0.19971001148223877,
      "learning_rate": 1.3731250000000002e-05,
      "loss": 0.0022,
      "step": 174090
    },
    {
      "epoch": 5.803333333333334,
      "grad_norm": 0.22832874953746796,
      "learning_rate": 1.3729166666666667e-05,
      "loss": 0.0019,
      "step": 174100
    },
    {
      "epoch": 5.8036666666666665,
      "grad_norm": 0.3138984739780426,
      "learning_rate": 1.3727083333333334e-05,
      "loss": 0.0019,
      "step": 174110
    },
    {
      "epoch": 5.804,
      "grad_norm": 0.4856225550174713,
      "learning_rate": 1.3725000000000002e-05,
      "loss": 0.0017,
      "step": 174120
    },
    {
      "epoch": 5.804333333333333,
      "grad_norm": 0.3741146922111511,
      "learning_rate": 1.3722916666666669e-05,
      "loss": 0.0027,
      "step": 174130
    },
    {
      "epoch": 5.804666666666667,
      "grad_norm": 0.2288946658372879,
      "learning_rate": 1.3720833333333333e-05,
      "loss": 0.0018,
      "step": 174140
    },
    {
      "epoch": 5.805,
      "grad_norm": 0.29645469784736633,
      "learning_rate": 1.371875e-05,
      "loss": 0.0015,
      "step": 174150
    },
    {
      "epoch": 5.8053333333333335,
      "grad_norm": 0.22524261474609375,
      "learning_rate": 1.3716666666666667e-05,
      "loss": 0.002,
      "step": 174160
    },
    {
      "epoch": 5.805666666666666,
      "grad_norm": 0.5794077515602112,
      "learning_rate": 1.3714583333333334e-05,
      "loss": 0.0017,
      "step": 174170
    },
    {
      "epoch": 5.806,
      "grad_norm": 0.19975362718105316,
      "learning_rate": 1.37125e-05,
      "loss": 0.0014,
      "step": 174180
    },
    {
      "epoch": 5.806333333333333,
      "grad_norm": 0.20004309713840485,
      "learning_rate": 1.3710416666666667e-05,
      "loss": 0.0016,
      "step": 174190
    },
    {
      "epoch": 5.806666666666667,
      "grad_norm": 0.4567827582359314,
      "learning_rate": 1.3708333333333334e-05,
      "loss": 0.0017,
      "step": 174200
    },
    {
      "epoch": 5.807,
      "grad_norm": 0.31373336911201477,
      "learning_rate": 1.3706250000000001e-05,
      "loss": 0.0019,
      "step": 174210
    },
    {
      "epoch": 5.807333333333333,
      "grad_norm": 0.11424697935581207,
      "learning_rate": 1.3704166666666668e-05,
      "loss": 0.0013,
      "step": 174220
    },
    {
      "epoch": 5.807666666666667,
      "grad_norm": 0.016422946006059647,
      "learning_rate": 1.3702083333333335e-05,
      "loss": 0.0018,
      "step": 174230
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.285643994808197,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.0015,
      "step": 174240
    },
    {
      "epoch": 5.808333333333334,
      "grad_norm": 0.20046143233776093,
      "learning_rate": 1.3697916666666668e-05,
      "loss": 0.0015,
      "step": 174250
    },
    {
      "epoch": 5.808666666666666,
      "grad_norm": 0.3436800241470337,
      "learning_rate": 1.3695833333333332e-05,
      "loss": 0.0011,
      "step": 174260
    },
    {
      "epoch": 5.809,
      "grad_norm": 0.28560957312583923,
      "learning_rate": 1.3693749999999999e-05,
      "loss": 0.0018,
      "step": 174270
    },
    {
      "epoch": 5.809333333333333,
      "grad_norm": 0.3995731472969055,
      "learning_rate": 1.3691666666666666e-05,
      "loss": 0.0019,
      "step": 174280
    },
    {
      "epoch": 5.809666666666667,
      "grad_norm": 0.11416562646627426,
      "learning_rate": 1.3689583333333333e-05,
      "loss": 0.0014,
      "step": 174290
    },
    {
      "epoch": 5.8100000000000005,
      "grad_norm": 0.2940661907196045,
      "learning_rate": 1.36875e-05,
      "loss": 0.0024,
      "step": 174300
    },
    {
      "epoch": 5.810333333333333,
      "grad_norm": 0.28017374873161316,
      "learning_rate": 1.3685416666666668e-05,
      "loss": 0.0016,
      "step": 174310
    },
    {
      "epoch": 5.810666666666666,
      "grad_norm": 0.2679978311061859,
      "learning_rate": 1.3683333333333333e-05,
      "loss": 0.0025,
      "step": 174320
    },
    {
      "epoch": 5.811,
      "grad_norm": 0.1722830832004547,
      "learning_rate": 1.368125e-05,
      "loss": 0.0013,
      "step": 174330
    },
    {
      "epoch": 5.811333333333334,
      "grad_norm": 0.11343632638454437,
      "learning_rate": 1.3679166666666668e-05,
      "loss": 0.0025,
      "step": 174340
    },
    {
      "epoch": 5.8116666666666665,
      "grad_norm": 0.013983430340886116,
      "learning_rate": 1.3677083333333335e-05,
      "loss": 0.0025,
      "step": 174350
    },
    {
      "epoch": 5.812,
      "grad_norm": 0.3427737355232239,
      "learning_rate": 1.3675000000000002e-05,
      "loss": 0.0018,
      "step": 174360
    },
    {
      "epoch": 5.812333333333333,
      "grad_norm": 0.05780251696705818,
      "learning_rate": 1.367291666666667e-05,
      "loss": 0.0019,
      "step": 174370
    },
    {
      "epoch": 5.812666666666667,
      "grad_norm": 0.5674229860305786,
      "learning_rate": 1.3670833333333333e-05,
      "loss": 0.0027,
      "step": 174380
    },
    {
      "epoch": 5.813,
      "grad_norm": 0.08571121096611023,
      "learning_rate": 1.366875e-05,
      "loss": 0.0019,
      "step": 174390
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.058188166469335556,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0016,
      "step": 174400
    },
    {
      "epoch": 5.813666666666666,
      "grad_norm": 0.05800691246986389,
      "learning_rate": 1.3664583333333333e-05,
      "loss": 0.0015,
      "step": 174410
    },
    {
      "epoch": 5.814,
      "grad_norm": 0.030665749683976173,
      "learning_rate": 1.36625e-05,
      "loss": 0.002,
      "step": 174420
    },
    {
      "epoch": 5.814333333333334,
      "grad_norm": 0.2571571171283722,
      "learning_rate": 1.3660416666666667e-05,
      "loss": 0.0013,
      "step": 174430
    },
    {
      "epoch": 5.814666666666667,
      "grad_norm": 0.0865611732006073,
      "learning_rate": 1.3658333333333334e-05,
      "loss": 0.0017,
      "step": 174440
    },
    {
      "epoch": 5.8149999999999995,
      "grad_norm": 0.00804904941469431,
      "learning_rate": 1.3656250000000002e-05,
      "loss": 0.0014,
      "step": 174450
    },
    {
      "epoch": 5.815333333333333,
      "grad_norm": 0.14338219165802002,
      "learning_rate": 1.3654166666666667e-05,
      "loss": 0.0017,
      "step": 174460
    },
    {
      "epoch": 5.815666666666667,
      "grad_norm": 0.11570008099079132,
      "learning_rate": 1.3652083333333334e-05,
      "loss": 0.0019,
      "step": 174470
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.03029620088636875,
      "learning_rate": 1.3650000000000001e-05,
      "loss": 0.002,
      "step": 174480
    },
    {
      "epoch": 5.816333333333334,
      "grad_norm": 0.20093384385108948,
      "learning_rate": 1.3647916666666669e-05,
      "loss": 0.0018,
      "step": 174490
    },
    {
      "epoch": 5.816666666666666,
      "grad_norm": 0.09040585905313492,
      "learning_rate": 1.3645833333333332e-05,
      "loss": 0.0024,
      "step": 174500
    },
    {
      "epoch": 5.817,
      "grad_norm": 0.03154982998967171,
      "learning_rate": 1.364375e-05,
      "loss": 0.0032,
      "step": 174510
    },
    {
      "epoch": 5.817333333333333,
      "grad_norm": 0.3400419354438782,
      "learning_rate": 1.3641666666666667e-05,
      "loss": 0.0016,
      "step": 174520
    },
    {
      "epoch": 5.817666666666667,
      "grad_norm": 0.029420239850878716,
      "learning_rate": 1.3639583333333334e-05,
      "loss": 0.0019,
      "step": 174530
    },
    {
      "epoch": 5.818,
      "grad_norm": 0.1435348391532898,
      "learning_rate": 1.36375e-05,
      "loss": 0.0019,
      "step": 174540
    },
    {
      "epoch": 5.818333333333333,
      "grad_norm": 0.28547534346580505,
      "learning_rate": 1.3635416666666667e-05,
      "loss": 0.002,
      "step": 174550
    },
    {
      "epoch": 5.818666666666667,
      "grad_norm": 0.0382089838385582,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0013,
      "step": 174560
    },
    {
      "epoch": 5.819,
      "grad_norm": 0.19692550599575043,
      "learning_rate": 1.3631250000000001e-05,
      "loss": 0.0028,
      "step": 174570
    },
    {
      "epoch": 5.819333333333334,
      "grad_norm": 0.1718551069498062,
      "learning_rate": 1.3629166666666668e-05,
      "loss": 0.0024,
      "step": 174580
    },
    {
      "epoch": 5.8196666666666665,
      "grad_norm": 0.22899706661701202,
      "learning_rate": 1.3627083333333335e-05,
      "loss": 0.0021,
      "step": 174590
    },
    {
      "epoch": 5.82,
      "grad_norm": 0.0857180804014206,
      "learning_rate": 1.3625e-05,
      "loss": 0.0017,
      "step": 174600
    },
    {
      "epoch": 5.820333333333333,
      "grad_norm": 0.40004777908325195,
      "learning_rate": 1.3622916666666668e-05,
      "loss": 0.0017,
      "step": 174610
    },
    {
      "epoch": 5.820666666666667,
      "grad_norm": 0.00898904912173748,
      "learning_rate": 1.3620833333333334e-05,
      "loss": 0.0022,
      "step": 174620
    },
    {
      "epoch": 5.821,
      "grad_norm": 0.19991052150726318,
      "learning_rate": 1.3618749999999999e-05,
      "loss": 0.0016,
      "step": 174630
    },
    {
      "epoch": 5.8213333333333335,
      "grad_norm": 0.12582607567310333,
      "learning_rate": 1.3616666666666666e-05,
      "loss": 0.0014,
      "step": 174640
    },
    {
      "epoch": 5.821666666666666,
      "grad_norm": 0.5807764530181885,
      "learning_rate": 1.3614583333333333e-05,
      "loss": 0.002,
      "step": 174650
    },
    {
      "epoch": 5.822,
      "grad_norm": 0.2536682188510895,
      "learning_rate": 1.36125e-05,
      "loss": 0.0015,
      "step": 174660
    },
    {
      "epoch": 5.822333333333333,
      "grad_norm": 0.0318717360496521,
      "learning_rate": 1.3610416666666668e-05,
      "loss": 0.0015,
      "step": 174670
    },
    {
      "epoch": 5.822666666666667,
      "grad_norm": 0.5861998796463013,
      "learning_rate": 1.3608333333333333e-05,
      "loss": 0.0019,
      "step": 174680
    },
    {
      "epoch": 5.823,
      "grad_norm": 0.314197301864624,
      "learning_rate": 1.360625e-05,
      "loss": 0.0026,
      "step": 174690
    },
    {
      "epoch": 5.823333333333333,
      "grad_norm": 0.08669867366552353,
      "learning_rate": 1.3604166666666668e-05,
      "loss": 0.002,
      "step": 174700
    },
    {
      "epoch": 5.823666666666667,
      "grad_norm": 0.08691160380840302,
      "learning_rate": 1.3602083333333335e-05,
      "loss": 0.0012,
      "step": 174710
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.2346656173467636,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.002,
      "step": 174720
    },
    {
      "epoch": 5.824333333333334,
      "grad_norm": 0.22819136083126068,
      "learning_rate": 1.3597916666666669e-05,
      "loss": 0.0014,
      "step": 174730
    },
    {
      "epoch": 5.824666666666666,
      "grad_norm": 0.34319809079170227,
      "learning_rate": 1.3595833333333333e-05,
      "loss": 0.0012,
      "step": 174740
    },
    {
      "epoch": 5.825,
      "grad_norm": 0.3728250563144684,
      "learning_rate": 1.359375e-05,
      "loss": 0.0018,
      "step": 174750
    },
    {
      "epoch": 5.825333333333333,
      "grad_norm": 0.029820317402482033,
      "learning_rate": 1.3591666666666667e-05,
      "loss": 0.0018,
      "step": 174760
    },
    {
      "epoch": 5.825666666666667,
      "grad_norm": 0.05755461007356644,
      "learning_rate": 1.3589583333333333e-05,
      "loss": 0.0019,
      "step": 174770
    },
    {
      "epoch": 5.826,
      "grad_norm": 0.1547689586877823,
      "learning_rate": 1.35875e-05,
      "loss": 0.0016,
      "step": 174780
    },
    {
      "epoch": 5.826333333333333,
      "grad_norm": 0.1147930920124054,
      "learning_rate": 1.3585416666666667e-05,
      "loss": 0.0023,
      "step": 174790
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.14341554045677185,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.0018,
      "step": 174800
    },
    {
      "epoch": 5.827,
      "grad_norm": 0.2001451998949051,
      "learning_rate": 1.3581250000000001e-05,
      "loss": 0.0021,
      "step": 174810
    },
    {
      "epoch": 5.827333333333334,
      "grad_norm": 0.08692982792854309,
      "learning_rate": 1.3579166666666669e-05,
      "loss": 0.0019,
      "step": 174820
    },
    {
      "epoch": 5.8276666666666666,
      "grad_norm": 0.28523457050323486,
      "learning_rate": 1.3577083333333334e-05,
      "loss": 0.0018,
      "step": 174830
    },
    {
      "epoch": 5.828,
      "grad_norm": 0.37215912342071533,
      "learning_rate": 1.3575000000000001e-05,
      "loss": 0.0024,
      "step": 174840
    },
    {
      "epoch": 5.828333333333333,
      "grad_norm": 0.20000354945659637,
      "learning_rate": 1.3572916666666668e-05,
      "loss": 0.002,
      "step": 174850
    },
    {
      "epoch": 5.828666666666667,
      "grad_norm": 0.0969977006316185,
      "learning_rate": 1.3570833333333332e-05,
      "loss": 0.0025,
      "step": 174860
    },
    {
      "epoch": 5.829,
      "grad_norm": 0.057894494384527206,
      "learning_rate": 1.356875e-05,
      "loss": 0.0021,
      "step": 174870
    },
    {
      "epoch": 5.8293333333333335,
      "grad_norm": 0.05789700523018837,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0018,
      "step": 174880
    },
    {
      "epoch": 5.829666666666666,
      "grad_norm": 0.08676034957170486,
      "learning_rate": 1.3564583333333334e-05,
      "loss": 0.0022,
      "step": 174890
    },
    {
      "epoch": 5.83,
      "grad_norm": 0.1719592809677124,
      "learning_rate": 1.3562500000000001e-05,
      "loss": 0.0016,
      "step": 174900
    },
    {
      "epoch": 5.830333333333334,
      "grad_norm": 0.2857000529766083,
      "learning_rate": 1.3560416666666667e-05,
      "loss": 0.0022,
      "step": 174910
    },
    {
      "epoch": 5.830666666666667,
      "grad_norm": 0.3068375289440155,
      "learning_rate": 1.3558333333333334e-05,
      "loss": 0.0017,
      "step": 174920
    },
    {
      "epoch": 5.8309999999999995,
      "grad_norm": 0.48531967401504517,
      "learning_rate": 1.3556250000000001e-05,
      "loss": 0.0018,
      "step": 174930
    },
    {
      "epoch": 5.831333333333333,
      "grad_norm": 0.399297833442688,
      "learning_rate": 1.3554166666666668e-05,
      "loss": 0.0017,
      "step": 174940
    },
    {
      "epoch": 5.831666666666667,
      "grad_norm": 0.08600572496652603,
      "learning_rate": 1.3552083333333335e-05,
      "loss": 0.0013,
      "step": 174950
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.030548227950930595,
      "learning_rate": 1.3550000000000002e-05,
      "loss": 0.002,
      "step": 174960
    },
    {
      "epoch": 5.832333333333334,
      "grad_norm": 0.02999689057469368,
      "learning_rate": 1.3547916666666668e-05,
      "loss": 0.0014,
      "step": 174970
    },
    {
      "epoch": 5.832666666666666,
      "grad_norm": 0.009187650866806507,
      "learning_rate": 1.3545833333333333e-05,
      "loss": 0.0018,
      "step": 174980
    },
    {
      "epoch": 5.833,
      "grad_norm": 0.11457468569278717,
      "learning_rate": 1.3543749999999999e-05,
      "loss": 0.0017,
      "step": 174990
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 0.05897766724228859,
      "learning_rate": 1.3541666666666666e-05,
      "loss": 0.0014,
      "step": 175000
    },
    {
      "epoch": 5.833666666666667,
      "grad_norm": 0.1969098597764969,
      "learning_rate": 1.3539583333333333e-05,
      "loss": 0.0015,
      "step": 175010
    },
    {
      "epoch": 5.834,
      "grad_norm": 0.05769345536828041,
      "learning_rate": 1.35375e-05,
      "loss": 0.0014,
      "step": 175020
    },
    {
      "epoch": 5.834333333333333,
      "grad_norm": 0.03136449679732323,
      "learning_rate": 1.3535416666666668e-05,
      "loss": 0.0014,
      "step": 175030
    },
    {
      "epoch": 5.834666666666667,
      "grad_norm": 0.1145988255739212,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0012,
      "step": 175040
    },
    {
      "epoch": 5.835,
      "grad_norm": 0.14300450682640076,
      "learning_rate": 1.353125e-05,
      "loss": 0.0017,
      "step": 175050
    },
    {
      "epoch": 5.835333333333334,
      "grad_norm": 0.4281521737575531,
      "learning_rate": 1.3529166666666667e-05,
      "loss": 0.0013,
      "step": 175060
    },
    {
      "epoch": 5.835666666666667,
      "grad_norm": 0.05760727822780609,
      "learning_rate": 1.3527083333333335e-05,
      "loss": 0.0014,
      "step": 175070
    },
    {
      "epoch": 5.836,
      "grad_norm": 0.03201916441321373,
      "learning_rate": 1.3525000000000002e-05,
      "loss": 0.002,
      "step": 175080
    },
    {
      "epoch": 5.836333333333333,
      "grad_norm": 0.14285477995872498,
      "learning_rate": 1.3522916666666669e-05,
      "loss": 0.002,
      "step": 175090
    },
    {
      "epoch": 5.836666666666667,
      "grad_norm": 0.05773548036813736,
      "learning_rate": 1.3520833333333336e-05,
      "loss": 0.0021,
      "step": 175100
    },
    {
      "epoch": 5.837,
      "grad_norm": 0.39950504899024963,
      "learning_rate": 1.351875e-05,
      "loss": 0.0014,
      "step": 175110
    },
    {
      "epoch": 5.8373333333333335,
      "grad_norm": 0.05855225771665573,
      "learning_rate": 1.3516666666666667e-05,
      "loss": 0.0016,
      "step": 175120
    },
    {
      "epoch": 5.837666666666666,
      "grad_norm": 0.22817735373973846,
      "learning_rate": 1.3514583333333333e-05,
      "loss": 0.0018,
      "step": 175130
    },
    {
      "epoch": 5.838,
      "grad_norm": 0.11508243530988693,
      "learning_rate": 1.35125e-05,
      "loss": 0.002,
      "step": 175140
    },
    {
      "epoch": 5.838333333333333,
      "grad_norm": 0.17254538834095,
      "learning_rate": 1.3510416666666667e-05,
      "loss": 0.0013,
      "step": 175150
    },
    {
      "epoch": 5.838666666666667,
      "grad_norm": 0.11401071399450302,
      "learning_rate": 1.3508333333333334e-05,
      "loss": 0.0017,
      "step": 175160
    },
    {
      "epoch": 5.839,
      "grad_norm": 0.0580097995698452,
      "learning_rate": 1.3506250000000001e-05,
      "loss": 0.0017,
      "step": 175170
    },
    {
      "epoch": 5.839333333333333,
      "grad_norm": 0.42854243516921997,
      "learning_rate": 1.3504166666666669e-05,
      "loss": 0.0019,
      "step": 175180
    },
    {
      "epoch": 5.839666666666667,
      "grad_norm": 0.05802397429943085,
      "learning_rate": 1.3502083333333334e-05,
      "loss": 0.0016,
      "step": 175190
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.1425025463104248,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0018,
      "step": 175200
    },
    {
      "epoch": 5.840333333333334,
      "grad_norm": 0.342774361371994,
      "learning_rate": 1.3497916666666668e-05,
      "loss": 0.0023,
      "step": 175210
    },
    {
      "epoch": 5.8406666666666665,
      "grad_norm": 0.061330799013376236,
      "learning_rate": 1.3495833333333336e-05,
      "loss": 0.0019,
      "step": 175220
    },
    {
      "epoch": 5.841,
      "grad_norm": 0.2290254831314087,
      "learning_rate": 1.349375e-05,
      "loss": 0.0016,
      "step": 175230
    },
    {
      "epoch": 5.841333333333333,
      "grad_norm": 0.2572237551212311,
      "learning_rate": 1.3491666666666667e-05,
      "loss": 0.0013,
      "step": 175240
    },
    {
      "epoch": 5.841666666666667,
      "grad_norm": 0.2575606405735016,
      "learning_rate": 1.3489583333333334e-05,
      "loss": 0.0017,
      "step": 175250
    },
    {
      "epoch": 5.842,
      "grad_norm": 0.22854964435100555,
      "learning_rate": 1.3487500000000001e-05,
      "loss": 0.0023,
      "step": 175260
    },
    {
      "epoch": 5.842333333333333,
      "grad_norm": 0.1142028421163559,
      "learning_rate": 1.3485416666666666e-05,
      "loss": 0.0021,
      "step": 175270
    },
    {
      "epoch": 5.842666666666666,
      "grad_norm": 0.2581025958061218,
      "learning_rate": 1.3483333333333334e-05,
      "loss": 0.0015,
      "step": 175280
    },
    {
      "epoch": 5.843,
      "grad_norm": 0.08613722771406174,
      "learning_rate": 1.348125e-05,
      "loss": 0.0021,
      "step": 175290
    },
    {
      "epoch": 5.843333333333334,
      "grad_norm": 0.19960428774356842,
      "learning_rate": 1.3479166666666668e-05,
      "loss": 0.0016,
      "step": 175300
    },
    {
      "epoch": 5.843666666666667,
      "grad_norm": 0.2280925065279007,
      "learning_rate": 1.3477083333333335e-05,
      "loss": 0.0024,
      "step": 175310
    },
    {
      "epoch": 5.844,
      "grad_norm": 0.11477652937173843,
      "learning_rate": 1.3475000000000002e-05,
      "loss": 0.0012,
      "step": 175320
    },
    {
      "epoch": 5.844333333333333,
      "grad_norm": 0.06617199629545212,
      "learning_rate": 1.3472916666666668e-05,
      "loss": 0.002,
      "step": 175330
    },
    {
      "epoch": 5.844666666666667,
      "grad_norm": 0.3709932267665863,
      "learning_rate": 1.3470833333333335e-05,
      "loss": 0.0015,
      "step": 175340
    },
    {
      "epoch": 5.845,
      "grad_norm": 0.032190512865781784,
      "learning_rate": 1.3468749999999999e-05,
      "loss": 0.0023,
      "step": 175350
    },
    {
      "epoch": 5.8453333333333335,
      "grad_norm": 0.05914546549320221,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0012,
      "step": 175360
    },
    {
      "epoch": 5.845666666666666,
      "grad_norm": 0.03135939687490463,
      "learning_rate": 1.3464583333333333e-05,
      "loss": 0.0015,
      "step": 175370
    },
    {
      "epoch": 5.846,
      "grad_norm": 0.4570249021053314,
      "learning_rate": 1.34625e-05,
      "loss": 0.0016,
      "step": 175380
    },
    {
      "epoch": 5.846333333333334,
      "grad_norm": 0.19973301887512207,
      "learning_rate": 1.3460416666666667e-05,
      "loss": 0.0014,
      "step": 175390
    },
    {
      "epoch": 5.846666666666667,
      "grad_norm": 0.1428709775209427,
      "learning_rate": 1.3458333333333335e-05,
      "loss": 0.0012,
      "step": 175400
    },
    {
      "epoch": 5.8469999999999995,
      "grad_norm": 0.2321111559867859,
      "learning_rate": 1.345625e-05,
      "loss": 0.0018,
      "step": 175410
    },
    {
      "epoch": 5.847333333333333,
      "grad_norm": 0.011944134719669819,
      "learning_rate": 1.3454166666666667e-05,
      "loss": 0.0011,
      "step": 175420
    },
    {
      "epoch": 5.847666666666667,
      "grad_norm": 0.26278823614120483,
      "learning_rate": 1.3452083333333334e-05,
      "loss": 0.0022,
      "step": 175430
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.08549358695745468,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 0.0015,
      "step": 175440
    },
    {
      "epoch": 5.848333333333334,
      "grad_norm": 0.1148630678653717,
      "learning_rate": 1.3447916666666669e-05,
      "loss": 0.0019,
      "step": 175450
    },
    {
      "epoch": 5.8486666666666665,
      "grad_norm": 0.057466041296720505,
      "learning_rate": 1.3445833333333336e-05,
      "loss": 0.0018,
      "step": 175460
    },
    {
      "epoch": 5.849,
      "grad_norm": 0.02871578559279442,
      "learning_rate": 1.344375e-05,
      "loss": 0.0015,
      "step": 175470
    },
    {
      "epoch": 5.849333333333333,
      "grad_norm": 0.028956415131688118,
      "learning_rate": 1.3441666666666667e-05,
      "loss": 0.0015,
      "step": 175480
    },
    {
      "epoch": 5.849666666666667,
      "grad_norm": 0.030532196164131165,
      "learning_rate": 1.3439583333333332e-05,
      "loss": 0.0019,
      "step": 175490
    },
    {
      "epoch": 5.85,
      "grad_norm": 0.46927163004875183,
      "learning_rate": 1.34375e-05,
      "loss": 0.0014,
      "step": 175500
    },
    {
      "epoch": 5.850333333333333,
      "grad_norm": 0.37074044346809387,
      "learning_rate": 1.3435416666666667e-05,
      "loss": 0.0024,
      "step": 175510
    },
    {
      "epoch": 5.850666666666667,
      "grad_norm": 0.05846118927001953,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.0019,
      "step": 175520
    },
    {
      "epoch": 5.851,
      "grad_norm": 0.14307305216789246,
      "learning_rate": 1.3431250000000001e-05,
      "loss": 0.0019,
      "step": 175530
    },
    {
      "epoch": 5.851333333333334,
      "grad_norm": 0.37143656611442566,
      "learning_rate": 1.3429166666666668e-05,
      "loss": 0.0013,
      "step": 175540
    },
    {
      "epoch": 5.851666666666667,
      "grad_norm": 0.32856324315071106,
      "learning_rate": 1.3427083333333334e-05,
      "loss": 0.002,
      "step": 175550
    },
    {
      "epoch": 5.852,
      "grad_norm": 0.05772379785776138,
      "learning_rate": 1.3425000000000001e-05,
      "loss": 0.0019,
      "step": 175560
    },
    {
      "epoch": 5.852333333333333,
      "grad_norm": 0.2284380942583084,
      "learning_rate": 1.3422916666666668e-05,
      "loss": 0.0019,
      "step": 175570
    },
    {
      "epoch": 5.852666666666667,
      "grad_norm": 0.25704237818717957,
      "learning_rate": 1.3420833333333335e-05,
      "loss": 0.0016,
      "step": 175580
    },
    {
      "epoch": 5.853,
      "grad_norm": 0.08649736642837524,
      "learning_rate": 1.341875e-05,
      "loss": 0.0014,
      "step": 175590
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 0.17158682644367218,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.0024,
      "step": 175600
    },
    {
      "epoch": 5.853666666666666,
      "grad_norm": 0.45752033591270447,
      "learning_rate": 1.3414583333333334e-05,
      "loss": 0.0023,
      "step": 175610
    },
    {
      "epoch": 5.854,
      "grad_norm": 0.12163962423801422,
      "learning_rate": 1.34125e-05,
      "loss": 0.0025,
      "step": 175620
    },
    {
      "epoch": 5.854333333333333,
      "grad_norm": 0.04855787754058838,
      "learning_rate": 1.3410416666666666e-05,
      "loss": 0.0015,
      "step": 175630
    },
    {
      "epoch": 5.854666666666667,
      "grad_norm": 0.25732019543647766,
      "learning_rate": 1.3408333333333333e-05,
      "loss": 0.0012,
      "step": 175640
    },
    {
      "epoch": 5.855,
      "grad_norm": 0.200118750333786,
      "learning_rate": 1.340625e-05,
      "loss": 0.0014,
      "step": 175650
    },
    {
      "epoch": 5.855333333333333,
      "grad_norm": 0.11486010998487473,
      "learning_rate": 1.3404166666666668e-05,
      "loss": 0.0013,
      "step": 175660
    },
    {
      "epoch": 5.855666666666667,
      "grad_norm": 0.17214246094226837,
      "learning_rate": 1.3402083333333335e-05,
      "loss": 0.002,
      "step": 175670
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.14256064593791962,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0016,
      "step": 175680
    },
    {
      "epoch": 5.856333333333334,
      "grad_norm": 0.22943714261054993,
      "learning_rate": 1.3397916666666668e-05,
      "loss": 0.0014,
      "step": 175690
    },
    {
      "epoch": 5.8566666666666665,
      "grad_norm": 0.11449196934700012,
      "learning_rate": 1.3395833333333335e-05,
      "loss": 0.0026,
      "step": 175700
    },
    {
      "epoch": 5.857,
      "grad_norm": 0.08582888543605804,
      "learning_rate": 1.3393749999999999e-05,
      "loss": 0.0018,
      "step": 175710
    },
    {
      "epoch": 5.857333333333333,
      "grad_norm": 0.3139229416847229,
      "learning_rate": 1.3391666666666666e-05,
      "loss": 0.0016,
      "step": 175720
    },
    {
      "epoch": 5.857666666666667,
      "grad_norm": 0.05815565958619118,
      "learning_rate": 1.3389583333333333e-05,
      "loss": 0.0014,
      "step": 175730
    },
    {
      "epoch": 5.858,
      "grad_norm": 0.11563177406787872,
      "learning_rate": 1.33875e-05,
      "loss": 0.0016,
      "step": 175740
    },
    {
      "epoch": 5.858333333333333,
      "grad_norm": 0.14319628477096558,
      "learning_rate": 1.3385416666666667e-05,
      "loss": 0.002,
      "step": 175750
    },
    {
      "epoch": 5.858666666666666,
      "grad_norm": 0.08579451590776443,
      "learning_rate": 1.3383333333333335e-05,
      "loss": 0.003,
      "step": 175760
    },
    {
      "epoch": 5.859,
      "grad_norm": 0.38765469193458557,
      "learning_rate": 1.338125e-05,
      "loss": 0.0014,
      "step": 175770
    },
    {
      "epoch": 5.859333333333334,
      "grad_norm": 0.269260972738266,
      "learning_rate": 1.3379166666666667e-05,
      "loss": 0.0015,
      "step": 175780
    },
    {
      "epoch": 5.859666666666667,
      "grad_norm": 0.22868986427783966,
      "learning_rate": 1.3377083333333334e-05,
      "loss": 0.0022,
      "step": 175790
    },
    {
      "epoch": 5.86,
      "grad_norm": 0.18453580141067505,
      "learning_rate": 1.3375000000000002e-05,
      "loss": 0.0017,
      "step": 175800
    },
    {
      "epoch": 5.860333333333333,
      "grad_norm": 0.4591316282749176,
      "learning_rate": 1.3372916666666669e-05,
      "loss": 0.0013,
      "step": 175810
    },
    {
      "epoch": 5.860666666666667,
      "grad_norm": 0.22899970412254333,
      "learning_rate": 1.3370833333333336e-05,
      "loss": 0.0016,
      "step": 175820
    },
    {
      "epoch": 5.861,
      "grad_norm": 0.030912505462765694,
      "learning_rate": 1.336875e-05,
      "loss": 0.0015,
      "step": 175830
    },
    {
      "epoch": 5.8613333333333335,
      "grad_norm": 0.08649847656488419,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.002,
      "step": 175840
    },
    {
      "epoch": 5.861666666666666,
      "grad_norm": 0.11546715348958969,
      "learning_rate": 1.3364583333333332e-05,
      "loss": 0.0013,
      "step": 175850
    },
    {
      "epoch": 5.862,
      "grad_norm": 0.34281662106513977,
      "learning_rate": 1.33625e-05,
      "loss": 0.0024,
      "step": 175860
    },
    {
      "epoch": 5.862333333333333,
      "grad_norm": 0.03008524887263775,
      "learning_rate": 1.3360416666666667e-05,
      "loss": 0.0025,
      "step": 175870
    },
    {
      "epoch": 5.862666666666667,
      "grad_norm": 0.17120952904224396,
      "learning_rate": 1.3358333333333334e-05,
      "loss": 0.0027,
      "step": 175880
    },
    {
      "epoch": 5.8629999999999995,
      "grad_norm": 0.4002632200717926,
      "learning_rate": 1.3356250000000001e-05,
      "loss": 0.0015,
      "step": 175890
    },
    {
      "epoch": 5.863333333333333,
      "grad_norm": 0.02967217192053795,
      "learning_rate": 1.3354166666666668e-05,
      "loss": 0.0023,
      "step": 175900
    },
    {
      "epoch": 5.863666666666667,
      "grad_norm": 0.20016388595104218,
      "learning_rate": 1.3352083333333334e-05,
      "loss": 0.0017,
      "step": 175910
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.17125698924064636,
      "learning_rate": 1.3350000000000001e-05,
      "loss": 0.0017,
      "step": 175920
    },
    {
      "epoch": 5.864333333333334,
      "grad_norm": 0.22889812290668488,
      "learning_rate": 1.3347916666666668e-05,
      "loss": 0.0017,
      "step": 175930
    },
    {
      "epoch": 5.8646666666666665,
      "grad_norm": 0.1713968813419342,
      "learning_rate": 1.3345833333333335e-05,
      "loss": 0.0014,
      "step": 175940
    },
    {
      "epoch": 5.865,
      "grad_norm": 0.11531998217105865,
      "learning_rate": 1.3343749999999999e-05,
      "loss": 0.0025,
      "step": 175950
    },
    {
      "epoch": 5.865333333333333,
      "grad_norm": 0.05751868709921837,
      "learning_rate": 1.3341666666666666e-05,
      "loss": 0.0025,
      "step": 175960
    },
    {
      "epoch": 5.865666666666667,
      "grad_norm": 0.11546102911233902,
      "learning_rate": 1.3339583333333333e-05,
      "loss": 0.002,
      "step": 175970
    },
    {
      "epoch": 5.866,
      "grad_norm": 0.22940589487552643,
      "learning_rate": 1.33375e-05,
      "loss": 0.0015,
      "step": 175980
    },
    {
      "epoch": 5.866333333333333,
      "grad_norm": 0.2571764886379242,
      "learning_rate": 1.3335416666666666e-05,
      "loss": 0.0013,
      "step": 175990
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.030703527852892876,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0024,
      "step": 176000
    },
    {
      "epoch": 5.867,
      "grad_norm": 0.08604682981967926,
      "learning_rate": 1.333125e-05,
      "loss": 0.0021,
      "step": 176010
    },
    {
      "epoch": 5.867333333333333,
      "grad_norm": 0.5426848530769348,
      "learning_rate": 1.3329166666666668e-05,
      "loss": 0.0015,
      "step": 176020
    },
    {
      "epoch": 5.867666666666667,
      "grad_norm": 0.21371179819107056,
      "learning_rate": 1.3327083333333335e-05,
      "loss": 0.0014,
      "step": 176030
    },
    {
      "epoch": 5.868,
      "grad_norm": 0.40641433000564575,
      "learning_rate": 1.3325000000000002e-05,
      "loss": 0.0017,
      "step": 176040
    },
    {
      "epoch": 5.868333333333333,
      "grad_norm": 0.11646919697523117,
      "learning_rate": 1.3322916666666668e-05,
      "loss": 0.0022,
      "step": 176050
    },
    {
      "epoch": 5.868666666666667,
      "grad_norm": 0.2005598396062851,
      "learning_rate": 1.3320833333333335e-05,
      "loss": 0.0015,
      "step": 176060
    },
    {
      "epoch": 5.869,
      "grad_norm": 0.34238719940185547,
      "learning_rate": 1.3318749999999998e-05,
      "loss": 0.0013,
      "step": 176070
    },
    {
      "epoch": 5.8693333333333335,
      "grad_norm": 0.4281899333000183,
      "learning_rate": 1.3316666666666666e-05,
      "loss": 0.0028,
      "step": 176080
    },
    {
      "epoch": 5.869666666666666,
      "grad_norm": 0.2854610085487366,
      "learning_rate": 1.3314583333333333e-05,
      "loss": 0.0015,
      "step": 176090
    },
    {
      "epoch": 5.87,
      "grad_norm": 0.05733231082558632,
      "learning_rate": 1.33125e-05,
      "loss": 0.0016,
      "step": 176100
    },
    {
      "epoch": 5.870333333333333,
      "grad_norm": 0.05850335583090782,
      "learning_rate": 1.3310416666666667e-05,
      "loss": 0.002,
      "step": 176110
    },
    {
      "epoch": 5.870666666666667,
      "grad_norm": 0.22903434932231903,
      "learning_rate": 1.3308333333333334e-05,
      "loss": 0.0014,
      "step": 176120
    },
    {
      "epoch": 5.871,
      "grad_norm": 0.05901706591248512,
      "learning_rate": 1.330625e-05,
      "loss": 0.0018,
      "step": 176130
    },
    {
      "epoch": 5.871333333333333,
      "grad_norm": 0.2002175748348236,
      "learning_rate": 1.3304166666666667e-05,
      "loss": 0.0017,
      "step": 176140
    },
    {
      "epoch": 5.871666666666667,
      "grad_norm": 0.1147986575961113,
      "learning_rate": 1.3302083333333334e-05,
      "loss": 0.0018,
      "step": 176150
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.058484192937612534,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.0019,
      "step": 176160
    },
    {
      "epoch": 5.872333333333334,
      "grad_norm": 0.2002415955066681,
      "learning_rate": 1.3297916666666669e-05,
      "loss": 0.0018,
      "step": 176170
    },
    {
      "epoch": 5.8726666666666665,
      "grad_norm": 0.11445078998804092,
      "learning_rate": 1.3295833333333336e-05,
      "loss": 0.0019,
      "step": 176180
    },
    {
      "epoch": 5.873,
      "grad_norm": 0.1430349498987198,
      "learning_rate": 1.329375e-05,
      "loss": 0.0017,
      "step": 176190
    },
    {
      "epoch": 5.873333333333333,
      "grad_norm": 0.17160841822624207,
      "learning_rate": 1.3291666666666667e-05,
      "loss": 0.0015,
      "step": 176200
    },
    {
      "epoch": 5.873666666666667,
      "grad_norm": 0.14310972392559052,
      "learning_rate": 1.3289583333333332e-05,
      "loss": 0.0015,
      "step": 176210
    },
    {
      "epoch": 5.874,
      "grad_norm": 0.08750408887863159,
      "learning_rate": 1.32875e-05,
      "loss": 0.0015,
      "step": 176220
    },
    {
      "epoch": 5.874333333333333,
      "grad_norm": 0.1714469939470291,
      "learning_rate": 1.3285416666666667e-05,
      "loss": 0.0015,
      "step": 176230
    },
    {
      "epoch": 5.874666666666666,
      "grad_norm": 0.28499823808670044,
      "learning_rate": 1.3283333333333334e-05,
      "loss": 0.0018,
      "step": 176240
    },
    {
      "epoch": 5.875,
      "grad_norm": 0.05789626017212868,
      "learning_rate": 1.3281250000000001e-05,
      "loss": 0.0019,
      "step": 176250
    },
    {
      "epoch": 5.875333333333334,
      "grad_norm": 0.4564228355884552,
      "learning_rate": 1.3279166666666668e-05,
      "loss": 0.0016,
      "step": 176260
    },
    {
      "epoch": 5.875666666666667,
      "grad_norm": 0.12632788717746735,
      "learning_rate": 1.3277083333333334e-05,
      "loss": 0.0021,
      "step": 176270
    },
    {
      "epoch": 5.876,
      "grad_norm": 0.0858403667807579,
      "learning_rate": 1.3275e-05,
      "loss": 0.0014,
      "step": 176280
    },
    {
      "epoch": 5.876333333333333,
      "grad_norm": 0.17446674406528473,
      "learning_rate": 1.3272916666666668e-05,
      "loss": 0.0018,
      "step": 176290
    },
    {
      "epoch": 5.876666666666667,
      "grad_norm": 0.0775144174695015,
      "learning_rate": 1.3270833333333335e-05,
      "loss": 0.0018,
      "step": 176300
    },
    {
      "epoch": 5.877,
      "grad_norm": 0.34027642011642456,
      "learning_rate": 1.3268750000000002e-05,
      "loss": 0.0016,
      "step": 176310
    },
    {
      "epoch": 5.8773333333333335,
      "grad_norm": 0.20141863822937012,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0014,
      "step": 176320
    },
    {
      "epoch": 5.877666666666666,
      "grad_norm": 0.0071755144745111465,
      "learning_rate": 1.3264583333333333e-05,
      "loss": 0.0015,
      "step": 176330
    },
    {
      "epoch": 5.878,
      "grad_norm": 0.0299882423132658,
      "learning_rate": 1.32625e-05,
      "loss": 0.0022,
      "step": 176340
    },
    {
      "epoch": 5.878333333333333,
      "grad_norm": 0.031161950901150703,
      "learning_rate": 1.3260416666666666e-05,
      "loss": 0.0023,
      "step": 176350
    },
    {
      "epoch": 5.878666666666667,
      "grad_norm": 0.5035151243209839,
      "learning_rate": 1.3258333333333333e-05,
      "loss": 0.0018,
      "step": 176360
    },
    {
      "epoch": 5.879,
      "grad_norm": 0.058798376470804214,
      "learning_rate": 1.325625e-05,
      "loss": 0.0019,
      "step": 176370
    },
    {
      "epoch": 5.879333333333333,
      "grad_norm": 0.20024138689041138,
      "learning_rate": 1.3254166666666668e-05,
      "loss": 0.0014,
      "step": 176380
    },
    {
      "epoch": 5.879666666666667,
      "grad_norm": 0.273616760969162,
      "learning_rate": 1.3252083333333335e-05,
      "loss": 0.0016,
      "step": 176390
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.34345176815986633,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.0013,
      "step": 176400
    },
    {
      "epoch": 5.880333333333334,
      "grad_norm": 0.22847490012645721,
      "learning_rate": 1.3247916666666667e-05,
      "loss": 0.0017,
      "step": 176410
    },
    {
      "epoch": 5.8806666666666665,
      "grad_norm": 0.25716862082481384,
      "learning_rate": 1.3245833333333335e-05,
      "loss": 0.0022,
      "step": 176420
    },
    {
      "epoch": 5.881,
      "grad_norm": 0.25671762228012085,
      "learning_rate": 1.3243750000000002e-05,
      "loss": 0.0018,
      "step": 176430
    },
    {
      "epoch": 5.881333333333333,
      "grad_norm": 0.3137454390525818,
      "learning_rate": 1.3241666666666666e-05,
      "loss": 0.0021,
      "step": 176440
    },
    {
      "epoch": 5.881666666666667,
      "grad_norm": 0.05971147492527962,
      "learning_rate": 1.3239583333333333e-05,
      "loss": 0.002,
      "step": 176450
    },
    {
      "epoch": 5.882,
      "grad_norm": 0.029842671006917953,
      "learning_rate": 1.32375e-05,
      "loss": 0.0016,
      "step": 176460
    },
    {
      "epoch": 5.882333333333333,
      "grad_norm": 0.228628471493721,
      "learning_rate": 1.3235416666666667e-05,
      "loss": 0.0013,
      "step": 176470
    },
    {
      "epoch": 5.882666666666667,
      "grad_norm": 0.2416086494922638,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0019,
      "step": 176480
    },
    {
      "epoch": 5.883,
      "grad_norm": 0.34266209602355957,
      "learning_rate": 1.3231250000000001e-05,
      "loss": 0.0023,
      "step": 176490
    },
    {
      "epoch": 5.883333333333333,
      "grad_norm": 0.1999872624874115,
      "learning_rate": 1.3229166666666667e-05,
      "loss": 0.0016,
      "step": 176500
    },
    {
      "epoch": 5.883666666666667,
      "grad_norm": 0.00699885468930006,
      "learning_rate": 1.3227083333333334e-05,
      "loss": 0.0014,
      "step": 176510
    },
    {
      "epoch": 5.884,
      "grad_norm": 0.11464899778366089,
      "learning_rate": 1.3225000000000001e-05,
      "loss": 0.0016,
      "step": 176520
    },
    {
      "epoch": 5.884333333333333,
      "grad_norm": 0.5709384083747864,
      "learning_rate": 1.3222916666666668e-05,
      "loss": 0.0017,
      "step": 176530
    },
    {
      "epoch": 5.884666666666667,
      "grad_norm": 0.05833829566836357,
      "learning_rate": 1.3220833333333336e-05,
      "loss": 0.0016,
      "step": 176540
    },
    {
      "epoch": 5.885,
      "grad_norm": 0.11474181711673737,
      "learning_rate": 1.3218750000000001e-05,
      "loss": 0.0012,
      "step": 176550
    },
    {
      "epoch": 5.8853333333333335,
      "grad_norm": 0.11600951850414276,
      "learning_rate": 1.3216666666666667e-05,
      "loss": 0.0016,
      "step": 176560
    },
    {
      "epoch": 5.885666666666666,
      "grad_norm": 0.2981647253036499,
      "learning_rate": 1.3214583333333334e-05,
      "loss": 0.0025,
      "step": 176570
    },
    {
      "epoch": 5.886,
      "grad_norm": 0.08562592417001724,
      "learning_rate": 1.32125e-05,
      "loss": 0.0016,
      "step": 176580
    },
    {
      "epoch": 5.886333333333333,
      "grad_norm": 0.03037523850798607,
      "learning_rate": 1.3210416666666666e-05,
      "loss": 0.0014,
      "step": 176590
    },
    {
      "epoch": 5.886666666666667,
      "grad_norm": 0.3116796612739563,
      "learning_rate": 1.3208333333333334e-05,
      "loss": 0.0015,
      "step": 176600
    },
    {
      "epoch": 5.8870000000000005,
      "grad_norm": 0.37138351798057556,
      "learning_rate": 1.320625e-05,
      "loss": 0.0024,
      "step": 176610
    },
    {
      "epoch": 5.887333333333333,
      "grad_norm": 0.6121987104415894,
      "learning_rate": 1.3204166666666668e-05,
      "loss": 0.0024,
      "step": 176620
    },
    {
      "epoch": 5.887666666666667,
      "grad_norm": 0.4333764910697937,
      "learning_rate": 1.3202083333333335e-05,
      "loss": 0.0024,
      "step": 176630
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.4850929379463196,
      "learning_rate": 1.32e-05,
      "loss": 0.0013,
      "step": 176640
    },
    {
      "epoch": 5.888333333333334,
      "grad_norm": 0.21155518293380737,
      "learning_rate": 1.3197916666666668e-05,
      "loss": 0.0018,
      "step": 176650
    },
    {
      "epoch": 5.8886666666666665,
      "grad_norm": 0.19989901781082153,
      "learning_rate": 1.3195833333333335e-05,
      "loss": 0.0014,
      "step": 176660
    },
    {
      "epoch": 5.889,
      "grad_norm": 0.17106299102306366,
      "learning_rate": 1.3193750000000002e-05,
      "loss": 0.0027,
      "step": 176670
    },
    {
      "epoch": 5.889333333333333,
      "grad_norm": 0.015369493514299393,
      "learning_rate": 1.3191666666666666e-05,
      "loss": 0.0016,
      "step": 176680
    },
    {
      "epoch": 5.889666666666667,
      "grad_norm": 0.012334472499787807,
      "learning_rate": 1.3189583333333333e-05,
      "loss": 0.0019,
      "step": 176690
    },
    {
      "epoch": 5.89,
      "grad_norm": 0.21060174703598022,
      "learning_rate": 1.31875e-05,
      "loss": 0.0019,
      "step": 176700
    },
    {
      "epoch": 5.890333333333333,
      "grad_norm": 0.3045244514942169,
      "learning_rate": 1.3185416666666668e-05,
      "loss": 0.0015,
      "step": 176710
    },
    {
      "epoch": 5.890666666666666,
      "grad_norm": 0.14404328167438507,
      "learning_rate": 1.3183333333333333e-05,
      "loss": 0.0014,
      "step": 176720
    },
    {
      "epoch": 5.891,
      "grad_norm": 0.20009370148181915,
      "learning_rate": 1.318125e-05,
      "loss": 0.0025,
      "step": 176730
    },
    {
      "epoch": 5.891333333333334,
      "grad_norm": 0.34335893392562866,
      "learning_rate": 1.3179166666666667e-05,
      "loss": 0.0013,
      "step": 176740
    },
    {
      "epoch": 5.891666666666667,
      "grad_norm": 0.05735267698764801,
      "learning_rate": 1.3177083333333335e-05,
      "loss": 0.0022,
      "step": 176750
    },
    {
      "epoch": 5.892,
      "grad_norm": 0.24693511426448822,
      "learning_rate": 1.3175000000000002e-05,
      "loss": 0.0023,
      "step": 176760
    },
    {
      "epoch": 5.892333333333333,
      "grad_norm": 0.007908850908279419,
      "learning_rate": 1.3172916666666669e-05,
      "loss": 0.0012,
      "step": 176770
    },
    {
      "epoch": 5.892666666666667,
      "grad_norm": 0.2568747103214264,
      "learning_rate": 1.3170833333333334e-05,
      "loss": 0.0017,
      "step": 176780
    },
    {
      "epoch": 5.893,
      "grad_norm": 0.2569974660873413,
      "learning_rate": 1.3168750000000002e-05,
      "loss": 0.0018,
      "step": 176790
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 0.42727524042129517,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.0013,
      "step": 176800
    },
    {
      "epoch": 5.893666666666666,
      "grad_norm": 0.25764816999435425,
      "learning_rate": 1.3164583333333333e-05,
      "loss": 0.0019,
      "step": 176810
    },
    {
      "epoch": 5.894,
      "grad_norm": 0.08578433841466904,
      "learning_rate": 1.31625e-05,
      "loss": 0.0014,
      "step": 176820
    },
    {
      "epoch": 5.894333333333333,
      "grad_norm": 0.08594666421413422,
      "learning_rate": 1.3160416666666667e-05,
      "loss": 0.0014,
      "step": 176830
    },
    {
      "epoch": 5.894666666666667,
      "grad_norm": 0.17158791422843933,
      "learning_rate": 1.3158333333333334e-05,
      "loss": 0.0017,
      "step": 176840
    },
    {
      "epoch": 5.895,
      "grad_norm": 0.2860962450504303,
      "learning_rate": 1.3156250000000001e-05,
      "loss": 0.0021,
      "step": 176850
    },
    {
      "epoch": 5.895333333333333,
      "grad_norm": 0.2566078305244446,
      "learning_rate": 1.3154166666666667e-05,
      "loss": 0.0019,
      "step": 176860
    },
    {
      "epoch": 5.895666666666667,
      "grad_norm": 0.34334468841552734,
      "learning_rate": 1.3152083333333334e-05,
      "loss": 0.0018,
      "step": 176870
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.20346814393997192,
      "learning_rate": 1.3150000000000001e-05,
      "loss": 0.002,
      "step": 176880
    },
    {
      "epoch": 5.896333333333334,
      "grad_norm": 0.37090161442756653,
      "learning_rate": 1.3147916666666668e-05,
      "loss": 0.0015,
      "step": 176890
    },
    {
      "epoch": 5.8966666666666665,
      "grad_norm": 0.25695785880088806,
      "learning_rate": 1.3145833333333336e-05,
      "loss": 0.0029,
      "step": 176900
    },
    {
      "epoch": 5.897,
      "grad_norm": 0.1709795445203781,
      "learning_rate": 1.3143750000000003e-05,
      "loss": 0.0016,
      "step": 176910
    },
    {
      "epoch": 5.897333333333333,
      "grad_norm": 0.14335089921951294,
      "learning_rate": 1.3141666666666666e-05,
      "loss": 0.0017,
      "step": 176920
    },
    {
      "epoch": 5.897666666666667,
      "grad_norm": 0.0076041920110583305,
      "learning_rate": 1.3139583333333334e-05,
      "loss": 0.0019,
      "step": 176930
    },
    {
      "epoch": 5.898,
      "grad_norm": 0.17137585580348969,
      "learning_rate": 1.31375e-05,
      "loss": 0.0019,
      "step": 176940
    },
    {
      "epoch": 5.898333333333333,
      "grad_norm": 0.14389920234680176,
      "learning_rate": 1.3135416666666666e-05,
      "loss": 0.0025,
      "step": 176950
    },
    {
      "epoch": 5.898666666666666,
      "grad_norm": 0.17093579471111298,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.002,
      "step": 176960
    },
    {
      "epoch": 5.899,
      "grad_norm": 0.05844235420227051,
      "learning_rate": 1.313125e-05,
      "loss": 0.0017,
      "step": 176970
    },
    {
      "epoch": 5.899333333333333,
      "grad_norm": 0.05795186385512352,
      "learning_rate": 1.3129166666666668e-05,
      "loss": 0.0023,
      "step": 176980
    },
    {
      "epoch": 5.899666666666667,
      "grad_norm": 0.011728417128324509,
      "learning_rate": 1.3127083333333335e-05,
      "loss": 0.0015,
      "step": 176990
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.05886867642402649,
      "learning_rate": 1.3125e-05,
      "loss": 0.0015,
      "step": 177000
    },
    {
      "epoch": 5.900333333333333,
      "grad_norm": 0.03479530289769173,
      "learning_rate": 1.3122916666666668e-05,
      "loss": 0.0018,
      "step": 177010
    },
    {
      "epoch": 5.900666666666667,
      "grad_norm": 0.14330191910266876,
      "learning_rate": 1.3120833333333335e-05,
      "loss": 0.0019,
      "step": 177020
    },
    {
      "epoch": 5.901,
      "grad_norm": 0.40000268816947937,
      "learning_rate": 1.3118750000000002e-05,
      "loss": 0.0014,
      "step": 177030
    },
    {
      "epoch": 5.9013333333333335,
      "grad_norm": 0.466513067483902,
      "learning_rate": 1.3116666666666666e-05,
      "loss": 0.002,
      "step": 177040
    },
    {
      "epoch": 5.901666666666666,
      "grad_norm": 0.20045344531536102,
      "learning_rate": 1.3114583333333333e-05,
      "loss": 0.0015,
      "step": 177050
    },
    {
      "epoch": 5.902,
      "grad_norm": 0.16340048611164093,
      "learning_rate": 1.31125e-05,
      "loss": 0.0017,
      "step": 177060
    },
    {
      "epoch": 5.902333333333333,
      "grad_norm": 0.22848857939243317,
      "learning_rate": 1.3110416666666667e-05,
      "loss": 0.0014,
      "step": 177070
    },
    {
      "epoch": 5.902666666666667,
      "grad_norm": 0.05735866725444794,
      "learning_rate": 1.3108333333333333e-05,
      "loss": 0.0017,
      "step": 177080
    },
    {
      "epoch": 5.9030000000000005,
      "grad_norm": 0.0856127217411995,
      "learning_rate": 1.310625e-05,
      "loss": 0.0019,
      "step": 177090
    },
    {
      "epoch": 5.903333333333333,
      "grad_norm": 0.15660914778709412,
      "learning_rate": 1.3104166666666667e-05,
      "loss": 0.0019,
      "step": 177100
    },
    {
      "epoch": 5.903666666666666,
      "grad_norm": 0.4567676782608032,
      "learning_rate": 1.3102083333333334e-05,
      "loss": 0.0019,
      "step": 177110
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.05866299942135811,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0016,
      "step": 177120
    },
    {
      "epoch": 5.904333333333334,
      "grad_norm": 0.2011699378490448,
      "learning_rate": 1.3097916666666669e-05,
      "loss": 0.002,
      "step": 177130
    },
    {
      "epoch": 5.9046666666666665,
      "grad_norm": 0.14424702525138855,
      "learning_rate": 1.3095833333333334e-05,
      "loss": 0.0023,
      "step": 177140
    },
    {
      "epoch": 5.905,
      "grad_norm": 0.08564493060112,
      "learning_rate": 1.3093750000000001e-05,
      "loss": 0.0027,
      "step": 177150
    },
    {
      "epoch": 5.905333333333333,
      "grad_norm": 0.6345390677452087,
      "learning_rate": 1.3091666666666665e-05,
      "loss": 0.0022,
      "step": 177160
    },
    {
      "epoch": 5.905666666666667,
      "grad_norm": 0.48524701595306396,
      "learning_rate": 1.3089583333333332e-05,
      "loss": 0.002,
      "step": 177170
    },
    {
      "epoch": 5.906,
      "grad_norm": 0.11721357703208923,
      "learning_rate": 1.30875e-05,
      "loss": 0.0013,
      "step": 177180
    },
    {
      "epoch": 5.906333333333333,
      "grad_norm": 0.21224552392959595,
      "learning_rate": 1.3085416666666667e-05,
      "loss": 0.0027,
      "step": 177190
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 0.2859565019607544,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.0013,
      "step": 177200
    },
    {
      "epoch": 5.907,
      "grad_norm": 0.11416551470756531,
      "learning_rate": 1.3081250000000001e-05,
      "loss": 0.0018,
      "step": 177210
    },
    {
      "epoch": 5.907333333333334,
      "grad_norm": 0.5139464139938354,
      "learning_rate": 1.3079166666666667e-05,
      "loss": 0.0023,
      "step": 177220
    },
    {
      "epoch": 5.907666666666667,
      "grad_norm": 0.03007761389017105,
      "learning_rate": 1.3077083333333334e-05,
      "loss": 0.0015,
      "step": 177230
    },
    {
      "epoch": 5.908,
      "grad_norm": 0.00935700349509716,
      "learning_rate": 1.3075000000000001e-05,
      "loss": 0.0021,
      "step": 177240
    },
    {
      "epoch": 5.908333333333333,
      "grad_norm": 0.030062038451433182,
      "learning_rate": 1.3072916666666668e-05,
      "loss": 0.0018,
      "step": 177250
    },
    {
      "epoch": 5.908666666666667,
      "grad_norm": 0.34295547008514404,
      "learning_rate": 1.3070833333333335e-05,
      "loss": 0.0017,
      "step": 177260
    },
    {
      "epoch": 5.909,
      "grad_norm": 0.2003619223833084,
      "learning_rate": 1.3068750000000003e-05,
      "loss": 0.0014,
      "step": 177270
    },
    {
      "epoch": 5.9093333333333335,
      "grad_norm": 0.10345938056707382,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0017,
      "step": 177280
    },
    {
      "epoch": 5.909666666666666,
      "grad_norm": 0.14290131628513336,
      "learning_rate": 1.3064583333333334e-05,
      "loss": 0.0015,
      "step": 177290
    },
    {
      "epoch": 5.91,
      "grad_norm": 0.009876002557575703,
      "learning_rate": 1.3062499999999999e-05,
      "loss": 0.0014,
      "step": 177300
    },
    {
      "epoch": 5.910333333333333,
      "grad_norm": 0.19971556961536407,
      "learning_rate": 1.3060416666666666e-05,
      "loss": 0.0024,
      "step": 177310
    },
    {
      "epoch": 5.910666666666667,
      "grad_norm": 0.06011829525232315,
      "learning_rate": 1.3058333333333333e-05,
      "loss": 0.0016,
      "step": 177320
    },
    {
      "epoch": 5.911,
      "grad_norm": 0.05848093330860138,
      "learning_rate": 1.305625e-05,
      "loss": 0.0019,
      "step": 177330
    },
    {
      "epoch": 5.911333333333333,
      "grad_norm": 0.14308801293373108,
      "learning_rate": 1.3054166666666668e-05,
      "loss": 0.0014,
      "step": 177340
    },
    {
      "epoch": 5.911666666666667,
      "grad_norm": 0.14271774888038635,
      "learning_rate": 1.3052083333333335e-05,
      "loss": 0.0013,
      "step": 177350
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.05823041871190071,
      "learning_rate": 1.305e-05,
      "loss": 0.0015,
      "step": 177360
    },
    {
      "epoch": 5.912333333333334,
      "grad_norm": 0.17135277390480042,
      "learning_rate": 1.3047916666666668e-05,
      "loss": 0.003,
      "step": 177370
    },
    {
      "epoch": 5.9126666666666665,
      "grad_norm": 0.029826730489730835,
      "learning_rate": 1.3045833333333335e-05,
      "loss": 0.0019,
      "step": 177380
    },
    {
      "epoch": 5.913,
      "grad_norm": 0.2871410846710205,
      "learning_rate": 1.3043750000000002e-05,
      "loss": 0.0015,
      "step": 177390
    },
    {
      "epoch": 5.913333333333333,
      "grad_norm": 0.19964063167572021,
      "learning_rate": 1.3041666666666666e-05,
      "loss": 0.0017,
      "step": 177400
    },
    {
      "epoch": 5.913666666666667,
      "grad_norm": 0.0302597526460886,
      "learning_rate": 1.3039583333333333e-05,
      "loss": 0.0019,
      "step": 177410
    },
    {
      "epoch": 5.914,
      "grad_norm": 0.12383915483951569,
      "learning_rate": 1.30375e-05,
      "loss": 0.0018,
      "step": 177420
    },
    {
      "epoch": 5.914333333333333,
      "grad_norm": 0.40012848377227783,
      "learning_rate": 1.3035416666666667e-05,
      "loss": 0.0017,
      "step": 177430
    },
    {
      "epoch": 5.914666666666666,
      "grad_norm": 0.25715699791908264,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.0019,
      "step": 177440
    },
    {
      "epoch": 5.915,
      "grad_norm": 0.39237457513809204,
      "learning_rate": 1.303125e-05,
      "loss": 0.0012,
      "step": 177450
    },
    {
      "epoch": 5.915333333333333,
      "grad_norm": 0.4285522401332855,
      "learning_rate": 1.3029166666666667e-05,
      "loss": 0.0014,
      "step": 177460
    },
    {
      "epoch": 5.915666666666667,
      "grad_norm": 0.23053520917892456,
      "learning_rate": 1.3027083333333334e-05,
      "loss": 0.002,
      "step": 177470
    },
    {
      "epoch": 5.916,
      "grad_norm": 0.031468916684389114,
      "learning_rate": 1.3025000000000002e-05,
      "loss": 0.0018,
      "step": 177480
    },
    {
      "epoch": 5.916333333333333,
      "grad_norm": 0.17101947963237762,
      "learning_rate": 1.3022916666666669e-05,
      "loss": 0.0017,
      "step": 177490
    },
    {
      "epoch": 5.916666666666667,
      "grad_norm": 0.22798274457454681,
      "learning_rate": 1.3020833333333334e-05,
      "loss": 0.002,
      "step": 177500
    },
    {
      "epoch": 5.917,
      "grad_norm": 0.059532299637794495,
      "learning_rate": 1.3018750000000001e-05,
      "loss": 0.0011,
      "step": 177510
    },
    {
      "epoch": 5.917333333333334,
      "grad_norm": 0.11451294273138046,
      "learning_rate": 1.3016666666666669e-05,
      "loss": 0.002,
      "step": 177520
    },
    {
      "epoch": 5.917666666666666,
      "grad_norm": 0.22951757907867432,
      "learning_rate": 1.3014583333333332e-05,
      "loss": 0.0014,
      "step": 177530
    },
    {
      "epoch": 5.918,
      "grad_norm": 0.3718443512916565,
      "learning_rate": 1.30125e-05,
      "loss": 0.0016,
      "step": 177540
    },
    {
      "epoch": 5.918333333333333,
      "grad_norm": 0.08665481954813004,
      "learning_rate": 1.3010416666666667e-05,
      "loss": 0.0016,
      "step": 177550
    },
    {
      "epoch": 5.918666666666667,
      "grad_norm": 0.03061414323747158,
      "learning_rate": 1.3008333333333334e-05,
      "loss": 0.0019,
      "step": 177560
    },
    {
      "epoch": 5.9190000000000005,
      "grad_norm": 0.2574734389781952,
      "learning_rate": 1.3006250000000001e-05,
      "loss": 0.0022,
      "step": 177570
    },
    {
      "epoch": 5.919333333333333,
      "grad_norm": 0.11436742544174194,
      "learning_rate": 1.3004166666666667e-05,
      "loss": 0.0026,
      "step": 177580
    },
    {
      "epoch": 5.919666666666666,
      "grad_norm": 0.03166395425796509,
      "learning_rate": 1.3002083333333334e-05,
      "loss": 0.0015,
      "step": 177590
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.14246174693107605,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0021,
      "step": 177600
    },
    {
      "epoch": 5.920333333333334,
      "grad_norm": 0.03143198788166046,
      "learning_rate": 1.2997916666666668e-05,
      "loss": 0.0014,
      "step": 177610
    },
    {
      "epoch": 5.9206666666666665,
      "grad_norm": 0.05747142806649208,
      "learning_rate": 1.2995833333333335e-05,
      "loss": 0.0016,
      "step": 177620
    },
    {
      "epoch": 5.921,
      "grad_norm": 0.3424200117588043,
      "learning_rate": 1.2993750000000002e-05,
      "loss": 0.0018,
      "step": 177630
    },
    {
      "epoch": 5.921333333333333,
      "grad_norm": 0.05733177065849304,
      "learning_rate": 1.2991666666666668e-05,
      "loss": 0.0021,
      "step": 177640
    },
    {
      "epoch": 5.921666666666667,
      "grad_norm": 0.06044027581810951,
      "learning_rate": 1.2989583333333333e-05,
      "loss": 0.0018,
      "step": 177650
    },
    {
      "epoch": 5.922,
      "grad_norm": 0.2476593255996704,
      "learning_rate": 1.2987499999999999e-05,
      "loss": 0.0018,
      "step": 177660
    },
    {
      "epoch": 5.9223333333333334,
      "grad_norm": 0.233029305934906,
      "learning_rate": 1.2985416666666666e-05,
      "loss": 0.0016,
      "step": 177670
    },
    {
      "epoch": 5.922666666666666,
      "grad_norm": 0.17181602120399475,
      "learning_rate": 1.2983333333333333e-05,
      "loss": 0.0017,
      "step": 177680
    },
    {
      "epoch": 5.923,
      "grad_norm": 0.14144553244113922,
      "learning_rate": 1.298125e-05,
      "loss": 0.0016,
      "step": 177690
    },
    {
      "epoch": 5.923333333333334,
      "grad_norm": 0.17161448299884796,
      "learning_rate": 1.2979166666666668e-05,
      "loss": 0.0018,
      "step": 177700
    },
    {
      "epoch": 5.923666666666667,
      "grad_norm": 0.2865579128265381,
      "learning_rate": 1.2977083333333335e-05,
      "loss": 0.0022,
      "step": 177710
    },
    {
      "epoch": 5.924,
      "grad_norm": 0.058171357959508896,
      "learning_rate": 1.2975e-05,
      "loss": 0.0014,
      "step": 177720
    },
    {
      "epoch": 5.924333333333333,
      "grad_norm": 0.22946122288703918,
      "learning_rate": 1.2972916666666667e-05,
      "loss": 0.0014,
      "step": 177730
    },
    {
      "epoch": 5.924666666666667,
      "grad_norm": 0.08932579308748245,
      "learning_rate": 1.2970833333333335e-05,
      "loss": 0.002,
      "step": 177740
    },
    {
      "epoch": 5.925,
      "grad_norm": 0.1718495786190033,
      "learning_rate": 1.2968750000000002e-05,
      "loss": 0.0018,
      "step": 177750
    },
    {
      "epoch": 5.925333333333334,
      "grad_norm": 0.08538682013750076,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0018,
      "step": 177760
    },
    {
      "epoch": 5.925666666666666,
      "grad_norm": 0.08672259002923965,
      "learning_rate": 1.2964583333333333e-05,
      "loss": 0.002,
      "step": 177770
    },
    {
      "epoch": 5.926,
      "grad_norm": 0.3424156606197357,
      "learning_rate": 1.29625e-05,
      "loss": 0.0017,
      "step": 177780
    },
    {
      "epoch": 5.926333333333333,
      "grad_norm": 0.18440116941928864,
      "learning_rate": 1.2960416666666667e-05,
      "loss": 0.0019,
      "step": 177790
    },
    {
      "epoch": 5.926666666666667,
      "grad_norm": 0.3294917345046997,
      "learning_rate": 1.2958333333333333e-05,
      "loss": 0.0019,
      "step": 177800
    },
    {
      "epoch": 5.927,
      "grad_norm": 0.007906300947070122,
      "learning_rate": 1.295625e-05,
      "loss": 0.0014,
      "step": 177810
    },
    {
      "epoch": 5.927333333333333,
      "grad_norm": 0.03113941289484501,
      "learning_rate": 1.2954166666666667e-05,
      "loss": 0.0014,
      "step": 177820
    },
    {
      "epoch": 5.927666666666667,
      "grad_norm": 0.14268562197685242,
      "learning_rate": 1.2952083333333334e-05,
      "loss": 0.0018,
      "step": 177830
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.20182554423809052,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 0.002,
      "step": 177840
    },
    {
      "epoch": 5.928333333333334,
      "grad_norm": 0.057700250297784805,
      "learning_rate": 1.2947916666666669e-05,
      "loss": 0.0016,
      "step": 177850
    },
    {
      "epoch": 5.9286666666666665,
      "grad_norm": 0.004784391727298498,
      "learning_rate": 1.2945833333333334e-05,
      "loss": 0.0014,
      "step": 177860
    },
    {
      "epoch": 5.929,
      "grad_norm": 0.08585390448570251,
      "learning_rate": 1.2943750000000001e-05,
      "loss": 0.0022,
      "step": 177870
    },
    {
      "epoch": 5.929333333333333,
      "grad_norm": 0.19973668456077576,
      "learning_rate": 1.2941666666666668e-05,
      "loss": 0.0016,
      "step": 177880
    },
    {
      "epoch": 5.929666666666667,
      "grad_norm": 0.01975053735077381,
      "learning_rate": 1.2939583333333332e-05,
      "loss": 0.0017,
      "step": 177890
    },
    {
      "epoch": 5.93,
      "grad_norm": 0.17138086259365082,
      "learning_rate": 1.29375e-05,
      "loss": 0.0016,
      "step": 177900
    },
    {
      "epoch": 5.9303333333333335,
      "grad_norm": 0.2607412040233612,
      "learning_rate": 1.2935416666666667e-05,
      "loss": 0.0015,
      "step": 177910
    },
    {
      "epoch": 5.930666666666666,
      "grad_norm": 0.14258769154548645,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0018,
      "step": 177920
    },
    {
      "epoch": 5.931,
      "grad_norm": 0.3442875146865845,
      "learning_rate": 1.2931250000000001e-05,
      "loss": 0.0022,
      "step": 177930
    },
    {
      "epoch": 5.931333333333333,
      "grad_norm": 0.0294883381575346,
      "learning_rate": 1.2929166666666666e-05,
      "loss": 0.0015,
      "step": 177940
    },
    {
      "epoch": 5.931666666666667,
      "grad_norm": 0.34826767444610596,
      "learning_rate": 1.2927083333333334e-05,
      "loss": 0.0022,
      "step": 177950
    },
    {
      "epoch": 5.932,
      "grad_norm": 0.2859179675579071,
      "learning_rate": 1.2925e-05,
      "loss": 0.0022,
      "step": 177960
    },
    {
      "epoch": 5.932333333333333,
      "grad_norm": 0.05824275314807892,
      "learning_rate": 1.2922916666666668e-05,
      "loss": 0.0016,
      "step": 177970
    },
    {
      "epoch": 5.932666666666667,
      "grad_norm": 0.08553589135408401,
      "learning_rate": 1.2920833333333335e-05,
      "loss": 0.0017,
      "step": 177980
    },
    {
      "epoch": 5.933,
      "grad_norm": 0.5569067597389221,
      "learning_rate": 1.2918750000000002e-05,
      "loss": 0.0014,
      "step": 177990
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.030001787468791008,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0019,
      "step": 178000
    },
    {
      "epoch": 5.933666666666666,
      "grad_norm": 0.39691951870918274,
      "learning_rate": 1.2914583333333333e-05,
      "loss": 0.0019,
      "step": 178010
    },
    {
      "epoch": 5.934,
      "grad_norm": 0.20315349102020264,
      "learning_rate": 1.2912499999999999e-05,
      "loss": 0.0015,
      "step": 178020
    },
    {
      "epoch": 5.934333333333333,
      "grad_norm": 0.05723746493458748,
      "learning_rate": 1.2910416666666666e-05,
      "loss": 0.0021,
      "step": 178030
    },
    {
      "epoch": 5.934666666666667,
      "grad_norm": 0.0859997421503067,
      "learning_rate": 1.2908333333333333e-05,
      "loss": 0.0016,
      "step": 178040
    },
    {
      "epoch": 5.9350000000000005,
      "grad_norm": 0.22850900888442993,
      "learning_rate": 1.290625e-05,
      "loss": 0.0018,
      "step": 178050
    },
    {
      "epoch": 5.935333333333333,
      "grad_norm": 0.1495702862739563,
      "learning_rate": 1.2904166666666667e-05,
      "loss": 0.0015,
      "step": 178060
    },
    {
      "epoch": 5.935666666666666,
      "grad_norm": 0.14338409900665283,
      "learning_rate": 1.2902083333333335e-05,
      "loss": 0.0018,
      "step": 178070
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.423536092042923,
      "learning_rate": 1.29e-05,
      "loss": 0.0017,
      "step": 178080
    },
    {
      "epoch": 5.936333333333334,
      "grad_norm": 0.17074035108089447,
      "learning_rate": 1.2897916666666667e-05,
      "loss": 0.0013,
      "step": 178090
    },
    {
      "epoch": 5.9366666666666665,
      "grad_norm": 0.36353132128715515,
      "learning_rate": 1.2895833333333335e-05,
      "loss": 0.0027,
      "step": 178100
    },
    {
      "epoch": 5.937,
      "grad_norm": 0.030459117144346237,
      "learning_rate": 1.2893750000000002e-05,
      "loss": 0.0027,
      "step": 178110
    },
    {
      "epoch": 5.937333333333333,
      "grad_norm": 0.2288515865802765,
      "learning_rate": 1.2891666666666669e-05,
      "loss": 0.0015,
      "step": 178120
    },
    {
      "epoch": 5.937666666666667,
      "grad_norm": 0.08609113097190857,
      "learning_rate": 1.2889583333333333e-05,
      "loss": 0.0015,
      "step": 178130
    },
    {
      "epoch": 5.938,
      "grad_norm": 0.12075184285640717,
      "learning_rate": 1.28875e-05,
      "loss": 0.0022,
      "step": 178140
    },
    {
      "epoch": 5.9383333333333335,
      "grad_norm": 0.03375617414712906,
      "learning_rate": 1.2885416666666667e-05,
      "loss": 0.0017,
      "step": 178150
    },
    {
      "epoch": 5.938666666666666,
      "grad_norm": 0.0867549404501915,
      "learning_rate": 1.2883333333333333e-05,
      "loss": 0.0019,
      "step": 178160
    },
    {
      "epoch": 5.939,
      "grad_norm": 0.17368589341640472,
      "learning_rate": 1.288125e-05,
      "loss": 0.0017,
      "step": 178170
    },
    {
      "epoch": 5.939333333333334,
      "grad_norm": 0.2908610999584198,
      "learning_rate": 1.2879166666666667e-05,
      "loss": 0.0021,
      "step": 178180
    },
    {
      "epoch": 5.939666666666667,
      "grad_norm": 0.026722975075244904,
      "learning_rate": 1.2877083333333334e-05,
      "loss": 0.0019,
      "step": 178190
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 0.08634543418884277,
      "learning_rate": 1.2875000000000001e-05,
      "loss": 0.0015,
      "step": 178200
    },
    {
      "epoch": 5.940333333333333,
      "grad_norm": 0.2002241462469101,
      "learning_rate": 1.2872916666666668e-05,
      "loss": 0.0021,
      "step": 178210
    },
    {
      "epoch": 5.940666666666667,
      "grad_norm": 0.05743907392024994,
      "learning_rate": 1.2870833333333334e-05,
      "loss": 0.0015,
      "step": 178220
    },
    {
      "epoch": 5.941,
      "grad_norm": 0.22861821949481964,
      "learning_rate": 1.2868750000000001e-05,
      "loss": 0.0018,
      "step": 178230
    },
    {
      "epoch": 5.941333333333334,
      "grad_norm": 0.7127071022987366,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0023,
      "step": 178240
    },
    {
      "epoch": 5.941666666666666,
      "grad_norm": 0.25703758001327515,
      "learning_rate": 1.2864583333333332e-05,
      "loss": 0.0014,
      "step": 178250
    },
    {
      "epoch": 5.942,
      "grad_norm": 0.143719881772995,
      "learning_rate": 1.28625e-05,
      "loss": 0.0015,
      "step": 178260
    },
    {
      "epoch": 5.942333333333333,
      "grad_norm": 0.09032871574163437,
      "learning_rate": 1.2860416666666666e-05,
      "loss": 0.0013,
      "step": 178270
    },
    {
      "epoch": 5.942666666666667,
      "grad_norm": 0.3139634132385254,
      "learning_rate": 1.2858333333333334e-05,
      "loss": 0.0022,
      "step": 178280
    },
    {
      "epoch": 5.943,
      "grad_norm": 0.17160111665725708,
      "learning_rate": 1.285625e-05,
      "loss": 0.0022,
      "step": 178290
    },
    {
      "epoch": 5.943333333333333,
      "grad_norm": 0.22810500860214233,
      "learning_rate": 1.2854166666666668e-05,
      "loss": 0.0018,
      "step": 178300
    },
    {
      "epoch": 5.943666666666667,
      "grad_norm": 0.08565538376569748,
      "learning_rate": 1.2852083333333333e-05,
      "loss": 0.0016,
      "step": 178310
    },
    {
      "epoch": 5.944,
      "grad_norm": 0.45102259516716003,
      "learning_rate": 1.285e-05,
      "loss": 0.0022,
      "step": 178320
    },
    {
      "epoch": 5.944333333333334,
      "grad_norm": 0.057897139340639114,
      "learning_rate": 1.2847916666666668e-05,
      "loss": 0.0015,
      "step": 178330
    },
    {
      "epoch": 5.9446666666666665,
      "grad_norm": 0.2773738205432892,
      "learning_rate": 1.2845833333333335e-05,
      "loss": 0.002,
      "step": 178340
    },
    {
      "epoch": 5.945,
      "grad_norm": 0.16994762420654297,
      "learning_rate": 1.2843750000000002e-05,
      "loss": 0.0012,
      "step": 178350
    },
    {
      "epoch": 5.945333333333333,
      "grad_norm": 0.20048651099205017,
      "learning_rate": 1.2841666666666668e-05,
      "loss": 0.0015,
      "step": 178360
    },
    {
      "epoch": 5.945666666666667,
      "grad_norm": 0.011913290247321129,
      "learning_rate": 1.2839583333333333e-05,
      "loss": 0.0014,
      "step": 178370
    },
    {
      "epoch": 5.946,
      "grad_norm": 0.2571609318256378,
      "learning_rate": 1.28375e-05,
      "loss": 0.002,
      "step": 178380
    },
    {
      "epoch": 5.9463333333333335,
      "grad_norm": 0.008121419697999954,
      "learning_rate": 1.2835416666666666e-05,
      "loss": 0.0028,
      "step": 178390
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 0.02968081831932068,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0015,
      "step": 178400
    },
    {
      "epoch": 5.947,
      "grad_norm": 0.058362800627946854,
      "learning_rate": 1.283125e-05,
      "loss": 0.0019,
      "step": 178410
    },
    {
      "epoch": 5.947333333333333,
      "grad_norm": 0.1945383995771408,
      "learning_rate": 1.2829166666666667e-05,
      "loss": 0.0014,
      "step": 178420
    },
    {
      "epoch": 5.947666666666667,
      "grad_norm": 0.45720377564430237,
      "learning_rate": 1.2827083333333335e-05,
      "loss": 0.0021,
      "step": 178430
    },
    {
      "epoch": 5.948,
      "grad_norm": 0.14299193024635315,
      "learning_rate": 1.2825000000000002e-05,
      "loss": 0.0021,
      "step": 178440
    },
    {
      "epoch": 5.948333333333333,
      "grad_norm": 0.9919539093971252,
      "learning_rate": 1.2822916666666667e-05,
      "loss": 0.0023,
      "step": 178450
    },
    {
      "epoch": 5.948666666666667,
      "grad_norm": 0.2834412753582001,
      "learning_rate": 1.2820833333333334e-05,
      "loss": 0.0015,
      "step": 178460
    },
    {
      "epoch": 5.949,
      "grad_norm": 0.03091198392212391,
      "learning_rate": 1.2818750000000002e-05,
      "loss": 0.002,
      "step": 178470
    },
    {
      "epoch": 5.949333333333334,
      "grad_norm": 0.2569478750228882,
      "learning_rate": 1.2816666666666669e-05,
      "loss": 0.0021,
      "step": 178480
    },
    {
      "epoch": 5.949666666666666,
      "grad_norm": 0.2857811748981476,
      "learning_rate": 1.2814583333333333e-05,
      "loss": 0.0014,
      "step": 178490
    },
    {
      "epoch": 5.95,
      "grad_norm": 0.1997300684452057,
      "learning_rate": 1.28125e-05,
      "loss": 0.0011,
      "step": 178500
    },
    {
      "epoch": 5.950333333333333,
      "grad_norm": 0.13478079438209534,
      "learning_rate": 1.2810416666666667e-05,
      "loss": 0.0024,
      "step": 178510
    },
    {
      "epoch": 5.950666666666667,
      "grad_norm": 0.28534963726997375,
      "learning_rate": 1.2808333333333334e-05,
      "loss": 0.002,
      "step": 178520
    },
    {
      "epoch": 5.951,
      "grad_norm": 0.11465214192867279,
      "learning_rate": 1.280625e-05,
      "loss": 0.0021,
      "step": 178530
    },
    {
      "epoch": 5.951333333333333,
      "grad_norm": 0.08607872575521469,
      "learning_rate": 1.2804166666666667e-05,
      "loss": 0.002,
      "step": 178540
    },
    {
      "epoch": 5.951666666666666,
      "grad_norm": 0.14287623763084412,
      "learning_rate": 1.2802083333333334e-05,
      "loss": 0.0018,
      "step": 178550
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.17164401710033417,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0011,
      "step": 178560
    },
    {
      "epoch": 5.952333333333334,
      "grad_norm": 0.17128363251686096,
      "learning_rate": 1.2797916666666668e-05,
      "loss": 0.0015,
      "step": 178570
    },
    {
      "epoch": 5.9526666666666666,
      "grad_norm": 0.14570890367031097,
      "learning_rate": 1.2795833333333335e-05,
      "loss": 0.0016,
      "step": 178580
    },
    {
      "epoch": 5.953,
      "grad_norm": 0.1436033844947815,
      "learning_rate": 1.2793750000000001e-05,
      "loss": 0.0012,
      "step": 178590
    },
    {
      "epoch": 5.953333333333333,
      "grad_norm": 0.14487917721271515,
      "learning_rate": 1.2791666666666668e-05,
      "loss": 0.0025,
      "step": 178600
    },
    {
      "epoch": 5.953666666666667,
      "grad_norm": 0.26785290241241455,
      "learning_rate": 1.2789583333333332e-05,
      "loss": 0.0017,
      "step": 178610
    },
    {
      "epoch": 5.954,
      "grad_norm": 0.36889418959617615,
      "learning_rate": 1.2787499999999999e-05,
      "loss": 0.0025,
      "step": 178620
    },
    {
      "epoch": 5.9543333333333335,
      "grad_norm": 0.029355844482779503,
      "learning_rate": 1.2785416666666666e-05,
      "loss": 0.0025,
      "step": 178630
    },
    {
      "epoch": 5.954666666666666,
      "grad_norm": 0.28592297434806824,
      "learning_rate": 1.2783333333333333e-05,
      "loss": 0.0012,
      "step": 178640
    },
    {
      "epoch": 5.955,
      "grad_norm": 0.20025742053985596,
      "learning_rate": 1.278125e-05,
      "loss": 0.0023,
      "step": 178650
    },
    {
      "epoch": 5.955333333333334,
      "grad_norm": 0.2285158336162567,
      "learning_rate": 1.2779166666666668e-05,
      "loss": 0.0018,
      "step": 178660
    },
    {
      "epoch": 5.955666666666667,
      "grad_norm": 0.40277308225631714,
      "learning_rate": 1.2777083333333333e-05,
      "loss": 0.0019,
      "step": 178670
    },
    {
      "epoch": 5.9559999999999995,
      "grad_norm": 0.05881515517830849,
      "learning_rate": 1.2775e-05,
      "loss": 0.0017,
      "step": 178680
    },
    {
      "epoch": 5.956333333333333,
      "grad_norm": 0.008547737263143063,
      "learning_rate": 1.2772916666666668e-05,
      "loss": 0.0025,
      "step": 178690
    },
    {
      "epoch": 5.956666666666667,
      "grad_norm": 0.1146891713142395,
      "learning_rate": 1.2770833333333335e-05,
      "loss": 0.002,
      "step": 178700
    },
    {
      "epoch": 5.957,
      "grad_norm": 0.2857455313205719,
      "learning_rate": 1.2768750000000002e-05,
      "loss": 0.0016,
      "step": 178710
    },
    {
      "epoch": 5.957333333333334,
      "grad_norm": 0.20021672546863556,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0021,
      "step": 178720
    },
    {
      "epoch": 5.957666666666666,
      "grad_norm": 0.11463112384080887,
      "learning_rate": 1.2764583333333333e-05,
      "loss": 0.0013,
      "step": 178730
    },
    {
      "epoch": 5.958,
      "grad_norm": 0.17143261432647705,
      "learning_rate": 1.27625e-05,
      "loss": 0.0019,
      "step": 178740
    },
    {
      "epoch": 5.958333333333333,
      "grad_norm": 0.08610032498836517,
      "learning_rate": 1.2760416666666666e-05,
      "loss": 0.0015,
      "step": 178750
    },
    {
      "epoch": 5.958666666666667,
      "grad_norm": 0.20755897462368011,
      "learning_rate": 1.2758333333333333e-05,
      "loss": 0.0017,
      "step": 178760
    },
    {
      "epoch": 5.959,
      "grad_norm": 0.032957687973976135,
      "learning_rate": 1.275625e-05,
      "loss": 0.0014,
      "step": 178770
    },
    {
      "epoch": 5.959333333333333,
      "grad_norm": 0.08578488230705261,
      "learning_rate": 1.2754166666666667e-05,
      "loss": 0.0013,
      "step": 178780
    },
    {
      "epoch": 5.959666666666667,
      "grad_norm": 0.17168180644512177,
      "learning_rate": 1.2752083333333334e-05,
      "loss": 0.0021,
      "step": 178790
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.4562617242336273,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.0013,
      "step": 178800
    },
    {
      "epoch": 5.960333333333334,
      "grad_norm": 0.14366306364536285,
      "learning_rate": 1.2747916666666667e-05,
      "loss": 0.0015,
      "step": 178810
    },
    {
      "epoch": 5.960666666666667,
      "grad_norm": 0.08952724188566208,
      "learning_rate": 1.2745833333333334e-05,
      "loss": 0.0016,
      "step": 178820
    },
    {
      "epoch": 5.961,
      "grad_norm": 0.030061418190598488,
      "learning_rate": 1.2743750000000001e-05,
      "loss": 0.0017,
      "step": 178830
    },
    {
      "epoch": 5.961333333333333,
      "grad_norm": 0.28586524724960327,
      "learning_rate": 1.2741666666666669e-05,
      "loss": 0.0019,
      "step": 178840
    },
    {
      "epoch": 5.961666666666667,
      "grad_norm": 0.39960411190986633,
      "learning_rate": 1.2739583333333336e-05,
      "loss": 0.0018,
      "step": 178850
    },
    {
      "epoch": 5.962,
      "grad_norm": 0.1992969661951065,
      "learning_rate": 1.27375e-05,
      "loss": 0.0024,
      "step": 178860
    },
    {
      "epoch": 5.9623333333333335,
      "grad_norm": 0.3150547444820404,
      "learning_rate": 1.2735416666666667e-05,
      "loss": 0.0021,
      "step": 178870
    },
    {
      "epoch": 5.962666666666666,
      "grad_norm": 0.08657767623662949,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0018,
      "step": 178880
    },
    {
      "epoch": 5.963,
      "grad_norm": 0.05764174461364746,
      "learning_rate": 1.273125e-05,
      "loss": 0.0022,
      "step": 178890
    },
    {
      "epoch": 5.963333333333333,
      "grad_norm": 0.03056376986205578,
      "learning_rate": 1.2729166666666667e-05,
      "loss": 0.0014,
      "step": 178900
    },
    {
      "epoch": 5.963666666666667,
      "grad_norm": 0.2001291811466217,
      "learning_rate": 1.2727083333333334e-05,
      "loss": 0.0014,
      "step": 178910
    },
    {
      "epoch": 5.964,
      "grad_norm": 0.2282794862985611,
      "learning_rate": 1.2725000000000001e-05,
      "loss": 0.0019,
      "step": 178920
    },
    {
      "epoch": 5.964333333333333,
      "grad_norm": 0.2280297577381134,
      "learning_rate": 1.2722916666666668e-05,
      "loss": 0.0017,
      "step": 178930
    },
    {
      "epoch": 5.964666666666667,
      "grad_norm": 0.2002181112766266,
      "learning_rate": 1.2720833333333335e-05,
      "loss": 0.0015,
      "step": 178940
    },
    {
      "epoch": 5.965,
      "grad_norm": 0.4757083058357239,
      "learning_rate": 1.271875e-05,
      "loss": 0.0022,
      "step": 178950
    },
    {
      "epoch": 5.965333333333334,
      "grad_norm": 0.033964335918426514,
      "learning_rate": 1.2716666666666668e-05,
      "loss": 0.0014,
      "step": 178960
    },
    {
      "epoch": 5.9656666666666665,
      "grad_norm": 0.08660347014665604,
      "learning_rate": 1.2714583333333335e-05,
      "loss": 0.003,
      "step": 178970
    },
    {
      "epoch": 5.966,
      "grad_norm": 0.05798187479376793,
      "learning_rate": 1.2712499999999999e-05,
      "loss": 0.0015,
      "step": 178980
    },
    {
      "epoch": 5.966333333333333,
      "grad_norm": 0.17101718485355377,
      "learning_rate": 1.2710416666666666e-05,
      "loss": 0.0014,
      "step": 178990
    },
    {
      "epoch": 5.966666666666667,
      "grad_norm": 0.1426761895418167,
      "learning_rate": 1.2708333333333333e-05,
      "loss": 0.0019,
      "step": 179000
    },
    {
      "epoch": 5.967,
      "grad_norm": 0.08611064404249191,
      "learning_rate": 1.270625e-05,
      "loss": 0.0029,
      "step": 179010
    },
    {
      "epoch": 5.967333333333333,
      "grad_norm": 0.20048661530017853,
      "learning_rate": 1.2704166666666668e-05,
      "loss": 0.0019,
      "step": 179020
    },
    {
      "epoch": 5.967666666666666,
      "grad_norm": 0.3141341209411621,
      "learning_rate": 1.2702083333333333e-05,
      "loss": 0.0021,
      "step": 179030
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.11423972249031067,
      "learning_rate": 1.27e-05,
      "loss": 0.0017,
      "step": 179040
    },
    {
      "epoch": 5.968333333333334,
      "grad_norm": 0.0584556944668293,
      "learning_rate": 1.2697916666666668e-05,
      "loss": 0.0017,
      "step": 179050
    },
    {
      "epoch": 5.968666666666667,
      "grad_norm": 0.2857038080692291,
      "learning_rate": 1.2695833333333335e-05,
      "loss": 0.0014,
      "step": 179060
    },
    {
      "epoch": 5.969,
      "grad_norm": 0.1715780794620514,
      "learning_rate": 1.2693750000000002e-05,
      "loss": 0.0018,
      "step": 179070
    },
    {
      "epoch": 5.969333333333333,
      "grad_norm": 0.171522319316864,
      "learning_rate": 1.2691666666666669e-05,
      "loss": 0.0015,
      "step": 179080
    },
    {
      "epoch": 5.969666666666667,
      "grad_norm": 0.03119903989136219,
      "learning_rate": 1.2689583333333335e-05,
      "loss": 0.0017,
      "step": 179090
    },
    {
      "epoch": 5.97,
      "grad_norm": 0.1720445305109024,
      "learning_rate": 1.26875e-05,
      "loss": 0.0016,
      "step": 179100
    },
    {
      "epoch": 5.9703333333333335,
      "grad_norm": 0.009358330629765987,
      "learning_rate": 1.2685416666666666e-05,
      "loss": 0.0017,
      "step": 179110
    },
    {
      "epoch": 5.970666666666666,
      "grad_norm": 0.14337007701396942,
      "learning_rate": 1.2683333333333333e-05,
      "loss": 0.0017,
      "step": 179120
    },
    {
      "epoch": 5.971,
      "grad_norm": 0.1142507791519165,
      "learning_rate": 1.268125e-05,
      "loss": 0.0016,
      "step": 179130
    },
    {
      "epoch": 5.971333333333334,
      "grad_norm": 0.08610756695270538,
      "learning_rate": 1.2679166666666667e-05,
      "loss": 0.0016,
      "step": 179140
    },
    {
      "epoch": 5.971666666666667,
      "grad_norm": 0.14380086958408356,
      "learning_rate": 1.2677083333333334e-05,
      "loss": 0.0018,
      "step": 179150
    },
    {
      "epoch": 5.9719999999999995,
      "grad_norm": 0.08565331250429153,
      "learning_rate": 1.2675000000000001e-05,
      "loss": 0.0014,
      "step": 179160
    },
    {
      "epoch": 5.972333333333333,
      "grad_norm": 0.009438344277441502,
      "learning_rate": 1.2672916666666667e-05,
      "loss": 0.0022,
      "step": 179170
    },
    {
      "epoch": 5.972666666666667,
      "grad_norm": 0.17202109098434448,
      "learning_rate": 1.2670833333333334e-05,
      "loss": 0.0016,
      "step": 179180
    },
    {
      "epoch": 5.973,
      "grad_norm": 0.20043472945690155,
      "learning_rate": 1.2668750000000001e-05,
      "loss": 0.002,
      "step": 179190
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.11443035304546356,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0023,
      "step": 179200
    },
    {
      "epoch": 5.9736666666666665,
      "grad_norm": 0.05763746425509453,
      "learning_rate": 1.2664583333333336e-05,
      "loss": 0.0014,
      "step": 179210
    },
    {
      "epoch": 5.974,
      "grad_norm": 0.2249007523059845,
      "learning_rate": 1.26625e-05,
      "loss": 0.0015,
      "step": 179220
    },
    {
      "epoch": 5.974333333333333,
      "grad_norm": 0.029773138463497162,
      "learning_rate": 1.2660416666666667e-05,
      "loss": 0.0016,
      "step": 179230
    },
    {
      "epoch": 5.974666666666667,
      "grad_norm": 0.03043483756482601,
      "learning_rate": 1.2658333333333334e-05,
      "loss": 0.0017,
      "step": 179240
    },
    {
      "epoch": 5.975,
      "grad_norm": 0.14270499348640442,
      "learning_rate": 1.265625e-05,
      "loss": 0.0023,
      "step": 179250
    },
    {
      "epoch": 5.975333333333333,
      "grad_norm": 0.08610974252223969,
      "learning_rate": 1.2654166666666666e-05,
      "loss": 0.0018,
      "step": 179260
    },
    {
      "epoch": 5.975666666666667,
      "grad_norm": 0.17092466354370117,
      "learning_rate": 1.2652083333333334e-05,
      "loss": 0.0019,
      "step": 179270
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.313662052154541,
      "learning_rate": 1.2650000000000001e-05,
      "loss": 0.0019,
      "step": 179280
    },
    {
      "epoch": 5.976333333333334,
      "grad_norm": 0.030188951641321182,
      "learning_rate": 1.2647916666666668e-05,
      "loss": 0.0021,
      "step": 179290
    },
    {
      "epoch": 5.976666666666667,
      "grad_norm": 0.09227345138788223,
      "learning_rate": 1.2645833333333335e-05,
      "loss": 0.0017,
      "step": 179300
    },
    {
      "epoch": 5.977,
      "grad_norm": 0.11597663164138794,
      "learning_rate": 1.264375e-05,
      "loss": 0.0017,
      "step": 179310
    },
    {
      "epoch": 5.977333333333333,
      "grad_norm": 0.2565561830997467,
      "learning_rate": 1.2641666666666668e-05,
      "loss": 0.0021,
      "step": 179320
    },
    {
      "epoch": 5.977666666666667,
      "grad_norm": 0.19986942410469055,
      "learning_rate": 1.2639583333333335e-05,
      "loss": 0.0013,
      "step": 179330
    },
    {
      "epoch": 5.978,
      "grad_norm": 0.05801338702440262,
      "learning_rate": 1.2637499999999999e-05,
      "loss": 0.0016,
      "step": 179340
    },
    {
      "epoch": 5.9783333333333335,
      "grad_norm": 0.1143268570303917,
      "learning_rate": 1.2635416666666666e-05,
      "loss": 0.002,
      "step": 179350
    },
    {
      "epoch": 5.978666666666666,
      "grad_norm": 0.17858560383319855,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.0017,
      "step": 179360
    },
    {
      "epoch": 5.979,
      "grad_norm": 0.08608989417552948,
      "learning_rate": 1.263125e-05,
      "loss": 0.002,
      "step": 179370
    },
    {
      "epoch": 5.979333333333333,
      "grad_norm": 0.05798518285155296,
      "learning_rate": 1.2629166666666668e-05,
      "loss": 0.0021,
      "step": 179380
    },
    {
      "epoch": 5.979666666666667,
      "grad_norm": 0.030944714322686195,
      "learning_rate": 1.2627083333333333e-05,
      "loss": 0.0022,
      "step": 179390
    },
    {
      "epoch": 5.98,
      "grad_norm": 0.49052977561950684,
      "learning_rate": 1.2625e-05,
      "loss": 0.0015,
      "step": 179400
    },
    {
      "epoch": 5.980333333333333,
      "grad_norm": 0.31369373202323914,
      "learning_rate": 1.2622916666666667e-05,
      "loss": 0.0015,
      "step": 179410
    },
    {
      "epoch": 5.980666666666667,
      "grad_norm": 0.5138242244720459,
      "learning_rate": 1.2620833333333335e-05,
      "loss": 0.0014,
      "step": 179420
    },
    {
      "epoch": 5.981,
      "grad_norm": 0.17186613380908966,
      "learning_rate": 1.2618750000000002e-05,
      "loss": 0.0015,
      "step": 179430
    },
    {
      "epoch": 5.981333333333334,
      "grad_norm": 0.4358290135860443,
      "learning_rate": 1.2616666666666669e-05,
      "loss": 0.0029,
      "step": 179440
    },
    {
      "epoch": 5.9816666666666665,
      "grad_norm": 0.14303112030029297,
      "learning_rate": 1.2614583333333334e-05,
      "loss": 0.0014,
      "step": 179450
    },
    {
      "epoch": 5.982,
      "grad_norm": 0.3435839116573334,
      "learning_rate": 1.26125e-05,
      "loss": 0.0018,
      "step": 179460
    },
    {
      "epoch": 5.982333333333333,
      "grad_norm": 0.1429232954978943,
      "learning_rate": 1.2610416666666665e-05,
      "loss": 0.0016,
      "step": 179470
    },
    {
      "epoch": 5.982666666666667,
      "grad_norm": 0.057371724396944046,
      "learning_rate": 1.2608333333333333e-05,
      "loss": 0.0015,
      "step": 179480
    },
    {
      "epoch": 5.983,
      "grad_norm": 0.08730699121952057,
      "learning_rate": 1.260625e-05,
      "loss": 0.0018,
      "step": 179490
    },
    {
      "epoch": 5.983333333333333,
      "grad_norm": 0.2001856565475464,
      "learning_rate": 1.2604166666666667e-05,
      "loss": 0.0021,
      "step": 179500
    },
    {
      "epoch": 5.983666666666666,
      "grad_norm": 0.05747101455926895,
      "learning_rate": 1.2602083333333334e-05,
      "loss": 0.0019,
      "step": 179510
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.17137819528579712,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0018,
      "step": 179520
    },
    {
      "epoch": 5.984333333333334,
      "grad_norm": 0.11594909429550171,
      "learning_rate": 1.2597916666666667e-05,
      "loss": 0.0015,
      "step": 179530
    },
    {
      "epoch": 5.984666666666667,
      "grad_norm": 0.31411153078079224,
      "learning_rate": 1.2595833333333334e-05,
      "loss": 0.002,
      "step": 179540
    },
    {
      "epoch": 5.985,
      "grad_norm": 0.22885359823703766,
      "learning_rate": 1.2593750000000001e-05,
      "loss": 0.0021,
      "step": 179550
    },
    {
      "epoch": 5.985333333333333,
      "grad_norm": 0.0087033212184906,
      "learning_rate": 1.2591666666666668e-05,
      "loss": 0.0022,
      "step": 179560
    },
    {
      "epoch": 5.985666666666667,
      "grad_norm": 0.08553682267665863,
      "learning_rate": 1.2589583333333336e-05,
      "loss": 0.0032,
      "step": 179570
    },
    {
      "epoch": 5.986,
      "grad_norm": 0.25690147280693054,
      "learning_rate": 1.25875e-05,
      "loss": 0.0018,
      "step": 179580
    },
    {
      "epoch": 5.9863333333333335,
      "grad_norm": 0.08615011721849442,
      "learning_rate": 1.2585416666666667e-05,
      "loss": 0.0016,
      "step": 179590
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 0.05784660577774048,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.0019,
      "step": 179600
    },
    {
      "epoch": 5.987,
      "grad_norm": 0.12818187475204468,
      "learning_rate": 1.258125e-05,
      "loss": 0.0014,
      "step": 179610
    },
    {
      "epoch": 5.987333333333333,
      "grad_norm": 0.08640872687101364,
      "learning_rate": 1.2579166666666666e-05,
      "loss": 0.0016,
      "step": 179620
    },
    {
      "epoch": 5.987666666666667,
      "grad_norm": 0.11413504183292389,
      "learning_rate": 1.2577083333333334e-05,
      "loss": 0.0019,
      "step": 179630
    },
    {
      "epoch": 5.9879999999999995,
      "grad_norm": 0.08949998021125793,
      "learning_rate": 1.2575e-05,
      "loss": 0.0022,
      "step": 179640
    },
    {
      "epoch": 5.988333333333333,
      "grad_norm": 0.22824043035507202,
      "learning_rate": 1.2572916666666668e-05,
      "loss": 0.002,
      "step": 179650
    },
    {
      "epoch": 5.988666666666667,
      "grad_norm": 0.11436864733695984,
      "learning_rate": 1.2570833333333335e-05,
      "loss": 0.0023,
      "step": 179660
    },
    {
      "epoch": 5.989,
      "grad_norm": 0.42778587341308594,
      "learning_rate": 1.256875e-05,
      "loss": 0.002,
      "step": 179670
    },
    {
      "epoch": 5.989333333333334,
      "grad_norm": 0.11465448141098022,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0018,
      "step": 179680
    },
    {
      "epoch": 5.9896666666666665,
      "grad_norm": 0.17135362327098846,
      "learning_rate": 1.2564583333333335e-05,
      "loss": 0.0014,
      "step": 179690
    },
    {
      "epoch": 5.99,
      "grad_norm": 0.1993727833032608,
      "learning_rate": 1.2562499999999999e-05,
      "loss": 0.0013,
      "step": 179700
    },
    {
      "epoch": 5.990333333333333,
      "grad_norm": 0.17246487736701965,
      "learning_rate": 1.2560416666666666e-05,
      "loss": 0.0032,
      "step": 179710
    },
    {
      "epoch": 5.990666666666667,
      "grad_norm": 0.3998844623565674,
      "learning_rate": 1.2558333333333333e-05,
      "loss": 0.002,
      "step": 179720
    },
    {
      "epoch": 5.991,
      "grad_norm": 0.14266589283943176,
      "learning_rate": 1.255625e-05,
      "loss": 0.0014,
      "step": 179730
    },
    {
      "epoch": 5.991333333333333,
      "grad_norm": 0.11473777145147324,
      "learning_rate": 1.2554166666666667e-05,
      "loss": 0.0018,
      "step": 179740
    },
    {
      "epoch": 5.991666666666667,
      "grad_norm": 0.11638551950454712,
      "learning_rate": 1.2552083333333333e-05,
      "loss": 0.0017,
      "step": 179750
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.2286667823791504,
      "learning_rate": 1.255e-05,
      "loss": 0.0012,
      "step": 179760
    },
    {
      "epoch": 5.992333333333333,
      "grad_norm": 0.029511699452996254,
      "learning_rate": 1.2547916666666667e-05,
      "loss": 0.0015,
      "step": 179770
    },
    {
      "epoch": 5.992666666666667,
      "grad_norm": 0.5144031047821045,
      "learning_rate": 1.2545833333333334e-05,
      "loss": 0.0024,
      "step": 179780
    },
    {
      "epoch": 5.993,
      "grad_norm": 0.11851787567138672,
      "learning_rate": 1.2543750000000002e-05,
      "loss": 0.0017,
      "step": 179790
    },
    {
      "epoch": 5.993333333333333,
      "grad_norm": 0.1151103749871254,
      "learning_rate": 1.2541666666666669e-05,
      "loss": 0.0016,
      "step": 179800
    },
    {
      "epoch": 5.993666666666667,
      "grad_norm": 0.057599835097789764,
      "learning_rate": 1.2539583333333334e-05,
      "loss": 0.0015,
      "step": 179810
    },
    {
      "epoch": 5.994,
      "grad_norm": 0.17163455486297607,
      "learning_rate": 1.25375e-05,
      "loss": 0.0019,
      "step": 179820
    },
    {
      "epoch": 5.9943333333333335,
      "grad_norm": 0.029064983129501343,
      "learning_rate": 1.2535416666666665e-05,
      "loss": 0.0016,
      "step": 179830
    },
    {
      "epoch": 5.994666666666666,
      "grad_norm": 0.1434180587530136,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0017,
      "step": 179840
    },
    {
      "epoch": 5.995,
      "grad_norm": 0.17194496095180511,
      "learning_rate": 1.253125e-05,
      "loss": 0.0015,
      "step": 179850
    },
    {
      "epoch": 5.995333333333333,
      "grad_norm": 0.28538358211517334,
      "learning_rate": 1.2529166666666667e-05,
      "loss": 0.0021,
      "step": 179860
    },
    {
      "epoch": 5.995666666666667,
      "grad_norm": 0.3147532343864441,
      "learning_rate": 1.2527083333333334e-05,
      "loss": 0.0018,
      "step": 179870
    },
    {
      "epoch": 5.996,
      "grad_norm": 0.05759209766983986,
      "learning_rate": 1.2525000000000001e-05,
      "loss": 0.0016,
      "step": 179880
    },
    {
      "epoch": 5.996333333333333,
      "grad_norm": 0.25658485293388367,
      "learning_rate": 1.2522916666666667e-05,
      "loss": 0.0017,
      "step": 179890
    },
    {
      "epoch": 5.996666666666667,
      "grad_norm": 0.009655618108808994,
      "learning_rate": 1.2520833333333334e-05,
      "loss": 0.0017,
      "step": 179900
    },
    {
      "epoch": 5.997,
      "grad_norm": 0.28612497448921204,
      "learning_rate": 1.2518750000000001e-05,
      "loss": 0.0013,
      "step": 179910
    },
    {
      "epoch": 5.997333333333334,
      "grad_norm": 0.17144793272018433,
      "learning_rate": 1.2516666666666668e-05,
      "loss": 0.0024,
      "step": 179920
    },
    {
      "epoch": 5.9976666666666665,
      "grad_norm": 0.14281494915485382,
      "learning_rate": 1.2514583333333335e-05,
      "loss": 0.0014,
      "step": 179930
    },
    {
      "epoch": 5.998,
      "grad_norm": 0.11431702226400375,
      "learning_rate": 1.25125e-05,
      "loss": 0.002,
      "step": 179940
    },
    {
      "epoch": 5.998333333333333,
      "grad_norm": 0.25041699409484863,
      "learning_rate": 1.2510416666666666e-05,
      "loss": 0.002,
      "step": 179950
    },
    {
      "epoch": 5.998666666666667,
      "grad_norm": 0.20084618031978607,
      "learning_rate": 1.2508333333333334e-05,
      "loss": 0.0018,
      "step": 179960
    },
    {
      "epoch": 5.999,
      "grad_norm": 0.37040647864341736,
      "learning_rate": 1.250625e-05,
      "loss": 0.0014,
      "step": 179970
    },
    {
      "epoch": 5.999333333333333,
      "grad_norm": 0.20001909136772156,
      "learning_rate": 1.2504166666666666e-05,
      "loss": 0.0019,
      "step": 179980
    },
    {
      "epoch": 5.999666666666666,
      "grad_norm": 0.10357145220041275,
      "learning_rate": 1.2502083333333333e-05,
      "loss": 0.0016,
      "step": 179990
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.3428910970687866,
      "learning_rate": 1.25e-05,
      "loss": 0.0019,
      "step": 180000
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.0017644831677898765,
      "eval_runtime": 131.1669,
      "eval_samples_per_second": 1524.774,
      "eval_steps_per_second": 38.119,
      "step": 180000
    },
    {
      "epoch": 6.000333333333334,
      "grad_norm": 0.14249075949192047,
      "learning_rate": 1.2497916666666668e-05,
      "loss": 0.0016,
      "step": 180010
    },
    {
      "epoch": 6.000666666666667,
      "grad_norm": 0.20000126957893372,
      "learning_rate": 1.2495833333333335e-05,
      "loss": 0.0018,
      "step": 180020
    },
    {
      "epoch": 6.001,
      "grad_norm": 0.17168352007865906,
      "learning_rate": 1.249375e-05,
      "loss": 0.0016,
      "step": 180030
    },
    {
      "epoch": 6.001333333333333,
      "grad_norm": 0.17248734831809998,
      "learning_rate": 1.2491666666666668e-05,
      "loss": 0.0021,
      "step": 180040
    },
    {
      "epoch": 6.001666666666667,
      "grad_norm": 0.11469561606645584,
      "learning_rate": 1.2489583333333333e-05,
      "loss": 0.0015,
      "step": 180050
    },
    {
      "epoch": 6.002,
      "grad_norm": 0.02967328391969204,
      "learning_rate": 1.24875e-05,
      "loss": 0.0027,
      "step": 180060
    },
    {
      "epoch": 6.0023333333333335,
      "grad_norm": 0.1719534695148468,
      "learning_rate": 1.2485416666666667e-05,
      "loss": 0.0024,
      "step": 180070
    },
    {
      "epoch": 6.002666666666666,
      "grad_norm": 0.20005375146865845,
      "learning_rate": 1.2483333333333335e-05,
      "loss": 0.002,
      "step": 180080
    },
    {
      "epoch": 6.003,
      "grad_norm": 0.2571103870868683,
      "learning_rate": 1.2481250000000002e-05,
      "loss": 0.0014,
      "step": 180090
    },
    {
      "epoch": 6.003333333333333,
      "grad_norm": 0.19985902309417725,
      "learning_rate": 1.2479166666666667e-05,
      "loss": 0.0016,
      "step": 180100
    },
    {
      "epoch": 6.003666666666667,
      "grad_norm": 0.05734816566109657,
      "learning_rate": 1.2477083333333335e-05,
      "loss": 0.0016,
      "step": 180110
    },
    {
      "epoch": 6.004,
      "grad_norm": 0.34622615575790405,
      "learning_rate": 1.2475e-05,
      "loss": 0.0026,
      "step": 180120
    },
    {
      "epoch": 6.004333333333333,
      "grad_norm": 0.11412058025598526,
      "learning_rate": 1.2472916666666667e-05,
      "loss": 0.0015,
      "step": 180130
    },
    {
      "epoch": 6.004666666666667,
      "grad_norm": 0.20064140856266022,
      "learning_rate": 1.2470833333333334e-05,
      "loss": 0.0012,
      "step": 180140
    },
    {
      "epoch": 6.005,
      "grad_norm": 0.6273139715194702,
      "learning_rate": 1.2468750000000002e-05,
      "loss": 0.0021,
      "step": 180150
    },
    {
      "epoch": 6.005333333333334,
      "grad_norm": 0.05751223862171173,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0012,
      "step": 180160
    },
    {
      "epoch": 6.0056666666666665,
      "grad_norm": 0.31354472041130066,
      "learning_rate": 1.2464583333333334e-05,
      "loss": 0.0021,
      "step": 180170
    },
    {
      "epoch": 6.006,
      "grad_norm": 0.2571393847465515,
      "learning_rate": 1.2462500000000001e-05,
      "loss": 0.0025,
      "step": 180180
    },
    {
      "epoch": 6.006333333333333,
      "grad_norm": 0.14287169277668,
      "learning_rate": 1.2460416666666667e-05,
      "loss": 0.0014,
      "step": 180190
    },
    {
      "epoch": 6.006666666666667,
      "grad_norm": 0.34238189458847046,
      "learning_rate": 1.2458333333333334e-05,
      "loss": 0.003,
      "step": 180200
    },
    {
      "epoch": 6.007,
      "grad_norm": 0.25672656297683716,
      "learning_rate": 1.2456250000000001e-05,
      "loss": 0.0017,
      "step": 180210
    },
    {
      "epoch": 6.007333333333333,
      "grad_norm": 0.03175155818462372,
      "learning_rate": 1.2454166666666667e-05,
      "loss": 0.0013,
      "step": 180220
    },
    {
      "epoch": 6.007666666666666,
      "grad_norm": 0.4567769765853882,
      "learning_rate": 1.2452083333333334e-05,
      "loss": 0.0018,
      "step": 180230
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.08866164833307266,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 0.0022,
      "step": 180240
    },
    {
      "epoch": 6.008333333333334,
      "grad_norm": 0.2663436233997345,
      "learning_rate": 1.2447916666666668e-05,
      "loss": 0.002,
      "step": 180250
    },
    {
      "epoch": 6.008666666666667,
      "grad_norm": 0.17163880169391632,
      "learning_rate": 1.2445833333333334e-05,
      "loss": 0.0016,
      "step": 180260
    },
    {
      "epoch": 6.009,
      "grad_norm": 0.4285418689250946,
      "learning_rate": 1.2443750000000001e-05,
      "loss": 0.0019,
      "step": 180270
    },
    {
      "epoch": 6.009333333333333,
      "grad_norm": 0.20069465041160583,
      "learning_rate": 1.2441666666666666e-05,
      "loss": 0.0022,
      "step": 180280
    },
    {
      "epoch": 6.009666666666667,
      "grad_norm": 0.058037612587213516,
      "learning_rate": 1.2439583333333334e-05,
      "loss": 0.0016,
      "step": 180290
    },
    {
      "epoch": 6.01,
      "grad_norm": 0.37099212408065796,
      "learning_rate": 1.24375e-05,
      "loss": 0.0017,
      "step": 180300
    },
    {
      "epoch": 6.0103333333333335,
      "grad_norm": 0.3968299627304077,
      "learning_rate": 1.2435416666666668e-05,
      "loss": 0.0019,
      "step": 180310
    },
    {
      "epoch": 6.010666666666666,
      "grad_norm": 0.11523359268903732,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0014,
      "step": 180320
    },
    {
      "epoch": 6.011,
      "grad_norm": 0.14320243895053864,
      "learning_rate": 1.243125e-05,
      "loss": 0.0012,
      "step": 180330
    },
    {
      "epoch": 6.011333333333333,
      "grad_norm": 0.14284221827983856,
      "learning_rate": 1.2429166666666666e-05,
      "loss": 0.0018,
      "step": 180340
    },
    {
      "epoch": 6.011666666666667,
      "grad_norm": 0.1462291032075882,
      "learning_rate": 1.2427083333333333e-05,
      "loss": 0.0013,
      "step": 180350
    },
    {
      "epoch": 6.012,
      "grad_norm": 0.32257699966430664,
      "learning_rate": 1.2425e-05,
      "loss": 0.0012,
      "step": 180360
    },
    {
      "epoch": 6.012333333333333,
      "grad_norm": 0.17200398445129395,
      "learning_rate": 1.2422916666666668e-05,
      "loss": 0.0014,
      "step": 180370
    },
    {
      "epoch": 6.012666666666667,
      "grad_norm": 0.17175613343715668,
      "learning_rate": 1.2420833333333335e-05,
      "loss": 0.0017,
      "step": 180380
    },
    {
      "epoch": 6.013,
      "grad_norm": 0.0602751187980175,
      "learning_rate": 1.2418750000000002e-05,
      "loss": 0.0019,
      "step": 180390
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 0.08637305349111557,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.0017,
      "step": 180400
    },
    {
      "epoch": 6.0136666666666665,
      "grad_norm": 0.607187032699585,
      "learning_rate": 1.2414583333333333e-05,
      "loss": 0.0014,
      "step": 180410
    },
    {
      "epoch": 6.014,
      "grad_norm": 0.48530852794647217,
      "learning_rate": 1.24125e-05,
      "loss": 0.0017,
      "step": 180420
    },
    {
      "epoch": 6.014333333333333,
      "grad_norm": 0.44432276487350464,
      "learning_rate": 1.2410416666666667e-05,
      "loss": 0.0017,
      "step": 180430
    },
    {
      "epoch": 6.014666666666667,
      "grad_norm": 0.058357879519462585,
      "learning_rate": 1.2408333333333335e-05,
      "loss": 0.0012,
      "step": 180440
    },
    {
      "epoch": 6.015,
      "grad_norm": 0.05763619393110275,
      "learning_rate": 1.2406250000000002e-05,
      "loss": 0.0018,
      "step": 180450
    },
    {
      "epoch": 6.015333333333333,
      "grad_norm": 0.05767888203263283,
      "learning_rate": 1.2404166666666667e-05,
      "loss": 0.0021,
      "step": 180460
    },
    {
      "epoch": 6.015666666666666,
      "grad_norm": 0.6284497380256653,
      "learning_rate": 1.2402083333333334e-05,
      "loss": 0.0017,
      "step": 180470
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.46258074045181274,
      "learning_rate": 1.24e-05,
      "loss": 0.0023,
      "step": 180480
    },
    {
      "epoch": 6.016333333333334,
      "grad_norm": 0.3112233281135559,
      "learning_rate": 1.2397916666666667e-05,
      "loss": 0.0015,
      "step": 180490
    },
    {
      "epoch": 6.016666666666667,
      "grad_norm": 0.2001848667860031,
      "learning_rate": 1.2395833333333334e-05,
      "loss": 0.0014,
      "step": 180500
    },
    {
      "epoch": 6.017,
      "grad_norm": 0.05791223794221878,
      "learning_rate": 1.2393750000000001e-05,
      "loss": 0.0016,
      "step": 180510
    },
    {
      "epoch": 6.017333333333333,
      "grad_norm": 0.35095295310020447,
      "learning_rate": 1.2391666666666667e-05,
      "loss": 0.0025,
      "step": 180520
    },
    {
      "epoch": 6.017666666666667,
      "grad_norm": 0.029554352164268494,
      "learning_rate": 1.2389583333333334e-05,
      "loss": 0.002,
      "step": 180530
    },
    {
      "epoch": 6.018,
      "grad_norm": 0.030196979641914368,
      "learning_rate": 1.2387500000000001e-05,
      "loss": 0.0017,
      "step": 180540
    },
    {
      "epoch": 6.0183333333333335,
      "grad_norm": 0.44531795382499695,
      "learning_rate": 1.2385416666666667e-05,
      "loss": 0.0015,
      "step": 180550
    },
    {
      "epoch": 6.018666666666666,
      "grad_norm": 0.17119812965393066,
      "learning_rate": 1.2383333333333334e-05,
      "loss": 0.0016,
      "step": 180560
    },
    {
      "epoch": 6.019,
      "grad_norm": 0.1429263800382614,
      "learning_rate": 1.2381250000000001e-05,
      "loss": 0.0012,
      "step": 180570
    },
    {
      "epoch": 6.019333333333333,
      "grad_norm": 0.2854911983013153,
      "learning_rate": 1.2379166666666667e-05,
      "loss": 0.0021,
      "step": 180580
    },
    {
      "epoch": 6.019666666666667,
      "grad_norm": 0.18502600491046906,
      "learning_rate": 1.2377083333333334e-05,
      "loss": 0.0019,
      "step": 180590
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.22853432595729828,
      "learning_rate": 1.2375000000000001e-05,
      "loss": 0.0025,
      "step": 180600
    },
    {
      "epoch": 6.020333333333333,
      "grad_norm": 0.08649537712335587,
      "learning_rate": 1.2372916666666668e-05,
      "loss": 0.0019,
      "step": 180610
    },
    {
      "epoch": 6.020666666666667,
      "grad_norm": 0.11757345497608185,
      "learning_rate": 1.2370833333333334e-05,
      "loss": 0.0015,
      "step": 180620
    },
    {
      "epoch": 6.021,
      "grad_norm": 0.20614029467105865,
      "learning_rate": 1.236875e-05,
      "loss": 0.0021,
      "step": 180630
    },
    {
      "epoch": 6.021333333333334,
      "grad_norm": 0.20032338798046112,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0021,
      "step": 180640
    },
    {
      "epoch": 6.0216666666666665,
      "grad_norm": 0.12259942293167114,
      "learning_rate": 1.2364583333333333e-05,
      "loss": 0.002,
      "step": 180650
    },
    {
      "epoch": 6.022,
      "grad_norm": 0.31420403718948364,
      "learning_rate": 1.23625e-05,
      "loss": 0.002,
      "step": 180660
    },
    {
      "epoch": 6.022333333333333,
      "grad_norm": 0.28349828720092773,
      "learning_rate": 1.2360416666666668e-05,
      "loss": 0.0014,
      "step": 180670
    },
    {
      "epoch": 6.022666666666667,
      "grad_norm": 0.11474607139825821,
      "learning_rate": 1.2358333333333335e-05,
      "loss": 0.0022,
      "step": 180680
    },
    {
      "epoch": 6.023,
      "grad_norm": 0.3136933445930481,
      "learning_rate": 1.235625e-05,
      "loss": 0.0017,
      "step": 180690
    },
    {
      "epoch": 6.023333333333333,
      "grad_norm": 0.00958858709782362,
      "learning_rate": 1.2354166666666666e-05,
      "loss": 0.0013,
      "step": 180700
    },
    {
      "epoch": 6.023666666666666,
      "grad_norm": 0.200669065117836,
      "learning_rate": 1.2352083333333333e-05,
      "loss": 0.002,
      "step": 180710
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.15106850862503052,
      "learning_rate": 1.235e-05,
      "loss": 0.0017,
      "step": 180720
    },
    {
      "epoch": 6.024333333333334,
      "grad_norm": 0.2511498034000397,
      "learning_rate": 1.2347916666666668e-05,
      "loss": 0.002,
      "step": 180730
    },
    {
      "epoch": 6.024666666666667,
      "grad_norm": 0.17122292518615723,
      "learning_rate": 1.2345833333333335e-05,
      "loss": 0.0019,
      "step": 180740
    },
    {
      "epoch": 6.025,
      "grad_norm": 0.22829091548919678,
      "learning_rate": 1.2343750000000002e-05,
      "loss": 0.0019,
      "step": 180750
    },
    {
      "epoch": 6.025333333333333,
      "grad_norm": 0.13793693482875824,
      "learning_rate": 1.2341666666666667e-05,
      "loss": 0.0013,
      "step": 180760
    },
    {
      "epoch": 6.025666666666667,
      "grad_norm": 0.2567046880722046,
      "learning_rate": 1.2339583333333333e-05,
      "loss": 0.0018,
      "step": 180770
    },
    {
      "epoch": 6.026,
      "grad_norm": 0.14367008209228516,
      "learning_rate": 1.23375e-05,
      "loss": 0.0016,
      "step": 180780
    },
    {
      "epoch": 6.0263333333333335,
      "grad_norm": 0.010419296100735664,
      "learning_rate": 1.2335416666666667e-05,
      "loss": 0.0016,
      "step": 180790
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.05808217078447342,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0018,
      "step": 180800
    },
    {
      "epoch": 6.027,
      "grad_norm": 0.17027835547924042,
      "learning_rate": 1.2331250000000002e-05,
      "loss": 0.0022,
      "step": 180810
    },
    {
      "epoch": 6.027333333333333,
      "grad_norm": 0.06371460109949112,
      "learning_rate": 1.2329166666666667e-05,
      "loss": 0.0031,
      "step": 180820
    },
    {
      "epoch": 6.027666666666667,
      "grad_norm": 0.05873692408204079,
      "learning_rate": 1.2327083333333334e-05,
      "loss": 0.0022,
      "step": 180830
    },
    {
      "epoch": 6.028,
      "grad_norm": 0.3426629602909088,
      "learning_rate": 1.2325e-05,
      "loss": 0.0017,
      "step": 180840
    },
    {
      "epoch": 6.028333333333333,
      "grad_norm": 0.11422519385814667,
      "learning_rate": 1.2322916666666667e-05,
      "loss": 0.0017,
      "step": 180850
    },
    {
      "epoch": 6.028666666666667,
      "grad_norm": 0.14249934256076813,
      "learning_rate": 1.2320833333333334e-05,
      "loss": 0.0025,
      "step": 180860
    },
    {
      "epoch": 6.029,
      "grad_norm": 0.19996732473373413,
      "learning_rate": 1.2318750000000001e-05,
      "loss": 0.0018,
      "step": 180870
    },
    {
      "epoch": 6.029333333333334,
      "grad_norm": 0.14282013475894928,
      "learning_rate": 1.2316666666666667e-05,
      "loss": 0.0023,
      "step": 180880
    },
    {
      "epoch": 6.0296666666666665,
      "grad_norm": 0.03462376073002815,
      "learning_rate": 1.2314583333333334e-05,
      "loss": 0.0019,
      "step": 180890
    },
    {
      "epoch": 6.03,
      "grad_norm": 0.35780009627342224,
      "learning_rate": 1.2312500000000001e-05,
      "loss": 0.0024,
      "step": 180900
    },
    {
      "epoch": 6.030333333333333,
      "grad_norm": 0.029726872220635414,
      "learning_rate": 1.2310416666666667e-05,
      "loss": 0.0019,
      "step": 180910
    },
    {
      "epoch": 6.030666666666667,
      "grad_norm": 0.28537964820861816,
      "learning_rate": 1.2308333333333334e-05,
      "loss": 0.0013,
      "step": 180920
    },
    {
      "epoch": 6.031,
      "grad_norm": 0.2571774125099182,
      "learning_rate": 1.2306250000000001e-05,
      "loss": 0.0015,
      "step": 180930
    },
    {
      "epoch": 6.031333333333333,
      "grad_norm": 0.057480067014694214,
      "learning_rate": 1.2304166666666666e-05,
      "loss": 0.0018,
      "step": 180940
    },
    {
      "epoch": 6.031666666666666,
      "grad_norm": 0.2369261234998703,
      "learning_rate": 1.2302083333333334e-05,
      "loss": 0.0016,
      "step": 180950
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.08612295985221863,
      "learning_rate": 1.23e-05,
      "loss": 0.0018,
      "step": 180960
    },
    {
      "epoch": 6.032333333333334,
      "grad_norm": 0.1143396720290184,
      "learning_rate": 1.2297916666666668e-05,
      "loss": 0.0027,
      "step": 180970
    },
    {
      "epoch": 6.032666666666667,
      "grad_norm": 0.14642155170440674,
      "learning_rate": 1.2295833333333333e-05,
      "loss": 0.002,
      "step": 180980
    },
    {
      "epoch": 6.033,
      "grad_norm": 0.0572153739631176,
      "learning_rate": 1.229375e-05,
      "loss": 0.0024,
      "step": 180990
    },
    {
      "epoch": 6.033333333333333,
      "grad_norm": 0.062081702053546906,
      "learning_rate": 1.2291666666666666e-05,
      "loss": 0.0012,
      "step": 181000
    },
    {
      "epoch": 6.033666666666667,
      "grad_norm": 0.22853152453899384,
      "learning_rate": 1.2289583333333333e-05,
      "loss": 0.0014,
      "step": 181010
    },
    {
      "epoch": 6.034,
      "grad_norm": 0.1143256351351738,
      "learning_rate": 1.22875e-05,
      "loss": 0.0017,
      "step": 181020
    },
    {
      "epoch": 6.0343333333333335,
      "grad_norm": 0.08559573441743851,
      "learning_rate": 1.2285416666666668e-05,
      "loss": 0.0015,
      "step": 181030
    },
    {
      "epoch": 6.034666666666666,
      "grad_norm": 0.22835873067378998,
      "learning_rate": 1.2283333333333335e-05,
      "loss": 0.0016,
      "step": 181040
    },
    {
      "epoch": 6.035,
      "grad_norm": 0.059494636952877045,
      "learning_rate": 1.228125e-05,
      "loss": 0.0014,
      "step": 181050
    },
    {
      "epoch": 6.035333333333333,
      "grad_norm": 0.34984806180000305,
      "learning_rate": 1.2279166666666666e-05,
      "loss": 0.0018,
      "step": 181060
    },
    {
      "epoch": 6.035666666666667,
      "grad_norm": 0.17597462236881256,
      "learning_rate": 1.2277083333333333e-05,
      "loss": 0.0019,
      "step": 181070
    },
    {
      "epoch": 6.036,
      "grad_norm": 0.012404162436723709,
      "learning_rate": 1.2275e-05,
      "loss": 0.0019,
      "step": 181080
    },
    {
      "epoch": 6.036333333333333,
      "grad_norm": 0.2007153034210205,
      "learning_rate": 1.2272916666666667e-05,
      "loss": 0.0012,
      "step": 181090
    },
    {
      "epoch": 6.036666666666667,
      "grad_norm": 0.08671585470438004,
      "learning_rate": 1.2270833333333335e-05,
      "loss": 0.0025,
      "step": 181100
    },
    {
      "epoch": 6.037,
      "grad_norm": 0.11529532819986343,
      "learning_rate": 1.2268750000000002e-05,
      "loss": 0.0012,
      "step": 181110
    },
    {
      "epoch": 6.037333333333334,
      "grad_norm": 0.17119599878787994,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0018,
      "step": 181120
    },
    {
      "epoch": 6.0376666666666665,
      "grad_norm": 0.05804185941815376,
      "learning_rate": 1.2264583333333333e-05,
      "loss": 0.0013,
      "step": 181130
    },
    {
      "epoch": 6.038,
      "grad_norm": 0.08727449923753738,
      "learning_rate": 1.22625e-05,
      "loss": 0.0017,
      "step": 181140
    },
    {
      "epoch": 6.038333333333333,
      "grad_norm": 0.05887977033853531,
      "learning_rate": 1.2260416666666667e-05,
      "loss": 0.0016,
      "step": 181150
    },
    {
      "epoch": 6.038666666666667,
      "grad_norm": 0.14383336901664734,
      "learning_rate": 1.2258333333333334e-05,
      "loss": 0.0013,
      "step": 181160
    },
    {
      "epoch": 6.039,
      "grad_norm": 0.19035843014717102,
      "learning_rate": 1.2256250000000001e-05,
      "loss": 0.0029,
      "step": 181170
    },
    {
      "epoch": 6.039333333333333,
      "grad_norm": 0.1462365984916687,
      "learning_rate": 1.2254166666666667e-05,
      "loss": 0.0015,
      "step": 181180
    },
    {
      "epoch": 6.039666666666666,
      "grad_norm": 0.36927133798599243,
      "learning_rate": 1.2252083333333334e-05,
      "loss": 0.0026,
      "step": 181190
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.04242795333266258,
      "learning_rate": 1.225e-05,
      "loss": 0.002,
      "step": 181200
    },
    {
      "epoch": 6.040333333333333,
      "grad_norm": 0.20088650286197662,
      "learning_rate": 1.2247916666666667e-05,
      "loss": 0.0017,
      "step": 181210
    },
    {
      "epoch": 6.040666666666667,
      "grad_norm": 0.03016474097967148,
      "learning_rate": 1.2245833333333334e-05,
      "loss": 0.0019,
      "step": 181220
    },
    {
      "epoch": 6.041,
      "grad_norm": 0.08653637766838074,
      "learning_rate": 1.2243750000000001e-05,
      "loss": 0.0015,
      "step": 181230
    },
    {
      "epoch": 6.041333333333333,
      "grad_norm": 0.11467131972312927,
      "learning_rate": 1.2241666666666667e-05,
      "loss": 0.0026,
      "step": 181240
    },
    {
      "epoch": 6.041666666666667,
      "grad_norm": 0.19247080385684967,
      "learning_rate": 1.2239583333333334e-05,
      "loss": 0.0013,
      "step": 181250
    },
    {
      "epoch": 6.042,
      "grad_norm": 0.02947477623820305,
      "learning_rate": 1.2237500000000001e-05,
      "loss": 0.0013,
      "step": 181260
    },
    {
      "epoch": 6.042333333333334,
      "grad_norm": 0.14143124222755432,
      "learning_rate": 1.2235416666666666e-05,
      "loss": 0.0018,
      "step": 181270
    },
    {
      "epoch": 6.042666666666666,
      "grad_norm": 0.22459547221660614,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0015,
      "step": 181280
    },
    {
      "epoch": 6.043,
      "grad_norm": 0.2160799652338028,
      "learning_rate": 1.223125e-05,
      "loss": 0.0016,
      "step": 181290
    },
    {
      "epoch": 6.043333333333333,
      "grad_norm": 0.086805060505867,
      "learning_rate": 1.2229166666666668e-05,
      "loss": 0.0016,
      "step": 181300
    },
    {
      "epoch": 6.043666666666667,
      "grad_norm": 0.2282879501581192,
      "learning_rate": 1.2227083333333334e-05,
      "loss": 0.0019,
      "step": 181310
    },
    {
      "epoch": 6.044,
      "grad_norm": 0.05766141414642334,
      "learning_rate": 1.2225e-05,
      "loss": 0.0018,
      "step": 181320
    },
    {
      "epoch": 6.044333333333333,
      "grad_norm": 0.400180459022522,
      "learning_rate": 1.2222916666666668e-05,
      "loss": 0.0018,
      "step": 181330
    },
    {
      "epoch": 6.044666666666667,
      "grad_norm": 0.01259863842278719,
      "learning_rate": 1.2220833333333333e-05,
      "loss": 0.0021,
      "step": 181340
    },
    {
      "epoch": 6.045,
      "grad_norm": 0.26751452684402466,
      "learning_rate": 1.221875e-05,
      "loss": 0.0027,
      "step": 181350
    },
    {
      "epoch": 6.045333333333334,
      "grad_norm": 0.02936537377536297,
      "learning_rate": 1.2216666666666668e-05,
      "loss": 0.0012,
      "step": 181360
    },
    {
      "epoch": 6.0456666666666665,
      "grad_norm": 0.2891082167625427,
      "learning_rate": 1.2214583333333333e-05,
      "loss": 0.001,
      "step": 181370
    },
    {
      "epoch": 6.046,
      "grad_norm": 0.3426460325717926,
      "learning_rate": 1.22125e-05,
      "loss": 0.0018,
      "step": 181380
    },
    {
      "epoch": 6.046333333333333,
      "grad_norm": 0.030748551711440086,
      "learning_rate": 1.2210416666666668e-05,
      "loss": 0.0015,
      "step": 181390
    },
    {
      "epoch": 6.046666666666667,
      "grad_norm": 0.08659578859806061,
      "learning_rate": 1.2208333333333335e-05,
      "loss": 0.0018,
      "step": 181400
    },
    {
      "epoch": 6.047,
      "grad_norm": 0.5422397255897522,
      "learning_rate": 1.220625e-05,
      "loss": 0.0014,
      "step": 181410
    },
    {
      "epoch": 6.0473333333333334,
      "grad_norm": 0.28543326258659363,
      "learning_rate": 1.2204166666666667e-05,
      "loss": 0.0017,
      "step": 181420
    },
    {
      "epoch": 6.047666666666666,
      "grad_norm": 0.20143859088420868,
      "learning_rate": 1.2202083333333333e-05,
      "loss": 0.0016,
      "step": 181430
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.31428980827331543,
      "learning_rate": 1.22e-05,
      "loss": 0.0016,
      "step": 181440
    },
    {
      "epoch": 6.048333333333333,
      "grad_norm": 0.2569867670536041,
      "learning_rate": 1.2197916666666667e-05,
      "loss": 0.0014,
      "step": 181450
    },
    {
      "epoch": 6.048666666666667,
      "grad_norm": 0.08735638111829758,
      "learning_rate": 1.2195833333333334e-05,
      "loss": 0.0014,
      "step": 181460
    },
    {
      "epoch": 6.049,
      "grad_norm": 0.11449652910232544,
      "learning_rate": 1.2193750000000002e-05,
      "loss": 0.0015,
      "step": 181470
    },
    {
      "epoch": 6.049333333333333,
      "grad_norm": 0.057452671229839325,
      "learning_rate": 1.2191666666666667e-05,
      "loss": 0.002,
      "step": 181480
    },
    {
      "epoch": 6.049666666666667,
      "grad_norm": 0.4956987202167511,
      "learning_rate": 1.2189583333333333e-05,
      "loss": 0.0014,
      "step": 181490
    },
    {
      "epoch": 6.05,
      "grad_norm": 0.008814248256385326,
      "learning_rate": 1.21875e-05,
      "loss": 0.0015,
      "step": 181500
    },
    {
      "epoch": 6.050333333333334,
      "grad_norm": 0.25739943981170654,
      "learning_rate": 1.2185416666666667e-05,
      "loss": 0.0017,
      "step": 181510
    },
    {
      "epoch": 6.050666666666666,
      "grad_norm": 0.2849833369255066,
      "learning_rate": 1.2183333333333334e-05,
      "loss": 0.0016,
      "step": 181520
    },
    {
      "epoch": 6.051,
      "grad_norm": 0.22872160375118256,
      "learning_rate": 1.2181250000000001e-05,
      "loss": 0.0017,
      "step": 181530
    },
    {
      "epoch": 6.051333333333333,
      "grad_norm": 0.14299477636814117,
      "learning_rate": 1.2179166666666669e-05,
      "loss": 0.0025,
      "step": 181540
    },
    {
      "epoch": 6.051666666666667,
      "grad_norm": 0.08567769080400467,
      "learning_rate": 1.2177083333333334e-05,
      "loss": 0.002,
      "step": 181550
    },
    {
      "epoch": 6.052,
      "grad_norm": 0.2567940354347229,
      "learning_rate": 1.2175e-05,
      "loss": 0.0013,
      "step": 181560
    },
    {
      "epoch": 6.052333333333333,
      "grad_norm": 0.228557288646698,
      "learning_rate": 1.2172916666666667e-05,
      "loss": 0.0016,
      "step": 181570
    },
    {
      "epoch": 6.052666666666667,
      "grad_norm": 0.2287190854549408,
      "learning_rate": 1.2170833333333334e-05,
      "loss": 0.0015,
      "step": 181580
    },
    {
      "epoch": 6.053,
      "grad_norm": 0.3141317367553711,
      "learning_rate": 1.2168750000000001e-05,
      "loss": 0.0013,
      "step": 181590
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 0.1721048504114151,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0016,
      "step": 181600
    },
    {
      "epoch": 6.0536666666666665,
      "grad_norm": 0.11420872807502747,
      "learning_rate": 1.2164583333333334e-05,
      "loss": 0.0022,
      "step": 181610
    },
    {
      "epoch": 6.054,
      "grad_norm": 0.172645702958107,
      "learning_rate": 1.2162500000000001e-05,
      "loss": 0.0022,
      "step": 181620
    },
    {
      "epoch": 6.054333333333333,
      "grad_norm": 0.030028149485588074,
      "learning_rate": 1.2160416666666666e-05,
      "loss": 0.0016,
      "step": 181630
    },
    {
      "epoch": 6.054666666666667,
      "grad_norm": 0.2855336666107178,
      "learning_rate": 1.2158333333333334e-05,
      "loss": 0.0015,
      "step": 181640
    },
    {
      "epoch": 6.055,
      "grad_norm": 0.05855240672826767,
      "learning_rate": 1.215625e-05,
      "loss": 0.0015,
      "step": 181650
    },
    {
      "epoch": 6.0553333333333335,
      "grad_norm": 0.43739208579063416,
      "learning_rate": 1.2154166666666668e-05,
      "loss": 0.0015,
      "step": 181660
    },
    {
      "epoch": 6.055666666666666,
      "grad_norm": 0.03411552309989929,
      "learning_rate": 1.2152083333333333e-05,
      "loss": 0.0016,
      "step": 181670
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.2001858353614807,
      "learning_rate": 1.215e-05,
      "loss": 0.0013,
      "step": 181680
    },
    {
      "epoch": 6.056333333333333,
      "grad_norm": 0.3995583951473236,
      "learning_rate": 1.2147916666666668e-05,
      "loss": 0.0018,
      "step": 181690
    },
    {
      "epoch": 6.056666666666667,
      "grad_norm": 0.08579421043395996,
      "learning_rate": 1.2145833333333333e-05,
      "loss": 0.0014,
      "step": 181700
    },
    {
      "epoch": 6.057,
      "grad_norm": 0.5204465389251709,
      "learning_rate": 1.214375e-05,
      "loss": 0.0018,
      "step": 181710
    },
    {
      "epoch": 6.057333333333333,
      "grad_norm": 0.14398643374443054,
      "learning_rate": 1.2141666666666668e-05,
      "loss": 0.0018,
      "step": 181720
    },
    {
      "epoch": 6.057666666666667,
      "grad_norm": 0.1718733012676239,
      "learning_rate": 1.2139583333333333e-05,
      "loss": 0.002,
      "step": 181730
    },
    {
      "epoch": 6.058,
      "grad_norm": 0.1437680870294571,
      "learning_rate": 1.21375e-05,
      "loss": 0.0014,
      "step": 181740
    },
    {
      "epoch": 6.058333333333334,
      "grad_norm": 0.2574473023414612,
      "learning_rate": 1.2135416666666667e-05,
      "loss": 0.0019,
      "step": 181750
    },
    {
      "epoch": 6.058666666666666,
      "grad_norm": 0.0580797977745533,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0019,
      "step": 181760
    },
    {
      "epoch": 6.059,
      "grad_norm": 0.2567621171474457,
      "learning_rate": 1.213125e-05,
      "loss": 0.0014,
      "step": 181770
    },
    {
      "epoch": 6.059333333333333,
      "grad_norm": 0.005116698332130909,
      "learning_rate": 1.2129166666666667e-05,
      "loss": 0.0012,
      "step": 181780
    },
    {
      "epoch": 6.059666666666667,
      "grad_norm": 0.31430137157440186,
      "learning_rate": 1.2127083333333333e-05,
      "loss": 0.0016,
      "step": 181790
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.2570717930793762,
      "learning_rate": 1.2125e-05,
      "loss": 0.0013,
      "step": 181800
    },
    {
      "epoch": 6.060333333333333,
      "grad_norm": 0.28592589497566223,
      "learning_rate": 1.2122916666666667e-05,
      "loss": 0.0016,
      "step": 181810
    },
    {
      "epoch": 6.060666666666667,
      "grad_norm": 0.22813190519809723,
      "learning_rate": 1.2120833333333334e-05,
      "loss": 0.0011,
      "step": 181820
    },
    {
      "epoch": 6.061,
      "grad_norm": 0.2292754054069519,
      "learning_rate": 1.2118750000000001e-05,
      "loss": 0.0018,
      "step": 181830
    },
    {
      "epoch": 6.061333333333334,
      "grad_norm": 0.2046954482793808,
      "learning_rate": 1.2116666666666669e-05,
      "loss": 0.0018,
      "step": 181840
    },
    {
      "epoch": 6.0616666666666665,
      "grad_norm": 0.14270558953285217,
      "learning_rate": 1.2114583333333334e-05,
      "loss": 0.0021,
      "step": 181850
    },
    {
      "epoch": 6.062,
      "grad_norm": 0.08651739358901978,
      "learning_rate": 1.21125e-05,
      "loss": 0.0024,
      "step": 181860
    },
    {
      "epoch": 6.062333333333333,
      "grad_norm": 0.17121732234954834,
      "learning_rate": 1.2110416666666667e-05,
      "loss": 0.0019,
      "step": 181870
    },
    {
      "epoch": 6.062666666666667,
      "grad_norm": 0.32268792390823364,
      "learning_rate": 1.2108333333333334e-05,
      "loss": 0.0015,
      "step": 181880
    },
    {
      "epoch": 6.063,
      "grad_norm": 0.1713656187057495,
      "learning_rate": 1.2106250000000001e-05,
      "loss": 0.0017,
      "step": 181890
    },
    {
      "epoch": 6.0633333333333335,
      "grad_norm": 0.0574740469455719,
      "learning_rate": 1.2104166666666668e-05,
      "loss": 0.0015,
      "step": 181900
    },
    {
      "epoch": 6.063666666666666,
      "grad_norm": 0.23513339459896088,
      "learning_rate": 1.2102083333333334e-05,
      "loss": 0.0021,
      "step": 181910
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.031282924115657806,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.0015,
      "step": 181920
    },
    {
      "epoch": 6.064333333333333,
      "grad_norm": 0.08608823269605637,
      "learning_rate": 1.2097916666666667e-05,
      "loss": 0.0014,
      "step": 181930
    },
    {
      "epoch": 6.064666666666667,
      "grad_norm": 0.1719241440296173,
      "learning_rate": 1.2095833333333334e-05,
      "loss": 0.0019,
      "step": 181940
    },
    {
      "epoch": 6.065,
      "grad_norm": 0.31372904777526855,
      "learning_rate": 1.2093750000000001e-05,
      "loss": 0.0016,
      "step": 181950
    },
    {
      "epoch": 6.065333333333333,
      "grad_norm": 0.08755388855934143,
      "learning_rate": 1.2091666666666668e-05,
      "loss": 0.0018,
      "step": 181960
    },
    {
      "epoch": 6.065666666666667,
      "grad_norm": 0.011341042816638947,
      "learning_rate": 1.2089583333333334e-05,
      "loss": 0.0019,
      "step": 181970
    },
    {
      "epoch": 6.066,
      "grad_norm": 0.20001478493213654,
      "learning_rate": 1.20875e-05,
      "loss": 0.0019,
      "step": 181980
    },
    {
      "epoch": 6.066333333333334,
      "grad_norm": 0.24160020053386688,
      "learning_rate": 1.2085416666666668e-05,
      "loss": 0.002,
      "step": 181990
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.1714586615562439,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.0016,
      "step": 182000
    },
    {
      "epoch": 6.067,
      "grad_norm": 0.05770977586507797,
      "learning_rate": 1.208125e-05,
      "loss": 0.0025,
      "step": 182010
    },
    {
      "epoch": 6.067333333333333,
      "grad_norm": 0.2569485604763031,
      "learning_rate": 1.2079166666666668e-05,
      "loss": 0.0018,
      "step": 182020
    },
    {
      "epoch": 6.067666666666667,
      "grad_norm": 0.06492707133293152,
      "learning_rate": 1.2077083333333333e-05,
      "loss": 0.0015,
      "step": 182030
    },
    {
      "epoch": 6.068,
      "grad_norm": 0.15249693393707275,
      "learning_rate": 1.2075e-05,
      "loss": 0.0019,
      "step": 182040
    },
    {
      "epoch": 6.068333333333333,
      "grad_norm": 0.008198292925953865,
      "learning_rate": 1.2072916666666668e-05,
      "loss": 0.0014,
      "step": 182050
    },
    {
      "epoch": 6.068666666666667,
      "grad_norm": 0.1433139592409134,
      "learning_rate": 1.2070833333333335e-05,
      "loss": 0.0023,
      "step": 182060
    },
    {
      "epoch": 6.069,
      "grad_norm": 0.08701019734144211,
      "learning_rate": 1.206875e-05,
      "loss": 0.0019,
      "step": 182070
    },
    {
      "epoch": 6.069333333333334,
      "grad_norm": 0.08569414168596268,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0027,
      "step": 182080
    },
    {
      "epoch": 6.0696666666666665,
      "grad_norm": 0.18864195048809052,
      "learning_rate": 1.2064583333333333e-05,
      "loss": 0.0016,
      "step": 182090
    },
    {
      "epoch": 6.07,
      "grad_norm": 0.14292415976524353,
      "learning_rate": 1.20625e-05,
      "loss": 0.0021,
      "step": 182100
    },
    {
      "epoch": 6.070333333333333,
      "grad_norm": 0.14569300413131714,
      "learning_rate": 1.2060416666666667e-05,
      "loss": 0.0018,
      "step": 182110
    },
    {
      "epoch": 6.070666666666667,
      "grad_norm": 0.14297519624233246,
      "learning_rate": 1.2058333333333334e-05,
      "loss": 0.0016,
      "step": 182120
    },
    {
      "epoch": 6.071,
      "grad_norm": 0.27431556582450867,
      "learning_rate": 1.2056250000000002e-05,
      "loss": 0.002,
      "step": 182130
    },
    {
      "epoch": 6.0713333333333335,
      "grad_norm": 0.38969290256500244,
      "learning_rate": 1.2054166666666667e-05,
      "loss": 0.002,
      "step": 182140
    },
    {
      "epoch": 6.071666666666666,
      "grad_norm": 0.030105112120509148,
      "learning_rate": 1.2052083333333333e-05,
      "loss": 0.0014,
      "step": 182150
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.2035607546567917,
      "learning_rate": 1.205e-05,
      "loss": 0.0021,
      "step": 182160
    },
    {
      "epoch": 6.072333333333333,
      "grad_norm": 0.22844669222831726,
      "learning_rate": 1.2047916666666667e-05,
      "loss": 0.0014,
      "step": 182170
    },
    {
      "epoch": 6.072666666666667,
      "grad_norm": 0.32722705602645874,
      "learning_rate": 1.2045833333333334e-05,
      "loss": 0.0026,
      "step": 182180
    },
    {
      "epoch": 6.073,
      "grad_norm": 0.030224433168768883,
      "learning_rate": 1.2043750000000001e-05,
      "loss": 0.0018,
      "step": 182190
    },
    {
      "epoch": 6.073333333333333,
      "grad_norm": 0.4400382936000824,
      "learning_rate": 1.2041666666666669e-05,
      "loss": 0.002,
      "step": 182200
    },
    {
      "epoch": 6.073666666666667,
      "grad_norm": 0.017251603305339813,
      "learning_rate": 1.2039583333333334e-05,
      "loss": 0.0015,
      "step": 182210
    },
    {
      "epoch": 6.074,
      "grad_norm": 0.5127111673355103,
      "learning_rate": 1.20375e-05,
      "loss": 0.0016,
      "step": 182220
    },
    {
      "epoch": 6.074333333333334,
      "grad_norm": 0.1819862574338913,
      "learning_rate": 1.2035416666666667e-05,
      "loss": 0.0017,
      "step": 182230
    },
    {
      "epoch": 6.074666666666666,
      "grad_norm": 0.05846560001373291,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0019,
      "step": 182240
    },
    {
      "epoch": 6.075,
      "grad_norm": 0.3688124120235443,
      "learning_rate": 1.2031250000000001e-05,
      "loss": 0.0017,
      "step": 182250
    },
    {
      "epoch": 6.075333333333333,
      "grad_norm": 0.11443155258893967,
      "learning_rate": 1.2029166666666668e-05,
      "loss": 0.0014,
      "step": 182260
    },
    {
      "epoch": 6.075666666666667,
      "grad_norm": 0.1150827556848526,
      "learning_rate": 1.2027083333333334e-05,
      "loss": 0.0014,
      "step": 182270
    },
    {
      "epoch": 6.076,
      "grad_norm": 0.010190257802605629,
      "learning_rate": 1.2025000000000001e-05,
      "loss": 0.0014,
      "step": 182280
    },
    {
      "epoch": 6.076333333333333,
      "grad_norm": 0.2853398025035858,
      "learning_rate": 1.2022916666666666e-05,
      "loss": 0.0021,
      "step": 182290
    },
    {
      "epoch": 6.076666666666666,
      "grad_norm": 0.26057571172714233,
      "learning_rate": 1.2020833333333334e-05,
      "loss": 0.0016,
      "step": 182300
    },
    {
      "epoch": 6.077,
      "grad_norm": 0.03006449155509472,
      "learning_rate": 1.201875e-05,
      "loss": 0.0017,
      "step": 182310
    },
    {
      "epoch": 6.077333333333334,
      "grad_norm": 0.03584567457437515,
      "learning_rate": 1.2016666666666668e-05,
      "loss": 0.0014,
      "step": 182320
    },
    {
      "epoch": 6.0776666666666666,
      "grad_norm": 0.199724942445755,
      "learning_rate": 1.2014583333333333e-05,
      "loss": 0.0016,
      "step": 182330
    },
    {
      "epoch": 6.078,
      "grad_norm": 0.14841516315937042,
      "learning_rate": 1.20125e-05,
      "loss": 0.002,
      "step": 182340
    },
    {
      "epoch": 6.078333333333333,
      "grad_norm": 0.11554404348134995,
      "learning_rate": 1.2010416666666668e-05,
      "loss": 0.0019,
      "step": 182350
    },
    {
      "epoch": 6.078666666666667,
      "grad_norm": 0.4284417927265167,
      "learning_rate": 1.2008333333333333e-05,
      "loss": 0.0019,
      "step": 182360
    },
    {
      "epoch": 6.079,
      "grad_norm": 0.08732660114765167,
      "learning_rate": 1.200625e-05,
      "loss": 0.0019,
      "step": 182370
    },
    {
      "epoch": 6.0793333333333335,
      "grad_norm": 0.19991633296012878,
      "learning_rate": 1.2004166666666668e-05,
      "loss": 0.0018,
      "step": 182380
    },
    {
      "epoch": 6.079666666666666,
      "grad_norm": 0.029492629691958427,
      "learning_rate": 1.2002083333333333e-05,
      "loss": 0.0018,
      "step": 182390
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.09285737574100494,
      "learning_rate": 1.2e-05,
      "loss": 0.0019,
      "step": 182400
    },
    {
      "epoch": 6.080333333333333,
      "grad_norm": 0.29823940992355347,
      "learning_rate": 1.1997916666666667e-05,
      "loss": 0.0022,
      "step": 182410
    },
    {
      "epoch": 6.080666666666667,
      "grad_norm": 0.1149604320526123,
      "learning_rate": 1.1995833333333335e-05,
      "loss": 0.0022,
      "step": 182420
    },
    {
      "epoch": 6.081,
      "grad_norm": 0.08575267344713211,
      "learning_rate": 1.199375e-05,
      "loss": 0.0016,
      "step": 182430
    },
    {
      "epoch": 6.081333333333333,
      "grad_norm": 0.030629193410277367,
      "learning_rate": 1.1991666666666667e-05,
      "loss": 0.0017,
      "step": 182440
    },
    {
      "epoch": 6.081666666666667,
      "grad_norm": 0.08594765514135361,
      "learning_rate": 1.1989583333333333e-05,
      "loss": 0.0012,
      "step": 182450
    },
    {
      "epoch": 6.082,
      "grad_norm": 0.18605315685272217,
      "learning_rate": 1.19875e-05,
      "loss": 0.002,
      "step": 182460
    },
    {
      "epoch": 6.082333333333334,
      "grad_norm": 0.47091904282569885,
      "learning_rate": 1.1985416666666667e-05,
      "loss": 0.0019,
      "step": 182470
    },
    {
      "epoch": 6.082666666666666,
      "grad_norm": 0.20991560816764832,
      "learning_rate": 1.1983333333333334e-05,
      "loss": 0.0018,
      "step": 182480
    },
    {
      "epoch": 6.083,
      "grad_norm": 0.2855740785598755,
      "learning_rate": 1.1981250000000002e-05,
      "loss": 0.0015,
      "step": 182490
    },
    {
      "epoch": 6.083333333333333,
      "grad_norm": 0.22832748293876648,
      "learning_rate": 1.1979166666666667e-05,
      "loss": 0.002,
      "step": 182500
    },
    {
      "epoch": 6.083666666666667,
      "grad_norm": 0.3146568834781647,
      "learning_rate": 1.1977083333333334e-05,
      "loss": 0.002,
      "step": 182510
    },
    {
      "epoch": 6.084,
      "grad_norm": 0.08586535602807999,
      "learning_rate": 1.1975e-05,
      "loss": 0.0018,
      "step": 182520
    },
    {
      "epoch": 6.084333333333333,
      "grad_norm": 0.20037353038787842,
      "learning_rate": 1.1972916666666667e-05,
      "loss": 0.0014,
      "step": 182530
    },
    {
      "epoch": 6.084666666666667,
      "grad_norm": 0.11441409587860107,
      "learning_rate": 1.1970833333333334e-05,
      "loss": 0.0014,
      "step": 182540
    },
    {
      "epoch": 6.085,
      "grad_norm": 0.3533385396003723,
      "learning_rate": 1.1968750000000001e-05,
      "loss": 0.0019,
      "step": 182550
    },
    {
      "epoch": 6.085333333333334,
      "grad_norm": 0.041161514818668365,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0017,
      "step": 182560
    },
    {
      "epoch": 6.085666666666667,
      "grad_norm": 0.22842133045196533,
      "learning_rate": 1.1964583333333334e-05,
      "loss": 0.0017,
      "step": 182570
    },
    {
      "epoch": 6.086,
      "grad_norm": 0.1712697148323059,
      "learning_rate": 1.19625e-05,
      "loss": 0.0021,
      "step": 182580
    },
    {
      "epoch": 6.086333333333333,
      "grad_norm": 0.2000718116760254,
      "learning_rate": 1.1960416666666667e-05,
      "loss": 0.0013,
      "step": 182590
    },
    {
      "epoch": 6.086666666666667,
      "grad_norm": 0.059288423508405685,
      "learning_rate": 1.1958333333333334e-05,
      "loss": 0.0017,
      "step": 182600
    },
    {
      "epoch": 6.087,
      "grad_norm": 0.23259331285953522,
      "learning_rate": 1.1956250000000001e-05,
      "loss": 0.0025,
      "step": 182610
    },
    {
      "epoch": 6.0873333333333335,
      "grad_norm": 0.28547826409339905,
      "learning_rate": 1.1954166666666668e-05,
      "loss": 0.0016,
      "step": 182620
    },
    {
      "epoch": 6.087666666666666,
      "grad_norm": 0.17131398618221283,
      "learning_rate": 1.1952083333333335e-05,
      "loss": 0.0024,
      "step": 182630
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.17271541059017181,
      "learning_rate": 1.195e-05,
      "loss": 0.0022,
      "step": 182640
    },
    {
      "epoch": 6.088333333333333,
      "grad_norm": 0.1998024582862854,
      "learning_rate": 1.1947916666666666e-05,
      "loss": 0.0018,
      "step": 182650
    },
    {
      "epoch": 6.088666666666667,
      "grad_norm": 0.035871971398591995,
      "learning_rate": 1.1945833333333333e-05,
      "loss": 0.0015,
      "step": 182660
    },
    {
      "epoch": 6.089,
      "grad_norm": 0.314071387052536,
      "learning_rate": 1.194375e-05,
      "loss": 0.0019,
      "step": 182670
    },
    {
      "epoch": 6.089333333333333,
      "grad_norm": 0.20147211849689484,
      "learning_rate": 1.1941666666666668e-05,
      "loss": 0.0013,
      "step": 182680
    },
    {
      "epoch": 6.089666666666667,
      "grad_norm": 0.27302467823028564,
      "learning_rate": 1.1939583333333335e-05,
      "loss": 0.0017,
      "step": 182690
    },
    {
      "epoch": 6.09,
      "grad_norm": 0.030181607231497765,
      "learning_rate": 1.19375e-05,
      "loss": 0.0018,
      "step": 182700
    },
    {
      "epoch": 6.090333333333334,
      "grad_norm": 0.14356710016727448,
      "learning_rate": 1.1935416666666668e-05,
      "loss": 0.0014,
      "step": 182710
    },
    {
      "epoch": 6.0906666666666665,
      "grad_norm": 0.01148148626089096,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0016,
      "step": 182720
    },
    {
      "epoch": 6.091,
      "grad_norm": 0.008011064492166042,
      "learning_rate": 1.193125e-05,
      "loss": 0.0017,
      "step": 182730
    },
    {
      "epoch": 6.091333333333333,
      "grad_norm": 0.14364838600158691,
      "learning_rate": 1.1929166666666668e-05,
      "loss": 0.0022,
      "step": 182740
    },
    {
      "epoch": 6.091666666666667,
      "grad_norm": 0.086419478058815,
      "learning_rate": 1.1927083333333335e-05,
      "loss": 0.0014,
      "step": 182750
    },
    {
      "epoch": 6.092,
      "grad_norm": 0.5422254800796509,
      "learning_rate": 1.1925e-05,
      "loss": 0.0014,
      "step": 182760
    },
    {
      "epoch": 6.092333333333333,
      "grad_norm": 0.14321036636829376,
      "learning_rate": 1.1922916666666667e-05,
      "loss": 0.0015,
      "step": 182770
    },
    {
      "epoch": 6.092666666666666,
      "grad_norm": 0.17160753905773163,
      "learning_rate": 1.1920833333333335e-05,
      "loss": 0.002,
      "step": 182780
    },
    {
      "epoch": 6.093,
      "grad_norm": 0.08602575957775116,
      "learning_rate": 1.191875e-05,
      "loss": 0.0014,
      "step": 182790
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 0.05753953009843826,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.0017,
      "step": 182800
    },
    {
      "epoch": 6.093666666666667,
      "grad_norm": 0.39948856830596924,
      "learning_rate": 1.1914583333333334e-05,
      "loss": 0.0014,
      "step": 182810
    },
    {
      "epoch": 6.094,
      "grad_norm": 0.08963974565267563,
      "learning_rate": 1.19125e-05,
      "loss": 0.002,
      "step": 182820
    },
    {
      "epoch": 6.094333333333333,
      "grad_norm": 0.057763583958148956,
      "learning_rate": 1.1910416666666667e-05,
      "loss": 0.0018,
      "step": 182830
    },
    {
      "epoch": 6.094666666666667,
      "grad_norm": 0.057507239282131195,
      "learning_rate": 1.1908333333333334e-05,
      "loss": 0.0016,
      "step": 182840
    },
    {
      "epoch": 6.095,
      "grad_norm": 0.7735025882720947,
      "learning_rate": 1.1906250000000001e-05,
      "loss": 0.0017,
      "step": 182850
    },
    {
      "epoch": 6.0953333333333335,
      "grad_norm": 0.08615618199110031,
      "learning_rate": 1.1904166666666667e-05,
      "loss": 0.0012,
      "step": 182860
    },
    {
      "epoch": 6.095666666666666,
      "grad_norm": 0.37064328789711,
      "learning_rate": 1.1902083333333334e-05,
      "loss": 0.0015,
      "step": 182870
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.2568981945514679,
      "learning_rate": 1.19e-05,
      "loss": 0.0017,
      "step": 182880
    },
    {
      "epoch": 6.096333333333333,
      "grad_norm": 0.19976994395256042,
      "learning_rate": 1.1897916666666667e-05,
      "loss": 0.0022,
      "step": 182890
    },
    {
      "epoch": 6.096666666666667,
      "grad_norm": 0.1428360641002655,
      "learning_rate": 1.1895833333333334e-05,
      "loss": 0.0017,
      "step": 182900
    },
    {
      "epoch": 6.097,
      "grad_norm": 0.11625928431749344,
      "learning_rate": 1.1893750000000001e-05,
      "loss": 0.0013,
      "step": 182910
    },
    {
      "epoch": 6.097333333333333,
      "grad_norm": 0.058070141822099686,
      "learning_rate": 1.1891666666666668e-05,
      "loss": 0.0023,
      "step": 182920
    },
    {
      "epoch": 6.097666666666667,
      "grad_norm": 0.2574480175971985,
      "learning_rate": 1.1889583333333334e-05,
      "loss": 0.0012,
      "step": 182930
    },
    {
      "epoch": 6.098,
      "grad_norm": 0.08830390125513077,
      "learning_rate": 1.18875e-05,
      "loss": 0.0014,
      "step": 182940
    },
    {
      "epoch": 6.098333333333334,
      "grad_norm": 0.17236736416816711,
      "learning_rate": 1.1885416666666666e-05,
      "loss": 0.0017,
      "step": 182950
    },
    {
      "epoch": 6.0986666666666665,
      "grad_norm": 0.20020979642868042,
      "learning_rate": 1.1883333333333334e-05,
      "loss": 0.0025,
      "step": 182960
    },
    {
      "epoch": 6.099,
      "grad_norm": 0.200349360704422,
      "learning_rate": 1.188125e-05,
      "loss": 0.0021,
      "step": 182970
    },
    {
      "epoch": 6.099333333333333,
      "grad_norm": 0.3979836404323578,
      "learning_rate": 1.1879166666666668e-05,
      "loss": 0.0021,
      "step": 182980
    },
    {
      "epoch": 6.099666666666667,
      "grad_norm": 0.17156288027763367,
      "learning_rate": 1.1877083333333335e-05,
      "loss": 0.0015,
      "step": 182990
    },
    {
      "epoch": 6.1,
      "grad_norm": 0.2573243975639343,
      "learning_rate": 1.1875e-05,
      "loss": 0.002,
      "step": 183000
    },
    {
      "epoch": 6.100333333333333,
      "grad_norm": 0.11444572359323502,
      "learning_rate": 1.1872916666666666e-05,
      "loss": 0.0022,
      "step": 183010
    },
    {
      "epoch": 6.100666666666666,
      "grad_norm": 0.05782710760831833,
      "learning_rate": 1.1870833333333333e-05,
      "loss": 0.0016,
      "step": 183020
    },
    {
      "epoch": 6.101,
      "grad_norm": 0.02983744442462921,
      "learning_rate": 1.186875e-05,
      "loss": 0.0016,
      "step": 183030
    },
    {
      "epoch": 6.101333333333334,
      "grad_norm": 0.21974743902683258,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0016,
      "step": 183040
    },
    {
      "epoch": 6.101666666666667,
      "grad_norm": 0.030256599187850952,
      "learning_rate": 1.1864583333333335e-05,
      "loss": 0.0012,
      "step": 183050
    },
    {
      "epoch": 6.102,
      "grad_norm": 0.1150922104716301,
      "learning_rate": 1.18625e-05,
      "loss": 0.0024,
      "step": 183060
    },
    {
      "epoch": 6.102333333333333,
      "grad_norm": 0.06374610215425491,
      "learning_rate": 1.1860416666666668e-05,
      "loss": 0.0013,
      "step": 183070
    },
    {
      "epoch": 6.102666666666667,
      "grad_norm": 0.5730350613594055,
      "learning_rate": 1.1858333333333333e-05,
      "loss": 0.002,
      "step": 183080
    },
    {
      "epoch": 6.103,
      "grad_norm": 0.17095018923282623,
      "learning_rate": 1.185625e-05,
      "loss": 0.0025,
      "step": 183090
    },
    {
      "epoch": 6.1033333333333335,
      "grad_norm": 0.11442931741476059,
      "learning_rate": 1.1854166666666667e-05,
      "loss": 0.002,
      "step": 183100
    },
    {
      "epoch": 6.103666666666666,
      "grad_norm": 0.1428367793560028,
      "learning_rate": 1.1852083333333335e-05,
      "loss": 0.0013,
      "step": 183110
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.14307570457458496,
      "learning_rate": 1.185e-05,
      "loss": 0.0013,
      "step": 183120
    },
    {
      "epoch": 6.104333333333333,
      "grad_norm": 0.11547625809907913,
      "learning_rate": 1.1847916666666667e-05,
      "loss": 0.002,
      "step": 183130
    },
    {
      "epoch": 6.104666666666667,
      "grad_norm": 0.0060551967471838,
      "learning_rate": 1.1845833333333334e-05,
      "loss": 0.0017,
      "step": 183140
    },
    {
      "epoch": 6.105,
      "grad_norm": 0.08548460155725479,
      "learning_rate": 1.184375e-05,
      "loss": 0.0016,
      "step": 183150
    },
    {
      "epoch": 6.105333333333333,
      "grad_norm": 0.11416777223348618,
      "learning_rate": 1.1841666666666667e-05,
      "loss": 0.0025,
      "step": 183160
    },
    {
      "epoch": 6.105666666666667,
      "grad_norm": 0.01148420013487339,
      "learning_rate": 1.1839583333333334e-05,
      "loss": 0.0017,
      "step": 183170
    },
    {
      "epoch": 6.106,
      "grad_norm": 0.1999608874320984,
      "learning_rate": 1.18375e-05,
      "loss": 0.0019,
      "step": 183180
    },
    {
      "epoch": 6.106333333333334,
      "grad_norm": 0.2000254988670349,
      "learning_rate": 1.1835416666666667e-05,
      "loss": 0.0022,
      "step": 183190
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 0.14272062480449677,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.0012,
      "step": 183200
    },
    {
      "epoch": 6.107,
      "grad_norm": 0.22831133008003235,
      "learning_rate": 1.1831250000000001e-05,
      "loss": 0.0016,
      "step": 183210
    },
    {
      "epoch": 6.107333333333333,
      "grad_norm": 0.2002960443496704,
      "learning_rate": 1.1829166666666667e-05,
      "loss": 0.0016,
      "step": 183220
    },
    {
      "epoch": 6.107666666666667,
      "grad_norm": 0.031672947108745575,
      "learning_rate": 1.1827083333333334e-05,
      "loss": 0.0013,
      "step": 183230
    },
    {
      "epoch": 6.108,
      "grad_norm": 0.228846475481987,
      "learning_rate": 1.1825e-05,
      "loss": 0.0024,
      "step": 183240
    },
    {
      "epoch": 6.108333333333333,
      "grad_norm": 0.08593519777059555,
      "learning_rate": 1.1822916666666667e-05,
      "loss": 0.0015,
      "step": 183250
    },
    {
      "epoch": 6.108666666666666,
      "grad_norm": 0.03062347322702408,
      "learning_rate": 1.1820833333333334e-05,
      "loss": 0.0013,
      "step": 183260
    },
    {
      "epoch": 6.109,
      "grad_norm": 0.010336003266274929,
      "learning_rate": 1.1818750000000001e-05,
      "loss": 0.0016,
      "step": 183270
    },
    {
      "epoch": 6.109333333333334,
      "grad_norm": 0.08613119274377823,
      "learning_rate": 1.1816666666666668e-05,
      "loss": 0.0015,
      "step": 183280
    },
    {
      "epoch": 6.109666666666667,
      "grad_norm": 0.11465626955032349,
      "learning_rate": 1.1814583333333334e-05,
      "loss": 0.0017,
      "step": 183290
    },
    {
      "epoch": 6.11,
      "grad_norm": 0.11495167016983032,
      "learning_rate": 1.1812499999999999e-05,
      "loss": 0.0013,
      "step": 183300
    },
    {
      "epoch": 6.110333333333333,
      "grad_norm": 0.5089160203933716,
      "learning_rate": 1.1810416666666666e-05,
      "loss": 0.0017,
      "step": 183310
    },
    {
      "epoch": 6.110666666666667,
      "grad_norm": 0.2576194107532501,
      "learning_rate": 1.1808333333333333e-05,
      "loss": 0.0016,
      "step": 183320
    },
    {
      "epoch": 6.111,
      "grad_norm": 0.1998431533575058,
      "learning_rate": 1.180625e-05,
      "loss": 0.002,
      "step": 183330
    },
    {
      "epoch": 6.1113333333333335,
      "grad_norm": 0.036539770662784576,
      "learning_rate": 1.1804166666666668e-05,
      "loss": 0.0019,
      "step": 183340
    },
    {
      "epoch": 6.111666666666666,
      "grad_norm": 0.14308036863803864,
      "learning_rate": 1.1802083333333335e-05,
      "loss": 0.0017,
      "step": 183350
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.17130513489246368,
      "learning_rate": 1.18e-05,
      "loss": 0.0018,
      "step": 183360
    },
    {
      "epoch": 6.112333333333333,
      "grad_norm": 0.6565564274787903,
      "learning_rate": 1.1797916666666666e-05,
      "loss": 0.0018,
      "step": 183370
    },
    {
      "epoch": 6.112666666666667,
      "grad_norm": 0.17132481932640076,
      "learning_rate": 1.1795833333333333e-05,
      "loss": 0.0019,
      "step": 183380
    },
    {
      "epoch": 6.113,
      "grad_norm": 0.007752642035484314,
      "learning_rate": 1.179375e-05,
      "loss": 0.002,
      "step": 183390
    },
    {
      "epoch": 6.113333333333333,
      "grad_norm": 0.08584681898355484,
      "learning_rate": 1.1791666666666668e-05,
      "loss": 0.0019,
      "step": 183400
    },
    {
      "epoch": 6.113666666666667,
      "grad_norm": 0.31404373049736023,
      "learning_rate": 1.1789583333333335e-05,
      "loss": 0.0018,
      "step": 183410
    },
    {
      "epoch": 6.114,
      "grad_norm": 0.34294557571411133,
      "learning_rate": 1.17875e-05,
      "loss": 0.0017,
      "step": 183420
    },
    {
      "epoch": 6.114333333333334,
      "grad_norm": 0.5009887218475342,
      "learning_rate": 1.1785416666666667e-05,
      "loss": 0.0018,
      "step": 183430
    },
    {
      "epoch": 6.1146666666666665,
      "grad_norm": 0.030791830271482468,
      "learning_rate": 1.1783333333333333e-05,
      "loss": 0.0017,
      "step": 183440
    },
    {
      "epoch": 6.115,
      "grad_norm": 0.029300536960363388,
      "learning_rate": 1.178125e-05,
      "loss": 0.0018,
      "step": 183450
    },
    {
      "epoch": 6.115333333333333,
      "grad_norm": 0.25712841749191284,
      "learning_rate": 1.1779166666666667e-05,
      "loss": 0.0018,
      "step": 183460
    },
    {
      "epoch": 6.115666666666667,
      "grad_norm": 0.22879193723201752,
      "learning_rate": 1.1777083333333334e-05,
      "loss": 0.0017,
      "step": 183470
    },
    {
      "epoch": 6.116,
      "grad_norm": 0.17863427102565765,
      "learning_rate": 1.1775e-05,
      "loss": 0.0021,
      "step": 183480
    },
    {
      "epoch": 6.116333333333333,
      "grad_norm": 0.20497578382492065,
      "learning_rate": 1.1772916666666667e-05,
      "loss": 0.0021,
      "step": 183490
    },
    {
      "epoch": 6.116666666666666,
      "grad_norm": 0.05775924026966095,
      "learning_rate": 1.1770833333333334e-05,
      "loss": 0.0017,
      "step": 183500
    },
    {
      "epoch": 6.117,
      "grad_norm": 0.31479111313819885,
      "learning_rate": 1.176875e-05,
      "loss": 0.0021,
      "step": 183510
    },
    {
      "epoch": 6.117333333333334,
      "grad_norm": 0.17155712842941284,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0016,
      "step": 183520
    },
    {
      "epoch": 6.117666666666667,
      "grad_norm": 0.031880997121334076,
      "learning_rate": 1.1764583333333334e-05,
      "loss": 0.0012,
      "step": 183530
    },
    {
      "epoch": 6.118,
      "grad_norm": 0.08602128177881241,
      "learning_rate": 1.17625e-05,
      "loss": 0.0013,
      "step": 183540
    },
    {
      "epoch": 6.118333333333333,
      "grad_norm": 0.08674008399248123,
      "learning_rate": 1.1760416666666667e-05,
      "loss": 0.0017,
      "step": 183550
    },
    {
      "epoch": 6.118666666666667,
      "grad_norm": 0.11445869505405426,
      "learning_rate": 1.1758333333333334e-05,
      "loss": 0.0022,
      "step": 183560
    },
    {
      "epoch": 6.119,
      "grad_norm": 0.08741788566112518,
      "learning_rate": 1.1756250000000001e-05,
      "loss": 0.0014,
      "step": 183570
    },
    {
      "epoch": 6.1193333333333335,
      "grad_norm": 0.1424221396446228,
      "learning_rate": 1.1754166666666668e-05,
      "loss": 0.002,
      "step": 183580
    },
    {
      "epoch": 6.119666666666666,
      "grad_norm": 0.11403030902147293,
      "learning_rate": 1.1752083333333334e-05,
      "loss": 0.0019,
      "step": 183590
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.3708767294883728,
      "learning_rate": 1.175e-05,
      "loss": 0.0018,
      "step": 183600
    },
    {
      "epoch": 6.120333333333333,
      "grad_norm": 0.030033985152840614,
      "learning_rate": 1.1747916666666666e-05,
      "loss": 0.0018,
      "step": 183610
    },
    {
      "epoch": 6.120666666666667,
      "grad_norm": 0.030071990564465523,
      "learning_rate": 1.1745833333333334e-05,
      "loss": 0.002,
      "step": 183620
    },
    {
      "epoch": 6.121,
      "grad_norm": 0.11887430399656296,
      "learning_rate": 1.1743750000000001e-05,
      "loss": 0.002,
      "step": 183630
    },
    {
      "epoch": 6.121333333333333,
      "grad_norm": 0.0861242413520813,
      "learning_rate": 1.1741666666666668e-05,
      "loss": 0.002,
      "step": 183640
    },
    {
      "epoch": 6.121666666666667,
      "grad_norm": 0.08989442884922028,
      "learning_rate": 1.1739583333333335e-05,
      "loss": 0.0013,
      "step": 183650
    },
    {
      "epoch": 6.122,
      "grad_norm": 0.6225925087928772,
      "learning_rate": 1.17375e-05,
      "loss": 0.002,
      "step": 183660
    },
    {
      "epoch": 6.122333333333334,
      "grad_norm": 0.057744719088077545,
      "learning_rate": 1.1735416666666666e-05,
      "loss": 0.0022,
      "step": 183670
    },
    {
      "epoch": 6.1226666666666665,
      "grad_norm": 0.052023280411958694,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0013,
      "step": 183680
    },
    {
      "epoch": 6.123,
      "grad_norm": 0.1426474004983902,
      "learning_rate": 1.173125e-05,
      "loss": 0.0015,
      "step": 183690
    },
    {
      "epoch": 6.123333333333333,
      "grad_norm": 0.3145015239715576,
      "learning_rate": 1.1729166666666668e-05,
      "loss": 0.0014,
      "step": 183700
    },
    {
      "epoch": 6.123666666666667,
      "grad_norm": 0.08602525293827057,
      "learning_rate": 1.1727083333333335e-05,
      "loss": 0.0017,
      "step": 183710
    },
    {
      "epoch": 6.124,
      "grad_norm": 0.2282838672399521,
      "learning_rate": 1.1725e-05,
      "loss": 0.0021,
      "step": 183720
    },
    {
      "epoch": 6.124333333333333,
      "grad_norm": 0.007908771745860577,
      "learning_rate": 1.1722916666666668e-05,
      "loss": 0.0018,
      "step": 183730
    },
    {
      "epoch": 6.124666666666666,
      "grad_norm": 0.03052033670246601,
      "learning_rate": 1.1720833333333333e-05,
      "loss": 0.0018,
      "step": 183740
    },
    {
      "epoch": 6.125,
      "grad_norm": 0.0622902438044548,
      "learning_rate": 1.171875e-05,
      "loss": 0.0013,
      "step": 183750
    },
    {
      "epoch": 6.125333333333334,
      "grad_norm": 0.31405049562454224,
      "learning_rate": 1.1716666666666667e-05,
      "loss": 0.0014,
      "step": 183760
    },
    {
      "epoch": 6.125666666666667,
      "grad_norm": 0.11429574340581894,
      "learning_rate": 1.1714583333333335e-05,
      "loss": 0.0015,
      "step": 183770
    },
    {
      "epoch": 6.126,
      "grad_norm": 0.010977032594382763,
      "learning_rate": 1.1712500000000002e-05,
      "loss": 0.0014,
      "step": 183780
    },
    {
      "epoch": 6.126333333333333,
      "grad_norm": 0.205429807305336,
      "learning_rate": 1.1710416666666667e-05,
      "loss": 0.0017,
      "step": 183790
    },
    {
      "epoch": 6.126666666666667,
      "grad_norm": 0.0857340395450592,
      "learning_rate": 1.1708333333333334e-05,
      "loss": 0.0022,
      "step": 183800
    },
    {
      "epoch": 6.127,
      "grad_norm": 0.058578718453645706,
      "learning_rate": 1.170625e-05,
      "loss": 0.0017,
      "step": 183810
    },
    {
      "epoch": 6.1273333333333335,
      "grad_norm": 0.058744046837091446,
      "learning_rate": 1.1704166666666667e-05,
      "loss": 0.0017,
      "step": 183820
    },
    {
      "epoch": 6.127666666666666,
      "grad_norm": 0.3706955909729004,
      "learning_rate": 1.1702083333333334e-05,
      "loss": 0.0015,
      "step": 183830
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.2854515016078949,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.0016,
      "step": 183840
    },
    {
      "epoch": 6.128333333333333,
      "grad_norm": 0.00967465154826641,
      "learning_rate": 1.1697916666666667e-05,
      "loss": 0.0021,
      "step": 183850
    },
    {
      "epoch": 6.128666666666667,
      "grad_norm": 0.20386983454227448,
      "learning_rate": 1.1695833333333334e-05,
      "loss": 0.0027,
      "step": 183860
    },
    {
      "epoch": 6.129,
      "grad_norm": 0.17142488062381744,
      "learning_rate": 1.1693750000000001e-05,
      "loss": 0.0026,
      "step": 183870
    },
    {
      "epoch": 6.129333333333333,
      "grad_norm": 0.08550422638654709,
      "learning_rate": 1.1691666666666667e-05,
      "loss": 0.0013,
      "step": 183880
    },
    {
      "epoch": 6.129666666666667,
      "grad_norm": 0.11439109593629837,
      "learning_rate": 1.1689583333333334e-05,
      "loss": 0.0022,
      "step": 183890
    },
    {
      "epoch": 6.13,
      "grad_norm": 0.057430513203144073,
      "learning_rate": 1.1687500000000001e-05,
      "loss": 0.0014,
      "step": 183900
    },
    {
      "epoch": 6.130333333333334,
      "grad_norm": 0.17151233553886414,
      "learning_rate": 1.1685416666666667e-05,
      "loss": 0.0022,
      "step": 183910
    },
    {
      "epoch": 6.1306666666666665,
      "grad_norm": 0.057989396154880524,
      "learning_rate": 1.1683333333333334e-05,
      "loss": 0.0016,
      "step": 183920
    },
    {
      "epoch": 6.131,
      "grad_norm": 0.171608105301857,
      "learning_rate": 1.1681250000000001e-05,
      "loss": 0.0016,
      "step": 183930
    },
    {
      "epoch": 6.131333333333333,
      "grad_norm": 0.05870116502046585,
      "learning_rate": 1.1679166666666668e-05,
      "loss": 0.0017,
      "step": 183940
    },
    {
      "epoch": 6.131666666666667,
      "grad_norm": 0.22920723259449005,
      "learning_rate": 1.1677083333333334e-05,
      "loss": 0.0018,
      "step": 183950
    },
    {
      "epoch": 6.132,
      "grad_norm": 0.11442898958921432,
      "learning_rate": 1.1675000000000001e-05,
      "loss": 0.0014,
      "step": 183960
    },
    {
      "epoch": 6.132333333333333,
      "grad_norm": 0.030088983476161957,
      "learning_rate": 1.1672916666666666e-05,
      "loss": 0.0015,
      "step": 183970
    },
    {
      "epoch": 6.132666666666666,
      "grad_norm": 0.016158731654286385,
      "learning_rate": 1.1670833333333334e-05,
      "loss": 0.0024,
      "step": 183980
    },
    {
      "epoch": 6.133,
      "grad_norm": 0.2566436231136322,
      "learning_rate": 1.166875e-05,
      "loss": 0.0012,
      "step": 183990
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.11485029011964798,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0013,
      "step": 184000
    },
    {
      "epoch": 6.133666666666667,
      "grad_norm": 0.17117397487163544,
      "learning_rate": 1.1664583333333335e-05,
      "loss": 0.0019,
      "step": 184010
    },
    {
      "epoch": 6.134,
      "grad_norm": 0.22890132665634155,
      "learning_rate": 1.16625e-05,
      "loss": 0.0016,
      "step": 184020
    },
    {
      "epoch": 6.134333333333333,
      "grad_norm": 0.371033638715744,
      "learning_rate": 1.1660416666666666e-05,
      "loss": 0.0016,
      "step": 184030
    },
    {
      "epoch": 6.134666666666667,
      "grad_norm": 0.10224398225545883,
      "learning_rate": 1.1658333333333333e-05,
      "loss": 0.0023,
      "step": 184040
    },
    {
      "epoch": 6.135,
      "grad_norm": 0.17140765488147736,
      "learning_rate": 1.165625e-05,
      "loss": 0.0026,
      "step": 184050
    },
    {
      "epoch": 6.1353333333333335,
      "grad_norm": 0.31432241201400757,
      "learning_rate": 1.1654166666666668e-05,
      "loss": 0.0024,
      "step": 184060
    },
    {
      "epoch": 6.135666666666666,
      "grad_norm": 0.17316517233848572,
      "learning_rate": 1.1652083333333335e-05,
      "loss": 0.0018,
      "step": 184070
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.28554314374923706,
      "learning_rate": 1.1650000000000002e-05,
      "loss": 0.0015,
      "step": 184080
    },
    {
      "epoch": 6.136333333333333,
      "grad_norm": 0.39928337931632996,
      "learning_rate": 1.1647916666666667e-05,
      "loss": 0.0019,
      "step": 184090
    },
    {
      "epoch": 6.136666666666667,
      "grad_norm": 0.16178882122039795,
      "learning_rate": 1.1645833333333333e-05,
      "loss": 0.0017,
      "step": 184100
    },
    {
      "epoch": 6.1370000000000005,
      "grad_norm": 0.11599011719226837,
      "learning_rate": 1.164375e-05,
      "loss": 0.0024,
      "step": 184110
    },
    {
      "epoch": 6.137333333333333,
      "grad_norm": 0.25767701864242554,
      "learning_rate": 1.1641666666666667e-05,
      "loss": 0.0014,
      "step": 184120
    },
    {
      "epoch": 6.137666666666667,
      "grad_norm": 0.0873517170548439,
      "learning_rate": 1.1639583333333334e-05,
      "loss": 0.0016,
      "step": 184130
    },
    {
      "epoch": 6.138,
      "grad_norm": 0.28557372093200684,
      "learning_rate": 1.1637500000000002e-05,
      "loss": 0.0015,
      "step": 184140
    },
    {
      "epoch": 6.138333333333334,
      "grad_norm": 0.2850346565246582,
      "learning_rate": 1.1635416666666667e-05,
      "loss": 0.0014,
      "step": 184150
    },
    {
      "epoch": 6.1386666666666665,
      "grad_norm": 0.029091568663716316,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0018,
      "step": 184160
    },
    {
      "epoch": 6.139,
      "grad_norm": 0.31598880887031555,
      "learning_rate": 1.163125e-05,
      "loss": 0.002,
      "step": 184170
    },
    {
      "epoch": 6.139333333333333,
      "grad_norm": 0.2581351399421692,
      "learning_rate": 1.1629166666666667e-05,
      "loss": 0.0015,
      "step": 184180
    },
    {
      "epoch": 6.139666666666667,
      "grad_norm": 0.28515154123306274,
      "learning_rate": 1.1627083333333334e-05,
      "loss": 0.0021,
      "step": 184190
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.06099380925297737,
      "learning_rate": 1.1625000000000001e-05,
      "loss": 0.0015,
      "step": 184200
    },
    {
      "epoch": 6.140333333333333,
      "grad_norm": 0.17134921252727509,
      "learning_rate": 1.1622916666666667e-05,
      "loss": 0.0015,
      "step": 184210
    },
    {
      "epoch": 6.140666666666666,
      "grad_norm": 0.03052724339067936,
      "learning_rate": 1.1620833333333334e-05,
      "loss": 0.0017,
      "step": 184220
    },
    {
      "epoch": 6.141,
      "grad_norm": 0.40261441469192505,
      "learning_rate": 1.1618750000000001e-05,
      "loss": 0.0019,
      "step": 184230
    },
    {
      "epoch": 6.141333333333334,
      "grad_norm": 0.08561766147613525,
      "learning_rate": 1.1616666666666667e-05,
      "loss": 0.0016,
      "step": 184240
    },
    {
      "epoch": 6.141666666666667,
      "grad_norm": 0.11491187661886215,
      "learning_rate": 1.1614583333333334e-05,
      "loss": 0.0014,
      "step": 184250
    },
    {
      "epoch": 6.142,
      "grad_norm": 0.22807833552360535,
      "learning_rate": 1.1612500000000001e-05,
      "loss": 0.0012,
      "step": 184260
    },
    {
      "epoch": 6.142333333333333,
      "grad_norm": 0.029778262600302696,
      "learning_rate": 1.1610416666666667e-05,
      "loss": 0.0017,
      "step": 184270
    },
    {
      "epoch": 6.142666666666667,
      "grad_norm": 0.28564396500587463,
      "learning_rate": 1.1608333333333334e-05,
      "loss": 0.0016,
      "step": 184280
    },
    {
      "epoch": 6.143,
      "grad_norm": 0.06265424191951752,
      "learning_rate": 1.1606250000000001e-05,
      "loss": 0.0025,
      "step": 184290
    },
    {
      "epoch": 6.1433333333333335,
      "grad_norm": 0.5646045804023743,
      "learning_rate": 1.1604166666666668e-05,
      "loss": 0.0017,
      "step": 184300
    },
    {
      "epoch": 6.143666666666666,
      "grad_norm": 0.2860264480113983,
      "learning_rate": 1.1602083333333334e-05,
      "loss": 0.0017,
      "step": 184310
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.08607413619756699,
      "learning_rate": 1.16e-05,
      "loss": 0.0017,
      "step": 184320
    },
    {
      "epoch": 6.144333333333333,
      "grad_norm": 0.37403661012649536,
      "learning_rate": 1.1597916666666666e-05,
      "loss": 0.0019,
      "step": 184330
    },
    {
      "epoch": 6.144666666666667,
      "grad_norm": 0.18743865191936493,
      "learning_rate": 1.1595833333333333e-05,
      "loss": 0.0016,
      "step": 184340
    },
    {
      "epoch": 6.145,
      "grad_norm": 0.08156871050596237,
      "learning_rate": 1.159375e-05,
      "loss": 0.0016,
      "step": 184350
    },
    {
      "epoch": 6.145333333333333,
      "grad_norm": 0.11477816849946976,
      "learning_rate": 1.1591666666666668e-05,
      "loss": 0.0017,
      "step": 184360
    },
    {
      "epoch": 6.145666666666667,
      "grad_norm": 0.057932764291763306,
      "learning_rate": 1.1589583333333335e-05,
      "loss": 0.0017,
      "step": 184370
    },
    {
      "epoch": 6.146,
      "grad_norm": 0.057226259261369705,
      "learning_rate": 1.15875e-05,
      "loss": 0.0017,
      "step": 184380
    },
    {
      "epoch": 6.146333333333334,
      "grad_norm": 0.01129229087382555,
      "learning_rate": 1.1585416666666666e-05,
      "loss": 0.0017,
      "step": 184390
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 0.41560447216033936,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.0015,
      "step": 184400
    },
    {
      "epoch": 6.147,
      "grad_norm": 0.2578273415565491,
      "learning_rate": 1.158125e-05,
      "loss": 0.0021,
      "step": 184410
    },
    {
      "epoch": 6.147333333333333,
      "grad_norm": 0.1430518627166748,
      "learning_rate": 1.1579166666666667e-05,
      "loss": 0.0017,
      "step": 184420
    },
    {
      "epoch": 6.147666666666667,
      "grad_norm": 0.0861114040017128,
      "learning_rate": 1.1577083333333335e-05,
      "loss": 0.0014,
      "step": 184430
    },
    {
      "epoch": 6.148,
      "grad_norm": 0.31417909264564514,
      "learning_rate": 1.1575000000000002e-05,
      "loss": 0.0021,
      "step": 184440
    },
    {
      "epoch": 6.148333333333333,
      "grad_norm": 0.3276011049747467,
      "learning_rate": 1.1572916666666667e-05,
      "loss": 0.0023,
      "step": 184450
    },
    {
      "epoch": 6.148666666666666,
      "grad_norm": 0.00803565327078104,
      "learning_rate": 1.1570833333333333e-05,
      "loss": 0.0014,
      "step": 184460
    },
    {
      "epoch": 6.149,
      "grad_norm": 0.05831298977136612,
      "learning_rate": 1.156875e-05,
      "loss": 0.0022,
      "step": 184470
    },
    {
      "epoch": 6.149333333333334,
      "grad_norm": 0.27168479561805725,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.0013,
      "step": 184480
    },
    {
      "epoch": 6.149666666666667,
      "grad_norm": 0.2567065954208374,
      "learning_rate": 1.1564583333333334e-05,
      "loss": 0.0011,
      "step": 184490
    },
    {
      "epoch": 6.15,
      "grad_norm": 0.07747408747673035,
      "learning_rate": 1.1562500000000002e-05,
      "loss": 0.0014,
      "step": 184500
    },
    {
      "epoch": 6.150333333333333,
      "grad_norm": 0.10009896010160446,
      "learning_rate": 1.1560416666666667e-05,
      "loss": 0.0016,
      "step": 184510
    },
    {
      "epoch": 6.150666666666667,
      "grad_norm": 0.3445456027984619,
      "learning_rate": 1.1558333333333334e-05,
      "loss": 0.0019,
      "step": 184520
    },
    {
      "epoch": 6.151,
      "grad_norm": 0.029483389109373093,
      "learning_rate": 1.155625e-05,
      "loss": 0.0014,
      "step": 184530
    },
    {
      "epoch": 6.1513333333333335,
      "grad_norm": 0.058094657957553864,
      "learning_rate": 1.1554166666666667e-05,
      "loss": 0.0015,
      "step": 184540
    },
    {
      "epoch": 6.151666666666666,
      "grad_norm": 0.37667742371559143,
      "learning_rate": 1.1552083333333334e-05,
      "loss": 0.0015,
      "step": 184550
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.08665835112333298,
      "learning_rate": 1.1550000000000001e-05,
      "loss": 0.0017,
      "step": 184560
    },
    {
      "epoch": 6.152333333333333,
      "grad_norm": 0.14322417974472046,
      "learning_rate": 1.1547916666666667e-05,
      "loss": 0.0027,
      "step": 184570
    },
    {
      "epoch": 6.152666666666667,
      "grad_norm": 0.02993517555296421,
      "learning_rate": 1.1545833333333334e-05,
      "loss": 0.0018,
      "step": 184580
    },
    {
      "epoch": 6.153,
      "grad_norm": 0.25687071681022644,
      "learning_rate": 1.1543750000000001e-05,
      "loss": 0.0022,
      "step": 184590
    },
    {
      "epoch": 6.153333333333333,
      "grad_norm": 0.17249800264835358,
      "learning_rate": 1.1541666666666667e-05,
      "loss": 0.0016,
      "step": 184600
    },
    {
      "epoch": 6.153666666666667,
      "grad_norm": 0.14430782198905945,
      "learning_rate": 1.1539583333333334e-05,
      "loss": 0.0015,
      "step": 184610
    },
    {
      "epoch": 6.154,
      "grad_norm": 0.1729097217321396,
      "learning_rate": 1.1537500000000001e-05,
      "loss": 0.0014,
      "step": 184620
    },
    {
      "epoch": 6.154333333333334,
      "grad_norm": 0.26058295369148254,
      "learning_rate": 1.1535416666666666e-05,
      "loss": 0.0015,
      "step": 184630
    },
    {
      "epoch": 6.1546666666666665,
      "grad_norm": 0.0309279914945364,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0012,
      "step": 184640
    },
    {
      "epoch": 6.155,
      "grad_norm": 0.06234820932149887,
      "learning_rate": 1.153125e-05,
      "loss": 0.0031,
      "step": 184650
    },
    {
      "epoch": 6.155333333333333,
      "grad_norm": 0.1666175127029419,
      "learning_rate": 1.1529166666666668e-05,
      "loss": 0.0017,
      "step": 184660
    },
    {
      "epoch": 6.155666666666667,
      "grad_norm": 0.36495158076286316,
      "learning_rate": 1.1527083333333333e-05,
      "loss": 0.0023,
      "step": 184670
    },
    {
      "epoch": 6.156,
      "grad_norm": 0.2863536477088928,
      "learning_rate": 1.1525e-05,
      "loss": 0.0013,
      "step": 184680
    },
    {
      "epoch": 6.156333333333333,
      "grad_norm": 0.31423282623291016,
      "learning_rate": 1.1522916666666666e-05,
      "loss": 0.001,
      "step": 184690
    },
    {
      "epoch": 6.156666666666666,
      "grad_norm": 0.3422096073627472,
      "learning_rate": 1.1520833333333333e-05,
      "loss": 0.0023,
      "step": 184700
    },
    {
      "epoch": 6.157,
      "grad_norm": 0.1998109221458435,
      "learning_rate": 1.151875e-05,
      "loss": 0.0023,
      "step": 184710
    },
    {
      "epoch": 6.157333333333334,
      "grad_norm": 0.11449795961380005,
      "learning_rate": 1.1516666666666668e-05,
      "loss": 0.0014,
      "step": 184720
    },
    {
      "epoch": 6.157666666666667,
      "grad_norm": 0.10020051896572113,
      "learning_rate": 1.1514583333333335e-05,
      "loss": 0.0019,
      "step": 184730
    },
    {
      "epoch": 6.158,
      "grad_norm": 0.2295590192079544,
      "learning_rate": 1.15125e-05,
      "loss": 0.0013,
      "step": 184740
    },
    {
      "epoch": 6.158333333333333,
      "grad_norm": 0.34234315156936646,
      "learning_rate": 1.1510416666666666e-05,
      "loss": 0.002,
      "step": 184750
    },
    {
      "epoch": 6.158666666666667,
      "grad_norm": 0.28547200560569763,
      "learning_rate": 1.1508333333333333e-05,
      "loss": 0.0015,
      "step": 184760
    },
    {
      "epoch": 6.159,
      "grad_norm": 0.007511497009545565,
      "learning_rate": 1.150625e-05,
      "loss": 0.0014,
      "step": 184770
    },
    {
      "epoch": 6.1593333333333335,
      "grad_norm": 0.22860437631607056,
      "learning_rate": 1.1504166666666667e-05,
      "loss": 0.002,
      "step": 184780
    },
    {
      "epoch": 6.159666666666666,
      "grad_norm": 0.17141427099704742,
      "learning_rate": 1.1502083333333335e-05,
      "loss": 0.002,
      "step": 184790
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.14374594390392303,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0015,
      "step": 184800
    },
    {
      "epoch": 6.160333333333333,
      "grad_norm": 0.31395789980888367,
      "learning_rate": 1.1497916666666667e-05,
      "loss": 0.0024,
      "step": 184810
    },
    {
      "epoch": 6.160666666666667,
      "grad_norm": 0.057688597589731216,
      "learning_rate": 1.1495833333333333e-05,
      "loss": 0.0018,
      "step": 184820
    },
    {
      "epoch": 6.161,
      "grad_norm": 0.05824052169919014,
      "learning_rate": 1.149375e-05,
      "loss": 0.0015,
      "step": 184830
    },
    {
      "epoch": 6.161333333333333,
      "grad_norm": 0.2857764661312103,
      "learning_rate": 1.1491666666666667e-05,
      "loss": 0.002,
      "step": 184840
    },
    {
      "epoch": 6.161666666666667,
      "grad_norm": 0.14357440173625946,
      "learning_rate": 1.1489583333333334e-05,
      "loss": 0.0019,
      "step": 184850
    },
    {
      "epoch": 6.162,
      "grad_norm": 0.40112969279289246,
      "learning_rate": 1.1487500000000001e-05,
      "loss": 0.0013,
      "step": 184860
    },
    {
      "epoch": 6.162333333333334,
      "grad_norm": 0.1997961699962616,
      "learning_rate": 1.1485416666666667e-05,
      "loss": 0.0019,
      "step": 184870
    },
    {
      "epoch": 6.1626666666666665,
      "grad_norm": 0.014495722018182278,
      "learning_rate": 1.1483333333333334e-05,
      "loss": 0.0014,
      "step": 184880
    },
    {
      "epoch": 6.163,
      "grad_norm": 0.2569373548030853,
      "learning_rate": 1.148125e-05,
      "loss": 0.0016,
      "step": 184890
    },
    {
      "epoch": 6.163333333333333,
      "grad_norm": 0.08654782921075821,
      "learning_rate": 1.1479166666666667e-05,
      "loss": 0.0016,
      "step": 184900
    },
    {
      "epoch": 6.163666666666667,
      "grad_norm": 0.142683744430542,
      "learning_rate": 1.1477083333333334e-05,
      "loss": 0.0015,
      "step": 184910
    },
    {
      "epoch": 6.164,
      "grad_norm": 0.07074502110481262,
      "learning_rate": 1.1475000000000001e-05,
      "loss": 0.0024,
      "step": 184920
    },
    {
      "epoch": 6.164333333333333,
      "grad_norm": 0.28579312562942505,
      "learning_rate": 1.1472916666666667e-05,
      "loss": 0.0018,
      "step": 184930
    },
    {
      "epoch": 6.164666666666666,
      "grad_norm": 0.057482391595840454,
      "learning_rate": 1.1470833333333334e-05,
      "loss": 0.0017,
      "step": 184940
    },
    {
      "epoch": 6.165,
      "grad_norm": 0.12373969703912735,
      "learning_rate": 1.1468750000000001e-05,
      "loss": 0.0014,
      "step": 184950
    },
    {
      "epoch": 6.165333333333333,
      "grad_norm": 0.12749971449375153,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0021,
      "step": 184960
    },
    {
      "epoch": 6.165666666666667,
      "grad_norm": 0.4038597345352173,
      "learning_rate": 1.1464583333333334e-05,
      "loss": 0.0018,
      "step": 184970
    },
    {
      "epoch": 6.166,
      "grad_norm": 0.1438639909029007,
      "learning_rate": 1.14625e-05,
      "loss": 0.0017,
      "step": 184980
    },
    {
      "epoch": 6.166333333333333,
      "grad_norm": 0.3792182207107544,
      "learning_rate": 1.1460416666666666e-05,
      "loss": 0.0016,
      "step": 184990
    },
    {
      "epoch": 6.166666666666667,
      "grad_norm": 0.16862325370311737,
      "learning_rate": 1.1458333333333333e-05,
      "loss": 0.0017,
      "step": 185000
    },
    {
      "epoch": 6.167,
      "grad_norm": 0.05894193798303604,
      "learning_rate": 1.145625e-05,
      "loss": 0.0016,
      "step": 185010
    },
    {
      "epoch": 6.167333333333334,
      "grad_norm": 0.1427825540304184,
      "learning_rate": 1.1454166666666668e-05,
      "loss": 0.0023,
      "step": 185020
    },
    {
      "epoch": 6.167666666666666,
      "grad_norm": 0.1146017536520958,
      "learning_rate": 1.1452083333333333e-05,
      "loss": 0.0015,
      "step": 185030
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.030411269515752792,
      "learning_rate": 1.145e-05,
      "loss": 0.0015,
      "step": 185040
    },
    {
      "epoch": 6.168333333333333,
      "grad_norm": 0.17148098349571228,
      "learning_rate": 1.1447916666666668e-05,
      "loss": 0.0012,
      "step": 185050
    },
    {
      "epoch": 6.168666666666667,
      "grad_norm": 0.08638961613178253,
      "learning_rate": 1.1445833333333333e-05,
      "loss": 0.0014,
      "step": 185060
    },
    {
      "epoch": 6.169,
      "grad_norm": 0.1139381155371666,
      "learning_rate": 1.144375e-05,
      "loss": 0.0021,
      "step": 185070
    },
    {
      "epoch": 6.169333333333333,
      "grad_norm": 0.4447140395641327,
      "learning_rate": 1.1441666666666668e-05,
      "loss": 0.0015,
      "step": 185080
    },
    {
      "epoch": 6.169666666666667,
      "grad_norm": 0.25677451491355896,
      "learning_rate": 1.1439583333333335e-05,
      "loss": 0.0017,
      "step": 185090
    },
    {
      "epoch": 6.17,
      "grad_norm": 0.08648372441530228,
      "learning_rate": 1.14375e-05,
      "loss": 0.0013,
      "step": 185100
    },
    {
      "epoch": 6.170333333333334,
      "grad_norm": 0.17126725614070892,
      "learning_rate": 1.1435416666666667e-05,
      "loss": 0.0018,
      "step": 185110
    },
    {
      "epoch": 6.1706666666666665,
      "grad_norm": 0.08647044748067856,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.0012,
      "step": 185120
    },
    {
      "epoch": 6.171,
      "grad_norm": 0.22917522490024567,
      "learning_rate": 1.143125e-05,
      "loss": 0.0015,
      "step": 185130
    },
    {
      "epoch": 6.171333333333333,
      "grad_norm": 0.2853673994541168,
      "learning_rate": 1.1429166666666667e-05,
      "loss": 0.0016,
      "step": 185140
    },
    {
      "epoch": 6.171666666666667,
      "grad_norm": 0.11452370136976242,
      "learning_rate": 1.1427083333333334e-05,
      "loss": 0.002,
      "step": 185150
    },
    {
      "epoch": 6.172,
      "grad_norm": 0.19256648421287537,
      "learning_rate": 1.1425000000000002e-05,
      "loss": 0.0015,
      "step": 185160
    },
    {
      "epoch": 6.1723333333333334,
      "grad_norm": 0.3158870339393616,
      "learning_rate": 1.1422916666666667e-05,
      "loss": 0.0017,
      "step": 185170
    },
    {
      "epoch": 6.172666666666666,
      "grad_norm": 0.11432729661464691,
      "learning_rate": 1.1420833333333333e-05,
      "loss": 0.0013,
      "step": 185180
    },
    {
      "epoch": 6.173,
      "grad_norm": 0.11441696435213089,
      "learning_rate": 1.141875e-05,
      "loss": 0.0018,
      "step": 185190
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 0.03051075153052807,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.0015,
      "step": 185200
    },
    {
      "epoch": 6.173666666666667,
      "grad_norm": 0.4658692479133606,
      "learning_rate": 1.1414583333333334e-05,
      "loss": 0.0014,
      "step": 185210
    },
    {
      "epoch": 6.174,
      "grad_norm": 0.029417967423796654,
      "learning_rate": 1.1412500000000001e-05,
      "loss": 0.0013,
      "step": 185220
    },
    {
      "epoch": 6.174333333333333,
      "grad_norm": 0.08691662549972534,
      "learning_rate": 1.1410416666666668e-05,
      "loss": 0.0012,
      "step": 185230
    },
    {
      "epoch": 6.174666666666667,
      "grad_norm": 0.1422576904296875,
      "learning_rate": 1.1408333333333334e-05,
      "loss": 0.0013,
      "step": 185240
    },
    {
      "epoch": 6.175,
      "grad_norm": 0.20007658004760742,
      "learning_rate": 1.140625e-05,
      "loss": 0.0013,
      "step": 185250
    },
    {
      "epoch": 6.175333333333334,
      "grad_norm": 0.2285175323486328,
      "learning_rate": 1.1404166666666667e-05,
      "loss": 0.0014,
      "step": 185260
    },
    {
      "epoch": 6.175666666666666,
      "grad_norm": 0.48530521988868713,
      "learning_rate": 1.1402083333333334e-05,
      "loss": 0.0015,
      "step": 185270
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.3140658140182495,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.002,
      "step": 185280
    },
    {
      "epoch": 6.176333333333333,
      "grad_norm": 0.19975952804088593,
      "learning_rate": 1.1397916666666668e-05,
      "loss": 0.0024,
      "step": 185290
    },
    {
      "epoch": 6.176666666666667,
      "grad_norm": 0.08552765101194382,
      "learning_rate": 1.1395833333333334e-05,
      "loss": 0.0028,
      "step": 185300
    },
    {
      "epoch": 6.177,
      "grad_norm": 0.11736161261796951,
      "learning_rate": 1.139375e-05,
      "loss": 0.0031,
      "step": 185310
    },
    {
      "epoch": 6.177333333333333,
      "grad_norm": 0.11635483056306839,
      "learning_rate": 1.1391666666666668e-05,
      "loss": 0.0017,
      "step": 185320
    },
    {
      "epoch": 6.177666666666667,
      "grad_norm": 0.4571882486343384,
      "learning_rate": 1.1389583333333333e-05,
      "loss": 0.0013,
      "step": 185330
    },
    {
      "epoch": 6.178,
      "grad_norm": 0.05815288424491882,
      "learning_rate": 1.13875e-05,
      "loss": 0.0022,
      "step": 185340
    },
    {
      "epoch": 6.178333333333334,
      "grad_norm": 0.05838742107152939,
      "learning_rate": 1.1385416666666668e-05,
      "loss": 0.002,
      "step": 185350
    },
    {
      "epoch": 6.1786666666666665,
      "grad_norm": 0.11780290305614471,
      "learning_rate": 1.1383333333333333e-05,
      "loss": 0.0013,
      "step": 185360
    },
    {
      "epoch": 6.179,
      "grad_norm": 0.03000028431415558,
      "learning_rate": 1.138125e-05,
      "loss": 0.0014,
      "step": 185370
    },
    {
      "epoch": 6.179333333333333,
      "grad_norm": 0.5668731331825256,
      "learning_rate": 1.1379166666666668e-05,
      "loss": 0.0015,
      "step": 185380
    },
    {
      "epoch": 6.179666666666667,
      "grad_norm": 0.20028378069400787,
      "learning_rate": 1.1377083333333335e-05,
      "loss": 0.0023,
      "step": 185390
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.03055673837661743,
      "learning_rate": 1.1375e-05,
      "loss": 0.0018,
      "step": 185400
    },
    {
      "epoch": 6.1803333333333335,
      "grad_norm": 0.05863317474722862,
      "learning_rate": 1.1372916666666668e-05,
      "loss": 0.0016,
      "step": 185410
    },
    {
      "epoch": 6.180666666666666,
      "grad_norm": 0.20573754608631134,
      "learning_rate": 1.1370833333333333e-05,
      "loss": 0.0028,
      "step": 185420
    },
    {
      "epoch": 6.181,
      "grad_norm": 0.0591626912355423,
      "learning_rate": 1.136875e-05,
      "loss": 0.0019,
      "step": 185430
    },
    {
      "epoch": 6.181333333333333,
      "grad_norm": 0.22837725281715393,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0021,
      "step": 185440
    },
    {
      "epoch": 6.181666666666667,
      "grad_norm": 0.3427523672580719,
      "learning_rate": 1.1364583333333335e-05,
      "loss": 0.0022,
      "step": 185450
    },
    {
      "epoch": 6.182,
      "grad_norm": 0.26179784536361694,
      "learning_rate": 1.1362500000000002e-05,
      "loss": 0.0014,
      "step": 185460
    },
    {
      "epoch": 6.182333333333333,
      "grad_norm": 0.03134658560156822,
      "learning_rate": 1.1360416666666667e-05,
      "loss": 0.0015,
      "step": 185470
    },
    {
      "epoch": 6.182666666666667,
      "grad_norm": 0.25915536284446716,
      "learning_rate": 1.1358333333333333e-05,
      "loss": 0.0015,
      "step": 185480
    },
    {
      "epoch": 6.183,
      "grad_norm": 0.11483532935380936,
      "learning_rate": 1.135625e-05,
      "loss": 0.0015,
      "step": 185490
    },
    {
      "epoch": 6.183333333333334,
      "grad_norm": 0.060376573354005814,
      "learning_rate": 1.1354166666666667e-05,
      "loss": 0.0017,
      "step": 185500
    },
    {
      "epoch": 6.183666666666666,
      "grad_norm": 0.0314549058675766,
      "learning_rate": 1.1352083333333334e-05,
      "loss": 0.0014,
      "step": 185510
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.28026238083839417,
      "learning_rate": 1.1350000000000001e-05,
      "loss": 0.0018,
      "step": 185520
    },
    {
      "epoch": 6.184333333333333,
      "grad_norm": 0.08622799813747406,
      "learning_rate": 1.1347916666666669e-05,
      "loss": 0.0027,
      "step": 185530
    },
    {
      "epoch": 6.184666666666667,
      "grad_norm": 0.17132270336151123,
      "learning_rate": 1.1345833333333334e-05,
      "loss": 0.0016,
      "step": 185540
    },
    {
      "epoch": 6.185,
      "grad_norm": 0.1438039094209671,
      "learning_rate": 1.134375e-05,
      "loss": 0.0016,
      "step": 185550
    },
    {
      "epoch": 6.185333333333333,
      "grad_norm": 0.011897719465196133,
      "learning_rate": 1.1341666666666667e-05,
      "loss": 0.0021,
      "step": 185560
    },
    {
      "epoch": 6.185666666666667,
      "grad_norm": 0.08626679331064224,
      "learning_rate": 1.1339583333333334e-05,
      "loss": 0.0019,
      "step": 185570
    },
    {
      "epoch": 6.186,
      "grad_norm": 0.032903313636779785,
      "learning_rate": 1.1337500000000001e-05,
      "loss": 0.0015,
      "step": 185580
    },
    {
      "epoch": 6.186333333333334,
      "grad_norm": 0.07482189685106277,
      "learning_rate": 1.1335416666666668e-05,
      "loss": 0.0014,
      "step": 185590
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.12411309033632278,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0014,
      "step": 185600
    },
    {
      "epoch": 6.187,
      "grad_norm": 0.029450833797454834,
      "learning_rate": 1.1331250000000001e-05,
      "loss": 0.0015,
      "step": 185610
    },
    {
      "epoch": 6.187333333333333,
      "grad_norm": 0.010928690433502197,
      "learning_rate": 1.1329166666666666e-05,
      "loss": 0.0023,
      "step": 185620
    },
    {
      "epoch": 6.187666666666667,
      "grad_norm": 0.314085453748703,
      "learning_rate": 1.1327083333333334e-05,
      "loss": 0.0013,
      "step": 185630
    },
    {
      "epoch": 6.188,
      "grad_norm": 0.05827837809920311,
      "learning_rate": 1.1325e-05,
      "loss": 0.0017,
      "step": 185640
    },
    {
      "epoch": 6.1883333333333335,
      "grad_norm": 0.25621846318244934,
      "learning_rate": 1.1322916666666668e-05,
      "loss": 0.0021,
      "step": 185650
    },
    {
      "epoch": 6.188666666666666,
      "grad_norm": 0.24312005937099457,
      "learning_rate": 1.1320833333333334e-05,
      "loss": 0.0011,
      "step": 185660
    },
    {
      "epoch": 6.189,
      "grad_norm": 0.23681171238422394,
      "learning_rate": 1.131875e-05,
      "loss": 0.0012,
      "step": 185670
    },
    {
      "epoch": 6.189333333333333,
      "grad_norm": 0.3024252951145172,
      "learning_rate": 1.1316666666666668e-05,
      "loss": 0.003,
      "step": 185680
    },
    {
      "epoch": 6.189666666666667,
      "grad_norm": 0.17129357159137726,
      "learning_rate": 1.1314583333333333e-05,
      "loss": 0.0018,
      "step": 185690
    },
    {
      "epoch": 6.19,
      "grad_norm": 0.08699256181716919,
      "learning_rate": 1.13125e-05,
      "loss": 0.0019,
      "step": 185700
    },
    {
      "epoch": 6.190333333333333,
      "grad_norm": 0.02980731427669525,
      "learning_rate": 1.1310416666666668e-05,
      "loss": 0.0021,
      "step": 185710
    },
    {
      "epoch": 6.190666666666667,
      "grad_norm": 0.3986093997955322,
      "learning_rate": 1.1308333333333333e-05,
      "loss": 0.0014,
      "step": 185720
    },
    {
      "epoch": 6.191,
      "grad_norm": 0.057778116315603256,
      "learning_rate": 1.130625e-05,
      "loss": 0.0026,
      "step": 185730
    },
    {
      "epoch": 6.191333333333334,
      "grad_norm": 0.19949601590633392,
      "learning_rate": 1.1304166666666668e-05,
      "loss": 0.0015,
      "step": 185740
    },
    {
      "epoch": 6.191666666666666,
      "grad_norm": 0.3993861675262451,
      "learning_rate": 1.1302083333333335e-05,
      "loss": 0.0021,
      "step": 185750
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.22883819043636322,
      "learning_rate": 1.13e-05,
      "loss": 0.0022,
      "step": 185760
    },
    {
      "epoch": 6.192333333333333,
      "grad_norm": 0.4143821597099304,
      "learning_rate": 1.1297916666666667e-05,
      "loss": 0.0019,
      "step": 185770
    },
    {
      "epoch": 6.192666666666667,
      "grad_norm": 0.057726647704839706,
      "learning_rate": 1.1295833333333333e-05,
      "loss": 0.0017,
      "step": 185780
    },
    {
      "epoch": 6.193,
      "grad_norm": 0.08654855191707611,
      "learning_rate": 1.129375e-05,
      "loss": 0.0014,
      "step": 185790
    },
    {
      "epoch": 6.193333333333333,
      "grad_norm": 0.4561270773410797,
      "learning_rate": 1.1291666666666667e-05,
      "loss": 0.0019,
      "step": 185800
    },
    {
      "epoch": 6.193666666666667,
      "grad_norm": 0.11452747136354446,
      "learning_rate": 1.1289583333333334e-05,
      "loss": 0.0021,
      "step": 185810
    },
    {
      "epoch": 6.194,
      "grad_norm": 0.2851986587047577,
      "learning_rate": 1.1287500000000002e-05,
      "loss": 0.0013,
      "step": 185820
    },
    {
      "epoch": 6.194333333333334,
      "grad_norm": 0.0582660548388958,
      "learning_rate": 1.1285416666666667e-05,
      "loss": 0.0022,
      "step": 185830
    },
    {
      "epoch": 6.1946666666666665,
      "grad_norm": 0.03948139771819115,
      "learning_rate": 1.1283333333333333e-05,
      "loss": 0.0017,
      "step": 185840
    },
    {
      "epoch": 6.195,
      "grad_norm": 0.3711260259151459,
      "learning_rate": 1.128125e-05,
      "loss": 0.0017,
      "step": 185850
    },
    {
      "epoch": 6.195333333333333,
      "grad_norm": 0.14258596301078796,
      "learning_rate": 1.1279166666666667e-05,
      "loss": 0.0019,
      "step": 185860
    },
    {
      "epoch": 6.195666666666667,
      "grad_norm": 0.5141677260398865,
      "learning_rate": 1.1277083333333334e-05,
      "loss": 0.0022,
      "step": 185870
    },
    {
      "epoch": 6.196,
      "grad_norm": 0.06391054391860962,
      "learning_rate": 1.1275000000000001e-05,
      "loss": 0.0022,
      "step": 185880
    },
    {
      "epoch": 6.1963333333333335,
      "grad_norm": 0.4654475748538971,
      "learning_rate": 1.1272916666666668e-05,
      "loss": 0.0015,
      "step": 185890
    },
    {
      "epoch": 6.196666666666666,
      "grad_norm": 0.22869446873664856,
      "learning_rate": 1.1270833333333334e-05,
      "loss": 0.0016,
      "step": 185900
    },
    {
      "epoch": 6.197,
      "grad_norm": 0.1429414004087448,
      "learning_rate": 1.126875e-05,
      "loss": 0.0013,
      "step": 185910
    },
    {
      "epoch": 6.197333333333333,
      "grad_norm": 0.010143866762518883,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0017,
      "step": 185920
    },
    {
      "epoch": 6.197666666666667,
      "grad_norm": 0.11443481594324112,
      "learning_rate": 1.1264583333333334e-05,
      "loss": 0.0019,
      "step": 185930
    },
    {
      "epoch": 6.198,
      "grad_norm": 0.2003055214881897,
      "learning_rate": 1.1262500000000001e-05,
      "loss": 0.0013,
      "step": 185940
    },
    {
      "epoch": 6.198333333333333,
      "grad_norm": 0.09953153878450394,
      "learning_rate": 1.1260416666666668e-05,
      "loss": 0.0023,
      "step": 185950
    },
    {
      "epoch": 6.198666666666667,
      "grad_norm": 0.2856085002422333,
      "learning_rate": 1.1258333333333334e-05,
      "loss": 0.0013,
      "step": 185960
    },
    {
      "epoch": 6.199,
      "grad_norm": 0.08656936138868332,
      "learning_rate": 1.1256250000000001e-05,
      "loss": 0.0016,
      "step": 185970
    },
    {
      "epoch": 6.199333333333334,
      "grad_norm": 0.14356479048728943,
      "learning_rate": 1.1254166666666666e-05,
      "loss": 0.0018,
      "step": 185980
    },
    {
      "epoch": 6.199666666666666,
      "grad_norm": 0.22881969809532166,
      "learning_rate": 1.1252083333333334e-05,
      "loss": 0.0017,
      "step": 185990
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.02959863469004631,
      "learning_rate": 1.125e-05,
      "loss": 0.0019,
      "step": 186000
    },
    {
      "epoch": 6.200333333333333,
      "grad_norm": 0.03002707101404667,
      "learning_rate": 1.1247916666666668e-05,
      "loss": 0.0013,
      "step": 186010
    },
    {
      "epoch": 6.200666666666667,
      "grad_norm": 0.029997749254107475,
      "learning_rate": 1.1245833333333333e-05,
      "loss": 0.0015,
      "step": 186020
    },
    {
      "epoch": 6.201,
      "grad_norm": 0.5175445675849915,
      "learning_rate": 1.124375e-05,
      "loss": 0.0015,
      "step": 186030
    },
    {
      "epoch": 6.201333333333333,
      "grad_norm": 0.2861155867576599,
      "learning_rate": 1.1241666666666668e-05,
      "loss": 0.0012,
      "step": 186040
    },
    {
      "epoch": 6.201666666666666,
      "grad_norm": 0.03409837931394577,
      "learning_rate": 1.1239583333333333e-05,
      "loss": 0.0022,
      "step": 186050
    },
    {
      "epoch": 6.202,
      "grad_norm": 0.20893292129039764,
      "learning_rate": 1.12375e-05,
      "loss": 0.002,
      "step": 186060
    },
    {
      "epoch": 6.202333333333334,
      "grad_norm": 0.010106737725436687,
      "learning_rate": 1.1235416666666668e-05,
      "loss": 0.0025,
      "step": 186070
    },
    {
      "epoch": 6.2026666666666666,
      "grad_norm": 0.28649380803108215,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0018,
      "step": 186080
    },
    {
      "epoch": 6.203,
      "grad_norm": 0.2567613422870636,
      "learning_rate": 1.123125e-05,
      "loss": 0.002,
      "step": 186090
    },
    {
      "epoch": 6.203333333333333,
      "grad_norm": 0.031033361330628395,
      "learning_rate": 1.1229166666666667e-05,
      "loss": 0.0017,
      "step": 186100
    },
    {
      "epoch": 6.203666666666667,
      "grad_norm": 0.14323490858078003,
      "learning_rate": 1.1227083333333335e-05,
      "loss": 0.0013,
      "step": 186110
    },
    {
      "epoch": 6.204,
      "grad_norm": 0.05781329795718193,
      "learning_rate": 1.1225e-05,
      "loss": 0.0016,
      "step": 186120
    },
    {
      "epoch": 6.2043333333333335,
      "grad_norm": 0.3331519663333893,
      "learning_rate": 1.1222916666666667e-05,
      "loss": 0.0013,
      "step": 186130
    },
    {
      "epoch": 6.204666666666666,
      "grad_norm": 0.3467285633087158,
      "learning_rate": 1.1220833333333333e-05,
      "loss": 0.0021,
      "step": 186140
    },
    {
      "epoch": 6.205,
      "grad_norm": 0.2277865707874298,
      "learning_rate": 1.121875e-05,
      "loss": 0.0021,
      "step": 186150
    },
    {
      "epoch": 6.205333333333333,
      "grad_norm": 0.25656041502952576,
      "learning_rate": 1.1216666666666667e-05,
      "loss": 0.0019,
      "step": 186160
    },
    {
      "epoch": 6.205666666666667,
      "grad_norm": 0.22879156470298767,
      "learning_rate": 1.1214583333333334e-05,
      "loss": 0.0017,
      "step": 186170
    },
    {
      "epoch": 6.206,
      "grad_norm": 0.4177052080631256,
      "learning_rate": 1.1212500000000001e-05,
      "loss": 0.0019,
      "step": 186180
    },
    {
      "epoch": 6.206333333333333,
      "grad_norm": 0.033112380653619766,
      "learning_rate": 1.1210416666666667e-05,
      "loss": 0.0018,
      "step": 186190
    },
    {
      "epoch": 6.206666666666667,
      "grad_norm": 0.11520641297101974,
      "learning_rate": 1.1208333333333332e-05,
      "loss": 0.0021,
      "step": 186200
    },
    {
      "epoch": 6.207,
      "grad_norm": 0.20659887790679932,
      "learning_rate": 1.120625e-05,
      "loss": 0.0019,
      "step": 186210
    },
    {
      "epoch": 6.207333333333334,
      "grad_norm": 0.1429523527622223,
      "learning_rate": 1.1204166666666667e-05,
      "loss": 0.0018,
      "step": 186220
    },
    {
      "epoch": 6.207666666666666,
      "grad_norm": 0.08570881932973862,
      "learning_rate": 1.1202083333333334e-05,
      "loss": 0.0014,
      "step": 186230
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.5517096519470215,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0021,
      "step": 186240
    },
    {
      "epoch": 6.208333333333333,
      "grad_norm": 0.42864546179771423,
      "learning_rate": 1.1197916666666668e-05,
      "loss": 0.0021,
      "step": 186250
    },
    {
      "epoch": 6.208666666666667,
      "grad_norm": 0.2860944867134094,
      "learning_rate": 1.1195833333333334e-05,
      "loss": 0.0016,
      "step": 186260
    },
    {
      "epoch": 6.209,
      "grad_norm": 0.01255893800407648,
      "learning_rate": 1.119375e-05,
      "loss": 0.0013,
      "step": 186270
    },
    {
      "epoch": 6.209333333333333,
      "grad_norm": 0.14240898191928864,
      "learning_rate": 1.1191666666666667e-05,
      "loss": 0.0019,
      "step": 186280
    },
    {
      "epoch": 6.209666666666667,
      "grad_norm": 0.3711288869380951,
      "learning_rate": 1.1189583333333334e-05,
      "loss": 0.002,
      "step": 186290
    },
    {
      "epoch": 6.21,
      "grad_norm": 0.14335885643959045,
      "learning_rate": 1.1187500000000001e-05,
      "loss": 0.0018,
      "step": 186300
    },
    {
      "epoch": 6.210333333333334,
      "grad_norm": 0.05728045105934143,
      "learning_rate": 1.1185416666666668e-05,
      "loss": 0.0018,
      "step": 186310
    },
    {
      "epoch": 6.210666666666667,
      "grad_norm": 0.1468806266784668,
      "learning_rate": 1.1183333333333335e-05,
      "loss": 0.0019,
      "step": 186320
    },
    {
      "epoch": 6.211,
      "grad_norm": 0.05881394073367119,
      "learning_rate": 1.118125e-05,
      "loss": 0.0018,
      "step": 186330
    },
    {
      "epoch": 6.211333333333333,
      "grad_norm": 0.28507882356643677,
      "learning_rate": 1.1179166666666666e-05,
      "loss": 0.0014,
      "step": 186340
    },
    {
      "epoch": 6.211666666666667,
      "grad_norm": 0.029844047501683235,
      "learning_rate": 1.1177083333333333e-05,
      "loss": 0.0021,
      "step": 186350
    },
    {
      "epoch": 6.212,
      "grad_norm": 0.1998317390680313,
      "learning_rate": 1.1175e-05,
      "loss": 0.0015,
      "step": 186360
    },
    {
      "epoch": 6.2123333333333335,
      "grad_norm": 0.5080474615097046,
      "learning_rate": 1.1172916666666668e-05,
      "loss": 0.003,
      "step": 186370
    },
    {
      "epoch": 6.212666666666666,
      "grad_norm": 0.02940802089869976,
      "learning_rate": 1.1170833333333335e-05,
      "loss": 0.0019,
      "step": 186380
    },
    {
      "epoch": 6.213,
      "grad_norm": 0.31868356466293335,
      "learning_rate": 1.116875e-05,
      "loss": 0.0019,
      "step": 186390
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.17170946300029755,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0018,
      "step": 186400
    },
    {
      "epoch": 6.213666666666667,
      "grad_norm": 0.03567264974117279,
      "learning_rate": 1.1164583333333333e-05,
      "loss": 0.0025,
      "step": 186410
    },
    {
      "epoch": 6.214,
      "grad_norm": 0.38855451345443726,
      "learning_rate": 1.11625e-05,
      "loss": 0.0018,
      "step": 186420
    },
    {
      "epoch": 6.214333333333333,
      "grad_norm": 0.05753237381577492,
      "learning_rate": 1.1160416666666667e-05,
      "loss": 0.002,
      "step": 186430
    },
    {
      "epoch": 6.214666666666667,
      "grad_norm": 0.08602951467037201,
      "learning_rate": 1.1158333333333335e-05,
      "loss": 0.0015,
      "step": 186440
    },
    {
      "epoch": 6.215,
      "grad_norm": 0.029295766726136208,
      "learning_rate": 1.115625e-05,
      "loss": 0.0027,
      "step": 186450
    },
    {
      "epoch": 6.215333333333334,
      "grad_norm": 0.14324814081192017,
      "learning_rate": 1.1154166666666667e-05,
      "loss": 0.002,
      "step": 186460
    },
    {
      "epoch": 6.2156666666666665,
      "grad_norm": 0.02930426597595215,
      "learning_rate": 1.1152083333333334e-05,
      "loss": 0.0017,
      "step": 186470
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.485061913728714,
      "learning_rate": 1.115e-05,
      "loss": 0.0019,
      "step": 186480
    },
    {
      "epoch": 6.216333333333333,
      "grad_norm": 0.14352333545684814,
      "learning_rate": 1.1147916666666667e-05,
      "loss": 0.0019,
      "step": 186490
    },
    {
      "epoch": 6.216666666666667,
      "grad_norm": 0.31421464681625366,
      "learning_rate": 1.1145833333333334e-05,
      "loss": 0.0018,
      "step": 186500
    },
    {
      "epoch": 6.217,
      "grad_norm": 0.1718030869960785,
      "learning_rate": 1.114375e-05,
      "loss": 0.0019,
      "step": 186510
    },
    {
      "epoch": 6.217333333333333,
      "grad_norm": 0.011948997154831886,
      "learning_rate": 1.1141666666666667e-05,
      "loss": 0.0013,
      "step": 186520
    },
    {
      "epoch": 6.217666666666666,
      "grad_norm": 0.05790092423558235,
      "learning_rate": 1.1139583333333334e-05,
      "loss": 0.0024,
      "step": 186530
    },
    {
      "epoch": 6.218,
      "grad_norm": 0.14575940370559692,
      "learning_rate": 1.1137500000000001e-05,
      "loss": 0.0023,
      "step": 186540
    },
    {
      "epoch": 6.218333333333334,
      "grad_norm": 0.17154516279697418,
      "learning_rate": 1.1135416666666667e-05,
      "loss": 0.0021,
      "step": 186550
    },
    {
      "epoch": 6.218666666666667,
      "grad_norm": 0.05768890306353569,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0017,
      "step": 186560
    },
    {
      "epoch": 6.219,
      "grad_norm": 0.05774063244462013,
      "learning_rate": 1.113125e-05,
      "loss": 0.0016,
      "step": 186570
    },
    {
      "epoch": 6.219333333333333,
      "grad_norm": 0.0314149409532547,
      "learning_rate": 1.1129166666666667e-05,
      "loss": 0.0013,
      "step": 186580
    },
    {
      "epoch": 6.219666666666667,
      "grad_norm": 0.3992340564727783,
      "learning_rate": 1.1127083333333334e-05,
      "loss": 0.0018,
      "step": 186590
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.05764707922935486,
      "learning_rate": 1.1125000000000001e-05,
      "loss": 0.0014,
      "step": 186600
    },
    {
      "epoch": 6.2203333333333335,
      "grad_norm": 0.05778849497437477,
      "learning_rate": 1.1122916666666668e-05,
      "loss": 0.0016,
      "step": 186610
    },
    {
      "epoch": 6.220666666666666,
      "grad_norm": 0.3140449821949005,
      "learning_rate": 1.1120833333333334e-05,
      "loss": 0.0015,
      "step": 186620
    },
    {
      "epoch": 6.221,
      "grad_norm": 0.232926145195961,
      "learning_rate": 1.111875e-05,
      "loss": 0.0017,
      "step": 186630
    },
    {
      "epoch": 6.221333333333333,
      "grad_norm": 0.08685887604951859,
      "learning_rate": 1.1116666666666666e-05,
      "loss": 0.0021,
      "step": 186640
    },
    {
      "epoch": 6.221666666666667,
      "grad_norm": 0.1719878911972046,
      "learning_rate": 1.1114583333333334e-05,
      "loss": 0.0015,
      "step": 186650
    },
    {
      "epoch": 6.222,
      "grad_norm": 0.3147086799144745,
      "learning_rate": 1.11125e-05,
      "loss": 0.0018,
      "step": 186660
    },
    {
      "epoch": 6.222333333333333,
      "grad_norm": 0.032288286834955215,
      "learning_rate": 1.1110416666666668e-05,
      "loss": 0.0014,
      "step": 186670
    },
    {
      "epoch": 6.222666666666667,
      "grad_norm": 0.02976476214826107,
      "learning_rate": 1.1108333333333335e-05,
      "loss": 0.0018,
      "step": 186680
    },
    {
      "epoch": 6.223,
      "grad_norm": 0.20060577988624573,
      "learning_rate": 1.110625e-05,
      "loss": 0.0011,
      "step": 186690
    },
    {
      "epoch": 6.223333333333334,
      "grad_norm": 0.336854487657547,
      "learning_rate": 1.1104166666666666e-05,
      "loss": 0.0016,
      "step": 186700
    },
    {
      "epoch": 6.2236666666666665,
      "grad_norm": 0.11425349116325378,
      "learning_rate": 1.1102083333333333e-05,
      "loss": 0.0026,
      "step": 186710
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.11462143063545227,
      "learning_rate": 1.11e-05,
      "loss": 0.0014,
      "step": 186720
    },
    {
      "epoch": 6.224333333333333,
      "grad_norm": 0.11468742787837982,
      "learning_rate": 1.1097916666666668e-05,
      "loss": 0.0026,
      "step": 186730
    },
    {
      "epoch": 6.224666666666667,
      "grad_norm": 0.2989244759082794,
      "learning_rate": 1.1095833333333335e-05,
      "loss": 0.0016,
      "step": 186740
    },
    {
      "epoch": 6.225,
      "grad_norm": 0.060544658452272415,
      "learning_rate": 1.109375e-05,
      "loss": 0.0018,
      "step": 186750
    },
    {
      "epoch": 6.225333333333333,
      "grad_norm": 0.08619239926338196,
      "learning_rate": 1.1091666666666667e-05,
      "loss": 0.0015,
      "step": 186760
    },
    {
      "epoch": 6.225666666666666,
      "grad_norm": 0.012355326674878597,
      "learning_rate": 1.1089583333333333e-05,
      "loss": 0.0018,
      "step": 186770
    },
    {
      "epoch": 6.226,
      "grad_norm": 0.25728273391723633,
      "learning_rate": 1.10875e-05,
      "loss": 0.0022,
      "step": 186780
    },
    {
      "epoch": 6.226333333333334,
      "grad_norm": 0.384804904460907,
      "learning_rate": 1.1085416666666667e-05,
      "loss": 0.0016,
      "step": 186790
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 0.2525613605976105,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.0015,
      "step": 186800
    },
    {
      "epoch": 6.227,
      "grad_norm": 0.08603695780038834,
      "learning_rate": 1.108125e-05,
      "loss": 0.0022,
      "step": 186810
    },
    {
      "epoch": 6.227333333333333,
      "grad_norm": 0.2939327359199524,
      "learning_rate": 1.1079166666666667e-05,
      "loss": 0.0017,
      "step": 186820
    },
    {
      "epoch": 6.227666666666667,
      "grad_norm": 0.08621629327535629,
      "learning_rate": 1.1077083333333334e-05,
      "loss": 0.0014,
      "step": 186830
    },
    {
      "epoch": 6.228,
      "grad_norm": 0.2010706514120102,
      "learning_rate": 1.1075e-05,
      "loss": 0.0018,
      "step": 186840
    },
    {
      "epoch": 6.2283333333333335,
      "grad_norm": 0.20082685351371765,
      "learning_rate": 1.1072916666666667e-05,
      "loss": 0.0017,
      "step": 186850
    },
    {
      "epoch": 6.228666666666666,
      "grad_norm": 0.3141310214996338,
      "learning_rate": 1.1070833333333334e-05,
      "loss": 0.0015,
      "step": 186860
    },
    {
      "epoch": 6.229,
      "grad_norm": 0.09224577248096466,
      "learning_rate": 1.106875e-05,
      "loss": 0.0022,
      "step": 186870
    },
    {
      "epoch": 6.229333333333333,
      "grad_norm": 0.22867877781391144,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0018,
      "step": 186880
    },
    {
      "epoch": 6.229666666666667,
      "grad_norm": 0.14255252480506897,
      "learning_rate": 1.1064583333333334e-05,
      "loss": 0.0018,
      "step": 186890
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.05794845148921013,
      "learning_rate": 1.1062500000000001e-05,
      "loss": 0.0017,
      "step": 186900
    },
    {
      "epoch": 6.230333333333333,
      "grad_norm": 0.05815752595663071,
      "learning_rate": 1.1060416666666667e-05,
      "loss": 0.0018,
      "step": 186910
    },
    {
      "epoch": 6.230666666666667,
      "grad_norm": 0.286332905292511,
      "learning_rate": 1.1058333333333334e-05,
      "loss": 0.0013,
      "step": 186920
    },
    {
      "epoch": 6.231,
      "grad_norm": 0.33676326274871826,
      "learning_rate": 1.105625e-05,
      "loss": 0.0014,
      "step": 186930
    },
    {
      "epoch": 6.231333333333334,
      "grad_norm": 0.05814272165298462,
      "learning_rate": 1.1054166666666667e-05,
      "loss": 0.0019,
      "step": 186940
    },
    {
      "epoch": 6.2316666666666665,
      "grad_norm": 0.08665736019611359,
      "learning_rate": 1.1052083333333334e-05,
      "loss": 0.0011,
      "step": 186950
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.03099844790995121,
      "learning_rate": 1.1050000000000001e-05,
      "loss": 0.0023,
      "step": 186960
    },
    {
      "epoch": 6.232333333333333,
      "grad_norm": 0.2861958146095276,
      "learning_rate": 1.1047916666666668e-05,
      "loss": 0.0015,
      "step": 186970
    },
    {
      "epoch": 6.232666666666667,
      "grad_norm": 0.1712300181388855,
      "learning_rate": 1.1045833333333334e-05,
      "loss": 0.0019,
      "step": 186980
    },
    {
      "epoch": 6.233,
      "grad_norm": 0.14370110630989075,
      "learning_rate": 1.1043749999999999e-05,
      "loss": 0.0017,
      "step": 186990
    },
    {
      "epoch": 6.233333333333333,
      "grad_norm": 0.08612415939569473,
      "learning_rate": 1.1041666666666666e-05,
      "loss": 0.0013,
      "step": 187000
    },
    {
      "epoch": 6.233666666666666,
      "grad_norm": 0.23314912617206573,
      "learning_rate": 1.1039583333333333e-05,
      "loss": 0.0025,
      "step": 187010
    },
    {
      "epoch": 6.234,
      "grad_norm": 0.25669148564338684,
      "learning_rate": 1.10375e-05,
      "loss": 0.0017,
      "step": 187020
    },
    {
      "epoch": 6.234333333333334,
      "grad_norm": 0.2029762864112854,
      "learning_rate": 1.1035416666666668e-05,
      "loss": 0.0016,
      "step": 187030
    },
    {
      "epoch": 6.234666666666667,
      "grad_norm": 0.22833460569381714,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.0013,
      "step": 187040
    },
    {
      "epoch": 6.235,
      "grad_norm": 0.15907646715641022,
      "learning_rate": 1.103125e-05,
      "loss": 0.0025,
      "step": 187050
    },
    {
      "epoch": 6.235333333333333,
      "grad_norm": 0.18747787177562714,
      "learning_rate": 1.1029166666666668e-05,
      "loss": 0.002,
      "step": 187060
    },
    {
      "epoch": 6.235666666666667,
      "grad_norm": 0.14291825890541077,
      "learning_rate": 1.1027083333333333e-05,
      "loss": 0.0018,
      "step": 187070
    },
    {
      "epoch": 6.236,
      "grad_norm": 0.029076728969812393,
      "learning_rate": 1.1025e-05,
      "loss": 0.0017,
      "step": 187080
    },
    {
      "epoch": 6.2363333333333335,
      "grad_norm": 0.22822782397270203,
      "learning_rate": 1.1022916666666668e-05,
      "loss": 0.0022,
      "step": 187090
    },
    {
      "epoch": 6.236666666666666,
      "grad_norm": 0.09366872161626816,
      "learning_rate": 1.1020833333333335e-05,
      "loss": 0.0016,
      "step": 187100
    },
    {
      "epoch": 6.237,
      "grad_norm": 0.08590025454759598,
      "learning_rate": 1.101875e-05,
      "loss": 0.0023,
      "step": 187110
    },
    {
      "epoch": 6.237333333333333,
      "grad_norm": 0.17153240740299225,
      "learning_rate": 1.1016666666666667e-05,
      "loss": 0.0014,
      "step": 187120
    },
    {
      "epoch": 6.237666666666667,
      "grad_norm": 0.032033901661634445,
      "learning_rate": 1.1014583333333335e-05,
      "loss": 0.0018,
      "step": 187130
    },
    {
      "epoch": 6.2379999999999995,
      "grad_norm": 0.007122891489416361,
      "learning_rate": 1.10125e-05,
      "loss": 0.0019,
      "step": 187140
    },
    {
      "epoch": 6.238333333333333,
      "grad_norm": 0.02915361151099205,
      "learning_rate": 1.1010416666666667e-05,
      "loss": 0.0021,
      "step": 187150
    },
    {
      "epoch": 6.238666666666667,
      "grad_norm": 0.08667054772377014,
      "learning_rate": 1.1008333333333334e-05,
      "loss": 0.0018,
      "step": 187160
    },
    {
      "epoch": 6.239,
      "grad_norm": 0.3996678590774536,
      "learning_rate": 1.100625e-05,
      "loss": 0.0017,
      "step": 187170
    },
    {
      "epoch": 6.239333333333334,
      "grad_norm": 0.2601550817489624,
      "learning_rate": 1.1004166666666667e-05,
      "loss": 0.0018,
      "step": 187180
    },
    {
      "epoch": 6.2396666666666665,
      "grad_norm": 0.0860813781619072,
      "learning_rate": 1.1002083333333334e-05,
      "loss": 0.0016,
      "step": 187190
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.11455705016851425,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0015,
      "step": 187200
    },
    {
      "epoch": 6.240333333333333,
      "grad_norm": 0.06487179547548294,
      "learning_rate": 1.0997916666666667e-05,
      "loss": 0.0014,
      "step": 187210
    },
    {
      "epoch": 6.240666666666667,
      "grad_norm": 0.42773810029029846,
      "learning_rate": 1.0995833333333334e-05,
      "loss": 0.0018,
      "step": 187220
    },
    {
      "epoch": 6.241,
      "grad_norm": 0.20025798678398132,
      "learning_rate": 1.099375e-05,
      "loss": 0.0021,
      "step": 187230
    },
    {
      "epoch": 6.241333333333333,
      "grad_norm": 0.08649934828281403,
      "learning_rate": 1.0991666666666667e-05,
      "loss": 0.0012,
      "step": 187240
    },
    {
      "epoch": 6.241666666666666,
      "grad_norm": 0.11479330062866211,
      "learning_rate": 1.0989583333333334e-05,
      "loss": 0.0015,
      "step": 187250
    },
    {
      "epoch": 6.242,
      "grad_norm": 0.08553409576416016,
      "learning_rate": 1.0987500000000001e-05,
      "loss": 0.0019,
      "step": 187260
    },
    {
      "epoch": 6.242333333333334,
      "grad_norm": 0.20371954143047333,
      "learning_rate": 1.0985416666666668e-05,
      "loss": 0.0017,
      "step": 187270
    },
    {
      "epoch": 6.242666666666667,
      "grad_norm": 0.18542951345443726,
      "learning_rate": 1.0983333333333334e-05,
      "loss": 0.0011,
      "step": 187280
    },
    {
      "epoch": 6.243,
      "grad_norm": 0.286531925201416,
      "learning_rate": 1.098125e-05,
      "loss": 0.0016,
      "step": 187290
    },
    {
      "epoch": 6.243333333333333,
      "grad_norm": 0.11461971700191498,
      "learning_rate": 1.0979166666666666e-05,
      "loss": 0.0017,
      "step": 187300
    },
    {
      "epoch": 6.243666666666667,
      "grad_norm": 0.20004703104496002,
      "learning_rate": 1.0977083333333334e-05,
      "loss": 0.0018,
      "step": 187310
    },
    {
      "epoch": 6.244,
      "grad_norm": 0.14667990803718567,
      "learning_rate": 1.0975e-05,
      "loss": 0.0014,
      "step": 187320
    },
    {
      "epoch": 6.2443333333333335,
      "grad_norm": 0.2002304196357727,
      "learning_rate": 1.0972916666666668e-05,
      "loss": 0.002,
      "step": 187330
    },
    {
      "epoch": 6.244666666666666,
      "grad_norm": 0.14334483444690704,
      "learning_rate": 1.0970833333333335e-05,
      "loss": 0.0012,
      "step": 187340
    },
    {
      "epoch": 6.245,
      "grad_norm": 0.11495925486087799,
      "learning_rate": 1.096875e-05,
      "loss": 0.0018,
      "step": 187350
    },
    {
      "epoch": 6.245333333333333,
      "grad_norm": 0.14349082112312317,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.0016,
      "step": 187360
    },
    {
      "epoch": 6.245666666666667,
      "grad_norm": 0.008685391396284103,
      "learning_rate": 1.0964583333333333e-05,
      "loss": 0.0013,
      "step": 187370
    },
    {
      "epoch": 6.246,
      "grad_norm": 0.17152389883995056,
      "learning_rate": 1.09625e-05,
      "loss": 0.0016,
      "step": 187380
    },
    {
      "epoch": 6.246333333333333,
      "grad_norm": 0.05746881291270256,
      "learning_rate": 1.0960416666666668e-05,
      "loss": 0.0015,
      "step": 187390
    },
    {
      "epoch": 6.246666666666667,
      "grad_norm": 0.4150511622428894,
      "learning_rate": 1.0958333333333335e-05,
      "loss": 0.0017,
      "step": 187400
    },
    {
      "epoch": 6.247,
      "grad_norm": 0.25685399770736694,
      "learning_rate": 1.095625e-05,
      "loss": 0.0013,
      "step": 187410
    },
    {
      "epoch": 6.247333333333334,
      "grad_norm": 0.0861290991306305,
      "learning_rate": 1.0954166666666668e-05,
      "loss": 0.002,
      "step": 187420
    },
    {
      "epoch": 6.2476666666666665,
      "grad_norm": 0.2280740886926651,
      "learning_rate": 1.0952083333333333e-05,
      "loss": 0.0016,
      "step": 187430
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.1435321718454361,
      "learning_rate": 1.095e-05,
      "loss": 0.0022,
      "step": 187440
    },
    {
      "epoch": 6.248333333333333,
      "grad_norm": 0.058549728244543076,
      "learning_rate": 1.0947916666666667e-05,
      "loss": 0.0023,
      "step": 187450
    },
    {
      "epoch": 6.248666666666667,
      "grad_norm": 0.08748690783977509,
      "learning_rate": 1.0945833333333335e-05,
      "loss": 0.0017,
      "step": 187460
    },
    {
      "epoch": 6.249,
      "grad_norm": 0.14346645772457123,
      "learning_rate": 1.094375e-05,
      "loss": 0.0015,
      "step": 187470
    },
    {
      "epoch": 6.249333333333333,
      "grad_norm": 0.14295877516269684,
      "learning_rate": 1.0941666666666667e-05,
      "loss": 0.0018,
      "step": 187480
    },
    {
      "epoch": 6.249666666666666,
      "grad_norm": 0.2292485535144806,
      "learning_rate": 1.0939583333333334e-05,
      "loss": 0.0015,
      "step": 187490
    },
    {
      "epoch": 6.25,
      "grad_norm": 0.11421386897563934,
      "learning_rate": 1.09375e-05,
      "loss": 0.0018,
      "step": 187500
    },
    {
      "epoch": 6.250333333333334,
      "grad_norm": 0.057612959295511246,
      "learning_rate": 1.0935416666666667e-05,
      "loss": 0.0013,
      "step": 187510
    },
    {
      "epoch": 6.250666666666667,
      "grad_norm": 0.08584824204444885,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0013,
      "step": 187520
    },
    {
      "epoch": 6.251,
      "grad_norm": 0.3288019895553589,
      "learning_rate": 1.0931250000000001e-05,
      "loss": 0.0016,
      "step": 187530
    },
    {
      "epoch": 6.251333333333333,
      "grad_norm": 0.08595684915781021,
      "learning_rate": 1.0929166666666667e-05,
      "loss": 0.0014,
      "step": 187540
    },
    {
      "epoch": 6.251666666666667,
      "grad_norm": 0.057796694338321686,
      "learning_rate": 1.0927083333333334e-05,
      "loss": 0.0021,
      "step": 187550
    },
    {
      "epoch": 6.252,
      "grad_norm": 0.20025983452796936,
      "learning_rate": 1.0925000000000001e-05,
      "loss": 0.0024,
      "step": 187560
    },
    {
      "epoch": 6.2523333333333335,
      "grad_norm": 0.17518699169158936,
      "learning_rate": 1.0922916666666667e-05,
      "loss": 0.0017,
      "step": 187570
    },
    {
      "epoch": 6.252666666666666,
      "grad_norm": 0.11607690155506134,
      "learning_rate": 1.0920833333333334e-05,
      "loss": 0.0017,
      "step": 187580
    },
    {
      "epoch": 6.253,
      "grad_norm": 0.2577444612979889,
      "learning_rate": 1.0918750000000001e-05,
      "loss": 0.002,
      "step": 187590
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 0.19984018802642822,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.0014,
      "step": 187600
    },
    {
      "epoch": 6.253666666666667,
      "grad_norm": 0.2283133715391159,
      "learning_rate": 1.0914583333333334e-05,
      "loss": 0.0014,
      "step": 187610
    },
    {
      "epoch": 6.254,
      "grad_norm": 0.030532030388712883,
      "learning_rate": 1.0912500000000001e-05,
      "loss": 0.0017,
      "step": 187620
    },
    {
      "epoch": 6.254333333333333,
      "grad_norm": 0.1433909833431244,
      "learning_rate": 1.0910416666666668e-05,
      "loss": 0.0021,
      "step": 187630
    },
    {
      "epoch": 6.254666666666667,
      "grad_norm": 0.34303006529808044,
      "learning_rate": 1.0908333333333334e-05,
      "loss": 0.0016,
      "step": 187640
    },
    {
      "epoch": 6.255,
      "grad_norm": 0.007389478851109743,
      "learning_rate": 1.090625e-05,
      "loss": 0.0017,
      "step": 187650
    },
    {
      "epoch": 6.255333333333334,
      "grad_norm": 0.11440246552228928,
      "learning_rate": 1.0904166666666666e-05,
      "loss": 0.0014,
      "step": 187660
    },
    {
      "epoch": 6.2556666666666665,
      "grad_norm": 0.4565368890762329,
      "learning_rate": 1.0902083333333333e-05,
      "loss": 0.0027,
      "step": 187670
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.08722516149282455,
      "learning_rate": 1.09e-05,
      "loss": 0.002,
      "step": 187680
    },
    {
      "epoch": 6.256333333333333,
      "grad_norm": 0.05937115103006363,
      "learning_rate": 1.0897916666666668e-05,
      "loss": 0.0018,
      "step": 187690
    },
    {
      "epoch": 6.256666666666667,
      "grad_norm": 0.314452201128006,
      "learning_rate": 1.0895833333333335e-05,
      "loss": 0.0018,
      "step": 187700
    },
    {
      "epoch": 6.257,
      "grad_norm": 0.08679375052452087,
      "learning_rate": 1.089375e-05,
      "loss": 0.0018,
      "step": 187710
    },
    {
      "epoch": 6.257333333333333,
      "grad_norm": 0.11691602319478989,
      "learning_rate": 1.0891666666666666e-05,
      "loss": 0.0015,
      "step": 187720
    },
    {
      "epoch": 6.257666666666666,
      "grad_norm": 0.14351089298725128,
      "learning_rate": 1.0889583333333333e-05,
      "loss": 0.0017,
      "step": 187730
    },
    {
      "epoch": 6.258,
      "grad_norm": 0.0870528593659401,
      "learning_rate": 1.08875e-05,
      "loss": 0.0016,
      "step": 187740
    },
    {
      "epoch": 6.258333333333334,
      "grad_norm": 0.2850964069366455,
      "learning_rate": 1.0885416666666668e-05,
      "loss": 0.0017,
      "step": 187750
    },
    {
      "epoch": 6.258666666666667,
      "grad_norm": 0.3136289119720459,
      "learning_rate": 1.0883333333333335e-05,
      "loss": 0.002,
      "step": 187760
    },
    {
      "epoch": 6.259,
      "grad_norm": 0.3468993902206421,
      "learning_rate": 1.0881250000000002e-05,
      "loss": 0.0013,
      "step": 187770
    },
    {
      "epoch": 6.259333333333333,
      "grad_norm": 0.2697722017765045,
      "learning_rate": 1.0879166666666667e-05,
      "loss": 0.0018,
      "step": 187780
    },
    {
      "epoch": 6.259666666666667,
      "grad_norm": 0.3145976960659027,
      "learning_rate": 1.0877083333333333e-05,
      "loss": 0.002,
      "step": 187790
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.05849648267030716,
      "learning_rate": 1.0875e-05,
      "loss": 0.0016,
      "step": 187800
    },
    {
      "epoch": 6.2603333333333335,
      "grad_norm": 0.08585692197084427,
      "learning_rate": 1.0872916666666667e-05,
      "loss": 0.0016,
      "step": 187810
    },
    {
      "epoch": 6.260666666666666,
      "grad_norm": 0.17599400877952576,
      "learning_rate": 1.0870833333333334e-05,
      "loss": 0.0016,
      "step": 187820
    },
    {
      "epoch": 6.261,
      "grad_norm": 0.05773084983229637,
      "learning_rate": 1.0868750000000002e-05,
      "loss": 0.0014,
      "step": 187830
    },
    {
      "epoch": 6.261333333333333,
      "grad_norm": 0.37149372696876526,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0024,
      "step": 187840
    },
    {
      "epoch": 6.261666666666667,
      "grad_norm": 0.3296228051185608,
      "learning_rate": 1.0864583333333334e-05,
      "loss": 0.0014,
      "step": 187850
    },
    {
      "epoch": 6.2620000000000005,
      "grad_norm": 0.1715099811553955,
      "learning_rate": 1.08625e-05,
      "loss": 0.002,
      "step": 187860
    },
    {
      "epoch": 6.262333333333333,
      "grad_norm": 0.057916879653930664,
      "learning_rate": 1.0860416666666667e-05,
      "loss": 0.0016,
      "step": 187870
    },
    {
      "epoch": 6.262666666666667,
      "grad_norm": 0.22869421541690826,
      "learning_rate": 1.0858333333333334e-05,
      "loss": 0.0015,
      "step": 187880
    },
    {
      "epoch": 6.263,
      "grad_norm": 0.22855132818222046,
      "learning_rate": 1.0856250000000001e-05,
      "loss": 0.0019,
      "step": 187890
    },
    {
      "epoch": 6.263333333333334,
      "grad_norm": 0.146815687417984,
      "learning_rate": 1.0854166666666667e-05,
      "loss": 0.0026,
      "step": 187900
    },
    {
      "epoch": 6.2636666666666665,
      "grad_norm": 0.058102138340473175,
      "learning_rate": 1.0852083333333334e-05,
      "loss": 0.0021,
      "step": 187910
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.19977019727230072,
      "learning_rate": 1.0850000000000001e-05,
      "loss": 0.0018,
      "step": 187920
    },
    {
      "epoch": 6.264333333333333,
      "grad_norm": 0.20040979981422424,
      "learning_rate": 1.0847916666666667e-05,
      "loss": 0.0012,
      "step": 187930
    },
    {
      "epoch": 6.264666666666667,
      "grad_norm": 0.28517621755599976,
      "learning_rate": 1.0845833333333334e-05,
      "loss": 0.0013,
      "step": 187940
    },
    {
      "epoch": 6.265,
      "grad_norm": 0.14308179914951324,
      "learning_rate": 1.0843750000000001e-05,
      "loss": 0.0014,
      "step": 187950
    },
    {
      "epoch": 6.265333333333333,
      "grad_norm": 0.25690844655036926,
      "learning_rate": 1.0841666666666666e-05,
      "loss": 0.002,
      "step": 187960
    },
    {
      "epoch": 6.265666666666666,
      "grad_norm": 0.14299307763576508,
      "learning_rate": 1.0839583333333334e-05,
      "loss": 0.0017,
      "step": 187970
    },
    {
      "epoch": 6.266,
      "grad_norm": 0.03077845647931099,
      "learning_rate": 1.08375e-05,
      "loss": 0.0015,
      "step": 187980
    },
    {
      "epoch": 6.266333333333334,
      "grad_norm": 0.04781084880232811,
      "learning_rate": 1.0835416666666668e-05,
      "loss": 0.0016,
      "step": 187990
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.216183140873909,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0012,
      "step": 188000
    },
    {
      "epoch": 6.267,
      "grad_norm": 0.28606459498405457,
      "learning_rate": 1.083125e-05,
      "loss": 0.0017,
      "step": 188010
    },
    {
      "epoch": 6.267333333333333,
      "grad_norm": 0.3711700439453125,
      "learning_rate": 1.0829166666666666e-05,
      "loss": 0.0032,
      "step": 188020
    },
    {
      "epoch": 6.267666666666667,
      "grad_norm": 0.27281492948532104,
      "learning_rate": 1.0827083333333333e-05,
      "loss": 0.0014,
      "step": 188030
    },
    {
      "epoch": 6.268,
      "grad_norm": 0.057636648416519165,
      "learning_rate": 1.0825e-05,
      "loss": 0.0014,
      "step": 188040
    },
    {
      "epoch": 6.2683333333333335,
      "grad_norm": 0.2284569889307022,
      "learning_rate": 1.0822916666666668e-05,
      "loss": 0.0021,
      "step": 188050
    },
    {
      "epoch": 6.268666666666666,
      "grad_norm": 0.2001124769449234,
      "learning_rate": 1.0820833333333335e-05,
      "loss": 0.0015,
      "step": 188060
    },
    {
      "epoch": 6.269,
      "grad_norm": 0.08573069423437119,
      "learning_rate": 1.081875e-05,
      "loss": 0.0019,
      "step": 188070
    },
    {
      "epoch": 6.269333333333333,
      "grad_norm": 0.31378936767578125,
      "learning_rate": 1.0816666666666666e-05,
      "loss": 0.0015,
      "step": 188080
    },
    {
      "epoch": 6.269666666666667,
      "grad_norm": 0.01224876381456852,
      "learning_rate": 1.0814583333333333e-05,
      "loss": 0.0015,
      "step": 188090
    },
    {
      "epoch": 6.27,
      "grad_norm": 0.13649354875087738,
      "learning_rate": 1.08125e-05,
      "loss": 0.0015,
      "step": 188100
    },
    {
      "epoch": 6.270333333333333,
      "grad_norm": 0.012923991307616234,
      "learning_rate": 1.0810416666666667e-05,
      "loss": 0.002,
      "step": 188110
    },
    {
      "epoch": 6.270666666666667,
      "grad_norm": 0.22785401344299316,
      "learning_rate": 1.0808333333333335e-05,
      "loss": 0.0011,
      "step": 188120
    },
    {
      "epoch": 6.271,
      "grad_norm": 0.0862584114074707,
      "learning_rate": 1.0806250000000002e-05,
      "loss": 0.0018,
      "step": 188130
    },
    {
      "epoch": 6.271333333333334,
      "grad_norm": 0.25896260142326355,
      "learning_rate": 1.0804166666666667e-05,
      "loss": 0.0017,
      "step": 188140
    },
    {
      "epoch": 6.2716666666666665,
      "grad_norm": 0.05801267921924591,
      "learning_rate": 1.0802083333333333e-05,
      "loss": 0.0013,
      "step": 188150
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.4357960820198059,
      "learning_rate": 1.08e-05,
      "loss": 0.0019,
      "step": 188160
    },
    {
      "epoch": 6.272333333333333,
      "grad_norm": 0.0868973582983017,
      "learning_rate": 1.0797916666666667e-05,
      "loss": 0.002,
      "step": 188170
    },
    {
      "epoch": 6.272666666666667,
      "grad_norm": 0.08641737699508667,
      "learning_rate": 1.0795833333333334e-05,
      "loss": 0.0019,
      "step": 188180
    },
    {
      "epoch": 6.273,
      "grad_norm": 0.17100901901721954,
      "learning_rate": 1.0793750000000001e-05,
      "loss": 0.0017,
      "step": 188190
    },
    {
      "epoch": 6.273333333333333,
      "grad_norm": 0.2033867985010147,
      "learning_rate": 1.0791666666666667e-05,
      "loss": 0.002,
      "step": 188200
    },
    {
      "epoch": 6.273666666666666,
      "grad_norm": 0.17165030539035797,
      "learning_rate": 1.0789583333333334e-05,
      "loss": 0.0018,
      "step": 188210
    },
    {
      "epoch": 6.274,
      "grad_norm": 0.059667594730854034,
      "learning_rate": 1.07875e-05,
      "loss": 0.0018,
      "step": 188220
    },
    {
      "epoch": 6.274333333333333,
      "grad_norm": 0.007946120575070381,
      "learning_rate": 1.0785416666666667e-05,
      "loss": 0.0014,
      "step": 188230
    },
    {
      "epoch": 6.274666666666667,
      "grad_norm": 0.007985411211848259,
      "learning_rate": 1.0783333333333334e-05,
      "loss": 0.0014,
      "step": 188240
    },
    {
      "epoch": 6.275,
      "grad_norm": 0.05807184427976608,
      "learning_rate": 1.0781250000000001e-05,
      "loss": 0.0014,
      "step": 188250
    },
    {
      "epoch": 6.275333333333333,
      "grad_norm": 0.05857677757740021,
      "learning_rate": 1.0779166666666667e-05,
      "loss": 0.0014,
      "step": 188260
    },
    {
      "epoch": 6.275666666666667,
      "grad_norm": 0.201129749417305,
      "learning_rate": 1.0777083333333334e-05,
      "loss": 0.0018,
      "step": 188270
    },
    {
      "epoch": 6.276,
      "grad_norm": 0.05847053602337837,
      "learning_rate": 1.0775000000000001e-05,
      "loss": 0.0019,
      "step": 188280
    },
    {
      "epoch": 6.2763333333333335,
      "grad_norm": 0.14313063025474548,
      "learning_rate": 1.0772916666666667e-05,
      "loss": 0.0018,
      "step": 188290
    },
    {
      "epoch": 6.276666666666666,
      "grad_norm": 0.042814165353775024,
      "learning_rate": 1.0770833333333334e-05,
      "loss": 0.0019,
      "step": 188300
    },
    {
      "epoch": 6.277,
      "grad_norm": 0.14331340789794922,
      "learning_rate": 1.0768750000000001e-05,
      "loss": 0.0023,
      "step": 188310
    },
    {
      "epoch": 6.277333333333333,
      "grad_norm": 0.14311838150024414,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0023,
      "step": 188320
    },
    {
      "epoch": 6.277666666666667,
      "grad_norm": 0.08594831824302673,
      "learning_rate": 1.0764583333333334e-05,
      "loss": 0.0025,
      "step": 188330
    },
    {
      "epoch": 6.2780000000000005,
      "grad_norm": 0.2006053477525711,
      "learning_rate": 1.07625e-05,
      "loss": 0.0025,
      "step": 188340
    },
    {
      "epoch": 6.278333333333333,
      "grad_norm": 0.05788029730319977,
      "learning_rate": 1.0760416666666668e-05,
      "loss": 0.002,
      "step": 188350
    },
    {
      "epoch": 6.278666666666667,
      "grad_norm": 0.11461427807807922,
      "learning_rate": 1.0758333333333333e-05,
      "loss": 0.0013,
      "step": 188360
    },
    {
      "epoch": 6.279,
      "grad_norm": 0.058668747544288635,
      "learning_rate": 1.075625e-05,
      "loss": 0.0014,
      "step": 188370
    },
    {
      "epoch": 6.279333333333334,
      "grad_norm": 0.11421964317560196,
      "learning_rate": 1.0754166666666666e-05,
      "loss": 0.0025,
      "step": 188380
    },
    {
      "epoch": 6.2796666666666665,
      "grad_norm": 0.11468548327684402,
      "learning_rate": 1.0752083333333333e-05,
      "loss": 0.0016,
      "step": 188390
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.058750346302986145,
      "learning_rate": 1.075e-05,
      "loss": 0.0012,
      "step": 188400
    },
    {
      "epoch": 6.280333333333333,
      "grad_norm": 0.0573982298374176,
      "learning_rate": 1.0747916666666668e-05,
      "loss": 0.0015,
      "step": 188410
    },
    {
      "epoch": 6.280666666666667,
      "grad_norm": 0.1429302990436554,
      "learning_rate": 1.0745833333333335e-05,
      "loss": 0.0014,
      "step": 188420
    },
    {
      "epoch": 6.281,
      "grad_norm": 0.14307911694049835,
      "learning_rate": 1.074375e-05,
      "loss": 0.0017,
      "step": 188430
    },
    {
      "epoch": 6.281333333333333,
      "grad_norm": 0.2567472457885742,
      "learning_rate": 1.0741666666666666e-05,
      "loss": 0.0015,
      "step": 188440
    },
    {
      "epoch": 6.281666666666666,
      "grad_norm": 0.05861513689160347,
      "learning_rate": 1.0739583333333333e-05,
      "loss": 0.0019,
      "step": 188450
    },
    {
      "epoch": 6.282,
      "grad_norm": 0.09002142399549484,
      "learning_rate": 1.07375e-05,
      "loss": 0.0018,
      "step": 188460
    },
    {
      "epoch": 6.282333333333334,
      "grad_norm": 0.029822593554854393,
      "learning_rate": 1.0735416666666667e-05,
      "loss": 0.0019,
      "step": 188470
    },
    {
      "epoch": 6.282666666666667,
      "grad_norm": 0.11446137726306915,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0018,
      "step": 188480
    },
    {
      "epoch": 6.283,
      "grad_norm": 0.469449520111084,
      "learning_rate": 1.0731250000000002e-05,
      "loss": 0.0021,
      "step": 188490
    },
    {
      "epoch": 6.283333333333333,
      "grad_norm": 0.008510407991707325,
      "learning_rate": 1.0729166666666667e-05,
      "loss": 0.0015,
      "step": 188500
    },
    {
      "epoch": 6.283666666666667,
      "grad_norm": 0.11498512327671051,
      "learning_rate": 1.0727083333333333e-05,
      "loss": 0.0021,
      "step": 188510
    },
    {
      "epoch": 6.284,
      "grad_norm": 0.275248646736145,
      "learning_rate": 1.0725e-05,
      "loss": 0.0017,
      "step": 188520
    },
    {
      "epoch": 6.2843333333333335,
      "grad_norm": 0.4282325506210327,
      "learning_rate": 1.0722916666666667e-05,
      "loss": 0.0017,
      "step": 188530
    },
    {
      "epoch": 6.284666666666666,
      "grad_norm": 0.11457666009664536,
      "learning_rate": 1.0720833333333334e-05,
      "loss": 0.0018,
      "step": 188540
    },
    {
      "epoch": 6.285,
      "grad_norm": 0.23106317222118378,
      "learning_rate": 1.0718750000000001e-05,
      "loss": 0.0015,
      "step": 188550
    },
    {
      "epoch": 6.285333333333333,
      "grad_norm": 0.11520473659038544,
      "learning_rate": 1.0716666666666667e-05,
      "loss": 0.002,
      "step": 188560
    },
    {
      "epoch": 6.285666666666667,
      "grad_norm": 0.05874968692660332,
      "learning_rate": 1.0714583333333334e-05,
      "loss": 0.0014,
      "step": 188570
    },
    {
      "epoch": 6.286,
      "grad_norm": 0.20004290342330933,
      "learning_rate": 1.07125e-05,
      "loss": 0.0011,
      "step": 188580
    },
    {
      "epoch": 6.286333333333333,
      "grad_norm": 0.19922681152820587,
      "learning_rate": 1.0710416666666667e-05,
      "loss": 0.0019,
      "step": 188590
    },
    {
      "epoch": 6.286666666666667,
      "grad_norm": 0.20043377578258514,
      "learning_rate": 1.0708333333333334e-05,
      "loss": 0.0013,
      "step": 188600
    },
    {
      "epoch": 6.287,
      "grad_norm": 0.17153039574623108,
      "learning_rate": 1.0706250000000001e-05,
      "loss": 0.0021,
      "step": 188610
    },
    {
      "epoch": 6.287333333333334,
      "grad_norm": 0.05778937041759491,
      "learning_rate": 1.0704166666666667e-05,
      "loss": 0.002,
      "step": 188620
    },
    {
      "epoch": 6.2876666666666665,
      "grad_norm": 0.9331443309783936,
      "learning_rate": 1.0702083333333334e-05,
      "loss": 0.0024,
      "step": 188630
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.17107073962688446,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.0021,
      "step": 188640
    },
    {
      "epoch": 6.288333333333333,
      "grad_norm": 0.006673744414001703,
      "learning_rate": 1.0697916666666666e-05,
      "loss": 0.0021,
      "step": 188650
    },
    {
      "epoch": 6.288666666666667,
      "grad_norm": 0.2313556671142578,
      "learning_rate": 1.0695833333333334e-05,
      "loss": 0.0025,
      "step": 188660
    },
    {
      "epoch": 6.289,
      "grad_norm": 0.39309990406036377,
      "learning_rate": 1.069375e-05,
      "loss": 0.0012,
      "step": 188670
    },
    {
      "epoch": 6.289333333333333,
      "grad_norm": 0.27124515175819397,
      "learning_rate": 1.0691666666666666e-05,
      "loss": 0.0021,
      "step": 188680
    },
    {
      "epoch": 6.289666666666666,
      "grad_norm": 0.22872845828533173,
      "learning_rate": 1.0689583333333333e-05,
      "loss": 0.0014,
      "step": 188690
    },
    {
      "epoch": 6.29,
      "grad_norm": 0.14349113404750824,
      "learning_rate": 1.06875e-05,
      "loss": 0.0015,
      "step": 188700
    },
    {
      "epoch": 6.290333333333333,
      "grad_norm": 0.06401731818914413,
      "learning_rate": 1.0685416666666668e-05,
      "loss": 0.0016,
      "step": 188710
    },
    {
      "epoch": 6.290666666666667,
      "grad_norm": 0.1459486186504364,
      "learning_rate": 1.0683333333333333e-05,
      "loss": 0.0014,
      "step": 188720
    },
    {
      "epoch": 6.291,
      "grad_norm": 0.1516232043504715,
      "learning_rate": 1.068125e-05,
      "loss": 0.0016,
      "step": 188730
    },
    {
      "epoch": 6.291333333333333,
      "grad_norm": 0.05818120762705803,
      "learning_rate": 1.0679166666666666e-05,
      "loss": 0.0012,
      "step": 188740
    },
    {
      "epoch": 6.291666666666667,
      "grad_norm": 0.11526298522949219,
      "learning_rate": 1.0677083333333333e-05,
      "loss": 0.0019,
      "step": 188750
    },
    {
      "epoch": 6.292,
      "grad_norm": 0.22829069197177887,
      "learning_rate": 1.0675e-05,
      "loss": 0.0019,
      "step": 188760
    },
    {
      "epoch": 6.292333333333334,
      "grad_norm": 0.28596043586730957,
      "learning_rate": 1.0672916666666667e-05,
      "loss": 0.0018,
      "step": 188770
    },
    {
      "epoch": 6.292666666666666,
      "grad_norm": 0.48500949144363403,
      "learning_rate": 1.0670833333333335e-05,
      "loss": 0.0016,
      "step": 188780
    },
    {
      "epoch": 6.293,
      "grad_norm": 0.029637839645147324,
      "learning_rate": 1.066875e-05,
      "loss": 0.0022,
      "step": 188790
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 0.3136942386627197,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0012,
      "step": 188800
    },
    {
      "epoch": 6.293666666666667,
      "grad_norm": 0.25736090540885925,
      "learning_rate": 1.0664583333333333e-05,
      "loss": 0.0015,
      "step": 188810
    },
    {
      "epoch": 6.294,
      "grad_norm": 0.17147652804851532,
      "learning_rate": 1.06625e-05,
      "loss": 0.0015,
      "step": 188820
    },
    {
      "epoch": 6.294333333333333,
      "grad_norm": 0.20037391781806946,
      "learning_rate": 1.0660416666666667e-05,
      "loss": 0.0014,
      "step": 188830
    },
    {
      "epoch": 6.294666666666667,
      "grad_norm": 0.2889949679374695,
      "learning_rate": 1.0658333333333334e-05,
      "loss": 0.0017,
      "step": 188840
    },
    {
      "epoch": 6.295,
      "grad_norm": 0.31397679448127747,
      "learning_rate": 1.0656250000000002e-05,
      "loss": 0.002,
      "step": 188850
    },
    {
      "epoch": 6.295333333333334,
      "grad_norm": 0.4334210157394409,
      "learning_rate": 1.0654166666666667e-05,
      "loss": 0.0018,
      "step": 188860
    },
    {
      "epoch": 6.2956666666666665,
      "grad_norm": 0.490260511636734,
      "learning_rate": 1.0652083333333334e-05,
      "loss": 0.0031,
      "step": 188870
    },
    {
      "epoch": 6.296,
      "grad_norm": 0.05850625038146973,
      "learning_rate": 1.065e-05,
      "loss": 0.0018,
      "step": 188880
    },
    {
      "epoch": 6.296333333333333,
      "grad_norm": 0.11575726419687271,
      "learning_rate": 1.0647916666666667e-05,
      "loss": 0.0018,
      "step": 188890
    },
    {
      "epoch": 6.296666666666667,
      "grad_norm": 0.2289675623178482,
      "learning_rate": 1.0645833333333334e-05,
      "loss": 0.0014,
      "step": 188900
    },
    {
      "epoch": 6.297,
      "grad_norm": 0.11396785080432892,
      "learning_rate": 1.0643750000000001e-05,
      "loss": 0.0021,
      "step": 188910
    },
    {
      "epoch": 6.2973333333333334,
      "grad_norm": 0.17173762619495392,
      "learning_rate": 1.0641666666666668e-05,
      "loss": 0.002,
      "step": 188920
    },
    {
      "epoch": 6.297666666666666,
      "grad_norm": 0.31406131386756897,
      "learning_rate": 1.0639583333333334e-05,
      "loss": 0.0023,
      "step": 188930
    },
    {
      "epoch": 6.298,
      "grad_norm": 0.10644770413637161,
      "learning_rate": 1.0637500000000001e-05,
      "loss": 0.0015,
      "step": 188940
    },
    {
      "epoch": 6.298333333333334,
      "grad_norm": 0.11517082899808884,
      "learning_rate": 1.0635416666666667e-05,
      "loss": 0.0021,
      "step": 188950
    },
    {
      "epoch": 6.298666666666667,
      "grad_norm": 0.11525782942771912,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0014,
      "step": 188960
    },
    {
      "epoch": 6.299,
      "grad_norm": 0.1999114602804184,
      "learning_rate": 1.0631250000000001e-05,
      "loss": 0.002,
      "step": 188970
    },
    {
      "epoch": 6.299333333333333,
      "grad_norm": 0.257186621427536,
      "learning_rate": 1.0629166666666668e-05,
      "loss": 0.0021,
      "step": 188980
    },
    {
      "epoch": 6.299666666666667,
      "grad_norm": 0.2855680584907532,
      "learning_rate": 1.0627083333333334e-05,
      "loss": 0.0016,
      "step": 188990
    },
    {
      "epoch": 6.3,
      "grad_norm": 0.0866708979010582,
      "learning_rate": 1.0625e-05,
      "loss": 0.0017,
      "step": 189000
    },
    {
      "epoch": 6.300333333333334,
      "grad_norm": 0.1712493598461151,
      "learning_rate": 1.0622916666666668e-05,
      "loss": 0.0021,
      "step": 189010
    },
    {
      "epoch": 6.300666666666666,
      "grad_norm": 0.22901830077171326,
      "learning_rate": 1.0620833333333333e-05,
      "loss": 0.0015,
      "step": 189020
    },
    {
      "epoch": 6.301,
      "grad_norm": 0.28624412417411804,
      "learning_rate": 1.061875e-05,
      "loss": 0.0021,
      "step": 189030
    },
    {
      "epoch": 6.301333333333333,
      "grad_norm": 0.14360356330871582,
      "learning_rate": 1.0616666666666668e-05,
      "loss": 0.0013,
      "step": 189040
    },
    {
      "epoch": 6.301666666666667,
      "grad_norm": 0.20001612603664398,
      "learning_rate": 1.0614583333333333e-05,
      "loss": 0.0012,
      "step": 189050
    },
    {
      "epoch": 6.302,
      "grad_norm": 0.15625399351119995,
      "learning_rate": 1.06125e-05,
      "loss": 0.0019,
      "step": 189060
    },
    {
      "epoch": 6.302333333333333,
      "grad_norm": 0.514114260673523,
      "learning_rate": 1.0610416666666668e-05,
      "loss": 0.0015,
      "step": 189070
    },
    {
      "epoch": 6.302666666666667,
      "grad_norm": 0.28559958934783936,
      "learning_rate": 1.0608333333333335e-05,
      "loss": 0.0014,
      "step": 189080
    },
    {
      "epoch": 6.303,
      "grad_norm": 0.34242764115333557,
      "learning_rate": 1.060625e-05,
      "loss": 0.0014,
      "step": 189090
    },
    {
      "epoch": 6.303333333333334,
      "grad_norm": 0.5245442390441895,
      "learning_rate": 1.0604166666666667e-05,
      "loss": 0.0017,
      "step": 189100
    },
    {
      "epoch": 6.3036666666666665,
      "grad_norm": 0.1495877057313919,
      "learning_rate": 1.0602083333333333e-05,
      "loss": 0.0027,
      "step": 189110
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.19966965913772583,
      "learning_rate": 1.06e-05,
      "loss": 0.0017,
      "step": 189120
    },
    {
      "epoch": 6.304333333333333,
      "grad_norm": 0.42838647961616516,
      "learning_rate": 1.0597916666666667e-05,
      "loss": 0.0023,
      "step": 189130
    },
    {
      "epoch": 6.304666666666667,
      "grad_norm": 0.11530198156833649,
      "learning_rate": 1.0595833333333335e-05,
      "loss": 0.0015,
      "step": 189140
    },
    {
      "epoch": 6.305,
      "grad_norm": 0.029464522376656532,
      "learning_rate": 1.0593750000000002e-05,
      "loss": 0.0011,
      "step": 189150
    },
    {
      "epoch": 6.3053333333333335,
      "grad_norm": 0.22828666865825653,
      "learning_rate": 1.0591666666666667e-05,
      "loss": 0.0018,
      "step": 189160
    },
    {
      "epoch": 6.305666666666666,
      "grad_norm": 0.03023708425462246,
      "learning_rate": 1.0589583333333333e-05,
      "loss": 0.0021,
      "step": 189170
    },
    {
      "epoch": 6.306,
      "grad_norm": 0.05776986852288246,
      "learning_rate": 1.05875e-05,
      "loss": 0.0024,
      "step": 189180
    },
    {
      "epoch": 6.306333333333333,
      "grad_norm": 0.1998901069164276,
      "learning_rate": 1.0585416666666667e-05,
      "loss": 0.0014,
      "step": 189190
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 0.1713801622390747,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.0022,
      "step": 189200
    },
    {
      "epoch": 6.307,
      "grad_norm": 0.25733864307403564,
      "learning_rate": 1.0581250000000001e-05,
      "loss": 0.0022,
      "step": 189210
    },
    {
      "epoch": 6.307333333333333,
      "grad_norm": 0.11492197215557098,
      "learning_rate": 1.0579166666666669e-05,
      "loss": 0.0014,
      "step": 189220
    },
    {
      "epoch": 6.307666666666667,
      "grad_norm": 0.03136546164751053,
      "learning_rate": 1.0577083333333334e-05,
      "loss": 0.0019,
      "step": 189230
    },
    {
      "epoch": 6.308,
      "grad_norm": 0.12221524119377136,
      "learning_rate": 1.0575e-05,
      "loss": 0.0014,
      "step": 189240
    },
    {
      "epoch": 6.308333333333334,
      "grad_norm": 0.22863049805164337,
      "learning_rate": 1.0572916666666667e-05,
      "loss": 0.0015,
      "step": 189250
    },
    {
      "epoch": 6.308666666666666,
      "grad_norm": 0.17099234461784363,
      "learning_rate": 1.0570833333333334e-05,
      "loss": 0.0017,
      "step": 189260
    },
    {
      "epoch": 6.309,
      "grad_norm": 0.05779244005680084,
      "learning_rate": 1.0568750000000001e-05,
      "loss": 0.0017,
      "step": 189270
    },
    {
      "epoch": 6.309333333333333,
      "grad_norm": 0.22799937427043915,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0017,
      "step": 189280
    },
    {
      "epoch": 6.309666666666667,
      "grad_norm": 0.0857815369963646,
      "learning_rate": 1.0564583333333334e-05,
      "loss": 0.0015,
      "step": 189290
    },
    {
      "epoch": 6.31,
      "grad_norm": 0.08754923194646835,
      "learning_rate": 1.0562500000000001e-05,
      "loss": 0.0018,
      "step": 189300
    },
    {
      "epoch": 6.310333333333333,
      "grad_norm": 0.1996111124753952,
      "learning_rate": 1.0560416666666666e-05,
      "loss": 0.0018,
      "step": 189310
    },
    {
      "epoch": 6.310666666666666,
      "grad_norm": 0.2853088974952698,
      "learning_rate": 1.0558333333333334e-05,
      "loss": 0.0024,
      "step": 189320
    },
    {
      "epoch": 6.311,
      "grad_norm": 0.038239866495132446,
      "learning_rate": 1.055625e-05,
      "loss": 0.0019,
      "step": 189330
    },
    {
      "epoch": 6.311333333333334,
      "grad_norm": 0.17106585204601288,
      "learning_rate": 1.0554166666666668e-05,
      "loss": 0.0012,
      "step": 189340
    },
    {
      "epoch": 6.3116666666666665,
      "grad_norm": 0.143448606133461,
      "learning_rate": 1.0552083333333333e-05,
      "loss": 0.0019,
      "step": 189350
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.008009952493011951,
      "learning_rate": 1.055e-05,
      "loss": 0.0021,
      "step": 189360
    },
    {
      "epoch": 6.312333333333333,
      "grad_norm": 0.1144886389374733,
      "learning_rate": 1.0547916666666668e-05,
      "loss": 0.0016,
      "step": 189370
    },
    {
      "epoch": 6.312666666666667,
      "grad_norm": 0.19998224079608917,
      "learning_rate": 1.0545833333333333e-05,
      "loss": 0.002,
      "step": 189380
    },
    {
      "epoch": 6.313,
      "grad_norm": 0.39982137084007263,
      "learning_rate": 1.054375e-05,
      "loss": 0.0021,
      "step": 189390
    },
    {
      "epoch": 6.3133333333333335,
      "grad_norm": 0.11441551893949509,
      "learning_rate": 1.0541666666666668e-05,
      "loss": 0.0015,
      "step": 189400
    },
    {
      "epoch": 6.313666666666666,
      "grad_norm": 0.11596493422985077,
      "learning_rate": 1.0539583333333333e-05,
      "loss": 0.0015,
      "step": 189410
    },
    {
      "epoch": 6.314,
      "grad_norm": 0.08656555414199829,
      "learning_rate": 1.05375e-05,
      "loss": 0.0015,
      "step": 189420
    },
    {
      "epoch": 6.314333333333334,
      "grad_norm": 0.2572671175003052,
      "learning_rate": 1.0535416666666668e-05,
      "loss": 0.0013,
      "step": 189430
    },
    {
      "epoch": 6.314666666666667,
      "grad_norm": 0.20388320088386536,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.002,
      "step": 189440
    },
    {
      "epoch": 6.315,
      "grad_norm": 0.15509970486164093,
      "learning_rate": 1.053125e-05,
      "loss": 0.0017,
      "step": 189450
    },
    {
      "epoch": 6.315333333333333,
      "grad_norm": 0.4523749351501465,
      "learning_rate": 1.0529166666666667e-05,
      "loss": 0.0018,
      "step": 189460
    },
    {
      "epoch": 6.315666666666667,
      "grad_norm": 0.11488231271505356,
      "learning_rate": 1.0527083333333333e-05,
      "loss": 0.0027,
      "step": 189470
    },
    {
      "epoch": 6.316,
      "grad_norm": 0.48527267575263977,
      "learning_rate": 1.0525e-05,
      "loss": 0.0012,
      "step": 189480
    },
    {
      "epoch": 6.316333333333334,
      "grad_norm": 0.22913405299186707,
      "learning_rate": 1.0522916666666667e-05,
      "loss": 0.0015,
      "step": 189490
    },
    {
      "epoch": 6.316666666666666,
      "grad_norm": 0.08637547492980957,
      "learning_rate": 1.0520833333333334e-05,
      "loss": 0.0015,
      "step": 189500
    },
    {
      "epoch": 6.317,
      "grad_norm": 0.14336459338665009,
      "learning_rate": 1.0518750000000002e-05,
      "loss": 0.0016,
      "step": 189510
    },
    {
      "epoch": 6.317333333333333,
      "grad_norm": 0.17190007865428925,
      "learning_rate": 1.0516666666666667e-05,
      "loss": 0.0021,
      "step": 189520
    },
    {
      "epoch": 6.317666666666667,
      "grad_norm": 0.01339637953788042,
      "learning_rate": 1.0514583333333333e-05,
      "loss": 0.0026,
      "step": 189530
    },
    {
      "epoch": 6.318,
      "grad_norm": 0.5084471702575684,
      "learning_rate": 1.05125e-05,
      "loss": 0.0018,
      "step": 189540
    },
    {
      "epoch": 6.318333333333333,
      "grad_norm": 0.24660147726535797,
      "learning_rate": 1.0510416666666667e-05,
      "loss": 0.0014,
      "step": 189550
    },
    {
      "epoch": 6.318666666666667,
      "grad_norm": 0.1437496691942215,
      "learning_rate": 1.0508333333333334e-05,
      "loss": 0.0018,
      "step": 189560
    },
    {
      "epoch": 6.319,
      "grad_norm": 0.2575516402721405,
      "learning_rate": 1.0506250000000001e-05,
      "loss": 0.0021,
      "step": 189570
    },
    {
      "epoch": 6.319333333333334,
      "grad_norm": 0.010755573399364948,
      "learning_rate": 1.0504166666666668e-05,
      "loss": 0.0021,
      "step": 189580
    },
    {
      "epoch": 6.3196666666666665,
      "grad_norm": 0.4873448312282562,
      "learning_rate": 1.0502083333333334e-05,
      "loss": 0.0021,
      "step": 189590
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.40013620257377625,
      "learning_rate": 1.05e-05,
      "loss": 0.0022,
      "step": 189600
    },
    {
      "epoch": 6.320333333333333,
      "grad_norm": 0.22849847376346588,
      "learning_rate": 1.0497916666666667e-05,
      "loss": 0.0012,
      "step": 189610
    },
    {
      "epoch": 6.320666666666667,
      "grad_norm": 0.20010575652122498,
      "learning_rate": 1.0495833333333334e-05,
      "loss": 0.0024,
      "step": 189620
    },
    {
      "epoch": 6.321,
      "grad_norm": 0.5549229383468628,
      "learning_rate": 1.0493750000000001e-05,
      "loss": 0.0013,
      "step": 189630
    },
    {
      "epoch": 6.3213333333333335,
      "grad_norm": 0.25826093554496765,
      "learning_rate": 1.0491666666666668e-05,
      "loss": 0.0015,
      "step": 189640
    },
    {
      "epoch": 6.321666666666666,
      "grad_norm": 0.08587770164012909,
      "learning_rate": 1.0489583333333334e-05,
      "loss": 0.0019,
      "step": 189650
    },
    {
      "epoch": 6.322,
      "grad_norm": 0.17129307985305786,
      "learning_rate": 1.04875e-05,
      "loss": 0.0019,
      "step": 189660
    },
    {
      "epoch": 6.322333333333333,
      "grad_norm": 0.08656914532184601,
      "learning_rate": 1.0485416666666666e-05,
      "loss": 0.0014,
      "step": 189670
    },
    {
      "epoch": 6.322666666666667,
      "grad_norm": 0.24051427841186523,
      "learning_rate": 1.0483333333333333e-05,
      "loss": 0.0021,
      "step": 189680
    },
    {
      "epoch": 6.323,
      "grad_norm": 0.17144601047039032,
      "learning_rate": 1.048125e-05,
      "loss": 0.0017,
      "step": 189690
    },
    {
      "epoch": 6.323333333333333,
      "grad_norm": 0.34385135769844055,
      "learning_rate": 1.0479166666666668e-05,
      "loss": 0.002,
      "step": 189700
    },
    {
      "epoch": 6.323666666666667,
      "grad_norm": 0.029613254591822624,
      "learning_rate": 1.0477083333333333e-05,
      "loss": 0.0022,
      "step": 189710
    },
    {
      "epoch": 6.324,
      "grad_norm": 0.05758475884795189,
      "learning_rate": 1.0475e-05,
      "loss": 0.0014,
      "step": 189720
    },
    {
      "epoch": 6.324333333333334,
      "grad_norm": 0.029135551303625107,
      "learning_rate": 1.0472916666666668e-05,
      "loss": 0.0017,
      "step": 189730
    },
    {
      "epoch": 6.324666666666666,
      "grad_norm": 0.5423921346664429,
      "learning_rate": 1.0470833333333333e-05,
      "loss": 0.0017,
      "step": 189740
    },
    {
      "epoch": 6.325,
      "grad_norm": 0.3709908723831177,
      "learning_rate": 1.046875e-05,
      "loss": 0.0018,
      "step": 189750
    },
    {
      "epoch": 6.325333333333333,
      "grad_norm": 0.1711929887533188,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0019,
      "step": 189760
    },
    {
      "epoch": 6.325666666666667,
      "grad_norm": 0.008216070011258125,
      "learning_rate": 1.0464583333333333e-05,
      "loss": 0.0018,
      "step": 189770
    },
    {
      "epoch": 6.326,
      "grad_norm": 0.20025883615016937,
      "learning_rate": 1.04625e-05,
      "loss": 0.0013,
      "step": 189780
    },
    {
      "epoch": 6.326333333333333,
      "grad_norm": 0.14291967451572418,
      "learning_rate": 1.0460416666666667e-05,
      "loss": 0.0025,
      "step": 189790
    },
    {
      "epoch": 6.326666666666666,
      "grad_norm": 0.11503375321626663,
      "learning_rate": 1.0458333333333335e-05,
      "loss": 0.0018,
      "step": 189800
    },
    {
      "epoch": 6.327,
      "grad_norm": 0.11627960950136185,
      "learning_rate": 1.045625e-05,
      "loss": 0.0022,
      "step": 189810
    },
    {
      "epoch": 6.327333333333334,
      "grad_norm": 0.3138529360294342,
      "learning_rate": 1.0454166666666667e-05,
      "loss": 0.0021,
      "step": 189820
    },
    {
      "epoch": 6.3276666666666666,
      "grad_norm": 0.0286207664757967,
      "learning_rate": 1.0452083333333333e-05,
      "loss": 0.0027,
      "step": 189830
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.08766890317201614,
      "learning_rate": 1.045e-05,
      "loss": 0.002,
      "step": 189840
    },
    {
      "epoch": 6.328333333333333,
      "grad_norm": 0.2000923603773117,
      "learning_rate": 1.0447916666666667e-05,
      "loss": 0.0018,
      "step": 189850
    },
    {
      "epoch": 6.328666666666667,
      "grad_norm": 0.3439076840877533,
      "learning_rate": 1.0445833333333334e-05,
      "loss": 0.0024,
      "step": 189860
    },
    {
      "epoch": 6.329,
      "grad_norm": 0.3426353633403778,
      "learning_rate": 1.0443750000000001e-05,
      "loss": 0.0021,
      "step": 189870
    },
    {
      "epoch": 6.3293333333333335,
      "grad_norm": 0.11505746096372604,
      "learning_rate": 1.0441666666666667e-05,
      "loss": 0.0017,
      "step": 189880
    },
    {
      "epoch": 6.329666666666666,
      "grad_norm": 0.4082353115081787,
      "learning_rate": 1.0439583333333332e-05,
      "loss": 0.0019,
      "step": 189890
    },
    {
      "epoch": 6.33,
      "grad_norm": 0.01167922094464302,
      "learning_rate": 1.04375e-05,
      "loss": 0.0015,
      "step": 189900
    },
    {
      "epoch": 6.330333333333333,
      "grad_norm": 0.08614358305931091,
      "learning_rate": 1.0435416666666667e-05,
      "loss": 0.0016,
      "step": 189910
    },
    {
      "epoch": 6.330666666666667,
      "grad_norm": 0.12361649423837662,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.0014,
      "step": 189920
    },
    {
      "epoch": 6.331,
      "grad_norm": 0.17140044271945953,
      "learning_rate": 1.0431250000000001e-05,
      "loss": 0.0025,
      "step": 189930
    },
    {
      "epoch": 6.331333333333333,
      "grad_norm": 0.1997366100549698,
      "learning_rate": 1.0429166666666668e-05,
      "loss": 0.0018,
      "step": 189940
    },
    {
      "epoch": 6.331666666666667,
      "grad_norm": 0.08658917993307114,
      "learning_rate": 1.0427083333333334e-05,
      "loss": 0.0026,
      "step": 189950
    },
    {
      "epoch": 6.332,
      "grad_norm": 0.02941426821053028,
      "learning_rate": 1.0425e-05,
      "loss": 0.0012,
      "step": 189960
    },
    {
      "epoch": 6.332333333333334,
      "grad_norm": 0.6076952815055847,
      "learning_rate": 1.0422916666666666e-05,
      "loss": 0.0023,
      "step": 189970
    },
    {
      "epoch": 6.332666666666666,
      "grad_norm": 0.45577511191368103,
      "learning_rate": 1.0420833333333334e-05,
      "loss": 0.0016,
      "step": 189980
    },
    {
      "epoch": 6.333,
      "grad_norm": 0.057970546185970306,
      "learning_rate": 1.041875e-05,
      "loss": 0.0022,
      "step": 189990
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.08643485605716705,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.0018,
      "step": 190000
    },
    {
      "epoch": 6.333666666666667,
      "grad_norm": 0.22887994349002838,
      "learning_rate": 1.0414583333333335e-05,
      "loss": 0.0021,
      "step": 190010
    },
    {
      "epoch": 6.334,
      "grad_norm": 0.05813246965408325,
      "learning_rate": 1.04125e-05,
      "loss": 0.0024,
      "step": 190020
    },
    {
      "epoch": 6.334333333333333,
      "grad_norm": 0.5131673812866211,
      "learning_rate": 1.0410416666666666e-05,
      "loss": 0.0015,
      "step": 190030
    },
    {
      "epoch": 6.334666666666667,
      "grad_norm": 0.1426609456539154,
      "learning_rate": 1.0408333333333333e-05,
      "loss": 0.0013,
      "step": 190040
    },
    {
      "epoch": 6.335,
      "grad_norm": 0.345136433839798,
      "learning_rate": 1.040625e-05,
      "loss": 0.0015,
      "step": 190050
    },
    {
      "epoch": 6.335333333333334,
      "grad_norm": 0.05814259126782417,
      "learning_rate": 1.0404166666666668e-05,
      "loss": 0.002,
      "step": 190060
    },
    {
      "epoch": 6.335666666666667,
      "grad_norm": 0.22905825078487396,
      "learning_rate": 1.0402083333333335e-05,
      "loss": 0.002,
      "step": 190070
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.03146908059716225,
      "learning_rate": 1.04e-05,
      "loss": 0.0016,
      "step": 190080
    },
    {
      "epoch": 6.336333333333333,
      "grad_norm": 0.014635704457759857,
      "learning_rate": 1.0397916666666668e-05,
      "loss": 0.0021,
      "step": 190090
    },
    {
      "epoch": 6.336666666666667,
      "grad_norm": 0.058628302067518234,
      "learning_rate": 1.0395833333333333e-05,
      "loss": 0.0012,
      "step": 190100
    },
    {
      "epoch": 6.337,
      "grad_norm": 0.019479064270853996,
      "learning_rate": 1.039375e-05,
      "loss": 0.0019,
      "step": 190110
    },
    {
      "epoch": 6.3373333333333335,
      "grad_norm": 0.011896668002009392,
      "learning_rate": 1.0391666666666667e-05,
      "loss": 0.0025,
      "step": 190120
    },
    {
      "epoch": 6.337666666666666,
      "grad_norm": 0.18020321428775787,
      "learning_rate": 1.0389583333333335e-05,
      "loss": 0.0025,
      "step": 190130
    },
    {
      "epoch": 6.338,
      "grad_norm": 0.08654751628637314,
      "learning_rate": 1.03875e-05,
      "loss": 0.0012,
      "step": 190140
    },
    {
      "epoch": 6.338333333333333,
      "grad_norm": 0.008568870835006237,
      "learning_rate": 1.0385416666666667e-05,
      "loss": 0.0018,
      "step": 190150
    },
    {
      "epoch": 6.338666666666667,
      "grad_norm": 0.05764104053378105,
      "learning_rate": 1.0383333333333334e-05,
      "loss": 0.0022,
      "step": 190160
    },
    {
      "epoch": 6.339,
      "grad_norm": 0.08638469129800797,
      "learning_rate": 1.038125e-05,
      "loss": 0.0012,
      "step": 190170
    },
    {
      "epoch": 6.339333333333333,
      "grad_norm": 0.1145654022693634,
      "learning_rate": 1.0379166666666667e-05,
      "loss": 0.0036,
      "step": 190180
    },
    {
      "epoch": 6.339666666666667,
      "grad_norm": 0.029963385313749313,
      "learning_rate": 1.0377083333333334e-05,
      "loss": 0.0015,
      "step": 190190
    },
    {
      "epoch": 6.34,
      "grad_norm": 0.19969938695430756,
      "learning_rate": 1.0375e-05,
      "loss": 0.0018,
      "step": 190200
    },
    {
      "epoch": 6.340333333333334,
      "grad_norm": 0.2856552302837372,
      "learning_rate": 1.0372916666666667e-05,
      "loss": 0.0015,
      "step": 190210
    },
    {
      "epoch": 6.3406666666666665,
      "grad_norm": 0.2004692256450653,
      "learning_rate": 1.0370833333333334e-05,
      "loss": 0.0018,
      "step": 190220
    },
    {
      "epoch": 6.341,
      "grad_norm": 0.22843065857887268,
      "learning_rate": 1.0368750000000001e-05,
      "loss": 0.0018,
      "step": 190230
    },
    {
      "epoch": 6.341333333333333,
      "grad_norm": 0.057481370866298676,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0014,
      "step": 190240
    },
    {
      "epoch": 6.341666666666667,
      "grad_norm": 0.08046477288007736,
      "learning_rate": 1.0364583333333334e-05,
      "loss": 0.0024,
      "step": 190250
    },
    {
      "epoch": 6.342,
      "grad_norm": 0.03113185428082943,
      "learning_rate": 1.03625e-05,
      "loss": 0.0022,
      "step": 190260
    },
    {
      "epoch": 6.342333333333333,
      "grad_norm": 0.286180704832077,
      "learning_rate": 1.0360416666666667e-05,
      "loss": 0.0021,
      "step": 190270
    },
    {
      "epoch": 6.342666666666666,
      "grad_norm": 0.2852207124233246,
      "learning_rate": 1.0358333333333334e-05,
      "loss": 0.0021,
      "step": 190280
    },
    {
      "epoch": 6.343,
      "grad_norm": 0.2573259174823761,
      "learning_rate": 1.0356250000000001e-05,
      "loss": 0.002,
      "step": 190290
    },
    {
      "epoch": 6.343333333333334,
      "grad_norm": 0.11432401090860367,
      "learning_rate": 1.0354166666666668e-05,
      "loss": 0.0022,
      "step": 190300
    },
    {
      "epoch": 6.343666666666667,
      "grad_norm": 0.11421798914670944,
      "learning_rate": 1.0352083333333334e-05,
      "loss": 0.0018,
      "step": 190310
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.08622220158576965,
      "learning_rate": 1.035e-05,
      "loss": 0.0017,
      "step": 190320
    },
    {
      "epoch": 6.344333333333333,
      "grad_norm": 0.05746889486908913,
      "learning_rate": 1.0347916666666666e-05,
      "loss": 0.0023,
      "step": 190330
    },
    {
      "epoch": 6.344666666666667,
      "grad_norm": 0.05843190476298332,
      "learning_rate": 1.0345833333333334e-05,
      "loss": 0.0018,
      "step": 190340
    },
    {
      "epoch": 6.345,
      "grad_norm": 0.22791826725006104,
      "learning_rate": 1.034375e-05,
      "loss": 0.0016,
      "step": 190350
    },
    {
      "epoch": 6.3453333333333335,
      "grad_norm": 0.14270031452178955,
      "learning_rate": 1.0341666666666668e-05,
      "loss": 0.002,
      "step": 190360
    },
    {
      "epoch": 6.345666666666666,
      "grad_norm": 0.1708868443965912,
      "learning_rate": 1.0339583333333335e-05,
      "loss": 0.0015,
      "step": 190370
    },
    {
      "epoch": 6.346,
      "grad_norm": 0.17098616063594818,
      "learning_rate": 1.03375e-05,
      "loss": 0.0021,
      "step": 190380
    },
    {
      "epoch": 6.346333333333333,
      "grad_norm": 0.37072375416755676,
      "learning_rate": 1.0335416666666666e-05,
      "loss": 0.0018,
      "step": 190390
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.11515983194112778,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.002,
      "step": 190400
    },
    {
      "epoch": 6.3469999999999995,
      "grad_norm": 0.030577722936868668,
      "learning_rate": 1.033125e-05,
      "loss": 0.0017,
      "step": 190410
    },
    {
      "epoch": 6.347333333333333,
      "grad_norm": 0.2849356532096863,
      "learning_rate": 1.0329166666666668e-05,
      "loss": 0.0018,
      "step": 190420
    },
    {
      "epoch": 6.347666666666667,
      "grad_norm": 0.17173980176448822,
      "learning_rate": 1.0327083333333335e-05,
      "loss": 0.0017,
      "step": 190430
    },
    {
      "epoch": 6.348,
      "grad_norm": 0.14349377155303955,
      "learning_rate": 1.0325e-05,
      "loss": 0.002,
      "step": 190440
    },
    {
      "epoch": 6.348333333333334,
      "grad_norm": 0.03148045018315315,
      "learning_rate": 1.0322916666666667e-05,
      "loss": 0.0021,
      "step": 190450
    },
    {
      "epoch": 6.3486666666666665,
      "grad_norm": 0.17494121193885803,
      "learning_rate": 1.0320833333333333e-05,
      "loss": 0.0012,
      "step": 190460
    },
    {
      "epoch": 6.349,
      "grad_norm": 0.1432361602783203,
      "learning_rate": 1.031875e-05,
      "loss": 0.0016,
      "step": 190470
    },
    {
      "epoch": 6.349333333333333,
      "grad_norm": 0.05802852287888527,
      "learning_rate": 1.0316666666666667e-05,
      "loss": 0.0014,
      "step": 190480
    },
    {
      "epoch": 6.349666666666667,
      "grad_norm": 0.2569456994533539,
      "learning_rate": 1.0314583333333334e-05,
      "loss": 0.0013,
      "step": 190490
    },
    {
      "epoch": 6.35,
      "grad_norm": 0.010342276655137539,
      "learning_rate": 1.03125e-05,
      "loss": 0.0016,
      "step": 190500
    },
    {
      "epoch": 6.350333333333333,
      "grad_norm": 0.0963355004787445,
      "learning_rate": 1.0310416666666667e-05,
      "loss": 0.0017,
      "step": 190510
    },
    {
      "epoch": 6.350666666666667,
      "grad_norm": 0.3988547623157501,
      "learning_rate": 1.0308333333333334e-05,
      "loss": 0.0019,
      "step": 190520
    },
    {
      "epoch": 6.351,
      "grad_norm": 0.01622139848768711,
      "learning_rate": 1.030625e-05,
      "loss": 0.0021,
      "step": 190530
    },
    {
      "epoch": 6.351333333333334,
      "grad_norm": 0.22911246120929718,
      "learning_rate": 1.0304166666666667e-05,
      "loss": 0.0021,
      "step": 190540
    },
    {
      "epoch": 6.351666666666667,
      "grad_norm": 0.14673887193202972,
      "learning_rate": 1.0302083333333334e-05,
      "loss": 0.0013,
      "step": 190550
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.2565859258174896,
      "learning_rate": 1.03e-05,
      "loss": 0.0017,
      "step": 190560
    },
    {
      "epoch": 6.352333333333333,
      "grad_norm": 0.11455570161342621,
      "learning_rate": 1.0297916666666667e-05,
      "loss": 0.0023,
      "step": 190570
    },
    {
      "epoch": 6.352666666666667,
      "grad_norm": 0.343234121799469,
      "learning_rate": 1.0295833333333334e-05,
      "loss": 0.0013,
      "step": 190580
    },
    {
      "epoch": 6.353,
      "grad_norm": 0.3424038290977478,
      "learning_rate": 1.0293750000000001e-05,
      "loss": 0.0014,
      "step": 190590
    },
    {
      "epoch": 6.3533333333333335,
      "grad_norm": 0.3011130392551422,
      "learning_rate": 1.0291666666666667e-05,
      "loss": 0.0022,
      "step": 190600
    },
    {
      "epoch": 6.353666666666666,
      "grad_norm": 0.17352451384067535,
      "learning_rate": 1.0289583333333334e-05,
      "loss": 0.0017,
      "step": 190610
    },
    {
      "epoch": 6.354,
      "grad_norm": 0.2284678965806961,
      "learning_rate": 1.02875e-05,
      "loss": 0.0016,
      "step": 190620
    },
    {
      "epoch": 6.354333333333333,
      "grad_norm": 0.03117327019572258,
      "learning_rate": 1.0285416666666667e-05,
      "loss": 0.0019,
      "step": 190630
    },
    {
      "epoch": 6.354666666666667,
      "grad_norm": 0.37110164761543274,
      "learning_rate": 1.0283333333333334e-05,
      "loss": 0.0023,
      "step": 190640
    },
    {
      "epoch": 6.355,
      "grad_norm": 0.3990228772163391,
      "learning_rate": 1.0281250000000001e-05,
      "loss": 0.0029,
      "step": 190650
    },
    {
      "epoch": 6.355333333333333,
      "grad_norm": 0.05869252234697342,
      "learning_rate": 1.0279166666666668e-05,
      "loss": 0.0018,
      "step": 190660
    },
    {
      "epoch": 6.355666666666667,
      "grad_norm": 0.058868300169706345,
      "learning_rate": 1.0277083333333335e-05,
      "loss": 0.002,
      "step": 190670
    },
    {
      "epoch": 6.356,
      "grad_norm": 0.37055736780166626,
      "learning_rate": 1.0275e-05,
      "loss": 0.0015,
      "step": 190680
    },
    {
      "epoch": 6.356333333333334,
      "grad_norm": 0.032005228102207184,
      "learning_rate": 1.0272916666666666e-05,
      "loss": 0.0027,
      "step": 190690
    },
    {
      "epoch": 6.3566666666666665,
      "grad_norm": 0.14281484484672546,
      "learning_rate": 1.0270833333333333e-05,
      "loss": 0.0016,
      "step": 190700
    },
    {
      "epoch": 6.357,
      "grad_norm": 0.5135712623596191,
      "learning_rate": 1.026875e-05,
      "loss": 0.0016,
      "step": 190710
    },
    {
      "epoch": 6.357333333333333,
      "grad_norm": 0.03298521414399147,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0016,
      "step": 190720
    },
    {
      "epoch": 6.357666666666667,
      "grad_norm": 0.11331439763307571,
      "learning_rate": 1.0264583333333335e-05,
      "loss": 0.0021,
      "step": 190730
    },
    {
      "epoch": 6.358,
      "grad_norm": 0.06033315509557724,
      "learning_rate": 1.02625e-05,
      "loss": 0.0016,
      "step": 190740
    },
    {
      "epoch": 6.358333333333333,
      "grad_norm": 0.28523537516593933,
      "learning_rate": 1.0260416666666668e-05,
      "loss": 0.0021,
      "step": 190750
    },
    {
      "epoch": 6.358666666666666,
      "grad_norm": 0.057503897696733475,
      "learning_rate": 1.0258333333333333e-05,
      "loss": 0.0018,
      "step": 190760
    },
    {
      "epoch": 6.359,
      "grad_norm": 0.0866367518901825,
      "learning_rate": 1.025625e-05,
      "loss": 0.0023,
      "step": 190770
    },
    {
      "epoch": 6.359333333333334,
      "grad_norm": 0.31413981318473816,
      "learning_rate": 1.0254166666666667e-05,
      "loss": 0.0012,
      "step": 190780
    },
    {
      "epoch": 6.359666666666667,
      "grad_norm": 0.057383567094802856,
      "learning_rate": 1.0252083333333335e-05,
      "loss": 0.0017,
      "step": 190790
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.2281884402036667,
      "learning_rate": 1.025e-05,
      "loss": 0.0018,
      "step": 190800
    },
    {
      "epoch": 6.360333333333333,
      "grad_norm": 0.23392194509506226,
      "learning_rate": 1.0247916666666667e-05,
      "loss": 0.0022,
      "step": 190810
    },
    {
      "epoch": 6.360666666666667,
      "grad_norm": 0.1430002897977829,
      "learning_rate": 1.0245833333333334e-05,
      "loss": 0.002,
      "step": 190820
    },
    {
      "epoch": 6.361,
      "grad_norm": 0.17173291742801666,
      "learning_rate": 1.024375e-05,
      "loss": 0.002,
      "step": 190830
    },
    {
      "epoch": 6.3613333333333335,
      "grad_norm": 0.34284254908561707,
      "learning_rate": 1.0241666666666667e-05,
      "loss": 0.0023,
      "step": 190840
    },
    {
      "epoch": 6.361666666666666,
      "grad_norm": 0.20038925111293793,
      "learning_rate": 1.0239583333333334e-05,
      "loss": 0.0014,
      "step": 190850
    },
    {
      "epoch": 6.362,
      "grad_norm": 0.20033107697963715,
      "learning_rate": 1.02375e-05,
      "loss": 0.0018,
      "step": 190860
    },
    {
      "epoch": 6.362333333333333,
      "grad_norm": 0.11204920709133148,
      "learning_rate": 1.0235416666666667e-05,
      "loss": 0.0011,
      "step": 190870
    },
    {
      "epoch": 6.362666666666667,
      "grad_norm": 0.058070648461580276,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.0016,
      "step": 190880
    },
    {
      "epoch": 6.3629999999999995,
      "grad_norm": 0.17136713862419128,
      "learning_rate": 1.0231250000000001e-05,
      "loss": 0.0017,
      "step": 190890
    },
    {
      "epoch": 6.363333333333333,
      "grad_norm": 0.0581437423825264,
      "learning_rate": 1.0229166666666667e-05,
      "loss": 0.0019,
      "step": 190900
    },
    {
      "epoch": 6.363666666666667,
      "grad_norm": 0.1431189924478531,
      "learning_rate": 1.0227083333333334e-05,
      "loss": 0.0016,
      "step": 190910
    },
    {
      "epoch": 6.364,
      "grad_norm": 0.19990722835063934,
      "learning_rate": 1.0225e-05,
      "loss": 0.0013,
      "step": 190920
    },
    {
      "epoch": 6.364333333333334,
      "grad_norm": 0.05902335047721863,
      "learning_rate": 1.0222916666666667e-05,
      "loss": 0.0017,
      "step": 190930
    },
    {
      "epoch": 6.3646666666666665,
      "grad_norm": 0.2852630019187927,
      "learning_rate": 1.0220833333333334e-05,
      "loss": 0.0017,
      "step": 190940
    },
    {
      "epoch": 6.365,
      "grad_norm": 0.1429578810930252,
      "learning_rate": 1.0218750000000001e-05,
      "loss": 0.0015,
      "step": 190950
    },
    {
      "epoch": 6.365333333333333,
      "grad_norm": 0.08672996610403061,
      "learning_rate": 1.0216666666666668e-05,
      "loss": 0.0015,
      "step": 190960
    },
    {
      "epoch": 6.365666666666667,
      "grad_norm": 0.05810105428099632,
      "learning_rate": 1.0214583333333334e-05,
      "loss": 0.0013,
      "step": 190970
    },
    {
      "epoch": 6.366,
      "grad_norm": 0.11564262956380844,
      "learning_rate": 1.02125e-05,
      "loss": 0.0012,
      "step": 190980
    },
    {
      "epoch": 6.366333333333333,
      "grad_norm": 0.2284332662820816,
      "learning_rate": 1.0210416666666666e-05,
      "loss": 0.0018,
      "step": 190990
    },
    {
      "epoch": 6.366666666666666,
      "grad_norm": 0.08587390929460526,
      "learning_rate": 1.0208333333333334e-05,
      "loss": 0.0022,
      "step": 191000
    },
    {
      "epoch": 6.367,
      "grad_norm": 0.0870228186249733,
      "learning_rate": 1.020625e-05,
      "loss": 0.0022,
      "step": 191010
    },
    {
      "epoch": 6.367333333333334,
      "grad_norm": 0.012853840366005898,
      "learning_rate": 1.0204166666666668e-05,
      "loss": 0.0021,
      "step": 191020
    },
    {
      "epoch": 6.367666666666667,
      "grad_norm": 0.05812307819724083,
      "learning_rate": 1.0202083333333335e-05,
      "loss": 0.0016,
      "step": 191030
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.030054554343223572,
      "learning_rate": 1.02e-05,
      "loss": 0.0014,
      "step": 191040
    },
    {
      "epoch": 6.368333333333333,
      "grad_norm": 0.01666950434446335,
      "learning_rate": 1.0197916666666666e-05,
      "loss": 0.0014,
      "step": 191050
    },
    {
      "epoch": 6.368666666666667,
      "grad_norm": 0.012980058789253235,
      "learning_rate": 1.0195833333333333e-05,
      "loss": 0.0021,
      "step": 191060
    },
    {
      "epoch": 6.369,
      "grad_norm": 0.28593114018440247,
      "learning_rate": 1.019375e-05,
      "loss": 0.0017,
      "step": 191070
    },
    {
      "epoch": 6.3693333333333335,
      "grad_norm": 0.1275254786014557,
      "learning_rate": 1.0191666666666668e-05,
      "loss": 0.0012,
      "step": 191080
    },
    {
      "epoch": 6.369666666666666,
      "grad_norm": 0.009409107267856598,
      "learning_rate": 1.0189583333333335e-05,
      "loss": 0.0018,
      "step": 191090
    },
    {
      "epoch": 6.37,
      "grad_norm": 0.3175176978111267,
      "learning_rate": 1.01875e-05,
      "loss": 0.0015,
      "step": 191100
    },
    {
      "epoch": 6.370333333333333,
      "grad_norm": 0.17154528200626373,
      "learning_rate": 1.0185416666666667e-05,
      "loss": 0.0016,
      "step": 191110
    },
    {
      "epoch": 6.370666666666667,
      "grad_norm": 0.11512282490730286,
      "learning_rate": 1.0183333333333333e-05,
      "loss": 0.0014,
      "step": 191120
    },
    {
      "epoch": 6.371,
      "grad_norm": 0.029738690704107285,
      "learning_rate": 1.018125e-05,
      "loss": 0.0017,
      "step": 191130
    },
    {
      "epoch": 6.371333333333333,
      "grad_norm": 0.3480303883552551,
      "learning_rate": 1.0179166666666667e-05,
      "loss": 0.0018,
      "step": 191140
    },
    {
      "epoch": 6.371666666666667,
      "grad_norm": 0.17123015224933624,
      "learning_rate": 1.0177083333333335e-05,
      "loss": 0.0019,
      "step": 191150
    },
    {
      "epoch": 6.372,
      "grad_norm": 0.40381118655204773,
      "learning_rate": 1.0175e-05,
      "loss": 0.0012,
      "step": 191160
    },
    {
      "epoch": 6.372333333333334,
      "grad_norm": 0.11462181061506271,
      "learning_rate": 1.0172916666666667e-05,
      "loss": 0.0018,
      "step": 191170
    },
    {
      "epoch": 6.3726666666666665,
      "grad_norm": 0.25749874114990234,
      "learning_rate": 1.0170833333333334e-05,
      "loss": 0.0013,
      "step": 191180
    },
    {
      "epoch": 6.373,
      "grad_norm": 0.2853754162788391,
      "learning_rate": 1.016875e-05,
      "loss": 0.0013,
      "step": 191190
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 0.4788048565387726,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0015,
      "step": 191200
    },
    {
      "epoch": 6.373666666666667,
      "grad_norm": 0.1997768133878708,
      "learning_rate": 1.0164583333333334e-05,
      "loss": 0.0022,
      "step": 191210
    },
    {
      "epoch": 6.374,
      "grad_norm": 0.3717332184314728,
      "learning_rate": 1.01625e-05,
      "loss": 0.0018,
      "step": 191220
    },
    {
      "epoch": 6.374333333333333,
      "grad_norm": 0.05874383822083473,
      "learning_rate": 1.0160416666666667e-05,
      "loss": 0.002,
      "step": 191230
    },
    {
      "epoch": 6.374666666666666,
      "grad_norm": 0.2008519023656845,
      "learning_rate": 1.0158333333333334e-05,
      "loss": 0.0012,
      "step": 191240
    },
    {
      "epoch": 6.375,
      "grad_norm": 0.03695514425635338,
      "learning_rate": 1.0156250000000001e-05,
      "loss": 0.0016,
      "step": 191250
    },
    {
      "epoch": 6.375333333333334,
      "grad_norm": 0.11549846827983856,
      "learning_rate": 1.0154166666666667e-05,
      "loss": 0.0014,
      "step": 191260
    },
    {
      "epoch": 6.375666666666667,
      "grad_norm": 0.3707627058029175,
      "learning_rate": 1.0152083333333334e-05,
      "loss": 0.0016,
      "step": 191270
    },
    {
      "epoch": 6.376,
      "grad_norm": 0.22650516033172607,
      "learning_rate": 1.0150000000000001e-05,
      "loss": 0.002,
      "step": 191280
    },
    {
      "epoch": 6.376333333333333,
      "grad_norm": 0.11571908742189407,
      "learning_rate": 1.0147916666666667e-05,
      "loss": 0.0023,
      "step": 191290
    },
    {
      "epoch": 6.376666666666667,
      "grad_norm": 0.2568868100643158,
      "learning_rate": 1.0145833333333334e-05,
      "loss": 0.0016,
      "step": 191300
    },
    {
      "epoch": 6.377,
      "grad_norm": 0.034159839153289795,
      "learning_rate": 1.0143750000000001e-05,
      "loss": 0.0015,
      "step": 191310
    },
    {
      "epoch": 6.3773333333333335,
      "grad_norm": 0.14322024583816528,
      "learning_rate": 1.0141666666666668e-05,
      "loss": 0.0022,
      "step": 191320
    },
    {
      "epoch": 6.377666666666666,
      "grad_norm": 0.23585987091064453,
      "learning_rate": 1.0139583333333334e-05,
      "loss": 0.002,
      "step": 191330
    },
    {
      "epoch": 6.378,
      "grad_norm": 0.1980598419904709,
      "learning_rate": 1.01375e-05,
      "loss": 0.0018,
      "step": 191340
    },
    {
      "epoch": 6.378333333333333,
      "grad_norm": 0.34250256419181824,
      "learning_rate": 1.0135416666666666e-05,
      "loss": 0.002,
      "step": 191350
    },
    {
      "epoch": 6.378666666666667,
      "grad_norm": 0.31448107957839966,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0013,
      "step": 191360
    },
    {
      "epoch": 6.379,
      "grad_norm": 0.17139503359794617,
      "learning_rate": 1.013125e-05,
      "loss": 0.0014,
      "step": 191370
    },
    {
      "epoch": 6.379333333333333,
      "grad_norm": 0.48021161556243896,
      "learning_rate": 1.0129166666666668e-05,
      "loss": 0.0021,
      "step": 191380
    },
    {
      "epoch": 6.379666666666667,
      "grad_norm": 0.22821886837482452,
      "learning_rate": 1.0127083333333335e-05,
      "loss": 0.0022,
      "step": 191390
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.12595976889133453,
      "learning_rate": 1.0125e-05,
      "loss": 0.0017,
      "step": 191400
    },
    {
      "epoch": 6.380333333333334,
      "grad_norm": 0.05799487605690956,
      "learning_rate": 1.0122916666666666e-05,
      "loss": 0.0017,
      "step": 191410
    },
    {
      "epoch": 6.3806666666666665,
      "grad_norm": 0.23076538741588593,
      "learning_rate": 1.0120833333333333e-05,
      "loss": 0.0025,
      "step": 191420
    },
    {
      "epoch": 6.381,
      "grad_norm": 0.05848931521177292,
      "learning_rate": 1.011875e-05,
      "loss": 0.0019,
      "step": 191430
    },
    {
      "epoch": 6.381333333333333,
      "grad_norm": 0.37273919582366943,
      "learning_rate": 1.0116666666666667e-05,
      "loss": 0.0017,
      "step": 191440
    },
    {
      "epoch": 6.381666666666667,
      "grad_norm": 0.17171427607536316,
      "learning_rate": 1.0114583333333335e-05,
      "loss": 0.0018,
      "step": 191450
    },
    {
      "epoch": 6.382,
      "grad_norm": 0.3137333393096924,
      "learning_rate": 1.0112500000000002e-05,
      "loss": 0.0018,
      "step": 191460
    },
    {
      "epoch": 6.382333333333333,
      "grad_norm": 0.5796896815299988,
      "learning_rate": 1.0110416666666667e-05,
      "loss": 0.0015,
      "step": 191470
    },
    {
      "epoch": 6.382666666666666,
      "grad_norm": 0.030913416296243668,
      "learning_rate": 1.0108333333333333e-05,
      "loss": 0.0015,
      "step": 191480
    },
    {
      "epoch": 6.383,
      "grad_norm": 0.11449132859706879,
      "learning_rate": 1.010625e-05,
      "loss": 0.0015,
      "step": 191490
    },
    {
      "epoch": 6.383333333333334,
      "grad_norm": 0.3594672381877899,
      "learning_rate": 1.0104166666666667e-05,
      "loss": 0.0025,
      "step": 191500
    },
    {
      "epoch": 6.383666666666667,
      "grad_norm": 0.11437086760997772,
      "learning_rate": 1.0102083333333334e-05,
      "loss": 0.0014,
      "step": 191510
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.059444356709718704,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.0016,
      "step": 191520
    },
    {
      "epoch": 6.384333333333333,
      "grad_norm": 0.17145852744579315,
      "learning_rate": 1.0097916666666667e-05,
      "loss": 0.0019,
      "step": 191530
    },
    {
      "epoch": 6.384666666666667,
      "grad_norm": 0.03336169943213463,
      "learning_rate": 1.0095833333333334e-05,
      "loss": 0.0022,
      "step": 191540
    },
    {
      "epoch": 6.385,
      "grad_norm": 0.05734117701649666,
      "learning_rate": 1.009375e-05,
      "loss": 0.0016,
      "step": 191550
    },
    {
      "epoch": 6.3853333333333335,
      "grad_norm": 0.20380733907222748,
      "learning_rate": 1.0091666666666667e-05,
      "loss": 0.0022,
      "step": 191560
    },
    {
      "epoch": 6.385666666666666,
      "grad_norm": 0.11438428610563278,
      "learning_rate": 1.0089583333333334e-05,
      "loss": 0.0017,
      "step": 191570
    },
    {
      "epoch": 6.386,
      "grad_norm": 0.4855411946773529,
      "learning_rate": 1.0087500000000001e-05,
      "loss": 0.0026,
      "step": 191580
    },
    {
      "epoch": 6.386333333333333,
      "grad_norm": 0.3143554925918579,
      "learning_rate": 1.0085416666666667e-05,
      "loss": 0.0016,
      "step": 191590
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 0.1144440546631813,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.002,
      "step": 191600
    },
    {
      "epoch": 6.3870000000000005,
      "grad_norm": 0.37196579575538635,
      "learning_rate": 1.0081250000000001e-05,
      "loss": 0.0018,
      "step": 191610
    },
    {
      "epoch": 6.387333333333333,
      "grad_norm": 0.04860486835241318,
      "learning_rate": 1.0079166666666667e-05,
      "loss": 0.0015,
      "step": 191620
    },
    {
      "epoch": 6.387666666666667,
      "grad_norm": 0.22859933972358704,
      "learning_rate": 1.0077083333333334e-05,
      "loss": 0.0015,
      "step": 191630
    },
    {
      "epoch": 6.388,
      "grad_norm": 0.2292747050523758,
      "learning_rate": 1.0075000000000001e-05,
      "loss": 0.0025,
      "step": 191640
    },
    {
      "epoch": 6.388333333333334,
      "grad_norm": 0.20002366602420807,
      "learning_rate": 1.0072916666666666e-05,
      "loss": 0.002,
      "step": 191650
    },
    {
      "epoch": 6.3886666666666665,
      "grad_norm": 0.19993990659713745,
      "learning_rate": 1.0070833333333334e-05,
      "loss": 0.0019,
      "step": 191660
    },
    {
      "epoch": 6.389,
      "grad_norm": 0.08616761118173599,
      "learning_rate": 1.006875e-05,
      "loss": 0.0014,
      "step": 191670
    },
    {
      "epoch": 6.389333333333333,
      "grad_norm": 0.17242148518562317,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0015,
      "step": 191680
    },
    {
      "epoch": 6.389666666666667,
      "grad_norm": 0.05842484161257744,
      "learning_rate": 1.0064583333333333e-05,
      "loss": 0.0015,
      "step": 191690
    },
    {
      "epoch": 6.39,
      "grad_norm": 0.05900615453720093,
      "learning_rate": 1.00625e-05,
      "loss": 0.0027,
      "step": 191700
    },
    {
      "epoch": 6.390333333333333,
      "grad_norm": 0.22928133606910706,
      "learning_rate": 1.0060416666666666e-05,
      "loss": 0.0017,
      "step": 191710
    },
    {
      "epoch": 6.390666666666666,
      "grad_norm": 0.14254045486450195,
      "learning_rate": 1.0058333333333333e-05,
      "loss": 0.0015,
      "step": 191720
    },
    {
      "epoch": 6.391,
      "grad_norm": 0.22871999442577362,
      "learning_rate": 1.005625e-05,
      "loss": 0.0015,
      "step": 191730
    },
    {
      "epoch": 6.391333333333334,
      "grad_norm": 0.3986114263534546,
      "learning_rate": 1.0054166666666668e-05,
      "loss": 0.0018,
      "step": 191740
    },
    {
      "epoch": 6.391666666666667,
      "grad_norm": 0.08688388764858246,
      "learning_rate": 1.0052083333333335e-05,
      "loss": 0.0022,
      "step": 191750
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.008755273185670376,
      "learning_rate": 1.005e-05,
      "loss": 0.0016,
      "step": 191760
    },
    {
      "epoch": 6.392333333333333,
      "grad_norm": 0.023520974442362785,
      "learning_rate": 1.0047916666666666e-05,
      "loss": 0.0016,
      "step": 191770
    },
    {
      "epoch": 6.392666666666667,
      "grad_norm": 0.08655581623315811,
      "learning_rate": 1.0045833333333333e-05,
      "loss": 0.0019,
      "step": 191780
    },
    {
      "epoch": 6.393,
      "grad_norm": 0.1452171504497528,
      "learning_rate": 1.004375e-05,
      "loss": 0.0014,
      "step": 191790
    },
    {
      "epoch": 6.3933333333333335,
      "grad_norm": 0.0117259556427598,
      "learning_rate": 1.0041666666666667e-05,
      "loss": 0.0019,
      "step": 191800
    },
    {
      "epoch": 6.393666666666666,
      "grad_norm": 0.11588986217975616,
      "learning_rate": 1.0039583333333335e-05,
      "loss": 0.0013,
      "step": 191810
    },
    {
      "epoch": 6.394,
      "grad_norm": 0.03428640589118004,
      "learning_rate": 1.0037500000000002e-05,
      "loss": 0.0015,
      "step": 191820
    },
    {
      "epoch": 6.394333333333333,
      "grad_norm": 0.15668705105781555,
      "learning_rate": 1.0035416666666667e-05,
      "loss": 0.0016,
      "step": 191830
    },
    {
      "epoch": 6.394666666666667,
      "grad_norm": 0.08647094666957855,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0028,
      "step": 191840
    },
    {
      "epoch": 6.395,
      "grad_norm": 0.08623822033405304,
      "learning_rate": 1.003125e-05,
      "loss": 0.0018,
      "step": 191850
    },
    {
      "epoch": 6.395333333333333,
      "grad_norm": 0.08648814260959625,
      "learning_rate": 1.0029166666666667e-05,
      "loss": 0.0014,
      "step": 191860
    },
    {
      "epoch": 6.395666666666667,
      "grad_norm": 0.11411090195178986,
      "learning_rate": 1.0027083333333334e-05,
      "loss": 0.0019,
      "step": 191870
    },
    {
      "epoch": 6.396,
      "grad_norm": 0.14295971393585205,
      "learning_rate": 1.0025000000000001e-05,
      "loss": 0.0015,
      "step": 191880
    },
    {
      "epoch": 6.396333333333334,
      "grad_norm": 0.14266829192638397,
      "learning_rate": 1.0022916666666667e-05,
      "loss": 0.0015,
      "step": 191890
    },
    {
      "epoch": 6.3966666666666665,
      "grad_norm": 0.4576755464076996,
      "learning_rate": 1.0020833333333334e-05,
      "loss": 0.0016,
      "step": 191900
    },
    {
      "epoch": 6.397,
      "grad_norm": 0.03537970036268234,
      "learning_rate": 1.001875e-05,
      "loss": 0.0027,
      "step": 191910
    },
    {
      "epoch": 6.397333333333333,
      "grad_norm": 0.012451231479644775,
      "learning_rate": 1.0016666666666667e-05,
      "loss": 0.0018,
      "step": 191920
    },
    {
      "epoch": 6.397666666666667,
      "grad_norm": 0.029961127787828445,
      "learning_rate": 1.0014583333333334e-05,
      "loss": 0.0025,
      "step": 191930
    },
    {
      "epoch": 6.398,
      "grad_norm": 0.032954420894384384,
      "learning_rate": 1.0012500000000001e-05,
      "loss": 0.0017,
      "step": 191940
    },
    {
      "epoch": 6.398333333333333,
      "grad_norm": 0.2856130003929138,
      "learning_rate": 1.0010416666666667e-05,
      "loss": 0.0023,
      "step": 191950
    },
    {
      "epoch": 6.398666666666666,
      "grad_norm": 0.0582721121609211,
      "learning_rate": 1.0008333333333334e-05,
      "loss": 0.0015,
      "step": 191960
    },
    {
      "epoch": 6.399,
      "grad_norm": 0.08590985834598541,
      "learning_rate": 1.0006250000000001e-05,
      "loss": 0.0016,
      "step": 191970
    },
    {
      "epoch": 6.399333333333333,
      "grad_norm": 0.2208084762096405,
      "learning_rate": 1.0004166666666666e-05,
      "loss": 0.0017,
      "step": 191980
    },
    {
      "epoch": 6.399666666666667,
      "grad_norm": 0.5131749510765076,
      "learning_rate": 1.0002083333333334e-05,
      "loss": 0.0019,
      "step": 191990
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.1445929855108261,
      "learning_rate": 1e-05,
      "loss": 0.0023,
      "step": 192000
    },
    {
      "epoch": 6.400333333333333,
      "grad_norm": 0.12451721727848053,
      "learning_rate": 9.997916666666666e-06,
      "loss": 0.0015,
      "step": 192010
    },
    {
      "epoch": 6.400666666666667,
      "grad_norm": 0.3422914147377014,
      "learning_rate": 9.995833333333333e-06,
      "loss": 0.0017,
      "step": 192020
    },
    {
      "epoch": 6.401,
      "grad_norm": 0.045235998928546906,
      "learning_rate": 9.99375e-06,
      "loss": 0.0029,
      "step": 192030
    },
    {
      "epoch": 6.4013333333333335,
      "grad_norm": 0.05802256986498833,
      "learning_rate": 9.991666666666668e-06,
      "loss": 0.0022,
      "step": 192040
    },
    {
      "epoch": 6.401666666666666,
      "grad_norm": 0.28529635071754456,
      "learning_rate": 9.989583333333333e-06,
      "loss": 0.0016,
      "step": 192050
    },
    {
      "epoch": 6.402,
      "grad_norm": 0.34247246384620667,
      "learning_rate": 9.9875e-06,
      "loss": 0.0019,
      "step": 192060
    },
    {
      "epoch": 6.402333333333333,
      "grad_norm": 0.0062394230626523495,
      "learning_rate": 9.985416666666666e-06,
      "loss": 0.0013,
      "step": 192070
    },
    {
      "epoch": 6.402666666666667,
      "grad_norm": 0.3707534074783325,
      "learning_rate": 9.983333333333333e-06,
      "loss": 0.0014,
      "step": 192080
    },
    {
      "epoch": 6.4030000000000005,
      "grad_norm": 0.05712958797812462,
      "learning_rate": 9.98125e-06,
      "loss": 0.0014,
      "step": 192090
    },
    {
      "epoch": 6.403333333333333,
      "grad_norm": 0.1712741255760193,
      "learning_rate": 9.979166666666668e-06,
      "loss": 0.002,
      "step": 192100
    },
    {
      "epoch": 6.403666666666667,
      "grad_norm": 0.030261237174272537,
      "learning_rate": 9.977083333333335e-06,
      "loss": 0.0025,
      "step": 192110
    },
    {
      "epoch": 6.404,
      "grad_norm": 0.03254630044102669,
      "learning_rate": 9.975e-06,
      "loss": 0.0013,
      "step": 192120
    },
    {
      "epoch": 6.404333333333334,
      "grad_norm": 0.08371478319168091,
      "learning_rate": 9.972916666666666e-06,
      "loss": 0.0018,
      "step": 192130
    },
    {
      "epoch": 6.4046666666666665,
      "grad_norm": 0.2855069637298584,
      "learning_rate": 9.970833333333333e-06,
      "loss": 0.0015,
      "step": 192140
    },
    {
      "epoch": 6.405,
      "grad_norm": 0.059758737683296204,
      "learning_rate": 9.96875e-06,
      "loss": 0.0011,
      "step": 192150
    },
    {
      "epoch": 6.405333333333333,
      "grad_norm": 0.28706157207489014,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0014,
      "step": 192160
    },
    {
      "epoch": 6.405666666666667,
      "grad_norm": 0.342674195766449,
      "learning_rate": 9.964583333333334e-06,
      "loss": 0.0014,
      "step": 192170
    },
    {
      "epoch": 6.406,
      "grad_norm": 0.2281871885061264,
      "learning_rate": 9.962500000000002e-06,
      "loss": 0.0011,
      "step": 192180
    },
    {
      "epoch": 6.406333333333333,
      "grad_norm": 0.14362189173698425,
      "learning_rate": 9.960416666666667e-06,
      "loss": 0.0014,
      "step": 192190
    },
    {
      "epoch": 6.406666666666666,
      "grad_norm": 0.11460918933153152,
      "learning_rate": 9.958333333333333e-06,
      "loss": 0.002,
      "step": 192200
    },
    {
      "epoch": 6.407,
      "grad_norm": 0.36983928084373474,
      "learning_rate": 9.95625e-06,
      "loss": 0.002,
      "step": 192210
    },
    {
      "epoch": 6.407333333333334,
      "grad_norm": 0.02948784828186035,
      "learning_rate": 9.954166666666667e-06,
      "loss": 0.0014,
      "step": 192220
    },
    {
      "epoch": 6.407666666666667,
      "grad_norm": 0.1716306209564209,
      "learning_rate": 9.952083333333334e-06,
      "loss": 0.0022,
      "step": 192230
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.22847676277160645,
      "learning_rate": 9.950000000000001e-06,
      "loss": 0.0017,
      "step": 192240
    },
    {
      "epoch": 6.408333333333333,
      "grad_norm": 0.14356660842895508,
      "learning_rate": 9.947916666666667e-06,
      "loss": 0.0019,
      "step": 192250
    },
    {
      "epoch": 6.408666666666667,
      "grad_norm": 0.6852217316627502,
      "learning_rate": 9.945833333333334e-06,
      "loss": 0.0021,
      "step": 192260
    },
    {
      "epoch": 6.409,
      "grad_norm": 0.2573200762271881,
      "learning_rate": 9.94375e-06,
      "loss": 0.0011,
      "step": 192270
    },
    {
      "epoch": 6.4093333333333335,
      "grad_norm": 0.05766947939991951,
      "learning_rate": 9.941666666666667e-06,
      "loss": 0.0019,
      "step": 192280
    },
    {
      "epoch": 6.409666666666666,
      "grad_norm": 0.2847974896430969,
      "learning_rate": 9.939583333333334e-06,
      "loss": 0.0018,
      "step": 192290
    },
    {
      "epoch": 6.41,
      "grad_norm": 0.2149239480495453,
      "learning_rate": 9.937500000000001e-06,
      "loss": 0.0016,
      "step": 192300
    },
    {
      "epoch": 6.410333333333333,
      "grad_norm": 0.37292203307151794,
      "learning_rate": 9.935416666666666e-06,
      "loss": 0.0017,
      "step": 192310
    },
    {
      "epoch": 6.410666666666667,
      "grad_norm": 0.14323939383029938,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0016,
      "step": 192320
    },
    {
      "epoch": 6.411,
      "grad_norm": 0.08637315779924393,
      "learning_rate": 9.93125e-06,
      "loss": 0.0015,
      "step": 192330
    },
    {
      "epoch": 6.411333333333333,
      "grad_norm": 0.00807487778365612,
      "learning_rate": 9.929166666666666e-06,
      "loss": 0.0018,
      "step": 192340
    },
    {
      "epoch": 6.411666666666667,
      "grad_norm": 0.2002721130847931,
      "learning_rate": 9.927083333333334e-06,
      "loss": 0.0011,
      "step": 192350
    },
    {
      "epoch": 6.412,
      "grad_norm": 0.20005328953266144,
      "learning_rate": 9.925e-06,
      "loss": 0.0014,
      "step": 192360
    },
    {
      "epoch": 6.412333333333334,
      "grad_norm": 0.11449045687913895,
      "learning_rate": 9.922916666666666e-06,
      "loss": 0.0024,
      "step": 192370
    },
    {
      "epoch": 6.4126666666666665,
      "grad_norm": 0.34279030561447144,
      "learning_rate": 9.920833333333333e-06,
      "loss": 0.0021,
      "step": 192380
    },
    {
      "epoch": 6.413,
      "grad_norm": 0.285393625497818,
      "learning_rate": 9.91875e-06,
      "loss": 0.0018,
      "step": 192390
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 0.3147662281990051,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.0021,
      "step": 192400
    },
    {
      "epoch": 6.413666666666667,
      "grad_norm": 0.08629664033651352,
      "learning_rate": 9.914583333333335e-06,
      "loss": 0.0013,
      "step": 192410
    },
    {
      "epoch": 6.414,
      "grad_norm": 0.2565135061740875,
      "learning_rate": 9.9125e-06,
      "loss": 0.0011,
      "step": 192420
    },
    {
      "epoch": 6.414333333333333,
      "grad_norm": 0.22847136855125427,
      "learning_rate": 9.910416666666666e-06,
      "loss": 0.0018,
      "step": 192430
    },
    {
      "epoch": 6.414666666666666,
      "grad_norm": 0.4852053225040436,
      "learning_rate": 9.908333333333333e-06,
      "loss": 0.0018,
      "step": 192440
    },
    {
      "epoch": 6.415,
      "grad_norm": 0.08771715313196182,
      "learning_rate": 9.90625e-06,
      "loss": 0.0023,
      "step": 192450
    },
    {
      "epoch": 6.415333333333333,
      "grad_norm": 0.2857684791088104,
      "learning_rate": 9.904166666666667e-06,
      "loss": 0.0028,
      "step": 192460
    },
    {
      "epoch": 6.415666666666667,
      "grad_norm": 0.4869052469730377,
      "learning_rate": 9.902083333333335e-06,
      "loss": 0.0014,
      "step": 192470
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.0576302744448185,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0023,
      "step": 192480
    },
    {
      "epoch": 6.416333333333333,
      "grad_norm": 0.3141213357448578,
      "learning_rate": 9.897916666666667e-06,
      "loss": 0.0013,
      "step": 192490
    },
    {
      "epoch": 6.416666666666667,
      "grad_norm": 0.25654682517051697,
      "learning_rate": 9.895833333333333e-06,
      "loss": 0.0017,
      "step": 192500
    },
    {
      "epoch": 6.417,
      "grad_norm": 0.2281457632780075,
      "learning_rate": 9.89375e-06,
      "loss": 0.0017,
      "step": 192510
    },
    {
      "epoch": 6.417333333333334,
      "grad_norm": 0.39927929639816284,
      "learning_rate": 9.891666666666667e-06,
      "loss": 0.0023,
      "step": 192520
    },
    {
      "epoch": 6.417666666666666,
      "grad_norm": 0.031853146851062775,
      "learning_rate": 9.889583333333334e-06,
      "loss": 0.0014,
      "step": 192530
    },
    {
      "epoch": 6.418,
      "grad_norm": 0.10915136337280273,
      "learning_rate": 9.887500000000001e-06,
      "loss": 0.0015,
      "step": 192540
    },
    {
      "epoch": 6.418333333333333,
      "grad_norm": 0.22807250916957855,
      "learning_rate": 9.885416666666669e-06,
      "loss": 0.0021,
      "step": 192550
    },
    {
      "epoch": 6.418666666666667,
      "grad_norm": 0.02944212779402733,
      "learning_rate": 9.883333333333334e-06,
      "loss": 0.0016,
      "step": 192560
    },
    {
      "epoch": 6.419,
      "grad_norm": 0.19941335916519165,
      "learning_rate": 9.88125e-06,
      "loss": 0.0016,
      "step": 192570
    },
    {
      "epoch": 6.419333333333333,
      "grad_norm": 0.25946858525276184,
      "learning_rate": 9.879166666666667e-06,
      "loss": 0.0022,
      "step": 192580
    },
    {
      "epoch": 6.419666666666667,
      "grad_norm": 0.2735031843185425,
      "learning_rate": 9.877083333333334e-06,
      "loss": 0.0017,
      "step": 192590
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.0303913913667202,
      "learning_rate": 9.875000000000001e-06,
      "loss": 0.002,
      "step": 192600
    },
    {
      "epoch": 6.420333333333334,
      "grad_norm": 0.4146296977996826,
      "learning_rate": 9.872916666666668e-06,
      "loss": 0.002,
      "step": 192610
    },
    {
      "epoch": 6.4206666666666665,
      "grad_norm": 0.2375982701778412,
      "learning_rate": 9.870833333333334e-06,
      "loss": 0.0015,
      "step": 192620
    },
    {
      "epoch": 6.421,
      "grad_norm": 0.11492788791656494,
      "learning_rate": 9.868750000000001e-06,
      "loss": 0.0014,
      "step": 192630
    },
    {
      "epoch": 6.421333333333333,
      "grad_norm": 0.5052591562271118,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0025,
      "step": 192640
    },
    {
      "epoch": 6.421666666666667,
      "grad_norm": 0.20410320162773132,
      "learning_rate": 9.864583333333334e-06,
      "loss": 0.0016,
      "step": 192650
    },
    {
      "epoch": 6.422,
      "grad_norm": 0.058759331703186035,
      "learning_rate": 9.862500000000001e-06,
      "loss": 0.0016,
      "step": 192660
    },
    {
      "epoch": 6.4223333333333334,
      "grad_norm": 0.6423763632774353,
      "learning_rate": 9.860416666666668e-06,
      "loss": 0.0013,
      "step": 192670
    },
    {
      "epoch": 6.422666666666666,
      "grad_norm": 0.5138222575187683,
      "learning_rate": 9.858333333333334e-06,
      "loss": 0.0016,
      "step": 192680
    },
    {
      "epoch": 6.423,
      "grad_norm": 0.19948208332061768,
      "learning_rate": 9.85625e-06,
      "loss": 0.0025,
      "step": 192690
    },
    {
      "epoch": 6.423333333333334,
      "grad_norm": 0.14349985122680664,
      "learning_rate": 9.854166666666668e-06,
      "loss": 0.0011,
      "step": 192700
    },
    {
      "epoch": 6.423666666666667,
      "grad_norm": 0.11579983681440353,
      "learning_rate": 9.852083333333333e-06,
      "loss": 0.0019,
      "step": 192710
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.19573041796684265,
      "learning_rate": 9.85e-06,
      "loss": 0.0019,
      "step": 192720
    },
    {
      "epoch": 6.424333333333333,
      "grad_norm": 0.14314007759094238,
      "learning_rate": 9.847916666666668e-06,
      "loss": 0.0016,
      "step": 192730
    },
    {
      "epoch": 6.424666666666667,
      "grad_norm": 0.37088459730148315,
      "learning_rate": 9.845833333333333e-06,
      "loss": 0.0013,
      "step": 192740
    },
    {
      "epoch": 6.425,
      "grad_norm": 0.11405764520168304,
      "learning_rate": 9.84375e-06,
      "loss": 0.0023,
      "step": 192750
    },
    {
      "epoch": 6.425333333333334,
      "grad_norm": 0.1142071858048439,
      "learning_rate": 9.841666666666668e-06,
      "loss": 0.0017,
      "step": 192760
    },
    {
      "epoch": 6.425666666666666,
      "grad_norm": 0.03510615974664688,
      "learning_rate": 9.839583333333335e-06,
      "loss": 0.0019,
      "step": 192770
    },
    {
      "epoch": 6.426,
      "grad_norm": 0.058088190853595734,
      "learning_rate": 9.8375e-06,
      "loss": 0.0014,
      "step": 192780
    },
    {
      "epoch": 6.426333333333333,
      "grad_norm": 0.3999272584915161,
      "learning_rate": 9.835416666666667e-06,
      "loss": 0.0016,
      "step": 192790
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 0.20686717331409454,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0019,
      "step": 192800
    },
    {
      "epoch": 6.427,
      "grad_norm": 0.029416171833872795,
      "learning_rate": 9.83125e-06,
      "loss": 0.0018,
      "step": 192810
    },
    {
      "epoch": 6.427333333333333,
      "grad_norm": 0.05728597193956375,
      "learning_rate": 9.829166666666667e-06,
      "loss": 0.0017,
      "step": 192820
    },
    {
      "epoch": 6.427666666666667,
      "grad_norm": 0.08672414720058441,
      "learning_rate": 9.827083333333334e-06,
      "loss": 0.0017,
      "step": 192830
    },
    {
      "epoch": 6.428,
      "grad_norm": 0.2567645311355591,
      "learning_rate": 9.825000000000002e-06,
      "loss": 0.0015,
      "step": 192840
    },
    {
      "epoch": 6.428333333333334,
      "grad_norm": 0.36768838763237,
      "learning_rate": 9.822916666666667e-06,
      "loss": 0.0015,
      "step": 192850
    },
    {
      "epoch": 6.4286666666666665,
      "grad_norm": 0.1710212379693985,
      "learning_rate": 9.820833333333333e-06,
      "loss": 0.0014,
      "step": 192860
    },
    {
      "epoch": 6.429,
      "grad_norm": 0.20011068880558014,
      "learning_rate": 9.81875e-06,
      "loss": 0.0021,
      "step": 192870
    },
    {
      "epoch": 6.429333333333333,
      "grad_norm": 0.5706567168235779,
      "learning_rate": 9.816666666666667e-06,
      "loss": 0.0014,
      "step": 192880
    },
    {
      "epoch": 6.429666666666667,
      "grad_norm": 0.4307056963443756,
      "learning_rate": 9.814583333333334e-06,
      "loss": 0.0019,
      "step": 192890
    },
    {
      "epoch": 6.43,
      "grad_norm": 0.08592269569635391,
      "learning_rate": 9.812500000000001e-06,
      "loss": 0.0013,
      "step": 192900
    },
    {
      "epoch": 6.4303333333333335,
      "grad_norm": 0.38939809799194336,
      "learning_rate": 9.810416666666669e-06,
      "loss": 0.0017,
      "step": 192910
    },
    {
      "epoch": 6.430666666666666,
      "grad_norm": 0.14326436817646027,
      "learning_rate": 9.808333333333334e-06,
      "loss": 0.0018,
      "step": 192920
    },
    {
      "epoch": 6.431,
      "grad_norm": 0.17347076535224915,
      "learning_rate": 9.80625e-06,
      "loss": 0.0017,
      "step": 192930
    },
    {
      "epoch": 6.431333333333333,
      "grad_norm": 0.057737089693546295,
      "learning_rate": 9.804166666666667e-06,
      "loss": 0.0014,
      "step": 192940
    },
    {
      "epoch": 6.431666666666667,
      "grad_norm": 0.23505763709545135,
      "learning_rate": 9.802083333333334e-06,
      "loss": 0.0014,
      "step": 192950
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.03496899828314781,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0021,
      "step": 192960
    },
    {
      "epoch": 6.432333333333333,
      "grad_norm": 0.313953697681427,
      "learning_rate": 9.797916666666668e-06,
      "loss": 0.0016,
      "step": 192970
    },
    {
      "epoch": 6.432666666666667,
      "grad_norm": 0.05788585916161537,
      "learning_rate": 9.795833333333334e-06,
      "loss": 0.0015,
      "step": 192980
    },
    {
      "epoch": 6.433,
      "grad_norm": 0.2575676441192627,
      "learning_rate": 9.793750000000001e-06,
      "loss": 0.0019,
      "step": 192990
    },
    {
      "epoch": 6.433333333333334,
      "grad_norm": 0.3427746891975403,
      "learning_rate": 9.791666666666666e-06,
      "loss": 0.0015,
      "step": 193000
    },
    {
      "epoch": 6.433666666666666,
      "grad_norm": 0.057288385927677155,
      "learning_rate": 9.789583333333334e-06,
      "loss": 0.0021,
      "step": 193010
    },
    {
      "epoch": 6.434,
      "grad_norm": 0.14410798251628876,
      "learning_rate": 9.7875e-06,
      "loss": 0.002,
      "step": 193020
    },
    {
      "epoch": 6.434333333333333,
      "grad_norm": 0.3428994119167328,
      "learning_rate": 9.785416666666668e-06,
      "loss": 0.0014,
      "step": 193030
    },
    {
      "epoch": 6.434666666666667,
      "grad_norm": 0.031972311437129974,
      "learning_rate": 9.783333333333333e-06,
      "loss": 0.0014,
      "step": 193040
    },
    {
      "epoch": 6.435,
      "grad_norm": 0.3071121871471405,
      "learning_rate": 9.78125e-06,
      "loss": 0.0022,
      "step": 193050
    },
    {
      "epoch": 6.435333333333333,
      "grad_norm": 0.029410911723971367,
      "learning_rate": 9.779166666666668e-06,
      "loss": 0.0015,
      "step": 193060
    },
    {
      "epoch": 6.435666666666666,
      "grad_norm": 0.1169964000582695,
      "learning_rate": 9.777083333333333e-06,
      "loss": 0.0019,
      "step": 193070
    },
    {
      "epoch": 6.436,
      "grad_norm": 0.4766395688056946,
      "learning_rate": 9.775e-06,
      "loss": 0.0023,
      "step": 193080
    },
    {
      "epoch": 6.436333333333334,
      "grad_norm": 0.514009952545166,
      "learning_rate": 9.772916666666668e-06,
      "loss": 0.0016,
      "step": 193090
    },
    {
      "epoch": 6.4366666666666665,
      "grad_norm": 0.029811454936861992,
      "learning_rate": 9.770833333333333e-06,
      "loss": 0.0012,
      "step": 193100
    },
    {
      "epoch": 6.437,
      "grad_norm": 0.2310492992401123,
      "learning_rate": 9.76875e-06,
      "loss": 0.0023,
      "step": 193110
    },
    {
      "epoch": 6.437333333333333,
      "grad_norm": 0.3134608864784241,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.0015,
      "step": 193120
    },
    {
      "epoch": 6.437666666666667,
      "grad_norm": 0.7763717770576477,
      "learning_rate": 9.764583333333335e-06,
      "loss": 0.0025,
      "step": 193130
    },
    {
      "epoch": 6.438,
      "grad_norm": 0.1205637976527214,
      "learning_rate": 9.7625e-06,
      "loss": 0.0015,
      "step": 193140
    },
    {
      "epoch": 6.4383333333333335,
      "grad_norm": 0.03354578837752342,
      "learning_rate": 9.760416666666667e-06,
      "loss": 0.0014,
      "step": 193150
    },
    {
      "epoch": 6.438666666666666,
      "grad_norm": 0.03031258098781109,
      "learning_rate": 9.758333333333333e-06,
      "loss": 0.0027,
      "step": 193160
    },
    {
      "epoch": 6.439,
      "grad_norm": 0.06039470434188843,
      "learning_rate": 9.75625e-06,
      "loss": 0.0018,
      "step": 193170
    },
    {
      "epoch": 6.439333333333334,
      "grad_norm": 0.11404652148485184,
      "learning_rate": 9.754166666666667e-06,
      "loss": 0.0018,
      "step": 193180
    },
    {
      "epoch": 6.439666666666667,
      "grad_norm": 0.05869623273611069,
      "learning_rate": 9.752083333333334e-06,
      "loss": 0.002,
      "step": 193190
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.11500829458236694,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.002,
      "step": 193200
    },
    {
      "epoch": 6.440333333333333,
      "grad_norm": 0.029621025547385216,
      "learning_rate": 9.747916666666667e-06,
      "loss": 0.0017,
      "step": 193210
    },
    {
      "epoch": 6.440666666666667,
      "grad_norm": 0.20034456253051758,
      "learning_rate": 9.745833333333332e-06,
      "loss": 0.0016,
      "step": 193220
    },
    {
      "epoch": 6.441,
      "grad_norm": 0.14257016777992249,
      "learning_rate": 9.74375e-06,
      "loss": 0.0016,
      "step": 193230
    },
    {
      "epoch": 6.441333333333334,
      "grad_norm": 0.22811414301395416,
      "learning_rate": 9.741666666666667e-06,
      "loss": 0.0016,
      "step": 193240
    },
    {
      "epoch": 6.441666666666666,
      "grad_norm": 0.14339709281921387,
      "learning_rate": 9.739583333333334e-06,
      "loss": 0.0017,
      "step": 193250
    },
    {
      "epoch": 6.442,
      "grad_norm": 0.20059041678905487,
      "learning_rate": 9.737500000000001e-06,
      "loss": 0.0018,
      "step": 193260
    },
    {
      "epoch": 6.442333333333333,
      "grad_norm": 0.08589217811822891,
      "learning_rate": 9.735416666666668e-06,
      "loss": 0.0015,
      "step": 193270
    },
    {
      "epoch": 6.442666666666667,
      "grad_norm": 0.34218546748161316,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0014,
      "step": 193280
    },
    {
      "epoch": 6.443,
      "grad_norm": 0.005911197979003191,
      "learning_rate": 9.73125e-06,
      "loss": 0.0018,
      "step": 193290
    },
    {
      "epoch": 6.443333333333333,
      "grad_norm": 0.030731361359357834,
      "learning_rate": 9.729166666666667e-06,
      "loss": 0.0017,
      "step": 193300
    },
    {
      "epoch": 6.443666666666667,
      "grad_norm": 0.31404241919517517,
      "learning_rate": 9.727083333333334e-06,
      "loss": 0.0016,
      "step": 193310
    },
    {
      "epoch": 6.444,
      "grad_norm": 0.08580069243907928,
      "learning_rate": 9.725000000000001e-06,
      "loss": 0.0019,
      "step": 193320
    },
    {
      "epoch": 6.444333333333334,
      "grad_norm": 0.029848754405975342,
      "learning_rate": 9.722916666666668e-06,
      "loss": 0.0015,
      "step": 193330
    },
    {
      "epoch": 6.4446666666666665,
      "grad_norm": 0.08607763051986694,
      "learning_rate": 9.720833333333334e-06,
      "loss": 0.0017,
      "step": 193340
    },
    {
      "epoch": 6.445,
      "grad_norm": 0.2566944658756256,
      "learning_rate": 9.71875e-06,
      "loss": 0.0012,
      "step": 193350
    },
    {
      "epoch": 6.445333333333333,
      "grad_norm": 0.34081366658210754,
      "learning_rate": 9.716666666666666e-06,
      "loss": 0.0021,
      "step": 193360
    },
    {
      "epoch": 6.445666666666667,
      "grad_norm": 0.14298760890960693,
      "learning_rate": 9.714583333333333e-06,
      "loss": 0.0019,
      "step": 193370
    },
    {
      "epoch": 6.446,
      "grad_norm": 0.20008933544158936,
      "learning_rate": 9.7125e-06,
      "loss": 0.002,
      "step": 193380
    },
    {
      "epoch": 6.4463333333333335,
      "grad_norm": 0.05839640274643898,
      "learning_rate": 9.710416666666668e-06,
      "loss": 0.0017,
      "step": 193390
    },
    {
      "epoch": 6.446666666666666,
      "grad_norm": 0.11532340198755264,
      "learning_rate": 9.708333333333333e-06,
      "loss": 0.0016,
      "step": 193400
    },
    {
      "epoch": 6.447,
      "grad_norm": 0.22839690744876862,
      "learning_rate": 9.70625e-06,
      "loss": 0.0017,
      "step": 193410
    },
    {
      "epoch": 6.447333333333333,
      "grad_norm": 0.00719320448115468,
      "learning_rate": 9.704166666666668e-06,
      "loss": 0.0012,
      "step": 193420
    },
    {
      "epoch": 6.447666666666667,
      "grad_norm": 0.14361007511615753,
      "learning_rate": 9.702083333333333e-06,
      "loss": 0.0019,
      "step": 193430
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.08622445911169052,
      "learning_rate": 9.7e-06,
      "loss": 0.0022,
      "step": 193440
    },
    {
      "epoch": 6.448333333333333,
      "grad_norm": 0.11458448320627213,
      "learning_rate": 9.697916666666667e-06,
      "loss": 0.0016,
      "step": 193450
    },
    {
      "epoch": 6.448666666666667,
      "grad_norm": 0.035127248615026474,
      "learning_rate": 9.695833333333333e-06,
      "loss": 0.0015,
      "step": 193460
    },
    {
      "epoch": 6.449,
      "grad_norm": 0.2444709688425064,
      "learning_rate": 9.69375e-06,
      "loss": 0.0024,
      "step": 193470
    },
    {
      "epoch": 6.449333333333334,
      "grad_norm": 0.229140043258667,
      "learning_rate": 9.691666666666667e-06,
      "loss": 0.0019,
      "step": 193480
    },
    {
      "epoch": 6.449666666666666,
      "grad_norm": 0.08678948879241943,
      "learning_rate": 9.689583333333335e-06,
      "loss": 0.0016,
      "step": 193490
    },
    {
      "epoch": 6.45,
      "grad_norm": 0.11430125683546066,
      "learning_rate": 9.6875e-06,
      "loss": 0.0016,
      "step": 193500
    },
    {
      "epoch": 6.450333333333333,
      "grad_norm": 0.02997920662164688,
      "learning_rate": 9.685416666666667e-06,
      "loss": 0.0018,
      "step": 193510
    },
    {
      "epoch": 6.450666666666667,
      "grad_norm": 0.25732675194740295,
      "learning_rate": 9.683333333333333e-06,
      "loss": 0.002,
      "step": 193520
    },
    {
      "epoch": 6.451,
      "grad_norm": 0.08596132695674896,
      "learning_rate": 9.68125e-06,
      "loss": 0.0013,
      "step": 193530
    },
    {
      "epoch": 6.451333333333333,
      "grad_norm": 0.14037784934043884,
      "learning_rate": 9.679166666666667e-06,
      "loss": 0.0015,
      "step": 193540
    },
    {
      "epoch": 6.451666666666666,
      "grad_norm": 0.007955091074109077,
      "learning_rate": 9.677083333333334e-06,
      "loss": 0.0025,
      "step": 193550
    },
    {
      "epoch": 6.452,
      "grad_norm": 0.030930835753679276,
      "learning_rate": 9.675000000000001e-06,
      "loss": 0.0017,
      "step": 193560
    },
    {
      "epoch": 6.452333333333334,
      "grad_norm": 0.058980610221624374,
      "learning_rate": 9.672916666666667e-06,
      "loss": 0.002,
      "step": 193570
    },
    {
      "epoch": 6.4526666666666666,
      "grad_norm": 0.02987297624349594,
      "learning_rate": 9.670833333333332e-06,
      "loss": 0.0021,
      "step": 193580
    },
    {
      "epoch": 6.453,
      "grad_norm": 0.05728437006473541,
      "learning_rate": 9.66875e-06,
      "loss": 0.0018,
      "step": 193590
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.11441091448068619,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0025,
      "step": 193600
    },
    {
      "epoch": 6.453666666666667,
      "grad_norm": 0.030015423893928528,
      "learning_rate": 9.664583333333334e-06,
      "loss": 0.0015,
      "step": 193610
    },
    {
      "epoch": 6.454,
      "grad_norm": 0.1439722627401352,
      "learning_rate": 9.662500000000001e-06,
      "loss": 0.0019,
      "step": 193620
    },
    {
      "epoch": 6.4543333333333335,
      "grad_norm": 0.029167218133807182,
      "learning_rate": 9.660416666666668e-06,
      "loss": 0.0023,
      "step": 193630
    },
    {
      "epoch": 6.454666666666666,
      "grad_norm": 0.2851153016090393,
      "learning_rate": 9.658333333333334e-06,
      "loss": 0.0021,
      "step": 193640
    },
    {
      "epoch": 6.455,
      "grad_norm": 0.11460435390472412,
      "learning_rate": 9.65625e-06,
      "loss": 0.0019,
      "step": 193650
    },
    {
      "epoch": 6.455333333333333,
      "grad_norm": 0.17182600498199463,
      "learning_rate": 9.654166666666666e-06,
      "loss": 0.0015,
      "step": 193660
    },
    {
      "epoch": 6.455666666666667,
      "grad_norm": 0.25764042139053345,
      "learning_rate": 9.652083333333334e-06,
      "loss": 0.0018,
      "step": 193670
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.08574045449495316,
      "learning_rate": 9.65e-06,
      "loss": 0.0016,
      "step": 193680
    },
    {
      "epoch": 6.456333333333333,
      "grad_norm": 0.029549771919846535,
      "learning_rate": 9.647916666666668e-06,
      "loss": 0.0017,
      "step": 193690
    },
    {
      "epoch": 6.456666666666667,
      "grad_norm": 0.14307589828968048,
      "learning_rate": 9.645833333333333e-06,
      "loss": 0.0021,
      "step": 193700
    },
    {
      "epoch": 6.457,
      "grad_norm": 0.1999635547399521,
      "learning_rate": 9.64375e-06,
      "loss": 0.0017,
      "step": 193710
    },
    {
      "epoch": 6.457333333333334,
      "grad_norm": 0.19974075257778168,
      "learning_rate": 9.641666666666666e-06,
      "loss": 0.0018,
      "step": 193720
    },
    {
      "epoch": 6.457666666666666,
      "grad_norm": 0.14328700304031372,
      "learning_rate": 9.639583333333333e-06,
      "loss": 0.0015,
      "step": 193730
    },
    {
      "epoch": 6.458,
      "grad_norm": 0.08856050670146942,
      "learning_rate": 9.6375e-06,
      "loss": 0.0018,
      "step": 193740
    },
    {
      "epoch": 6.458333333333333,
      "grad_norm": 0.05816962197422981,
      "learning_rate": 9.635416666666668e-06,
      "loss": 0.0018,
      "step": 193750
    },
    {
      "epoch": 6.458666666666667,
      "grad_norm": 0.11666423827409744,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0016,
      "step": 193760
    },
    {
      "epoch": 6.459,
      "grad_norm": 0.07757379114627838,
      "learning_rate": 9.63125e-06,
      "loss": 0.0013,
      "step": 193770
    },
    {
      "epoch": 6.459333333333333,
      "grad_norm": 0.03153667971491814,
      "learning_rate": 9.629166666666668e-06,
      "loss": 0.0014,
      "step": 193780
    },
    {
      "epoch": 6.459666666666667,
      "grad_norm": 0.030962971970438957,
      "learning_rate": 9.627083333333333e-06,
      "loss": 0.0019,
      "step": 193790
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.11442410200834274,
      "learning_rate": 9.625e-06,
      "loss": 0.0017,
      "step": 193800
    },
    {
      "epoch": 6.460333333333334,
      "grad_norm": 0.1775316745042801,
      "learning_rate": 9.622916666666667e-06,
      "loss": 0.002,
      "step": 193810
    },
    {
      "epoch": 6.460666666666667,
      "grad_norm": 0.06057559698820114,
      "learning_rate": 9.620833333333335e-06,
      "loss": 0.0017,
      "step": 193820
    },
    {
      "epoch": 6.461,
      "grad_norm": 0.04244410619139671,
      "learning_rate": 9.61875e-06,
      "loss": 0.002,
      "step": 193830
    },
    {
      "epoch": 6.461333333333333,
      "grad_norm": 0.2989494800567627,
      "learning_rate": 9.616666666666667e-06,
      "loss": 0.0023,
      "step": 193840
    },
    {
      "epoch": 6.461666666666667,
      "grad_norm": 0.2855384647846222,
      "learning_rate": 9.614583333333334e-06,
      "loss": 0.0019,
      "step": 193850
    },
    {
      "epoch": 6.462,
      "grad_norm": 0.17153891921043396,
      "learning_rate": 9.6125e-06,
      "loss": 0.0012,
      "step": 193860
    },
    {
      "epoch": 6.4623333333333335,
      "grad_norm": 0.031076326966285706,
      "learning_rate": 9.610416666666667e-06,
      "loss": 0.0019,
      "step": 193870
    },
    {
      "epoch": 6.462666666666666,
      "grad_norm": 0.05744493380188942,
      "learning_rate": 9.608333333333334e-06,
      "loss": 0.0018,
      "step": 193880
    },
    {
      "epoch": 6.463,
      "grad_norm": 0.029531650245189667,
      "learning_rate": 9.60625e-06,
      "loss": 0.0019,
      "step": 193890
    },
    {
      "epoch": 6.463333333333333,
      "grad_norm": 0.1999889612197876,
      "learning_rate": 9.604166666666667e-06,
      "loss": 0.0017,
      "step": 193900
    },
    {
      "epoch": 6.463666666666667,
      "grad_norm": 0.029967382550239563,
      "learning_rate": 9.602083333333334e-06,
      "loss": 0.0022,
      "step": 193910
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.11477728188037872,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0022,
      "step": 193920
    },
    {
      "epoch": 6.464333333333333,
      "grad_norm": 0.008848480880260468,
      "learning_rate": 9.597916666666667e-06,
      "loss": 0.0015,
      "step": 193930
    },
    {
      "epoch": 6.464666666666667,
      "grad_norm": 0.1713237762451172,
      "learning_rate": 9.595833333333334e-06,
      "loss": 0.0019,
      "step": 193940
    },
    {
      "epoch": 6.465,
      "grad_norm": 0.11485545337200165,
      "learning_rate": 9.59375e-06,
      "loss": 0.0026,
      "step": 193950
    },
    {
      "epoch": 6.465333333333334,
      "grad_norm": 0.08624161779880524,
      "learning_rate": 9.591666666666667e-06,
      "loss": 0.002,
      "step": 193960
    },
    {
      "epoch": 6.4656666666666665,
      "grad_norm": 0.42865151166915894,
      "learning_rate": 9.589583333333334e-06,
      "loss": 0.0013,
      "step": 193970
    },
    {
      "epoch": 6.466,
      "grad_norm": 0.05732157453894615,
      "learning_rate": 9.587500000000001e-06,
      "loss": 0.002,
      "step": 193980
    },
    {
      "epoch": 6.466333333333333,
      "grad_norm": 0.08682888746261597,
      "learning_rate": 9.585416666666668e-06,
      "loss": 0.0019,
      "step": 193990
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.2854968011379242,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.0023,
      "step": 194000
    },
    {
      "epoch": 6.467,
      "grad_norm": 0.0864531397819519,
      "learning_rate": 9.581249999999999e-06,
      "loss": 0.0016,
      "step": 194010
    },
    {
      "epoch": 6.467333333333333,
      "grad_norm": 0.17290709912776947,
      "learning_rate": 9.579166666666666e-06,
      "loss": 0.0013,
      "step": 194020
    },
    {
      "epoch": 6.467666666666666,
      "grad_norm": 0.006789813749492168,
      "learning_rate": 9.577083333333333e-06,
      "loss": 0.002,
      "step": 194030
    },
    {
      "epoch": 6.468,
      "grad_norm": 0.6376007199287415,
      "learning_rate": 9.575e-06,
      "loss": 0.0021,
      "step": 194040
    },
    {
      "epoch": 6.468333333333334,
      "grad_norm": 0.48562121391296387,
      "learning_rate": 9.572916666666668e-06,
      "loss": 0.0026,
      "step": 194050
    },
    {
      "epoch": 6.468666666666667,
      "grad_norm": 0.31395840644836426,
      "learning_rate": 9.570833333333335e-06,
      "loss": 0.0017,
      "step": 194060
    },
    {
      "epoch": 6.469,
      "grad_norm": 0.1714482307434082,
      "learning_rate": 9.56875e-06,
      "loss": 0.0021,
      "step": 194070
    },
    {
      "epoch": 6.469333333333333,
      "grad_norm": 0.4280041456222534,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0021,
      "step": 194080
    },
    {
      "epoch": 6.469666666666667,
      "grad_norm": 0.11697626858949661,
      "learning_rate": 9.564583333333333e-06,
      "loss": 0.0021,
      "step": 194090
    },
    {
      "epoch": 6.47,
      "grad_norm": 0.08613882958889008,
      "learning_rate": 9.5625e-06,
      "loss": 0.0017,
      "step": 194100
    },
    {
      "epoch": 6.4703333333333335,
      "grad_norm": 0.25686049461364746,
      "learning_rate": 9.560416666666668e-06,
      "loss": 0.0021,
      "step": 194110
    },
    {
      "epoch": 6.470666666666666,
      "grad_norm": 0.06353794038295746,
      "learning_rate": 9.558333333333335e-06,
      "loss": 0.0028,
      "step": 194120
    },
    {
      "epoch": 6.471,
      "grad_norm": 0.08593820780515671,
      "learning_rate": 9.55625e-06,
      "loss": 0.0016,
      "step": 194130
    },
    {
      "epoch": 6.471333333333333,
      "grad_norm": 0.032248664647340775,
      "learning_rate": 9.554166666666667e-06,
      "loss": 0.0014,
      "step": 194140
    },
    {
      "epoch": 6.471666666666667,
      "grad_norm": 0.06936389952898026,
      "learning_rate": 9.552083333333335e-06,
      "loss": 0.002,
      "step": 194150
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.14323127269744873,
      "learning_rate": 9.55e-06,
      "loss": 0.0015,
      "step": 194160
    },
    {
      "epoch": 6.472333333333333,
      "grad_norm": 0.24616560339927673,
      "learning_rate": 9.547916666666667e-06,
      "loss": 0.0019,
      "step": 194170
    },
    {
      "epoch": 6.472666666666667,
      "grad_norm": 0.1151447743177414,
      "learning_rate": 9.545833333333334e-06,
      "loss": 0.0023,
      "step": 194180
    },
    {
      "epoch": 6.473,
      "grad_norm": 0.14307522773742676,
      "learning_rate": 9.54375e-06,
      "loss": 0.0014,
      "step": 194190
    },
    {
      "epoch": 6.473333333333334,
      "grad_norm": 0.11782101541757584,
      "learning_rate": 9.541666666666667e-06,
      "loss": 0.0021,
      "step": 194200
    },
    {
      "epoch": 6.4736666666666665,
      "grad_norm": 0.1708705574274063,
      "learning_rate": 9.539583333333334e-06,
      "loss": 0.0013,
      "step": 194210
    },
    {
      "epoch": 6.474,
      "grad_norm": 0.008351105265319347,
      "learning_rate": 9.537500000000001e-06,
      "loss": 0.0013,
      "step": 194220
    },
    {
      "epoch": 6.474333333333333,
      "grad_norm": 0.030607957392930984,
      "learning_rate": 9.535416666666667e-06,
      "loss": 0.0014,
      "step": 194230
    },
    {
      "epoch": 6.474666666666667,
      "grad_norm": 0.1998443901538849,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0017,
      "step": 194240
    },
    {
      "epoch": 6.475,
      "grad_norm": 0.11482802033424377,
      "learning_rate": 9.53125e-06,
      "loss": 0.0019,
      "step": 194250
    },
    {
      "epoch": 6.475333333333333,
      "grad_norm": 0.3426526188850403,
      "learning_rate": 9.529166666666667e-06,
      "loss": 0.002,
      "step": 194260
    },
    {
      "epoch": 6.475666666666667,
      "grad_norm": 0.17128758132457733,
      "learning_rate": 9.527083333333334e-06,
      "loss": 0.0017,
      "step": 194270
    },
    {
      "epoch": 6.476,
      "grad_norm": 0.30597642064094543,
      "learning_rate": 9.525000000000001e-06,
      "loss": 0.0012,
      "step": 194280
    },
    {
      "epoch": 6.476333333333334,
      "grad_norm": 0.07052779942750931,
      "learning_rate": 9.522916666666668e-06,
      "loss": 0.0019,
      "step": 194290
    },
    {
      "epoch": 6.476666666666667,
      "grad_norm": 0.03217389062047005,
      "learning_rate": 9.520833333333334e-06,
      "loss": 0.0012,
      "step": 194300
    },
    {
      "epoch": 6.477,
      "grad_norm": 0.08620666712522507,
      "learning_rate": 9.51875e-06,
      "loss": 0.0015,
      "step": 194310
    },
    {
      "epoch": 6.477333333333333,
      "grad_norm": 0.18647004663944244,
      "learning_rate": 9.516666666666666e-06,
      "loss": 0.0016,
      "step": 194320
    },
    {
      "epoch": 6.477666666666667,
      "grad_norm": 0.2770415246486664,
      "learning_rate": 9.514583333333334e-06,
      "loss": 0.0019,
      "step": 194330
    },
    {
      "epoch": 6.478,
      "grad_norm": 0.14338092505931854,
      "learning_rate": 9.5125e-06,
      "loss": 0.0015,
      "step": 194340
    },
    {
      "epoch": 6.4783333333333335,
      "grad_norm": 0.3318023681640625,
      "learning_rate": 9.510416666666668e-06,
      "loss": 0.0016,
      "step": 194350
    },
    {
      "epoch": 6.478666666666666,
      "grad_norm": 0.1456647366285324,
      "learning_rate": 9.508333333333335e-06,
      "loss": 0.002,
      "step": 194360
    },
    {
      "epoch": 6.479,
      "grad_norm": 0.20057328045368195,
      "learning_rate": 9.50625e-06,
      "loss": 0.0015,
      "step": 194370
    },
    {
      "epoch": 6.479333333333333,
      "grad_norm": 0.15016482770442963,
      "learning_rate": 9.504166666666666e-06,
      "loss": 0.0026,
      "step": 194380
    },
    {
      "epoch": 6.479666666666667,
      "grad_norm": 0.08707151561975479,
      "learning_rate": 9.502083333333333e-06,
      "loss": 0.0017,
      "step": 194390
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.1898581087589264,
      "learning_rate": 9.5e-06,
      "loss": 0.0017,
      "step": 194400
    },
    {
      "epoch": 6.480333333333333,
      "grad_norm": 0.0861680805683136,
      "learning_rate": 9.497916666666668e-06,
      "loss": 0.0025,
      "step": 194410
    },
    {
      "epoch": 6.480666666666667,
      "grad_norm": 0.11463648080825806,
      "learning_rate": 9.495833333333335e-06,
      "loss": 0.0022,
      "step": 194420
    },
    {
      "epoch": 6.481,
      "grad_norm": 0.059482503682374954,
      "learning_rate": 9.49375e-06,
      "loss": 0.0013,
      "step": 194430
    },
    {
      "epoch": 6.481333333333334,
      "grad_norm": 0.21785476803779602,
      "learning_rate": 9.491666666666668e-06,
      "loss": 0.0023,
      "step": 194440
    },
    {
      "epoch": 6.4816666666666665,
      "grad_norm": 0.313564270734787,
      "learning_rate": 9.489583333333333e-06,
      "loss": 0.0015,
      "step": 194450
    },
    {
      "epoch": 6.482,
      "grad_norm": 0.14332301914691925,
      "learning_rate": 9.4875e-06,
      "loss": 0.0012,
      "step": 194460
    },
    {
      "epoch": 6.482333333333333,
      "grad_norm": 0.1812739074230194,
      "learning_rate": 9.485416666666667e-06,
      "loss": 0.0013,
      "step": 194470
    },
    {
      "epoch": 6.482666666666667,
      "grad_norm": 0.11478254944086075,
      "learning_rate": 9.483333333333335e-06,
      "loss": 0.0014,
      "step": 194480
    },
    {
      "epoch": 6.483,
      "grad_norm": 0.3465365171432495,
      "learning_rate": 9.48125e-06,
      "loss": 0.0017,
      "step": 194490
    },
    {
      "epoch": 6.483333333333333,
      "grad_norm": 0.029452497139573097,
      "learning_rate": 9.479166666666667e-06,
      "loss": 0.0021,
      "step": 194500
    },
    {
      "epoch": 6.483666666666666,
      "grad_norm": 0.2314550280570984,
      "learning_rate": 9.477083333333334e-06,
      "loss": 0.0027,
      "step": 194510
    },
    {
      "epoch": 6.484,
      "grad_norm": 0.2572906017303467,
      "learning_rate": 9.475e-06,
      "loss": 0.0015,
      "step": 194520
    },
    {
      "epoch": 6.484333333333334,
      "grad_norm": 0.28819379210472107,
      "learning_rate": 9.472916666666667e-06,
      "loss": 0.0016,
      "step": 194530
    },
    {
      "epoch": 6.484666666666667,
      "grad_norm": 0.0590018555521965,
      "learning_rate": 9.470833333333334e-06,
      "loss": 0.0015,
      "step": 194540
    },
    {
      "epoch": 6.485,
      "grad_norm": 0.42750269174575806,
      "learning_rate": 9.46875e-06,
      "loss": 0.0016,
      "step": 194550
    },
    {
      "epoch": 6.485333333333333,
      "grad_norm": 0.1437351554632187,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0014,
      "step": 194560
    },
    {
      "epoch": 6.485666666666667,
      "grad_norm": 0.02994505874812603,
      "learning_rate": 9.464583333333334e-06,
      "loss": 0.0011,
      "step": 194570
    },
    {
      "epoch": 6.486,
      "grad_norm": 0.19995948672294617,
      "learning_rate": 9.462500000000001e-06,
      "loss": 0.0017,
      "step": 194580
    },
    {
      "epoch": 6.4863333333333335,
      "grad_norm": 0.31698501110076904,
      "learning_rate": 9.460416666666667e-06,
      "loss": 0.0017,
      "step": 194590
    },
    {
      "epoch": 6.486666666666666,
      "grad_norm": 0.14253449440002441,
      "learning_rate": 9.458333333333334e-06,
      "loss": 0.0023,
      "step": 194600
    },
    {
      "epoch": 6.487,
      "grad_norm": 0.285675048828125,
      "learning_rate": 9.45625e-06,
      "loss": 0.0014,
      "step": 194610
    },
    {
      "epoch": 6.487333333333333,
      "grad_norm": 0.17173300683498383,
      "learning_rate": 9.454166666666667e-06,
      "loss": 0.0023,
      "step": 194620
    },
    {
      "epoch": 6.487666666666667,
      "grad_norm": 0.058089617639780045,
      "learning_rate": 9.452083333333334e-06,
      "loss": 0.0012,
      "step": 194630
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.20146913826465607,
      "learning_rate": 9.450000000000001e-06,
      "loss": 0.002,
      "step": 194640
    },
    {
      "epoch": 6.488333333333333,
      "grad_norm": 0.25701069831848145,
      "learning_rate": 9.447916666666668e-06,
      "loss": 0.0023,
      "step": 194650
    },
    {
      "epoch": 6.488666666666667,
      "grad_norm": 0.22879992425441742,
      "learning_rate": 9.445833333333334e-06,
      "loss": 0.0021,
      "step": 194660
    },
    {
      "epoch": 6.489,
      "grad_norm": 0.057956550270318985,
      "learning_rate": 9.44375e-06,
      "loss": 0.0018,
      "step": 194670
    },
    {
      "epoch": 6.489333333333334,
      "grad_norm": 0.1147981509566307,
      "learning_rate": 9.441666666666666e-06,
      "loss": 0.0014,
      "step": 194680
    },
    {
      "epoch": 6.4896666666666665,
      "grad_norm": 0.06967253983020782,
      "learning_rate": 9.439583333333334e-06,
      "loss": 0.0014,
      "step": 194690
    },
    {
      "epoch": 6.49,
      "grad_norm": 0.057898618280887604,
      "learning_rate": 9.4375e-06,
      "loss": 0.0017,
      "step": 194700
    },
    {
      "epoch": 6.490333333333333,
      "grad_norm": 0.0679105669260025,
      "learning_rate": 9.435416666666668e-06,
      "loss": 0.0014,
      "step": 194710
    },
    {
      "epoch": 6.490666666666667,
      "grad_norm": 0.1429823487997055,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0015,
      "step": 194720
    },
    {
      "epoch": 6.491,
      "grad_norm": 0.01220330037176609,
      "learning_rate": 9.43125e-06,
      "loss": 0.0015,
      "step": 194730
    },
    {
      "epoch": 6.491333333333333,
      "grad_norm": 0.1433834731578827,
      "learning_rate": 9.429166666666666e-06,
      "loss": 0.0017,
      "step": 194740
    },
    {
      "epoch": 6.491666666666666,
      "grad_norm": 0.08591757714748383,
      "learning_rate": 9.427083333333333e-06,
      "loss": 0.0013,
      "step": 194750
    },
    {
      "epoch": 6.492,
      "grad_norm": 0.2288258671760559,
      "learning_rate": 9.425e-06,
      "loss": 0.0019,
      "step": 194760
    },
    {
      "epoch": 6.492333333333334,
      "grad_norm": 0.19983360171318054,
      "learning_rate": 9.422916666666668e-06,
      "loss": 0.0019,
      "step": 194770
    },
    {
      "epoch": 6.492666666666667,
      "grad_norm": 0.057695239782333374,
      "learning_rate": 9.420833333333335e-06,
      "loss": 0.0014,
      "step": 194780
    },
    {
      "epoch": 6.493,
      "grad_norm": 0.3133384585380554,
      "learning_rate": 9.41875e-06,
      "loss": 0.0022,
      "step": 194790
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 0.17123061418533325,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.0013,
      "step": 194800
    },
    {
      "epoch": 6.493666666666667,
      "grad_norm": 0.20037181675434113,
      "learning_rate": 9.414583333333333e-06,
      "loss": 0.0028,
      "step": 194810
    },
    {
      "epoch": 6.494,
      "grad_norm": 0.1716907024383545,
      "learning_rate": 9.4125e-06,
      "loss": 0.0016,
      "step": 194820
    },
    {
      "epoch": 6.4943333333333335,
      "grad_norm": 0.05751877278089523,
      "learning_rate": 9.410416666666667e-06,
      "loss": 0.0016,
      "step": 194830
    },
    {
      "epoch": 6.494666666666666,
      "grad_norm": 0.03079100139439106,
      "learning_rate": 9.408333333333334e-06,
      "loss": 0.0023,
      "step": 194840
    },
    {
      "epoch": 6.495,
      "grad_norm": 0.11702405661344528,
      "learning_rate": 9.40625e-06,
      "loss": 0.0013,
      "step": 194850
    },
    {
      "epoch": 6.495333333333333,
      "grad_norm": 0.060756802558898926,
      "learning_rate": 9.404166666666667e-06,
      "loss": 0.0018,
      "step": 194860
    },
    {
      "epoch": 6.495666666666667,
      "grad_norm": 0.2858690917491913,
      "learning_rate": 9.402083333333334e-06,
      "loss": 0.0026,
      "step": 194870
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.17211733758449554,
      "learning_rate": 9.4e-06,
      "loss": 0.0014,
      "step": 194880
    },
    {
      "epoch": 6.496333333333333,
      "grad_norm": 0.08586478978395462,
      "learning_rate": 9.397916666666667e-06,
      "loss": 0.0023,
      "step": 194890
    },
    {
      "epoch": 6.496666666666667,
      "grad_norm": 0.029462384060025215,
      "learning_rate": 9.395833333333334e-06,
      "loss": 0.0019,
      "step": 194900
    },
    {
      "epoch": 6.497,
      "grad_norm": 0.08655162155628204,
      "learning_rate": 9.39375e-06,
      "loss": 0.0018,
      "step": 194910
    },
    {
      "epoch": 6.497333333333334,
      "grad_norm": 0.3136337399482727,
      "learning_rate": 9.391666666666667e-06,
      "loss": 0.0011,
      "step": 194920
    },
    {
      "epoch": 6.4976666666666665,
      "grad_norm": 0.17126719653606415,
      "learning_rate": 9.389583333333334e-06,
      "loss": 0.0014,
      "step": 194930
    },
    {
      "epoch": 6.498,
      "grad_norm": 0.11425794661045074,
      "learning_rate": 9.387500000000001e-06,
      "loss": 0.0014,
      "step": 194940
    },
    {
      "epoch": 6.498333333333333,
      "grad_norm": 0.1424354910850525,
      "learning_rate": 9.385416666666667e-06,
      "loss": 0.0017,
      "step": 194950
    },
    {
      "epoch": 6.498666666666667,
      "grad_norm": 0.03009432926774025,
      "learning_rate": 9.383333333333334e-06,
      "loss": 0.0016,
      "step": 194960
    },
    {
      "epoch": 6.499,
      "grad_norm": 0.03087562695145607,
      "learning_rate": 9.38125e-06,
      "loss": 0.0016,
      "step": 194970
    },
    {
      "epoch": 6.499333333333333,
      "grad_norm": 0.19983530044555664,
      "learning_rate": 9.379166666666667e-06,
      "loss": 0.002,
      "step": 194980
    },
    {
      "epoch": 6.499666666666666,
      "grad_norm": 0.030881015583872795,
      "learning_rate": 9.377083333333334e-06,
      "loss": 0.0021,
      "step": 194990
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.029445214197039604,
      "learning_rate": 9.375000000000001e-06,
      "loss": 0.0025,
      "step": 195000
    },
    {
      "epoch": 6.500333333333334,
      "grad_norm": 0.01030335295945406,
      "learning_rate": 9.372916666666668e-06,
      "loss": 0.0016,
      "step": 195010
    },
    {
      "epoch": 6.500666666666667,
      "grad_norm": 0.1997653692960739,
      "learning_rate": 9.370833333333334e-06,
      "loss": 0.0016,
      "step": 195020
    },
    {
      "epoch": 6.501,
      "grad_norm": 0.1993657648563385,
      "learning_rate": 9.36875e-06,
      "loss": 0.0017,
      "step": 195030
    },
    {
      "epoch": 6.501333333333333,
      "grad_norm": 0.14329245686531067,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0013,
      "step": 195040
    },
    {
      "epoch": 6.501666666666667,
      "grad_norm": 0.1153186485171318,
      "learning_rate": 9.364583333333333e-06,
      "loss": 0.0019,
      "step": 195050
    },
    {
      "epoch": 6.502,
      "grad_norm": 0.11394365131855011,
      "learning_rate": 9.3625e-06,
      "loss": 0.0013,
      "step": 195060
    },
    {
      "epoch": 6.5023333333333335,
      "grad_norm": 0.08563707768917084,
      "learning_rate": 9.360416666666668e-06,
      "loss": 0.0015,
      "step": 195070
    },
    {
      "epoch": 6.502666666666666,
      "grad_norm": 0.22858262062072754,
      "learning_rate": 9.358333333333335e-06,
      "loss": 0.0018,
      "step": 195080
    },
    {
      "epoch": 6.503,
      "grad_norm": 0.01278684101998806,
      "learning_rate": 9.35625e-06,
      "loss": 0.0018,
      "step": 195090
    },
    {
      "epoch": 6.503333333333333,
      "grad_norm": 0.11494352668523788,
      "learning_rate": 9.354166666666666e-06,
      "loss": 0.0027,
      "step": 195100
    },
    {
      "epoch": 6.503666666666667,
      "grad_norm": 0.1142892837524414,
      "learning_rate": 9.352083333333333e-06,
      "loss": 0.0018,
      "step": 195110
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.08656345307826996,
      "learning_rate": 9.35e-06,
      "loss": 0.0015,
      "step": 195120
    },
    {
      "epoch": 6.504333333333333,
      "grad_norm": 0.22759640216827393,
      "learning_rate": 9.347916666666667e-06,
      "loss": 0.0026,
      "step": 195130
    },
    {
      "epoch": 6.504666666666667,
      "grad_norm": 0.37889575958251953,
      "learning_rate": 9.345833333333335e-06,
      "loss": 0.0021,
      "step": 195140
    },
    {
      "epoch": 6.505,
      "grad_norm": 0.005806206725537777,
      "learning_rate": 9.343750000000002e-06,
      "loss": 0.0023,
      "step": 195150
    },
    {
      "epoch": 6.505333333333334,
      "grad_norm": 0.20123766362667084,
      "learning_rate": 9.341666666666667e-06,
      "loss": 0.0013,
      "step": 195160
    },
    {
      "epoch": 6.5056666666666665,
      "grad_norm": 0.4000720679759979,
      "learning_rate": 9.339583333333333e-06,
      "loss": 0.0024,
      "step": 195170
    },
    {
      "epoch": 6.506,
      "grad_norm": 0.3459198474884033,
      "learning_rate": 9.3375e-06,
      "loss": 0.0016,
      "step": 195180
    },
    {
      "epoch": 6.506333333333333,
      "grad_norm": 0.4562526345252991,
      "learning_rate": 9.335416666666667e-06,
      "loss": 0.0016,
      "step": 195190
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.1711365133523941,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0014,
      "step": 195200
    },
    {
      "epoch": 6.507,
      "grad_norm": 0.05922246351838112,
      "learning_rate": 9.331250000000002e-06,
      "loss": 0.0019,
      "step": 195210
    },
    {
      "epoch": 6.507333333333333,
      "grad_norm": 0.11465435475111008,
      "learning_rate": 9.329166666666667e-06,
      "loss": 0.002,
      "step": 195220
    },
    {
      "epoch": 6.507666666666667,
      "grad_norm": 0.19992098212242126,
      "learning_rate": 9.327083333333334e-06,
      "loss": 0.0011,
      "step": 195230
    },
    {
      "epoch": 6.508,
      "grad_norm": 0.05903342738747597,
      "learning_rate": 9.325e-06,
      "loss": 0.0013,
      "step": 195240
    },
    {
      "epoch": 6.508333333333333,
      "grad_norm": 0.08583257347345352,
      "learning_rate": 9.322916666666667e-06,
      "loss": 0.0016,
      "step": 195250
    },
    {
      "epoch": 6.508666666666667,
      "grad_norm": 0.11511442810297012,
      "learning_rate": 9.320833333333334e-06,
      "loss": 0.002,
      "step": 195260
    },
    {
      "epoch": 6.509,
      "grad_norm": 0.07535374909639359,
      "learning_rate": 9.318750000000001e-06,
      "loss": 0.0014,
      "step": 195270
    },
    {
      "epoch": 6.509333333333333,
      "grad_norm": 0.11440054327249527,
      "learning_rate": 9.316666666666667e-06,
      "loss": 0.0018,
      "step": 195280
    },
    {
      "epoch": 6.509666666666667,
      "grad_norm": 0.1162450984120369,
      "learning_rate": 9.314583333333334e-06,
      "loss": 0.002,
      "step": 195290
    },
    {
      "epoch": 6.51,
      "grad_norm": 0.39930570125579834,
      "learning_rate": 9.312500000000001e-06,
      "loss": 0.0015,
      "step": 195300
    },
    {
      "epoch": 6.5103333333333335,
      "grad_norm": 0.05774803087115288,
      "learning_rate": 9.310416666666667e-06,
      "loss": 0.0017,
      "step": 195310
    },
    {
      "epoch": 6.510666666666666,
      "grad_norm": 0.14301887154579163,
      "learning_rate": 9.308333333333334e-06,
      "loss": 0.0015,
      "step": 195320
    },
    {
      "epoch": 6.511,
      "grad_norm": 0.08707231283187866,
      "learning_rate": 9.306250000000001e-06,
      "loss": 0.0023,
      "step": 195330
    },
    {
      "epoch": 6.511333333333333,
      "grad_norm": 0.14284436404705048,
      "learning_rate": 9.304166666666666e-06,
      "loss": 0.0022,
      "step": 195340
    },
    {
      "epoch": 6.511666666666667,
      "grad_norm": 0.24532778561115265,
      "learning_rate": 9.302083333333334e-06,
      "loss": 0.0026,
      "step": 195350
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.23007379472255707,
      "learning_rate": 9.3e-06,
      "loss": 0.0022,
      "step": 195360
    },
    {
      "epoch": 6.512333333333333,
      "grad_norm": 0.2566230595111847,
      "learning_rate": 9.297916666666668e-06,
      "loss": 0.0023,
      "step": 195370
    },
    {
      "epoch": 6.512666666666667,
      "grad_norm": 0.19895018637180328,
      "learning_rate": 9.295833333333333e-06,
      "loss": 0.002,
      "step": 195380
    },
    {
      "epoch": 6.513,
      "grad_norm": 0.17176055908203125,
      "learning_rate": 9.29375e-06,
      "loss": 0.0017,
      "step": 195390
    },
    {
      "epoch": 6.513333333333334,
      "grad_norm": 0.371342271566391,
      "learning_rate": 9.291666666666666e-06,
      "loss": 0.0018,
      "step": 195400
    },
    {
      "epoch": 6.5136666666666665,
      "grad_norm": 0.3423680365085602,
      "learning_rate": 9.289583333333333e-06,
      "loss": 0.0023,
      "step": 195410
    },
    {
      "epoch": 6.514,
      "grad_norm": 0.05803341791033745,
      "learning_rate": 9.2875e-06,
      "loss": 0.0022,
      "step": 195420
    },
    {
      "epoch": 6.514333333333333,
      "grad_norm": 0.3136608898639679,
      "learning_rate": 9.285416666666668e-06,
      "loss": 0.0013,
      "step": 195430
    },
    {
      "epoch": 6.514666666666667,
      "grad_norm": 0.030492736026644707,
      "learning_rate": 9.283333333333335e-06,
      "loss": 0.0014,
      "step": 195440
    },
    {
      "epoch": 6.515,
      "grad_norm": 0.3140353262424469,
      "learning_rate": 9.28125e-06,
      "loss": 0.0021,
      "step": 195450
    },
    {
      "epoch": 6.515333333333333,
      "grad_norm": 0.31406861543655396,
      "learning_rate": 9.279166666666666e-06,
      "loss": 0.0022,
      "step": 195460
    },
    {
      "epoch": 6.515666666666666,
      "grad_norm": 0.23170985281467438,
      "learning_rate": 9.277083333333333e-06,
      "loss": 0.0017,
      "step": 195470
    },
    {
      "epoch": 6.516,
      "grad_norm": 0.029894549399614334,
      "learning_rate": 9.275e-06,
      "loss": 0.0016,
      "step": 195480
    },
    {
      "epoch": 6.516333333333334,
      "grad_norm": 0.20975258946418762,
      "learning_rate": 9.272916666666667e-06,
      "loss": 0.0017,
      "step": 195490
    },
    {
      "epoch": 6.516666666666667,
      "grad_norm": 0.03164469078183174,
      "learning_rate": 9.270833333333334e-06,
      "loss": 0.0013,
      "step": 195500
    },
    {
      "epoch": 6.517,
      "grad_norm": 0.3991755545139313,
      "learning_rate": 9.268750000000002e-06,
      "loss": 0.0013,
      "step": 195510
    },
    {
      "epoch": 6.517333333333333,
      "grad_norm": 0.20120704174041748,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0018,
      "step": 195520
    },
    {
      "epoch": 6.517666666666667,
      "grad_norm": 0.14329113066196442,
      "learning_rate": 9.264583333333333e-06,
      "loss": 0.0027,
      "step": 195530
    },
    {
      "epoch": 6.518,
      "grad_norm": 0.20024645328521729,
      "learning_rate": 9.2625e-06,
      "loss": 0.0013,
      "step": 195540
    },
    {
      "epoch": 6.5183333333333335,
      "grad_norm": 0.2279670685529709,
      "learning_rate": 9.260416666666667e-06,
      "loss": 0.0018,
      "step": 195550
    },
    {
      "epoch": 6.518666666666666,
      "grad_norm": 0.14349591732025146,
      "learning_rate": 9.258333333333334e-06,
      "loss": 0.0014,
      "step": 195560
    },
    {
      "epoch": 6.519,
      "grad_norm": 0.0299360491335392,
      "learning_rate": 9.256250000000001e-06,
      "loss": 0.0014,
      "step": 195570
    },
    {
      "epoch": 6.519333333333333,
      "grad_norm": 0.11482860893011093,
      "learning_rate": 9.254166666666667e-06,
      "loss": 0.0015,
      "step": 195580
    },
    {
      "epoch": 6.519666666666667,
      "grad_norm": 0.301161527633667,
      "learning_rate": 9.252083333333334e-06,
      "loss": 0.0016,
      "step": 195590
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.04842296987771988,
      "learning_rate": 9.25e-06,
      "loss": 0.0015,
      "step": 195600
    },
    {
      "epoch": 6.520333333333333,
      "grad_norm": 0.35664162039756775,
      "learning_rate": 9.247916666666667e-06,
      "loss": 0.0013,
      "step": 195610
    },
    {
      "epoch": 6.520666666666667,
      "grad_norm": 0.08587120473384857,
      "learning_rate": 9.245833333333334e-06,
      "loss": 0.0016,
      "step": 195620
    },
    {
      "epoch": 6.521,
      "grad_norm": 0.37170103192329407,
      "learning_rate": 9.243750000000001e-06,
      "loss": 0.0027,
      "step": 195630
    },
    {
      "epoch": 6.521333333333334,
      "grad_norm": 0.314054936170578,
      "learning_rate": 9.241666666666667e-06,
      "loss": 0.0016,
      "step": 195640
    },
    {
      "epoch": 6.5216666666666665,
      "grad_norm": 0.057615749537944794,
      "learning_rate": 9.239583333333334e-06,
      "loss": 0.0018,
      "step": 195650
    },
    {
      "epoch": 6.522,
      "grad_norm": 0.3141308128833771,
      "learning_rate": 9.237500000000001e-06,
      "loss": 0.0015,
      "step": 195660
    },
    {
      "epoch": 6.522333333333333,
      "grad_norm": 0.2285783737897873,
      "learning_rate": 9.235416666666666e-06,
      "loss": 0.0019,
      "step": 195670
    },
    {
      "epoch": 6.522666666666667,
      "grad_norm": 0.21703402698040009,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0015,
      "step": 195680
    },
    {
      "epoch": 6.523,
      "grad_norm": 0.3140641748905182,
      "learning_rate": 9.23125e-06,
      "loss": 0.0018,
      "step": 195690
    },
    {
      "epoch": 6.523333333333333,
      "grad_norm": 0.2869648039340973,
      "learning_rate": 9.229166666666666e-06,
      "loss": 0.0018,
      "step": 195700
    },
    {
      "epoch": 6.523666666666666,
      "grad_norm": 0.11474163085222244,
      "learning_rate": 9.227083333333333e-06,
      "loss": 0.0016,
      "step": 195710
    },
    {
      "epoch": 6.524,
      "grad_norm": 0.029535265639424324,
      "learning_rate": 9.225e-06,
      "loss": 0.0017,
      "step": 195720
    },
    {
      "epoch": 6.524333333333333,
      "grad_norm": 0.143108069896698,
      "learning_rate": 9.222916666666668e-06,
      "loss": 0.0017,
      "step": 195730
    },
    {
      "epoch": 6.524666666666667,
      "grad_norm": 0.08574184775352478,
      "learning_rate": 9.220833333333333e-06,
      "loss": 0.0013,
      "step": 195740
    },
    {
      "epoch": 6.525,
      "grad_norm": 0.22845254838466644,
      "learning_rate": 9.21875e-06,
      "loss": 0.0019,
      "step": 195750
    },
    {
      "epoch": 6.525333333333333,
      "grad_norm": 0.1142534539103508,
      "learning_rate": 9.216666666666666e-06,
      "loss": 0.0013,
      "step": 195760
    },
    {
      "epoch": 6.525666666666667,
      "grad_norm": 0.22818662226200104,
      "learning_rate": 9.214583333333333e-06,
      "loss": 0.0019,
      "step": 195770
    },
    {
      "epoch": 6.526,
      "grad_norm": 0.22847113013267517,
      "learning_rate": 9.2125e-06,
      "loss": 0.0019,
      "step": 195780
    },
    {
      "epoch": 6.5263333333333335,
      "grad_norm": 0.22847408056259155,
      "learning_rate": 9.210416666666667e-06,
      "loss": 0.0015,
      "step": 195790
    },
    {
      "epoch": 6.526666666666666,
      "grad_norm": 0.2852407693862915,
      "learning_rate": 9.208333333333335e-06,
      "loss": 0.0016,
      "step": 195800
    },
    {
      "epoch": 6.527,
      "grad_norm": 0.14292258024215698,
      "learning_rate": 9.20625e-06,
      "loss": 0.0017,
      "step": 195810
    },
    {
      "epoch": 6.527333333333333,
      "grad_norm": 0.10891661792993546,
      "learning_rate": 9.204166666666667e-06,
      "loss": 0.002,
      "step": 195820
    },
    {
      "epoch": 6.527666666666667,
      "grad_norm": 0.05910737067461014,
      "learning_rate": 9.202083333333333e-06,
      "loss": 0.002,
      "step": 195830
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.04766817018389702,
      "learning_rate": 9.2e-06,
      "loss": 0.0021,
      "step": 195840
    },
    {
      "epoch": 6.528333333333333,
      "grad_norm": 0.03045480139553547,
      "learning_rate": 9.197916666666667e-06,
      "loss": 0.0019,
      "step": 195850
    },
    {
      "epoch": 6.528666666666666,
      "grad_norm": 0.08818114548921585,
      "learning_rate": 9.195833333333334e-06,
      "loss": 0.0013,
      "step": 195860
    },
    {
      "epoch": 6.529,
      "grad_norm": 0.22902119159698486,
      "learning_rate": 9.193750000000002e-06,
      "loss": 0.002,
      "step": 195870
    },
    {
      "epoch": 6.529333333333334,
      "grad_norm": 0.29062655568122864,
      "learning_rate": 9.191666666666667e-06,
      "loss": 0.0018,
      "step": 195880
    },
    {
      "epoch": 6.5296666666666665,
      "grad_norm": 0.14284111559391022,
      "learning_rate": 9.189583333333334e-06,
      "loss": 0.0015,
      "step": 195890
    },
    {
      "epoch": 6.53,
      "grad_norm": 0.11444173753261566,
      "learning_rate": 9.1875e-06,
      "loss": 0.0021,
      "step": 195900
    },
    {
      "epoch": 6.530333333333333,
      "grad_norm": 0.17105711996555328,
      "learning_rate": 9.185416666666667e-06,
      "loss": 0.0013,
      "step": 195910
    },
    {
      "epoch": 6.530666666666667,
      "grad_norm": 0.030435968190431595,
      "learning_rate": 9.183333333333334e-06,
      "loss": 0.0019,
      "step": 195920
    },
    {
      "epoch": 6.531,
      "grad_norm": 0.3424281179904938,
      "learning_rate": 9.181250000000001e-06,
      "loss": 0.0017,
      "step": 195930
    },
    {
      "epoch": 6.531333333333333,
      "grad_norm": 0.12066170573234558,
      "learning_rate": 9.179166666666667e-06,
      "loss": 0.0014,
      "step": 195940
    },
    {
      "epoch": 6.531666666666666,
      "grad_norm": 0.0576465018093586,
      "learning_rate": 9.177083333333334e-06,
      "loss": 0.0021,
      "step": 195950
    },
    {
      "epoch": 6.532,
      "grad_norm": 0.11370691657066345,
      "learning_rate": 9.175000000000001e-06,
      "loss": 0.0012,
      "step": 195960
    },
    {
      "epoch": 6.532333333333334,
      "grad_norm": 0.06285372376441956,
      "learning_rate": 9.172916666666667e-06,
      "loss": 0.0012,
      "step": 195970
    },
    {
      "epoch": 6.532666666666667,
      "grad_norm": 0.14253340661525726,
      "learning_rate": 9.170833333333334e-06,
      "loss": 0.0015,
      "step": 195980
    },
    {
      "epoch": 6.533,
      "grad_norm": 0.05803647264838219,
      "learning_rate": 9.168750000000001e-06,
      "loss": 0.0019,
      "step": 195990
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.05731949955224991,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0015,
      "step": 196000
    },
    {
      "epoch": 6.533666666666667,
      "grad_norm": 0.2565535604953766,
      "learning_rate": 9.164583333333334e-06,
      "loss": 0.0014,
      "step": 196010
    },
    {
      "epoch": 6.534,
      "grad_norm": 0.11424852162599564,
      "learning_rate": 9.1625e-06,
      "loss": 0.0015,
      "step": 196020
    },
    {
      "epoch": 6.5343333333333335,
      "grad_norm": 0.14248263835906982,
      "learning_rate": 9.160416666666668e-06,
      "loss": 0.0018,
      "step": 196030
    },
    {
      "epoch": 6.534666666666666,
      "grad_norm": 0.034136973321437836,
      "learning_rate": 9.158333333333333e-06,
      "loss": 0.0021,
      "step": 196040
    },
    {
      "epoch": 6.535,
      "grad_norm": 0.04102007672190666,
      "learning_rate": 9.15625e-06,
      "loss": 0.0017,
      "step": 196050
    },
    {
      "epoch": 6.535333333333333,
      "grad_norm": 0.37682756781578064,
      "learning_rate": 9.154166666666666e-06,
      "loss": 0.0019,
      "step": 196060
    },
    {
      "epoch": 6.535666666666667,
      "grad_norm": 0.011640357784926891,
      "learning_rate": 9.152083333333333e-06,
      "loss": 0.0015,
      "step": 196070
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.08617381751537323,
      "learning_rate": 9.15e-06,
      "loss": 0.0015,
      "step": 196080
    },
    {
      "epoch": 6.536333333333333,
      "grad_norm": 0.057605862617492676,
      "learning_rate": 9.147916666666668e-06,
      "loss": 0.0014,
      "step": 196090
    },
    {
      "epoch": 6.536666666666667,
      "grad_norm": 0.3707210123538971,
      "learning_rate": 9.145833333333335e-06,
      "loss": 0.0015,
      "step": 196100
    },
    {
      "epoch": 6.537,
      "grad_norm": 0.22321215271949768,
      "learning_rate": 9.14375e-06,
      "loss": 0.0019,
      "step": 196110
    },
    {
      "epoch": 6.537333333333334,
      "grad_norm": 0.3343357741832733,
      "learning_rate": 9.141666666666666e-06,
      "loss": 0.0019,
      "step": 196120
    },
    {
      "epoch": 6.5376666666666665,
      "grad_norm": 0.11450653523206711,
      "learning_rate": 9.139583333333333e-06,
      "loss": 0.0014,
      "step": 196130
    },
    {
      "epoch": 6.538,
      "grad_norm": 0.22828643023967743,
      "learning_rate": 9.1375e-06,
      "loss": 0.0013,
      "step": 196140
    },
    {
      "epoch": 6.538333333333333,
      "grad_norm": 0.08588317036628723,
      "learning_rate": 9.135416666666667e-06,
      "loss": 0.0019,
      "step": 196150
    },
    {
      "epoch": 6.538666666666667,
      "grad_norm": 0.09171279519796371,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0024,
      "step": 196160
    },
    {
      "epoch": 6.539,
      "grad_norm": 0.3706648647785187,
      "learning_rate": 9.131250000000002e-06,
      "loss": 0.0014,
      "step": 196170
    },
    {
      "epoch": 6.539333333333333,
      "grad_norm": 0.3721054494380951,
      "learning_rate": 9.129166666666667e-06,
      "loss": 0.0016,
      "step": 196180
    },
    {
      "epoch": 6.539666666666666,
      "grad_norm": 0.11515208333730698,
      "learning_rate": 9.127083333333333e-06,
      "loss": 0.0015,
      "step": 196190
    },
    {
      "epoch": 6.54,
      "grad_norm": 0.228461354970932,
      "learning_rate": 9.125e-06,
      "loss": 0.0016,
      "step": 196200
    },
    {
      "epoch": 6.540333333333333,
      "grad_norm": 0.03036665730178356,
      "learning_rate": 9.122916666666667e-06,
      "loss": 0.0015,
      "step": 196210
    },
    {
      "epoch": 6.540666666666667,
      "grad_norm": 0.1431966871023178,
      "learning_rate": 9.120833333333334e-06,
      "loss": 0.0014,
      "step": 196220
    },
    {
      "epoch": 6.541,
      "grad_norm": 0.2576461434364319,
      "learning_rate": 9.118750000000001e-06,
      "loss": 0.0024,
      "step": 196230
    },
    {
      "epoch": 6.541333333333333,
      "grad_norm": 0.257024347782135,
      "learning_rate": 9.116666666666667e-06,
      "loss": 0.002,
      "step": 196240
    },
    {
      "epoch": 6.541666666666667,
      "grad_norm": 0.3343614637851715,
      "learning_rate": 9.114583333333334e-06,
      "loss": 0.0019,
      "step": 196250
    },
    {
      "epoch": 6.542,
      "grad_norm": 0.009959465824067593,
      "learning_rate": 9.1125e-06,
      "loss": 0.0023,
      "step": 196260
    },
    {
      "epoch": 6.542333333333334,
      "grad_norm": 0.030480170622467995,
      "learning_rate": 9.110416666666667e-06,
      "loss": 0.0015,
      "step": 196270
    },
    {
      "epoch": 6.542666666666666,
      "grad_norm": 0.17142309248447418,
      "learning_rate": 9.108333333333334e-06,
      "loss": 0.0017,
      "step": 196280
    },
    {
      "epoch": 6.543,
      "grad_norm": 0.029853248968720436,
      "learning_rate": 9.106250000000001e-06,
      "loss": 0.0014,
      "step": 196290
    },
    {
      "epoch": 6.543333333333333,
      "grad_norm": 0.18974579870700836,
      "learning_rate": 9.104166666666668e-06,
      "loss": 0.0017,
      "step": 196300
    },
    {
      "epoch": 6.543666666666667,
      "grad_norm": 0.11466111987829208,
      "learning_rate": 9.102083333333334e-06,
      "loss": 0.0017,
      "step": 196310
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.029937686398625374,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.0016,
      "step": 196320
    },
    {
      "epoch": 6.544333333333333,
      "grad_norm": 0.17332039773464203,
      "learning_rate": 9.097916666666666e-06,
      "loss": 0.0022,
      "step": 196330
    },
    {
      "epoch": 6.544666666666666,
      "grad_norm": 0.20028342306613922,
      "learning_rate": 9.095833333333334e-06,
      "loss": 0.0018,
      "step": 196340
    },
    {
      "epoch": 6.545,
      "grad_norm": 0.25221893191337585,
      "learning_rate": 9.09375e-06,
      "loss": 0.0017,
      "step": 196350
    },
    {
      "epoch": 6.545333333333334,
      "grad_norm": 0.1717270463705063,
      "learning_rate": 9.091666666666668e-06,
      "loss": 0.0015,
      "step": 196360
    },
    {
      "epoch": 6.5456666666666665,
      "grad_norm": 0.1151403859257698,
      "learning_rate": 9.089583333333333e-06,
      "loss": 0.0012,
      "step": 196370
    },
    {
      "epoch": 6.546,
      "grad_norm": 0.03292608633637428,
      "learning_rate": 9.0875e-06,
      "loss": 0.0011,
      "step": 196380
    },
    {
      "epoch": 6.546333333333333,
      "grad_norm": 0.2005719691514969,
      "learning_rate": 9.085416666666668e-06,
      "loss": 0.0015,
      "step": 196390
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 0.2564735412597656,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.002,
      "step": 196400
    },
    {
      "epoch": 6.547,
      "grad_norm": 0.3146510124206543,
      "learning_rate": 9.08125e-06,
      "loss": 0.0012,
      "step": 196410
    },
    {
      "epoch": 6.5473333333333334,
      "grad_norm": 0.03210698813199997,
      "learning_rate": 9.079166666666668e-06,
      "loss": 0.0024,
      "step": 196420
    },
    {
      "epoch": 6.547666666666666,
      "grad_norm": 0.1428273320198059,
      "learning_rate": 9.077083333333333e-06,
      "loss": 0.0018,
      "step": 196430
    },
    {
      "epoch": 6.548,
      "grad_norm": 0.02168465219438076,
      "learning_rate": 9.075e-06,
      "loss": 0.0017,
      "step": 196440
    },
    {
      "epoch": 6.548333333333334,
      "grad_norm": 0.14560236036777496,
      "learning_rate": 9.072916666666668e-06,
      "loss": 0.002,
      "step": 196450
    },
    {
      "epoch": 6.548666666666667,
      "grad_norm": 0.361910879611969,
      "learning_rate": 9.070833333333335e-06,
      "loss": 0.0015,
      "step": 196460
    },
    {
      "epoch": 6.549,
      "grad_norm": 0.08666853606700897,
      "learning_rate": 9.06875e-06,
      "loss": 0.0012,
      "step": 196470
    },
    {
      "epoch": 6.549333333333333,
      "grad_norm": 0.08707602322101593,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0017,
      "step": 196480
    },
    {
      "epoch": 6.549666666666667,
      "grad_norm": 0.02956111915409565,
      "learning_rate": 9.064583333333333e-06,
      "loss": 0.0018,
      "step": 196490
    },
    {
      "epoch": 6.55,
      "grad_norm": 0.45653948187828064,
      "learning_rate": 9.0625e-06,
      "loss": 0.0014,
      "step": 196500
    },
    {
      "epoch": 6.550333333333334,
      "grad_norm": 0.08568863570690155,
      "learning_rate": 9.060416666666667e-06,
      "loss": 0.0013,
      "step": 196510
    },
    {
      "epoch": 6.550666666666666,
      "grad_norm": 0.28575485944747925,
      "learning_rate": 9.058333333333334e-06,
      "loss": 0.0019,
      "step": 196520
    },
    {
      "epoch": 6.551,
      "grad_norm": 0.08603478223085403,
      "learning_rate": 9.056250000000002e-06,
      "loss": 0.0014,
      "step": 196530
    },
    {
      "epoch": 6.551333333333333,
      "grad_norm": 0.3137500286102295,
      "learning_rate": 9.054166666666667e-06,
      "loss": 0.0017,
      "step": 196540
    },
    {
      "epoch": 6.551666666666667,
      "grad_norm": 0.0858071967959404,
      "learning_rate": 9.052083333333333e-06,
      "loss": 0.0017,
      "step": 196550
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.17004847526550293,
      "learning_rate": 9.05e-06,
      "loss": 0.0015,
      "step": 196560
    },
    {
      "epoch": 6.552333333333333,
      "grad_norm": 0.14423765242099762,
      "learning_rate": 9.047916666666667e-06,
      "loss": 0.0013,
      "step": 196570
    },
    {
      "epoch": 6.552666666666667,
      "grad_norm": 0.021102821454405785,
      "learning_rate": 9.045833333333334e-06,
      "loss": 0.0018,
      "step": 196580
    },
    {
      "epoch": 6.553,
      "grad_norm": 0.058086246252059937,
      "learning_rate": 9.043750000000001e-06,
      "loss": 0.002,
      "step": 196590
    },
    {
      "epoch": 6.553333333333334,
      "grad_norm": 0.0860292986035347,
      "learning_rate": 9.041666666666668e-06,
      "loss": 0.0018,
      "step": 196600
    },
    {
      "epoch": 6.5536666666666665,
      "grad_norm": 0.17238855361938477,
      "learning_rate": 9.039583333333334e-06,
      "loss": 0.0019,
      "step": 196610
    },
    {
      "epoch": 6.554,
      "grad_norm": 0.2283082902431488,
      "learning_rate": 9.0375e-06,
      "loss": 0.0012,
      "step": 196620
    },
    {
      "epoch": 6.554333333333333,
      "grad_norm": 0.08576793968677521,
      "learning_rate": 9.035416666666667e-06,
      "loss": 0.0014,
      "step": 196630
    },
    {
      "epoch": 6.554666666666667,
      "grad_norm": 0.08607444167137146,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0019,
      "step": 196640
    },
    {
      "epoch": 6.555,
      "grad_norm": 0.2849890887737274,
      "learning_rate": 9.031250000000001e-06,
      "loss": 0.0021,
      "step": 196650
    },
    {
      "epoch": 6.5553333333333335,
      "grad_norm": 0.14639084041118622,
      "learning_rate": 9.029166666666668e-06,
      "loss": 0.0016,
      "step": 196660
    },
    {
      "epoch": 6.555666666666666,
      "grad_norm": 0.15185067057609558,
      "learning_rate": 9.027083333333334e-06,
      "loss": 0.0019,
      "step": 196670
    },
    {
      "epoch": 6.556,
      "grad_norm": 0.3141447603702545,
      "learning_rate": 9.025e-06,
      "loss": 0.0013,
      "step": 196680
    },
    {
      "epoch": 6.556333333333333,
      "grad_norm": 0.4769892692565918,
      "learning_rate": 9.022916666666666e-06,
      "loss": 0.0016,
      "step": 196690
    },
    {
      "epoch": 6.556666666666667,
      "grad_norm": 0.03094811551272869,
      "learning_rate": 9.020833333333334e-06,
      "loss": 0.0021,
      "step": 196700
    },
    {
      "epoch": 6.557,
      "grad_norm": 0.11145616322755814,
      "learning_rate": 9.01875e-06,
      "loss": 0.002,
      "step": 196710
    },
    {
      "epoch": 6.557333333333333,
      "grad_norm": 0.4073956608772278,
      "learning_rate": 9.016666666666668e-06,
      "loss": 0.0022,
      "step": 196720
    },
    {
      "epoch": 6.557666666666667,
      "grad_norm": 0.08730684965848923,
      "learning_rate": 9.014583333333333e-06,
      "loss": 0.0023,
      "step": 196730
    },
    {
      "epoch": 6.558,
      "grad_norm": 0.22864112257957458,
      "learning_rate": 9.0125e-06,
      "loss": 0.0023,
      "step": 196740
    },
    {
      "epoch": 6.558333333333334,
      "grad_norm": 0.22914573550224304,
      "learning_rate": 9.010416666666668e-06,
      "loss": 0.0012,
      "step": 196750
    },
    {
      "epoch": 6.558666666666666,
      "grad_norm": 0.2572261095046997,
      "learning_rate": 9.008333333333333e-06,
      "loss": 0.0014,
      "step": 196760
    },
    {
      "epoch": 6.559,
      "grad_norm": 0.17112219333648682,
      "learning_rate": 9.00625e-06,
      "loss": 0.0018,
      "step": 196770
    },
    {
      "epoch": 6.559333333333333,
      "grad_norm": 0.2613276541233063,
      "learning_rate": 9.004166666666668e-06,
      "loss": 0.0017,
      "step": 196780
    },
    {
      "epoch": 6.559666666666667,
      "grad_norm": 0.2285042554140091,
      "learning_rate": 9.002083333333333e-06,
      "loss": 0.0035,
      "step": 196790
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.3427709639072418,
      "learning_rate": 9e-06,
      "loss": 0.0016,
      "step": 196800
    },
    {
      "epoch": 6.560333333333333,
      "grad_norm": 0.2281031608581543,
      "learning_rate": 8.997916666666667e-06,
      "loss": 0.0019,
      "step": 196810
    },
    {
      "epoch": 6.560666666666666,
      "grad_norm": 0.03039485216140747,
      "learning_rate": 8.995833333333335e-06,
      "loss": 0.0017,
      "step": 196820
    },
    {
      "epoch": 6.561,
      "grad_norm": 0.05990823730826378,
      "learning_rate": 8.99375e-06,
      "loss": 0.0023,
      "step": 196830
    },
    {
      "epoch": 6.561333333333334,
      "grad_norm": 0.20120683312416077,
      "learning_rate": 8.991666666666667e-06,
      "loss": 0.0018,
      "step": 196840
    },
    {
      "epoch": 6.5616666666666665,
      "grad_norm": 0.08614829927682877,
      "learning_rate": 8.989583333333333e-06,
      "loss": 0.0012,
      "step": 196850
    },
    {
      "epoch": 6.562,
      "grad_norm": 0.42858192324638367,
      "learning_rate": 8.9875e-06,
      "loss": 0.0018,
      "step": 196860
    },
    {
      "epoch": 6.562333333333333,
      "grad_norm": 0.11473194509744644,
      "learning_rate": 8.985416666666667e-06,
      "loss": 0.0018,
      "step": 196870
    },
    {
      "epoch": 6.562666666666667,
      "grad_norm": 0.14321187138557434,
      "learning_rate": 8.983333333333334e-06,
      "loss": 0.0015,
      "step": 196880
    },
    {
      "epoch": 6.563,
      "grad_norm": 0.3418492078781128,
      "learning_rate": 8.981250000000001e-06,
      "loss": 0.0022,
      "step": 196890
    },
    {
      "epoch": 6.5633333333333335,
      "grad_norm": 0.19978131353855133,
      "learning_rate": 8.979166666666667e-06,
      "loss": 0.0022,
      "step": 196900
    },
    {
      "epoch": 6.563666666666666,
      "grad_norm": 0.1441684514284134,
      "learning_rate": 8.977083333333332e-06,
      "loss": 0.0013,
      "step": 196910
    },
    {
      "epoch": 6.564,
      "grad_norm": 0.4566192626953125,
      "learning_rate": 8.975e-06,
      "loss": 0.0016,
      "step": 196920
    },
    {
      "epoch": 6.564333333333334,
      "grad_norm": 0.057372596114873886,
      "learning_rate": 8.972916666666667e-06,
      "loss": 0.002,
      "step": 196930
    },
    {
      "epoch": 6.564666666666667,
      "grad_norm": 0.3028944432735443,
      "learning_rate": 8.970833333333334e-06,
      "loss": 0.0024,
      "step": 196940
    },
    {
      "epoch": 6.5649999999999995,
      "grad_norm": 0.10810039192438126,
      "learning_rate": 8.968750000000001e-06,
      "loss": 0.0026,
      "step": 196950
    },
    {
      "epoch": 6.565333333333333,
      "grad_norm": 0.20418094098567963,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0011,
      "step": 196960
    },
    {
      "epoch": 6.565666666666667,
      "grad_norm": 0.2286750078201294,
      "learning_rate": 8.964583333333334e-06,
      "loss": 0.0019,
      "step": 196970
    },
    {
      "epoch": 6.566,
      "grad_norm": 0.38468530774116516,
      "learning_rate": 8.9625e-06,
      "loss": 0.002,
      "step": 196980
    },
    {
      "epoch": 6.566333333333334,
      "grad_norm": 0.31391486525535583,
      "learning_rate": 8.960416666666666e-06,
      "loss": 0.002,
      "step": 196990
    },
    {
      "epoch": 6.566666666666666,
      "grad_norm": 0.2056717723608017,
      "learning_rate": 8.958333333333334e-06,
      "loss": 0.002,
      "step": 197000
    },
    {
      "epoch": 6.567,
      "grad_norm": 0.05821729823946953,
      "learning_rate": 8.956250000000001e-06,
      "loss": 0.002,
      "step": 197010
    },
    {
      "epoch": 6.567333333333333,
      "grad_norm": 0.19985772669315338,
      "learning_rate": 8.954166666666668e-06,
      "loss": 0.0014,
      "step": 197020
    },
    {
      "epoch": 6.567666666666667,
      "grad_norm": 0.3717697262763977,
      "learning_rate": 8.952083333333334e-06,
      "loss": 0.0013,
      "step": 197030
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.45697543025016785,
      "learning_rate": 8.95e-06,
      "loss": 0.0017,
      "step": 197040
    },
    {
      "epoch": 6.568333333333333,
      "grad_norm": 0.14335264265537262,
      "learning_rate": 8.947916666666666e-06,
      "loss": 0.0012,
      "step": 197050
    },
    {
      "epoch": 6.568666666666667,
      "grad_norm": 0.00903724692761898,
      "learning_rate": 8.945833333333333e-06,
      "loss": 0.0015,
      "step": 197060
    },
    {
      "epoch": 6.569,
      "grad_norm": 0.029369600117206573,
      "learning_rate": 8.94375e-06,
      "loss": 0.002,
      "step": 197070
    },
    {
      "epoch": 6.569333333333334,
      "grad_norm": 0.03029903583228588,
      "learning_rate": 8.941666666666668e-06,
      "loss": 0.0018,
      "step": 197080
    },
    {
      "epoch": 6.5696666666666665,
      "grad_norm": 0.006158256437629461,
      "learning_rate": 8.939583333333333e-06,
      "loss": 0.0017,
      "step": 197090
    },
    {
      "epoch": 6.57,
      "grad_norm": 0.3424830734729767,
      "learning_rate": 8.9375e-06,
      "loss": 0.0021,
      "step": 197100
    },
    {
      "epoch": 6.570333333333333,
      "grad_norm": 0.127105250954628,
      "learning_rate": 8.935416666666668e-06,
      "loss": 0.0016,
      "step": 197110
    },
    {
      "epoch": 6.570666666666667,
      "grad_norm": 0.19931632280349731,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0018,
      "step": 197120
    },
    {
      "epoch": 6.571,
      "grad_norm": 0.20036979019641876,
      "learning_rate": 8.93125e-06,
      "loss": 0.0021,
      "step": 197130
    },
    {
      "epoch": 6.5713333333333335,
      "grad_norm": 0.31673577427864075,
      "learning_rate": 8.929166666666667e-06,
      "loss": 0.0022,
      "step": 197140
    },
    {
      "epoch": 6.571666666666666,
      "grad_norm": 0.3180449306964874,
      "learning_rate": 8.927083333333333e-06,
      "loss": 0.0016,
      "step": 197150
    },
    {
      "epoch": 6.572,
      "grad_norm": 0.3041554391384125,
      "learning_rate": 8.925e-06,
      "loss": 0.0017,
      "step": 197160
    },
    {
      "epoch": 6.572333333333333,
      "grad_norm": 0.058081500232219696,
      "learning_rate": 8.922916666666667e-06,
      "loss": 0.0012,
      "step": 197170
    },
    {
      "epoch": 6.572666666666667,
      "grad_norm": 0.300324410200119,
      "learning_rate": 8.920833333333334e-06,
      "loss": 0.0021,
      "step": 197180
    },
    {
      "epoch": 6.573,
      "grad_norm": 0.1778302937746048,
      "learning_rate": 8.91875e-06,
      "loss": 0.0017,
      "step": 197190
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 0.14299651980400085,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.0014,
      "step": 197200
    },
    {
      "epoch": 6.573666666666667,
      "grad_norm": 0.22857612371444702,
      "learning_rate": 8.914583333333333e-06,
      "loss": 0.0019,
      "step": 197210
    },
    {
      "epoch": 6.574,
      "grad_norm": 0.11425454914569855,
      "learning_rate": 8.9125e-06,
      "loss": 0.002,
      "step": 197220
    },
    {
      "epoch": 6.574333333333334,
      "grad_norm": 0.14326557517051697,
      "learning_rate": 8.910416666666667e-06,
      "loss": 0.0013,
      "step": 197230
    },
    {
      "epoch": 6.574666666666666,
      "grad_norm": 0.029927249997854233,
      "learning_rate": 8.908333333333334e-06,
      "loss": 0.0027,
      "step": 197240
    },
    {
      "epoch": 6.575,
      "grad_norm": 0.17146830260753632,
      "learning_rate": 8.906250000000001e-06,
      "loss": 0.0012,
      "step": 197250
    },
    {
      "epoch": 6.575333333333333,
      "grad_norm": 0.08581490069627762,
      "learning_rate": 8.904166666666667e-06,
      "loss": 0.0014,
      "step": 197260
    },
    {
      "epoch": 6.575666666666667,
      "grad_norm": 0.11446848511695862,
      "learning_rate": 8.902083333333332e-06,
      "loss": 0.0016,
      "step": 197270
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.34521323442459106,
      "learning_rate": 8.9e-06,
      "loss": 0.0015,
      "step": 197280
    },
    {
      "epoch": 6.576333333333333,
      "grad_norm": 0.1441727578639984,
      "learning_rate": 8.897916666666667e-06,
      "loss": 0.0013,
      "step": 197290
    },
    {
      "epoch": 6.576666666666666,
      "grad_norm": 0.20007765293121338,
      "learning_rate": 8.895833333333334e-06,
      "loss": 0.0017,
      "step": 197300
    },
    {
      "epoch": 6.577,
      "grad_norm": 0.14416876435279846,
      "learning_rate": 8.893750000000001e-06,
      "loss": 0.002,
      "step": 197310
    },
    {
      "epoch": 6.577333333333334,
      "grad_norm": 0.33729538321495056,
      "learning_rate": 8.891666666666668e-06,
      "loss": 0.0018,
      "step": 197320
    },
    {
      "epoch": 6.5776666666666666,
      "grad_norm": 0.031690564006567,
      "learning_rate": 8.889583333333334e-06,
      "loss": 0.0019,
      "step": 197330
    },
    {
      "epoch": 6.578,
      "grad_norm": 0.031123057007789612,
      "learning_rate": 8.8875e-06,
      "loss": 0.0014,
      "step": 197340
    },
    {
      "epoch": 6.578333333333333,
      "grad_norm": 0.11456993222236633,
      "learning_rate": 8.885416666666666e-06,
      "loss": 0.0012,
      "step": 197350
    },
    {
      "epoch": 6.578666666666667,
      "grad_norm": 0.05839578062295914,
      "learning_rate": 8.883333333333334e-06,
      "loss": 0.0012,
      "step": 197360
    },
    {
      "epoch": 6.579,
      "grad_norm": 0.05906045436859131,
      "learning_rate": 8.88125e-06,
      "loss": 0.0017,
      "step": 197370
    },
    {
      "epoch": 6.5793333333333335,
      "grad_norm": 0.17148461937904358,
      "learning_rate": 8.879166666666668e-06,
      "loss": 0.0016,
      "step": 197380
    },
    {
      "epoch": 6.579666666666666,
      "grad_norm": 0.2545601725578308,
      "learning_rate": 8.877083333333333e-06,
      "loss": 0.0016,
      "step": 197390
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.08643707633018494,
      "learning_rate": 8.875e-06,
      "loss": 0.002,
      "step": 197400
    },
    {
      "epoch": 6.580333333333334,
      "grad_norm": 0.14304205775260925,
      "learning_rate": 8.872916666666666e-06,
      "loss": 0.0021,
      "step": 197410
    },
    {
      "epoch": 6.580666666666667,
      "grad_norm": 0.25703248381614685,
      "learning_rate": 8.870833333333333e-06,
      "loss": 0.0014,
      "step": 197420
    },
    {
      "epoch": 6.5809999999999995,
      "grad_norm": 0.342476487159729,
      "learning_rate": 8.86875e-06,
      "loss": 0.0019,
      "step": 197430
    },
    {
      "epoch": 6.581333333333333,
      "grad_norm": 0.2856300473213196,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0023,
      "step": 197440
    },
    {
      "epoch": 6.581666666666667,
      "grad_norm": 0.1439681351184845,
      "learning_rate": 8.864583333333333e-06,
      "loss": 0.002,
      "step": 197450
    },
    {
      "epoch": 6.582,
      "grad_norm": 0.1722218096256256,
      "learning_rate": 8.8625e-06,
      "loss": 0.0016,
      "step": 197460
    },
    {
      "epoch": 6.582333333333334,
      "grad_norm": 0.5422499775886536,
      "learning_rate": 8.860416666666667e-06,
      "loss": 0.0023,
      "step": 197470
    },
    {
      "epoch": 6.582666666666666,
      "grad_norm": 0.49860459566116333,
      "learning_rate": 8.858333333333333e-06,
      "loss": 0.0022,
      "step": 197480
    },
    {
      "epoch": 6.583,
      "grad_norm": 0.08665917068719864,
      "learning_rate": 8.85625e-06,
      "loss": 0.0014,
      "step": 197490
    },
    {
      "epoch": 6.583333333333333,
      "grad_norm": 0.03197093307971954,
      "learning_rate": 8.854166666666667e-06,
      "loss": 0.0018,
      "step": 197500
    },
    {
      "epoch": 6.583666666666667,
      "grad_norm": 0.11709865182638168,
      "learning_rate": 8.852083333333334e-06,
      "loss": 0.0018,
      "step": 197510
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.2056141346693039,
      "learning_rate": 8.85e-06,
      "loss": 0.0012,
      "step": 197520
    },
    {
      "epoch": 6.584333333333333,
      "grad_norm": 0.05715376138687134,
      "learning_rate": 8.847916666666667e-06,
      "loss": 0.0015,
      "step": 197530
    },
    {
      "epoch": 6.584666666666667,
      "grad_norm": 0.14286647737026215,
      "learning_rate": 8.845833333333334e-06,
      "loss": 0.0013,
      "step": 197540
    },
    {
      "epoch": 6.585,
      "grad_norm": 0.08634497225284576,
      "learning_rate": 8.84375e-06,
      "loss": 0.0013,
      "step": 197550
    },
    {
      "epoch": 6.585333333333334,
      "grad_norm": 0.25705957412719727,
      "learning_rate": 8.841666666666667e-06,
      "loss": 0.0019,
      "step": 197560
    },
    {
      "epoch": 6.585666666666667,
      "grad_norm": 0.23452170193195343,
      "learning_rate": 8.839583333333334e-06,
      "loss": 0.0012,
      "step": 197570
    },
    {
      "epoch": 6.586,
      "grad_norm": 0.17159710824489594,
      "learning_rate": 8.8375e-06,
      "loss": 0.0016,
      "step": 197580
    },
    {
      "epoch": 6.586333333333333,
      "grad_norm": 0.031540583819150925,
      "learning_rate": 8.835416666666667e-06,
      "loss": 0.0016,
      "step": 197590
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 0.23612461984157562,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0023,
      "step": 197600
    },
    {
      "epoch": 6.587,
      "grad_norm": 0.22856034338474274,
      "learning_rate": 8.831250000000001e-06,
      "loss": 0.0012,
      "step": 197610
    },
    {
      "epoch": 6.5873333333333335,
      "grad_norm": 0.12449896335601807,
      "learning_rate": 8.829166666666667e-06,
      "loss": 0.0021,
      "step": 197620
    },
    {
      "epoch": 6.587666666666666,
      "grad_norm": 0.14286023378372192,
      "learning_rate": 8.827083333333334e-06,
      "loss": 0.0015,
      "step": 197630
    },
    {
      "epoch": 6.588,
      "grad_norm": 0.011298315599560738,
      "learning_rate": 8.825e-06,
      "loss": 0.0014,
      "step": 197640
    },
    {
      "epoch": 6.588333333333333,
      "grad_norm": 0.0347595252096653,
      "learning_rate": 8.822916666666667e-06,
      "loss": 0.0017,
      "step": 197650
    },
    {
      "epoch": 6.588666666666667,
      "grad_norm": 0.08633102476596832,
      "learning_rate": 8.820833333333334e-06,
      "loss": 0.0024,
      "step": 197660
    },
    {
      "epoch": 6.589,
      "grad_norm": 0.2006223201751709,
      "learning_rate": 8.818750000000001e-06,
      "loss": 0.0026,
      "step": 197670
    },
    {
      "epoch": 6.589333333333333,
      "grad_norm": 0.14323537051677704,
      "learning_rate": 8.816666666666668e-06,
      "loss": 0.0015,
      "step": 197680
    },
    {
      "epoch": 6.589666666666667,
      "grad_norm": 0.0924854651093483,
      "learning_rate": 8.814583333333334e-06,
      "loss": 0.0015,
      "step": 197690
    },
    {
      "epoch": 6.59,
      "grad_norm": 0.2854941487312317,
      "learning_rate": 8.8125e-06,
      "loss": 0.0014,
      "step": 197700
    },
    {
      "epoch": 6.590333333333334,
      "grad_norm": 0.31405308842658997,
      "learning_rate": 8.810416666666666e-06,
      "loss": 0.0015,
      "step": 197710
    },
    {
      "epoch": 6.5906666666666665,
      "grad_norm": 0.015407543629407883,
      "learning_rate": 8.808333333333333e-06,
      "loss": 0.0014,
      "step": 197720
    },
    {
      "epoch": 6.591,
      "grad_norm": 0.1142473816871643,
      "learning_rate": 8.80625e-06,
      "loss": 0.0012,
      "step": 197730
    },
    {
      "epoch": 6.591333333333333,
      "grad_norm": 0.20103619992733002,
      "learning_rate": 8.804166666666668e-06,
      "loss": 0.0015,
      "step": 197740
    },
    {
      "epoch": 6.591666666666667,
      "grad_norm": 0.1428421437740326,
      "learning_rate": 8.802083333333335e-06,
      "loss": 0.0018,
      "step": 197750
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.11442796140909195,
      "learning_rate": 8.8e-06,
      "loss": 0.0015,
      "step": 197760
    },
    {
      "epoch": 6.592333333333333,
      "grad_norm": 0.2577023208141327,
      "learning_rate": 8.797916666666668e-06,
      "loss": 0.0019,
      "step": 197770
    },
    {
      "epoch": 6.592666666666666,
      "grad_norm": 0.28554511070251465,
      "learning_rate": 8.795833333333333e-06,
      "loss": 0.0019,
      "step": 197780
    },
    {
      "epoch": 6.593,
      "grad_norm": 0.1187496930360794,
      "learning_rate": 8.79375e-06,
      "loss": 0.0013,
      "step": 197790
    },
    {
      "epoch": 6.593333333333334,
      "grad_norm": 0.033383339643478394,
      "learning_rate": 8.791666666666667e-06,
      "loss": 0.0011,
      "step": 197800
    },
    {
      "epoch": 6.593666666666667,
      "grad_norm": 0.12340828031301498,
      "learning_rate": 8.789583333333335e-06,
      "loss": 0.0025,
      "step": 197810
    },
    {
      "epoch": 6.594,
      "grad_norm": 0.20006610453128815,
      "learning_rate": 8.7875e-06,
      "loss": 0.002,
      "step": 197820
    },
    {
      "epoch": 6.594333333333333,
      "grad_norm": 0.2856135964393616,
      "learning_rate": 8.785416666666667e-06,
      "loss": 0.0017,
      "step": 197830
    },
    {
      "epoch": 6.594666666666667,
      "grad_norm": 0.1978408694267273,
      "learning_rate": 8.783333333333335e-06,
      "loss": 0.0023,
      "step": 197840
    },
    {
      "epoch": 6.595,
      "grad_norm": 0.1156095638871193,
      "learning_rate": 8.78125e-06,
      "loss": 0.0021,
      "step": 197850
    },
    {
      "epoch": 6.5953333333333335,
      "grad_norm": 0.1137932538986206,
      "learning_rate": 8.779166666666667e-06,
      "loss": 0.0012,
      "step": 197860
    },
    {
      "epoch": 6.595666666666666,
      "grad_norm": 0.08634459972381592,
      "learning_rate": 8.777083333333334e-06,
      "loss": 0.0014,
      "step": 197870
    },
    {
      "epoch": 6.596,
      "grad_norm": 0.09103786945343018,
      "learning_rate": 8.775e-06,
      "loss": 0.0017,
      "step": 197880
    },
    {
      "epoch": 6.596333333333334,
      "grad_norm": 0.1713150441646576,
      "learning_rate": 8.772916666666667e-06,
      "loss": 0.0014,
      "step": 197890
    },
    {
      "epoch": 6.596666666666667,
      "grad_norm": 0.3422580659389496,
      "learning_rate": 8.770833333333334e-06,
      "loss": 0.0016,
      "step": 197900
    },
    {
      "epoch": 6.5969999999999995,
      "grad_norm": 0.19979383051395416,
      "learning_rate": 8.768750000000001e-06,
      "loss": 0.0019,
      "step": 197910
    },
    {
      "epoch": 6.597333333333333,
      "grad_norm": 0.17214877903461456,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.0023,
      "step": 197920
    },
    {
      "epoch": 6.597666666666667,
      "grad_norm": 0.02917073667049408,
      "learning_rate": 8.764583333333334e-06,
      "loss": 0.0022,
      "step": 197930
    },
    {
      "epoch": 6.598,
      "grad_norm": 0.08596312254667282,
      "learning_rate": 8.7625e-06,
      "loss": 0.0013,
      "step": 197940
    },
    {
      "epoch": 6.598333333333334,
      "grad_norm": 0.11473532766103745,
      "learning_rate": 8.760416666666667e-06,
      "loss": 0.0014,
      "step": 197950
    },
    {
      "epoch": 6.5986666666666665,
      "grad_norm": 0.19975784420967102,
      "learning_rate": 8.758333333333334e-06,
      "loss": 0.0011,
      "step": 197960
    },
    {
      "epoch": 6.599,
      "grad_norm": 0.22859786450862885,
      "learning_rate": 8.756250000000001e-06,
      "loss": 0.0015,
      "step": 197970
    },
    {
      "epoch": 6.599333333333333,
      "grad_norm": 0.3994442820549011,
      "learning_rate": 8.754166666666668e-06,
      "loss": 0.0017,
      "step": 197980
    },
    {
      "epoch": 6.599666666666667,
      "grad_norm": 0.06179335340857506,
      "learning_rate": 8.752083333333334e-06,
      "loss": 0.0018,
      "step": 197990
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.08620531111955643,
      "learning_rate": 8.75e-06,
      "loss": 0.0011,
      "step": 198000
    },
    {
      "epoch": 6.600333333333333,
      "grad_norm": 0.13818290829658508,
      "learning_rate": 8.747916666666666e-06,
      "loss": 0.0022,
      "step": 198010
    },
    {
      "epoch": 6.600666666666667,
      "grad_norm": 0.3457454741001129,
      "learning_rate": 8.745833333333334e-06,
      "loss": 0.0016,
      "step": 198020
    },
    {
      "epoch": 6.601,
      "grad_norm": 0.05828791856765747,
      "learning_rate": 8.74375e-06,
      "loss": 0.0013,
      "step": 198030
    },
    {
      "epoch": 6.601333333333334,
      "grad_norm": 0.3956378996372223,
      "learning_rate": 8.741666666666668e-06,
      "loss": 0.002,
      "step": 198040
    },
    {
      "epoch": 6.601666666666667,
      "grad_norm": 0.11515834182500839,
      "learning_rate": 8.739583333333335e-06,
      "loss": 0.0018,
      "step": 198050
    },
    {
      "epoch": 6.602,
      "grad_norm": 0.1713503748178482,
      "learning_rate": 8.7375e-06,
      "loss": 0.0014,
      "step": 198060
    },
    {
      "epoch": 6.602333333333333,
      "grad_norm": 0.029946615919470787,
      "learning_rate": 8.735416666666666e-06,
      "loss": 0.0018,
      "step": 198070
    },
    {
      "epoch": 6.602666666666667,
      "grad_norm": 0.4750921428203583,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0022,
      "step": 198080
    },
    {
      "epoch": 6.603,
      "grad_norm": 0.007829072885215282,
      "learning_rate": 8.73125e-06,
      "loss": 0.0013,
      "step": 198090
    },
    {
      "epoch": 6.6033333333333335,
      "grad_norm": 0.2852993309497833,
      "learning_rate": 8.729166666666668e-06,
      "loss": 0.0023,
      "step": 198100
    },
    {
      "epoch": 6.603666666666666,
      "grad_norm": 0.20056574046611786,
      "learning_rate": 8.727083333333335e-06,
      "loss": 0.0018,
      "step": 198110
    },
    {
      "epoch": 6.604,
      "grad_norm": 0.07036742568016052,
      "learning_rate": 8.725e-06,
      "loss": 0.0013,
      "step": 198120
    },
    {
      "epoch": 6.604333333333333,
      "grad_norm": 0.029554415494203568,
      "learning_rate": 8.722916666666668e-06,
      "loss": 0.0017,
      "step": 198130
    },
    {
      "epoch": 6.604666666666667,
      "grad_norm": 0.3718566596508026,
      "learning_rate": 8.720833333333333e-06,
      "loss": 0.0017,
      "step": 198140
    },
    {
      "epoch": 6.605,
      "grad_norm": 0.030370641499757767,
      "learning_rate": 8.71875e-06,
      "loss": 0.0019,
      "step": 198150
    },
    {
      "epoch": 6.605333333333333,
      "grad_norm": 0.2854440212249756,
      "learning_rate": 8.716666666666667e-06,
      "loss": 0.0019,
      "step": 198160
    },
    {
      "epoch": 6.605666666666667,
      "grad_norm": 0.22947807610034943,
      "learning_rate": 8.714583333333335e-06,
      "loss": 0.0016,
      "step": 198170
    },
    {
      "epoch": 6.606,
      "grad_norm": 0.030450019985437393,
      "learning_rate": 8.7125e-06,
      "loss": 0.0014,
      "step": 198180
    },
    {
      "epoch": 6.606333333333334,
      "grad_norm": 0.17156438529491425,
      "learning_rate": 8.710416666666667e-06,
      "loss": 0.0014,
      "step": 198190
    },
    {
      "epoch": 6.6066666666666665,
      "grad_norm": 0.15266555547714233,
      "learning_rate": 8.708333333333334e-06,
      "loss": 0.0021,
      "step": 198200
    },
    {
      "epoch": 6.607,
      "grad_norm": 0.0162942036986351,
      "learning_rate": 8.70625e-06,
      "loss": 0.0022,
      "step": 198210
    },
    {
      "epoch": 6.607333333333333,
      "grad_norm": 0.11507029831409454,
      "learning_rate": 8.704166666666667e-06,
      "loss": 0.0015,
      "step": 198220
    },
    {
      "epoch": 6.607666666666667,
      "grad_norm": 0.14334070682525635,
      "learning_rate": 8.702083333333334e-06,
      "loss": 0.0017,
      "step": 198230
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.11460979282855988,
      "learning_rate": 8.7e-06,
      "loss": 0.0018,
      "step": 198240
    },
    {
      "epoch": 6.608333333333333,
      "grad_norm": 0.20173504948616028,
      "learning_rate": 8.697916666666667e-06,
      "loss": 0.003,
      "step": 198250
    },
    {
      "epoch": 6.608666666666666,
      "grad_norm": 0.1715337485074997,
      "learning_rate": 8.695833333333334e-06,
      "loss": 0.002,
      "step": 198260
    },
    {
      "epoch": 6.609,
      "grad_norm": 0.34275826811790466,
      "learning_rate": 8.693750000000001e-06,
      "loss": 0.0015,
      "step": 198270
    },
    {
      "epoch": 6.609333333333334,
      "grad_norm": 0.22860820591449738,
      "learning_rate": 8.691666666666667e-06,
      "loss": 0.0017,
      "step": 198280
    },
    {
      "epoch": 6.609666666666667,
      "grad_norm": 0.1995033472776413,
      "learning_rate": 8.689583333333334e-06,
      "loss": 0.0013,
      "step": 198290
    },
    {
      "epoch": 6.61,
      "grad_norm": 0.1438673585653305,
      "learning_rate": 8.6875e-06,
      "loss": 0.0019,
      "step": 198300
    },
    {
      "epoch": 6.610333333333333,
      "grad_norm": 0.5140459537506104,
      "learning_rate": 8.685416666666667e-06,
      "loss": 0.0019,
      "step": 198310
    },
    {
      "epoch": 6.610666666666667,
      "grad_norm": 0.08606114983558655,
      "learning_rate": 8.683333333333334e-06,
      "loss": 0.0024,
      "step": 198320
    },
    {
      "epoch": 6.611,
      "grad_norm": 0.28548985719680786,
      "learning_rate": 8.681250000000001e-06,
      "loss": 0.0015,
      "step": 198330
    },
    {
      "epoch": 6.6113333333333335,
      "grad_norm": 0.25750264525413513,
      "learning_rate": 8.679166666666668e-06,
      "loss": 0.0019,
      "step": 198340
    },
    {
      "epoch": 6.611666666666666,
      "grad_norm": 0.34287911653518677,
      "learning_rate": 8.677083333333334e-06,
      "loss": 0.0014,
      "step": 198350
    },
    {
      "epoch": 6.612,
      "grad_norm": 0.058560676872730255,
      "learning_rate": 8.674999999999999e-06,
      "loss": 0.0013,
      "step": 198360
    },
    {
      "epoch": 6.612333333333333,
      "grad_norm": 0.1431506872177124,
      "learning_rate": 8.672916666666666e-06,
      "loss": 0.0023,
      "step": 198370
    },
    {
      "epoch": 6.612666666666667,
      "grad_norm": 0.033475715667009354,
      "learning_rate": 8.670833333333333e-06,
      "loss": 0.0018,
      "step": 198380
    },
    {
      "epoch": 6.6129999999999995,
      "grad_norm": 0.07805239409208298,
      "learning_rate": 8.66875e-06,
      "loss": 0.0015,
      "step": 198390
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.3426651954650879,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0017,
      "step": 198400
    },
    {
      "epoch": 6.613666666666667,
      "grad_norm": 0.11523109674453735,
      "learning_rate": 8.664583333333335e-06,
      "loss": 0.0016,
      "step": 198410
    },
    {
      "epoch": 6.614,
      "grad_norm": 0.059710998088121414,
      "learning_rate": 8.6625e-06,
      "loss": 0.0015,
      "step": 198420
    },
    {
      "epoch": 6.614333333333334,
      "grad_norm": 0.03377051278948784,
      "learning_rate": 8.660416666666666e-06,
      "loss": 0.0011,
      "step": 198430
    },
    {
      "epoch": 6.6146666666666665,
      "grad_norm": 0.08624632656574249,
      "learning_rate": 8.658333333333333e-06,
      "loss": 0.0019,
      "step": 198440
    },
    {
      "epoch": 6.615,
      "grad_norm": 0.14305849373340607,
      "learning_rate": 8.65625e-06,
      "loss": 0.0014,
      "step": 198450
    },
    {
      "epoch": 6.615333333333333,
      "grad_norm": 0.05799537152051926,
      "learning_rate": 8.654166666666668e-06,
      "loss": 0.0013,
      "step": 198460
    },
    {
      "epoch": 6.615666666666667,
      "grad_norm": 0.11435161530971527,
      "learning_rate": 8.652083333333335e-06,
      "loss": 0.0016,
      "step": 198470
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.14349974691867828,
      "learning_rate": 8.65e-06,
      "loss": 0.0014,
      "step": 198480
    },
    {
      "epoch": 6.616333333333333,
      "grad_norm": 0.060284923762083054,
      "learning_rate": 8.647916666666667e-06,
      "loss": 0.0016,
      "step": 198490
    },
    {
      "epoch": 6.616666666666667,
      "grad_norm": 0.2577299177646637,
      "learning_rate": 8.645833333333333e-06,
      "loss": 0.0014,
      "step": 198500
    },
    {
      "epoch": 6.617,
      "grad_norm": 0.14371861517429352,
      "learning_rate": 8.64375e-06,
      "loss": 0.0013,
      "step": 198510
    },
    {
      "epoch": 6.617333333333333,
      "grad_norm": 0.20050926506519318,
      "learning_rate": 8.641666666666667e-06,
      "loss": 0.0019,
      "step": 198520
    },
    {
      "epoch": 6.617666666666667,
      "grad_norm": 0.029247621074318886,
      "learning_rate": 8.639583333333334e-06,
      "loss": 0.0018,
      "step": 198530
    },
    {
      "epoch": 6.618,
      "grad_norm": 0.11465582251548767,
      "learning_rate": 8.6375e-06,
      "loss": 0.0023,
      "step": 198540
    },
    {
      "epoch": 6.618333333333333,
      "grad_norm": 0.029122574254870415,
      "learning_rate": 8.635416666666667e-06,
      "loss": 0.0013,
      "step": 198550
    },
    {
      "epoch": 6.618666666666667,
      "grad_norm": 0.01359604112803936,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.0016,
      "step": 198560
    },
    {
      "epoch": 6.619,
      "grad_norm": 0.05725398659706116,
      "learning_rate": 8.63125e-06,
      "loss": 0.0018,
      "step": 198570
    },
    {
      "epoch": 6.6193333333333335,
      "grad_norm": 0.05912056937813759,
      "learning_rate": 8.629166666666667e-06,
      "loss": 0.0013,
      "step": 198580
    },
    {
      "epoch": 6.619666666666666,
      "grad_norm": 0.25754454731941223,
      "learning_rate": 8.627083333333334e-06,
      "loss": 0.0014,
      "step": 198590
    },
    {
      "epoch": 6.62,
      "grad_norm": 0.08875502645969391,
      "learning_rate": 8.625e-06,
      "loss": 0.0011,
      "step": 198600
    },
    {
      "epoch": 6.620333333333333,
      "grad_norm": 0.08670780807733536,
      "learning_rate": 8.622916666666667e-06,
      "loss": 0.0019,
      "step": 198610
    },
    {
      "epoch": 6.620666666666667,
      "grad_norm": 0.3194991946220398,
      "learning_rate": 8.620833333333334e-06,
      "loss": 0.0019,
      "step": 198620
    },
    {
      "epoch": 6.621,
      "grad_norm": 0.030316771939396858,
      "learning_rate": 8.618750000000001e-06,
      "loss": 0.0021,
      "step": 198630
    },
    {
      "epoch": 6.621333333333333,
      "grad_norm": 0.2041294276714325,
      "learning_rate": 8.616666666666667e-06,
      "loss": 0.0013,
      "step": 198640
    },
    {
      "epoch": 6.621666666666667,
      "grad_norm": 0.4688202440738678,
      "learning_rate": 8.614583333333334e-06,
      "loss": 0.0021,
      "step": 198650
    },
    {
      "epoch": 6.622,
      "grad_norm": 0.37912991642951965,
      "learning_rate": 8.6125e-06,
      "loss": 0.0023,
      "step": 198660
    },
    {
      "epoch": 6.622333333333334,
      "grad_norm": 0.009866210632026196,
      "learning_rate": 8.610416666666666e-06,
      "loss": 0.0019,
      "step": 198670
    },
    {
      "epoch": 6.6226666666666665,
      "grad_norm": 0.33462443947792053,
      "learning_rate": 8.608333333333334e-06,
      "loss": 0.002,
      "step": 198680
    },
    {
      "epoch": 6.623,
      "grad_norm": 0.25718173384666443,
      "learning_rate": 8.60625e-06,
      "loss": 0.0016,
      "step": 198690
    },
    {
      "epoch": 6.623333333333333,
      "grad_norm": 0.07132983952760696,
      "learning_rate": 8.604166666666668e-06,
      "loss": 0.0017,
      "step": 198700
    },
    {
      "epoch": 6.623666666666667,
      "grad_norm": 0.05700989440083504,
      "learning_rate": 8.602083333333333e-06,
      "loss": 0.0014,
      "step": 198710
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.11517960578203201,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0014,
      "step": 198720
    },
    {
      "epoch": 6.624333333333333,
      "grad_norm": 0.05743533372879028,
      "learning_rate": 8.597916666666666e-06,
      "loss": 0.0024,
      "step": 198730
    },
    {
      "epoch": 6.624666666666666,
      "grad_norm": 0.057151276618242264,
      "learning_rate": 8.595833333333333e-06,
      "loss": 0.0014,
      "step": 198740
    },
    {
      "epoch": 6.625,
      "grad_norm": 0.308864951133728,
      "learning_rate": 8.59375e-06,
      "loss": 0.002,
      "step": 198750
    },
    {
      "epoch": 6.625333333333334,
      "grad_norm": 0.22889937460422516,
      "learning_rate": 8.591666666666668e-06,
      "loss": 0.0018,
      "step": 198760
    },
    {
      "epoch": 6.625666666666667,
      "grad_norm": 0.28598207235336304,
      "learning_rate": 8.589583333333335e-06,
      "loss": 0.002,
      "step": 198770
    },
    {
      "epoch": 6.626,
      "grad_norm": 0.19952961802482605,
      "learning_rate": 8.5875e-06,
      "loss": 0.0023,
      "step": 198780
    },
    {
      "epoch": 6.626333333333333,
      "grad_norm": 0.08709530532360077,
      "learning_rate": 8.585416666666666e-06,
      "loss": 0.0017,
      "step": 198790
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.1151137501001358,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.0024,
      "step": 198800
    },
    {
      "epoch": 6.627,
      "grad_norm": 0.028981119394302368,
      "learning_rate": 8.58125e-06,
      "loss": 0.0019,
      "step": 198810
    },
    {
      "epoch": 6.6273333333333335,
      "grad_norm": 0.0816725417971611,
      "learning_rate": 8.579166666666667e-06,
      "loss": 0.002,
      "step": 198820
    },
    {
      "epoch": 6.627666666666666,
      "grad_norm": 0.1147850975394249,
      "learning_rate": 8.577083333333335e-06,
      "loss": 0.0017,
      "step": 198830
    },
    {
      "epoch": 6.628,
      "grad_norm": 0.4281921684741974,
      "learning_rate": 8.575000000000002e-06,
      "loss": 0.0018,
      "step": 198840
    },
    {
      "epoch": 6.628333333333333,
      "grad_norm": 0.3153214156627655,
      "learning_rate": 8.572916666666667e-06,
      "loss": 0.002,
      "step": 198850
    },
    {
      "epoch": 6.628666666666667,
      "grad_norm": 0.05770869180560112,
      "learning_rate": 8.570833333333333e-06,
      "loss": 0.0017,
      "step": 198860
    },
    {
      "epoch": 6.629,
      "grad_norm": 0.20010383427143097,
      "learning_rate": 8.56875e-06,
      "loss": 0.0014,
      "step": 198870
    },
    {
      "epoch": 6.629333333333333,
      "grad_norm": 0.3387906849384308,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0028,
      "step": 198880
    },
    {
      "epoch": 6.629666666666667,
      "grad_norm": 0.11388823390007019,
      "learning_rate": 8.564583333333334e-06,
      "loss": 0.0014,
      "step": 198890
    },
    {
      "epoch": 6.63,
      "grad_norm": 0.143669992685318,
      "learning_rate": 8.562500000000001e-06,
      "loss": 0.0019,
      "step": 198900
    },
    {
      "epoch": 6.630333333333334,
      "grad_norm": 0.34372174739837646,
      "learning_rate": 8.560416666666667e-06,
      "loss": 0.0016,
      "step": 198910
    },
    {
      "epoch": 6.6306666666666665,
      "grad_norm": 0.14384260773658752,
      "learning_rate": 8.558333333333334e-06,
      "loss": 0.0013,
      "step": 198920
    },
    {
      "epoch": 6.631,
      "grad_norm": 0.14306946098804474,
      "learning_rate": 8.55625e-06,
      "loss": 0.0015,
      "step": 198930
    },
    {
      "epoch": 6.631333333333333,
      "grad_norm": 0.31456005573272705,
      "learning_rate": 8.554166666666667e-06,
      "loss": 0.0015,
      "step": 198940
    },
    {
      "epoch": 6.631666666666667,
      "grad_norm": 0.20143954455852509,
      "learning_rate": 8.552083333333334e-06,
      "loss": 0.0023,
      "step": 198950
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.22796539962291718,
      "learning_rate": 8.550000000000001e-06,
      "loss": 0.0017,
      "step": 198960
    },
    {
      "epoch": 6.632333333333333,
      "grad_norm": 0.007995528168976307,
      "learning_rate": 8.547916666666667e-06,
      "loss": 0.0019,
      "step": 198970
    },
    {
      "epoch": 6.632666666666667,
      "grad_norm": 0.08569960296154022,
      "learning_rate": 8.545833333333334e-06,
      "loss": 0.0024,
      "step": 198980
    },
    {
      "epoch": 6.633,
      "grad_norm": 0.19945131242275238,
      "learning_rate": 8.543750000000001e-06,
      "loss": 0.0021,
      "step": 198990
    },
    {
      "epoch": 6.633333333333333,
      "grad_norm": 0.14347560703754425,
      "learning_rate": 8.541666666666666e-06,
      "loss": 0.0015,
      "step": 199000
    },
    {
      "epoch": 6.633666666666667,
      "grad_norm": 0.08635111898183823,
      "learning_rate": 8.539583333333334e-06,
      "loss": 0.0015,
      "step": 199010
    },
    {
      "epoch": 6.634,
      "grad_norm": 0.08562108129262924,
      "learning_rate": 8.5375e-06,
      "loss": 0.0015,
      "step": 199020
    },
    {
      "epoch": 6.634333333333333,
      "grad_norm": 0.1151740550994873,
      "learning_rate": 8.535416666666666e-06,
      "loss": 0.0016,
      "step": 199030
    },
    {
      "epoch": 6.634666666666667,
      "grad_norm": 0.37094831466674805,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0016,
      "step": 199040
    },
    {
      "epoch": 6.635,
      "grad_norm": 0.144350066781044,
      "learning_rate": 8.53125e-06,
      "loss": 0.0019,
      "step": 199050
    },
    {
      "epoch": 6.6353333333333335,
      "grad_norm": 0.28548482060432434,
      "learning_rate": 8.529166666666668e-06,
      "loss": 0.002,
      "step": 199060
    },
    {
      "epoch": 6.635666666666666,
      "grad_norm": 0.4281075894832611,
      "learning_rate": 8.527083333333333e-06,
      "loss": 0.0022,
      "step": 199070
    },
    {
      "epoch": 6.636,
      "grad_norm": 0.14438286423683167,
      "learning_rate": 8.525e-06,
      "loss": 0.002,
      "step": 199080
    },
    {
      "epoch": 6.636333333333333,
      "grad_norm": 0.030718931928277016,
      "learning_rate": 8.522916666666666e-06,
      "loss": 0.0019,
      "step": 199090
    },
    {
      "epoch": 6.636666666666667,
      "grad_norm": 0.029744885861873627,
      "learning_rate": 8.520833333333333e-06,
      "loss": 0.0026,
      "step": 199100
    },
    {
      "epoch": 6.6370000000000005,
      "grad_norm": 0.28565341234207153,
      "learning_rate": 8.51875e-06,
      "loss": 0.0014,
      "step": 199110
    },
    {
      "epoch": 6.637333333333333,
      "grad_norm": 0.11443568766117096,
      "learning_rate": 8.516666666666668e-06,
      "loss": 0.0014,
      "step": 199120
    },
    {
      "epoch": 6.637666666666667,
      "grad_norm": 0.11532745510339737,
      "learning_rate": 8.514583333333335e-06,
      "loss": 0.0016,
      "step": 199130
    },
    {
      "epoch": 6.638,
      "grad_norm": 0.21312330663204193,
      "learning_rate": 8.5125e-06,
      "loss": 0.0019,
      "step": 199140
    },
    {
      "epoch": 6.638333333333334,
      "grad_norm": 0.14284200966358185,
      "learning_rate": 8.510416666666666e-06,
      "loss": 0.0017,
      "step": 199150
    },
    {
      "epoch": 6.6386666666666665,
      "grad_norm": 0.08610154688358307,
      "learning_rate": 8.508333333333333e-06,
      "loss": 0.0023,
      "step": 199160
    },
    {
      "epoch": 6.639,
      "grad_norm": 0.3142753541469574,
      "learning_rate": 8.50625e-06,
      "loss": 0.0021,
      "step": 199170
    },
    {
      "epoch": 6.639333333333333,
      "grad_norm": 0.40648403763771057,
      "learning_rate": 8.504166666666667e-06,
      "loss": 0.0014,
      "step": 199180
    },
    {
      "epoch": 6.639666666666667,
      "grad_norm": 0.05854309722781181,
      "learning_rate": 8.502083333333334e-06,
      "loss": 0.002,
      "step": 199190
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.1713627278804779,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0014,
      "step": 199200
    },
    {
      "epoch": 6.640333333333333,
      "grad_norm": 0.06443023681640625,
      "learning_rate": 8.497916666666667e-06,
      "loss": 0.0021,
      "step": 199210
    },
    {
      "epoch": 6.640666666666666,
      "grad_norm": 0.008348004892468452,
      "learning_rate": 8.495833333333333e-06,
      "loss": 0.0018,
      "step": 199220
    },
    {
      "epoch": 6.641,
      "grad_norm": 0.17142711579799652,
      "learning_rate": 8.49375e-06,
      "loss": 0.0016,
      "step": 199230
    },
    {
      "epoch": 6.641333333333334,
      "grad_norm": 0.06445932388305664,
      "learning_rate": 8.491666666666667e-06,
      "loss": 0.0016,
      "step": 199240
    },
    {
      "epoch": 6.641666666666667,
      "grad_norm": 0.400331050157547,
      "learning_rate": 8.489583333333334e-06,
      "loss": 0.0012,
      "step": 199250
    },
    {
      "epoch": 6.642,
      "grad_norm": 0.030618321150541306,
      "learning_rate": 8.487500000000001e-06,
      "loss": 0.002,
      "step": 199260
    },
    {
      "epoch": 6.642333333333333,
      "grad_norm": 0.2571604549884796,
      "learning_rate": 8.485416666666667e-06,
      "loss": 0.0021,
      "step": 199270
    },
    {
      "epoch": 6.642666666666667,
      "grad_norm": 0.39971473813056946,
      "learning_rate": 8.483333333333334e-06,
      "loss": 0.0012,
      "step": 199280
    },
    {
      "epoch": 6.643,
      "grad_norm": 0.37089529633522034,
      "learning_rate": 8.48125e-06,
      "loss": 0.0013,
      "step": 199290
    },
    {
      "epoch": 6.6433333333333335,
      "grad_norm": 0.05941575765609741,
      "learning_rate": 8.479166666666667e-06,
      "loss": 0.0024,
      "step": 199300
    },
    {
      "epoch": 6.643666666666666,
      "grad_norm": 0.08610253036022186,
      "learning_rate": 8.477083333333334e-06,
      "loss": 0.0024,
      "step": 199310
    },
    {
      "epoch": 6.644,
      "grad_norm": 0.34262993931770325,
      "learning_rate": 8.475000000000001e-06,
      "loss": 0.0017,
      "step": 199320
    },
    {
      "epoch": 6.644333333333333,
      "grad_norm": 0.11635083705186844,
      "learning_rate": 8.472916666666667e-06,
      "loss": 0.0022,
      "step": 199330
    },
    {
      "epoch": 6.644666666666667,
      "grad_norm": 0.03109956532716751,
      "learning_rate": 8.470833333333334e-06,
      "loss": 0.0014,
      "step": 199340
    },
    {
      "epoch": 6.645,
      "grad_norm": 0.2288675606250763,
      "learning_rate": 8.468750000000001e-06,
      "loss": 0.0015,
      "step": 199350
    },
    {
      "epoch": 6.645333333333333,
      "grad_norm": 0.3148379623889923,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0016,
      "step": 199360
    },
    {
      "epoch": 6.645666666666667,
      "grad_norm": 0.11489647626876831,
      "learning_rate": 8.464583333333334e-06,
      "loss": 0.0015,
      "step": 199370
    },
    {
      "epoch": 6.646,
      "grad_norm": 0.11435622721910477,
      "learning_rate": 8.4625e-06,
      "loss": 0.0015,
      "step": 199380
    },
    {
      "epoch": 6.646333333333334,
      "grad_norm": 0.5531845092773438,
      "learning_rate": 8.460416666666666e-06,
      "loss": 0.0019,
      "step": 199390
    },
    {
      "epoch": 6.6466666666666665,
      "grad_norm": 0.31405240297317505,
      "learning_rate": 8.458333333333333e-06,
      "loss": 0.0018,
      "step": 199400
    },
    {
      "epoch": 6.647,
      "grad_norm": 0.15498405694961548,
      "learning_rate": 8.45625e-06,
      "loss": 0.0015,
      "step": 199410
    },
    {
      "epoch": 6.647333333333333,
      "grad_norm": 0.21025122702121735,
      "learning_rate": 8.454166666666668e-06,
      "loss": 0.0017,
      "step": 199420
    },
    {
      "epoch": 6.647666666666667,
      "grad_norm": 0.6015198230743408,
      "learning_rate": 8.452083333333333e-06,
      "loss": 0.0022,
      "step": 199430
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.2282111942768097,
      "learning_rate": 8.45e-06,
      "loss": 0.0016,
      "step": 199440
    },
    {
      "epoch": 6.648333333333333,
      "grad_norm": 0.030622044578194618,
      "learning_rate": 8.447916666666666e-06,
      "loss": 0.0016,
      "step": 199450
    },
    {
      "epoch": 6.648666666666666,
      "grad_norm": 0.22793172299861908,
      "learning_rate": 8.445833333333333e-06,
      "loss": 0.0018,
      "step": 199460
    },
    {
      "epoch": 6.649,
      "grad_norm": 0.0858973041176796,
      "learning_rate": 8.44375e-06,
      "loss": 0.0014,
      "step": 199470
    },
    {
      "epoch": 6.649333333333333,
      "grad_norm": 0.08642415702342987,
      "learning_rate": 8.441666666666667e-06,
      "loss": 0.0019,
      "step": 199480
    },
    {
      "epoch": 6.649666666666667,
      "grad_norm": 0.11471136659383774,
      "learning_rate": 8.439583333333335e-06,
      "loss": 0.0017,
      "step": 199490
    },
    {
      "epoch": 6.65,
      "grad_norm": 0.2281099557876587,
      "learning_rate": 8.437500000000002e-06,
      "loss": 0.0016,
      "step": 199500
    },
    {
      "epoch": 6.650333333333333,
      "grad_norm": 0.1998676061630249,
      "learning_rate": 8.435416666666667e-06,
      "loss": 0.0017,
      "step": 199510
    },
    {
      "epoch": 6.650666666666667,
      "grad_norm": 0.2815222442150116,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0015,
      "step": 199520
    },
    {
      "epoch": 6.651,
      "grad_norm": 0.2848108112812042,
      "learning_rate": 8.43125e-06,
      "loss": 0.0018,
      "step": 199530
    },
    {
      "epoch": 6.6513333333333335,
      "grad_norm": 0.006768381223082542,
      "learning_rate": 8.429166666666667e-06,
      "loss": 0.0023,
      "step": 199540
    },
    {
      "epoch": 6.651666666666666,
      "grad_norm": 0.06164662912487984,
      "learning_rate": 8.427083333333334e-06,
      "loss": 0.0024,
      "step": 199550
    },
    {
      "epoch": 6.652,
      "grad_norm": 0.3879700303077698,
      "learning_rate": 8.425000000000001e-06,
      "loss": 0.0028,
      "step": 199560
    },
    {
      "epoch": 6.652333333333333,
      "grad_norm": 0.11456439644098282,
      "learning_rate": 8.422916666666667e-06,
      "loss": 0.002,
      "step": 199570
    },
    {
      "epoch": 6.652666666666667,
      "grad_norm": 0.05781102925539017,
      "learning_rate": 8.420833333333334e-06,
      "loss": 0.0016,
      "step": 199580
    },
    {
      "epoch": 6.6530000000000005,
      "grad_norm": 0.008506977930665016,
      "learning_rate": 8.41875e-06,
      "loss": 0.0017,
      "step": 199590
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 0.41247478127479553,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.0015,
      "step": 199600
    },
    {
      "epoch": 6.653666666666666,
      "grad_norm": 0.011577770113945007,
      "learning_rate": 8.414583333333334e-06,
      "loss": 0.0017,
      "step": 199610
    },
    {
      "epoch": 6.654,
      "grad_norm": 0.010510889813303947,
      "learning_rate": 8.412500000000001e-06,
      "loss": 0.0022,
      "step": 199620
    },
    {
      "epoch": 6.654333333333334,
      "grad_norm": 0.314083993434906,
      "learning_rate": 8.410416666666667e-06,
      "loss": 0.0021,
      "step": 199630
    },
    {
      "epoch": 6.6546666666666665,
      "grad_norm": 0.09033174067735672,
      "learning_rate": 8.408333333333334e-06,
      "loss": 0.0018,
      "step": 199640
    },
    {
      "epoch": 6.655,
      "grad_norm": 0.11418694257736206,
      "learning_rate": 8.406250000000001e-06,
      "loss": 0.0019,
      "step": 199650
    },
    {
      "epoch": 6.655333333333333,
      "grad_norm": 0.31441840529441833,
      "learning_rate": 8.404166666666667e-06,
      "loss": 0.0015,
      "step": 199660
    },
    {
      "epoch": 6.655666666666667,
      "grad_norm": 0.27182644605636597,
      "learning_rate": 8.402083333333334e-06,
      "loss": 0.0012,
      "step": 199670
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.030959947034716606,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0017,
      "step": 199680
    },
    {
      "epoch": 6.656333333333333,
      "grad_norm": 0.05744859203696251,
      "learning_rate": 8.397916666666666e-06,
      "loss": 0.0012,
      "step": 199690
    },
    {
      "epoch": 6.656666666666666,
      "grad_norm": 0.03264957666397095,
      "learning_rate": 8.395833333333334e-06,
      "loss": 0.0015,
      "step": 199700
    },
    {
      "epoch": 6.657,
      "grad_norm": 0.4510476589202881,
      "learning_rate": 8.39375e-06,
      "loss": 0.0017,
      "step": 199710
    },
    {
      "epoch": 6.657333333333334,
      "grad_norm": 0.3116583526134491,
      "learning_rate": 8.391666666666668e-06,
      "loss": 0.0023,
      "step": 199720
    },
    {
      "epoch": 6.657666666666667,
      "grad_norm": 0.3242007791996002,
      "learning_rate": 8.389583333333333e-06,
      "loss": 0.002,
      "step": 199730
    },
    {
      "epoch": 6.658,
      "grad_norm": 0.2286338061094284,
      "learning_rate": 8.3875e-06,
      "loss": 0.0015,
      "step": 199740
    },
    {
      "epoch": 6.658333333333333,
      "grad_norm": 0.2152828425168991,
      "learning_rate": 8.385416666666666e-06,
      "loss": 0.002,
      "step": 199750
    },
    {
      "epoch": 6.658666666666667,
      "grad_norm": 0.16558675467967987,
      "learning_rate": 8.383333333333333e-06,
      "loss": 0.0019,
      "step": 199760
    },
    {
      "epoch": 6.659,
      "grad_norm": 0.08675101399421692,
      "learning_rate": 8.38125e-06,
      "loss": 0.0016,
      "step": 199770
    },
    {
      "epoch": 6.6593333333333335,
      "grad_norm": 0.05710720270872116,
      "learning_rate": 8.379166666666668e-06,
      "loss": 0.0015,
      "step": 199780
    },
    {
      "epoch": 6.659666666666666,
      "grad_norm": 0.4492359757423401,
      "learning_rate": 8.377083333333335e-06,
      "loss": 0.002,
      "step": 199790
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.03270275518298149,
      "learning_rate": 8.375e-06,
      "loss": 0.0022,
      "step": 199800
    },
    {
      "epoch": 6.660333333333333,
      "grad_norm": 0.029773691669106483,
      "learning_rate": 8.372916666666666e-06,
      "loss": 0.0019,
      "step": 199810
    },
    {
      "epoch": 6.660666666666667,
      "grad_norm": 0.11384586244821548,
      "learning_rate": 8.370833333333333e-06,
      "loss": 0.002,
      "step": 199820
    },
    {
      "epoch": 6.661,
      "grad_norm": 0.3136320412158966,
      "learning_rate": 8.36875e-06,
      "loss": 0.0021,
      "step": 199830
    },
    {
      "epoch": 6.661333333333333,
      "grad_norm": 0.0112567488104105,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0019,
      "step": 199840
    },
    {
      "epoch": 6.661666666666667,
      "grad_norm": 0.006534719839692116,
      "learning_rate": 8.364583333333334e-06,
      "loss": 0.0016,
      "step": 199850
    },
    {
      "epoch": 6.662,
      "grad_norm": 0.0873255729675293,
      "learning_rate": 8.362500000000002e-06,
      "loss": 0.002,
      "step": 199860
    },
    {
      "epoch": 6.662333333333334,
      "grad_norm": 0.08603403717279434,
      "learning_rate": 8.360416666666667e-06,
      "loss": 0.0021,
      "step": 199870
    },
    {
      "epoch": 6.6626666666666665,
      "grad_norm": 0.1973770260810852,
      "learning_rate": 8.358333333333333e-06,
      "loss": 0.0021,
      "step": 199880
    },
    {
      "epoch": 6.663,
      "grad_norm": 0.11504961550235748,
      "learning_rate": 8.35625e-06,
      "loss": 0.0016,
      "step": 199890
    },
    {
      "epoch": 6.663333333333333,
      "grad_norm": 0.23468832671642303,
      "learning_rate": 8.354166666666667e-06,
      "loss": 0.002,
      "step": 199900
    },
    {
      "epoch": 6.663666666666667,
      "grad_norm": 0.007039044983685017,
      "learning_rate": 8.352083333333334e-06,
      "loss": 0.0015,
      "step": 199910
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.08608505874872208,
      "learning_rate": 8.350000000000001e-06,
      "loss": 0.0027,
      "step": 199920
    },
    {
      "epoch": 6.664333333333333,
      "grad_norm": 0.17138423025608063,
      "learning_rate": 8.347916666666667e-06,
      "loss": 0.0014,
      "step": 199930
    },
    {
      "epoch": 6.664666666666666,
      "grad_norm": 0.059168554842472076,
      "learning_rate": 8.345833333333334e-06,
      "loss": 0.0018,
      "step": 199940
    },
    {
      "epoch": 6.665,
      "grad_norm": 0.11466281116008759,
      "learning_rate": 8.34375e-06,
      "loss": 0.0014,
      "step": 199950
    },
    {
      "epoch": 6.665333333333333,
      "grad_norm": 0.42837268114089966,
      "learning_rate": 8.341666666666667e-06,
      "loss": 0.0019,
      "step": 199960
    },
    {
      "epoch": 6.665666666666667,
      "grad_norm": 0.4081224501132965,
      "learning_rate": 8.339583333333334e-06,
      "loss": 0.0018,
      "step": 199970
    },
    {
      "epoch": 6.666,
      "grad_norm": 0.25963157415390015,
      "learning_rate": 8.337500000000001e-06,
      "loss": 0.0012,
      "step": 199980
    },
    {
      "epoch": 6.666333333333333,
      "grad_norm": 0.2856566607952118,
      "learning_rate": 8.335416666666667e-06,
      "loss": 0.0016,
      "step": 199990
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.14315466582775116,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0015,
      "step": 200000
    },
    {
      "epoch": 6.667,
      "grad_norm": 0.007501441519707441,
      "learning_rate": 8.331250000000001e-06,
      "loss": 0.0014,
      "step": 200010
    },
    {
      "epoch": 6.667333333333334,
      "grad_norm": 0.1156000941991806,
      "learning_rate": 8.329166666666666e-06,
      "loss": 0.0016,
      "step": 200020
    },
    {
      "epoch": 6.667666666666666,
      "grad_norm": 0.029139306396245956,
      "learning_rate": 8.327083333333334e-06,
      "loss": 0.002,
      "step": 200030
    },
    {
      "epoch": 6.668,
      "grad_norm": 0.19132933020591736,
      "learning_rate": 8.325e-06,
      "loss": 0.0024,
      "step": 200040
    },
    {
      "epoch": 6.668333333333333,
      "grad_norm": 0.6458311080932617,
      "learning_rate": 8.322916666666668e-06,
      "loss": 0.0017,
      "step": 200050
    },
    {
      "epoch": 6.668666666666667,
      "grad_norm": 0.4294278025627136,
      "learning_rate": 8.320833333333333e-06,
      "loss": 0.0023,
      "step": 200060
    },
    {
      "epoch": 6.6690000000000005,
      "grad_norm": 0.4568004310131073,
      "learning_rate": 8.31875e-06,
      "loss": 0.0015,
      "step": 200070
    },
    {
      "epoch": 6.669333333333333,
      "grad_norm": 0.3717661201953888,
      "learning_rate": 8.316666666666668e-06,
      "loss": 0.0011,
      "step": 200080
    },
    {
      "epoch": 6.669666666666666,
      "grad_norm": 0.009012502618134022,
      "learning_rate": 8.314583333333333e-06,
      "loss": 0.0016,
      "step": 200090
    },
    {
      "epoch": 6.67,
      "grad_norm": 0.05702410638332367,
      "learning_rate": 8.3125e-06,
      "loss": 0.002,
      "step": 200100
    },
    {
      "epoch": 6.670333333333334,
      "grad_norm": 0.029782798141241074,
      "learning_rate": 8.310416666666668e-06,
      "loss": 0.0021,
      "step": 200110
    },
    {
      "epoch": 6.6706666666666665,
      "grad_norm": 0.11495031416416168,
      "learning_rate": 8.308333333333333e-06,
      "loss": 0.0016,
      "step": 200120
    },
    {
      "epoch": 6.671,
      "grad_norm": 0.11188499629497528,
      "learning_rate": 8.30625e-06,
      "loss": 0.0021,
      "step": 200130
    },
    {
      "epoch": 6.671333333333333,
      "grad_norm": 0.16911934316158295,
      "learning_rate": 8.304166666666667e-06,
      "loss": 0.0025,
      "step": 200140
    },
    {
      "epoch": 6.671666666666667,
      "grad_norm": 0.057094309478998184,
      "learning_rate": 8.302083333333335e-06,
      "loss": 0.0012,
      "step": 200150
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.42770248651504517,
      "learning_rate": 8.3e-06,
      "loss": 0.0011,
      "step": 200160
    },
    {
      "epoch": 6.6723333333333334,
      "grad_norm": 0.029309997335076332,
      "learning_rate": 8.297916666666667e-06,
      "loss": 0.0017,
      "step": 200170
    },
    {
      "epoch": 6.672666666666666,
      "grad_norm": 0.3143331706523895,
      "learning_rate": 8.295833333333333e-06,
      "loss": 0.0014,
      "step": 200180
    },
    {
      "epoch": 6.673,
      "grad_norm": 0.05733581632375717,
      "learning_rate": 8.29375e-06,
      "loss": 0.0014,
      "step": 200190
    },
    {
      "epoch": 6.673333333333334,
      "grad_norm": 0.01107463426887989,
      "learning_rate": 8.291666666666667e-06,
      "loss": 0.0014,
      "step": 200200
    },
    {
      "epoch": 6.673666666666667,
      "grad_norm": 0.11411531269550323,
      "learning_rate": 8.289583333333334e-06,
      "loss": 0.0018,
      "step": 200210
    },
    {
      "epoch": 6.674,
      "grad_norm": 0.058532822877168655,
      "learning_rate": 8.287500000000002e-06,
      "loss": 0.0013,
      "step": 200220
    },
    {
      "epoch": 6.674333333333333,
      "grad_norm": 0.11477404832839966,
      "learning_rate": 8.285416666666667e-06,
      "loss": 0.0019,
      "step": 200230
    },
    {
      "epoch": 6.674666666666667,
      "grad_norm": 0.08552347123622894,
      "learning_rate": 8.283333333333333e-06,
      "loss": 0.0018,
      "step": 200240
    },
    {
      "epoch": 6.675,
      "grad_norm": 0.03231560438871384,
      "learning_rate": 8.28125e-06,
      "loss": 0.0018,
      "step": 200250
    },
    {
      "epoch": 6.675333333333334,
      "grad_norm": 0.17165544629096985,
      "learning_rate": 8.279166666666667e-06,
      "loss": 0.0018,
      "step": 200260
    },
    {
      "epoch": 6.675666666666666,
      "grad_norm": 0.11516771465539932,
      "learning_rate": 8.277083333333334e-06,
      "loss": 0.0017,
      "step": 200270
    },
    {
      "epoch": 6.676,
      "grad_norm": 0.22832241654396057,
      "learning_rate": 8.275000000000001e-06,
      "loss": 0.0014,
      "step": 200280
    },
    {
      "epoch": 6.676333333333333,
      "grad_norm": 0.06122174113988876,
      "learning_rate": 8.272916666666668e-06,
      "loss": 0.0015,
      "step": 200290
    },
    {
      "epoch": 6.676666666666667,
      "grad_norm": 0.2856244444847107,
      "learning_rate": 8.270833333333334e-06,
      "loss": 0.0019,
      "step": 200300
    },
    {
      "epoch": 6.677,
      "grad_norm": 0.25664022564888,
      "learning_rate": 8.26875e-06,
      "loss": 0.0021,
      "step": 200310
    },
    {
      "epoch": 6.677333333333333,
      "grad_norm": 0.05911745876073837,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0015,
      "step": 200320
    },
    {
      "epoch": 6.677666666666667,
      "grad_norm": 0.5535953640937805,
      "learning_rate": 8.264583333333334e-06,
      "loss": 0.0014,
      "step": 200330
    },
    {
      "epoch": 6.678,
      "grad_norm": 0.22877931594848633,
      "learning_rate": 8.262500000000001e-06,
      "loss": 0.0016,
      "step": 200340
    },
    {
      "epoch": 6.678333333333334,
      "grad_norm": 0.37194448709487915,
      "learning_rate": 8.260416666666668e-06,
      "loss": 0.0022,
      "step": 200350
    },
    {
      "epoch": 6.6786666666666665,
      "grad_norm": 0.08595103770494461,
      "learning_rate": 8.258333333333334e-06,
      "loss": 0.0022,
      "step": 200360
    },
    {
      "epoch": 6.679,
      "grad_norm": 0.012768242508172989,
      "learning_rate": 8.25625e-06,
      "loss": 0.0018,
      "step": 200370
    },
    {
      "epoch": 6.679333333333333,
      "grad_norm": 0.05909983441233635,
      "learning_rate": 8.254166666666666e-06,
      "loss": 0.0016,
      "step": 200380
    },
    {
      "epoch": 6.679666666666667,
      "grad_norm": 0.5972930192947388,
      "learning_rate": 8.252083333333333e-06,
      "loss": 0.0018,
      "step": 200390
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.18539570271968842,
      "learning_rate": 8.25e-06,
      "loss": 0.0023,
      "step": 200400
    },
    {
      "epoch": 6.6803333333333335,
      "grad_norm": 0.11487054824829102,
      "learning_rate": 8.247916666666668e-06,
      "loss": 0.0023,
      "step": 200410
    },
    {
      "epoch": 6.680666666666666,
      "grad_norm": 0.17245537042617798,
      "learning_rate": 8.245833333333333e-06,
      "loss": 0.0016,
      "step": 200420
    },
    {
      "epoch": 6.681,
      "grad_norm": 0.1140110194683075,
      "learning_rate": 8.24375e-06,
      "loss": 0.0014,
      "step": 200430
    },
    {
      "epoch": 6.681333333333333,
      "grad_norm": 0.030815010890364647,
      "learning_rate": 8.241666666666668e-06,
      "loss": 0.0018,
      "step": 200440
    },
    {
      "epoch": 6.681666666666667,
      "grad_norm": 0.14246273040771484,
      "learning_rate": 8.239583333333333e-06,
      "loss": 0.0018,
      "step": 200450
    },
    {
      "epoch": 6.682,
      "grad_norm": 0.08643466234207153,
      "learning_rate": 8.2375e-06,
      "loss": 0.002,
      "step": 200460
    },
    {
      "epoch": 6.682333333333333,
      "grad_norm": 0.23204998672008514,
      "learning_rate": 8.235416666666668e-06,
      "loss": 0.0022,
      "step": 200470
    },
    {
      "epoch": 6.682666666666667,
      "grad_norm": 0.17121882736682892,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.0016,
      "step": 200480
    },
    {
      "epoch": 6.683,
      "grad_norm": 0.20226120948791504,
      "learning_rate": 8.23125e-06,
      "loss": 0.0014,
      "step": 200490
    },
    {
      "epoch": 6.683333333333334,
      "grad_norm": 0.39980223774909973,
      "learning_rate": 8.229166666666667e-06,
      "loss": 0.0025,
      "step": 200500
    },
    {
      "epoch": 6.683666666666666,
      "grad_norm": 0.008449099026620388,
      "learning_rate": 8.227083333333335e-06,
      "loss": 0.0014,
      "step": 200510
    },
    {
      "epoch": 6.684,
      "grad_norm": 0.06951576471328735,
      "learning_rate": 8.225e-06,
      "loss": 0.0016,
      "step": 200520
    },
    {
      "epoch": 6.684333333333333,
      "grad_norm": 0.1425907164812088,
      "learning_rate": 8.222916666666667e-06,
      "loss": 0.0023,
      "step": 200530
    },
    {
      "epoch": 6.684666666666667,
      "grad_norm": 0.029484029859304428,
      "learning_rate": 8.220833333333333e-06,
      "loss": 0.0017,
      "step": 200540
    },
    {
      "epoch": 6.6850000000000005,
      "grad_norm": 0.20132200419902802,
      "learning_rate": 8.21875e-06,
      "loss": 0.0019,
      "step": 200550
    },
    {
      "epoch": 6.685333333333333,
      "grad_norm": 0.05748283490538597,
      "learning_rate": 8.216666666666667e-06,
      "loss": 0.0017,
      "step": 200560
    },
    {
      "epoch": 6.685666666666666,
      "grad_norm": 0.19947046041488647,
      "learning_rate": 8.214583333333334e-06,
      "loss": 0.0022,
      "step": 200570
    },
    {
      "epoch": 6.686,
      "grad_norm": 0.45306047797203064,
      "learning_rate": 8.212500000000001e-06,
      "loss": 0.0019,
      "step": 200580
    },
    {
      "epoch": 6.686333333333334,
      "grad_norm": 0.05854244902729988,
      "learning_rate": 8.210416666666667e-06,
      "loss": 0.0014,
      "step": 200590
    },
    {
      "epoch": 6.6866666666666665,
      "grad_norm": 0.20026741921901703,
      "learning_rate": 8.208333333333332e-06,
      "loss": 0.0019,
      "step": 200600
    },
    {
      "epoch": 6.687,
      "grad_norm": 0.08626250922679901,
      "learning_rate": 8.20625e-06,
      "loss": 0.0022,
      "step": 200610
    },
    {
      "epoch": 6.687333333333333,
      "grad_norm": 0.416753888130188,
      "learning_rate": 8.204166666666667e-06,
      "loss": 0.0023,
      "step": 200620
    },
    {
      "epoch": 6.687666666666667,
      "grad_norm": 0.41072729229927063,
      "learning_rate": 8.202083333333334e-06,
      "loss": 0.0017,
      "step": 200630
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.1719556301832199,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0015,
      "step": 200640
    },
    {
      "epoch": 6.6883333333333335,
      "grad_norm": 0.11578793823719025,
      "learning_rate": 8.197916666666668e-06,
      "loss": 0.002,
      "step": 200650
    },
    {
      "epoch": 6.688666666666666,
      "grad_norm": 0.20014965534210205,
      "learning_rate": 8.195833333333334e-06,
      "loss": 0.0015,
      "step": 200660
    },
    {
      "epoch": 6.689,
      "grad_norm": 0.1999916285276413,
      "learning_rate": 8.19375e-06,
      "loss": 0.0022,
      "step": 200670
    },
    {
      "epoch": 6.689333333333334,
      "grad_norm": 0.17169760167598724,
      "learning_rate": 8.191666666666666e-06,
      "loss": 0.0016,
      "step": 200680
    },
    {
      "epoch": 6.689666666666667,
      "grad_norm": 0.2850790321826935,
      "learning_rate": 8.189583333333334e-06,
      "loss": 0.0016,
      "step": 200690
    },
    {
      "epoch": 6.6899999999999995,
      "grad_norm": 0.11501538008451462,
      "learning_rate": 8.1875e-06,
      "loss": 0.002,
      "step": 200700
    },
    {
      "epoch": 6.690333333333333,
      "grad_norm": 0.01157363597303629,
      "learning_rate": 8.185416666666668e-06,
      "loss": 0.0017,
      "step": 200710
    },
    {
      "epoch": 6.690666666666667,
      "grad_norm": 0.17184048891067505,
      "learning_rate": 8.183333333333333e-06,
      "loss": 0.0014,
      "step": 200720
    },
    {
      "epoch": 6.691,
      "grad_norm": 0.08644384145736694,
      "learning_rate": 8.18125e-06,
      "loss": 0.0015,
      "step": 200730
    },
    {
      "epoch": 6.691333333333334,
      "grad_norm": 0.14268815517425537,
      "learning_rate": 8.179166666666666e-06,
      "loss": 0.0016,
      "step": 200740
    },
    {
      "epoch": 6.691666666666666,
      "grad_norm": 0.14314992725849152,
      "learning_rate": 8.177083333333333e-06,
      "loss": 0.0015,
      "step": 200750
    },
    {
      "epoch": 6.692,
      "grad_norm": 0.05766576901078224,
      "learning_rate": 8.175e-06,
      "loss": 0.0014,
      "step": 200760
    },
    {
      "epoch": 6.692333333333333,
      "grad_norm": 0.058053158223629,
      "learning_rate": 8.172916666666668e-06,
      "loss": 0.0015,
      "step": 200770
    },
    {
      "epoch": 6.692666666666667,
      "grad_norm": 0.16259248554706573,
      "learning_rate": 8.170833333333333e-06,
      "loss": 0.0021,
      "step": 200780
    },
    {
      "epoch": 6.693,
      "grad_norm": 0.20017726719379425,
      "learning_rate": 8.16875e-06,
      "loss": 0.002,
      "step": 200790
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 0.11461521685123444,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.0022,
      "step": 200800
    },
    {
      "epoch": 6.693666666666667,
      "grad_norm": 0.08596569299697876,
      "learning_rate": 8.164583333333333e-06,
      "loss": 0.001,
      "step": 200810
    },
    {
      "epoch": 6.694,
      "grad_norm": 0.05863775685429573,
      "learning_rate": 8.1625e-06,
      "loss": 0.002,
      "step": 200820
    },
    {
      "epoch": 6.694333333333334,
      "grad_norm": 0.03037956729531288,
      "learning_rate": 8.160416666666667e-06,
      "loss": 0.0014,
      "step": 200830
    },
    {
      "epoch": 6.6946666666666665,
      "grad_norm": 0.1718033105134964,
      "learning_rate": 8.158333333333333e-06,
      "loss": 0.0017,
      "step": 200840
    },
    {
      "epoch": 6.695,
      "grad_norm": 0.22807209193706512,
      "learning_rate": 8.15625e-06,
      "loss": 0.0025,
      "step": 200850
    },
    {
      "epoch": 6.695333333333333,
      "grad_norm": 0.20000657439231873,
      "learning_rate": 8.154166666666667e-06,
      "loss": 0.0013,
      "step": 200860
    },
    {
      "epoch": 6.695666666666667,
      "grad_norm": 0.2872978448867798,
      "learning_rate": 8.152083333333334e-06,
      "loss": 0.0014,
      "step": 200870
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.3425918221473694,
      "learning_rate": 8.15e-06,
      "loss": 0.0019,
      "step": 200880
    },
    {
      "epoch": 6.6963333333333335,
      "grad_norm": 0.04037395492196083,
      "learning_rate": 8.147916666666667e-06,
      "loss": 0.0016,
      "step": 200890
    },
    {
      "epoch": 6.696666666666666,
      "grad_norm": 0.057884953916072845,
      "learning_rate": 8.145833333333333e-06,
      "loss": 0.003,
      "step": 200900
    },
    {
      "epoch": 6.697,
      "grad_norm": 0.10251341015100479,
      "learning_rate": 8.14375e-06,
      "loss": 0.0018,
      "step": 200910
    },
    {
      "epoch": 6.697333333333333,
      "grad_norm": 0.2517116963863373,
      "learning_rate": 8.141666666666667e-06,
      "loss": 0.0019,
      "step": 200920
    },
    {
      "epoch": 6.697666666666667,
      "grad_norm": 0.05871865525841713,
      "learning_rate": 8.139583333333334e-06,
      "loss": 0.0012,
      "step": 200930
    },
    {
      "epoch": 6.698,
      "grad_norm": 0.2533927261829376,
      "learning_rate": 8.137500000000001e-06,
      "loss": 0.0022,
      "step": 200940
    },
    {
      "epoch": 6.698333333333333,
      "grad_norm": 0.11484234035015106,
      "learning_rate": 8.135416666666667e-06,
      "loss": 0.002,
      "step": 200950
    },
    {
      "epoch": 6.698666666666667,
      "grad_norm": 0.2657708525657654,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0017,
      "step": 200960
    },
    {
      "epoch": 6.699,
      "grad_norm": 0.16428320109844208,
      "learning_rate": 8.13125e-06,
      "loss": 0.0015,
      "step": 200970
    },
    {
      "epoch": 6.699333333333334,
      "grad_norm": 0.08934212476015091,
      "learning_rate": 8.129166666666667e-06,
      "loss": 0.0028,
      "step": 200980
    },
    {
      "epoch": 6.699666666666666,
      "grad_norm": 0.01161620207130909,
      "learning_rate": 8.127083333333334e-06,
      "loss": 0.0015,
      "step": 200990
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.029976632446050644,
      "learning_rate": 8.125000000000001e-06,
      "loss": 0.0018,
      "step": 201000
    },
    {
      "epoch": 6.700333333333333,
      "grad_norm": 0.14256852865219116,
      "learning_rate": 8.122916666666668e-06,
      "loss": 0.0015,
      "step": 201010
    },
    {
      "epoch": 6.700666666666667,
      "grad_norm": 0.1630643606185913,
      "learning_rate": 8.120833333333334e-06,
      "loss": 0.0021,
      "step": 201020
    },
    {
      "epoch": 6.701,
      "grad_norm": 0.25694823265075684,
      "learning_rate": 8.118749999999999e-06,
      "loss": 0.0018,
      "step": 201030
    },
    {
      "epoch": 6.701333333333333,
      "grad_norm": 0.05789289250969887,
      "learning_rate": 8.116666666666666e-06,
      "loss": 0.0018,
      "step": 201040
    },
    {
      "epoch": 6.701666666666666,
      "grad_norm": 0.08676520735025406,
      "learning_rate": 8.114583333333333e-06,
      "loss": 0.0018,
      "step": 201050
    },
    {
      "epoch": 6.702,
      "grad_norm": 0.17244206368923187,
      "learning_rate": 8.1125e-06,
      "loss": 0.0014,
      "step": 201060
    },
    {
      "epoch": 6.702333333333334,
      "grad_norm": 0.19987425208091736,
      "learning_rate": 8.110416666666668e-06,
      "loss": 0.0025,
      "step": 201070
    },
    {
      "epoch": 6.7026666666666666,
      "grad_norm": 0.3423353135585785,
      "learning_rate": 8.108333333333333e-06,
      "loss": 0.0017,
      "step": 201080
    },
    {
      "epoch": 6.703,
      "grad_norm": 0.05883113667368889,
      "learning_rate": 8.10625e-06,
      "loss": 0.0017,
      "step": 201090
    },
    {
      "epoch": 6.703333333333333,
      "grad_norm": 0.3993239998817444,
      "learning_rate": 8.104166666666666e-06,
      "loss": 0.0012,
      "step": 201100
    },
    {
      "epoch": 6.703666666666667,
      "grad_norm": 0.08705161511898041,
      "learning_rate": 8.102083333333333e-06,
      "loss": 0.0016,
      "step": 201110
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.14385107159614563,
      "learning_rate": 8.1e-06,
      "loss": 0.0011,
      "step": 201120
    },
    {
      "epoch": 6.7043333333333335,
      "grad_norm": 0.1476355940103531,
      "learning_rate": 8.097916666666668e-06,
      "loss": 0.002,
      "step": 201130
    },
    {
      "epoch": 6.704666666666666,
      "grad_norm": 0.11463529616594315,
      "learning_rate": 8.095833333333333e-06,
      "loss": 0.0012,
      "step": 201140
    },
    {
      "epoch": 6.705,
      "grad_norm": 0.05870679020881653,
      "learning_rate": 8.09375e-06,
      "loss": 0.0024,
      "step": 201150
    },
    {
      "epoch": 6.705333333333334,
      "grad_norm": 0.2695775628089905,
      "learning_rate": 8.091666666666667e-06,
      "loss": 0.0013,
      "step": 201160
    },
    {
      "epoch": 6.705666666666667,
      "grad_norm": 0.030366497114300728,
      "learning_rate": 8.089583333333333e-06,
      "loss": 0.0015,
      "step": 201170
    },
    {
      "epoch": 6.7059999999999995,
      "grad_norm": 0.1710633933544159,
      "learning_rate": 8.0875e-06,
      "loss": 0.0018,
      "step": 201180
    },
    {
      "epoch": 6.706333333333333,
      "grad_norm": 0.11454854905605316,
      "learning_rate": 8.085416666666667e-06,
      "loss": 0.0019,
      "step": 201190
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 0.39857327938079834,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.0014,
      "step": 201200
    },
    {
      "epoch": 6.707,
      "grad_norm": 0.22780002653598785,
      "learning_rate": 8.08125e-06,
      "loss": 0.0018,
      "step": 201210
    },
    {
      "epoch": 6.707333333333334,
      "grad_norm": 0.34229061007499695,
      "learning_rate": 8.079166666666667e-06,
      "loss": 0.0019,
      "step": 201220
    },
    {
      "epoch": 6.707666666666666,
      "grad_norm": 0.0913497656583786,
      "learning_rate": 8.077083333333334e-06,
      "loss": 0.0016,
      "step": 201230
    },
    {
      "epoch": 6.708,
      "grad_norm": 0.05484992638230324,
      "learning_rate": 8.075000000000001e-06,
      "loss": 0.0018,
      "step": 201240
    },
    {
      "epoch": 6.708333333333333,
      "grad_norm": 0.12444953620433807,
      "learning_rate": 8.072916666666667e-06,
      "loss": 0.0018,
      "step": 201250
    },
    {
      "epoch": 6.708666666666667,
      "grad_norm": 0.030678343027830124,
      "learning_rate": 8.070833333333334e-06,
      "loss": 0.0019,
      "step": 201260
    },
    {
      "epoch": 6.709,
      "grad_norm": 0.0108070382848382,
      "learning_rate": 8.06875e-06,
      "loss": 0.0018,
      "step": 201270
    },
    {
      "epoch": 6.709333333333333,
      "grad_norm": 0.14357545971870422,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0022,
      "step": 201280
    },
    {
      "epoch": 6.709666666666667,
      "grad_norm": 0.1719662845134735,
      "learning_rate": 8.064583333333334e-06,
      "loss": 0.0015,
      "step": 201290
    },
    {
      "epoch": 6.71,
      "grad_norm": 0.08646705746650696,
      "learning_rate": 8.062500000000001e-06,
      "loss": 0.002,
      "step": 201300
    },
    {
      "epoch": 6.710333333333334,
      "grad_norm": 0.06062554940581322,
      "learning_rate": 8.060416666666668e-06,
      "loss": 0.0019,
      "step": 201310
    },
    {
      "epoch": 6.710666666666667,
      "grad_norm": 0.3140886425971985,
      "learning_rate": 8.058333333333334e-06,
      "loss": 0.0016,
      "step": 201320
    },
    {
      "epoch": 6.711,
      "grad_norm": 0.05811825767159462,
      "learning_rate": 8.05625e-06,
      "loss": 0.0019,
      "step": 201330
    },
    {
      "epoch": 6.711333333333333,
      "grad_norm": 0.029752537608146667,
      "learning_rate": 8.054166666666666e-06,
      "loss": 0.0019,
      "step": 201340
    },
    {
      "epoch": 6.711666666666667,
      "grad_norm": 0.2286851406097412,
      "learning_rate": 8.052083333333334e-06,
      "loss": 0.0017,
      "step": 201350
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.08999989926815033,
      "learning_rate": 8.050000000000001e-06,
      "loss": 0.002,
      "step": 201360
    },
    {
      "epoch": 6.7123333333333335,
      "grad_norm": 0.3144717812538147,
      "learning_rate": 8.047916666666668e-06,
      "loss": 0.0016,
      "step": 201370
    },
    {
      "epoch": 6.712666666666666,
      "grad_norm": 0.007505850400775671,
      "learning_rate": 8.045833333333335e-06,
      "loss": 0.0025,
      "step": 201380
    },
    {
      "epoch": 6.713,
      "grad_norm": 0.42877453565597534,
      "learning_rate": 8.04375e-06,
      "loss": 0.0024,
      "step": 201390
    },
    {
      "epoch": 6.713333333333333,
      "grad_norm": 0.010456318035721779,
      "learning_rate": 8.041666666666666e-06,
      "loss": 0.0014,
      "step": 201400
    },
    {
      "epoch": 6.713666666666667,
      "grad_norm": 0.08692300319671631,
      "learning_rate": 8.039583333333333e-06,
      "loss": 0.0027,
      "step": 201410
    },
    {
      "epoch": 6.714,
      "grad_norm": 0.03198442608118057,
      "learning_rate": 8.0375e-06,
      "loss": 0.0019,
      "step": 201420
    },
    {
      "epoch": 6.714333333333333,
      "grad_norm": 0.2687533497810364,
      "learning_rate": 8.035416666666668e-06,
      "loss": 0.0018,
      "step": 201430
    },
    {
      "epoch": 6.714666666666667,
      "grad_norm": 0.2909660041332245,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0019,
      "step": 201440
    },
    {
      "epoch": 6.715,
      "grad_norm": 0.029540734365582466,
      "learning_rate": 8.03125e-06,
      "loss": 0.0014,
      "step": 201450
    },
    {
      "epoch": 6.715333333333334,
      "grad_norm": 0.17871153354644775,
      "learning_rate": 8.029166666666668e-06,
      "loss": 0.0015,
      "step": 201460
    },
    {
      "epoch": 6.7156666666666665,
      "grad_norm": 0.3716827929019928,
      "learning_rate": 8.027083333333333e-06,
      "loss": 0.0027,
      "step": 201470
    },
    {
      "epoch": 6.716,
      "grad_norm": 0.08578745275735855,
      "learning_rate": 8.025e-06,
      "loss": 0.0016,
      "step": 201480
    },
    {
      "epoch": 6.716333333333333,
      "grad_norm": 0.1717013269662857,
      "learning_rate": 8.022916666666667e-06,
      "loss": 0.0013,
      "step": 201490
    },
    {
      "epoch": 6.716666666666667,
      "grad_norm": 0.1151484027504921,
      "learning_rate": 8.020833333333335e-06,
      "loss": 0.0019,
      "step": 201500
    },
    {
      "epoch": 6.717,
      "grad_norm": 0.22852565348148346,
      "learning_rate": 8.01875e-06,
      "loss": 0.0015,
      "step": 201510
    },
    {
      "epoch": 6.717333333333333,
      "grad_norm": 0.17172230780124664,
      "learning_rate": 8.016666666666667e-06,
      "loss": 0.0014,
      "step": 201520
    },
    {
      "epoch": 6.717666666666666,
      "grad_norm": 0.20001637935638428,
      "learning_rate": 8.014583333333334e-06,
      "loss": 0.0013,
      "step": 201530
    },
    {
      "epoch": 6.718,
      "grad_norm": 0.4566623568534851,
      "learning_rate": 8.0125e-06,
      "loss": 0.002,
      "step": 201540
    },
    {
      "epoch": 6.718333333333334,
      "grad_norm": 0.028846368193626404,
      "learning_rate": 8.010416666666667e-06,
      "loss": 0.0027,
      "step": 201550
    },
    {
      "epoch": 6.718666666666667,
      "grad_norm": 0.009623195976018906,
      "learning_rate": 8.008333333333334e-06,
      "loss": 0.0019,
      "step": 201560
    },
    {
      "epoch": 6.719,
      "grad_norm": 0.31347134709358215,
      "learning_rate": 8.00625e-06,
      "loss": 0.0012,
      "step": 201570
    },
    {
      "epoch": 6.719333333333333,
      "grad_norm": 0.08653542399406433,
      "learning_rate": 8.004166666666667e-06,
      "loss": 0.0017,
      "step": 201580
    },
    {
      "epoch": 6.719666666666667,
      "grad_norm": 0.17145654559135437,
      "learning_rate": 8.002083333333334e-06,
      "loss": 0.0012,
      "step": 201590
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.0716002807021141,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0018,
      "step": 201600
    },
    {
      "epoch": 6.7203333333333335,
      "grad_norm": 0.14235036075115204,
      "learning_rate": 7.997916666666667e-06,
      "loss": 0.0019,
      "step": 201610
    },
    {
      "epoch": 6.720666666666666,
      "grad_norm": 0.3426092863082886,
      "learning_rate": 7.995833333333334e-06,
      "loss": 0.0017,
      "step": 201620
    },
    {
      "epoch": 6.721,
      "grad_norm": 0.23941315710544586,
      "learning_rate": 7.99375e-06,
      "loss": 0.0022,
      "step": 201630
    },
    {
      "epoch": 6.721333333333334,
      "grad_norm": 0.0873776227235794,
      "learning_rate": 7.991666666666667e-06,
      "loss": 0.0012,
      "step": 201640
    },
    {
      "epoch": 6.721666666666667,
      "grad_norm": 0.0576433464884758,
      "learning_rate": 7.989583333333334e-06,
      "loss": 0.0015,
      "step": 201650
    },
    {
      "epoch": 6.7219999999999995,
      "grad_norm": 0.228652685880661,
      "learning_rate": 7.987500000000001e-06,
      "loss": 0.0023,
      "step": 201660
    },
    {
      "epoch": 6.722333333333333,
      "grad_norm": 0.030031688511371613,
      "learning_rate": 7.985416666666668e-06,
      "loss": 0.0014,
      "step": 201670
    },
    {
      "epoch": 6.722666666666667,
      "grad_norm": 0.2288014441728592,
      "learning_rate": 7.983333333333334e-06,
      "loss": 0.0018,
      "step": 201680
    },
    {
      "epoch": 6.723,
      "grad_norm": 0.17176052927970886,
      "learning_rate": 7.98125e-06,
      "loss": 0.0016,
      "step": 201690
    },
    {
      "epoch": 6.723333333333334,
      "grad_norm": 0.34241169691085815,
      "learning_rate": 7.979166666666666e-06,
      "loss": 0.0013,
      "step": 201700
    },
    {
      "epoch": 6.7236666666666665,
      "grad_norm": 0.23857343196868896,
      "learning_rate": 7.977083333333334e-06,
      "loss": 0.0014,
      "step": 201710
    },
    {
      "epoch": 6.724,
      "grad_norm": 0.08547697961330414,
      "learning_rate": 7.975e-06,
      "loss": 0.0022,
      "step": 201720
    },
    {
      "epoch": 6.724333333333333,
      "grad_norm": 0.009363722987473011,
      "learning_rate": 7.972916666666668e-06,
      "loss": 0.0017,
      "step": 201730
    },
    {
      "epoch": 6.724666666666667,
      "grad_norm": 0.25692957639694214,
      "learning_rate": 7.970833333333335e-06,
      "loss": 0.0012,
      "step": 201740
    },
    {
      "epoch": 6.725,
      "grad_norm": 0.2283569723367691,
      "learning_rate": 7.96875e-06,
      "loss": 0.0023,
      "step": 201750
    },
    {
      "epoch": 6.725333333333333,
      "grad_norm": 0.02974383905529976,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0016,
      "step": 201760
    },
    {
      "epoch": 6.725666666666667,
      "grad_norm": 0.2856940031051636,
      "learning_rate": 7.964583333333333e-06,
      "loss": 0.0018,
      "step": 201770
    },
    {
      "epoch": 6.726,
      "grad_norm": 0.1999758630990982,
      "learning_rate": 7.9625e-06,
      "loss": 0.0016,
      "step": 201780
    },
    {
      "epoch": 6.726333333333334,
      "grad_norm": 0.17178580164909363,
      "learning_rate": 7.960416666666668e-06,
      "loss": 0.002,
      "step": 201790
    },
    {
      "epoch": 6.726666666666667,
      "grad_norm": 0.057574182748794556,
      "learning_rate": 7.958333333333335e-06,
      "loss": 0.0014,
      "step": 201800
    },
    {
      "epoch": 6.727,
      "grad_norm": 0.31367969512939453,
      "learning_rate": 7.95625e-06,
      "loss": 0.0016,
      "step": 201810
    },
    {
      "epoch": 6.727333333333333,
      "grad_norm": 0.1713402271270752,
      "learning_rate": 7.954166666666667e-06,
      "loss": 0.0017,
      "step": 201820
    },
    {
      "epoch": 6.727666666666667,
      "grad_norm": 0.24689720571041107,
      "learning_rate": 7.952083333333333e-06,
      "loss": 0.0016,
      "step": 201830
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.08674226701259613,
      "learning_rate": 7.95e-06,
      "loss": 0.0018,
      "step": 201840
    },
    {
      "epoch": 6.7283333333333335,
      "grad_norm": 0.05739469453692436,
      "learning_rate": 7.947916666666667e-06,
      "loss": 0.0024,
      "step": 201850
    },
    {
      "epoch": 6.728666666666666,
      "grad_norm": 0.20767046511173248,
      "learning_rate": 7.945833333333334e-06,
      "loss": 0.0023,
      "step": 201860
    },
    {
      "epoch": 6.729,
      "grad_norm": 0.14276976883411407,
      "learning_rate": 7.94375e-06,
      "loss": 0.0017,
      "step": 201870
    },
    {
      "epoch": 6.729333333333333,
      "grad_norm": 0.3137947618961334,
      "learning_rate": 7.941666666666667e-06,
      "loss": 0.0014,
      "step": 201880
    },
    {
      "epoch": 6.729666666666667,
      "grad_norm": 0.1430610716342926,
      "learning_rate": 7.939583333333334e-06,
      "loss": 0.002,
      "step": 201890
    },
    {
      "epoch": 6.73,
      "grad_norm": 0.11520572006702423,
      "learning_rate": 7.9375e-06,
      "loss": 0.0019,
      "step": 201900
    },
    {
      "epoch": 6.730333333333333,
      "grad_norm": 0.3534733057022095,
      "learning_rate": 7.935416666666667e-06,
      "loss": 0.0018,
      "step": 201910
    },
    {
      "epoch": 6.730666666666667,
      "grad_norm": 0.14343824982643127,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0016,
      "step": 201920
    },
    {
      "epoch": 6.731,
      "grad_norm": 0.2563270330429077,
      "learning_rate": 7.93125e-06,
      "loss": 0.0019,
      "step": 201930
    },
    {
      "epoch": 6.731333333333334,
      "grad_norm": 0.08357967436313629,
      "learning_rate": 7.929166666666667e-06,
      "loss": 0.0017,
      "step": 201940
    },
    {
      "epoch": 6.7316666666666665,
      "grad_norm": 0.45592865347862244,
      "learning_rate": 7.927083333333334e-06,
      "loss": 0.0021,
      "step": 201950
    },
    {
      "epoch": 6.732,
      "grad_norm": 0.17283859848976135,
      "learning_rate": 7.925000000000001e-06,
      "loss": 0.0017,
      "step": 201960
    },
    {
      "epoch": 6.732333333333333,
      "grad_norm": 0.11460725963115692,
      "learning_rate": 7.922916666666667e-06,
      "loss": 0.0016,
      "step": 201970
    },
    {
      "epoch": 6.732666666666667,
      "grad_norm": 0.2564515471458435,
      "learning_rate": 7.920833333333334e-06,
      "loss": 0.0017,
      "step": 201980
    },
    {
      "epoch": 6.733,
      "grad_norm": 0.2160150557756424,
      "learning_rate": 7.91875e-06,
      "loss": 0.0023,
      "step": 201990
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.030460378155112267,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.002,
      "step": 202000
    },
    {
      "epoch": 6.733666666666666,
      "grad_norm": 0.3139272630214691,
      "learning_rate": 7.914583333333334e-06,
      "loss": 0.0013,
      "step": 202010
    },
    {
      "epoch": 6.734,
      "grad_norm": 0.14291912317276,
      "learning_rate": 7.912500000000001e-06,
      "loss": 0.0016,
      "step": 202020
    },
    {
      "epoch": 6.734333333333334,
      "grad_norm": 0.11466124653816223,
      "learning_rate": 7.910416666666668e-06,
      "loss": 0.0018,
      "step": 202030
    },
    {
      "epoch": 6.734666666666667,
      "grad_norm": 0.17113623023033142,
      "learning_rate": 7.908333333333334e-06,
      "loss": 0.0017,
      "step": 202040
    },
    {
      "epoch": 6.735,
      "grad_norm": 0.05747375264763832,
      "learning_rate": 7.906249999999999e-06,
      "loss": 0.0019,
      "step": 202050
    },
    {
      "epoch": 6.735333333333333,
      "grad_norm": 0.11406916379928589,
      "learning_rate": 7.904166666666666e-06,
      "loss": 0.0023,
      "step": 202060
    },
    {
      "epoch": 6.735666666666667,
      "grad_norm": 0.14346061646938324,
      "learning_rate": 7.902083333333333e-06,
      "loss": 0.0012,
      "step": 202070
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.17189009487628937,
      "learning_rate": 7.9e-06,
      "loss": 0.0012,
      "step": 202080
    },
    {
      "epoch": 6.7363333333333335,
      "grad_norm": 0.11508776247501373,
      "learning_rate": 7.897916666666668e-06,
      "loss": 0.0015,
      "step": 202090
    },
    {
      "epoch": 6.736666666666666,
      "grad_norm": 0.05741241201758385,
      "learning_rate": 7.895833333333335e-06,
      "loss": 0.0014,
      "step": 202100
    },
    {
      "epoch": 6.737,
      "grad_norm": 0.086143858730793,
      "learning_rate": 7.89375e-06,
      "loss": 0.002,
      "step": 202110
    },
    {
      "epoch": 6.737333333333333,
      "grad_norm": 0.2678128778934479,
      "learning_rate": 7.891666666666666e-06,
      "loss": 0.0018,
      "step": 202120
    },
    {
      "epoch": 6.737666666666667,
      "grad_norm": 0.46882548928260803,
      "learning_rate": 7.889583333333333e-06,
      "loss": 0.0017,
      "step": 202130
    },
    {
      "epoch": 6.7379999999999995,
      "grad_norm": 0.08654141426086426,
      "learning_rate": 7.8875e-06,
      "loss": 0.0017,
      "step": 202140
    },
    {
      "epoch": 6.738333333333333,
      "grad_norm": 0.029779868200421333,
      "learning_rate": 7.885416666666667e-06,
      "loss": 0.002,
      "step": 202150
    },
    {
      "epoch": 6.738666666666667,
      "grad_norm": 0.033036284148693085,
      "learning_rate": 7.883333333333335e-06,
      "loss": 0.0017,
      "step": 202160
    },
    {
      "epoch": 6.739,
      "grad_norm": 0.032161910086870193,
      "learning_rate": 7.88125e-06,
      "loss": 0.0013,
      "step": 202170
    },
    {
      "epoch": 6.739333333333334,
      "grad_norm": 0.2277560979127884,
      "learning_rate": 7.879166666666667e-06,
      "loss": 0.0019,
      "step": 202180
    },
    {
      "epoch": 6.7396666666666665,
      "grad_norm": 0.4282008707523346,
      "learning_rate": 7.877083333333333e-06,
      "loss": 0.0022,
      "step": 202190
    },
    {
      "epoch": 6.74,
      "grad_norm": 0.14512692391872406,
      "learning_rate": 7.875e-06,
      "loss": 0.0012,
      "step": 202200
    },
    {
      "epoch": 6.740333333333333,
      "grad_norm": 0.0579628087580204,
      "learning_rate": 7.872916666666667e-06,
      "loss": 0.0014,
      "step": 202210
    },
    {
      "epoch": 6.740666666666667,
      "grad_norm": 0.0302155539393425,
      "learning_rate": 7.870833333333334e-06,
      "loss": 0.0017,
      "step": 202220
    },
    {
      "epoch": 6.741,
      "grad_norm": 0.11422634869813919,
      "learning_rate": 7.86875e-06,
      "loss": 0.0011,
      "step": 202230
    },
    {
      "epoch": 6.741333333333333,
      "grad_norm": 0.05798160284757614,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0024,
      "step": 202240
    },
    {
      "epoch": 6.741666666666667,
      "grad_norm": 0.08603637665510178,
      "learning_rate": 7.864583333333334e-06,
      "loss": 0.0016,
      "step": 202250
    },
    {
      "epoch": 6.742,
      "grad_norm": 0.14283740520477295,
      "learning_rate": 7.8625e-06,
      "loss": 0.0017,
      "step": 202260
    },
    {
      "epoch": 6.742333333333333,
      "grad_norm": 0.34224629402160645,
      "learning_rate": 7.860416666666667e-06,
      "loss": 0.0016,
      "step": 202270
    },
    {
      "epoch": 6.742666666666667,
      "grad_norm": 0.1428280472755432,
      "learning_rate": 7.858333333333334e-06,
      "loss": 0.0017,
      "step": 202280
    },
    {
      "epoch": 6.743,
      "grad_norm": 0.2000856250524521,
      "learning_rate": 7.85625e-06,
      "loss": 0.0016,
      "step": 202290
    },
    {
      "epoch": 6.743333333333333,
      "grad_norm": 0.17142480611801147,
      "learning_rate": 7.854166666666667e-06,
      "loss": 0.0017,
      "step": 202300
    },
    {
      "epoch": 6.743666666666667,
      "grad_norm": 0.2012854665517807,
      "learning_rate": 7.852083333333334e-06,
      "loss": 0.0021,
      "step": 202310
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.3718600571155548,
      "learning_rate": 7.850000000000001e-06,
      "loss": 0.0021,
      "step": 202320
    },
    {
      "epoch": 6.7443333333333335,
      "grad_norm": 0.029553258791565895,
      "learning_rate": 7.847916666666667e-06,
      "loss": 0.0015,
      "step": 202330
    },
    {
      "epoch": 6.744666666666666,
      "grad_norm": 0.057722460478544235,
      "learning_rate": 7.845833333333334e-06,
      "loss": 0.0024,
      "step": 202340
    },
    {
      "epoch": 6.745,
      "grad_norm": 0.11475194990634918,
      "learning_rate": 7.84375e-06,
      "loss": 0.0015,
      "step": 202350
    },
    {
      "epoch": 6.745333333333333,
      "grad_norm": 0.2850004732608795,
      "learning_rate": 7.841666666666666e-06,
      "loss": 0.0022,
      "step": 202360
    },
    {
      "epoch": 6.745666666666667,
      "grad_norm": 0.427756667137146,
      "learning_rate": 7.839583333333334e-06,
      "loss": 0.0018,
      "step": 202370
    },
    {
      "epoch": 6.746,
      "grad_norm": 0.01873648539185524,
      "learning_rate": 7.8375e-06,
      "loss": 0.0015,
      "step": 202380
    },
    {
      "epoch": 6.746333333333333,
      "grad_norm": 0.19975833594799042,
      "learning_rate": 7.835416666666668e-06,
      "loss": 0.0015,
      "step": 202390
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.08614671975374222,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.0012,
      "step": 202400
    },
    {
      "epoch": 6.747,
      "grad_norm": 0.05764898285269737,
      "learning_rate": 7.831249999999999e-06,
      "loss": 0.0016,
      "step": 202410
    },
    {
      "epoch": 6.747333333333334,
      "grad_norm": 0.25701332092285156,
      "learning_rate": 7.829166666666666e-06,
      "loss": 0.002,
      "step": 202420
    },
    {
      "epoch": 6.7476666666666665,
      "grad_norm": 0.05779498815536499,
      "learning_rate": 7.827083333333333e-06,
      "loss": 0.0013,
      "step": 202430
    },
    {
      "epoch": 6.748,
      "grad_norm": 0.08698402345180511,
      "learning_rate": 7.825e-06,
      "loss": 0.0021,
      "step": 202440
    },
    {
      "epoch": 6.748333333333333,
      "grad_norm": 0.030887166038155556,
      "learning_rate": 7.822916666666668e-06,
      "loss": 0.0015,
      "step": 202450
    },
    {
      "epoch": 6.748666666666667,
      "grad_norm": 0.1433810293674469,
      "learning_rate": 7.820833333333335e-06,
      "loss": 0.0028,
      "step": 202460
    },
    {
      "epoch": 6.749,
      "grad_norm": 0.22360169887542725,
      "learning_rate": 7.81875e-06,
      "loss": 0.0015,
      "step": 202470
    },
    {
      "epoch": 6.749333333333333,
      "grad_norm": 0.25669142603874207,
      "learning_rate": 7.816666666666666e-06,
      "loss": 0.0021,
      "step": 202480
    },
    {
      "epoch": 6.749666666666666,
      "grad_norm": 0.3141121566295624,
      "learning_rate": 7.814583333333333e-06,
      "loss": 0.0015,
      "step": 202490
    },
    {
      "epoch": 6.75,
      "grad_norm": 0.05814894288778305,
      "learning_rate": 7.8125e-06,
      "loss": 0.0024,
      "step": 202500
    },
    {
      "epoch": 6.750333333333334,
      "grad_norm": 0.2688126862049103,
      "learning_rate": 7.810416666666667e-06,
      "loss": 0.0022,
      "step": 202510
    },
    {
      "epoch": 6.750666666666667,
      "grad_norm": 0.057342950254678726,
      "learning_rate": 7.808333333333335e-06,
      "loss": 0.0015,
      "step": 202520
    },
    {
      "epoch": 6.751,
      "grad_norm": 0.47249892354011536,
      "learning_rate": 7.806250000000002e-06,
      "loss": 0.0023,
      "step": 202530
    },
    {
      "epoch": 6.751333333333333,
      "grad_norm": 0.17151209712028503,
      "learning_rate": 7.804166666666667e-06,
      "loss": 0.0018,
      "step": 202540
    },
    {
      "epoch": 6.751666666666667,
      "grad_norm": 0.03137316554784775,
      "learning_rate": 7.802083333333333e-06,
      "loss": 0.0023,
      "step": 202550
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.01235460676252842,
      "learning_rate": 7.8e-06,
      "loss": 0.0019,
      "step": 202560
    },
    {
      "epoch": 6.7523333333333335,
      "grad_norm": 0.28270745277404785,
      "learning_rate": 7.797916666666667e-06,
      "loss": 0.0022,
      "step": 202570
    },
    {
      "epoch": 6.752666666666666,
      "grad_norm": 0.2003336250782013,
      "learning_rate": 7.795833333333334e-06,
      "loss": 0.0013,
      "step": 202580
    },
    {
      "epoch": 6.753,
      "grad_norm": 0.05779187008738518,
      "learning_rate": 7.793750000000001e-06,
      "loss": 0.0021,
      "step": 202590
    },
    {
      "epoch": 6.753333333333333,
      "grad_norm": 0.1146261915564537,
      "learning_rate": 7.791666666666667e-06,
      "loss": 0.0015,
      "step": 202600
    },
    {
      "epoch": 6.753666666666667,
      "grad_norm": 0.3274259865283966,
      "learning_rate": 7.789583333333334e-06,
      "loss": 0.0019,
      "step": 202610
    },
    {
      "epoch": 6.754,
      "grad_norm": 0.11703338474035263,
      "learning_rate": 7.7875e-06,
      "loss": 0.0016,
      "step": 202620
    },
    {
      "epoch": 6.754333333333333,
      "grad_norm": 0.030949819833040237,
      "learning_rate": 7.785416666666667e-06,
      "loss": 0.0015,
      "step": 202630
    },
    {
      "epoch": 6.754666666666667,
      "grad_norm": 0.285337895154953,
      "learning_rate": 7.783333333333334e-06,
      "loss": 0.0019,
      "step": 202640
    },
    {
      "epoch": 6.755,
      "grad_norm": 0.19984708726406097,
      "learning_rate": 7.781250000000001e-06,
      "loss": 0.0025,
      "step": 202650
    },
    {
      "epoch": 6.755333333333334,
      "grad_norm": 0.08613777160644531,
      "learning_rate": 7.779166666666667e-06,
      "loss": 0.0014,
      "step": 202660
    },
    {
      "epoch": 6.7556666666666665,
      "grad_norm": 0.3399948179721832,
      "learning_rate": 7.777083333333334e-06,
      "loss": 0.0016,
      "step": 202670
    },
    {
      "epoch": 6.756,
      "grad_norm": 0.07725895941257477,
      "learning_rate": 7.775000000000001e-06,
      "loss": 0.0018,
      "step": 202680
    },
    {
      "epoch": 6.756333333333333,
      "grad_norm": 0.030622605234384537,
      "learning_rate": 7.772916666666666e-06,
      "loss": 0.0015,
      "step": 202690
    },
    {
      "epoch": 6.756666666666667,
      "grad_norm": 0.17233265936374664,
      "learning_rate": 7.770833333333334e-06,
      "loss": 0.0015,
      "step": 202700
    },
    {
      "epoch": 6.757,
      "grad_norm": 0.20175887644290924,
      "learning_rate": 7.76875e-06,
      "loss": 0.0014,
      "step": 202710
    },
    {
      "epoch": 6.757333333333333,
      "grad_norm": 0.03248003125190735,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0018,
      "step": 202720
    },
    {
      "epoch": 6.757666666666667,
      "grad_norm": 0.4950903356075287,
      "learning_rate": 7.764583333333333e-06,
      "loss": 0.0021,
      "step": 202730
    },
    {
      "epoch": 6.758,
      "grad_norm": 0.08601471781730652,
      "learning_rate": 7.7625e-06,
      "loss": 0.0015,
      "step": 202740
    },
    {
      "epoch": 6.758333333333333,
      "grad_norm": 0.2569858431816101,
      "learning_rate": 7.760416666666668e-06,
      "loss": 0.0015,
      "step": 202750
    },
    {
      "epoch": 6.758666666666667,
      "grad_norm": 0.19984638690948486,
      "learning_rate": 7.758333333333333e-06,
      "loss": 0.0021,
      "step": 202760
    },
    {
      "epoch": 6.759,
      "grad_norm": 0.142746701836586,
      "learning_rate": 7.75625e-06,
      "loss": 0.0018,
      "step": 202770
    },
    {
      "epoch": 6.759333333333333,
      "grad_norm": 0.1715213805437088,
      "learning_rate": 7.754166666666666e-06,
      "loss": 0.0014,
      "step": 202780
    },
    {
      "epoch": 6.759666666666667,
      "grad_norm": 0.32707828283309937,
      "learning_rate": 7.752083333333333e-06,
      "loss": 0.0019,
      "step": 202790
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.28533610701560974,
      "learning_rate": 7.75e-06,
      "loss": 0.0021,
      "step": 202800
    },
    {
      "epoch": 6.7603333333333335,
      "grad_norm": 0.03185610473155975,
      "learning_rate": 7.747916666666668e-06,
      "loss": 0.0016,
      "step": 202810
    },
    {
      "epoch": 6.760666666666666,
      "grad_norm": 0.11440988630056381,
      "learning_rate": 7.745833333333335e-06,
      "loss": 0.0016,
      "step": 202820
    },
    {
      "epoch": 6.761,
      "grad_norm": 0.11517726629972458,
      "learning_rate": 7.74375e-06,
      "loss": 0.0023,
      "step": 202830
    },
    {
      "epoch": 6.761333333333333,
      "grad_norm": 0.26166045665740967,
      "learning_rate": 7.741666666666666e-06,
      "loss": 0.0019,
      "step": 202840
    },
    {
      "epoch": 6.761666666666667,
      "grad_norm": 0.006137033924460411,
      "learning_rate": 7.739583333333333e-06,
      "loss": 0.0023,
      "step": 202850
    },
    {
      "epoch": 6.7620000000000005,
      "grad_norm": 0.14296852052211761,
      "learning_rate": 7.7375e-06,
      "loss": 0.0013,
      "step": 202860
    },
    {
      "epoch": 6.762333333333333,
      "grad_norm": 0.1795695424079895,
      "learning_rate": 7.735416666666667e-06,
      "loss": 0.0016,
      "step": 202870
    },
    {
      "epoch": 6.762666666666667,
      "grad_norm": 0.13925978541374207,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0015,
      "step": 202880
    },
    {
      "epoch": 6.763,
      "grad_norm": 0.08708231896162033,
      "learning_rate": 7.731250000000002e-06,
      "loss": 0.0022,
      "step": 202890
    },
    {
      "epoch": 6.763333333333334,
      "grad_norm": 0.13758838176727295,
      "learning_rate": 7.729166666666667e-06,
      "loss": 0.002,
      "step": 202900
    },
    {
      "epoch": 6.7636666666666665,
      "grad_norm": 0.05833813548088074,
      "learning_rate": 7.727083333333334e-06,
      "loss": 0.0028,
      "step": 202910
    },
    {
      "epoch": 6.764,
      "grad_norm": 0.26273682713508606,
      "learning_rate": 7.725e-06,
      "loss": 0.0015,
      "step": 202920
    },
    {
      "epoch": 6.764333333333333,
      "grad_norm": 0.010477263480424881,
      "learning_rate": 7.722916666666667e-06,
      "loss": 0.0015,
      "step": 202930
    },
    {
      "epoch": 6.764666666666667,
      "grad_norm": 0.14272572100162506,
      "learning_rate": 7.720833333333334e-06,
      "loss": 0.0013,
      "step": 202940
    },
    {
      "epoch": 6.765,
      "grad_norm": 0.19985425472259521,
      "learning_rate": 7.718750000000001e-06,
      "loss": 0.002,
      "step": 202950
    },
    {
      "epoch": 6.765333333333333,
      "grad_norm": 0.0138409323990345,
      "learning_rate": 7.716666666666667e-06,
      "loss": 0.0017,
      "step": 202960
    },
    {
      "epoch": 6.765666666666666,
      "grad_norm": 0.2566147446632385,
      "learning_rate": 7.714583333333334e-06,
      "loss": 0.002,
      "step": 202970
    },
    {
      "epoch": 6.766,
      "grad_norm": 0.1434139460325241,
      "learning_rate": 7.712500000000001e-06,
      "loss": 0.0019,
      "step": 202980
    },
    {
      "epoch": 6.766333333333334,
      "grad_norm": 0.030623825266957283,
      "learning_rate": 7.710416666666667e-06,
      "loss": 0.002,
      "step": 202990
    },
    {
      "epoch": 6.766666666666667,
      "grad_norm": 0.5526617765426636,
      "learning_rate": 7.708333333333334e-06,
      "loss": 0.002,
      "step": 203000
    },
    {
      "epoch": 6.767,
      "grad_norm": 0.08584536612033844,
      "learning_rate": 7.706250000000001e-06,
      "loss": 0.0013,
      "step": 203010
    },
    {
      "epoch": 6.767333333333333,
      "grad_norm": 0.14300668239593506,
      "learning_rate": 7.704166666666666e-06,
      "loss": 0.0026,
      "step": 203020
    },
    {
      "epoch": 6.767666666666667,
      "grad_norm": 0.01647442765533924,
      "learning_rate": 7.702083333333334e-06,
      "loss": 0.0022,
      "step": 203030
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.057730548083782196,
      "learning_rate": 7.7e-06,
      "loss": 0.0018,
      "step": 203040
    },
    {
      "epoch": 6.7683333333333335,
      "grad_norm": 0.05769510194659233,
      "learning_rate": 7.697916666666668e-06,
      "loss": 0.0018,
      "step": 203050
    },
    {
      "epoch": 6.768666666666666,
      "grad_norm": 0.11399730294942856,
      "learning_rate": 7.695833333333333e-06,
      "loss": 0.0023,
      "step": 203060
    },
    {
      "epoch": 6.769,
      "grad_norm": 0.058039527386426926,
      "learning_rate": 7.69375e-06,
      "loss": 0.0021,
      "step": 203070
    },
    {
      "epoch": 6.769333333333333,
      "grad_norm": 0.11467424035072327,
      "learning_rate": 7.691666666666666e-06,
      "loss": 0.0011,
      "step": 203080
    },
    {
      "epoch": 6.769666666666667,
      "grad_norm": 0.013323694467544556,
      "learning_rate": 7.689583333333333e-06,
      "loss": 0.0013,
      "step": 203090
    },
    {
      "epoch": 6.77,
      "grad_norm": 0.08668428659439087,
      "learning_rate": 7.6875e-06,
      "loss": 0.0013,
      "step": 203100
    },
    {
      "epoch": 6.770333333333333,
      "grad_norm": 0.3993748426437378,
      "learning_rate": 7.685416666666668e-06,
      "loss": 0.0026,
      "step": 203110
    },
    {
      "epoch": 6.770666666666667,
      "grad_norm": 0.22776475548744202,
      "learning_rate": 7.683333333333335e-06,
      "loss": 0.0014,
      "step": 203120
    },
    {
      "epoch": 6.771,
      "grad_norm": 0.2858680784702301,
      "learning_rate": 7.68125e-06,
      "loss": 0.0013,
      "step": 203130
    },
    {
      "epoch": 6.771333333333334,
      "grad_norm": 0.058895543217659,
      "learning_rate": 7.679166666666666e-06,
      "loss": 0.0014,
      "step": 203140
    },
    {
      "epoch": 6.7716666666666665,
      "grad_norm": 0.05830122157931328,
      "learning_rate": 7.677083333333333e-06,
      "loss": 0.0023,
      "step": 203150
    },
    {
      "epoch": 6.772,
      "grad_norm": 0.26049232482910156,
      "learning_rate": 7.675e-06,
      "loss": 0.0016,
      "step": 203160
    },
    {
      "epoch": 6.772333333333333,
      "grad_norm": 0.2008313238620758,
      "learning_rate": 7.672916666666667e-06,
      "loss": 0.0015,
      "step": 203170
    },
    {
      "epoch": 6.772666666666667,
      "grad_norm": 0.06535071134567261,
      "learning_rate": 7.670833333333335e-06,
      "loss": 0.002,
      "step": 203180
    },
    {
      "epoch": 6.773,
      "grad_norm": 0.40612897276878357,
      "learning_rate": 7.668750000000002e-06,
      "loss": 0.0016,
      "step": 203190
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.0859944075345993,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0017,
      "step": 203200
    },
    {
      "epoch": 6.773666666666666,
      "grad_norm": 0.03082406334578991,
      "learning_rate": 7.664583333333333e-06,
      "loss": 0.0023,
      "step": 203210
    },
    {
      "epoch": 6.774,
      "grad_norm": 0.4899364709854126,
      "learning_rate": 7.6625e-06,
      "loss": 0.0018,
      "step": 203220
    },
    {
      "epoch": 6.774333333333333,
      "grad_norm": 0.2001018226146698,
      "learning_rate": 7.660416666666667e-06,
      "loss": 0.0025,
      "step": 203230
    },
    {
      "epoch": 6.774666666666667,
      "grad_norm": 0.28519466519355774,
      "learning_rate": 7.658333333333334e-06,
      "loss": 0.0025,
      "step": 203240
    },
    {
      "epoch": 6.775,
      "grad_norm": 0.3995460867881775,
      "learning_rate": 7.656250000000001e-06,
      "loss": 0.0016,
      "step": 203250
    },
    {
      "epoch": 6.775333333333333,
      "grad_norm": 0.05029840022325516,
      "learning_rate": 7.654166666666667e-06,
      "loss": 0.0024,
      "step": 203260
    },
    {
      "epoch": 6.775666666666667,
      "grad_norm": 0.199916809797287,
      "learning_rate": 7.652083333333334e-06,
      "loss": 0.0022,
      "step": 203270
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.2288096845149994,
      "learning_rate": 7.65e-06,
      "loss": 0.0018,
      "step": 203280
    },
    {
      "epoch": 6.7763333333333335,
      "grad_norm": 0.029521416872739792,
      "learning_rate": 7.647916666666667e-06,
      "loss": 0.0014,
      "step": 203290
    },
    {
      "epoch": 6.776666666666666,
      "grad_norm": 0.14333805441856384,
      "learning_rate": 7.645833333333334e-06,
      "loss": 0.0013,
      "step": 203300
    },
    {
      "epoch": 6.777,
      "grad_norm": 0.171416774392128,
      "learning_rate": 7.643750000000001e-06,
      "loss": 0.0015,
      "step": 203310
    },
    {
      "epoch": 6.777333333333333,
      "grad_norm": 0.14325442910194397,
      "learning_rate": 7.641666666666667e-06,
      "loss": 0.0013,
      "step": 203320
    },
    {
      "epoch": 6.777666666666667,
      "grad_norm": 0.11455328017473221,
      "learning_rate": 7.639583333333334e-06,
      "loss": 0.0017,
      "step": 203330
    },
    {
      "epoch": 6.7780000000000005,
      "grad_norm": 0.25831058621406555,
      "learning_rate": 7.637500000000001e-06,
      "loss": 0.0037,
      "step": 203340
    },
    {
      "epoch": 6.778333333333333,
      "grad_norm": 0.12245422601699829,
      "learning_rate": 7.635416666666666e-06,
      "loss": 0.0018,
      "step": 203350
    },
    {
      "epoch": 6.778666666666666,
      "grad_norm": 0.23496553301811218,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.002,
      "step": 203360
    },
    {
      "epoch": 6.779,
      "grad_norm": 0.05845584720373154,
      "learning_rate": 7.63125e-06,
      "loss": 0.0013,
      "step": 203370
    },
    {
      "epoch": 6.779333333333334,
      "grad_norm": 0.28505054116249084,
      "learning_rate": 7.629166666666666e-06,
      "loss": 0.0017,
      "step": 203380
    },
    {
      "epoch": 6.7796666666666665,
      "grad_norm": 0.10458233952522278,
      "learning_rate": 7.6270833333333335e-06,
      "loss": 0.002,
      "step": 203390
    },
    {
      "epoch": 6.78,
      "grad_norm": 0.11944076418876648,
      "learning_rate": 7.625e-06,
      "loss": 0.0023,
      "step": 203400
    },
    {
      "epoch": 6.780333333333333,
      "grad_norm": 0.22866396605968475,
      "learning_rate": 7.622916666666667e-06,
      "loss": 0.0012,
      "step": 203410
    },
    {
      "epoch": 6.780666666666667,
      "grad_norm": 0.37077030539512634,
      "learning_rate": 7.620833333333334e-06,
      "loss": 0.0016,
      "step": 203420
    },
    {
      "epoch": 6.781,
      "grad_norm": 0.4860192835330963,
      "learning_rate": 7.6187500000000005e-06,
      "loss": 0.0024,
      "step": 203430
    },
    {
      "epoch": 6.781333333333333,
      "grad_norm": 0.14294743537902832,
      "learning_rate": 7.616666666666666e-06,
      "loss": 0.0012,
      "step": 203440
    },
    {
      "epoch": 6.781666666666666,
      "grad_norm": 0.20297056436538696,
      "learning_rate": 7.614583333333333e-06,
      "loss": 0.0016,
      "step": 203450
    },
    {
      "epoch": 6.782,
      "grad_norm": 0.05780256167054176,
      "learning_rate": 7.6125e-06,
      "loss": 0.0016,
      "step": 203460
    },
    {
      "epoch": 6.782333333333334,
      "grad_norm": 0.19078119099140167,
      "learning_rate": 7.610416666666667e-06,
      "loss": 0.0023,
      "step": 203470
    },
    {
      "epoch": 6.782666666666667,
      "grad_norm": 0.22825215756893158,
      "learning_rate": 7.608333333333334e-06,
      "loss": 0.0022,
      "step": 203480
    },
    {
      "epoch": 6.783,
      "grad_norm": 0.011086191982030869,
      "learning_rate": 7.606250000000001e-06,
      "loss": 0.0023,
      "step": 203490
    },
    {
      "epoch": 6.783333333333333,
      "grad_norm": 0.3709181845188141,
      "learning_rate": 7.6041666666666666e-06,
      "loss": 0.0015,
      "step": 203500
    },
    {
      "epoch": 6.783666666666667,
      "grad_norm": 0.30528855323791504,
      "learning_rate": 7.602083333333333e-06,
      "loss": 0.0017,
      "step": 203510
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.029898062348365784,
      "learning_rate": 7.6e-06,
      "loss": 0.0013,
      "step": 203520
    },
    {
      "epoch": 6.7843333333333335,
      "grad_norm": 0.200065016746521,
      "learning_rate": 7.597916666666667e-06,
      "loss": 0.0013,
      "step": 203530
    },
    {
      "epoch": 6.784666666666666,
      "grad_norm": 0.011133972555398941,
      "learning_rate": 7.595833333333334e-06,
      "loss": 0.0014,
      "step": 203540
    },
    {
      "epoch": 6.785,
      "grad_norm": 0.1429423838853836,
      "learning_rate": 7.593750000000001e-06,
      "loss": 0.0016,
      "step": 203550
    },
    {
      "epoch": 6.785333333333333,
      "grad_norm": 0.14320255815982819,
      "learning_rate": 7.591666666666666e-06,
      "loss": 0.0013,
      "step": 203560
    },
    {
      "epoch": 6.785666666666667,
      "grad_norm": 0.057686980813741684,
      "learning_rate": 7.5895833333333334e-06,
      "loss": 0.0015,
      "step": 203570
    },
    {
      "epoch": 6.786,
      "grad_norm": 0.2281329184770584,
      "learning_rate": 7.5875e-06,
      "loss": 0.002,
      "step": 203580
    },
    {
      "epoch": 6.786333333333333,
      "grad_norm": 0.17210517823696136,
      "learning_rate": 7.585416666666667e-06,
      "loss": 0.0013,
      "step": 203590
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.16968262195587158,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.002,
      "step": 203600
    },
    {
      "epoch": 6.787,
      "grad_norm": 0.14697906374931335,
      "learning_rate": 7.5812500000000005e-06,
      "loss": 0.0016,
      "step": 203610
    },
    {
      "epoch": 6.787333333333334,
      "grad_norm": 0.199747234582901,
      "learning_rate": 7.579166666666666e-06,
      "loss": 0.0014,
      "step": 203620
    },
    {
      "epoch": 6.7876666666666665,
      "grad_norm": 0.1999933421611786,
      "learning_rate": 7.577083333333333e-06,
      "loss": 0.0019,
      "step": 203630
    },
    {
      "epoch": 6.788,
      "grad_norm": 0.14687888324260712,
      "learning_rate": 7.575e-06,
      "loss": 0.0019,
      "step": 203640
    },
    {
      "epoch": 6.788333333333333,
      "grad_norm": 0.06016680225729942,
      "learning_rate": 7.572916666666667e-06,
      "loss": 0.0023,
      "step": 203650
    },
    {
      "epoch": 6.788666666666667,
      "grad_norm": 0.17186294496059418,
      "learning_rate": 7.570833333333334e-06,
      "loss": 0.002,
      "step": 203660
    },
    {
      "epoch": 6.789,
      "grad_norm": 0.2000388652086258,
      "learning_rate": 7.568750000000001e-06,
      "loss": 0.002,
      "step": 203670
    },
    {
      "epoch": 6.789333333333333,
      "grad_norm": 0.19998224079608917,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0014,
      "step": 203680
    },
    {
      "epoch": 6.789666666666666,
      "grad_norm": 0.03163480758666992,
      "learning_rate": 7.564583333333333e-06,
      "loss": 0.0021,
      "step": 203690
    },
    {
      "epoch": 6.79,
      "grad_norm": 0.1482624113559723,
      "learning_rate": 7.5625e-06,
      "loss": 0.0017,
      "step": 203700
    },
    {
      "epoch": 6.790333333333333,
      "grad_norm": 0.31419485807418823,
      "learning_rate": 7.560416666666667e-06,
      "loss": 0.0014,
      "step": 203710
    },
    {
      "epoch": 6.790666666666667,
      "grad_norm": 0.2850782871246338,
      "learning_rate": 7.5583333333333335e-06,
      "loss": 0.0015,
      "step": 203720
    },
    {
      "epoch": 6.791,
      "grad_norm": 0.28567051887512207,
      "learning_rate": 7.556250000000001e-06,
      "loss": 0.0018,
      "step": 203730
    },
    {
      "epoch": 6.791333333333333,
      "grad_norm": 0.1427697241306305,
      "learning_rate": 7.554166666666666e-06,
      "loss": 0.0014,
      "step": 203740
    },
    {
      "epoch": 6.791666666666667,
      "grad_norm": 0.11481346935033798,
      "learning_rate": 7.552083333333333e-06,
      "loss": 0.0012,
      "step": 203750
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.0612478069961071,
      "learning_rate": 7.55e-06,
      "loss": 0.0015,
      "step": 203760
    },
    {
      "epoch": 6.792333333333334,
      "grad_norm": 0.11463271081447601,
      "learning_rate": 7.547916666666667e-06,
      "loss": 0.0015,
      "step": 203770
    },
    {
      "epoch": 6.792666666666666,
      "grad_norm": 0.0584406815469265,
      "learning_rate": 7.545833333333334e-06,
      "loss": 0.0017,
      "step": 203780
    },
    {
      "epoch": 6.793,
      "grad_norm": 0.14336270093917847,
      "learning_rate": 7.54375e-06,
      "loss": 0.0016,
      "step": 203790
    },
    {
      "epoch": 6.793333333333333,
      "grad_norm": 0.05936587229371071,
      "learning_rate": 7.541666666666668e-06,
      "loss": 0.0013,
      "step": 203800
    },
    {
      "epoch": 6.793666666666667,
      "grad_norm": 0.17129071056842804,
      "learning_rate": 7.539583333333333e-06,
      "loss": 0.0013,
      "step": 203810
    },
    {
      "epoch": 6.7940000000000005,
      "grad_norm": 0.08654236048460007,
      "learning_rate": 7.5375e-06,
      "loss": 0.0014,
      "step": 203820
    },
    {
      "epoch": 6.794333333333333,
      "grad_norm": 0.17206762731075287,
      "learning_rate": 7.5354166666666674e-06,
      "loss": 0.002,
      "step": 203830
    },
    {
      "epoch": 6.794666666666666,
      "grad_norm": 0.03121965005993843,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0015,
      "step": 203840
    },
    {
      "epoch": 6.795,
      "grad_norm": 0.2029159963130951,
      "learning_rate": 7.531250000000001e-06,
      "loss": 0.0022,
      "step": 203850
    },
    {
      "epoch": 6.795333333333334,
      "grad_norm": 0.22860006988048553,
      "learning_rate": 7.529166666666667e-06,
      "loss": 0.0012,
      "step": 203860
    },
    {
      "epoch": 6.7956666666666665,
      "grad_norm": 0.04851897805929184,
      "learning_rate": 7.527083333333334e-06,
      "loss": 0.0016,
      "step": 203870
    },
    {
      "epoch": 6.796,
      "grad_norm": 0.01451034564524889,
      "learning_rate": 7.525e-06,
      "loss": 0.0018,
      "step": 203880
    },
    {
      "epoch": 6.796333333333333,
      "grad_norm": 0.4006543457508087,
      "learning_rate": 7.522916666666667e-06,
      "loss": 0.0019,
      "step": 203890
    },
    {
      "epoch": 6.796666666666667,
      "grad_norm": 0.10721614211797714,
      "learning_rate": 7.520833333333334e-06,
      "loss": 0.0014,
      "step": 203900
    },
    {
      "epoch": 6.797,
      "grad_norm": 0.0058007026091217995,
      "learning_rate": 7.518750000000001e-06,
      "loss": 0.0017,
      "step": 203910
    },
    {
      "epoch": 6.7973333333333334,
      "grad_norm": 0.2004508525133133,
      "learning_rate": 7.516666666666668e-06,
      "loss": 0.0014,
      "step": 203920
    },
    {
      "epoch": 6.797666666666666,
      "grad_norm": 0.0917045995593071,
      "learning_rate": 7.514583333333333e-06,
      "loss": 0.0017,
      "step": 203930
    },
    {
      "epoch": 6.798,
      "grad_norm": 0.17260350286960602,
      "learning_rate": 7.5125000000000005e-06,
      "loss": 0.0017,
      "step": 203940
    },
    {
      "epoch": 6.798333333333334,
      "grad_norm": 0.14394696056842804,
      "learning_rate": 7.510416666666667e-06,
      "loss": 0.0021,
      "step": 203950
    },
    {
      "epoch": 6.798666666666667,
      "grad_norm": 0.11487974971532822,
      "learning_rate": 7.508333333333334e-06,
      "loss": 0.002,
      "step": 203960
    },
    {
      "epoch": 6.799,
      "grad_norm": 0.17529115080833435,
      "learning_rate": 7.506250000000001e-06,
      "loss": 0.0016,
      "step": 203970
    },
    {
      "epoch": 6.799333333333333,
      "grad_norm": 0.314811110496521,
      "learning_rate": 7.5041666666666675e-06,
      "loss": 0.0021,
      "step": 203980
    },
    {
      "epoch": 6.799666666666667,
      "grad_norm": 0.11459517478942871,
      "learning_rate": 7.502083333333333e-06,
      "loss": 0.0016,
      "step": 203990
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.19083335995674133,
      "learning_rate": 7.5e-06,
      "loss": 0.0015,
      "step": 204000
    },
    {
      "epoch": 6.800333333333334,
      "grad_norm": 0.14318139851093292,
      "learning_rate": 7.497916666666667e-06,
      "loss": 0.0015,
      "step": 204010
    },
    {
      "epoch": 6.800666666666666,
      "grad_norm": 0.08632750809192657,
      "learning_rate": 7.495833333333334e-06,
      "loss": 0.0017,
      "step": 204020
    },
    {
      "epoch": 6.801,
      "grad_norm": 0.08779171109199524,
      "learning_rate": 7.493750000000001e-06,
      "loss": 0.0013,
      "step": 204030
    },
    {
      "epoch": 6.801333333333333,
      "grad_norm": 0.1717655211687088,
      "learning_rate": 7.491666666666668e-06,
      "loss": 0.0018,
      "step": 204040
    },
    {
      "epoch": 6.801666666666667,
      "grad_norm": 0.45858609676361084,
      "learning_rate": 7.4895833333333336e-06,
      "loss": 0.0018,
      "step": 204050
    },
    {
      "epoch": 6.802,
      "grad_norm": 0.058062274008989334,
      "learning_rate": 7.4875e-06,
      "loss": 0.002,
      "step": 204060
    },
    {
      "epoch": 6.802333333333333,
      "grad_norm": 0.22844728827476501,
      "learning_rate": 7.485416666666667e-06,
      "loss": 0.0022,
      "step": 204070
    },
    {
      "epoch": 6.802666666666667,
      "grad_norm": 0.09397874027490616,
      "learning_rate": 7.483333333333334e-06,
      "loss": 0.0011,
      "step": 204080
    },
    {
      "epoch": 6.803,
      "grad_norm": 0.14298270642757416,
      "learning_rate": 7.481250000000001e-06,
      "loss": 0.0015,
      "step": 204090
    },
    {
      "epoch": 6.803333333333334,
      "grad_norm": 0.20059289038181305,
      "learning_rate": 7.479166666666668e-06,
      "loss": 0.0014,
      "step": 204100
    },
    {
      "epoch": 6.8036666666666665,
      "grad_norm": 0.08693819493055344,
      "learning_rate": 7.477083333333333e-06,
      "loss": 0.0016,
      "step": 204110
    },
    {
      "epoch": 6.804,
      "grad_norm": 0.033134959638118744,
      "learning_rate": 7.4750000000000004e-06,
      "loss": 0.0011,
      "step": 204120
    },
    {
      "epoch": 6.804333333333333,
      "grad_norm": 0.04317712038755417,
      "learning_rate": 7.472916666666667e-06,
      "loss": 0.0017,
      "step": 204130
    },
    {
      "epoch": 6.804666666666667,
      "grad_norm": 0.2278212308883667,
      "learning_rate": 7.470833333333334e-06,
      "loss": 0.0013,
      "step": 204140
    },
    {
      "epoch": 6.805,
      "grad_norm": 0.14324791729450226,
      "learning_rate": 7.468750000000001e-06,
      "loss": 0.0017,
      "step": 204150
    },
    {
      "epoch": 6.8053333333333335,
      "grad_norm": 0.36286604404449463,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0014,
      "step": 204160
    },
    {
      "epoch": 6.805666666666666,
      "grad_norm": 0.42762693762779236,
      "learning_rate": 7.464583333333333e-06,
      "loss": 0.0023,
      "step": 204170
    },
    {
      "epoch": 6.806,
      "grad_norm": 0.31441450119018555,
      "learning_rate": 7.4625e-06,
      "loss": 0.0019,
      "step": 204180
    },
    {
      "epoch": 6.806333333333333,
      "grad_norm": 0.008318235166370869,
      "learning_rate": 7.460416666666667e-06,
      "loss": 0.0018,
      "step": 204190
    },
    {
      "epoch": 6.806666666666667,
      "grad_norm": 0.05779268965125084,
      "learning_rate": 7.458333333333334e-06,
      "loss": 0.0019,
      "step": 204200
    },
    {
      "epoch": 6.807,
      "grad_norm": 0.2269800752401352,
      "learning_rate": 7.456250000000001e-06,
      "loss": 0.0016,
      "step": 204210
    },
    {
      "epoch": 6.807333333333333,
      "grad_norm": 0.05009034276008606,
      "learning_rate": 7.454166666666668e-06,
      "loss": 0.0014,
      "step": 204220
    },
    {
      "epoch": 6.807666666666667,
      "grad_norm": 0.06969985365867615,
      "learning_rate": 7.4520833333333335e-06,
      "loss": 0.0021,
      "step": 204230
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.15407992899417877,
      "learning_rate": 7.45e-06,
      "loss": 0.0023,
      "step": 204240
    },
    {
      "epoch": 6.808333333333334,
      "grad_norm": 0.2293529361486435,
      "learning_rate": 7.447916666666667e-06,
      "loss": 0.0021,
      "step": 204250
    },
    {
      "epoch": 6.808666666666666,
      "grad_norm": 0.2857067286968231,
      "learning_rate": 7.445833333333334e-06,
      "loss": 0.0014,
      "step": 204260
    },
    {
      "epoch": 6.809,
      "grad_norm": 0.03222696855664253,
      "learning_rate": 7.4437500000000005e-06,
      "loss": 0.0012,
      "step": 204270
    },
    {
      "epoch": 6.809333333333333,
      "grad_norm": 0.19821831583976746,
      "learning_rate": 7.441666666666668e-06,
      "loss": 0.0021,
      "step": 204280
    },
    {
      "epoch": 6.809666666666667,
      "grad_norm": 0.17222504317760468,
      "learning_rate": 7.439583333333333e-06,
      "loss": 0.0017,
      "step": 204290
    },
    {
      "epoch": 6.8100000000000005,
      "grad_norm": 0.31460610032081604,
      "learning_rate": 7.4375e-06,
      "loss": 0.0016,
      "step": 204300
    },
    {
      "epoch": 6.810333333333333,
      "grad_norm": 0.3423556685447693,
      "learning_rate": 7.435416666666667e-06,
      "loss": 0.0015,
      "step": 204310
    },
    {
      "epoch": 6.810666666666666,
      "grad_norm": 0.05794498696923256,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.0021,
      "step": 204320
    },
    {
      "epoch": 6.811,
      "grad_norm": 0.143101766705513,
      "learning_rate": 7.431250000000001e-06,
      "loss": 0.0012,
      "step": 204330
    },
    {
      "epoch": 6.811333333333334,
      "grad_norm": 0.3421750068664551,
      "learning_rate": 7.429166666666667e-06,
      "loss": 0.0016,
      "step": 204340
    },
    {
      "epoch": 6.8116666666666665,
      "grad_norm": 0.05915452167391777,
      "learning_rate": 7.427083333333333e-06,
      "loss": 0.0013,
      "step": 204350
    },
    {
      "epoch": 6.812,
      "grad_norm": 0.16022592782974243,
      "learning_rate": 7.425e-06,
      "loss": 0.0015,
      "step": 204360
    },
    {
      "epoch": 6.812333333333333,
      "grad_norm": 0.03258189186453819,
      "learning_rate": 7.422916666666667e-06,
      "loss": 0.0015,
      "step": 204370
    },
    {
      "epoch": 6.812666666666667,
      "grad_norm": 0.08629796653985977,
      "learning_rate": 7.4208333333333336e-06,
      "loss": 0.0018,
      "step": 204380
    },
    {
      "epoch": 6.813,
      "grad_norm": 0.42925816774368286,
      "learning_rate": 7.418750000000001e-06,
      "loss": 0.0023,
      "step": 204390
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 0.1429952085018158,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.0012,
      "step": 204400
    },
    {
      "epoch": 6.813666666666666,
      "grad_norm": 0.14251121878623962,
      "learning_rate": 7.4145833333333334e-06,
      "loss": 0.0013,
      "step": 204410
    },
    {
      "epoch": 6.814,
      "grad_norm": 0.19276680052280426,
      "learning_rate": 7.4125e-06,
      "loss": 0.0023,
      "step": 204420
    },
    {
      "epoch": 6.814333333333334,
      "grad_norm": 0.20005486905574799,
      "learning_rate": 7.410416666666667e-06,
      "loss": 0.0012,
      "step": 204430
    },
    {
      "epoch": 6.814666666666667,
      "grad_norm": 0.11452300101518631,
      "learning_rate": 7.408333333333334e-06,
      "loss": 0.0013,
      "step": 204440
    },
    {
      "epoch": 6.8149999999999995,
      "grad_norm": 0.17181655764579773,
      "learning_rate": 7.4062500000000005e-06,
      "loss": 0.0026,
      "step": 204450
    },
    {
      "epoch": 6.815333333333333,
      "grad_norm": 0.4573502838611603,
      "learning_rate": 7.404166666666668e-06,
      "loss": 0.0014,
      "step": 204460
    },
    {
      "epoch": 6.815666666666667,
      "grad_norm": 0.3433769941329956,
      "learning_rate": 7.402083333333333e-06,
      "loss": 0.0019,
      "step": 204470
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.030206896364688873,
      "learning_rate": 7.4e-06,
      "loss": 0.0015,
      "step": 204480
    },
    {
      "epoch": 6.816333333333334,
      "grad_norm": 0.11531393975019455,
      "learning_rate": 7.397916666666667e-06,
      "loss": 0.0016,
      "step": 204490
    },
    {
      "epoch": 6.816666666666666,
      "grad_norm": 0.17271587252616882,
      "learning_rate": 7.395833333333334e-06,
      "loss": 0.0016,
      "step": 204500
    },
    {
      "epoch": 6.817,
      "grad_norm": 0.300506591796875,
      "learning_rate": 7.393750000000001e-06,
      "loss": 0.0022,
      "step": 204510
    },
    {
      "epoch": 6.817333333333333,
      "grad_norm": 0.14299845695495605,
      "learning_rate": 7.391666666666667e-06,
      "loss": 0.0021,
      "step": 204520
    },
    {
      "epoch": 6.817666666666667,
      "grad_norm": 0.17084982991218567,
      "learning_rate": 7.389583333333333e-06,
      "loss": 0.0015,
      "step": 204530
    },
    {
      "epoch": 6.818,
      "grad_norm": 0.20638540387153625,
      "learning_rate": 7.3875e-06,
      "loss": 0.0012,
      "step": 204540
    },
    {
      "epoch": 6.818333333333333,
      "grad_norm": 0.08653215318918228,
      "learning_rate": 7.385416666666667e-06,
      "loss": 0.0015,
      "step": 204550
    },
    {
      "epoch": 6.818666666666667,
      "grad_norm": 0.029560433700680733,
      "learning_rate": 7.3833333333333335e-06,
      "loss": 0.0018,
      "step": 204560
    },
    {
      "epoch": 6.819,
      "grad_norm": 0.14326438307762146,
      "learning_rate": 7.381250000000001e-06,
      "loss": 0.0014,
      "step": 204570
    },
    {
      "epoch": 6.819333333333334,
      "grad_norm": 0.4081193506717682,
      "learning_rate": 7.379166666666668e-06,
      "loss": 0.0016,
      "step": 204580
    },
    {
      "epoch": 6.8196666666666665,
      "grad_norm": 0.31401175260543823,
      "learning_rate": 7.377083333333333e-06,
      "loss": 0.0021,
      "step": 204590
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.22831706702709198,
      "learning_rate": 7.375e-06,
      "loss": 0.0018,
      "step": 204600
    },
    {
      "epoch": 6.820333333333333,
      "grad_norm": 0.1156865581870079,
      "learning_rate": 7.372916666666667e-06,
      "loss": 0.0022,
      "step": 204610
    },
    {
      "epoch": 6.820666666666667,
      "grad_norm": 0.22873114049434662,
      "learning_rate": 7.370833333333334e-06,
      "loss": 0.0016,
      "step": 204620
    },
    {
      "epoch": 6.821,
      "grad_norm": 0.031388700008392334,
      "learning_rate": 7.36875e-06,
      "loss": 0.0016,
      "step": 204630
    },
    {
      "epoch": 6.8213333333333335,
      "grad_norm": 0.1713779717683792,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0012,
      "step": 204640
    },
    {
      "epoch": 6.821666666666666,
      "grad_norm": 0.1434524953365326,
      "learning_rate": 7.364583333333333e-06,
      "loss": 0.0019,
      "step": 204650
    },
    {
      "epoch": 6.822,
      "grad_norm": 0.23024480044841766,
      "learning_rate": 7.3625e-06,
      "loss": 0.0025,
      "step": 204660
    },
    {
      "epoch": 6.822333333333333,
      "grad_norm": 0.053941383957862854,
      "learning_rate": 7.3604166666666666e-06,
      "loss": 0.002,
      "step": 204670
    },
    {
      "epoch": 6.822666666666667,
      "grad_norm": 0.11457376927137375,
      "learning_rate": 7.358333333333334e-06,
      "loss": 0.0018,
      "step": 204680
    },
    {
      "epoch": 6.823,
      "grad_norm": 0.05814499408006668,
      "learning_rate": 7.356250000000001e-06,
      "loss": 0.0016,
      "step": 204690
    },
    {
      "epoch": 6.823333333333333,
      "grad_norm": 0.1151108369231224,
      "learning_rate": 7.354166666666667e-06,
      "loss": 0.002,
      "step": 204700
    },
    {
      "epoch": 6.823666666666667,
      "grad_norm": 0.142837256193161,
      "learning_rate": 7.352083333333333e-06,
      "loss": 0.0015,
      "step": 204710
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.2570318877696991,
      "learning_rate": 7.35e-06,
      "loss": 0.002,
      "step": 204720
    },
    {
      "epoch": 6.824333333333334,
      "grad_norm": 0.344396710395813,
      "learning_rate": 7.347916666666667e-06,
      "loss": 0.0028,
      "step": 204730
    },
    {
      "epoch": 6.824666666666666,
      "grad_norm": 0.05776175111532211,
      "learning_rate": 7.3458333333333334e-06,
      "loss": 0.0025,
      "step": 204740
    },
    {
      "epoch": 6.825,
      "grad_norm": 0.03475787490606308,
      "learning_rate": 7.343750000000001e-06,
      "loss": 0.0014,
      "step": 204750
    },
    {
      "epoch": 6.825333333333333,
      "grad_norm": 0.05768468976020813,
      "learning_rate": 7.341666666666668e-06,
      "loss": 0.0021,
      "step": 204760
    },
    {
      "epoch": 6.825666666666667,
      "grad_norm": 0.228296160697937,
      "learning_rate": 7.339583333333333e-06,
      "loss": 0.0025,
      "step": 204770
    },
    {
      "epoch": 6.826,
      "grad_norm": 0.5698426365852356,
      "learning_rate": 7.3375e-06,
      "loss": 0.0015,
      "step": 204780
    },
    {
      "epoch": 6.826333333333333,
      "grad_norm": 0.07650215923786163,
      "learning_rate": 7.335416666666667e-06,
      "loss": 0.0021,
      "step": 204790
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.1145583838224411,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0017,
      "step": 204800
    },
    {
      "epoch": 6.827,
      "grad_norm": 0.17120623588562012,
      "learning_rate": 7.33125e-06,
      "loss": 0.0017,
      "step": 204810
    },
    {
      "epoch": 6.827333333333334,
      "grad_norm": 0.3368871808052063,
      "learning_rate": 7.3291666666666675e-06,
      "loss": 0.0016,
      "step": 204820
    },
    {
      "epoch": 6.8276666666666666,
      "grad_norm": 0.11428666859865189,
      "learning_rate": 7.327083333333333e-06,
      "loss": 0.0021,
      "step": 204830
    },
    {
      "epoch": 6.828,
      "grad_norm": 0.01882348768413067,
      "learning_rate": 7.325e-06,
      "loss": 0.0019,
      "step": 204840
    },
    {
      "epoch": 6.828333333333333,
      "grad_norm": 0.25672465562820435,
      "learning_rate": 7.3229166666666665e-06,
      "loss": 0.0024,
      "step": 204850
    },
    {
      "epoch": 6.828666666666667,
      "grad_norm": 0.14247135818004608,
      "learning_rate": 7.320833333333334e-06,
      "loss": 0.0015,
      "step": 204860
    },
    {
      "epoch": 6.829,
      "grad_norm": 0.19975385069847107,
      "learning_rate": 7.318750000000001e-06,
      "loss": 0.0016,
      "step": 204870
    },
    {
      "epoch": 6.8293333333333335,
      "grad_norm": 0.08636324852705002,
      "learning_rate": 7.316666666666667e-06,
      "loss": 0.0022,
      "step": 204880
    },
    {
      "epoch": 6.829666666666666,
      "grad_norm": 0.08615915477275848,
      "learning_rate": 7.314583333333333e-06,
      "loss": 0.0016,
      "step": 204890
    },
    {
      "epoch": 6.83,
      "grad_norm": 0.2861914038658142,
      "learning_rate": 7.3125e-06,
      "loss": 0.0015,
      "step": 204900
    },
    {
      "epoch": 6.830333333333334,
      "grad_norm": 0.057783495634794235,
      "learning_rate": 7.310416666666667e-06,
      "loss": 0.0013,
      "step": 204910
    },
    {
      "epoch": 6.830666666666667,
      "grad_norm": 0.22923952341079712,
      "learning_rate": 7.308333333333333e-06,
      "loss": 0.0022,
      "step": 204920
    },
    {
      "epoch": 6.8309999999999995,
      "grad_norm": 0.0297702569514513,
      "learning_rate": 7.3062500000000006e-06,
      "loss": 0.002,
      "step": 204930
    },
    {
      "epoch": 6.831333333333333,
      "grad_norm": 0.015379874035716057,
      "learning_rate": 7.304166666666668e-06,
      "loss": 0.0017,
      "step": 204940
    },
    {
      "epoch": 6.831666666666667,
      "grad_norm": 0.28522375226020813,
      "learning_rate": 7.302083333333333e-06,
      "loss": 0.0016,
      "step": 204950
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.3240756392478943,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0017,
      "step": 204960
    },
    {
      "epoch": 6.832333333333334,
      "grad_norm": 0.37560081481933594,
      "learning_rate": 7.297916666666667e-06,
      "loss": 0.0022,
      "step": 204970
    },
    {
      "epoch": 6.832666666666666,
      "grad_norm": 0.0595397986471653,
      "learning_rate": 7.295833333333334e-06,
      "loss": 0.002,
      "step": 204980
    },
    {
      "epoch": 6.833,
      "grad_norm": 0.28449752926826477,
      "learning_rate": 7.29375e-06,
      "loss": 0.0014,
      "step": 204990
    },
    {
      "epoch": 6.833333333333333,
      "grad_norm": 0.19970260560512543,
      "learning_rate": 7.2916666666666674e-06,
      "loss": 0.0017,
      "step": 205000
    },
    {
      "epoch": 6.833666666666667,
      "grad_norm": 0.1447659134864807,
      "learning_rate": 7.289583333333335e-06,
      "loss": 0.0013,
      "step": 205010
    },
    {
      "epoch": 6.834,
      "grad_norm": 0.2516941726207733,
      "learning_rate": 7.2875e-06,
      "loss": 0.0017,
      "step": 205020
    },
    {
      "epoch": 6.834333333333333,
      "grad_norm": 0.045069802552461624,
      "learning_rate": 7.2854166666666664e-06,
      "loss": 0.0021,
      "step": 205030
    },
    {
      "epoch": 6.834666666666667,
      "grad_norm": 0.22863410413265228,
      "learning_rate": 7.283333333333334e-06,
      "loss": 0.0015,
      "step": 205040
    },
    {
      "epoch": 6.835,
      "grad_norm": 0.14317934215068817,
      "learning_rate": 7.281250000000001e-06,
      "loss": 0.0017,
      "step": 205050
    },
    {
      "epoch": 6.835333333333334,
      "grad_norm": 0.030954934656620026,
      "learning_rate": 7.279166666666667e-06,
      "loss": 0.0021,
      "step": 205060
    },
    {
      "epoch": 6.835666666666667,
      "grad_norm": 0.37100133299827576,
      "learning_rate": 7.277083333333334e-06,
      "loss": 0.0012,
      "step": 205070
    },
    {
      "epoch": 6.836,
      "grad_norm": 0.059068940579891205,
      "learning_rate": 7.275e-06,
      "loss": 0.0021,
      "step": 205080
    },
    {
      "epoch": 6.836333333333333,
      "grad_norm": 0.1711038500070572,
      "learning_rate": 7.272916666666667e-06,
      "loss": 0.0015,
      "step": 205090
    },
    {
      "epoch": 6.836666666666667,
      "grad_norm": 0.059185940772295,
      "learning_rate": 7.270833333333333e-06,
      "loss": 0.0014,
      "step": 205100
    },
    {
      "epoch": 6.837,
      "grad_norm": 0.08752887696027756,
      "learning_rate": 7.2687500000000005e-06,
      "loss": 0.0022,
      "step": 205110
    },
    {
      "epoch": 6.8373333333333335,
      "grad_norm": 0.1998092383146286,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0024,
      "step": 205120
    },
    {
      "epoch": 6.837666666666666,
      "grad_norm": 0.28628185391426086,
      "learning_rate": 7.264583333333334e-06,
      "loss": 0.0022,
      "step": 205130
    },
    {
      "epoch": 6.838,
      "grad_norm": 0.05983886495232582,
      "learning_rate": 7.2624999999999995e-06,
      "loss": 0.0015,
      "step": 205140
    },
    {
      "epoch": 6.838333333333333,
      "grad_norm": 0.1432567834854126,
      "learning_rate": 7.260416666666667e-06,
      "loss": 0.0022,
      "step": 205150
    },
    {
      "epoch": 6.838666666666667,
      "grad_norm": 0.17217083275318146,
      "learning_rate": 7.258333333333334e-06,
      "loss": 0.0021,
      "step": 205160
    },
    {
      "epoch": 6.839,
      "grad_norm": 0.2708002030849457,
      "learning_rate": 7.25625e-06,
      "loss": 0.0015,
      "step": 205170
    },
    {
      "epoch": 6.839333333333333,
      "grad_norm": 0.42838525772094727,
      "learning_rate": 7.254166666666667e-06,
      "loss": 0.0013,
      "step": 205180
    },
    {
      "epoch": 6.839666666666667,
      "grad_norm": 0.11410816013813019,
      "learning_rate": 7.2520833333333346e-06,
      "loss": 0.0017,
      "step": 205190
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.2000310868024826,
      "learning_rate": 7.25e-06,
      "loss": 0.0015,
      "step": 205200
    },
    {
      "epoch": 6.840333333333334,
      "grad_norm": 0.031002184376120567,
      "learning_rate": 7.247916666666666e-06,
      "loss": 0.0017,
      "step": 205210
    },
    {
      "epoch": 6.8406666666666665,
      "grad_norm": 0.2228914350271225,
      "learning_rate": 7.2458333333333336e-06,
      "loss": 0.0014,
      "step": 205220
    },
    {
      "epoch": 6.841,
      "grad_norm": 0.19947078824043274,
      "learning_rate": 7.243750000000001e-06,
      "loss": 0.0015,
      "step": 205230
    },
    {
      "epoch": 6.841333333333333,
      "grad_norm": 0.058125898241996765,
      "learning_rate": 7.241666666666667e-06,
      "loss": 0.0015,
      "step": 205240
    },
    {
      "epoch": 6.841666666666667,
      "grad_norm": 0.08555947989225388,
      "learning_rate": 7.239583333333334e-06,
      "loss": 0.0017,
      "step": 205250
    },
    {
      "epoch": 6.842,
      "grad_norm": 0.2791386544704437,
      "learning_rate": 7.2375e-06,
      "loss": 0.0016,
      "step": 205260
    },
    {
      "epoch": 6.842333333333333,
      "grad_norm": 0.17114487290382385,
      "learning_rate": 7.235416666666667e-06,
      "loss": 0.0027,
      "step": 205270
    },
    {
      "epoch": 6.842666666666666,
      "grad_norm": 0.2289806604385376,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0015,
      "step": 205280
    },
    {
      "epoch": 6.843,
      "grad_norm": 0.008681113831698895,
      "learning_rate": 7.2312500000000004e-06,
      "loss": 0.0016,
      "step": 205290
    },
    {
      "epoch": 6.843333333333334,
      "grad_norm": 0.05194980278611183,
      "learning_rate": 7.229166666666668e-06,
      "loss": 0.0014,
      "step": 205300
    },
    {
      "epoch": 6.843666666666667,
      "grad_norm": 0.22962835431098938,
      "learning_rate": 7.227083333333334e-06,
      "loss": 0.002,
      "step": 205310
    },
    {
      "epoch": 6.844,
      "grad_norm": 0.14301180839538574,
      "learning_rate": 7.2249999999999994e-06,
      "loss": 0.002,
      "step": 205320
    },
    {
      "epoch": 6.844333333333333,
      "grad_norm": 0.14355602860450745,
      "learning_rate": 7.222916666666667e-06,
      "loss": 0.0017,
      "step": 205330
    },
    {
      "epoch": 6.844666666666667,
      "grad_norm": 0.22822365164756775,
      "learning_rate": 7.220833333333334e-06,
      "loss": 0.0016,
      "step": 205340
    },
    {
      "epoch": 6.845,
      "grad_norm": 0.316151887178421,
      "learning_rate": 7.21875e-06,
      "loss": 0.002,
      "step": 205350
    },
    {
      "epoch": 6.8453333333333335,
      "grad_norm": 0.17189686000347137,
      "learning_rate": 7.216666666666667e-06,
      "loss": 0.0019,
      "step": 205360
    },
    {
      "epoch": 6.845666666666666,
      "grad_norm": 0.256522536277771,
      "learning_rate": 7.2145833333333345e-06,
      "loss": 0.0012,
      "step": 205370
    },
    {
      "epoch": 6.846,
      "grad_norm": 0.08630069345235825,
      "learning_rate": 7.2125e-06,
      "loss": 0.0012,
      "step": 205380
    },
    {
      "epoch": 6.846333333333334,
      "grad_norm": 0.25819912552833557,
      "learning_rate": 7.210416666666666e-06,
      "loss": 0.0016,
      "step": 205390
    },
    {
      "epoch": 6.846666666666667,
      "grad_norm": 0.42791685461997986,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 0.0019,
      "step": 205400
    },
    {
      "epoch": 6.8469999999999995,
      "grad_norm": 0.08675815165042877,
      "learning_rate": 7.206250000000001e-06,
      "loss": 0.0021,
      "step": 205410
    },
    {
      "epoch": 6.847333333333333,
      "grad_norm": 0.4574480652809143,
      "learning_rate": 7.204166666666667e-06,
      "loss": 0.0014,
      "step": 205420
    },
    {
      "epoch": 6.847666666666667,
      "grad_norm": 0.03149187192320824,
      "learning_rate": 7.202083333333334e-06,
      "loss": 0.002,
      "step": 205430
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.2363201230764389,
      "learning_rate": 7.2e-06,
      "loss": 0.0017,
      "step": 205440
    },
    {
      "epoch": 6.848333333333334,
      "grad_norm": 0.010085009038448334,
      "learning_rate": 7.197916666666667e-06,
      "loss": 0.0019,
      "step": 205450
    },
    {
      "epoch": 6.8486666666666665,
      "grad_norm": 0.17153993248939514,
      "learning_rate": 7.195833333333333e-06,
      "loss": 0.0019,
      "step": 205460
    },
    {
      "epoch": 6.849,
      "grad_norm": 0.030490003526210785,
      "learning_rate": 7.19375e-06,
      "loss": 0.0019,
      "step": 205470
    },
    {
      "epoch": 6.849333333333333,
      "grad_norm": 0.03199732303619385,
      "learning_rate": 7.1916666666666676e-06,
      "loss": 0.0013,
      "step": 205480
    },
    {
      "epoch": 6.849666666666667,
      "grad_norm": 0.22932735085487366,
      "learning_rate": 7.189583333333334e-06,
      "loss": 0.0017,
      "step": 205490
    },
    {
      "epoch": 6.85,
      "grad_norm": 0.399679958820343,
      "learning_rate": 7.187499999999999e-06,
      "loss": 0.0024,
      "step": 205500
    },
    {
      "epoch": 6.850333333333333,
      "grad_norm": 0.316466748714447,
      "learning_rate": 7.1854166666666666e-06,
      "loss": 0.002,
      "step": 205510
    },
    {
      "epoch": 6.850666666666667,
      "grad_norm": 0.34247979521751404,
      "learning_rate": 7.183333333333334e-06,
      "loss": 0.0017,
      "step": 205520
    },
    {
      "epoch": 6.851,
      "grad_norm": 0.1997118592262268,
      "learning_rate": 7.18125e-06,
      "loss": 0.0018,
      "step": 205530
    },
    {
      "epoch": 6.851333333333334,
      "grad_norm": 0.17179913818836212,
      "learning_rate": 7.179166666666667e-06,
      "loss": 0.0013,
      "step": 205540
    },
    {
      "epoch": 6.851666666666667,
      "grad_norm": 0.058088455349206924,
      "learning_rate": 7.1770833333333344e-06,
      "loss": 0.0017,
      "step": 205550
    },
    {
      "epoch": 6.852,
      "grad_norm": 0.2854848802089691,
      "learning_rate": 7.175e-06,
      "loss": 0.0011,
      "step": 205560
    },
    {
      "epoch": 6.852333333333333,
      "grad_norm": 0.3718999922275543,
      "learning_rate": 7.172916666666667e-06,
      "loss": 0.0016,
      "step": 205570
    },
    {
      "epoch": 6.852666666666667,
      "grad_norm": 0.017557715997099876,
      "learning_rate": 7.1708333333333334e-06,
      "loss": 0.0021,
      "step": 205580
    },
    {
      "epoch": 6.853,
      "grad_norm": 0.030570415779948235,
      "learning_rate": 7.168750000000001e-06,
      "loss": 0.0018,
      "step": 205590
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 0.2862066626548767,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0019,
      "step": 205600
    },
    {
      "epoch": 6.853666666666666,
      "grad_norm": 0.02909763529896736,
      "learning_rate": 7.164583333333334e-06,
      "loss": 0.0018,
      "step": 205610
    },
    {
      "epoch": 6.854,
      "grad_norm": 0.0337558314204216,
      "learning_rate": 7.1625e-06,
      "loss": 0.0019,
      "step": 205620
    },
    {
      "epoch": 6.854333333333333,
      "grad_norm": 0.4274037480354309,
      "learning_rate": 7.160416666666667e-06,
      "loss": 0.0018,
      "step": 205630
    },
    {
      "epoch": 6.854666666666667,
      "grad_norm": 0.25798410177230835,
      "learning_rate": 7.158333333333334e-06,
      "loss": 0.0017,
      "step": 205640
    },
    {
      "epoch": 6.855,
      "grad_norm": 0.17118287086486816,
      "learning_rate": 7.15625e-06,
      "loss": 0.0017,
      "step": 205650
    },
    {
      "epoch": 6.855333333333333,
      "grad_norm": 0.2416711151599884,
      "learning_rate": 7.1541666666666675e-06,
      "loss": 0.0019,
      "step": 205660
    },
    {
      "epoch": 6.855666666666667,
      "grad_norm": 0.2814222276210785,
      "learning_rate": 7.152083333333335e-06,
      "loss": 0.0017,
      "step": 205670
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.19339995086193085,
      "learning_rate": 7.15e-06,
      "loss": 0.0017,
      "step": 205680
    },
    {
      "epoch": 6.856333333333334,
      "grad_norm": 0.12202399969100952,
      "learning_rate": 7.1479166666666665e-06,
      "loss": 0.0016,
      "step": 205690
    },
    {
      "epoch": 6.8566666666666665,
      "grad_norm": 0.029084045439958572,
      "learning_rate": 7.145833333333334e-06,
      "loss": 0.0018,
      "step": 205700
    },
    {
      "epoch": 6.857,
      "grad_norm": 0.11526612937450409,
      "learning_rate": 7.143750000000001e-06,
      "loss": 0.002,
      "step": 205710
    },
    {
      "epoch": 6.857333333333333,
      "grad_norm": 0.058332789689302444,
      "learning_rate": 7.141666666666667e-06,
      "loss": 0.0015,
      "step": 205720
    },
    {
      "epoch": 6.857666666666667,
      "grad_norm": 0.14326027035713196,
      "learning_rate": 7.139583333333334e-06,
      "loss": 0.002,
      "step": 205730
    },
    {
      "epoch": 6.858,
      "grad_norm": 0.06009027734398842,
      "learning_rate": 7.1375e-06,
      "loss": 0.0025,
      "step": 205740
    },
    {
      "epoch": 6.858333333333333,
      "grad_norm": 0.17264699935913086,
      "learning_rate": 7.135416666666667e-06,
      "loss": 0.0019,
      "step": 205750
    },
    {
      "epoch": 6.858666666666666,
      "grad_norm": 0.2285442352294922,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0025,
      "step": 205760
    },
    {
      "epoch": 6.859,
      "grad_norm": 0.22806601226329803,
      "learning_rate": 7.1312500000000005e-06,
      "loss": 0.0019,
      "step": 205770
    },
    {
      "epoch": 6.859333333333334,
      "grad_norm": 0.08679522573947906,
      "learning_rate": 7.129166666666668e-06,
      "loss": 0.0015,
      "step": 205780
    },
    {
      "epoch": 6.859666666666667,
      "grad_norm": 0.14287278056144714,
      "learning_rate": 7.127083333333334e-06,
      "loss": 0.0021,
      "step": 205790
    },
    {
      "epoch": 6.86,
      "grad_norm": 0.031226415187120438,
      "learning_rate": 7.1249999999999995e-06,
      "loss": 0.0013,
      "step": 205800
    },
    {
      "epoch": 6.860333333333333,
      "grad_norm": 0.22818636894226074,
      "learning_rate": 7.122916666666667e-06,
      "loss": 0.0019,
      "step": 205810
    },
    {
      "epoch": 6.860666666666667,
      "grad_norm": 0.17634278535842896,
      "learning_rate": 7.120833333333334e-06,
      "loss": 0.0017,
      "step": 205820
    },
    {
      "epoch": 6.861,
      "grad_norm": 0.14385753870010376,
      "learning_rate": 7.11875e-06,
      "loss": 0.0026,
      "step": 205830
    },
    {
      "epoch": 6.8613333333333335,
      "grad_norm": 0.01474184263497591,
      "learning_rate": 7.116666666666667e-06,
      "loss": 0.0016,
      "step": 205840
    },
    {
      "epoch": 6.861666666666666,
      "grad_norm": 0.11399310827255249,
      "learning_rate": 7.114583333333335e-06,
      "loss": 0.0017,
      "step": 205850
    },
    {
      "epoch": 6.862,
      "grad_norm": 0.3189113140106201,
      "learning_rate": 7.1125e-06,
      "loss": 0.0019,
      "step": 205860
    },
    {
      "epoch": 6.862333333333333,
      "grad_norm": 0.087032251060009,
      "learning_rate": 7.110416666666666e-06,
      "loss": 0.0021,
      "step": 205870
    },
    {
      "epoch": 6.862666666666667,
      "grad_norm": 0.3742181360721588,
      "learning_rate": 7.108333333333334e-06,
      "loss": 0.0016,
      "step": 205880
    },
    {
      "epoch": 6.8629999999999995,
      "grad_norm": 0.1145550012588501,
      "learning_rate": 7.106250000000001e-06,
      "loss": 0.0015,
      "step": 205890
    },
    {
      "epoch": 6.863333333333333,
      "grad_norm": 0.22848601639270782,
      "learning_rate": 7.104166666666667e-06,
      "loss": 0.002,
      "step": 205900
    },
    {
      "epoch": 6.863666666666667,
      "grad_norm": 0.035895876586437225,
      "learning_rate": 7.102083333333334e-06,
      "loss": 0.0018,
      "step": 205910
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.20116914808750153,
      "learning_rate": 7.1e-06,
      "loss": 0.0015,
      "step": 205920
    },
    {
      "epoch": 6.864333333333334,
      "grad_norm": 0.031694184988737106,
      "learning_rate": 7.097916666666667e-06,
      "loss": 0.0013,
      "step": 205930
    },
    {
      "epoch": 6.8646666666666665,
      "grad_norm": 0.114716075360775,
      "learning_rate": 7.095833333333333e-06,
      "loss": 0.0019,
      "step": 205940
    },
    {
      "epoch": 6.865,
      "grad_norm": 0.1428263634443283,
      "learning_rate": 7.0937500000000005e-06,
      "loss": 0.0016,
      "step": 205950
    },
    {
      "epoch": 6.865333333333333,
      "grad_norm": 0.08590081334114075,
      "learning_rate": 7.091666666666668e-06,
      "loss": 0.0011,
      "step": 205960
    },
    {
      "epoch": 6.865666666666667,
      "grad_norm": 0.1753913313150406,
      "learning_rate": 7.089583333333334e-06,
      "loss": 0.0016,
      "step": 205970
    },
    {
      "epoch": 6.866,
      "grad_norm": 0.030991539359092712,
      "learning_rate": 7.0874999999999995e-06,
      "loss": 0.0014,
      "step": 205980
    },
    {
      "epoch": 6.866333333333333,
      "grad_norm": 0.1188536286354065,
      "learning_rate": 7.085416666666667e-06,
      "loss": 0.001,
      "step": 205990
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.015149136073887348,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.0012,
      "step": 206000
    },
    {
      "epoch": 6.867,
      "grad_norm": 0.2914297580718994,
      "learning_rate": 7.08125e-06,
      "loss": 0.0022,
      "step": 206010
    },
    {
      "epoch": 6.867333333333333,
      "grad_norm": 0.11448847502470016,
      "learning_rate": 7.079166666666667e-06,
      "loss": 0.002,
      "step": 206020
    },
    {
      "epoch": 6.867666666666667,
      "grad_norm": 0.05832953378558159,
      "learning_rate": 7.0770833333333345e-06,
      "loss": 0.0017,
      "step": 206030
    },
    {
      "epoch": 6.868,
      "grad_norm": 0.22817933559417725,
      "learning_rate": 7.075e-06,
      "loss": 0.0017,
      "step": 206040
    },
    {
      "epoch": 6.868333333333333,
      "grad_norm": 0.11525537818670273,
      "learning_rate": 7.072916666666666e-06,
      "loss": 0.0019,
      "step": 206050
    },
    {
      "epoch": 6.868666666666667,
      "grad_norm": 0.030084943398833275,
      "learning_rate": 7.0708333333333335e-06,
      "loss": 0.0014,
      "step": 206060
    },
    {
      "epoch": 6.869,
      "grad_norm": 0.34354642033576965,
      "learning_rate": 7.068750000000001e-06,
      "loss": 0.0013,
      "step": 206070
    },
    {
      "epoch": 6.8693333333333335,
      "grad_norm": 0.17131634056568146,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.002,
      "step": 206080
    },
    {
      "epoch": 6.869666666666666,
      "grad_norm": 0.2571891248226166,
      "learning_rate": 7.064583333333334e-06,
      "loss": 0.0016,
      "step": 206090
    },
    {
      "epoch": 6.87,
      "grad_norm": 0.08755692094564438,
      "learning_rate": 7.0625e-06,
      "loss": 0.0012,
      "step": 206100
    },
    {
      "epoch": 6.870333333333333,
      "grad_norm": 0.1161135584115982,
      "learning_rate": 7.060416666666667e-06,
      "loss": 0.002,
      "step": 206110
    },
    {
      "epoch": 6.870666666666667,
      "grad_norm": 0.08770070970058441,
      "learning_rate": 7.058333333333333e-06,
      "loss": 0.002,
      "step": 206120
    },
    {
      "epoch": 6.871,
      "grad_norm": 0.08650929480791092,
      "learning_rate": 7.05625e-06,
      "loss": 0.0014,
      "step": 206130
    },
    {
      "epoch": 6.871333333333333,
      "grad_norm": 0.23468559980392456,
      "learning_rate": 7.054166666666668e-06,
      "loss": 0.0016,
      "step": 206140
    },
    {
      "epoch": 6.871666666666667,
      "grad_norm": 0.05915441736578941,
      "learning_rate": 7.052083333333334e-06,
      "loss": 0.0023,
      "step": 206150
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.17130255699157715,
      "learning_rate": 7.049999999999999e-06,
      "loss": 0.0012,
      "step": 206160
    },
    {
      "epoch": 6.872333333333334,
      "grad_norm": 0.14314371347427368,
      "learning_rate": 7.047916666666667e-06,
      "loss": 0.002,
      "step": 206170
    },
    {
      "epoch": 6.8726666666666665,
      "grad_norm": 0.17271307110786438,
      "learning_rate": 7.045833333333334e-06,
      "loss": 0.0015,
      "step": 206180
    },
    {
      "epoch": 6.873,
      "grad_norm": 0.0311311986297369,
      "learning_rate": 7.04375e-06,
      "loss": 0.0014,
      "step": 206190
    },
    {
      "epoch": 6.873333333333333,
      "grad_norm": 0.14272867143154144,
      "learning_rate": 7.041666666666667e-06,
      "loss": 0.0022,
      "step": 206200
    },
    {
      "epoch": 6.873666666666667,
      "grad_norm": 0.3706095218658447,
      "learning_rate": 7.0395833333333345e-06,
      "loss": 0.002,
      "step": 206210
    },
    {
      "epoch": 6.874,
      "grad_norm": 0.1997617781162262,
      "learning_rate": 7.0375e-06,
      "loss": 0.0017,
      "step": 206220
    },
    {
      "epoch": 6.874333333333333,
      "grad_norm": 0.17120762169361115,
      "learning_rate": 7.035416666666666e-06,
      "loss": 0.0013,
      "step": 206230
    },
    {
      "epoch": 6.874666666666666,
      "grad_norm": 0.20017921924591064,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.0012,
      "step": 206240
    },
    {
      "epoch": 6.875,
      "grad_norm": 0.08627144247293472,
      "learning_rate": 7.031250000000001e-06,
      "loss": 0.0013,
      "step": 206250
    },
    {
      "epoch": 6.875333333333334,
      "grad_norm": 0.05801541358232498,
      "learning_rate": 7.029166666666667e-06,
      "loss": 0.0014,
      "step": 206260
    },
    {
      "epoch": 6.875666666666667,
      "grad_norm": 0.05817478895187378,
      "learning_rate": 7.027083333333334e-06,
      "loss": 0.0015,
      "step": 206270
    },
    {
      "epoch": 6.876,
      "grad_norm": 0.2872554659843445,
      "learning_rate": 7.025000000000001e-06,
      "loss": 0.0014,
      "step": 206280
    },
    {
      "epoch": 6.876333333333333,
      "grad_norm": 0.011138644069433212,
      "learning_rate": 7.022916666666667e-06,
      "loss": 0.0014,
      "step": 206290
    },
    {
      "epoch": 6.876666666666667,
      "grad_norm": 0.17142356932163239,
      "learning_rate": 7.020833333333333e-06,
      "loss": 0.002,
      "step": 206300
    },
    {
      "epoch": 6.877,
      "grad_norm": 0.014728954061865807,
      "learning_rate": 7.01875e-06,
      "loss": 0.0028,
      "step": 206310
    },
    {
      "epoch": 6.8773333333333335,
      "grad_norm": 0.02890358492732048,
      "learning_rate": 7.0166666666666675e-06,
      "loss": 0.0013,
      "step": 206320
    },
    {
      "epoch": 6.877666666666666,
      "grad_norm": 0.2851707935333252,
      "learning_rate": 7.014583333333334e-06,
      "loss": 0.0027,
      "step": 206330
    },
    {
      "epoch": 6.878,
      "grad_norm": 0.2422540783882141,
      "learning_rate": 7.012500000000001e-06,
      "loss": 0.0018,
      "step": 206340
    },
    {
      "epoch": 6.878333333333333,
      "grad_norm": 0.11763972789049149,
      "learning_rate": 7.0104166666666665e-06,
      "loss": 0.0015,
      "step": 206350
    },
    {
      "epoch": 6.878666666666667,
      "grad_norm": 0.1142190471291542,
      "learning_rate": 7.008333333333334e-06,
      "loss": 0.0015,
      "step": 206360
    },
    {
      "epoch": 6.879,
      "grad_norm": 0.029439108446240425,
      "learning_rate": 7.00625e-06,
      "loss": 0.0022,
      "step": 206370
    },
    {
      "epoch": 6.879333333333333,
      "grad_norm": 0.009935665875673294,
      "learning_rate": 7.004166666666667e-06,
      "loss": 0.0016,
      "step": 206380
    },
    {
      "epoch": 6.879666666666667,
      "grad_norm": 0.342427134513855,
      "learning_rate": 7.002083333333334e-06,
      "loss": 0.0021,
      "step": 206390
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.323511004447937,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0016,
      "step": 206400
    },
    {
      "epoch": 6.880333333333334,
      "grad_norm": 0.05857541784644127,
      "learning_rate": 6.997916666666666e-06,
      "loss": 0.0021,
      "step": 206410
    },
    {
      "epoch": 6.8806666666666665,
      "grad_norm": 0.03246643766760826,
      "learning_rate": 6.995833333333333e-06,
      "loss": 0.0015,
      "step": 206420
    },
    {
      "epoch": 6.881,
      "grad_norm": 0.01811305060982704,
      "learning_rate": 6.993750000000001e-06,
      "loss": 0.0018,
      "step": 206430
    },
    {
      "epoch": 6.881333333333333,
      "grad_norm": 0.22863154113292694,
      "learning_rate": 6.991666666666667e-06,
      "loss": 0.0015,
      "step": 206440
    },
    {
      "epoch": 6.881666666666667,
      "grad_norm": 0.039921555668115616,
      "learning_rate": 6.989583333333334e-06,
      "loss": 0.0017,
      "step": 206450
    },
    {
      "epoch": 6.882,
      "grad_norm": 0.2568321228027344,
      "learning_rate": 6.987500000000001e-06,
      "loss": 0.0016,
      "step": 206460
    },
    {
      "epoch": 6.882333333333333,
      "grad_norm": 0.25754326581954956,
      "learning_rate": 6.985416666666667e-06,
      "loss": 0.0019,
      "step": 206470
    },
    {
      "epoch": 6.882666666666667,
      "grad_norm": 0.03238068148493767,
      "learning_rate": 6.983333333333333e-06,
      "loss": 0.0018,
      "step": 206480
    },
    {
      "epoch": 6.883,
      "grad_norm": 0.20048366487026215,
      "learning_rate": 6.98125e-06,
      "loss": 0.0025,
      "step": 206490
    },
    {
      "epoch": 6.883333333333333,
      "grad_norm": 0.343425452709198,
      "learning_rate": 6.9791666666666675e-06,
      "loss": 0.0017,
      "step": 206500
    },
    {
      "epoch": 6.883666666666667,
      "grad_norm": 0.17159663140773773,
      "learning_rate": 6.977083333333334e-06,
      "loss": 0.0014,
      "step": 206510
    },
    {
      "epoch": 6.884,
      "grad_norm": 0.610458254814148,
      "learning_rate": 6.975000000000001e-06,
      "loss": 0.0014,
      "step": 206520
    },
    {
      "epoch": 6.884333333333333,
      "grad_norm": 0.02982316166162491,
      "learning_rate": 6.9729166666666665e-06,
      "loss": 0.0014,
      "step": 206530
    },
    {
      "epoch": 6.884666666666667,
      "grad_norm": 0.1996670365333557,
      "learning_rate": 6.970833333333334e-06,
      "loss": 0.0013,
      "step": 206540
    },
    {
      "epoch": 6.885,
      "grad_norm": 0.14327970147132874,
      "learning_rate": 6.96875e-06,
      "loss": 0.0017,
      "step": 206550
    },
    {
      "epoch": 6.8853333333333335,
      "grad_norm": 0.31363609433174133,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0016,
      "step": 206560
    },
    {
      "epoch": 6.885666666666666,
      "grad_norm": 0.2850499749183655,
      "learning_rate": 6.964583333333334e-06,
      "loss": 0.0017,
      "step": 206570
    },
    {
      "epoch": 6.886,
      "grad_norm": 0.031521979719400406,
      "learning_rate": 6.962500000000001e-06,
      "loss": 0.0024,
      "step": 206580
    },
    {
      "epoch": 6.886333333333333,
      "grad_norm": 0.25694310665130615,
      "learning_rate": 6.960416666666666e-06,
      "loss": 0.0019,
      "step": 206590
    },
    {
      "epoch": 6.886666666666667,
      "grad_norm": 0.3423680067062378,
      "learning_rate": 6.958333333333333e-06,
      "loss": 0.0034,
      "step": 206600
    },
    {
      "epoch": 6.8870000000000005,
      "grad_norm": 0.08902550488710403,
      "learning_rate": 6.9562500000000005e-06,
      "loss": 0.0017,
      "step": 206610
    },
    {
      "epoch": 6.887333333333333,
      "grad_norm": 0.14247730374336243,
      "learning_rate": 6.954166666666667e-06,
      "loss": 0.0015,
      "step": 206620
    },
    {
      "epoch": 6.887666666666667,
      "grad_norm": 0.08575835078954697,
      "learning_rate": 6.952083333333334e-06,
      "loss": 0.0016,
      "step": 206630
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.08663522452116013,
      "learning_rate": 6.950000000000001e-06,
      "loss": 0.0022,
      "step": 206640
    },
    {
      "epoch": 6.888333333333334,
      "grad_norm": 0.08563033491373062,
      "learning_rate": 6.947916666666667e-06,
      "loss": 0.0012,
      "step": 206650
    },
    {
      "epoch": 6.8886666666666665,
      "grad_norm": 0.19945617020130157,
      "learning_rate": 6.945833333333333e-06,
      "loss": 0.0021,
      "step": 206660
    },
    {
      "epoch": 6.889,
      "grad_norm": 0.08966812491416931,
      "learning_rate": 6.94375e-06,
      "loss": 0.0014,
      "step": 206670
    },
    {
      "epoch": 6.889333333333333,
      "grad_norm": 0.2622579038143158,
      "learning_rate": 6.941666666666667e-06,
      "loss": 0.0019,
      "step": 206680
    },
    {
      "epoch": 6.889666666666667,
      "grad_norm": 0.14388947188854218,
      "learning_rate": 6.939583333333334e-06,
      "loss": 0.0016,
      "step": 206690
    },
    {
      "epoch": 6.89,
      "grad_norm": 0.059544987976551056,
      "learning_rate": 6.937500000000001e-06,
      "loss": 0.0017,
      "step": 206700
    },
    {
      "epoch": 6.890333333333333,
      "grad_norm": 0.03247958421707153,
      "learning_rate": 6.935416666666666e-06,
      "loss": 0.002,
      "step": 206710
    },
    {
      "epoch": 6.890666666666666,
      "grad_norm": 0.08664190769195557,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0013,
      "step": 206720
    },
    {
      "epoch": 6.891,
      "grad_norm": 0.11658430844545364,
      "learning_rate": 6.93125e-06,
      "loss": 0.0017,
      "step": 206730
    },
    {
      "epoch": 6.891333333333334,
      "grad_norm": 0.22814370691776276,
      "learning_rate": 6.929166666666667e-06,
      "loss": 0.0015,
      "step": 206740
    },
    {
      "epoch": 6.891666666666667,
      "grad_norm": 0.342255562543869,
      "learning_rate": 6.927083333333334e-06,
      "loss": 0.0014,
      "step": 206750
    },
    {
      "epoch": 6.892,
      "grad_norm": 0.3995899558067322,
      "learning_rate": 6.925000000000001e-06,
      "loss": 0.0026,
      "step": 206760
    },
    {
      "epoch": 6.892333333333333,
      "grad_norm": 0.059280604124069214,
      "learning_rate": 6.922916666666666e-06,
      "loss": 0.0014,
      "step": 206770
    },
    {
      "epoch": 6.892666666666667,
      "grad_norm": 0.3997041881084442,
      "learning_rate": 6.920833333333333e-06,
      "loss": 0.0015,
      "step": 206780
    },
    {
      "epoch": 6.893,
      "grad_norm": 0.00909004919230938,
      "learning_rate": 6.9187500000000005e-06,
      "loss": 0.0014,
      "step": 206790
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 0.11470861732959747,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.0019,
      "step": 206800
    },
    {
      "epoch": 6.893666666666666,
      "grad_norm": 0.25690314173698425,
      "learning_rate": 6.914583333333334e-06,
      "loss": 0.0018,
      "step": 206810
    },
    {
      "epoch": 6.894,
      "grad_norm": 0.08612736314535141,
      "learning_rate": 6.912500000000001e-06,
      "loss": 0.0014,
      "step": 206820
    },
    {
      "epoch": 6.894333333333333,
      "grad_norm": 0.03006354719400406,
      "learning_rate": 6.910416666666667e-06,
      "loss": 0.0013,
      "step": 206830
    },
    {
      "epoch": 6.894666666666667,
      "grad_norm": 0.5932283401489258,
      "learning_rate": 6.908333333333333e-06,
      "loss": 0.0014,
      "step": 206840
    },
    {
      "epoch": 6.895,
      "grad_norm": 0.34387606382369995,
      "learning_rate": 6.90625e-06,
      "loss": 0.0016,
      "step": 206850
    },
    {
      "epoch": 6.895333333333333,
      "grad_norm": 0.08687124401330948,
      "learning_rate": 6.904166666666667e-06,
      "loss": 0.0011,
      "step": 206860
    },
    {
      "epoch": 6.895666666666667,
      "grad_norm": 0.34973105788230896,
      "learning_rate": 6.902083333333334e-06,
      "loss": 0.0014,
      "step": 206870
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.37089353799819946,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0015,
      "step": 206880
    },
    {
      "epoch": 6.896333333333334,
      "grad_norm": 0.17219655215740204,
      "learning_rate": 6.897916666666666e-06,
      "loss": 0.0019,
      "step": 206890
    },
    {
      "epoch": 6.8966666666666665,
      "grad_norm": 0.14645642042160034,
      "learning_rate": 6.8958333333333335e-06,
      "loss": 0.0014,
      "step": 206900
    },
    {
      "epoch": 6.897,
      "grad_norm": 0.06291243433952332,
      "learning_rate": 6.89375e-06,
      "loss": 0.0012,
      "step": 206910
    },
    {
      "epoch": 6.897333333333333,
      "grad_norm": 0.25222986936569214,
      "learning_rate": 6.891666666666667e-06,
      "loss": 0.0012,
      "step": 206920
    },
    {
      "epoch": 6.897666666666667,
      "grad_norm": 0.15053650736808777,
      "learning_rate": 6.889583333333334e-06,
      "loss": 0.0019,
      "step": 206930
    },
    {
      "epoch": 6.898,
      "grad_norm": 0.08551542460918427,
      "learning_rate": 6.8875000000000005e-06,
      "loss": 0.0015,
      "step": 206940
    },
    {
      "epoch": 6.898333333333333,
      "grad_norm": 0.1711529642343521,
      "learning_rate": 6.885416666666666e-06,
      "loss": 0.002,
      "step": 206950
    },
    {
      "epoch": 6.898666666666666,
      "grad_norm": 0.009874338284134865,
      "learning_rate": 6.883333333333333e-06,
      "loss": 0.0015,
      "step": 206960
    },
    {
      "epoch": 6.899,
      "grad_norm": 0.011060334742069244,
      "learning_rate": 6.88125e-06,
      "loss": 0.0017,
      "step": 206970
    },
    {
      "epoch": 6.899333333333333,
      "grad_norm": 0.20044368505477905,
      "learning_rate": 6.879166666666667e-06,
      "loss": 0.0021,
      "step": 206980
    },
    {
      "epoch": 6.899666666666667,
      "grad_norm": 0.08624312281608582,
      "learning_rate": 6.877083333333334e-06,
      "loss": 0.0019,
      "step": 206990
    },
    {
      "epoch": 6.9,
      "grad_norm": 0.05921533703804016,
      "learning_rate": 6.875000000000001e-06,
      "loss": 0.0011,
      "step": 207000
    },
    {
      "epoch": 6.900333333333333,
      "grad_norm": 0.2858377993106842,
      "learning_rate": 6.872916666666667e-06,
      "loss": 0.0014,
      "step": 207010
    },
    {
      "epoch": 6.900666666666667,
      "grad_norm": 0.08707346767187119,
      "learning_rate": 6.870833333333333e-06,
      "loss": 0.0019,
      "step": 207020
    },
    {
      "epoch": 6.901,
      "grad_norm": 0.14319449663162231,
      "learning_rate": 6.86875e-06,
      "loss": 0.0019,
      "step": 207030
    },
    {
      "epoch": 6.9013333333333335,
      "grad_norm": 0.08646493405103683,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0016,
      "step": 207040
    },
    {
      "epoch": 6.901666666666666,
      "grad_norm": 0.22948384284973145,
      "learning_rate": 6.864583333333334e-06,
      "loss": 0.0018,
      "step": 207050
    },
    {
      "epoch": 6.902,
      "grad_norm": 0.02909567765891552,
      "learning_rate": 6.862500000000001e-06,
      "loss": 0.0016,
      "step": 207060
    },
    {
      "epoch": 6.902333333333333,
      "grad_norm": 0.14770881831645966,
      "learning_rate": 6.860416666666666e-06,
      "loss": 0.0025,
      "step": 207070
    },
    {
      "epoch": 6.902666666666667,
      "grad_norm": 0.22822345793247223,
      "learning_rate": 6.8583333333333335e-06,
      "loss": 0.0015,
      "step": 207080
    },
    {
      "epoch": 6.9030000000000005,
      "grad_norm": 0.2571122646331787,
      "learning_rate": 6.85625e-06,
      "loss": 0.002,
      "step": 207090
    },
    {
      "epoch": 6.903333333333333,
      "grad_norm": 0.11725106090307236,
      "learning_rate": 6.854166666666667e-06,
      "loss": 0.0024,
      "step": 207100
    },
    {
      "epoch": 6.903666666666666,
      "grad_norm": 0.11890479922294617,
      "learning_rate": 6.852083333333334e-06,
      "loss": 0.0014,
      "step": 207110
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.33289748430252075,
      "learning_rate": 6.8500000000000005e-06,
      "loss": 0.0018,
      "step": 207120
    },
    {
      "epoch": 6.904333333333334,
      "grad_norm": 0.11383068561553955,
      "learning_rate": 6.847916666666666e-06,
      "loss": 0.0024,
      "step": 207130
    },
    {
      "epoch": 6.9046666666666665,
      "grad_norm": 0.2939515709877014,
      "learning_rate": 6.845833333333333e-06,
      "loss": 0.0016,
      "step": 207140
    },
    {
      "epoch": 6.905,
      "grad_norm": 0.2287660390138626,
      "learning_rate": 6.84375e-06,
      "loss": 0.0016,
      "step": 207150
    },
    {
      "epoch": 6.905333333333333,
      "grad_norm": 0.08717972040176392,
      "learning_rate": 6.841666666666667e-06,
      "loss": 0.0014,
      "step": 207160
    },
    {
      "epoch": 6.905666666666667,
      "grad_norm": 0.17120428383350372,
      "learning_rate": 6.839583333333334e-06,
      "loss": 0.0017,
      "step": 207170
    },
    {
      "epoch": 6.906,
      "grad_norm": 0.1427411437034607,
      "learning_rate": 6.837500000000001e-06,
      "loss": 0.0013,
      "step": 207180
    },
    {
      "epoch": 6.906333333333333,
      "grad_norm": 0.37062111496925354,
      "learning_rate": 6.8354166666666665e-06,
      "loss": 0.0012,
      "step": 207190
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 0.031116092577576637,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0016,
      "step": 207200
    },
    {
      "epoch": 6.907,
      "grad_norm": 0.13896295428276062,
      "learning_rate": 6.83125e-06,
      "loss": 0.0018,
      "step": 207210
    },
    {
      "epoch": 6.907333333333334,
      "grad_norm": 0.1442776620388031,
      "learning_rate": 6.829166666666667e-06,
      "loss": 0.0019,
      "step": 207220
    },
    {
      "epoch": 6.907666666666667,
      "grad_norm": 0.05857846885919571,
      "learning_rate": 6.8270833333333335e-06,
      "loss": 0.0013,
      "step": 207230
    },
    {
      "epoch": 6.908,
      "grad_norm": 0.25915586948394775,
      "learning_rate": 6.825000000000001e-06,
      "loss": 0.0016,
      "step": 207240
    },
    {
      "epoch": 6.908333333333333,
      "grad_norm": 0.06006468087434769,
      "learning_rate": 6.822916666666666e-06,
      "loss": 0.0014,
      "step": 207250
    },
    {
      "epoch": 6.908666666666667,
      "grad_norm": 0.030118495225906372,
      "learning_rate": 6.820833333333333e-06,
      "loss": 0.0023,
      "step": 207260
    },
    {
      "epoch": 6.909,
      "grad_norm": 0.25663667917251587,
      "learning_rate": 6.81875e-06,
      "loss": 0.0015,
      "step": 207270
    },
    {
      "epoch": 6.9093333333333335,
      "grad_norm": 0.012958182953298092,
      "learning_rate": 6.816666666666667e-06,
      "loss": 0.0017,
      "step": 207280
    },
    {
      "epoch": 6.909666666666666,
      "grad_norm": 0.1715027391910553,
      "learning_rate": 6.814583333333334e-06,
      "loss": 0.0023,
      "step": 207290
    },
    {
      "epoch": 6.91,
      "grad_norm": 0.21921272575855255,
      "learning_rate": 6.8125e-06,
      "loss": 0.0015,
      "step": 207300
    },
    {
      "epoch": 6.910333333333333,
      "grad_norm": 0.0917038768529892,
      "learning_rate": 6.810416666666667e-06,
      "loss": 0.0015,
      "step": 207310
    },
    {
      "epoch": 6.910666666666667,
      "grad_norm": 0.21537895500659943,
      "learning_rate": 6.808333333333333e-06,
      "loss": 0.0018,
      "step": 207320
    },
    {
      "epoch": 6.911,
      "grad_norm": 0.08736052364110947,
      "learning_rate": 6.80625e-06,
      "loss": 0.0016,
      "step": 207330
    },
    {
      "epoch": 6.911333333333333,
      "grad_norm": 0.08701920509338379,
      "learning_rate": 6.804166666666667e-06,
      "loss": 0.0016,
      "step": 207340
    },
    {
      "epoch": 6.911666666666667,
      "grad_norm": 0.28573769330978394,
      "learning_rate": 6.802083333333334e-06,
      "loss": 0.0016,
      "step": 207350
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.06914526224136353,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0017,
      "step": 207360
    },
    {
      "epoch": 6.912333333333334,
      "grad_norm": 0.41995489597320557,
      "learning_rate": 6.7979166666666664e-06,
      "loss": 0.0015,
      "step": 207370
    },
    {
      "epoch": 6.9126666666666665,
      "grad_norm": 0.2289578914642334,
      "learning_rate": 6.795833333333334e-06,
      "loss": 0.0019,
      "step": 207380
    },
    {
      "epoch": 6.913,
      "grad_norm": 0.17210160195827484,
      "learning_rate": 6.79375e-06,
      "loss": 0.0017,
      "step": 207390
    },
    {
      "epoch": 6.913333333333333,
      "grad_norm": 0.17650915682315826,
      "learning_rate": 6.791666666666667e-06,
      "loss": 0.0017,
      "step": 207400
    },
    {
      "epoch": 6.913666666666667,
      "grad_norm": 0.3421316146850586,
      "learning_rate": 6.789583333333334e-06,
      "loss": 0.0012,
      "step": 207410
    },
    {
      "epoch": 6.914,
      "grad_norm": 0.011242923326790333,
      "learning_rate": 6.787500000000001e-06,
      "loss": 0.002,
      "step": 207420
    },
    {
      "epoch": 6.914333333333333,
      "grad_norm": 0.05799316614866257,
      "learning_rate": 6.785416666666666e-06,
      "loss": 0.0018,
      "step": 207430
    },
    {
      "epoch": 6.914666666666666,
      "grad_norm": 0.2563842236995697,
      "learning_rate": 6.783333333333333e-06,
      "loss": 0.0011,
      "step": 207440
    },
    {
      "epoch": 6.915,
      "grad_norm": 0.11156785488128662,
      "learning_rate": 6.7812500000000005e-06,
      "loss": 0.0017,
      "step": 207450
    },
    {
      "epoch": 6.915333333333333,
      "grad_norm": 0.08874969929456711,
      "learning_rate": 6.779166666666667e-06,
      "loss": 0.0016,
      "step": 207460
    },
    {
      "epoch": 6.915666666666667,
      "grad_norm": 0.1430775672197342,
      "learning_rate": 6.777083333333334e-06,
      "loss": 0.0023,
      "step": 207470
    },
    {
      "epoch": 6.916,
      "grad_norm": 0.4060269892215729,
      "learning_rate": 6.775000000000001e-06,
      "loss": 0.0018,
      "step": 207480
    },
    {
      "epoch": 6.916333333333333,
      "grad_norm": 0.05803539603948593,
      "learning_rate": 6.772916666666667e-06,
      "loss": 0.0021,
      "step": 207490
    },
    {
      "epoch": 6.916666666666667,
      "grad_norm": 0.01311353500932455,
      "learning_rate": 6.770833333333333e-06,
      "loss": 0.0015,
      "step": 207500
    },
    {
      "epoch": 6.917,
      "grad_norm": 0.11433357745409012,
      "learning_rate": 6.76875e-06,
      "loss": 0.0016,
      "step": 207510
    },
    {
      "epoch": 6.917333333333334,
      "grad_norm": 0.17128172516822815,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.002,
      "step": 207520
    },
    {
      "epoch": 6.917666666666666,
      "grad_norm": 0.010496275499463081,
      "learning_rate": 6.764583333333334e-06,
      "loss": 0.0017,
      "step": 207530
    },
    {
      "epoch": 6.918,
      "grad_norm": 0.6010318398475647,
      "learning_rate": 6.762500000000001e-06,
      "loss": 0.0015,
      "step": 207540
    },
    {
      "epoch": 6.918333333333333,
      "grad_norm": 0.5576955676078796,
      "learning_rate": 6.760416666666668e-06,
      "loss": 0.0015,
      "step": 207550
    },
    {
      "epoch": 6.918666666666667,
      "grad_norm": 0.2731853425502777,
      "learning_rate": 6.7583333333333336e-06,
      "loss": 0.0025,
      "step": 207560
    },
    {
      "epoch": 6.9190000000000005,
      "grad_norm": 0.05743691697716713,
      "learning_rate": 6.75625e-06,
      "loss": 0.0034,
      "step": 207570
    },
    {
      "epoch": 6.919333333333333,
      "grad_norm": 0.19997845590114594,
      "learning_rate": 6.754166666666667e-06,
      "loss": 0.0018,
      "step": 207580
    },
    {
      "epoch": 6.919666666666666,
      "grad_norm": 0.1431620717048645,
      "learning_rate": 6.752083333333334e-06,
      "loss": 0.0013,
      "step": 207590
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.06780225038528442,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.0015,
      "step": 207600
    },
    {
      "epoch": 6.920333333333334,
      "grad_norm": 0.028886159881949425,
      "learning_rate": 6.747916666666668e-06,
      "loss": 0.0016,
      "step": 207610
    },
    {
      "epoch": 6.9206666666666665,
      "grad_norm": 0.08693613111972809,
      "learning_rate": 6.745833333333333e-06,
      "loss": 0.0017,
      "step": 207620
    },
    {
      "epoch": 6.921,
      "grad_norm": 0.513630211353302,
      "learning_rate": 6.7437500000000004e-06,
      "loss": 0.0013,
      "step": 207630
    },
    {
      "epoch": 6.921333333333333,
      "grad_norm": 0.21483390033245087,
      "learning_rate": 6.741666666666667e-06,
      "loss": 0.0016,
      "step": 207640
    },
    {
      "epoch": 6.921666666666667,
      "grad_norm": 0.2855302393436432,
      "learning_rate": 6.739583333333334e-06,
      "loss": 0.0018,
      "step": 207650
    },
    {
      "epoch": 6.922,
      "grad_norm": 0.05714063718914986,
      "learning_rate": 6.737500000000001e-06,
      "loss": 0.0023,
      "step": 207660
    },
    {
      "epoch": 6.9223333333333334,
      "grad_norm": 0.08617425709962845,
      "learning_rate": 6.7354166666666675e-06,
      "loss": 0.0016,
      "step": 207670
    },
    {
      "epoch": 6.922666666666666,
      "grad_norm": 0.31785067915916443,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0014,
      "step": 207680
    },
    {
      "epoch": 6.923,
      "grad_norm": 0.25690919160842896,
      "learning_rate": 6.73125e-06,
      "loss": 0.0015,
      "step": 207690
    },
    {
      "epoch": 6.923333333333334,
      "grad_norm": 0.15072259306907654,
      "learning_rate": 6.729166666666667e-06,
      "loss": 0.0015,
      "step": 207700
    },
    {
      "epoch": 6.923666666666667,
      "grad_norm": 0.14294500648975372,
      "learning_rate": 6.727083333333334e-06,
      "loss": 0.0015,
      "step": 207710
    },
    {
      "epoch": 6.924,
      "grad_norm": 0.08649024367332458,
      "learning_rate": 6.725000000000001e-06,
      "loss": 0.0015,
      "step": 207720
    },
    {
      "epoch": 6.924333333333333,
      "grad_norm": 0.05943629890680313,
      "learning_rate": 6.722916666666668e-06,
      "loss": 0.0016,
      "step": 207730
    },
    {
      "epoch": 6.924666666666667,
      "grad_norm": 0.08738526701927185,
      "learning_rate": 6.7208333333333335e-06,
      "loss": 0.0016,
      "step": 207740
    },
    {
      "epoch": 6.925,
      "grad_norm": 0.11416655033826828,
      "learning_rate": 6.71875e-06,
      "loss": 0.0013,
      "step": 207750
    },
    {
      "epoch": 6.925333333333334,
      "grad_norm": 0.22895964980125427,
      "learning_rate": 6.716666666666667e-06,
      "loss": 0.0011,
      "step": 207760
    },
    {
      "epoch": 6.925666666666666,
      "grad_norm": 0.033368032425642014,
      "learning_rate": 6.714583333333334e-06,
      "loss": 0.0014,
      "step": 207770
    },
    {
      "epoch": 6.926,
      "grad_norm": 0.30767112970352173,
      "learning_rate": 6.7125000000000005e-06,
      "loss": 0.0014,
      "step": 207780
    },
    {
      "epoch": 6.926333333333333,
      "grad_norm": 0.05938094109296799,
      "learning_rate": 6.710416666666668e-06,
      "loss": 0.0012,
      "step": 207790
    },
    {
      "epoch": 6.926666666666667,
      "grad_norm": 0.25629931688308716,
      "learning_rate": 6.708333333333333e-06,
      "loss": 0.0019,
      "step": 207800
    },
    {
      "epoch": 6.927,
      "grad_norm": 0.08679325878620148,
      "learning_rate": 6.70625e-06,
      "loss": 0.0017,
      "step": 207810
    },
    {
      "epoch": 6.927333333333333,
      "grad_norm": 0.11488219350576401,
      "learning_rate": 6.704166666666667e-06,
      "loss": 0.0019,
      "step": 207820
    },
    {
      "epoch": 6.927666666666667,
      "grad_norm": 0.0594446063041687,
      "learning_rate": 6.702083333333334e-06,
      "loss": 0.0011,
      "step": 207830
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.11426587402820587,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0019,
      "step": 207840
    },
    {
      "epoch": 6.928333333333334,
      "grad_norm": 0.0581207200884819,
      "learning_rate": 6.697916666666667e-06,
      "loss": 0.0014,
      "step": 207850
    },
    {
      "epoch": 6.9286666666666665,
      "grad_norm": 0.11505728960037231,
      "learning_rate": 6.695833333333333e-06,
      "loss": 0.0014,
      "step": 207860
    },
    {
      "epoch": 6.929,
      "grad_norm": 0.14332863688468933,
      "learning_rate": 6.69375e-06,
      "loss": 0.0013,
      "step": 207870
    },
    {
      "epoch": 6.929333333333333,
      "grad_norm": 0.008463150821626186,
      "learning_rate": 6.691666666666667e-06,
      "loss": 0.0026,
      "step": 207880
    },
    {
      "epoch": 6.929666666666667,
      "grad_norm": 0.09511151164770126,
      "learning_rate": 6.689583333333334e-06,
      "loss": 0.0014,
      "step": 207890
    },
    {
      "epoch": 6.93,
      "grad_norm": 0.199879452586174,
      "learning_rate": 6.687500000000001e-06,
      "loss": 0.0018,
      "step": 207900
    },
    {
      "epoch": 6.9303333333333335,
      "grad_norm": 0.17404943704605103,
      "learning_rate": 6.685416666666668e-06,
      "loss": 0.0015,
      "step": 207910
    },
    {
      "epoch": 6.930666666666666,
      "grad_norm": 0.11955296993255615,
      "learning_rate": 6.6833333333333334e-06,
      "loss": 0.0019,
      "step": 207920
    },
    {
      "epoch": 6.931,
      "grad_norm": 0.11460425704717636,
      "learning_rate": 6.68125e-06,
      "loss": 0.0014,
      "step": 207930
    },
    {
      "epoch": 6.931333333333333,
      "grad_norm": 0.1718260794878006,
      "learning_rate": 6.679166666666667e-06,
      "loss": 0.0014,
      "step": 207940
    },
    {
      "epoch": 6.931666666666667,
      "grad_norm": 0.32727646827697754,
      "learning_rate": 6.677083333333334e-06,
      "loss": 0.0028,
      "step": 207950
    },
    {
      "epoch": 6.932,
      "grad_norm": 0.06733804941177368,
      "learning_rate": 6.6750000000000005e-06,
      "loss": 0.002,
      "step": 207960
    },
    {
      "epoch": 6.932333333333333,
      "grad_norm": 0.2031755894422531,
      "learning_rate": 6.672916666666668e-06,
      "loss": 0.0011,
      "step": 207970
    },
    {
      "epoch": 6.932666666666667,
      "grad_norm": 0.04349924251437187,
      "learning_rate": 6.670833333333333e-06,
      "loss": 0.0013,
      "step": 207980
    },
    {
      "epoch": 6.933,
      "grad_norm": 0.3713630437850952,
      "learning_rate": 6.66875e-06,
      "loss": 0.0015,
      "step": 207990
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.09207258373498917,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0026,
      "step": 208000
    },
    {
      "epoch": 6.933666666666666,
      "grad_norm": 0.08588878065347672,
      "learning_rate": 6.664583333333334e-06,
      "loss": 0.0017,
      "step": 208010
    },
    {
      "epoch": 6.934,
      "grad_norm": 0.057890698313713074,
      "learning_rate": 6.662500000000001e-06,
      "loss": 0.0024,
      "step": 208020
    },
    {
      "epoch": 6.934333333333333,
      "grad_norm": 0.05699223279953003,
      "learning_rate": 6.660416666666667e-06,
      "loss": 0.0015,
      "step": 208030
    },
    {
      "epoch": 6.934666666666667,
      "grad_norm": 0.11936071515083313,
      "learning_rate": 6.658333333333333e-06,
      "loss": 0.0023,
      "step": 208040
    },
    {
      "epoch": 6.9350000000000005,
      "grad_norm": 0.012263668701052666,
      "learning_rate": 6.65625e-06,
      "loss": 0.0015,
      "step": 208050
    },
    {
      "epoch": 6.935333333333333,
      "grad_norm": 0.2572900652885437,
      "learning_rate": 6.654166666666667e-06,
      "loss": 0.0016,
      "step": 208060
    },
    {
      "epoch": 6.935666666666666,
      "grad_norm": 0.200581356883049,
      "learning_rate": 6.6520833333333335e-06,
      "loss": 0.0015,
      "step": 208070
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.33978572487831116,
      "learning_rate": 6.650000000000001e-06,
      "loss": 0.0024,
      "step": 208080
    },
    {
      "epoch": 6.936333333333334,
      "grad_norm": 0.08631470054388046,
      "learning_rate": 6.647916666666668e-06,
      "loss": 0.0012,
      "step": 208090
    },
    {
      "epoch": 6.9366666666666665,
      "grad_norm": 0.0860433578491211,
      "learning_rate": 6.645833333333333e-06,
      "loss": 0.0015,
      "step": 208100
    },
    {
      "epoch": 6.937,
      "grad_norm": 0.022295482456684113,
      "learning_rate": 6.64375e-06,
      "loss": 0.0015,
      "step": 208110
    },
    {
      "epoch": 6.937333333333333,
      "grad_norm": 0.2572465240955353,
      "learning_rate": 6.641666666666667e-06,
      "loss": 0.0012,
      "step": 208120
    },
    {
      "epoch": 6.937666666666667,
      "grad_norm": 0.05782517045736313,
      "learning_rate": 6.639583333333334e-06,
      "loss": 0.0015,
      "step": 208130
    },
    {
      "epoch": 6.938,
      "grad_norm": 0.03230348601937294,
      "learning_rate": 6.6375e-06,
      "loss": 0.0019,
      "step": 208140
    },
    {
      "epoch": 6.9383333333333335,
      "grad_norm": 0.03082824870944023,
      "learning_rate": 6.635416666666668e-06,
      "loss": 0.0012,
      "step": 208150
    },
    {
      "epoch": 6.938666666666666,
      "grad_norm": 0.08974005281925201,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.0017,
      "step": 208160
    },
    {
      "epoch": 6.939,
      "grad_norm": 0.3715039789676666,
      "learning_rate": 6.63125e-06,
      "loss": 0.0016,
      "step": 208170
    },
    {
      "epoch": 6.939333333333334,
      "grad_norm": 0.28688564896583557,
      "learning_rate": 6.629166666666667e-06,
      "loss": 0.0011,
      "step": 208180
    },
    {
      "epoch": 6.939666666666667,
      "grad_norm": 0.08573237806558609,
      "learning_rate": 6.627083333333334e-06,
      "loss": 0.0022,
      "step": 208190
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 0.1439572274684906,
      "learning_rate": 6.625000000000001e-06,
      "loss": 0.0013,
      "step": 208200
    },
    {
      "epoch": 6.940333333333333,
      "grad_norm": 0.08826342225074768,
      "learning_rate": 6.622916666666667e-06,
      "loss": 0.002,
      "step": 208210
    },
    {
      "epoch": 6.940666666666667,
      "grad_norm": 0.029899194836616516,
      "learning_rate": 6.620833333333333e-06,
      "loss": 0.0016,
      "step": 208220
    },
    {
      "epoch": 6.941,
      "grad_norm": 0.11449895799160004,
      "learning_rate": 6.61875e-06,
      "loss": 0.0013,
      "step": 208230
    },
    {
      "epoch": 6.941333333333334,
      "grad_norm": 0.371849924325943,
      "learning_rate": 6.616666666666667e-06,
      "loss": 0.0014,
      "step": 208240
    },
    {
      "epoch": 6.941666666666666,
      "grad_norm": 0.08804971724748611,
      "learning_rate": 6.6145833333333335e-06,
      "loss": 0.0018,
      "step": 208250
    },
    {
      "epoch": 6.942,
      "grad_norm": 0.3714110255241394,
      "learning_rate": 6.612500000000001e-06,
      "loss": 0.0019,
      "step": 208260
    },
    {
      "epoch": 6.942333333333333,
      "grad_norm": 0.058013834059238434,
      "learning_rate": 6.610416666666668e-06,
      "loss": 0.0018,
      "step": 208270
    },
    {
      "epoch": 6.942666666666667,
      "grad_norm": 0.34252962470054626,
      "learning_rate": 6.608333333333333e-06,
      "loss": 0.0013,
      "step": 208280
    },
    {
      "epoch": 6.943,
      "grad_norm": 0.1710887998342514,
      "learning_rate": 6.60625e-06,
      "loss": 0.0014,
      "step": 208290
    },
    {
      "epoch": 6.943333333333333,
      "grad_norm": 0.05733880028128624,
      "learning_rate": 6.604166666666667e-06,
      "loss": 0.0013,
      "step": 208300
    },
    {
      "epoch": 6.943666666666667,
      "grad_norm": 0.08610949665307999,
      "learning_rate": 6.602083333333334e-06,
      "loss": 0.0022,
      "step": 208310
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.24482782185077667,
      "learning_rate": 6.6e-06,
      "loss": 0.0017,
      "step": 208320
    },
    {
      "epoch": 6.944333333333334,
      "grad_norm": 0.058114971965551376,
      "learning_rate": 6.5979166666666675e-06,
      "loss": 0.0013,
      "step": 208330
    },
    {
      "epoch": 6.9446666666666665,
      "grad_norm": 0.007097315043210983,
      "learning_rate": 6.595833333333333e-06,
      "loss": 0.0014,
      "step": 208340
    },
    {
      "epoch": 6.945,
      "grad_norm": 0.2856135070323944,
      "learning_rate": 6.59375e-06,
      "loss": 0.0016,
      "step": 208350
    },
    {
      "epoch": 6.945333333333333,
      "grad_norm": 0.14472800493240356,
      "learning_rate": 6.5916666666666665e-06,
      "loss": 0.0017,
      "step": 208360
    },
    {
      "epoch": 6.945666666666667,
      "grad_norm": 0.08573734760284424,
      "learning_rate": 6.589583333333334e-06,
      "loss": 0.0015,
      "step": 208370
    },
    {
      "epoch": 6.946,
      "grad_norm": 0.2564576268196106,
      "learning_rate": 6.587500000000001e-06,
      "loss": 0.0015,
      "step": 208380
    },
    {
      "epoch": 6.9463333333333335,
      "grad_norm": 0.02992144599556923,
      "learning_rate": 6.585416666666667e-06,
      "loss": 0.0012,
      "step": 208390
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 0.22794081270694733,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.0017,
      "step": 208400
    },
    {
      "epoch": 6.947,
      "grad_norm": 0.11411065608263016,
      "learning_rate": 6.58125e-06,
      "loss": 0.002,
      "step": 208410
    },
    {
      "epoch": 6.947333333333333,
      "grad_norm": 0.171975776553154,
      "learning_rate": 6.579166666666667e-06,
      "loss": 0.0017,
      "step": 208420
    },
    {
      "epoch": 6.947666666666667,
      "grad_norm": 0.031451646238565445,
      "learning_rate": 6.577083333333333e-06,
      "loss": 0.0025,
      "step": 208430
    },
    {
      "epoch": 6.948,
      "grad_norm": 0.17265383899211884,
      "learning_rate": 6.5750000000000006e-06,
      "loss": 0.0016,
      "step": 208440
    },
    {
      "epoch": 6.948333333333333,
      "grad_norm": 0.24386419355869293,
      "learning_rate": 6.572916666666668e-06,
      "loss": 0.0015,
      "step": 208450
    },
    {
      "epoch": 6.948666666666667,
      "grad_norm": 0.22804898023605347,
      "learning_rate": 6.570833333333333e-06,
      "loss": 0.0015,
      "step": 208460
    },
    {
      "epoch": 6.949,
      "grad_norm": 0.11456050723791122,
      "learning_rate": 6.56875e-06,
      "loss": 0.0015,
      "step": 208470
    },
    {
      "epoch": 6.949333333333334,
      "grad_norm": 0.4282267689704895,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0012,
      "step": 208480
    },
    {
      "epoch": 6.949666666666666,
      "grad_norm": 0.11454403400421143,
      "learning_rate": 6.564583333333334e-06,
      "loss": 0.0016,
      "step": 208490
    },
    {
      "epoch": 6.95,
      "grad_norm": 0.28512853384017944,
      "learning_rate": 6.5625e-06,
      "loss": 0.0025,
      "step": 208500
    },
    {
      "epoch": 6.950333333333333,
      "grad_norm": 0.2851414680480957,
      "learning_rate": 6.5604166666666675e-06,
      "loss": 0.0016,
      "step": 208510
    },
    {
      "epoch": 6.950666666666667,
      "grad_norm": 0.599665641784668,
      "learning_rate": 6.558333333333333e-06,
      "loss": 0.0016,
      "step": 208520
    },
    {
      "epoch": 6.951,
      "grad_norm": 0.08626646548509598,
      "learning_rate": 6.55625e-06,
      "loss": 0.0016,
      "step": 208530
    },
    {
      "epoch": 6.951333333333333,
      "grad_norm": 0.3712972104549408,
      "learning_rate": 6.5541666666666665e-06,
      "loss": 0.0013,
      "step": 208540
    },
    {
      "epoch": 6.951666666666666,
      "grad_norm": 0.3142503798007965,
      "learning_rate": 6.552083333333334e-06,
      "loss": 0.0015,
      "step": 208550
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.3710409104824066,
      "learning_rate": 6.550000000000001e-06,
      "loss": 0.0018,
      "step": 208560
    },
    {
      "epoch": 6.952333333333334,
      "grad_norm": 0.015991797670722008,
      "learning_rate": 6.547916666666667e-06,
      "loss": 0.0014,
      "step": 208570
    },
    {
      "epoch": 6.9526666666666666,
      "grad_norm": 0.08607324957847595,
      "learning_rate": 6.545833333333333e-06,
      "loss": 0.0018,
      "step": 208580
    },
    {
      "epoch": 6.953,
      "grad_norm": 0.17120999097824097,
      "learning_rate": 6.54375e-06,
      "loss": 0.0013,
      "step": 208590
    },
    {
      "epoch": 6.953333333333333,
      "grad_norm": 0.14284874498844147,
      "learning_rate": 6.541666666666667e-06,
      "loss": 0.0022,
      "step": 208600
    },
    {
      "epoch": 6.953666666666667,
      "grad_norm": 0.25656816363334656,
      "learning_rate": 6.539583333333333e-06,
      "loss": 0.0014,
      "step": 208610
    },
    {
      "epoch": 6.954,
      "grad_norm": 0.030264778062701225,
      "learning_rate": 6.5375000000000005e-06,
      "loss": 0.0016,
      "step": 208620
    },
    {
      "epoch": 6.9543333333333335,
      "grad_norm": 0.39934805035591125,
      "learning_rate": 6.535416666666668e-06,
      "loss": 0.0021,
      "step": 208630
    },
    {
      "epoch": 6.954666666666666,
      "grad_norm": 0.032362304627895355,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0019,
      "step": 208640
    },
    {
      "epoch": 6.955,
      "grad_norm": 0.030163679271936417,
      "learning_rate": 6.5312499999999995e-06,
      "loss": 0.0015,
      "step": 208650
    },
    {
      "epoch": 6.955333333333334,
      "grad_norm": 0.2292224019765854,
      "learning_rate": 6.529166666666667e-06,
      "loss": 0.0015,
      "step": 208660
    },
    {
      "epoch": 6.955666666666667,
      "grad_norm": 0.2571236193180084,
      "learning_rate": 6.527083333333334e-06,
      "loss": 0.0015,
      "step": 208670
    },
    {
      "epoch": 6.9559999999999995,
      "grad_norm": 0.32532620429992676,
      "learning_rate": 6.525e-06,
      "loss": 0.0018,
      "step": 208680
    },
    {
      "epoch": 6.956333333333333,
      "grad_norm": 0.14455801248550415,
      "learning_rate": 6.522916666666667e-06,
      "loss": 0.0016,
      "step": 208690
    },
    {
      "epoch": 6.956666666666667,
      "grad_norm": 0.05848805233836174,
      "learning_rate": 6.520833333333333e-06,
      "loss": 0.0014,
      "step": 208700
    },
    {
      "epoch": 6.957,
      "grad_norm": 0.1622643917798996,
      "learning_rate": 6.51875e-06,
      "loss": 0.0014,
      "step": 208710
    },
    {
      "epoch": 6.957333333333334,
      "grad_norm": 0.22832325100898743,
      "learning_rate": 6.516666666666666e-06,
      "loss": 0.0017,
      "step": 208720
    },
    {
      "epoch": 6.957666666666666,
      "grad_norm": 0.2572825849056244,
      "learning_rate": 6.5145833333333336e-06,
      "loss": 0.0015,
      "step": 208730
    },
    {
      "epoch": 6.958,
      "grad_norm": 0.2569988965988159,
      "learning_rate": 6.512500000000001e-06,
      "loss": 0.0014,
      "step": 208740
    },
    {
      "epoch": 6.958333333333333,
      "grad_norm": 0.4504813551902771,
      "learning_rate": 6.510416666666667e-06,
      "loss": 0.0013,
      "step": 208750
    },
    {
      "epoch": 6.958666666666667,
      "grad_norm": 0.11532345414161682,
      "learning_rate": 6.508333333333334e-06,
      "loss": 0.0011,
      "step": 208760
    },
    {
      "epoch": 6.959,
      "grad_norm": 0.22789080440998077,
      "learning_rate": 6.50625e-06,
      "loss": 0.0022,
      "step": 208770
    },
    {
      "epoch": 6.959333333333333,
      "grad_norm": 0.2821659445762634,
      "learning_rate": 6.504166666666667e-06,
      "loss": 0.0023,
      "step": 208780
    },
    {
      "epoch": 6.959666666666667,
      "grad_norm": 0.15634852647781372,
      "learning_rate": 6.502083333333333e-06,
      "loss": 0.0019,
      "step": 208790
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.22871354222297668,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0012,
      "step": 208800
    },
    {
      "epoch": 6.960333333333334,
      "grad_norm": 0.22193065285682678,
      "learning_rate": 6.497916666666668e-06,
      "loss": 0.0014,
      "step": 208810
    },
    {
      "epoch": 6.960666666666667,
      "grad_norm": 0.23568035662174225,
      "learning_rate": 6.495833333333334e-06,
      "loss": 0.0019,
      "step": 208820
    },
    {
      "epoch": 6.961,
      "grad_norm": 0.06026541814208031,
      "learning_rate": 6.4937499999999994e-06,
      "loss": 0.0013,
      "step": 208830
    },
    {
      "epoch": 6.961333333333333,
      "grad_norm": 0.07624757289886475,
      "learning_rate": 6.491666666666667e-06,
      "loss": 0.0014,
      "step": 208840
    },
    {
      "epoch": 6.961666666666667,
      "grad_norm": 0.05826535448431969,
      "learning_rate": 6.489583333333334e-06,
      "loss": 0.0025,
      "step": 208850
    },
    {
      "epoch": 6.962,
      "grad_norm": 0.39957714080810547,
      "learning_rate": 6.4875e-06,
      "loss": 0.002,
      "step": 208860
    },
    {
      "epoch": 6.9623333333333335,
      "grad_norm": 0.031062645837664604,
      "learning_rate": 6.485416666666667e-06,
      "loss": 0.0015,
      "step": 208870
    },
    {
      "epoch": 6.962666666666666,
      "grad_norm": 0.17108558118343353,
      "learning_rate": 6.4833333333333345e-06,
      "loss": 0.0021,
      "step": 208880
    },
    {
      "epoch": 6.963,
      "grad_norm": 0.3329699635505676,
      "learning_rate": 6.48125e-06,
      "loss": 0.0013,
      "step": 208890
    },
    {
      "epoch": 6.963333333333333,
      "grad_norm": 0.1271495223045349,
      "learning_rate": 6.479166666666666e-06,
      "loss": 0.0013,
      "step": 208900
    },
    {
      "epoch": 6.963666666666667,
      "grad_norm": 0.08611394464969635,
      "learning_rate": 6.4770833333333335e-06,
      "loss": 0.0021,
      "step": 208910
    },
    {
      "epoch": 6.964,
      "grad_norm": 0.05909539386630058,
      "learning_rate": 6.475000000000001e-06,
      "loss": 0.0016,
      "step": 208920
    },
    {
      "epoch": 6.964333333333333,
      "grad_norm": 0.284980833530426,
      "learning_rate": 6.472916666666667e-06,
      "loss": 0.0021,
      "step": 208930
    },
    {
      "epoch": 6.964666666666667,
      "grad_norm": 0.17190495133399963,
      "learning_rate": 6.470833333333334e-06,
      "loss": 0.0022,
      "step": 208940
    },
    {
      "epoch": 6.965,
      "grad_norm": 0.24161846935749054,
      "learning_rate": 6.46875e-06,
      "loss": 0.0012,
      "step": 208950
    },
    {
      "epoch": 6.965333333333334,
      "grad_norm": 0.20006674528121948,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.002,
      "step": 208960
    },
    {
      "epoch": 6.9656666666666665,
      "grad_norm": 0.11418908834457397,
      "learning_rate": 6.464583333333333e-06,
      "loss": 0.0018,
      "step": 208970
    },
    {
      "epoch": 6.966,
      "grad_norm": 0.05894911661744118,
      "learning_rate": 6.4625e-06,
      "loss": 0.0017,
      "step": 208980
    },
    {
      "epoch": 6.966333333333333,
      "grad_norm": 0.19987688958644867,
      "learning_rate": 6.4604166666666676e-06,
      "loss": 0.0013,
      "step": 208990
    },
    {
      "epoch": 6.966666666666667,
      "grad_norm": 0.05763715133070946,
      "learning_rate": 6.458333333333334e-06,
      "loss": 0.0019,
      "step": 209000
    },
    {
      "epoch": 6.967,
      "grad_norm": 0.11530935019254684,
      "learning_rate": 6.456249999999999e-06,
      "loss": 0.0019,
      "step": 209010
    },
    {
      "epoch": 6.967333333333333,
      "grad_norm": 0.011231690645217896,
      "learning_rate": 6.4541666666666666e-06,
      "loss": 0.0015,
      "step": 209020
    },
    {
      "epoch": 6.967666666666666,
      "grad_norm": 0.14325454831123352,
      "learning_rate": 6.452083333333334e-06,
      "loss": 0.0018,
      "step": 209030
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.1996164321899414,
      "learning_rate": 6.45e-06,
      "loss": 0.0013,
      "step": 209040
    },
    {
      "epoch": 6.968333333333334,
      "grad_norm": 0.014627397060394287,
      "learning_rate": 6.447916666666667e-06,
      "loss": 0.002,
      "step": 209050
    },
    {
      "epoch": 6.968666666666667,
      "grad_norm": 0.08616615831851959,
      "learning_rate": 6.4458333333333344e-06,
      "loss": 0.0019,
      "step": 209060
    },
    {
      "epoch": 6.969,
      "grad_norm": 0.1588299572467804,
      "learning_rate": 6.44375e-06,
      "loss": 0.0017,
      "step": 209070
    },
    {
      "epoch": 6.969333333333333,
      "grad_norm": 0.02961794286966324,
      "learning_rate": 6.441666666666666e-06,
      "loss": 0.0014,
      "step": 209080
    },
    {
      "epoch": 6.969666666666667,
      "grad_norm": 0.25692328810691833,
      "learning_rate": 6.4395833333333334e-06,
      "loss": 0.0011,
      "step": 209090
    },
    {
      "epoch": 6.97,
      "grad_norm": 0.14350850880146027,
      "learning_rate": 6.437500000000001e-06,
      "loss": 0.0018,
      "step": 209100
    },
    {
      "epoch": 6.9703333333333335,
      "grad_norm": 0.25700557231903076,
      "learning_rate": 6.435416666666667e-06,
      "loss": 0.0014,
      "step": 209110
    },
    {
      "epoch": 6.970666666666666,
      "grad_norm": 0.08554054796695709,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.0016,
      "step": 209120
    },
    {
      "epoch": 6.971,
      "grad_norm": 0.02950955182313919,
      "learning_rate": 6.43125e-06,
      "loss": 0.0013,
      "step": 209130
    },
    {
      "epoch": 6.971333333333334,
      "grad_norm": 0.22862541675567627,
      "learning_rate": 6.429166666666667e-06,
      "loss": 0.0024,
      "step": 209140
    },
    {
      "epoch": 6.971666666666667,
      "grad_norm": 0.2950233817100525,
      "learning_rate": 6.427083333333334e-06,
      "loss": 0.0011,
      "step": 209150
    },
    {
      "epoch": 6.9719999999999995,
      "grad_norm": 0.08850565552711487,
      "learning_rate": 6.425e-06,
      "loss": 0.0019,
      "step": 209160
    },
    {
      "epoch": 6.972333333333333,
      "grad_norm": 0.34011566638946533,
      "learning_rate": 6.4229166666666675e-06,
      "loss": 0.0024,
      "step": 209170
    },
    {
      "epoch": 6.972666666666667,
      "grad_norm": 0.14444600045681,
      "learning_rate": 6.420833333333334e-06,
      "loss": 0.0017,
      "step": 209180
    },
    {
      "epoch": 6.973,
      "grad_norm": 0.033302273601293564,
      "learning_rate": 6.41875e-06,
      "loss": 0.002,
      "step": 209190
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 0.0580739825963974,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.0015,
      "step": 209200
    },
    {
      "epoch": 6.9736666666666665,
      "grad_norm": 0.11572273075580597,
      "learning_rate": 6.414583333333334e-06,
      "loss": 0.0013,
      "step": 209210
    },
    {
      "epoch": 6.974,
      "grad_norm": 0.21070362627506256,
      "learning_rate": 6.412500000000001e-06,
      "loss": 0.0015,
      "step": 209220
    },
    {
      "epoch": 6.974333333333333,
      "grad_norm": 0.08605746179819107,
      "learning_rate": 6.410416666666667e-06,
      "loss": 0.0022,
      "step": 209230
    },
    {
      "epoch": 6.974666666666667,
      "grad_norm": 0.030317658558487892,
      "learning_rate": 6.408333333333334e-06,
      "loss": 0.0016,
      "step": 209240
    },
    {
      "epoch": 6.975,
      "grad_norm": 0.11464246362447739,
      "learning_rate": 6.40625e-06,
      "loss": 0.0018,
      "step": 209250
    },
    {
      "epoch": 6.975333333333333,
      "grad_norm": 0.28604671359062195,
      "learning_rate": 6.404166666666667e-06,
      "loss": 0.0014,
      "step": 209260
    },
    {
      "epoch": 6.975666666666667,
      "grad_norm": 0.117012619972229,
      "learning_rate": 6.402083333333333e-06,
      "loss": 0.0013,
      "step": 209270
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.0078662671148777,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0015,
      "step": 209280
    },
    {
      "epoch": 6.976333333333334,
      "grad_norm": 0.14364926517009735,
      "learning_rate": 6.397916666666668e-06,
      "loss": 0.0016,
      "step": 209290
    },
    {
      "epoch": 6.976666666666667,
      "grad_norm": 0.22828377783298492,
      "learning_rate": 6.395833333333334e-06,
      "loss": 0.0018,
      "step": 209300
    },
    {
      "epoch": 6.977,
      "grad_norm": 0.10151985287666321,
      "learning_rate": 6.3937499999999996e-06,
      "loss": 0.0018,
      "step": 209310
    },
    {
      "epoch": 6.977333333333333,
      "grad_norm": 0.058149851858615875,
      "learning_rate": 6.391666666666667e-06,
      "loss": 0.0013,
      "step": 209320
    },
    {
      "epoch": 6.977666666666667,
      "grad_norm": 0.1365974098443985,
      "learning_rate": 6.389583333333334e-06,
      "loss": 0.0016,
      "step": 209330
    },
    {
      "epoch": 6.978,
      "grad_norm": 0.22890599071979523,
      "learning_rate": 6.3875e-06,
      "loss": 0.0017,
      "step": 209340
    },
    {
      "epoch": 6.9783333333333335,
      "grad_norm": 0.14322003722190857,
      "learning_rate": 6.3854166666666674e-06,
      "loss": 0.0023,
      "step": 209350
    },
    {
      "epoch": 6.978666666666666,
      "grad_norm": 0.02968580089509487,
      "learning_rate": 6.383333333333335e-06,
      "loss": 0.0019,
      "step": 209360
    },
    {
      "epoch": 6.979,
      "grad_norm": 0.17089971899986267,
      "learning_rate": 6.38125e-06,
      "loss": 0.0017,
      "step": 209370
    },
    {
      "epoch": 6.979333333333333,
      "grad_norm": 0.057796429842710495,
      "learning_rate": 6.3791666666666664e-06,
      "loss": 0.0012,
      "step": 209380
    },
    {
      "epoch": 6.979666666666667,
      "grad_norm": 0.05816946551203728,
      "learning_rate": 6.377083333333334e-06,
      "loss": 0.0023,
      "step": 209390
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.1580214947462082,
      "learning_rate": 6.375000000000001e-06,
      "loss": 0.0019,
      "step": 209400
    },
    {
      "epoch": 6.980333333333333,
      "grad_norm": 0.22878685593605042,
      "learning_rate": 6.372916666666667e-06,
      "loss": 0.0014,
      "step": 209410
    },
    {
      "epoch": 6.980666666666667,
      "grad_norm": 0.005930175073444843,
      "learning_rate": 6.370833333333334e-06,
      "loss": 0.0015,
      "step": 209420
    },
    {
      "epoch": 6.981,
      "grad_norm": 0.25662532448768616,
      "learning_rate": 6.36875e-06,
      "loss": 0.0025,
      "step": 209430
    },
    {
      "epoch": 6.981333333333334,
      "grad_norm": 0.03049970045685768,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0016,
      "step": 209440
    },
    {
      "epoch": 6.9816666666666665,
      "grad_norm": 0.17166446149349213,
      "learning_rate": 6.364583333333333e-06,
      "loss": 0.0023,
      "step": 209450
    },
    {
      "epoch": 6.982,
      "grad_norm": 0.23691998422145844,
      "learning_rate": 6.3625000000000005e-06,
      "loss": 0.0014,
      "step": 209460
    },
    {
      "epoch": 6.982333333333333,
      "grad_norm": 0.08701928704977036,
      "learning_rate": 6.360416666666668e-06,
      "loss": 0.0012,
      "step": 209470
    },
    {
      "epoch": 6.982666666666667,
      "grad_norm": 0.11507318913936615,
      "learning_rate": 6.358333333333334e-06,
      "loss": 0.0016,
      "step": 209480
    },
    {
      "epoch": 6.983,
      "grad_norm": 0.03275568410754204,
      "learning_rate": 6.3562499999999995e-06,
      "loss": 0.0014,
      "step": 209490
    },
    {
      "epoch": 6.983333333333333,
      "grad_norm": 0.17412734031677246,
      "learning_rate": 6.354166666666667e-06,
      "loss": 0.0013,
      "step": 209500
    },
    {
      "epoch": 6.983666666666666,
      "grad_norm": 0.22777827084064484,
      "learning_rate": 6.352083333333334e-06,
      "loss": 0.0017,
      "step": 209510
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.11440104246139526,
      "learning_rate": 6.35e-06,
      "loss": 0.0016,
      "step": 209520
    },
    {
      "epoch": 6.984333333333334,
      "grad_norm": 0.3708200454711914,
      "learning_rate": 6.347916666666667e-06,
      "loss": 0.0023,
      "step": 209530
    },
    {
      "epoch": 6.984666666666667,
      "grad_norm": 0.11506638675928116,
      "learning_rate": 6.3458333333333346e-06,
      "loss": 0.0019,
      "step": 209540
    },
    {
      "epoch": 6.985,
      "grad_norm": 0.17372983694076538,
      "learning_rate": 6.34375e-06,
      "loss": 0.0026,
      "step": 209550
    },
    {
      "epoch": 6.985333333333333,
      "grad_norm": 0.0597052201628685,
      "learning_rate": 6.341666666666666e-06,
      "loss": 0.0012,
      "step": 209560
    },
    {
      "epoch": 6.985666666666667,
      "grad_norm": 0.10789923369884491,
      "learning_rate": 6.3395833333333336e-06,
      "loss": 0.0012,
      "step": 209570
    },
    {
      "epoch": 6.986,
      "grad_norm": 0.12117648124694824,
      "learning_rate": 6.337500000000001e-06,
      "loss": 0.0023,
      "step": 209580
    },
    {
      "epoch": 6.9863333333333335,
      "grad_norm": 0.22812597453594208,
      "learning_rate": 6.335416666666667e-06,
      "loss": 0.0013,
      "step": 209590
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.009454448707401752,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0017,
      "step": 209600
    },
    {
      "epoch": 6.987,
      "grad_norm": 0.11513518542051315,
      "learning_rate": 6.33125e-06,
      "loss": 0.0023,
      "step": 209610
    },
    {
      "epoch": 6.987333333333333,
      "grad_norm": 0.1124534010887146,
      "learning_rate": 6.329166666666667e-06,
      "loss": 0.0016,
      "step": 209620
    },
    {
      "epoch": 6.987666666666667,
      "grad_norm": 0.011050601489841938,
      "learning_rate": 6.327083333333333e-06,
      "loss": 0.0022,
      "step": 209630
    },
    {
      "epoch": 6.9879999999999995,
      "grad_norm": 0.20019052922725677,
      "learning_rate": 6.3250000000000004e-06,
      "loss": 0.0019,
      "step": 209640
    },
    {
      "epoch": 6.988333333333333,
      "grad_norm": 0.1999107450246811,
      "learning_rate": 6.322916666666668e-06,
      "loss": 0.0021,
      "step": 209650
    },
    {
      "epoch": 6.988666666666667,
      "grad_norm": 0.22287049889564514,
      "learning_rate": 6.320833333333334e-06,
      "loss": 0.0014,
      "step": 209660
    },
    {
      "epoch": 6.989,
      "grad_norm": 0.1434108316898346,
      "learning_rate": 6.3187499999999994e-06,
      "loss": 0.0025,
      "step": 209670
    },
    {
      "epoch": 6.989333333333334,
      "grad_norm": 0.14338751137256622,
      "learning_rate": 6.316666666666667e-06,
      "loss": 0.0019,
      "step": 209680
    },
    {
      "epoch": 6.9896666666666665,
      "grad_norm": 0.058003466576337814,
      "learning_rate": 6.314583333333334e-06,
      "loss": 0.0025,
      "step": 209690
    },
    {
      "epoch": 6.99,
      "grad_norm": 0.17119252681732178,
      "learning_rate": 6.3125e-06,
      "loss": 0.0015,
      "step": 209700
    },
    {
      "epoch": 6.990333333333333,
      "grad_norm": 0.2569490671157837,
      "learning_rate": 6.310416666666667e-06,
      "loss": 0.0015,
      "step": 209710
    },
    {
      "epoch": 6.990666666666667,
      "grad_norm": 0.08635371178388596,
      "learning_rate": 6.3083333333333345e-06,
      "loss": 0.0014,
      "step": 209720
    },
    {
      "epoch": 6.991,
      "grad_norm": 0.030181551352143288,
      "learning_rate": 6.30625e-06,
      "loss": 0.0014,
      "step": 209730
    },
    {
      "epoch": 6.991333333333333,
      "grad_norm": 0.22853738069534302,
      "learning_rate": 6.304166666666666e-06,
      "loss": 0.0019,
      "step": 209740
    },
    {
      "epoch": 6.991666666666667,
      "grad_norm": 0.4660150408744812,
      "learning_rate": 6.3020833333333335e-06,
      "loss": 0.0026,
      "step": 209750
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.015135860070586205,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0018,
      "step": 209760
    },
    {
      "epoch": 6.992333333333333,
      "grad_norm": 0.11452444642782211,
      "learning_rate": 6.297916666666667e-06,
      "loss": 0.0014,
      "step": 209770
    },
    {
      "epoch": 6.992666666666667,
      "grad_norm": 0.2677105665206909,
      "learning_rate": 6.295833333333334e-06,
      "loss": 0.0019,
      "step": 209780
    },
    {
      "epoch": 6.993,
      "grad_norm": 0.03391389548778534,
      "learning_rate": 6.29375e-06,
      "loss": 0.0014,
      "step": 209790
    },
    {
      "epoch": 6.993333333333333,
      "grad_norm": 0.09675178676843643,
      "learning_rate": 6.291666666666667e-06,
      "loss": 0.0019,
      "step": 209800
    },
    {
      "epoch": 6.993666666666667,
      "grad_norm": 0.11457523703575134,
      "learning_rate": 6.289583333333333e-06,
      "loss": 0.0015,
      "step": 209810
    },
    {
      "epoch": 6.994,
      "grad_norm": 0.11466663330793381,
      "learning_rate": 6.2875e-06,
      "loss": 0.0023,
      "step": 209820
    },
    {
      "epoch": 6.9943333333333335,
      "grad_norm": 0.2983438968658447,
      "learning_rate": 6.2854166666666675e-06,
      "loss": 0.0016,
      "step": 209830
    },
    {
      "epoch": 6.994666666666666,
      "grad_norm": 0.03076087310910225,
      "learning_rate": 6.283333333333334e-06,
      "loss": 0.0013,
      "step": 209840
    },
    {
      "epoch": 6.995,
      "grad_norm": 0.22835198044776917,
      "learning_rate": 6.281249999999999e-06,
      "loss": 0.002,
      "step": 209850
    },
    {
      "epoch": 6.995333333333333,
      "grad_norm": 0.2007264941930771,
      "learning_rate": 6.2791666666666665e-06,
      "loss": 0.0013,
      "step": 209860
    },
    {
      "epoch": 6.995666666666667,
      "grad_norm": 0.31354576349258423,
      "learning_rate": 6.277083333333334e-06,
      "loss": 0.0023,
      "step": 209870
    },
    {
      "epoch": 6.996,
      "grad_norm": 0.3744891285896301,
      "learning_rate": 6.275e-06,
      "loss": 0.0023,
      "step": 209880
    },
    {
      "epoch": 6.996333333333333,
      "grad_norm": 0.257167786359787,
      "learning_rate": 6.272916666666667e-06,
      "loss": 0.0015,
      "step": 209890
    },
    {
      "epoch": 6.996666666666667,
      "grad_norm": 0.08827176690101624,
      "learning_rate": 6.270833333333334e-06,
      "loss": 0.0012,
      "step": 209900
    },
    {
      "epoch": 6.997,
      "grad_norm": 0.008417924866080284,
      "learning_rate": 6.26875e-06,
      "loss": 0.0012,
      "step": 209910
    },
    {
      "epoch": 6.997333333333334,
      "grad_norm": 0.39981529116630554,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0015,
      "step": 209920
    },
    {
      "epoch": 6.9976666666666665,
      "grad_norm": 0.4280306398868561,
      "learning_rate": 6.264583333333333e-06,
      "loss": 0.0017,
      "step": 209930
    },
    {
      "epoch": 6.998,
      "grad_norm": 0.2564958333969116,
      "learning_rate": 6.262500000000001e-06,
      "loss": 0.0016,
      "step": 209940
    },
    {
      "epoch": 6.998333333333333,
      "grad_norm": 0.030431346967816353,
      "learning_rate": 6.260416666666667e-06,
      "loss": 0.0015,
      "step": 209950
    },
    {
      "epoch": 6.998666666666667,
      "grad_norm": 0.008742691949009895,
      "learning_rate": 6.258333333333334e-06,
      "loss": 0.0016,
      "step": 209960
    },
    {
      "epoch": 6.999,
      "grad_norm": 0.2568662166595459,
      "learning_rate": 6.25625e-06,
      "loss": 0.0014,
      "step": 209970
    },
    {
      "epoch": 6.999333333333333,
      "grad_norm": 0.028998324647545815,
      "learning_rate": 6.254166666666667e-06,
      "loss": 0.0013,
      "step": 209980
    },
    {
      "epoch": 6.999666666666666,
      "grad_norm": 0.029830895364284515,
      "learning_rate": 6.252083333333333e-06,
      "loss": 0.0014,
      "step": 209990
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.48445552587509155,
      "learning_rate": 6.25e-06,
      "loss": 0.0017,
      "step": 210000
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.001690576784312725,
      "eval_runtime": 128.5867,
      "eval_samples_per_second": 1555.371,
      "eval_steps_per_second": 38.884,
      "step": 210000
    },
    {
      "epoch": 7.000333333333334,
      "grad_norm": 0.31485480070114136,
      "learning_rate": 6.2479166666666675e-06,
      "loss": 0.0012,
      "step": 210010
    },
    {
      "epoch": 7.000666666666667,
      "grad_norm": 0.11539436131715775,
      "learning_rate": 6.245833333333334e-06,
      "loss": 0.002,
      "step": 210020
    },
    {
      "epoch": 7.001,
      "grad_norm": 0.197199746966362,
      "learning_rate": 6.24375e-06,
      "loss": 0.0014,
      "step": 210030
    },
    {
      "epoch": 7.001333333333333,
      "grad_norm": 0.05785413458943367,
      "learning_rate": 6.241666666666667e-06,
      "loss": 0.0015,
      "step": 210040
    },
    {
      "epoch": 7.001666666666667,
      "grad_norm": 0.03138536959886551,
      "learning_rate": 6.239583333333334e-06,
      "loss": 0.0016,
      "step": 210050
    },
    {
      "epoch": 7.002,
      "grad_norm": 0.062295254319906235,
      "learning_rate": 6.2375e-06,
      "loss": 0.0018,
      "step": 210060
    },
    {
      "epoch": 7.0023333333333335,
      "grad_norm": 0.14336006343364716,
      "learning_rate": 6.235416666666667e-06,
      "loss": 0.0017,
      "step": 210070
    },
    {
      "epoch": 7.002666666666666,
      "grad_norm": 0.11551631987094879,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0017,
      "step": 210080
    },
    {
      "epoch": 7.003,
      "grad_norm": 0.08628375083208084,
      "learning_rate": 6.231250000000001e-06,
      "loss": 0.0018,
      "step": 210090
    },
    {
      "epoch": 7.003333333333333,
      "grad_norm": 0.05821208283305168,
      "learning_rate": 6.229166666666667e-06,
      "loss": 0.0014,
      "step": 210100
    },
    {
      "epoch": 7.003666666666667,
      "grad_norm": 0.1468367576599121,
      "learning_rate": 6.227083333333333e-06,
      "loss": 0.0016,
      "step": 210110
    },
    {
      "epoch": 7.004,
      "grad_norm": 0.038307033479213715,
      "learning_rate": 6.2250000000000005e-06,
      "loss": 0.0014,
      "step": 210120
    },
    {
      "epoch": 7.004333333333333,
      "grad_norm": 0.08784322440624237,
      "learning_rate": 6.222916666666667e-06,
      "loss": 0.0018,
      "step": 210130
    },
    {
      "epoch": 7.004666666666667,
      "grad_norm": 0.25735336542129517,
      "learning_rate": 6.220833333333333e-06,
      "loss": 0.0014,
      "step": 210140
    },
    {
      "epoch": 7.005,
      "grad_norm": 0.03029797226190567,
      "learning_rate": 6.21875e-06,
      "loss": 0.0014,
      "step": 210150
    },
    {
      "epoch": 7.005333333333334,
      "grad_norm": 0.2374480962753296,
      "learning_rate": 6.2166666666666676e-06,
      "loss": 0.0015,
      "step": 210160
    },
    {
      "epoch": 7.0056666666666665,
      "grad_norm": 0.14334599673748016,
      "learning_rate": 6.214583333333333e-06,
      "loss": 0.0021,
      "step": 210170
    },
    {
      "epoch": 7.006,
      "grad_norm": 0.048539917916059494,
      "learning_rate": 6.2125e-06,
      "loss": 0.0018,
      "step": 210180
    },
    {
      "epoch": 7.006333333333333,
      "grad_norm": 0.02965913526713848,
      "learning_rate": 6.210416666666667e-06,
      "loss": 0.0016,
      "step": 210190
    },
    {
      "epoch": 7.006666666666667,
      "grad_norm": 0.3707605302333832,
      "learning_rate": 6.208333333333334e-06,
      "loss": 0.0018,
      "step": 210200
    },
    {
      "epoch": 7.007,
      "grad_norm": 0.20070861279964447,
      "learning_rate": 6.20625e-06,
      "loss": 0.0011,
      "step": 210210
    },
    {
      "epoch": 7.007333333333333,
      "grad_norm": 0.11481975018978119,
      "learning_rate": 6.204166666666667e-06,
      "loss": 0.002,
      "step": 210220
    },
    {
      "epoch": 7.007666666666666,
      "grad_norm": 0.25697699189186096,
      "learning_rate": 6.202083333333334e-06,
      "loss": 0.0029,
      "step": 210230
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.11476060748100281,
      "learning_rate": 6.2e-06,
      "loss": 0.0013,
      "step": 210240
    },
    {
      "epoch": 7.008333333333334,
      "grad_norm": 0.2001946121454239,
      "learning_rate": 6.197916666666667e-06,
      "loss": 0.0015,
      "step": 210250
    },
    {
      "epoch": 7.008666666666667,
      "grad_norm": 0.14311420917510986,
      "learning_rate": 6.1958333333333334e-06,
      "loss": 0.0018,
      "step": 210260
    },
    {
      "epoch": 7.009,
      "grad_norm": 0.3770151734352112,
      "learning_rate": 6.193750000000001e-06,
      "loss": 0.0015,
      "step": 210270
    },
    {
      "epoch": 7.009333333333333,
      "grad_norm": 0.17149296402931213,
      "learning_rate": 6.191666666666667e-06,
      "loss": 0.0019,
      "step": 210280
    },
    {
      "epoch": 7.009666666666667,
      "grad_norm": 0.17136262357234955,
      "learning_rate": 6.189583333333333e-06,
      "loss": 0.0022,
      "step": 210290
    },
    {
      "epoch": 7.01,
      "grad_norm": 0.20028778910636902,
      "learning_rate": 6.1875000000000005e-06,
      "loss": 0.0019,
      "step": 210300
    },
    {
      "epoch": 7.0103333333333335,
      "grad_norm": 0.23004847764968872,
      "learning_rate": 6.185416666666667e-06,
      "loss": 0.0018,
      "step": 210310
    },
    {
      "epoch": 7.010666666666666,
      "grad_norm": 0.2567525804042816,
      "learning_rate": 6.183333333333333e-06,
      "loss": 0.0021,
      "step": 210320
    },
    {
      "epoch": 7.011,
      "grad_norm": 0.0859227403998375,
      "learning_rate": 6.18125e-06,
      "loss": 0.0019,
      "step": 210330
    },
    {
      "epoch": 7.011333333333333,
      "grad_norm": 0.029733268544077873,
      "learning_rate": 6.1791666666666675e-06,
      "loss": 0.0015,
      "step": 210340
    },
    {
      "epoch": 7.011666666666667,
      "grad_norm": 0.2569008767604828,
      "learning_rate": 6.177083333333333e-06,
      "loss": 0.0011,
      "step": 210350
    },
    {
      "epoch": 7.012,
      "grad_norm": 0.09472194314002991,
      "learning_rate": 6.175e-06,
      "loss": 0.0018,
      "step": 210360
    },
    {
      "epoch": 7.012333333333333,
      "grad_norm": 0.08588673174381256,
      "learning_rate": 6.172916666666667e-06,
      "loss": 0.0017,
      "step": 210370
    },
    {
      "epoch": 7.012666666666667,
      "grad_norm": 0.399733304977417,
      "learning_rate": 6.170833333333334e-06,
      "loss": 0.0019,
      "step": 210380
    },
    {
      "epoch": 7.013,
      "grad_norm": 0.42767035961151123,
      "learning_rate": 6.16875e-06,
      "loss": 0.0015,
      "step": 210390
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 0.2861517667770386,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0018,
      "step": 210400
    },
    {
      "epoch": 7.0136666666666665,
      "grad_norm": 0.0443895123898983,
      "learning_rate": 6.1645833333333335e-06,
      "loss": 0.0016,
      "step": 210410
    },
    {
      "epoch": 7.014,
      "grad_norm": 0.11619365215301514,
      "learning_rate": 6.1625e-06,
      "loss": 0.0018,
      "step": 210420
    },
    {
      "epoch": 7.014333333333333,
      "grad_norm": 0.25689083337783813,
      "learning_rate": 6.160416666666667e-06,
      "loss": 0.0016,
      "step": 210430
    },
    {
      "epoch": 7.014666666666667,
      "grad_norm": 0.22870026528835297,
      "learning_rate": 6.158333333333333e-06,
      "loss": 0.0017,
      "step": 210440
    },
    {
      "epoch": 7.015,
      "grad_norm": 0.17082710564136505,
      "learning_rate": 6.1562500000000006e-06,
      "loss": 0.0017,
      "step": 210450
    },
    {
      "epoch": 7.015333333333333,
      "grad_norm": 0.058446481823921204,
      "learning_rate": 6.154166666666667e-06,
      "loss": 0.0019,
      "step": 210460
    },
    {
      "epoch": 7.015666666666666,
      "grad_norm": 0.42676234245300293,
      "learning_rate": 6.152083333333333e-06,
      "loss": 0.0019,
      "step": 210470
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.3704701364040375,
      "learning_rate": 6.15e-06,
      "loss": 0.0019,
      "step": 210480
    },
    {
      "epoch": 7.016333333333334,
      "grad_norm": 0.1716233193874359,
      "learning_rate": 6.147916666666667e-06,
      "loss": 0.0018,
      "step": 210490
    },
    {
      "epoch": 7.016666666666667,
      "grad_norm": 0.08602869510650635,
      "learning_rate": 6.145833333333333e-06,
      "loss": 0.0013,
      "step": 210500
    },
    {
      "epoch": 7.017,
      "grad_norm": 0.05848009139299393,
      "learning_rate": 6.14375e-06,
      "loss": 0.0017,
      "step": 210510
    },
    {
      "epoch": 7.017333333333333,
      "grad_norm": 0.34189897775650024,
      "learning_rate": 6.1416666666666674e-06,
      "loss": 0.0018,
      "step": 210520
    },
    {
      "epoch": 7.017666666666667,
      "grad_norm": 0.08633583784103394,
      "learning_rate": 6.139583333333333e-06,
      "loss": 0.0013,
      "step": 210530
    },
    {
      "epoch": 7.018,
      "grad_norm": 0.015668783336877823,
      "learning_rate": 6.1375e-06,
      "loss": 0.0013,
      "step": 210540
    },
    {
      "epoch": 7.0183333333333335,
      "grad_norm": 0.1880042850971222,
      "learning_rate": 6.135416666666667e-06,
      "loss": 0.0013,
      "step": 210550
    },
    {
      "epoch": 7.018666666666666,
      "grad_norm": 0.1717228889465332,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0014,
      "step": 210560
    },
    {
      "epoch": 7.019,
      "grad_norm": 0.17127953469753265,
      "learning_rate": 6.13125e-06,
      "loss": 0.0015,
      "step": 210570
    },
    {
      "epoch": 7.019333333333333,
      "grad_norm": 0.14699076116085052,
      "learning_rate": 6.129166666666667e-06,
      "loss": 0.0014,
      "step": 210580
    },
    {
      "epoch": 7.019666666666667,
      "grad_norm": 0.115130715072155,
      "learning_rate": 6.1270833333333335e-06,
      "loss": 0.0016,
      "step": 210590
    },
    {
      "epoch": 7.02,
      "grad_norm": 0.08695905655622482,
      "learning_rate": 6.125e-06,
      "loss": 0.0013,
      "step": 210600
    },
    {
      "epoch": 7.020333333333333,
      "grad_norm": 0.02946777455508709,
      "learning_rate": 6.122916666666667e-06,
      "loss": 0.0014,
      "step": 210610
    },
    {
      "epoch": 7.020666666666667,
      "grad_norm": 0.25599315762519836,
      "learning_rate": 6.120833333333333e-06,
      "loss": 0.0014,
      "step": 210620
    },
    {
      "epoch": 7.021,
      "grad_norm": 0.2346503883600235,
      "learning_rate": 6.1187500000000005e-06,
      "loss": 0.0019,
      "step": 210630
    },
    {
      "epoch": 7.021333333333334,
      "grad_norm": 0.08559445291757584,
      "learning_rate": 6.116666666666667e-06,
      "loss": 0.0013,
      "step": 210640
    },
    {
      "epoch": 7.0216666666666665,
      "grad_norm": 0.39985689520835876,
      "learning_rate": 6.114583333333334e-06,
      "loss": 0.0023,
      "step": 210650
    },
    {
      "epoch": 7.022,
      "grad_norm": 0.2855067253112793,
      "learning_rate": 6.1125e-06,
      "loss": 0.0014,
      "step": 210660
    },
    {
      "epoch": 7.022333333333333,
      "grad_norm": 0.19936668872833252,
      "learning_rate": 6.110416666666667e-06,
      "loss": 0.0016,
      "step": 210670
    },
    {
      "epoch": 7.022666666666667,
      "grad_norm": 0.1770513355731964,
      "learning_rate": 6.108333333333334e-06,
      "loss": 0.0012,
      "step": 210680
    },
    {
      "epoch": 7.023,
      "grad_norm": 0.029872462153434753,
      "learning_rate": 6.10625e-06,
      "loss": 0.0014,
      "step": 210690
    },
    {
      "epoch": 7.023333333333333,
      "grad_norm": 0.058120809495449066,
      "learning_rate": 6.104166666666667e-06,
      "loss": 0.0016,
      "step": 210700
    },
    {
      "epoch": 7.023666666666666,
      "grad_norm": 0.03038940019905567,
      "learning_rate": 6.102083333333334e-06,
      "loss": 0.0015,
      "step": 210710
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.05807756632566452,
      "learning_rate": 6.1e-06,
      "loss": 0.0019,
      "step": 210720
    },
    {
      "epoch": 7.024333333333334,
      "grad_norm": 0.17143678665161133,
      "learning_rate": 6.097916666666667e-06,
      "loss": 0.0025,
      "step": 210730
    },
    {
      "epoch": 7.024666666666667,
      "grad_norm": 0.11961394548416138,
      "learning_rate": 6.0958333333333336e-06,
      "loss": 0.0024,
      "step": 210740
    },
    {
      "epoch": 7.025,
      "grad_norm": 0.08569709211587906,
      "learning_rate": 6.09375e-06,
      "loss": 0.0014,
      "step": 210750
    },
    {
      "epoch": 7.025333333333333,
      "grad_norm": 0.3993907868862152,
      "learning_rate": 6.091666666666667e-06,
      "loss": 0.0014,
      "step": 210760
    },
    {
      "epoch": 7.025666666666667,
      "grad_norm": 0.2857162654399872,
      "learning_rate": 6.089583333333334e-06,
      "loss": 0.0014,
      "step": 210770
    },
    {
      "epoch": 7.026,
      "grad_norm": 0.030576298013329506,
      "learning_rate": 6.0875e-06,
      "loss": 0.0016,
      "step": 210780
    },
    {
      "epoch": 7.0263333333333335,
      "grad_norm": 0.08621927350759506,
      "learning_rate": 6.085416666666667e-06,
      "loss": 0.0015,
      "step": 210790
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 0.25657474994659424,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.002,
      "step": 210800
    },
    {
      "epoch": 7.027,
      "grad_norm": 0.31378674507141113,
      "learning_rate": 6.0812500000000004e-06,
      "loss": 0.0018,
      "step": 210810
    },
    {
      "epoch": 7.027333333333333,
      "grad_norm": 0.11455673724412918,
      "learning_rate": 6.079166666666667e-06,
      "loss": 0.0018,
      "step": 210820
    },
    {
      "epoch": 7.027666666666667,
      "grad_norm": 0.4848580062389374,
      "learning_rate": 6.077083333333334e-06,
      "loss": 0.0019,
      "step": 210830
    },
    {
      "epoch": 7.028,
      "grad_norm": 0.3138103783130646,
      "learning_rate": 6.075e-06,
      "loss": 0.0013,
      "step": 210840
    },
    {
      "epoch": 7.028333333333333,
      "grad_norm": 0.2852189540863037,
      "learning_rate": 6.072916666666667e-06,
      "loss": 0.0019,
      "step": 210850
    },
    {
      "epoch": 7.028666666666667,
      "grad_norm": 0.009234381839632988,
      "learning_rate": 6.070833333333334e-06,
      "loss": 0.0022,
      "step": 210860
    },
    {
      "epoch": 7.029,
      "grad_norm": 0.17101801931858063,
      "learning_rate": 6.06875e-06,
      "loss": 0.0016,
      "step": 210870
    },
    {
      "epoch": 7.029333333333334,
      "grad_norm": 0.17592814564704895,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.002,
      "step": 210880
    },
    {
      "epoch": 7.0296666666666665,
      "grad_norm": 0.17101693153381348,
      "learning_rate": 6.064583333333334e-06,
      "loss": 0.0021,
      "step": 210890
    },
    {
      "epoch": 7.03,
      "grad_norm": 0.2926751673221588,
      "learning_rate": 6.0625e-06,
      "loss": 0.0016,
      "step": 210900
    },
    {
      "epoch": 7.030333333333333,
      "grad_norm": 0.08750931173563004,
      "learning_rate": 6.060416666666667e-06,
      "loss": 0.0013,
      "step": 210910
    },
    {
      "epoch": 7.030666666666667,
      "grad_norm": 0.033739566802978516,
      "learning_rate": 6.058333333333334e-06,
      "loss": 0.0015,
      "step": 210920
    },
    {
      "epoch": 7.031,
      "grad_norm": 0.1445656716823578,
      "learning_rate": 6.05625e-06,
      "loss": 0.0019,
      "step": 210930
    },
    {
      "epoch": 7.031333333333333,
      "grad_norm": 0.11446470022201538,
      "learning_rate": 6.054166666666667e-06,
      "loss": 0.0019,
      "step": 210940
    },
    {
      "epoch": 7.031666666666666,
      "grad_norm": 0.08643373101949692,
      "learning_rate": 6.052083333333334e-06,
      "loss": 0.0016,
      "step": 210950
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.19307515025138855,
      "learning_rate": 6.0500000000000005e-06,
      "loss": 0.0013,
      "step": 210960
    },
    {
      "epoch": 7.032333333333334,
      "grad_norm": 0.3997661769390106,
      "learning_rate": 6.047916666666667e-06,
      "loss": 0.0018,
      "step": 210970
    },
    {
      "epoch": 7.032666666666667,
      "grad_norm": 0.1440616101026535,
      "learning_rate": 6.045833333333334e-06,
      "loss": 0.002,
      "step": 210980
    },
    {
      "epoch": 7.033,
      "grad_norm": 0.19070640206336975,
      "learning_rate": 6.04375e-06,
      "loss": 0.002,
      "step": 210990
    },
    {
      "epoch": 7.033333333333333,
      "grad_norm": 0.22849725186824799,
      "learning_rate": 6.041666666666667e-06,
      "loss": 0.0018,
      "step": 211000
    },
    {
      "epoch": 7.033666666666667,
      "grad_norm": 0.00961008109152317,
      "learning_rate": 6.039583333333334e-06,
      "loss": 0.0018,
      "step": 211010
    },
    {
      "epoch": 7.034,
      "grad_norm": 0.08694351464509964,
      "learning_rate": 6.0375e-06,
      "loss": 0.0015,
      "step": 211020
    },
    {
      "epoch": 7.0343333333333335,
      "grad_norm": 0.3913290500640869,
      "learning_rate": 6.035416666666667e-06,
      "loss": 0.0019,
      "step": 211030
    },
    {
      "epoch": 7.034666666666666,
      "grad_norm": 0.08745245635509491,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0014,
      "step": 211040
    },
    {
      "epoch": 7.035,
      "grad_norm": 0.23921260237693787,
      "learning_rate": 6.03125e-06,
      "loss": 0.0015,
      "step": 211050
    },
    {
      "epoch": 7.035333333333333,
      "grad_norm": 0.2851797044277191,
      "learning_rate": 6.029166666666667e-06,
      "loss": 0.0015,
      "step": 211060
    },
    {
      "epoch": 7.035666666666667,
      "grad_norm": 0.031080355867743492,
      "learning_rate": 6.027083333333334e-06,
      "loss": 0.0026,
      "step": 211070
    },
    {
      "epoch": 7.036,
      "grad_norm": 0.3298993706703186,
      "learning_rate": 6.025e-06,
      "loss": 0.0024,
      "step": 211080
    },
    {
      "epoch": 7.036333333333333,
      "grad_norm": 0.48482343554496765,
      "learning_rate": 6.022916666666667e-06,
      "loss": 0.0017,
      "step": 211090
    },
    {
      "epoch": 7.036666666666667,
      "grad_norm": 0.26218733191490173,
      "learning_rate": 6.020833333333334e-06,
      "loss": 0.0022,
      "step": 211100
    },
    {
      "epoch": 7.037,
      "grad_norm": 0.4026108980178833,
      "learning_rate": 6.01875e-06,
      "loss": 0.0017,
      "step": 211110
    },
    {
      "epoch": 7.037333333333334,
      "grad_norm": 0.20015156269073486,
      "learning_rate": 6.016666666666667e-06,
      "loss": 0.0013,
      "step": 211120
    },
    {
      "epoch": 7.0376666666666665,
      "grad_norm": 0.059136368334293365,
      "learning_rate": 6.014583333333334e-06,
      "loss": 0.0012,
      "step": 211130
    },
    {
      "epoch": 7.038,
      "grad_norm": 0.4191473126411438,
      "learning_rate": 6.0125000000000005e-06,
      "loss": 0.0016,
      "step": 211140
    },
    {
      "epoch": 7.038333333333333,
      "grad_norm": 0.4322107434272766,
      "learning_rate": 6.010416666666667e-06,
      "loss": 0.0012,
      "step": 211150
    },
    {
      "epoch": 7.038666666666667,
      "grad_norm": 0.2289898842573166,
      "learning_rate": 6.008333333333334e-06,
      "loss": 0.0019,
      "step": 211160
    },
    {
      "epoch": 7.039,
      "grad_norm": 0.036602683365345,
      "learning_rate": 6.00625e-06,
      "loss": 0.0015,
      "step": 211170
    },
    {
      "epoch": 7.039333333333333,
      "grad_norm": 0.2293901890516281,
      "learning_rate": 6.004166666666667e-06,
      "loss": 0.0017,
      "step": 211180
    },
    {
      "epoch": 7.039666666666666,
      "grad_norm": 0.17298240959644318,
      "learning_rate": 6.002083333333334e-06,
      "loss": 0.0024,
      "step": 211190
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.1427210569381714,
      "learning_rate": 6e-06,
      "loss": 0.0014,
      "step": 211200
    },
    {
      "epoch": 7.040333333333333,
      "grad_norm": 0.11462873220443726,
      "learning_rate": 5.997916666666667e-06,
      "loss": 0.0031,
      "step": 211210
    },
    {
      "epoch": 7.040666666666667,
      "grad_norm": 0.171659916639328,
      "learning_rate": 5.995833333333334e-06,
      "loss": 0.0023,
      "step": 211220
    },
    {
      "epoch": 7.041,
      "grad_norm": 0.3709244728088379,
      "learning_rate": 5.99375e-06,
      "loss": 0.0018,
      "step": 211230
    },
    {
      "epoch": 7.041333333333333,
      "grad_norm": 0.08553184568881989,
      "learning_rate": 5.991666666666667e-06,
      "loss": 0.0022,
      "step": 211240
    },
    {
      "epoch": 7.041666666666667,
      "grad_norm": 0.1430753618478775,
      "learning_rate": 5.9895833333333335e-06,
      "loss": 0.0013,
      "step": 211250
    },
    {
      "epoch": 7.042,
      "grad_norm": 0.05928704887628555,
      "learning_rate": 5.9875e-06,
      "loss": 0.0012,
      "step": 211260
    },
    {
      "epoch": 7.042333333333334,
      "grad_norm": 0.031820423901081085,
      "learning_rate": 5.985416666666667e-06,
      "loss": 0.0018,
      "step": 211270
    },
    {
      "epoch": 7.042666666666666,
      "grad_norm": 0.029273374006152153,
      "learning_rate": 5.983333333333334e-06,
      "loss": 0.0024,
      "step": 211280
    },
    {
      "epoch": 7.043,
      "grad_norm": 0.3140425384044647,
      "learning_rate": 5.98125e-06,
      "loss": 0.0022,
      "step": 211290
    },
    {
      "epoch": 7.043333333333333,
      "grad_norm": 0.08564161509275436,
      "learning_rate": 5.979166666666667e-06,
      "loss": 0.0016,
      "step": 211300
    },
    {
      "epoch": 7.043666666666667,
      "grad_norm": 0.2569514513015747,
      "learning_rate": 5.977083333333334e-06,
      "loss": 0.0014,
      "step": 211310
    },
    {
      "epoch": 7.044,
      "grad_norm": 0.17154425382614136,
      "learning_rate": 5.975e-06,
      "loss": 0.0015,
      "step": 211320
    },
    {
      "epoch": 7.044333333333333,
      "grad_norm": 0.2563965618610382,
      "learning_rate": 5.972916666666667e-06,
      "loss": 0.0014,
      "step": 211330
    },
    {
      "epoch": 7.044666666666667,
      "grad_norm": 0.39923128485679626,
      "learning_rate": 5.970833333333334e-06,
      "loss": 0.0022,
      "step": 211340
    },
    {
      "epoch": 7.045,
      "grad_norm": 0.20645864307880402,
      "learning_rate": 5.96875e-06,
      "loss": 0.002,
      "step": 211350
    },
    {
      "epoch": 7.045333333333334,
      "grad_norm": 0.17099617421627045,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0018,
      "step": 211360
    },
    {
      "epoch": 7.0456666666666665,
      "grad_norm": 0.14554764330387115,
      "learning_rate": 5.964583333333334e-06,
      "loss": 0.0022,
      "step": 211370
    },
    {
      "epoch": 7.046,
      "grad_norm": 0.033554162830114365,
      "learning_rate": 5.9625e-06,
      "loss": 0.0014,
      "step": 211380
    },
    {
      "epoch": 7.046333333333333,
      "grad_norm": 0.08615525811910629,
      "learning_rate": 5.960416666666667e-06,
      "loss": 0.0012,
      "step": 211390
    },
    {
      "epoch": 7.046666666666667,
      "grad_norm": 0.02931058406829834,
      "learning_rate": 5.958333333333334e-06,
      "loss": 0.0014,
      "step": 211400
    },
    {
      "epoch": 7.047,
      "grad_norm": 0.029614964500069618,
      "learning_rate": 5.95625e-06,
      "loss": 0.0011,
      "step": 211410
    },
    {
      "epoch": 7.0473333333333334,
      "grad_norm": 0.20055226981639862,
      "learning_rate": 5.954166666666667e-06,
      "loss": 0.0016,
      "step": 211420
    },
    {
      "epoch": 7.047666666666666,
      "grad_norm": 0.007301908452063799,
      "learning_rate": 5.9520833333333335e-06,
      "loss": 0.0016,
      "step": 211430
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.3715033233165741,
      "learning_rate": 5.95e-06,
      "loss": 0.0022,
      "step": 211440
    },
    {
      "epoch": 7.048333333333333,
      "grad_norm": 0.2002263367176056,
      "learning_rate": 5.947916666666667e-06,
      "loss": 0.0013,
      "step": 211450
    },
    {
      "epoch": 7.048666666666667,
      "grad_norm": 0.03542985022068024,
      "learning_rate": 5.945833333333334e-06,
      "loss": 0.0018,
      "step": 211460
    },
    {
      "epoch": 7.049,
      "grad_norm": 0.17168740928173065,
      "learning_rate": 5.94375e-06,
      "loss": 0.0017,
      "step": 211470
    },
    {
      "epoch": 7.049333333333333,
      "grad_norm": 0.03901514410972595,
      "learning_rate": 5.941666666666667e-06,
      "loss": 0.0018,
      "step": 211480
    },
    {
      "epoch": 7.049666666666667,
      "grad_norm": 0.007546747103333473,
      "learning_rate": 5.939583333333334e-06,
      "loss": 0.0023,
      "step": 211490
    },
    {
      "epoch": 7.05,
      "grad_norm": 0.05825940892100334,
      "learning_rate": 5.9375e-06,
      "loss": 0.0015,
      "step": 211500
    },
    {
      "epoch": 7.050333333333334,
      "grad_norm": 0.08602689206600189,
      "learning_rate": 5.935416666666667e-06,
      "loss": 0.0022,
      "step": 211510
    },
    {
      "epoch": 7.050666666666666,
      "grad_norm": 0.030845921486616135,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0019,
      "step": 211520
    },
    {
      "epoch": 7.051,
      "grad_norm": 0.30578771233558655,
      "learning_rate": 5.93125e-06,
      "loss": 0.0021,
      "step": 211530
    },
    {
      "epoch": 7.051333333333333,
      "grad_norm": 0.1975984424352646,
      "learning_rate": 5.9291666666666665e-06,
      "loss": 0.0018,
      "step": 211540
    },
    {
      "epoch": 7.051666666666667,
      "grad_norm": 0.2568957507610321,
      "learning_rate": 5.927083333333334e-06,
      "loss": 0.0017,
      "step": 211550
    },
    {
      "epoch": 7.052,
      "grad_norm": 0.19983060657978058,
      "learning_rate": 5.925e-06,
      "loss": 0.0012,
      "step": 211560
    },
    {
      "epoch": 7.052333333333333,
      "grad_norm": 0.012373977340757847,
      "learning_rate": 5.922916666666667e-06,
      "loss": 0.0023,
      "step": 211570
    },
    {
      "epoch": 7.052666666666667,
      "grad_norm": 0.11534040421247482,
      "learning_rate": 5.9208333333333335e-06,
      "loss": 0.0018,
      "step": 211580
    },
    {
      "epoch": 7.053,
      "grad_norm": 0.24100326001644135,
      "learning_rate": 5.91875e-06,
      "loss": 0.0016,
      "step": 211590
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 0.057216987013816833,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.0019,
      "step": 211600
    },
    {
      "epoch": 7.0536666666666665,
      "grad_norm": 0.1432936042547226,
      "learning_rate": 5.914583333333333e-06,
      "loss": 0.0013,
      "step": 211610
    },
    {
      "epoch": 7.054,
      "grad_norm": 0.20017248392105103,
      "learning_rate": 5.9125e-06,
      "loss": 0.0026,
      "step": 211620
    },
    {
      "epoch": 7.054333333333333,
      "grad_norm": 0.22856505215168,
      "learning_rate": 5.910416666666667e-06,
      "loss": 0.0018,
      "step": 211630
    },
    {
      "epoch": 7.054666666666667,
      "grad_norm": 0.05745115876197815,
      "learning_rate": 5.908333333333334e-06,
      "loss": 0.0018,
      "step": 211640
    },
    {
      "epoch": 7.055,
      "grad_norm": 0.1711553931236267,
      "learning_rate": 5.9062499999999996e-06,
      "loss": 0.0017,
      "step": 211650
    },
    {
      "epoch": 7.0553333333333335,
      "grad_norm": 0.5099658370018005,
      "learning_rate": 5.904166666666667e-06,
      "loss": 0.0018,
      "step": 211660
    },
    {
      "epoch": 7.055666666666666,
      "grad_norm": 0.25591546297073364,
      "learning_rate": 5.902083333333334e-06,
      "loss": 0.002,
      "step": 211670
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.008220675401389599,
      "learning_rate": 5.9e-06,
      "loss": 0.0014,
      "step": 211680
    },
    {
      "epoch": 7.056333333333333,
      "grad_norm": 0.03036443330347538,
      "learning_rate": 5.897916666666667e-06,
      "loss": 0.0015,
      "step": 211690
    },
    {
      "epoch": 7.056666666666667,
      "grad_norm": 0.02912292256951332,
      "learning_rate": 5.895833333333334e-06,
      "loss": 0.0011,
      "step": 211700
    },
    {
      "epoch": 7.057,
      "grad_norm": 0.005986365489661694,
      "learning_rate": 5.89375e-06,
      "loss": 0.0021,
      "step": 211710
    },
    {
      "epoch": 7.057333333333333,
      "grad_norm": 0.08666803687810898,
      "learning_rate": 5.8916666666666664e-06,
      "loss": 0.002,
      "step": 211720
    },
    {
      "epoch": 7.057666666666667,
      "grad_norm": 0.4768401086330414,
      "learning_rate": 5.889583333333334e-06,
      "loss": 0.0016,
      "step": 211730
    },
    {
      "epoch": 7.058,
      "grad_norm": 0.2011239379644394,
      "learning_rate": 5.8875e-06,
      "loss": 0.0021,
      "step": 211740
    },
    {
      "epoch": 7.058333333333334,
      "grad_norm": 0.11505748331546783,
      "learning_rate": 5.885416666666667e-06,
      "loss": 0.0024,
      "step": 211750
    },
    {
      "epoch": 7.058666666666666,
      "grad_norm": 0.14295409619808197,
      "learning_rate": 5.8833333333333335e-06,
      "loss": 0.0014,
      "step": 211760
    },
    {
      "epoch": 7.059,
      "grad_norm": 0.17173515260219574,
      "learning_rate": 5.88125e-06,
      "loss": 0.0017,
      "step": 211770
    },
    {
      "epoch": 7.059333333333333,
      "grad_norm": 0.1717078685760498,
      "learning_rate": 5.879166666666667e-06,
      "loss": 0.0015,
      "step": 211780
    },
    {
      "epoch": 7.059666666666667,
      "grad_norm": 0.5135332942008972,
      "learning_rate": 5.877083333333334e-06,
      "loss": 0.0027,
      "step": 211790
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.11482792347669601,
      "learning_rate": 5.875e-06,
      "loss": 0.0018,
      "step": 211800
    },
    {
      "epoch": 7.060333333333333,
      "grad_norm": 0.4636518955230713,
      "learning_rate": 5.872916666666667e-06,
      "loss": 0.0022,
      "step": 211810
    },
    {
      "epoch": 7.060666666666667,
      "grad_norm": 0.27263638377189636,
      "learning_rate": 5.870833333333334e-06,
      "loss": 0.0013,
      "step": 211820
    },
    {
      "epoch": 7.061,
      "grad_norm": 0.11614907532930374,
      "learning_rate": 5.86875e-06,
      "loss": 0.0015,
      "step": 211830
    },
    {
      "epoch": 7.061333333333334,
      "grad_norm": 0.015668746083974838,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0018,
      "step": 211840
    },
    {
      "epoch": 7.0616666666666665,
      "grad_norm": 0.2018107771873474,
      "learning_rate": 5.864583333333334e-06,
      "loss": 0.0018,
      "step": 211850
    },
    {
      "epoch": 7.062,
      "grad_norm": 0.030128303915262222,
      "learning_rate": 5.8625e-06,
      "loss": 0.0017,
      "step": 211860
    },
    {
      "epoch": 7.062333333333333,
      "grad_norm": 0.2132190465927124,
      "learning_rate": 5.8604166666666665e-06,
      "loss": 0.0017,
      "step": 211870
    },
    {
      "epoch": 7.062666666666667,
      "grad_norm": 0.142537921667099,
      "learning_rate": 5.858333333333334e-06,
      "loss": 0.0014,
      "step": 211880
    },
    {
      "epoch": 7.063,
      "grad_norm": 0.3425065577030182,
      "learning_rate": 5.856250000000001e-06,
      "loss": 0.0018,
      "step": 211890
    },
    {
      "epoch": 7.0633333333333335,
      "grad_norm": 0.21083679795265198,
      "learning_rate": 5.854166666666667e-06,
      "loss": 0.0026,
      "step": 211900
    },
    {
      "epoch": 7.063666666666666,
      "grad_norm": 0.09129512310028076,
      "learning_rate": 5.8520833333333336e-06,
      "loss": 0.0022,
      "step": 211910
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.11436653137207031,
      "learning_rate": 5.850000000000001e-06,
      "loss": 0.0012,
      "step": 211920
    },
    {
      "epoch": 7.064333333333333,
      "grad_norm": 0.059596993029117584,
      "learning_rate": 5.847916666666667e-06,
      "loss": 0.0023,
      "step": 211930
    },
    {
      "epoch": 7.064666666666667,
      "grad_norm": 0.3383067846298218,
      "learning_rate": 5.845833333333333e-06,
      "loss": 0.0016,
      "step": 211940
    },
    {
      "epoch": 7.065,
      "grad_norm": 0.20096784830093384,
      "learning_rate": 5.843750000000001e-06,
      "loss": 0.0027,
      "step": 211950
    },
    {
      "epoch": 7.065333333333333,
      "grad_norm": 0.14367814362049103,
      "learning_rate": 5.841666666666667e-06,
      "loss": 0.0019,
      "step": 211960
    },
    {
      "epoch": 7.065666666666667,
      "grad_norm": 0.058568425476551056,
      "learning_rate": 5.839583333333334e-06,
      "loss": 0.0022,
      "step": 211970
    },
    {
      "epoch": 7.066,
      "grad_norm": 0.057437777519226074,
      "learning_rate": 5.8375000000000004e-06,
      "loss": 0.0014,
      "step": 211980
    },
    {
      "epoch": 7.066333333333334,
      "grad_norm": 0.24816343188285828,
      "learning_rate": 5.835416666666667e-06,
      "loss": 0.0017,
      "step": 211990
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.05782473459839821,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0024,
      "step": 212000
    },
    {
      "epoch": 7.067,
      "grad_norm": 0.31671905517578125,
      "learning_rate": 5.83125e-06,
      "loss": 0.0014,
      "step": 212010
    },
    {
      "epoch": 7.067333333333333,
      "grad_norm": 0.02325969561934471,
      "learning_rate": 5.829166666666667e-06,
      "loss": 0.0012,
      "step": 212020
    },
    {
      "epoch": 7.067666666666667,
      "grad_norm": 0.47204506397247314,
      "learning_rate": 5.827083333333334e-06,
      "loss": 0.0017,
      "step": 212030
    },
    {
      "epoch": 7.068,
      "grad_norm": 0.05736064538359642,
      "learning_rate": 5.825000000000001e-06,
      "loss": 0.0021,
      "step": 212040
    },
    {
      "epoch": 7.068333333333333,
      "grad_norm": 0.012800254859030247,
      "learning_rate": 5.8229166666666665e-06,
      "loss": 0.0016,
      "step": 212050
    },
    {
      "epoch": 7.068666666666667,
      "grad_norm": 0.22879955172538757,
      "learning_rate": 5.820833333333334e-06,
      "loss": 0.0019,
      "step": 212060
    },
    {
      "epoch": 7.069,
      "grad_norm": 0.029619894921779633,
      "learning_rate": 5.818750000000001e-06,
      "loss": 0.0018,
      "step": 212070
    },
    {
      "epoch": 7.069333333333334,
      "grad_norm": 0.36476609110832214,
      "learning_rate": 5.816666666666667e-06,
      "loss": 0.0014,
      "step": 212080
    },
    {
      "epoch": 7.0696666666666665,
      "grad_norm": 0.34311443567276,
      "learning_rate": 5.8145833333333335e-06,
      "loss": 0.0019,
      "step": 212090
    },
    {
      "epoch": 7.07,
      "grad_norm": 0.09225962311029434,
      "learning_rate": 5.812500000000001e-06,
      "loss": 0.0014,
      "step": 212100
    },
    {
      "epoch": 7.070333333333333,
      "grad_norm": 0.02949279174208641,
      "learning_rate": 5.810416666666667e-06,
      "loss": 0.0023,
      "step": 212110
    },
    {
      "epoch": 7.070666666666667,
      "grad_norm": 0.007710292469710112,
      "learning_rate": 5.808333333333333e-06,
      "loss": 0.0027,
      "step": 212120
    },
    {
      "epoch": 7.071,
      "grad_norm": 0.08617891371250153,
      "learning_rate": 5.8062500000000005e-06,
      "loss": 0.0021,
      "step": 212130
    },
    {
      "epoch": 7.0713333333333335,
      "grad_norm": 0.08769138157367706,
      "learning_rate": 5.804166666666667e-06,
      "loss": 0.0017,
      "step": 212140
    },
    {
      "epoch": 7.071666666666666,
      "grad_norm": 0.31403079628944397,
      "learning_rate": 5.802083333333334e-06,
      "loss": 0.0019,
      "step": 212150
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.14270296692848206,
      "learning_rate": 5.8e-06,
      "loss": 0.0018,
      "step": 212160
    },
    {
      "epoch": 7.072333333333333,
      "grad_norm": 0.37160399556159973,
      "learning_rate": 5.797916666666667e-06,
      "loss": 0.0016,
      "step": 212170
    },
    {
      "epoch": 7.072666666666667,
      "grad_norm": 0.18773335218429565,
      "learning_rate": 5.795833333333334e-06,
      "loss": 0.0018,
      "step": 212180
    },
    {
      "epoch": 7.073,
      "grad_norm": 0.031062455847859383,
      "learning_rate": 5.79375e-06,
      "loss": 0.0016,
      "step": 212190
    },
    {
      "epoch": 7.073333333333333,
      "grad_norm": 0.14243678748607635,
      "learning_rate": 5.7916666666666666e-06,
      "loss": 0.0015,
      "step": 212200
    },
    {
      "epoch": 7.073666666666667,
      "grad_norm": 0.572338342666626,
      "learning_rate": 5.789583333333334e-06,
      "loss": 0.0012,
      "step": 212210
    },
    {
      "epoch": 7.074,
      "grad_norm": 0.25677767395973206,
      "learning_rate": 5.787500000000001e-06,
      "loss": 0.0015,
      "step": 212220
    },
    {
      "epoch": 7.074333333333334,
      "grad_norm": 0.3705202341079712,
      "learning_rate": 5.785416666666666e-06,
      "loss": 0.0014,
      "step": 212230
    },
    {
      "epoch": 7.074666666666666,
      "grad_norm": 0.008920480497181416,
      "learning_rate": 5.783333333333334e-06,
      "loss": 0.0013,
      "step": 212240
    },
    {
      "epoch": 7.075,
      "grad_norm": 0.11488085985183716,
      "learning_rate": 5.781250000000001e-06,
      "loss": 0.0016,
      "step": 212250
    },
    {
      "epoch": 7.075333333333333,
      "grad_norm": 0.3140750527381897,
      "learning_rate": 5.779166666666667e-06,
      "loss": 0.0018,
      "step": 212260
    },
    {
      "epoch": 7.075666666666667,
      "grad_norm": 0.010656910948455334,
      "learning_rate": 5.7770833333333334e-06,
      "loss": 0.0012,
      "step": 212270
    },
    {
      "epoch": 7.076,
      "grad_norm": 0.08590280264616013,
      "learning_rate": 5.775000000000001e-06,
      "loss": 0.0015,
      "step": 212280
    },
    {
      "epoch": 7.076333333333333,
      "grad_norm": 0.3134727478027344,
      "learning_rate": 5.772916666666667e-06,
      "loss": 0.0015,
      "step": 212290
    },
    {
      "epoch": 7.076666666666666,
      "grad_norm": 0.5433347821235657,
      "learning_rate": 5.770833333333333e-06,
      "loss": 0.0017,
      "step": 212300
    },
    {
      "epoch": 7.077,
      "grad_norm": 0.03547823056578636,
      "learning_rate": 5.7687500000000005e-06,
      "loss": 0.0019,
      "step": 212310
    },
    {
      "epoch": 7.077333333333334,
      "grad_norm": 0.22801534831523895,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0013,
      "step": 212320
    },
    {
      "epoch": 7.0776666666666666,
      "grad_norm": 0.029249751940369606,
      "learning_rate": 5.764583333333334e-06,
      "loss": 0.0017,
      "step": 212330
    },
    {
      "epoch": 7.078,
      "grad_norm": 0.058092884719371796,
      "learning_rate": 5.7625e-06,
      "loss": 0.0024,
      "step": 212340
    },
    {
      "epoch": 7.078333333333333,
      "grad_norm": 0.03318386897444725,
      "learning_rate": 5.760416666666667e-06,
      "loss": 0.0015,
      "step": 212350
    },
    {
      "epoch": 7.078666666666667,
      "grad_norm": 0.22879478335380554,
      "learning_rate": 5.758333333333334e-06,
      "loss": 0.0016,
      "step": 212360
    },
    {
      "epoch": 7.079,
      "grad_norm": 0.14308813214302063,
      "learning_rate": 5.75625e-06,
      "loss": 0.0015,
      "step": 212370
    },
    {
      "epoch": 7.0793333333333335,
      "grad_norm": 0.1156555563211441,
      "learning_rate": 5.7541666666666665e-06,
      "loss": 0.0017,
      "step": 212380
    },
    {
      "epoch": 7.079666666666666,
      "grad_norm": 0.2198580652475357,
      "learning_rate": 5.752083333333334e-06,
      "loss": 0.0017,
      "step": 212390
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.03408433496952057,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.0022,
      "step": 212400
    },
    {
      "epoch": 7.080333333333333,
      "grad_norm": 0.006146300584077835,
      "learning_rate": 5.747916666666666e-06,
      "loss": 0.0015,
      "step": 212410
    },
    {
      "epoch": 7.080666666666667,
      "grad_norm": 0.11497090756893158,
      "learning_rate": 5.7458333333333335e-06,
      "loss": 0.0018,
      "step": 212420
    },
    {
      "epoch": 7.081,
      "grad_norm": 0.3139428198337555,
      "learning_rate": 5.743750000000001e-06,
      "loss": 0.0016,
      "step": 212430
    },
    {
      "epoch": 7.081333333333333,
      "grad_norm": 0.22832807898521423,
      "learning_rate": 5.741666666666667e-06,
      "loss": 0.0016,
      "step": 212440
    },
    {
      "epoch": 7.081666666666667,
      "grad_norm": 0.25722599029541016,
      "learning_rate": 5.739583333333333e-06,
      "loss": 0.0012,
      "step": 212450
    },
    {
      "epoch": 7.082,
      "grad_norm": 0.08634307235479355,
      "learning_rate": 5.7375000000000005e-06,
      "loss": 0.002,
      "step": 212460
    },
    {
      "epoch": 7.082333333333334,
      "grad_norm": 0.0866592526435852,
      "learning_rate": 5.735416666666667e-06,
      "loss": 0.0017,
      "step": 212470
    },
    {
      "epoch": 7.082666666666666,
      "grad_norm": 0.22813020646572113,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0017,
      "step": 212480
    },
    {
      "epoch": 7.083,
      "grad_norm": 0.2131727933883667,
      "learning_rate": 5.73125e-06,
      "loss": 0.0015,
      "step": 212490
    },
    {
      "epoch": 7.083333333333333,
      "grad_norm": 0.2564726173877716,
      "learning_rate": 5.729166666666667e-06,
      "loss": 0.002,
      "step": 212500
    },
    {
      "epoch": 7.083666666666667,
      "grad_norm": 0.03226246312260628,
      "learning_rate": 5.727083333333334e-06,
      "loss": 0.0014,
      "step": 212510
    },
    {
      "epoch": 7.084,
      "grad_norm": 0.29388749599456787,
      "learning_rate": 5.725e-06,
      "loss": 0.002,
      "step": 212520
    },
    {
      "epoch": 7.084333333333333,
      "grad_norm": 0.39932504296302795,
      "learning_rate": 5.722916666666667e-06,
      "loss": 0.0013,
      "step": 212530
    },
    {
      "epoch": 7.084666666666667,
      "grad_norm": 0.34241050481796265,
      "learning_rate": 5.720833333333334e-06,
      "loss": 0.0017,
      "step": 212540
    },
    {
      "epoch": 7.085,
      "grad_norm": 0.14235007762908936,
      "learning_rate": 5.71875e-06,
      "loss": 0.0013,
      "step": 212550
    },
    {
      "epoch": 7.085333333333334,
      "grad_norm": 0.25648048520088196,
      "learning_rate": 5.7166666666666664e-06,
      "loss": 0.0015,
      "step": 212560
    },
    {
      "epoch": 7.085666666666667,
      "grad_norm": 0.44623392820358276,
      "learning_rate": 5.714583333333334e-06,
      "loss": 0.0015,
      "step": 212570
    },
    {
      "epoch": 7.086,
      "grad_norm": 0.5136703848838806,
      "learning_rate": 5.712500000000001e-06,
      "loss": 0.0016,
      "step": 212580
    },
    {
      "epoch": 7.086333333333333,
      "grad_norm": 0.011460410431027412,
      "learning_rate": 5.710416666666666e-06,
      "loss": 0.0014,
      "step": 212590
    },
    {
      "epoch": 7.086666666666667,
      "grad_norm": 0.31429195404052734,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 0.0013,
      "step": 212600
    },
    {
      "epoch": 7.087,
      "grad_norm": 0.25649911165237427,
      "learning_rate": 5.706250000000001e-06,
      "loss": 0.0016,
      "step": 212610
    },
    {
      "epoch": 7.0873333333333335,
      "grad_norm": 0.05746331438422203,
      "learning_rate": 5.704166666666667e-06,
      "loss": 0.0014,
      "step": 212620
    },
    {
      "epoch": 7.087666666666666,
      "grad_norm": 0.28513303399086,
      "learning_rate": 5.702083333333333e-06,
      "loss": 0.0022,
      "step": 212630
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.20035938918590546,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0017,
      "step": 212640
    },
    {
      "epoch": 7.088333333333333,
      "grad_norm": 0.08568819612264633,
      "learning_rate": 5.697916666666667e-06,
      "loss": 0.0025,
      "step": 212650
    },
    {
      "epoch": 7.088666666666667,
      "grad_norm": 0.1430777758359909,
      "learning_rate": 5.695833333333334e-06,
      "loss": 0.0021,
      "step": 212660
    },
    {
      "epoch": 7.089,
      "grad_norm": 0.17137449979782104,
      "learning_rate": 5.69375e-06,
      "loss": 0.0015,
      "step": 212670
    },
    {
      "epoch": 7.089333333333333,
      "grad_norm": 0.31435427069664,
      "learning_rate": 5.691666666666667e-06,
      "loss": 0.0013,
      "step": 212680
    },
    {
      "epoch": 7.089666666666667,
      "grad_norm": 0.336824893951416,
      "learning_rate": 5.689583333333334e-06,
      "loss": 0.0016,
      "step": 212690
    },
    {
      "epoch": 7.09,
      "grad_norm": 0.18946582078933716,
      "learning_rate": 5.6875e-06,
      "loss": 0.0021,
      "step": 212700
    },
    {
      "epoch": 7.090333333333334,
      "grad_norm": 0.057374726980924606,
      "learning_rate": 5.6854166666666665e-06,
      "loss": 0.0015,
      "step": 212710
    },
    {
      "epoch": 7.0906666666666665,
      "grad_norm": 0.0867200493812561,
      "learning_rate": 5.683333333333334e-06,
      "loss": 0.0018,
      "step": 212720
    },
    {
      "epoch": 7.091,
      "grad_norm": 0.2568502128124237,
      "learning_rate": 5.681250000000001e-06,
      "loss": 0.0018,
      "step": 212730
    },
    {
      "epoch": 7.091333333333333,
      "grad_norm": 0.11536173522472382,
      "learning_rate": 5.679166666666666e-06,
      "loss": 0.0011,
      "step": 212740
    },
    {
      "epoch": 7.091666666666667,
      "grad_norm": 0.08703983575105667,
      "learning_rate": 5.6770833333333335e-06,
      "loss": 0.0019,
      "step": 212750
    },
    {
      "epoch": 7.092,
      "grad_norm": 0.029981544241309166,
      "learning_rate": 5.675000000000001e-06,
      "loss": 0.0017,
      "step": 212760
    },
    {
      "epoch": 7.092333333333333,
      "grad_norm": 0.17114360630512238,
      "learning_rate": 5.672916666666667e-06,
      "loss": 0.0015,
      "step": 212770
    },
    {
      "epoch": 7.092666666666666,
      "grad_norm": 0.1713220477104187,
      "learning_rate": 5.670833333333333e-06,
      "loss": 0.0012,
      "step": 212780
    },
    {
      "epoch": 7.093,
      "grad_norm": 0.22917033731937408,
      "learning_rate": 5.6687500000000006e-06,
      "loss": 0.0018,
      "step": 212790
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.20080170035362244,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.002,
      "step": 212800
    },
    {
      "epoch": 7.093666666666667,
      "grad_norm": 0.08575727045536041,
      "learning_rate": 5.664583333333333e-06,
      "loss": 0.0012,
      "step": 212810
    },
    {
      "epoch": 7.094,
      "grad_norm": 0.11447323858737946,
      "learning_rate": 5.6625e-06,
      "loss": 0.0016,
      "step": 212820
    },
    {
      "epoch": 7.094333333333333,
      "grad_norm": 0.17334726452827454,
      "learning_rate": 5.660416666666667e-06,
      "loss": 0.0016,
      "step": 212830
    },
    {
      "epoch": 7.094666666666667,
      "grad_norm": 0.08650773018598557,
      "learning_rate": 5.658333333333334e-06,
      "loss": 0.0017,
      "step": 212840
    },
    {
      "epoch": 7.095,
      "grad_norm": 0.2569291293621063,
      "learning_rate": 5.65625e-06,
      "loss": 0.0016,
      "step": 212850
    },
    {
      "epoch": 7.0953333333333335,
      "grad_norm": 0.17151476442813873,
      "learning_rate": 5.654166666666667e-06,
      "loss": 0.002,
      "step": 212860
    },
    {
      "epoch": 7.095666666666666,
      "grad_norm": 0.05198582634329796,
      "learning_rate": 5.652083333333334e-06,
      "loss": 0.0016,
      "step": 212870
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.08712215721607208,
      "learning_rate": 5.65e-06,
      "loss": 0.0016,
      "step": 212880
    },
    {
      "epoch": 7.096333333333333,
      "grad_norm": 0.38564470410346985,
      "learning_rate": 5.6479166666666664e-06,
      "loss": 0.002,
      "step": 212890
    },
    {
      "epoch": 7.096666666666667,
      "grad_norm": 0.029259391129016876,
      "learning_rate": 5.645833333333334e-06,
      "loss": 0.0024,
      "step": 212900
    },
    {
      "epoch": 7.097,
      "grad_norm": 0.07412653416395187,
      "learning_rate": 5.643750000000001e-06,
      "loss": 0.0017,
      "step": 212910
    },
    {
      "epoch": 7.097333333333333,
      "grad_norm": 0.031080100685358047,
      "learning_rate": 5.641666666666666e-06,
      "loss": 0.0015,
      "step": 212920
    },
    {
      "epoch": 7.097666666666667,
      "grad_norm": 0.08820383995771408,
      "learning_rate": 5.6395833333333335e-06,
      "loss": 0.0019,
      "step": 212930
    },
    {
      "epoch": 7.098,
      "grad_norm": 0.1724102646112442,
      "learning_rate": 5.637500000000001e-06,
      "loss": 0.0013,
      "step": 212940
    },
    {
      "epoch": 7.098333333333334,
      "grad_norm": 0.3422801196575165,
      "learning_rate": 5.635416666666667e-06,
      "loss": 0.0019,
      "step": 212950
    },
    {
      "epoch": 7.0986666666666665,
      "grad_norm": 0.22846391797065735,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0033,
      "step": 212960
    },
    {
      "epoch": 7.099,
      "grad_norm": 0.058023687452077866,
      "learning_rate": 5.6312500000000005e-06,
      "loss": 0.0019,
      "step": 212970
    },
    {
      "epoch": 7.099333333333333,
      "grad_norm": 0.3996434509754181,
      "learning_rate": 5.629166666666667e-06,
      "loss": 0.002,
      "step": 212980
    },
    {
      "epoch": 7.099666666666667,
      "grad_norm": 0.12291030585765839,
      "learning_rate": 5.627083333333333e-06,
      "loss": 0.0016,
      "step": 212990
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.0574931763112545,
      "learning_rate": 5.625e-06,
      "loss": 0.0014,
      "step": 213000
    },
    {
      "epoch": 7.100333333333333,
      "grad_norm": 0.30236759781837463,
      "learning_rate": 5.622916666666667e-06,
      "loss": 0.0016,
      "step": 213010
    },
    {
      "epoch": 7.100666666666666,
      "grad_norm": 0.2871769070625305,
      "learning_rate": 5.620833333333334e-06,
      "loss": 0.0021,
      "step": 213020
    },
    {
      "epoch": 7.101,
      "grad_norm": 0.20342856645584106,
      "learning_rate": 5.61875e-06,
      "loss": 0.0015,
      "step": 213030
    },
    {
      "epoch": 7.101333333333334,
      "grad_norm": 0.05824141949415207,
      "learning_rate": 5.6166666666666665e-06,
      "loss": 0.0017,
      "step": 213040
    },
    {
      "epoch": 7.101666666666667,
      "grad_norm": 0.3999462425708771,
      "learning_rate": 5.614583333333334e-06,
      "loss": 0.0017,
      "step": 213050
    },
    {
      "epoch": 7.102,
      "grad_norm": 0.4566824734210968,
      "learning_rate": 5.6125e-06,
      "loss": 0.0015,
      "step": 213060
    },
    {
      "epoch": 7.102333333333333,
      "grad_norm": 0.15376585721969604,
      "learning_rate": 5.610416666666666e-06,
      "loss": 0.0023,
      "step": 213070
    },
    {
      "epoch": 7.102666666666667,
      "grad_norm": 0.12233537435531616,
      "learning_rate": 5.6083333333333336e-06,
      "loss": 0.0018,
      "step": 213080
    },
    {
      "epoch": 7.103,
      "grad_norm": 0.11508060246706009,
      "learning_rate": 5.606250000000001e-06,
      "loss": 0.002,
      "step": 213090
    },
    {
      "epoch": 7.1033333333333335,
      "grad_norm": 0.17118243873119354,
      "learning_rate": 5.604166666666666e-06,
      "loss": 0.0014,
      "step": 213100
    },
    {
      "epoch": 7.103666666666666,
      "grad_norm": 0.18255364894866943,
      "learning_rate": 5.602083333333333e-06,
      "loss": 0.0019,
      "step": 213110
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.03062627650797367,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0014,
      "step": 213120
    },
    {
      "epoch": 7.104333333333333,
      "grad_norm": 0.0551246702671051,
      "learning_rate": 5.597916666666667e-06,
      "loss": 0.002,
      "step": 213130
    },
    {
      "epoch": 7.104666666666667,
      "grad_norm": 0.11502379179000854,
      "learning_rate": 5.595833333333333e-06,
      "loss": 0.0018,
      "step": 213140
    },
    {
      "epoch": 7.105,
      "grad_norm": 0.23054055869579315,
      "learning_rate": 5.5937500000000004e-06,
      "loss": 0.0014,
      "step": 213150
    },
    {
      "epoch": 7.105333333333333,
      "grad_norm": 0.058163631707429886,
      "learning_rate": 5.591666666666668e-06,
      "loss": 0.0014,
      "step": 213160
    },
    {
      "epoch": 7.105666666666667,
      "grad_norm": 0.05934784561395645,
      "learning_rate": 5.589583333333333e-06,
      "loss": 0.0013,
      "step": 213170
    },
    {
      "epoch": 7.106,
      "grad_norm": 0.05762232095003128,
      "learning_rate": 5.5875e-06,
      "loss": 0.0014,
      "step": 213180
    },
    {
      "epoch": 7.106333333333334,
      "grad_norm": 0.28519338369369507,
      "learning_rate": 5.5854166666666675e-06,
      "loss": 0.0014,
      "step": 213190
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 0.22902293503284454,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.0024,
      "step": 213200
    },
    {
      "epoch": 7.107,
      "grad_norm": 0.23040522634983063,
      "learning_rate": 5.58125e-06,
      "loss": 0.0024,
      "step": 213210
    },
    {
      "epoch": 7.107333333333333,
      "grad_norm": 0.21151193976402283,
      "learning_rate": 5.579166666666667e-06,
      "loss": 0.0017,
      "step": 213220
    },
    {
      "epoch": 7.107666666666667,
      "grad_norm": 0.3726148307323456,
      "learning_rate": 5.577083333333334e-06,
      "loss": 0.0016,
      "step": 213230
    },
    {
      "epoch": 7.108,
      "grad_norm": 0.05213291570544243,
      "learning_rate": 5.575e-06,
      "loss": 0.002,
      "step": 213240
    },
    {
      "epoch": 7.108333333333333,
      "grad_norm": 0.34269431233406067,
      "learning_rate": 5.572916666666667e-06,
      "loss": 0.0018,
      "step": 213250
    },
    {
      "epoch": 7.108666666666666,
      "grad_norm": 0.3717646896839142,
      "learning_rate": 5.5708333333333335e-06,
      "loss": 0.0016,
      "step": 213260
    },
    {
      "epoch": 7.109,
      "grad_norm": 0.1713869869709015,
      "learning_rate": 5.568750000000001e-06,
      "loss": 0.0016,
      "step": 213270
    },
    {
      "epoch": 7.109333333333334,
      "grad_norm": 0.029435545206069946,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0017,
      "step": 213280
    },
    {
      "epoch": 7.109666666666667,
      "grad_norm": 0.11515630036592484,
      "learning_rate": 5.564583333333333e-06,
      "loss": 0.0022,
      "step": 213290
    },
    {
      "epoch": 7.11,
      "grad_norm": 0.029987284913659096,
      "learning_rate": 5.5625000000000005e-06,
      "loss": 0.0017,
      "step": 213300
    },
    {
      "epoch": 7.110333333333333,
      "grad_norm": 0.17138661444187164,
      "learning_rate": 5.560416666666667e-06,
      "loss": 0.0015,
      "step": 213310
    },
    {
      "epoch": 7.110666666666667,
      "grad_norm": 0.011095891706645489,
      "learning_rate": 5.558333333333333e-06,
      "loss": 0.0015,
      "step": 213320
    },
    {
      "epoch": 7.111,
      "grad_norm": 0.2842473089694977,
      "learning_rate": 5.55625e-06,
      "loss": 0.0018,
      "step": 213330
    },
    {
      "epoch": 7.1113333333333335,
      "grad_norm": 0.08561316132545471,
      "learning_rate": 5.5541666666666676e-06,
      "loss": 0.0013,
      "step": 213340
    },
    {
      "epoch": 7.111666666666666,
      "grad_norm": 0.22881443798542023,
      "learning_rate": 5.552083333333333e-06,
      "loss": 0.0014,
      "step": 213350
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.08715132623910904,
      "learning_rate": 5.55e-06,
      "loss": 0.0019,
      "step": 213360
    },
    {
      "epoch": 7.112333333333333,
      "grad_norm": 0.06074689328670502,
      "learning_rate": 5.547916666666667e-06,
      "loss": 0.0018,
      "step": 213370
    },
    {
      "epoch": 7.112666666666667,
      "grad_norm": 0.01090040523558855,
      "learning_rate": 5.545833333333334e-06,
      "loss": 0.0022,
      "step": 213380
    },
    {
      "epoch": 7.113,
      "grad_norm": 0.01055149082094431,
      "learning_rate": 5.54375e-06,
      "loss": 0.0019,
      "step": 213390
    },
    {
      "epoch": 7.113333333333333,
      "grad_norm": 0.09396897256374359,
      "learning_rate": 5.541666666666667e-06,
      "loss": 0.0024,
      "step": 213400
    },
    {
      "epoch": 7.113666666666667,
      "grad_norm": 0.22809453308582306,
      "learning_rate": 5.539583333333334e-06,
      "loss": 0.0016,
      "step": 213410
    },
    {
      "epoch": 7.114,
      "grad_norm": 0.2291971743106842,
      "learning_rate": 5.5375e-06,
      "loss": 0.0016,
      "step": 213420
    },
    {
      "epoch": 7.114333333333334,
      "grad_norm": 0.17133839428424835,
      "learning_rate": 5.535416666666667e-06,
      "loss": 0.0012,
      "step": 213430
    },
    {
      "epoch": 7.1146666666666665,
      "grad_norm": 0.28514617681503296,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.002,
      "step": 213440
    },
    {
      "epoch": 7.115,
      "grad_norm": 0.16946899890899658,
      "learning_rate": 5.531250000000001e-06,
      "loss": 0.0019,
      "step": 213450
    },
    {
      "epoch": 7.115333333333333,
      "grad_norm": 0.19967223703861237,
      "learning_rate": 5.529166666666667e-06,
      "loss": 0.0017,
      "step": 213460
    },
    {
      "epoch": 7.115666666666667,
      "grad_norm": 0.22881074249744415,
      "learning_rate": 5.527083333333333e-06,
      "loss": 0.0012,
      "step": 213470
    },
    {
      "epoch": 7.116,
      "grad_norm": 0.11488647758960724,
      "learning_rate": 5.5250000000000005e-06,
      "loss": 0.0013,
      "step": 213480
    },
    {
      "epoch": 7.116333333333333,
      "grad_norm": 0.25695857405662537,
      "learning_rate": 5.522916666666667e-06,
      "loss": 0.0014,
      "step": 213490
    },
    {
      "epoch": 7.116666666666666,
      "grad_norm": 0.3424355089664459,
      "learning_rate": 5.520833333333333e-06,
      "loss": 0.0019,
      "step": 213500
    },
    {
      "epoch": 7.117,
      "grad_norm": 0.39406153559684753,
      "learning_rate": 5.51875e-06,
      "loss": 0.0015,
      "step": 213510
    },
    {
      "epoch": 7.117333333333334,
      "grad_norm": 0.06163076311349869,
      "learning_rate": 5.5166666666666675e-06,
      "loss": 0.0014,
      "step": 213520
    },
    {
      "epoch": 7.117666666666667,
      "grad_norm": 0.10061731189489365,
      "learning_rate": 5.514583333333334e-06,
      "loss": 0.0014,
      "step": 213530
    },
    {
      "epoch": 7.118,
      "grad_norm": 0.08574594557285309,
      "learning_rate": 5.5125e-06,
      "loss": 0.0022,
      "step": 213540
    },
    {
      "epoch": 7.118333333333333,
      "grad_norm": 0.12202572077512741,
      "learning_rate": 5.510416666666667e-06,
      "loss": 0.0012,
      "step": 213550
    },
    {
      "epoch": 7.118666666666667,
      "grad_norm": 0.058195196092128754,
      "learning_rate": 5.508333333333334e-06,
      "loss": 0.0017,
      "step": 213560
    },
    {
      "epoch": 7.119,
      "grad_norm": 0.03209296241402626,
      "learning_rate": 5.50625e-06,
      "loss": 0.0013,
      "step": 213570
    },
    {
      "epoch": 7.1193333333333335,
      "grad_norm": 0.1139235720038414,
      "learning_rate": 5.504166666666667e-06,
      "loss": 0.0024,
      "step": 213580
    },
    {
      "epoch": 7.119666666666666,
      "grad_norm": 0.04628386348485947,
      "learning_rate": 5.5020833333333335e-06,
      "loss": 0.0018,
      "step": 213590
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.4278927147388458,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.0015,
      "step": 213600
    },
    {
      "epoch": 7.120333333333333,
      "grad_norm": 0.37075066566467285,
      "learning_rate": 5.497916666666667e-06,
      "loss": 0.002,
      "step": 213610
    },
    {
      "epoch": 7.120666666666667,
      "grad_norm": 0.1438589096069336,
      "learning_rate": 5.495833333333333e-06,
      "loss": 0.0016,
      "step": 213620
    },
    {
      "epoch": 7.121,
      "grad_norm": 0.030328020453453064,
      "learning_rate": 5.4937500000000006e-06,
      "loss": 0.0018,
      "step": 213630
    },
    {
      "epoch": 7.121333333333333,
      "grad_norm": 0.08711735159158707,
      "learning_rate": 5.491666666666667e-06,
      "loss": 0.0012,
      "step": 213640
    },
    {
      "epoch": 7.121666666666667,
      "grad_norm": 0.017541706562042236,
      "learning_rate": 5.489583333333333e-06,
      "loss": 0.0017,
      "step": 213650
    },
    {
      "epoch": 7.122,
      "grad_norm": 0.2850954830646515,
      "learning_rate": 5.4875e-06,
      "loss": 0.0019,
      "step": 213660
    },
    {
      "epoch": 7.122333333333334,
      "grad_norm": 0.08639159053564072,
      "learning_rate": 5.485416666666668e-06,
      "loss": 0.0027,
      "step": 213670
    },
    {
      "epoch": 7.1226666666666665,
      "grad_norm": 0.029282519593834877,
      "learning_rate": 5.483333333333333e-06,
      "loss": 0.0014,
      "step": 213680
    },
    {
      "epoch": 7.123,
      "grad_norm": 0.08624370396137238,
      "learning_rate": 5.48125e-06,
      "loss": 0.0016,
      "step": 213690
    },
    {
      "epoch": 7.123333333333333,
      "grad_norm": 0.030305201187729836,
      "learning_rate": 5.4791666666666674e-06,
      "loss": 0.0013,
      "step": 213700
    },
    {
      "epoch": 7.123666666666667,
      "grad_norm": 0.029563387855887413,
      "learning_rate": 5.477083333333334e-06,
      "loss": 0.0029,
      "step": 213710
    },
    {
      "epoch": 7.124,
      "grad_norm": 0.1423577070236206,
      "learning_rate": 5.475e-06,
      "loss": 0.0015,
      "step": 213720
    },
    {
      "epoch": 7.124333333333333,
      "grad_norm": 0.22874534130096436,
      "learning_rate": 5.472916666666667e-06,
      "loss": 0.0014,
      "step": 213730
    },
    {
      "epoch": 7.124666666666666,
      "grad_norm": 0.03080376237630844,
      "learning_rate": 5.470833333333334e-06,
      "loss": 0.0016,
      "step": 213740
    },
    {
      "epoch": 7.125,
      "grad_norm": 0.4386155307292938,
      "learning_rate": 5.46875e-06,
      "loss": 0.0016,
      "step": 213750
    },
    {
      "epoch": 7.125333333333334,
      "grad_norm": 0.22898228466510773,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0012,
      "step": 213760
    },
    {
      "epoch": 7.125666666666667,
      "grad_norm": 0.14356978237628937,
      "learning_rate": 5.4645833333333335e-06,
      "loss": 0.0016,
      "step": 213770
    },
    {
      "epoch": 7.126,
      "grad_norm": 0.11411825567483902,
      "learning_rate": 5.462500000000001e-06,
      "loss": 0.0016,
      "step": 213780
    },
    {
      "epoch": 7.126333333333333,
      "grad_norm": 0.11445190012454987,
      "learning_rate": 5.460416666666667e-06,
      "loss": 0.0013,
      "step": 213790
    },
    {
      "epoch": 7.126666666666667,
      "grad_norm": 0.4288879632949829,
      "learning_rate": 5.458333333333333e-06,
      "loss": 0.0026,
      "step": 213800
    },
    {
      "epoch": 7.127,
      "grad_norm": 0.08643601834774017,
      "learning_rate": 5.4562500000000005e-06,
      "loss": 0.0015,
      "step": 213810
    },
    {
      "epoch": 7.1273333333333335,
      "grad_norm": 0.08561500161886215,
      "learning_rate": 5.454166666666667e-06,
      "loss": 0.0014,
      "step": 213820
    },
    {
      "epoch": 7.127666666666666,
      "grad_norm": 0.1944749802350998,
      "learning_rate": 5.452083333333333e-06,
      "loss": 0.0013,
      "step": 213830
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.5131926536560059,
      "learning_rate": 5.45e-06,
      "loss": 0.0014,
      "step": 213840
    },
    {
      "epoch": 7.128333333333333,
      "grad_norm": 0.058168139308691025,
      "learning_rate": 5.4479166666666675e-06,
      "loss": 0.0021,
      "step": 213850
    },
    {
      "epoch": 7.128666666666667,
      "grad_norm": 0.015064519830048084,
      "learning_rate": 5.445833333333333e-06,
      "loss": 0.0021,
      "step": 213860
    },
    {
      "epoch": 7.129,
      "grad_norm": 0.20070497691631317,
      "learning_rate": 5.44375e-06,
      "loss": 0.0021,
      "step": 213870
    },
    {
      "epoch": 7.129333333333333,
      "grad_norm": 0.08647534251213074,
      "learning_rate": 5.441666666666667e-06,
      "loss": 0.0023,
      "step": 213880
    },
    {
      "epoch": 7.129666666666667,
      "grad_norm": 0.6284254789352417,
      "learning_rate": 5.439583333333334e-06,
      "loss": 0.0017,
      "step": 213890
    },
    {
      "epoch": 7.13,
      "grad_norm": 0.25627657771110535,
      "learning_rate": 5.4375e-06,
      "loss": 0.0018,
      "step": 213900
    },
    {
      "epoch": 7.130333333333334,
      "grad_norm": 0.03058590367436409,
      "learning_rate": 5.435416666666667e-06,
      "loss": 0.0011,
      "step": 213910
    },
    {
      "epoch": 7.1306666666666665,
      "grad_norm": 0.11471106112003326,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.0018,
      "step": 213920
    },
    {
      "epoch": 7.131,
      "grad_norm": 0.31351691484451294,
      "learning_rate": 5.43125e-06,
      "loss": 0.0028,
      "step": 213930
    },
    {
      "epoch": 7.131333333333333,
      "grad_norm": 0.2564294934272766,
      "learning_rate": 5.429166666666667e-06,
      "loss": 0.0014,
      "step": 213940
    },
    {
      "epoch": 7.131666666666667,
      "grad_norm": 0.22866013646125793,
      "learning_rate": 5.427083333333333e-06,
      "loss": 0.0015,
      "step": 213950
    },
    {
      "epoch": 7.132,
      "grad_norm": 0.22859229147434235,
      "learning_rate": 5.4250000000000006e-06,
      "loss": 0.0021,
      "step": 213960
    },
    {
      "epoch": 7.132333333333333,
      "grad_norm": 0.1145586296916008,
      "learning_rate": 5.422916666666667e-06,
      "loss": 0.0016,
      "step": 213970
    },
    {
      "epoch": 7.132666666666666,
      "grad_norm": 0.009874345734715462,
      "learning_rate": 5.420833333333333e-06,
      "loss": 0.0015,
      "step": 213980
    },
    {
      "epoch": 7.133,
      "grad_norm": 0.009583093225955963,
      "learning_rate": 5.41875e-06,
      "loss": 0.0014,
      "step": 213990
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.20121482014656067,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.0018,
      "step": 214000
    },
    {
      "epoch": 7.133666666666667,
      "grad_norm": 0.08615562319755554,
      "learning_rate": 5.414583333333333e-06,
      "loss": 0.0021,
      "step": 214010
    },
    {
      "epoch": 7.134,
      "grad_norm": 0.22886642813682556,
      "learning_rate": 5.4125e-06,
      "loss": 0.0015,
      "step": 214020
    },
    {
      "epoch": 7.134333333333333,
      "grad_norm": 0.2848283648490906,
      "learning_rate": 5.4104166666666675e-06,
      "loss": 0.0014,
      "step": 214030
    },
    {
      "epoch": 7.134666666666667,
      "grad_norm": 0.20000071823596954,
      "learning_rate": 5.408333333333333e-06,
      "loss": 0.0018,
      "step": 214040
    },
    {
      "epoch": 7.135,
      "grad_norm": 0.2866218388080597,
      "learning_rate": 5.40625e-06,
      "loss": 0.0014,
      "step": 214050
    },
    {
      "epoch": 7.1353333333333335,
      "grad_norm": 0.17183363437652588,
      "learning_rate": 5.404166666666667e-06,
      "loss": 0.0017,
      "step": 214060
    },
    {
      "epoch": 7.135666666666666,
      "grad_norm": 0.030173514038324356,
      "learning_rate": 5.402083333333334e-06,
      "loss": 0.0015,
      "step": 214070
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.11471738666296005,
      "learning_rate": 5.4e-06,
      "loss": 0.0014,
      "step": 214080
    },
    {
      "epoch": 7.136333333333333,
      "grad_norm": 0.1815481185913086,
      "learning_rate": 5.397916666666667e-06,
      "loss": 0.0015,
      "step": 214090
    },
    {
      "epoch": 7.136666666666667,
      "grad_norm": 0.14389711618423462,
      "learning_rate": 5.3958333333333335e-06,
      "loss": 0.0025,
      "step": 214100
    },
    {
      "epoch": 7.1370000000000005,
      "grad_norm": 0.03175438195466995,
      "learning_rate": 5.39375e-06,
      "loss": 0.0016,
      "step": 214110
    },
    {
      "epoch": 7.137333333333333,
      "grad_norm": 0.010377457365393639,
      "learning_rate": 5.391666666666667e-06,
      "loss": 0.0016,
      "step": 214120
    },
    {
      "epoch": 7.137666666666667,
      "grad_norm": 0.26738137006759644,
      "learning_rate": 5.389583333333333e-06,
      "loss": 0.0017,
      "step": 214130
    },
    {
      "epoch": 7.138,
      "grad_norm": 0.22835226356983185,
      "learning_rate": 5.3875000000000005e-06,
      "loss": 0.0023,
      "step": 214140
    },
    {
      "epoch": 7.138333333333334,
      "grad_norm": 0.14327645301818848,
      "learning_rate": 5.385416666666667e-06,
      "loss": 0.0018,
      "step": 214150
    },
    {
      "epoch": 7.1386666666666665,
      "grad_norm": 0.08724688738584518,
      "learning_rate": 5.383333333333333e-06,
      "loss": 0.0015,
      "step": 214160
    },
    {
      "epoch": 7.139,
      "grad_norm": 0.17103075981140137,
      "learning_rate": 5.38125e-06,
      "loss": 0.002,
      "step": 214170
    },
    {
      "epoch": 7.139333333333333,
      "grad_norm": 0.4694146513938904,
      "learning_rate": 5.379166666666667e-06,
      "loss": 0.0022,
      "step": 214180
    },
    {
      "epoch": 7.139666666666667,
      "grad_norm": 0.05769706144928932,
      "learning_rate": 5.377083333333333e-06,
      "loss": 0.0015,
      "step": 214190
    },
    {
      "epoch": 7.14,
      "grad_norm": 0.31390461325645447,
      "learning_rate": 5.375e-06,
      "loss": 0.0017,
      "step": 214200
    },
    {
      "epoch": 7.140333333333333,
      "grad_norm": 0.11391526460647583,
      "learning_rate": 5.372916666666667e-06,
      "loss": 0.0019,
      "step": 214210
    },
    {
      "epoch": 7.140666666666666,
      "grad_norm": 0.2458808720111847,
      "learning_rate": 5.370833333333333e-06,
      "loss": 0.0015,
      "step": 214220
    },
    {
      "epoch": 7.141,
      "grad_norm": 0.14317646622657776,
      "learning_rate": 5.36875e-06,
      "loss": 0.0011,
      "step": 214230
    },
    {
      "epoch": 7.141333333333334,
      "grad_norm": 0.17154110968112946,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.0013,
      "step": 214240
    },
    {
      "epoch": 7.141666666666667,
      "grad_norm": 0.012501313351094723,
      "learning_rate": 5.3645833333333336e-06,
      "loss": 0.0013,
      "step": 214250
    },
    {
      "epoch": 7.142,
      "grad_norm": 0.05928576737642288,
      "learning_rate": 5.3625e-06,
      "loss": 0.0015,
      "step": 214260
    },
    {
      "epoch": 7.142333333333333,
      "grad_norm": 0.08594326674938202,
      "learning_rate": 5.360416666666667e-06,
      "loss": 0.0013,
      "step": 214270
    },
    {
      "epoch": 7.142666666666667,
      "grad_norm": 0.25649428367614746,
      "learning_rate": 5.358333333333333e-06,
      "loss": 0.0011,
      "step": 214280
    },
    {
      "epoch": 7.143,
      "grad_norm": 0.3426502048969269,
      "learning_rate": 5.35625e-06,
      "loss": 0.002,
      "step": 214290
    },
    {
      "epoch": 7.1433333333333335,
      "grad_norm": 0.059547603130340576,
      "learning_rate": 5.354166666666667e-06,
      "loss": 0.0013,
      "step": 214300
    },
    {
      "epoch": 7.143666666666666,
      "grad_norm": 0.15165212750434875,
      "learning_rate": 5.352083333333333e-06,
      "loss": 0.0022,
      "step": 214310
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.34436583518981934,
      "learning_rate": 5.3500000000000004e-06,
      "loss": 0.0015,
      "step": 214320
    },
    {
      "epoch": 7.144333333333333,
      "grad_norm": 0.314777135848999,
      "learning_rate": 5.347916666666667e-06,
      "loss": 0.0018,
      "step": 214330
    },
    {
      "epoch": 7.144666666666667,
      "grad_norm": 0.029818300157785416,
      "learning_rate": 5.345833333333333e-06,
      "loss": 0.0017,
      "step": 214340
    },
    {
      "epoch": 7.145,
      "grad_norm": 0.06433520466089249,
      "learning_rate": 5.34375e-06,
      "loss": 0.0019,
      "step": 214350
    },
    {
      "epoch": 7.145333333333333,
      "grad_norm": 0.031224029138684273,
      "learning_rate": 5.341666666666667e-06,
      "loss": 0.0024,
      "step": 214360
    },
    {
      "epoch": 7.145666666666667,
      "grad_norm": 0.1999283730983734,
      "learning_rate": 5.339583333333333e-06,
      "loss": 0.0017,
      "step": 214370
    },
    {
      "epoch": 7.146,
      "grad_norm": 0.1153767928481102,
      "learning_rate": 5.3375e-06,
      "loss": 0.0022,
      "step": 214380
    },
    {
      "epoch": 7.146333333333334,
      "grad_norm": 0.28527453541755676,
      "learning_rate": 5.335416666666667e-06,
      "loss": 0.0016,
      "step": 214390
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.03185921534895897,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0013,
      "step": 214400
    },
    {
      "epoch": 7.147,
      "grad_norm": 0.11449600756168365,
      "learning_rate": 5.33125e-06,
      "loss": 0.0014,
      "step": 214410
    },
    {
      "epoch": 7.147333333333333,
      "grad_norm": 0.4393337368965149,
      "learning_rate": 5.329166666666667e-06,
      "loss": 0.0019,
      "step": 214420
    },
    {
      "epoch": 7.147666666666667,
      "grad_norm": 0.03534778207540512,
      "learning_rate": 5.3270833333333335e-06,
      "loss": 0.0018,
      "step": 214430
    },
    {
      "epoch": 7.148,
      "grad_norm": 0.25676992535591125,
      "learning_rate": 5.325e-06,
      "loss": 0.0012,
      "step": 214440
    },
    {
      "epoch": 7.148333333333333,
      "grad_norm": 0.2863721251487732,
      "learning_rate": 5.322916666666667e-06,
      "loss": 0.0018,
      "step": 214450
    },
    {
      "epoch": 7.148666666666666,
      "grad_norm": 0.360244482755661,
      "learning_rate": 5.320833333333334e-06,
      "loss": 0.0015,
      "step": 214460
    },
    {
      "epoch": 7.149,
      "grad_norm": 0.20037493109703064,
      "learning_rate": 5.3187500000000005e-06,
      "loss": 0.0018,
      "step": 214470
    },
    {
      "epoch": 7.149333333333334,
      "grad_norm": 0.1999804675579071,
      "learning_rate": 5.316666666666667e-06,
      "loss": 0.0015,
      "step": 214480
    },
    {
      "epoch": 7.149666666666667,
      "grad_norm": 0.19960132241249084,
      "learning_rate": 5.314583333333334e-06,
      "loss": 0.0013,
      "step": 214490
    },
    {
      "epoch": 7.15,
      "grad_norm": 0.060236431658267975,
      "learning_rate": 5.3125e-06,
      "loss": 0.0021,
      "step": 214500
    },
    {
      "epoch": 7.150333333333333,
      "grad_norm": 0.3188316524028778,
      "learning_rate": 5.310416666666667e-06,
      "loss": 0.0024,
      "step": 214510
    },
    {
      "epoch": 7.150666666666667,
      "grad_norm": 0.17168474197387695,
      "learning_rate": 5.308333333333334e-06,
      "loss": 0.0021,
      "step": 214520
    },
    {
      "epoch": 7.151,
      "grad_norm": 0.14303898811340332,
      "learning_rate": 5.30625e-06,
      "loss": 0.0016,
      "step": 214530
    },
    {
      "epoch": 7.1513333333333335,
      "grad_norm": 0.11527800559997559,
      "learning_rate": 5.304166666666667e-06,
      "loss": 0.0017,
      "step": 214540
    },
    {
      "epoch": 7.151666666666666,
      "grad_norm": 0.19764754176139832,
      "learning_rate": 5.302083333333334e-06,
      "loss": 0.0032,
      "step": 214550
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.14292307198047638,
      "learning_rate": 5.3e-06,
      "loss": 0.0014,
      "step": 214560
    },
    {
      "epoch": 7.152333333333333,
      "grad_norm": 0.02929147332906723,
      "learning_rate": 5.297916666666667e-06,
      "loss": 0.0024,
      "step": 214570
    },
    {
      "epoch": 7.152666666666667,
      "grad_norm": 0.7666894793510437,
      "learning_rate": 5.295833333333334e-06,
      "loss": 0.0015,
      "step": 214580
    },
    {
      "epoch": 7.153,
      "grad_norm": 0.1144695058465004,
      "learning_rate": 5.29375e-06,
      "loss": 0.0022,
      "step": 214590
    },
    {
      "epoch": 7.153333333333333,
      "grad_norm": 0.14310824871063232,
      "learning_rate": 5.291666666666667e-06,
      "loss": 0.0014,
      "step": 214600
    },
    {
      "epoch": 7.153666666666667,
      "grad_norm": 0.17415279150009155,
      "learning_rate": 5.289583333333334e-06,
      "loss": 0.0019,
      "step": 214610
    },
    {
      "epoch": 7.154,
      "grad_norm": 0.37847965955734253,
      "learning_rate": 5.2875e-06,
      "loss": 0.0016,
      "step": 214620
    },
    {
      "epoch": 7.154333333333334,
      "grad_norm": 0.14246766269207,
      "learning_rate": 5.285416666666667e-06,
      "loss": 0.0019,
      "step": 214630
    },
    {
      "epoch": 7.1546666666666665,
      "grad_norm": 0.1435435563325882,
      "learning_rate": 5.283333333333334e-06,
      "loss": 0.002,
      "step": 214640
    },
    {
      "epoch": 7.155,
      "grad_norm": 0.310294508934021,
      "learning_rate": 5.2812500000000005e-06,
      "loss": 0.002,
      "step": 214650
    },
    {
      "epoch": 7.155333333333333,
      "grad_norm": 0.17087936401367188,
      "learning_rate": 5.279166666666667e-06,
      "loss": 0.0014,
      "step": 214660
    },
    {
      "epoch": 7.155666666666667,
      "grad_norm": 0.17217160761356354,
      "learning_rate": 5.277083333333334e-06,
      "loss": 0.0016,
      "step": 214670
    },
    {
      "epoch": 7.156,
      "grad_norm": 0.012856253422796726,
      "learning_rate": 5.275e-06,
      "loss": 0.0014,
      "step": 214680
    },
    {
      "epoch": 7.156333333333333,
      "grad_norm": 0.08820156008005142,
      "learning_rate": 5.272916666666667e-06,
      "loss": 0.0017,
      "step": 214690
    },
    {
      "epoch": 7.156666666666666,
      "grad_norm": 0.3434089124202728,
      "learning_rate": 5.270833333333334e-06,
      "loss": 0.0016,
      "step": 214700
    },
    {
      "epoch": 7.157,
      "grad_norm": 0.26436886191368103,
      "learning_rate": 5.26875e-06,
      "loss": 0.0013,
      "step": 214710
    },
    {
      "epoch": 7.157333333333334,
      "grad_norm": 0.0330498106777668,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0021,
      "step": 214720
    },
    {
      "epoch": 7.157666666666667,
      "grad_norm": 0.03219678997993469,
      "learning_rate": 5.264583333333334e-06,
      "loss": 0.0016,
      "step": 214730
    },
    {
      "epoch": 7.158,
      "grad_norm": 0.2753553092479706,
      "learning_rate": 5.2625e-06,
      "loss": 0.0017,
      "step": 214740
    },
    {
      "epoch": 7.158333333333333,
      "grad_norm": 0.17174044251441956,
      "learning_rate": 5.260416666666667e-06,
      "loss": 0.0012,
      "step": 214750
    },
    {
      "epoch": 7.158666666666667,
      "grad_norm": 0.23527950048446655,
      "learning_rate": 5.2583333333333335e-06,
      "loss": 0.0015,
      "step": 214760
    },
    {
      "epoch": 7.159,
      "grad_norm": 0.2100643366575241,
      "learning_rate": 5.25625e-06,
      "loss": 0.0018,
      "step": 214770
    },
    {
      "epoch": 7.1593333333333335,
      "grad_norm": 0.37065255641937256,
      "learning_rate": 5.254166666666667e-06,
      "loss": 0.0013,
      "step": 214780
    },
    {
      "epoch": 7.159666666666666,
      "grad_norm": 0.16651245951652527,
      "learning_rate": 5.252083333333334e-06,
      "loss": 0.0018,
      "step": 214790
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.058350712060928345,
      "learning_rate": 5.25e-06,
      "loss": 0.0018,
      "step": 214800
    },
    {
      "epoch": 7.160333333333333,
      "grad_norm": 0.14238959550857544,
      "learning_rate": 5.247916666666667e-06,
      "loss": 0.0016,
      "step": 214810
    },
    {
      "epoch": 7.160666666666667,
      "grad_norm": 0.29278939962387085,
      "learning_rate": 5.245833333333334e-06,
      "loss": 0.0024,
      "step": 214820
    },
    {
      "epoch": 7.161,
      "grad_norm": 0.030056403949856758,
      "learning_rate": 5.24375e-06,
      "loss": 0.0018,
      "step": 214830
    },
    {
      "epoch": 7.161333333333333,
      "grad_norm": 0.030356494709849358,
      "learning_rate": 5.241666666666667e-06,
      "loss": 0.0014,
      "step": 214840
    },
    {
      "epoch": 7.161666666666667,
      "grad_norm": 0.0865219384431839,
      "learning_rate": 5.239583333333334e-06,
      "loss": 0.0025,
      "step": 214850
    },
    {
      "epoch": 7.162,
      "grad_norm": 0.07387103885412216,
      "learning_rate": 5.2375e-06,
      "loss": 0.0018,
      "step": 214860
    },
    {
      "epoch": 7.162333333333334,
      "grad_norm": 0.03181745111942291,
      "learning_rate": 5.235416666666667e-06,
      "loss": 0.0011,
      "step": 214870
    },
    {
      "epoch": 7.1626666666666665,
      "grad_norm": 0.28491199016571045,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.0012,
      "step": 214880
    },
    {
      "epoch": 7.163,
      "grad_norm": 0.31028735637664795,
      "learning_rate": 5.23125e-06,
      "loss": 0.0011,
      "step": 214890
    },
    {
      "epoch": 7.163333333333333,
      "grad_norm": 0.08687685430049896,
      "learning_rate": 5.229166666666667e-06,
      "loss": 0.003,
      "step": 214900
    },
    {
      "epoch": 7.163666666666667,
      "grad_norm": 0.17181944847106934,
      "learning_rate": 5.227083333333334e-06,
      "loss": 0.0013,
      "step": 214910
    },
    {
      "epoch": 7.164,
      "grad_norm": 0.030692603439092636,
      "learning_rate": 5.225e-06,
      "loss": 0.0012,
      "step": 214920
    },
    {
      "epoch": 7.164333333333333,
      "grad_norm": 0.05819561704993248,
      "learning_rate": 5.222916666666667e-06,
      "loss": 0.0026,
      "step": 214930
    },
    {
      "epoch": 7.164666666666666,
      "grad_norm": 0.20000004768371582,
      "learning_rate": 5.2208333333333335e-06,
      "loss": 0.0015,
      "step": 214940
    },
    {
      "epoch": 7.165,
      "grad_norm": 0.0336986668407917,
      "learning_rate": 5.21875e-06,
      "loss": 0.0015,
      "step": 214950
    },
    {
      "epoch": 7.165333333333333,
      "grad_norm": 0.057811349630355835,
      "learning_rate": 5.216666666666667e-06,
      "loss": 0.0017,
      "step": 214960
    },
    {
      "epoch": 7.165666666666667,
      "grad_norm": 0.25673145055770874,
      "learning_rate": 5.214583333333334e-06,
      "loss": 0.0021,
      "step": 214970
    },
    {
      "epoch": 7.166,
      "grad_norm": 0.06090566888451576,
      "learning_rate": 5.2125e-06,
      "loss": 0.0017,
      "step": 214980
    },
    {
      "epoch": 7.166333333333333,
      "grad_norm": 0.14298397302627563,
      "learning_rate": 5.210416666666667e-06,
      "loss": 0.002,
      "step": 214990
    },
    {
      "epoch": 7.166666666666667,
      "grad_norm": 0.058204565197229385,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.0014,
      "step": 215000
    },
    {
      "epoch": 7.167,
      "grad_norm": 0.11413967609405518,
      "learning_rate": 5.20625e-06,
      "loss": 0.0016,
      "step": 215010
    },
    {
      "epoch": 7.167333333333334,
      "grad_norm": 0.22878746688365936,
      "learning_rate": 5.204166666666667e-06,
      "loss": 0.0024,
      "step": 215020
    },
    {
      "epoch": 7.167666666666666,
      "grad_norm": 0.1996907889842987,
      "learning_rate": 5.202083333333334e-06,
      "loss": 0.0014,
      "step": 215030
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.014893530867993832,
      "learning_rate": 5.2e-06,
      "loss": 0.002,
      "step": 215040
    },
    {
      "epoch": 7.168333333333333,
      "grad_norm": 0.17651741206645966,
      "learning_rate": 5.1979166666666665e-06,
      "loss": 0.0019,
      "step": 215050
    },
    {
      "epoch": 7.168666666666667,
      "grad_norm": 0.03637813404202461,
      "learning_rate": 5.195833333333334e-06,
      "loss": 0.0022,
      "step": 215060
    },
    {
      "epoch": 7.169,
      "grad_norm": 0.1428508311510086,
      "learning_rate": 5.19375e-06,
      "loss": 0.0013,
      "step": 215070
    },
    {
      "epoch": 7.169333333333333,
      "grad_norm": 0.05775400623679161,
      "learning_rate": 5.191666666666667e-06,
      "loss": 0.0016,
      "step": 215080
    },
    {
      "epoch": 7.169666666666667,
      "grad_norm": 0.010636315681040287,
      "learning_rate": 5.1895833333333336e-06,
      "loss": 0.0014,
      "step": 215090
    },
    {
      "epoch": 7.17,
      "grad_norm": 0.02995643950998783,
      "learning_rate": 5.1875e-06,
      "loss": 0.0015,
      "step": 215100
    },
    {
      "epoch": 7.170333333333334,
      "grad_norm": 0.05865604430437088,
      "learning_rate": 5.185416666666667e-06,
      "loss": 0.0015,
      "step": 215110
    },
    {
      "epoch": 7.1706666666666665,
      "grad_norm": 0.2570202648639679,
      "learning_rate": 5.183333333333333e-06,
      "loss": 0.0018,
      "step": 215120
    },
    {
      "epoch": 7.171,
      "grad_norm": 0.20025542378425598,
      "learning_rate": 5.18125e-06,
      "loss": 0.0017,
      "step": 215130
    },
    {
      "epoch": 7.171333333333333,
      "grad_norm": 0.08537234365940094,
      "learning_rate": 5.179166666666667e-06,
      "loss": 0.0015,
      "step": 215140
    },
    {
      "epoch": 7.171666666666667,
      "grad_norm": 0.4582497477531433,
      "learning_rate": 5.177083333333334e-06,
      "loss": 0.0018,
      "step": 215150
    },
    {
      "epoch": 7.172,
      "grad_norm": 0.038855597376823425,
      "learning_rate": 5.175e-06,
      "loss": 0.0013,
      "step": 215160
    },
    {
      "epoch": 7.1723333333333334,
      "grad_norm": 0.20031346380710602,
      "learning_rate": 5.172916666666667e-06,
      "loss": 0.0016,
      "step": 215170
    },
    {
      "epoch": 7.172666666666666,
      "grad_norm": 0.06979412585496902,
      "learning_rate": 5.170833333333334e-06,
      "loss": 0.0019,
      "step": 215180
    },
    {
      "epoch": 7.173,
      "grad_norm": 0.22782209515571594,
      "learning_rate": 5.16875e-06,
      "loss": 0.0012,
      "step": 215190
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 0.22886453568935394,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.002,
      "step": 215200
    },
    {
      "epoch": 7.173666666666667,
      "grad_norm": 0.39959588646888733,
      "learning_rate": 5.164583333333334e-06,
      "loss": 0.002,
      "step": 215210
    },
    {
      "epoch": 7.174,
      "grad_norm": 0.05747447535395622,
      "learning_rate": 5.1625e-06,
      "loss": 0.0014,
      "step": 215220
    },
    {
      "epoch": 7.174333333333333,
      "grad_norm": 0.05712915584445,
      "learning_rate": 5.1604166666666665e-06,
      "loss": 0.0018,
      "step": 215230
    },
    {
      "epoch": 7.174666666666667,
      "grad_norm": 0.17179274559020996,
      "learning_rate": 5.158333333333334e-06,
      "loss": 0.0015,
      "step": 215240
    },
    {
      "epoch": 7.175,
      "grad_norm": 0.06381765753030777,
      "learning_rate": 5.15625e-06,
      "loss": 0.0019,
      "step": 215250
    },
    {
      "epoch": 7.175333333333334,
      "grad_norm": 0.1443236768245697,
      "learning_rate": 5.154166666666667e-06,
      "loss": 0.0014,
      "step": 215260
    },
    {
      "epoch": 7.175666666666666,
      "grad_norm": 0.21263401210308075,
      "learning_rate": 5.1520833333333335e-06,
      "loss": 0.0022,
      "step": 215270
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.0857866033911705,
      "learning_rate": 5.15e-06,
      "loss": 0.0011,
      "step": 215280
    },
    {
      "epoch": 7.176333333333333,
      "grad_norm": 0.08706653118133545,
      "learning_rate": 5.147916666666667e-06,
      "loss": 0.0019,
      "step": 215290
    },
    {
      "epoch": 7.176666666666667,
      "grad_norm": 0.22845664620399475,
      "learning_rate": 5.145833333333333e-06,
      "loss": 0.0018,
      "step": 215300
    },
    {
      "epoch": 7.177,
      "grad_norm": 0.17156490683555603,
      "learning_rate": 5.14375e-06,
      "loss": 0.0014,
      "step": 215310
    },
    {
      "epoch": 7.177333333333333,
      "grad_norm": 0.3098868429660797,
      "learning_rate": 5.141666666666667e-06,
      "loss": 0.0015,
      "step": 215320
    },
    {
      "epoch": 7.177666666666667,
      "grad_norm": 0.16048522293567657,
      "learning_rate": 5.139583333333334e-06,
      "loss": 0.0013,
      "step": 215330
    },
    {
      "epoch": 7.178,
      "grad_norm": 0.1156640499830246,
      "learning_rate": 5.1375e-06,
      "loss": 0.0021,
      "step": 215340
    },
    {
      "epoch": 7.178333333333334,
      "grad_norm": 0.2954789400100708,
      "learning_rate": 5.135416666666667e-06,
      "loss": 0.0026,
      "step": 215350
    },
    {
      "epoch": 7.1786666666666665,
      "grad_norm": 0.08682044595479965,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0015,
      "step": 215360
    },
    {
      "epoch": 7.179,
      "grad_norm": 0.033194854855537415,
      "learning_rate": 5.13125e-06,
      "loss": 0.0018,
      "step": 215370
    },
    {
      "epoch": 7.179333333333333,
      "grad_norm": 0.11608806997537613,
      "learning_rate": 5.1291666666666665e-06,
      "loss": 0.0013,
      "step": 215380
    },
    {
      "epoch": 7.179666666666667,
      "grad_norm": 0.1437438726425171,
      "learning_rate": 5.127083333333334e-06,
      "loss": 0.0016,
      "step": 215390
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.17693930864334106,
      "learning_rate": 5.125e-06,
      "loss": 0.002,
      "step": 215400
    },
    {
      "epoch": 7.1803333333333335,
      "grad_norm": 0.371681809425354,
      "learning_rate": 5.122916666666667e-06,
      "loss": 0.0015,
      "step": 215410
    },
    {
      "epoch": 7.180666666666666,
      "grad_norm": 0.142547607421875,
      "learning_rate": 5.1208333333333336e-06,
      "loss": 0.0025,
      "step": 215420
    },
    {
      "epoch": 7.181,
      "grad_norm": 0.1711757630109787,
      "learning_rate": 5.11875e-06,
      "loss": 0.0015,
      "step": 215430
    },
    {
      "epoch": 7.181333333333333,
      "grad_norm": 0.11477004736661911,
      "learning_rate": 5.116666666666667e-06,
      "loss": 0.0018,
      "step": 215440
    },
    {
      "epoch": 7.181666666666667,
      "grad_norm": 0.257316529750824,
      "learning_rate": 5.114583333333333e-06,
      "loss": 0.0015,
      "step": 215450
    },
    {
      "epoch": 7.182,
      "grad_norm": 0.03981144353747368,
      "learning_rate": 5.1125e-06,
      "loss": 0.0015,
      "step": 215460
    },
    {
      "epoch": 7.182333333333333,
      "grad_norm": 0.1431148797273636,
      "learning_rate": 5.110416666666667e-06,
      "loss": 0.0016,
      "step": 215470
    },
    {
      "epoch": 7.182666666666667,
      "grad_norm": 0.16659465432167053,
      "learning_rate": 5.108333333333334e-06,
      "loss": 0.0015,
      "step": 215480
    },
    {
      "epoch": 7.183,
      "grad_norm": 0.3707336187362671,
      "learning_rate": 5.10625e-06,
      "loss": 0.0016,
      "step": 215490
    },
    {
      "epoch": 7.183333333333334,
      "grad_norm": 0.3506324589252472,
      "learning_rate": 5.104166666666667e-06,
      "loss": 0.0013,
      "step": 215500
    },
    {
      "epoch": 7.183666666666666,
      "grad_norm": 0.5052095055580139,
      "learning_rate": 5.102083333333334e-06,
      "loss": 0.0016,
      "step": 215510
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.1710030734539032,
      "learning_rate": 5.1e-06,
      "loss": 0.002,
      "step": 215520
    },
    {
      "epoch": 7.184333333333333,
      "grad_norm": 0.17154936492443085,
      "learning_rate": 5.097916666666667e-06,
      "loss": 0.0017,
      "step": 215530
    },
    {
      "epoch": 7.184666666666667,
      "grad_norm": 0.010828494094312191,
      "learning_rate": 5.095833333333334e-06,
      "loss": 0.0015,
      "step": 215540
    },
    {
      "epoch": 7.185,
      "grad_norm": 0.0857103168964386,
      "learning_rate": 5.09375e-06,
      "loss": 0.0016,
      "step": 215550
    },
    {
      "epoch": 7.185333333333333,
      "grad_norm": 0.11421404778957367,
      "learning_rate": 5.0916666666666665e-06,
      "loss": 0.0013,
      "step": 215560
    },
    {
      "epoch": 7.185666666666667,
      "grad_norm": 0.031347520649433136,
      "learning_rate": 5.089583333333334e-06,
      "loss": 0.0017,
      "step": 215570
    },
    {
      "epoch": 7.186,
      "grad_norm": 0.06014417111873627,
      "learning_rate": 5.0875e-06,
      "loss": 0.0019,
      "step": 215580
    },
    {
      "epoch": 7.186333333333334,
      "grad_norm": 0.2940196990966797,
      "learning_rate": 5.085416666666667e-06,
      "loss": 0.0019,
      "step": 215590
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 0.08623486012220383,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.0012,
      "step": 215600
    },
    {
      "epoch": 7.187,
      "grad_norm": 0.11501962691545486,
      "learning_rate": 5.08125e-06,
      "loss": 0.0018,
      "step": 215610
    },
    {
      "epoch": 7.187333333333333,
      "grad_norm": 0.030911508947610855,
      "learning_rate": 5.079166666666667e-06,
      "loss": 0.0018,
      "step": 215620
    },
    {
      "epoch": 7.187666666666667,
      "grad_norm": 0.14247025549411774,
      "learning_rate": 5.077083333333333e-06,
      "loss": 0.0011,
      "step": 215630
    },
    {
      "epoch": 7.188,
      "grad_norm": 0.2855294942855835,
      "learning_rate": 5.0750000000000005e-06,
      "loss": 0.0016,
      "step": 215640
    },
    {
      "epoch": 7.1883333333333335,
      "grad_norm": 0.029492657631635666,
      "learning_rate": 5.072916666666667e-06,
      "loss": 0.0015,
      "step": 215650
    },
    {
      "epoch": 7.188666666666666,
      "grad_norm": 0.03150380402803421,
      "learning_rate": 5.070833333333334e-06,
      "loss": 0.0012,
      "step": 215660
    },
    {
      "epoch": 7.189,
      "grad_norm": 0.2566468119621277,
      "learning_rate": 5.06875e-06,
      "loss": 0.0017,
      "step": 215670
    },
    {
      "epoch": 7.189333333333333,
      "grad_norm": 0.17127303779125214,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0026,
      "step": 215680
    },
    {
      "epoch": 7.189666666666667,
      "grad_norm": 0.1172649934887886,
      "learning_rate": 5.064583333333334e-06,
      "loss": 0.0016,
      "step": 215690
    },
    {
      "epoch": 7.19,
      "grad_norm": 0.0320969820022583,
      "learning_rate": 5.0625e-06,
      "loss": 0.0013,
      "step": 215700
    },
    {
      "epoch": 7.190333333333333,
      "grad_norm": 0.19979384541511536,
      "learning_rate": 5.0604166666666666e-06,
      "loss": 0.0015,
      "step": 215710
    },
    {
      "epoch": 7.190666666666667,
      "grad_norm": 0.20055416226387024,
      "learning_rate": 5.058333333333334e-06,
      "loss": 0.0022,
      "step": 215720
    },
    {
      "epoch": 7.191,
      "grad_norm": 0.06770242005586624,
      "learning_rate": 5.056250000000001e-06,
      "loss": 0.0017,
      "step": 215730
    },
    {
      "epoch": 7.191333333333334,
      "grad_norm": 0.37351900339126587,
      "learning_rate": 5.054166666666666e-06,
      "loss": 0.0018,
      "step": 215740
    },
    {
      "epoch": 7.191666666666666,
      "grad_norm": 0.39263004064559937,
      "learning_rate": 5.052083333333334e-06,
      "loss": 0.0019,
      "step": 215750
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.14682583510875702,
      "learning_rate": 5.050000000000001e-06,
      "loss": 0.0017,
      "step": 215760
    },
    {
      "epoch": 7.192333333333333,
      "grad_norm": 0.11613835394382477,
      "learning_rate": 5.047916666666667e-06,
      "loss": 0.0017,
      "step": 215770
    },
    {
      "epoch": 7.192666666666667,
      "grad_norm": 0.08850172162055969,
      "learning_rate": 5.0458333333333334e-06,
      "loss": 0.0017,
      "step": 215780
    },
    {
      "epoch": 7.193,
      "grad_norm": 0.2568664252758026,
      "learning_rate": 5.043750000000001e-06,
      "loss": 0.0012,
      "step": 215790
    },
    {
      "epoch": 7.193333333333333,
      "grad_norm": 0.040201373398303986,
      "learning_rate": 5.041666666666667e-06,
      "loss": 0.0017,
      "step": 215800
    },
    {
      "epoch": 7.193666666666667,
      "grad_norm": 0.014872288331389427,
      "learning_rate": 5.039583333333333e-06,
      "loss": 0.0019,
      "step": 215810
    },
    {
      "epoch": 7.194,
      "grad_norm": 0.28614869713783264,
      "learning_rate": 5.0375000000000005e-06,
      "loss": 0.0015,
      "step": 215820
    },
    {
      "epoch": 7.194333333333334,
      "grad_norm": 0.03079187497496605,
      "learning_rate": 5.035416666666667e-06,
      "loss": 0.0017,
      "step": 215830
    },
    {
      "epoch": 7.1946666666666665,
      "grad_norm": 0.05800825357437134,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0021,
      "step": 215840
    },
    {
      "epoch": 7.195,
      "grad_norm": 0.03050048276782036,
      "learning_rate": 5.03125e-06,
      "loss": 0.0015,
      "step": 215850
    },
    {
      "epoch": 7.195333333333333,
      "grad_norm": 0.031830720603466034,
      "learning_rate": 5.029166666666667e-06,
      "loss": 0.0014,
      "step": 215860
    },
    {
      "epoch": 7.195666666666667,
      "grad_norm": 0.08570412546396255,
      "learning_rate": 5.027083333333334e-06,
      "loss": 0.0021,
      "step": 215870
    },
    {
      "epoch": 7.196,
      "grad_norm": 0.30097696185112,
      "learning_rate": 5.025e-06,
      "loss": 0.0014,
      "step": 215880
    },
    {
      "epoch": 7.1963333333333335,
      "grad_norm": 0.1143273413181305,
      "learning_rate": 5.0229166666666665e-06,
      "loss": 0.0019,
      "step": 215890
    },
    {
      "epoch": 7.196666666666666,
      "grad_norm": 0.22806648910045624,
      "learning_rate": 5.020833333333334e-06,
      "loss": 0.0014,
      "step": 215900
    },
    {
      "epoch": 7.197,
      "grad_norm": 0.17110002040863037,
      "learning_rate": 5.018750000000001e-06,
      "loss": 0.0018,
      "step": 215910
    },
    {
      "epoch": 7.197333333333333,
      "grad_norm": 0.08584704250097275,
      "learning_rate": 5.016666666666666e-06,
      "loss": 0.0033,
      "step": 215920
    },
    {
      "epoch": 7.197666666666667,
      "grad_norm": 0.031013362109661102,
      "learning_rate": 5.0145833333333335e-06,
      "loss": 0.0015,
      "step": 215930
    },
    {
      "epoch": 7.198,
      "grad_norm": 0.257151335477829,
      "learning_rate": 5.012500000000001e-06,
      "loss": 0.0015,
      "step": 215940
    },
    {
      "epoch": 7.198333333333333,
      "grad_norm": 0.08611389249563217,
      "learning_rate": 5.010416666666667e-06,
      "loss": 0.0014,
      "step": 215950
    },
    {
      "epoch": 7.198666666666667,
      "grad_norm": 0.42826929688453674,
      "learning_rate": 5.008333333333333e-06,
      "loss": 0.0011,
      "step": 215960
    },
    {
      "epoch": 7.199,
      "grad_norm": 0.03139319270849228,
      "learning_rate": 5.0062500000000006e-06,
      "loss": 0.002,
      "step": 215970
    },
    {
      "epoch": 7.199333333333334,
      "grad_norm": 0.4562358558177948,
      "learning_rate": 5.004166666666667e-06,
      "loss": 0.0015,
      "step": 215980
    },
    {
      "epoch": 7.199666666666666,
      "grad_norm": 0.19924673438072205,
      "learning_rate": 5.002083333333333e-06,
      "loss": 0.0019,
      "step": 215990
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.11450575292110443,
      "learning_rate": 5e-06,
      "loss": 0.0013,
      "step": 216000
    },
    {
      "epoch": 7.200333333333333,
      "grad_norm": 0.14300848543643951,
      "learning_rate": 4.997916666666667e-06,
      "loss": 0.0015,
      "step": 216010
    },
    {
      "epoch": 7.200666666666667,
      "grad_norm": 0.2853468060493469,
      "learning_rate": 4.995833333333334e-06,
      "loss": 0.0015,
      "step": 216020
    },
    {
      "epoch": 7.201,
      "grad_norm": 0.23061294853687286,
      "learning_rate": 4.99375e-06,
      "loss": 0.0016,
      "step": 216030
    },
    {
      "epoch": 7.201333333333333,
      "grad_norm": 0.25676780939102173,
      "learning_rate": 4.991666666666667e-06,
      "loss": 0.0012,
      "step": 216040
    },
    {
      "epoch": 7.201666666666666,
      "grad_norm": 0.05771632492542267,
      "learning_rate": 4.989583333333334e-06,
      "loss": 0.003,
      "step": 216050
    },
    {
      "epoch": 7.202,
      "grad_norm": 0.08671148866415024,
      "learning_rate": 4.9875e-06,
      "loss": 0.0019,
      "step": 216060
    },
    {
      "epoch": 7.202333333333334,
      "grad_norm": 0.256430059671402,
      "learning_rate": 4.9854166666666664e-06,
      "loss": 0.0017,
      "step": 216070
    },
    {
      "epoch": 7.2026666666666666,
      "grad_norm": 0.08588387072086334,
      "learning_rate": 4.983333333333334e-06,
      "loss": 0.0018,
      "step": 216080
    },
    {
      "epoch": 7.203,
      "grad_norm": 0.05877593159675598,
      "learning_rate": 4.981250000000001e-06,
      "loss": 0.0015,
      "step": 216090
    },
    {
      "epoch": 7.203333333333333,
      "grad_norm": 0.31380125880241394,
      "learning_rate": 4.979166666666666e-06,
      "loss": 0.0016,
      "step": 216100
    },
    {
      "epoch": 7.203666666666667,
      "grad_norm": 0.11449898034334183,
      "learning_rate": 4.9770833333333335e-06,
      "loss": 0.0014,
      "step": 216110
    },
    {
      "epoch": 7.204,
      "grad_norm": 0.11400102078914642,
      "learning_rate": 4.975000000000001e-06,
      "loss": 0.0021,
      "step": 216120
    },
    {
      "epoch": 7.2043333333333335,
      "grad_norm": 0.20114704966545105,
      "learning_rate": 4.972916666666667e-06,
      "loss": 0.0024,
      "step": 216130
    },
    {
      "epoch": 7.204666666666666,
      "grad_norm": 0.14311809837818146,
      "learning_rate": 4.970833333333333e-06,
      "loss": 0.0015,
      "step": 216140
    },
    {
      "epoch": 7.205,
      "grad_norm": 0.08638444542884827,
      "learning_rate": 4.9687500000000005e-06,
      "loss": 0.0019,
      "step": 216150
    },
    {
      "epoch": 7.205333333333333,
      "grad_norm": 0.2285732477903366,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0011,
      "step": 216160
    },
    {
      "epoch": 7.205666666666667,
      "grad_norm": 0.0778847262263298,
      "learning_rate": 4.964583333333333e-06,
      "loss": 0.0017,
      "step": 216170
    },
    {
      "epoch": 7.206,
      "grad_norm": 0.14528296887874603,
      "learning_rate": 4.9625e-06,
      "loss": 0.0011,
      "step": 216180
    },
    {
      "epoch": 7.206333333333333,
      "grad_norm": 0.08621840924024582,
      "learning_rate": 4.960416666666667e-06,
      "loss": 0.0019,
      "step": 216190
    },
    {
      "epoch": 7.206666666666667,
      "grad_norm": 0.1711677759885788,
      "learning_rate": 4.958333333333334e-06,
      "loss": 0.0016,
      "step": 216200
    },
    {
      "epoch": 7.207,
      "grad_norm": 0.2284606248140335,
      "learning_rate": 4.95625e-06,
      "loss": 0.0017,
      "step": 216210
    },
    {
      "epoch": 7.207333333333334,
      "grad_norm": 0.02891085296869278,
      "learning_rate": 4.9541666666666665e-06,
      "loss": 0.0014,
      "step": 216220
    },
    {
      "epoch": 7.207666666666666,
      "grad_norm": 0.19546353816986084,
      "learning_rate": 4.952083333333334e-06,
      "loss": 0.0018,
      "step": 216230
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.14403662085533142,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.0014,
      "step": 216240
    },
    {
      "epoch": 7.208333333333333,
      "grad_norm": 0.4279625117778778,
      "learning_rate": 4.947916666666666e-06,
      "loss": 0.0013,
      "step": 216250
    },
    {
      "epoch": 7.208666666666667,
      "grad_norm": 0.008662437088787556,
      "learning_rate": 4.9458333333333336e-06,
      "loss": 0.0015,
      "step": 216260
    },
    {
      "epoch": 7.209,
      "grad_norm": 0.25189951062202454,
      "learning_rate": 4.943750000000001e-06,
      "loss": 0.0018,
      "step": 216270
    },
    {
      "epoch": 7.209333333333333,
      "grad_norm": 0.12203140556812286,
      "learning_rate": 4.941666666666667e-06,
      "loss": 0.0013,
      "step": 216280
    },
    {
      "epoch": 7.209666666666667,
      "grad_norm": 0.03037211112678051,
      "learning_rate": 4.939583333333333e-06,
      "loss": 0.0021,
      "step": 216290
    },
    {
      "epoch": 7.21,
      "grad_norm": 0.34219080209732056,
      "learning_rate": 4.937500000000001e-06,
      "loss": 0.0018,
      "step": 216300
    },
    {
      "epoch": 7.210333333333334,
      "grad_norm": 0.05800910294055939,
      "learning_rate": 4.935416666666667e-06,
      "loss": 0.0021,
      "step": 216310
    },
    {
      "epoch": 7.210666666666667,
      "grad_norm": 0.08639523386955261,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0014,
      "step": 216320
    },
    {
      "epoch": 7.211,
      "grad_norm": 0.14391134679317474,
      "learning_rate": 4.9312500000000004e-06,
      "loss": 0.0016,
      "step": 216330
    },
    {
      "epoch": 7.211333333333333,
      "grad_norm": 0.057984527200460434,
      "learning_rate": 4.929166666666667e-06,
      "loss": 0.0017,
      "step": 216340
    },
    {
      "epoch": 7.211666666666667,
      "grad_norm": 0.20030789077281952,
      "learning_rate": 4.927083333333334e-06,
      "loss": 0.0021,
      "step": 216350
    },
    {
      "epoch": 7.212,
      "grad_norm": 0.22805564105510712,
      "learning_rate": 4.925e-06,
      "loss": 0.0012,
      "step": 216360
    },
    {
      "epoch": 7.2123333333333335,
      "grad_norm": 0.30325302481651306,
      "learning_rate": 4.922916666666667e-06,
      "loss": 0.0014,
      "step": 216370
    },
    {
      "epoch": 7.212666666666666,
      "grad_norm": 0.2856348156929016,
      "learning_rate": 4.920833333333334e-06,
      "loss": 0.0014,
      "step": 216380
    },
    {
      "epoch": 7.213,
      "grad_norm": 0.08950009942054749,
      "learning_rate": 4.91875e-06,
      "loss": 0.0017,
      "step": 216390
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 0.06380206346511841,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.0016,
      "step": 216400
    },
    {
      "epoch": 7.213666666666667,
      "grad_norm": 0.058193057775497437,
      "learning_rate": 4.914583333333334e-06,
      "loss": 0.0011,
      "step": 216410
    },
    {
      "epoch": 7.214,
      "grad_norm": 0.25793060660362244,
      "learning_rate": 4.912500000000001e-06,
      "loss": 0.002,
      "step": 216420
    },
    {
      "epoch": 7.214333333333333,
      "grad_norm": 0.2572163939476013,
      "learning_rate": 4.910416666666666e-06,
      "loss": 0.0017,
      "step": 216430
    },
    {
      "epoch": 7.214666666666667,
      "grad_norm": 0.2278977483510971,
      "learning_rate": 4.9083333333333335e-06,
      "loss": 0.0014,
      "step": 216440
    },
    {
      "epoch": 7.215,
      "grad_norm": 0.19993595778942108,
      "learning_rate": 4.906250000000001e-06,
      "loss": 0.0016,
      "step": 216450
    },
    {
      "epoch": 7.215333333333334,
      "grad_norm": 0.08656814694404602,
      "learning_rate": 4.904166666666667e-06,
      "loss": 0.0011,
      "step": 216460
    },
    {
      "epoch": 7.2156666666666665,
      "grad_norm": 0.0864519327878952,
      "learning_rate": 4.902083333333333e-06,
      "loss": 0.0018,
      "step": 216470
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.11484461277723312,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0014,
      "step": 216480
    },
    {
      "epoch": 7.216333333333333,
      "grad_norm": 0.12792107462882996,
      "learning_rate": 4.897916666666667e-06,
      "loss": 0.0019,
      "step": 216490
    },
    {
      "epoch": 7.216666666666667,
      "grad_norm": 0.06028955802321434,
      "learning_rate": 4.895833333333333e-06,
      "loss": 0.0021,
      "step": 216500
    },
    {
      "epoch": 7.217,
      "grad_norm": 0.1438508778810501,
      "learning_rate": 4.89375e-06,
      "loss": 0.0016,
      "step": 216510
    },
    {
      "epoch": 7.217333333333333,
      "grad_norm": 0.2570757269859314,
      "learning_rate": 4.891666666666667e-06,
      "loss": 0.0013,
      "step": 216520
    },
    {
      "epoch": 7.217666666666666,
      "grad_norm": 0.013095364905893803,
      "learning_rate": 4.889583333333334e-06,
      "loss": 0.0022,
      "step": 216530
    },
    {
      "epoch": 7.218,
      "grad_norm": 0.03536996990442276,
      "learning_rate": 4.8875e-06,
      "loss": 0.0027,
      "step": 216540
    },
    {
      "epoch": 7.218333333333334,
      "grad_norm": 0.019658323377370834,
      "learning_rate": 4.8854166666666665e-06,
      "loss": 0.0019,
      "step": 216550
    },
    {
      "epoch": 7.218666666666667,
      "grad_norm": 0.31374120712280273,
      "learning_rate": 4.883333333333334e-06,
      "loss": 0.002,
      "step": 216560
    },
    {
      "epoch": 7.219,
      "grad_norm": 0.2850421369075775,
      "learning_rate": 4.88125e-06,
      "loss": 0.0025,
      "step": 216570
    },
    {
      "epoch": 7.219333333333333,
      "grad_norm": 0.4029289782047272,
      "learning_rate": 4.879166666666666e-06,
      "loss": 0.0016,
      "step": 216580
    },
    {
      "epoch": 7.219666666666667,
      "grad_norm": 0.08570073544979095,
      "learning_rate": 4.877083333333334e-06,
      "loss": 0.0018,
      "step": 216590
    },
    {
      "epoch": 7.22,
      "grad_norm": 0.029753053560853004,
      "learning_rate": 4.875000000000001e-06,
      "loss": 0.0016,
      "step": 216600
    },
    {
      "epoch": 7.2203333333333335,
      "grad_norm": 0.11470086127519608,
      "learning_rate": 4.872916666666666e-06,
      "loss": 0.0015,
      "step": 216610
    },
    {
      "epoch": 7.220666666666666,
      "grad_norm": 0.057862646877765656,
      "learning_rate": 4.8708333333333334e-06,
      "loss": 0.0013,
      "step": 216620
    },
    {
      "epoch": 7.221,
      "grad_norm": 0.1441957950592041,
      "learning_rate": 4.868750000000001e-06,
      "loss": 0.0015,
      "step": 216630
    },
    {
      "epoch": 7.221333333333333,
      "grad_norm": 0.2934170365333557,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0015,
      "step": 216640
    },
    {
      "epoch": 7.221666666666667,
      "grad_norm": 0.4947230815887451,
      "learning_rate": 4.864583333333333e-06,
      "loss": 0.0017,
      "step": 216650
    },
    {
      "epoch": 7.222,
      "grad_norm": 0.07794134318828583,
      "learning_rate": 4.8625000000000005e-06,
      "loss": 0.0013,
      "step": 216660
    },
    {
      "epoch": 7.222333333333333,
      "grad_norm": 0.17149952054023743,
      "learning_rate": 4.860416666666667e-06,
      "loss": 0.0014,
      "step": 216670
    },
    {
      "epoch": 7.222666666666667,
      "grad_norm": 0.3554362952709198,
      "learning_rate": 4.858333333333333e-06,
      "loss": 0.0017,
      "step": 216680
    },
    {
      "epoch": 7.223,
      "grad_norm": 0.11732283979654312,
      "learning_rate": 4.85625e-06,
      "loss": 0.0021,
      "step": 216690
    },
    {
      "epoch": 7.223333333333334,
      "grad_norm": 0.03735054284334183,
      "learning_rate": 4.854166666666667e-06,
      "loss": 0.0012,
      "step": 216700
    },
    {
      "epoch": 7.2236666666666665,
      "grad_norm": 0.28574076294898987,
      "learning_rate": 4.852083333333334e-06,
      "loss": 0.0024,
      "step": 216710
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.1709691286087036,
      "learning_rate": 4.85e-06,
      "loss": 0.0013,
      "step": 216720
    },
    {
      "epoch": 7.224333333333333,
      "grad_norm": 0.11449439078569412,
      "learning_rate": 4.8479166666666665e-06,
      "loss": 0.0015,
      "step": 216730
    },
    {
      "epoch": 7.224666666666667,
      "grad_norm": 0.1707315742969513,
      "learning_rate": 4.845833333333334e-06,
      "loss": 0.0029,
      "step": 216740
    },
    {
      "epoch": 7.225,
      "grad_norm": 0.14453579485416412,
      "learning_rate": 4.84375e-06,
      "loss": 0.0012,
      "step": 216750
    },
    {
      "epoch": 7.225333333333333,
      "grad_norm": 0.1144268736243248,
      "learning_rate": 4.841666666666666e-06,
      "loss": 0.0015,
      "step": 216760
    },
    {
      "epoch": 7.225666666666666,
      "grad_norm": 0.05786285549402237,
      "learning_rate": 4.8395833333333335e-06,
      "loss": 0.0019,
      "step": 216770
    },
    {
      "epoch": 7.226,
      "grad_norm": 0.11473572254180908,
      "learning_rate": 4.837500000000001e-06,
      "loss": 0.002,
      "step": 216780
    },
    {
      "epoch": 7.226333333333334,
      "grad_norm": 0.08976180851459503,
      "learning_rate": 4.835416666666666e-06,
      "loss": 0.0024,
      "step": 216790
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 0.029960663989186287,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0019,
      "step": 216800
    },
    {
      "epoch": 7.227,
      "grad_norm": 0.23421338200569153,
      "learning_rate": 4.8312500000000005e-06,
      "loss": 0.0032,
      "step": 216810
    },
    {
      "epoch": 7.227333333333333,
      "grad_norm": 0.03535446897149086,
      "learning_rate": 4.829166666666667e-06,
      "loss": 0.0014,
      "step": 216820
    },
    {
      "epoch": 7.227666666666667,
      "grad_norm": 0.1718725860118866,
      "learning_rate": 4.827083333333333e-06,
      "loss": 0.0017,
      "step": 216830
    },
    {
      "epoch": 7.228,
      "grad_norm": 0.08611495792865753,
      "learning_rate": 4.825e-06,
      "loss": 0.0016,
      "step": 216840
    },
    {
      "epoch": 7.2283333333333335,
      "grad_norm": 0.12103558331727982,
      "learning_rate": 4.822916666666667e-06,
      "loss": 0.0017,
      "step": 216850
    },
    {
      "epoch": 7.228666666666666,
      "grad_norm": 0.3434538245201111,
      "learning_rate": 4.820833333333333e-06,
      "loss": 0.0023,
      "step": 216860
    },
    {
      "epoch": 7.229,
      "grad_norm": 0.22875437140464783,
      "learning_rate": 4.81875e-06,
      "loss": 0.0019,
      "step": 216870
    },
    {
      "epoch": 7.229333333333333,
      "grad_norm": 0.1428118795156479,
      "learning_rate": 4.816666666666667e-06,
      "loss": 0.0014,
      "step": 216880
    },
    {
      "epoch": 7.229666666666667,
      "grad_norm": 0.05944059416651726,
      "learning_rate": 4.814583333333334e-06,
      "loss": 0.0015,
      "step": 216890
    },
    {
      "epoch": 7.23,
      "grad_norm": 0.11527284234762192,
      "learning_rate": 4.8125e-06,
      "loss": 0.0015,
      "step": 216900
    },
    {
      "epoch": 7.230333333333333,
      "grad_norm": 0.086458720266819,
      "learning_rate": 4.810416666666667e-06,
      "loss": 0.0014,
      "step": 216910
    },
    {
      "epoch": 7.230666666666667,
      "grad_norm": 0.14312487840652466,
      "learning_rate": 4.808333333333334e-06,
      "loss": 0.0019,
      "step": 216920
    },
    {
      "epoch": 7.231,
      "grad_norm": 0.08706911653280258,
      "learning_rate": 4.80625e-06,
      "loss": 0.0012,
      "step": 216930
    },
    {
      "epoch": 7.231333333333334,
      "grad_norm": 0.058325327932834625,
      "learning_rate": 4.804166666666667e-06,
      "loss": 0.0022,
      "step": 216940
    },
    {
      "epoch": 7.2316666666666665,
      "grad_norm": 0.11491944640874863,
      "learning_rate": 4.8020833333333334e-06,
      "loss": 0.0013,
      "step": 216950
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.20006707310676575,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0031,
      "step": 216960
    },
    {
      "epoch": 7.232333333333333,
      "grad_norm": 0.05795633792877197,
      "learning_rate": 4.797916666666667e-06,
      "loss": 0.0021,
      "step": 216970
    },
    {
      "epoch": 7.232666666666667,
      "grad_norm": 0.1773063987493515,
      "learning_rate": 4.795833333333333e-06,
      "loss": 0.0014,
      "step": 216980
    },
    {
      "epoch": 7.233,
      "grad_norm": 0.03163651004433632,
      "learning_rate": 4.7937500000000005e-06,
      "loss": 0.002,
      "step": 216990
    },
    {
      "epoch": 7.233333333333333,
      "grad_norm": 0.014074211940169334,
      "learning_rate": 4.791666666666667e-06,
      "loss": 0.0012,
      "step": 217000
    },
    {
      "epoch": 7.233666666666666,
      "grad_norm": 0.114292673766613,
      "learning_rate": 4.789583333333333e-06,
      "loss": 0.0013,
      "step": 217010
    },
    {
      "epoch": 7.234,
      "grad_norm": 0.17162318527698517,
      "learning_rate": 4.7875e-06,
      "loss": 0.0016,
      "step": 217020
    },
    {
      "epoch": 7.234333333333334,
      "grad_norm": 0.09083782136440277,
      "learning_rate": 4.7854166666666675e-06,
      "loss": 0.0019,
      "step": 217030
    },
    {
      "epoch": 7.234666666666667,
      "grad_norm": 0.2431448996067047,
      "learning_rate": 4.783333333333333e-06,
      "loss": 0.0024,
      "step": 217040
    },
    {
      "epoch": 7.235,
      "grad_norm": 0.013348512351512909,
      "learning_rate": 4.78125e-06,
      "loss": 0.0016,
      "step": 217050
    },
    {
      "epoch": 7.235333333333333,
      "grad_norm": 0.030096035450696945,
      "learning_rate": 4.779166666666667e-06,
      "loss": 0.0017,
      "step": 217060
    },
    {
      "epoch": 7.235666666666667,
      "grad_norm": 0.18301767110824585,
      "learning_rate": 4.777083333333334e-06,
      "loss": 0.0013,
      "step": 217070
    },
    {
      "epoch": 7.236,
      "grad_norm": 0.09357097744941711,
      "learning_rate": 4.775e-06,
      "loss": 0.0014,
      "step": 217080
    },
    {
      "epoch": 7.2363333333333335,
      "grad_norm": 0.4694973826408386,
      "learning_rate": 4.772916666666667e-06,
      "loss": 0.0016,
      "step": 217090
    },
    {
      "epoch": 7.236666666666666,
      "grad_norm": 0.0578727200627327,
      "learning_rate": 4.7708333333333335e-06,
      "loss": 0.0018,
      "step": 217100
    },
    {
      "epoch": 7.237,
      "grad_norm": 0.3147750198841095,
      "learning_rate": 4.768750000000001e-06,
      "loss": 0.0015,
      "step": 217110
    },
    {
      "epoch": 7.237333333333333,
      "grad_norm": 0.14357583224773407,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.0015,
      "step": 217120
    },
    {
      "epoch": 7.237666666666667,
      "grad_norm": 0.014705114997923374,
      "learning_rate": 4.764583333333333e-06,
      "loss": 0.0012,
      "step": 217130
    },
    {
      "epoch": 7.2379999999999995,
      "grad_norm": 0.010770568624138832,
      "learning_rate": 4.7625000000000006e-06,
      "loss": 0.0016,
      "step": 217140
    },
    {
      "epoch": 7.238333333333333,
      "grad_norm": 0.25651946663856506,
      "learning_rate": 4.760416666666667e-06,
      "loss": 0.0013,
      "step": 217150
    },
    {
      "epoch": 7.238666666666667,
      "grad_norm": 0.08723191171884537,
      "learning_rate": 4.758333333333333e-06,
      "loss": 0.0029,
      "step": 217160
    },
    {
      "epoch": 7.239,
      "grad_norm": 0.08624435216188431,
      "learning_rate": 4.75625e-06,
      "loss": 0.0015,
      "step": 217170
    },
    {
      "epoch": 7.239333333333334,
      "grad_norm": 0.22832134366035461,
      "learning_rate": 4.754166666666668e-06,
      "loss": 0.0022,
      "step": 217180
    },
    {
      "epoch": 7.2396666666666665,
      "grad_norm": 0.11646279692649841,
      "learning_rate": 4.752083333333333e-06,
      "loss": 0.0017,
      "step": 217190
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.28501564264297485,
      "learning_rate": 4.75e-06,
      "loss": 0.0013,
      "step": 217200
    },
    {
      "epoch": 7.240333333333333,
      "grad_norm": 0.012161550112068653,
      "learning_rate": 4.7479166666666674e-06,
      "loss": 0.0016,
      "step": 217210
    },
    {
      "epoch": 7.240666666666667,
      "grad_norm": 0.1424769014120102,
      "learning_rate": 4.745833333333334e-06,
      "loss": 0.0015,
      "step": 217220
    },
    {
      "epoch": 7.241,
      "grad_norm": 0.010172749869525433,
      "learning_rate": 4.74375e-06,
      "loss": 0.0017,
      "step": 217230
    },
    {
      "epoch": 7.241333333333333,
      "grad_norm": 0.03333988040685654,
      "learning_rate": 4.741666666666667e-06,
      "loss": 0.0019,
      "step": 217240
    },
    {
      "epoch": 7.241666666666666,
      "grad_norm": 0.25777360796928406,
      "learning_rate": 4.739583333333334e-06,
      "loss": 0.0021,
      "step": 217250
    },
    {
      "epoch": 7.242,
      "grad_norm": 0.033540014177560806,
      "learning_rate": 4.7375e-06,
      "loss": 0.0016,
      "step": 217260
    },
    {
      "epoch": 7.242333333333334,
      "grad_norm": 0.05025109648704529,
      "learning_rate": 4.735416666666667e-06,
      "loss": 0.0019,
      "step": 217270
    },
    {
      "epoch": 7.242666666666667,
      "grad_norm": 0.11463358253240585,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0014,
      "step": 217280
    },
    {
      "epoch": 7.243,
      "grad_norm": 0.03026564233005047,
      "learning_rate": 4.731250000000001e-06,
      "loss": 0.0017,
      "step": 217290
    },
    {
      "epoch": 7.243333333333333,
      "grad_norm": 0.17193129658699036,
      "learning_rate": 4.729166666666667e-06,
      "loss": 0.0017,
      "step": 217300
    },
    {
      "epoch": 7.243666666666667,
      "grad_norm": 0.20300160348415375,
      "learning_rate": 4.727083333333333e-06,
      "loss": 0.0015,
      "step": 217310
    },
    {
      "epoch": 7.244,
      "grad_norm": 0.39910104870796204,
      "learning_rate": 4.7250000000000005e-06,
      "loss": 0.0014,
      "step": 217320
    },
    {
      "epoch": 7.2443333333333335,
      "grad_norm": 0.2563982903957367,
      "learning_rate": 4.722916666666667e-06,
      "loss": 0.0022,
      "step": 217330
    },
    {
      "epoch": 7.244666666666666,
      "grad_norm": 0.11807981133460999,
      "learning_rate": 4.720833333333333e-06,
      "loss": 0.0013,
      "step": 217340
    },
    {
      "epoch": 7.245,
      "grad_norm": 0.28584253787994385,
      "learning_rate": 4.71875e-06,
      "loss": 0.0021,
      "step": 217350
    },
    {
      "epoch": 7.245333333333333,
      "grad_norm": 0.22799192368984222,
      "learning_rate": 4.7166666666666675e-06,
      "loss": 0.0018,
      "step": 217360
    },
    {
      "epoch": 7.245666666666667,
      "grad_norm": 0.008720610290765762,
      "learning_rate": 4.714583333333333e-06,
      "loss": 0.0015,
      "step": 217370
    },
    {
      "epoch": 7.246,
      "grad_norm": 0.3425085246562958,
      "learning_rate": 4.7125e-06,
      "loss": 0.0021,
      "step": 217380
    },
    {
      "epoch": 7.246333333333333,
      "grad_norm": 0.11592771857976913,
      "learning_rate": 4.710416666666667e-06,
      "loss": 0.0014,
      "step": 217390
    },
    {
      "epoch": 7.246666666666667,
      "grad_norm": 0.17190299928188324,
      "learning_rate": 4.708333333333334e-06,
      "loss": 0.0027,
      "step": 217400
    },
    {
      "epoch": 7.247,
      "grad_norm": 0.11405885219573975,
      "learning_rate": 4.70625e-06,
      "loss": 0.0021,
      "step": 217410
    },
    {
      "epoch": 7.247333333333334,
      "grad_norm": 0.19957853853702545,
      "learning_rate": 4.704166666666667e-06,
      "loss": 0.0015,
      "step": 217420
    },
    {
      "epoch": 7.2476666666666665,
      "grad_norm": 0.08683355897665024,
      "learning_rate": 4.7020833333333336e-06,
      "loss": 0.0012,
      "step": 217430
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.03042025677859783,
      "learning_rate": 4.7e-06,
      "loss": 0.0019,
      "step": 217440
    },
    {
      "epoch": 7.248333333333333,
      "grad_norm": 0.34297099709510803,
      "learning_rate": 4.697916666666667e-06,
      "loss": 0.0011,
      "step": 217450
    },
    {
      "epoch": 7.248666666666667,
      "grad_norm": 0.05725075304508209,
      "learning_rate": 4.695833333333333e-06,
      "loss": 0.0015,
      "step": 217460
    },
    {
      "epoch": 7.249,
      "grad_norm": 0.14394095540046692,
      "learning_rate": 4.693750000000001e-06,
      "loss": 0.0016,
      "step": 217470
    },
    {
      "epoch": 7.249333333333333,
      "grad_norm": 0.17117738723754883,
      "learning_rate": 4.691666666666667e-06,
      "loss": 0.0021,
      "step": 217480
    },
    {
      "epoch": 7.249666666666666,
      "grad_norm": 0.34328407049179077,
      "learning_rate": 4.689583333333333e-06,
      "loss": 0.0024,
      "step": 217490
    },
    {
      "epoch": 7.25,
      "grad_norm": 0.17076720297336578,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 0.0014,
      "step": 217500
    },
    {
      "epoch": 7.250333333333334,
      "grad_norm": 0.12069519609212875,
      "learning_rate": 4.685416666666667e-06,
      "loss": 0.0016,
      "step": 217510
    },
    {
      "epoch": 7.250666666666667,
      "grad_norm": 0.08622140437364578,
      "learning_rate": 4.683333333333333e-06,
      "loss": 0.0019,
      "step": 217520
    },
    {
      "epoch": 7.251,
      "grad_norm": 0.0881907269358635,
      "learning_rate": 4.68125e-06,
      "loss": 0.0012,
      "step": 217530
    },
    {
      "epoch": 7.251333333333333,
      "grad_norm": 0.17157016694545746,
      "learning_rate": 4.6791666666666675e-06,
      "loss": 0.0022,
      "step": 217540
    },
    {
      "epoch": 7.251666666666667,
      "grad_norm": 0.08635345846414566,
      "learning_rate": 4.677083333333333e-06,
      "loss": 0.0012,
      "step": 217550
    },
    {
      "epoch": 7.252,
      "grad_norm": 0.17647536098957062,
      "learning_rate": 4.675e-06,
      "loss": 0.0022,
      "step": 217560
    },
    {
      "epoch": 7.2523333333333335,
      "grad_norm": 0.25701797008514404,
      "learning_rate": 4.672916666666667e-06,
      "loss": 0.0012,
      "step": 217570
    },
    {
      "epoch": 7.252666666666666,
      "grad_norm": 0.1998106837272644,
      "learning_rate": 4.670833333333334e-06,
      "loss": 0.0018,
      "step": 217580
    },
    {
      "epoch": 7.253,
      "grad_norm": 0.08660896122455597,
      "learning_rate": 4.66875e-06,
      "loss": 0.0017,
      "step": 217590
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.17198699712753296,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0019,
      "step": 217600
    },
    {
      "epoch": 7.253666666666667,
      "grad_norm": 0.17127257585525513,
      "learning_rate": 4.6645833333333335e-06,
      "loss": 0.0014,
      "step": 217610
    },
    {
      "epoch": 7.254,
      "grad_norm": 0.03235580399632454,
      "learning_rate": 4.6625e-06,
      "loss": 0.0025,
      "step": 217620
    },
    {
      "epoch": 7.254333333333333,
      "grad_norm": 0.03043709509074688,
      "learning_rate": 4.660416666666667e-06,
      "loss": 0.0021,
      "step": 217630
    },
    {
      "epoch": 7.254666666666667,
      "grad_norm": 0.26695510745048523,
      "learning_rate": 4.658333333333333e-06,
      "loss": 0.0017,
      "step": 217640
    },
    {
      "epoch": 7.255,
      "grad_norm": 0.02954707108438015,
      "learning_rate": 4.6562500000000005e-06,
      "loss": 0.0021,
      "step": 217650
    },
    {
      "epoch": 7.255333333333334,
      "grad_norm": 0.03677920252084732,
      "learning_rate": 4.654166666666667e-06,
      "loss": 0.0022,
      "step": 217660
    },
    {
      "epoch": 7.2556666666666665,
      "grad_norm": 0.058646585792303085,
      "learning_rate": 4.652083333333333e-06,
      "loss": 0.0023,
      "step": 217670
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.08702671527862549,
      "learning_rate": 4.65e-06,
      "loss": 0.0027,
      "step": 217680
    },
    {
      "epoch": 7.256333333333333,
      "grad_norm": 0.059260547161102295,
      "learning_rate": 4.647916666666667e-06,
      "loss": 0.0022,
      "step": 217690
    },
    {
      "epoch": 7.256666666666667,
      "grad_norm": 0.142875075340271,
      "learning_rate": 4.645833333333333e-06,
      "loss": 0.0015,
      "step": 217700
    },
    {
      "epoch": 7.257,
      "grad_norm": 0.08643266558647156,
      "learning_rate": 4.64375e-06,
      "loss": 0.002,
      "step": 217710
    },
    {
      "epoch": 7.257333333333333,
      "grad_norm": 0.17167246341705322,
      "learning_rate": 4.641666666666667e-06,
      "loss": 0.0017,
      "step": 217720
    },
    {
      "epoch": 7.257666666666666,
      "grad_norm": 0.17135527729988098,
      "learning_rate": 4.639583333333333e-06,
      "loss": 0.0015,
      "step": 217730
    },
    {
      "epoch": 7.258,
      "grad_norm": 0.14308369159698486,
      "learning_rate": 4.6375e-06,
      "loss": 0.0024,
      "step": 217740
    },
    {
      "epoch": 7.258333333333334,
      "grad_norm": 0.22944703698158264,
      "learning_rate": 4.635416666666667e-06,
      "loss": 0.0013,
      "step": 217750
    },
    {
      "epoch": 7.258666666666667,
      "grad_norm": 0.2573554217815399,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0016,
      "step": 217760
    },
    {
      "epoch": 7.259,
      "grad_norm": 0.0876525491476059,
      "learning_rate": 4.63125e-06,
      "loss": 0.0014,
      "step": 217770
    },
    {
      "epoch": 7.259333333333333,
      "grad_norm": 0.313274085521698,
      "learning_rate": 4.629166666666667e-06,
      "loss": 0.0013,
      "step": 217780
    },
    {
      "epoch": 7.259666666666667,
      "grad_norm": 0.05764291435480118,
      "learning_rate": 4.6270833333333334e-06,
      "loss": 0.0016,
      "step": 217790
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.370757520198822,
      "learning_rate": 4.625e-06,
      "loss": 0.0017,
      "step": 217800
    },
    {
      "epoch": 7.2603333333333335,
      "grad_norm": 0.22867900133132935,
      "learning_rate": 4.622916666666667e-06,
      "loss": 0.0021,
      "step": 217810
    },
    {
      "epoch": 7.260666666666666,
      "grad_norm": 0.2576468288898468,
      "learning_rate": 4.620833333333333e-06,
      "loss": 0.0015,
      "step": 217820
    },
    {
      "epoch": 7.261,
      "grad_norm": 0.0856781080365181,
      "learning_rate": 4.6187500000000005e-06,
      "loss": 0.0013,
      "step": 217830
    },
    {
      "epoch": 7.261333333333333,
      "grad_norm": 0.0864008292555809,
      "learning_rate": 4.616666666666667e-06,
      "loss": 0.001,
      "step": 217840
    },
    {
      "epoch": 7.261666666666667,
      "grad_norm": 0.10891946405172348,
      "learning_rate": 4.614583333333333e-06,
      "loss": 0.0015,
      "step": 217850
    },
    {
      "epoch": 7.2620000000000005,
      "grad_norm": 0.0576828233897686,
      "learning_rate": 4.6125e-06,
      "loss": 0.0018,
      "step": 217860
    },
    {
      "epoch": 7.262333333333333,
      "grad_norm": 0.27315542101860046,
      "learning_rate": 4.610416666666667e-06,
      "loss": 0.0018,
      "step": 217870
    },
    {
      "epoch": 7.262666666666667,
      "grad_norm": 0.08830941468477249,
      "learning_rate": 4.608333333333333e-06,
      "loss": 0.0016,
      "step": 217880
    },
    {
      "epoch": 7.263,
      "grad_norm": 0.11472763121128082,
      "learning_rate": 4.60625e-06,
      "loss": 0.0013,
      "step": 217890
    },
    {
      "epoch": 7.263333333333334,
      "grad_norm": 0.14246992766857147,
      "learning_rate": 4.604166666666667e-06,
      "loss": 0.0017,
      "step": 217900
    },
    {
      "epoch": 7.2636666666666665,
      "grad_norm": 0.19940420985221863,
      "learning_rate": 4.602083333333334e-06,
      "loss": 0.0011,
      "step": 217910
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.14302562177181244,
      "learning_rate": 4.6e-06,
      "loss": 0.002,
      "step": 217920
    },
    {
      "epoch": 7.264333333333333,
      "grad_norm": 0.14255169034004211,
      "learning_rate": 4.597916666666667e-06,
      "loss": 0.0018,
      "step": 217930
    },
    {
      "epoch": 7.264666666666667,
      "grad_norm": 0.1719265729188919,
      "learning_rate": 4.5958333333333335e-06,
      "loss": 0.0013,
      "step": 217940
    },
    {
      "epoch": 7.265,
      "grad_norm": 0.08615843951702118,
      "learning_rate": 4.59375e-06,
      "loss": 0.0014,
      "step": 217950
    },
    {
      "epoch": 7.265333333333333,
      "grad_norm": 0.11674050241708755,
      "learning_rate": 4.591666666666667e-06,
      "loss": 0.0014,
      "step": 217960
    },
    {
      "epoch": 7.265666666666666,
      "grad_norm": 0.25792038440704346,
      "learning_rate": 4.589583333333333e-06,
      "loss": 0.0019,
      "step": 217970
    },
    {
      "epoch": 7.266,
      "grad_norm": 0.4846019148826599,
      "learning_rate": 4.5875000000000005e-06,
      "loss": 0.0011,
      "step": 217980
    },
    {
      "epoch": 7.266333333333334,
      "grad_norm": 0.029092131182551384,
      "learning_rate": 4.585416666666667e-06,
      "loss": 0.0015,
      "step": 217990
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.342254102230072,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.0015,
      "step": 218000
    },
    {
      "epoch": 7.267,
      "grad_norm": 0.19898948073387146,
      "learning_rate": 4.58125e-06,
      "loss": 0.0015,
      "step": 218010
    },
    {
      "epoch": 7.267333333333333,
      "grad_norm": 0.059393662959337234,
      "learning_rate": 4.579166666666667e-06,
      "loss": 0.0016,
      "step": 218020
    },
    {
      "epoch": 7.267666666666667,
      "grad_norm": 0.20508785545825958,
      "learning_rate": 4.577083333333333e-06,
      "loss": 0.0023,
      "step": 218030
    },
    {
      "epoch": 7.268,
      "grad_norm": 0.05891698971390724,
      "learning_rate": 4.575e-06,
      "loss": 0.0016,
      "step": 218040
    },
    {
      "epoch": 7.2683333333333335,
      "grad_norm": 0.08620216697454453,
      "learning_rate": 4.5729166666666674e-06,
      "loss": 0.0019,
      "step": 218050
    },
    {
      "epoch": 7.268666666666666,
      "grad_norm": 0.07668007910251617,
      "learning_rate": 4.570833333333333e-06,
      "loss": 0.0012,
      "step": 218060
    },
    {
      "epoch": 7.269,
      "grad_norm": 0.007602666970342398,
      "learning_rate": 4.56875e-06,
      "loss": 0.0016,
      "step": 218070
    },
    {
      "epoch": 7.269333333333333,
      "grad_norm": 0.0706486701965332,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0014,
      "step": 218080
    },
    {
      "epoch": 7.269666666666667,
      "grad_norm": 0.07578639686107635,
      "learning_rate": 4.564583333333334e-06,
      "loss": 0.0015,
      "step": 218090
    },
    {
      "epoch": 7.27,
      "grad_norm": 0.06875622272491455,
      "learning_rate": 4.5625e-06,
      "loss": 0.0024,
      "step": 218100
    },
    {
      "epoch": 7.270333333333333,
      "grad_norm": 0.040765129029750824,
      "learning_rate": 4.560416666666667e-06,
      "loss": 0.0015,
      "step": 218110
    },
    {
      "epoch": 7.270666666666667,
      "grad_norm": 0.08653663843870163,
      "learning_rate": 4.5583333333333335e-06,
      "loss": 0.0014,
      "step": 218120
    },
    {
      "epoch": 7.271,
      "grad_norm": 0.20054133236408234,
      "learning_rate": 4.55625e-06,
      "loss": 0.0013,
      "step": 218130
    },
    {
      "epoch": 7.271333333333334,
      "grad_norm": 0.11448774486780167,
      "learning_rate": 4.554166666666667e-06,
      "loss": 0.0014,
      "step": 218140
    },
    {
      "epoch": 7.2716666666666665,
      "grad_norm": 0.2850903272628784,
      "learning_rate": 4.552083333333334e-06,
      "loss": 0.002,
      "step": 218150
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.22391201555728912,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 0.0014,
      "step": 218160
    },
    {
      "epoch": 7.272333333333333,
      "grad_norm": 0.1142520159482956,
      "learning_rate": 4.547916666666667e-06,
      "loss": 0.0016,
      "step": 218170
    },
    {
      "epoch": 7.272666666666667,
      "grad_norm": 0.22612138092517853,
      "learning_rate": 4.545833333333334e-06,
      "loss": 0.002,
      "step": 218180
    },
    {
      "epoch": 7.273,
      "grad_norm": 0.1380263864994049,
      "learning_rate": 4.54375e-06,
      "loss": 0.0019,
      "step": 218190
    },
    {
      "epoch": 7.273333333333333,
      "grad_norm": 0.3431072235107422,
      "learning_rate": 4.541666666666667e-06,
      "loss": 0.0018,
      "step": 218200
    },
    {
      "epoch": 7.273666666666666,
      "grad_norm": 0.11416026204824448,
      "learning_rate": 4.539583333333334e-06,
      "loss": 0.0019,
      "step": 218210
    },
    {
      "epoch": 7.274,
      "grad_norm": 0.37082260847091675,
      "learning_rate": 4.5375e-06,
      "loss": 0.0016,
      "step": 218220
    },
    {
      "epoch": 7.274333333333333,
      "grad_norm": 0.08580140024423599,
      "learning_rate": 4.535416666666667e-06,
      "loss": 0.0013,
      "step": 218230
    },
    {
      "epoch": 7.274666666666667,
      "grad_norm": 0.05827556923031807,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0017,
      "step": 218240
    },
    {
      "epoch": 7.275,
      "grad_norm": 0.08677516132593155,
      "learning_rate": 4.53125e-06,
      "loss": 0.0014,
      "step": 218250
    },
    {
      "epoch": 7.275333333333333,
      "grad_norm": 0.014355846680700779,
      "learning_rate": 4.529166666666667e-06,
      "loss": 0.0014,
      "step": 218260
    },
    {
      "epoch": 7.275666666666667,
      "grad_norm": 0.08802519738674164,
      "learning_rate": 4.5270833333333335e-06,
      "loss": 0.0013,
      "step": 218270
    },
    {
      "epoch": 7.276,
      "grad_norm": 0.006936838384717703,
      "learning_rate": 4.525e-06,
      "loss": 0.0016,
      "step": 218280
    },
    {
      "epoch": 7.2763333333333335,
      "grad_norm": 0.22809334099292755,
      "learning_rate": 4.522916666666667e-06,
      "loss": 0.0017,
      "step": 218290
    },
    {
      "epoch": 7.276666666666666,
      "grad_norm": 0.18378955125808716,
      "learning_rate": 4.520833333333334e-06,
      "loss": 0.0018,
      "step": 218300
    },
    {
      "epoch": 7.277,
      "grad_norm": 0.5142040848731995,
      "learning_rate": 4.51875e-06,
      "loss": 0.0017,
      "step": 218310
    },
    {
      "epoch": 7.277333333333333,
      "grad_norm": 0.2856018543243408,
      "learning_rate": 4.516666666666667e-06,
      "loss": 0.0018,
      "step": 218320
    },
    {
      "epoch": 7.277666666666667,
      "grad_norm": 0.5133231282234192,
      "learning_rate": 4.514583333333334e-06,
      "loss": 0.0015,
      "step": 218330
    },
    {
      "epoch": 7.2780000000000005,
      "grad_norm": 0.11376266926527023,
      "learning_rate": 4.5125e-06,
      "loss": 0.0015,
      "step": 218340
    },
    {
      "epoch": 7.278333333333333,
      "grad_norm": 0.030295446515083313,
      "learning_rate": 4.510416666666667e-06,
      "loss": 0.0023,
      "step": 218350
    },
    {
      "epoch": 7.278666666666667,
      "grad_norm": 0.034456148743629456,
      "learning_rate": 4.508333333333334e-06,
      "loss": 0.0012,
      "step": 218360
    },
    {
      "epoch": 7.279,
      "grad_norm": 0.05847218260169029,
      "learning_rate": 4.50625e-06,
      "loss": 0.0016,
      "step": 218370
    },
    {
      "epoch": 7.279333333333334,
      "grad_norm": 0.23039031028747559,
      "learning_rate": 4.504166666666667e-06,
      "loss": 0.0021,
      "step": 218380
    },
    {
      "epoch": 7.2796666666666665,
      "grad_norm": 0.5547195076942444,
      "learning_rate": 4.502083333333334e-06,
      "loss": 0.0022,
      "step": 218390
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.11521876603364944,
      "learning_rate": 4.5e-06,
      "loss": 0.0019,
      "step": 218400
    },
    {
      "epoch": 7.280333333333333,
      "grad_norm": 0.1711016297340393,
      "learning_rate": 4.497916666666667e-06,
      "loss": 0.0014,
      "step": 218410
    },
    {
      "epoch": 7.280666666666667,
      "grad_norm": 0.1717468947172165,
      "learning_rate": 4.495833333333334e-06,
      "loss": 0.0015,
      "step": 218420
    },
    {
      "epoch": 7.281,
      "grad_norm": 0.031168842688202858,
      "learning_rate": 4.49375e-06,
      "loss": 0.0016,
      "step": 218430
    },
    {
      "epoch": 7.281333333333333,
      "grad_norm": 0.0300652626901865,
      "learning_rate": 4.491666666666667e-06,
      "loss": 0.0024,
      "step": 218440
    },
    {
      "epoch": 7.281666666666666,
      "grad_norm": 0.19978931546211243,
      "learning_rate": 4.4895833333333335e-06,
      "loss": 0.0026,
      "step": 218450
    },
    {
      "epoch": 7.282,
      "grad_norm": 0.17146413028240204,
      "learning_rate": 4.4875e-06,
      "loss": 0.0016,
      "step": 218460
    },
    {
      "epoch": 7.282333333333334,
      "grad_norm": 0.11423534154891968,
      "learning_rate": 4.485416666666667e-06,
      "loss": 0.0016,
      "step": 218470
    },
    {
      "epoch": 7.282666666666667,
      "grad_norm": 0.284844309091568,
      "learning_rate": 4.483333333333334e-06,
      "loss": 0.0015,
      "step": 218480
    },
    {
      "epoch": 7.283,
      "grad_norm": 0.11496036499738693,
      "learning_rate": 4.48125e-06,
      "loss": 0.0011,
      "step": 218490
    },
    {
      "epoch": 7.283333333333333,
      "grad_norm": 0.057330500334501266,
      "learning_rate": 4.479166666666667e-06,
      "loss": 0.0018,
      "step": 218500
    },
    {
      "epoch": 7.283666666666667,
      "grad_norm": 0.11458265036344528,
      "learning_rate": 4.477083333333334e-06,
      "loss": 0.0015,
      "step": 218510
    },
    {
      "epoch": 7.284,
      "grad_norm": 0.28632479906082153,
      "learning_rate": 4.475e-06,
      "loss": 0.0014,
      "step": 218520
    },
    {
      "epoch": 7.2843333333333335,
      "grad_norm": 0.03229141980409622,
      "learning_rate": 4.472916666666667e-06,
      "loss": 0.0013,
      "step": 218530
    },
    {
      "epoch": 7.284666666666666,
      "grad_norm": 0.008604566566646099,
      "learning_rate": 4.470833333333334e-06,
      "loss": 0.0015,
      "step": 218540
    },
    {
      "epoch": 7.285,
      "grad_norm": 0.2580118179321289,
      "learning_rate": 4.46875e-06,
      "loss": 0.0015,
      "step": 218550
    },
    {
      "epoch": 7.285333333333333,
      "grad_norm": 0.010955681093037128,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.002,
      "step": 218560
    },
    {
      "epoch": 7.285666666666667,
      "grad_norm": 0.1424286663532257,
      "learning_rate": 4.464583333333334e-06,
      "loss": 0.0015,
      "step": 218570
    },
    {
      "epoch": 7.286,
      "grad_norm": 0.3154999017715454,
      "learning_rate": 4.4625e-06,
      "loss": 0.0017,
      "step": 218580
    },
    {
      "epoch": 7.286333333333333,
      "grad_norm": 0.17136673629283905,
      "learning_rate": 4.460416666666667e-06,
      "loss": 0.0015,
      "step": 218590
    },
    {
      "epoch": 7.286666666666667,
      "grad_norm": 0.2851846516132355,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 0.0015,
      "step": 218600
    },
    {
      "epoch": 7.287,
      "grad_norm": 0.007253893651068211,
      "learning_rate": 4.45625e-06,
      "loss": 0.0011,
      "step": 218610
    },
    {
      "epoch": 7.287333333333334,
      "grad_norm": 0.1587093621492386,
      "learning_rate": 4.454166666666667e-06,
      "loss": 0.0016,
      "step": 218620
    },
    {
      "epoch": 7.2876666666666665,
      "grad_norm": 0.05803298577666283,
      "learning_rate": 4.452083333333333e-06,
      "loss": 0.0016,
      "step": 218630
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.1435047835111618,
      "learning_rate": 4.45e-06,
      "loss": 0.0015,
      "step": 218640
    },
    {
      "epoch": 7.288333333333333,
      "grad_norm": 0.20220880210399628,
      "learning_rate": 4.447916666666667e-06,
      "loss": 0.0016,
      "step": 218650
    },
    {
      "epoch": 7.288666666666667,
      "grad_norm": 0.17445853352546692,
      "learning_rate": 4.445833333333334e-06,
      "loss": 0.0017,
      "step": 218660
    },
    {
      "epoch": 7.289,
      "grad_norm": 0.17117521166801453,
      "learning_rate": 4.44375e-06,
      "loss": 0.0016,
      "step": 218670
    },
    {
      "epoch": 7.289333333333333,
      "grad_norm": 0.14274317026138306,
      "learning_rate": 4.441666666666667e-06,
      "loss": 0.0013,
      "step": 218680
    },
    {
      "epoch": 7.289666666666666,
      "grad_norm": 0.17154954373836517,
      "learning_rate": 4.439583333333334e-06,
      "loss": 0.0015,
      "step": 218690
    },
    {
      "epoch": 7.29,
      "grad_norm": 0.20124934613704681,
      "learning_rate": 4.4375e-06,
      "loss": 0.0019,
      "step": 218700
    },
    {
      "epoch": 7.290333333333333,
      "grad_norm": 0.14352154731750488,
      "learning_rate": 4.435416666666667e-06,
      "loss": 0.0014,
      "step": 218710
    },
    {
      "epoch": 7.290666666666667,
      "grad_norm": 0.13626018166542053,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0019,
      "step": 218720
    },
    {
      "epoch": 7.291,
      "grad_norm": 0.17237994074821472,
      "learning_rate": 4.43125e-06,
      "loss": 0.0022,
      "step": 218730
    },
    {
      "epoch": 7.291333333333333,
      "grad_norm": 0.22936423122882843,
      "learning_rate": 4.4291666666666665e-06,
      "loss": 0.0011,
      "step": 218740
    },
    {
      "epoch": 7.291666666666667,
      "grad_norm": 0.08763203769922256,
      "learning_rate": 4.427083333333334e-06,
      "loss": 0.0012,
      "step": 218750
    },
    {
      "epoch": 7.292,
      "grad_norm": 0.2282639741897583,
      "learning_rate": 4.425e-06,
      "loss": 0.0018,
      "step": 218760
    },
    {
      "epoch": 7.292333333333334,
      "grad_norm": 0.22948575019836426,
      "learning_rate": 4.422916666666667e-06,
      "loss": 0.0012,
      "step": 218770
    },
    {
      "epoch": 7.292666666666666,
      "grad_norm": 0.2566918432712555,
      "learning_rate": 4.4208333333333335e-06,
      "loss": 0.0016,
      "step": 218780
    },
    {
      "epoch": 7.293,
      "grad_norm": 0.14267021417617798,
      "learning_rate": 4.41875e-06,
      "loss": 0.002,
      "step": 218790
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 0.08676108717918396,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.0014,
      "step": 218800
    },
    {
      "epoch": 7.293666666666667,
      "grad_norm": 0.031114408746361732,
      "learning_rate": 4.414583333333333e-06,
      "loss": 0.0014,
      "step": 218810
    },
    {
      "epoch": 7.294,
      "grad_norm": 0.05994905158877373,
      "learning_rate": 4.4125e-06,
      "loss": 0.0014,
      "step": 218820
    },
    {
      "epoch": 7.294333333333333,
      "grad_norm": 0.0878707617521286,
      "learning_rate": 4.410416666666667e-06,
      "loss": 0.0014,
      "step": 218830
    },
    {
      "epoch": 7.294666666666667,
      "grad_norm": 0.2849648594856262,
      "learning_rate": 4.408333333333334e-06,
      "loss": 0.0013,
      "step": 218840
    },
    {
      "epoch": 7.295,
      "grad_norm": 0.20069338381290436,
      "learning_rate": 4.40625e-06,
      "loss": 0.0023,
      "step": 218850
    },
    {
      "epoch": 7.295333333333334,
      "grad_norm": 0.48569655418395996,
      "learning_rate": 4.404166666666667e-06,
      "loss": 0.0015,
      "step": 218860
    },
    {
      "epoch": 7.2956666666666665,
      "grad_norm": 0.14352518320083618,
      "learning_rate": 4.402083333333334e-06,
      "loss": 0.0012,
      "step": 218870
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.058758869767189026,
      "learning_rate": 4.4e-06,
      "loss": 0.0017,
      "step": 218880
    },
    {
      "epoch": 7.296333333333333,
      "grad_norm": 0.17403851449489594,
      "learning_rate": 4.3979166666666666e-06,
      "loss": 0.0015,
      "step": 218890
    },
    {
      "epoch": 7.296666666666667,
      "grad_norm": 0.0872865542769432,
      "learning_rate": 4.395833333333334e-06,
      "loss": 0.0018,
      "step": 218900
    },
    {
      "epoch": 7.297,
      "grad_norm": 0.20144407451152802,
      "learning_rate": 4.39375e-06,
      "loss": 0.0025,
      "step": 218910
    },
    {
      "epoch": 7.2973333333333334,
      "grad_norm": 0.4000532925128937,
      "learning_rate": 4.391666666666667e-06,
      "loss": 0.0013,
      "step": 218920
    },
    {
      "epoch": 7.297666666666666,
      "grad_norm": 0.39966142177581787,
      "learning_rate": 4.389583333333334e-06,
      "loss": 0.0019,
      "step": 218930
    },
    {
      "epoch": 7.298,
      "grad_norm": 0.15878906846046448,
      "learning_rate": 4.3875e-06,
      "loss": 0.0013,
      "step": 218940
    },
    {
      "epoch": 7.298333333333334,
      "grad_norm": 0.2570109963417053,
      "learning_rate": 4.385416666666667e-06,
      "loss": 0.0018,
      "step": 218950
    },
    {
      "epoch": 7.298666666666667,
      "grad_norm": 0.22847118973731995,
      "learning_rate": 4.3833333333333334e-06,
      "loss": 0.0014,
      "step": 218960
    },
    {
      "epoch": 7.299,
      "grad_norm": 0.05782601609826088,
      "learning_rate": 4.38125e-06,
      "loss": 0.0017,
      "step": 218970
    },
    {
      "epoch": 7.299333333333333,
      "grad_norm": 0.05720451846718788,
      "learning_rate": 4.379166666666667e-06,
      "loss": 0.0015,
      "step": 218980
    },
    {
      "epoch": 7.299666666666667,
      "grad_norm": 0.228220596909523,
      "learning_rate": 4.377083333333334e-06,
      "loss": 0.0017,
      "step": 218990
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.27196383476257324,
      "learning_rate": 4.375e-06,
      "loss": 0.0015,
      "step": 219000
    },
    {
      "epoch": 7.300333333333334,
      "grad_norm": 0.4235834777355194,
      "learning_rate": 4.372916666666667e-06,
      "loss": 0.0019,
      "step": 219010
    },
    {
      "epoch": 7.300666666666666,
      "grad_norm": 0.25530388951301575,
      "learning_rate": 4.370833333333334e-06,
      "loss": 0.0012,
      "step": 219020
    },
    {
      "epoch": 7.301,
      "grad_norm": 0.02071419730782509,
      "learning_rate": 4.36875e-06,
      "loss": 0.0023,
      "step": 219030
    },
    {
      "epoch": 7.301333333333333,
      "grad_norm": 0.2756878435611725,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.0014,
      "step": 219040
    },
    {
      "epoch": 7.301666666666667,
      "grad_norm": 0.19972501695156097,
      "learning_rate": 4.364583333333334e-06,
      "loss": 0.002,
      "step": 219050
    },
    {
      "epoch": 7.302,
      "grad_norm": 0.25694718956947327,
      "learning_rate": 4.3625e-06,
      "loss": 0.0016,
      "step": 219060
    },
    {
      "epoch": 7.302333333333333,
      "grad_norm": 0.057842060923576355,
      "learning_rate": 4.3604166666666665e-06,
      "loss": 0.0018,
      "step": 219070
    },
    {
      "epoch": 7.302666666666667,
      "grad_norm": 0.3268339931964874,
      "learning_rate": 4.358333333333334e-06,
      "loss": 0.0012,
      "step": 219080
    },
    {
      "epoch": 7.303,
      "grad_norm": 0.14606259763240814,
      "learning_rate": 4.35625e-06,
      "loss": 0.0014,
      "step": 219090
    },
    {
      "epoch": 7.303333333333334,
      "grad_norm": 0.11553998291492462,
      "learning_rate": 4.354166666666667e-06,
      "loss": 0.0023,
      "step": 219100
    },
    {
      "epoch": 7.3036666666666665,
      "grad_norm": 0.2292405664920807,
      "learning_rate": 4.3520833333333335e-06,
      "loss": 0.0016,
      "step": 219110
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.17115628719329834,
      "learning_rate": 4.35e-06,
      "loss": 0.0014,
      "step": 219120
    },
    {
      "epoch": 7.304333333333333,
      "grad_norm": 0.20068451762199402,
      "learning_rate": 4.347916666666667e-06,
      "loss": 0.0014,
      "step": 219130
    },
    {
      "epoch": 7.304666666666667,
      "grad_norm": 0.0865919217467308,
      "learning_rate": 4.345833333333333e-06,
      "loss": 0.0016,
      "step": 219140
    },
    {
      "epoch": 7.305,
      "grad_norm": 0.029325565323233604,
      "learning_rate": 4.34375e-06,
      "loss": 0.0022,
      "step": 219150
    },
    {
      "epoch": 7.3053333333333335,
      "grad_norm": 0.4563409984111786,
      "learning_rate": 4.341666666666667e-06,
      "loss": 0.0026,
      "step": 219160
    },
    {
      "epoch": 7.305666666666666,
      "grad_norm": 0.28578004240989685,
      "learning_rate": 4.339583333333334e-06,
      "loss": 0.0015,
      "step": 219170
    },
    {
      "epoch": 7.306,
      "grad_norm": 0.32184526324272156,
      "learning_rate": 4.3374999999999996e-06,
      "loss": 0.0014,
      "step": 219180
    },
    {
      "epoch": 7.306333333333333,
      "grad_norm": 0.22869563102722168,
      "learning_rate": 4.335416666666667e-06,
      "loss": 0.0025,
      "step": 219190
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 0.058734796941280365,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0012,
      "step": 219200
    },
    {
      "epoch": 7.307,
      "grad_norm": 0.0295944232493639,
      "learning_rate": 4.33125e-06,
      "loss": 0.0017,
      "step": 219210
    },
    {
      "epoch": 7.307333333333333,
      "grad_norm": 0.15566875040531158,
      "learning_rate": 4.329166666666667e-06,
      "loss": 0.0018,
      "step": 219220
    },
    {
      "epoch": 7.307666666666667,
      "grad_norm": 0.12779399752616882,
      "learning_rate": 4.327083333333334e-06,
      "loss": 0.0017,
      "step": 219230
    },
    {
      "epoch": 7.308,
      "grad_norm": 0.11488468199968338,
      "learning_rate": 4.325e-06,
      "loss": 0.0034,
      "step": 219240
    },
    {
      "epoch": 7.308333333333334,
      "grad_norm": 0.03033156879246235,
      "learning_rate": 4.3229166666666664e-06,
      "loss": 0.0018,
      "step": 219250
    },
    {
      "epoch": 7.308666666666666,
      "grad_norm": 0.26483744382858276,
      "learning_rate": 4.320833333333334e-06,
      "loss": 0.0025,
      "step": 219260
    },
    {
      "epoch": 7.309,
      "grad_norm": 0.14548231661319733,
      "learning_rate": 4.31875e-06,
      "loss": 0.0016,
      "step": 219270
    },
    {
      "epoch": 7.309333333333333,
      "grad_norm": 0.14340336620807648,
      "learning_rate": 4.316666666666667e-06,
      "loss": 0.0018,
      "step": 219280
    },
    {
      "epoch": 7.309666666666667,
      "grad_norm": 0.11513978242874146,
      "learning_rate": 4.3145833333333335e-06,
      "loss": 0.0023,
      "step": 219290
    },
    {
      "epoch": 7.31,
      "grad_norm": 0.10860685259103775,
      "learning_rate": 4.3125e-06,
      "loss": 0.0024,
      "step": 219300
    },
    {
      "epoch": 7.310333333333333,
      "grad_norm": 0.16227589547634125,
      "learning_rate": 4.310416666666667e-06,
      "loss": 0.0014,
      "step": 219310
    },
    {
      "epoch": 7.310666666666666,
      "grad_norm": 0.17194250226020813,
      "learning_rate": 4.308333333333333e-06,
      "loss": 0.0018,
      "step": 219320
    },
    {
      "epoch": 7.311,
      "grad_norm": 0.25644776225090027,
      "learning_rate": 4.30625e-06,
      "loss": 0.002,
      "step": 219330
    },
    {
      "epoch": 7.311333333333334,
      "grad_norm": 0.1719796508550644,
      "learning_rate": 4.304166666666667e-06,
      "loss": 0.0016,
      "step": 219340
    },
    {
      "epoch": 7.3116666666666665,
      "grad_norm": 0.3984028100967407,
      "learning_rate": 4.302083333333334e-06,
      "loss": 0.0025,
      "step": 219350
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.14263057708740234,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0015,
      "step": 219360
    },
    {
      "epoch": 7.312333333333333,
      "grad_norm": 0.2498234361410141,
      "learning_rate": 4.297916666666667e-06,
      "loss": 0.0014,
      "step": 219370
    },
    {
      "epoch": 7.312666666666667,
      "grad_norm": 0.2572120130062103,
      "learning_rate": 4.295833333333334e-06,
      "loss": 0.0023,
      "step": 219380
    },
    {
      "epoch": 7.313,
      "grad_norm": 0.07396088540554047,
      "learning_rate": 4.29375e-06,
      "loss": 0.0017,
      "step": 219390
    },
    {
      "epoch": 7.3133333333333335,
      "grad_norm": 0.08573070913553238,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 0.0021,
      "step": 219400
    },
    {
      "epoch": 7.313666666666666,
      "grad_norm": 0.1459132581949234,
      "learning_rate": 4.289583333333334e-06,
      "loss": 0.0014,
      "step": 219410
    },
    {
      "epoch": 7.314,
      "grad_norm": 0.023378852754831314,
      "learning_rate": 4.287500000000001e-06,
      "loss": 0.0016,
      "step": 219420
    },
    {
      "epoch": 7.314333333333334,
      "grad_norm": 0.1844586282968521,
      "learning_rate": 4.285416666666666e-06,
      "loss": 0.0018,
      "step": 219430
    },
    {
      "epoch": 7.314666666666667,
      "grad_norm": 0.2850225567817688,
      "learning_rate": 4.2833333333333335e-06,
      "loss": 0.002,
      "step": 219440
    },
    {
      "epoch": 7.315,
      "grad_norm": 0.14341269433498383,
      "learning_rate": 4.281250000000001e-06,
      "loss": 0.0017,
      "step": 219450
    },
    {
      "epoch": 7.315333333333333,
      "grad_norm": 0.2084059715270996,
      "learning_rate": 4.279166666666667e-06,
      "loss": 0.0015,
      "step": 219460
    },
    {
      "epoch": 7.315666666666667,
      "grad_norm": 0.1998971551656723,
      "learning_rate": 4.277083333333333e-06,
      "loss": 0.0013,
      "step": 219470
    },
    {
      "epoch": 7.316,
      "grad_norm": 0.058915942907333374,
      "learning_rate": 4.2750000000000006e-06,
      "loss": 0.0017,
      "step": 219480
    },
    {
      "epoch": 7.316333333333334,
      "grad_norm": 0.2011125385761261,
      "learning_rate": 4.272916666666667e-06,
      "loss": 0.0014,
      "step": 219490
    },
    {
      "epoch": 7.316666666666666,
      "grad_norm": 0.28628066182136536,
      "learning_rate": 4.270833333333333e-06,
      "loss": 0.0027,
      "step": 219500
    },
    {
      "epoch": 7.317,
      "grad_norm": 0.10567891597747803,
      "learning_rate": 4.26875e-06,
      "loss": 0.0016,
      "step": 219510
    },
    {
      "epoch": 7.317333333333333,
      "grad_norm": 0.4014606475830078,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0016,
      "step": 219520
    },
    {
      "epoch": 7.317666666666667,
      "grad_norm": 0.03347071260213852,
      "learning_rate": 4.264583333333334e-06,
      "loss": 0.0018,
      "step": 219530
    },
    {
      "epoch": 7.318,
      "grad_norm": 0.22800752520561218,
      "learning_rate": 4.2625e-06,
      "loss": 0.0015,
      "step": 219540
    },
    {
      "epoch": 7.318333333333333,
      "grad_norm": 0.20050738751888275,
      "learning_rate": 4.260416666666667e-06,
      "loss": 0.0016,
      "step": 219550
    },
    {
      "epoch": 7.318666666666667,
      "grad_norm": 0.2859055697917938,
      "learning_rate": 4.258333333333334e-06,
      "loss": 0.0014,
      "step": 219560
    },
    {
      "epoch": 7.319,
      "grad_norm": 0.08618073910474777,
      "learning_rate": 4.25625e-06,
      "loss": 0.0017,
      "step": 219570
    },
    {
      "epoch": 7.319333333333334,
      "grad_norm": 0.17230091989040375,
      "learning_rate": 4.2541666666666665e-06,
      "loss": 0.002,
      "step": 219580
    },
    {
      "epoch": 7.3196666666666665,
      "grad_norm": 0.2285841554403305,
      "learning_rate": 4.252083333333334e-06,
      "loss": 0.0014,
      "step": 219590
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.08629915118217468,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.0019,
      "step": 219600
    },
    {
      "epoch": 7.320333333333333,
      "grad_norm": 0.03058685176074505,
      "learning_rate": 4.247916666666666e-06,
      "loss": 0.0016,
      "step": 219610
    },
    {
      "epoch": 7.320666666666667,
      "grad_norm": 0.03111928142607212,
      "learning_rate": 4.2458333333333335e-06,
      "loss": 0.0023,
      "step": 219620
    },
    {
      "epoch": 7.321,
      "grad_norm": 0.05918791517615318,
      "learning_rate": 4.243750000000001e-06,
      "loss": 0.0012,
      "step": 219630
    },
    {
      "epoch": 7.3213333333333335,
      "grad_norm": 0.34252995252609253,
      "learning_rate": 4.241666666666667e-06,
      "loss": 0.0017,
      "step": 219640
    },
    {
      "epoch": 7.321666666666666,
      "grad_norm": 0.03051770105957985,
      "learning_rate": 4.239583333333333e-06,
      "loss": 0.0012,
      "step": 219650
    },
    {
      "epoch": 7.322,
      "grad_norm": 0.2567015588283539,
      "learning_rate": 4.2375000000000005e-06,
      "loss": 0.0018,
      "step": 219660
    },
    {
      "epoch": 7.322333333333333,
      "grad_norm": 0.011730778031051159,
      "learning_rate": 4.235416666666667e-06,
      "loss": 0.0019,
      "step": 219670
    },
    {
      "epoch": 7.322666666666667,
      "grad_norm": 0.22849702835083008,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.0017,
      "step": 219680
    },
    {
      "epoch": 7.323,
      "grad_norm": 0.513285756111145,
      "learning_rate": 4.23125e-06,
      "loss": 0.0016,
      "step": 219690
    },
    {
      "epoch": 7.323333333333333,
      "grad_norm": 0.3988282382488251,
      "learning_rate": 4.229166666666667e-06,
      "loss": 0.0023,
      "step": 219700
    },
    {
      "epoch": 7.323666666666667,
      "grad_norm": 0.39964085817337036,
      "learning_rate": 4.227083333333334e-06,
      "loss": 0.0021,
      "step": 219710
    },
    {
      "epoch": 7.324,
      "grad_norm": 0.08673505485057831,
      "learning_rate": 4.225e-06,
      "loss": 0.0017,
      "step": 219720
    },
    {
      "epoch": 7.324333333333334,
      "grad_norm": 0.12127956748008728,
      "learning_rate": 4.2229166666666665e-06,
      "loss": 0.0013,
      "step": 219730
    },
    {
      "epoch": 7.324666666666666,
      "grad_norm": 0.25726285576820374,
      "learning_rate": 4.220833333333334e-06,
      "loss": 0.0027,
      "step": 219740
    },
    {
      "epoch": 7.325,
      "grad_norm": 0.05845261737704277,
      "learning_rate": 4.218750000000001e-06,
      "loss": 0.0012,
      "step": 219750
    },
    {
      "epoch": 7.325333333333333,
      "grad_norm": 0.11553853750228882,
      "learning_rate": 4.216666666666666e-06,
      "loss": 0.0019,
      "step": 219760
    },
    {
      "epoch": 7.325666666666667,
      "grad_norm": 0.20034590363502502,
      "learning_rate": 4.2145833333333336e-06,
      "loss": 0.0017,
      "step": 219770
    },
    {
      "epoch": 7.326,
      "grad_norm": 0.057468995451927185,
      "learning_rate": 4.212500000000001e-06,
      "loss": 0.0015,
      "step": 219780
    },
    {
      "epoch": 7.326333333333333,
      "grad_norm": 0.20079238712787628,
      "learning_rate": 4.210416666666667e-06,
      "loss": 0.0017,
      "step": 219790
    },
    {
      "epoch": 7.326666666666666,
      "grad_norm": 0.11502258479595184,
      "learning_rate": 4.208333333333333e-06,
      "loss": 0.0019,
      "step": 219800
    },
    {
      "epoch": 7.327,
      "grad_norm": 0.029906168580055237,
      "learning_rate": 4.206250000000001e-06,
      "loss": 0.0016,
      "step": 219810
    },
    {
      "epoch": 7.327333333333334,
      "grad_norm": 0.2904335558414459,
      "learning_rate": 4.204166666666667e-06,
      "loss": 0.0019,
      "step": 219820
    },
    {
      "epoch": 7.3276666666666666,
      "grad_norm": 0.03254811838269234,
      "learning_rate": 4.202083333333333e-06,
      "loss": 0.0015,
      "step": 219830
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.14267224073410034,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0015,
      "step": 219840
    },
    {
      "epoch": 7.328333333333333,
      "grad_norm": 0.0316871739923954,
      "learning_rate": 4.197916666666667e-06,
      "loss": 0.0013,
      "step": 219850
    },
    {
      "epoch": 7.328666666666667,
      "grad_norm": 0.03079908899962902,
      "learning_rate": 4.195833333333334e-06,
      "loss": 0.0013,
      "step": 219860
    },
    {
      "epoch": 7.329,
      "grad_norm": 0.10769885778427124,
      "learning_rate": 4.19375e-06,
      "loss": 0.0014,
      "step": 219870
    },
    {
      "epoch": 7.3293333333333335,
      "grad_norm": 0.22908596694469452,
      "learning_rate": 4.191666666666667e-06,
      "loss": 0.0014,
      "step": 219880
    },
    {
      "epoch": 7.329666666666666,
      "grad_norm": 0.21525432169437408,
      "learning_rate": 4.189583333333334e-06,
      "loss": 0.0014,
      "step": 219890
    },
    {
      "epoch": 7.33,
      "grad_norm": 0.08595328778028488,
      "learning_rate": 4.1875e-06,
      "loss": 0.0018,
      "step": 219900
    },
    {
      "epoch": 7.330333333333333,
      "grad_norm": 0.0313309021294117,
      "learning_rate": 4.1854166666666665e-06,
      "loss": 0.002,
      "step": 219910
    },
    {
      "epoch": 7.330666666666667,
      "grad_norm": 0.03083309903740883,
      "learning_rate": 4.183333333333334e-06,
      "loss": 0.0013,
      "step": 219920
    },
    {
      "epoch": 7.331,
      "grad_norm": 0.1729097068309784,
      "learning_rate": 4.181250000000001e-06,
      "loss": 0.0013,
      "step": 219930
    },
    {
      "epoch": 7.331333333333333,
      "grad_norm": 0.08648760616779327,
      "learning_rate": 4.179166666666666e-06,
      "loss": 0.0015,
      "step": 219940
    },
    {
      "epoch": 7.331666666666667,
      "grad_norm": 0.2148566097021103,
      "learning_rate": 4.1770833333333335e-06,
      "loss": 0.0022,
      "step": 219950
    },
    {
      "epoch": 7.332,
      "grad_norm": 0.33417829871177673,
      "learning_rate": 4.175000000000001e-06,
      "loss": 0.0017,
      "step": 219960
    },
    {
      "epoch": 7.332333333333334,
      "grad_norm": 0.06865829974412918,
      "learning_rate": 4.172916666666667e-06,
      "loss": 0.0014,
      "step": 219970
    },
    {
      "epoch": 7.332666666666666,
      "grad_norm": 0.34289050102233887,
      "learning_rate": 4.170833333333333e-06,
      "loss": 0.0015,
      "step": 219980
    },
    {
      "epoch": 7.333,
      "grad_norm": 0.03001389279961586,
      "learning_rate": 4.1687500000000005e-06,
      "loss": 0.002,
      "step": 219990
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.08632813394069672,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0014,
      "step": 220000
    },
    {
      "epoch": 7.333666666666667,
      "grad_norm": 0.22904419898986816,
      "learning_rate": 4.164583333333333e-06,
      "loss": 0.0013,
      "step": 220010
    },
    {
      "epoch": 7.334,
      "grad_norm": 0.20060881972312927,
      "learning_rate": 4.1625e-06,
      "loss": 0.0013,
      "step": 220020
    },
    {
      "epoch": 7.334333333333333,
      "grad_norm": 0.010721254162490368,
      "learning_rate": 4.160416666666667e-06,
      "loss": 0.0022,
      "step": 220030
    },
    {
      "epoch": 7.334666666666667,
      "grad_norm": 0.030630866065621376,
      "learning_rate": 4.158333333333334e-06,
      "loss": 0.0013,
      "step": 220040
    },
    {
      "epoch": 7.335,
      "grad_norm": 0.17156179249286652,
      "learning_rate": 4.15625e-06,
      "loss": 0.0023,
      "step": 220050
    },
    {
      "epoch": 7.335333333333334,
      "grad_norm": 0.11521126329898834,
      "learning_rate": 4.1541666666666666e-06,
      "loss": 0.0021,
      "step": 220060
    },
    {
      "epoch": 7.335666666666667,
      "grad_norm": 0.3600398302078247,
      "learning_rate": 4.152083333333334e-06,
      "loss": 0.0018,
      "step": 220070
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.1148662269115448,
      "learning_rate": 4.15e-06,
      "loss": 0.0014,
      "step": 220080
    },
    {
      "epoch": 7.336333333333333,
      "grad_norm": 0.03228778392076492,
      "learning_rate": 4.147916666666666e-06,
      "loss": 0.0012,
      "step": 220090
    },
    {
      "epoch": 7.336666666666667,
      "grad_norm": 0.146020770072937,
      "learning_rate": 4.145833333333334e-06,
      "loss": 0.0015,
      "step": 220100
    },
    {
      "epoch": 7.337,
      "grad_norm": 0.05767695978283882,
      "learning_rate": 4.143750000000001e-06,
      "loss": 0.0021,
      "step": 220110
    },
    {
      "epoch": 7.3373333333333335,
      "grad_norm": 0.011134256608784199,
      "learning_rate": 4.141666666666666e-06,
      "loss": 0.0023,
      "step": 220120
    },
    {
      "epoch": 7.337666666666666,
      "grad_norm": 0.2675875723361969,
      "learning_rate": 4.1395833333333334e-06,
      "loss": 0.0019,
      "step": 220130
    },
    {
      "epoch": 7.338,
      "grad_norm": 0.01456278096884489,
      "learning_rate": 4.137500000000001e-06,
      "loss": 0.0013,
      "step": 220140
    },
    {
      "epoch": 7.338333333333333,
      "grad_norm": 0.588363766670227,
      "learning_rate": 4.135416666666667e-06,
      "loss": 0.0018,
      "step": 220150
    },
    {
      "epoch": 7.338666666666667,
      "grad_norm": 0.22831608355045319,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0013,
      "step": 220160
    },
    {
      "epoch": 7.339,
      "grad_norm": 0.058988336473703384,
      "learning_rate": 4.1312500000000005e-06,
      "loss": 0.0013,
      "step": 220170
    },
    {
      "epoch": 7.339333333333333,
      "grad_norm": 0.3724379539489746,
      "learning_rate": 4.129166666666667e-06,
      "loss": 0.0015,
      "step": 220180
    },
    {
      "epoch": 7.339666666666667,
      "grad_norm": 0.057323940098285675,
      "learning_rate": 4.127083333333333e-06,
      "loss": 0.0016,
      "step": 220190
    },
    {
      "epoch": 7.34,
      "grad_norm": 0.28557419776916504,
      "learning_rate": 4.125e-06,
      "loss": 0.0018,
      "step": 220200
    },
    {
      "epoch": 7.340333333333334,
      "grad_norm": 0.05784102529287338,
      "learning_rate": 4.122916666666667e-06,
      "loss": 0.0012,
      "step": 220210
    },
    {
      "epoch": 7.3406666666666665,
      "grad_norm": 0.08573856949806213,
      "learning_rate": 4.120833333333334e-06,
      "loss": 0.0013,
      "step": 220220
    },
    {
      "epoch": 7.341,
      "grad_norm": 0.08695460110902786,
      "learning_rate": 4.11875e-06,
      "loss": 0.0022,
      "step": 220230
    },
    {
      "epoch": 7.341333333333333,
      "grad_norm": 0.059017714112997055,
      "learning_rate": 4.1166666666666665e-06,
      "loss": 0.0012,
      "step": 220240
    },
    {
      "epoch": 7.341666666666667,
      "grad_norm": 0.16205507516860962,
      "learning_rate": 4.114583333333334e-06,
      "loss": 0.0028,
      "step": 220250
    },
    {
      "epoch": 7.342,
      "grad_norm": 0.1714065968990326,
      "learning_rate": 4.1125e-06,
      "loss": 0.0017,
      "step": 220260
    },
    {
      "epoch": 7.342333333333333,
      "grad_norm": 0.19993114471435547,
      "learning_rate": 4.110416666666666e-06,
      "loss": 0.0019,
      "step": 220270
    },
    {
      "epoch": 7.342666666666666,
      "grad_norm": 0.3146432936191559,
      "learning_rate": 4.1083333333333335e-06,
      "loss": 0.0025,
      "step": 220280
    },
    {
      "epoch": 7.343,
      "grad_norm": 0.20033974945545197,
      "learning_rate": 4.106250000000001e-06,
      "loss": 0.0012,
      "step": 220290
    },
    {
      "epoch": 7.343333333333334,
      "grad_norm": 0.058508239686489105,
      "learning_rate": 4.104166666666666e-06,
      "loss": 0.0016,
      "step": 220300
    },
    {
      "epoch": 7.343666666666667,
      "grad_norm": 0.01059250719845295,
      "learning_rate": 4.102083333333333e-06,
      "loss": 0.0016,
      "step": 220310
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.08614412695169449,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.0013,
      "step": 220320
    },
    {
      "epoch": 7.344333333333333,
      "grad_norm": 0.2286221981048584,
      "learning_rate": 4.097916666666667e-06,
      "loss": 0.0019,
      "step": 220330
    },
    {
      "epoch": 7.344666666666667,
      "grad_norm": 0.14911746978759766,
      "learning_rate": 4.095833333333333e-06,
      "loss": 0.0017,
      "step": 220340
    },
    {
      "epoch": 7.345,
      "grad_norm": 0.2851225435733795,
      "learning_rate": 4.09375e-06,
      "loss": 0.0019,
      "step": 220350
    },
    {
      "epoch": 7.3453333333333335,
      "grad_norm": 0.0301470085978508,
      "learning_rate": 4.091666666666667e-06,
      "loss": 0.0021,
      "step": 220360
    },
    {
      "epoch": 7.345666666666666,
      "grad_norm": 0.22785739600658417,
      "learning_rate": 4.089583333333333e-06,
      "loss": 0.0012,
      "step": 220370
    },
    {
      "epoch": 7.346,
      "grad_norm": 0.31622830033302307,
      "learning_rate": 4.0875e-06,
      "loss": 0.0015,
      "step": 220380
    },
    {
      "epoch": 7.346333333333333,
      "grad_norm": 0.4070633053779602,
      "learning_rate": 4.085416666666667e-06,
      "loss": 0.0026,
      "step": 220390
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 0.057582221925258636,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.0012,
      "step": 220400
    },
    {
      "epoch": 7.3469999999999995,
      "grad_norm": 0.009258835576474667,
      "learning_rate": 4.08125e-06,
      "loss": 0.0015,
      "step": 220410
    },
    {
      "epoch": 7.347333333333333,
      "grad_norm": 0.00967945996671915,
      "learning_rate": 4.0791666666666664e-06,
      "loss": 0.0016,
      "step": 220420
    },
    {
      "epoch": 7.347666666666667,
      "grad_norm": 0.166534885764122,
      "learning_rate": 4.077083333333334e-06,
      "loss": 0.0017,
      "step": 220430
    },
    {
      "epoch": 7.348,
      "grad_norm": 0.058090608566999435,
      "learning_rate": 4.075e-06,
      "loss": 0.0015,
      "step": 220440
    },
    {
      "epoch": 7.348333333333334,
      "grad_norm": 0.22016745805740356,
      "learning_rate": 4.072916666666666e-06,
      "loss": 0.0019,
      "step": 220450
    },
    {
      "epoch": 7.3486666666666665,
      "grad_norm": 0.40209105610847473,
      "learning_rate": 4.0708333333333335e-06,
      "loss": 0.0016,
      "step": 220460
    },
    {
      "epoch": 7.349,
      "grad_norm": 0.3604572117328644,
      "learning_rate": 4.068750000000001e-06,
      "loss": 0.0016,
      "step": 220470
    },
    {
      "epoch": 7.349333333333333,
      "grad_norm": 0.19983163475990295,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0013,
      "step": 220480
    },
    {
      "epoch": 7.349666666666667,
      "grad_norm": 0.2822340726852417,
      "learning_rate": 4.064583333333333e-06,
      "loss": 0.0018,
      "step": 220490
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.14295172691345215,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 0.002,
      "step": 220500
    },
    {
      "epoch": 7.350333333333333,
      "grad_norm": 0.030329734086990356,
      "learning_rate": 4.060416666666667e-06,
      "loss": 0.0016,
      "step": 220510
    },
    {
      "epoch": 7.350666666666667,
      "grad_norm": 0.02990049682557583,
      "learning_rate": 4.058333333333333e-06,
      "loss": 0.0016,
      "step": 220520
    },
    {
      "epoch": 7.351,
      "grad_norm": 0.22902336716651917,
      "learning_rate": 4.05625e-06,
      "loss": 0.0018,
      "step": 220530
    },
    {
      "epoch": 7.351333333333334,
      "grad_norm": 0.13370393216609955,
      "learning_rate": 4.054166666666667e-06,
      "loss": 0.0017,
      "step": 220540
    },
    {
      "epoch": 7.351666666666667,
      "grad_norm": 0.14394448697566986,
      "learning_rate": 4.052083333333333e-06,
      "loss": 0.0027,
      "step": 220550
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.19678376615047455,
      "learning_rate": 4.05e-06,
      "loss": 0.002,
      "step": 220560
    },
    {
      "epoch": 7.352333333333333,
      "grad_norm": 0.14397819340229034,
      "learning_rate": 4.0479166666666665e-06,
      "loss": 0.0018,
      "step": 220570
    },
    {
      "epoch": 7.352666666666667,
      "grad_norm": 0.05810028314590454,
      "learning_rate": 4.045833333333334e-06,
      "loss": 0.0017,
      "step": 220580
    },
    {
      "epoch": 7.353,
      "grad_norm": 0.14269790053367615,
      "learning_rate": 4.04375e-06,
      "loss": 0.0013,
      "step": 220590
    },
    {
      "epoch": 7.3533333333333335,
      "grad_norm": 0.059096455574035645,
      "learning_rate": 4.041666666666666e-06,
      "loss": 0.0019,
      "step": 220600
    },
    {
      "epoch": 7.353666666666666,
      "grad_norm": 0.257735937833786,
      "learning_rate": 4.0395833333333335e-06,
      "loss": 0.0016,
      "step": 220610
    },
    {
      "epoch": 7.354,
      "grad_norm": 0.25840893387794495,
      "learning_rate": 4.037500000000001e-06,
      "loss": 0.0014,
      "step": 220620
    },
    {
      "epoch": 7.354333333333333,
      "grad_norm": 0.08617455512285233,
      "learning_rate": 4.035416666666667e-06,
      "loss": 0.0018,
      "step": 220630
    },
    {
      "epoch": 7.354666666666667,
      "grad_norm": 0.44371485710144043,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0019,
      "step": 220640
    },
    {
      "epoch": 7.355,
      "grad_norm": 0.27893874049186707,
      "learning_rate": 4.031250000000001e-06,
      "loss": 0.0014,
      "step": 220650
    },
    {
      "epoch": 7.355333333333333,
      "grad_norm": 0.030845748260617256,
      "learning_rate": 4.029166666666667e-06,
      "loss": 0.0013,
      "step": 220660
    },
    {
      "epoch": 7.355666666666667,
      "grad_norm": 0.20495520532131195,
      "learning_rate": 4.027083333333333e-06,
      "loss": 0.0016,
      "step": 220670
    },
    {
      "epoch": 7.356,
      "grad_norm": 0.012427800334990025,
      "learning_rate": 4.0250000000000004e-06,
      "loss": 0.0016,
      "step": 220680
    },
    {
      "epoch": 7.356333333333334,
      "grad_norm": 0.2283778041601181,
      "learning_rate": 4.022916666666668e-06,
      "loss": 0.0021,
      "step": 220690
    },
    {
      "epoch": 7.3566666666666665,
      "grad_norm": 0.08757293224334717,
      "learning_rate": 4.020833333333333e-06,
      "loss": 0.0015,
      "step": 220700
    },
    {
      "epoch": 7.357,
      "grad_norm": 0.25846830010414124,
      "learning_rate": 4.01875e-06,
      "loss": 0.0021,
      "step": 220710
    },
    {
      "epoch": 7.357333333333333,
      "grad_norm": 0.14303363859653473,
      "learning_rate": 4.0166666666666675e-06,
      "loss": 0.0018,
      "step": 220720
    },
    {
      "epoch": 7.357666666666667,
      "grad_norm": 0.010552873834967613,
      "learning_rate": 4.014583333333334e-06,
      "loss": 0.0019,
      "step": 220730
    },
    {
      "epoch": 7.358,
      "grad_norm": 0.14326775074005127,
      "learning_rate": 4.0125e-06,
      "loss": 0.0026,
      "step": 220740
    },
    {
      "epoch": 7.358333333333333,
      "grad_norm": 0.31350088119506836,
      "learning_rate": 4.010416666666667e-06,
      "loss": 0.0017,
      "step": 220750
    },
    {
      "epoch": 7.358666666666666,
      "grad_norm": 0.31817442178726196,
      "learning_rate": 4.008333333333334e-06,
      "loss": 0.0021,
      "step": 220760
    },
    {
      "epoch": 7.359,
      "grad_norm": 0.33911803364753723,
      "learning_rate": 4.00625e-06,
      "loss": 0.0018,
      "step": 220770
    },
    {
      "epoch": 7.359333333333334,
      "grad_norm": 0.03681551292538643,
      "learning_rate": 4.004166666666667e-06,
      "loss": 0.0021,
      "step": 220780
    },
    {
      "epoch": 7.359666666666667,
      "grad_norm": 0.05801663175225258,
      "learning_rate": 4.0020833333333335e-06,
      "loss": 0.0018,
      "step": 220790
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.37217646837234497,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0021,
      "step": 220800
    },
    {
      "epoch": 7.360333333333333,
      "grad_norm": 0.03265544772148132,
      "learning_rate": 3.997916666666667e-06,
      "loss": 0.0014,
      "step": 220810
    },
    {
      "epoch": 7.360666666666667,
      "grad_norm": 0.2862720191478729,
      "learning_rate": 3.995833333333333e-06,
      "loss": 0.0018,
      "step": 220820
    },
    {
      "epoch": 7.361,
      "grad_norm": 0.11419032514095306,
      "learning_rate": 3.9937500000000005e-06,
      "loss": 0.0016,
      "step": 220830
    },
    {
      "epoch": 7.3613333333333335,
      "grad_norm": 0.25658807158470154,
      "learning_rate": 3.991666666666667e-06,
      "loss": 0.0018,
      "step": 220840
    },
    {
      "epoch": 7.361666666666666,
      "grad_norm": 0.11449301242828369,
      "learning_rate": 3.989583333333333e-06,
      "loss": 0.0015,
      "step": 220850
    },
    {
      "epoch": 7.362,
      "grad_norm": 0.05763415992259979,
      "learning_rate": 3.9875e-06,
      "loss": 0.0012,
      "step": 220860
    },
    {
      "epoch": 7.362333333333333,
      "grad_norm": 0.07511240988969803,
      "learning_rate": 3.9854166666666675e-06,
      "loss": 0.0022,
      "step": 220870
    },
    {
      "epoch": 7.362666666666667,
      "grad_norm": 0.05732213705778122,
      "learning_rate": 3.983333333333333e-06,
      "loss": 0.0015,
      "step": 220880
    },
    {
      "epoch": 7.3629999999999995,
      "grad_norm": 0.04735157638788223,
      "learning_rate": 3.98125e-06,
      "loss": 0.0013,
      "step": 220890
    },
    {
      "epoch": 7.363333333333333,
      "grad_norm": 0.34346622228622437,
      "learning_rate": 3.979166666666667e-06,
      "loss": 0.0018,
      "step": 220900
    },
    {
      "epoch": 7.363666666666667,
      "grad_norm": 0.08663934469223022,
      "learning_rate": 3.977083333333334e-06,
      "loss": 0.0015,
      "step": 220910
    },
    {
      "epoch": 7.364,
      "grad_norm": 0.0717850923538208,
      "learning_rate": 3.975e-06,
      "loss": 0.0012,
      "step": 220920
    },
    {
      "epoch": 7.364333333333334,
      "grad_norm": 0.0648084282875061,
      "learning_rate": 3.972916666666667e-06,
      "loss": 0.0015,
      "step": 220930
    },
    {
      "epoch": 7.3646666666666665,
      "grad_norm": 0.11519608646631241,
      "learning_rate": 3.9708333333333336e-06,
      "loss": 0.0012,
      "step": 220940
    },
    {
      "epoch": 7.365,
      "grad_norm": 0.1999131292104721,
      "learning_rate": 3.96875e-06,
      "loss": 0.0028,
      "step": 220950
    },
    {
      "epoch": 7.365333333333333,
      "grad_norm": 0.39934879541397095,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.002,
      "step": 220960
    },
    {
      "epoch": 7.365666666666667,
      "grad_norm": 0.3425990343093872,
      "learning_rate": 3.964583333333333e-06,
      "loss": 0.0015,
      "step": 220970
    },
    {
      "epoch": 7.366,
      "grad_norm": 0.08767644315958023,
      "learning_rate": 3.962500000000001e-06,
      "loss": 0.0026,
      "step": 220980
    },
    {
      "epoch": 7.366333333333333,
      "grad_norm": 0.2565578818321228,
      "learning_rate": 3.960416666666667e-06,
      "loss": 0.0023,
      "step": 220990
    },
    {
      "epoch": 7.366666666666666,
      "grad_norm": 0.1715364307165146,
      "learning_rate": 3.958333333333333e-06,
      "loss": 0.0011,
      "step": 221000
    },
    {
      "epoch": 7.367,
      "grad_norm": 0.17216917872428894,
      "learning_rate": 3.9562500000000004e-06,
      "loss": 0.0016,
      "step": 221010
    },
    {
      "epoch": 7.367333333333334,
      "grad_norm": 0.17179585993289948,
      "learning_rate": 3.954166666666667e-06,
      "loss": 0.002,
      "step": 221020
    },
    {
      "epoch": 7.367666666666667,
      "grad_norm": 0.07576211541891098,
      "learning_rate": 3.952083333333333e-06,
      "loss": 0.0017,
      "step": 221030
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.2692594528198242,
      "learning_rate": 3.95e-06,
      "loss": 0.0022,
      "step": 221040
    },
    {
      "epoch": 7.368333333333333,
      "grad_norm": 0.5172024965286255,
      "learning_rate": 3.9479166666666675e-06,
      "loss": 0.0019,
      "step": 221050
    },
    {
      "epoch": 7.368666666666667,
      "grad_norm": 0.17889507114887238,
      "learning_rate": 3.945833333333333e-06,
      "loss": 0.0018,
      "step": 221060
    },
    {
      "epoch": 7.369,
      "grad_norm": 0.22911350429058075,
      "learning_rate": 3.94375e-06,
      "loss": 0.002,
      "step": 221070
    },
    {
      "epoch": 7.3693333333333335,
      "grad_norm": 0.2854934334754944,
      "learning_rate": 3.941666666666667e-06,
      "loss": 0.0022,
      "step": 221080
    },
    {
      "epoch": 7.369666666666666,
      "grad_norm": 0.14422805607318878,
      "learning_rate": 3.939583333333334e-06,
      "loss": 0.0012,
      "step": 221090
    },
    {
      "epoch": 7.37,
      "grad_norm": 0.08667140454053879,
      "learning_rate": 3.9375e-06,
      "loss": 0.0012,
      "step": 221100
    },
    {
      "epoch": 7.370333333333333,
      "grad_norm": 0.14301013946533203,
      "learning_rate": 3.935416666666667e-06,
      "loss": 0.0013,
      "step": 221110
    },
    {
      "epoch": 7.370666666666667,
      "grad_norm": 0.03599173203110695,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0012,
      "step": 221120
    },
    {
      "epoch": 7.371,
      "grad_norm": 0.11613405495882034,
      "learning_rate": 3.93125e-06,
      "loss": 0.0018,
      "step": 221130
    },
    {
      "epoch": 7.371333333333333,
      "grad_norm": 0.012176447547972202,
      "learning_rate": 3.929166666666667e-06,
      "loss": 0.0016,
      "step": 221140
    },
    {
      "epoch": 7.371666666666667,
      "grad_norm": 0.11433153599500656,
      "learning_rate": 3.927083333333333e-06,
      "loss": 0.0021,
      "step": 221150
    },
    {
      "epoch": 7.372,
      "grad_norm": 0.030561616644263268,
      "learning_rate": 3.9250000000000005e-06,
      "loss": 0.0014,
      "step": 221160
    },
    {
      "epoch": 7.372333333333334,
      "grad_norm": 0.14245308935642242,
      "learning_rate": 3.922916666666667e-06,
      "loss": 0.0012,
      "step": 221170
    },
    {
      "epoch": 7.3726666666666665,
      "grad_norm": 0.14344967901706696,
      "learning_rate": 3.920833333333333e-06,
      "loss": 0.0016,
      "step": 221180
    },
    {
      "epoch": 7.373,
      "grad_norm": 0.17169183492660522,
      "learning_rate": 3.91875e-06,
      "loss": 0.0016,
      "step": 221190
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 0.24602371454238892,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.0024,
      "step": 221200
    },
    {
      "epoch": 7.373666666666667,
      "grad_norm": 0.22840788960456848,
      "learning_rate": 3.914583333333333e-06,
      "loss": 0.0018,
      "step": 221210
    },
    {
      "epoch": 7.374,
      "grad_norm": 0.1426640748977661,
      "learning_rate": 3.9125e-06,
      "loss": 0.0012,
      "step": 221220
    },
    {
      "epoch": 7.374333333333333,
      "grad_norm": 0.14585623145103455,
      "learning_rate": 3.910416666666667e-06,
      "loss": 0.0015,
      "step": 221230
    },
    {
      "epoch": 7.374666666666666,
      "grad_norm": 0.4560216963291168,
      "learning_rate": 3.908333333333333e-06,
      "loss": 0.0017,
      "step": 221240
    },
    {
      "epoch": 7.375,
      "grad_norm": 0.1416953206062317,
      "learning_rate": 3.90625e-06,
      "loss": 0.0017,
      "step": 221250
    },
    {
      "epoch": 7.375333333333334,
      "grad_norm": 0.2863413393497467,
      "learning_rate": 3.904166666666667e-06,
      "loss": 0.0019,
      "step": 221260
    },
    {
      "epoch": 7.375666666666667,
      "grad_norm": 0.25701504945755005,
      "learning_rate": 3.902083333333334e-06,
      "loss": 0.0025,
      "step": 221270
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.1325802206993103,
      "learning_rate": 3.9e-06,
      "loss": 0.0021,
      "step": 221280
    },
    {
      "epoch": 7.376333333333333,
      "grad_norm": 0.22862742841243744,
      "learning_rate": 3.897916666666667e-06,
      "loss": 0.0018,
      "step": 221290
    },
    {
      "epoch": 7.376666666666667,
      "grad_norm": 0.1436193287372589,
      "learning_rate": 3.8958333333333334e-06,
      "loss": 0.0016,
      "step": 221300
    },
    {
      "epoch": 7.377,
      "grad_norm": 0.28531596064567566,
      "learning_rate": 3.89375e-06,
      "loss": 0.0019,
      "step": 221310
    },
    {
      "epoch": 7.3773333333333335,
      "grad_norm": 0.2284441888332367,
      "learning_rate": 3.891666666666667e-06,
      "loss": 0.0026,
      "step": 221320
    },
    {
      "epoch": 7.377666666666666,
      "grad_norm": 0.14287351071834564,
      "learning_rate": 3.889583333333333e-06,
      "loss": 0.0014,
      "step": 221330
    },
    {
      "epoch": 7.378,
      "grad_norm": 0.08736734837293625,
      "learning_rate": 3.8875000000000005e-06,
      "loss": 0.0017,
      "step": 221340
    },
    {
      "epoch": 7.378333333333333,
      "grad_norm": 0.313413143157959,
      "learning_rate": 3.885416666666667e-06,
      "loss": 0.0016,
      "step": 221350
    },
    {
      "epoch": 7.378666666666667,
      "grad_norm": 0.03131091967225075,
      "learning_rate": 3.883333333333333e-06,
      "loss": 0.0014,
      "step": 221360
    },
    {
      "epoch": 7.379,
      "grad_norm": 0.057468317449092865,
      "learning_rate": 3.88125e-06,
      "loss": 0.0016,
      "step": 221370
    },
    {
      "epoch": 7.379333333333333,
      "grad_norm": 0.08604658395051956,
      "learning_rate": 3.879166666666667e-06,
      "loss": 0.0015,
      "step": 221380
    },
    {
      "epoch": 7.379666666666667,
      "grad_norm": 0.058984339237213135,
      "learning_rate": 3.877083333333333e-06,
      "loss": 0.0015,
      "step": 221390
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.029385430738329887,
      "learning_rate": 3.875e-06,
      "loss": 0.0018,
      "step": 221400
    },
    {
      "epoch": 7.380333333333334,
      "grad_norm": 0.030952291563153267,
      "learning_rate": 3.872916666666667e-06,
      "loss": 0.0015,
      "step": 221410
    },
    {
      "epoch": 7.3806666666666665,
      "grad_norm": 0.11516759544610977,
      "learning_rate": 3.870833333333333e-06,
      "loss": 0.0012,
      "step": 221420
    },
    {
      "epoch": 7.381,
      "grad_norm": 0.1710149198770523,
      "learning_rate": 3.86875e-06,
      "loss": 0.002,
      "step": 221430
    },
    {
      "epoch": 7.381333333333333,
      "grad_norm": 0.03314743563532829,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0015,
      "step": 221440
    },
    {
      "epoch": 7.381666666666667,
      "grad_norm": 0.20034043490886688,
      "learning_rate": 3.8645833333333335e-06,
      "loss": 0.0013,
      "step": 221450
    },
    {
      "epoch": 7.382,
      "grad_norm": 0.22367256879806519,
      "learning_rate": 3.8625e-06,
      "loss": 0.0017,
      "step": 221460
    },
    {
      "epoch": 7.382333333333333,
      "grad_norm": 0.2001430094242096,
      "learning_rate": 3.860416666666667e-06,
      "loss": 0.002,
      "step": 221470
    },
    {
      "epoch": 7.382666666666666,
      "grad_norm": 0.1993616670370102,
      "learning_rate": 3.858333333333333e-06,
      "loss": 0.0017,
      "step": 221480
    },
    {
      "epoch": 7.383,
      "grad_norm": 0.08715090155601501,
      "learning_rate": 3.8562500000000006e-06,
      "loss": 0.0014,
      "step": 221490
    },
    {
      "epoch": 7.383333333333334,
      "grad_norm": 0.31118741631507874,
      "learning_rate": 3.854166666666667e-06,
      "loss": 0.0015,
      "step": 221500
    },
    {
      "epoch": 7.383666666666667,
      "grad_norm": 0.11507748812437057,
      "learning_rate": 3.852083333333333e-06,
      "loss": 0.0017,
      "step": 221510
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.03010278195142746,
      "learning_rate": 3.85e-06,
      "loss": 0.0015,
      "step": 221520
    },
    {
      "epoch": 7.384333333333333,
      "grad_norm": 0.08624307066202164,
      "learning_rate": 3.847916666666667e-06,
      "loss": 0.0013,
      "step": 221530
    },
    {
      "epoch": 7.384666666666667,
      "grad_norm": 0.05819709226489067,
      "learning_rate": 3.845833333333333e-06,
      "loss": 0.0017,
      "step": 221540
    },
    {
      "epoch": 7.385,
      "grad_norm": 0.42743954062461853,
      "learning_rate": 3.84375e-06,
      "loss": 0.0018,
      "step": 221550
    },
    {
      "epoch": 7.3853333333333335,
      "grad_norm": 0.3714613914489746,
      "learning_rate": 3.8416666666666674e-06,
      "loss": 0.0014,
      "step": 221560
    },
    {
      "epoch": 7.385666666666666,
      "grad_norm": 0.20014391839504242,
      "learning_rate": 3.839583333333333e-06,
      "loss": 0.0012,
      "step": 221570
    },
    {
      "epoch": 7.386,
      "grad_norm": 0.31442734599113464,
      "learning_rate": 3.8375e-06,
      "loss": 0.0018,
      "step": 221580
    },
    {
      "epoch": 7.386333333333333,
      "grad_norm": 0.010506589896976948,
      "learning_rate": 3.835416666666667e-06,
      "loss": 0.0016,
      "step": 221590
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.2569645941257477,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.0012,
      "step": 221600
    },
    {
      "epoch": 7.3870000000000005,
      "grad_norm": 0.11455750465393066,
      "learning_rate": 3.83125e-06,
      "loss": 0.0016,
      "step": 221610
    },
    {
      "epoch": 7.387333333333333,
      "grad_norm": 0.05780763551592827,
      "learning_rate": 3.829166666666667e-06,
      "loss": 0.0015,
      "step": 221620
    },
    {
      "epoch": 7.387666666666667,
      "grad_norm": 0.14340786635875702,
      "learning_rate": 3.8270833333333335e-06,
      "loss": 0.0018,
      "step": 221630
    },
    {
      "epoch": 7.388,
      "grad_norm": 0.11426866799592972,
      "learning_rate": 3.825e-06,
      "loss": 0.0018,
      "step": 221640
    },
    {
      "epoch": 7.388333333333334,
      "grad_norm": 0.02987421676516533,
      "learning_rate": 3.822916666666667e-06,
      "loss": 0.0014,
      "step": 221650
    },
    {
      "epoch": 7.3886666666666665,
      "grad_norm": 0.25974833965301514,
      "learning_rate": 3.820833333333333e-06,
      "loss": 0.0019,
      "step": 221660
    },
    {
      "epoch": 7.389,
      "grad_norm": 0.30014845728874207,
      "learning_rate": 3.8187500000000005e-06,
      "loss": 0.0026,
      "step": 221670
    },
    {
      "epoch": 7.389333333333333,
      "grad_norm": 0.31403160095214844,
      "learning_rate": 3.816666666666667e-06,
      "loss": 0.0017,
      "step": 221680
    },
    {
      "epoch": 7.389666666666667,
      "grad_norm": 0.057518597692251205,
      "learning_rate": 3.814583333333333e-06,
      "loss": 0.0013,
      "step": 221690
    },
    {
      "epoch": 7.39,
      "grad_norm": 0.5109555125236511,
      "learning_rate": 3.8125e-06,
      "loss": 0.0024,
      "step": 221700
    },
    {
      "epoch": 7.390333333333333,
      "grad_norm": 0.47719189524650574,
      "learning_rate": 3.810416666666667e-06,
      "loss": 0.0015,
      "step": 221710
    },
    {
      "epoch": 7.390666666666666,
      "grad_norm": 0.39081892371177673,
      "learning_rate": 3.808333333333333e-06,
      "loss": 0.0023,
      "step": 221720
    },
    {
      "epoch": 7.391,
      "grad_norm": 0.22643961012363434,
      "learning_rate": 3.80625e-06,
      "loss": 0.0015,
      "step": 221730
    },
    {
      "epoch": 7.391333333333334,
      "grad_norm": 0.3322138488292694,
      "learning_rate": 3.804166666666667e-06,
      "loss": 0.0022,
      "step": 221740
    },
    {
      "epoch": 7.391666666666667,
      "grad_norm": 0.19971995055675507,
      "learning_rate": 3.8020833333333333e-06,
      "loss": 0.0019,
      "step": 221750
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.08661548793315887,
      "learning_rate": 3.8e-06,
      "loss": 0.0021,
      "step": 221760
    },
    {
      "epoch": 7.392333333333333,
      "grad_norm": 0.0574871264398098,
      "learning_rate": 3.797916666666667e-06,
      "loss": 0.0018,
      "step": 221770
    },
    {
      "epoch": 7.392666666666667,
      "grad_norm": 0.058470718562603,
      "learning_rate": 3.795833333333333e-06,
      "loss": 0.0016,
      "step": 221780
    },
    {
      "epoch": 7.393,
      "grad_norm": 0.14359892904758453,
      "learning_rate": 3.79375e-06,
      "loss": 0.0018,
      "step": 221790
    },
    {
      "epoch": 7.3933333333333335,
      "grad_norm": 0.25659632682800293,
      "learning_rate": 3.791666666666667e-06,
      "loss": 0.0025,
      "step": 221800
    },
    {
      "epoch": 7.393666666666666,
      "grad_norm": 0.2570190727710724,
      "learning_rate": 3.789583333333333e-06,
      "loss": 0.0014,
      "step": 221810
    },
    {
      "epoch": 7.394,
      "grad_norm": 0.17113548517227173,
      "learning_rate": 3.7875e-06,
      "loss": 0.0014,
      "step": 221820
    },
    {
      "epoch": 7.394333333333333,
      "grad_norm": 0.0311052817851305,
      "learning_rate": 3.785416666666667e-06,
      "loss": 0.0023,
      "step": 221830
    },
    {
      "epoch": 7.394666666666667,
      "grad_norm": 0.030367247760295868,
      "learning_rate": 3.7833333333333333e-06,
      "loss": 0.0014,
      "step": 221840
    },
    {
      "epoch": 7.395,
      "grad_norm": 0.14297208189964294,
      "learning_rate": 3.78125e-06,
      "loss": 0.0014,
      "step": 221850
    },
    {
      "epoch": 7.395333333333333,
      "grad_norm": 0.08652491122484207,
      "learning_rate": 3.7791666666666668e-06,
      "loss": 0.0015,
      "step": 221860
    },
    {
      "epoch": 7.395666666666667,
      "grad_norm": 0.3143039345741272,
      "learning_rate": 3.777083333333333e-06,
      "loss": 0.0015,
      "step": 221870
    },
    {
      "epoch": 7.396,
      "grad_norm": 0.14293143153190613,
      "learning_rate": 3.775e-06,
      "loss": 0.0013,
      "step": 221880
    },
    {
      "epoch": 7.396333333333334,
      "grad_norm": 0.1427191197872162,
      "learning_rate": 3.772916666666667e-06,
      "loss": 0.0018,
      "step": 221890
    },
    {
      "epoch": 7.3966666666666665,
      "grad_norm": 0.14312982559204102,
      "learning_rate": 3.770833333333334e-06,
      "loss": 0.0018,
      "step": 221900
    },
    {
      "epoch": 7.397,
      "grad_norm": 0.17081443965435028,
      "learning_rate": 3.76875e-06,
      "loss": 0.0022,
      "step": 221910
    },
    {
      "epoch": 7.397333333333333,
      "grad_norm": 0.030970752239227295,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0016,
      "step": 221920
    },
    {
      "epoch": 7.397666666666667,
      "grad_norm": 0.1142975315451622,
      "learning_rate": 3.7645833333333336e-06,
      "loss": 0.0015,
      "step": 221930
    },
    {
      "epoch": 7.398,
      "grad_norm": 0.200264111161232,
      "learning_rate": 3.7625e-06,
      "loss": 0.0017,
      "step": 221940
    },
    {
      "epoch": 7.398333333333333,
      "grad_norm": 0.1998283416032791,
      "learning_rate": 3.760416666666667e-06,
      "loss": 0.0019,
      "step": 221950
    },
    {
      "epoch": 7.398666666666666,
      "grad_norm": 0.5994265675544739,
      "learning_rate": 3.758333333333334e-06,
      "loss": 0.0021,
      "step": 221960
    },
    {
      "epoch": 7.399,
      "grad_norm": 0.010186690837144852,
      "learning_rate": 3.7562500000000002e-06,
      "loss": 0.0015,
      "step": 221970
    },
    {
      "epoch": 7.399333333333333,
      "grad_norm": 0.22869019210338593,
      "learning_rate": 3.754166666666667e-06,
      "loss": 0.0015,
      "step": 221980
    },
    {
      "epoch": 7.399666666666667,
      "grad_norm": 0.16917400062084198,
      "learning_rate": 3.7520833333333338e-06,
      "loss": 0.002,
      "step": 221990
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.2288542091846466,
      "learning_rate": 3.75e-06,
      "loss": 0.0015,
      "step": 222000
    },
    {
      "epoch": 7.400333333333333,
      "grad_norm": 0.3988558351993561,
      "learning_rate": 3.747916666666667e-06,
      "loss": 0.0019,
      "step": 222010
    },
    {
      "epoch": 7.400666666666667,
      "grad_norm": 0.1715231090784073,
      "learning_rate": 3.745833333333334e-06,
      "loss": 0.0013,
      "step": 222020
    },
    {
      "epoch": 7.401,
      "grad_norm": 0.05816824734210968,
      "learning_rate": 3.74375e-06,
      "loss": 0.0015,
      "step": 222030
    },
    {
      "epoch": 7.4013333333333335,
      "grad_norm": 0.14274626970291138,
      "learning_rate": 3.741666666666667e-06,
      "loss": 0.0015,
      "step": 222040
    },
    {
      "epoch": 7.401666666666666,
      "grad_norm": 0.2852708697319031,
      "learning_rate": 3.739583333333334e-06,
      "loss": 0.0026,
      "step": 222050
    },
    {
      "epoch": 7.402,
      "grad_norm": 0.342265784740448,
      "learning_rate": 3.7375000000000002e-06,
      "loss": 0.0016,
      "step": 222060
    },
    {
      "epoch": 7.402333333333333,
      "grad_norm": 0.09507417678833008,
      "learning_rate": 3.735416666666667e-06,
      "loss": 0.0017,
      "step": 222070
    },
    {
      "epoch": 7.402666666666667,
      "grad_norm": 0.3809339702129364,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0011,
      "step": 222080
    },
    {
      "epoch": 7.4030000000000005,
      "grad_norm": 0.1428183913230896,
      "learning_rate": 3.73125e-06,
      "loss": 0.0025,
      "step": 222090
    },
    {
      "epoch": 7.403333333333333,
      "grad_norm": 0.14257268607616425,
      "learning_rate": 3.729166666666667e-06,
      "loss": 0.002,
      "step": 222100
    },
    {
      "epoch": 7.403666666666667,
      "grad_norm": 0.3997178077697754,
      "learning_rate": 3.727083333333334e-06,
      "loss": 0.0012,
      "step": 222110
    },
    {
      "epoch": 7.404,
      "grad_norm": 0.37044674158096313,
      "learning_rate": 3.725e-06,
      "loss": 0.0026,
      "step": 222120
    },
    {
      "epoch": 7.404333333333334,
      "grad_norm": 0.19951863586902618,
      "learning_rate": 3.722916666666667e-06,
      "loss": 0.0012,
      "step": 222130
    },
    {
      "epoch": 7.4046666666666665,
      "grad_norm": 0.08733050525188446,
      "learning_rate": 3.720833333333334e-06,
      "loss": 0.0013,
      "step": 222140
    },
    {
      "epoch": 7.405,
      "grad_norm": 0.08565385639667511,
      "learning_rate": 3.71875e-06,
      "loss": 0.0014,
      "step": 222150
    },
    {
      "epoch": 7.405333333333333,
      "grad_norm": 0.08604538440704346,
      "learning_rate": 3.716666666666667e-06,
      "loss": 0.0015,
      "step": 222160
    },
    {
      "epoch": 7.405666666666667,
      "grad_norm": 0.47744014859199524,
      "learning_rate": 3.7145833333333337e-06,
      "loss": 0.0019,
      "step": 222170
    },
    {
      "epoch": 7.406,
      "grad_norm": 0.057825684547424316,
      "learning_rate": 3.7125e-06,
      "loss": 0.0016,
      "step": 222180
    },
    {
      "epoch": 7.406333333333333,
      "grad_norm": 0.11508451402187347,
      "learning_rate": 3.7104166666666668e-06,
      "loss": 0.0012,
      "step": 222190
    },
    {
      "epoch": 7.406666666666666,
      "grad_norm": 0.10546732693910599,
      "learning_rate": 3.708333333333334e-06,
      "loss": 0.0013,
      "step": 222200
    },
    {
      "epoch": 7.407,
      "grad_norm": 0.39905622601509094,
      "learning_rate": 3.70625e-06,
      "loss": 0.0016,
      "step": 222210
    },
    {
      "epoch": 7.407333333333334,
      "grad_norm": 0.08648906648159027,
      "learning_rate": 3.704166666666667e-06,
      "loss": 0.0021,
      "step": 222220
    },
    {
      "epoch": 7.407666666666667,
      "grad_norm": 0.22782067954540253,
      "learning_rate": 3.702083333333334e-06,
      "loss": 0.0021,
      "step": 222230
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.08604080229997635,
      "learning_rate": 3.7e-06,
      "loss": 0.0014,
      "step": 222240
    },
    {
      "epoch": 7.408333333333333,
      "grad_norm": 0.5998958945274353,
      "learning_rate": 3.697916666666667e-06,
      "loss": 0.0024,
      "step": 222250
    },
    {
      "epoch": 7.408666666666667,
      "grad_norm": 0.19266903400421143,
      "learning_rate": 3.6958333333333337e-06,
      "loss": 0.0013,
      "step": 222260
    },
    {
      "epoch": 7.409,
      "grad_norm": 0.25714558362960815,
      "learning_rate": 3.69375e-06,
      "loss": 0.0019,
      "step": 222270
    },
    {
      "epoch": 7.4093333333333335,
      "grad_norm": 0.11450695246458054,
      "learning_rate": 3.6916666666666668e-06,
      "loss": 0.0015,
      "step": 222280
    },
    {
      "epoch": 7.409666666666666,
      "grad_norm": 0.22803567349910736,
      "learning_rate": 3.689583333333334e-06,
      "loss": 0.0016,
      "step": 222290
    },
    {
      "epoch": 7.41,
      "grad_norm": 0.008581376634538174,
      "learning_rate": 3.6875e-06,
      "loss": 0.0014,
      "step": 222300
    },
    {
      "epoch": 7.410333333333333,
      "grad_norm": 0.11496125906705856,
      "learning_rate": 3.685416666666667e-06,
      "loss": 0.0019,
      "step": 222310
    },
    {
      "epoch": 7.410666666666667,
      "grad_norm": 0.05797310918569565,
      "learning_rate": 3.6833333333333338e-06,
      "loss": 0.0019,
      "step": 222320
    },
    {
      "epoch": 7.411,
      "grad_norm": 0.34223613142967224,
      "learning_rate": 3.68125e-06,
      "loss": 0.0019,
      "step": 222330
    },
    {
      "epoch": 7.411333333333333,
      "grad_norm": 0.42779648303985596,
      "learning_rate": 3.679166666666667e-06,
      "loss": 0.0015,
      "step": 222340
    },
    {
      "epoch": 7.411666666666667,
      "grad_norm": 0.029774431139230728,
      "learning_rate": 3.6770833333333336e-06,
      "loss": 0.0012,
      "step": 222350
    },
    {
      "epoch": 7.412,
      "grad_norm": 0.20828548073768616,
      "learning_rate": 3.675e-06,
      "loss": 0.0017,
      "step": 222360
    },
    {
      "epoch": 7.412333333333334,
      "grad_norm": 0.1426255851984024,
      "learning_rate": 3.6729166666666667e-06,
      "loss": 0.0016,
      "step": 222370
    },
    {
      "epoch": 7.4126666666666665,
      "grad_norm": 0.26835915446281433,
      "learning_rate": 3.670833333333334e-06,
      "loss": 0.0017,
      "step": 222380
    },
    {
      "epoch": 7.413,
      "grad_norm": 0.14516152441501617,
      "learning_rate": 3.66875e-06,
      "loss": 0.0013,
      "step": 222390
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.14271000027656555,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0022,
      "step": 222400
    },
    {
      "epoch": 7.413666666666667,
      "grad_norm": 0.05819878354668617,
      "learning_rate": 3.6645833333333338e-06,
      "loss": 0.0018,
      "step": 222410
    },
    {
      "epoch": 7.414,
      "grad_norm": 0.05995817482471466,
      "learning_rate": 3.6625e-06,
      "loss": 0.0019,
      "step": 222420
    },
    {
      "epoch": 7.414333333333333,
      "grad_norm": 0.08627939224243164,
      "learning_rate": 3.660416666666667e-06,
      "loss": 0.0019,
      "step": 222430
    },
    {
      "epoch": 7.414666666666666,
      "grad_norm": 0.1427619457244873,
      "learning_rate": 3.6583333333333336e-06,
      "loss": 0.0017,
      "step": 222440
    },
    {
      "epoch": 7.415,
      "grad_norm": 0.4025891423225403,
      "learning_rate": 3.65625e-06,
      "loss": 0.0019,
      "step": 222450
    },
    {
      "epoch": 7.415333333333333,
      "grad_norm": 0.13894304633140564,
      "learning_rate": 3.6541666666666667e-06,
      "loss": 0.002,
      "step": 222460
    },
    {
      "epoch": 7.415666666666667,
      "grad_norm": 0.31319501996040344,
      "learning_rate": 3.652083333333334e-06,
      "loss": 0.0019,
      "step": 222470
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.3141680955886841,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 0.0021,
      "step": 222480
    },
    {
      "epoch": 7.416333333333333,
      "grad_norm": 0.09404037892818451,
      "learning_rate": 3.647916666666667e-06,
      "loss": 0.0014,
      "step": 222490
    },
    {
      "epoch": 7.416666666666667,
      "grad_norm": 0.0576283261179924,
      "learning_rate": 3.6458333333333337e-06,
      "loss": 0.0015,
      "step": 222500
    },
    {
      "epoch": 7.417,
      "grad_norm": 0.11481104791164398,
      "learning_rate": 3.64375e-06,
      "loss": 0.002,
      "step": 222510
    },
    {
      "epoch": 7.417333333333334,
      "grad_norm": 0.057770658284425735,
      "learning_rate": 3.641666666666667e-06,
      "loss": 0.0026,
      "step": 222520
    },
    {
      "epoch": 7.417666666666666,
      "grad_norm": 0.3507821559906006,
      "learning_rate": 3.6395833333333336e-06,
      "loss": 0.0016,
      "step": 222530
    },
    {
      "epoch": 7.418,
      "grad_norm": 0.031178617849946022,
      "learning_rate": 3.6375e-06,
      "loss": 0.0012,
      "step": 222540
    },
    {
      "epoch": 7.418333333333333,
      "grad_norm": 0.45632824301719666,
      "learning_rate": 3.6354166666666667e-06,
      "loss": 0.0019,
      "step": 222550
    },
    {
      "epoch": 7.418666666666667,
      "grad_norm": 0.5094849467277527,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.002,
      "step": 222560
    },
    {
      "epoch": 7.419,
      "grad_norm": 0.11532071977853775,
      "learning_rate": 3.6312499999999997e-06,
      "loss": 0.0018,
      "step": 222570
    },
    {
      "epoch": 7.419333333333333,
      "grad_norm": 0.01940000243484974,
      "learning_rate": 3.629166666666667e-06,
      "loss": 0.0017,
      "step": 222580
    },
    {
      "epoch": 7.419666666666667,
      "grad_norm": 0.17224447429180145,
      "learning_rate": 3.6270833333333337e-06,
      "loss": 0.0013,
      "step": 222590
    },
    {
      "epoch": 7.42,
      "grad_norm": 0.2761656641960144,
      "learning_rate": 3.625e-06,
      "loss": 0.0012,
      "step": 222600
    },
    {
      "epoch": 7.420333333333334,
      "grad_norm": 0.19956234097480774,
      "learning_rate": 3.6229166666666668e-06,
      "loss": 0.0016,
      "step": 222610
    },
    {
      "epoch": 7.4206666666666665,
      "grad_norm": 0.030530331656336784,
      "learning_rate": 3.6208333333333335e-06,
      "loss": 0.0018,
      "step": 222620
    },
    {
      "epoch": 7.421,
      "grad_norm": 0.42758703231811523,
      "learning_rate": 3.61875e-06,
      "loss": 0.0016,
      "step": 222630
    },
    {
      "epoch": 7.421333333333333,
      "grad_norm": 0.08595076948404312,
      "learning_rate": 3.6166666666666666e-06,
      "loss": 0.0019,
      "step": 222640
    },
    {
      "epoch": 7.421666666666667,
      "grad_norm": 0.06708372384309769,
      "learning_rate": 3.614583333333334e-06,
      "loss": 0.0013,
      "step": 222650
    },
    {
      "epoch": 7.422,
      "grad_norm": 0.17130762338638306,
      "learning_rate": 3.6124999999999997e-06,
      "loss": 0.0021,
      "step": 222660
    },
    {
      "epoch": 7.4223333333333334,
      "grad_norm": 0.22864389419555664,
      "learning_rate": 3.610416666666667e-06,
      "loss": 0.0014,
      "step": 222670
    },
    {
      "epoch": 7.422666666666666,
      "grad_norm": 0.22866187989711761,
      "learning_rate": 3.6083333333333337e-06,
      "loss": 0.0011,
      "step": 222680
    },
    {
      "epoch": 7.423,
      "grad_norm": 0.11518742889165878,
      "learning_rate": 3.60625e-06,
      "loss": 0.002,
      "step": 222690
    },
    {
      "epoch": 7.423333333333334,
      "grad_norm": 0.11422441899776459,
      "learning_rate": 3.6041666666666667e-06,
      "loss": 0.0018,
      "step": 222700
    },
    {
      "epoch": 7.423666666666667,
      "grad_norm": 0.11450303345918655,
      "learning_rate": 3.6020833333333335e-06,
      "loss": 0.0015,
      "step": 222710
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.42741358280181885,
      "learning_rate": 3.6e-06,
      "loss": 0.0016,
      "step": 222720
    },
    {
      "epoch": 7.424333333333333,
      "grad_norm": 0.06574970483779907,
      "learning_rate": 3.5979166666666666e-06,
      "loss": 0.0013,
      "step": 222730
    },
    {
      "epoch": 7.424666666666667,
      "grad_norm": 0.016699273139238358,
      "learning_rate": 3.5958333333333338e-06,
      "loss": 0.002,
      "step": 222740
    },
    {
      "epoch": 7.425,
      "grad_norm": 0.11499209702014923,
      "learning_rate": 3.5937499999999997e-06,
      "loss": 0.0018,
      "step": 222750
    },
    {
      "epoch": 7.425333333333334,
      "grad_norm": 0.1709277629852295,
      "learning_rate": 3.591666666666667e-06,
      "loss": 0.0021,
      "step": 222760
    },
    {
      "epoch": 7.425666666666666,
      "grad_norm": 0.14276346564292908,
      "learning_rate": 3.5895833333333336e-06,
      "loss": 0.0015,
      "step": 222770
    },
    {
      "epoch": 7.426,
      "grad_norm": 0.3210635781288147,
      "learning_rate": 3.5875e-06,
      "loss": 0.0023,
      "step": 222780
    },
    {
      "epoch": 7.426333333333333,
      "grad_norm": 0.20010018348693848,
      "learning_rate": 3.5854166666666667e-06,
      "loss": 0.002,
      "step": 222790
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 0.48535192012786865,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.0018,
      "step": 222800
    },
    {
      "epoch": 7.427,
      "grad_norm": 0.2586194574832916,
      "learning_rate": 3.58125e-06,
      "loss": 0.0021,
      "step": 222810
    },
    {
      "epoch": 7.427333333333333,
      "grad_norm": 0.2582477033138275,
      "learning_rate": 3.579166666666667e-06,
      "loss": 0.0011,
      "step": 222820
    },
    {
      "epoch": 7.427666666666667,
      "grad_norm": 0.20360435545444489,
      "learning_rate": 3.5770833333333337e-06,
      "loss": 0.0016,
      "step": 222830
    },
    {
      "epoch": 7.428,
      "grad_norm": 0.26823675632476807,
      "learning_rate": 3.575e-06,
      "loss": 0.0015,
      "step": 222840
    },
    {
      "epoch": 7.428333333333334,
      "grad_norm": 0.06385096907615662,
      "learning_rate": 3.572916666666667e-06,
      "loss": 0.0012,
      "step": 222850
    },
    {
      "epoch": 7.4286666666666665,
      "grad_norm": 0.2777569890022278,
      "learning_rate": 3.5708333333333336e-06,
      "loss": 0.0014,
      "step": 222860
    },
    {
      "epoch": 7.429,
      "grad_norm": 0.14410293102264404,
      "learning_rate": 3.56875e-06,
      "loss": 0.0016,
      "step": 222870
    },
    {
      "epoch": 7.429333333333333,
      "grad_norm": 0.5789778828620911,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.0014,
      "step": 222880
    },
    {
      "epoch": 7.429666666666667,
      "grad_norm": 0.03016461245715618,
      "learning_rate": 3.564583333333334e-06,
      "loss": 0.0016,
      "step": 222890
    },
    {
      "epoch": 7.43,
      "grad_norm": 0.22859260439872742,
      "learning_rate": 3.5624999999999998e-06,
      "loss": 0.0023,
      "step": 222900
    },
    {
      "epoch": 7.4303333333333335,
      "grad_norm": 0.2000507414340973,
      "learning_rate": 3.560416666666667e-06,
      "loss": 0.0014,
      "step": 222910
    },
    {
      "epoch": 7.430666666666666,
      "grad_norm": 0.28755563497543335,
      "learning_rate": 3.5583333333333337e-06,
      "loss": 0.002,
      "step": 222920
    },
    {
      "epoch": 7.431,
      "grad_norm": 0.6861324906349182,
      "learning_rate": 3.55625e-06,
      "loss": 0.0014,
      "step": 222930
    },
    {
      "epoch": 7.431333333333333,
      "grad_norm": 0.13917289674282074,
      "learning_rate": 3.554166666666667e-06,
      "loss": 0.0022,
      "step": 222940
    },
    {
      "epoch": 7.431666666666667,
      "grad_norm": 0.2004735916852951,
      "learning_rate": 3.5520833333333336e-06,
      "loss": 0.0018,
      "step": 222950
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.08603022247552872,
      "learning_rate": 3.55e-06,
      "loss": 0.002,
      "step": 222960
    },
    {
      "epoch": 7.432333333333333,
      "grad_norm": 0.3421720862388611,
      "learning_rate": 3.5479166666666666e-06,
      "loss": 0.0017,
      "step": 222970
    },
    {
      "epoch": 7.432666666666667,
      "grad_norm": 0.22801685333251953,
      "learning_rate": 3.545833333333334e-06,
      "loss": 0.0014,
      "step": 222980
    },
    {
      "epoch": 7.433,
      "grad_norm": 0.11447565257549286,
      "learning_rate": 3.5437499999999997e-06,
      "loss": 0.0014,
      "step": 222990
    },
    {
      "epoch": 7.433333333333334,
      "grad_norm": 0.04152378439903259,
      "learning_rate": 3.541666666666667e-06,
      "loss": 0.0032,
      "step": 223000
    },
    {
      "epoch": 7.433666666666666,
      "grad_norm": 0.07533261179924011,
      "learning_rate": 3.5395833333333337e-06,
      "loss": 0.0016,
      "step": 223010
    },
    {
      "epoch": 7.434,
      "grad_norm": 0.05803242325782776,
      "learning_rate": 3.5375e-06,
      "loss": 0.0016,
      "step": 223020
    },
    {
      "epoch": 7.434333333333333,
      "grad_norm": 0.4161096513271332,
      "learning_rate": 3.5354166666666668e-06,
      "loss": 0.0016,
      "step": 223030
    },
    {
      "epoch": 7.434666666666667,
      "grad_norm": 0.05734359472990036,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0018,
      "step": 223040
    },
    {
      "epoch": 7.435,
      "grad_norm": 0.14350129663944244,
      "learning_rate": 3.53125e-06,
      "loss": 0.0012,
      "step": 223050
    },
    {
      "epoch": 7.435333333333333,
      "grad_norm": 0.08653608709573746,
      "learning_rate": 3.5291666666666666e-06,
      "loss": 0.0018,
      "step": 223060
    },
    {
      "epoch": 7.435666666666666,
      "grad_norm": 0.05940890312194824,
      "learning_rate": 3.527083333333334e-06,
      "loss": 0.0022,
      "step": 223070
    },
    {
      "epoch": 7.436,
      "grad_norm": 0.05851701274514198,
      "learning_rate": 3.5249999999999997e-06,
      "loss": 0.002,
      "step": 223080
    },
    {
      "epoch": 7.436333333333334,
      "grad_norm": 0.010839710012078285,
      "learning_rate": 3.522916666666667e-06,
      "loss": 0.0017,
      "step": 223090
    },
    {
      "epoch": 7.4366666666666665,
      "grad_norm": 0.2310645878314972,
      "learning_rate": 3.5208333333333336e-06,
      "loss": 0.0014,
      "step": 223100
    },
    {
      "epoch": 7.437,
      "grad_norm": 0.24358944594860077,
      "learning_rate": 3.51875e-06,
      "loss": 0.0022,
      "step": 223110
    },
    {
      "epoch": 7.437333333333333,
      "grad_norm": 0.03036869317293167,
      "learning_rate": 3.5166666666666667e-06,
      "loss": 0.0016,
      "step": 223120
    },
    {
      "epoch": 7.437666666666667,
      "grad_norm": 0.08676966279745102,
      "learning_rate": 3.5145833333333335e-06,
      "loss": 0.0012,
      "step": 223130
    },
    {
      "epoch": 7.438,
      "grad_norm": 0.08654692769050598,
      "learning_rate": 3.5125000000000007e-06,
      "loss": 0.0016,
      "step": 223140
    },
    {
      "epoch": 7.4383333333333335,
      "grad_norm": 0.14301857352256775,
      "learning_rate": 3.5104166666666666e-06,
      "loss": 0.0022,
      "step": 223150
    },
    {
      "epoch": 7.438666666666666,
      "grad_norm": 0.17479903995990753,
      "learning_rate": 3.5083333333333338e-06,
      "loss": 0.0019,
      "step": 223160
    },
    {
      "epoch": 7.439,
      "grad_norm": 0.285001277923584,
      "learning_rate": 3.5062500000000005e-06,
      "loss": 0.0014,
      "step": 223170
    },
    {
      "epoch": 7.439333333333334,
      "grad_norm": 0.31446221470832825,
      "learning_rate": 3.504166666666667e-06,
      "loss": 0.0014,
      "step": 223180
    },
    {
      "epoch": 7.439666666666667,
      "grad_norm": 0.1146625354886055,
      "learning_rate": 3.5020833333333336e-06,
      "loss": 0.0013,
      "step": 223190
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.2852723002433777,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0015,
      "step": 223200
    },
    {
      "epoch": 7.440333333333333,
      "grad_norm": 0.34271475672721863,
      "learning_rate": 3.4979166666666667e-06,
      "loss": 0.0019,
      "step": 223210
    },
    {
      "epoch": 7.440666666666667,
      "grad_norm": 0.09254752099514008,
      "learning_rate": 3.4958333333333335e-06,
      "loss": 0.0016,
      "step": 223220
    },
    {
      "epoch": 7.441,
      "grad_norm": 0.049724601209163666,
      "learning_rate": 3.4937500000000006e-06,
      "loss": 0.0015,
      "step": 223230
    },
    {
      "epoch": 7.441333333333334,
      "grad_norm": 0.15264475345611572,
      "learning_rate": 3.4916666666666666e-06,
      "loss": 0.0017,
      "step": 223240
    },
    {
      "epoch": 7.441666666666666,
      "grad_norm": 0.14283870160579681,
      "learning_rate": 3.4895833333333337e-06,
      "loss": 0.0024,
      "step": 223250
    },
    {
      "epoch": 7.442,
      "grad_norm": 0.09129051864147186,
      "learning_rate": 3.4875000000000005e-06,
      "loss": 0.0013,
      "step": 223260
    },
    {
      "epoch": 7.442333333333333,
      "grad_norm": 0.22891178727149963,
      "learning_rate": 3.485416666666667e-06,
      "loss": 0.0018,
      "step": 223270
    },
    {
      "epoch": 7.442666666666667,
      "grad_norm": 0.25697118043899536,
      "learning_rate": 3.4833333333333336e-06,
      "loss": 0.0015,
      "step": 223280
    },
    {
      "epoch": 7.443,
      "grad_norm": 0.11522019654512405,
      "learning_rate": 3.4812500000000003e-06,
      "loss": 0.0018,
      "step": 223290
    },
    {
      "epoch": 7.443333333333333,
      "grad_norm": 0.1142999455332756,
      "learning_rate": 3.4791666666666667e-06,
      "loss": 0.0014,
      "step": 223300
    },
    {
      "epoch": 7.443666666666667,
      "grad_norm": 0.22903668880462646,
      "learning_rate": 3.4770833333333334e-06,
      "loss": 0.0018,
      "step": 223310
    },
    {
      "epoch": 7.444,
      "grad_norm": 0.1997174769639969,
      "learning_rate": 3.4750000000000006e-06,
      "loss": 0.001,
      "step": 223320
    },
    {
      "epoch": 7.444333333333334,
      "grad_norm": 0.031125854700803757,
      "learning_rate": 3.4729166666666665e-06,
      "loss": 0.0015,
      "step": 223330
    },
    {
      "epoch": 7.4446666666666665,
      "grad_norm": 0.2579866349697113,
      "learning_rate": 3.4708333333333337e-06,
      "loss": 0.0022,
      "step": 223340
    },
    {
      "epoch": 7.445,
      "grad_norm": 0.2017846256494522,
      "learning_rate": 3.4687500000000005e-06,
      "loss": 0.0018,
      "step": 223350
    },
    {
      "epoch": 7.445333333333333,
      "grad_norm": 0.19972459971904755,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0016,
      "step": 223360
    },
    {
      "epoch": 7.445666666666667,
      "grad_norm": 0.1995304375886917,
      "learning_rate": 3.4645833333333335e-06,
      "loss": 0.0014,
      "step": 223370
    },
    {
      "epoch": 7.446,
      "grad_norm": 0.15012682974338531,
      "learning_rate": 3.4625000000000003e-06,
      "loss": 0.0018,
      "step": 223380
    },
    {
      "epoch": 7.4463333333333335,
      "grad_norm": 0.032696448266506195,
      "learning_rate": 3.4604166666666666e-06,
      "loss": 0.0016,
      "step": 223390
    },
    {
      "epoch": 7.446666666666666,
      "grad_norm": 0.2854611277580261,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 0.0027,
      "step": 223400
    },
    {
      "epoch": 7.447,
      "grad_norm": 0.22867490351200104,
      "learning_rate": 3.4562500000000006e-06,
      "loss": 0.0014,
      "step": 223410
    },
    {
      "epoch": 7.447333333333333,
      "grad_norm": 0.45621827244758606,
      "learning_rate": 3.4541666666666665e-06,
      "loss": 0.0014,
      "step": 223420
    },
    {
      "epoch": 7.447666666666667,
      "grad_norm": 0.31384724378585815,
      "learning_rate": 3.4520833333333337e-06,
      "loss": 0.0016,
      "step": 223430
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.029920388013124466,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 0.0011,
      "step": 223440
    },
    {
      "epoch": 7.448333333333333,
      "grad_norm": 0.14254915714263916,
      "learning_rate": 3.4479166666666668e-06,
      "loss": 0.0019,
      "step": 223450
    },
    {
      "epoch": 7.448666666666667,
      "grad_norm": 0.09058422595262527,
      "learning_rate": 3.4458333333333335e-06,
      "loss": 0.0019,
      "step": 223460
    },
    {
      "epoch": 7.449,
      "grad_norm": 0.27335864305496216,
      "learning_rate": 3.4437500000000003e-06,
      "loss": 0.0013,
      "step": 223470
    },
    {
      "epoch": 7.449333333333334,
      "grad_norm": 0.22880670428276062,
      "learning_rate": 3.4416666666666666e-06,
      "loss": 0.0017,
      "step": 223480
    },
    {
      "epoch": 7.449666666666666,
      "grad_norm": 0.05800897628068924,
      "learning_rate": 3.4395833333333334e-06,
      "loss": 0.0017,
      "step": 223490
    },
    {
      "epoch": 7.45,
      "grad_norm": 0.058879829943180084,
      "learning_rate": 3.4375000000000005e-06,
      "loss": 0.0016,
      "step": 223500
    },
    {
      "epoch": 7.450333333333333,
      "grad_norm": 0.15502318739891052,
      "learning_rate": 3.4354166666666665e-06,
      "loss": 0.0016,
      "step": 223510
    },
    {
      "epoch": 7.450666666666667,
      "grad_norm": 0.029448356479406357,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0017,
      "step": 223520
    },
    {
      "epoch": 7.451,
      "grad_norm": 0.14396411180496216,
      "learning_rate": 3.4312500000000004e-06,
      "loss": 0.0018,
      "step": 223530
    },
    {
      "epoch": 7.451333333333333,
      "grad_norm": 0.03553688898682594,
      "learning_rate": 3.4291666666666667e-06,
      "loss": 0.0012,
      "step": 223540
    },
    {
      "epoch": 7.451666666666666,
      "grad_norm": 0.368023157119751,
      "learning_rate": 3.4270833333333335e-06,
      "loss": 0.0023,
      "step": 223550
    },
    {
      "epoch": 7.452,
      "grad_norm": 0.11445709317922592,
      "learning_rate": 3.4250000000000002e-06,
      "loss": 0.0022,
      "step": 223560
    },
    {
      "epoch": 7.452333333333334,
      "grad_norm": 0.3770206868648529,
      "learning_rate": 3.4229166666666666e-06,
      "loss": 0.0013,
      "step": 223570
    },
    {
      "epoch": 7.4526666666666666,
      "grad_norm": 0.19973750412464142,
      "learning_rate": 3.4208333333333333e-06,
      "loss": 0.0017,
      "step": 223580
    },
    {
      "epoch": 7.453,
      "grad_norm": 0.10631871223449707,
      "learning_rate": 3.4187500000000005e-06,
      "loss": 0.0012,
      "step": 223590
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 0.17248015105724335,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.0015,
      "step": 223600
    },
    {
      "epoch": 7.453666666666667,
      "grad_norm": 0.11526447534561157,
      "learning_rate": 3.4145833333333336e-06,
      "loss": 0.0024,
      "step": 223610
    },
    {
      "epoch": 7.454,
      "grad_norm": 0.08666937798261642,
      "learning_rate": 3.4125000000000004e-06,
      "loss": 0.0017,
      "step": 223620
    },
    {
      "epoch": 7.4543333333333335,
      "grad_norm": 0.09512034058570862,
      "learning_rate": 3.4104166666666667e-06,
      "loss": 0.001,
      "step": 223630
    },
    {
      "epoch": 7.454666666666666,
      "grad_norm": 0.11455448716878891,
      "learning_rate": 3.4083333333333335e-06,
      "loss": 0.0027,
      "step": 223640
    },
    {
      "epoch": 7.455,
      "grad_norm": 0.11428152769804001,
      "learning_rate": 3.40625e-06,
      "loss": 0.0013,
      "step": 223650
    },
    {
      "epoch": 7.455333333333333,
      "grad_norm": 0.1718004047870636,
      "learning_rate": 3.4041666666666665e-06,
      "loss": 0.002,
      "step": 223660
    },
    {
      "epoch": 7.455666666666667,
      "grad_norm": 0.315894216299057,
      "learning_rate": 3.4020833333333333e-06,
      "loss": 0.0015,
      "step": 223670
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.34069985151290894,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0014,
      "step": 223680
    },
    {
      "epoch": 7.456333333333333,
      "grad_norm": 0.06739813089370728,
      "learning_rate": 3.397916666666667e-06,
      "loss": 0.0015,
      "step": 223690
    },
    {
      "epoch": 7.456666666666667,
      "grad_norm": 0.34120798110961914,
      "learning_rate": 3.3958333333333336e-06,
      "loss": 0.0016,
      "step": 223700
    },
    {
      "epoch": 7.457,
      "grad_norm": 0.14291760325431824,
      "learning_rate": 3.3937500000000003e-06,
      "loss": 0.0012,
      "step": 223710
    },
    {
      "epoch": 7.457333333333334,
      "grad_norm": 0.11586567759513855,
      "learning_rate": 3.3916666666666667e-06,
      "loss": 0.0022,
      "step": 223720
    },
    {
      "epoch": 7.457666666666666,
      "grad_norm": 0.1714807003736496,
      "learning_rate": 3.3895833333333334e-06,
      "loss": 0.0032,
      "step": 223730
    },
    {
      "epoch": 7.458,
      "grad_norm": 0.08762204647064209,
      "learning_rate": 3.3875000000000006e-06,
      "loss": 0.0017,
      "step": 223740
    },
    {
      "epoch": 7.458333333333333,
      "grad_norm": 0.19987834990024567,
      "learning_rate": 3.3854166666666665e-06,
      "loss": 0.0013,
      "step": 223750
    },
    {
      "epoch": 7.458666666666667,
      "grad_norm": 0.17497572302818298,
      "learning_rate": 3.3833333333333337e-06,
      "loss": 0.0019,
      "step": 223760
    },
    {
      "epoch": 7.459,
      "grad_norm": 0.25845393538475037,
      "learning_rate": 3.3812500000000004e-06,
      "loss": 0.0013,
      "step": 223770
    },
    {
      "epoch": 7.459333333333333,
      "grad_norm": 0.37141162157058716,
      "learning_rate": 3.3791666666666668e-06,
      "loss": 0.0015,
      "step": 223780
    },
    {
      "epoch": 7.459666666666667,
      "grad_norm": 0.054784372448921204,
      "learning_rate": 3.3770833333333335e-06,
      "loss": 0.0013,
      "step": 223790
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.057951685041189194,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 0.002,
      "step": 223800
    },
    {
      "epoch": 7.460333333333334,
      "grad_norm": 0.22861705720424652,
      "learning_rate": 3.3729166666666666e-06,
      "loss": 0.0017,
      "step": 223810
    },
    {
      "epoch": 7.460666666666667,
      "grad_norm": 0.11455455422401428,
      "learning_rate": 3.3708333333333334e-06,
      "loss": 0.0012,
      "step": 223820
    },
    {
      "epoch": 7.461,
      "grad_norm": 0.08689725399017334,
      "learning_rate": 3.3687500000000006e-06,
      "loss": 0.0013,
      "step": 223830
    },
    {
      "epoch": 7.461333333333333,
      "grad_norm": 0.3236834704875946,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.0013,
      "step": 223840
    },
    {
      "epoch": 7.461666666666667,
      "grad_norm": 0.11580189317464828,
      "learning_rate": 3.3645833333333337e-06,
      "loss": 0.0013,
      "step": 223850
    },
    {
      "epoch": 7.462,
      "grad_norm": 0.11440947651863098,
      "learning_rate": 3.3625000000000004e-06,
      "loss": 0.0015,
      "step": 223860
    },
    {
      "epoch": 7.4623333333333335,
      "grad_norm": 0.11261103302240372,
      "learning_rate": 3.3604166666666668e-06,
      "loss": 0.0017,
      "step": 223870
    },
    {
      "epoch": 7.462666666666666,
      "grad_norm": 0.25945737957954407,
      "learning_rate": 3.3583333333333335e-06,
      "loss": 0.0012,
      "step": 223880
    },
    {
      "epoch": 7.463,
      "grad_norm": 0.058227069675922394,
      "learning_rate": 3.3562500000000003e-06,
      "loss": 0.0016,
      "step": 223890
    },
    {
      "epoch": 7.463333333333333,
      "grad_norm": 0.05885078385472298,
      "learning_rate": 3.3541666666666666e-06,
      "loss": 0.0018,
      "step": 223900
    },
    {
      "epoch": 7.463666666666667,
      "grad_norm": 0.21572349965572357,
      "learning_rate": 3.3520833333333334e-06,
      "loss": 0.0017,
      "step": 223910
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.086026132106781,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 0.0013,
      "step": 223920
    },
    {
      "epoch": 7.464333333333333,
      "grad_norm": 0.2322704941034317,
      "learning_rate": 3.3479166666666664e-06,
      "loss": 0.0018,
      "step": 223930
    },
    {
      "epoch": 7.464666666666667,
      "grad_norm": 0.009371621534228325,
      "learning_rate": 3.3458333333333336e-06,
      "loss": 0.0013,
      "step": 223940
    },
    {
      "epoch": 7.465,
      "grad_norm": 0.2566980719566345,
      "learning_rate": 3.3437500000000004e-06,
      "loss": 0.0018,
      "step": 223950
    },
    {
      "epoch": 7.465333333333334,
      "grad_norm": 0.20160402357578278,
      "learning_rate": 3.3416666666666667e-06,
      "loss": 0.0018,
      "step": 223960
    },
    {
      "epoch": 7.4656666666666665,
      "grad_norm": 0.26639124751091003,
      "learning_rate": 3.3395833333333335e-06,
      "loss": 0.0015,
      "step": 223970
    },
    {
      "epoch": 7.466,
      "grad_norm": 0.2283303439617157,
      "learning_rate": 3.3375000000000002e-06,
      "loss": 0.0014,
      "step": 223980
    },
    {
      "epoch": 7.466333333333333,
      "grad_norm": 0.17178189754486084,
      "learning_rate": 3.3354166666666666e-06,
      "loss": 0.0017,
      "step": 223990
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.010682513006031513,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0022,
      "step": 224000
    },
    {
      "epoch": 7.467,
      "grad_norm": 0.08645542711019516,
      "learning_rate": 3.3312500000000005e-06,
      "loss": 0.0017,
      "step": 224010
    },
    {
      "epoch": 7.467333333333333,
      "grad_norm": 0.14287763833999634,
      "learning_rate": 3.3291666666666664e-06,
      "loss": 0.0022,
      "step": 224020
    },
    {
      "epoch": 7.467666666666666,
      "grad_norm": 0.14844200015068054,
      "learning_rate": 3.3270833333333336e-06,
      "loss": 0.0015,
      "step": 224030
    },
    {
      "epoch": 7.468,
      "grad_norm": 0.14963659644126892,
      "learning_rate": 3.3250000000000004e-06,
      "loss": 0.002,
      "step": 224040
    },
    {
      "epoch": 7.468333333333334,
      "grad_norm": 0.1463608294725418,
      "learning_rate": 3.3229166666666667e-06,
      "loss": 0.0023,
      "step": 224050
    },
    {
      "epoch": 7.468666666666667,
      "grad_norm": 0.02943592332303524,
      "learning_rate": 3.3208333333333334e-06,
      "loss": 0.002,
      "step": 224060
    },
    {
      "epoch": 7.469,
      "grad_norm": 0.1996496617794037,
      "learning_rate": 3.31875e-06,
      "loss": 0.0017,
      "step": 224070
    },
    {
      "epoch": 7.469333333333333,
      "grad_norm": 0.22808192670345306,
      "learning_rate": 3.3166666666666665e-06,
      "loss": 0.0014,
      "step": 224080
    },
    {
      "epoch": 7.469666666666667,
      "grad_norm": 0.14397408068180084,
      "learning_rate": 3.3145833333333333e-06,
      "loss": 0.0016,
      "step": 224090
    },
    {
      "epoch": 7.47,
      "grad_norm": 0.11418604105710983,
      "learning_rate": 3.3125000000000005e-06,
      "loss": 0.0018,
      "step": 224100
    },
    {
      "epoch": 7.4703333333333335,
      "grad_norm": 0.37112656235694885,
      "learning_rate": 3.3104166666666664e-06,
      "loss": 0.0021,
      "step": 224110
    },
    {
      "epoch": 7.470666666666666,
      "grad_norm": 0.11457588523626328,
      "learning_rate": 3.3083333333333336e-06,
      "loss": 0.0018,
      "step": 224120
    },
    {
      "epoch": 7.471,
      "grad_norm": 0.02965889312326908,
      "learning_rate": 3.3062500000000003e-06,
      "loss": 0.0014,
      "step": 224130
    },
    {
      "epoch": 7.471333333333333,
      "grad_norm": 0.11413855105638504,
      "learning_rate": 3.3041666666666667e-06,
      "loss": 0.0012,
      "step": 224140
    },
    {
      "epoch": 7.471666666666667,
      "grad_norm": 0.030836565420031548,
      "learning_rate": 3.3020833333333334e-06,
      "loss": 0.0019,
      "step": 224150
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.3171272873878479,
      "learning_rate": 3.3e-06,
      "loss": 0.0016,
      "step": 224160
    },
    {
      "epoch": 7.472333333333333,
      "grad_norm": 0.010658563114702702,
      "learning_rate": 3.2979166666666665e-06,
      "loss": 0.0015,
      "step": 224170
    },
    {
      "epoch": 7.472666666666667,
      "grad_norm": 0.14344437420368195,
      "learning_rate": 3.2958333333333333e-06,
      "loss": 0.0016,
      "step": 224180
    },
    {
      "epoch": 7.473,
      "grad_norm": 0.08666732162237167,
      "learning_rate": 3.2937500000000004e-06,
      "loss": 0.0012,
      "step": 224190
    },
    {
      "epoch": 7.473333333333334,
      "grad_norm": 0.11449306458234787,
      "learning_rate": 3.2916666666666664e-06,
      "loss": 0.0014,
      "step": 224200
    },
    {
      "epoch": 7.4736666666666665,
      "grad_norm": 0.029236378148198128,
      "learning_rate": 3.2895833333333335e-06,
      "loss": 0.0014,
      "step": 224210
    },
    {
      "epoch": 7.474,
      "grad_norm": 0.058747798204422,
      "learning_rate": 3.2875000000000003e-06,
      "loss": 0.0014,
      "step": 224220
    },
    {
      "epoch": 7.474333333333333,
      "grad_norm": 0.1434667408466339,
      "learning_rate": 3.2854166666666666e-06,
      "loss": 0.0018,
      "step": 224230
    },
    {
      "epoch": 7.474666666666667,
      "grad_norm": 0.2563928961753845,
      "learning_rate": 3.2833333333333334e-06,
      "loss": 0.0013,
      "step": 224240
    },
    {
      "epoch": 7.475,
      "grad_norm": 0.21084901690483093,
      "learning_rate": 3.28125e-06,
      "loss": 0.0013,
      "step": 224250
    },
    {
      "epoch": 7.475333333333333,
      "grad_norm": 0.0862198919057846,
      "learning_rate": 3.2791666666666665e-06,
      "loss": 0.0016,
      "step": 224260
    },
    {
      "epoch": 7.475666666666667,
      "grad_norm": 0.2439509928226471,
      "learning_rate": 3.2770833333333332e-06,
      "loss": 0.0017,
      "step": 224270
    },
    {
      "epoch": 7.476,
      "grad_norm": 0.11437753587961197,
      "learning_rate": 3.2750000000000004e-06,
      "loss": 0.0024,
      "step": 224280
    },
    {
      "epoch": 7.476333333333334,
      "grad_norm": 0.05784476920962334,
      "learning_rate": 3.2729166666666663e-06,
      "loss": 0.0017,
      "step": 224290
    },
    {
      "epoch": 7.476666666666667,
      "grad_norm": 0.2600969076156616,
      "learning_rate": 3.2708333333333335e-06,
      "loss": 0.0019,
      "step": 224300
    },
    {
      "epoch": 7.477,
      "grad_norm": 0.008700041100382805,
      "learning_rate": 3.2687500000000003e-06,
      "loss": 0.0013,
      "step": 224310
    },
    {
      "epoch": 7.477333333333333,
      "grad_norm": 0.08550309389829636,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0019,
      "step": 224320
    },
    {
      "epoch": 7.477666666666667,
      "grad_norm": 0.22857870161533356,
      "learning_rate": 3.2645833333333333e-06,
      "loss": 0.0017,
      "step": 224330
    },
    {
      "epoch": 7.478,
      "grad_norm": 0.08719608187675476,
      "learning_rate": 3.2625e-06,
      "loss": 0.0024,
      "step": 224340
    },
    {
      "epoch": 7.4783333333333335,
      "grad_norm": 0.014928936958312988,
      "learning_rate": 3.2604166666666664e-06,
      "loss": 0.0018,
      "step": 224350
    },
    {
      "epoch": 7.478666666666666,
      "grad_norm": 0.03014635108411312,
      "learning_rate": 3.258333333333333e-06,
      "loss": 0.0012,
      "step": 224360
    },
    {
      "epoch": 7.479,
      "grad_norm": 0.17123444378376007,
      "learning_rate": 3.2562500000000004e-06,
      "loss": 0.0027,
      "step": 224370
    },
    {
      "epoch": 7.479333333333333,
      "grad_norm": 0.31568196415901184,
      "learning_rate": 3.254166666666667e-06,
      "loss": 0.0012,
      "step": 224380
    },
    {
      "epoch": 7.479666666666667,
      "grad_norm": 0.08707790076732635,
      "learning_rate": 3.2520833333333335e-06,
      "loss": 0.0019,
      "step": 224390
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.030380498617887497,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.0031,
      "step": 224400
    },
    {
      "epoch": 7.480333333333333,
      "grad_norm": 0.19956739246845245,
      "learning_rate": 3.247916666666667e-06,
      "loss": 0.0025,
      "step": 224410
    },
    {
      "epoch": 7.480666666666667,
      "grad_norm": 0.11534903943538666,
      "learning_rate": 3.2458333333333333e-06,
      "loss": 0.0015,
      "step": 224420
    },
    {
      "epoch": 7.481,
      "grad_norm": 0.11586843430995941,
      "learning_rate": 3.24375e-06,
      "loss": 0.0016,
      "step": 224430
    },
    {
      "epoch": 7.481333333333334,
      "grad_norm": 0.030527407303452492,
      "learning_rate": 3.2416666666666673e-06,
      "loss": 0.0012,
      "step": 224440
    },
    {
      "epoch": 7.4816666666666665,
      "grad_norm": 0.17184865474700928,
      "learning_rate": 3.239583333333333e-06,
      "loss": 0.0021,
      "step": 224450
    },
    {
      "epoch": 7.482,
      "grad_norm": 0.030524061992764473,
      "learning_rate": 3.2375000000000003e-06,
      "loss": 0.0013,
      "step": 224460
    },
    {
      "epoch": 7.482333333333333,
      "grad_norm": 0.31398481130599976,
      "learning_rate": 3.235416666666667e-06,
      "loss": 0.0031,
      "step": 224470
    },
    {
      "epoch": 7.482666666666667,
      "grad_norm": 0.11449719220399857,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0018,
      "step": 224480
    },
    {
      "epoch": 7.483,
      "grad_norm": 0.22880887985229492,
      "learning_rate": 3.23125e-06,
      "loss": 0.0017,
      "step": 224490
    },
    {
      "epoch": 7.483333333333333,
      "grad_norm": 0.17993786931037903,
      "learning_rate": 3.229166666666667e-06,
      "loss": 0.0024,
      "step": 224500
    },
    {
      "epoch": 7.483666666666666,
      "grad_norm": 0.06119498237967491,
      "learning_rate": 3.2270833333333333e-06,
      "loss": 0.0014,
      "step": 224510
    },
    {
      "epoch": 7.484,
      "grad_norm": 0.2872375249862671,
      "learning_rate": 3.225e-06,
      "loss": 0.0024,
      "step": 224520
    },
    {
      "epoch": 7.484333333333334,
      "grad_norm": 0.08587999641895294,
      "learning_rate": 3.2229166666666672e-06,
      "loss": 0.0016,
      "step": 224530
    },
    {
      "epoch": 7.484666666666667,
      "grad_norm": 0.17204801738262177,
      "learning_rate": 3.220833333333333e-06,
      "loss": 0.0022,
      "step": 224540
    },
    {
      "epoch": 7.485,
      "grad_norm": 0.029385561123490334,
      "learning_rate": 3.2187500000000003e-06,
      "loss": 0.0018,
      "step": 224550
    },
    {
      "epoch": 7.485333333333333,
      "grad_norm": 0.029356548562645912,
      "learning_rate": 3.216666666666667e-06,
      "loss": 0.0019,
      "step": 224560
    },
    {
      "epoch": 7.485666666666667,
      "grad_norm": 0.18051958084106445,
      "learning_rate": 3.2145833333333334e-06,
      "loss": 0.0018,
      "step": 224570
    },
    {
      "epoch": 7.486,
      "grad_norm": 0.010548165999352932,
      "learning_rate": 3.2125e-06,
      "loss": 0.0027,
      "step": 224580
    },
    {
      "epoch": 7.4863333333333335,
      "grad_norm": 0.11947067826986313,
      "learning_rate": 3.210416666666667e-06,
      "loss": 0.0016,
      "step": 224590
    },
    {
      "epoch": 7.486666666666666,
      "grad_norm": 0.08733467757701874,
      "learning_rate": 3.2083333333333332e-06,
      "loss": 0.0017,
      "step": 224600
    },
    {
      "epoch": 7.487,
      "grad_norm": 0.03516529127955437,
      "learning_rate": 3.2062500000000004e-06,
      "loss": 0.0016,
      "step": 224610
    },
    {
      "epoch": 7.487333333333333,
      "grad_norm": 0.5435490608215332,
      "learning_rate": 3.204166666666667e-06,
      "loss": 0.0028,
      "step": 224620
    },
    {
      "epoch": 7.487666666666667,
      "grad_norm": 0.17179009318351746,
      "learning_rate": 3.2020833333333335e-06,
      "loss": 0.0012,
      "step": 224630
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.08739294856786728,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0013,
      "step": 224640
    },
    {
      "epoch": 7.488333333333333,
      "grad_norm": 0.20918412506580353,
      "learning_rate": 3.197916666666667e-06,
      "loss": 0.0013,
      "step": 224650
    },
    {
      "epoch": 7.488666666666667,
      "grad_norm": 0.2562847435474396,
      "learning_rate": 3.1958333333333334e-06,
      "loss": 0.0013,
      "step": 224660
    },
    {
      "epoch": 7.489,
      "grad_norm": 0.23077787458896637,
      "learning_rate": 3.19375e-06,
      "loss": 0.0019,
      "step": 224670
    },
    {
      "epoch": 7.489333333333334,
      "grad_norm": 0.057732172310352325,
      "learning_rate": 3.1916666666666673e-06,
      "loss": 0.0011,
      "step": 224680
    },
    {
      "epoch": 7.4896666666666665,
      "grad_norm": 0.22800694406032562,
      "learning_rate": 3.1895833333333332e-06,
      "loss": 0.0016,
      "step": 224690
    },
    {
      "epoch": 7.49,
      "grad_norm": 0.04348093271255493,
      "learning_rate": 3.1875000000000004e-06,
      "loss": 0.0018,
      "step": 224700
    },
    {
      "epoch": 7.490333333333333,
      "grad_norm": 0.0787404403090477,
      "learning_rate": 3.185416666666667e-06,
      "loss": 0.0022,
      "step": 224710
    },
    {
      "epoch": 7.490666666666667,
      "grad_norm": 0.19979813694953918,
      "learning_rate": 3.1833333333333335e-06,
      "loss": 0.0014,
      "step": 224720
    },
    {
      "epoch": 7.491,
      "grad_norm": 0.24306459724903107,
      "learning_rate": 3.1812500000000002e-06,
      "loss": 0.0018,
      "step": 224730
    },
    {
      "epoch": 7.491333333333333,
      "grad_norm": 0.22842159867286682,
      "learning_rate": 3.179166666666667e-06,
      "loss": 0.0015,
      "step": 224740
    },
    {
      "epoch": 7.491666666666666,
      "grad_norm": 0.058677349239587784,
      "learning_rate": 3.1770833333333333e-06,
      "loss": 0.0021,
      "step": 224750
    },
    {
      "epoch": 7.492,
      "grad_norm": 0.39912810921669006,
      "learning_rate": 3.175e-06,
      "loss": 0.0016,
      "step": 224760
    },
    {
      "epoch": 7.492333333333334,
      "grad_norm": 0.05767669528722763,
      "learning_rate": 3.1729166666666673e-06,
      "loss": 0.0026,
      "step": 224770
    },
    {
      "epoch": 7.492666666666667,
      "grad_norm": 0.15995648503303528,
      "learning_rate": 3.170833333333333e-06,
      "loss": 0.0015,
      "step": 224780
    },
    {
      "epoch": 7.493,
      "grad_norm": 0.1031280905008316,
      "learning_rate": 3.1687500000000004e-06,
      "loss": 0.0014,
      "step": 224790
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 0.03002896159887314,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0017,
      "step": 224800
    },
    {
      "epoch": 7.493666666666667,
      "grad_norm": 0.008061828091740608,
      "learning_rate": 3.1645833333333335e-06,
      "loss": 0.0019,
      "step": 224810
    },
    {
      "epoch": 7.494,
      "grad_norm": 0.34271368384361267,
      "learning_rate": 3.1625000000000002e-06,
      "loss": 0.0018,
      "step": 224820
    },
    {
      "epoch": 7.4943333333333335,
      "grad_norm": 0.029744315892457962,
      "learning_rate": 3.160416666666667e-06,
      "loss": 0.0029,
      "step": 224830
    },
    {
      "epoch": 7.494666666666666,
      "grad_norm": 0.014170536771416664,
      "learning_rate": 3.1583333333333333e-06,
      "loss": 0.0012,
      "step": 224840
    },
    {
      "epoch": 7.495,
      "grad_norm": 0.05756039544939995,
      "learning_rate": 3.15625e-06,
      "loss": 0.0019,
      "step": 224850
    },
    {
      "epoch": 7.495333333333333,
      "grad_norm": 0.030697166919708252,
      "learning_rate": 3.1541666666666672e-06,
      "loss": 0.0014,
      "step": 224860
    },
    {
      "epoch": 7.495666666666667,
      "grad_norm": 0.03195502609014511,
      "learning_rate": 3.152083333333333e-06,
      "loss": 0.002,
      "step": 224870
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.14295275509357452,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 0.0022,
      "step": 224880
    },
    {
      "epoch": 7.496333333333333,
      "grad_norm": 0.228252112865448,
      "learning_rate": 3.147916666666667e-06,
      "loss": 0.0016,
      "step": 224890
    },
    {
      "epoch": 7.496666666666667,
      "grad_norm": 0.22849290072917938,
      "learning_rate": 3.1458333333333334e-06,
      "loss": 0.0018,
      "step": 224900
    },
    {
      "epoch": 7.497,
      "grad_norm": 0.05831749364733696,
      "learning_rate": 3.14375e-06,
      "loss": 0.0015,
      "step": 224910
    },
    {
      "epoch": 7.497333333333334,
      "grad_norm": 0.030229680240154266,
      "learning_rate": 3.141666666666667e-06,
      "loss": 0.0013,
      "step": 224920
    },
    {
      "epoch": 7.4976666666666665,
      "grad_norm": 0.1428202986717224,
      "learning_rate": 3.1395833333333333e-06,
      "loss": 0.0018,
      "step": 224930
    },
    {
      "epoch": 7.498,
      "grad_norm": 0.11511760205030441,
      "learning_rate": 3.1375e-06,
      "loss": 0.0014,
      "step": 224940
    },
    {
      "epoch": 7.498333333333333,
      "grad_norm": 0.2568708062171936,
      "learning_rate": 3.135416666666667e-06,
      "loss": 0.0013,
      "step": 224950
    },
    {
      "epoch": 7.498666666666667,
      "grad_norm": 0.20001152157783508,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0019,
      "step": 224960
    },
    {
      "epoch": 7.499,
      "grad_norm": 0.28747957944869995,
      "learning_rate": 3.1312500000000003e-06,
      "loss": 0.0015,
      "step": 224970
    },
    {
      "epoch": 7.499333333333333,
      "grad_norm": 0.3692529797554016,
      "learning_rate": 3.129166666666667e-06,
      "loss": 0.0017,
      "step": 224980
    },
    {
      "epoch": 7.499666666666666,
      "grad_norm": 0.06511922925710678,
      "learning_rate": 3.1270833333333334e-06,
      "loss": 0.002,
      "step": 224990
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.24210844933986664,
      "learning_rate": 3.125e-06,
      "loss": 0.0019,
      "step": 225000
    },
    {
      "epoch": 7.500333333333334,
      "grad_norm": 0.20048142969608307,
      "learning_rate": 3.122916666666667e-06,
      "loss": 0.0019,
      "step": 225010
    },
    {
      "epoch": 7.500666666666667,
      "grad_norm": 0.030630555003881454,
      "learning_rate": 3.1208333333333337e-06,
      "loss": 0.0015,
      "step": 225020
    },
    {
      "epoch": 7.501,
      "grad_norm": 0.1282682567834854,
      "learning_rate": 3.11875e-06,
      "loss": 0.0018,
      "step": 225030
    },
    {
      "epoch": 7.501333333333333,
      "grad_norm": 0.03126947954297066,
      "learning_rate": 3.1166666666666668e-06,
      "loss": 0.0014,
      "step": 225040
    },
    {
      "epoch": 7.501666666666667,
      "grad_norm": 0.39213377237319946,
      "learning_rate": 3.1145833333333335e-06,
      "loss": 0.0015,
      "step": 225050
    },
    {
      "epoch": 7.502,
      "grad_norm": 0.2754759192466736,
      "learning_rate": 3.1125000000000003e-06,
      "loss": 0.0022,
      "step": 225060
    },
    {
      "epoch": 7.5023333333333335,
      "grad_norm": 0.17923565208911896,
      "learning_rate": 3.1104166666666666e-06,
      "loss": 0.0014,
      "step": 225070
    },
    {
      "epoch": 7.502666666666666,
      "grad_norm": 0.08774933218955994,
      "learning_rate": 3.1083333333333338e-06,
      "loss": 0.0017,
      "step": 225080
    },
    {
      "epoch": 7.503,
      "grad_norm": 0.2509597837924957,
      "learning_rate": 3.10625e-06,
      "loss": 0.0021,
      "step": 225090
    },
    {
      "epoch": 7.503333333333333,
      "grad_norm": 0.229224294424057,
      "learning_rate": 3.104166666666667e-06,
      "loss": 0.0027,
      "step": 225100
    },
    {
      "epoch": 7.503666666666667,
      "grad_norm": 0.25857359170913696,
      "learning_rate": 3.1020833333333336e-06,
      "loss": 0.0016,
      "step": 225110
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.1718846559524536,
      "learning_rate": 3.1e-06,
      "loss": 0.0017,
      "step": 225120
    },
    {
      "epoch": 7.504333333333333,
      "grad_norm": 0.14506086707115173,
      "learning_rate": 3.0979166666666667e-06,
      "loss": 0.0015,
      "step": 225130
    },
    {
      "epoch": 7.504666666666667,
      "grad_norm": 0.11443406343460083,
      "learning_rate": 3.0958333333333335e-06,
      "loss": 0.0018,
      "step": 225140
    },
    {
      "epoch": 7.505,
      "grad_norm": 0.2571982145309448,
      "learning_rate": 3.0937500000000002e-06,
      "loss": 0.0018,
      "step": 225150
    },
    {
      "epoch": 7.505333333333334,
      "grad_norm": 0.05712425336241722,
      "learning_rate": 3.0916666666666666e-06,
      "loss": 0.0011,
      "step": 225160
    },
    {
      "epoch": 7.5056666666666665,
      "grad_norm": 0.05739634484052658,
      "learning_rate": 3.0895833333333338e-06,
      "loss": 0.0018,
      "step": 225170
    },
    {
      "epoch": 7.506,
      "grad_norm": 0.13572171330451965,
      "learning_rate": 3.0875e-06,
      "loss": 0.0017,
      "step": 225180
    },
    {
      "epoch": 7.506333333333333,
      "grad_norm": 0.08699990063905716,
      "learning_rate": 3.085416666666667e-06,
      "loss": 0.0013,
      "step": 225190
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 0.2322472184896469,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.0013,
      "step": 225200
    },
    {
      "epoch": 7.507,
      "grad_norm": 0.20515303313732147,
      "learning_rate": 3.08125e-06,
      "loss": 0.0013,
      "step": 225210
    },
    {
      "epoch": 7.507333333333333,
      "grad_norm": 0.030417269095778465,
      "learning_rate": 3.0791666666666667e-06,
      "loss": 0.0012,
      "step": 225220
    },
    {
      "epoch": 7.507666666666667,
      "grad_norm": 0.25683966279029846,
      "learning_rate": 3.0770833333333334e-06,
      "loss": 0.0014,
      "step": 225230
    },
    {
      "epoch": 7.508,
      "grad_norm": 0.1715610772371292,
      "learning_rate": 3.075e-06,
      "loss": 0.0013,
      "step": 225240
    },
    {
      "epoch": 7.508333333333333,
      "grad_norm": 0.03349312022328377,
      "learning_rate": 3.0729166666666665e-06,
      "loss": 0.0017,
      "step": 225250
    },
    {
      "epoch": 7.508666666666667,
      "grad_norm": 0.03678439185023308,
      "learning_rate": 3.0708333333333337e-06,
      "loss": 0.0017,
      "step": 225260
    },
    {
      "epoch": 7.509,
      "grad_norm": 0.08708448708057404,
      "learning_rate": 3.06875e-06,
      "loss": 0.0015,
      "step": 225270
    },
    {
      "epoch": 7.509333333333333,
      "grad_norm": 0.17234565317630768,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0016,
      "step": 225280
    },
    {
      "epoch": 7.509666666666667,
      "grad_norm": 0.14252197742462158,
      "learning_rate": 3.0645833333333336e-06,
      "loss": 0.0014,
      "step": 225290
    },
    {
      "epoch": 7.51,
      "grad_norm": 0.285366415977478,
      "learning_rate": 3.0625e-06,
      "loss": 0.0016,
      "step": 225300
    },
    {
      "epoch": 7.5103333333333335,
      "grad_norm": 0.2567566931247711,
      "learning_rate": 3.0604166666666667e-06,
      "loss": 0.0013,
      "step": 225310
    },
    {
      "epoch": 7.510666666666666,
      "grad_norm": 0.22780847549438477,
      "learning_rate": 3.0583333333333334e-06,
      "loss": 0.002,
      "step": 225320
    },
    {
      "epoch": 7.511,
      "grad_norm": 0.030904483050107956,
      "learning_rate": 3.05625e-06,
      "loss": 0.0013,
      "step": 225330
    },
    {
      "epoch": 7.511333333333333,
      "grad_norm": 0.05848318338394165,
      "learning_rate": 3.054166666666667e-06,
      "loss": 0.0015,
      "step": 225340
    },
    {
      "epoch": 7.511666666666667,
      "grad_norm": 0.1720990240573883,
      "learning_rate": 3.0520833333333337e-06,
      "loss": 0.0016,
      "step": 225350
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.029567256569862366,
      "learning_rate": 3.05e-06,
      "loss": 0.0015,
      "step": 225360
    },
    {
      "epoch": 7.512333333333333,
      "grad_norm": 0.2853788137435913,
      "learning_rate": 3.0479166666666668e-06,
      "loss": 0.0022,
      "step": 225370
    },
    {
      "epoch": 7.512666666666667,
      "grad_norm": 0.08658912777900696,
      "learning_rate": 3.0458333333333335e-06,
      "loss": 0.0013,
      "step": 225380
    },
    {
      "epoch": 7.513,
      "grad_norm": 0.008778558112680912,
      "learning_rate": 3.04375e-06,
      "loss": 0.0023,
      "step": 225390
    },
    {
      "epoch": 7.513333333333334,
      "grad_norm": 0.17279477417469025,
      "learning_rate": 3.041666666666667e-06,
      "loss": 0.0014,
      "step": 225400
    },
    {
      "epoch": 7.5136666666666665,
      "grad_norm": 0.19981910288333893,
      "learning_rate": 3.0395833333333334e-06,
      "loss": 0.0017,
      "step": 225410
    },
    {
      "epoch": 7.514,
      "grad_norm": 0.14300169050693512,
      "learning_rate": 3.0375e-06,
      "loss": 0.0015,
      "step": 225420
    },
    {
      "epoch": 7.514333333333333,
      "grad_norm": 0.37065982818603516,
      "learning_rate": 3.035416666666667e-06,
      "loss": 0.0018,
      "step": 225430
    },
    {
      "epoch": 7.514666666666667,
      "grad_norm": 0.08611663430929184,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.0013,
      "step": 225440
    },
    {
      "epoch": 7.515,
      "grad_norm": 0.06930485367774963,
      "learning_rate": 3.03125e-06,
      "loss": 0.002,
      "step": 225450
    },
    {
      "epoch": 7.515333333333333,
      "grad_norm": 0.08562055230140686,
      "learning_rate": 3.029166666666667e-06,
      "loss": 0.0015,
      "step": 225460
    },
    {
      "epoch": 7.515666666666666,
      "grad_norm": 0.2570122182369232,
      "learning_rate": 3.0270833333333335e-06,
      "loss": 0.0018,
      "step": 225470
    },
    {
      "epoch": 7.516,
      "grad_norm": 0.1461443305015564,
      "learning_rate": 3.0250000000000003e-06,
      "loss": 0.0016,
      "step": 225480
    },
    {
      "epoch": 7.516333333333334,
      "grad_norm": 0.011104361154139042,
      "learning_rate": 3.022916666666667e-06,
      "loss": 0.002,
      "step": 225490
    },
    {
      "epoch": 7.516666666666667,
      "grad_norm": 0.17169080674648285,
      "learning_rate": 3.0208333333333334e-06,
      "loss": 0.0029,
      "step": 225500
    },
    {
      "epoch": 7.517,
      "grad_norm": 0.3141448497772217,
      "learning_rate": 3.01875e-06,
      "loss": 0.0018,
      "step": 225510
    },
    {
      "epoch": 7.517333333333333,
      "grad_norm": 0.19939839839935303,
      "learning_rate": 3.016666666666667e-06,
      "loss": 0.0016,
      "step": 225520
    },
    {
      "epoch": 7.517666666666667,
      "grad_norm": 0.05852142348885536,
      "learning_rate": 3.0145833333333336e-06,
      "loss": 0.002,
      "step": 225530
    },
    {
      "epoch": 7.518,
      "grad_norm": 0.1995852142572403,
      "learning_rate": 3.0125e-06,
      "loss": 0.0013,
      "step": 225540
    },
    {
      "epoch": 7.5183333333333335,
      "grad_norm": 0.058182526379823685,
      "learning_rate": 3.010416666666667e-06,
      "loss": 0.0015,
      "step": 225550
    },
    {
      "epoch": 7.518666666666666,
      "grad_norm": 0.2853437066078186,
      "learning_rate": 3.0083333333333335e-06,
      "loss": 0.002,
      "step": 225560
    },
    {
      "epoch": 7.519,
      "grad_norm": 0.16587097942829132,
      "learning_rate": 3.0062500000000002e-06,
      "loss": 0.0013,
      "step": 225570
    },
    {
      "epoch": 7.519333333333333,
      "grad_norm": 0.060290124267339706,
      "learning_rate": 3.004166666666667e-06,
      "loss": 0.0017,
      "step": 225580
    },
    {
      "epoch": 7.519666666666667,
      "grad_norm": 0.17125271260738373,
      "learning_rate": 3.0020833333333333e-06,
      "loss": 0.0015,
      "step": 225590
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.03002399019896984,
      "learning_rate": 3e-06,
      "loss": 0.0013,
      "step": 225600
    },
    {
      "epoch": 7.520333333333333,
      "grad_norm": 0.4842519462108612,
      "learning_rate": 2.997916666666667e-06,
      "loss": 0.0015,
      "step": 225610
    },
    {
      "epoch": 7.520666666666667,
      "grad_norm": 0.05945613235235214,
      "learning_rate": 2.9958333333333336e-06,
      "loss": 0.0012,
      "step": 225620
    },
    {
      "epoch": 7.521,
      "grad_norm": 0.17183242738246918,
      "learning_rate": 2.99375e-06,
      "loss": 0.0014,
      "step": 225630
    },
    {
      "epoch": 7.521333333333334,
      "grad_norm": 0.4574195444583893,
      "learning_rate": 2.991666666666667e-06,
      "loss": 0.0016,
      "step": 225640
    },
    {
      "epoch": 7.5216666666666665,
      "grad_norm": 0.22721342742443085,
      "learning_rate": 2.9895833333333334e-06,
      "loss": 0.0014,
      "step": 225650
    },
    {
      "epoch": 7.522,
      "grad_norm": 0.14310874044895172,
      "learning_rate": 2.9875e-06,
      "loss": 0.0018,
      "step": 225660
    },
    {
      "epoch": 7.522333333333333,
      "grad_norm": 0.11448439210653305,
      "learning_rate": 2.985416666666667e-06,
      "loss": 0.0027,
      "step": 225670
    },
    {
      "epoch": 7.522666666666667,
      "grad_norm": 0.11426927894353867,
      "learning_rate": 2.9833333333333333e-06,
      "loss": 0.0018,
      "step": 225680
    },
    {
      "epoch": 7.523,
      "grad_norm": 0.030116965994238853,
      "learning_rate": 2.98125e-06,
      "loss": 0.0016,
      "step": 225690
    },
    {
      "epoch": 7.523333333333333,
      "grad_norm": 0.3827718496322632,
      "learning_rate": 2.979166666666667e-06,
      "loss": 0.0021,
      "step": 225700
    },
    {
      "epoch": 7.523666666666666,
      "grad_norm": 0.2290584295988083,
      "learning_rate": 2.9770833333333336e-06,
      "loss": 0.0016,
      "step": 225710
    },
    {
      "epoch": 7.524,
      "grad_norm": 0.08624953776597977,
      "learning_rate": 2.975e-06,
      "loss": 0.0027,
      "step": 225720
    },
    {
      "epoch": 7.524333333333333,
      "grad_norm": 0.029343372210860252,
      "learning_rate": 2.972916666666667e-06,
      "loss": 0.0019,
      "step": 225730
    },
    {
      "epoch": 7.524666666666667,
      "grad_norm": 0.06263823062181473,
      "learning_rate": 2.9708333333333334e-06,
      "loss": 0.0013,
      "step": 225740
    },
    {
      "epoch": 7.525,
      "grad_norm": 0.16459205746650696,
      "learning_rate": 2.96875e-06,
      "loss": 0.0018,
      "step": 225750
    },
    {
      "epoch": 7.525333333333333,
      "grad_norm": 0.014426532201468945,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0024,
      "step": 225760
    },
    {
      "epoch": 7.525666666666667,
      "grad_norm": 0.1173572838306427,
      "learning_rate": 2.9645833333333333e-06,
      "loss": 0.0017,
      "step": 225770
    },
    {
      "epoch": 7.526,
      "grad_norm": 0.08880456537008286,
      "learning_rate": 2.9625e-06,
      "loss": 0.0014,
      "step": 225780
    },
    {
      "epoch": 7.5263333333333335,
      "grad_norm": 0.11454397439956665,
      "learning_rate": 2.9604166666666668e-06,
      "loss": 0.0019,
      "step": 225790
    },
    {
      "epoch": 7.526666666666666,
      "grad_norm": 0.08562932163476944,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 0.0013,
      "step": 225800
    },
    {
      "epoch": 7.527,
      "grad_norm": 0.3421275019645691,
      "learning_rate": 2.95625e-06,
      "loss": 0.0016,
      "step": 225810
    },
    {
      "epoch": 7.527333333333333,
      "grad_norm": 0.11450018733739853,
      "learning_rate": 2.954166666666667e-06,
      "loss": 0.0021,
      "step": 225820
    },
    {
      "epoch": 7.527666666666667,
      "grad_norm": 0.03164798393845558,
      "learning_rate": 2.9520833333333334e-06,
      "loss": 0.0011,
      "step": 225830
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.3313394784927368,
      "learning_rate": 2.95e-06,
      "loss": 0.0017,
      "step": 225840
    },
    {
      "epoch": 7.528333333333333,
      "grad_norm": 0.008579904213547707,
      "learning_rate": 2.947916666666667e-06,
      "loss": 0.0014,
      "step": 225850
    },
    {
      "epoch": 7.528666666666666,
      "grad_norm": 0.31541162729263306,
      "learning_rate": 2.9458333333333332e-06,
      "loss": 0.0018,
      "step": 225860
    },
    {
      "epoch": 7.529,
      "grad_norm": 0.5421912670135498,
      "learning_rate": 2.94375e-06,
      "loss": 0.002,
      "step": 225870
    },
    {
      "epoch": 7.529333333333334,
      "grad_norm": 0.05787025764584541,
      "learning_rate": 2.9416666666666667e-06,
      "loss": 0.0017,
      "step": 225880
    },
    {
      "epoch": 7.5296666666666665,
      "grad_norm": 0.5388210415840149,
      "learning_rate": 2.9395833333333335e-06,
      "loss": 0.0023,
      "step": 225890
    },
    {
      "epoch": 7.53,
      "grad_norm": 0.22490674257278442,
      "learning_rate": 2.9375e-06,
      "loss": 0.002,
      "step": 225900
    },
    {
      "epoch": 7.530333333333333,
      "grad_norm": 0.20069926977157593,
      "learning_rate": 2.935416666666667e-06,
      "loss": 0.0018,
      "step": 225910
    },
    {
      "epoch": 7.530666666666667,
      "grad_norm": 0.14223098754882812,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0018,
      "step": 225920
    },
    {
      "epoch": 7.531,
      "grad_norm": 0.4566781520843506,
      "learning_rate": 2.93125e-06,
      "loss": 0.0017,
      "step": 225930
    },
    {
      "epoch": 7.531333333333333,
      "grad_norm": 0.1722431182861328,
      "learning_rate": 2.929166666666667e-06,
      "loss": 0.0012,
      "step": 225940
    },
    {
      "epoch": 7.531666666666666,
      "grad_norm": 0.05812570080161095,
      "learning_rate": 2.9270833333333336e-06,
      "loss": 0.0026,
      "step": 225950
    },
    {
      "epoch": 7.532,
      "grad_norm": 0.05875474214553833,
      "learning_rate": 2.9250000000000004e-06,
      "loss": 0.0019,
      "step": 225960
    },
    {
      "epoch": 7.532333333333334,
      "grad_norm": 0.05831509456038475,
      "learning_rate": 2.9229166666666667e-06,
      "loss": 0.0023,
      "step": 225970
    },
    {
      "epoch": 7.532666666666667,
      "grad_norm": 0.0063741435296833515,
      "learning_rate": 2.9208333333333335e-06,
      "loss": 0.0016,
      "step": 225980
    },
    {
      "epoch": 7.533,
      "grad_norm": 0.2282959669828415,
      "learning_rate": 2.9187500000000002e-06,
      "loss": 0.0015,
      "step": 225990
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.3124545216560364,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0019,
      "step": 226000
    },
    {
      "epoch": 7.533666666666667,
      "grad_norm": 0.22842563688755035,
      "learning_rate": 2.9145833333333333e-06,
      "loss": 0.002,
      "step": 226010
    },
    {
      "epoch": 7.534,
      "grad_norm": 0.05968884751200676,
      "learning_rate": 2.9125000000000005e-06,
      "loss": 0.002,
      "step": 226020
    },
    {
      "epoch": 7.5343333333333335,
      "grad_norm": 0.1148812398314476,
      "learning_rate": 2.910416666666667e-06,
      "loss": 0.0014,
      "step": 226030
    },
    {
      "epoch": 7.534666666666666,
      "grad_norm": 0.2863214313983917,
      "learning_rate": 2.9083333333333336e-06,
      "loss": 0.002,
      "step": 226040
    },
    {
      "epoch": 7.535,
      "grad_norm": 0.14670932292938232,
      "learning_rate": 2.9062500000000003e-06,
      "loss": 0.0021,
      "step": 226050
    },
    {
      "epoch": 7.535333333333333,
      "grad_norm": 0.11413082480430603,
      "learning_rate": 2.9041666666666667e-06,
      "loss": 0.0016,
      "step": 226060
    },
    {
      "epoch": 7.535666666666667,
      "grad_norm": 0.1264040768146515,
      "learning_rate": 2.9020833333333334e-06,
      "loss": 0.0027,
      "step": 226070
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.17186641693115234,
      "learning_rate": 2.9e-06,
      "loss": 0.0013,
      "step": 226080
    },
    {
      "epoch": 7.536333333333333,
      "grad_norm": 0.05738754943013191,
      "learning_rate": 2.897916666666667e-06,
      "loss": 0.0018,
      "step": 226090
    },
    {
      "epoch": 7.536666666666667,
      "grad_norm": 0.05792143940925598,
      "learning_rate": 2.8958333333333333e-06,
      "loss": 0.0018,
      "step": 226100
    },
    {
      "epoch": 7.537,
      "grad_norm": 0.4024354815483093,
      "learning_rate": 2.8937500000000005e-06,
      "loss": 0.002,
      "step": 226110
    },
    {
      "epoch": 7.537333333333334,
      "grad_norm": 0.22863422334194183,
      "learning_rate": 2.891666666666667e-06,
      "loss": 0.0013,
      "step": 226120
    },
    {
      "epoch": 7.5376666666666665,
      "grad_norm": 0.3139150142669678,
      "learning_rate": 2.8895833333333335e-06,
      "loss": 0.0022,
      "step": 226130
    },
    {
      "epoch": 7.538,
      "grad_norm": 0.034732624888420105,
      "learning_rate": 2.8875000000000003e-06,
      "loss": 0.0013,
      "step": 226140
    },
    {
      "epoch": 7.538333333333333,
      "grad_norm": 0.08657915145158768,
      "learning_rate": 2.8854166666666666e-06,
      "loss": 0.0016,
      "step": 226150
    },
    {
      "epoch": 7.538666666666667,
      "grad_norm": 0.12350617349147797,
      "learning_rate": 2.8833333333333334e-06,
      "loss": 0.0019,
      "step": 226160
    },
    {
      "epoch": 7.539,
      "grad_norm": 0.03160930052399635,
      "learning_rate": 2.88125e-06,
      "loss": 0.0015,
      "step": 226170
    },
    {
      "epoch": 7.539333333333333,
      "grad_norm": 0.25709497928619385,
      "learning_rate": 2.879166666666667e-06,
      "loss": 0.0019,
      "step": 226180
    },
    {
      "epoch": 7.539666666666666,
      "grad_norm": 0.0318954698741436,
      "learning_rate": 2.8770833333333332e-06,
      "loss": 0.0025,
      "step": 226190
    },
    {
      "epoch": 7.54,
      "grad_norm": 0.25696542859077454,
      "learning_rate": 2.8750000000000004e-06,
      "loss": 0.0016,
      "step": 226200
    },
    {
      "epoch": 7.540333333333333,
      "grad_norm": 0.08624192327260971,
      "learning_rate": 2.8729166666666668e-06,
      "loss": 0.0017,
      "step": 226210
    },
    {
      "epoch": 7.540666666666667,
      "grad_norm": 0.0855841413140297,
      "learning_rate": 2.8708333333333335e-06,
      "loss": 0.0018,
      "step": 226220
    },
    {
      "epoch": 7.541,
      "grad_norm": 0.03131222724914551,
      "learning_rate": 2.8687500000000003e-06,
      "loss": 0.0013,
      "step": 226230
    },
    {
      "epoch": 7.541333333333333,
      "grad_norm": 0.3226841986179352,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0017,
      "step": 226240
    },
    {
      "epoch": 7.541666666666667,
      "grad_norm": 0.2570813000202179,
      "learning_rate": 2.8645833333333334e-06,
      "loss": 0.0018,
      "step": 226250
    },
    {
      "epoch": 7.542,
      "grad_norm": 0.11521181464195251,
      "learning_rate": 2.8625e-06,
      "loss": 0.002,
      "step": 226260
    },
    {
      "epoch": 7.542333333333334,
      "grad_norm": 0.1277150809764862,
      "learning_rate": 2.860416666666667e-06,
      "loss": 0.0016,
      "step": 226270
    },
    {
      "epoch": 7.542666666666666,
      "grad_norm": 0.3136374056339264,
      "learning_rate": 2.8583333333333332e-06,
      "loss": 0.0018,
      "step": 226280
    },
    {
      "epoch": 7.543,
      "grad_norm": 0.14380517601966858,
      "learning_rate": 2.8562500000000004e-06,
      "loss": 0.0015,
      "step": 226290
    },
    {
      "epoch": 7.543333333333333,
      "grad_norm": 0.4275905191898346,
      "learning_rate": 2.8541666666666667e-06,
      "loss": 0.0012,
      "step": 226300
    },
    {
      "epoch": 7.543666666666667,
      "grad_norm": 0.17186744511127472,
      "learning_rate": 2.8520833333333335e-06,
      "loss": 0.0019,
      "step": 226310
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.08634224534034729,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 0.0022,
      "step": 226320
    },
    {
      "epoch": 7.544333333333333,
      "grad_norm": 0.24823589622974396,
      "learning_rate": 2.847916666666667e-06,
      "loss": 0.0013,
      "step": 226330
    },
    {
      "epoch": 7.544666666666666,
      "grad_norm": 0.2568407356739044,
      "learning_rate": 2.8458333333333333e-06,
      "loss": 0.0017,
      "step": 226340
    },
    {
      "epoch": 7.545,
      "grad_norm": 0.2569328248500824,
      "learning_rate": 2.84375e-06,
      "loss": 0.0013,
      "step": 226350
    },
    {
      "epoch": 7.545333333333334,
      "grad_norm": 0.08634082973003387,
      "learning_rate": 2.841666666666667e-06,
      "loss": 0.0026,
      "step": 226360
    },
    {
      "epoch": 7.5456666666666665,
      "grad_norm": 0.08594029396772385,
      "learning_rate": 2.839583333333333e-06,
      "loss": 0.0017,
      "step": 226370
    },
    {
      "epoch": 7.546,
      "grad_norm": 0.3158651888370514,
      "learning_rate": 2.8375000000000004e-06,
      "loss": 0.0012,
      "step": 226380
    },
    {
      "epoch": 7.546333333333333,
      "grad_norm": 0.15116196870803833,
      "learning_rate": 2.8354166666666667e-06,
      "loss": 0.0018,
      "step": 226390
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 0.05753288045525551,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.002,
      "step": 226400
    },
    {
      "epoch": 7.547,
      "grad_norm": 0.14707055687904358,
      "learning_rate": 2.83125e-06,
      "loss": 0.0015,
      "step": 226410
    },
    {
      "epoch": 7.5473333333333334,
      "grad_norm": 0.34274619817733765,
      "learning_rate": 2.829166666666667e-06,
      "loss": 0.0017,
      "step": 226420
    },
    {
      "epoch": 7.547666666666666,
      "grad_norm": 0.010206812992691994,
      "learning_rate": 2.8270833333333333e-06,
      "loss": 0.0018,
      "step": 226430
    },
    {
      "epoch": 7.548,
      "grad_norm": 0.0092709269374609,
      "learning_rate": 2.825e-06,
      "loss": 0.0015,
      "step": 226440
    },
    {
      "epoch": 7.548333333333334,
      "grad_norm": 0.0578453354537487,
      "learning_rate": 2.822916666666667e-06,
      "loss": 0.0013,
      "step": 226450
    },
    {
      "epoch": 7.548666666666667,
      "grad_norm": 0.22801926732063293,
      "learning_rate": 2.820833333333333e-06,
      "loss": 0.0017,
      "step": 226460
    },
    {
      "epoch": 7.549,
      "grad_norm": 0.0340060219168663,
      "learning_rate": 2.8187500000000003e-06,
      "loss": 0.0011,
      "step": 226470
    },
    {
      "epoch": 7.549333333333333,
      "grad_norm": 0.34593677520751953,
      "learning_rate": 2.8166666666666667e-06,
      "loss": 0.0018,
      "step": 226480
    },
    {
      "epoch": 7.549666666666667,
      "grad_norm": 0.08710186928510666,
      "learning_rate": 2.8145833333333334e-06,
      "loss": 0.0015,
      "step": 226490
    },
    {
      "epoch": 7.55,
      "grad_norm": 0.14308148622512817,
      "learning_rate": 2.8125e-06,
      "loss": 0.0019,
      "step": 226500
    },
    {
      "epoch": 7.550333333333334,
      "grad_norm": 0.2567847967147827,
      "learning_rate": 2.810416666666667e-06,
      "loss": 0.0022,
      "step": 226510
    },
    {
      "epoch": 7.550666666666666,
      "grad_norm": 0.08628273010253906,
      "learning_rate": 2.8083333333333333e-06,
      "loss": 0.0014,
      "step": 226520
    },
    {
      "epoch": 7.551,
      "grad_norm": 0.03903022035956383,
      "learning_rate": 2.80625e-06,
      "loss": 0.0013,
      "step": 226530
    },
    {
      "epoch": 7.551333333333333,
      "grad_norm": 0.08691102266311646,
      "learning_rate": 2.8041666666666668e-06,
      "loss": 0.0013,
      "step": 226540
    },
    {
      "epoch": 7.551666666666667,
      "grad_norm": 0.17135676741600037,
      "learning_rate": 2.802083333333333e-06,
      "loss": 0.0014,
      "step": 226550
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.057844363152980804,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0012,
      "step": 226560
    },
    {
      "epoch": 7.552333333333333,
      "grad_norm": 0.13552290201187134,
      "learning_rate": 2.7979166666666666e-06,
      "loss": 0.002,
      "step": 226570
    },
    {
      "epoch": 7.552666666666667,
      "grad_norm": 0.22829948365688324,
      "learning_rate": 2.795833333333334e-06,
      "loss": 0.0022,
      "step": 226580
    },
    {
      "epoch": 7.553,
      "grad_norm": 0.1142347976565361,
      "learning_rate": 2.79375e-06,
      "loss": 0.0015,
      "step": 226590
    },
    {
      "epoch": 7.553333333333334,
      "grad_norm": 0.11396228522062302,
      "learning_rate": 2.791666666666667e-06,
      "loss": 0.0012,
      "step": 226600
    },
    {
      "epoch": 7.5536666666666665,
      "grad_norm": 0.1141810268163681,
      "learning_rate": 2.7895833333333337e-06,
      "loss": 0.002,
      "step": 226610
    },
    {
      "epoch": 7.554,
      "grad_norm": 0.042420387268066406,
      "learning_rate": 2.7875e-06,
      "loss": 0.0013,
      "step": 226620
    },
    {
      "epoch": 7.554333333333333,
      "grad_norm": 0.017000027000904083,
      "learning_rate": 2.7854166666666668e-06,
      "loss": 0.0016,
      "step": 226630
    },
    {
      "epoch": 7.554666666666667,
      "grad_norm": 0.05923406407237053,
      "learning_rate": 2.7833333333333335e-06,
      "loss": 0.0013,
      "step": 226640
    },
    {
      "epoch": 7.555,
      "grad_norm": 0.314283162355423,
      "learning_rate": 2.7812500000000003e-06,
      "loss": 0.0016,
      "step": 226650
    },
    {
      "epoch": 7.5553333333333335,
      "grad_norm": 0.2854182720184326,
      "learning_rate": 2.7791666666666666e-06,
      "loss": 0.0015,
      "step": 226660
    },
    {
      "epoch": 7.555666666666666,
      "grad_norm": 0.1429644525051117,
      "learning_rate": 2.7770833333333338e-06,
      "loss": 0.0016,
      "step": 226670
    },
    {
      "epoch": 7.556,
      "grad_norm": 0.19945234060287476,
      "learning_rate": 2.775e-06,
      "loss": 0.0013,
      "step": 226680
    },
    {
      "epoch": 7.556333333333333,
      "grad_norm": 0.08663514256477356,
      "learning_rate": 2.772916666666667e-06,
      "loss": 0.0021,
      "step": 226690
    },
    {
      "epoch": 7.556666666666667,
      "grad_norm": 0.11423683911561966,
      "learning_rate": 2.7708333333333336e-06,
      "loss": 0.0014,
      "step": 226700
    },
    {
      "epoch": 7.557,
      "grad_norm": 0.3438214957714081,
      "learning_rate": 2.76875e-06,
      "loss": 0.0013,
      "step": 226710
    },
    {
      "epoch": 7.557333333333333,
      "grad_norm": 0.14352625608444214,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.001,
      "step": 226720
    },
    {
      "epoch": 7.557666666666667,
      "grad_norm": 0.14262424409389496,
      "learning_rate": 2.7645833333333335e-06,
      "loss": 0.0013,
      "step": 226730
    },
    {
      "epoch": 7.558,
      "grad_norm": 0.22887137532234192,
      "learning_rate": 2.7625000000000002e-06,
      "loss": 0.0012,
      "step": 226740
    },
    {
      "epoch": 7.558333333333334,
      "grad_norm": 0.14402823150157928,
      "learning_rate": 2.7604166666666666e-06,
      "loss": 0.0021,
      "step": 226750
    },
    {
      "epoch": 7.558666666666666,
      "grad_norm": 0.1996772736310959,
      "learning_rate": 2.7583333333333337e-06,
      "loss": 0.002,
      "step": 226760
    },
    {
      "epoch": 7.559,
      "grad_norm": 0.011957251466810703,
      "learning_rate": 2.75625e-06,
      "loss": 0.0023,
      "step": 226770
    },
    {
      "epoch": 7.559333333333333,
      "grad_norm": 0.016200510784983635,
      "learning_rate": 2.754166666666667e-06,
      "loss": 0.0016,
      "step": 226780
    },
    {
      "epoch": 7.559666666666667,
      "grad_norm": 0.0681622177362442,
      "learning_rate": 2.7520833333333336e-06,
      "loss": 0.0017,
      "step": 226790
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.19993069767951965,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.0014,
      "step": 226800
    },
    {
      "epoch": 7.560333333333333,
      "grad_norm": 0.05790453404188156,
      "learning_rate": 2.7479166666666667e-06,
      "loss": 0.0012,
      "step": 226810
    },
    {
      "epoch": 7.560666666666666,
      "grad_norm": 0.08645311743021011,
      "learning_rate": 2.7458333333333334e-06,
      "loss": 0.0016,
      "step": 226820
    },
    {
      "epoch": 7.561,
      "grad_norm": 0.06385425478219986,
      "learning_rate": 2.74375e-06,
      "loss": 0.0023,
      "step": 226830
    },
    {
      "epoch": 7.561333333333334,
      "grad_norm": 0.15258097648620605,
      "learning_rate": 2.7416666666666665e-06,
      "loss": 0.0023,
      "step": 226840
    },
    {
      "epoch": 7.5616666666666665,
      "grad_norm": 0.05744394659996033,
      "learning_rate": 2.7395833333333337e-06,
      "loss": 0.0018,
      "step": 226850
    },
    {
      "epoch": 7.562,
      "grad_norm": 0.06123447045683861,
      "learning_rate": 2.7375e-06,
      "loss": 0.0013,
      "step": 226860
    },
    {
      "epoch": 7.562333333333333,
      "grad_norm": 0.14358839392662048,
      "learning_rate": 2.735416666666667e-06,
      "loss": 0.0014,
      "step": 226870
    },
    {
      "epoch": 7.562666666666667,
      "grad_norm": 0.14323607087135315,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0012,
      "step": 226880
    },
    {
      "epoch": 7.563,
      "grad_norm": 0.14320023357868195,
      "learning_rate": 2.7312500000000003e-06,
      "loss": 0.0014,
      "step": 226890
    },
    {
      "epoch": 7.5633333333333335,
      "grad_norm": 0.11482042819261551,
      "learning_rate": 2.7291666666666667e-06,
      "loss": 0.0013,
      "step": 226900
    },
    {
      "epoch": 7.563666666666666,
      "grad_norm": 0.25702938437461853,
      "learning_rate": 2.7270833333333334e-06,
      "loss": 0.0012,
      "step": 226910
    },
    {
      "epoch": 7.564,
      "grad_norm": 0.14287510514259338,
      "learning_rate": 2.725e-06,
      "loss": 0.0018,
      "step": 226920
    },
    {
      "epoch": 7.564333333333334,
      "grad_norm": 0.05051266774535179,
      "learning_rate": 2.7229166666666665e-06,
      "loss": 0.0017,
      "step": 226930
    },
    {
      "epoch": 7.564666666666667,
      "grad_norm": 0.25767767429351807,
      "learning_rate": 2.7208333333333337e-06,
      "loss": 0.0017,
      "step": 226940
    },
    {
      "epoch": 7.5649999999999995,
      "grad_norm": 0.015560925006866455,
      "learning_rate": 2.71875e-06,
      "loss": 0.0015,
      "step": 226950
    },
    {
      "epoch": 7.565333333333333,
      "grad_norm": 0.11502131819725037,
      "learning_rate": 2.7166666666666668e-06,
      "loss": 0.0015,
      "step": 226960
    },
    {
      "epoch": 7.565666666666667,
      "grad_norm": 0.1997479498386383,
      "learning_rate": 2.7145833333333335e-06,
      "loss": 0.0016,
      "step": 226970
    },
    {
      "epoch": 7.566,
      "grad_norm": 0.17076005041599274,
      "learning_rate": 2.7125000000000003e-06,
      "loss": 0.002,
      "step": 226980
    },
    {
      "epoch": 7.566333333333334,
      "grad_norm": 0.42768776416778564,
      "learning_rate": 2.7104166666666666e-06,
      "loss": 0.0019,
      "step": 226990
    },
    {
      "epoch": 7.566666666666666,
      "grad_norm": 0.3328782320022583,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 0.0023,
      "step": 227000
    },
    {
      "epoch": 7.567,
      "grad_norm": 0.05728805437684059,
      "learning_rate": 2.70625e-06,
      "loss": 0.0014,
      "step": 227010
    },
    {
      "epoch": 7.567333333333333,
      "grad_norm": 0.2281758189201355,
      "learning_rate": 2.7041666666666665e-06,
      "loss": 0.0015,
      "step": 227020
    },
    {
      "epoch": 7.567666666666667,
      "grad_norm": 0.030776744708418846,
      "learning_rate": 2.7020833333333337e-06,
      "loss": 0.0014,
      "step": 227030
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.34248238801956177,
      "learning_rate": 2.7e-06,
      "loss": 0.0015,
      "step": 227040
    },
    {
      "epoch": 7.568333333333333,
      "grad_norm": 0.02975904755294323,
      "learning_rate": 2.6979166666666667e-06,
      "loss": 0.0021,
      "step": 227050
    },
    {
      "epoch": 7.568666666666667,
      "grad_norm": 0.14274293184280396,
      "learning_rate": 2.6958333333333335e-06,
      "loss": 0.0012,
      "step": 227060
    },
    {
      "epoch": 7.569,
      "grad_norm": 0.057633835822343826,
      "learning_rate": 2.6937500000000003e-06,
      "loss": 0.0013,
      "step": 227070
    },
    {
      "epoch": 7.569333333333334,
      "grad_norm": 0.20021294057369232,
      "learning_rate": 2.6916666666666666e-06,
      "loss": 0.0019,
      "step": 227080
    },
    {
      "epoch": 7.5696666666666665,
      "grad_norm": 0.10912805050611496,
      "learning_rate": 2.6895833333333333e-06,
      "loss": 0.0015,
      "step": 227090
    },
    {
      "epoch": 7.57,
      "grad_norm": 0.22842226922512054,
      "learning_rate": 2.6875e-06,
      "loss": 0.0019,
      "step": 227100
    },
    {
      "epoch": 7.570333333333333,
      "grad_norm": 0.39903920888900757,
      "learning_rate": 2.6854166666666664e-06,
      "loss": 0.0013,
      "step": 227110
    },
    {
      "epoch": 7.570666666666667,
      "grad_norm": 0.14286376535892487,
      "learning_rate": 2.6833333333333336e-06,
      "loss": 0.0015,
      "step": 227120
    },
    {
      "epoch": 7.571,
      "grad_norm": 0.11460840702056885,
      "learning_rate": 2.68125e-06,
      "loss": 0.0011,
      "step": 227130
    },
    {
      "epoch": 7.5713333333333335,
      "grad_norm": 0.1720026582479477,
      "learning_rate": 2.6791666666666667e-06,
      "loss": 0.0015,
      "step": 227140
    },
    {
      "epoch": 7.571666666666666,
      "grad_norm": 0.40030810236930847,
      "learning_rate": 2.6770833333333335e-06,
      "loss": 0.002,
      "step": 227150
    },
    {
      "epoch": 7.572,
      "grad_norm": 0.08669338375329971,
      "learning_rate": 2.6750000000000002e-06,
      "loss": 0.002,
      "step": 227160
    },
    {
      "epoch": 7.572333333333333,
      "grad_norm": 0.2359110713005066,
      "learning_rate": 2.6729166666666666e-06,
      "loss": 0.0014,
      "step": 227170
    },
    {
      "epoch": 7.572666666666667,
      "grad_norm": 0.2285005897283554,
      "learning_rate": 2.6708333333333333e-06,
      "loss": 0.0031,
      "step": 227180
    },
    {
      "epoch": 7.573,
      "grad_norm": 0.2550038993358612,
      "learning_rate": 2.66875e-06,
      "loss": 0.0016,
      "step": 227190
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.11282282322645187,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0016,
      "step": 227200
    },
    {
      "epoch": 7.573666666666667,
      "grad_norm": 0.1146145835518837,
      "learning_rate": 2.6645833333333336e-06,
      "loss": 0.0018,
      "step": 227210
    },
    {
      "epoch": 7.574,
      "grad_norm": 0.14252616465091705,
      "learning_rate": 2.6625e-06,
      "loss": 0.0014,
      "step": 227220
    },
    {
      "epoch": 7.574333333333334,
      "grad_norm": 0.256791889667511,
      "learning_rate": 2.660416666666667e-06,
      "loss": 0.0016,
      "step": 227230
    },
    {
      "epoch": 7.574666666666666,
      "grad_norm": 0.17129726707935333,
      "learning_rate": 2.6583333333333334e-06,
      "loss": 0.002,
      "step": 227240
    },
    {
      "epoch": 7.575,
      "grad_norm": 0.17141257226467133,
      "learning_rate": 2.65625e-06,
      "loss": 0.0016,
      "step": 227250
    },
    {
      "epoch": 7.575333333333333,
      "grad_norm": 0.11615540087223053,
      "learning_rate": 2.654166666666667e-06,
      "loss": 0.0018,
      "step": 227260
    },
    {
      "epoch": 7.575666666666667,
      "grad_norm": 0.20259933173656464,
      "learning_rate": 2.6520833333333337e-06,
      "loss": 0.0012,
      "step": 227270
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.05807315930724144,
      "learning_rate": 2.65e-06,
      "loss": 0.0021,
      "step": 227280
    },
    {
      "epoch": 7.576333333333333,
      "grad_norm": 0.06292856484651566,
      "learning_rate": 2.647916666666667e-06,
      "loss": 0.0015,
      "step": 227290
    },
    {
      "epoch": 7.576666666666666,
      "grad_norm": 0.05871226266026497,
      "learning_rate": 2.6458333333333336e-06,
      "loss": 0.0016,
      "step": 227300
    },
    {
      "epoch": 7.577,
      "grad_norm": 0.17149478197097778,
      "learning_rate": 2.64375e-06,
      "loss": 0.0017,
      "step": 227310
    },
    {
      "epoch": 7.577333333333334,
      "grad_norm": 0.19979441165924072,
      "learning_rate": 2.641666666666667e-06,
      "loss": 0.0015,
      "step": 227320
    },
    {
      "epoch": 7.5776666666666666,
      "grad_norm": 0.03126998990774155,
      "learning_rate": 2.6395833333333334e-06,
      "loss": 0.0013,
      "step": 227330
    },
    {
      "epoch": 7.578,
      "grad_norm": 0.2010924518108368,
      "learning_rate": 2.6375e-06,
      "loss": 0.0017,
      "step": 227340
    },
    {
      "epoch": 7.578333333333333,
      "grad_norm": 0.25736916065216064,
      "learning_rate": 2.635416666666667e-06,
      "loss": 0.0013,
      "step": 227350
    },
    {
      "epoch": 7.578666666666667,
      "grad_norm": 0.015601533465087414,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.0019,
      "step": 227360
    },
    {
      "epoch": 7.579,
      "grad_norm": 0.27705225348472595,
      "learning_rate": 2.63125e-06,
      "loss": 0.0015,
      "step": 227370
    },
    {
      "epoch": 7.5793333333333335,
      "grad_norm": 0.2287474274635315,
      "learning_rate": 2.6291666666666668e-06,
      "loss": 0.0016,
      "step": 227380
    },
    {
      "epoch": 7.579666666666666,
      "grad_norm": 0.25714412331581116,
      "learning_rate": 2.6270833333333335e-06,
      "loss": 0.0021,
      "step": 227390
    },
    {
      "epoch": 7.58,
      "grad_norm": 0.16995778679847717,
      "learning_rate": 2.625e-06,
      "loss": 0.0016,
      "step": 227400
    },
    {
      "epoch": 7.580333333333334,
      "grad_norm": 0.03166839852929115,
      "learning_rate": 2.622916666666667e-06,
      "loss": 0.0014,
      "step": 227410
    },
    {
      "epoch": 7.580666666666667,
      "grad_norm": 0.22868068516254425,
      "learning_rate": 2.6208333333333334e-06,
      "loss": 0.0014,
      "step": 227420
    },
    {
      "epoch": 7.5809999999999995,
      "grad_norm": 0.08618374168872833,
      "learning_rate": 2.61875e-06,
      "loss": 0.0013,
      "step": 227430
    },
    {
      "epoch": 7.581333333333333,
      "grad_norm": 0.17138761281967163,
      "learning_rate": 2.616666666666667e-06,
      "loss": 0.0013,
      "step": 227440
    },
    {
      "epoch": 7.581666666666667,
      "grad_norm": 0.0860433578491211,
      "learning_rate": 2.6145833333333336e-06,
      "loss": 0.0019,
      "step": 227450
    },
    {
      "epoch": 7.582,
      "grad_norm": 0.1717357486486435,
      "learning_rate": 2.6125e-06,
      "loss": 0.0014,
      "step": 227460
    },
    {
      "epoch": 7.582333333333334,
      "grad_norm": 0.17147161066532135,
      "learning_rate": 2.6104166666666667e-06,
      "loss": 0.0028,
      "step": 227470
    },
    {
      "epoch": 7.582666666666666,
      "grad_norm": 0.14300483465194702,
      "learning_rate": 2.6083333333333335e-06,
      "loss": 0.0025,
      "step": 227480
    },
    {
      "epoch": 7.583,
      "grad_norm": 0.17116384208202362,
      "learning_rate": 2.60625e-06,
      "loss": 0.0014,
      "step": 227490
    },
    {
      "epoch": 7.583333333333333,
      "grad_norm": 0.08621124178171158,
      "learning_rate": 2.604166666666667e-06,
      "loss": 0.0016,
      "step": 227500
    },
    {
      "epoch": 7.583666666666667,
      "grad_norm": 0.3961047828197479,
      "learning_rate": 2.6020833333333333e-06,
      "loss": 0.0014,
      "step": 227510
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.2311238795518875,
      "learning_rate": 2.6e-06,
      "loss": 0.0016,
      "step": 227520
    },
    {
      "epoch": 7.584333333333333,
      "grad_norm": 0.030583573505282402,
      "learning_rate": 2.597916666666667e-06,
      "loss": 0.0016,
      "step": 227530
    },
    {
      "epoch": 7.584666666666667,
      "grad_norm": 0.020641960203647614,
      "learning_rate": 2.5958333333333336e-06,
      "loss": 0.0016,
      "step": 227540
    },
    {
      "epoch": 7.585,
      "grad_norm": 0.014739957638084888,
      "learning_rate": 2.59375e-06,
      "loss": 0.0014,
      "step": 227550
    },
    {
      "epoch": 7.585333333333334,
      "grad_norm": 0.03320744261145592,
      "learning_rate": 2.5916666666666667e-06,
      "loss": 0.0019,
      "step": 227560
    },
    {
      "epoch": 7.585666666666667,
      "grad_norm": 0.28475743532180786,
      "learning_rate": 2.5895833333333335e-06,
      "loss": 0.0011,
      "step": 227570
    },
    {
      "epoch": 7.586,
      "grad_norm": 0.08647484332323074,
      "learning_rate": 2.5875e-06,
      "loss": 0.0018,
      "step": 227580
    },
    {
      "epoch": 7.586333333333333,
      "grad_norm": 0.45645779371261597,
      "learning_rate": 2.585416666666667e-06,
      "loss": 0.0013,
      "step": 227590
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 0.14274908602237701,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.0014,
      "step": 227600
    },
    {
      "epoch": 7.587,
      "grad_norm": 0.20033016800880432,
      "learning_rate": 2.58125e-06,
      "loss": 0.0019,
      "step": 227610
    },
    {
      "epoch": 7.5873333333333335,
      "grad_norm": 0.18944327533245087,
      "learning_rate": 2.579166666666667e-06,
      "loss": 0.0023,
      "step": 227620
    },
    {
      "epoch": 7.587666666666666,
      "grad_norm": 0.2852451801300049,
      "learning_rate": 2.5770833333333336e-06,
      "loss": 0.0024,
      "step": 227630
    },
    {
      "epoch": 7.588,
      "grad_norm": 0.2966025471687317,
      "learning_rate": 2.575e-06,
      "loss": 0.0012,
      "step": 227640
    },
    {
      "epoch": 7.588333333333333,
      "grad_norm": 0.03343023732304573,
      "learning_rate": 2.5729166666666667e-06,
      "loss": 0.0019,
      "step": 227650
    },
    {
      "epoch": 7.588666666666667,
      "grad_norm": 0.11464475840330124,
      "learning_rate": 2.5708333333333334e-06,
      "loss": 0.0014,
      "step": 227660
    },
    {
      "epoch": 7.589,
      "grad_norm": 0.18130053579807281,
      "learning_rate": 2.56875e-06,
      "loss": 0.0015,
      "step": 227670
    },
    {
      "epoch": 7.589333333333333,
      "grad_norm": 0.029676293954253197,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.0021,
      "step": 227680
    },
    {
      "epoch": 7.589666666666667,
      "grad_norm": 0.2283371537923813,
      "learning_rate": 2.5645833333333333e-06,
      "loss": 0.0014,
      "step": 227690
    },
    {
      "epoch": 7.59,
      "grad_norm": 0.17144618928432465,
      "learning_rate": 2.5625e-06,
      "loss": 0.0018,
      "step": 227700
    },
    {
      "epoch": 7.590333333333334,
      "grad_norm": 0.11514322459697723,
      "learning_rate": 2.5604166666666668e-06,
      "loss": 0.0017,
      "step": 227710
    },
    {
      "epoch": 7.5906666666666665,
      "grad_norm": 0.17198611795902252,
      "learning_rate": 2.5583333333333335e-06,
      "loss": 0.0015,
      "step": 227720
    },
    {
      "epoch": 7.591,
      "grad_norm": 0.08684959262609482,
      "learning_rate": 2.55625e-06,
      "loss": 0.0017,
      "step": 227730
    },
    {
      "epoch": 7.591333333333333,
      "grad_norm": 0.05900576710700989,
      "learning_rate": 2.554166666666667e-06,
      "loss": 0.0015,
      "step": 227740
    },
    {
      "epoch": 7.591666666666667,
      "grad_norm": 0.1995423585176468,
      "learning_rate": 2.5520833333333334e-06,
      "loss": 0.0014,
      "step": 227750
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.180414617061615,
      "learning_rate": 2.55e-06,
      "loss": 0.0013,
      "step": 227760
    },
    {
      "epoch": 7.592333333333333,
      "grad_norm": 0.3697250485420227,
      "learning_rate": 2.547916666666667e-06,
      "loss": 0.0012,
      "step": 227770
    },
    {
      "epoch": 7.592666666666666,
      "grad_norm": 0.2853606045246124,
      "learning_rate": 2.5458333333333332e-06,
      "loss": 0.0017,
      "step": 227780
    },
    {
      "epoch": 7.593,
      "grad_norm": 0.3146005868911743,
      "learning_rate": 2.54375e-06,
      "loss": 0.0021,
      "step": 227790
    },
    {
      "epoch": 7.593333333333334,
      "grad_norm": 0.20592093467712402,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 0.0018,
      "step": 227800
    },
    {
      "epoch": 7.593666666666667,
      "grad_norm": 0.21818231046199799,
      "learning_rate": 2.5395833333333335e-06,
      "loss": 0.0019,
      "step": 227810
    },
    {
      "epoch": 7.594,
      "grad_norm": 0.046278513967990875,
      "learning_rate": 2.5375000000000003e-06,
      "loss": 0.0015,
      "step": 227820
    },
    {
      "epoch": 7.594333333333333,
      "grad_norm": 0.1712142527103424,
      "learning_rate": 2.535416666666667e-06,
      "loss": 0.0015,
      "step": 227830
    },
    {
      "epoch": 7.594666666666667,
      "grad_norm": 0.2853725552558899,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0014,
      "step": 227840
    },
    {
      "epoch": 7.595,
      "grad_norm": 0.34203603863716125,
      "learning_rate": 2.53125e-06,
      "loss": 0.0014,
      "step": 227850
    },
    {
      "epoch": 7.5953333333333335,
      "grad_norm": 0.03147219121456146,
      "learning_rate": 2.529166666666667e-06,
      "loss": 0.0018,
      "step": 227860
    },
    {
      "epoch": 7.595666666666666,
      "grad_norm": 0.029125625267624855,
      "learning_rate": 2.527083333333333e-06,
      "loss": 0.0019,
      "step": 227870
    },
    {
      "epoch": 7.596,
      "grad_norm": 0.1881924420595169,
      "learning_rate": 2.5250000000000004e-06,
      "loss": 0.0014,
      "step": 227880
    },
    {
      "epoch": 7.596333333333334,
      "grad_norm": 0.11463423818349838,
      "learning_rate": 2.5229166666666667e-06,
      "loss": 0.0013,
      "step": 227890
    },
    {
      "epoch": 7.596666666666667,
      "grad_norm": 0.11920123547315598,
      "learning_rate": 2.5208333333333335e-06,
      "loss": 0.0018,
      "step": 227900
    },
    {
      "epoch": 7.5969999999999995,
      "grad_norm": 0.2436283528804779,
      "learning_rate": 2.5187500000000002e-06,
      "loss": 0.0015,
      "step": 227910
    },
    {
      "epoch": 7.597333333333333,
      "grad_norm": 0.08590465039014816,
      "learning_rate": 2.516666666666667e-06,
      "loss": 0.0018,
      "step": 227920
    },
    {
      "epoch": 7.597666666666667,
      "grad_norm": 0.009347179904580116,
      "learning_rate": 2.5145833333333333e-06,
      "loss": 0.0017,
      "step": 227930
    },
    {
      "epoch": 7.598,
      "grad_norm": 0.2569691836833954,
      "learning_rate": 2.5125e-06,
      "loss": 0.0013,
      "step": 227940
    },
    {
      "epoch": 7.598333333333334,
      "grad_norm": 0.03133552893996239,
      "learning_rate": 2.510416666666667e-06,
      "loss": 0.0018,
      "step": 227950
    },
    {
      "epoch": 7.5986666666666665,
      "grad_norm": 0.01304764673113823,
      "learning_rate": 2.508333333333333e-06,
      "loss": 0.0012,
      "step": 227960
    },
    {
      "epoch": 7.599,
      "grad_norm": 0.17131589353084564,
      "learning_rate": 2.5062500000000004e-06,
      "loss": 0.0012,
      "step": 227970
    },
    {
      "epoch": 7.599333333333333,
      "grad_norm": 0.14307941496372223,
      "learning_rate": 2.5041666666666667e-06,
      "loss": 0.0013,
      "step": 227980
    },
    {
      "epoch": 7.599666666666667,
      "grad_norm": 0.17152544856071472,
      "learning_rate": 2.5020833333333334e-06,
      "loss": 0.0015,
      "step": 227990
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.058846909552812576,
      "learning_rate": 2.5e-06,
      "loss": 0.0018,
      "step": 228000
    },
    {
      "epoch": 7.600333333333333,
      "grad_norm": 0.09933822602033615,
      "learning_rate": 2.497916666666667e-06,
      "loss": 0.0012,
      "step": 228010
    },
    {
      "epoch": 7.600666666666667,
      "grad_norm": 0.14356279373168945,
      "learning_rate": 2.4958333333333333e-06,
      "loss": 0.0017,
      "step": 228020
    },
    {
      "epoch": 7.601,
      "grad_norm": 0.31830182671546936,
      "learning_rate": 2.49375e-06,
      "loss": 0.0018,
      "step": 228030
    },
    {
      "epoch": 7.601333333333334,
      "grad_norm": 0.14270618557929993,
      "learning_rate": 2.491666666666667e-06,
      "loss": 0.0018,
      "step": 228040
    },
    {
      "epoch": 7.601666666666667,
      "grad_norm": 0.22850580513477325,
      "learning_rate": 2.489583333333333e-06,
      "loss": 0.0016,
      "step": 228050
    },
    {
      "epoch": 7.602,
      "grad_norm": 0.2776116728782654,
      "learning_rate": 2.4875000000000003e-06,
      "loss": 0.0014,
      "step": 228060
    },
    {
      "epoch": 7.602333333333333,
      "grad_norm": 0.1721222698688507,
      "learning_rate": 2.4854166666666667e-06,
      "loss": 0.0022,
      "step": 228070
    },
    {
      "epoch": 7.602666666666667,
      "grad_norm": 0.018222425132989883,
      "learning_rate": 2.4833333333333334e-06,
      "loss": 0.0019,
      "step": 228080
    },
    {
      "epoch": 7.603,
      "grad_norm": 0.012106850743293762,
      "learning_rate": 2.48125e-06,
      "loss": 0.0014,
      "step": 228090
    },
    {
      "epoch": 7.6033333333333335,
      "grad_norm": 0.03376543149352074,
      "learning_rate": 2.479166666666667e-06,
      "loss": 0.0017,
      "step": 228100
    },
    {
      "epoch": 7.603666666666666,
      "grad_norm": 0.12095075845718384,
      "learning_rate": 2.4770833333333333e-06,
      "loss": 0.0013,
      "step": 228110
    },
    {
      "epoch": 7.604,
      "grad_norm": 0.14309929311275482,
      "learning_rate": 2.4750000000000004e-06,
      "loss": 0.0019,
      "step": 228120
    },
    {
      "epoch": 7.604333333333333,
      "grad_norm": 0.14770974218845367,
      "learning_rate": 2.4729166666666668e-06,
      "loss": 0.0016,
      "step": 228130
    },
    {
      "epoch": 7.604666666666667,
      "grad_norm": 0.03085348755121231,
      "learning_rate": 2.4708333333333335e-06,
      "loss": 0.0012,
      "step": 228140
    },
    {
      "epoch": 7.605,
      "grad_norm": 0.48620352149009705,
      "learning_rate": 2.4687500000000003e-06,
      "loss": 0.002,
      "step": 228150
    },
    {
      "epoch": 7.605333333333333,
      "grad_norm": 0.08572845906019211,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0017,
      "step": 228160
    },
    {
      "epoch": 7.605666666666667,
      "grad_norm": 0.08668219298124313,
      "learning_rate": 2.4645833333333334e-06,
      "loss": 0.0023,
      "step": 228170
    },
    {
      "epoch": 7.606,
      "grad_norm": 0.10298099368810654,
      "learning_rate": 2.4625e-06,
      "loss": 0.0012,
      "step": 228180
    },
    {
      "epoch": 7.606333333333334,
      "grad_norm": 0.31668543815612793,
      "learning_rate": 2.460416666666667e-06,
      "loss": 0.0023,
      "step": 228190
    },
    {
      "epoch": 7.6066666666666665,
      "grad_norm": 0.32007309794425964,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 0.0019,
      "step": 228200
    },
    {
      "epoch": 7.607,
      "grad_norm": 0.2246423065662384,
      "learning_rate": 2.4562500000000004e-06,
      "loss": 0.0018,
      "step": 228210
    },
    {
      "epoch": 7.607333333333333,
      "grad_norm": 0.2858114242553711,
      "learning_rate": 2.4541666666666667e-06,
      "loss": 0.0015,
      "step": 228220
    },
    {
      "epoch": 7.607666666666667,
      "grad_norm": 0.171770378947258,
      "learning_rate": 2.4520833333333335e-06,
      "loss": 0.0014,
      "step": 228230
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.029512016102671623,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 0.0019,
      "step": 228240
    },
    {
      "epoch": 7.608333333333333,
      "grad_norm": 0.46263307332992554,
      "learning_rate": 2.4479166666666666e-06,
      "loss": 0.0015,
      "step": 228250
    },
    {
      "epoch": 7.608666666666666,
      "grad_norm": 0.05694887787103653,
      "learning_rate": 2.4458333333333334e-06,
      "loss": 0.0022,
      "step": 228260
    },
    {
      "epoch": 7.609,
      "grad_norm": 0.009600262157619,
      "learning_rate": 2.44375e-06,
      "loss": 0.002,
      "step": 228270
    },
    {
      "epoch": 7.609333333333334,
      "grad_norm": 0.038188815116882324,
      "learning_rate": 2.441666666666667e-06,
      "loss": 0.0017,
      "step": 228280
    },
    {
      "epoch": 7.609666666666667,
      "grad_norm": 0.058956388384103775,
      "learning_rate": 2.439583333333333e-06,
      "loss": 0.0014,
      "step": 228290
    },
    {
      "epoch": 7.61,
      "grad_norm": 0.11506528407335281,
      "learning_rate": 2.4375000000000004e-06,
      "loss": 0.0016,
      "step": 228300
    },
    {
      "epoch": 7.610333333333333,
      "grad_norm": 0.17146436870098114,
      "learning_rate": 2.4354166666666667e-06,
      "loss": 0.0018,
      "step": 228310
    },
    {
      "epoch": 7.610666666666667,
      "grad_norm": 0.22916296124458313,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0014,
      "step": 228320
    },
    {
      "epoch": 7.611,
      "grad_norm": 0.025525201112031937,
      "learning_rate": 2.4312500000000002e-06,
      "loss": 0.0019,
      "step": 228330
    },
    {
      "epoch": 7.6113333333333335,
      "grad_norm": 0.3123534619808197,
      "learning_rate": 2.4291666666666666e-06,
      "loss": 0.0025,
      "step": 228340
    },
    {
      "epoch": 7.611666666666666,
      "grad_norm": 0.17126521468162537,
      "learning_rate": 2.4270833333333333e-06,
      "loss": 0.0013,
      "step": 228350
    },
    {
      "epoch": 7.612,
      "grad_norm": 0.03114893287420273,
      "learning_rate": 2.425e-06,
      "loss": 0.0013,
      "step": 228360
    },
    {
      "epoch": 7.612333333333333,
      "grad_norm": 0.3875219225883484,
      "learning_rate": 2.422916666666667e-06,
      "loss": 0.0018,
      "step": 228370
    },
    {
      "epoch": 7.612666666666667,
      "grad_norm": 0.4288449287414551,
      "learning_rate": 2.420833333333333e-06,
      "loss": 0.0016,
      "step": 228380
    },
    {
      "epoch": 7.6129999999999995,
      "grad_norm": 0.36868494749069214,
      "learning_rate": 2.4187500000000003e-06,
      "loss": 0.0019,
      "step": 228390
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 0.17127710580825806,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.0011,
      "step": 228400
    },
    {
      "epoch": 7.613666666666667,
      "grad_norm": 0.11582132428884506,
      "learning_rate": 2.4145833333333334e-06,
      "loss": 0.0017,
      "step": 228410
    },
    {
      "epoch": 7.614,
      "grad_norm": 0.058572206646203995,
      "learning_rate": 2.4125e-06,
      "loss": 0.0019,
      "step": 228420
    },
    {
      "epoch": 7.614333333333334,
      "grad_norm": 0.19989316165447235,
      "learning_rate": 2.4104166666666665e-06,
      "loss": 0.0026,
      "step": 228430
    },
    {
      "epoch": 7.6146666666666665,
      "grad_norm": 0.2570428252220154,
      "learning_rate": 2.4083333333333337e-06,
      "loss": 0.0019,
      "step": 228440
    },
    {
      "epoch": 7.615,
      "grad_norm": 0.2598646879196167,
      "learning_rate": 2.40625e-06,
      "loss": 0.0018,
      "step": 228450
    },
    {
      "epoch": 7.615333333333333,
      "grad_norm": 0.4560333490371704,
      "learning_rate": 2.404166666666667e-06,
      "loss": 0.0018,
      "step": 228460
    },
    {
      "epoch": 7.615666666666667,
      "grad_norm": 0.2860884666442871,
      "learning_rate": 2.4020833333333336e-06,
      "loss": 0.0014,
      "step": 228470
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.19945739209651947,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0016,
      "step": 228480
    },
    {
      "epoch": 7.616333333333333,
      "grad_norm": 0.02954346127808094,
      "learning_rate": 2.3979166666666666e-06,
      "loss": 0.0018,
      "step": 228490
    },
    {
      "epoch": 7.616666666666667,
      "grad_norm": 0.06330393999814987,
      "learning_rate": 2.3958333333333334e-06,
      "loss": 0.0015,
      "step": 228500
    },
    {
      "epoch": 7.617,
      "grad_norm": 0.2749718129634857,
      "learning_rate": 2.39375e-06,
      "loss": 0.0015,
      "step": 228510
    },
    {
      "epoch": 7.617333333333333,
      "grad_norm": 0.05774093419313431,
      "learning_rate": 2.3916666666666665e-06,
      "loss": 0.0022,
      "step": 228520
    },
    {
      "epoch": 7.617666666666667,
      "grad_norm": 0.2857420742511749,
      "learning_rate": 2.3895833333333337e-06,
      "loss": 0.0016,
      "step": 228530
    },
    {
      "epoch": 7.618,
      "grad_norm": 0.032445162534713745,
      "learning_rate": 2.3875e-06,
      "loss": 0.0014,
      "step": 228540
    },
    {
      "epoch": 7.618333333333333,
      "grad_norm": 0.11445643752813339,
      "learning_rate": 2.3854166666666668e-06,
      "loss": 0.0013,
      "step": 228550
    },
    {
      "epoch": 7.618666666666667,
      "grad_norm": 0.14290781319141388,
      "learning_rate": 2.3833333333333335e-06,
      "loss": 0.0021,
      "step": 228560
    },
    {
      "epoch": 7.619,
      "grad_norm": 0.031026417389512062,
      "learning_rate": 2.3812500000000003e-06,
      "loss": 0.0013,
      "step": 228570
    },
    {
      "epoch": 7.6193333333333335,
      "grad_norm": 0.1724901795387268,
      "learning_rate": 2.3791666666666666e-06,
      "loss": 0.0018,
      "step": 228580
    },
    {
      "epoch": 7.619666666666666,
      "grad_norm": 0.02659722790122032,
      "learning_rate": 2.377083333333334e-06,
      "loss": 0.0021,
      "step": 228590
    },
    {
      "epoch": 7.62,
      "grad_norm": 0.17074203491210938,
      "learning_rate": 2.375e-06,
      "loss": 0.0027,
      "step": 228600
    },
    {
      "epoch": 7.620333333333333,
      "grad_norm": 0.2286640703678131,
      "learning_rate": 2.372916666666667e-06,
      "loss": 0.0014,
      "step": 228610
    },
    {
      "epoch": 7.620666666666667,
      "grad_norm": 0.2284320890903473,
      "learning_rate": 2.3708333333333336e-06,
      "loss": 0.0014,
      "step": 228620
    },
    {
      "epoch": 7.621,
      "grad_norm": 0.2002030611038208,
      "learning_rate": 2.36875e-06,
      "loss": 0.0017,
      "step": 228630
    },
    {
      "epoch": 7.621333333333333,
      "grad_norm": 0.28596508502960205,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.0019,
      "step": 228640
    },
    {
      "epoch": 7.621666666666667,
      "grad_norm": 0.031776800751686096,
      "learning_rate": 2.3645833333333335e-06,
      "loss": 0.0016,
      "step": 228650
    },
    {
      "epoch": 7.622,
      "grad_norm": 0.010258637368679047,
      "learning_rate": 2.3625000000000003e-06,
      "loss": 0.0021,
      "step": 228660
    },
    {
      "epoch": 7.622333333333334,
      "grad_norm": 0.11551937460899353,
      "learning_rate": 2.3604166666666666e-06,
      "loss": 0.0013,
      "step": 228670
    },
    {
      "epoch": 7.6226666666666665,
      "grad_norm": 0.1714973896741867,
      "learning_rate": 2.3583333333333338e-06,
      "loss": 0.0016,
      "step": 228680
    },
    {
      "epoch": 7.623,
      "grad_norm": 0.021510180085897446,
      "learning_rate": 2.35625e-06,
      "loss": 0.0011,
      "step": 228690
    },
    {
      "epoch": 7.623333333333333,
      "grad_norm": 0.058198221027851105,
      "learning_rate": 2.354166666666667e-06,
      "loss": 0.0013,
      "step": 228700
    },
    {
      "epoch": 7.623666666666667,
      "grad_norm": 0.03131914511322975,
      "learning_rate": 2.3520833333333336e-06,
      "loss": 0.0015,
      "step": 228710
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.11398924887180328,
      "learning_rate": 2.35e-06,
      "loss": 0.0017,
      "step": 228720
    },
    {
      "epoch": 7.624333333333333,
      "grad_norm": 0.17146120965480804,
      "learning_rate": 2.3479166666666667e-06,
      "loss": 0.0017,
      "step": 228730
    },
    {
      "epoch": 7.624666666666666,
      "grad_norm": 0.1713988035917282,
      "learning_rate": 2.3458333333333335e-06,
      "loss": 0.0014,
      "step": 228740
    },
    {
      "epoch": 7.625,
      "grad_norm": 0.11447418481111526,
      "learning_rate": 2.3437500000000002e-06,
      "loss": 0.0017,
      "step": 228750
    },
    {
      "epoch": 7.625333333333334,
      "grad_norm": 0.1455174684524536,
      "learning_rate": 2.3416666666666666e-06,
      "loss": 0.0021,
      "step": 228760
    },
    {
      "epoch": 7.625666666666667,
      "grad_norm": 0.14294366538524628,
      "learning_rate": 2.3395833333333337e-06,
      "loss": 0.0014,
      "step": 228770
    },
    {
      "epoch": 7.626,
      "grad_norm": 0.11453241109848022,
      "learning_rate": 2.3375e-06,
      "loss": 0.0015,
      "step": 228780
    },
    {
      "epoch": 7.626333333333333,
      "grad_norm": 0.22896820306777954,
      "learning_rate": 2.335416666666667e-06,
      "loss": 0.0017,
      "step": 228790
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.14354366064071655,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0016,
      "step": 228800
    },
    {
      "epoch": 7.627,
      "grad_norm": 0.06109421327710152,
      "learning_rate": 2.33125e-06,
      "loss": 0.0015,
      "step": 228810
    },
    {
      "epoch": 7.6273333333333335,
      "grad_norm": 0.05755433067679405,
      "learning_rate": 2.3291666666666667e-06,
      "loss": 0.0011,
      "step": 228820
    },
    {
      "epoch": 7.627666666666666,
      "grad_norm": 0.26215460896492004,
      "learning_rate": 2.3270833333333334e-06,
      "loss": 0.0018,
      "step": 228830
    },
    {
      "epoch": 7.628,
      "grad_norm": 0.1717732697725296,
      "learning_rate": 2.325e-06,
      "loss": 0.0019,
      "step": 228840
    },
    {
      "epoch": 7.628333333333333,
      "grad_norm": 0.34545522928237915,
      "learning_rate": 2.3229166666666665e-06,
      "loss": 0.0013,
      "step": 228850
    },
    {
      "epoch": 7.628666666666667,
      "grad_norm": 0.28606724739074707,
      "learning_rate": 2.3208333333333337e-06,
      "loss": 0.0017,
      "step": 228860
    },
    {
      "epoch": 7.629,
      "grad_norm": 0.256887286901474,
      "learning_rate": 2.31875e-06,
      "loss": 0.0015,
      "step": 228870
    },
    {
      "epoch": 7.629333333333333,
      "grad_norm": 0.1432265341281891,
      "learning_rate": 2.316666666666667e-06,
      "loss": 0.0017,
      "step": 228880
    },
    {
      "epoch": 7.629666666666667,
      "grad_norm": 0.14298854768276215,
      "learning_rate": 2.3145833333333335e-06,
      "loss": 0.0015,
      "step": 228890
    },
    {
      "epoch": 7.63,
      "grad_norm": 0.012032312341034412,
      "learning_rate": 2.3125e-06,
      "loss": 0.0022,
      "step": 228900
    },
    {
      "epoch": 7.630333333333334,
      "grad_norm": 0.11438095569610596,
      "learning_rate": 2.3104166666666666e-06,
      "loss": 0.0023,
      "step": 228910
    },
    {
      "epoch": 7.6306666666666665,
      "grad_norm": 0.11592426896095276,
      "learning_rate": 2.3083333333333334e-06,
      "loss": 0.0017,
      "step": 228920
    },
    {
      "epoch": 7.631,
      "grad_norm": 0.22787834703922272,
      "learning_rate": 2.30625e-06,
      "loss": 0.0015,
      "step": 228930
    },
    {
      "epoch": 7.631333333333333,
      "grad_norm": 0.03231494128704071,
      "learning_rate": 2.3041666666666665e-06,
      "loss": 0.0017,
      "step": 228940
    },
    {
      "epoch": 7.631666666666667,
      "grad_norm": 0.031490590423345566,
      "learning_rate": 2.3020833333333337e-06,
      "loss": 0.0014,
      "step": 228950
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.22782884538173676,
      "learning_rate": 2.3e-06,
      "loss": 0.0017,
      "step": 228960
    },
    {
      "epoch": 7.632333333333333,
      "grad_norm": 0.231183722615242,
      "learning_rate": 2.2979166666666668e-06,
      "loss": 0.0013,
      "step": 228970
    },
    {
      "epoch": 7.632666666666667,
      "grad_norm": 0.08647079765796661,
      "learning_rate": 2.2958333333333335e-06,
      "loss": 0.0017,
      "step": 228980
    },
    {
      "epoch": 7.633,
      "grad_norm": 0.1710890531539917,
      "learning_rate": 2.2937500000000003e-06,
      "loss": 0.0012,
      "step": 228990
    },
    {
      "epoch": 7.633333333333333,
      "grad_norm": 0.05824820697307587,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 0.0011,
      "step": 229000
    },
    {
      "epoch": 7.633666666666667,
      "grad_norm": 0.20665496587753296,
      "learning_rate": 2.2895833333333334e-06,
      "loss": 0.0021,
      "step": 229010
    },
    {
      "epoch": 7.634,
      "grad_norm": 0.06023399159312248,
      "learning_rate": 2.2875e-06,
      "loss": 0.0015,
      "step": 229020
    },
    {
      "epoch": 7.634333333333333,
      "grad_norm": 0.3990413248538971,
      "learning_rate": 2.2854166666666665e-06,
      "loss": 0.0027,
      "step": 229030
    },
    {
      "epoch": 7.634666666666667,
      "grad_norm": 0.03376372158527374,
      "learning_rate": 2.2833333333333336e-06,
      "loss": 0.0019,
      "step": 229040
    },
    {
      "epoch": 7.635,
      "grad_norm": 0.28529122471809387,
      "learning_rate": 2.28125e-06,
      "loss": 0.0017,
      "step": 229050
    },
    {
      "epoch": 7.6353333333333335,
      "grad_norm": 0.11451045423746109,
      "learning_rate": 2.2791666666666667e-06,
      "loss": 0.0015,
      "step": 229060
    },
    {
      "epoch": 7.635666666666666,
      "grad_norm": 0.2574748694896698,
      "learning_rate": 2.2770833333333335e-06,
      "loss": 0.0017,
      "step": 229070
    },
    {
      "epoch": 7.636,
      "grad_norm": 0.11496256291866302,
      "learning_rate": 2.2750000000000002e-06,
      "loss": 0.0014,
      "step": 229080
    },
    {
      "epoch": 7.636333333333333,
      "grad_norm": 0.24653398990631104,
      "learning_rate": 2.272916666666667e-06,
      "loss": 0.0017,
      "step": 229090
    },
    {
      "epoch": 7.636666666666667,
      "grad_norm": 0.17113341391086578,
      "learning_rate": 2.2708333333333333e-06,
      "loss": 0.0017,
      "step": 229100
    },
    {
      "epoch": 7.6370000000000005,
      "grad_norm": 0.05919879674911499,
      "learning_rate": 2.26875e-06,
      "loss": 0.0015,
      "step": 229110
    },
    {
      "epoch": 7.637333333333333,
      "grad_norm": 0.17133080959320068,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0017,
      "step": 229120
    },
    {
      "epoch": 7.637666666666667,
      "grad_norm": 0.06069424748420715,
      "learning_rate": 2.2645833333333336e-06,
      "loss": 0.0019,
      "step": 229130
    },
    {
      "epoch": 7.638,
      "grad_norm": 0.25491243600845337,
      "learning_rate": 2.2625e-06,
      "loss": 0.0013,
      "step": 229140
    },
    {
      "epoch": 7.638333333333334,
      "grad_norm": 0.11436328291893005,
      "learning_rate": 2.260416666666667e-06,
      "loss": 0.0015,
      "step": 229150
    },
    {
      "epoch": 7.6386666666666665,
      "grad_norm": 0.08713769167661667,
      "learning_rate": 2.2583333333333335e-06,
      "loss": 0.0018,
      "step": 229160
    },
    {
      "epoch": 7.639,
      "grad_norm": 0.17154602706432343,
      "learning_rate": 2.25625e-06,
      "loss": 0.0018,
      "step": 229170
    },
    {
      "epoch": 7.639333333333333,
      "grad_norm": 0.38665008544921875,
      "learning_rate": 2.254166666666667e-06,
      "loss": 0.0022,
      "step": 229180
    },
    {
      "epoch": 7.639666666666667,
      "grad_norm": 0.029436325654387474,
      "learning_rate": 2.2520833333333333e-06,
      "loss": 0.0018,
      "step": 229190
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.05767016112804413,
      "learning_rate": 2.25e-06,
      "loss": 0.0017,
      "step": 229200
    },
    {
      "epoch": 7.640333333333333,
      "grad_norm": 0.05875411629676819,
      "learning_rate": 2.247916666666667e-06,
      "loss": 0.0012,
      "step": 229210
    },
    {
      "epoch": 7.640666666666666,
      "grad_norm": 0.17219319939613342,
      "learning_rate": 2.2458333333333336e-06,
      "loss": 0.0013,
      "step": 229220
    },
    {
      "epoch": 7.641,
      "grad_norm": 0.17156577110290527,
      "learning_rate": 2.24375e-06,
      "loss": 0.0018,
      "step": 229230
    },
    {
      "epoch": 7.641333333333334,
      "grad_norm": 0.19973637163639069,
      "learning_rate": 2.241666666666667e-06,
      "loss": 0.0018,
      "step": 229240
    },
    {
      "epoch": 7.641666666666667,
      "grad_norm": 0.17160296440124512,
      "learning_rate": 2.2395833333333334e-06,
      "loss": 0.0011,
      "step": 229250
    },
    {
      "epoch": 7.642,
      "grad_norm": 0.30222025513648987,
      "learning_rate": 2.2375e-06,
      "loss": 0.0013,
      "step": 229260
    },
    {
      "epoch": 7.642333333333333,
      "grad_norm": 0.11532649397850037,
      "learning_rate": 2.235416666666667e-06,
      "loss": 0.0015,
      "step": 229270
    },
    {
      "epoch": 7.642666666666667,
      "grad_norm": 0.28571510314941406,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0015,
      "step": 229280
    },
    {
      "epoch": 7.643,
      "grad_norm": 0.05870315060019493,
      "learning_rate": 2.23125e-06,
      "loss": 0.0017,
      "step": 229290
    },
    {
      "epoch": 7.6433333333333335,
      "grad_norm": 0.14272578060626984,
      "learning_rate": 2.2291666666666668e-06,
      "loss": 0.0021,
      "step": 229300
    },
    {
      "epoch": 7.643666666666666,
      "grad_norm": 0.0590660497546196,
      "learning_rate": 2.2270833333333335e-06,
      "loss": 0.0018,
      "step": 229310
    },
    {
      "epoch": 7.644,
      "grad_norm": 0.0859014242887497,
      "learning_rate": 2.225e-06,
      "loss": 0.0015,
      "step": 229320
    },
    {
      "epoch": 7.644333333333333,
      "grad_norm": 0.14242666959762573,
      "learning_rate": 2.222916666666667e-06,
      "loss": 0.0011,
      "step": 229330
    },
    {
      "epoch": 7.644666666666667,
      "grad_norm": 0.06208200380206108,
      "learning_rate": 2.2208333333333334e-06,
      "loss": 0.0017,
      "step": 229340
    },
    {
      "epoch": 7.645,
      "grad_norm": 0.017777180299162865,
      "learning_rate": 2.21875e-06,
      "loss": 0.0013,
      "step": 229350
    },
    {
      "epoch": 7.645333333333333,
      "grad_norm": 0.11439800262451172,
      "learning_rate": 2.216666666666667e-06,
      "loss": 0.0014,
      "step": 229360
    },
    {
      "epoch": 7.645666666666667,
      "grad_norm": 0.058457374572753906,
      "learning_rate": 2.2145833333333332e-06,
      "loss": 0.0014,
      "step": 229370
    },
    {
      "epoch": 7.646,
      "grad_norm": 0.1996166706085205,
      "learning_rate": 2.2125e-06,
      "loss": 0.0021,
      "step": 229380
    },
    {
      "epoch": 7.646333333333334,
      "grad_norm": 0.34262821078300476,
      "learning_rate": 2.2104166666666667e-06,
      "loss": 0.0027,
      "step": 229390
    },
    {
      "epoch": 7.6466666666666665,
      "grad_norm": 0.199811190366745,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 0.0016,
      "step": 229400
    },
    {
      "epoch": 7.647,
      "grad_norm": 0.03138171508908272,
      "learning_rate": 2.20625e-06,
      "loss": 0.0013,
      "step": 229410
    },
    {
      "epoch": 7.647333333333333,
      "grad_norm": 0.08600021153688431,
      "learning_rate": 2.204166666666667e-06,
      "loss": 0.0018,
      "step": 229420
    },
    {
      "epoch": 7.647666666666667,
      "grad_norm": 0.03090466745197773,
      "learning_rate": 2.2020833333333334e-06,
      "loss": 0.0026,
      "step": 229430
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.22839808464050293,
      "learning_rate": 2.2e-06,
      "loss": 0.0014,
      "step": 229440
    },
    {
      "epoch": 7.648333333333333,
      "grad_norm": 0.08580777794122696,
      "learning_rate": 2.197916666666667e-06,
      "loss": 0.0012,
      "step": 229450
    },
    {
      "epoch": 7.648666666666666,
      "grad_norm": 0.05969701707363129,
      "learning_rate": 2.1958333333333336e-06,
      "loss": 0.0013,
      "step": 229460
    },
    {
      "epoch": 7.649,
      "grad_norm": 0.14394834637641907,
      "learning_rate": 2.19375e-06,
      "loss": 0.0021,
      "step": 229470
    },
    {
      "epoch": 7.649333333333333,
      "grad_norm": 0.34208711981773376,
      "learning_rate": 2.1916666666666667e-06,
      "loss": 0.0018,
      "step": 229480
    },
    {
      "epoch": 7.649666666666667,
      "grad_norm": 0.030541740357875824,
      "learning_rate": 2.1895833333333335e-06,
      "loss": 0.0014,
      "step": 229490
    },
    {
      "epoch": 7.65,
      "grad_norm": 0.008585402742028236,
      "learning_rate": 2.1875e-06,
      "loss": 0.0022,
      "step": 229500
    },
    {
      "epoch": 7.650333333333333,
      "grad_norm": 0.2570330500602722,
      "learning_rate": 2.185416666666667e-06,
      "loss": 0.0014,
      "step": 229510
    },
    {
      "epoch": 7.650666666666667,
      "grad_norm": 0.05755867809057236,
      "learning_rate": 2.1833333333333333e-06,
      "loss": 0.0018,
      "step": 229520
    },
    {
      "epoch": 7.651,
      "grad_norm": 0.14254730939865112,
      "learning_rate": 2.18125e-06,
      "loss": 0.0017,
      "step": 229530
    },
    {
      "epoch": 7.6513333333333335,
      "grad_norm": 0.31494396924972534,
      "learning_rate": 2.179166666666667e-06,
      "loss": 0.0022,
      "step": 229540
    },
    {
      "epoch": 7.651666666666666,
      "grad_norm": 0.25852978229522705,
      "learning_rate": 2.1770833333333336e-06,
      "loss": 0.0019,
      "step": 229550
    },
    {
      "epoch": 7.652,
      "grad_norm": 0.08659972995519638,
      "learning_rate": 2.175e-06,
      "loss": 0.0028,
      "step": 229560
    },
    {
      "epoch": 7.652333333333333,
      "grad_norm": 0.08627769351005554,
      "learning_rate": 2.1729166666666667e-06,
      "loss": 0.0014,
      "step": 229570
    },
    {
      "epoch": 7.652666666666667,
      "grad_norm": 0.2747829854488373,
      "learning_rate": 2.1708333333333334e-06,
      "loss": 0.0016,
      "step": 229580
    },
    {
      "epoch": 7.6530000000000005,
      "grad_norm": 0.057084839791059494,
      "learning_rate": 2.1687499999999998e-06,
      "loss": 0.0016,
      "step": 229590
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 0.22834981977939606,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.0012,
      "step": 229600
    },
    {
      "epoch": 7.653666666666666,
      "grad_norm": 0.15832200646400452,
      "learning_rate": 2.1645833333333333e-06,
      "loss": 0.0014,
      "step": 229610
    },
    {
      "epoch": 7.654,
      "grad_norm": 0.1708468794822693,
      "learning_rate": 2.1625e-06,
      "loss": 0.0016,
      "step": 229620
    },
    {
      "epoch": 7.654333333333334,
      "grad_norm": 0.11509250104427338,
      "learning_rate": 2.160416666666667e-06,
      "loss": 0.0018,
      "step": 229630
    },
    {
      "epoch": 7.6546666666666665,
      "grad_norm": 0.31459715962409973,
      "learning_rate": 2.1583333333333336e-06,
      "loss": 0.0014,
      "step": 229640
    },
    {
      "epoch": 7.655,
      "grad_norm": 0.171524316072464,
      "learning_rate": 2.15625e-06,
      "loss": 0.0012,
      "step": 229650
    },
    {
      "epoch": 7.655333333333333,
      "grad_norm": 0.2983315885066986,
      "learning_rate": 2.1541666666666667e-06,
      "loss": 0.0025,
      "step": 229660
    },
    {
      "epoch": 7.655666666666667,
      "grad_norm": 0.058879390358924866,
      "learning_rate": 2.1520833333333334e-06,
      "loss": 0.0012,
      "step": 229670
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.08713190257549286,
      "learning_rate": 2.1499999999999997e-06,
      "loss": 0.0016,
      "step": 229680
    },
    {
      "epoch": 7.656333333333333,
      "grad_norm": 0.26735392212867737,
      "learning_rate": 2.147916666666667e-06,
      "loss": 0.0018,
      "step": 229690
    },
    {
      "epoch": 7.656666666666666,
      "grad_norm": 0.22827449440956116,
      "learning_rate": 2.1458333333333333e-06,
      "loss": 0.0013,
      "step": 229700
    },
    {
      "epoch": 7.657,
      "grad_norm": 0.11419559270143509,
      "learning_rate": 2.1437500000000004e-06,
      "loss": 0.0017,
      "step": 229710
    },
    {
      "epoch": 7.657333333333334,
      "grad_norm": 0.42260491847991943,
      "learning_rate": 2.1416666666666668e-06,
      "loss": 0.0014,
      "step": 229720
    },
    {
      "epoch": 7.657666666666667,
      "grad_norm": 0.05792088434100151,
      "learning_rate": 2.1395833333333335e-06,
      "loss": 0.002,
      "step": 229730
    },
    {
      "epoch": 7.658,
      "grad_norm": 0.11468178033828735,
      "learning_rate": 2.1375000000000003e-06,
      "loss": 0.0024,
      "step": 229740
    },
    {
      "epoch": 7.658333333333333,
      "grad_norm": 0.009741110727190971,
      "learning_rate": 2.1354166666666666e-06,
      "loss": 0.0017,
      "step": 229750
    },
    {
      "epoch": 7.658666666666667,
      "grad_norm": 0.030249623581767082,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0013,
      "step": 229760
    },
    {
      "epoch": 7.659,
      "grad_norm": 0.2627590298652649,
      "learning_rate": 2.13125e-06,
      "loss": 0.0012,
      "step": 229770
    },
    {
      "epoch": 7.6593333333333335,
      "grad_norm": 0.1051129698753357,
      "learning_rate": 2.129166666666667e-06,
      "loss": 0.0015,
      "step": 229780
    },
    {
      "epoch": 7.659666666666666,
      "grad_norm": 0.029463913291692734,
      "learning_rate": 2.1270833333333332e-06,
      "loss": 0.0016,
      "step": 229790
    },
    {
      "epoch": 7.66,
      "grad_norm": 0.114487424492836,
      "learning_rate": 2.1250000000000004e-06,
      "loss": 0.0017,
      "step": 229800
    },
    {
      "epoch": 7.660333333333333,
      "grad_norm": 0.14246822893619537,
      "learning_rate": 2.1229166666666667e-06,
      "loss": 0.0013,
      "step": 229810
    },
    {
      "epoch": 7.660666666666667,
      "grad_norm": 0.3994840383529663,
      "learning_rate": 2.1208333333333335e-06,
      "loss": 0.0025,
      "step": 229820
    },
    {
      "epoch": 7.661,
      "grad_norm": 0.017274819314479828,
      "learning_rate": 2.1187500000000003e-06,
      "loss": 0.0019,
      "step": 229830
    },
    {
      "epoch": 7.661333333333333,
      "grad_norm": 0.029827170073986053,
      "learning_rate": 2.1166666666666666e-06,
      "loss": 0.0017,
      "step": 229840
    },
    {
      "epoch": 7.661666666666667,
      "grad_norm": 0.03377113863825798,
      "learning_rate": 2.1145833333333333e-06,
      "loss": 0.0013,
      "step": 229850
    },
    {
      "epoch": 7.662,
      "grad_norm": 0.03286350145936012,
      "learning_rate": 2.1125e-06,
      "loss": 0.002,
      "step": 229860
    },
    {
      "epoch": 7.662333333333334,
      "grad_norm": 0.08592597395181656,
      "learning_rate": 2.110416666666667e-06,
      "loss": 0.0015,
      "step": 229870
    },
    {
      "epoch": 7.6626666666666665,
      "grad_norm": 0.2856931984424591,
      "learning_rate": 2.108333333333333e-06,
      "loss": 0.0015,
      "step": 229880
    },
    {
      "epoch": 7.663,
      "grad_norm": 0.356166809797287,
      "learning_rate": 2.1062500000000004e-06,
      "loss": 0.0015,
      "step": 229890
    },
    {
      "epoch": 7.663333333333333,
      "grad_norm": 0.23999540507793427,
      "learning_rate": 2.1041666666666667e-06,
      "loss": 0.0015,
      "step": 229900
    },
    {
      "epoch": 7.663666666666667,
      "grad_norm": 0.08690734207630157,
      "learning_rate": 2.1020833333333335e-06,
      "loss": 0.0014,
      "step": 229910
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.25683680176734924,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.0016,
      "step": 229920
    },
    {
      "epoch": 7.664333333333333,
      "grad_norm": 0.11468325555324554,
      "learning_rate": 2.097916666666667e-06,
      "loss": 0.0015,
      "step": 229930
    },
    {
      "epoch": 7.664666666666666,
      "grad_norm": 0.22808818519115448,
      "learning_rate": 2.0958333333333333e-06,
      "loss": 0.0014,
      "step": 229940
    },
    {
      "epoch": 7.665,
      "grad_norm": 0.08912377804517746,
      "learning_rate": 2.09375e-06,
      "loss": 0.0014,
      "step": 229950
    },
    {
      "epoch": 7.665333333333333,
      "grad_norm": 0.17453064024448395,
      "learning_rate": 2.091666666666667e-06,
      "loss": 0.0022,
      "step": 229960
    },
    {
      "epoch": 7.665666666666667,
      "grad_norm": 0.19991327822208405,
      "learning_rate": 2.089583333333333e-06,
      "loss": 0.0014,
      "step": 229970
    },
    {
      "epoch": 7.666,
      "grad_norm": 0.05466544255614281,
      "learning_rate": 2.0875000000000003e-06,
      "loss": 0.0019,
      "step": 229980
    },
    {
      "epoch": 7.666333333333333,
      "grad_norm": 0.08670353144407272,
      "learning_rate": 2.0854166666666667e-06,
      "loss": 0.0018,
      "step": 229990
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.1431649625301361,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.002,
      "step": 230000
    },
    {
      "epoch": 7.667,
      "grad_norm": 0.3755747675895691,
      "learning_rate": 2.08125e-06,
      "loss": 0.0022,
      "step": 230010
    },
    {
      "epoch": 7.667333333333334,
      "grad_norm": 0.06762315332889557,
      "learning_rate": 2.079166666666667e-06,
      "loss": 0.0021,
      "step": 230020
    },
    {
      "epoch": 7.667666666666666,
      "grad_norm": 0.49411341547966003,
      "learning_rate": 2.0770833333333333e-06,
      "loss": 0.0021,
      "step": 230030
    },
    {
      "epoch": 7.668,
      "grad_norm": 0.11593082547187805,
      "learning_rate": 2.075e-06,
      "loss": 0.0014,
      "step": 230040
    },
    {
      "epoch": 7.668333333333333,
      "grad_norm": 0.05780677869915962,
      "learning_rate": 2.072916666666667e-06,
      "loss": 0.0017,
      "step": 230050
    },
    {
      "epoch": 7.668666666666667,
      "grad_norm": 0.39508679509162903,
      "learning_rate": 2.070833333333333e-06,
      "loss": 0.002,
      "step": 230060
    },
    {
      "epoch": 7.6690000000000005,
      "grad_norm": 0.37057965993881226,
      "learning_rate": 2.0687500000000003e-06,
      "loss": 0.0017,
      "step": 230070
    },
    {
      "epoch": 7.669333333333333,
      "grad_norm": 0.17104020714759827,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0016,
      "step": 230080
    },
    {
      "epoch": 7.669666666666666,
      "grad_norm": 0.05797450244426727,
      "learning_rate": 2.0645833333333334e-06,
      "loss": 0.0017,
      "step": 230090
    },
    {
      "epoch": 7.67,
      "grad_norm": 0.058928243815898895,
      "learning_rate": 2.0625e-06,
      "loss": 0.0018,
      "step": 230100
    },
    {
      "epoch": 7.670333333333334,
      "grad_norm": 0.14340829849243164,
      "learning_rate": 2.060416666666667e-06,
      "loss": 0.002,
      "step": 230110
    },
    {
      "epoch": 7.6706666666666665,
      "grad_norm": 0.08593864738941193,
      "learning_rate": 2.0583333333333332e-06,
      "loss": 0.0024,
      "step": 230120
    },
    {
      "epoch": 7.671,
      "grad_norm": 0.058482881635427475,
      "learning_rate": 2.05625e-06,
      "loss": 0.0016,
      "step": 230130
    },
    {
      "epoch": 7.671333333333333,
      "grad_norm": 0.114765465259552,
      "learning_rate": 2.0541666666666668e-06,
      "loss": 0.0014,
      "step": 230140
    },
    {
      "epoch": 7.671666666666667,
      "grad_norm": 0.14304061233997345,
      "learning_rate": 2.052083333333333e-06,
      "loss": 0.0019,
      "step": 230150
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.1154225543141365,
      "learning_rate": 2.0500000000000003e-06,
      "loss": 0.0018,
      "step": 230160
    },
    {
      "epoch": 7.6723333333333334,
      "grad_norm": 0.14256203174591064,
      "learning_rate": 2.0479166666666666e-06,
      "loss": 0.0019,
      "step": 230170
    },
    {
      "epoch": 7.672666666666666,
      "grad_norm": 0.05958600714802742,
      "learning_rate": 2.0458333333333334e-06,
      "loss": 0.0013,
      "step": 230180
    },
    {
      "epoch": 7.673,
      "grad_norm": 0.3144168555736542,
      "learning_rate": 2.04375e-06,
      "loss": 0.0013,
      "step": 230190
    },
    {
      "epoch": 7.673333333333334,
      "grad_norm": 0.6967464089393616,
      "learning_rate": 2.041666666666667e-06,
      "loss": 0.0021,
      "step": 230200
    },
    {
      "epoch": 7.673666666666667,
      "grad_norm": 0.0665896013379097,
      "learning_rate": 2.0395833333333332e-06,
      "loss": 0.0023,
      "step": 230210
    },
    {
      "epoch": 7.674,
      "grad_norm": 0.1964644342660904,
      "learning_rate": 2.0375e-06,
      "loss": 0.002,
      "step": 230220
    },
    {
      "epoch": 7.674333333333333,
      "grad_norm": 0.07789602130651474,
      "learning_rate": 2.0354166666666667e-06,
      "loss": 0.0017,
      "step": 230230
    },
    {
      "epoch": 7.674666666666667,
      "grad_norm": 0.22746407985687256,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.0018,
      "step": 230240
    },
    {
      "epoch": 7.675,
      "grad_norm": 0.030120279639959335,
      "learning_rate": 2.0312500000000002e-06,
      "loss": 0.0016,
      "step": 230250
    },
    {
      "epoch": 7.675333333333334,
      "grad_norm": 0.1229056641459465,
      "learning_rate": 2.0291666666666666e-06,
      "loss": 0.0015,
      "step": 230260
    },
    {
      "epoch": 7.675666666666666,
      "grad_norm": 0.22838667035102844,
      "learning_rate": 2.0270833333333333e-06,
      "loss": 0.0021,
      "step": 230270
    },
    {
      "epoch": 7.676,
      "grad_norm": 0.05756859853863716,
      "learning_rate": 2.025e-06,
      "loss": 0.0018,
      "step": 230280
    },
    {
      "epoch": 7.676333333333333,
      "grad_norm": 0.028189530596137047,
      "learning_rate": 2.022916666666667e-06,
      "loss": 0.0012,
      "step": 230290
    },
    {
      "epoch": 7.676666666666667,
      "grad_norm": 0.08924481272697449,
      "learning_rate": 2.020833333333333e-06,
      "loss": 0.0013,
      "step": 230300
    },
    {
      "epoch": 7.677,
      "grad_norm": 0.029567912220954895,
      "learning_rate": 2.0187500000000004e-06,
      "loss": 0.0012,
      "step": 230310
    },
    {
      "epoch": 7.677333333333333,
      "grad_norm": 0.2713705003261566,
      "learning_rate": 2.0166666666666667e-06,
      "loss": 0.0018,
      "step": 230320
    },
    {
      "epoch": 7.677666666666667,
      "grad_norm": 0.14362502098083496,
      "learning_rate": 2.0145833333333335e-06,
      "loss": 0.0022,
      "step": 230330
    },
    {
      "epoch": 7.678,
      "grad_norm": 0.3707164227962494,
      "learning_rate": 2.0125000000000002e-06,
      "loss": 0.0017,
      "step": 230340
    },
    {
      "epoch": 7.678333333333334,
      "grad_norm": 0.14286936819553375,
      "learning_rate": 2.0104166666666665e-06,
      "loss": 0.0017,
      "step": 230350
    },
    {
      "epoch": 7.6786666666666665,
      "grad_norm": 0.32731765508651733,
      "learning_rate": 2.0083333333333337e-06,
      "loss": 0.0012,
      "step": 230360
    },
    {
      "epoch": 7.679,
      "grad_norm": 0.11462381482124329,
      "learning_rate": 2.00625e-06,
      "loss": 0.0016,
      "step": 230370
    },
    {
      "epoch": 7.679333333333333,
      "grad_norm": 0.19985248148441315,
      "learning_rate": 2.004166666666667e-06,
      "loss": 0.0022,
      "step": 230380
    },
    {
      "epoch": 7.679666666666667,
      "grad_norm": 0.058197543025016785,
      "learning_rate": 2.0020833333333336e-06,
      "loss": 0.0015,
      "step": 230390
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.03405401110649109,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0014,
      "step": 230400
    },
    {
      "epoch": 7.6803333333333335,
      "grad_norm": 0.4281289577484131,
      "learning_rate": 1.9979166666666667e-06,
      "loss": 0.0017,
      "step": 230410
    },
    {
      "epoch": 7.680666666666666,
      "grad_norm": 0.20052111148834229,
      "learning_rate": 1.9958333333333334e-06,
      "loss": 0.0014,
      "step": 230420
    },
    {
      "epoch": 7.681,
      "grad_norm": 0.03005371056497097,
      "learning_rate": 1.99375e-06,
      "loss": 0.0016,
      "step": 230430
    },
    {
      "epoch": 7.681333333333333,
      "grad_norm": 0.14649751782417297,
      "learning_rate": 1.9916666666666665e-06,
      "loss": 0.0015,
      "step": 230440
    },
    {
      "epoch": 7.681666666666667,
      "grad_norm": 0.05720355361700058,
      "learning_rate": 1.9895833333333337e-06,
      "loss": 0.0016,
      "step": 230450
    },
    {
      "epoch": 7.682,
      "grad_norm": 0.24734993278980255,
      "learning_rate": 1.9875e-06,
      "loss": 0.0015,
      "step": 230460
    },
    {
      "epoch": 7.682333333333333,
      "grad_norm": 0.19972869753837585,
      "learning_rate": 1.9854166666666668e-06,
      "loss": 0.0014,
      "step": 230470
    },
    {
      "epoch": 7.682666666666667,
      "grad_norm": 0.1150427833199501,
      "learning_rate": 1.9833333333333335e-06,
      "loss": 0.0021,
      "step": 230480
    },
    {
      "epoch": 7.683,
      "grad_norm": 0.17113278806209564,
      "learning_rate": 1.9812500000000003e-06,
      "loss": 0.0013,
      "step": 230490
    },
    {
      "epoch": 7.683333333333334,
      "grad_norm": 0.25646522641181946,
      "learning_rate": 1.9791666666666666e-06,
      "loss": 0.0019,
      "step": 230500
    },
    {
      "epoch": 7.683666666666666,
      "grad_norm": 0.2852362394332886,
      "learning_rate": 1.9770833333333334e-06,
      "loss": 0.0025,
      "step": 230510
    },
    {
      "epoch": 7.684,
      "grad_norm": 0.39994004368782043,
      "learning_rate": 1.975e-06,
      "loss": 0.0017,
      "step": 230520
    },
    {
      "epoch": 7.684333333333333,
      "grad_norm": 0.05809364095330238,
      "learning_rate": 1.9729166666666665e-06,
      "loss": 0.0018,
      "step": 230530
    },
    {
      "epoch": 7.684666666666667,
      "grad_norm": 0.2153901755809784,
      "learning_rate": 1.9708333333333337e-06,
      "loss": 0.0017,
      "step": 230540
    },
    {
      "epoch": 7.6850000000000005,
      "grad_norm": 0.11511872708797455,
      "learning_rate": 1.96875e-06,
      "loss": 0.0012,
      "step": 230550
    },
    {
      "epoch": 7.685333333333333,
      "grad_norm": 0.22794629633426666,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.0016,
      "step": 230560
    },
    {
      "epoch": 7.685666666666666,
      "grad_norm": 0.17208682000637054,
      "learning_rate": 1.9645833333333335e-06,
      "loss": 0.0015,
      "step": 230570
    },
    {
      "epoch": 7.686,
      "grad_norm": 0.2566796541213989,
      "learning_rate": 1.9625000000000003e-06,
      "loss": 0.0014,
      "step": 230580
    },
    {
      "epoch": 7.686333333333334,
      "grad_norm": 0.19971966743469238,
      "learning_rate": 1.9604166666666666e-06,
      "loss": 0.0014,
      "step": 230590
    },
    {
      "epoch": 7.6866666666666665,
      "grad_norm": 0.46033310890197754,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 0.0013,
      "step": 230600
    },
    {
      "epoch": 7.687,
      "grad_norm": 0.2278391420841217,
      "learning_rate": 1.95625e-06,
      "loss": 0.0015,
      "step": 230610
    },
    {
      "epoch": 7.687333333333333,
      "grad_norm": 0.47089508175849915,
      "learning_rate": 1.9541666666666665e-06,
      "loss": 0.002,
      "step": 230620
    },
    {
      "epoch": 7.687666666666667,
      "grad_norm": 0.08596701920032501,
      "learning_rate": 1.9520833333333336e-06,
      "loss": 0.0014,
      "step": 230630
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.029731638729572296,
      "learning_rate": 1.95e-06,
      "loss": 0.0016,
      "step": 230640
    },
    {
      "epoch": 7.6883333333333335,
      "grad_norm": 0.01276746392250061,
      "learning_rate": 1.9479166666666667e-06,
      "loss": 0.002,
      "step": 230650
    },
    {
      "epoch": 7.688666666666666,
      "grad_norm": 0.1714591532945633,
      "learning_rate": 1.9458333333333335e-06,
      "loss": 0.0018,
      "step": 230660
    },
    {
      "epoch": 7.689,
      "grad_norm": 0.37125590443611145,
      "learning_rate": 1.9437500000000002e-06,
      "loss": 0.003,
      "step": 230670
    },
    {
      "epoch": 7.689333333333334,
      "grad_norm": 0.08645504713058472,
      "learning_rate": 1.9416666666666666e-06,
      "loss": 0.0024,
      "step": 230680
    },
    {
      "epoch": 7.689666666666667,
      "grad_norm": 0.11455772072076797,
      "learning_rate": 1.9395833333333333e-06,
      "loss": 0.0015,
      "step": 230690
    },
    {
      "epoch": 7.6899999999999995,
      "grad_norm": 0.2897767722606659,
      "learning_rate": 1.9375e-06,
      "loss": 0.0022,
      "step": 230700
    },
    {
      "epoch": 7.690333333333333,
      "grad_norm": 0.029535293579101562,
      "learning_rate": 1.9354166666666664e-06,
      "loss": 0.0014,
      "step": 230710
    },
    {
      "epoch": 7.690666666666667,
      "grad_norm": 0.1999187469482422,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0017,
      "step": 230720
    },
    {
      "epoch": 7.691,
      "grad_norm": 0.39283767342567444,
      "learning_rate": 1.93125e-06,
      "loss": 0.0019,
      "step": 230730
    },
    {
      "epoch": 7.691333333333334,
      "grad_norm": 0.5420860648155212,
      "learning_rate": 1.9291666666666667e-06,
      "loss": 0.0014,
      "step": 230740
    },
    {
      "epoch": 7.691666666666666,
      "grad_norm": 0.15557090938091278,
      "learning_rate": 1.9270833333333334e-06,
      "loss": 0.002,
      "step": 230750
    },
    {
      "epoch": 7.692,
      "grad_norm": 0.11412264406681061,
      "learning_rate": 1.925e-06,
      "loss": 0.0013,
      "step": 230760
    },
    {
      "epoch": 7.692333333333333,
      "grad_norm": 0.13410566747188568,
      "learning_rate": 1.9229166666666665e-06,
      "loss": 0.0015,
      "step": 230770
    },
    {
      "epoch": 7.692666666666667,
      "grad_norm": 0.1710425466299057,
      "learning_rate": 1.9208333333333337e-06,
      "loss": 0.0015,
      "step": 230780
    },
    {
      "epoch": 7.693,
      "grad_norm": 0.09497670084238052,
      "learning_rate": 1.91875e-06,
      "loss": 0.0013,
      "step": 230790
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.3709622621536255,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.0014,
      "step": 230800
    },
    {
      "epoch": 7.693666666666667,
      "grad_norm": 0.20029300451278687,
      "learning_rate": 1.9145833333333336e-06,
      "loss": 0.002,
      "step": 230810
    },
    {
      "epoch": 7.694,
      "grad_norm": 0.14276458323001862,
      "learning_rate": 1.9125e-06,
      "loss": 0.0015,
      "step": 230820
    },
    {
      "epoch": 7.694333333333334,
      "grad_norm": 0.41745832562446594,
      "learning_rate": 1.9104166666666667e-06,
      "loss": 0.0019,
      "step": 230830
    },
    {
      "epoch": 7.6946666666666665,
      "grad_norm": 0.19435951113700867,
      "learning_rate": 1.9083333333333334e-06,
      "loss": 0.0013,
      "step": 230840
    },
    {
      "epoch": 7.695,
      "grad_norm": 0.01528757531195879,
      "learning_rate": 1.90625e-06,
      "loss": 0.0015,
      "step": 230850
    },
    {
      "epoch": 7.695333333333333,
      "grad_norm": 0.0870911180973053,
      "learning_rate": 1.9041666666666665e-06,
      "loss": 0.0012,
      "step": 230860
    },
    {
      "epoch": 7.695666666666667,
      "grad_norm": 0.19934336841106415,
      "learning_rate": 1.9020833333333335e-06,
      "loss": 0.0024,
      "step": 230870
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.15046018362045288,
      "learning_rate": 1.9e-06,
      "loss": 0.0016,
      "step": 230880
    },
    {
      "epoch": 7.6963333333333335,
      "grad_norm": 0.03218347206711769,
      "learning_rate": 1.8979166666666666e-06,
      "loss": 0.0018,
      "step": 230890
    },
    {
      "epoch": 7.696666666666666,
      "grad_norm": 0.28564581274986267,
      "learning_rate": 1.8958333333333335e-06,
      "loss": 0.0012,
      "step": 230900
    },
    {
      "epoch": 7.697,
      "grad_norm": 0.22908316552639008,
      "learning_rate": 1.89375e-06,
      "loss": 0.0016,
      "step": 230910
    },
    {
      "epoch": 7.697333333333333,
      "grad_norm": 0.03026799112558365,
      "learning_rate": 1.8916666666666666e-06,
      "loss": 0.0014,
      "step": 230920
    },
    {
      "epoch": 7.697666666666667,
      "grad_norm": 0.057347703725099564,
      "learning_rate": 1.8895833333333334e-06,
      "loss": 0.0017,
      "step": 230930
    },
    {
      "epoch": 7.698,
      "grad_norm": 0.22880011796951294,
      "learning_rate": 1.8875e-06,
      "loss": 0.0019,
      "step": 230940
    },
    {
      "epoch": 7.698333333333333,
      "grad_norm": 0.030563998967409134,
      "learning_rate": 1.885416666666667e-06,
      "loss": 0.0015,
      "step": 230950
    },
    {
      "epoch": 7.698666666666667,
      "grad_norm": 0.20010966062545776,
      "learning_rate": 1.8833333333333334e-06,
      "loss": 0.0016,
      "step": 230960
    },
    {
      "epoch": 7.699,
      "grad_norm": 0.3141798973083496,
      "learning_rate": 1.88125e-06,
      "loss": 0.0017,
      "step": 230970
    },
    {
      "epoch": 7.699333333333334,
      "grad_norm": 0.2502552270889282,
      "learning_rate": 1.879166666666667e-06,
      "loss": 0.0014,
      "step": 230980
    },
    {
      "epoch": 7.699666666666666,
      "grad_norm": 0.1686132550239563,
      "learning_rate": 1.8770833333333335e-06,
      "loss": 0.0013,
      "step": 230990
    },
    {
      "epoch": 7.7,
      "grad_norm": 0.08570260554552078,
      "learning_rate": 1.875e-06,
      "loss": 0.0013,
      "step": 231000
    },
    {
      "epoch": 7.700333333333333,
      "grad_norm": 0.012995647266507149,
      "learning_rate": 1.872916666666667e-06,
      "loss": 0.0022,
      "step": 231010
    },
    {
      "epoch": 7.700666666666667,
      "grad_norm": 0.2009626179933548,
      "learning_rate": 1.8708333333333336e-06,
      "loss": 0.0013,
      "step": 231020
    },
    {
      "epoch": 7.701,
      "grad_norm": 0.31632715463638306,
      "learning_rate": 1.8687500000000001e-06,
      "loss": 0.0015,
      "step": 231030
    },
    {
      "epoch": 7.701333333333333,
      "grad_norm": 0.4269964098930359,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0013,
      "step": 231040
    },
    {
      "epoch": 7.701666666666666,
      "grad_norm": 0.28598374128341675,
      "learning_rate": 1.8645833333333334e-06,
      "loss": 0.0014,
      "step": 231050
    },
    {
      "epoch": 7.702,
      "grad_norm": 0.14291061460971832,
      "learning_rate": 1.8625e-06,
      "loss": 0.0015,
      "step": 231060
    },
    {
      "epoch": 7.702333333333334,
      "grad_norm": 0.17130492627620697,
      "learning_rate": 1.860416666666667e-06,
      "loss": 0.0015,
      "step": 231070
    },
    {
      "epoch": 7.7026666666666666,
      "grad_norm": 0.11476084589958191,
      "learning_rate": 1.8583333333333335e-06,
      "loss": 0.0016,
      "step": 231080
    },
    {
      "epoch": 7.703,
      "grad_norm": 0.030215194448828697,
      "learning_rate": 1.85625e-06,
      "loss": 0.0017,
      "step": 231090
    },
    {
      "epoch": 7.703333333333333,
      "grad_norm": 0.11539412289857864,
      "learning_rate": 1.854166666666667e-06,
      "loss": 0.0018,
      "step": 231100
    },
    {
      "epoch": 7.703666666666667,
      "grad_norm": 0.058567509055137634,
      "learning_rate": 1.8520833333333335e-06,
      "loss": 0.0013,
      "step": 231110
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.17739662528038025,
      "learning_rate": 1.85e-06,
      "loss": 0.0024,
      "step": 231120
    },
    {
      "epoch": 7.7043333333333335,
      "grad_norm": 0.14283788204193115,
      "learning_rate": 1.8479166666666668e-06,
      "loss": 0.0011,
      "step": 231130
    },
    {
      "epoch": 7.704666666666666,
      "grad_norm": 0.3993065059185028,
      "learning_rate": 1.8458333333333334e-06,
      "loss": 0.0021,
      "step": 231140
    },
    {
      "epoch": 7.705,
      "grad_norm": 0.17240746319293976,
      "learning_rate": 1.84375e-06,
      "loss": 0.0019,
      "step": 231150
    },
    {
      "epoch": 7.705333333333334,
      "grad_norm": 0.25698524713516235,
      "learning_rate": 1.8416666666666669e-06,
      "loss": 0.0018,
      "step": 231160
    },
    {
      "epoch": 7.705666666666667,
      "grad_norm": 0.2728758454322815,
      "learning_rate": 1.8395833333333334e-06,
      "loss": 0.0015,
      "step": 231170
    },
    {
      "epoch": 7.7059999999999995,
      "grad_norm": 0.1894093155860901,
      "learning_rate": 1.8375e-06,
      "loss": 0.0019,
      "step": 231180
    },
    {
      "epoch": 7.706333333333333,
      "grad_norm": 0.18236321210861206,
      "learning_rate": 1.835416666666667e-06,
      "loss": 0.0016,
      "step": 231190
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.37062200903892517,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0015,
      "step": 231200
    },
    {
      "epoch": 7.707,
      "grad_norm": 0.06116263568401337,
      "learning_rate": 1.83125e-06,
      "loss": 0.0014,
      "step": 231210
    },
    {
      "epoch": 7.707333333333334,
      "grad_norm": 0.5135377645492554,
      "learning_rate": 1.8291666666666668e-06,
      "loss": 0.0013,
      "step": 231220
    },
    {
      "epoch": 7.707666666666666,
      "grad_norm": 0.4170171916484833,
      "learning_rate": 1.8270833333333333e-06,
      "loss": 0.0016,
      "step": 231230
    },
    {
      "epoch": 7.708,
      "grad_norm": 0.14792171120643616,
      "learning_rate": 1.8249999999999999e-06,
      "loss": 0.0018,
      "step": 231240
    },
    {
      "epoch": 7.708333333333333,
      "grad_norm": 0.0895114466547966,
      "learning_rate": 1.8229166666666669e-06,
      "loss": 0.0015,
      "step": 231250
    },
    {
      "epoch": 7.708666666666667,
      "grad_norm": 0.05757815018296242,
      "learning_rate": 1.8208333333333334e-06,
      "loss": 0.0013,
      "step": 231260
    },
    {
      "epoch": 7.709,
      "grad_norm": 0.012671960517764091,
      "learning_rate": 1.81875e-06,
      "loss": 0.0016,
      "step": 231270
    },
    {
      "epoch": 7.709333333333333,
      "grad_norm": 0.28553399443626404,
      "learning_rate": 1.816666666666667e-06,
      "loss": 0.0014,
      "step": 231280
    },
    {
      "epoch": 7.709666666666667,
      "grad_norm": 0.1430489718914032,
      "learning_rate": 1.8145833333333335e-06,
      "loss": 0.0013,
      "step": 231290
    },
    {
      "epoch": 7.71,
      "grad_norm": 0.4490662217140198,
      "learning_rate": 1.8125e-06,
      "loss": 0.0019,
      "step": 231300
    },
    {
      "epoch": 7.710333333333334,
      "grad_norm": 0.35556721687316895,
      "learning_rate": 1.8104166666666668e-06,
      "loss": 0.0014,
      "step": 231310
    },
    {
      "epoch": 7.710666666666667,
      "grad_norm": 0.14408917725086212,
      "learning_rate": 1.8083333333333333e-06,
      "loss": 0.0015,
      "step": 231320
    },
    {
      "epoch": 7.711,
      "grad_norm": 0.14197798073291779,
      "learning_rate": 1.8062499999999999e-06,
      "loss": 0.0017,
      "step": 231330
    },
    {
      "epoch": 7.711333333333333,
      "grad_norm": 0.0585956797003746,
      "learning_rate": 1.8041666666666668e-06,
      "loss": 0.0014,
      "step": 231340
    },
    {
      "epoch": 7.711666666666667,
      "grad_norm": 0.033857714384794235,
      "learning_rate": 1.8020833333333334e-06,
      "loss": 0.0021,
      "step": 231350
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.11409465968608856,
      "learning_rate": 1.8e-06,
      "loss": 0.0016,
      "step": 231360
    },
    {
      "epoch": 7.7123333333333335,
      "grad_norm": 0.029223807156085968,
      "learning_rate": 1.7979166666666669e-06,
      "loss": 0.0012,
      "step": 231370
    },
    {
      "epoch": 7.712666666666666,
      "grad_norm": 0.4389669597148895,
      "learning_rate": 1.7958333333333334e-06,
      "loss": 0.0018,
      "step": 231380
    },
    {
      "epoch": 7.713,
      "grad_norm": 0.1740759164094925,
      "learning_rate": 1.79375e-06,
      "loss": 0.0023,
      "step": 231390
    },
    {
      "epoch": 7.713333333333333,
      "grad_norm": 0.05734878033399582,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 0.0018,
      "step": 231400
    },
    {
      "epoch": 7.713666666666667,
      "grad_norm": 0.08622205257415771,
      "learning_rate": 1.7895833333333335e-06,
      "loss": 0.0027,
      "step": 231410
    },
    {
      "epoch": 7.714,
      "grad_norm": 0.3435709774494171,
      "learning_rate": 1.7875e-06,
      "loss": 0.0021,
      "step": 231420
    },
    {
      "epoch": 7.714333333333333,
      "grad_norm": 0.2846260070800781,
      "learning_rate": 1.7854166666666668e-06,
      "loss": 0.0014,
      "step": 231430
    },
    {
      "epoch": 7.714666666666667,
      "grad_norm": 0.058032698929309845,
      "learning_rate": 1.7833333333333333e-06,
      "loss": 0.0017,
      "step": 231440
    },
    {
      "epoch": 7.715,
      "grad_norm": 0.1722000390291214,
      "learning_rate": 1.7812499999999999e-06,
      "loss": 0.002,
      "step": 231450
    },
    {
      "epoch": 7.715333333333334,
      "grad_norm": 0.1427471935749054,
      "learning_rate": 1.7791666666666669e-06,
      "loss": 0.0018,
      "step": 231460
    },
    {
      "epoch": 7.7156666666666665,
      "grad_norm": 0.23680301010608673,
      "learning_rate": 1.7770833333333334e-06,
      "loss": 0.0014,
      "step": 231470
    },
    {
      "epoch": 7.716,
      "grad_norm": 0.2728111743927002,
      "learning_rate": 1.775e-06,
      "loss": 0.0013,
      "step": 231480
    },
    {
      "epoch": 7.716333333333333,
      "grad_norm": 0.14246444404125214,
      "learning_rate": 1.772916666666667e-06,
      "loss": 0.0014,
      "step": 231490
    },
    {
      "epoch": 7.716666666666667,
      "grad_norm": 0.058316443115472794,
      "learning_rate": 1.7708333333333335e-06,
      "loss": 0.0022,
      "step": 231500
    },
    {
      "epoch": 7.717,
      "grad_norm": 0.39962711930274963,
      "learning_rate": 1.76875e-06,
      "loss": 0.0017,
      "step": 231510
    },
    {
      "epoch": 7.717333333333333,
      "grad_norm": 0.2574346661567688,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0018,
      "step": 231520
    },
    {
      "epoch": 7.717666666666666,
      "grad_norm": 0.060712989419698715,
      "learning_rate": 1.7645833333333333e-06,
      "loss": 0.0012,
      "step": 231530
    },
    {
      "epoch": 7.718,
      "grad_norm": 0.06134772300720215,
      "learning_rate": 1.7624999999999999e-06,
      "loss": 0.0018,
      "step": 231540
    },
    {
      "epoch": 7.718333333333334,
      "grad_norm": 0.2289423942565918,
      "learning_rate": 1.7604166666666668e-06,
      "loss": 0.0021,
      "step": 231550
    },
    {
      "epoch": 7.718666666666667,
      "grad_norm": 0.17208397388458252,
      "learning_rate": 1.7583333333333334e-06,
      "loss": 0.0016,
      "step": 231560
    },
    {
      "epoch": 7.719,
      "grad_norm": 0.030415549874305725,
      "learning_rate": 1.7562500000000003e-06,
      "loss": 0.0018,
      "step": 231570
    },
    {
      "epoch": 7.719333333333333,
      "grad_norm": 0.11512136459350586,
      "learning_rate": 1.7541666666666669e-06,
      "loss": 0.0016,
      "step": 231580
    },
    {
      "epoch": 7.719666666666667,
      "grad_norm": 0.08125409483909607,
      "learning_rate": 1.7520833333333334e-06,
      "loss": 0.0013,
      "step": 231590
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.11417561024427414,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.0017,
      "step": 231600
    },
    {
      "epoch": 7.7203333333333335,
      "grad_norm": 0.1432729810476303,
      "learning_rate": 1.7479166666666667e-06,
      "loss": 0.0011,
      "step": 231610
    },
    {
      "epoch": 7.720666666666666,
      "grad_norm": 0.08885475993156433,
      "learning_rate": 1.7458333333333333e-06,
      "loss": 0.0016,
      "step": 231620
    },
    {
      "epoch": 7.721,
      "grad_norm": 0.20018239319324493,
      "learning_rate": 1.7437500000000002e-06,
      "loss": 0.0019,
      "step": 231630
    },
    {
      "epoch": 7.721333333333334,
      "grad_norm": 0.23840922117233276,
      "learning_rate": 1.7416666666666668e-06,
      "loss": 0.002,
      "step": 231640
    },
    {
      "epoch": 7.721666666666667,
      "grad_norm": 0.4763118028640747,
      "learning_rate": 1.7395833333333333e-06,
      "loss": 0.0017,
      "step": 231650
    },
    {
      "epoch": 7.7219999999999995,
      "grad_norm": 0.19943049550056458,
      "learning_rate": 1.7375000000000003e-06,
      "loss": 0.0028,
      "step": 231660
    },
    {
      "epoch": 7.722333333333333,
      "grad_norm": 0.31418466567993164,
      "learning_rate": 1.7354166666666669e-06,
      "loss": 0.0016,
      "step": 231670
    },
    {
      "epoch": 7.722666666666667,
      "grad_norm": 0.029779847711324692,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0016,
      "step": 231680
    },
    {
      "epoch": 7.723,
      "grad_norm": 0.1425028145313263,
      "learning_rate": 1.7312500000000002e-06,
      "loss": 0.0014,
      "step": 231690
    },
    {
      "epoch": 7.723333333333334,
      "grad_norm": 0.11437281221151352,
      "learning_rate": 1.7291666666666667e-06,
      "loss": 0.0012,
      "step": 231700
    },
    {
      "epoch": 7.7236666666666665,
      "grad_norm": 0.22979217767715454,
      "learning_rate": 1.7270833333333332e-06,
      "loss": 0.0014,
      "step": 231710
    },
    {
      "epoch": 7.724,
      "grad_norm": 0.2567797601222992,
      "learning_rate": 1.7250000000000002e-06,
      "loss": 0.0014,
      "step": 231720
    },
    {
      "epoch": 7.724333333333333,
      "grad_norm": 0.032869063317775726,
      "learning_rate": 1.7229166666666668e-06,
      "loss": 0.0015,
      "step": 231730
    },
    {
      "epoch": 7.724666666666667,
      "grad_norm": 0.20352603495121002,
      "learning_rate": 1.7208333333333333e-06,
      "loss": 0.0022,
      "step": 231740
    },
    {
      "epoch": 7.725,
      "grad_norm": 0.08731544762849808,
      "learning_rate": 1.7187500000000003e-06,
      "loss": 0.0013,
      "step": 231750
    },
    {
      "epoch": 7.725333333333333,
      "grad_norm": 0.05947965011000633,
      "learning_rate": 1.7166666666666668e-06,
      "loss": 0.0018,
      "step": 231760
    },
    {
      "epoch": 7.725666666666667,
      "grad_norm": 0.06402698159217834,
      "learning_rate": 1.7145833333333334e-06,
      "loss": 0.0018,
      "step": 231770
    },
    {
      "epoch": 7.726,
      "grad_norm": 0.15405791997909546,
      "learning_rate": 1.7125000000000001e-06,
      "loss": 0.0025,
      "step": 231780
    },
    {
      "epoch": 7.726333333333334,
      "grad_norm": 0.08576325327157974,
      "learning_rate": 1.7104166666666667e-06,
      "loss": 0.0018,
      "step": 231790
    },
    {
      "epoch": 7.726666666666667,
      "grad_norm": 0.08765634894371033,
      "learning_rate": 1.7083333333333332e-06,
      "loss": 0.0013,
      "step": 231800
    },
    {
      "epoch": 7.727,
      "grad_norm": 0.01832273229956627,
      "learning_rate": 1.7062500000000002e-06,
      "loss": 0.0015,
      "step": 231810
    },
    {
      "epoch": 7.727333333333333,
      "grad_norm": 0.2283208966255188,
      "learning_rate": 1.7041666666666667e-06,
      "loss": 0.0014,
      "step": 231820
    },
    {
      "epoch": 7.727666666666667,
      "grad_norm": 0.31384891271591187,
      "learning_rate": 1.7020833333333333e-06,
      "loss": 0.0016,
      "step": 231830
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.3712400496006012,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.0027,
      "step": 231840
    },
    {
      "epoch": 7.7283333333333335,
      "grad_norm": 0.17665353417396545,
      "learning_rate": 1.6979166666666668e-06,
      "loss": 0.0019,
      "step": 231850
    },
    {
      "epoch": 7.728666666666666,
      "grad_norm": 0.34233859181404114,
      "learning_rate": 1.6958333333333333e-06,
      "loss": 0.0017,
      "step": 231860
    },
    {
      "epoch": 7.729,
      "grad_norm": 0.057398874312639236,
      "learning_rate": 1.6937500000000003e-06,
      "loss": 0.0017,
      "step": 231870
    },
    {
      "epoch": 7.729333333333333,
      "grad_norm": 0.3136768341064453,
      "learning_rate": 1.6916666666666668e-06,
      "loss": 0.0012,
      "step": 231880
    },
    {
      "epoch": 7.729666666666667,
      "grad_norm": 0.1432650089263916,
      "learning_rate": 1.6895833333333334e-06,
      "loss": 0.0021,
      "step": 231890
    },
    {
      "epoch": 7.73,
      "grad_norm": 0.058090947568416595,
      "learning_rate": 1.6875000000000001e-06,
      "loss": 0.0021,
      "step": 231900
    },
    {
      "epoch": 7.730333333333333,
      "grad_norm": 0.08623605966567993,
      "learning_rate": 1.6854166666666667e-06,
      "loss": 0.0025,
      "step": 231910
    },
    {
      "epoch": 7.730666666666667,
      "grad_norm": 0.14356379210948944,
      "learning_rate": 1.6833333333333332e-06,
      "loss": 0.0017,
      "step": 231920
    },
    {
      "epoch": 7.731,
      "grad_norm": 0.14387807250022888,
      "learning_rate": 1.6812500000000002e-06,
      "loss": 0.0021,
      "step": 231930
    },
    {
      "epoch": 7.731333333333334,
      "grad_norm": 0.31390121579170227,
      "learning_rate": 1.6791666666666668e-06,
      "loss": 0.001,
      "step": 231940
    },
    {
      "epoch": 7.7316666666666665,
      "grad_norm": 0.057981088757514954,
      "learning_rate": 1.6770833333333333e-06,
      "loss": 0.0017,
      "step": 231950
    },
    {
      "epoch": 7.732,
      "grad_norm": 0.1477997750043869,
      "learning_rate": 1.6750000000000003e-06,
      "loss": 0.0013,
      "step": 231960
    },
    {
      "epoch": 7.732333333333333,
      "grad_norm": 0.1160653680562973,
      "learning_rate": 1.6729166666666668e-06,
      "loss": 0.0017,
      "step": 231970
    },
    {
      "epoch": 7.732666666666667,
      "grad_norm": 0.2227986603975296,
      "learning_rate": 1.6708333333333334e-06,
      "loss": 0.0018,
      "step": 231980
    },
    {
      "epoch": 7.733,
      "grad_norm": 0.17883029580116272,
      "learning_rate": 1.6687500000000001e-06,
      "loss": 0.0015,
      "step": 231990
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.08674032986164093,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0014,
      "step": 232000
    },
    {
      "epoch": 7.733666666666666,
      "grad_norm": 0.17291104793548584,
      "learning_rate": 1.6645833333333332e-06,
      "loss": 0.0017,
      "step": 232010
    },
    {
      "epoch": 7.734,
      "grad_norm": 0.11495925486087799,
      "learning_rate": 1.6625000000000002e-06,
      "loss": 0.0016,
      "step": 232020
    },
    {
      "epoch": 7.734333333333334,
      "grad_norm": 0.03181196749210358,
      "learning_rate": 1.6604166666666667e-06,
      "loss": 0.0013,
      "step": 232030
    },
    {
      "epoch": 7.734666666666667,
      "grad_norm": 0.1426478922367096,
      "learning_rate": 1.6583333333333333e-06,
      "loss": 0.0012,
      "step": 232040
    },
    {
      "epoch": 7.735,
      "grad_norm": 0.3266982138156891,
      "learning_rate": 1.6562500000000002e-06,
      "loss": 0.0021,
      "step": 232050
    },
    {
      "epoch": 7.735333333333333,
      "grad_norm": 0.11466953158378601,
      "learning_rate": 1.6541666666666668e-06,
      "loss": 0.0016,
      "step": 232060
    },
    {
      "epoch": 7.735666666666667,
      "grad_norm": 0.0310012586414814,
      "learning_rate": 1.6520833333333333e-06,
      "loss": 0.0015,
      "step": 232070
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.010808421298861504,
      "learning_rate": 1.65e-06,
      "loss": 0.002,
      "step": 232080
    },
    {
      "epoch": 7.7363333333333335,
      "grad_norm": 0.11853529512882233,
      "learning_rate": 1.6479166666666666e-06,
      "loss": 0.0017,
      "step": 232090
    },
    {
      "epoch": 7.736666666666666,
      "grad_norm": 0.12103442847728729,
      "learning_rate": 1.6458333333333332e-06,
      "loss": 0.0015,
      "step": 232100
    },
    {
      "epoch": 7.737,
      "grad_norm": 0.0597488172352314,
      "learning_rate": 1.6437500000000001e-06,
      "loss": 0.0014,
      "step": 232110
    },
    {
      "epoch": 7.737333333333333,
      "grad_norm": 0.08704973012208939,
      "learning_rate": 1.6416666666666667e-06,
      "loss": 0.0013,
      "step": 232120
    },
    {
      "epoch": 7.737666666666667,
      "grad_norm": 0.1441134363412857,
      "learning_rate": 1.6395833333333332e-06,
      "loss": 0.0012,
      "step": 232130
    },
    {
      "epoch": 7.7379999999999995,
      "grad_norm": 0.12474120408296585,
      "learning_rate": 1.6375000000000002e-06,
      "loss": 0.0017,
      "step": 232140
    },
    {
      "epoch": 7.738333333333333,
      "grad_norm": 0.09406595677137375,
      "learning_rate": 1.6354166666666667e-06,
      "loss": 0.0026,
      "step": 232150
    },
    {
      "epoch": 7.738666666666667,
      "grad_norm": 0.15509270131587982,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0018,
      "step": 232160
    },
    {
      "epoch": 7.739,
      "grad_norm": 0.046983737498521805,
      "learning_rate": 1.63125e-06,
      "loss": 0.0015,
      "step": 232170
    },
    {
      "epoch": 7.739333333333334,
      "grad_norm": 0.03364591673016548,
      "learning_rate": 1.6291666666666666e-06,
      "loss": 0.0017,
      "step": 232180
    },
    {
      "epoch": 7.7396666666666665,
      "grad_norm": 0.3594326376914978,
      "learning_rate": 1.6270833333333336e-06,
      "loss": 0.002,
      "step": 232190
    },
    {
      "epoch": 7.74,
      "grad_norm": 0.14372092485427856,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 0.0015,
      "step": 232200
    },
    {
      "epoch": 7.740333333333333,
      "grad_norm": 0.12119221687316895,
      "learning_rate": 1.6229166666666667e-06,
      "loss": 0.0017,
      "step": 232210
    },
    {
      "epoch": 7.740666666666667,
      "grad_norm": 0.3425185978412628,
      "learning_rate": 1.6208333333333336e-06,
      "loss": 0.0013,
      "step": 232220
    },
    {
      "epoch": 7.741,
      "grad_norm": 0.08578404039144516,
      "learning_rate": 1.6187500000000002e-06,
      "loss": 0.0015,
      "step": 232230
    },
    {
      "epoch": 7.741333333333333,
      "grad_norm": 0.05921047925949097,
      "learning_rate": 1.6166666666666667e-06,
      "loss": 0.0015,
      "step": 232240
    },
    {
      "epoch": 7.741666666666667,
      "grad_norm": 0.2850840091705322,
      "learning_rate": 1.6145833333333335e-06,
      "loss": 0.0028,
      "step": 232250
    },
    {
      "epoch": 7.742,
      "grad_norm": 0.17098984122276306,
      "learning_rate": 1.6125e-06,
      "loss": 0.0014,
      "step": 232260
    },
    {
      "epoch": 7.742333333333333,
      "grad_norm": 0.22843673825263977,
      "learning_rate": 1.6104166666666666e-06,
      "loss": 0.0013,
      "step": 232270
    },
    {
      "epoch": 7.742666666666667,
      "grad_norm": 0.17461590468883514,
      "learning_rate": 1.6083333333333335e-06,
      "loss": 0.0018,
      "step": 232280
    },
    {
      "epoch": 7.743,
      "grad_norm": 0.0579121857881546,
      "learning_rate": 1.60625e-06,
      "loss": 0.0027,
      "step": 232290
    },
    {
      "epoch": 7.743333333333333,
      "grad_norm": 0.05770467221736908,
      "learning_rate": 1.6041666666666666e-06,
      "loss": 0.0018,
      "step": 232300
    },
    {
      "epoch": 7.743666666666667,
      "grad_norm": 0.415162593126297,
      "learning_rate": 1.6020833333333336e-06,
      "loss": 0.0012,
      "step": 232310
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.3704524040222168,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0014,
      "step": 232320
    },
    {
      "epoch": 7.7443333333333335,
      "grad_norm": 0.1712842434644699,
      "learning_rate": 1.5979166666666667e-06,
      "loss": 0.0015,
      "step": 232330
    },
    {
      "epoch": 7.744666666666666,
      "grad_norm": 0.08598293364048004,
      "learning_rate": 1.5958333333333337e-06,
      "loss": 0.0016,
      "step": 232340
    },
    {
      "epoch": 7.745,
      "grad_norm": 0.3993375301361084,
      "learning_rate": 1.5937500000000002e-06,
      "loss": 0.0013,
      "step": 232350
    },
    {
      "epoch": 7.745333333333333,
      "grad_norm": 0.030247539281845093,
      "learning_rate": 1.5916666666666667e-06,
      "loss": 0.0016,
      "step": 232360
    },
    {
      "epoch": 7.745666666666667,
      "grad_norm": 0.2859228253364563,
      "learning_rate": 1.5895833333333335e-06,
      "loss": 0.0021,
      "step": 232370
    },
    {
      "epoch": 7.746,
      "grad_norm": 0.31984925270080566,
      "learning_rate": 1.5875e-06,
      "loss": 0.0018,
      "step": 232380
    },
    {
      "epoch": 7.746333333333333,
      "grad_norm": 0.5027883052825928,
      "learning_rate": 1.5854166666666666e-06,
      "loss": 0.0018,
      "step": 232390
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.28542816638946533,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.0017,
      "step": 232400
    },
    {
      "epoch": 7.747,
      "grad_norm": 0.15512114763259888,
      "learning_rate": 1.5812500000000001e-06,
      "loss": 0.0024,
      "step": 232410
    },
    {
      "epoch": 7.747333333333334,
      "grad_norm": 0.08589241653680801,
      "learning_rate": 1.5791666666666667e-06,
      "loss": 0.0015,
      "step": 232420
    },
    {
      "epoch": 7.7476666666666665,
      "grad_norm": 0.3527781665325165,
      "learning_rate": 1.5770833333333336e-06,
      "loss": 0.0018,
      "step": 232430
    },
    {
      "epoch": 7.748,
      "grad_norm": 0.12470906972885132,
      "learning_rate": 1.5750000000000002e-06,
      "loss": 0.0022,
      "step": 232440
    },
    {
      "epoch": 7.748333333333333,
      "grad_norm": 0.19965185225009918,
      "learning_rate": 1.5729166666666667e-06,
      "loss": 0.0023,
      "step": 232450
    },
    {
      "epoch": 7.748666666666667,
      "grad_norm": 0.3417753279209137,
      "learning_rate": 1.5708333333333335e-06,
      "loss": 0.0021,
      "step": 232460
    },
    {
      "epoch": 7.749,
      "grad_norm": 0.31356334686279297,
      "learning_rate": 1.56875e-06,
      "loss": 0.0013,
      "step": 232470
    },
    {
      "epoch": 7.749333333333333,
      "grad_norm": 0.11699533462524414,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0013,
      "step": 232480
    },
    {
      "epoch": 7.749666666666666,
      "grad_norm": 0.22895203530788422,
      "learning_rate": 1.5645833333333335e-06,
      "loss": 0.0012,
      "step": 232490
    },
    {
      "epoch": 7.75,
      "grad_norm": 0.08599844574928284,
      "learning_rate": 1.5625e-06,
      "loss": 0.0015,
      "step": 232500
    },
    {
      "epoch": 7.750333333333334,
      "grad_norm": 0.17866462469100952,
      "learning_rate": 1.5604166666666668e-06,
      "loss": 0.0014,
      "step": 232510
    },
    {
      "epoch": 7.750666666666667,
      "grad_norm": 0.14292645454406738,
      "learning_rate": 1.5583333333333334e-06,
      "loss": 0.0011,
      "step": 232520
    },
    {
      "epoch": 7.751,
      "grad_norm": 0.17242170870304108,
      "learning_rate": 1.5562500000000001e-06,
      "loss": 0.0016,
      "step": 232530
    },
    {
      "epoch": 7.751333333333333,
      "grad_norm": 0.10481471568346024,
      "learning_rate": 1.5541666666666669e-06,
      "loss": 0.0022,
      "step": 232540
    },
    {
      "epoch": 7.751666666666667,
      "grad_norm": 0.08648230880498886,
      "learning_rate": 1.5520833333333334e-06,
      "loss": 0.0012,
      "step": 232550
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.2569947838783264,
      "learning_rate": 1.55e-06,
      "loss": 0.0022,
      "step": 232560
    },
    {
      "epoch": 7.7523333333333335,
      "grad_norm": 0.17164549231529236,
      "learning_rate": 1.5479166666666667e-06,
      "loss": 0.0013,
      "step": 232570
    },
    {
      "epoch": 7.752666666666666,
      "grad_norm": 0.22861583530902863,
      "learning_rate": 1.5458333333333333e-06,
      "loss": 0.0026,
      "step": 232580
    },
    {
      "epoch": 7.753,
      "grad_norm": 0.07399185001850128,
      "learning_rate": 1.54375e-06,
      "loss": 0.0016,
      "step": 232590
    },
    {
      "epoch": 7.753333333333333,
      "grad_norm": 0.030637864023447037,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 0.0011,
      "step": 232600
    },
    {
      "epoch": 7.753666666666667,
      "grad_norm": 0.36697816848754883,
      "learning_rate": 1.5395833333333333e-06,
      "loss": 0.0017,
      "step": 232610
    },
    {
      "epoch": 7.754,
      "grad_norm": 0.08784963935613632,
      "learning_rate": 1.5375e-06,
      "loss": 0.0017,
      "step": 232620
    },
    {
      "epoch": 7.754333333333333,
      "grad_norm": 0.34426969289779663,
      "learning_rate": 1.5354166666666669e-06,
      "loss": 0.0017,
      "step": 232630
    },
    {
      "epoch": 7.754666666666667,
      "grad_norm": 0.17144663631916046,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0024,
      "step": 232640
    },
    {
      "epoch": 7.755,
      "grad_norm": 0.11732877045869827,
      "learning_rate": 1.53125e-06,
      "loss": 0.0018,
      "step": 232650
    },
    {
      "epoch": 7.755333333333334,
      "grad_norm": 0.008428393863141537,
      "learning_rate": 1.5291666666666667e-06,
      "loss": 0.0017,
      "step": 232660
    },
    {
      "epoch": 7.7556666666666665,
      "grad_norm": 0.14352041482925415,
      "learning_rate": 1.5270833333333335e-06,
      "loss": 0.0027,
      "step": 232670
    },
    {
      "epoch": 7.756,
      "grad_norm": 0.19946354627609253,
      "learning_rate": 1.525e-06,
      "loss": 0.0023,
      "step": 232680
    },
    {
      "epoch": 7.756333333333333,
      "grad_norm": 0.17194417119026184,
      "learning_rate": 1.5229166666666668e-06,
      "loss": 0.0019,
      "step": 232690
    },
    {
      "epoch": 7.756666666666667,
      "grad_norm": 0.007051034364849329,
      "learning_rate": 1.5208333333333335e-06,
      "loss": 0.0012,
      "step": 232700
    },
    {
      "epoch": 7.757,
      "grad_norm": 0.11559109389781952,
      "learning_rate": 1.51875e-06,
      "loss": 0.0014,
      "step": 232710
    },
    {
      "epoch": 7.757333333333333,
      "grad_norm": 0.08715596795082092,
      "learning_rate": 1.5166666666666668e-06,
      "loss": 0.0012,
      "step": 232720
    },
    {
      "epoch": 7.757666666666667,
      "grad_norm": 0.03518351912498474,
      "learning_rate": 1.5145833333333336e-06,
      "loss": 0.0013,
      "step": 232730
    },
    {
      "epoch": 7.758,
      "grad_norm": 0.37090176343917847,
      "learning_rate": 1.5125000000000001e-06,
      "loss": 0.0019,
      "step": 232740
    },
    {
      "epoch": 7.758333333333333,
      "grad_norm": 0.14403294026851654,
      "learning_rate": 1.5104166666666667e-06,
      "loss": 0.0023,
      "step": 232750
    },
    {
      "epoch": 7.758666666666667,
      "grad_norm": 0.17564068734645844,
      "learning_rate": 1.5083333333333334e-06,
      "loss": 0.0014,
      "step": 232760
    },
    {
      "epoch": 7.759,
      "grad_norm": 0.09531354159116745,
      "learning_rate": 1.50625e-06,
      "loss": 0.0021,
      "step": 232770
    },
    {
      "epoch": 7.759333333333333,
      "grad_norm": 0.19976720213890076,
      "learning_rate": 1.5041666666666667e-06,
      "loss": 0.0023,
      "step": 232780
    },
    {
      "epoch": 7.759666666666667,
      "grad_norm": 0.0326337032020092,
      "learning_rate": 1.5020833333333335e-06,
      "loss": 0.0015,
      "step": 232790
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.285932332277298,
      "learning_rate": 1.5e-06,
      "loss": 0.0013,
      "step": 232800
    },
    {
      "epoch": 7.7603333333333335,
      "grad_norm": 0.14348521828651428,
      "learning_rate": 1.4979166666666668e-06,
      "loss": 0.0023,
      "step": 232810
    },
    {
      "epoch": 7.760666666666666,
      "grad_norm": 0.05922028049826622,
      "learning_rate": 1.4958333333333336e-06,
      "loss": 0.0018,
      "step": 232820
    },
    {
      "epoch": 7.761,
      "grad_norm": 0.31345662474632263,
      "learning_rate": 1.49375e-06,
      "loss": 0.0016,
      "step": 232830
    },
    {
      "epoch": 7.761333333333333,
      "grad_norm": 0.08582653850317001,
      "learning_rate": 1.4916666666666666e-06,
      "loss": 0.0018,
      "step": 232840
    },
    {
      "epoch": 7.761666666666667,
      "grad_norm": 0.029453083872795105,
      "learning_rate": 1.4895833333333334e-06,
      "loss": 0.0021,
      "step": 232850
    },
    {
      "epoch": 7.7620000000000005,
      "grad_norm": 0.061860330402851105,
      "learning_rate": 1.4875e-06,
      "loss": 0.0017,
      "step": 232860
    },
    {
      "epoch": 7.762333333333333,
      "grad_norm": 0.1425156593322754,
      "learning_rate": 1.4854166666666667e-06,
      "loss": 0.0011,
      "step": 232870
    },
    {
      "epoch": 7.762666666666667,
      "grad_norm": 0.2981983721256256,
      "learning_rate": 1.4833333333333335e-06,
      "loss": 0.0014,
      "step": 232880
    },
    {
      "epoch": 7.763,
      "grad_norm": 0.37091130018234253,
      "learning_rate": 1.48125e-06,
      "loss": 0.0014,
      "step": 232890
    },
    {
      "epoch": 7.763333333333334,
      "grad_norm": 0.05822617933154106,
      "learning_rate": 1.4791666666666668e-06,
      "loss": 0.0015,
      "step": 232900
    },
    {
      "epoch": 7.7636666666666665,
      "grad_norm": 0.1625891476869583,
      "learning_rate": 1.4770833333333335e-06,
      "loss": 0.0015,
      "step": 232910
    },
    {
      "epoch": 7.764,
      "grad_norm": 0.2853454053401947,
      "learning_rate": 1.475e-06,
      "loss": 0.002,
      "step": 232920
    },
    {
      "epoch": 7.764333333333333,
      "grad_norm": 0.05823285132646561,
      "learning_rate": 1.4729166666666666e-06,
      "loss": 0.0017,
      "step": 232930
    },
    {
      "epoch": 7.764666666666667,
      "grad_norm": 0.17131483554840088,
      "learning_rate": 1.4708333333333334e-06,
      "loss": 0.0014,
      "step": 232940
    },
    {
      "epoch": 7.765,
      "grad_norm": 0.32357388734817505,
      "learning_rate": 1.46875e-06,
      "loss": 0.0018,
      "step": 232950
    },
    {
      "epoch": 7.765333333333333,
      "grad_norm": 0.08632993698120117,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0013,
      "step": 232960
    },
    {
      "epoch": 7.765666666666666,
      "grad_norm": 0.2004397213459015,
      "learning_rate": 1.4645833333333334e-06,
      "loss": 0.0028,
      "step": 232970
    },
    {
      "epoch": 7.766,
      "grad_norm": 0.11501243710517883,
      "learning_rate": 1.4625000000000002e-06,
      "loss": 0.0013,
      "step": 232980
    },
    {
      "epoch": 7.766333333333334,
      "grad_norm": 0.17202898859977722,
      "learning_rate": 1.4604166666666667e-06,
      "loss": 0.0017,
      "step": 232990
    },
    {
      "epoch": 7.766666666666667,
      "grad_norm": 0.3142620027065277,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 0.0019,
      "step": 233000
    },
    {
      "epoch": 7.767,
      "grad_norm": 0.06414572894573212,
      "learning_rate": 1.4562500000000002e-06,
      "loss": 0.0026,
      "step": 233010
    },
    {
      "epoch": 7.767333333333333,
      "grad_norm": 0.03007069230079651,
      "learning_rate": 1.4541666666666668e-06,
      "loss": 0.0018,
      "step": 233020
    },
    {
      "epoch": 7.767666666666667,
      "grad_norm": 0.20412878692150116,
      "learning_rate": 1.4520833333333333e-06,
      "loss": 0.0014,
      "step": 233030
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.2296905219554901,
      "learning_rate": 1.45e-06,
      "loss": 0.0025,
      "step": 233040
    },
    {
      "epoch": 7.7683333333333335,
      "grad_norm": 0.22945626080036163,
      "learning_rate": 1.4479166666666666e-06,
      "loss": 0.0013,
      "step": 233050
    },
    {
      "epoch": 7.768666666666666,
      "grad_norm": 0.05928292125463486,
      "learning_rate": 1.4458333333333334e-06,
      "loss": 0.0019,
      "step": 233060
    },
    {
      "epoch": 7.769,
      "grad_norm": 0.22923240065574646,
      "learning_rate": 1.4437500000000002e-06,
      "loss": 0.0016,
      "step": 233070
    },
    {
      "epoch": 7.769333333333333,
      "grad_norm": 0.20975881814956665,
      "learning_rate": 1.4416666666666667e-06,
      "loss": 0.0016,
      "step": 233080
    },
    {
      "epoch": 7.769666666666667,
      "grad_norm": 0.09493353962898254,
      "learning_rate": 1.4395833333333335e-06,
      "loss": 0.0014,
      "step": 233090
    },
    {
      "epoch": 7.77,
      "grad_norm": 0.20019178092479706,
      "learning_rate": 1.4375000000000002e-06,
      "loss": 0.0017,
      "step": 233100
    },
    {
      "epoch": 7.770333333333333,
      "grad_norm": 0.029606718569993973,
      "learning_rate": 1.4354166666666668e-06,
      "loss": 0.0021,
      "step": 233110
    },
    {
      "epoch": 7.770666666666667,
      "grad_norm": 0.14670707285404205,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.0017,
      "step": 233120
    },
    {
      "epoch": 7.771,
      "grad_norm": 0.08545231819152832,
      "learning_rate": 1.43125e-06,
      "loss": 0.0012,
      "step": 233130
    },
    {
      "epoch": 7.771333333333334,
      "grad_norm": 0.1719503253698349,
      "learning_rate": 1.4291666666666666e-06,
      "loss": 0.0014,
      "step": 233140
    },
    {
      "epoch": 7.7716666666666665,
      "grad_norm": 0.2294391691684723,
      "learning_rate": 1.4270833333333334e-06,
      "loss": 0.0013,
      "step": 233150
    },
    {
      "epoch": 7.772,
      "grad_norm": 0.17123521864414215,
      "learning_rate": 1.4250000000000001e-06,
      "loss": 0.0014,
      "step": 233160
    },
    {
      "epoch": 7.772333333333333,
      "grad_norm": 0.4280334413051605,
      "learning_rate": 1.4229166666666667e-06,
      "loss": 0.0019,
      "step": 233170
    },
    {
      "epoch": 7.772666666666667,
      "grad_norm": 0.17170707881450653,
      "learning_rate": 1.4208333333333334e-06,
      "loss": 0.0013,
      "step": 233180
    },
    {
      "epoch": 7.773,
      "grad_norm": 0.09397707134485245,
      "learning_rate": 1.4187500000000002e-06,
      "loss": 0.0016,
      "step": 233190
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 0.2858213484287262,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.0016,
      "step": 233200
    },
    {
      "epoch": 7.773666666666666,
      "grad_norm": 0.22840523719787598,
      "learning_rate": 1.4145833333333335e-06,
      "loss": 0.0016,
      "step": 233210
    },
    {
      "epoch": 7.774,
      "grad_norm": 0.05821982026100159,
      "learning_rate": 1.4125e-06,
      "loss": 0.0017,
      "step": 233220
    },
    {
      "epoch": 7.774333333333333,
      "grad_norm": 0.14321982860565186,
      "learning_rate": 1.4104166666666666e-06,
      "loss": 0.0013,
      "step": 233230
    },
    {
      "epoch": 7.774666666666667,
      "grad_norm": 0.03269899636507034,
      "learning_rate": 1.4083333333333333e-06,
      "loss": 0.0013,
      "step": 233240
    },
    {
      "epoch": 7.775,
      "grad_norm": 0.14428383111953735,
      "learning_rate": 1.40625e-06,
      "loss": 0.0016,
      "step": 233250
    },
    {
      "epoch": 7.775333333333333,
      "grad_norm": 0.058054376393556595,
      "learning_rate": 1.4041666666666666e-06,
      "loss": 0.0015,
      "step": 233260
    },
    {
      "epoch": 7.775666666666667,
      "grad_norm": 0.2282017469406128,
      "learning_rate": 1.4020833333333334e-06,
      "loss": 0.0022,
      "step": 233270
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.15734362602233887,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0015,
      "step": 233280
    },
    {
      "epoch": 7.7763333333333335,
      "grad_norm": 0.08558045327663422,
      "learning_rate": 1.397916666666667e-06,
      "loss": 0.0021,
      "step": 233290
    },
    {
      "epoch": 7.776666666666666,
      "grad_norm": 0.17126449942588806,
      "learning_rate": 1.3958333333333335e-06,
      "loss": 0.0016,
      "step": 233300
    },
    {
      "epoch": 7.777,
      "grad_norm": 0.029514355584979057,
      "learning_rate": 1.39375e-06,
      "loss": 0.0012,
      "step": 233310
    },
    {
      "epoch": 7.777333333333333,
      "grad_norm": 0.05848066508769989,
      "learning_rate": 1.3916666666666668e-06,
      "loss": 0.0022,
      "step": 233320
    },
    {
      "epoch": 7.777666666666667,
      "grad_norm": 0.06961821764707565,
      "learning_rate": 1.3895833333333333e-06,
      "loss": 0.0022,
      "step": 233330
    },
    {
      "epoch": 7.7780000000000005,
      "grad_norm": 0.37026044726371765,
      "learning_rate": 1.3875e-06,
      "loss": 0.0018,
      "step": 233340
    },
    {
      "epoch": 7.778333333333333,
      "grad_norm": 0.08583928644657135,
      "learning_rate": 1.3854166666666668e-06,
      "loss": 0.0016,
      "step": 233350
    },
    {
      "epoch": 7.778666666666666,
      "grad_norm": 0.2860822081565857,
      "learning_rate": 1.3833333333333334e-06,
      "loss": 0.0018,
      "step": 233360
    },
    {
      "epoch": 7.779,
      "grad_norm": 0.14296674728393555,
      "learning_rate": 1.3812500000000001e-06,
      "loss": 0.0015,
      "step": 233370
    },
    {
      "epoch": 7.779333333333334,
      "grad_norm": 0.1144411638379097,
      "learning_rate": 1.3791666666666669e-06,
      "loss": 0.0015,
      "step": 233380
    },
    {
      "epoch": 7.7796666666666665,
      "grad_norm": 0.3141842484474182,
      "learning_rate": 1.3770833333333334e-06,
      "loss": 0.0014,
      "step": 233390
    },
    {
      "epoch": 7.78,
      "grad_norm": 0.06548219919204712,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 0.0011,
      "step": 233400
    },
    {
      "epoch": 7.780333333333333,
      "grad_norm": 0.17123663425445557,
      "learning_rate": 1.3729166666666667e-06,
      "loss": 0.0014,
      "step": 233410
    },
    {
      "epoch": 7.780666666666667,
      "grad_norm": 0.25738605856895447,
      "learning_rate": 1.3708333333333333e-06,
      "loss": 0.0012,
      "step": 233420
    },
    {
      "epoch": 7.781,
      "grad_norm": 0.1428823322057724,
      "learning_rate": 1.36875e-06,
      "loss": 0.002,
      "step": 233430
    },
    {
      "epoch": 7.781333333333333,
      "grad_norm": 0.08633090555667877,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0017,
      "step": 233440
    },
    {
      "epoch": 7.781666666666666,
      "grad_norm": 0.19089901447296143,
      "learning_rate": 1.3645833333333333e-06,
      "loss": 0.0024,
      "step": 233450
    },
    {
      "epoch": 7.782,
      "grad_norm": 0.033169034868478775,
      "learning_rate": 1.3625e-06,
      "loss": 0.0022,
      "step": 233460
    },
    {
      "epoch": 7.782333333333334,
      "grad_norm": 0.15719960629940033,
      "learning_rate": 1.3604166666666668e-06,
      "loss": 0.0019,
      "step": 233470
    },
    {
      "epoch": 7.782666666666667,
      "grad_norm": 0.08720000088214874,
      "learning_rate": 1.3583333333333334e-06,
      "loss": 0.0014,
      "step": 233480
    },
    {
      "epoch": 7.783,
      "grad_norm": 0.11579658836126328,
      "learning_rate": 1.3562500000000001e-06,
      "loss": 0.0017,
      "step": 233490
    },
    {
      "epoch": 7.783333333333333,
      "grad_norm": 0.010857963003218174,
      "learning_rate": 1.3541666666666667e-06,
      "loss": 0.0017,
      "step": 233500
    },
    {
      "epoch": 7.783666666666667,
      "grad_norm": 0.1996835619211197,
      "learning_rate": 1.3520833333333332e-06,
      "loss": 0.0013,
      "step": 233510
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.14408400654792786,
      "learning_rate": 1.35e-06,
      "loss": 0.0015,
      "step": 233520
    },
    {
      "epoch": 7.7843333333333335,
      "grad_norm": 0.1438770443201065,
      "learning_rate": 1.3479166666666667e-06,
      "loss": 0.0015,
      "step": 233530
    },
    {
      "epoch": 7.784666666666666,
      "grad_norm": 0.31318289041519165,
      "learning_rate": 1.3458333333333333e-06,
      "loss": 0.0024,
      "step": 233540
    },
    {
      "epoch": 7.785,
      "grad_norm": 0.2292211502790451,
      "learning_rate": 1.34375e-06,
      "loss": 0.0013,
      "step": 233550
    },
    {
      "epoch": 7.785333333333333,
      "grad_norm": 0.45693936944007874,
      "learning_rate": 1.3416666666666668e-06,
      "loss": 0.0017,
      "step": 233560
    },
    {
      "epoch": 7.785666666666667,
      "grad_norm": 0.030129510909318924,
      "learning_rate": 1.3395833333333334e-06,
      "loss": 0.0015,
      "step": 233570
    },
    {
      "epoch": 7.786,
      "grad_norm": 0.19995711743831635,
      "learning_rate": 1.3375000000000001e-06,
      "loss": 0.0016,
      "step": 233580
    },
    {
      "epoch": 7.786333333333333,
      "grad_norm": 0.08570495992898941,
      "learning_rate": 1.3354166666666667e-06,
      "loss": 0.0016,
      "step": 233590
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.015329193323850632,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0016,
      "step": 233600
    },
    {
      "epoch": 7.787,
      "grad_norm": 0.24638275802135468,
      "learning_rate": 1.33125e-06,
      "loss": 0.002,
      "step": 233610
    },
    {
      "epoch": 7.787333333333334,
      "grad_norm": 0.2811054587364197,
      "learning_rate": 1.3291666666666667e-06,
      "loss": 0.0013,
      "step": 233620
    },
    {
      "epoch": 7.7876666666666665,
      "grad_norm": 0.07423748075962067,
      "learning_rate": 1.3270833333333335e-06,
      "loss": 0.0016,
      "step": 233630
    },
    {
      "epoch": 7.788,
      "grad_norm": 0.061094895005226135,
      "learning_rate": 1.325e-06,
      "loss": 0.0017,
      "step": 233640
    },
    {
      "epoch": 7.788333333333333,
      "grad_norm": 0.17117741703987122,
      "learning_rate": 1.3229166666666668e-06,
      "loss": 0.0019,
      "step": 233650
    },
    {
      "epoch": 7.788666666666667,
      "grad_norm": 0.013306302018463612,
      "learning_rate": 1.3208333333333335e-06,
      "loss": 0.0016,
      "step": 233660
    },
    {
      "epoch": 7.789,
      "grad_norm": 0.14379462599754333,
      "learning_rate": 1.31875e-06,
      "loss": 0.0016,
      "step": 233670
    },
    {
      "epoch": 7.789333333333333,
      "grad_norm": 0.08566399663686752,
      "learning_rate": 1.3166666666666668e-06,
      "loss": 0.0014,
      "step": 233680
    },
    {
      "epoch": 7.789666666666666,
      "grad_norm": 0.22877751290798187,
      "learning_rate": 1.3145833333333334e-06,
      "loss": 0.0015,
      "step": 233690
    },
    {
      "epoch": 7.79,
      "grad_norm": 0.14253093302249908,
      "learning_rate": 1.3125e-06,
      "loss": 0.0015,
      "step": 233700
    },
    {
      "epoch": 7.790333333333333,
      "grad_norm": 0.17166851460933685,
      "learning_rate": 1.3104166666666667e-06,
      "loss": 0.0016,
      "step": 233710
    },
    {
      "epoch": 7.790666666666667,
      "grad_norm": 0.2866347134113312,
      "learning_rate": 1.3083333333333334e-06,
      "loss": 0.0013,
      "step": 233720
    },
    {
      "epoch": 7.791,
      "grad_norm": 0.22084026038646698,
      "learning_rate": 1.30625e-06,
      "loss": 0.0014,
      "step": 233730
    },
    {
      "epoch": 7.791333333333333,
      "grad_norm": 0.03974519297480583,
      "learning_rate": 1.3041666666666667e-06,
      "loss": 0.0019,
      "step": 233740
    },
    {
      "epoch": 7.791666666666667,
      "grad_norm": 0.1174696832895279,
      "learning_rate": 1.3020833333333335e-06,
      "loss": 0.0013,
      "step": 233750
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.3997947573661804,
      "learning_rate": 1.3e-06,
      "loss": 0.0012,
      "step": 233760
    },
    {
      "epoch": 7.792333333333334,
      "grad_norm": 0.050401471555233,
      "learning_rate": 1.2979166666666668e-06,
      "loss": 0.0016,
      "step": 233770
    },
    {
      "epoch": 7.792666666666666,
      "grad_norm": 0.06305330246686935,
      "learning_rate": 1.2958333333333333e-06,
      "loss": 0.0021,
      "step": 233780
    },
    {
      "epoch": 7.793,
      "grad_norm": 0.29917487502098083,
      "learning_rate": 1.29375e-06,
      "loss": 0.0013,
      "step": 233790
    },
    {
      "epoch": 7.793333333333333,
      "grad_norm": 0.4279765486717224,
      "learning_rate": 1.2916666666666667e-06,
      "loss": 0.0024,
      "step": 233800
    },
    {
      "epoch": 7.793666666666667,
      "grad_norm": 0.34491100907325745,
      "learning_rate": 1.2895833333333334e-06,
      "loss": 0.0027,
      "step": 233810
    },
    {
      "epoch": 7.7940000000000005,
      "grad_norm": 0.40094390511512756,
      "learning_rate": 1.2875e-06,
      "loss": 0.0013,
      "step": 233820
    },
    {
      "epoch": 7.794333333333333,
      "grad_norm": 0.3195013403892517,
      "learning_rate": 1.2854166666666667e-06,
      "loss": 0.0033,
      "step": 233830
    },
    {
      "epoch": 7.794666666666666,
      "grad_norm": 0.0304204560816288,
      "learning_rate": 1.2833333333333335e-06,
      "loss": 0.0014,
      "step": 233840
    },
    {
      "epoch": 7.795,
      "grad_norm": 0.3636684715747833,
      "learning_rate": 1.28125e-06,
      "loss": 0.0022,
      "step": 233850
    },
    {
      "epoch": 7.795333333333334,
      "grad_norm": 0.057899359613657,
      "learning_rate": 1.2791666666666668e-06,
      "loss": 0.0013,
      "step": 233860
    },
    {
      "epoch": 7.7956666666666665,
      "grad_norm": 0.0878816694021225,
      "learning_rate": 1.2770833333333335e-06,
      "loss": 0.0015,
      "step": 233870
    },
    {
      "epoch": 7.796,
      "grad_norm": 0.2344445288181305,
      "learning_rate": 1.275e-06,
      "loss": 0.0017,
      "step": 233880
    },
    {
      "epoch": 7.796333333333333,
      "grad_norm": 0.02967364527285099,
      "learning_rate": 1.2729166666666666e-06,
      "loss": 0.0016,
      "step": 233890
    },
    {
      "epoch": 7.796666666666667,
      "grad_norm": 0.22817249596118927,
      "learning_rate": 1.2708333333333334e-06,
      "loss": 0.0014,
      "step": 233900
    },
    {
      "epoch": 7.797,
      "grad_norm": 0.22849269211292267,
      "learning_rate": 1.2687500000000001e-06,
      "loss": 0.0013,
      "step": 233910
    },
    {
      "epoch": 7.7973333333333334,
      "grad_norm": 0.01020901370793581,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0018,
      "step": 233920
    },
    {
      "epoch": 7.797666666666666,
      "grad_norm": 0.0382242426276207,
      "learning_rate": 1.2645833333333334e-06,
      "loss": 0.0021,
      "step": 233930
    },
    {
      "epoch": 7.798,
      "grad_norm": 0.3427681028842926,
      "learning_rate": 1.2625000000000002e-06,
      "loss": 0.0021,
      "step": 233940
    },
    {
      "epoch": 7.798333333333334,
      "grad_norm": 0.1429877132177353,
      "learning_rate": 1.2604166666666667e-06,
      "loss": 0.0015,
      "step": 233950
    },
    {
      "epoch": 7.798666666666667,
      "grad_norm": 0.03220782428979874,
      "learning_rate": 1.2583333333333335e-06,
      "loss": 0.0015,
      "step": 233960
    },
    {
      "epoch": 7.799,
      "grad_norm": 0.261879563331604,
      "learning_rate": 1.25625e-06,
      "loss": 0.0018,
      "step": 233970
    },
    {
      "epoch": 7.799333333333333,
      "grad_norm": 0.11471472680568695,
      "learning_rate": 1.2541666666666666e-06,
      "loss": 0.0018,
      "step": 233980
    },
    {
      "epoch": 7.799666666666667,
      "grad_norm": 0.49528059363365173,
      "learning_rate": 1.2520833333333333e-06,
      "loss": 0.0021,
      "step": 233990
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.17282234132289886,
      "learning_rate": 1.25e-06,
      "loss": 0.0011,
      "step": 234000
    },
    {
      "epoch": 7.800333333333334,
      "grad_norm": 0.11462265998125076,
      "learning_rate": 1.2479166666666666e-06,
      "loss": 0.0015,
      "step": 234010
    },
    {
      "epoch": 7.800666666666666,
      "grad_norm": 0.2867880165576935,
      "learning_rate": 1.2458333333333334e-06,
      "loss": 0.0014,
      "step": 234020
    },
    {
      "epoch": 7.801,
      "grad_norm": 0.284916490316391,
      "learning_rate": 1.2437500000000002e-06,
      "loss": 0.0018,
      "step": 234030
    },
    {
      "epoch": 7.801333333333333,
      "grad_norm": 0.016977358609437943,
      "learning_rate": 1.2416666666666667e-06,
      "loss": 0.0012,
      "step": 234040
    },
    {
      "epoch": 7.801666666666667,
      "grad_norm": 0.2284165769815445,
      "learning_rate": 1.2395833333333335e-06,
      "loss": 0.0021,
      "step": 234050
    },
    {
      "epoch": 7.802,
      "grad_norm": 0.08640498667955399,
      "learning_rate": 1.2375000000000002e-06,
      "loss": 0.0016,
      "step": 234060
    },
    {
      "epoch": 7.802333333333333,
      "grad_norm": 0.2578382194042206,
      "learning_rate": 1.2354166666666668e-06,
      "loss": 0.0013,
      "step": 234070
    },
    {
      "epoch": 7.802666666666667,
      "grad_norm": 0.22971679270267487,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0014,
      "step": 234080
    },
    {
      "epoch": 7.803,
      "grad_norm": 0.17233771085739136,
      "learning_rate": 1.23125e-06,
      "loss": 0.0015,
      "step": 234090
    },
    {
      "epoch": 7.803333333333334,
      "grad_norm": 0.39903467893600464,
      "learning_rate": 1.2291666666666666e-06,
      "loss": 0.0014,
      "step": 234100
    },
    {
      "epoch": 7.8036666666666665,
      "grad_norm": 0.22834055125713348,
      "learning_rate": 1.2270833333333334e-06,
      "loss": 0.0021,
      "step": 234110
    },
    {
      "epoch": 7.804,
      "grad_norm": 0.19944234192371368,
      "learning_rate": 1.2250000000000001e-06,
      "loss": 0.0023,
      "step": 234120
    },
    {
      "epoch": 7.804333333333333,
      "grad_norm": 0.5628665089607239,
      "learning_rate": 1.2229166666666667e-06,
      "loss": 0.0021,
      "step": 234130
    },
    {
      "epoch": 7.804666666666667,
      "grad_norm": 0.11552377790212631,
      "learning_rate": 1.2208333333333334e-06,
      "loss": 0.0016,
      "step": 234140
    },
    {
      "epoch": 7.805,
      "grad_norm": 0.34242695569992065,
      "learning_rate": 1.2187500000000002e-06,
      "loss": 0.0012,
      "step": 234150
    },
    {
      "epoch": 7.8053333333333335,
      "grad_norm": 0.31077811121940613,
      "learning_rate": 1.2166666666666667e-06,
      "loss": 0.0013,
      "step": 234160
    },
    {
      "epoch": 7.805666666666666,
      "grad_norm": 0.14316962659358978,
      "learning_rate": 1.2145833333333333e-06,
      "loss": 0.0016,
      "step": 234170
    },
    {
      "epoch": 7.806,
      "grad_norm": 0.14417505264282227,
      "learning_rate": 1.2125e-06,
      "loss": 0.0016,
      "step": 234180
    },
    {
      "epoch": 7.806333333333333,
      "grad_norm": 0.1726287454366684,
      "learning_rate": 1.2104166666666666e-06,
      "loss": 0.0018,
      "step": 234190
    },
    {
      "epoch": 7.806666666666667,
      "grad_norm": 0.0578259639441967,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 0.0015,
      "step": 234200
    },
    {
      "epoch": 7.807,
      "grad_norm": 0.229736790060997,
      "learning_rate": 1.20625e-06,
      "loss": 0.0016,
      "step": 234210
    },
    {
      "epoch": 7.807333333333333,
      "grad_norm": 0.11609898507595062,
      "learning_rate": 1.2041666666666669e-06,
      "loss": 0.0017,
      "step": 234220
    },
    {
      "epoch": 7.807666666666667,
      "grad_norm": 0.1431472897529602,
      "learning_rate": 1.2020833333333334e-06,
      "loss": 0.0019,
      "step": 234230
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.031028801575303078,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0018,
      "step": 234240
    },
    {
      "epoch": 7.808333333333334,
      "grad_norm": 0.15889227390289307,
      "learning_rate": 1.1979166666666667e-06,
      "loss": 0.0019,
      "step": 234250
    },
    {
      "epoch": 7.808666666666666,
      "grad_norm": 0.09751807153224945,
      "learning_rate": 1.1958333333333332e-06,
      "loss": 0.002,
      "step": 234260
    },
    {
      "epoch": 7.809,
      "grad_norm": 0.1436111479997635,
      "learning_rate": 1.19375e-06,
      "loss": 0.0018,
      "step": 234270
    },
    {
      "epoch": 7.809333333333333,
      "grad_norm": 0.02927999757230282,
      "learning_rate": 1.1916666666666668e-06,
      "loss": 0.0015,
      "step": 234280
    },
    {
      "epoch": 7.809666666666667,
      "grad_norm": 0.4023217558860779,
      "learning_rate": 1.1895833333333333e-06,
      "loss": 0.0013,
      "step": 234290
    },
    {
      "epoch": 7.8100000000000005,
      "grad_norm": 0.05809001252055168,
      "learning_rate": 1.1875e-06,
      "loss": 0.0014,
      "step": 234300
    },
    {
      "epoch": 7.810333333333333,
      "grad_norm": 0.34214991331100464,
      "learning_rate": 1.1854166666666668e-06,
      "loss": 0.0014,
      "step": 234310
    },
    {
      "epoch": 7.810666666666666,
      "grad_norm": 0.012248038314282894,
      "learning_rate": 1.1833333333333334e-06,
      "loss": 0.0013,
      "step": 234320
    },
    {
      "epoch": 7.811,
      "grad_norm": 0.2694854140281677,
      "learning_rate": 1.1812500000000001e-06,
      "loss": 0.0014,
      "step": 234330
    },
    {
      "epoch": 7.811333333333334,
      "grad_norm": 0.007506595458835363,
      "learning_rate": 1.1791666666666669e-06,
      "loss": 0.0013,
      "step": 234340
    },
    {
      "epoch": 7.8116666666666665,
      "grad_norm": 0.2581879794597626,
      "learning_rate": 1.1770833333333334e-06,
      "loss": 0.0013,
      "step": 234350
    },
    {
      "epoch": 7.812,
      "grad_norm": 0.0390213243663311,
      "learning_rate": 1.175e-06,
      "loss": 0.0015,
      "step": 234360
    },
    {
      "epoch": 7.812333333333333,
      "grad_norm": 0.32174667716026306,
      "learning_rate": 1.1729166666666667e-06,
      "loss": 0.0017,
      "step": 234370
    },
    {
      "epoch": 7.812666666666667,
      "grad_norm": 0.2000749111175537,
      "learning_rate": 1.1708333333333333e-06,
      "loss": 0.0013,
      "step": 234380
    },
    {
      "epoch": 7.813,
      "grad_norm": 0.5987867712974548,
      "learning_rate": 1.16875e-06,
      "loss": 0.0014,
      "step": 234390
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.0863952562212944,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0012,
      "step": 234400
    },
    {
      "epoch": 7.813666666666666,
      "grad_norm": 0.19995398819446564,
      "learning_rate": 1.1645833333333333e-06,
      "loss": 0.0014,
      "step": 234410
    },
    {
      "epoch": 7.814,
      "grad_norm": 0.2855134904384613,
      "learning_rate": 1.1625e-06,
      "loss": 0.0017,
      "step": 234420
    },
    {
      "epoch": 7.814333333333334,
      "grad_norm": 0.1146545261144638,
      "learning_rate": 1.1604166666666669e-06,
      "loss": 0.0016,
      "step": 234430
    },
    {
      "epoch": 7.814666666666667,
      "grad_norm": 0.11650615930557251,
      "learning_rate": 1.1583333333333334e-06,
      "loss": 0.0025,
      "step": 234440
    },
    {
      "epoch": 7.8149999999999995,
      "grad_norm": 0.17221815884113312,
      "learning_rate": 1.15625e-06,
      "loss": 0.0024,
      "step": 234450
    },
    {
      "epoch": 7.815333333333333,
      "grad_norm": 0.15679647028446198,
      "learning_rate": 1.1541666666666667e-06,
      "loss": 0.0018,
      "step": 234460
    },
    {
      "epoch": 7.815666666666667,
      "grad_norm": 0.2566213309764862,
      "learning_rate": 1.1520833333333332e-06,
      "loss": 0.0019,
      "step": 234470
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.15554147958755493,
      "learning_rate": 1.15e-06,
      "loss": 0.002,
      "step": 234480
    },
    {
      "epoch": 7.816333333333334,
      "grad_norm": 0.2287277728319168,
      "learning_rate": 1.1479166666666668e-06,
      "loss": 0.0012,
      "step": 234490
    },
    {
      "epoch": 7.816666666666666,
      "grad_norm": 0.11456210911273956,
      "learning_rate": 1.1458333333333333e-06,
      "loss": 0.0022,
      "step": 234500
    },
    {
      "epoch": 7.817,
      "grad_norm": 0.3295734226703644,
      "learning_rate": 1.14375e-06,
      "loss": 0.002,
      "step": 234510
    },
    {
      "epoch": 7.817333333333333,
      "grad_norm": 0.14238296449184418,
      "learning_rate": 1.1416666666666668e-06,
      "loss": 0.0016,
      "step": 234520
    },
    {
      "epoch": 7.817666666666667,
      "grad_norm": 0.20011483132839203,
      "learning_rate": 1.1395833333333334e-06,
      "loss": 0.0018,
      "step": 234530
    },
    {
      "epoch": 7.818,
      "grad_norm": 0.03367861732840538,
      "learning_rate": 1.1375000000000001e-06,
      "loss": 0.0023,
      "step": 234540
    },
    {
      "epoch": 7.818333333333333,
      "grad_norm": 0.03464580327272415,
      "learning_rate": 1.1354166666666667e-06,
      "loss": 0.0023,
      "step": 234550
    },
    {
      "epoch": 7.818666666666667,
      "grad_norm": 0.2566642463207245,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0014,
      "step": 234560
    },
    {
      "epoch": 7.819,
      "grad_norm": 0.28569507598876953,
      "learning_rate": 1.13125e-06,
      "loss": 0.0015,
      "step": 234570
    },
    {
      "epoch": 7.819333333333334,
      "grad_norm": 0.08647297322750092,
      "learning_rate": 1.1291666666666667e-06,
      "loss": 0.0013,
      "step": 234580
    },
    {
      "epoch": 7.8196666666666665,
      "grad_norm": 0.08603569120168686,
      "learning_rate": 1.1270833333333335e-06,
      "loss": 0.0016,
      "step": 234590
    },
    {
      "epoch": 7.82,
      "grad_norm": 0.08805657178163528,
      "learning_rate": 1.125e-06,
      "loss": 0.0014,
      "step": 234600
    },
    {
      "epoch": 7.820333333333333,
      "grad_norm": 0.43955978751182556,
      "learning_rate": 1.1229166666666668e-06,
      "loss": 0.0022,
      "step": 234610
    },
    {
      "epoch": 7.820666666666667,
      "grad_norm": 0.010577701032161713,
      "learning_rate": 1.1208333333333335e-06,
      "loss": 0.0014,
      "step": 234620
    },
    {
      "epoch": 7.821,
      "grad_norm": 0.11621955037117004,
      "learning_rate": 1.11875e-06,
      "loss": 0.0019,
      "step": 234630
    },
    {
      "epoch": 7.8213333333333335,
      "grad_norm": 0.11577314883470535,
      "learning_rate": 1.1166666666666666e-06,
      "loss": 0.0012,
      "step": 234640
    },
    {
      "epoch": 7.821666666666666,
      "grad_norm": 0.31136515736579895,
      "learning_rate": 1.1145833333333334e-06,
      "loss": 0.0032,
      "step": 234650
    },
    {
      "epoch": 7.822,
      "grad_norm": 0.22840608656406403,
      "learning_rate": 1.1125e-06,
      "loss": 0.0016,
      "step": 234660
    },
    {
      "epoch": 7.822333333333333,
      "grad_norm": 0.37111425399780273,
      "learning_rate": 1.1104166666666667e-06,
      "loss": 0.0015,
      "step": 234670
    },
    {
      "epoch": 7.822666666666667,
      "grad_norm": 0.20041336119174957,
      "learning_rate": 1.1083333333333335e-06,
      "loss": 0.0014,
      "step": 234680
    },
    {
      "epoch": 7.823,
      "grad_norm": 0.20011919736862183,
      "learning_rate": 1.10625e-06,
      "loss": 0.0016,
      "step": 234690
    },
    {
      "epoch": 7.823333333333333,
      "grad_norm": 0.03582300990819931,
      "learning_rate": 1.1041666666666668e-06,
      "loss": 0.002,
      "step": 234700
    },
    {
      "epoch": 7.823666666666667,
      "grad_norm": 0.11762291938066483,
      "learning_rate": 1.1020833333333335e-06,
      "loss": 0.0014,
      "step": 234710
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.41928836703300476,
      "learning_rate": 1.1e-06,
      "loss": 0.0016,
      "step": 234720
    },
    {
      "epoch": 7.824333333333334,
      "grad_norm": 0.06112136319279671,
      "learning_rate": 1.0979166666666668e-06,
      "loss": 0.002,
      "step": 234730
    },
    {
      "epoch": 7.824666666666666,
      "grad_norm": 0.0709274411201477,
      "learning_rate": 1.0958333333333334e-06,
      "loss": 0.0027,
      "step": 234740
    },
    {
      "epoch": 7.825,
      "grad_norm": 0.2852143347263336,
      "learning_rate": 1.09375e-06,
      "loss": 0.0024,
      "step": 234750
    },
    {
      "epoch": 7.825333333333333,
      "grad_norm": 0.2786320447921753,
      "learning_rate": 1.0916666666666667e-06,
      "loss": 0.0019,
      "step": 234760
    },
    {
      "epoch": 7.825666666666667,
      "grad_norm": 0.17150083184242249,
      "learning_rate": 1.0895833333333334e-06,
      "loss": 0.0019,
      "step": 234770
    },
    {
      "epoch": 7.826,
      "grad_norm": 0.25103136897087097,
      "learning_rate": 1.0875e-06,
      "loss": 0.002,
      "step": 234780
    },
    {
      "epoch": 7.826333333333333,
      "grad_norm": 0.28614816069602966,
      "learning_rate": 1.0854166666666667e-06,
      "loss": 0.0011,
      "step": 234790
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 0.19988225400447845,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.0022,
      "step": 234800
    },
    {
      "epoch": 7.827,
      "grad_norm": 0.03033105470240116,
      "learning_rate": 1.08125e-06,
      "loss": 0.0014,
      "step": 234810
    },
    {
      "epoch": 7.827333333333334,
      "grad_norm": 0.02604733780026436,
      "learning_rate": 1.0791666666666668e-06,
      "loss": 0.0018,
      "step": 234820
    },
    {
      "epoch": 7.8276666666666666,
      "grad_norm": 0.08790166676044464,
      "learning_rate": 1.0770833333333333e-06,
      "loss": 0.0016,
      "step": 234830
    },
    {
      "epoch": 7.828,
      "grad_norm": 0.08831667900085449,
      "learning_rate": 1.0749999999999999e-06,
      "loss": 0.0026,
      "step": 234840
    },
    {
      "epoch": 7.828333333333333,
      "grad_norm": 0.4747660160064697,
      "learning_rate": 1.0729166666666666e-06,
      "loss": 0.0015,
      "step": 234850
    },
    {
      "epoch": 7.828666666666667,
      "grad_norm": 0.04159987345337868,
      "learning_rate": 1.0708333333333334e-06,
      "loss": 0.0024,
      "step": 234860
    },
    {
      "epoch": 7.829,
      "grad_norm": 0.31345272064208984,
      "learning_rate": 1.0687500000000001e-06,
      "loss": 0.0013,
      "step": 234870
    },
    {
      "epoch": 7.8293333333333335,
      "grad_norm": 0.2854347825050354,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0019,
      "step": 234880
    },
    {
      "epoch": 7.829666666666666,
      "grad_norm": 0.39986270666122437,
      "learning_rate": 1.0645833333333334e-06,
      "loss": 0.0017,
      "step": 234890
    },
    {
      "epoch": 7.83,
      "grad_norm": 0.17187613248825073,
      "learning_rate": 1.0625000000000002e-06,
      "loss": 0.0019,
      "step": 234900
    },
    {
      "epoch": 7.830333333333334,
      "grad_norm": 0.08981937915086746,
      "learning_rate": 1.0604166666666667e-06,
      "loss": 0.0015,
      "step": 234910
    },
    {
      "epoch": 7.830666666666667,
      "grad_norm": 0.1329626590013504,
      "learning_rate": 1.0583333333333333e-06,
      "loss": 0.0015,
      "step": 234920
    },
    {
      "epoch": 7.8309999999999995,
      "grad_norm": 0.23260197043418884,
      "learning_rate": 1.05625e-06,
      "loss": 0.003,
      "step": 234930
    },
    {
      "epoch": 7.831333333333333,
      "grad_norm": 0.1441488116979599,
      "learning_rate": 1.0541666666666666e-06,
      "loss": 0.0018,
      "step": 234940
    },
    {
      "epoch": 7.831666666666667,
      "grad_norm": 0.06178651750087738,
      "learning_rate": 1.0520833333333334e-06,
      "loss": 0.0013,
      "step": 234950
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.08605001866817474,
      "learning_rate": 1.0500000000000001e-06,
      "loss": 0.0018,
      "step": 234960
    },
    {
      "epoch": 7.832333333333334,
      "grad_norm": 0.24082736670970917,
      "learning_rate": 1.0479166666666667e-06,
      "loss": 0.0014,
      "step": 234970
    },
    {
      "epoch": 7.832666666666666,
      "grad_norm": 0.012855983339250088,
      "learning_rate": 1.0458333333333334e-06,
      "loss": 0.0016,
      "step": 234980
    },
    {
      "epoch": 7.833,
      "grad_norm": 0.3142593801021576,
      "learning_rate": 1.0437500000000002e-06,
      "loss": 0.0012,
      "step": 234990
    },
    {
      "epoch": 7.833333333333333,
      "grad_norm": 0.2850417494773865,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 0.0015,
      "step": 235000
    },
    {
      "epoch": 7.833666666666667,
      "grad_norm": 0.05831575393676758,
      "learning_rate": 1.0395833333333335e-06,
      "loss": 0.0012,
      "step": 235010
    },
    {
      "epoch": 7.834,
      "grad_norm": 0.035881415009498596,
      "learning_rate": 1.0375e-06,
      "loss": 0.0014,
      "step": 235020
    },
    {
      "epoch": 7.834333333333333,
      "grad_norm": 0.02149280533194542,
      "learning_rate": 1.0354166666666666e-06,
      "loss": 0.0019,
      "step": 235030
    },
    {
      "epoch": 7.834666666666667,
      "grad_norm": 0.14312288165092468,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.0022,
      "step": 235040
    },
    {
      "epoch": 7.835,
      "grad_norm": 0.14328739047050476,
      "learning_rate": 1.03125e-06,
      "loss": 0.0011,
      "step": 235050
    },
    {
      "epoch": 7.835333333333334,
      "grad_norm": 0.14280225336551666,
      "learning_rate": 1.0291666666666666e-06,
      "loss": 0.0016,
      "step": 235060
    },
    {
      "epoch": 7.835666666666667,
      "grad_norm": 0.033569611608982086,
      "learning_rate": 1.0270833333333334e-06,
      "loss": 0.001,
      "step": 235070
    },
    {
      "epoch": 7.836,
      "grad_norm": 0.3718949854373932,
      "learning_rate": 1.0250000000000001e-06,
      "loss": 0.0014,
      "step": 235080
    },
    {
      "epoch": 7.836333333333333,
      "grad_norm": 0.008127597160637379,
      "learning_rate": 1.0229166666666667e-06,
      "loss": 0.0019,
      "step": 235090
    },
    {
      "epoch": 7.836666666666667,
      "grad_norm": 0.2001107782125473,
      "learning_rate": 1.0208333333333334e-06,
      "loss": 0.0015,
      "step": 235100
    },
    {
      "epoch": 7.837,
      "grad_norm": 0.37080299854278564,
      "learning_rate": 1.01875e-06,
      "loss": 0.0017,
      "step": 235110
    },
    {
      "epoch": 7.8373333333333335,
      "grad_norm": 0.03686690330505371,
      "learning_rate": 1.0166666666666665e-06,
      "loss": 0.0017,
      "step": 235120
    },
    {
      "epoch": 7.837666666666666,
      "grad_norm": 0.4000651240348816,
      "learning_rate": 1.0145833333333333e-06,
      "loss": 0.0026,
      "step": 235130
    },
    {
      "epoch": 7.838,
      "grad_norm": 0.1726691871881485,
      "learning_rate": 1.0125e-06,
      "loss": 0.0012,
      "step": 235140
    },
    {
      "epoch": 7.838333333333333,
      "grad_norm": 0.36976081132888794,
      "learning_rate": 1.0104166666666666e-06,
      "loss": 0.0021,
      "step": 235150
    },
    {
      "epoch": 7.838666666666667,
      "grad_norm": 0.058517519384622574,
      "learning_rate": 1.0083333333333333e-06,
      "loss": 0.0022,
      "step": 235160
    },
    {
      "epoch": 7.839,
      "grad_norm": 0.06394437700510025,
      "learning_rate": 1.0062500000000001e-06,
      "loss": 0.0016,
      "step": 235170
    },
    {
      "epoch": 7.839333333333333,
      "grad_norm": 0.0905318483710289,
      "learning_rate": 1.0041666666666669e-06,
      "loss": 0.0019,
      "step": 235180
    },
    {
      "epoch": 7.839666666666667,
      "grad_norm": 0.029830703511834145,
      "learning_rate": 1.0020833333333334e-06,
      "loss": 0.0017,
      "step": 235190
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.11554095149040222,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0018,
      "step": 235200
    },
    {
      "epoch": 7.840333333333334,
      "grad_norm": 0.1431034803390503,
      "learning_rate": 9.979166666666667e-07,
      "loss": 0.0012,
      "step": 235210
    },
    {
      "epoch": 7.8406666666666665,
      "grad_norm": 0.08723405003547668,
      "learning_rate": 9.958333333333333e-07,
      "loss": 0.0013,
      "step": 235220
    },
    {
      "epoch": 7.841,
      "grad_norm": 0.14406868815422058,
      "learning_rate": 9.9375e-07,
      "loss": 0.0015,
      "step": 235230
    },
    {
      "epoch": 7.841333333333333,
      "grad_norm": 0.08749059587717056,
      "learning_rate": 9.916666666666668e-07,
      "loss": 0.0017,
      "step": 235240
    },
    {
      "epoch": 7.841666666666667,
      "grad_norm": 0.015493471175432205,
      "learning_rate": 9.895833333333333e-07,
      "loss": 0.0017,
      "step": 235250
    },
    {
      "epoch": 7.842,
      "grad_norm": 0.293753057718277,
      "learning_rate": 9.875e-07,
      "loss": 0.0019,
      "step": 235260
    },
    {
      "epoch": 7.842333333333333,
      "grad_norm": 0.0299870353192091,
      "learning_rate": 9.854166666666668e-07,
      "loss": 0.0019,
      "step": 235270
    },
    {
      "epoch": 7.842666666666666,
      "grad_norm": 0.2851102352142334,
      "learning_rate": 9.833333333333334e-07,
      "loss": 0.0011,
      "step": 235280
    },
    {
      "epoch": 7.843,
      "grad_norm": 0.19953608512878418,
      "learning_rate": 9.812500000000001e-07,
      "loss": 0.0015,
      "step": 235290
    },
    {
      "epoch": 7.843333333333334,
      "grad_norm": 0.0313543826341629,
      "learning_rate": 9.791666666666667e-07,
      "loss": 0.0016,
      "step": 235300
    },
    {
      "epoch": 7.843666666666667,
      "grad_norm": 0.029630104079842567,
      "learning_rate": 9.770833333333332e-07,
      "loss": 0.0012,
      "step": 235310
    },
    {
      "epoch": 7.844,
      "grad_norm": 0.0859498530626297,
      "learning_rate": 9.75e-07,
      "loss": 0.0018,
      "step": 235320
    },
    {
      "epoch": 7.844333333333333,
      "grad_norm": 0.4276142120361328,
      "learning_rate": 9.729166666666667e-07,
      "loss": 0.0022,
      "step": 235330
    },
    {
      "epoch": 7.844666666666667,
      "grad_norm": 0.030786855146288872,
      "learning_rate": 9.708333333333333e-07,
      "loss": 0.0016,
      "step": 235340
    },
    {
      "epoch": 7.845,
      "grad_norm": 0.14294999837875366,
      "learning_rate": 9.6875e-07,
      "loss": 0.0015,
      "step": 235350
    },
    {
      "epoch": 7.8453333333333335,
      "grad_norm": 0.1452292799949646,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.0016,
      "step": 235360
    },
    {
      "epoch": 7.845666666666666,
      "grad_norm": 0.3156745731830597,
      "learning_rate": 9.645833333333333e-07,
      "loss": 0.0015,
      "step": 235370
    },
    {
      "epoch": 7.846,
      "grad_norm": 0.14306262135505676,
      "learning_rate": 9.625e-07,
      "loss": 0.0021,
      "step": 235380
    },
    {
      "epoch": 7.846333333333334,
      "grad_norm": 0.013222071342170238,
      "learning_rate": 9.604166666666669e-07,
      "loss": 0.0012,
      "step": 235390
    },
    {
      "epoch": 7.846666666666667,
      "grad_norm": 0.25652840733528137,
      "learning_rate": 9.583333333333334e-07,
      "loss": 0.0013,
      "step": 235400
    },
    {
      "epoch": 7.8469999999999995,
      "grad_norm": 0.06770152598619461,
      "learning_rate": 9.5625e-07,
      "loss": 0.0016,
      "step": 235410
    },
    {
      "epoch": 7.847333333333333,
      "grad_norm": 0.2860372066497803,
      "learning_rate": 9.541666666666667e-07,
      "loss": 0.0019,
      "step": 235420
    },
    {
      "epoch": 7.847666666666667,
      "grad_norm": 0.25675976276397705,
      "learning_rate": 9.520833333333333e-07,
      "loss": 0.0019,
      "step": 235430
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.044756367802619934,
      "learning_rate": 9.5e-07,
      "loss": 0.0013,
      "step": 235440
    },
    {
      "epoch": 7.848333333333334,
      "grad_norm": 0.031175408512353897,
      "learning_rate": 9.479166666666668e-07,
      "loss": 0.0014,
      "step": 235450
    },
    {
      "epoch": 7.8486666666666665,
      "grad_norm": 0.3992452621459961,
      "learning_rate": 9.458333333333333e-07,
      "loss": 0.0012,
      "step": 235460
    },
    {
      "epoch": 7.849,
      "grad_norm": 0.17113928496837616,
      "learning_rate": 9.4375e-07,
      "loss": 0.0016,
      "step": 235470
    },
    {
      "epoch": 7.849333333333333,
      "grad_norm": 0.03345338627696037,
      "learning_rate": 9.416666666666667e-07,
      "loss": 0.0016,
      "step": 235480
    },
    {
      "epoch": 7.849666666666667,
      "grad_norm": 0.20016780495643616,
      "learning_rate": 9.395833333333335e-07,
      "loss": 0.0017,
      "step": 235490
    },
    {
      "epoch": 7.85,
      "grad_norm": 0.030428346246480942,
      "learning_rate": 9.375e-07,
      "loss": 0.002,
      "step": 235500
    },
    {
      "epoch": 7.850333333333333,
      "grad_norm": 0.05783265084028244,
      "learning_rate": 9.354166666666668e-07,
      "loss": 0.0014,
      "step": 235510
    },
    {
      "epoch": 7.850666666666667,
      "grad_norm": 0.2282782793045044,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0016,
      "step": 235520
    },
    {
      "epoch": 7.851,
      "grad_norm": 0.11481373757123947,
      "learning_rate": 9.3125e-07,
      "loss": 0.0013,
      "step": 235530
    },
    {
      "epoch": 7.851333333333334,
      "grad_norm": 0.1428179293870926,
      "learning_rate": 9.291666666666667e-07,
      "loss": 0.0021,
      "step": 235540
    },
    {
      "epoch": 7.851666666666667,
      "grad_norm": 0.22829432785511017,
      "learning_rate": 9.270833333333335e-07,
      "loss": 0.002,
      "step": 235550
    },
    {
      "epoch": 7.852,
      "grad_norm": 0.08616144210100174,
      "learning_rate": 9.25e-07,
      "loss": 0.001,
      "step": 235560
    },
    {
      "epoch": 7.852333333333333,
      "grad_norm": 0.11473629623651505,
      "learning_rate": 9.229166666666667e-07,
      "loss": 0.0013,
      "step": 235570
    },
    {
      "epoch": 7.852666666666667,
      "grad_norm": 0.1150919571518898,
      "learning_rate": 9.208333333333334e-07,
      "loss": 0.0017,
      "step": 235580
    },
    {
      "epoch": 7.853,
      "grad_norm": 0.11414234340190887,
      "learning_rate": 9.1875e-07,
      "loss": 0.0022,
      "step": 235590
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.0297780130058527,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.0016,
      "step": 235600
    },
    {
      "epoch": 7.853666666666666,
      "grad_norm": 0.2571476995944977,
      "learning_rate": 9.145833333333334e-07,
      "loss": 0.0013,
      "step": 235610
    },
    {
      "epoch": 7.854,
      "grad_norm": 0.2643643319606781,
      "learning_rate": 9.124999999999999e-07,
      "loss": 0.002,
      "step": 235620
    },
    {
      "epoch": 7.854333333333333,
      "grad_norm": 0.20101018249988556,
      "learning_rate": 9.104166666666667e-07,
      "loss": 0.0017,
      "step": 235630
    },
    {
      "epoch": 7.854666666666667,
      "grad_norm": 0.030360544100403786,
      "learning_rate": 9.083333333333335e-07,
      "loss": 0.002,
      "step": 235640
    },
    {
      "epoch": 7.855,
      "grad_norm": 0.22956408560276031,
      "learning_rate": 9.0625e-07,
      "loss": 0.0011,
      "step": 235650
    },
    {
      "epoch": 7.855333333333333,
      "grad_norm": 0.032825835049152374,
      "learning_rate": 9.041666666666667e-07,
      "loss": 0.0018,
      "step": 235660
    },
    {
      "epoch": 7.855666666666667,
      "grad_norm": 0.171617329120636,
      "learning_rate": 9.020833333333334e-07,
      "loss": 0.0014,
      "step": 235670
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.2014254778623581,
      "learning_rate": 9e-07,
      "loss": 0.0016,
      "step": 235680
    },
    {
      "epoch": 7.856333333333334,
      "grad_norm": 0.11471160501241684,
      "learning_rate": 8.979166666666667e-07,
      "loss": 0.0016,
      "step": 235690
    },
    {
      "epoch": 7.8566666666666665,
      "grad_norm": 0.22805571556091309,
      "learning_rate": 8.958333333333334e-07,
      "loss": 0.0019,
      "step": 235700
    },
    {
      "epoch": 7.857,
      "grad_norm": 0.5702197551727295,
      "learning_rate": 8.9375e-07,
      "loss": 0.0012,
      "step": 235710
    },
    {
      "epoch": 7.857333333333333,
      "grad_norm": 0.42754703760147095,
      "learning_rate": 8.916666666666667e-07,
      "loss": 0.0022,
      "step": 235720
    },
    {
      "epoch": 7.857666666666667,
      "grad_norm": 0.1717361956834793,
      "learning_rate": 8.895833333333334e-07,
      "loss": 0.0017,
      "step": 235730
    },
    {
      "epoch": 7.858,
      "grad_norm": 0.11515387892723083,
      "learning_rate": 8.875e-07,
      "loss": 0.0018,
      "step": 235740
    },
    {
      "epoch": 7.858333333333333,
      "grad_norm": 0.06231709197163582,
      "learning_rate": 8.854166666666667e-07,
      "loss": 0.0013,
      "step": 235750
    },
    {
      "epoch": 7.858666666666666,
      "grad_norm": 0.20072200894355774,
      "learning_rate": 8.833333333333334e-07,
      "loss": 0.0022,
      "step": 235760
    },
    {
      "epoch": 7.859,
      "grad_norm": 0.029856299981474876,
      "learning_rate": 8.812499999999999e-07,
      "loss": 0.0013,
      "step": 235770
    },
    {
      "epoch": 7.859333333333334,
      "grad_norm": 0.029816286638379097,
      "learning_rate": 8.791666666666667e-07,
      "loss": 0.0018,
      "step": 235780
    },
    {
      "epoch": 7.859666666666667,
      "grad_norm": 0.14276935160160065,
      "learning_rate": 8.770833333333334e-07,
      "loss": 0.0022,
      "step": 235790
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.20112672448158264,
      "learning_rate": 8.750000000000001e-07,
      "loss": 0.0023,
      "step": 235800
    },
    {
      "epoch": 7.860333333333333,
      "grad_norm": 0.035048093646764755,
      "learning_rate": 8.729166666666666e-07,
      "loss": 0.0018,
      "step": 235810
    },
    {
      "epoch": 7.860666666666667,
      "grad_norm": 0.0593893863260746,
      "learning_rate": 8.708333333333334e-07,
      "loss": 0.0021,
      "step": 235820
    },
    {
      "epoch": 7.861,
      "grad_norm": 0.04659058898687363,
      "learning_rate": 8.687500000000002e-07,
      "loss": 0.0015,
      "step": 235830
    },
    {
      "epoch": 7.8613333333333335,
      "grad_norm": 0.37073421478271484,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0015,
      "step": 235840
    },
    {
      "epoch": 7.861666666666666,
      "grad_norm": 0.19971664249897003,
      "learning_rate": 8.645833333333333e-07,
      "loss": 0.0011,
      "step": 235850
    },
    {
      "epoch": 7.862,
      "grad_norm": 0.03116869181394577,
      "learning_rate": 8.625000000000001e-07,
      "loss": 0.0018,
      "step": 235860
    },
    {
      "epoch": 7.862333333333333,
      "grad_norm": 0.21356482803821564,
      "learning_rate": 8.604166666666667e-07,
      "loss": 0.0023,
      "step": 235870
    },
    {
      "epoch": 7.862666666666667,
      "grad_norm": 0.11438076943159103,
      "learning_rate": 8.583333333333334e-07,
      "loss": 0.0012,
      "step": 235880
    },
    {
      "epoch": 7.8629999999999995,
      "grad_norm": 0.31436073780059814,
      "learning_rate": 8.562500000000001e-07,
      "loss": 0.0013,
      "step": 235890
    },
    {
      "epoch": 7.863333333333333,
      "grad_norm": 0.07034751772880554,
      "learning_rate": 8.541666666666666e-07,
      "loss": 0.0018,
      "step": 235900
    },
    {
      "epoch": 7.863666666666667,
      "grad_norm": 0.11418599635362625,
      "learning_rate": 8.520833333333334e-07,
      "loss": 0.0015,
      "step": 235910
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.32073742151260376,
      "learning_rate": 8.500000000000001e-07,
      "loss": 0.0015,
      "step": 235920
    },
    {
      "epoch": 7.864333333333334,
      "grad_norm": 0.11513672024011612,
      "learning_rate": 8.479166666666667e-07,
      "loss": 0.0024,
      "step": 235930
    },
    {
      "epoch": 7.8646666666666665,
      "grad_norm": 0.08621932566165924,
      "learning_rate": 8.458333333333334e-07,
      "loss": 0.0013,
      "step": 235940
    },
    {
      "epoch": 7.865,
      "grad_norm": 0.017772438004612923,
      "learning_rate": 8.437500000000001e-07,
      "loss": 0.0012,
      "step": 235950
    },
    {
      "epoch": 7.865333333333333,
      "grad_norm": 0.370815247297287,
      "learning_rate": 8.416666666666666e-07,
      "loss": 0.0011,
      "step": 235960
    },
    {
      "epoch": 7.865666666666667,
      "grad_norm": 0.2571687400341034,
      "learning_rate": 8.395833333333334e-07,
      "loss": 0.0017,
      "step": 235970
    },
    {
      "epoch": 7.866,
      "grad_norm": 0.1436767280101776,
      "learning_rate": 8.375000000000001e-07,
      "loss": 0.0016,
      "step": 235980
    },
    {
      "epoch": 7.866333333333333,
      "grad_norm": 0.02958684042096138,
      "learning_rate": 8.354166666666667e-07,
      "loss": 0.0015,
      "step": 235990
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.11571087688207626,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0025,
      "step": 236000
    },
    {
      "epoch": 7.867,
      "grad_norm": 0.08629699796438217,
      "learning_rate": 8.312500000000001e-07,
      "loss": 0.002,
      "step": 236010
    },
    {
      "epoch": 7.867333333333333,
      "grad_norm": 0.0436907634139061,
      "learning_rate": 8.291666666666666e-07,
      "loss": 0.0019,
      "step": 236020
    },
    {
      "epoch": 7.867666666666667,
      "grad_norm": 0.031142408028244972,
      "learning_rate": 8.270833333333334e-07,
      "loss": 0.0015,
      "step": 236030
    },
    {
      "epoch": 7.868,
      "grad_norm": 0.088313989341259,
      "learning_rate": 8.25e-07,
      "loss": 0.0017,
      "step": 236040
    },
    {
      "epoch": 7.868333333333333,
      "grad_norm": 0.11433359235525131,
      "learning_rate": 8.229166666666666e-07,
      "loss": 0.002,
      "step": 236050
    },
    {
      "epoch": 7.868666666666667,
      "grad_norm": 0.19809414446353912,
      "learning_rate": 8.208333333333333e-07,
      "loss": 0.0018,
      "step": 236060
    },
    {
      "epoch": 7.869,
      "grad_norm": 0.17244209349155426,
      "learning_rate": 8.187500000000001e-07,
      "loss": 0.0012,
      "step": 236070
    },
    {
      "epoch": 7.8693333333333335,
      "grad_norm": 0.08543839305639267,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0015,
      "step": 236080
    },
    {
      "epoch": 7.869666666666666,
      "grad_norm": 0.08637642115354538,
      "learning_rate": 8.145833333333333e-07,
      "loss": 0.0018,
      "step": 236090
    },
    {
      "epoch": 7.87,
      "grad_norm": 0.05766427144408226,
      "learning_rate": 8.125000000000001e-07,
      "loss": 0.0019,
      "step": 236100
    },
    {
      "epoch": 7.870333333333333,
      "grad_norm": 0.030257785692811012,
      "learning_rate": 8.104166666666668e-07,
      "loss": 0.0015,
      "step": 236110
    },
    {
      "epoch": 7.870666666666667,
      "grad_norm": 0.08604808151721954,
      "learning_rate": 8.083333333333334e-07,
      "loss": 0.0018,
      "step": 236120
    },
    {
      "epoch": 7.871,
      "grad_norm": 0.17145219445228577,
      "learning_rate": 8.0625e-07,
      "loss": 0.0014,
      "step": 236130
    },
    {
      "epoch": 7.871333333333333,
      "grad_norm": 0.06599346548318863,
      "learning_rate": 8.041666666666668e-07,
      "loss": 0.0021,
      "step": 236140
    },
    {
      "epoch": 7.871666666666667,
      "grad_norm": 0.5131941437721252,
      "learning_rate": 8.020833333333333e-07,
      "loss": 0.002,
      "step": 236150
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.019592545926570892,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0019,
      "step": 236160
    },
    {
      "epoch": 7.872333333333334,
      "grad_norm": 0.06388463079929352,
      "learning_rate": 7.979166666666668e-07,
      "loss": 0.0015,
      "step": 236170
    },
    {
      "epoch": 7.8726666666666665,
      "grad_norm": 0.05749189853668213,
      "learning_rate": 7.958333333333334e-07,
      "loss": 0.0019,
      "step": 236180
    },
    {
      "epoch": 7.873,
      "grad_norm": 0.17166441679000854,
      "learning_rate": 7.9375e-07,
      "loss": 0.0013,
      "step": 236190
    },
    {
      "epoch": 7.873333333333333,
      "grad_norm": 0.05012941360473633,
      "learning_rate": 7.916666666666668e-07,
      "loss": 0.0016,
      "step": 236200
    },
    {
      "epoch": 7.873666666666667,
      "grad_norm": 0.20000465214252472,
      "learning_rate": 7.895833333333333e-07,
      "loss": 0.0013,
      "step": 236210
    },
    {
      "epoch": 7.874,
      "grad_norm": 0.22809486091136932,
      "learning_rate": 7.875000000000001e-07,
      "loss": 0.0014,
      "step": 236220
    },
    {
      "epoch": 7.874333333333333,
      "grad_norm": 0.12220156937837601,
      "learning_rate": 7.854166666666667e-07,
      "loss": 0.0016,
      "step": 236230
    },
    {
      "epoch": 7.874666666666666,
      "grad_norm": 0.14315131306648254,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.0015,
      "step": 236240
    },
    {
      "epoch": 7.875,
      "grad_norm": 0.20015858113765717,
      "learning_rate": 7.8125e-07,
      "loss": 0.0012,
      "step": 236250
    },
    {
      "epoch": 7.875333333333334,
      "grad_norm": 0.0578431561589241,
      "learning_rate": 7.791666666666667e-07,
      "loss": 0.0015,
      "step": 236260
    },
    {
      "epoch": 7.875666666666667,
      "grad_norm": 0.030915068462491035,
      "learning_rate": 7.770833333333334e-07,
      "loss": 0.0021,
      "step": 236270
    },
    {
      "epoch": 7.876,
      "grad_norm": 0.05770079046487808,
      "learning_rate": 7.75e-07,
      "loss": 0.0014,
      "step": 236280
    },
    {
      "epoch": 7.876333333333333,
      "grad_norm": 0.059165939688682556,
      "learning_rate": 7.729166666666666e-07,
      "loss": 0.002,
      "step": 236290
    },
    {
      "epoch": 7.876666666666667,
      "grad_norm": 0.037283867597579956,
      "learning_rate": 7.708333333333334e-07,
      "loss": 0.0017,
      "step": 236300
    },
    {
      "epoch": 7.877,
      "grad_norm": 0.11614937335252762,
      "learning_rate": 7.6875e-07,
      "loss": 0.0015,
      "step": 236310
    },
    {
      "epoch": 7.8773333333333335,
      "grad_norm": 0.2671445608139038,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0015,
      "step": 236320
    },
    {
      "epoch": 7.877666666666666,
      "grad_norm": 0.0893259197473526,
      "learning_rate": 7.645833333333334e-07,
      "loss": 0.0018,
      "step": 236330
    },
    {
      "epoch": 7.878,
      "grad_norm": 0.0584796704351902,
      "learning_rate": 7.625e-07,
      "loss": 0.0015,
      "step": 236340
    },
    {
      "epoch": 7.878333333333333,
      "grad_norm": 0.21450914442539215,
      "learning_rate": 7.604166666666668e-07,
      "loss": 0.0017,
      "step": 236350
    },
    {
      "epoch": 7.878666666666667,
      "grad_norm": 0.11542415618896484,
      "learning_rate": 7.583333333333334e-07,
      "loss": 0.0013,
      "step": 236360
    },
    {
      "epoch": 7.879,
      "grad_norm": 0.37118083238601685,
      "learning_rate": 7.562500000000001e-07,
      "loss": 0.002,
      "step": 236370
    },
    {
      "epoch": 7.879333333333333,
      "grad_norm": 0.3425370752811432,
      "learning_rate": 7.541666666666667e-07,
      "loss": 0.0015,
      "step": 236380
    },
    {
      "epoch": 7.879666666666667,
      "grad_norm": 0.1164674460887909,
      "learning_rate": 7.520833333333334e-07,
      "loss": 0.0013,
      "step": 236390
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.15148663520812988,
      "learning_rate": 7.5e-07,
      "loss": 0.0017,
      "step": 236400
    },
    {
      "epoch": 7.880333333333334,
      "grad_norm": 0.10254514217376709,
      "learning_rate": 7.479166666666668e-07,
      "loss": 0.0024,
      "step": 236410
    },
    {
      "epoch": 7.8806666666666665,
      "grad_norm": 0.04471390321850777,
      "learning_rate": 7.458333333333333e-07,
      "loss": 0.0019,
      "step": 236420
    },
    {
      "epoch": 7.881,
      "grad_norm": 0.01422732975333929,
      "learning_rate": 7.4375e-07,
      "loss": 0.0015,
      "step": 236430
    },
    {
      "epoch": 7.881333333333333,
      "grad_norm": 0.11450923979282379,
      "learning_rate": 7.416666666666667e-07,
      "loss": 0.0015,
      "step": 236440
    },
    {
      "epoch": 7.881666666666667,
      "grad_norm": 0.2277839034795761,
      "learning_rate": 7.395833333333334e-07,
      "loss": 0.0031,
      "step": 236450
    },
    {
      "epoch": 7.882,
      "grad_norm": 0.2001962810754776,
      "learning_rate": 7.375e-07,
      "loss": 0.0013,
      "step": 236460
    },
    {
      "epoch": 7.882333333333333,
      "grad_norm": 0.14319442212581635,
      "learning_rate": 7.354166666666667e-07,
      "loss": 0.0013,
      "step": 236470
    },
    {
      "epoch": 7.882666666666667,
      "grad_norm": 0.08762316405773163,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0015,
      "step": 236480
    },
    {
      "epoch": 7.883,
      "grad_norm": 0.01530957780778408,
      "learning_rate": 7.312500000000001e-07,
      "loss": 0.0014,
      "step": 236490
    },
    {
      "epoch": 7.883333333333333,
      "grad_norm": 0.07461538165807724,
      "learning_rate": 7.291666666666667e-07,
      "loss": 0.0014,
      "step": 236500
    },
    {
      "epoch": 7.883666666666667,
      "grad_norm": 0.11449027806520462,
      "learning_rate": 7.270833333333334e-07,
      "loss": 0.0016,
      "step": 236510
    },
    {
      "epoch": 7.884,
      "grad_norm": 0.057412587106227875,
      "learning_rate": 7.25e-07,
      "loss": 0.0022,
      "step": 236520
    },
    {
      "epoch": 7.884333333333333,
      "grad_norm": 0.27385371923446655,
      "learning_rate": 7.229166666666667e-07,
      "loss": 0.0016,
      "step": 236530
    },
    {
      "epoch": 7.884666666666667,
      "grad_norm": 0.030114704743027687,
      "learning_rate": 7.208333333333333e-07,
      "loss": 0.0019,
      "step": 236540
    },
    {
      "epoch": 7.885,
      "grad_norm": 0.2852480113506317,
      "learning_rate": 7.187500000000001e-07,
      "loss": 0.0012,
      "step": 236550
    },
    {
      "epoch": 7.8853333333333335,
      "grad_norm": 0.058441661298274994,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.0015,
      "step": 236560
    },
    {
      "epoch": 7.885666666666666,
      "grad_norm": 0.1183096393942833,
      "learning_rate": 7.145833333333333e-07,
      "loss": 0.0013,
      "step": 236570
    },
    {
      "epoch": 7.886,
      "grad_norm": 0.0944346934556961,
      "learning_rate": 7.125000000000001e-07,
      "loss": 0.0013,
      "step": 236580
    },
    {
      "epoch": 7.886333333333333,
      "grad_norm": 0.03372421860694885,
      "learning_rate": 7.104166666666667e-07,
      "loss": 0.0012,
      "step": 236590
    },
    {
      "epoch": 7.886666666666667,
      "grad_norm": 0.17161674797534943,
      "learning_rate": 7.083333333333334e-07,
      "loss": 0.0017,
      "step": 236600
    },
    {
      "epoch": 7.8870000000000005,
      "grad_norm": 0.17146781086921692,
      "learning_rate": 7.0625e-07,
      "loss": 0.0014,
      "step": 236610
    },
    {
      "epoch": 7.887333333333333,
      "grad_norm": 0.596413791179657,
      "learning_rate": 7.041666666666667e-07,
      "loss": 0.0014,
      "step": 236620
    },
    {
      "epoch": 7.887666666666667,
      "grad_norm": 0.31396663188934326,
      "learning_rate": 7.020833333333333e-07,
      "loss": 0.0015,
      "step": 236630
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.03766029328107834,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0017,
      "step": 236640
    },
    {
      "epoch": 7.888333333333334,
      "grad_norm": 0.08606266975402832,
      "learning_rate": 6.979166666666667e-07,
      "loss": 0.0011,
      "step": 236650
    },
    {
      "epoch": 7.8886666666666665,
      "grad_norm": 0.11580489575862885,
      "learning_rate": 6.958333333333334e-07,
      "loss": 0.0011,
      "step": 236660
    },
    {
      "epoch": 7.889,
      "grad_norm": 0.11455540359020233,
      "learning_rate": 6.9375e-07,
      "loss": 0.002,
      "step": 236670
    },
    {
      "epoch": 7.889333333333333,
      "grad_norm": 0.1751730740070343,
      "learning_rate": 6.916666666666667e-07,
      "loss": 0.0018,
      "step": 236680
    },
    {
      "epoch": 7.889666666666667,
      "grad_norm": 0.05723485350608826,
      "learning_rate": 6.895833333333334e-07,
      "loss": 0.0016,
      "step": 236690
    },
    {
      "epoch": 7.89,
      "grad_norm": 0.28560692071914673,
      "learning_rate": 6.875000000000001e-07,
      "loss": 0.0019,
      "step": 236700
    },
    {
      "epoch": 7.890333333333333,
      "grad_norm": 0.2852592468261719,
      "learning_rate": 6.854166666666666e-07,
      "loss": 0.0015,
      "step": 236710
    },
    {
      "epoch": 7.890666666666666,
      "grad_norm": 0.030523547902703285,
      "learning_rate": 6.833333333333334e-07,
      "loss": 0.0015,
      "step": 236720
    },
    {
      "epoch": 7.891,
      "grad_norm": 0.030881457030773163,
      "learning_rate": 6.8125e-07,
      "loss": 0.0015,
      "step": 236730
    },
    {
      "epoch": 7.891333333333334,
      "grad_norm": 0.11402516067028046,
      "learning_rate": 6.791666666666667e-07,
      "loss": 0.0017,
      "step": 236740
    },
    {
      "epoch": 7.891666666666667,
      "grad_norm": 0.37068602442741394,
      "learning_rate": 6.770833333333333e-07,
      "loss": 0.0021,
      "step": 236750
    },
    {
      "epoch": 7.892,
      "grad_norm": 0.2852916419506073,
      "learning_rate": 6.75e-07,
      "loss": 0.0017,
      "step": 236760
    },
    {
      "epoch": 7.892333333333333,
      "grad_norm": 0.20037521421909332,
      "learning_rate": 6.729166666666666e-07,
      "loss": 0.0012,
      "step": 236770
    },
    {
      "epoch": 7.892666666666667,
      "grad_norm": 0.02002887986600399,
      "learning_rate": 6.708333333333334e-07,
      "loss": 0.0017,
      "step": 236780
    },
    {
      "epoch": 7.893,
      "grad_norm": 0.14572185277938843,
      "learning_rate": 6.687500000000001e-07,
      "loss": 0.0014,
      "step": 236790
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.28621697425842285,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0015,
      "step": 236800
    },
    {
      "epoch": 7.893666666666666,
      "grad_norm": 0.1421489715576172,
      "learning_rate": 6.645833333333334e-07,
      "loss": 0.0016,
      "step": 236810
    },
    {
      "epoch": 7.894,
      "grad_norm": 0.06095350161194801,
      "learning_rate": 6.625e-07,
      "loss": 0.0014,
      "step": 236820
    },
    {
      "epoch": 7.894333333333333,
      "grad_norm": 0.19026914238929749,
      "learning_rate": 6.604166666666668e-07,
      "loss": 0.0017,
      "step": 236830
    },
    {
      "epoch": 7.894666666666667,
      "grad_norm": 0.03595653176307678,
      "learning_rate": 6.583333333333334e-07,
      "loss": 0.0028,
      "step": 236840
    },
    {
      "epoch": 7.895,
      "grad_norm": 0.08848801255226135,
      "learning_rate": 6.5625e-07,
      "loss": 0.0019,
      "step": 236850
    },
    {
      "epoch": 7.895333333333333,
      "grad_norm": 0.2281511276960373,
      "learning_rate": 6.541666666666667e-07,
      "loss": 0.0018,
      "step": 236860
    },
    {
      "epoch": 7.895666666666667,
      "grad_norm": 0.05780711770057678,
      "learning_rate": 6.520833333333334e-07,
      "loss": 0.0018,
      "step": 236870
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.20012712478637695,
      "learning_rate": 6.5e-07,
      "loss": 0.0018,
      "step": 236880
    },
    {
      "epoch": 7.896333333333334,
      "grad_norm": 0.31438732147216797,
      "learning_rate": 6.479166666666667e-07,
      "loss": 0.0013,
      "step": 236890
    },
    {
      "epoch": 7.8966666666666665,
      "grad_norm": 0.11449418216943741,
      "learning_rate": 6.458333333333333e-07,
      "loss": 0.0015,
      "step": 236900
    },
    {
      "epoch": 7.897,
      "grad_norm": 0.2838880717754364,
      "learning_rate": 6.4375e-07,
      "loss": 0.002,
      "step": 236910
    },
    {
      "epoch": 7.897333333333333,
      "grad_norm": 0.34264880418777466,
      "learning_rate": 6.416666666666667e-07,
      "loss": 0.0016,
      "step": 236920
    },
    {
      "epoch": 7.897666666666667,
      "grad_norm": 0.19872622191905975,
      "learning_rate": 6.395833333333334e-07,
      "loss": 0.0015,
      "step": 236930
    },
    {
      "epoch": 7.898,
      "grad_norm": 0.03001595288515091,
      "learning_rate": 6.375e-07,
      "loss": 0.0013,
      "step": 236940
    },
    {
      "epoch": 7.898333333333333,
      "grad_norm": 0.33684584498405457,
      "learning_rate": 6.354166666666667e-07,
      "loss": 0.0022,
      "step": 236950
    },
    {
      "epoch": 7.898666666666666,
      "grad_norm": 0.060938235372304916,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.0017,
      "step": 236960
    },
    {
      "epoch": 7.899,
      "grad_norm": 0.08647951483726501,
      "learning_rate": 6.312500000000001e-07,
      "loss": 0.0028,
      "step": 236970
    },
    {
      "epoch": 7.899333333333333,
      "grad_norm": 0.1712367832660675,
      "learning_rate": 6.291666666666667e-07,
      "loss": 0.0014,
      "step": 236980
    },
    {
      "epoch": 7.899666666666667,
      "grad_norm": 0.010868867859244347,
      "learning_rate": 6.270833333333333e-07,
      "loss": 0.0015,
      "step": 236990
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.030617820098996162,
      "learning_rate": 6.25e-07,
      "loss": 0.0012,
      "step": 237000
    },
    {
      "epoch": 7.900333333333333,
      "grad_norm": 0.48536136746406555,
      "learning_rate": 6.229166666666667e-07,
      "loss": 0.0015,
      "step": 237010
    },
    {
      "epoch": 7.900666666666667,
      "grad_norm": 0.11604007333517075,
      "learning_rate": 6.208333333333334e-07,
      "loss": 0.0025,
      "step": 237020
    },
    {
      "epoch": 7.901,
      "grad_norm": 0.058730993419885635,
      "learning_rate": 6.187500000000001e-07,
      "loss": 0.0014,
      "step": 237030
    },
    {
      "epoch": 7.9013333333333335,
      "grad_norm": 0.14250041544437408,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.0016,
      "step": 237040
    },
    {
      "epoch": 7.901666666666666,
      "grad_norm": 0.46341243386268616,
      "learning_rate": 6.145833333333333e-07,
      "loss": 0.0018,
      "step": 237050
    },
    {
      "epoch": 7.902,
      "grad_norm": 0.059078749269247055,
      "learning_rate": 6.125000000000001e-07,
      "loss": 0.0016,
      "step": 237060
    },
    {
      "epoch": 7.902333333333333,
      "grad_norm": 0.2855023443698883,
      "learning_rate": 6.104166666666667e-07,
      "loss": 0.0013,
      "step": 237070
    },
    {
      "epoch": 7.902666666666667,
      "grad_norm": 0.2965107560157776,
      "learning_rate": 6.083333333333334e-07,
      "loss": 0.0013,
      "step": 237080
    },
    {
      "epoch": 7.9030000000000005,
      "grad_norm": 0.3718264102935791,
      "learning_rate": 6.0625e-07,
      "loss": 0.0017,
      "step": 237090
    },
    {
      "epoch": 7.903333333333333,
      "grad_norm": 0.3993225395679474,
      "learning_rate": 6.041666666666667e-07,
      "loss": 0.0016,
      "step": 237100
    },
    {
      "epoch": 7.903666666666666,
      "grad_norm": 0.314312607049942,
      "learning_rate": 6.020833333333334e-07,
      "loss": 0.0014,
      "step": 237110
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.08628572523593903,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.0012,
      "step": 237120
    },
    {
      "epoch": 7.904333333333334,
      "grad_norm": 0.03159100189805031,
      "learning_rate": 5.979166666666666e-07,
      "loss": 0.0012,
      "step": 237130
    },
    {
      "epoch": 7.9046666666666665,
      "grad_norm": 0.1249258890748024,
      "learning_rate": 5.958333333333334e-07,
      "loss": 0.002,
      "step": 237140
    },
    {
      "epoch": 7.905,
      "grad_norm": 0.3377242386341095,
      "learning_rate": 5.9375e-07,
      "loss": 0.002,
      "step": 237150
    },
    {
      "epoch": 7.905333333333333,
      "grad_norm": 0.2570672035217285,
      "learning_rate": 5.916666666666667e-07,
      "loss": 0.0015,
      "step": 237160
    },
    {
      "epoch": 7.905666666666667,
      "grad_norm": 0.11520937830209732,
      "learning_rate": 5.895833333333334e-07,
      "loss": 0.0015,
      "step": 237170
    },
    {
      "epoch": 7.906,
      "grad_norm": 0.1719946712255478,
      "learning_rate": 5.875e-07,
      "loss": 0.0013,
      "step": 237180
    },
    {
      "epoch": 7.906333333333333,
      "grad_norm": 0.20018938183784485,
      "learning_rate": 5.854166666666666e-07,
      "loss": 0.0017,
      "step": 237190
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.11424870043992996,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.0013,
      "step": 237200
    },
    {
      "epoch": 7.907,
      "grad_norm": 0.06125304102897644,
      "learning_rate": 5.8125e-07,
      "loss": 0.0012,
      "step": 237210
    },
    {
      "epoch": 7.907333333333334,
      "grad_norm": 0.20373310148715973,
      "learning_rate": 5.791666666666667e-07,
      "loss": 0.0017,
      "step": 237220
    },
    {
      "epoch": 7.907666666666667,
      "grad_norm": 0.029526863247156143,
      "learning_rate": 5.770833333333333e-07,
      "loss": 0.0016,
      "step": 237230
    },
    {
      "epoch": 7.908,
      "grad_norm": 0.14254114031791687,
      "learning_rate": 5.75e-07,
      "loss": 0.0025,
      "step": 237240
    },
    {
      "epoch": 7.908333333333333,
      "grad_norm": 0.1434360295534134,
      "learning_rate": 5.729166666666667e-07,
      "loss": 0.0012,
      "step": 237250
    },
    {
      "epoch": 7.908666666666667,
      "grad_norm": 0.09510869532823563,
      "learning_rate": 5.708333333333334e-07,
      "loss": 0.0014,
      "step": 237260
    },
    {
      "epoch": 7.909,
      "grad_norm": 0.057653144001960754,
      "learning_rate": 5.687500000000001e-07,
      "loss": 0.0021,
      "step": 237270
    },
    {
      "epoch": 7.9093333333333335,
      "grad_norm": 0.2569677233695984,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.0019,
      "step": 237280
    },
    {
      "epoch": 7.909666666666666,
      "grad_norm": 0.1714780628681183,
      "learning_rate": 5.645833333333334e-07,
      "loss": 0.0028,
      "step": 237290
    },
    {
      "epoch": 7.91,
      "grad_norm": 0.08594290912151337,
      "learning_rate": 5.625e-07,
      "loss": 0.002,
      "step": 237300
    },
    {
      "epoch": 7.910333333333333,
      "grad_norm": 0.48456984758377075,
      "learning_rate": 5.604166666666668e-07,
      "loss": 0.0017,
      "step": 237310
    },
    {
      "epoch": 7.910666666666667,
      "grad_norm": 0.22649061679840088,
      "learning_rate": 5.583333333333333e-07,
      "loss": 0.0019,
      "step": 237320
    },
    {
      "epoch": 7.911,
      "grad_norm": 0.3424893319606781,
      "learning_rate": 5.5625e-07,
      "loss": 0.0014,
      "step": 237330
    },
    {
      "epoch": 7.911333333333333,
      "grad_norm": 0.11511252075433731,
      "learning_rate": 5.541666666666667e-07,
      "loss": 0.0015,
      "step": 237340
    },
    {
      "epoch": 7.911666666666667,
      "grad_norm": 0.08833450824022293,
      "learning_rate": 5.520833333333334e-07,
      "loss": 0.0013,
      "step": 237350
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.03660888969898224,
      "learning_rate": 5.5e-07,
      "loss": 0.0012,
      "step": 237360
    },
    {
      "epoch": 7.912333333333334,
      "grad_norm": 0.03202919661998749,
      "learning_rate": 5.479166666666667e-07,
      "loss": 0.0014,
      "step": 237370
    },
    {
      "epoch": 7.9126666666666665,
      "grad_norm": 0.580503523349762,
      "learning_rate": 5.458333333333333e-07,
      "loss": 0.0022,
      "step": 237380
    },
    {
      "epoch": 7.913,
      "grad_norm": 0.05770152434706688,
      "learning_rate": 5.4375e-07,
      "loss": 0.0023,
      "step": 237390
    },
    {
      "epoch": 7.913333333333333,
      "grad_norm": 0.08575009554624557,
      "learning_rate": 5.416666666666667e-07,
      "loss": 0.0036,
      "step": 237400
    },
    {
      "epoch": 7.913666666666667,
      "grad_norm": 0.14331795275211334,
      "learning_rate": 5.395833333333334e-07,
      "loss": 0.0016,
      "step": 237410
    },
    {
      "epoch": 7.914,
      "grad_norm": 0.14366766810417175,
      "learning_rate": 5.374999999999999e-07,
      "loss": 0.0015,
      "step": 237420
    },
    {
      "epoch": 7.914333333333333,
      "grad_norm": 0.08734919130802155,
      "learning_rate": 5.354166666666667e-07,
      "loss": 0.0013,
      "step": 237430
    },
    {
      "epoch": 7.914666666666666,
      "grad_norm": 0.057835597544908524,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0017,
      "step": 237440
    },
    {
      "epoch": 7.915,
      "grad_norm": 0.08848032355308533,
      "learning_rate": 5.312500000000001e-07,
      "loss": 0.0015,
      "step": 237450
    },
    {
      "epoch": 7.915333333333333,
      "grad_norm": 0.17223097383975983,
      "learning_rate": 5.291666666666666e-07,
      "loss": 0.0018,
      "step": 237460
    },
    {
      "epoch": 7.915666666666667,
      "grad_norm": 0.0575135238468647,
      "learning_rate": 5.270833333333333e-07,
      "loss": 0.0018,
      "step": 237470
    },
    {
      "epoch": 7.916,
      "grad_norm": 0.01169616635888815,
      "learning_rate": 5.250000000000001e-07,
      "loss": 0.002,
      "step": 237480
    },
    {
      "epoch": 7.916333333333333,
      "grad_norm": 0.27530133724212646,
      "learning_rate": 5.229166666666667e-07,
      "loss": 0.0015,
      "step": 237490
    },
    {
      "epoch": 7.916666666666667,
      "grad_norm": 0.057615287601947784,
      "learning_rate": 5.208333333333334e-07,
      "loss": 0.0014,
      "step": 237500
    },
    {
      "epoch": 7.917,
      "grad_norm": 0.008198207244277,
      "learning_rate": 5.1875e-07,
      "loss": 0.0013,
      "step": 237510
    },
    {
      "epoch": 7.917333333333334,
      "grad_norm": 0.48470208048820496,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.0013,
      "step": 237520
    },
    {
      "epoch": 7.917666666666666,
      "grad_norm": 0.1434488594532013,
      "learning_rate": 5.145833333333333e-07,
      "loss": 0.0013,
      "step": 237530
    },
    {
      "epoch": 7.918,
      "grad_norm": 0.031065843999385834,
      "learning_rate": 5.125000000000001e-07,
      "loss": 0.0022,
      "step": 237540
    },
    {
      "epoch": 7.918333333333333,
      "grad_norm": 0.1721421778202057,
      "learning_rate": 5.104166666666667e-07,
      "loss": 0.0016,
      "step": 237550
    },
    {
      "epoch": 7.918666666666667,
      "grad_norm": 0.22892199456691742,
      "learning_rate": 5.083333333333333e-07,
      "loss": 0.0019,
      "step": 237560
    },
    {
      "epoch": 7.9190000000000005,
      "grad_norm": 0.1997842937707901,
      "learning_rate": 5.0625e-07,
      "loss": 0.0013,
      "step": 237570
    },
    {
      "epoch": 7.919333333333333,
      "grad_norm": 0.08554164320230484,
      "learning_rate": 5.041666666666667e-07,
      "loss": 0.0014,
      "step": 237580
    },
    {
      "epoch": 7.919666666666666,
      "grad_norm": 0.4099618196487427,
      "learning_rate": 5.020833333333334e-07,
      "loss": 0.0013,
      "step": 237590
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.012324344366788864,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0017,
      "step": 237600
    },
    {
      "epoch": 7.920333333333334,
      "grad_norm": 0.2923152446746826,
      "learning_rate": 4.979166666666666e-07,
      "loss": 0.0019,
      "step": 237610
    },
    {
      "epoch": 7.9206666666666665,
      "grad_norm": 0.14664235711097717,
      "learning_rate": 4.958333333333334e-07,
      "loss": 0.0016,
      "step": 237620
    },
    {
      "epoch": 7.921,
      "grad_norm": 0.08698421716690063,
      "learning_rate": 4.9375e-07,
      "loss": 0.0012,
      "step": 237630
    },
    {
      "epoch": 7.921333333333333,
      "grad_norm": 0.2572290003299713,
      "learning_rate": 4.916666666666667e-07,
      "loss": 0.0023,
      "step": 237640
    },
    {
      "epoch": 7.921666666666667,
      "grad_norm": 0.24231630563735962,
      "learning_rate": 4.895833333333333e-07,
      "loss": 0.0015,
      "step": 237650
    },
    {
      "epoch": 7.922,
      "grad_norm": 0.17360177636146545,
      "learning_rate": 4.875e-07,
      "loss": 0.0014,
      "step": 237660
    },
    {
      "epoch": 7.9223333333333334,
      "grad_norm": 0.17294618487358093,
      "learning_rate": 4.854166666666666e-07,
      "loss": 0.0014,
      "step": 237670
    },
    {
      "epoch": 7.922666666666666,
      "grad_norm": 0.2282455712556839,
      "learning_rate": 4.833333333333334e-07,
      "loss": 0.0015,
      "step": 237680
    },
    {
      "epoch": 7.923,
      "grad_norm": 0.36503317952156067,
      "learning_rate": 4.8125e-07,
      "loss": 0.0013,
      "step": 237690
    },
    {
      "epoch": 7.923333333333334,
      "grad_norm": 0.14332787692546844,
      "learning_rate": 4.791666666666667e-07,
      "loss": 0.002,
      "step": 237700
    },
    {
      "epoch": 7.923666666666667,
      "grad_norm": 0.059711337089538574,
      "learning_rate": 4.770833333333334e-07,
      "loss": 0.0021,
      "step": 237710
    },
    {
      "epoch": 7.924,
      "grad_norm": 0.08654595911502838,
      "learning_rate": 4.75e-07,
      "loss": 0.0014,
      "step": 237720
    },
    {
      "epoch": 7.924333333333333,
      "grad_norm": 0.1421946883201599,
      "learning_rate": 4.7291666666666666e-07,
      "loss": 0.0015,
      "step": 237730
    },
    {
      "epoch": 7.924666666666667,
      "grad_norm": 0.05791812390089035,
      "learning_rate": 4.7083333333333336e-07,
      "loss": 0.0013,
      "step": 237740
    },
    {
      "epoch": 7.925,
      "grad_norm": 0.08577860891819,
      "learning_rate": 4.6875e-07,
      "loss": 0.0018,
      "step": 237750
    },
    {
      "epoch": 7.925333333333334,
      "grad_norm": 0.0582205168902874,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.0018,
      "step": 237760
    },
    {
      "epoch": 7.925666666666666,
      "grad_norm": 0.14280299842357635,
      "learning_rate": 4.6458333333333337e-07,
      "loss": 0.0019,
      "step": 237770
    },
    {
      "epoch": 7.926,
      "grad_norm": 0.22910115122795105,
      "learning_rate": 4.625e-07,
      "loss": 0.0022,
      "step": 237780
    },
    {
      "epoch": 7.926333333333333,
      "grad_norm": 0.05791303142905235,
      "learning_rate": 4.604166666666667e-07,
      "loss": 0.0017,
      "step": 237790
    },
    {
      "epoch": 7.926666666666667,
      "grad_norm": 0.0864633172750473,
      "learning_rate": 4.583333333333334e-07,
      "loss": 0.0016,
      "step": 237800
    },
    {
      "epoch": 7.927,
      "grad_norm": 0.11618632823228836,
      "learning_rate": 4.5624999999999997e-07,
      "loss": 0.0026,
      "step": 237810
    },
    {
      "epoch": 7.927333333333333,
      "grad_norm": 0.058570753782987595,
      "learning_rate": 4.5416666666666673e-07,
      "loss": 0.0016,
      "step": 237820
    },
    {
      "epoch": 7.927666666666667,
      "grad_norm": 0.14310111105442047,
      "learning_rate": 4.5208333333333333e-07,
      "loss": 0.0021,
      "step": 237830
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.008590258657932281,
      "learning_rate": 4.5e-07,
      "loss": 0.0016,
      "step": 237840
    },
    {
      "epoch": 7.928333333333334,
      "grad_norm": 0.34228846430778503,
      "learning_rate": 4.479166666666667e-07,
      "loss": 0.0013,
      "step": 237850
    },
    {
      "epoch": 7.9286666666666665,
      "grad_norm": 0.2957412600517273,
      "learning_rate": 4.4583333333333334e-07,
      "loss": 0.0013,
      "step": 237860
    },
    {
      "epoch": 7.929,
      "grad_norm": 0.14342783391475677,
      "learning_rate": 4.4375e-07,
      "loss": 0.0015,
      "step": 237870
    },
    {
      "epoch": 7.929333333333333,
      "grad_norm": 0.11512768268585205,
      "learning_rate": 4.416666666666667e-07,
      "loss": 0.0017,
      "step": 237880
    },
    {
      "epoch": 7.929666666666667,
      "grad_norm": 0.3754608631134033,
      "learning_rate": 4.3958333333333334e-07,
      "loss": 0.0019,
      "step": 237890
    },
    {
      "epoch": 7.93,
      "grad_norm": 0.010610783472657204,
      "learning_rate": 4.3750000000000005e-07,
      "loss": 0.0025,
      "step": 237900
    },
    {
      "epoch": 7.9303333333333335,
      "grad_norm": 0.14472095668315887,
      "learning_rate": 4.354166666666667e-07,
      "loss": 0.0016,
      "step": 237910
    },
    {
      "epoch": 7.930666666666666,
      "grad_norm": 0.0575358048081398,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.0012,
      "step": 237920
    },
    {
      "epoch": 7.931,
      "grad_norm": 0.18083713948726654,
      "learning_rate": 4.3125000000000005e-07,
      "loss": 0.0016,
      "step": 237930
    },
    {
      "epoch": 7.931333333333333,
      "grad_norm": 0.08624060451984406,
      "learning_rate": 4.291666666666667e-07,
      "loss": 0.002,
      "step": 237940
    },
    {
      "epoch": 7.931666666666667,
      "grad_norm": 0.2857406437397003,
      "learning_rate": 4.270833333333333e-07,
      "loss": 0.0012,
      "step": 237950
    },
    {
      "epoch": 7.932,
      "grad_norm": 0.3134390115737915,
      "learning_rate": 4.2500000000000006e-07,
      "loss": 0.0011,
      "step": 237960
    },
    {
      "epoch": 7.932333333333333,
      "grad_norm": 0.2637869417667389,
      "learning_rate": 4.229166666666667e-07,
      "loss": 0.0011,
      "step": 237970
    },
    {
      "epoch": 7.932666666666667,
      "grad_norm": 0.058346133679151535,
      "learning_rate": 4.208333333333333e-07,
      "loss": 0.0015,
      "step": 237980
    },
    {
      "epoch": 7.933,
      "grad_norm": 0.011759677901864052,
      "learning_rate": 4.1875000000000007e-07,
      "loss": 0.0016,
      "step": 237990
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.4442541003227234,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.0018,
      "step": 238000
    },
    {
      "epoch": 7.933666666666666,
      "grad_norm": 0.17104947566986084,
      "learning_rate": 4.145833333333333e-07,
      "loss": 0.0018,
      "step": 238010
    },
    {
      "epoch": 7.934,
      "grad_norm": 0.27321311831474304,
      "learning_rate": 4.125e-07,
      "loss": 0.0016,
      "step": 238020
    },
    {
      "epoch": 7.934333333333333,
      "grad_norm": 0.22874297201633453,
      "learning_rate": 4.1041666666666667e-07,
      "loss": 0.0017,
      "step": 238030
    },
    {
      "epoch": 7.934666666666667,
      "grad_norm": 0.3143481910228729,
      "learning_rate": 4.083333333333333e-07,
      "loss": 0.0028,
      "step": 238040
    },
    {
      "epoch": 7.9350000000000005,
      "grad_norm": 0.16312523186206818,
      "learning_rate": 4.0625000000000003e-07,
      "loss": 0.0014,
      "step": 238050
    },
    {
      "epoch": 7.935333333333333,
      "grad_norm": 0.010737658478319645,
      "learning_rate": 4.041666666666667e-07,
      "loss": 0.0017,
      "step": 238060
    },
    {
      "epoch": 7.935666666666666,
      "grad_norm": 0.06014188751578331,
      "learning_rate": 4.020833333333334e-07,
      "loss": 0.0025,
      "step": 238070
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.11467315256595612,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.0013,
      "step": 238080
    },
    {
      "epoch": 7.936333333333334,
      "grad_norm": 0.05874800682067871,
      "learning_rate": 3.979166666666667e-07,
      "loss": 0.0016,
      "step": 238090
    },
    {
      "epoch": 7.9366666666666665,
      "grad_norm": 0.216771200299263,
      "learning_rate": 3.958333333333334e-07,
      "loss": 0.002,
      "step": 238100
    },
    {
      "epoch": 7.937,
      "grad_norm": 0.10819914191961288,
      "learning_rate": 3.9375000000000004e-07,
      "loss": 0.0018,
      "step": 238110
    },
    {
      "epoch": 7.937333333333333,
      "grad_norm": 0.151888906955719,
      "learning_rate": 3.9166666666666664e-07,
      "loss": 0.0023,
      "step": 238120
    },
    {
      "epoch": 7.937666666666667,
      "grad_norm": 0.228965625166893,
      "learning_rate": 3.8958333333333334e-07,
      "loss": 0.0012,
      "step": 238130
    },
    {
      "epoch": 7.938,
      "grad_norm": 0.11437055468559265,
      "learning_rate": 3.875e-07,
      "loss": 0.0018,
      "step": 238140
    },
    {
      "epoch": 7.9383333333333335,
      "grad_norm": 0.030797475948929787,
      "learning_rate": 3.854166666666667e-07,
      "loss": 0.0014,
      "step": 238150
    },
    {
      "epoch": 7.938666666666666,
      "grad_norm": 0.017464272677898407,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 0.0016,
      "step": 238160
    },
    {
      "epoch": 7.939,
      "grad_norm": 0.029949579387903214,
      "learning_rate": 3.8125e-07,
      "loss": 0.0014,
      "step": 238170
    },
    {
      "epoch": 7.939333333333334,
      "grad_norm": 0.19991908967494965,
      "learning_rate": 3.791666666666667e-07,
      "loss": 0.0015,
      "step": 238180
    },
    {
      "epoch": 7.939666666666667,
      "grad_norm": 0.0862717404961586,
      "learning_rate": 3.7708333333333336e-07,
      "loss": 0.0013,
      "step": 238190
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 0.1430482566356659,
      "learning_rate": 3.75e-07,
      "loss": 0.0015,
      "step": 238200
    },
    {
      "epoch": 7.940333333333333,
      "grad_norm": 0.17209342122077942,
      "learning_rate": 3.7291666666666666e-07,
      "loss": 0.002,
      "step": 238210
    },
    {
      "epoch": 7.940666666666667,
      "grad_norm": 0.18316972255706787,
      "learning_rate": 3.7083333333333337e-07,
      "loss": 0.0014,
      "step": 238220
    },
    {
      "epoch": 7.941,
      "grad_norm": 0.08584629744291306,
      "learning_rate": 3.6875e-07,
      "loss": 0.0014,
      "step": 238230
    },
    {
      "epoch": 7.941333333333334,
      "grad_norm": 0.11557396501302719,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.0017,
      "step": 238240
    },
    {
      "epoch": 7.941666666666666,
      "grad_norm": 0.08651268482208252,
      "learning_rate": 3.6458333333333337e-07,
      "loss": 0.0019,
      "step": 238250
    },
    {
      "epoch": 7.942,
      "grad_norm": 0.015570119023323059,
      "learning_rate": 3.625e-07,
      "loss": 0.0015,
      "step": 238260
    },
    {
      "epoch": 7.942333333333333,
      "grad_norm": 0.2005331963300705,
      "learning_rate": 3.604166666666667e-07,
      "loss": 0.0012,
      "step": 238270
    },
    {
      "epoch": 7.942666666666667,
      "grad_norm": 0.14319881796836853,
      "learning_rate": 3.583333333333333e-07,
      "loss": 0.0011,
      "step": 238280
    },
    {
      "epoch": 7.943,
      "grad_norm": 0.03061377815902233,
      "learning_rate": 3.5625000000000003e-07,
      "loss": 0.0023,
      "step": 238290
    },
    {
      "epoch": 7.943333333333333,
      "grad_norm": 0.00979730673134327,
      "learning_rate": 3.541666666666667e-07,
      "loss": 0.0015,
      "step": 238300
    },
    {
      "epoch": 7.943666666666667,
      "grad_norm": 0.1711246520280838,
      "learning_rate": 3.5208333333333333e-07,
      "loss": 0.0017,
      "step": 238310
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.4117770195007324,
      "learning_rate": 3.5000000000000004e-07,
      "loss": 0.0018,
      "step": 238320
    },
    {
      "epoch": 7.944333333333334,
      "grad_norm": 0.06044132262468338,
      "learning_rate": 3.479166666666667e-07,
      "loss": 0.0019,
      "step": 238330
    },
    {
      "epoch": 7.9446666666666665,
      "grad_norm": 0.009507939219474792,
      "learning_rate": 3.4583333333333334e-07,
      "loss": 0.0014,
      "step": 238340
    },
    {
      "epoch": 7.945,
      "grad_norm": 0.22813430428504944,
      "learning_rate": 3.4375000000000004e-07,
      "loss": 0.0021,
      "step": 238350
    },
    {
      "epoch": 7.945333333333333,
      "grad_norm": 0.03280777856707573,
      "learning_rate": 3.416666666666667e-07,
      "loss": 0.0014,
      "step": 238360
    },
    {
      "epoch": 7.945666666666667,
      "grad_norm": 0.15659469366073608,
      "learning_rate": 3.3958333333333335e-07,
      "loss": 0.0015,
      "step": 238370
    },
    {
      "epoch": 7.946,
      "grad_norm": 0.03458552807569504,
      "learning_rate": 3.375e-07,
      "loss": 0.002,
      "step": 238380
    },
    {
      "epoch": 7.9463333333333335,
      "grad_norm": 0.2287696897983551,
      "learning_rate": 3.354166666666667e-07,
      "loss": 0.0017,
      "step": 238390
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.37075626850128174,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.0014,
      "step": 238400
    },
    {
      "epoch": 7.947,
      "grad_norm": 0.2279999703168869,
      "learning_rate": 3.3125e-07,
      "loss": 0.0016,
      "step": 238410
    },
    {
      "epoch": 7.947333333333333,
      "grad_norm": 0.057894766330718994,
      "learning_rate": 3.291666666666667e-07,
      "loss": 0.002,
      "step": 238420
    },
    {
      "epoch": 7.947666666666667,
      "grad_norm": 0.20026105642318726,
      "learning_rate": 3.2708333333333336e-07,
      "loss": 0.0017,
      "step": 238430
    },
    {
      "epoch": 7.948,
      "grad_norm": 0.14520645141601562,
      "learning_rate": 3.25e-07,
      "loss": 0.0016,
      "step": 238440
    },
    {
      "epoch": 7.948333333333333,
      "grad_norm": 0.20089717209339142,
      "learning_rate": 3.2291666666666666e-07,
      "loss": 0.0012,
      "step": 238450
    },
    {
      "epoch": 7.948666666666667,
      "grad_norm": 0.3427750766277313,
      "learning_rate": 3.2083333333333337e-07,
      "loss": 0.0016,
      "step": 238460
    },
    {
      "epoch": 7.949,
      "grad_norm": 0.14364302158355713,
      "learning_rate": 3.1875e-07,
      "loss": 0.0018,
      "step": 238470
    },
    {
      "epoch": 7.949333333333334,
      "grad_norm": 0.030759522691369057,
      "learning_rate": 3.1666666666666667e-07,
      "loss": 0.0014,
      "step": 238480
    },
    {
      "epoch": 7.949666666666666,
      "grad_norm": 0.20373593270778656,
      "learning_rate": 3.145833333333334e-07,
      "loss": 0.0021,
      "step": 238490
    },
    {
      "epoch": 7.95,
      "grad_norm": 0.17497001588344574,
      "learning_rate": 3.125e-07,
      "loss": 0.0023,
      "step": 238500
    },
    {
      "epoch": 7.950333333333333,
      "grad_norm": 0.2852165699005127,
      "learning_rate": 3.104166666666667e-07,
      "loss": 0.0014,
      "step": 238510
    },
    {
      "epoch": 7.950666666666667,
      "grad_norm": 0.11506149917840958,
      "learning_rate": 3.0833333333333333e-07,
      "loss": 0.0023,
      "step": 238520
    },
    {
      "epoch": 7.951,
      "grad_norm": 0.14333106577396393,
      "learning_rate": 3.0625000000000003e-07,
      "loss": 0.0026,
      "step": 238530
    },
    {
      "epoch": 7.951333333333333,
      "grad_norm": 0.04081542789936066,
      "learning_rate": 3.041666666666667e-07,
      "loss": 0.0013,
      "step": 238540
    },
    {
      "epoch": 7.951666666666666,
      "grad_norm": 0.14296481013298035,
      "learning_rate": 3.0208333333333334e-07,
      "loss": 0.0024,
      "step": 238550
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.1432742327451706,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.0018,
      "step": 238560
    },
    {
      "epoch": 7.952333333333334,
      "grad_norm": 0.032118137925863266,
      "learning_rate": 2.979166666666667e-07,
      "loss": 0.0014,
      "step": 238570
    },
    {
      "epoch": 7.9526666666666666,
      "grad_norm": 0.007787903770804405,
      "learning_rate": 2.9583333333333334e-07,
      "loss": 0.0023,
      "step": 238580
    },
    {
      "epoch": 7.953,
      "grad_norm": 0.21441255509853363,
      "learning_rate": 2.9375e-07,
      "loss": 0.0011,
      "step": 238590
    },
    {
      "epoch": 7.953333333333333,
      "grad_norm": 0.2604027986526489,
      "learning_rate": 2.916666666666667e-07,
      "loss": 0.0022,
      "step": 238600
    },
    {
      "epoch": 7.953666666666667,
      "grad_norm": 0.05798938497900963,
      "learning_rate": 2.8958333333333335e-07,
      "loss": 0.002,
      "step": 238610
    },
    {
      "epoch": 7.954,
      "grad_norm": 0.14636318385601044,
      "learning_rate": 2.875e-07,
      "loss": 0.0014,
      "step": 238620
    },
    {
      "epoch": 7.9543333333333335,
      "grad_norm": 0.05994445085525513,
      "learning_rate": 2.854166666666667e-07,
      "loss": 0.0017,
      "step": 238630
    },
    {
      "epoch": 7.954666666666666,
      "grad_norm": 0.029589828103780746,
      "learning_rate": 2.8333333333333336e-07,
      "loss": 0.0015,
      "step": 238640
    },
    {
      "epoch": 7.955,
      "grad_norm": 0.2292279452085495,
      "learning_rate": 2.8125e-07,
      "loss": 0.0022,
      "step": 238650
    },
    {
      "epoch": 7.955333333333334,
      "grad_norm": 0.058119747787714005,
      "learning_rate": 2.7916666666666666e-07,
      "loss": 0.0022,
      "step": 238660
    },
    {
      "epoch": 7.955666666666667,
      "grad_norm": 0.2258846014738083,
      "learning_rate": 2.7708333333333336e-07,
      "loss": 0.0016,
      "step": 238670
    },
    {
      "epoch": 7.9559999999999995,
      "grad_norm": 0.14398592710494995,
      "learning_rate": 2.75e-07,
      "loss": 0.0026,
      "step": 238680
    },
    {
      "epoch": 7.956333333333333,
      "grad_norm": 0.14518862962722778,
      "learning_rate": 2.7291666666666667e-07,
      "loss": 0.0013,
      "step": 238690
    },
    {
      "epoch": 7.956666666666667,
      "grad_norm": 0.10421004146337509,
      "learning_rate": 2.7083333333333337e-07,
      "loss": 0.0014,
      "step": 238700
    },
    {
      "epoch": 7.957,
      "grad_norm": 0.06343398988246918,
      "learning_rate": 2.6874999999999997e-07,
      "loss": 0.0017,
      "step": 238710
    },
    {
      "epoch": 7.957333333333334,
      "grad_norm": 0.34294062852859497,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.0011,
      "step": 238720
    },
    {
      "epoch": 7.957666666666666,
      "grad_norm": 0.08686204999685287,
      "learning_rate": 2.645833333333333e-07,
      "loss": 0.0018,
      "step": 238730
    },
    {
      "epoch": 7.958,
      "grad_norm": 0.28502270579338074,
      "learning_rate": 2.6250000000000003e-07,
      "loss": 0.0016,
      "step": 238740
    },
    {
      "epoch": 7.958333333333333,
      "grad_norm": 0.2952064275741577,
      "learning_rate": 2.604166666666667e-07,
      "loss": 0.0014,
      "step": 238750
    },
    {
      "epoch": 7.958666666666667,
      "grad_norm": 0.08795417100191116,
      "learning_rate": 2.5833333333333333e-07,
      "loss": 0.0013,
      "step": 238760
    },
    {
      "epoch": 7.959,
      "grad_norm": 0.3709813952445984,
      "learning_rate": 2.5625000000000003e-07,
      "loss": 0.0015,
      "step": 238770
    },
    {
      "epoch": 7.959333333333333,
      "grad_norm": 0.009548863396048546,
      "learning_rate": 2.5416666666666663e-07,
      "loss": 0.0025,
      "step": 238780
    },
    {
      "epoch": 7.959666666666667,
      "grad_norm": 0.05821298062801361,
      "learning_rate": 2.5208333333333334e-07,
      "loss": 0.0018,
      "step": 238790
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.08549576252698898,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.0014,
      "step": 238800
    },
    {
      "epoch": 7.960333333333334,
      "grad_norm": 0.1994551718235016,
      "learning_rate": 2.479166666666667e-07,
      "loss": 0.0012,
      "step": 238810
    },
    {
      "epoch": 7.960666666666667,
      "grad_norm": 0.2856979966163635,
      "learning_rate": 2.4583333333333334e-07,
      "loss": 0.0025,
      "step": 238820
    },
    {
      "epoch": 7.961,
      "grad_norm": 0.3427320122718811,
      "learning_rate": 2.4375e-07,
      "loss": 0.0018,
      "step": 238830
    },
    {
      "epoch": 7.961333333333333,
      "grad_norm": 0.2568376064300537,
      "learning_rate": 2.416666666666667e-07,
      "loss": 0.0016,
      "step": 238840
    },
    {
      "epoch": 7.961666666666667,
      "grad_norm": 0.256425678730011,
      "learning_rate": 2.3958333333333335e-07,
      "loss": 0.0015,
      "step": 238850
    },
    {
      "epoch": 7.962,
      "grad_norm": 0.08681051433086395,
      "learning_rate": 2.375e-07,
      "loss": 0.0014,
      "step": 238860
    },
    {
      "epoch": 7.9623333333333335,
      "grad_norm": 0.030583670362830162,
      "learning_rate": 2.3541666666666668e-07,
      "loss": 0.0011,
      "step": 238870
    },
    {
      "epoch": 7.962666666666666,
      "grad_norm": 0.11661782115697861,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 0.0015,
      "step": 238880
    },
    {
      "epoch": 7.963,
      "grad_norm": 0.34236106276512146,
      "learning_rate": 2.3125e-07,
      "loss": 0.0016,
      "step": 238890
    },
    {
      "epoch": 7.963333333333333,
      "grad_norm": 0.05842626094818115,
      "learning_rate": 2.291666666666667e-07,
      "loss": 0.0023,
      "step": 238900
    },
    {
      "epoch": 7.963666666666667,
      "grad_norm": 0.14265559613704681,
      "learning_rate": 2.2708333333333336e-07,
      "loss": 0.002,
      "step": 238910
    },
    {
      "epoch": 7.964,
      "grad_norm": 0.03159061446785927,
      "learning_rate": 2.25e-07,
      "loss": 0.0017,
      "step": 238920
    },
    {
      "epoch": 7.964333333333333,
      "grad_norm": 0.01360058318823576,
      "learning_rate": 2.2291666666666667e-07,
      "loss": 0.0015,
      "step": 238930
    },
    {
      "epoch": 7.964666666666667,
      "grad_norm": 0.1196419820189476,
      "learning_rate": 2.2083333333333335e-07,
      "loss": 0.0018,
      "step": 238940
    },
    {
      "epoch": 7.965,
      "grad_norm": 0.08554635941982269,
      "learning_rate": 2.1875000000000002e-07,
      "loss": 0.0019,
      "step": 238950
    },
    {
      "epoch": 7.965333333333334,
      "grad_norm": 0.021037129685282707,
      "learning_rate": 2.1666666666666667e-07,
      "loss": 0.0016,
      "step": 238960
    },
    {
      "epoch": 7.9656666666666665,
      "grad_norm": 0.1249108612537384,
      "learning_rate": 2.1458333333333335e-07,
      "loss": 0.0016,
      "step": 238970
    },
    {
      "epoch": 7.966,
      "grad_norm": 0.0587717741727829,
      "learning_rate": 2.1250000000000003e-07,
      "loss": 0.001,
      "step": 238980
    },
    {
      "epoch": 7.966333333333333,
      "grad_norm": 0.11624611914157867,
      "learning_rate": 2.1041666666666665e-07,
      "loss": 0.0011,
      "step": 238990
    },
    {
      "epoch": 7.966666666666667,
      "grad_norm": 0.2856087386608124,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 0.0015,
      "step": 239000
    },
    {
      "epoch": 7.967,
      "grad_norm": 0.15947937965393066,
      "learning_rate": 2.0625e-07,
      "loss": 0.0021,
      "step": 239010
    },
    {
      "epoch": 7.967333333333333,
      "grad_norm": 0.03114774450659752,
      "learning_rate": 2.0416666666666666e-07,
      "loss": 0.0021,
      "step": 239020
    },
    {
      "epoch": 7.967666666666666,
      "grad_norm": 0.2277817279100418,
      "learning_rate": 2.0208333333333334e-07,
      "loss": 0.0016,
      "step": 239030
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.011311087757349014,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.0017,
      "step": 239040
    },
    {
      "epoch": 7.968333333333334,
      "grad_norm": 0.37092721462249756,
      "learning_rate": 1.979166666666667e-07,
      "loss": 0.0015,
      "step": 239050
    },
    {
      "epoch": 7.968666666666667,
      "grad_norm": 0.08625467866659164,
      "learning_rate": 1.9583333333333332e-07,
      "loss": 0.002,
      "step": 239060
    },
    {
      "epoch": 7.969,
      "grad_norm": 0.2431899607181549,
      "learning_rate": 1.9375e-07,
      "loss": 0.0017,
      "step": 239070
    },
    {
      "epoch": 7.969333333333333,
      "grad_norm": 0.05831640958786011,
      "learning_rate": 1.9166666666666668e-07,
      "loss": 0.0014,
      "step": 239080
    },
    {
      "epoch": 7.969666666666667,
      "grad_norm": 0.1719362586736679,
      "learning_rate": 1.8958333333333335e-07,
      "loss": 0.0015,
      "step": 239090
    },
    {
      "epoch": 7.97,
      "grad_norm": 0.3428036868572235,
      "learning_rate": 1.875e-07,
      "loss": 0.0011,
      "step": 239100
    },
    {
      "epoch": 7.9703333333333335,
      "grad_norm": 0.26153144240379333,
      "learning_rate": 1.8541666666666668e-07,
      "loss": 0.0022,
      "step": 239110
    },
    {
      "epoch": 7.970666666666666,
      "grad_norm": 0.3426875174045563,
      "learning_rate": 1.8333333333333333e-07,
      "loss": 0.0021,
      "step": 239120
    },
    {
      "epoch": 7.971,
      "grad_norm": 0.029161367565393448,
      "learning_rate": 1.8125e-07,
      "loss": 0.0019,
      "step": 239130
    },
    {
      "epoch": 7.971333333333334,
      "grad_norm": 0.28276297450065613,
      "learning_rate": 1.7916666666666666e-07,
      "loss": 0.0015,
      "step": 239140
    },
    {
      "epoch": 7.971666666666667,
      "grad_norm": 0.031546104699373245,
      "learning_rate": 1.7708333333333334e-07,
      "loss": 0.0014,
      "step": 239150
    },
    {
      "epoch": 7.9719999999999995,
      "grad_norm": 0.1428220272064209,
      "learning_rate": 1.7500000000000002e-07,
      "loss": 0.0021,
      "step": 239160
    },
    {
      "epoch": 7.972333333333333,
      "grad_norm": 0.14488889276981354,
      "learning_rate": 1.7291666666666667e-07,
      "loss": 0.0012,
      "step": 239170
    },
    {
      "epoch": 7.972666666666667,
      "grad_norm": 0.08633573353290558,
      "learning_rate": 1.7083333333333335e-07,
      "loss": 0.0014,
      "step": 239180
    },
    {
      "epoch": 7.973,
      "grad_norm": 0.057721927762031555,
      "learning_rate": 1.6875e-07,
      "loss": 0.0018,
      "step": 239190
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 0.06304760277271271,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.0012,
      "step": 239200
    },
    {
      "epoch": 7.9736666666666665,
      "grad_norm": 0.11484270542860031,
      "learning_rate": 1.6458333333333335e-07,
      "loss": 0.0017,
      "step": 239210
    },
    {
      "epoch": 7.974,
      "grad_norm": 0.2855217754840851,
      "learning_rate": 1.625e-07,
      "loss": 0.002,
      "step": 239220
    },
    {
      "epoch": 7.974333333333333,
      "grad_norm": 0.059234075248241425,
      "learning_rate": 1.6041666666666668e-07,
      "loss": 0.0012,
      "step": 239230
    },
    {
      "epoch": 7.974666666666667,
      "grad_norm": 0.1718737930059433,
      "learning_rate": 1.5833333333333333e-07,
      "loss": 0.0012,
      "step": 239240
    },
    {
      "epoch": 7.975,
      "grad_norm": 0.3996383547782898,
      "learning_rate": 1.5625e-07,
      "loss": 0.0011,
      "step": 239250
    },
    {
      "epoch": 7.975333333333333,
      "grad_norm": 0.07373153418302536,
      "learning_rate": 1.5416666666666666e-07,
      "loss": 0.0028,
      "step": 239260
    },
    {
      "epoch": 7.975666666666667,
      "grad_norm": 0.08363492786884308,
      "learning_rate": 1.5208333333333334e-07,
      "loss": 0.0023,
      "step": 239270
    },
    {
      "epoch": 7.976,
      "grad_norm": 0.142902210354805,
      "learning_rate": 1.5000000000000002e-07,
      "loss": 0.0019,
      "step": 239280
    },
    {
      "epoch": 7.976333333333334,
      "grad_norm": 0.17167341709136963,
      "learning_rate": 1.4791666666666667e-07,
      "loss": 0.0012,
      "step": 239290
    },
    {
      "epoch": 7.976666666666667,
      "grad_norm": 0.25750693678855896,
      "learning_rate": 1.4583333333333335e-07,
      "loss": 0.0015,
      "step": 239300
    },
    {
      "epoch": 7.977,
      "grad_norm": 0.09448333084583282,
      "learning_rate": 1.4375e-07,
      "loss": 0.0017,
      "step": 239310
    },
    {
      "epoch": 7.977333333333333,
      "grad_norm": 0.1718190461397171,
      "learning_rate": 1.4166666666666668e-07,
      "loss": 0.0024,
      "step": 239320
    },
    {
      "epoch": 7.977666666666667,
      "grad_norm": 0.26526209712028503,
      "learning_rate": 1.3958333333333333e-07,
      "loss": 0.0017,
      "step": 239330
    },
    {
      "epoch": 7.978,
      "grad_norm": 0.08848981559276581,
      "learning_rate": 1.375e-07,
      "loss": 0.0022,
      "step": 239340
    },
    {
      "epoch": 7.9783333333333335,
      "grad_norm": 0.08757840096950531,
      "learning_rate": 1.3541666666666668e-07,
      "loss": 0.0016,
      "step": 239350
    },
    {
      "epoch": 7.978666666666666,
      "grad_norm": 0.25686323642730713,
      "learning_rate": 1.3333333333333334e-07,
      "loss": 0.0027,
      "step": 239360
    },
    {
      "epoch": 7.979,
      "grad_norm": 0.08632451295852661,
      "learning_rate": 1.3125000000000001e-07,
      "loss": 0.0017,
      "step": 239370
    },
    {
      "epoch": 7.979333333333333,
      "grad_norm": 0.25716304779052734,
      "learning_rate": 1.2916666666666667e-07,
      "loss": 0.0016,
      "step": 239380
    },
    {
      "epoch": 7.979666666666667,
      "grad_norm": 0.11409961432218552,
      "learning_rate": 1.2708333333333332e-07,
      "loss": 0.0014,
      "step": 239390
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.03045743517577648,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 0.0013,
      "step": 239400
    },
    {
      "epoch": 7.980333333333333,
      "grad_norm": 0.17142269015312195,
      "learning_rate": 1.2291666666666667e-07,
      "loss": 0.0017,
      "step": 239410
    },
    {
      "epoch": 7.980666666666667,
      "grad_norm": 0.04026173800230026,
      "learning_rate": 1.2083333333333335e-07,
      "loss": 0.0015,
      "step": 239420
    },
    {
      "epoch": 7.981,
      "grad_norm": 0.27389249205589294,
      "learning_rate": 1.1875e-07,
      "loss": 0.0028,
      "step": 239430
    },
    {
      "epoch": 7.981333333333334,
      "grad_norm": 0.25714045763015747,
      "learning_rate": 1.1666666666666668e-07,
      "loss": 0.0018,
      "step": 239440
    },
    {
      "epoch": 7.9816666666666665,
      "grad_norm": 0.031701914966106415,
      "learning_rate": 1.1458333333333334e-07,
      "loss": 0.0019,
      "step": 239450
    },
    {
      "epoch": 7.982,
      "grad_norm": 0.03159257769584656,
      "learning_rate": 1.125e-07,
      "loss": 0.0016,
      "step": 239460
    },
    {
      "epoch": 7.982333333333333,
      "grad_norm": 0.030435308814048767,
      "learning_rate": 1.1041666666666667e-07,
      "loss": 0.0017,
      "step": 239470
    },
    {
      "epoch": 7.982666666666667,
      "grad_norm": 0.23010045289993286,
      "learning_rate": 1.0833333333333334e-07,
      "loss": 0.0016,
      "step": 239480
    },
    {
      "epoch": 7.983,
      "grad_norm": 0.060418274253606796,
      "learning_rate": 1.0625000000000002e-07,
      "loss": 0.0019,
      "step": 239490
    },
    {
      "epoch": 7.983333333333333,
      "grad_norm": 0.01437968760728836,
      "learning_rate": 1.0416666666666667e-07,
      "loss": 0.0014,
      "step": 239500
    }
  ],
  "logging_steps": 10,
  "max_steps": 240000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 90000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008888888888888889,
      "grad_norm": 1.0058865547180176,
      "learning_rate": 4.9994444444444446e-05,
      "loss": 0.0248,
      "step": 10
    },
    {
      "epoch": 0.0017777777777777779,
      "grad_norm": 1.1152418851852417,
      "learning_rate": 4.998888888888889e-05,
      "loss": 0.0092,
      "step": 20
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 2.2214930057525635,
      "learning_rate": 4.998333333333334e-05,
      "loss": 0.0075,
      "step": 30
    },
    {
      "epoch": 0.0035555555555555557,
      "grad_norm": 2.8738033771514893,
      "learning_rate": 4.997777777777778e-05,
      "loss": 0.0088,
      "step": 40
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 2.5528154373168945,
      "learning_rate": 4.997222222222223e-05,
      "loss": 0.0095,
      "step": 50
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 2.8456811904907227,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.0079,
      "step": 60
    },
    {
      "epoch": 0.006222222222222222,
      "grad_norm": 2.986514091491699,
      "learning_rate": 4.9961111111111114e-05,
      "loss": 0.0097,
      "step": 70
    },
    {
      "epoch": 0.0071111111111111115,
      "grad_norm": 2.037348985671997,
      "learning_rate": 4.995555555555556e-05,
      "loss": 0.0076,
      "step": 80
    },
    {
      "epoch": 0.008,
      "grad_norm": 2.4842283725738525,
      "learning_rate": 4.995e-05,
      "loss": 0.0067,
      "step": 90
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 2.472472906112671,
      "learning_rate": 4.994444444444445e-05,
      "loss": 0.009,
      "step": 100
    },
    {
      "epoch": 0.009777777777777778,
      "grad_norm": 2.1454715728759766,
      "learning_rate": 4.993888888888889e-05,
      "loss": 0.0083,
      "step": 110
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.26883602142334,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0072,
      "step": 120
    },
    {
      "epoch": 0.011555555555555555,
      "grad_norm": 2.909520149230957,
      "learning_rate": 4.992777777777778e-05,
      "loss": 0.0078,
      "step": 130
    },
    {
      "epoch": 0.012444444444444444,
      "grad_norm": 1.66462242603302,
      "learning_rate": 4.9922222222222226e-05,
      "loss": 0.0084,
      "step": 140
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 1.8161556720733643,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.0089,
      "step": 150
    },
    {
      "epoch": 0.014222222222222223,
      "grad_norm": 2.4091806411743164,
      "learning_rate": 4.991111111111111e-05,
      "loss": 0.0077,
      "step": 160
    },
    {
      "epoch": 0.015111111111111112,
      "grad_norm": 1.7788102626800537,
      "learning_rate": 4.9905555555555556e-05,
      "loss": 0.0072,
      "step": 170
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.5675177574157715,
      "learning_rate": 4.99e-05,
      "loss": 0.0078,
      "step": 180
    },
    {
      "epoch": 0.016888888888888887,
      "grad_norm": 2.6734611988067627,
      "learning_rate": 4.989444444444445e-05,
      "loss": 0.0075,
      "step": 190
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 1.1945000886917114,
      "learning_rate": 4.9888888888888894e-05,
      "loss": 0.0074,
      "step": 200
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 1.7788752317428589,
      "learning_rate": 4.988333333333334e-05,
      "loss": 0.009,
      "step": 210
    },
    {
      "epoch": 0.019555555555555555,
      "grad_norm": 2.492239236831665,
      "learning_rate": 4.987777777777778e-05,
      "loss": 0.0076,
      "step": 220
    },
    {
      "epoch": 0.020444444444444446,
      "grad_norm": 2.348092555999756,
      "learning_rate": 4.9872222222222225e-05,
      "loss": 0.0086,
      "step": 230
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 2.0431182384490967,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0081,
      "step": 240
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 1.6188994646072388,
      "learning_rate": 4.986111111111111e-05,
      "loss": 0.006,
      "step": 250
    },
    {
      "epoch": 0.02311111111111111,
      "grad_norm": 2.6093432903289795,
      "learning_rate": 4.9855555555555555e-05,
      "loss": 0.0084,
      "step": 260
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.1853266954421997,
      "learning_rate": 4.9850000000000006e-05,
      "loss": 0.0082,
      "step": 270
    },
    {
      "epoch": 0.024888888888888887,
      "grad_norm": 1.7232749462127686,
      "learning_rate": 4.984444444444445e-05,
      "loss": 0.007,
      "step": 280
    },
    {
      "epoch": 0.025777777777777778,
      "grad_norm": 2.454345703125,
      "learning_rate": 4.983888888888889e-05,
      "loss": 0.0067,
      "step": 290
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 2.270277261734009,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0077,
      "step": 300
    },
    {
      "epoch": 0.027555555555555555,
      "grad_norm": 1.1251806020736694,
      "learning_rate": 4.982777777777778e-05,
      "loss": 0.0068,
      "step": 310
    },
    {
      "epoch": 0.028444444444444446,
      "grad_norm": 1.6739821434020996,
      "learning_rate": 4.982222222222222e-05,
      "loss": 0.0078,
      "step": 320
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 1.7977583408355713,
      "learning_rate": 4.981666666666667e-05,
      "loss": 0.0079,
      "step": 330
    },
    {
      "epoch": 0.030222222222222223,
      "grad_norm": 2.2435262203216553,
      "learning_rate": 4.981111111111112e-05,
      "loss": 0.0088,
      "step": 340
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 2.6491520404815674,
      "learning_rate": 4.9805555555555554e-05,
      "loss": 0.0076,
      "step": 350
    },
    {
      "epoch": 0.032,
      "grad_norm": 2.7986092567443848,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0068,
      "step": 360
    },
    {
      "epoch": 0.03288888888888889,
      "grad_norm": 1.8083606958389282,
      "learning_rate": 4.979444444444445e-05,
      "loss": 0.0074,
      "step": 370
    },
    {
      "epoch": 0.033777777777777775,
      "grad_norm": 0.5970253944396973,
      "learning_rate": 4.978888888888889e-05,
      "loss": 0.0061,
      "step": 380
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.9443082213401794,
      "learning_rate": 4.9783333333333335e-05,
      "loss": 0.0068,
      "step": 390
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 1.496795892715454,
      "learning_rate": 4.977777777777778e-05,
      "loss": 0.0078,
      "step": 400
    },
    {
      "epoch": 0.036444444444444446,
      "grad_norm": 2.1721460819244385,
      "learning_rate": 4.977222222222223e-05,
      "loss": 0.0075,
      "step": 410
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 2.4287049770355225,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0061,
      "step": 420
    },
    {
      "epoch": 0.03822222222222222,
      "grad_norm": 2.5854358673095703,
      "learning_rate": 4.9761111111111116e-05,
      "loss": 0.0073,
      "step": 430
    },
    {
      "epoch": 0.03911111111111111,
      "grad_norm": 0.2766145169734955,
      "learning_rate": 4.975555555555555e-05,
      "loss": 0.0079,
      "step": 440
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.771511435508728,
      "learning_rate": 4.975e-05,
      "loss": 0.0071,
      "step": 450
    },
    {
      "epoch": 0.04088888888888889,
      "grad_norm": 0.46499547362327576,
      "learning_rate": 4.974444444444445e-05,
      "loss": 0.0064,
      "step": 460
    },
    {
      "epoch": 0.041777777777777775,
      "grad_norm": 0.7482231259346008,
      "learning_rate": 4.973888888888889e-05,
      "loss": 0.0064,
      "step": 470
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 1.1907857656478882,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0079,
      "step": 480
    },
    {
      "epoch": 0.043555555555555556,
      "grad_norm": 1.7313627004623413,
      "learning_rate": 4.972777777777778e-05,
      "loss": 0.0069,
      "step": 490
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.3424313962459564,
      "learning_rate": 4.972222222222223e-05,
      "loss": 0.007,
      "step": 500
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 1.4681031703948975,
      "learning_rate": 4.9716666666666664e-05,
      "loss": 0.0075,
      "step": 510
    },
    {
      "epoch": 0.04622222222222222,
      "grad_norm": 1.2265840768814087,
      "learning_rate": 4.9711111111111115e-05,
      "loss": 0.0076,
      "step": 520
    },
    {
      "epoch": 0.04711111111111111,
      "grad_norm": 2.2507386207580566,
      "learning_rate": 4.970555555555556e-05,
      "loss": 0.0077,
      "step": 530
    },
    {
      "epoch": 0.048,
      "grad_norm": 2.6440787315368652,
      "learning_rate": 4.97e-05,
      "loss": 0.0084,
      "step": 540
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 1.7044011354446411,
      "learning_rate": 4.969444444444445e-05,
      "loss": 0.006,
      "step": 550
    },
    {
      "epoch": 0.049777777777777775,
      "grad_norm": 0.5594878196716309,
      "learning_rate": 4.968888888888889e-05,
      "loss": 0.0063,
      "step": 560
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.6958593130111694,
      "learning_rate": 4.968333333333334e-05,
      "loss": 0.0058,
      "step": 570
    },
    {
      "epoch": 0.051555555555555556,
      "grad_norm": 1.5621360540390015,
      "learning_rate": 4.9677777777777776e-05,
      "loss": 0.0074,
      "step": 580
    },
    {
      "epoch": 0.052444444444444446,
      "grad_norm": 2.6067330837249756,
      "learning_rate": 4.9672222222222226e-05,
      "loss": 0.007,
      "step": 590
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.2900829017162323,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0051,
      "step": 600
    },
    {
      "epoch": 0.05422222222222222,
      "grad_norm": 2.366481065750122,
      "learning_rate": 4.9661111111111114e-05,
      "loss": 0.0067,
      "step": 610
    },
    {
      "epoch": 0.05511111111111111,
      "grad_norm": 1.8325105905532837,
      "learning_rate": 4.965555555555556e-05,
      "loss": 0.0077,
      "step": 620
    },
    {
      "epoch": 0.056,
      "grad_norm": 2.240269660949707,
      "learning_rate": 4.965e-05,
      "loss": 0.0052,
      "step": 630
    },
    {
      "epoch": 0.05688888888888889,
      "grad_norm": 1.2850548028945923,
      "learning_rate": 4.964444444444445e-05,
      "loss": 0.0055,
      "step": 640
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 0.7818819284439087,
      "learning_rate": 4.963888888888889e-05,
      "loss": 0.0057,
      "step": 650
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 2.3110005855560303,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0071,
      "step": 660
    },
    {
      "epoch": 0.059555555555555556,
      "grad_norm": 1.9384543895721436,
      "learning_rate": 4.962777777777778e-05,
      "loss": 0.0074,
      "step": 670
    },
    {
      "epoch": 0.060444444444444446,
      "grad_norm": 1.515081763267517,
      "learning_rate": 4.9622222222222225e-05,
      "loss": 0.0081,
      "step": 680
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 2.3200178146362305,
      "learning_rate": 4.961666666666667e-05,
      "loss": 0.0073,
      "step": 690
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 1.438421607017517,
      "learning_rate": 4.961111111111111e-05,
      "loss": 0.0063,
      "step": 700
    },
    {
      "epoch": 0.06311111111111112,
      "grad_norm": 0.3632861077785492,
      "learning_rate": 4.960555555555556e-05,
      "loss": 0.0065,
      "step": 710
    },
    {
      "epoch": 0.064,
      "grad_norm": 2.2726526260375977,
      "learning_rate": 4.96e-05,
      "loss": 0.0067,
      "step": 720
    },
    {
      "epoch": 0.06488888888888888,
      "grad_norm": 2.271672010421753,
      "learning_rate": 4.959444444444445e-05,
      "loss": 0.0068,
      "step": 730
    },
    {
      "epoch": 0.06577777777777778,
      "grad_norm": 1.1649880409240723,
      "learning_rate": 4.958888888888889e-05,
      "loss": 0.0068,
      "step": 740
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.4472302496433258,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.0054,
      "step": 750
    },
    {
      "epoch": 0.06755555555555555,
      "grad_norm": 0.16842208802700043,
      "learning_rate": 4.957777777777778e-05,
      "loss": 0.0049,
      "step": 760
    },
    {
      "epoch": 0.06844444444444445,
      "grad_norm": 1.4896936416625977,
      "learning_rate": 4.9572222222222224e-05,
      "loss": 0.0066,
      "step": 770
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 2.1779630184173584,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.005,
      "step": 780
    },
    {
      "epoch": 0.07022222222222223,
      "grad_norm": 0.448914110660553,
      "learning_rate": 4.956111111111111e-05,
      "loss": 0.007,
      "step": 790
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.3958176374435425,
      "learning_rate": 4.955555555555556e-05,
      "loss": 0.0061,
      "step": 800
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.331656813621521,
      "learning_rate": 4.9550000000000005e-05,
      "loss": 0.0051,
      "step": 810
    },
    {
      "epoch": 0.07288888888888889,
      "grad_norm": 0.1987321525812149,
      "learning_rate": 4.954444444444445e-05,
      "loss": 0.0051,
      "step": 820
    },
    {
      "epoch": 0.07377777777777778,
      "grad_norm": 0.6386163830757141,
      "learning_rate": 4.953888888888889e-05,
      "loss": 0.005,
      "step": 830
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 1.7559609413146973,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0047,
      "step": 840
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 0.28548145294189453,
      "learning_rate": 4.952777777777778e-05,
      "loss": 0.0062,
      "step": 850
    },
    {
      "epoch": 0.07644444444444444,
      "grad_norm": 1.5057779550552368,
      "learning_rate": 4.952222222222222e-05,
      "loss": 0.0057,
      "step": 860
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 1.7362505197525024,
      "learning_rate": 4.9516666666666666e-05,
      "loss": 0.0055,
      "step": 870
    },
    {
      "epoch": 0.07822222222222222,
      "grad_norm": 0.8092470169067383,
      "learning_rate": 4.951111111111112e-05,
      "loss": 0.0058,
      "step": 880
    },
    {
      "epoch": 0.0791111111111111,
      "grad_norm": 0.7407155632972717,
      "learning_rate": 4.950555555555556e-05,
      "loss": 0.0053,
      "step": 890
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.3022291660308838,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.0061,
      "step": 900
    },
    {
      "epoch": 0.08088888888888889,
      "grad_norm": 1.0012919902801514,
      "learning_rate": 4.949444444444445e-05,
      "loss": 0.0052,
      "step": 910
    },
    {
      "epoch": 0.08177777777777778,
      "grad_norm": 2.237887144088745,
      "learning_rate": 4.948888888888889e-05,
      "loss": 0.0069,
      "step": 920
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.33652326464653015,
      "learning_rate": 4.9483333333333334e-05,
      "loss": 0.0036,
      "step": 930
    },
    {
      "epoch": 0.08355555555555555,
      "grad_norm": 1.0798547267913818,
      "learning_rate": 4.947777777777778e-05,
      "loss": 0.0067,
      "step": 940
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 1.7997081279754639,
      "learning_rate": 4.947222222222223e-05,
      "loss": 0.0056,
      "step": 950
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 1.7363988161087036,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0064,
      "step": 960
    },
    {
      "epoch": 0.08622222222222223,
      "grad_norm": 0.6661262512207031,
      "learning_rate": 4.9461111111111115e-05,
      "loss": 0.006,
      "step": 970
    },
    {
      "epoch": 0.08711111111111111,
      "grad_norm": 1.4021754264831543,
      "learning_rate": 4.945555555555556e-05,
      "loss": 0.0052,
      "step": 980
    },
    {
      "epoch": 0.088,
      "grad_norm": 2.10947585105896,
      "learning_rate": 4.945e-05,
      "loss": 0.0051,
      "step": 990
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.12787817418575287,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 0.0037,
      "step": 1000
    },
    {
      "epoch": 0.08977777777777778,
      "grad_norm": 0.5389087200164795,
      "learning_rate": 4.943888888888889e-05,
      "loss": 0.0045,
      "step": 1010
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 1.7750924825668335,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.0063,
      "step": 1020
    },
    {
      "epoch": 0.09155555555555556,
      "grad_norm": 0.28718116879463196,
      "learning_rate": 4.942777777777778e-05,
      "loss": 0.0068,
      "step": 1030
    },
    {
      "epoch": 0.09244444444444444,
      "grad_norm": 1.9101507663726807,
      "learning_rate": 4.942222222222223e-05,
      "loss": 0.0064,
      "step": 1040
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 1.486966848373413,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.0058,
      "step": 1050
    },
    {
      "epoch": 0.09422222222222222,
      "grad_norm": 1.2857941389083862,
      "learning_rate": 4.9411111111111114e-05,
      "loss": 0.0044,
      "step": 1060
    },
    {
      "epoch": 0.0951111111111111,
      "grad_norm": 1.8420605659484863,
      "learning_rate": 4.940555555555556e-05,
      "loss": 0.0047,
      "step": 1070
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.2348768711090088,
      "learning_rate": 4.94e-05,
      "loss": 0.0056,
      "step": 1080
    },
    {
      "epoch": 0.09688888888888889,
      "grad_norm": 1.6129757165908813,
      "learning_rate": 4.939444444444445e-05,
      "loss": 0.0058,
      "step": 1090
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 0.8646091818809509,
      "learning_rate": 4.938888888888889e-05,
      "loss": 0.0052,
      "step": 1100
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.1566975861787796,
      "learning_rate": 4.938333333333334e-05,
      "loss": 0.0057,
      "step": 1110
    },
    {
      "epoch": 0.09955555555555555,
      "grad_norm": 1.2520116567611694,
      "learning_rate": 4.9377777777777776e-05,
      "loss": 0.0051,
      "step": 1120
    },
    {
      "epoch": 0.10044444444444445,
      "grad_norm": 0.4515610337257385,
      "learning_rate": 4.9372222222222226e-05,
      "loss": 0.0059,
      "step": 1130
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 1.1569827795028687,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.0041,
      "step": 1140
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 2.0590686798095703,
      "learning_rate": 4.936111111111111e-05,
      "loss": 0.0051,
      "step": 1150
    },
    {
      "epoch": 0.10311111111111111,
      "grad_norm": 0.25449880957603455,
      "learning_rate": 4.935555555555556e-05,
      "loss": 0.0051,
      "step": 1160
    },
    {
      "epoch": 0.104,
      "grad_norm": 1.76955246925354,
      "learning_rate": 4.935e-05,
      "loss": 0.0059,
      "step": 1170
    },
    {
      "epoch": 0.10488888888888889,
      "grad_norm": 1.8492486476898193,
      "learning_rate": 4.934444444444445e-05,
      "loss": 0.0046,
      "step": 1180
    },
    {
      "epoch": 0.10577777777777778,
      "grad_norm": 0.20431038737297058,
      "learning_rate": 4.933888888888889e-05,
      "loss": 0.0051,
      "step": 1190
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.3272477090358734,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0047,
      "step": 1200
    },
    {
      "epoch": 0.10755555555555556,
      "grad_norm": 0.9884911179542542,
      "learning_rate": 4.932777777777778e-05,
      "loss": 0.0048,
      "step": 1210
    },
    {
      "epoch": 0.10844444444444444,
      "grad_norm": 1.0933648347854614,
      "learning_rate": 4.9322222222222225e-05,
      "loss": 0.0058,
      "step": 1220
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.3109815716743469,
      "learning_rate": 4.931666666666667e-05,
      "loss": 0.0053,
      "step": 1230
    },
    {
      "epoch": 0.11022222222222222,
      "grad_norm": 0.990935206413269,
      "learning_rate": 4.931111111111111e-05,
      "loss": 0.004,
      "step": 1240
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.2224503755569458,
      "learning_rate": 4.930555555555556e-05,
      "loss": 0.0034,
      "step": 1250
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.2573341131210327,
      "learning_rate": 4.93e-05,
      "loss": 0.0054,
      "step": 1260
    },
    {
      "epoch": 0.11288888888888889,
      "grad_norm": 2.049590826034546,
      "learning_rate": 4.929444444444445e-05,
      "loss": 0.0058,
      "step": 1270
    },
    {
      "epoch": 0.11377777777777778,
      "grad_norm": 0.37066566944122314,
      "learning_rate": 4.928888888888889e-05,
      "loss": 0.0051,
      "step": 1280
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 2.014678955078125,
      "learning_rate": 4.9283333333333336e-05,
      "loss": 0.006,
      "step": 1290
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 1.355678915977478,
      "learning_rate": 4.927777777777778e-05,
      "loss": 0.005,
      "step": 1300
    },
    {
      "epoch": 0.11644444444444445,
      "grad_norm": 1.5733685493469238,
      "learning_rate": 4.9272222222222223e-05,
      "loss": 0.0044,
      "step": 1310
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.40073707699775696,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0053,
      "step": 1320
    },
    {
      "epoch": 0.11822222222222223,
      "grad_norm": 0.9149960875511169,
      "learning_rate": 4.926111111111111e-05,
      "loss": 0.0054,
      "step": 1330
    },
    {
      "epoch": 0.11911111111111111,
      "grad_norm": 0.9749646782875061,
      "learning_rate": 4.925555555555556e-05,
      "loss": 0.0057,
      "step": 1340
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.3862179517745972,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.0055,
      "step": 1350
    },
    {
      "epoch": 0.12088888888888889,
      "grad_norm": 1.1034302711486816,
      "learning_rate": 4.924444444444445e-05,
      "loss": 0.0048,
      "step": 1360
    },
    {
      "epoch": 0.12177777777777778,
      "grad_norm": 1.2900527715682983,
      "learning_rate": 4.923888888888889e-05,
      "loss": 0.0042,
      "step": 1370
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 1.778457760810852,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.0048,
      "step": 1380
    },
    {
      "epoch": 0.12355555555555556,
      "grad_norm": 1.780293583869934,
      "learning_rate": 4.922777777777778e-05,
      "loss": 0.005,
      "step": 1390
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 1.993467092514038,
      "learning_rate": 4.922222222222222e-05,
      "loss": 0.0044,
      "step": 1400
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.7084925770759583,
      "learning_rate": 4.9216666666666666e-05,
      "loss": 0.0038,
      "step": 1410
    },
    {
      "epoch": 0.12622222222222224,
      "grad_norm": 1.924585223197937,
      "learning_rate": 4.9211111111111116e-05,
      "loss": 0.0035,
      "step": 1420
    },
    {
      "epoch": 0.12711111111111112,
      "grad_norm": 0.49325481057167053,
      "learning_rate": 4.920555555555556e-05,
      "loss": 0.0052,
      "step": 1430
    },
    {
      "epoch": 0.128,
      "grad_norm": 1.241200566291809,
      "learning_rate": 4.92e-05,
      "loss": 0.005,
      "step": 1440
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 0.3261032700538635,
      "learning_rate": 4.919444444444445e-05,
      "loss": 0.0054,
      "step": 1450
    },
    {
      "epoch": 0.12977777777777777,
      "grad_norm": 1.4705634117126465,
      "learning_rate": 4.918888888888889e-05,
      "loss": 0.0032,
      "step": 1460
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.27766817808151245,
      "learning_rate": 4.9183333333333334e-05,
      "loss": 0.0047,
      "step": 1470
    },
    {
      "epoch": 0.13155555555555556,
      "grad_norm": 1.560019850730896,
      "learning_rate": 4.917777777777778e-05,
      "loss": 0.0039,
      "step": 1480
    },
    {
      "epoch": 0.13244444444444445,
      "grad_norm": 0.8511191010475159,
      "learning_rate": 4.917222222222223e-05,
      "loss": 0.0042,
      "step": 1490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.1363540142774582,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0047,
      "step": 1500
    },
    {
      "epoch": 0.13422222222222221,
      "grad_norm": 2.171525001525879,
      "learning_rate": 4.9161111111111115e-05,
      "loss": 0.0053,
      "step": 1510
    },
    {
      "epoch": 0.1351111111111111,
      "grad_norm": 1.2301058769226074,
      "learning_rate": 4.915555555555556e-05,
      "loss": 0.0052,
      "step": 1520
    },
    {
      "epoch": 0.136,
      "grad_norm": 2.1419639587402344,
      "learning_rate": 4.915e-05,
      "loss": 0.0073,
      "step": 1530
    },
    {
      "epoch": 0.1368888888888889,
      "grad_norm": 1.407847285270691,
      "learning_rate": 4.9144444444444446e-05,
      "loss": 0.0057,
      "step": 1540
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 1.878830075263977,
      "learning_rate": 4.913888888888889e-05,
      "loss": 0.0063,
      "step": 1550
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 1.6310539245605469,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0055,
      "step": 1560
    },
    {
      "epoch": 0.13955555555555554,
      "grad_norm": 0.9532651901245117,
      "learning_rate": 4.9127777777777776e-05,
      "loss": 0.0034,
      "step": 1570
    },
    {
      "epoch": 0.14044444444444446,
      "grad_norm": 0.6700083017349243,
      "learning_rate": 4.912222222222223e-05,
      "loss": 0.0057,
      "step": 1580
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.8101390600204468,
      "learning_rate": 4.9116666666666663e-05,
      "loss": 0.0054,
      "step": 1590
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 1.2290120124816895,
      "learning_rate": 4.9111111111111114e-05,
      "loss": 0.0052,
      "step": 1600
    },
    {
      "epoch": 0.1431111111111111,
      "grad_norm": 0.6923857927322388,
      "learning_rate": 4.910555555555556e-05,
      "loss": 0.0057,
      "step": 1610
    },
    {
      "epoch": 0.144,
      "grad_norm": 1.4206546545028687,
      "learning_rate": 4.91e-05,
      "loss": 0.0045,
      "step": 1620
    },
    {
      "epoch": 0.1448888888888889,
      "grad_norm": 1.7094272375106812,
      "learning_rate": 4.909444444444445e-05,
      "loss": 0.0041,
      "step": 1630
    },
    {
      "epoch": 0.14577777777777778,
      "grad_norm": 0.2904006540775299,
      "learning_rate": 4.908888888888889e-05,
      "loss": 0.0049,
      "step": 1640
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 1.0079375505447388,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.0051,
      "step": 1650
    },
    {
      "epoch": 0.14755555555555555,
      "grad_norm": 1.2892345190048218,
      "learning_rate": 4.9077777777777775e-05,
      "loss": 0.0046,
      "step": 1660
    },
    {
      "epoch": 0.14844444444444443,
      "grad_norm": 1.5445280075073242,
      "learning_rate": 4.9072222222222225e-05,
      "loss": 0.0048,
      "step": 1670
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 1.0964292287826538,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0051,
      "step": 1680
    },
    {
      "epoch": 0.15022222222222223,
      "grad_norm": 0.6758669018745422,
      "learning_rate": 4.906111111111111e-05,
      "loss": 0.0044,
      "step": 1690
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 0.34991317987442017,
      "learning_rate": 4.905555555555556e-05,
      "loss": 0.0047,
      "step": 1700
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.27889305353164673,
      "learning_rate": 4.905e-05,
      "loss": 0.0033,
      "step": 1710
    },
    {
      "epoch": 0.15288888888888888,
      "grad_norm": 1.5726476907730103,
      "learning_rate": 4.904444444444445e-05,
      "loss": 0.0041,
      "step": 1720
    },
    {
      "epoch": 0.1537777777777778,
      "grad_norm": 0.5948866605758667,
      "learning_rate": 4.903888888888889e-05,
      "loss": 0.0042,
      "step": 1730
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.7430245876312256,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.0038,
      "step": 1740
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.5202186107635498,
      "learning_rate": 4.902777777777778e-05,
      "loss": 0.0046,
      "step": 1750
    },
    {
      "epoch": 0.15644444444444444,
      "grad_norm": 1.0539664030075073,
      "learning_rate": 4.9022222222222224e-05,
      "loss": 0.003,
      "step": 1760
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.8111815452575684,
      "learning_rate": 4.901666666666667e-05,
      "loss": 0.0036,
      "step": 1770
    },
    {
      "epoch": 0.1582222222222222,
      "grad_norm": 1.5616685152053833,
      "learning_rate": 4.901111111111111e-05,
      "loss": 0.0052,
      "step": 1780
    },
    {
      "epoch": 0.15911111111111112,
      "grad_norm": 1.9644194841384888,
      "learning_rate": 4.900555555555556e-05,
      "loss": 0.0045,
      "step": 1790
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.3608667850494385,
      "learning_rate": 4.9e-05,
      "loss": 0.0058,
      "step": 1800
    },
    {
      "epoch": 0.1608888888888889,
      "grad_norm": 1.137032389640808,
      "learning_rate": 4.899444444444445e-05,
      "loss": 0.0045,
      "step": 1810
    },
    {
      "epoch": 0.16177777777777777,
      "grad_norm": 1.5350701808929443,
      "learning_rate": 4.898888888888889e-05,
      "loss": 0.0045,
      "step": 1820
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.7069345116615295,
      "learning_rate": 4.8983333333333336e-05,
      "loss": 0.0047,
      "step": 1830
    },
    {
      "epoch": 0.16355555555555557,
      "grad_norm": 0.6255955100059509,
      "learning_rate": 4.897777777777778e-05,
      "loss": 0.0038,
      "step": 1840
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 0.9117360711097717,
      "learning_rate": 4.897222222222222e-05,
      "loss": 0.005,
      "step": 1850
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 1.621596097946167,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.0042,
      "step": 1860
    },
    {
      "epoch": 0.16622222222222222,
      "grad_norm": 1.3298892974853516,
      "learning_rate": 4.896111111111111e-05,
      "loss": 0.0049,
      "step": 1870
    },
    {
      "epoch": 0.1671111111111111,
      "grad_norm": 1.2578561305999756,
      "learning_rate": 4.895555555555556e-05,
      "loss": 0.0042,
      "step": 1880
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.45236340165138245,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 0.004,
      "step": 1890
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 1.3257943391799927,
      "learning_rate": 4.894444444444445e-05,
      "loss": 0.0062,
      "step": 1900
    },
    {
      "epoch": 0.16977777777777778,
      "grad_norm": 1.3598294258117676,
      "learning_rate": 4.893888888888889e-05,
      "loss": 0.0041,
      "step": 1910
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.2346586287021637,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0038,
      "step": 1920
    },
    {
      "epoch": 0.17155555555555554,
      "grad_norm": 0.5419094562530518,
      "learning_rate": 4.892777777777778e-05,
      "loss": 0.004,
      "step": 1930
    },
    {
      "epoch": 0.17244444444444446,
      "grad_norm": 0.7787532806396484,
      "learning_rate": 4.892222222222222e-05,
      "loss": 0.0041,
      "step": 1940
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 1.6218338012695312,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.004,
      "step": 1950
    },
    {
      "epoch": 0.17422222222222222,
      "grad_norm": 1.5489442348480225,
      "learning_rate": 4.8911111111111116e-05,
      "loss": 0.0046,
      "step": 1960
    },
    {
      "epoch": 0.1751111111111111,
      "grad_norm": 1.497907042503357,
      "learning_rate": 4.890555555555556e-05,
      "loss": 0.0044,
      "step": 1970
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.8301829099655151,
      "learning_rate": 4.89e-05,
      "loss": 0.0043,
      "step": 1980
    },
    {
      "epoch": 0.1768888888888889,
      "grad_norm": 0.3198533356189728,
      "learning_rate": 4.8894444444444446e-05,
      "loss": 0.0033,
      "step": 1990
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.74496328830719,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.0034,
      "step": 2000
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.8938893675804138,
      "learning_rate": 4.8883333333333333e-05,
      "loss": 0.0029,
      "step": 2010
    },
    {
      "epoch": 0.17955555555555555,
      "grad_norm": 1.9491313695907593,
      "learning_rate": 4.887777777777778e-05,
      "loss": 0.0044,
      "step": 2020
    },
    {
      "epoch": 0.18044444444444444,
      "grad_norm": 0.6242948770523071,
      "learning_rate": 4.887222222222223e-05,
      "loss": 0.0042,
      "step": 2030
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.3594146966934204,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0038,
      "step": 2040
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 0.9281483292579651,
      "learning_rate": 4.8861111111111114e-05,
      "loss": 0.003,
      "step": 2050
    },
    {
      "epoch": 0.1831111111111111,
      "grad_norm": 0.7043443322181702,
      "learning_rate": 4.885555555555556e-05,
      "loss": 0.0036,
      "step": 2060
    },
    {
      "epoch": 0.184,
      "grad_norm": 1.569361925125122,
      "learning_rate": 4.885e-05,
      "loss": 0.0039,
      "step": 2070
    },
    {
      "epoch": 0.18488888888888888,
      "grad_norm": 1.3971158266067505,
      "learning_rate": 4.8844444444444445e-05,
      "loss": 0.0042,
      "step": 2080
    },
    {
      "epoch": 0.18577777777777776,
      "grad_norm": 1.4188423156738281,
      "learning_rate": 4.883888888888889e-05,
      "loss": 0.003,
      "step": 2090
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.28980907797813416,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0046,
      "step": 2100
    },
    {
      "epoch": 0.18755555555555556,
      "grad_norm": 1.1480830907821655,
      "learning_rate": 4.8827777777777776e-05,
      "loss": 0.0045,
      "step": 2110
    },
    {
      "epoch": 0.18844444444444444,
      "grad_norm": 0.20737431943416595,
      "learning_rate": 4.8822222222222226e-05,
      "loss": 0.0035,
      "step": 2120
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 1.8343210220336914,
      "learning_rate": 4.881666666666667e-05,
      "loss": 0.0046,
      "step": 2130
    },
    {
      "epoch": 0.1902222222222222,
      "grad_norm": 0.4470334053039551,
      "learning_rate": 4.881111111111111e-05,
      "loss": 0.0029,
      "step": 2140
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 0.5224087834358215,
      "learning_rate": 4.880555555555556e-05,
      "loss": 0.0049,
      "step": 2150
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.3545196056365967,
      "learning_rate": 4.88e-05,
      "loss": 0.0036,
      "step": 2160
    },
    {
      "epoch": 0.1928888888888889,
      "grad_norm": 0.8354471921920776,
      "learning_rate": 4.879444444444445e-05,
      "loss": 0.0041,
      "step": 2170
    },
    {
      "epoch": 0.19377777777777777,
      "grad_norm": 1.813744306564331,
      "learning_rate": 4.878888888888889e-05,
      "loss": 0.0039,
      "step": 2180
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 1.6954097747802734,
      "learning_rate": 4.878333333333334e-05,
      "loss": 0.0045,
      "step": 2190
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 1.739729642868042,
      "learning_rate": 4.8777777777777775e-05,
      "loss": 0.0045,
      "step": 2200
    },
    {
      "epoch": 0.19644444444444445,
      "grad_norm": 0.15665645897388458,
      "learning_rate": 4.8772222222222225e-05,
      "loss": 0.0041,
      "step": 2210
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.6781585812568665,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0053,
      "step": 2220
    },
    {
      "epoch": 0.19822222222222222,
      "grad_norm": 0.26567649841308594,
      "learning_rate": 4.876111111111111e-05,
      "loss": 0.004,
      "step": 2230
    },
    {
      "epoch": 0.1991111111111111,
      "grad_norm": 1.3143529891967773,
      "learning_rate": 4.875555555555556e-05,
      "loss": 0.0041,
      "step": 2240
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.37681248784065247,
      "learning_rate": 4.875e-05,
      "loss": 0.0042,
      "step": 2250
    },
    {
      "epoch": 0.2008888888888889,
      "grad_norm": 1.0848761796951294,
      "learning_rate": 4.874444444444445e-05,
      "loss": 0.0037,
      "step": 2260
    },
    {
      "epoch": 0.20177777777777778,
      "grad_norm": 2.017284870147705,
      "learning_rate": 4.8738888888888886e-05,
      "loss": 0.0054,
      "step": 2270
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.577248215675354,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0038,
      "step": 2280
    },
    {
      "epoch": 0.20355555555555555,
      "grad_norm": 0.653678834438324,
      "learning_rate": 4.872777777777778e-05,
      "loss": 0.0035,
      "step": 2290
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 0.5289908647537231,
      "learning_rate": 4.8722222222222224e-05,
      "loss": 0.0031,
      "step": 2300
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.31865426898002625,
      "learning_rate": 4.8716666666666674e-05,
      "loss": 0.0032,
      "step": 2310
    },
    {
      "epoch": 0.20622222222222222,
      "grad_norm": 1.0605168342590332,
      "learning_rate": 4.871111111111111e-05,
      "loss": 0.0037,
      "step": 2320
    },
    {
      "epoch": 0.2071111111111111,
      "grad_norm": 0.9542979598045349,
      "learning_rate": 4.870555555555556e-05,
      "loss": 0.0053,
      "step": 2330
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.922962486743927,
      "learning_rate": 4.87e-05,
      "loss": 0.0039,
      "step": 2340
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 1.1696161031723022,
      "learning_rate": 4.869444444444445e-05,
      "loss": 0.0041,
      "step": 2350
    },
    {
      "epoch": 0.20977777777777779,
      "grad_norm": 0.4276700019836426,
      "learning_rate": 4.868888888888889e-05,
      "loss": 0.0037,
      "step": 2360
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.8571816086769104,
      "learning_rate": 4.8683333333333335e-05,
      "loss": 0.0032,
      "step": 2370
    },
    {
      "epoch": 0.21155555555555555,
      "grad_norm": 0.3120056390762329,
      "learning_rate": 4.867777777777778e-05,
      "loss": 0.0033,
      "step": 2380
    },
    {
      "epoch": 0.21244444444444444,
      "grad_norm": 0.44615161418914795,
      "learning_rate": 4.867222222222222e-05,
      "loss": 0.0035,
      "step": 2390
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.610835611820221,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0049,
      "step": 2400
    },
    {
      "epoch": 0.21422222222222223,
      "grad_norm": 1.4500356912612915,
      "learning_rate": 4.866111111111111e-05,
      "loss": 0.004,
      "step": 2410
    },
    {
      "epoch": 0.21511111111111111,
      "grad_norm": 1.3556379079818726,
      "learning_rate": 4.865555555555556e-05,
      "loss": 0.0042,
      "step": 2420
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.6599253416061401,
      "learning_rate": 4.8650000000000003e-05,
      "loss": 0.0034,
      "step": 2430
    },
    {
      "epoch": 0.21688888888888888,
      "grad_norm": 1.2565243244171143,
      "learning_rate": 4.864444444444445e-05,
      "loss": 0.0039,
      "step": 2440
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 0.8651545643806458,
      "learning_rate": 4.863888888888889e-05,
      "loss": 0.0048,
      "step": 2450
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.36136600375175476,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0038,
      "step": 2460
    },
    {
      "epoch": 0.21955555555555556,
      "grad_norm": 1.7308812141418457,
      "learning_rate": 4.862777777777778e-05,
      "loss": 0.0043,
      "step": 2470
    },
    {
      "epoch": 0.22044444444444444,
      "grad_norm": 1.290440320968628,
      "learning_rate": 4.862222222222222e-05,
      "loss": 0.0057,
      "step": 2480
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.6297768950462341,
      "learning_rate": 4.861666666666667e-05,
      "loss": 0.0041,
      "step": 2490
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.3150780200958252,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 0.004,
      "step": 2500
    },
    {
      "epoch": 0.22311111111111112,
      "grad_norm": 1.0321428775787354,
      "learning_rate": 4.860555555555556e-05,
      "loss": 0.0035,
      "step": 2510
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.9928624629974365,
      "learning_rate": 4.86e-05,
      "loss": 0.0045,
      "step": 2520
    },
    {
      "epoch": 0.2248888888888889,
      "grad_norm": 1.1302227973937988,
      "learning_rate": 4.8594444444444446e-05,
      "loss": 0.0039,
      "step": 2530
    },
    {
      "epoch": 0.22577777777777777,
      "grad_norm": 0.2672201097011566,
      "learning_rate": 4.858888888888889e-05,
      "loss": 0.0037,
      "step": 2540
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 1.758573055267334,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.0044,
      "step": 2550
    },
    {
      "epoch": 0.22755555555555557,
      "grad_norm": 1.3508987426757812,
      "learning_rate": 4.8577777777777776e-05,
      "loss": 0.0043,
      "step": 2560
    },
    {
      "epoch": 0.22844444444444445,
      "grad_norm": 0.34830695390701294,
      "learning_rate": 4.857222222222223e-05,
      "loss": 0.0043,
      "step": 2570
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.5875747799873352,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0046,
      "step": 2580
    },
    {
      "epoch": 0.23022222222222222,
      "grad_norm": 1.6912503242492676,
      "learning_rate": 4.8561111111111114e-05,
      "loss": 0.0032,
      "step": 2590
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.65300053358078,
      "learning_rate": 4.855555555555556e-05,
      "loss": 0.005,
      "step": 2600
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.5038430690765381,
      "learning_rate": 4.855e-05,
      "loss": 0.0043,
      "step": 2610
    },
    {
      "epoch": 0.2328888888888889,
      "grad_norm": 0.3129998445510864,
      "learning_rate": 4.8544444444444445e-05,
      "loss": 0.0036,
      "step": 2620
    },
    {
      "epoch": 0.23377777777777778,
      "grad_norm": 1.5940628051757812,
      "learning_rate": 4.853888888888889e-05,
      "loss": 0.0023,
      "step": 2630
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.8352364897727966,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0046,
      "step": 2640
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 1.0680426359176636,
      "learning_rate": 4.8527777777777775e-05,
      "loss": 0.0046,
      "step": 2650
    },
    {
      "epoch": 0.23644444444444446,
      "grad_norm": 1.0076998472213745,
      "learning_rate": 4.8522222222222226e-05,
      "loss": 0.0036,
      "step": 2660
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 1.3355910778045654,
      "learning_rate": 4.851666666666667e-05,
      "loss": 0.0035,
      "step": 2670
    },
    {
      "epoch": 0.23822222222222222,
      "grad_norm": 0.29713737964630127,
      "learning_rate": 4.851111111111111e-05,
      "loss": 0.0033,
      "step": 2680
    },
    {
      "epoch": 0.2391111111111111,
      "grad_norm": 1.4196608066558838,
      "learning_rate": 4.8505555555555556e-05,
      "loss": 0.0045,
      "step": 2690
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.47394999861717224,
      "learning_rate": 4.85e-05,
      "loss": 0.0054,
      "step": 2700
    },
    {
      "epoch": 0.2408888888888889,
      "grad_norm": 0.20457562804222107,
      "learning_rate": 4.849444444444445e-05,
      "loss": 0.005,
      "step": 2710
    },
    {
      "epoch": 0.24177777777777779,
      "grad_norm": 0.7117094397544861,
      "learning_rate": 4.848888888888889e-05,
      "loss": 0.0034,
      "step": 2720
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 1.7344579696655273,
      "learning_rate": 4.848333333333334e-05,
      "loss": 0.0051,
      "step": 2730
    },
    {
      "epoch": 0.24355555555555555,
      "grad_norm": 0.7677643895149231,
      "learning_rate": 4.847777777777778e-05,
      "loss": 0.0041,
      "step": 2740
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.3305893540382385,
      "learning_rate": 4.8472222222222224e-05,
      "loss": 0.004,
      "step": 2750
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 1.628044605255127,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0056,
      "step": 2760
    },
    {
      "epoch": 0.24622222222222223,
      "grad_norm": 1.2158715724945068,
      "learning_rate": 4.846111111111111e-05,
      "loss": 0.0035,
      "step": 2770
    },
    {
      "epoch": 0.24711111111111111,
      "grad_norm": 0.40428489446640015,
      "learning_rate": 4.845555555555556e-05,
      "loss": 0.0034,
      "step": 2780
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.4135515093803406,
      "learning_rate": 4.845e-05,
      "loss": 0.0037,
      "step": 2790
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.4250476658344269,
      "learning_rate": 4.844444444444445e-05,
      "loss": 0.0044,
      "step": 2800
    },
    {
      "epoch": 0.24977777777777777,
      "grad_norm": 0.4079533815383911,
      "learning_rate": 4.843888888888889e-05,
      "loss": 0.0029,
      "step": 2810
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.15310873091220856,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.0032,
      "step": 2820
    },
    {
      "epoch": 0.25155555555555553,
      "grad_norm": 1.0110862255096436,
      "learning_rate": 4.842777777777778e-05,
      "loss": 0.0039,
      "step": 2830
    },
    {
      "epoch": 0.25244444444444447,
      "grad_norm": 0.9491657018661499,
      "learning_rate": 4.842222222222222e-05,
      "loss": 0.0034,
      "step": 2840
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 1.839582920074463,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.0044,
      "step": 2850
    },
    {
      "epoch": 0.25422222222222224,
      "grad_norm": 1.5856291055679321,
      "learning_rate": 4.841111111111111e-05,
      "loss": 0.0044,
      "step": 2860
    },
    {
      "epoch": 0.2551111111111111,
      "grad_norm": 0.3423900008201599,
      "learning_rate": 4.840555555555556e-05,
      "loss": 0.0053,
      "step": 2870
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.1694028377532959,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0031,
      "step": 2880
    },
    {
      "epoch": 0.2568888888888889,
      "grad_norm": 0.1222953200340271,
      "learning_rate": 4.839444444444445e-05,
      "loss": 0.003,
      "step": 2890
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 1.1479908227920532,
      "learning_rate": 4.838888888888889e-05,
      "loss": 0.0033,
      "step": 2900
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 1.5737099647521973,
      "learning_rate": 4.8383333333333335e-05,
      "loss": 0.0067,
      "step": 2910
    },
    {
      "epoch": 0.25955555555555554,
      "grad_norm": 0.10319965332746506,
      "learning_rate": 4.837777777777778e-05,
      "loss": 0.0042,
      "step": 2920
    },
    {
      "epoch": 0.2604444444444444,
      "grad_norm": 0.21626029908657074,
      "learning_rate": 4.837222222222222e-05,
      "loss": 0.0032,
      "step": 2930
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.10679623484611511,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.004,
      "step": 2940
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 0.19823476672172546,
      "learning_rate": 4.8361111111111116e-05,
      "loss": 0.0038,
      "step": 2950
    },
    {
      "epoch": 0.26311111111111113,
      "grad_norm": 0.6609043478965759,
      "learning_rate": 4.835555555555556e-05,
      "loss": 0.0039,
      "step": 2960
    },
    {
      "epoch": 0.264,
      "grad_norm": 1.2951381206512451,
      "learning_rate": 4.835e-05,
      "loss": 0.0048,
      "step": 2970
    },
    {
      "epoch": 0.2648888888888889,
      "grad_norm": 1.6993709802627563,
      "learning_rate": 4.8344444444444447e-05,
      "loss": 0.0057,
      "step": 2980
    },
    {
      "epoch": 0.2657777777777778,
      "grad_norm": 0.7574601173400879,
      "learning_rate": 4.833888888888889e-05,
      "loss": 0.0044,
      "step": 2990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.664284110069275,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0041,
      "step": 3000
    },
    {
      "epoch": 0.26755555555555555,
      "grad_norm": 0.15832237899303436,
      "learning_rate": 4.832777777777778e-05,
      "loss": 0.0051,
      "step": 3010
    },
    {
      "epoch": 0.26844444444444443,
      "grad_norm": 0.5523068904876709,
      "learning_rate": 4.832222222222223e-05,
      "loss": 0.0034,
      "step": 3020
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 1.6166675090789795,
      "learning_rate": 4.831666666666667e-05,
      "loss": 0.0049,
      "step": 3030
    },
    {
      "epoch": 0.2702222222222222,
      "grad_norm": 0.23298971354961395,
      "learning_rate": 4.8311111111111115e-05,
      "loss": 0.0043,
      "step": 3040
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 1.593490719795227,
      "learning_rate": 4.830555555555556e-05,
      "loss": 0.0055,
      "step": 3050
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.3447561264038086,
      "learning_rate": 4.83e-05,
      "loss": 0.0061,
      "step": 3060
    },
    {
      "epoch": 0.2728888888888889,
      "grad_norm": 1.315528392791748,
      "learning_rate": 4.8294444444444445e-05,
      "loss": 0.0043,
      "step": 3070
    },
    {
      "epoch": 0.2737777777777778,
      "grad_norm": 0.3456258773803711,
      "learning_rate": 4.828888888888889e-05,
      "loss": 0.0035,
      "step": 3080
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.22598537802696228,
      "learning_rate": 4.828333333333334e-05,
      "loss": 0.0024,
      "step": 3090
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 0.6321589350700378,
      "learning_rate": 4.8277777777777776e-05,
      "loss": 0.0035,
      "step": 3100
    },
    {
      "epoch": 0.27644444444444444,
      "grad_norm": 0.2780330777168274,
      "learning_rate": 4.8272222222222226e-05,
      "loss": 0.0034,
      "step": 3110
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 2.085224151611328,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0041,
      "step": 3120
    },
    {
      "epoch": 0.2782222222222222,
      "grad_norm": 1.4496488571166992,
      "learning_rate": 4.8261111111111113e-05,
      "loss": 0.0039,
      "step": 3130
    },
    {
      "epoch": 0.2791111111111111,
      "grad_norm": 0.7941821813583374,
      "learning_rate": 4.825555555555556e-05,
      "loss": 0.0033,
      "step": 3140
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.2687126398086548,
      "learning_rate": 4.825e-05,
      "loss": 0.0047,
      "step": 3150
    },
    {
      "epoch": 0.2808888888888889,
      "grad_norm": 0.668154239654541,
      "learning_rate": 4.824444444444445e-05,
      "loss": 0.0048,
      "step": 3160
    },
    {
      "epoch": 0.2817777777777778,
      "grad_norm": 0.2122773379087448,
      "learning_rate": 4.823888888888889e-05,
      "loss": 0.0038,
      "step": 3170
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 1.5673247575759888,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.0036,
      "step": 3180
    },
    {
      "epoch": 0.28355555555555556,
      "grad_norm": 1.3963916301727295,
      "learning_rate": 4.822777777777778e-05,
      "loss": 0.0035,
      "step": 3190
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 1.5682871341705322,
      "learning_rate": 4.8222222222222225e-05,
      "loss": 0.0035,
      "step": 3200
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.4690450131893158,
      "learning_rate": 4.821666666666667e-05,
      "loss": 0.0028,
      "step": 3210
    },
    {
      "epoch": 0.2862222222222222,
      "grad_norm": 0.8779393434524536,
      "learning_rate": 4.821111111111111e-05,
      "loss": 0.0047,
      "step": 3220
    },
    {
      "epoch": 0.2871111111111111,
      "grad_norm": 0.2943074703216553,
      "learning_rate": 4.820555555555556e-05,
      "loss": 0.0028,
      "step": 3230
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.06743241101503372,
      "learning_rate": 4.82e-05,
      "loss": 0.0034,
      "step": 3240
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.9819375276565552,
      "learning_rate": 4.819444444444445e-05,
      "loss": 0.0047,
      "step": 3250
    },
    {
      "epoch": 0.2897777777777778,
      "grad_norm": 1.1751983165740967,
      "learning_rate": 4.8188888888888886e-05,
      "loss": 0.0042,
      "step": 3260
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.1399209201335907,
      "learning_rate": 4.818333333333334e-05,
      "loss": 0.0033,
      "step": 3270
    },
    {
      "epoch": 0.29155555555555557,
      "grad_norm": 0.3916890025138855,
      "learning_rate": 4.817777777777778e-05,
      "loss": 0.004,
      "step": 3280
    },
    {
      "epoch": 0.29244444444444445,
      "grad_norm": 2.135125160217285,
      "learning_rate": 4.8172222222222224e-05,
      "loss": 0.004,
      "step": 3290
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.6550684571266174,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.005,
      "step": 3300
    },
    {
      "epoch": 0.2942222222222222,
      "grad_norm": 1.0693432092666626,
      "learning_rate": 4.816111111111111e-05,
      "loss": 0.0059,
      "step": 3310
    },
    {
      "epoch": 0.2951111111111111,
      "grad_norm": 0.7950242161750793,
      "learning_rate": 4.815555555555556e-05,
      "loss": 0.005,
      "step": 3320
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.252400666475296,
      "learning_rate": 4.815e-05,
      "loss": 0.0038,
      "step": 3330
    },
    {
      "epoch": 0.29688888888888887,
      "grad_norm": 1.3048895597457886,
      "learning_rate": 4.814444444444445e-05,
      "loss": 0.0038,
      "step": 3340
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 1.2105038166046143,
      "learning_rate": 4.813888888888889e-05,
      "loss": 0.0032,
      "step": 3350
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.884843111038208,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0031,
      "step": 3360
    },
    {
      "epoch": 0.2995555555555556,
      "grad_norm": 0.42128077149391174,
      "learning_rate": 4.8127777777777786e-05,
      "loss": 0.0035,
      "step": 3370
    },
    {
      "epoch": 0.30044444444444446,
      "grad_norm": 0.44159942865371704,
      "learning_rate": 4.812222222222222e-05,
      "loss": 0.0034,
      "step": 3380
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.8344095349311829,
      "learning_rate": 4.811666666666667e-05,
      "loss": 0.0038,
      "step": 3390
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 1.3097678422927856,
      "learning_rate": 4.811111111111111e-05,
      "loss": 0.0035,
      "step": 3400
    },
    {
      "epoch": 0.3031111111111111,
      "grad_norm": 0.3908773362636566,
      "learning_rate": 4.810555555555556e-05,
      "loss": 0.0043,
      "step": 3410
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.6721104383468628,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0039,
      "step": 3420
    },
    {
      "epoch": 0.3048888888888889,
      "grad_norm": 0.8674739003181458,
      "learning_rate": 4.809444444444445e-05,
      "loss": 0.0034,
      "step": 3430
    },
    {
      "epoch": 0.30577777777777776,
      "grad_norm": 1.2589797973632812,
      "learning_rate": 4.808888888888889e-05,
      "loss": 0.0038,
      "step": 3440
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.43223845958709717,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.0035,
      "step": 3450
    },
    {
      "epoch": 0.3075555555555556,
      "grad_norm": 0.6338486075401306,
      "learning_rate": 4.8077777777777785e-05,
      "loss": 0.0034,
      "step": 3460
    },
    {
      "epoch": 0.30844444444444447,
      "grad_norm": 0.9471328854560852,
      "learning_rate": 4.807222222222222e-05,
      "loss": 0.0048,
      "step": 3470
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 1.1050599813461304,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0032,
      "step": 3480
    },
    {
      "epoch": 0.31022222222222223,
      "grad_norm": 0.11016958951950073,
      "learning_rate": 4.8061111111111115e-05,
      "loss": 0.0037,
      "step": 3490
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 1.1904544830322266,
      "learning_rate": 4.805555555555556e-05,
      "loss": 0.0035,
      "step": 3500
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.9797404408454895,
      "learning_rate": 4.805e-05,
      "loss": 0.0046,
      "step": 3510
    },
    {
      "epoch": 0.3128888888888889,
      "grad_norm": 1.025343894958496,
      "learning_rate": 4.8044444444444446e-05,
      "loss": 0.006,
      "step": 3520
    },
    {
      "epoch": 0.31377777777777777,
      "grad_norm": 1.318479061126709,
      "learning_rate": 4.803888888888889e-05,
      "loss": 0.0036,
      "step": 3530
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 1.0467196702957153,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.004,
      "step": 3540
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 1.0801975727081299,
      "learning_rate": 4.8027777777777783e-05,
      "loss": 0.0047,
      "step": 3550
    },
    {
      "epoch": 0.3164444444444444,
      "grad_norm": 0.4876480996608734,
      "learning_rate": 4.802222222222223e-05,
      "loss": 0.0028,
      "step": 3560
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 1.5317256450653076,
      "learning_rate": 4.801666666666667e-05,
      "loss": 0.004,
      "step": 3570
    },
    {
      "epoch": 0.31822222222222224,
      "grad_norm": 0.4313904941082001,
      "learning_rate": 4.8011111111111114e-05,
      "loss": 0.0032,
      "step": 3580
    },
    {
      "epoch": 0.3191111111111111,
      "grad_norm": 0.26516544818878174,
      "learning_rate": 4.800555555555556e-05,
      "loss": 0.0043,
      "step": 3590
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.8073723316192627,
      "learning_rate": 4.8e-05,
      "loss": 0.0051,
      "step": 3600
    },
    {
      "epoch": 0.3208888888888889,
      "grad_norm": 0.5057013630867004,
      "learning_rate": 4.7994444444444445e-05,
      "loss": 0.0032,
      "step": 3610
    },
    {
      "epoch": 0.3217777777777778,
      "grad_norm": 0.7027670741081238,
      "learning_rate": 4.798888888888889e-05,
      "loss": 0.0039,
      "step": 3620
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 1.5999466180801392,
      "learning_rate": 4.798333333333334e-05,
      "loss": 0.0036,
      "step": 3630
    },
    {
      "epoch": 0.32355555555555554,
      "grad_norm": 1.217632532119751,
      "learning_rate": 4.797777777777778e-05,
      "loss": 0.0035,
      "step": 3640
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 1.3800685405731201,
      "learning_rate": 4.7972222222222226e-05,
      "loss": 0.0027,
      "step": 3650
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.3405367434024811,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0043,
      "step": 3660
    },
    {
      "epoch": 0.32622222222222225,
      "grad_norm": 0.7177537083625793,
      "learning_rate": 4.796111111111111e-05,
      "loss": 0.003,
      "step": 3670
    },
    {
      "epoch": 0.32711111111111113,
      "grad_norm": 0.9300025701522827,
      "learning_rate": 4.7955555555555556e-05,
      "loss": 0.0026,
      "step": 3680
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.6204619407653809,
      "learning_rate": 4.795e-05,
      "loss": 0.0047,
      "step": 3690
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 0.5798526406288147,
      "learning_rate": 4.794444444444445e-05,
      "loss": 0.0037,
      "step": 3700
    },
    {
      "epoch": 0.3297777777777778,
      "grad_norm": 0.48287633061408997,
      "learning_rate": 4.793888888888889e-05,
      "loss": 0.0027,
      "step": 3710
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.7489743232727051,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0029,
      "step": 3720
    },
    {
      "epoch": 0.33155555555555555,
      "grad_norm": 0.3235376179218292,
      "learning_rate": 4.792777777777778e-05,
      "loss": 0.0035,
      "step": 3730
    },
    {
      "epoch": 0.33244444444444443,
      "grad_norm": 0.25667160749435425,
      "learning_rate": 4.7922222222222225e-05,
      "loss": 0.0042,
      "step": 3740
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.0162962675094604,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.0048,
      "step": 3750
    },
    {
      "epoch": 0.3342222222222222,
      "grad_norm": 0.11886440217494965,
      "learning_rate": 4.791111111111111e-05,
      "loss": 0.0033,
      "step": 3760
    },
    {
      "epoch": 0.33511111111111114,
      "grad_norm": 0.5364112257957458,
      "learning_rate": 4.790555555555556e-05,
      "loss": 0.0044,
      "step": 3770
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.2997666597366333,
      "learning_rate": 4.79e-05,
      "loss": 0.0029,
      "step": 3780
    },
    {
      "epoch": 0.3368888888888889,
      "grad_norm": 0.38588041067123413,
      "learning_rate": 4.789444444444445e-05,
      "loss": 0.0037,
      "step": 3790
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.9611064195632935,
      "learning_rate": 4.7888888888888886e-05,
      "loss": 0.0035,
      "step": 3800
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.9553822875022888,
      "learning_rate": 4.7883333333333336e-05,
      "loss": 0.0034,
      "step": 3810
    },
    {
      "epoch": 0.33955555555555555,
      "grad_norm": 1.219756007194519,
      "learning_rate": 4.787777777777778e-05,
      "loss": 0.0038,
      "step": 3820
    },
    {
      "epoch": 0.34044444444444444,
      "grad_norm": 0.42196124792099,
      "learning_rate": 4.787222222222222e-05,
      "loss": 0.0049,
      "step": 3830
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.49010705947875977,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0026,
      "step": 3840
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 1.2961982488632202,
      "learning_rate": 4.786111111111111e-05,
      "loss": 0.0037,
      "step": 3850
    },
    {
      "epoch": 0.3431111111111111,
      "grad_norm": 0.6609856486320496,
      "learning_rate": 4.785555555555556e-05,
      "loss": 0.0041,
      "step": 3860
    },
    {
      "epoch": 0.344,
      "grad_norm": 1.1894245147705078,
      "learning_rate": 4.785e-05,
      "loss": 0.0045,
      "step": 3870
    },
    {
      "epoch": 0.3448888888888889,
      "grad_norm": 0.17872223258018494,
      "learning_rate": 4.784444444444445e-05,
      "loss": 0.0051,
      "step": 3880
    },
    {
      "epoch": 0.3457777777777778,
      "grad_norm": 0.2211180329322815,
      "learning_rate": 4.783888888888889e-05,
      "loss": 0.0028,
      "step": 3890
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 1.2989434003829956,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0028,
      "step": 3900
    },
    {
      "epoch": 0.34755555555555556,
      "grad_norm": 0.16901066899299622,
      "learning_rate": 4.7827777777777785e-05,
      "loss": 0.0031,
      "step": 3910
    },
    {
      "epoch": 0.34844444444444445,
      "grad_norm": 0.49280408024787903,
      "learning_rate": 4.782222222222222e-05,
      "loss": 0.0032,
      "step": 3920
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 1.9755733013153076,
      "learning_rate": 4.781666666666667e-05,
      "loss": 0.0047,
      "step": 3930
    },
    {
      "epoch": 0.3502222222222222,
      "grad_norm": 0.8540298342704773,
      "learning_rate": 4.781111111111111e-05,
      "loss": 0.0028,
      "step": 3940
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 0.2697289288043976,
      "learning_rate": 4.780555555555556e-05,
      "loss": 0.0039,
      "step": 3950
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.3401455581188202,
      "learning_rate": 4.78e-05,
      "loss": 0.0032,
      "step": 3960
    },
    {
      "epoch": 0.35288888888888886,
      "grad_norm": 0.28690579533576965,
      "learning_rate": 4.779444444444445e-05,
      "loss": 0.0034,
      "step": 3970
    },
    {
      "epoch": 0.3537777777777778,
      "grad_norm": 0.5273467302322388,
      "learning_rate": 4.778888888888889e-05,
      "loss": 0.0034,
      "step": 3980
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 1.050641417503357,
      "learning_rate": 4.7783333333333334e-05,
      "loss": 0.0038,
      "step": 3990
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.41771453619003296,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 0.003,
      "step": 4000
    },
    {
      "epoch": 0.35644444444444445,
      "grad_norm": 0.4217582643032074,
      "learning_rate": 4.777222222222222e-05,
      "loss": 0.0045,
      "step": 4010
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.2725754380226135,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0038,
      "step": 4020
    },
    {
      "epoch": 0.3582222222222222,
      "grad_norm": 0.7420286536216736,
      "learning_rate": 4.7761111111111115e-05,
      "loss": 0.0045,
      "step": 4030
    },
    {
      "epoch": 0.3591111111111111,
      "grad_norm": 0.1292647272348404,
      "learning_rate": 4.775555555555556e-05,
      "loss": 0.0036,
      "step": 4040
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.7815485000610352,
      "learning_rate": 4.775e-05,
      "loss": 0.0042,
      "step": 4050
    },
    {
      "epoch": 0.36088888888888887,
      "grad_norm": 0.22846105694770813,
      "learning_rate": 4.7744444444444445e-05,
      "loss": 0.0028,
      "step": 4060
    },
    {
      "epoch": 0.36177777777777775,
      "grad_norm": 0.1929958313703537,
      "learning_rate": 4.773888888888889e-05,
      "loss": 0.0031,
      "step": 4070
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.7302044630050659,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0029,
      "step": 4080
    },
    {
      "epoch": 0.3635555555555556,
      "grad_norm": 0.6275882720947266,
      "learning_rate": 4.772777777777778e-05,
      "loss": 0.0037,
      "step": 4090
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 0.43070757389068604,
      "learning_rate": 4.7722222222222226e-05,
      "loss": 0.0039,
      "step": 4100
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 1.5203735828399658,
      "learning_rate": 4.771666666666667e-05,
      "loss": 0.0047,
      "step": 4110
    },
    {
      "epoch": 0.3662222222222222,
      "grad_norm": 2.0295612812042236,
      "learning_rate": 4.7711111111111114e-05,
      "loss": 0.0024,
      "step": 4120
    },
    {
      "epoch": 0.3671111111111111,
      "grad_norm": 1.338440179824829,
      "learning_rate": 4.770555555555556e-05,
      "loss": 0.0041,
      "step": 4130
    },
    {
      "epoch": 0.368,
      "grad_norm": 1.9167611598968506,
      "learning_rate": 4.77e-05,
      "loss": 0.0042,
      "step": 4140
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 1.8418679237365723,
      "learning_rate": 4.7694444444444444e-05,
      "loss": 0.0042,
      "step": 4150
    },
    {
      "epoch": 0.36977777777777776,
      "grad_norm": 0.43253058195114136,
      "learning_rate": 4.768888888888889e-05,
      "loss": 0.0034,
      "step": 4160
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.7417966723442078,
      "learning_rate": 4.768333333333334e-05,
      "loss": 0.0027,
      "step": 4170
    },
    {
      "epoch": 0.37155555555555553,
      "grad_norm": 1.7117414474487305,
      "learning_rate": 4.767777777777778e-05,
      "loss": 0.0041,
      "step": 4180
    },
    {
      "epoch": 0.37244444444444447,
      "grad_norm": 0.840330958366394,
      "learning_rate": 4.7672222222222225e-05,
      "loss": 0.0043,
      "step": 4190
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.1552523821592331,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0022,
      "step": 4200
    },
    {
      "epoch": 0.37422222222222223,
      "grad_norm": 1.291300892829895,
      "learning_rate": 4.766111111111111e-05,
      "loss": 0.0032,
      "step": 4210
    },
    {
      "epoch": 0.3751111111111111,
      "grad_norm": 0.3389570415019989,
      "learning_rate": 4.7655555555555556e-05,
      "loss": 0.0032,
      "step": 4220
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.2264178842306137,
      "learning_rate": 4.765e-05,
      "loss": 0.0043,
      "step": 4230
    },
    {
      "epoch": 0.3768888888888889,
      "grad_norm": 0.2126169502735138,
      "learning_rate": 4.764444444444445e-05,
      "loss": 0.0027,
      "step": 4240
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 0.28767770528793335,
      "learning_rate": 4.7638888888888887e-05,
      "loss": 0.0038,
      "step": 4250
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.7648115754127502,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.0048,
      "step": 4260
    },
    {
      "epoch": 0.37955555555555553,
      "grad_norm": 1.1591988801956177,
      "learning_rate": 4.762777777777778e-05,
      "loss": 0.0036,
      "step": 4270
    },
    {
      "epoch": 0.3804444444444444,
      "grad_norm": 1.1582287549972534,
      "learning_rate": 4.7622222222222224e-05,
      "loss": 0.004,
      "step": 4280
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.14707764983177185,
      "learning_rate": 4.761666666666667e-05,
      "loss": 0.0046,
      "step": 4290
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 2.0956366062164307,
      "learning_rate": 4.761111111111111e-05,
      "loss": 0.0053,
      "step": 4300
    },
    {
      "epoch": 0.3831111111111111,
      "grad_norm": 0.5373217463493347,
      "learning_rate": 4.760555555555556e-05,
      "loss": 0.0042,
      "step": 4310
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.794411301612854,
      "learning_rate": 4.76e-05,
      "loss": 0.0037,
      "step": 4320
    },
    {
      "epoch": 0.3848888888888889,
      "grad_norm": 0.3369787931442261,
      "learning_rate": 4.759444444444445e-05,
      "loss": 0.0031,
      "step": 4330
    },
    {
      "epoch": 0.3857777777777778,
      "grad_norm": 0.21724742650985718,
      "learning_rate": 4.7588888888888885e-05,
      "loss": 0.0048,
      "step": 4340
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.8245742321014404,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.0036,
      "step": 4350
    },
    {
      "epoch": 0.38755555555555554,
      "grad_norm": 0.3826577663421631,
      "learning_rate": 4.757777777777778e-05,
      "loss": 0.0037,
      "step": 4360
    },
    {
      "epoch": 0.3884444444444444,
      "grad_norm": 0.5509856939315796,
      "learning_rate": 4.757222222222222e-05,
      "loss": 0.0027,
      "step": 4370
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 1.4770309925079346,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.005,
      "step": 4380
    },
    {
      "epoch": 0.39022222222222225,
      "grad_norm": 1.201189398765564,
      "learning_rate": 4.756111111111111e-05,
      "loss": 0.0032,
      "step": 4390
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 0.3530949652194977,
      "learning_rate": 4.755555555555556e-05,
      "loss": 0.0048,
      "step": 4400
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.8284189105033875,
      "learning_rate": 4.755e-05,
      "loss": 0.0031,
      "step": 4410
    },
    {
      "epoch": 0.3928888888888889,
      "grad_norm": 0.8939140439033508,
      "learning_rate": 4.754444444444445e-05,
      "loss": 0.0045,
      "step": 4420
    },
    {
      "epoch": 0.3937777777777778,
      "grad_norm": 0.6404901146888733,
      "learning_rate": 4.753888888888889e-05,
      "loss": 0.0043,
      "step": 4430
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 1.6369456052780151,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0043,
      "step": 4440
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 0.8450610041618347,
      "learning_rate": 4.7527777777777785e-05,
      "loss": 0.003,
      "step": 4450
    },
    {
      "epoch": 0.39644444444444443,
      "grad_norm": 0.6198840737342834,
      "learning_rate": 4.752222222222222e-05,
      "loss": 0.0032,
      "step": 4460
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 1.654488205909729,
      "learning_rate": 4.751666666666667e-05,
      "loss": 0.0052,
      "step": 4470
    },
    {
      "epoch": 0.3982222222222222,
      "grad_norm": 0.5299087166786194,
      "learning_rate": 4.751111111111111e-05,
      "loss": 0.0039,
      "step": 4480
    },
    {
      "epoch": 0.39911111111111114,
      "grad_norm": 0.2333724945783615,
      "learning_rate": 4.750555555555556e-05,
      "loss": 0.0043,
      "step": 4490
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.0508137941360474,
      "learning_rate": 4.75e-05,
      "loss": 0.0027,
      "step": 4500
    },
    {
      "epoch": 0.4008888888888889,
      "grad_norm": 0.27046653628349304,
      "learning_rate": 4.7494444444444446e-05,
      "loss": 0.004,
      "step": 4510
    },
    {
      "epoch": 0.4017777777777778,
      "grad_norm": 1.1058027744293213,
      "learning_rate": 4.7488888888888897e-05,
      "loss": 0.0029,
      "step": 4520
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 1.3305820226669312,
      "learning_rate": 4.748333333333333e-05,
      "loss": 0.0036,
      "step": 4530
    },
    {
      "epoch": 0.40355555555555556,
      "grad_norm": 0.6264621019363403,
      "learning_rate": 4.7477777777777784e-05,
      "loss": 0.0034,
      "step": 4540
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 0.9665529131889343,
      "learning_rate": 4.747222222222222e-05,
      "loss": 0.0029,
      "step": 4550
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 1.4769240617752075,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0042,
      "step": 4560
    },
    {
      "epoch": 0.4062222222222222,
      "grad_norm": 0.5810900926589966,
      "learning_rate": 4.7461111111111114e-05,
      "loss": 0.0026,
      "step": 4570
    },
    {
      "epoch": 0.4071111111111111,
      "grad_norm": 0.7470480799674988,
      "learning_rate": 4.745555555555556e-05,
      "loss": 0.0023,
      "step": 4580
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.601682722568512,
      "learning_rate": 4.745e-05,
      "loss": 0.0035,
      "step": 4590
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.4727081060409546,
      "learning_rate": 4.7444444444444445e-05,
      "loss": 0.0035,
      "step": 4600
    },
    {
      "epoch": 0.4097777777777778,
      "grad_norm": 0.5662996172904968,
      "learning_rate": 4.7438888888888895e-05,
      "loss": 0.0033,
      "step": 4610
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 1.161553144454956,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0038,
      "step": 4620
    },
    {
      "epoch": 0.41155555555555556,
      "grad_norm": 0.3724733889102936,
      "learning_rate": 4.742777777777778e-05,
      "loss": 0.0038,
      "step": 4630
    },
    {
      "epoch": 0.41244444444444445,
      "grad_norm": 0.8802784085273743,
      "learning_rate": 4.7422222222222226e-05,
      "loss": 0.0026,
      "step": 4640
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.3419869542121887,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.0027,
      "step": 4650
    },
    {
      "epoch": 0.4142222222222222,
      "grad_norm": 1.4761799573898315,
      "learning_rate": 4.741111111111111e-05,
      "loss": 0.0037,
      "step": 4660
    },
    {
      "epoch": 0.4151111111111111,
      "grad_norm": 0.10939639806747437,
      "learning_rate": 4.740555555555556e-05,
      "loss": 0.0045,
      "step": 4670
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.4496338367462158,
      "learning_rate": 4.74e-05,
      "loss": 0.0025,
      "step": 4680
    },
    {
      "epoch": 0.41688888888888886,
      "grad_norm": 0.2690642178058624,
      "learning_rate": 4.7394444444444444e-05,
      "loss": 0.0038,
      "step": 4690
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 0.8960142135620117,
      "learning_rate": 4.7388888888888894e-05,
      "loss": 0.0028,
      "step": 4700
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.6007154583930969,
      "learning_rate": 4.738333333333334e-05,
      "loss": 0.0029,
      "step": 4710
    },
    {
      "epoch": 0.41955555555555557,
      "grad_norm": 1.1057246923446655,
      "learning_rate": 4.737777777777778e-05,
      "loss": 0.0041,
      "step": 4720
    },
    {
      "epoch": 0.42044444444444445,
      "grad_norm": 1.100647211074829,
      "learning_rate": 4.7372222222222225e-05,
      "loss": 0.003,
      "step": 4730
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 1.5508174896240234,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.0033,
      "step": 4740
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 1.2732837200164795,
      "learning_rate": 4.736111111111111e-05,
      "loss": 0.0035,
      "step": 4750
    },
    {
      "epoch": 0.4231111111111111,
      "grad_norm": 0.6266580820083618,
      "learning_rate": 4.7355555555555555e-05,
      "loss": 0.0041,
      "step": 4760
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.17468196153640747,
      "learning_rate": 4.735e-05,
      "loss": 0.0047,
      "step": 4770
    },
    {
      "epoch": 0.42488888888888887,
      "grad_norm": 0.6368531584739685,
      "learning_rate": 4.734444444444445e-05,
      "loss": 0.0036,
      "step": 4780
    },
    {
      "epoch": 0.42577777777777776,
      "grad_norm": 0.7265137434005737,
      "learning_rate": 4.733888888888889e-05,
      "loss": 0.0029,
      "step": 4790
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 1.1215441226959229,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0028,
      "step": 4800
    },
    {
      "epoch": 0.4275555555555556,
      "grad_norm": 0.5847569704055786,
      "learning_rate": 4.732777777777778e-05,
      "loss": 0.0043,
      "step": 4810
    },
    {
      "epoch": 0.42844444444444446,
      "grad_norm": 1.451737642288208,
      "learning_rate": 4.7322222222222224e-05,
      "loss": 0.0045,
      "step": 4820
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.1744147539138794,
      "learning_rate": 4.731666666666667e-05,
      "loss": 0.0034,
      "step": 4830
    },
    {
      "epoch": 0.43022222222222223,
      "grad_norm": 0.68229740858078,
      "learning_rate": 4.731111111111111e-05,
      "loss": 0.002,
      "step": 4840
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 0.1804955154657364,
      "learning_rate": 4.730555555555556e-05,
      "loss": 0.0037,
      "step": 4850
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.4106050431728363,
      "learning_rate": 4.73e-05,
      "loss": 0.0035,
      "step": 4860
    },
    {
      "epoch": 0.4328888888888889,
      "grad_norm": 1.275097131729126,
      "learning_rate": 4.729444444444445e-05,
      "loss": 0.0036,
      "step": 4870
    },
    {
      "epoch": 0.43377777777777776,
      "grad_norm": 0.8766255974769592,
      "learning_rate": 4.728888888888889e-05,
      "loss": 0.0034,
      "step": 4880
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.7037172913551331,
      "learning_rate": 4.7283333333333335e-05,
      "loss": 0.0038,
      "step": 4890
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 1.7390928268432617,
      "learning_rate": 4.727777777777778e-05,
      "loss": 0.0036,
      "step": 4900
    },
    {
      "epoch": 0.43644444444444447,
      "grad_norm": 1.05000638961792,
      "learning_rate": 4.727222222222222e-05,
      "loss": 0.005,
      "step": 4910
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 1.2308835983276367,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0044,
      "step": 4920
    },
    {
      "epoch": 0.43822222222222224,
      "grad_norm": 1.3454517126083374,
      "learning_rate": 4.726111111111111e-05,
      "loss": 0.005,
      "step": 4930
    },
    {
      "epoch": 0.4391111111111111,
      "grad_norm": 1.5576848983764648,
      "learning_rate": 4.725555555555556e-05,
      "loss": 0.0032,
      "step": 4940
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.0949831008911133,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.0038,
      "step": 4950
    },
    {
      "epoch": 0.4408888888888889,
      "grad_norm": 1.5193525552749634,
      "learning_rate": 4.724444444444445e-05,
      "loss": 0.0033,
      "step": 4960
    },
    {
      "epoch": 0.44177777777777777,
      "grad_norm": 1.5562106370925903,
      "learning_rate": 4.723888888888889e-05,
      "loss": 0.004,
      "step": 4970
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 1.59603750705719,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0031,
      "step": 4980
    },
    {
      "epoch": 0.44355555555555554,
      "grad_norm": 1.2011722326278687,
      "learning_rate": 4.7227777777777784e-05,
      "loss": 0.0041,
      "step": 4990
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.21283428370952606,
      "learning_rate": 4.722222222222222e-05,
      "loss": 0.0049,
      "step": 5000
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 1.4375708103179932,
      "learning_rate": 4.721666666666667e-05,
      "loss": 0.0027,
      "step": 5010
    },
    {
      "epoch": 0.44622222222222224,
      "grad_norm": 0.6088502407073975,
      "learning_rate": 4.721111111111111e-05,
      "loss": 0.0032,
      "step": 5020
    },
    {
      "epoch": 0.4471111111111111,
      "grad_norm": 0.46744444966316223,
      "learning_rate": 4.720555555555556e-05,
      "loss": 0.0041,
      "step": 5030
    },
    {
      "epoch": 0.448,
      "grad_norm": 1.0444321632385254,
      "learning_rate": 4.72e-05,
      "loss": 0.0025,
      "step": 5040
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 0.9758543968200684,
      "learning_rate": 4.7194444444444446e-05,
      "loss": 0.0037,
      "step": 5050
    },
    {
      "epoch": 0.4497777777777778,
      "grad_norm": 1.2014700174331665,
      "learning_rate": 4.7188888888888896e-05,
      "loss": 0.0037,
      "step": 5060
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.7753328084945679,
      "learning_rate": 4.718333333333333e-05,
      "loss": 0.003,
      "step": 5070
    },
    {
      "epoch": 0.45155555555555554,
      "grad_norm": 0.8560649752616882,
      "learning_rate": 4.717777777777778e-05,
      "loss": 0.0038,
      "step": 5080
    },
    {
      "epoch": 0.4524444444444444,
      "grad_norm": 0.8497953414916992,
      "learning_rate": 4.717222222222222e-05,
      "loss": 0.0048,
      "step": 5090
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 1.037326693534851,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0028,
      "step": 5100
    },
    {
      "epoch": 0.45422222222222225,
      "grad_norm": 0.25557631254196167,
      "learning_rate": 4.7161111111111114e-05,
      "loss": 0.0036,
      "step": 5110
    },
    {
      "epoch": 0.45511111111111113,
      "grad_norm": 0.5286280512809753,
      "learning_rate": 4.715555555555556e-05,
      "loss": 0.0046,
      "step": 5120
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.3863314390182495,
      "learning_rate": 4.715e-05,
      "loss": 0.0026,
      "step": 5130
    },
    {
      "epoch": 0.4568888888888889,
      "grad_norm": 0.8766920566558838,
      "learning_rate": 4.7144444444444444e-05,
      "loss": 0.0039,
      "step": 5140
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 0.9421693682670593,
      "learning_rate": 4.7138888888888895e-05,
      "loss": 0.0029,
      "step": 5150
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 1.545289397239685,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0035,
      "step": 5160
    },
    {
      "epoch": 0.45955555555555555,
      "grad_norm": 0.16441600024700165,
      "learning_rate": 4.712777777777778e-05,
      "loss": 0.0034,
      "step": 5170
    },
    {
      "epoch": 0.46044444444444443,
      "grad_norm": 0.3482241630554199,
      "learning_rate": 4.7122222222222225e-05,
      "loss": 0.0044,
      "step": 5180
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.7021912336349487,
      "learning_rate": 4.711666666666667e-05,
      "loss": 0.004,
      "step": 5190
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 1.2683650255203247,
      "learning_rate": 4.711111111111111e-05,
      "loss": 0.0027,
      "step": 5200
    },
    {
      "epoch": 0.4631111111111111,
      "grad_norm": 0.7895876169204712,
      "learning_rate": 4.7105555555555556e-05,
      "loss": 0.0042,
      "step": 5210
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.21809187531471252,
      "learning_rate": 4.71e-05,
      "loss": 0.0034,
      "step": 5220
    },
    {
      "epoch": 0.4648888888888889,
      "grad_norm": 1.1589986085891724,
      "learning_rate": 4.709444444444444e-05,
      "loss": 0.0032,
      "step": 5230
    },
    {
      "epoch": 0.4657777777777778,
      "grad_norm": 1.4015527963638306,
      "learning_rate": 4.7088888888888894e-05,
      "loss": 0.0043,
      "step": 5240
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.85970139503479,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.004,
      "step": 5250
    },
    {
      "epoch": 0.46755555555555556,
      "grad_norm": 0.15823182463645935,
      "learning_rate": 4.707777777777778e-05,
      "loss": 0.0028,
      "step": 5260
    },
    {
      "epoch": 0.46844444444444444,
      "grad_norm": 0.565125048160553,
      "learning_rate": 4.7072222222222224e-05,
      "loss": 0.0032,
      "step": 5270
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 1.305336356163025,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0036,
      "step": 5280
    },
    {
      "epoch": 0.4702222222222222,
      "grad_norm": 0.3555966019630432,
      "learning_rate": 4.706111111111111e-05,
      "loss": 0.0037,
      "step": 5290
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 0.36454036831855774,
      "learning_rate": 4.7055555555555555e-05,
      "loss": 0.004,
      "step": 5300
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.18379847705364227,
      "learning_rate": 4.705e-05,
      "loss": 0.0025,
      "step": 5310
    },
    {
      "epoch": 0.4728888888888889,
      "grad_norm": 1.5987149477005005,
      "learning_rate": 4.704444444444445e-05,
      "loss": 0.0048,
      "step": 5320
    },
    {
      "epoch": 0.4737777777777778,
      "grad_norm": 1.6298407316207886,
      "learning_rate": 4.703888888888889e-05,
      "loss": 0.0029,
      "step": 5330
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.6762227416038513,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.0039,
      "step": 5340
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 1.7011946439743042,
      "learning_rate": 4.702777777777778e-05,
      "loss": 0.0053,
      "step": 5350
    },
    {
      "epoch": 0.47644444444444445,
      "grad_norm": 0.10099630802869797,
      "learning_rate": 4.702222222222222e-05,
      "loss": 0.0045,
      "step": 5360
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 1.0413122177124023,
      "learning_rate": 4.701666666666667e-05,
      "loss": 0.002,
      "step": 5370
    },
    {
      "epoch": 0.4782222222222222,
      "grad_norm": 1.1279186010360718,
      "learning_rate": 4.701111111111111e-05,
      "loss": 0.0031,
      "step": 5380
    },
    {
      "epoch": 0.4791111111111111,
      "grad_norm": 1.3273398876190186,
      "learning_rate": 4.700555555555556e-05,
      "loss": 0.003,
      "step": 5390
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.8643585443496704,
      "learning_rate": 4.7e-05,
      "loss": 0.0051,
      "step": 5400
    },
    {
      "epoch": 0.48088888888888887,
      "grad_norm": 0.577765941619873,
      "learning_rate": 4.699444444444445e-05,
      "loss": 0.0035,
      "step": 5410
    },
    {
      "epoch": 0.4817777777777778,
      "grad_norm": 0.9192690849304199,
      "learning_rate": 4.698888888888889e-05,
      "loss": 0.0036,
      "step": 5420
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.9453830122947693,
      "learning_rate": 4.6983333333333335e-05,
      "loss": 0.004,
      "step": 5430
    },
    {
      "epoch": 0.48355555555555557,
      "grad_norm": 0.11688704788684845,
      "learning_rate": 4.6977777777777785e-05,
      "loss": 0.0026,
      "step": 5440
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 1.3133318424224854,
      "learning_rate": 4.697222222222222e-05,
      "loss": 0.0033,
      "step": 5450
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.7764328122138977,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0028,
      "step": 5460
    },
    {
      "epoch": 0.4862222222222222,
      "grad_norm": 1.0357468128204346,
      "learning_rate": 4.696111111111111e-05,
      "loss": 0.0031,
      "step": 5470
    },
    {
      "epoch": 0.4871111111111111,
      "grad_norm": 0.2792527973651886,
      "learning_rate": 4.695555555555556e-05,
      "loss": 0.0026,
      "step": 5480
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.7426744699478149,
      "learning_rate": 4.695e-05,
      "loss": 0.0034,
      "step": 5490
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.4554588198661804,
      "learning_rate": 4.6944444444444446e-05,
      "loss": 0.0021,
      "step": 5500
    },
    {
      "epoch": 0.48977777777777776,
      "grad_norm": 1.6263643503189087,
      "learning_rate": 4.69388888888889e-05,
      "loss": 0.0046,
      "step": 5510
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 1.2450412511825562,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0033,
      "step": 5520
    },
    {
      "epoch": 0.4915555555555556,
      "grad_norm": 0.707605242729187,
      "learning_rate": 4.6927777777777784e-05,
      "loss": 0.0042,
      "step": 5530
    },
    {
      "epoch": 0.49244444444444446,
      "grad_norm": 0.29909399151802063,
      "learning_rate": 4.692222222222222e-05,
      "loss": 0.003,
      "step": 5540
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.4850643277168274,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.004,
      "step": 5550
    },
    {
      "epoch": 0.49422222222222223,
      "grad_norm": 1.2320263385772705,
      "learning_rate": 4.6911111111111114e-05,
      "loss": 0.0034,
      "step": 5560
    },
    {
      "epoch": 0.4951111111111111,
      "grad_norm": 0.28403210639953613,
      "learning_rate": 4.690555555555556e-05,
      "loss": 0.0046,
      "step": 5570
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.16204509139060974,
      "learning_rate": 4.69e-05,
      "loss": 0.0033,
      "step": 5580
    },
    {
      "epoch": 0.4968888888888889,
      "grad_norm": 0.536887526512146,
      "learning_rate": 4.6894444444444445e-05,
      "loss": 0.0042,
      "step": 5590
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 1.6853536367416382,
      "learning_rate": 4.6888888888888895e-05,
      "loss": 0.0034,
      "step": 5600
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.5025461912155151,
      "learning_rate": 4.688333333333333e-05,
      "loss": 0.0031,
      "step": 5610
    },
    {
      "epoch": 0.49955555555555553,
      "grad_norm": 0.4515863358974457,
      "learning_rate": 4.687777777777778e-05,
      "loss": 0.0036,
      "step": 5620
    },
    {
      "epoch": 0.5004444444444445,
      "grad_norm": 1.1273066997528076,
      "learning_rate": 4.6872222222222226e-05,
      "loss": 0.003,
      "step": 5630
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.16574740409851074,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0034,
      "step": 5640
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 0.8422215580940247,
      "learning_rate": 4.686111111111111e-05,
      "loss": 0.004,
      "step": 5650
    },
    {
      "epoch": 0.5031111111111111,
      "grad_norm": 0.8632802367210388,
      "learning_rate": 4.685555555555556e-05,
      "loss": 0.0029,
      "step": 5660
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.1454162895679474,
      "learning_rate": 4.685000000000001e-05,
      "loss": 0.0029,
      "step": 5670
    },
    {
      "epoch": 0.5048888888888889,
      "grad_norm": 1.3009791374206543,
      "learning_rate": 4.6844444444444444e-05,
      "loss": 0.0038,
      "step": 5680
    },
    {
      "epoch": 0.5057777777777778,
      "grad_norm": 1.0816848278045654,
      "learning_rate": 4.6838888888888894e-05,
      "loss": 0.0034,
      "step": 5690
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.14013731479644775,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0025,
      "step": 5700
    },
    {
      "epoch": 0.5075555555555555,
      "grad_norm": 0.41283780336380005,
      "learning_rate": 4.682777777777778e-05,
      "loss": 0.0028,
      "step": 5710
    },
    {
      "epoch": 0.5084444444444445,
      "grad_norm": 0.21322323381900787,
      "learning_rate": 4.6822222222222225e-05,
      "loss": 0.0028,
      "step": 5720
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.35812464356422424,
      "learning_rate": 4.681666666666667e-05,
      "loss": 0.0033,
      "step": 5730
    },
    {
      "epoch": 0.5102222222222222,
      "grad_norm": 0.6998212933540344,
      "learning_rate": 4.681111111111111e-05,
      "loss": 0.0028,
      "step": 5740
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 1.0169936418533325,
      "learning_rate": 4.6805555555555556e-05,
      "loss": 0.0035,
      "step": 5750
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.7322742342948914,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.003,
      "step": 5760
    },
    {
      "epoch": 0.5128888888888888,
      "grad_norm": 1.0182502269744873,
      "learning_rate": 4.679444444444445e-05,
      "loss": 0.0037,
      "step": 5770
    },
    {
      "epoch": 0.5137777777777778,
      "grad_norm": 0.2727157771587372,
      "learning_rate": 4.678888888888889e-05,
      "loss": 0.0022,
      "step": 5780
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 1.2904281616210938,
      "learning_rate": 4.6783333333333337e-05,
      "loss": 0.0036,
      "step": 5790
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 1.4544157981872559,
      "learning_rate": 4.677777777777778e-05,
      "loss": 0.0039,
      "step": 5800
    },
    {
      "epoch": 0.5164444444444445,
      "grad_norm": 1.2564152479171753,
      "learning_rate": 4.6772222222222224e-05,
      "loss": 0.0026,
      "step": 5810
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.29439568519592285,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.0025,
      "step": 5820
    },
    {
      "epoch": 0.5182222222222223,
      "grad_norm": 0.2923414409160614,
      "learning_rate": 4.676111111111111e-05,
      "loss": 0.0024,
      "step": 5830
    },
    {
      "epoch": 0.5191111111111111,
      "grad_norm": 0.1209537535905838,
      "learning_rate": 4.675555555555556e-05,
      "loss": 0.0034,
      "step": 5840
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.8836991190910339,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.0047,
      "step": 5850
    },
    {
      "epoch": 0.5208888888888888,
      "grad_norm": 0.7417191863059998,
      "learning_rate": 4.674444444444445e-05,
      "loss": 0.0048,
      "step": 5860
    },
    {
      "epoch": 0.5217777777777778,
      "grad_norm": 0.41361504793167114,
      "learning_rate": 4.673888888888889e-05,
      "loss": 0.0043,
      "step": 5870
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.42450281977653503,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0031,
      "step": 5880
    },
    {
      "epoch": 0.5235555555555556,
      "grad_norm": 0.9892945289611816,
      "learning_rate": 4.672777777777778e-05,
      "loss": 0.004,
      "step": 5890
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 1.1441729068756104,
      "learning_rate": 4.672222222222222e-05,
      "loss": 0.0042,
      "step": 5900
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 1.0994149446487427,
      "learning_rate": 4.671666666666667e-05,
      "loss": 0.0052,
      "step": 5910
    },
    {
      "epoch": 0.5262222222222223,
      "grad_norm": 0.9335158467292786,
      "learning_rate": 4.671111111111111e-05,
      "loss": 0.0031,
      "step": 5920
    },
    {
      "epoch": 0.5271111111111111,
      "grad_norm": 0.3596588373184204,
      "learning_rate": 4.670555555555556e-05,
      "loss": 0.0052,
      "step": 5930
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.7373448610305786,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.0028,
      "step": 5940
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 1.234833836555481,
      "learning_rate": 4.669444444444445e-05,
      "loss": 0.0028,
      "step": 5950
    },
    {
      "epoch": 0.5297777777777778,
      "grad_norm": 0.8780524730682373,
      "learning_rate": 4.668888888888889e-05,
      "loss": 0.0032,
      "step": 5960
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.11441604048013687,
      "learning_rate": 4.6683333333333334e-05,
      "loss": 0.0031,
      "step": 5970
    },
    {
      "epoch": 0.5315555555555556,
      "grad_norm": 0.268502414226532,
      "learning_rate": 4.6677777777777785e-05,
      "loss": 0.0037,
      "step": 5980
    },
    {
      "epoch": 0.5324444444444445,
      "grad_norm": 0.7763683199882507,
      "learning_rate": 4.667222222222222e-05,
      "loss": 0.003,
      "step": 5990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.7080007195472717,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.003,
      "step": 6000
    },
    {
      "epoch": 0.5342222222222223,
      "grad_norm": 1.1768306493759155,
      "learning_rate": 4.666111111111111e-05,
      "loss": 0.003,
      "step": 6010
    },
    {
      "epoch": 0.5351111111111111,
      "grad_norm": 0.871105432510376,
      "learning_rate": 4.665555555555556e-05,
      "loss": 0.0032,
      "step": 6020
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.148931622505188,
      "learning_rate": 4.665e-05,
      "loss": 0.0042,
      "step": 6030
    },
    {
      "epoch": 0.5368888888888889,
      "grad_norm": 0.3660305440425873,
      "learning_rate": 4.6644444444444446e-05,
      "loss": 0.0033,
      "step": 6040
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 0.467460960149765,
      "learning_rate": 4.6638888888888896e-05,
      "loss": 0.0035,
      "step": 6050
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.7335563898086548,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.0043,
      "step": 6060
    },
    {
      "epoch": 0.5395555555555556,
      "grad_norm": 0.3986937403678894,
      "learning_rate": 4.662777777777778e-05,
      "loss": 0.0023,
      "step": 6070
    },
    {
      "epoch": 0.5404444444444444,
      "grad_norm": 0.11504340171813965,
      "learning_rate": 4.662222222222222e-05,
      "loss": 0.0032,
      "step": 6080
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 1.2469221353530884,
      "learning_rate": 4.661666666666667e-05,
      "loss": 0.0041,
      "step": 6090
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 1.2151987552642822,
      "learning_rate": 4.6611111111111114e-05,
      "loss": 0.0037,
      "step": 6100
    },
    {
      "epoch": 0.5431111111111111,
      "grad_norm": 0.5850170254707336,
      "learning_rate": 4.660555555555556e-05,
      "loss": 0.0044,
      "step": 6110
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.7137278318405151,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0035,
      "step": 6120
    },
    {
      "epoch": 0.5448888888888889,
      "grad_norm": 0.20790955424308777,
      "learning_rate": 4.6594444444444445e-05,
      "loss": 0.0046,
      "step": 6130
    },
    {
      "epoch": 0.5457777777777778,
      "grad_norm": 1.1913032531738281,
      "learning_rate": 4.6588888888888895e-05,
      "loss": 0.0041,
      "step": 6140
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 1.1594520807266235,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.0033,
      "step": 6150
    },
    {
      "epoch": 0.5475555555555556,
      "grad_norm": 1.1541526317596436,
      "learning_rate": 4.657777777777778e-05,
      "loss": 0.0038,
      "step": 6160
    },
    {
      "epoch": 0.5484444444444444,
      "grad_norm": 0.16202078759670258,
      "learning_rate": 4.6572222222222226e-05,
      "loss": 0.0048,
      "step": 6170
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.4510871171951294,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0036,
      "step": 6180
    },
    {
      "epoch": 0.5502222222222222,
      "grad_norm": 0.6989791989326477,
      "learning_rate": 4.656111111111111e-05,
      "loss": 0.0036,
      "step": 6190
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.9637265801429749,
      "learning_rate": 4.6555555555555556e-05,
      "loss": 0.0023,
      "step": 6200
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.33793264627456665,
      "learning_rate": 4.655000000000001e-05,
      "loss": 0.0041,
      "step": 6210
    },
    {
      "epoch": 0.5528888888888889,
      "grad_norm": 0.7943553328514099,
      "learning_rate": 4.6544444444444443e-05,
      "loss": 0.0031,
      "step": 6220
    },
    {
      "epoch": 0.5537777777777778,
      "grad_norm": 0.876908540725708,
      "learning_rate": 4.6538888888888894e-05,
      "loss": 0.0041,
      "step": 6230
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.9334565997123718,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.004,
      "step": 6240
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.4672645032405853,
      "learning_rate": 4.652777777777778e-05,
      "loss": 0.0029,
      "step": 6250
    },
    {
      "epoch": 0.5564444444444444,
      "grad_norm": 0.7877547144889832,
      "learning_rate": 4.6522222222222224e-05,
      "loss": 0.0023,
      "step": 6260
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.7075985074043274,
      "learning_rate": 4.651666666666667e-05,
      "loss": 0.0035,
      "step": 6270
    },
    {
      "epoch": 0.5582222222222222,
      "grad_norm": 0.6156932711601257,
      "learning_rate": 4.651111111111111e-05,
      "loss": 0.0031,
      "step": 6280
    },
    {
      "epoch": 0.5591111111111111,
      "grad_norm": 1.4705933332443237,
      "learning_rate": 4.6505555555555555e-05,
      "loss": 0.0042,
      "step": 6290
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.3147915005683899,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0038,
      "step": 6300
    },
    {
      "epoch": 0.5608888888888889,
      "grad_norm": 0.46285784244537354,
      "learning_rate": 4.649444444444445e-05,
      "loss": 0.0032,
      "step": 6310
    },
    {
      "epoch": 0.5617777777777778,
      "grad_norm": 0.9373296499252319,
      "learning_rate": 4.648888888888889e-05,
      "loss": 0.0036,
      "step": 6320
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 1.0870378017425537,
      "learning_rate": 4.6483333333333336e-05,
      "loss": 0.003,
      "step": 6330
    },
    {
      "epoch": 0.5635555555555556,
      "grad_norm": 0.35900911688804626,
      "learning_rate": 4.647777777777778e-05,
      "loss": 0.0027,
      "step": 6340
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 0.12749606370925903,
      "learning_rate": 4.647222222222222e-05,
      "loss": 0.0036,
      "step": 6350
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.5218428373336792,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0028,
      "step": 6360
    },
    {
      "epoch": 0.5662222222222222,
      "grad_norm": 0.2744552493095398,
      "learning_rate": 4.646111111111111e-05,
      "loss": 0.0022,
      "step": 6370
    },
    {
      "epoch": 0.5671111111111111,
      "grad_norm": 1.0461922883987427,
      "learning_rate": 4.645555555555556e-05,
      "loss": 0.0035,
      "step": 6380
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.6756962537765503,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.0032,
      "step": 6390
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.871648371219635,
      "learning_rate": 4.644444444444445e-05,
      "loss": 0.004,
      "step": 6400
    },
    {
      "epoch": 0.5697777777777778,
      "grad_norm": 0.6945518255233765,
      "learning_rate": 4.643888888888889e-05,
      "loss": 0.0025,
      "step": 6410
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.6740970611572266,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.0054,
      "step": 6420
    },
    {
      "epoch": 0.5715555555555556,
      "grad_norm": 1.0159077644348145,
      "learning_rate": 4.642777777777778e-05,
      "loss": 0.0037,
      "step": 6430
    },
    {
      "epoch": 0.5724444444444444,
      "grad_norm": 1.4219090938568115,
      "learning_rate": 4.642222222222222e-05,
      "loss": 0.0034,
      "step": 6440
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.5860353112220764,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.0025,
      "step": 6450
    },
    {
      "epoch": 0.5742222222222222,
      "grad_norm": 1.4243465662002563,
      "learning_rate": 4.641111111111111e-05,
      "loss": 0.0033,
      "step": 6460
    },
    {
      "epoch": 0.5751111111111111,
      "grad_norm": 1.2543758153915405,
      "learning_rate": 4.640555555555556e-05,
      "loss": 0.0033,
      "step": 6470
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.7668129801750183,
      "learning_rate": 4.64e-05,
      "loss": 0.0038,
      "step": 6480
    },
    {
      "epoch": 0.5768888888888889,
      "grad_norm": 1.0686793327331543,
      "learning_rate": 4.6394444444444447e-05,
      "loss": 0.0035,
      "step": 6490
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.3276350498199463,
      "learning_rate": 4.638888888888889e-05,
      "loss": 0.0025,
      "step": 6500
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.5051100254058838,
      "learning_rate": 4.6383333333333334e-05,
      "loss": 0.0029,
      "step": 6510
    },
    {
      "epoch": 0.5795555555555556,
      "grad_norm": 1.1063045263290405,
      "learning_rate": 4.6377777777777784e-05,
      "loss": 0.0038,
      "step": 6520
    },
    {
      "epoch": 0.5804444444444444,
      "grad_norm": 1.4828556776046753,
      "learning_rate": 4.637222222222222e-05,
      "loss": 0.0035,
      "step": 6530
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 1.1659687757492065,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0037,
      "step": 6540
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 1.0515694618225098,
      "learning_rate": 4.636111111111111e-05,
      "loss": 0.0038,
      "step": 6550
    },
    {
      "epoch": 0.5831111111111111,
      "grad_norm": 0.23490038514137268,
      "learning_rate": 4.635555555555556e-05,
      "loss": 0.0039,
      "step": 6560
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.8989076018333435,
      "learning_rate": 4.635e-05,
      "loss": 0.0049,
      "step": 6570
    },
    {
      "epoch": 0.5848888888888889,
      "grad_norm": 0.7187893986701965,
      "learning_rate": 4.6344444444444445e-05,
      "loss": 0.0044,
      "step": 6580
    },
    {
      "epoch": 0.5857777777777777,
      "grad_norm": 0.5835983157157898,
      "learning_rate": 4.6338888888888896e-05,
      "loss": 0.0028,
      "step": 6590
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.6406484246253967,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0038,
      "step": 6600
    },
    {
      "epoch": 0.5875555555555556,
      "grad_norm": 0.7973467707633972,
      "learning_rate": 4.632777777777778e-05,
      "loss": 0.003,
      "step": 6610
    },
    {
      "epoch": 0.5884444444444444,
      "grad_norm": 0.34685540199279785,
      "learning_rate": 4.632222222222222e-05,
      "loss": 0.0032,
      "step": 6620
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.22431537508964539,
      "learning_rate": 4.631666666666667e-05,
      "loss": 0.0032,
      "step": 6630
    },
    {
      "epoch": 0.5902222222222222,
      "grad_norm": 1.1894867420196533,
      "learning_rate": 4.6311111111111113e-05,
      "loss": 0.0029,
      "step": 6640
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 1.2306463718414307,
      "learning_rate": 4.630555555555556e-05,
      "loss": 0.0032,
      "step": 6650
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.5012350082397461,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0041,
      "step": 6660
    },
    {
      "epoch": 0.5928888888888889,
      "grad_norm": 0.08046464622020721,
      "learning_rate": 4.6294444444444444e-05,
      "loss": 0.0035,
      "step": 6670
    },
    {
      "epoch": 0.5937777777777777,
      "grad_norm": 0.7667510509490967,
      "learning_rate": 4.6288888888888894e-05,
      "loss": 0.0027,
      "step": 6680
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.17668412625789642,
      "learning_rate": 4.628333333333333e-05,
      "loss": 0.0028,
      "step": 6690
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 0.24911421537399292,
      "learning_rate": 4.627777777777778e-05,
      "loss": 0.0039,
      "step": 6700
    },
    {
      "epoch": 0.5964444444444444,
      "grad_norm": 0.7876623272895813,
      "learning_rate": 4.6272222222222225e-05,
      "loss": 0.0029,
      "step": 6710
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.12496637552976608,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0026,
      "step": 6720
    },
    {
      "epoch": 0.5982222222222222,
      "grad_norm": 0.3479604125022888,
      "learning_rate": 4.626111111111111e-05,
      "loss": 0.0031,
      "step": 6730
    },
    {
      "epoch": 0.5991111111111111,
      "grad_norm": 0.21237462759017944,
      "learning_rate": 4.6255555555555556e-05,
      "loss": 0.004,
      "step": 6740
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.5327517986297607,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.0034,
      "step": 6750
    },
    {
      "epoch": 0.6008888888888889,
      "grad_norm": 0.8083033561706543,
      "learning_rate": 4.624444444444444e-05,
      "loss": 0.0029,
      "step": 6760
    },
    {
      "epoch": 0.6017777777777777,
      "grad_norm": 0.31886568665504456,
      "learning_rate": 4.623888888888889e-05,
      "loss": 0.0037,
      "step": 6770
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 1.4329882860183716,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.0035,
      "step": 6780
    },
    {
      "epoch": 0.6035555555555555,
      "grad_norm": 1.3391144275665283,
      "learning_rate": 4.622777777777778e-05,
      "loss": 0.0029,
      "step": 6790
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 1.2606815099716187,
      "learning_rate": 4.6222222222222224e-05,
      "loss": 0.003,
      "step": 6800
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 1.6295881271362305,
      "learning_rate": 4.621666666666667e-05,
      "loss": 0.0041,
      "step": 6810
    },
    {
      "epoch": 0.6062222222222222,
      "grad_norm": 0.3665122389793396,
      "learning_rate": 4.621111111111111e-05,
      "loss": 0.0047,
      "step": 6820
    },
    {
      "epoch": 0.6071111111111112,
      "grad_norm": 0.4973020851612091,
      "learning_rate": 4.6205555555555555e-05,
      "loss": 0.0035,
      "step": 6830
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.9921030402183533,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0029,
      "step": 6840
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 0.5116782188415527,
      "learning_rate": 4.619444444444445e-05,
      "loss": 0.0032,
      "step": 6850
    },
    {
      "epoch": 0.6097777777777778,
      "grad_norm": 0.5808131098747253,
      "learning_rate": 4.618888888888889e-05,
      "loss": 0.0037,
      "step": 6860
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.4812723696231842,
      "learning_rate": 4.6183333333333336e-05,
      "loss": 0.0032,
      "step": 6870
    },
    {
      "epoch": 0.6115555555555555,
      "grad_norm": 1.3016890287399292,
      "learning_rate": 4.617777777777778e-05,
      "loss": 0.0037,
      "step": 6880
    },
    {
      "epoch": 0.6124444444444445,
      "grad_norm": 0.5251157879829407,
      "learning_rate": 4.617222222222222e-05,
      "loss": 0.0024,
      "step": 6890
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.11489051580429077,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0038,
      "step": 6900
    },
    {
      "epoch": 0.6142222222222222,
      "grad_norm": 0.6187984943389893,
      "learning_rate": 4.6161111111111117e-05,
      "loss": 0.0035,
      "step": 6910
    },
    {
      "epoch": 0.6151111111111112,
      "grad_norm": 0.3882506191730499,
      "learning_rate": 4.615555555555556e-05,
      "loss": 0.0032,
      "step": 6920
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.14903610944747925,
      "learning_rate": 4.6150000000000004e-05,
      "loss": 0.005,
      "step": 6930
    },
    {
      "epoch": 0.6168888888888889,
      "grad_norm": 0.9811453819274902,
      "learning_rate": 4.614444444444445e-05,
      "loss": 0.0048,
      "step": 6940
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 0.4897863566875458,
      "learning_rate": 4.613888888888889e-05,
      "loss": 0.0036,
      "step": 6950
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.4034166634082794,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0036,
      "step": 6960
    },
    {
      "epoch": 0.6195555555555555,
      "grad_norm": 1.329491376876831,
      "learning_rate": 4.612777777777778e-05,
      "loss": 0.0032,
      "step": 6970
    },
    {
      "epoch": 0.6204444444444445,
      "grad_norm": 1.2010490894317627,
      "learning_rate": 4.612222222222222e-05,
      "loss": 0.003,
      "step": 6980
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 1.49024498462677,
      "learning_rate": 4.611666666666667e-05,
      "loss": 0.0032,
      "step": 6990
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.4012469053268433,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 0.0042,
      "step": 7000
    },
    {
      "epoch": 0.6231111111111111,
      "grad_norm": 0.6594359874725342,
      "learning_rate": 4.610555555555556e-05,
      "loss": 0.0024,
      "step": 7010
    },
    {
      "epoch": 0.624,
      "grad_norm": 1.4175876379013062,
      "learning_rate": 4.61e-05,
      "loss": 0.0025,
      "step": 7020
    },
    {
      "epoch": 0.6248888888888889,
      "grad_norm": 0.7453767657279968,
      "learning_rate": 4.6094444444444446e-05,
      "loss": 0.003,
      "step": 7030
    },
    {
      "epoch": 0.6257777777777778,
      "grad_norm": 0.9739654064178467,
      "learning_rate": 4.608888888888889e-05,
      "loss": 0.0036,
      "step": 7040
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.586682915687561,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.0032,
      "step": 7050
    },
    {
      "epoch": 0.6275555555555555,
      "grad_norm": 1.0435373783111572,
      "learning_rate": 4.6077777777777783e-05,
      "loss": 0.0027,
      "step": 7060
    },
    {
      "epoch": 0.6284444444444445,
      "grad_norm": 0.42109042406082153,
      "learning_rate": 4.607222222222222e-05,
      "loss": 0.0029,
      "step": 7070
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.4347051978111267,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.003,
      "step": 7080
    },
    {
      "epoch": 0.6302222222222222,
      "grad_norm": 1.3527761697769165,
      "learning_rate": 4.6061111111111114e-05,
      "loss": 0.0037,
      "step": 7090
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 1.038057565689087,
      "learning_rate": 4.605555555555556e-05,
      "loss": 0.0036,
      "step": 7100
    },
    {
      "epoch": 0.632,
      "grad_norm": 1.05715811252594,
      "learning_rate": 4.605e-05,
      "loss": 0.0029,
      "step": 7110
    },
    {
      "epoch": 0.6328888888888888,
      "grad_norm": 0.8700471520423889,
      "learning_rate": 4.6044444444444445e-05,
      "loss": 0.0034,
      "step": 7120
    },
    {
      "epoch": 0.6337777777777778,
      "grad_norm": 0.2686195373535156,
      "learning_rate": 4.6038888888888895e-05,
      "loss": 0.0042,
      "step": 7130
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.2555706202983856,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.0026,
      "step": 7140
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 1.151675820350647,
      "learning_rate": 4.602777777777778e-05,
      "loss": 0.0035,
      "step": 7150
    },
    {
      "epoch": 0.6364444444444445,
      "grad_norm": 1.4127079248428345,
      "learning_rate": 4.602222222222222e-05,
      "loss": 0.0043,
      "step": 7160
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.9277760982513428,
      "learning_rate": 4.601666666666667e-05,
      "loss": 0.0039,
      "step": 7170
    },
    {
      "epoch": 0.6382222222222222,
      "grad_norm": 0.6981105208396912,
      "learning_rate": 4.601111111111111e-05,
      "loss": 0.0036,
      "step": 7180
    },
    {
      "epoch": 0.6391111111111111,
      "grad_norm": 0.6437838077545166,
      "learning_rate": 4.6005555555555556e-05,
      "loss": 0.0041,
      "step": 7190
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.374874234199524,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0029,
      "step": 7200
    },
    {
      "epoch": 0.6408888888888888,
      "grad_norm": 0.33865028619766235,
      "learning_rate": 4.5994444444444444e-05,
      "loss": 0.0034,
      "step": 7210
    },
    {
      "epoch": 0.6417777777777778,
      "grad_norm": 0.38314372301101685,
      "learning_rate": 4.5988888888888894e-05,
      "loss": 0.0029,
      "step": 7220
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.15547822415828705,
      "learning_rate": 4.598333333333333e-05,
      "loss": 0.0026,
      "step": 7230
    },
    {
      "epoch": 0.6435555555555555,
      "grad_norm": 0.8132573962211609,
      "learning_rate": 4.597777777777778e-05,
      "loss": 0.0035,
      "step": 7240
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 1.0343438386917114,
      "learning_rate": 4.5972222222222225e-05,
      "loss": 0.0026,
      "step": 7250
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.9450242519378662,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.004,
      "step": 7260
    },
    {
      "epoch": 0.6462222222222223,
      "grad_norm": 0.10414052754640579,
      "learning_rate": 4.596111111111112e-05,
      "loss": 0.003,
      "step": 7270
    },
    {
      "epoch": 0.6471111111111111,
      "grad_norm": 0.3965442478656769,
      "learning_rate": 4.5955555555555555e-05,
      "loss": 0.0032,
      "step": 7280
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.38072898983955383,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 0.0041,
      "step": 7290
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 1.0790919065475464,
      "learning_rate": 4.594444444444444e-05,
      "loss": 0.0018,
      "step": 7300
    },
    {
      "epoch": 0.6497777777777778,
      "grad_norm": 1.2412562370300293,
      "learning_rate": 4.593888888888889e-05,
      "loss": 0.0028,
      "step": 7310
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.05437355116009712,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0029,
      "step": 7320
    },
    {
      "epoch": 0.6515555555555556,
      "grad_norm": 0.7006393671035767,
      "learning_rate": 4.592777777777778e-05,
      "loss": 0.0023,
      "step": 7330
    },
    {
      "epoch": 0.6524444444444445,
      "grad_norm": 0.1395697295665741,
      "learning_rate": 4.592222222222222e-05,
      "loss": 0.0042,
      "step": 7340
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.7691066861152649,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.0029,
      "step": 7350
    },
    {
      "epoch": 0.6542222222222223,
      "grad_norm": 1.0329195261001587,
      "learning_rate": 4.591111111111112e-05,
      "loss": 0.0032,
      "step": 7360
    },
    {
      "epoch": 0.6551111111111111,
      "grad_norm": 0.6057680249214172,
      "learning_rate": 4.5905555555555554e-05,
      "loss": 0.005,
      "step": 7370
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.47834932804107666,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0039,
      "step": 7380
    },
    {
      "epoch": 0.6568888888888889,
      "grad_norm": 0.7307875752449036,
      "learning_rate": 4.589444444444445e-05,
      "loss": 0.0035,
      "step": 7390
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 0.2762172520160675,
      "learning_rate": 4.588888888888889e-05,
      "loss": 0.0031,
      "step": 7400
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.24245940148830414,
      "learning_rate": 4.5883333333333335e-05,
      "loss": 0.0025,
      "step": 7410
    },
    {
      "epoch": 0.6595555555555556,
      "grad_norm": 0.8349162936210632,
      "learning_rate": 4.587777777777778e-05,
      "loss": 0.0023,
      "step": 7420
    },
    {
      "epoch": 0.6604444444444444,
      "grad_norm": 1.0056679248809814,
      "learning_rate": 4.587222222222222e-05,
      "loss": 0.0021,
      "step": 7430
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.22122521698474884,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0027,
      "step": 7440
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 0.777401328086853,
      "learning_rate": 4.5861111111111116e-05,
      "loss": 0.0058,
      "step": 7450
    },
    {
      "epoch": 0.6631111111111111,
      "grad_norm": 1.1779625415802002,
      "learning_rate": 4.585555555555556e-05,
      "loss": 0.0028,
      "step": 7460
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.4781860411167145,
      "learning_rate": 4.585e-05,
      "loss": 0.0032,
      "step": 7470
    },
    {
      "epoch": 0.6648888888888889,
      "grad_norm": 1.5584839582443237,
      "learning_rate": 4.584444444444445e-05,
      "loss": 0.0031,
      "step": 7480
    },
    {
      "epoch": 0.6657777777777778,
      "grad_norm": 0.1098218783736229,
      "learning_rate": 4.583888888888889e-05,
      "loss": 0.0021,
      "step": 7490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.7352544069290161,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0027,
      "step": 7500
    },
    {
      "epoch": 0.6675555555555556,
      "grad_norm": 0.1967444121837616,
      "learning_rate": 4.582777777777778e-05,
      "loss": 0.0045,
      "step": 7510
    },
    {
      "epoch": 0.6684444444444444,
      "grad_norm": 0.968978226184845,
      "learning_rate": 4.582222222222222e-05,
      "loss": 0.0036,
      "step": 7520
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.7892801761627197,
      "learning_rate": 4.581666666666667e-05,
      "loss": 0.0036,
      "step": 7530
    },
    {
      "epoch": 0.6702222222222223,
      "grad_norm": 0.17994491755962372,
      "learning_rate": 4.5811111111111115e-05,
      "loss": 0.0035,
      "step": 7540
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 0.17734919488430023,
      "learning_rate": 4.580555555555556e-05,
      "loss": 0.0033,
      "step": 7550
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.14744886755943298,
      "learning_rate": 4.58e-05,
      "loss": 0.0018,
      "step": 7560
    },
    {
      "epoch": 0.6728888888888889,
      "grad_norm": 0.14998771250247955,
      "learning_rate": 4.5794444444444446e-05,
      "loss": 0.0039,
      "step": 7570
    },
    {
      "epoch": 0.6737777777777778,
      "grad_norm": 0.40083998441696167,
      "learning_rate": 4.578888888888889e-05,
      "loss": 0.0023,
      "step": 7580
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.3546367883682251,
      "learning_rate": 4.578333333333333e-05,
      "loss": 0.0038,
      "step": 7590
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.4415990710258484,
      "learning_rate": 4.577777777777778e-05,
      "loss": 0.0026,
      "step": 7600
    },
    {
      "epoch": 0.6764444444444444,
      "grad_norm": 0.5348567962646484,
      "learning_rate": 4.577222222222222e-05,
      "loss": 0.0036,
      "step": 7610
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 1.2765997648239136,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.0036,
      "step": 7620
    },
    {
      "epoch": 0.6782222222222222,
      "grad_norm": 1.660498857498169,
      "learning_rate": 4.5761111111111114e-05,
      "loss": 0.0045,
      "step": 7630
    },
    {
      "epoch": 0.6791111111111111,
      "grad_norm": 0.1431792825460434,
      "learning_rate": 4.575555555555556e-05,
      "loss": 0.0024,
      "step": 7640
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.8448946475982666,
      "learning_rate": 4.575e-05,
      "loss": 0.0033,
      "step": 7650
    },
    {
      "epoch": 0.6808888888888889,
      "grad_norm": 0.6428186893463135,
      "learning_rate": 4.5744444444444444e-05,
      "loss": 0.0029,
      "step": 7660
    },
    {
      "epoch": 0.6817777777777778,
      "grad_norm": 0.510818600654602,
      "learning_rate": 4.5738888888888895e-05,
      "loss": 0.0024,
      "step": 7670
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 1.2633599042892456,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0033,
      "step": 7680
    },
    {
      "epoch": 0.6835555555555556,
      "grad_norm": 0.9716688394546509,
      "learning_rate": 4.572777777777778e-05,
      "loss": 0.0032,
      "step": 7690
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 0.11875621974468231,
      "learning_rate": 4.572222222222222e-05,
      "loss": 0.0037,
      "step": 7700
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.39895451068878174,
      "learning_rate": 4.571666666666667e-05,
      "loss": 0.003,
      "step": 7710
    },
    {
      "epoch": 0.6862222222222222,
      "grad_norm": 0.7896810173988342,
      "learning_rate": 4.571111111111111e-05,
      "loss": 0.0025,
      "step": 7720
    },
    {
      "epoch": 0.6871111111111111,
      "grad_norm": 0.8278156518936157,
      "learning_rate": 4.5705555555555556e-05,
      "loss": 0.0039,
      "step": 7730
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.5930774807929993,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.004,
      "step": 7740
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 1.1068482398986816,
      "learning_rate": 4.569444444444444e-05,
      "loss": 0.003,
      "step": 7750
    },
    {
      "epoch": 0.6897777777777778,
      "grad_norm": 0.637527585029602,
      "learning_rate": 4.5688888888888893e-05,
      "loss": 0.0024,
      "step": 7760
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 1.3023154735565186,
      "learning_rate": 4.568333333333333e-05,
      "loss": 0.0044,
      "step": 7770
    },
    {
      "epoch": 0.6915555555555556,
      "grad_norm": 1.598073124885559,
      "learning_rate": 4.567777777777778e-05,
      "loss": 0.0041,
      "step": 7780
    },
    {
      "epoch": 0.6924444444444444,
      "grad_norm": 1.5252223014831543,
      "learning_rate": 4.5672222222222224e-05,
      "loss": 0.0043,
      "step": 7790
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.229020118713379,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0038,
      "step": 7800
    },
    {
      "epoch": 0.6942222222222222,
      "grad_norm": 1.0427101850509644,
      "learning_rate": 4.566111111111112e-05,
      "loss": 0.0023,
      "step": 7810
    },
    {
      "epoch": 0.6951111111111111,
      "grad_norm": 0.6323387026786804,
      "learning_rate": 4.5655555555555555e-05,
      "loss": 0.0037,
      "step": 7820
    },
    {
      "epoch": 0.696,
      "grad_norm": 1.2200380563735962,
      "learning_rate": 4.5650000000000005e-05,
      "loss": 0.0035,
      "step": 7830
    },
    {
      "epoch": 0.6968888888888889,
      "grad_norm": 0.8677859306335449,
      "learning_rate": 4.564444444444444e-05,
      "loss": 0.0031,
      "step": 7840
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 0.13936886191368103,
      "learning_rate": 4.563888888888889e-05,
      "loss": 0.0023,
      "step": 7850
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.38630199432373047,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.0033,
      "step": 7860
    },
    {
      "epoch": 0.6995555555555556,
      "grad_norm": 1.0948748588562012,
      "learning_rate": 4.562777777777778e-05,
      "loss": 0.0028,
      "step": 7870
    },
    {
      "epoch": 0.7004444444444444,
      "grad_norm": 0.395514577627182,
      "learning_rate": 4.562222222222222e-05,
      "loss": 0.0032,
      "step": 7880
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.23201099038124084,
      "learning_rate": 4.5616666666666666e-05,
      "loss": 0.0029,
      "step": 7890
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 0.3815308213233948,
      "learning_rate": 4.561111111111112e-05,
      "loss": 0.0033,
      "step": 7900
    },
    {
      "epoch": 0.7031111111111111,
      "grad_norm": 0.27300363779067993,
      "learning_rate": 4.560555555555556e-05,
      "loss": 0.0033,
      "step": 7910
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.2508389949798584,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0033,
      "step": 7920
    },
    {
      "epoch": 0.7048888888888889,
      "grad_norm": 0.9139751195907593,
      "learning_rate": 4.559444444444445e-05,
      "loss": 0.0035,
      "step": 7930
    },
    {
      "epoch": 0.7057777777777777,
      "grad_norm": 0.09445610642433167,
      "learning_rate": 4.558888888888889e-05,
      "loss": 0.003,
      "step": 7940
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 1.609531044960022,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.005,
      "step": 7950
    },
    {
      "epoch": 0.7075555555555556,
      "grad_norm": 1.022519826889038,
      "learning_rate": 4.557777777777778e-05,
      "loss": 0.003,
      "step": 7960
    },
    {
      "epoch": 0.7084444444444444,
      "grad_norm": 0.36044925451278687,
      "learning_rate": 4.557222222222222e-05,
      "loss": 0.0028,
      "step": 7970
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.11321312934160233,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0029,
      "step": 7980
    },
    {
      "epoch": 0.7102222222222222,
      "grad_norm": 1.010376214981079,
      "learning_rate": 4.5561111111111116e-05,
      "loss": 0.0023,
      "step": 7990
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.49163946509361267,
      "learning_rate": 4.555555555555556e-05,
      "loss": 0.0024,
      "step": 8000
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.14652560651302338,
      "learning_rate": 4.555e-05,
      "loss": 0.0033,
      "step": 8010
    },
    {
      "epoch": 0.7128888888888889,
      "grad_norm": 0.3321814239025116,
      "learning_rate": 4.5544444444444446e-05,
      "loss": 0.0044,
      "step": 8020
    },
    {
      "epoch": 0.7137777777777777,
      "grad_norm": 0.38809970021247864,
      "learning_rate": 4.553888888888889e-05,
      "loss": 0.0037,
      "step": 8030
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.6189413666725159,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0032,
      "step": 8040
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 0.6993224620819092,
      "learning_rate": 4.5527777777777784e-05,
      "loss": 0.0022,
      "step": 8050
    },
    {
      "epoch": 0.7164444444444444,
      "grad_norm": 0.1096416711807251,
      "learning_rate": 4.552222222222222e-05,
      "loss": 0.0027,
      "step": 8060
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.7933667898178101,
      "learning_rate": 4.551666666666667e-05,
      "loss": 0.0037,
      "step": 8070
    },
    {
      "epoch": 0.7182222222222222,
      "grad_norm": 0.7538002133369446,
      "learning_rate": 4.5511111111111114e-05,
      "loss": 0.0032,
      "step": 8080
    },
    {
      "epoch": 0.7191111111111111,
      "grad_norm": 0.33405107259750366,
      "learning_rate": 4.550555555555556e-05,
      "loss": 0.003,
      "step": 8090
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.363583505153656,
      "learning_rate": 4.55e-05,
      "loss": 0.0028,
      "step": 8100
    },
    {
      "epoch": 0.7208888888888889,
      "grad_norm": 0.38584259152412415,
      "learning_rate": 4.5494444444444445e-05,
      "loss": 0.0053,
      "step": 8110
    },
    {
      "epoch": 0.7217777777777777,
      "grad_norm": 0.07219885289669037,
      "learning_rate": 4.5488888888888895e-05,
      "loss": 0.0034,
      "step": 8120
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.9416822791099548,
      "learning_rate": 4.548333333333333e-05,
      "loss": 0.0038,
      "step": 8130
    },
    {
      "epoch": 0.7235555555555555,
      "grad_norm": 1.2056947946548462,
      "learning_rate": 4.547777777777778e-05,
      "loss": 0.0037,
      "step": 8140
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 0.15819710493087769,
      "learning_rate": 4.5472222222222226e-05,
      "loss": 0.0037,
      "step": 8150
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.3639027178287506,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0034,
      "step": 8160
    },
    {
      "epoch": 0.7262222222222222,
      "grad_norm": 1.0390621423721313,
      "learning_rate": 4.546111111111111e-05,
      "loss": 0.0036,
      "step": 8170
    },
    {
      "epoch": 0.7271111111111112,
      "grad_norm": 1.1840333938598633,
      "learning_rate": 4.545555555555556e-05,
      "loss": 0.0037,
      "step": 8180
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.3868110477924347,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.0026,
      "step": 8190
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.8744695782661438,
      "learning_rate": 4.5444444444444444e-05,
      "loss": 0.0044,
      "step": 8200
    },
    {
      "epoch": 0.7297777777777777,
      "grad_norm": 0.948094367980957,
      "learning_rate": 4.5438888888888894e-05,
      "loss": 0.0027,
      "step": 8210
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.43466153740882874,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.0037,
      "step": 8220
    },
    {
      "epoch": 0.7315555555555555,
      "grad_norm": 0.09669579565525055,
      "learning_rate": 4.542777777777778e-05,
      "loss": 0.0039,
      "step": 8230
    },
    {
      "epoch": 0.7324444444444445,
      "grad_norm": 0.5619885325431824,
      "learning_rate": 4.5422222222222225e-05,
      "loss": 0.0031,
      "step": 8240
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.6391027569770813,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.0021,
      "step": 8250
    },
    {
      "epoch": 0.7342222222222222,
      "grad_norm": 0.4299793243408203,
      "learning_rate": 4.541111111111112e-05,
      "loss": 0.003,
      "step": 8260
    },
    {
      "epoch": 0.7351111111111112,
      "grad_norm": 0.8030096292495728,
      "learning_rate": 4.5405555555555555e-05,
      "loss": 0.0024,
      "step": 8270
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.6048178672790527,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0033,
      "step": 8280
    },
    {
      "epoch": 0.7368888888888889,
      "grad_norm": 0.9024702310562134,
      "learning_rate": 4.539444444444444e-05,
      "loss": 0.0023,
      "step": 8290
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 0.9301935434341431,
      "learning_rate": 4.538888888888889e-05,
      "loss": 0.0035,
      "step": 8300
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.1887361854314804,
      "learning_rate": 4.5383333333333336e-05,
      "loss": 0.0027,
      "step": 8310
    },
    {
      "epoch": 0.7395555555555555,
      "grad_norm": 0.9221217632293701,
      "learning_rate": 4.537777777777778e-05,
      "loss": 0.0027,
      "step": 8320
    },
    {
      "epoch": 0.7404444444444445,
      "grad_norm": 0.9512748718261719,
      "learning_rate": 4.537222222222223e-05,
      "loss": 0.0035,
      "step": 8330
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.3740537762641907,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.002,
      "step": 8340
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 0.5121207237243652,
      "learning_rate": 4.536111111111112e-05,
      "loss": 0.0024,
      "step": 8350
    },
    {
      "epoch": 0.7431111111111111,
      "grad_norm": 1.2993028163909912,
      "learning_rate": 4.5355555555555554e-05,
      "loss": 0.003,
      "step": 8360
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.8945072889328003,
      "learning_rate": 4.5350000000000005e-05,
      "loss": 0.0034,
      "step": 8370
    },
    {
      "epoch": 0.7448888888888889,
      "grad_norm": 0.6725797057151794,
      "learning_rate": 4.534444444444445e-05,
      "loss": 0.0034,
      "step": 8380
    },
    {
      "epoch": 0.7457777777777778,
      "grad_norm": 1.3009546995162964,
      "learning_rate": 4.533888888888889e-05,
      "loss": 0.0036,
      "step": 8390
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 1.0467946529388428,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0028,
      "step": 8400
    },
    {
      "epoch": 0.7475555555555555,
      "grad_norm": 0.6594732999801636,
      "learning_rate": 4.532777777777778e-05,
      "loss": 0.0029,
      "step": 8410
    },
    {
      "epoch": 0.7484444444444445,
      "grad_norm": 0.20475654304027557,
      "learning_rate": 4.532222222222223e-05,
      "loss": 0.0039,
      "step": 8420
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.7328811287879944,
      "learning_rate": 4.5316666666666666e-05,
      "loss": 0.0022,
      "step": 8430
    },
    {
      "epoch": 0.7502222222222222,
      "grad_norm": 1.220306634902954,
      "learning_rate": 4.5311111111111116e-05,
      "loss": 0.0036,
      "step": 8440
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 0.16656742990016937,
      "learning_rate": 4.530555555555556e-05,
      "loss": 0.0033,
      "step": 8450
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.7252445220947266,
      "learning_rate": 4.53e-05,
      "loss": 0.0041,
      "step": 8460
    },
    {
      "epoch": 0.7528888888888889,
      "grad_norm": 1.0561498403549194,
      "learning_rate": 4.529444444444445e-05,
      "loss": 0.0034,
      "step": 8470
    },
    {
      "epoch": 0.7537777777777778,
      "grad_norm": 0.7973622679710388,
      "learning_rate": 4.528888888888889e-05,
      "loss": 0.0031,
      "step": 8480
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.19877435266971588,
      "learning_rate": 4.5283333333333334e-05,
      "loss": 0.0037,
      "step": 8490
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.8074641227722168,
      "learning_rate": 4.527777777777778e-05,
      "loss": 0.0028,
      "step": 8500
    },
    {
      "epoch": 0.7564444444444445,
      "grad_norm": 0.4635229706764221,
      "learning_rate": 4.527222222222223e-05,
      "loss": 0.0022,
      "step": 8510
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.5566064715385437,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0033,
      "step": 8520
    },
    {
      "epoch": 0.7582222222222222,
      "grad_norm": 1.0061235427856445,
      "learning_rate": 4.5261111111111115e-05,
      "loss": 0.0041,
      "step": 8530
    },
    {
      "epoch": 0.7591111111111111,
      "grad_norm": 1.4440827369689941,
      "learning_rate": 4.525555555555556e-05,
      "loss": 0.0024,
      "step": 8540
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.014176607131958,
      "learning_rate": 4.525e-05,
      "loss": 0.0032,
      "step": 8550
    },
    {
      "epoch": 0.7608888888888888,
      "grad_norm": 0.8050523400306702,
      "learning_rate": 4.5244444444444446e-05,
      "loss": 0.0028,
      "step": 8560
    },
    {
      "epoch": 0.7617777777777778,
      "grad_norm": 0.33796337246894836,
      "learning_rate": 4.523888888888889e-05,
      "loss": 0.0042,
      "step": 8570
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.521507203578949,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.004,
      "step": 8580
    },
    {
      "epoch": 0.7635555555555555,
      "grad_norm": 0.45423516631126404,
      "learning_rate": 4.522777777777778e-05,
      "loss": 0.0034,
      "step": 8590
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.7331753373146057,
      "learning_rate": 4.522222222222223e-05,
      "loss": 0.0029,
      "step": 8600
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.6942942142486572,
      "learning_rate": 4.521666666666667e-05,
      "loss": 0.0046,
      "step": 8610
    },
    {
      "epoch": 0.7662222222222222,
      "grad_norm": 1.2373194694519043,
      "learning_rate": 4.5211111111111114e-05,
      "loss": 0.0039,
      "step": 8620
    },
    {
      "epoch": 0.7671111111111111,
      "grad_norm": 1.2399224042892456,
      "learning_rate": 4.520555555555556e-05,
      "loss": 0.0037,
      "step": 8630
    },
    {
      "epoch": 0.768,
      "grad_norm": 1.0227978229522705,
      "learning_rate": 4.52e-05,
      "loss": 0.0035,
      "step": 8640
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 0.7047317624092102,
      "learning_rate": 4.5194444444444444e-05,
      "loss": 0.0028,
      "step": 8650
    },
    {
      "epoch": 0.7697777777777778,
      "grad_norm": 0.5300398468971252,
      "learning_rate": 4.5188888888888895e-05,
      "loss": 0.0021,
      "step": 8660
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.4630192816257477,
      "learning_rate": 4.518333333333333e-05,
      "loss": 0.0028,
      "step": 8670
    },
    {
      "epoch": 0.7715555555555556,
      "grad_norm": 0.7608903646469116,
      "learning_rate": 4.517777777777778e-05,
      "loss": 0.0019,
      "step": 8680
    },
    {
      "epoch": 0.7724444444444445,
      "grad_norm": 0.1531548649072647,
      "learning_rate": 4.5172222222222225e-05,
      "loss": 0.0037,
      "step": 8690
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.30197930335998535,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0026,
      "step": 8700
    },
    {
      "epoch": 0.7742222222222223,
      "grad_norm": 0.8545926809310913,
      "learning_rate": 4.516111111111111e-05,
      "loss": 0.004,
      "step": 8710
    },
    {
      "epoch": 0.7751111111111111,
      "grad_norm": 0.34223413467407227,
      "learning_rate": 4.5155555555555556e-05,
      "loss": 0.0041,
      "step": 8720
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.35162121057510376,
      "learning_rate": 4.5150000000000006e-05,
      "loss": 0.0028,
      "step": 8730
    },
    {
      "epoch": 0.7768888888888889,
      "grad_norm": 0.09383265674114227,
      "learning_rate": 4.514444444444444e-05,
      "loss": 0.0037,
      "step": 8740
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.44317907094955444,
      "learning_rate": 4.5138888888888894e-05,
      "loss": 0.0026,
      "step": 8750
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.18349702656269073,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.004,
      "step": 8760
    },
    {
      "epoch": 0.7795555555555556,
      "grad_norm": 0.1623149961233139,
      "learning_rate": 4.512777777777778e-05,
      "loss": 0.0026,
      "step": 8770
    },
    {
      "epoch": 0.7804444444444445,
      "grad_norm": 0.09422057867050171,
      "learning_rate": 4.5122222222222224e-05,
      "loss": 0.0033,
      "step": 8780
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 1.260248064994812,
      "learning_rate": 4.511666666666667e-05,
      "loss": 0.0052,
      "step": 8790
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 0.6597500443458557,
      "learning_rate": 4.511111111111112e-05,
      "loss": 0.0037,
      "step": 8800
    },
    {
      "epoch": 0.7831111111111111,
      "grad_norm": 0.42428484559059143,
      "learning_rate": 4.5105555555555555e-05,
      "loss": 0.0031,
      "step": 8810
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.20670363306999207,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0022,
      "step": 8820
    },
    {
      "epoch": 0.7848888888888889,
      "grad_norm": 0.6199697256088257,
      "learning_rate": 4.509444444444444e-05,
      "loss": 0.0029,
      "step": 8830
    },
    {
      "epoch": 0.7857777777777778,
      "grad_norm": 0.6952165365219116,
      "learning_rate": 4.508888888888889e-05,
      "loss": 0.0027,
      "step": 8840
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.1903221756219864,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.0029,
      "step": 8850
    },
    {
      "epoch": 0.7875555555555556,
      "grad_norm": 0.7234081029891968,
      "learning_rate": 4.507777777777778e-05,
      "loss": 0.0037,
      "step": 8860
    },
    {
      "epoch": 0.7884444444444444,
      "grad_norm": 0.11764077842235565,
      "learning_rate": 4.507222222222223e-05,
      "loss": 0.0048,
      "step": 8870
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.0684187263250351,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0046,
      "step": 8880
    },
    {
      "epoch": 0.7902222222222223,
      "grad_norm": 0.4447457194328308,
      "learning_rate": 4.506111111111112e-05,
      "loss": 0.0034,
      "step": 8890
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 1.791521430015564,
      "learning_rate": 4.5055555555555554e-05,
      "loss": 0.0036,
      "step": 8900
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.9090429544448853,
      "learning_rate": 4.5050000000000004e-05,
      "loss": 0.0042,
      "step": 8910
    },
    {
      "epoch": 0.7928888888888889,
      "grad_norm": 0.16665643453598022,
      "learning_rate": 4.504444444444445e-05,
      "loss": 0.004,
      "step": 8920
    },
    {
      "epoch": 0.7937777777777778,
      "grad_norm": 0.7787624597549438,
      "learning_rate": 4.503888888888889e-05,
      "loss": 0.0028,
      "step": 8930
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.10566429793834686,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0024,
      "step": 8940
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 0.25593358278274536,
      "learning_rate": 4.502777777777778e-05,
      "loss": 0.0034,
      "step": 8950
    },
    {
      "epoch": 0.7964444444444444,
      "grad_norm": 0.2214004546403885,
      "learning_rate": 4.502222222222223e-05,
      "loss": 0.0033,
      "step": 8960
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.18112944066524506,
      "learning_rate": 4.5016666666666665e-05,
      "loss": 0.0037,
      "step": 8970
    },
    {
      "epoch": 0.7982222222222223,
      "grad_norm": 0.9522618055343628,
      "learning_rate": 4.5011111111111116e-05,
      "loss": 0.0033,
      "step": 8980
    },
    {
      "epoch": 0.7991111111111111,
      "grad_norm": 0.6646908521652222,
      "learning_rate": 4.500555555555556e-05,
      "loss": 0.0028,
      "step": 8990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5044896006584167,
      "learning_rate": 4.5e-05,
      "loss": 0.0037,
      "step": 9000
    },
    {
      "epoch": 0.8008888888888889,
      "grad_norm": 0.651396632194519,
      "learning_rate": 4.4994444444444446e-05,
      "loss": 0.0034,
      "step": 9010
    },
    {
      "epoch": 0.8017777777777778,
      "grad_norm": 0.1308683305978775,
      "learning_rate": 4.498888888888889e-05,
      "loss": 0.0027,
      "step": 9020
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 1.1002064943313599,
      "learning_rate": 4.4983333333333334e-05,
      "loss": 0.0045,
      "step": 9030
    },
    {
      "epoch": 0.8035555555555556,
      "grad_norm": 1.0455188751220703,
      "learning_rate": 4.497777777777778e-05,
      "loss": 0.0038,
      "step": 9040
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 0.1879047006368637,
      "learning_rate": 4.497222222222223e-05,
      "loss": 0.0031,
      "step": 9050
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.5210130214691162,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.0031,
      "step": 9060
    },
    {
      "epoch": 0.8062222222222222,
      "grad_norm": 1.4598114490509033,
      "learning_rate": 4.4961111111111115e-05,
      "loss": 0.0026,
      "step": 9070
    },
    {
      "epoch": 0.8071111111111111,
      "grad_norm": 1.0455976724624634,
      "learning_rate": 4.495555555555556e-05,
      "loss": 0.005,
      "step": 9080
    },
    {
      "epoch": 0.808,
      "grad_norm": 1.2389354705810547,
      "learning_rate": 4.495e-05,
      "loss": 0.0036,
      "step": 9090
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 0.2998811602592468,
      "learning_rate": 4.4944444444444445e-05,
      "loss": 0.004,
      "step": 9100
    },
    {
      "epoch": 0.8097777777777778,
      "grad_norm": 0.17965565621852875,
      "learning_rate": 4.493888888888889e-05,
      "loss": 0.0031,
      "step": 9110
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 1.2008498907089233,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0041,
      "step": 9120
    },
    {
      "epoch": 0.8115555555555556,
      "grad_norm": 0.3863871395587921,
      "learning_rate": 4.492777777777778e-05,
      "loss": 0.0019,
      "step": 9130
    },
    {
      "epoch": 0.8124444444444444,
      "grad_norm": 0.7826944589614868,
      "learning_rate": 4.4922222222222226e-05,
      "loss": 0.0032,
      "step": 9140
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.1057443618774414,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.003,
      "step": 9150
    },
    {
      "epoch": 0.8142222222222222,
      "grad_norm": 0.550745964050293,
      "learning_rate": 4.491111111111111e-05,
      "loss": 0.003,
      "step": 9160
    },
    {
      "epoch": 0.8151111111111111,
      "grad_norm": 0.6357496976852417,
      "learning_rate": 4.490555555555556e-05,
      "loss": 0.0021,
      "step": 9170
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.09413599222898483,
      "learning_rate": 4.49e-05,
      "loss": 0.0034,
      "step": 9180
    },
    {
      "epoch": 0.8168888888888889,
      "grad_norm": 0.6291766166687012,
      "learning_rate": 4.4894444444444444e-05,
      "loss": 0.0046,
      "step": 9190
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.10346720367670059,
      "learning_rate": 4.4888888888888894e-05,
      "loss": 0.0024,
      "step": 9200
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.08152599632740021,
      "learning_rate": 4.488333333333333e-05,
      "loss": 0.0037,
      "step": 9210
    },
    {
      "epoch": 0.8195555555555556,
      "grad_norm": 0.30853596329689026,
      "learning_rate": 4.487777777777778e-05,
      "loss": 0.003,
      "step": 9220
    },
    {
      "epoch": 0.8204444444444444,
      "grad_norm": 0.8810782432556152,
      "learning_rate": 4.4872222222222225e-05,
      "loss": 0.0028,
      "step": 9230
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.7590665221214294,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0037,
      "step": 9240
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.32867902517318726,
      "learning_rate": 4.486111111111111e-05,
      "loss": 0.003,
      "step": 9250
    },
    {
      "epoch": 0.8231111111111111,
      "grad_norm": 1.148875117301941,
      "learning_rate": 4.4855555555555556e-05,
      "loss": 0.0025,
      "step": 9260
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.41585278511047363,
      "learning_rate": 4.4850000000000006e-05,
      "loss": 0.0031,
      "step": 9270
    },
    {
      "epoch": 0.8248888888888889,
      "grad_norm": 0.7679381370544434,
      "learning_rate": 4.484444444444444e-05,
      "loss": 0.004,
      "step": 9280
    },
    {
      "epoch": 0.8257777777777778,
      "grad_norm": 1.0852278470993042,
      "learning_rate": 4.483888888888889e-05,
      "loss": 0.0029,
      "step": 9290
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.9697750210762024,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0028,
      "step": 9300
    },
    {
      "epoch": 0.8275555555555556,
      "grad_norm": 0.6599872708320618,
      "learning_rate": 4.482777777777778e-05,
      "loss": 0.003,
      "step": 9310
    },
    {
      "epoch": 0.8284444444444444,
      "grad_norm": 0.9340050220489502,
      "learning_rate": 4.4822222222222224e-05,
      "loss": 0.0031,
      "step": 9320
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.5090123414993286,
      "learning_rate": 4.481666666666667e-05,
      "loss": 0.0019,
      "step": 9330
    },
    {
      "epoch": 0.8302222222222222,
      "grad_norm": 0.7772893309593201,
      "learning_rate": 4.481111111111112e-05,
      "loss": 0.0037,
      "step": 9340
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 0.5526055693626404,
      "learning_rate": 4.4805555555555554e-05,
      "loss": 0.0031,
      "step": 9350
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.4518522024154663,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0025,
      "step": 9360
    },
    {
      "epoch": 0.8328888888888889,
      "grad_norm": 0.20199863612651825,
      "learning_rate": 4.479444444444444e-05,
      "loss": 0.0036,
      "step": 9370
    },
    {
      "epoch": 0.8337777777777777,
      "grad_norm": 1.1423691511154175,
      "learning_rate": 4.478888888888889e-05,
      "loss": 0.0038,
      "step": 9380
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.42910000681877136,
      "learning_rate": 4.4783333333333335e-05,
      "loss": 0.0036,
      "step": 9390
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.19129832088947296,
      "learning_rate": 4.477777777777778e-05,
      "loss": 0.0024,
      "step": 9400
    },
    {
      "epoch": 0.8364444444444444,
      "grad_norm": 0.16138269007205963,
      "learning_rate": 4.477222222222223e-05,
      "loss": 0.0029,
      "step": 9410
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.2772267460823059,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0022,
      "step": 9420
    },
    {
      "epoch": 0.8382222222222222,
      "grad_norm": 0.7847541570663452,
      "learning_rate": 4.4761111111111116e-05,
      "loss": 0.0035,
      "step": 9430
    },
    {
      "epoch": 0.8391111111111111,
      "grad_norm": 1.366609811782837,
      "learning_rate": 4.475555555555555e-05,
      "loss": 0.0034,
      "step": 9440
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.3325429856777191,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.0027,
      "step": 9450
    },
    {
      "epoch": 0.8408888888888889,
      "grad_norm": 0.2811696231365204,
      "learning_rate": 4.474444444444445e-05,
      "loss": 0.0036,
      "step": 9460
    },
    {
      "epoch": 0.8417777777777777,
      "grad_norm": 1.0882066488265991,
      "learning_rate": 4.473888888888889e-05,
      "loss": 0.0045,
      "step": 9470
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.42245155572891235,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.004,
      "step": 9480
    },
    {
      "epoch": 0.8435555555555555,
      "grad_norm": 0.13462543487548828,
      "learning_rate": 4.472777777777778e-05,
      "loss": 0.0026,
      "step": 9490
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.6947293281555176,
      "learning_rate": 4.472222222222223e-05,
      "loss": 0.0024,
      "step": 9500
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 1.232428789138794,
      "learning_rate": 4.4716666666666665e-05,
      "loss": 0.0032,
      "step": 9510
    },
    {
      "epoch": 0.8462222222222222,
      "grad_norm": 0.4203694462776184,
      "learning_rate": 4.4711111111111115e-05,
      "loss": 0.0037,
      "step": 9520
    },
    {
      "epoch": 0.8471111111111111,
      "grad_norm": 0.4154917597770691,
      "learning_rate": 4.470555555555556e-05,
      "loss": 0.004,
      "step": 9530
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.4768281877040863,
      "learning_rate": 4.47e-05,
      "loss": 0.0027,
      "step": 9540
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 0.10472749918699265,
      "learning_rate": 4.4694444444444446e-05,
      "loss": 0.004,
      "step": 9550
    },
    {
      "epoch": 0.8497777777777777,
      "grad_norm": 0.06531824171543121,
      "learning_rate": 4.468888888888889e-05,
      "loss": 0.0033,
      "step": 9560
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.5315172672271729,
      "learning_rate": 4.468333333333334e-05,
      "loss": 0.0027,
      "step": 9570
    },
    {
      "epoch": 0.8515555555555555,
      "grad_norm": 0.25179407000541687,
      "learning_rate": 4.4677777777777777e-05,
      "loss": 0.0028,
      "step": 9580
    },
    {
      "epoch": 0.8524444444444444,
      "grad_norm": 1.274828553199768,
      "learning_rate": 4.467222222222223e-05,
      "loss": 0.0048,
      "step": 9590
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.1744309514760971,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0028,
      "step": 9600
    },
    {
      "epoch": 0.8542222222222222,
      "grad_norm": 0.33278512954711914,
      "learning_rate": 4.4661111111111114e-05,
      "loss": 0.0034,
      "step": 9610
    },
    {
      "epoch": 0.8551111111111112,
      "grad_norm": 0.25667649507522583,
      "learning_rate": 4.465555555555556e-05,
      "loss": 0.0033,
      "step": 9620
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.874264657497406,
      "learning_rate": 4.465e-05,
      "loss": 0.0029,
      "step": 9630
    },
    {
      "epoch": 0.8568888888888889,
      "grad_norm": 0.6897897720336914,
      "learning_rate": 4.4644444444444445e-05,
      "loss": 0.0025,
      "step": 9640
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 0.12870287895202637,
      "learning_rate": 4.463888888888889e-05,
      "loss": 0.0033,
      "step": 9650
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.933256983757019,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0028,
      "step": 9660
    },
    {
      "epoch": 0.8595555555555555,
      "grad_norm": 0.2182086706161499,
      "learning_rate": 4.462777777777778e-05,
      "loss": 0.003,
      "step": 9670
    },
    {
      "epoch": 0.8604444444444445,
      "grad_norm": 1.6463289260864258,
      "learning_rate": 4.4622222222222226e-05,
      "loss": 0.0039,
      "step": 9680
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.5692312121391296,
      "learning_rate": 4.461666666666667e-05,
      "loss": 0.0032,
      "step": 9690
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 1.2150307893753052,
      "learning_rate": 4.461111111111111e-05,
      "loss": 0.0029,
      "step": 9700
    },
    {
      "epoch": 0.8631111111111112,
      "grad_norm": 0.42145732045173645,
      "learning_rate": 4.4605555555555556e-05,
      "loss": 0.0029,
      "step": 9710
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.6068651676177979,
      "learning_rate": 4.46e-05,
      "loss": 0.0033,
      "step": 9720
    },
    {
      "epoch": 0.8648888888888889,
      "grad_norm": 1.1567223072052002,
      "learning_rate": 4.4594444444444443e-05,
      "loss": 0.004,
      "step": 9730
    },
    {
      "epoch": 0.8657777777777778,
      "grad_norm": 0.8117451667785645,
      "learning_rate": 4.4588888888888894e-05,
      "loss": 0.0035,
      "step": 9740
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.2814250588417053,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.0033,
      "step": 9750
    },
    {
      "epoch": 0.8675555555555555,
      "grad_norm": 0.7193543910980225,
      "learning_rate": 4.457777777777778e-05,
      "loss": 0.0035,
      "step": 9760
    },
    {
      "epoch": 0.8684444444444445,
      "grad_norm": 0.13461753726005554,
      "learning_rate": 4.4572222222222224e-05,
      "loss": 0.0036,
      "step": 9770
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.588319718837738,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0041,
      "step": 9780
    },
    {
      "epoch": 0.8702222222222222,
      "grad_norm": 1.479089379310608,
      "learning_rate": 4.456111111111111e-05,
      "loss": 0.0035,
      "step": 9790
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.8587395548820496,
      "learning_rate": 4.4555555555555555e-05,
      "loss": 0.0028,
      "step": 9800
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.2862933278083801,
      "learning_rate": 4.4550000000000005e-05,
      "loss": 0.0041,
      "step": 9810
    },
    {
      "epoch": 0.8728888888888889,
      "grad_norm": 0.34058627486228943,
      "learning_rate": 4.454444444444444e-05,
      "loss": 0.0037,
      "step": 9820
    },
    {
      "epoch": 0.8737777777777778,
      "grad_norm": 0.28512898087501526,
      "learning_rate": 4.453888888888889e-05,
      "loss": 0.0034,
      "step": 9830
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.5506910085678101,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0031,
      "step": 9840
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 1.221135139465332,
      "learning_rate": 4.452777777777778e-05,
      "loss": 0.0031,
      "step": 9850
    },
    {
      "epoch": 0.8764444444444445,
      "grad_norm": 0.4316251575946808,
      "learning_rate": 4.452222222222222e-05,
      "loss": 0.0041,
      "step": 9860
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.25018396973609924,
      "learning_rate": 4.451666666666667e-05,
      "loss": 0.002,
      "step": 9870
    },
    {
      "epoch": 0.8782222222222222,
      "grad_norm": 0.34590551257133484,
      "learning_rate": 4.451111111111112e-05,
      "loss": 0.0028,
      "step": 9880
    },
    {
      "epoch": 0.8791111111111111,
      "grad_norm": 0.4078885614871979,
      "learning_rate": 4.4505555555555554e-05,
      "loss": 0.0024,
      "step": 9890
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.0807061195373535,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.0025,
      "step": 9900
    },
    {
      "epoch": 0.8808888888888889,
      "grad_norm": 0.18524669110774994,
      "learning_rate": 4.449444444444444e-05,
      "loss": 0.0025,
      "step": 9910
    },
    {
      "epoch": 0.8817777777777778,
      "grad_norm": 0.975618839263916,
      "learning_rate": 4.448888888888889e-05,
      "loss": 0.0025,
      "step": 9920
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.894573986530304,
      "learning_rate": 4.4483333333333335e-05,
      "loss": 0.0044,
      "step": 9930
    },
    {
      "epoch": 0.8835555555555555,
      "grad_norm": 0.5576706528663635,
      "learning_rate": 4.447777777777778e-05,
      "loss": 0.0043,
      "step": 9940
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 0.45674818754196167,
      "learning_rate": 4.447222222222223e-05,
      "loss": 0.0036,
      "step": 9950
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.7477623820304871,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0022,
      "step": 9960
    },
    {
      "epoch": 0.8862222222222222,
      "grad_norm": 0.3394097685813904,
      "learning_rate": 4.4461111111111116e-05,
      "loss": 0.0032,
      "step": 9970
    },
    {
      "epoch": 0.8871111111111111,
      "grad_norm": 0.8647582530975342,
      "learning_rate": 4.445555555555555e-05,
      "loss": 0.0037,
      "step": 9980
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.9406424164772034,
      "learning_rate": 4.445e-05,
      "loss": 0.0024,
      "step": 9990
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.06767310947179794,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.0036,
      "step": 10000
    },
    {
      "epoch": 0.8897777777777778,
      "grad_norm": 0.7091838121414185,
      "learning_rate": 4.443888888888889e-05,
      "loss": 0.0022,
      "step": 10010
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.9979401230812073,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.003,
      "step": 10020
    },
    {
      "epoch": 0.8915555555555555,
      "grad_norm": 0.5882302522659302,
      "learning_rate": 4.442777777777778e-05,
      "loss": 0.0031,
      "step": 10030
    },
    {
      "epoch": 0.8924444444444445,
      "grad_norm": 1.0887839794158936,
      "learning_rate": 4.442222222222223e-05,
      "loss": 0.0029,
      "step": 10040
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.8473995327949524,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.004,
      "step": 10050
    },
    {
      "epoch": 0.8942222222222223,
      "grad_norm": 0.6000543236732483,
      "learning_rate": 4.4411111111111115e-05,
      "loss": 0.0036,
      "step": 10060
    },
    {
      "epoch": 0.8951111111111111,
      "grad_norm": 0.6592229604721069,
      "learning_rate": 4.440555555555556e-05,
      "loss": 0.0028,
      "step": 10070
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.24799178540706635,
      "learning_rate": 4.44e-05,
      "loss": 0.004,
      "step": 10080
    },
    {
      "epoch": 0.8968888888888888,
      "grad_norm": 0.7056900858879089,
      "learning_rate": 4.4394444444444445e-05,
      "loss": 0.0035,
      "step": 10090
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 0.6389843225479126,
      "learning_rate": 4.438888888888889e-05,
      "loss": 0.0037,
      "step": 10100
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.48331400752067566,
      "learning_rate": 4.438333333333334e-05,
      "loss": 0.0034,
      "step": 10110
    },
    {
      "epoch": 0.8995555555555556,
      "grad_norm": 0.8323099613189697,
      "learning_rate": 4.4377777777777776e-05,
      "loss": 0.0031,
      "step": 10120
    },
    {
      "epoch": 0.9004444444444445,
      "grad_norm": 0.3502698540687561,
      "learning_rate": 4.4372222222222226e-05,
      "loss": 0.0031,
      "step": 10130
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.7051088809967041,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.003,
      "step": 10140
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 0.5515104532241821,
      "learning_rate": 4.4361111111111113e-05,
      "loss": 0.0029,
      "step": 10150
    },
    {
      "epoch": 0.9031111111111111,
      "grad_norm": 1.1021987199783325,
      "learning_rate": 4.435555555555556e-05,
      "loss": 0.0033,
      "step": 10160
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.6006137728691101,
      "learning_rate": 4.435e-05,
      "loss": 0.0021,
      "step": 10170
    },
    {
      "epoch": 0.9048888888888889,
      "grad_norm": 0.27378571033477783,
      "learning_rate": 4.4344444444444444e-05,
      "loss": 0.0039,
      "step": 10180
    },
    {
      "epoch": 0.9057777777777778,
      "grad_norm": 0.7147640585899353,
      "learning_rate": 4.433888888888889e-05,
      "loss": 0.0027,
      "step": 10190
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.49752652645111084,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0027,
      "step": 10200
    },
    {
      "epoch": 0.9075555555555556,
      "grad_norm": 0.1273076981306076,
      "learning_rate": 4.432777777777778e-05,
      "loss": 0.0018,
      "step": 10210
    },
    {
      "epoch": 0.9084444444444445,
      "grad_norm": 0.894051730632782,
      "learning_rate": 4.4322222222222225e-05,
      "loss": 0.0035,
      "step": 10220
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.1194184422492981,
      "learning_rate": 4.431666666666667e-05,
      "loss": 0.0027,
      "step": 10230
    },
    {
      "epoch": 0.9102222222222223,
      "grad_norm": 0.18684184551239014,
      "learning_rate": 4.431111111111111e-05,
      "loss": 0.0035,
      "step": 10240
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.6242409348487854,
      "learning_rate": 4.4305555555555556e-05,
      "loss": 0.0023,
      "step": 10250
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.29232338070869446,
      "learning_rate": 4.43e-05,
      "loss": 0.0037,
      "step": 10260
    },
    {
      "epoch": 0.9128888888888889,
      "grad_norm": 0.20097209513187408,
      "learning_rate": 4.429444444444444e-05,
      "loss": 0.0035,
      "step": 10270
    },
    {
      "epoch": 0.9137777777777778,
      "grad_norm": 0.057561732828617096,
      "learning_rate": 4.428888888888889e-05,
      "loss": 0.0029,
      "step": 10280
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.8145558834075928,
      "learning_rate": 4.428333333333334e-05,
      "loss": 0.0032,
      "step": 10290
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 0.2521936297416687,
      "learning_rate": 4.427777777777778e-05,
      "loss": 0.005,
      "step": 10300
    },
    {
      "epoch": 0.9164444444444444,
      "grad_norm": 0.6564039587974548,
      "learning_rate": 4.4272222222222224e-05,
      "loss": 0.0036,
      "step": 10310
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.8923126459121704,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0027,
      "step": 10320
    },
    {
      "epoch": 0.9182222222222223,
      "grad_norm": 0.16226308047771454,
      "learning_rate": 4.426111111111111e-05,
      "loss": 0.003,
      "step": 10330
    },
    {
      "epoch": 0.9191111111111111,
      "grad_norm": 0.09288012236356735,
      "learning_rate": 4.4255555555555555e-05,
      "loss": 0.0037,
      "step": 10340
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.23447386920452118,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.0029,
      "step": 10350
    },
    {
      "epoch": 0.9208888888888889,
      "grad_norm": 0.47767123579978943,
      "learning_rate": 4.424444444444444e-05,
      "loss": 0.0032,
      "step": 10360
    },
    {
      "epoch": 0.9217777777777778,
      "grad_norm": 0.15381665527820587,
      "learning_rate": 4.423888888888889e-05,
      "loss": 0.0035,
      "step": 10370
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.6219918131828308,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0031,
      "step": 10380
    },
    {
      "epoch": 0.9235555555555556,
      "grad_norm": 0.16072846949100494,
      "learning_rate": 4.422777777777778e-05,
      "loss": 0.0029,
      "step": 10390
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.30295529961586,
      "learning_rate": 4.422222222222222e-05,
      "loss": 0.0032,
      "step": 10400
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.376386433839798,
      "learning_rate": 4.4216666666666666e-05,
      "loss": 0.0028,
      "step": 10410
    },
    {
      "epoch": 0.9262222222222222,
      "grad_norm": 0.14659784734249115,
      "learning_rate": 4.4211111111111117e-05,
      "loss": 0.0031,
      "step": 10420
    },
    {
      "epoch": 0.9271111111111111,
      "grad_norm": 0.1513098180294037,
      "learning_rate": 4.420555555555555e-05,
      "loss": 0.0023,
      "step": 10430
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.8019694089889526,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0031,
      "step": 10440
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 0.43865466117858887,
      "learning_rate": 4.419444444444444e-05,
      "loss": 0.003,
      "step": 10450
    },
    {
      "epoch": 0.9297777777777778,
      "grad_norm": 0.945828914642334,
      "learning_rate": 4.418888888888889e-05,
      "loss": 0.003,
      "step": 10460
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.8640909194946289,
      "learning_rate": 4.4183333333333334e-05,
      "loss": 0.004,
      "step": 10470
    },
    {
      "epoch": 0.9315555555555556,
      "grad_norm": 0.3297975957393646,
      "learning_rate": 4.417777777777778e-05,
      "loss": 0.004,
      "step": 10480
    },
    {
      "epoch": 0.9324444444444444,
      "grad_norm": 0.9996064305305481,
      "learning_rate": 4.417222222222223e-05,
      "loss": 0.0039,
      "step": 10490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.3992007076740265,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0019,
      "step": 10500
    },
    {
      "epoch": 0.9342222222222222,
      "grad_norm": 1.1160651445388794,
      "learning_rate": 4.4161111111111115e-05,
      "loss": 0.004,
      "step": 10510
    },
    {
      "epoch": 0.9351111111111111,
      "grad_norm": 1.0361464023590088,
      "learning_rate": 4.415555555555556e-05,
      "loss": 0.0032,
      "step": 10520
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.4480597674846649,
      "learning_rate": 4.415e-05,
      "loss": 0.0027,
      "step": 10530
    },
    {
      "epoch": 0.9368888888888889,
      "grad_norm": 0.724744975566864,
      "learning_rate": 4.4144444444444446e-05,
      "loss": 0.0037,
      "step": 10540
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 0.49286535382270813,
      "learning_rate": 4.413888888888889e-05,
      "loss": 0.0025,
      "step": 10550
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.20643939077854156,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0038,
      "step": 10560
    },
    {
      "epoch": 0.9395555555555556,
      "grad_norm": 0.9224111437797546,
      "learning_rate": 4.412777777777778e-05,
      "loss": 0.0023,
      "step": 10570
    },
    {
      "epoch": 0.9404444444444444,
      "grad_norm": 0.8986856341362,
      "learning_rate": 4.412222222222223e-05,
      "loss": 0.0025,
      "step": 10580
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.8892507553100586,
      "learning_rate": 4.411666666666667e-05,
      "loss": 0.0044,
      "step": 10590
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.31052833795547485,
      "learning_rate": 4.4111111111111114e-05,
      "loss": 0.0037,
      "step": 10600
    },
    {
      "epoch": 0.9431111111111111,
      "grad_norm": 1.1937702894210815,
      "learning_rate": 4.410555555555556e-05,
      "loss": 0.0033,
      "step": 10610
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.9190178513526917,
      "learning_rate": 4.41e-05,
      "loss": 0.0034,
      "step": 10620
    },
    {
      "epoch": 0.9448888888888889,
      "grad_norm": 0.6153244376182556,
      "learning_rate": 4.409444444444445e-05,
      "loss": 0.0037,
      "step": 10630
    },
    {
      "epoch": 0.9457777777777778,
      "grad_norm": 0.4887966811656952,
      "learning_rate": 4.408888888888889e-05,
      "loss": 0.0027,
      "step": 10640
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.8958576321601868,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.0027,
      "step": 10650
    },
    {
      "epoch": 0.9475555555555556,
      "grad_norm": 0.20007173717021942,
      "learning_rate": 4.407777777777778e-05,
      "loss": 0.0034,
      "step": 10660
    },
    {
      "epoch": 0.9484444444444444,
      "grad_norm": 1.5277578830718994,
      "learning_rate": 4.4072222222222226e-05,
      "loss": 0.0031,
      "step": 10670
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.8975080847740173,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0028,
      "step": 10680
    },
    {
      "epoch": 0.9502222222222222,
      "grad_norm": 0.14980624616146088,
      "learning_rate": 4.406111111111111e-05,
      "loss": 0.0031,
      "step": 10690
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 0.6440573930740356,
      "learning_rate": 4.4055555555555557e-05,
      "loss": 0.0026,
      "step": 10700
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.06917641311883926,
      "learning_rate": 4.405e-05,
      "loss": 0.0027,
      "step": 10710
    },
    {
      "epoch": 0.9528888888888889,
      "grad_norm": 0.45235806703567505,
      "learning_rate": 4.404444444444445e-05,
      "loss": 0.0053,
      "step": 10720
    },
    {
      "epoch": 0.9537777777777777,
      "grad_norm": 0.2539547383785248,
      "learning_rate": 4.4038888888888894e-05,
      "loss": 0.0028,
      "step": 10730
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.41294488310813904,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.0045,
      "step": 10740
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 1.0931590795516968,
      "learning_rate": 4.402777777777778e-05,
      "loss": 0.003,
      "step": 10750
    },
    {
      "epoch": 0.9564444444444444,
      "grad_norm": 0.09070123732089996,
      "learning_rate": 4.4022222222222225e-05,
      "loss": 0.0048,
      "step": 10760
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.6695075035095215,
      "learning_rate": 4.401666666666667e-05,
      "loss": 0.0028,
      "step": 10770
    },
    {
      "epoch": 0.9582222222222222,
      "grad_norm": 0.2911321222782135,
      "learning_rate": 4.401111111111111e-05,
      "loss": 0.0021,
      "step": 10780
    },
    {
      "epoch": 0.9591111111111111,
      "grad_norm": 0.42568403482437134,
      "learning_rate": 4.4005555555555555e-05,
      "loss": 0.0037,
      "step": 10790
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.3404895067214966,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0023,
      "step": 10800
    },
    {
      "epoch": 0.9608888888888889,
      "grad_norm": 1.10556960105896,
      "learning_rate": 4.399444444444445e-05,
      "loss": 0.003,
      "step": 10810
    },
    {
      "epoch": 0.9617777777777777,
      "grad_norm": 0.9174990057945251,
      "learning_rate": 4.398888888888889e-05,
      "loss": 0.003,
      "step": 10820
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.7876389622688293,
      "learning_rate": 4.3983333333333336e-05,
      "loss": 0.0019,
      "step": 10830
    },
    {
      "epoch": 0.9635555555555556,
      "grad_norm": 0.33247435092926025,
      "learning_rate": 4.397777777777778e-05,
      "loss": 0.0034,
      "step": 10840
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 1.0905722379684448,
      "learning_rate": 4.3972222222222223e-05,
      "loss": 0.0031,
      "step": 10850
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 1.531059741973877,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0036,
      "step": 10860
    },
    {
      "epoch": 0.9662222222222222,
      "grad_norm": 0.23473165929317474,
      "learning_rate": 4.396111111111112e-05,
      "loss": 0.0026,
      "step": 10870
    },
    {
      "epoch": 0.9671111111111111,
      "grad_norm": 0.47415679693222046,
      "learning_rate": 4.3955555555555554e-05,
      "loss": 0.0038,
      "step": 10880
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.14122912287712097,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 0.0037,
      "step": 10890
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 0.23896828293800354,
      "learning_rate": 4.394444444444445e-05,
      "loss": 0.0031,
      "step": 10900
    },
    {
      "epoch": 0.9697777777777777,
      "grad_norm": 0.3906586170196533,
      "learning_rate": 4.393888888888889e-05,
      "loss": 0.0029,
      "step": 10910
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.28207769989967346,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0045,
      "step": 10920
    },
    {
      "epoch": 0.9715555555555555,
      "grad_norm": 0.375380277633667,
      "learning_rate": 4.392777777777778e-05,
      "loss": 0.0027,
      "step": 10930
    },
    {
      "epoch": 0.9724444444444444,
      "grad_norm": 0.7344698905944824,
      "learning_rate": 4.392222222222223e-05,
      "loss": 0.0031,
      "step": 10940
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 1.16274893283844,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.0024,
      "step": 10950
    },
    {
      "epoch": 0.9742222222222222,
      "grad_norm": 0.6958690285682678,
      "learning_rate": 4.3911111111111116e-05,
      "loss": 0.0032,
      "step": 10960
    },
    {
      "epoch": 0.9751111111111112,
      "grad_norm": 0.21749310195446014,
      "learning_rate": 4.390555555555555e-05,
      "loss": 0.0023,
      "step": 10970
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.14507678151130676,
      "learning_rate": 4.39e-05,
      "loss": 0.0031,
      "step": 10980
    },
    {
      "epoch": 0.9768888888888889,
      "grad_norm": 0.5360130071640015,
      "learning_rate": 4.389444444444445e-05,
      "loss": 0.0032,
      "step": 10990
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.25654417276382446,
      "learning_rate": 4.388888888888889e-05,
      "loss": 0.0034,
      "step": 11000
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.40590938925743103,
      "learning_rate": 4.388333333333334e-05,
      "loss": 0.0041,
      "step": 11010
    },
    {
      "epoch": 0.9795555555555555,
      "grad_norm": 0.19024796783924103,
      "learning_rate": 4.387777777777778e-05,
      "loss": 0.0035,
      "step": 11020
    },
    {
      "epoch": 0.9804444444444445,
      "grad_norm": 0.17456291615962982,
      "learning_rate": 4.387222222222223e-05,
      "loss": 0.0025,
      "step": 11030
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.15145178139209747,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0029,
      "step": 11040
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 0.6571993827819824,
      "learning_rate": 4.3861111111111115e-05,
      "loss": 0.0027,
      "step": 11050
    },
    {
      "epoch": 0.9831111111111112,
      "grad_norm": 0.07066933810710907,
      "learning_rate": 4.385555555555556e-05,
      "loss": 0.0025,
      "step": 11060
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.8643582463264465,
      "learning_rate": 4.385e-05,
      "loss": 0.0021,
      "step": 11070
    },
    {
      "epoch": 0.9848888888888889,
      "grad_norm": 0.5598185658454895,
      "learning_rate": 4.384444444444445e-05,
      "loss": 0.0037,
      "step": 11080
    },
    {
      "epoch": 0.9857777777777778,
      "grad_norm": 0.22143030166625977,
      "learning_rate": 4.383888888888889e-05,
      "loss": 0.0033,
      "step": 11090
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.4539887011051178,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.0039,
      "step": 11100
    },
    {
      "epoch": 0.9875555555555555,
      "grad_norm": 0.7870289087295532,
      "learning_rate": 4.3827777777777776e-05,
      "loss": 0.003,
      "step": 11110
    },
    {
      "epoch": 0.9884444444444445,
      "grad_norm": 0.9483734965324402,
      "learning_rate": 4.3822222222222227e-05,
      "loss": 0.0035,
      "step": 11120
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 1.0179260969161987,
      "learning_rate": 4.381666666666667e-05,
      "loss": 0.004,
      "step": 11130
    },
    {
      "epoch": 0.9902222222222222,
      "grad_norm": 1.2514305114746094,
      "learning_rate": 4.3811111111111114e-05,
      "loss": 0.0033,
      "step": 11140
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 0.4348991811275482,
      "learning_rate": 4.380555555555556e-05,
      "loss": 0.0036,
      "step": 11150
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.6682591438293457,
      "learning_rate": 4.38e-05,
      "loss": 0.0031,
      "step": 11160
    },
    {
      "epoch": 0.9928888888888889,
      "grad_norm": 0.09556692838668823,
      "learning_rate": 4.379444444444445e-05,
      "loss": 0.0026,
      "step": 11170
    },
    {
      "epoch": 0.9937777777777778,
      "grad_norm": 0.6698126792907715,
      "learning_rate": 4.378888888888889e-05,
      "loss": 0.0025,
      "step": 11180
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.726330041885376,
      "learning_rate": 4.378333333333334e-05,
      "loss": 0.003,
      "step": 11190
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.6208438277244568,
      "learning_rate": 4.377777777777778e-05,
      "loss": 0.0032,
      "step": 11200
    },
    {
      "epoch": 0.9964444444444445,
      "grad_norm": 0.9903480410575867,
      "learning_rate": 4.3772222222222225e-05,
      "loss": 0.0028,
      "step": 11210
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.39736783504486084,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0026,
      "step": 11220
    },
    {
      "epoch": 0.9982222222222222,
      "grad_norm": 0.2900053560733795,
      "learning_rate": 4.376111111111111e-05,
      "loss": 0.0022,
      "step": 11230
    },
    {
      "epoch": 0.9991111111111111,
      "grad_norm": 0.578719973564148,
      "learning_rate": 4.3755555555555556e-05,
      "loss": 0.0031,
      "step": 11240
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7829018235206604,
      "learning_rate": 4.375e-05,
      "loss": 0.003,
      "step": 11250
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0036009883042424917,
      "eval_runtime": 98.0521,
      "eval_samples_per_second": 1529.799,
      "eval_steps_per_second": 38.245,
      "step": 11250
    },
    {
      "epoch": 1.000888888888889,
      "grad_norm": 0.635394275188446,
      "learning_rate": 4.374444444444445e-05,
      "loss": 0.0044,
      "step": 11260
    },
    {
      "epoch": 1.0017777777777779,
      "grad_norm": 0.2265237271785736,
      "learning_rate": 4.3738888888888893e-05,
      "loss": 0.003,
      "step": 11270
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.3808118999004364,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0028,
      "step": 11280
    },
    {
      "epoch": 1.0035555555555555,
      "grad_norm": 0.81153404712677,
      "learning_rate": 4.372777777777778e-05,
      "loss": 0.0025,
      "step": 11290
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 0.803604006767273,
      "learning_rate": 4.3722222222222224e-05,
      "loss": 0.0031,
      "step": 11300
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.28390946984291077,
      "learning_rate": 4.371666666666667e-05,
      "loss": 0.0025,
      "step": 11310
    },
    {
      "epoch": 1.0062222222222221,
      "grad_norm": 1.3021526336669922,
      "learning_rate": 4.371111111111111e-05,
      "loss": 0.0042,
      "step": 11320
    },
    {
      "epoch": 1.007111111111111,
      "grad_norm": 0.25628456473350525,
      "learning_rate": 4.3705555555555555e-05,
      "loss": 0.003,
      "step": 11330
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.4888896942138672,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.002,
      "step": 11340
    },
    {
      "epoch": 1.008888888888889,
      "grad_norm": 0.9910843372344971,
      "learning_rate": 4.369444444444445e-05,
      "loss": 0.0027,
      "step": 11350
    },
    {
      "epoch": 1.0097777777777779,
      "grad_norm": 0.5062960982322693,
      "learning_rate": 4.368888888888889e-05,
      "loss": 0.0025,
      "step": 11360
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.05098867416381836,
      "learning_rate": 4.3683333333333336e-05,
      "loss": 0.0035,
      "step": 11370
    },
    {
      "epoch": 1.0115555555555555,
      "grad_norm": 0.8669632077217102,
      "learning_rate": 4.367777777777778e-05,
      "loss": 0.0041,
      "step": 11380
    },
    {
      "epoch": 1.0124444444444445,
      "grad_norm": 1.0941044092178345,
      "learning_rate": 4.367222222222222e-05,
      "loss": 0.0022,
      "step": 11390
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.8953132629394531,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0039,
      "step": 11400
    },
    {
      "epoch": 1.0142222222222221,
      "grad_norm": 0.8110688924789429,
      "learning_rate": 4.366111111111112e-05,
      "loss": 0.0028,
      "step": 11410
    },
    {
      "epoch": 1.015111111111111,
      "grad_norm": 0.7831413745880127,
      "learning_rate": 4.3655555555555554e-05,
      "loss": 0.0038,
      "step": 11420
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.31581997871398926,
      "learning_rate": 4.3650000000000004e-05,
      "loss": 0.0042,
      "step": 11430
    },
    {
      "epoch": 1.016888888888889,
      "grad_norm": 0.9028503894805908,
      "learning_rate": 4.364444444444445e-05,
      "loss": 0.0032,
      "step": 11440
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 0.2931082248687744,
      "learning_rate": 4.363888888888889e-05,
      "loss": 0.0029,
      "step": 11450
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.28490737080574036,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.004,
      "step": 11460
    },
    {
      "epoch": 1.0195555555555555,
      "grad_norm": 0.7653388381004333,
      "learning_rate": 4.362777777777778e-05,
      "loss": 0.0036,
      "step": 11470
    },
    {
      "epoch": 1.0204444444444445,
      "grad_norm": 0.611140787601471,
      "learning_rate": 4.362222222222223e-05,
      "loss": 0.0026,
      "step": 11480
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.3978647291660309,
      "learning_rate": 4.3616666666666665e-05,
      "loss": 0.0029,
      "step": 11490
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.6801101565361023,
      "learning_rate": 4.3611111111111116e-05,
      "loss": 0.0035,
      "step": 11500
    },
    {
      "epoch": 1.023111111111111,
      "grad_norm": 0.65406334400177,
      "learning_rate": 4.360555555555555e-05,
      "loss": 0.0035,
      "step": 11510
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.4714961647987366,
      "learning_rate": 4.36e-05,
      "loss": 0.0034,
      "step": 11520
    },
    {
      "epoch": 1.024888888888889,
      "grad_norm": 0.4452448785305023,
      "learning_rate": 4.3594444444444446e-05,
      "loss": 0.0024,
      "step": 11530
    },
    {
      "epoch": 1.0257777777777777,
      "grad_norm": 1.031485676765442,
      "learning_rate": 4.358888888888889e-05,
      "loss": 0.0026,
      "step": 11540
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.08803766220808029,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.0034,
      "step": 11550
    },
    {
      "epoch": 1.0275555555555556,
      "grad_norm": 0.7387800812721252,
      "learning_rate": 4.357777777777778e-05,
      "loss": 0.0037,
      "step": 11560
    },
    {
      "epoch": 1.0284444444444445,
      "grad_norm": 0.23249898850917816,
      "learning_rate": 4.357222222222223e-05,
      "loss": 0.003,
      "step": 11570
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.3292646110057831,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.003,
      "step": 11580
    },
    {
      "epoch": 1.0302222222222222,
      "grad_norm": 0.4815538227558136,
      "learning_rate": 4.3561111111111114e-05,
      "loss": 0.003,
      "step": 11590
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 0.7171255946159363,
      "learning_rate": 4.355555555555556e-05,
      "loss": 0.0025,
      "step": 11600
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.8329671025276184,
      "learning_rate": 4.355e-05,
      "loss": 0.0023,
      "step": 11610
    },
    {
      "epoch": 1.032888888888889,
      "grad_norm": 0.5423620939254761,
      "learning_rate": 4.354444444444445e-05,
      "loss": 0.0031,
      "step": 11620
    },
    {
      "epoch": 1.0337777777777777,
      "grad_norm": 0.6706644892692566,
      "learning_rate": 4.353888888888889e-05,
      "loss": 0.0031,
      "step": 11630
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.1934938132762909,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0031,
      "step": 11640
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.3676137328147888,
      "learning_rate": 4.3527777777777776e-05,
      "loss": 0.0038,
      "step": 11650
    },
    {
      "epoch": 1.0364444444444445,
      "grad_norm": 0.10582566261291504,
      "learning_rate": 4.3522222222222226e-05,
      "loss": 0.0038,
      "step": 11660
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.5716276168823242,
      "learning_rate": 4.351666666666667e-05,
      "loss": 0.0031,
      "step": 11670
    },
    {
      "epoch": 1.0382222222222222,
      "grad_norm": 0.6529156565666199,
      "learning_rate": 4.351111111111111e-05,
      "loss": 0.0034,
      "step": 11680
    },
    {
      "epoch": 1.039111111111111,
      "grad_norm": 0.9568498730659485,
      "learning_rate": 4.350555555555556e-05,
      "loss": 0.0023,
      "step": 11690
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.1523430496454239,
      "learning_rate": 4.35e-05,
      "loss": 0.0044,
      "step": 11700
    },
    {
      "epoch": 1.040888888888889,
      "grad_norm": 0.612315833568573,
      "learning_rate": 4.349444444444445e-05,
      "loss": 0.0022,
      "step": 11710
    },
    {
      "epoch": 1.0417777777777777,
      "grad_norm": 0.05869295448064804,
      "learning_rate": 4.348888888888889e-05,
      "loss": 0.0034,
      "step": 11720
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.34797239303588867,
      "learning_rate": 4.348333333333334e-05,
      "loss": 0.0046,
      "step": 11730
    },
    {
      "epoch": 1.0435555555555556,
      "grad_norm": 1.152698278427124,
      "learning_rate": 4.347777777777778e-05,
      "loss": 0.0036,
      "step": 11740
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 0.8550496697425842,
      "learning_rate": 4.3472222222222225e-05,
      "loss": 0.0039,
      "step": 11750
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.4541335701942444,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0028,
      "step": 11760
    },
    {
      "epoch": 1.0462222222222222,
      "grad_norm": 0.3315020203590393,
      "learning_rate": 4.346111111111111e-05,
      "loss": 0.0035,
      "step": 11770
    },
    {
      "epoch": 1.047111111111111,
      "grad_norm": 0.46102333068847656,
      "learning_rate": 4.3455555555555555e-05,
      "loss": 0.0033,
      "step": 11780
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.07068821042776108,
      "learning_rate": 4.345e-05,
      "loss": 0.0038,
      "step": 11790
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 0.5532928109169006,
      "learning_rate": 4.344444444444445e-05,
      "loss": 0.0032,
      "step": 11800
    },
    {
      "epoch": 1.0497777777777777,
      "grad_norm": 0.26504650712013245,
      "learning_rate": 4.343888888888889e-05,
      "loss": 0.0026,
      "step": 11810
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.7038185596466064,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0036,
      "step": 11820
    },
    {
      "epoch": 1.0515555555555556,
      "grad_norm": 0.4380171597003937,
      "learning_rate": 4.342777777777778e-05,
      "loss": 0.0023,
      "step": 11830
    },
    {
      "epoch": 1.0524444444444445,
      "grad_norm": 0.1621265709400177,
      "learning_rate": 4.3422222222222224e-05,
      "loss": 0.002,
      "step": 11840
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.5906186103820801,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.0039,
      "step": 11850
    },
    {
      "epoch": 1.0542222222222222,
      "grad_norm": 0.6601200699806213,
      "learning_rate": 4.341111111111111e-05,
      "loss": 0.0022,
      "step": 11860
    },
    {
      "epoch": 1.055111111111111,
      "grad_norm": 0.252581924200058,
      "learning_rate": 4.3405555555555554e-05,
      "loss": 0.0026,
      "step": 11870
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.40926098823547363,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.004,
      "step": 11880
    },
    {
      "epoch": 1.056888888888889,
      "grad_norm": 0.09566201269626617,
      "learning_rate": 4.339444444444445e-05,
      "loss": 0.0023,
      "step": 11890
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 0.38025224208831787,
      "learning_rate": 4.338888888888889e-05,
      "loss": 0.003,
      "step": 11900
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.5353982448577881,
      "learning_rate": 4.3383333333333335e-05,
      "loss": 0.0042,
      "step": 11910
    },
    {
      "epoch": 1.0595555555555556,
      "grad_norm": 0.37959054112434387,
      "learning_rate": 4.337777777777778e-05,
      "loss": 0.0023,
      "step": 11920
    },
    {
      "epoch": 1.0604444444444445,
      "grad_norm": 0.30436640977859497,
      "learning_rate": 4.337222222222222e-05,
      "loss": 0.0034,
      "step": 11930
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.4632054567337036,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0024,
      "step": 11940
    },
    {
      "epoch": 1.0622222222222222,
      "grad_norm": 0.4085494875907898,
      "learning_rate": 4.3361111111111116e-05,
      "loss": 0.0021,
      "step": 11950
    },
    {
      "epoch": 1.0631111111111111,
      "grad_norm": 0.6781633496284485,
      "learning_rate": 4.335555555555556e-05,
      "loss": 0.0032,
      "step": 11960
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.12656930088996887,
      "learning_rate": 4.335e-05,
      "loss": 0.0027,
      "step": 11970
    },
    {
      "epoch": 1.064888888888889,
      "grad_norm": 0.38485071063041687,
      "learning_rate": 4.334444444444445e-05,
      "loss": 0.003,
      "step": 11980
    },
    {
      "epoch": 1.0657777777777777,
      "grad_norm": 0.3037499487400055,
      "learning_rate": 4.333888888888889e-05,
      "loss": 0.0028,
      "step": 11990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.46448272466659546,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0044,
      "step": 12000
    },
    {
      "epoch": 1.0675555555555556,
      "grad_norm": 0.20806419849395752,
      "learning_rate": 4.332777777777778e-05,
      "loss": 0.0039,
      "step": 12010
    },
    {
      "epoch": 1.0684444444444445,
      "grad_norm": 0.42069366574287415,
      "learning_rate": 4.332222222222223e-05,
      "loss": 0.0037,
      "step": 12020
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.07060462981462479,
      "learning_rate": 4.3316666666666665e-05,
      "loss": 0.0035,
      "step": 12030
    },
    {
      "epoch": 1.0702222222222222,
      "grad_norm": 0.25094735622406006,
      "learning_rate": 4.3311111111111115e-05,
      "loss": 0.003,
      "step": 12040
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.07839342951774597,
      "learning_rate": 4.330555555555556e-05,
      "loss": 0.0027,
      "step": 12050
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.21048808097839355,
      "learning_rate": 4.33e-05,
      "loss": 0.0024,
      "step": 12060
    },
    {
      "epoch": 1.072888888888889,
      "grad_norm": 0.5121265053749084,
      "learning_rate": 4.3294444444444446e-05,
      "loss": 0.003,
      "step": 12070
    },
    {
      "epoch": 1.0737777777777777,
      "grad_norm": 0.7647085189819336,
      "learning_rate": 4.328888888888889e-05,
      "loss": 0.0032,
      "step": 12080
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.9116200804710388,
      "learning_rate": 4.328333333333334e-05,
      "loss": 0.0031,
      "step": 12090
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 0.9842276573181152,
      "learning_rate": 4.3277777777777776e-05,
      "loss": 0.0029,
      "step": 12100
    },
    {
      "epoch": 1.0764444444444445,
      "grad_norm": 0.4024580121040344,
      "learning_rate": 4.327222222222223e-05,
      "loss": 0.0026,
      "step": 12110
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.17826016247272491,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0034,
      "step": 12120
    },
    {
      "epoch": 1.0782222222222222,
      "grad_norm": 0.7093251347541809,
      "learning_rate": 4.3261111111111114e-05,
      "loss": 0.0026,
      "step": 12130
    },
    {
      "epoch": 1.0791111111111111,
      "grad_norm": 0.69560706615448,
      "learning_rate": 4.325555555555556e-05,
      "loss": 0.0043,
      "step": 12140
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.1346474438905716,
      "learning_rate": 4.325e-05,
      "loss": 0.003,
      "step": 12150
    },
    {
      "epoch": 1.0808888888888888,
      "grad_norm": 0.30253714323043823,
      "learning_rate": 4.324444444444445e-05,
      "loss": 0.0035,
      "step": 12160
    },
    {
      "epoch": 1.0817777777777777,
      "grad_norm": 0.7058501839637756,
      "learning_rate": 4.323888888888889e-05,
      "loss": 0.0033,
      "step": 12170
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.9336646795272827,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0029,
      "step": 12180
    },
    {
      "epoch": 1.0835555555555556,
      "grad_norm": 0.8078418374061584,
      "learning_rate": 4.3227777777777775e-05,
      "loss": 0.0047,
      "step": 12190
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 0.7594984769821167,
      "learning_rate": 4.3222222222222226e-05,
      "loss": 0.0034,
      "step": 12200
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.9187895059585571,
      "learning_rate": 4.321666666666667e-05,
      "loss": 0.003,
      "step": 12210
    },
    {
      "epoch": 1.0862222222222222,
      "grad_norm": 1.5618677139282227,
      "learning_rate": 4.321111111111111e-05,
      "loss": 0.003,
      "step": 12220
    },
    {
      "epoch": 1.0871111111111111,
      "grad_norm": 0.9959554076194763,
      "learning_rate": 4.320555555555556e-05,
      "loss": 0.0024,
      "step": 12230
    },
    {
      "epoch": 1.088,
      "grad_norm": 1.1162599325180054,
      "learning_rate": 4.32e-05,
      "loss": 0.0033,
      "step": 12240
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.2772497236728668,
      "learning_rate": 4.319444444444445e-05,
      "loss": 0.003,
      "step": 12250
    },
    {
      "epoch": 1.0897777777777777,
      "grad_norm": 0.6370680332183838,
      "learning_rate": 4.318888888888889e-05,
      "loss": 0.0034,
      "step": 12260
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.904666006565094,
      "learning_rate": 4.318333333333334e-05,
      "loss": 0.0024,
      "step": 12270
    },
    {
      "epoch": 1.0915555555555556,
      "grad_norm": 0.2598462998867035,
      "learning_rate": 4.317777777777778e-05,
      "loss": 0.0034,
      "step": 12280
    },
    {
      "epoch": 1.0924444444444443,
      "grad_norm": 0.42540138959884644,
      "learning_rate": 4.3172222222222224e-05,
      "loss": 0.0044,
      "step": 12290
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 1.070458173751831,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0034,
      "step": 12300
    },
    {
      "epoch": 1.0942222222222222,
      "grad_norm": 0.3668721616268158,
      "learning_rate": 4.316111111111111e-05,
      "loss": 0.0036,
      "step": 12310
    },
    {
      "epoch": 1.0951111111111111,
      "grad_norm": 1.2063202857971191,
      "learning_rate": 4.315555555555556e-05,
      "loss": 0.0025,
      "step": 12320
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.8546929359436035,
      "learning_rate": 4.315e-05,
      "loss": 0.003,
      "step": 12330
    },
    {
      "epoch": 1.0968888888888888,
      "grad_norm": 0.5126823782920837,
      "learning_rate": 4.314444444444445e-05,
      "loss": 0.0041,
      "step": 12340
    },
    {
      "epoch": 1.0977777777777777,
      "grad_norm": 0.24461711943149567,
      "learning_rate": 4.313888888888889e-05,
      "loss": 0.0053,
      "step": 12350
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.6412931680679321,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0021,
      "step": 12360
    },
    {
      "epoch": 1.0995555555555556,
      "grad_norm": 0.5499972701072693,
      "learning_rate": 4.312777777777778e-05,
      "loss": 0.0044,
      "step": 12370
    },
    {
      "epoch": 1.1004444444444443,
      "grad_norm": 0.5308835506439209,
      "learning_rate": 4.312222222222222e-05,
      "loss": 0.0026,
      "step": 12380
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.5386735200881958,
      "learning_rate": 4.311666666666667e-05,
      "loss": 0.0037,
      "step": 12390
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 0.9424393773078918,
      "learning_rate": 4.311111111111111e-05,
      "loss": 0.0032,
      "step": 12400
    },
    {
      "epoch": 1.1031111111111112,
      "grad_norm": 1.2770715951919556,
      "learning_rate": 4.310555555555556e-05,
      "loss": 0.0031,
      "step": 12410
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.7480890154838562,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.0033,
      "step": 12420
    },
    {
      "epoch": 1.1048888888888888,
      "grad_norm": 0.4197980761528015,
      "learning_rate": 4.309444444444445e-05,
      "loss": 0.0025,
      "step": 12430
    },
    {
      "epoch": 1.1057777777777777,
      "grad_norm": 0.9105686545372009,
      "learning_rate": 4.308888888888889e-05,
      "loss": 0.0023,
      "step": 12440
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.7423873543739319,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.0031,
      "step": 12450
    },
    {
      "epoch": 1.1075555555555556,
      "grad_norm": 0.4328465759754181,
      "learning_rate": 4.307777777777778e-05,
      "loss": 0.0036,
      "step": 12460
    },
    {
      "epoch": 1.1084444444444443,
      "grad_norm": 0.37377265095710754,
      "learning_rate": 4.307222222222222e-05,
      "loss": 0.0025,
      "step": 12470
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.1903696060180664,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0018,
      "step": 12480
    },
    {
      "epoch": 1.1102222222222222,
      "grad_norm": 0.2983822226524353,
      "learning_rate": 4.3061111111111116e-05,
      "loss": 0.0043,
      "step": 12490
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.4982466399669647,
      "learning_rate": 4.305555555555556e-05,
      "loss": 0.0029,
      "step": 12500
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.08903191238641739,
      "learning_rate": 4.305e-05,
      "loss": 0.0025,
      "step": 12510
    },
    {
      "epoch": 1.1128888888888888,
      "grad_norm": 0.3824019134044647,
      "learning_rate": 4.3044444444444446e-05,
      "loss": 0.003,
      "step": 12520
    },
    {
      "epoch": 1.1137777777777778,
      "grad_norm": 0.42043712735176086,
      "learning_rate": 4.303888888888889e-05,
      "loss": 0.0029,
      "step": 12530
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.13892383873462677,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.0025,
      "step": 12540
    },
    {
      "epoch": 1.1155555555555556,
      "grad_norm": 0.5176185369491577,
      "learning_rate": 4.302777777777778e-05,
      "loss": 0.0025,
      "step": 12550
    },
    {
      "epoch": 1.1164444444444444,
      "grad_norm": 0.8675685524940491,
      "learning_rate": 4.302222222222223e-05,
      "loss": 0.0022,
      "step": 12560
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 1.1897873878479004,
      "learning_rate": 4.3016666666666664e-05,
      "loss": 0.0039,
      "step": 12570
    },
    {
      "epoch": 1.1182222222222222,
      "grad_norm": 0.9463685750961304,
      "learning_rate": 4.3011111111111115e-05,
      "loss": 0.003,
      "step": 12580
    },
    {
      "epoch": 1.1191111111111112,
      "grad_norm": 1.193433403968811,
      "learning_rate": 4.300555555555556e-05,
      "loss": 0.0029,
      "step": 12590
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.7149993181228638,
      "learning_rate": 4.3e-05,
      "loss": 0.0031,
      "step": 12600
    },
    {
      "epoch": 1.1208888888888888,
      "grad_norm": 0.12015598267316818,
      "learning_rate": 4.2994444444444445e-05,
      "loss": 0.0034,
      "step": 12610
    },
    {
      "epoch": 1.1217777777777778,
      "grad_norm": 0.22984108328819275,
      "learning_rate": 4.298888888888889e-05,
      "loss": 0.0029,
      "step": 12620
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.13997653126716614,
      "learning_rate": 4.298333333333334e-05,
      "loss": 0.0024,
      "step": 12630
    },
    {
      "epoch": 1.1235555555555556,
      "grad_norm": 0.8749540448188782,
      "learning_rate": 4.2977777777777776e-05,
      "loss": 0.0043,
      "step": 12640
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 0.9948107600212097,
      "learning_rate": 4.2972222222222226e-05,
      "loss": 0.0021,
      "step": 12650
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.5880048274993896,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0031,
      "step": 12660
    },
    {
      "epoch": 1.1262222222222222,
      "grad_norm": 0.767368733882904,
      "learning_rate": 4.296111111111111e-05,
      "loss": 0.0044,
      "step": 12670
    },
    {
      "epoch": 1.1271111111111112,
      "grad_norm": 0.6027178168296814,
      "learning_rate": 4.295555555555556e-05,
      "loss": 0.0028,
      "step": 12680
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.5943073034286499,
      "learning_rate": 4.295e-05,
      "loss": 0.0038,
      "step": 12690
    },
    {
      "epoch": 1.1288888888888888,
      "grad_norm": 0.2190912961959839,
      "learning_rate": 4.294444444444445e-05,
      "loss": 0.0033,
      "step": 12700
    },
    {
      "epoch": 1.1297777777777778,
      "grad_norm": 0.9992035627365112,
      "learning_rate": 4.293888888888889e-05,
      "loss": 0.0032,
      "step": 12710
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.4715268909931183,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0026,
      "step": 12720
    },
    {
      "epoch": 1.1315555555555556,
      "grad_norm": 1.4272034168243408,
      "learning_rate": 4.2927777777777775e-05,
      "loss": 0.0029,
      "step": 12730
    },
    {
      "epoch": 1.1324444444444444,
      "grad_norm": 0.9488174319267273,
      "learning_rate": 4.2922222222222225e-05,
      "loss": 0.0032,
      "step": 12740
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 1.5181665420532227,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.0035,
      "step": 12750
    },
    {
      "epoch": 1.1342222222222222,
      "grad_norm": 0.40619751811027527,
      "learning_rate": 4.291111111111111e-05,
      "loss": 0.0035,
      "step": 12760
    },
    {
      "epoch": 1.1351111111111112,
      "grad_norm": 0.9302263855934143,
      "learning_rate": 4.290555555555556e-05,
      "loss": 0.004,
      "step": 12770
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.9976890087127686,
      "learning_rate": 4.29e-05,
      "loss": 0.003,
      "step": 12780
    },
    {
      "epoch": 1.1368888888888888,
      "grad_norm": 0.1568184643983841,
      "learning_rate": 4.289444444444445e-05,
      "loss": 0.0031,
      "step": 12790
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 0.6805627942085266,
      "learning_rate": 4.2888888888888886e-05,
      "loss": 0.0033,
      "step": 12800
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.1088804379105568,
      "learning_rate": 4.288333333333334e-05,
      "loss": 0.0031,
      "step": 12810
    },
    {
      "epoch": 1.1395555555555554,
      "grad_norm": 0.4828929603099823,
      "learning_rate": 4.287777777777778e-05,
      "loss": 0.0032,
      "step": 12820
    },
    {
      "epoch": 1.1404444444444444,
      "grad_norm": 0.5397811532020569,
      "learning_rate": 4.2872222222222224e-05,
      "loss": 0.0033,
      "step": 12830
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.5017498135566711,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0038,
      "step": 12840
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 0.5978725552558899,
      "learning_rate": 4.286111111111111e-05,
      "loss": 0.0032,
      "step": 12850
    },
    {
      "epoch": 1.1431111111111112,
      "grad_norm": 0.815567135810852,
      "learning_rate": 4.285555555555556e-05,
      "loss": 0.0037,
      "step": 12860
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.382662296295166,
      "learning_rate": 4.285e-05,
      "loss": 0.0029,
      "step": 12870
    },
    {
      "epoch": 1.1448888888888888,
      "grad_norm": 0.15739427506923676,
      "learning_rate": 4.284444444444445e-05,
      "loss": 0.0034,
      "step": 12880
    },
    {
      "epoch": 1.1457777777777778,
      "grad_norm": 0.2516721785068512,
      "learning_rate": 4.283888888888889e-05,
      "loss": 0.0031,
      "step": 12890
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 1.0348732471466064,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0032,
      "step": 12900
    },
    {
      "epoch": 1.1475555555555554,
      "grad_norm": 0.1302192211151123,
      "learning_rate": 4.282777777777778e-05,
      "loss": 0.0037,
      "step": 12910
    },
    {
      "epoch": 1.1484444444444444,
      "grad_norm": 0.5526540279388428,
      "learning_rate": 4.282222222222222e-05,
      "loss": 0.0021,
      "step": 12920
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 1.353103518486023,
      "learning_rate": 4.2816666666666666e-05,
      "loss": 0.0038,
      "step": 12930
    },
    {
      "epoch": 1.1502222222222223,
      "grad_norm": 0.2469734251499176,
      "learning_rate": 4.281111111111111e-05,
      "loss": 0.0023,
      "step": 12940
    },
    {
      "epoch": 1.1511111111111112,
      "grad_norm": 0.73768150806427,
      "learning_rate": 4.280555555555556e-05,
      "loss": 0.0032,
      "step": 12950
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.13903290033340454,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0035,
      "step": 12960
    },
    {
      "epoch": 1.1528888888888889,
      "grad_norm": 0.5340544581413269,
      "learning_rate": 4.279444444444445e-05,
      "loss": 0.0034,
      "step": 12970
    },
    {
      "epoch": 1.1537777777777778,
      "grad_norm": 0.22478148341178894,
      "learning_rate": 4.278888888888889e-05,
      "loss": 0.003,
      "step": 12980
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.6376462578773499,
      "learning_rate": 4.2783333333333334e-05,
      "loss": 0.0029,
      "step": 12990
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.6922375559806824,
      "learning_rate": 4.277777777777778e-05,
      "loss": 0.0021,
      "step": 13000
    },
    {
      "epoch": 1.1564444444444444,
      "grad_norm": 1.2347804307937622,
      "learning_rate": 4.277222222222222e-05,
      "loss": 0.002,
      "step": 13010
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.7063191533088684,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0044,
      "step": 13020
    },
    {
      "epoch": 1.1582222222222223,
      "grad_norm": 1.4424829483032227,
      "learning_rate": 4.2761111111111115e-05,
      "loss": 0.0041,
      "step": 13030
    },
    {
      "epoch": 1.1591111111111112,
      "grad_norm": 0.35150089859962463,
      "learning_rate": 4.275555555555556e-05,
      "loss": 0.0029,
      "step": 13040
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.9247695803642273,
      "learning_rate": 4.275e-05,
      "loss": 0.0031,
      "step": 13050
    },
    {
      "epoch": 1.1608888888888889,
      "grad_norm": 0.7296614050865173,
      "learning_rate": 4.2744444444444446e-05,
      "loss": 0.003,
      "step": 13060
    },
    {
      "epoch": 1.1617777777777778,
      "grad_norm": 1.0730726718902588,
      "learning_rate": 4.273888888888889e-05,
      "loss": 0.0037,
      "step": 13070
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.44823816418647766,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0027,
      "step": 13080
    },
    {
      "epoch": 1.1635555555555555,
      "grad_norm": 0.12634991109371185,
      "learning_rate": 4.2727777777777777e-05,
      "loss": 0.0035,
      "step": 13090
    },
    {
      "epoch": 1.1644444444444444,
      "grad_norm": 0.44526052474975586,
      "learning_rate": 4.272222222222223e-05,
      "loss": 0.0023,
      "step": 13100
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.10287734866142273,
      "learning_rate": 4.2716666666666664e-05,
      "loss": 0.0026,
      "step": 13110
    },
    {
      "epoch": 1.1662222222222223,
      "grad_norm": 0.40555447340011597,
      "learning_rate": 4.2711111111111114e-05,
      "loss": 0.0027,
      "step": 13120
    },
    {
      "epoch": 1.1671111111111112,
      "grad_norm": 0.38453900814056396,
      "learning_rate": 4.270555555555556e-05,
      "loss": 0.003,
      "step": 13130
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.42365241050720215,
      "learning_rate": 4.27e-05,
      "loss": 0.0037,
      "step": 13140
    },
    {
      "epoch": 1.1688888888888889,
      "grad_norm": 0.25664547085762024,
      "learning_rate": 4.2694444444444445e-05,
      "loss": 0.0035,
      "step": 13150
    },
    {
      "epoch": 1.1697777777777778,
      "grad_norm": 0.5354923009872437,
      "learning_rate": 4.268888888888889e-05,
      "loss": 0.0049,
      "step": 13160
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.23273107409477234,
      "learning_rate": 4.268333333333334e-05,
      "loss": 0.0032,
      "step": 13170
    },
    {
      "epoch": 1.1715555555555555,
      "grad_norm": 0.5556158423423767,
      "learning_rate": 4.2677777777777775e-05,
      "loss": 0.004,
      "step": 13180
    },
    {
      "epoch": 1.1724444444444444,
      "grad_norm": 0.12343408167362213,
      "learning_rate": 4.2672222222222226e-05,
      "loss": 0.0028,
      "step": 13190
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.5154846906661987,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0027,
      "step": 13200
    },
    {
      "epoch": 1.1742222222222223,
      "grad_norm": 0.48013967275619507,
      "learning_rate": 4.266111111111111e-05,
      "loss": 0.0024,
      "step": 13210
    },
    {
      "epoch": 1.1751111111111112,
      "grad_norm": 0.7918040156364441,
      "learning_rate": 4.2655555555555556e-05,
      "loss": 0.0025,
      "step": 13220
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.6784937381744385,
      "learning_rate": 4.265e-05,
      "loss": 0.0028,
      "step": 13230
    },
    {
      "epoch": 1.1768888888888889,
      "grad_norm": 0.927340567111969,
      "learning_rate": 4.264444444444445e-05,
      "loss": 0.0032,
      "step": 13240
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.23239125311374664,
      "learning_rate": 4.263888888888889e-05,
      "loss": 0.0026,
      "step": 13250
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.41196128726005554,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0037,
      "step": 13260
    },
    {
      "epoch": 1.1795555555555555,
      "grad_norm": 0.5858358144760132,
      "learning_rate": 4.262777777777778e-05,
      "loss": 0.0033,
      "step": 13270
    },
    {
      "epoch": 1.1804444444444444,
      "grad_norm": 0.4404771327972412,
      "learning_rate": 4.2622222222222224e-05,
      "loss": 0.0036,
      "step": 13280
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.05590236559510231,
      "learning_rate": 4.261666666666667e-05,
      "loss": 0.0026,
      "step": 13290
    },
    {
      "epoch": 1.1822222222222223,
      "grad_norm": 0.2352527230978012,
      "learning_rate": 4.261111111111111e-05,
      "loss": 0.0031,
      "step": 13300
    },
    {
      "epoch": 1.1831111111111112,
      "grad_norm": 0.6915019750595093,
      "learning_rate": 4.260555555555556e-05,
      "loss": 0.0032,
      "step": 13310
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.33659031987190247,
      "learning_rate": 4.26e-05,
      "loss": 0.003,
      "step": 13320
    },
    {
      "epoch": 1.1848888888888889,
      "grad_norm": 0.5092657804489136,
      "learning_rate": 4.259444444444445e-05,
      "loss": 0.003,
      "step": 13330
    },
    {
      "epoch": 1.1857777777777778,
      "grad_norm": 0.8139342069625854,
      "learning_rate": 4.258888888888889e-05,
      "loss": 0.0044,
      "step": 13340
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.5902674794197083,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.0035,
      "step": 13350
    },
    {
      "epoch": 1.1875555555555555,
      "grad_norm": 0.5568258166313171,
      "learning_rate": 4.257777777777778e-05,
      "loss": 0.0042,
      "step": 13360
    },
    {
      "epoch": 1.1884444444444444,
      "grad_norm": 0.1656116545200348,
      "learning_rate": 4.257222222222222e-05,
      "loss": 0.0031,
      "step": 13370
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 1.2066633701324463,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0042,
      "step": 13380
    },
    {
      "epoch": 1.1902222222222223,
      "grad_norm": 0.34960103034973145,
      "learning_rate": 4.256111111111111e-05,
      "loss": 0.004,
      "step": 13390
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 0.26376235485076904,
      "learning_rate": 4.255555555555556e-05,
      "loss": 0.0023,
      "step": 13400
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.18026615679264069,
      "learning_rate": 4.2550000000000004e-05,
      "loss": 0.0038,
      "step": 13410
    },
    {
      "epoch": 1.1928888888888889,
      "grad_norm": 0.4579484760761261,
      "learning_rate": 4.254444444444445e-05,
      "loss": 0.0021,
      "step": 13420
    },
    {
      "epoch": 1.1937777777777778,
      "grad_norm": 0.2445966601371765,
      "learning_rate": 4.253888888888889e-05,
      "loss": 0.0033,
      "step": 13430
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.548151433467865,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0016,
      "step": 13440
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 0.12703493237495422,
      "learning_rate": 4.252777777777778e-05,
      "loss": 0.0042,
      "step": 13450
    },
    {
      "epoch": 1.1964444444444444,
      "grad_norm": 0.9031703472137451,
      "learning_rate": 4.252222222222222e-05,
      "loss": 0.0036,
      "step": 13460
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.3848412036895752,
      "learning_rate": 4.251666666666667e-05,
      "loss": 0.0029,
      "step": 13470
    },
    {
      "epoch": 1.1982222222222223,
      "grad_norm": 0.7649243474006653,
      "learning_rate": 4.2511111111111116e-05,
      "loss": 0.0019,
      "step": 13480
    },
    {
      "epoch": 1.199111111111111,
      "grad_norm": 0.2595217227935791,
      "learning_rate": 4.250555555555556e-05,
      "loss": 0.0025,
      "step": 13490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.7927346229553223,
      "learning_rate": 4.25e-05,
      "loss": 0.0025,
      "step": 13500
    },
    {
      "epoch": 1.200888888888889,
      "grad_norm": 0.910490870475769,
      "learning_rate": 4.2494444444444447e-05,
      "loss": 0.0035,
      "step": 13510
    },
    {
      "epoch": 1.2017777777777778,
      "grad_norm": 0.8183749318122864,
      "learning_rate": 4.248888888888889e-05,
      "loss": 0.0029,
      "step": 13520
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.7107487916946411,
      "learning_rate": 4.2483333333333334e-05,
      "loss": 0.0022,
      "step": 13530
    },
    {
      "epoch": 1.2035555555555555,
      "grad_norm": 0.5411802530288696,
      "learning_rate": 4.247777777777778e-05,
      "loss": 0.0027,
      "step": 13540
    },
    {
      "epoch": 1.2044444444444444,
      "grad_norm": 0.4746144115924835,
      "learning_rate": 4.247222222222223e-05,
      "loss": 0.0033,
      "step": 13550
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.1710597723722458,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0034,
      "step": 13560
    },
    {
      "epoch": 1.2062222222222223,
      "grad_norm": 0.3906981647014618,
      "learning_rate": 4.2461111111111115e-05,
      "loss": 0.0036,
      "step": 13570
    },
    {
      "epoch": 1.207111111111111,
      "grad_norm": 0.5355432629585266,
      "learning_rate": 4.245555555555556e-05,
      "loss": 0.004,
      "step": 13580
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.2288558930158615,
      "learning_rate": 4.245e-05,
      "loss": 0.0029,
      "step": 13590
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 0.46535512804985046,
      "learning_rate": 4.2444444444444445e-05,
      "loss": 0.0037,
      "step": 13600
    },
    {
      "epoch": 1.2097777777777778,
      "grad_norm": 0.1902378499507904,
      "learning_rate": 4.243888888888889e-05,
      "loss": 0.0023,
      "step": 13610
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.6866748332977295,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.0025,
      "step": 13620
    },
    {
      "epoch": 1.2115555555555555,
      "grad_norm": 1.2106082439422607,
      "learning_rate": 4.2427777777777776e-05,
      "loss": 0.0026,
      "step": 13630
    },
    {
      "epoch": 1.2124444444444444,
      "grad_norm": 0.4421844482421875,
      "learning_rate": 4.2422222222222226e-05,
      "loss": 0.003,
      "step": 13640
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.13356733322143555,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.0017,
      "step": 13650
    },
    {
      "epoch": 1.2142222222222223,
      "grad_norm": 1.029200553894043,
      "learning_rate": 4.2411111111111114e-05,
      "loss": 0.0026,
      "step": 13660
    },
    {
      "epoch": 1.215111111111111,
      "grad_norm": 0.5766894221305847,
      "learning_rate": 4.240555555555556e-05,
      "loss": 0.0022,
      "step": 13670
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.45024576783180237,
      "learning_rate": 4.24e-05,
      "loss": 0.0027,
      "step": 13680
    },
    {
      "epoch": 1.216888888888889,
      "grad_norm": 1.066565990447998,
      "learning_rate": 4.239444444444445e-05,
      "loss": 0.0028,
      "step": 13690
    },
    {
      "epoch": 1.2177777777777778,
      "grad_norm": 0.3721521198749542,
      "learning_rate": 4.238888888888889e-05,
      "loss": 0.003,
      "step": 13700
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.9296013712882996,
      "learning_rate": 4.238333333333334e-05,
      "loss": 0.0026,
      "step": 13710
    },
    {
      "epoch": 1.2195555555555555,
      "grad_norm": 1.0710686445236206,
      "learning_rate": 4.2377777777777775e-05,
      "loss": 0.0033,
      "step": 13720
    },
    {
      "epoch": 1.2204444444444444,
      "grad_norm": 0.6377845406532288,
      "learning_rate": 4.2372222222222225e-05,
      "loss": 0.002,
      "step": 13730
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.7354795932769775,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0041,
      "step": 13740
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.6576043367385864,
      "learning_rate": 4.236111111111111e-05,
      "loss": 0.0036,
      "step": 13750
    },
    {
      "epoch": 1.223111111111111,
      "grad_norm": 0.681564450263977,
      "learning_rate": 4.235555555555556e-05,
      "loss": 0.0022,
      "step": 13760
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.8362131714820862,
      "learning_rate": 4.235e-05,
      "loss": 0.0029,
      "step": 13770
    },
    {
      "epoch": 1.224888888888889,
      "grad_norm": 1.2086516618728638,
      "learning_rate": 4.234444444444445e-05,
      "loss": 0.0023,
      "step": 13780
    },
    {
      "epoch": 1.2257777777777779,
      "grad_norm": 1.278354287147522,
      "learning_rate": 4.2338888888888887e-05,
      "loss": 0.0027,
      "step": 13790
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.48089590668678284,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0034,
      "step": 13800
    },
    {
      "epoch": 1.2275555555555555,
      "grad_norm": 0.8381449580192566,
      "learning_rate": 4.232777777777778e-05,
      "loss": 0.0033,
      "step": 13810
    },
    {
      "epoch": 1.2284444444444444,
      "grad_norm": 1.2732638120651245,
      "learning_rate": 4.2322222222222224e-05,
      "loss": 0.003,
      "step": 13820
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.22496278584003448,
      "learning_rate": 4.2316666666666674e-05,
      "loss": 0.0047,
      "step": 13830
    },
    {
      "epoch": 1.2302222222222223,
      "grad_norm": 1.1543172597885132,
      "learning_rate": 4.231111111111111e-05,
      "loss": 0.0034,
      "step": 13840
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 0.8435712456703186,
      "learning_rate": 4.230555555555556e-05,
      "loss": 0.0037,
      "step": 13850
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.8416734337806702,
      "learning_rate": 4.23e-05,
      "loss": 0.0037,
      "step": 13860
    },
    {
      "epoch": 1.232888888888889,
      "grad_norm": 0.0878017321228981,
      "learning_rate": 4.229444444444445e-05,
      "loss": 0.0035,
      "step": 13870
    },
    {
      "epoch": 1.2337777777777779,
      "grad_norm": 0.21857844293117523,
      "learning_rate": 4.228888888888889e-05,
      "loss": 0.0048,
      "step": 13880
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.44523850083351135,
      "learning_rate": 4.2283333333333336e-05,
      "loss": 0.0039,
      "step": 13890
    },
    {
      "epoch": 1.2355555555555555,
      "grad_norm": 0.7247166037559509,
      "learning_rate": 4.227777777777778e-05,
      "loss": 0.0032,
      "step": 13900
    },
    {
      "epoch": 1.2364444444444445,
      "grad_norm": 0.5358514785766602,
      "learning_rate": 4.227222222222222e-05,
      "loss": 0.0032,
      "step": 13910
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.9077351093292236,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0023,
      "step": 13920
    },
    {
      "epoch": 1.2382222222222223,
      "grad_norm": 0.849093496799469,
      "learning_rate": 4.226111111111111e-05,
      "loss": 0.0039,
      "step": 13930
    },
    {
      "epoch": 1.239111111111111,
      "grad_norm": 0.5520426034927368,
      "learning_rate": 4.225555555555556e-05,
      "loss": 0.0028,
      "step": 13940
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.41406261920928955,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.0035,
      "step": 13950
    },
    {
      "epoch": 1.240888888888889,
      "grad_norm": 1.2385510206222534,
      "learning_rate": 4.224444444444445e-05,
      "loss": 0.0032,
      "step": 13960
    },
    {
      "epoch": 1.2417777777777779,
      "grad_norm": 0.7309730052947998,
      "learning_rate": 4.223888888888889e-05,
      "loss": 0.0038,
      "step": 13970
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.38253748416900635,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.003,
      "step": 13980
    },
    {
      "epoch": 1.2435555555555555,
      "grad_norm": 0.6598687767982483,
      "learning_rate": 4.222777777777778e-05,
      "loss": 0.0038,
      "step": 13990
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.811789333820343,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.0039,
      "step": 14000
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 1.1504888534545898,
      "learning_rate": 4.221666666666667e-05,
      "loss": 0.0019,
      "step": 14010
    },
    {
      "epoch": 1.2462222222222223,
      "grad_norm": 0.9951453804969788,
      "learning_rate": 4.2211111111111115e-05,
      "loss": 0.0029,
      "step": 14020
    },
    {
      "epoch": 1.247111111111111,
      "grad_norm": 1.1168285608291626,
      "learning_rate": 4.220555555555556e-05,
      "loss": 0.0025,
      "step": 14030
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.3289337754249573,
      "learning_rate": 4.22e-05,
      "loss": 0.0032,
      "step": 14040
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 0.30146512389183044,
      "learning_rate": 4.2194444444444446e-05,
      "loss": 0.0037,
      "step": 14050
    },
    {
      "epoch": 1.2497777777777777,
      "grad_norm": 0.1388239711523056,
      "learning_rate": 4.218888888888889e-05,
      "loss": 0.0025,
      "step": 14060
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.3558855354785919,
      "learning_rate": 4.218333333333333e-05,
      "loss": 0.0023,
      "step": 14070
    },
    {
      "epoch": 1.2515555555555555,
      "grad_norm": 0.2520849108695984,
      "learning_rate": 4.217777777777778e-05,
      "loss": 0.0031,
      "step": 14080
    },
    {
      "epoch": 1.2524444444444445,
      "grad_norm": 0.5533458590507507,
      "learning_rate": 4.217222222222223e-05,
      "loss": 0.0036,
      "step": 14090
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.4793054759502411,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0032,
      "step": 14100
    },
    {
      "epoch": 1.2542222222222223,
      "grad_norm": 0.8595343828201294,
      "learning_rate": 4.2161111111111114e-05,
      "loss": 0.0023,
      "step": 14110
    },
    {
      "epoch": 1.255111111111111,
      "grad_norm": 0.06973252445459366,
      "learning_rate": 4.215555555555556e-05,
      "loss": 0.0032,
      "step": 14120
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.3397578001022339,
      "learning_rate": 4.215e-05,
      "loss": 0.0033,
      "step": 14130
    },
    {
      "epoch": 1.256888888888889,
      "grad_norm": 0.10910919308662415,
      "learning_rate": 4.2144444444444445e-05,
      "loss": 0.0028,
      "step": 14140
    },
    {
      "epoch": 1.2577777777777777,
      "grad_norm": 0.6490293145179749,
      "learning_rate": 4.213888888888889e-05,
      "loss": 0.0024,
      "step": 14150
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.6179269552230835,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0036,
      "step": 14160
    },
    {
      "epoch": 1.2595555555555555,
      "grad_norm": 0.1453123688697815,
      "learning_rate": 4.2127777777777776e-05,
      "loss": 0.0022,
      "step": 14170
    },
    {
      "epoch": 1.2604444444444445,
      "grad_norm": 0.6166296601295471,
      "learning_rate": 4.2122222222222226e-05,
      "loss": 0.0016,
      "step": 14180
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.376880943775177,
      "learning_rate": 4.211666666666667e-05,
      "loss": 0.003,
      "step": 14190
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 0.3490472137928009,
      "learning_rate": 4.211111111111111e-05,
      "loss": 0.0037,
      "step": 14200
    },
    {
      "epoch": 1.263111111111111,
      "grad_norm": 0.8027034401893616,
      "learning_rate": 4.2105555555555557e-05,
      "loss": 0.0032,
      "step": 14210
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.7619308829307556,
      "learning_rate": 4.21e-05,
      "loss": 0.0036,
      "step": 14220
    },
    {
      "epoch": 1.264888888888889,
      "grad_norm": 1.255147933959961,
      "learning_rate": 4.209444444444445e-05,
      "loss": 0.0025,
      "step": 14230
    },
    {
      "epoch": 1.2657777777777777,
      "grad_norm": 1.0722726583480835,
      "learning_rate": 4.208888888888889e-05,
      "loss": 0.0036,
      "step": 14240
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.4940342605113983,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.0031,
      "step": 14250
    },
    {
      "epoch": 1.2675555555555555,
      "grad_norm": 1.2178030014038086,
      "learning_rate": 4.2077777777777774e-05,
      "loss": 0.0031,
      "step": 14260
    },
    {
      "epoch": 1.2684444444444445,
      "grad_norm": 1.4649399518966675,
      "learning_rate": 4.2072222222222225e-05,
      "loss": 0.0028,
      "step": 14270
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.8737053275108337,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0026,
      "step": 14280
    },
    {
      "epoch": 1.2702222222222221,
      "grad_norm": 0.9282419681549072,
      "learning_rate": 4.206111111111111e-05,
      "loss": 0.0027,
      "step": 14290
    },
    {
      "epoch": 1.271111111111111,
      "grad_norm": 0.3466605246067047,
      "learning_rate": 4.205555555555556e-05,
      "loss": 0.0024,
      "step": 14300
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.33854371309280396,
      "learning_rate": 4.205e-05,
      "loss": 0.0032,
      "step": 14310
    },
    {
      "epoch": 1.272888888888889,
      "grad_norm": 0.2791893780231476,
      "learning_rate": 4.204444444444445e-05,
      "loss": 0.0035,
      "step": 14320
    },
    {
      "epoch": 1.2737777777777777,
      "grad_norm": 0.6946877837181091,
      "learning_rate": 4.2038888888888886e-05,
      "loss": 0.0028,
      "step": 14330
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.5259442329406738,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0035,
      "step": 14340
    },
    {
      "epoch": 1.2755555555555556,
      "grad_norm": 1.049745798110962,
      "learning_rate": 4.202777777777778e-05,
      "loss": 0.0036,
      "step": 14350
    },
    {
      "epoch": 1.2764444444444445,
      "grad_norm": 0.23218461871147156,
      "learning_rate": 4.2022222222222223e-05,
      "loss": 0.0035,
      "step": 14360
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.27500057220458984,
      "learning_rate": 4.2016666666666674e-05,
      "loss": 0.0035,
      "step": 14370
    },
    {
      "epoch": 1.2782222222222221,
      "grad_norm": 0.4150150716304779,
      "learning_rate": 4.201111111111111e-05,
      "loss": 0.0036,
      "step": 14380
    },
    {
      "epoch": 1.279111111111111,
      "grad_norm": 0.12543056905269623,
      "learning_rate": 4.200555555555556e-05,
      "loss": 0.0029,
      "step": 14390
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.2097570300102234,
      "learning_rate": 4.2e-05,
      "loss": 0.0036,
      "step": 14400
    },
    {
      "epoch": 1.280888888888889,
      "grad_norm": 0.8290567994117737,
      "learning_rate": 4.199444444444445e-05,
      "loss": 0.0021,
      "step": 14410
    },
    {
      "epoch": 1.2817777777777777,
      "grad_norm": 1.1031149625778198,
      "learning_rate": 4.198888888888889e-05,
      "loss": 0.0034,
      "step": 14420
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.33340761065483093,
      "learning_rate": 4.1983333333333335e-05,
      "loss": 0.0026,
      "step": 14430
    },
    {
      "epoch": 1.2835555555555556,
      "grad_norm": 0.7008141875267029,
      "learning_rate": 4.1977777777777785e-05,
      "loss": 0.0034,
      "step": 14440
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 0.719396710395813,
      "learning_rate": 4.197222222222222e-05,
      "loss": 0.0047,
      "step": 14450
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.190259650349617,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0028,
      "step": 14460
    },
    {
      "epoch": 1.2862222222222222,
      "grad_norm": 1.226439118385315,
      "learning_rate": 4.196111111111111e-05,
      "loss": 0.0021,
      "step": 14470
    },
    {
      "epoch": 1.287111111111111,
      "grad_norm": 0.9599602222442627,
      "learning_rate": 4.195555555555556e-05,
      "loss": 0.0026,
      "step": 14480
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.9737080335617065,
      "learning_rate": 4.195e-05,
      "loss": 0.004,
      "step": 14490
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.5582150816917419,
      "learning_rate": 4.194444444444445e-05,
      "loss": 0.003,
      "step": 14500
    },
    {
      "epoch": 1.2897777777777777,
      "grad_norm": 0.3499249517917633,
      "learning_rate": 4.193888888888889e-05,
      "loss": 0.0026,
      "step": 14510
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.14053897559642792,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0032,
      "step": 14520
    },
    {
      "epoch": 1.2915555555555556,
      "grad_norm": 0.3248876631259918,
      "learning_rate": 4.1927777777777784e-05,
      "loss": 0.003,
      "step": 14530
    },
    {
      "epoch": 1.2924444444444445,
      "grad_norm": 0.6791307330131531,
      "learning_rate": 4.192222222222222e-05,
      "loss": 0.0021,
      "step": 14540
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.1966225802898407,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.0038,
      "step": 14550
    },
    {
      "epoch": 1.2942222222222222,
      "grad_norm": 0.18400953710079193,
      "learning_rate": 4.1911111111111115e-05,
      "loss": 0.0029,
      "step": 14560
    },
    {
      "epoch": 1.295111111111111,
      "grad_norm": 0.6722518801689148,
      "learning_rate": 4.190555555555556e-05,
      "loss": 0.0034,
      "step": 14570
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.0101253986358643,
      "learning_rate": 4.19e-05,
      "loss": 0.0031,
      "step": 14580
    },
    {
      "epoch": 1.2968888888888888,
      "grad_norm": 0.24030299484729767,
      "learning_rate": 4.1894444444444446e-05,
      "loss": 0.003,
      "step": 14590
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 0.9424382448196411,
      "learning_rate": 4.188888888888889e-05,
      "loss": 0.0028,
      "step": 14600
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.31843045353889465,
      "learning_rate": 4.188333333333333e-05,
      "loss": 0.0037,
      "step": 14610
    },
    {
      "epoch": 1.2995555555555556,
      "grad_norm": 0.38718554377555847,
      "learning_rate": 4.187777777777778e-05,
      "loss": 0.0035,
      "step": 14620
    },
    {
      "epoch": 1.3004444444444445,
      "grad_norm": 1.0168222188949585,
      "learning_rate": 4.1872222222222227e-05,
      "loss": 0.0033,
      "step": 14630
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 1.0218664407730103,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0025,
      "step": 14640
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 0.42324450612068176,
      "learning_rate": 4.1861111111111114e-05,
      "loss": 0.0023,
      "step": 14650
    },
    {
      "epoch": 1.303111111111111,
      "grad_norm": 0.08353443443775177,
      "learning_rate": 4.185555555555556e-05,
      "loss": 0.0033,
      "step": 14660
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.7847203016281128,
      "learning_rate": 4.185e-05,
      "loss": 0.0034,
      "step": 14670
    },
    {
      "epoch": 1.3048888888888888,
      "grad_norm": 0.06140441074967384,
      "learning_rate": 4.1844444444444444e-05,
      "loss": 0.0049,
      "step": 14680
    },
    {
      "epoch": 1.3057777777777777,
      "grad_norm": 0.3244272768497467,
      "learning_rate": 4.183888888888889e-05,
      "loss": 0.0022,
      "step": 14690
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.34684324264526367,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0018,
      "step": 14700
    },
    {
      "epoch": 1.3075555555555556,
      "grad_norm": 0.5442863702774048,
      "learning_rate": 4.182777777777778e-05,
      "loss": 0.0024,
      "step": 14710
    },
    {
      "epoch": 1.3084444444444445,
      "grad_norm": 0.7526695728302002,
      "learning_rate": 4.1822222222222225e-05,
      "loss": 0.0034,
      "step": 14720
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.08688902109861374,
      "learning_rate": 4.181666666666667e-05,
      "loss": 0.004,
      "step": 14730
    },
    {
      "epoch": 1.3102222222222222,
      "grad_norm": 0.9608588218688965,
      "learning_rate": 4.181111111111111e-05,
      "loss": 0.004,
      "step": 14740
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 1.0886280536651611,
      "learning_rate": 4.1805555555555556e-05,
      "loss": 0.004,
      "step": 14750
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.7844555377960205,
      "learning_rate": 4.18e-05,
      "loss": 0.0032,
      "step": 14760
    },
    {
      "epoch": 1.3128888888888888,
      "grad_norm": 0.36588239669799805,
      "learning_rate": 4.179444444444445e-05,
      "loss": 0.0029,
      "step": 14770
    },
    {
      "epoch": 1.3137777777777777,
      "grad_norm": 0.5712001323699951,
      "learning_rate": 4.178888888888889e-05,
      "loss": 0.0023,
      "step": 14780
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.40016934275627136,
      "learning_rate": 4.178333333333334e-05,
      "loss": 0.0041,
      "step": 14790
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 0.4859746992588043,
      "learning_rate": 4.177777777777778e-05,
      "loss": 0.0034,
      "step": 14800
    },
    {
      "epoch": 1.3164444444444445,
      "grad_norm": 0.8513741493225098,
      "learning_rate": 4.1772222222222224e-05,
      "loss": 0.0042,
      "step": 14810
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.4684907793998718,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0023,
      "step": 14820
    },
    {
      "epoch": 1.3182222222222222,
      "grad_norm": 0.30366000533103943,
      "learning_rate": 4.176111111111111e-05,
      "loss": 0.002,
      "step": 14830
    },
    {
      "epoch": 1.3191111111111111,
      "grad_norm": 0.4649546444416046,
      "learning_rate": 4.175555555555556e-05,
      "loss": 0.0034,
      "step": 14840
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.23954622447490692,
      "learning_rate": 4.175e-05,
      "loss": 0.0033,
      "step": 14850
    },
    {
      "epoch": 1.3208888888888888,
      "grad_norm": 0.8944703340530396,
      "learning_rate": 4.174444444444445e-05,
      "loss": 0.002,
      "step": 14860
    },
    {
      "epoch": 1.3217777777777777,
      "grad_norm": 0.4197326898574829,
      "learning_rate": 4.1738888888888885e-05,
      "loss": 0.003,
      "step": 14870
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.48877716064453125,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0037,
      "step": 14880
    },
    {
      "epoch": 1.3235555555555556,
      "grad_norm": 0.6383486986160278,
      "learning_rate": 4.172777777777778e-05,
      "loss": 0.0035,
      "step": 14890
    },
    {
      "epoch": 1.3244444444444445,
      "grad_norm": 0.29858145117759705,
      "learning_rate": 4.172222222222222e-05,
      "loss": 0.0032,
      "step": 14900
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 1.4290564060211182,
      "learning_rate": 4.171666666666667e-05,
      "loss": 0.003,
      "step": 14910
    },
    {
      "epoch": 1.3262222222222222,
      "grad_norm": 0.25361886620521545,
      "learning_rate": 4.171111111111111e-05,
      "loss": 0.0025,
      "step": 14920
    },
    {
      "epoch": 1.3271111111111111,
      "grad_norm": 0.9957555532455444,
      "learning_rate": 4.170555555555556e-05,
      "loss": 0.0036,
      "step": 14930
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.06491602212190628,
      "learning_rate": 4.17e-05,
      "loss": 0.003,
      "step": 14940
    },
    {
      "epoch": 1.3288888888888888,
      "grad_norm": 0.7178038954734802,
      "learning_rate": 4.169444444444445e-05,
      "loss": 0.0028,
      "step": 14950
    },
    {
      "epoch": 1.3297777777777777,
      "grad_norm": 0.7130320072174072,
      "learning_rate": 4.168888888888889e-05,
      "loss": 0.0025,
      "step": 14960
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.622250497341156,
      "learning_rate": 4.1683333333333335e-05,
      "loss": 0.0037,
      "step": 14970
    },
    {
      "epoch": 1.3315555555555556,
      "grad_norm": 0.359631210565567,
      "learning_rate": 4.1677777777777785e-05,
      "loss": 0.0027,
      "step": 14980
    },
    {
      "epoch": 1.3324444444444445,
      "grad_norm": 0.15400195121765137,
      "learning_rate": 4.167222222222222e-05,
      "loss": 0.0022,
      "step": 14990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.2069793939590454,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0028,
      "step": 15000
    },
    {
      "epoch": 1.3342222222222222,
      "grad_norm": 0.4732433259487152,
      "learning_rate": 4.166111111111111e-05,
      "loss": 0.0025,
      "step": 15010
    },
    {
      "epoch": 1.3351111111111111,
      "grad_norm": 0.09083531051874161,
      "learning_rate": 4.165555555555556e-05,
      "loss": 0.0033,
      "step": 15020
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.8441754579544067,
      "learning_rate": 4.165e-05,
      "loss": 0.0027,
      "step": 15030
    },
    {
      "epoch": 1.3368888888888888,
      "grad_norm": 0.7058796286582947,
      "learning_rate": 4.1644444444444446e-05,
      "loss": 0.0042,
      "step": 15040
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 0.3244491219520569,
      "learning_rate": 4.163888888888889e-05,
      "loss": 0.0028,
      "step": 15050
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.400099515914917,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.004,
      "step": 15060
    },
    {
      "epoch": 1.3395555555555556,
      "grad_norm": 0.9297938942909241,
      "learning_rate": 4.1627777777777784e-05,
      "loss": 0.0036,
      "step": 15070
    },
    {
      "epoch": 1.3404444444444445,
      "grad_norm": 0.9468923807144165,
      "learning_rate": 4.162222222222222e-05,
      "loss": 0.0025,
      "step": 15080
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.46821022033691406,
      "learning_rate": 4.161666666666667e-05,
      "loss": 0.0028,
      "step": 15090
    },
    {
      "epoch": 1.3422222222222222,
      "grad_norm": 0.24310728907585144,
      "learning_rate": 4.1611111111111114e-05,
      "loss": 0.0031,
      "step": 15100
    },
    {
      "epoch": 1.3431111111111111,
      "grad_norm": 0.6931477189064026,
      "learning_rate": 4.160555555555556e-05,
      "loss": 0.0023,
      "step": 15110
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 1.2698640823364258,
      "learning_rate": 4.16e-05,
      "loss": 0.0027,
      "step": 15120
    },
    {
      "epoch": 1.3448888888888888,
      "grad_norm": 1.0928621292114258,
      "learning_rate": 4.1594444444444445e-05,
      "loss": 0.0035,
      "step": 15130
    },
    {
      "epoch": 1.3457777777777777,
      "grad_norm": 0.6345466375350952,
      "learning_rate": 4.158888888888889e-05,
      "loss": 0.0029,
      "step": 15140
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.12412331998348236,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.0024,
      "step": 15150
    },
    {
      "epoch": 1.3475555555555556,
      "grad_norm": 0.8282561898231506,
      "learning_rate": 4.157777777777778e-05,
      "loss": 0.0031,
      "step": 15160
    },
    {
      "epoch": 1.3484444444444446,
      "grad_norm": 0.5582107305526733,
      "learning_rate": 4.1572222222222226e-05,
      "loss": 0.0039,
      "step": 15170
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.1376699060201645,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.002,
      "step": 15180
    },
    {
      "epoch": 1.3502222222222222,
      "grad_norm": 0.18167567253112793,
      "learning_rate": 4.156111111111111e-05,
      "loss": 0.0032,
      "step": 15190
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 0.6652611494064331,
      "learning_rate": 4.155555555555556e-05,
      "loss": 0.0032,
      "step": 15200
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.8932170867919922,
      "learning_rate": 4.155e-05,
      "loss": 0.0021,
      "step": 15210
    },
    {
      "epoch": 1.3528888888888888,
      "grad_norm": 1.0529037714004517,
      "learning_rate": 4.1544444444444444e-05,
      "loss": 0.0021,
      "step": 15220
    },
    {
      "epoch": 1.3537777777777777,
      "grad_norm": 0.8501182198524475,
      "learning_rate": 4.153888888888889e-05,
      "loss": 0.0031,
      "step": 15230
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.48404446244239807,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0046,
      "step": 15240
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.8359455466270447,
      "learning_rate": 4.152777777777778e-05,
      "loss": 0.003,
      "step": 15250
    },
    {
      "epoch": 1.3564444444444446,
      "grad_norm": 0.6474345922470093,
      "learning_rate": 4.1522222222222225e-05,
      "loss": 0.0026,
      "step": 15260
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.60750812292099,
      "learning_rate": 4.151666666666667e-05,
      "loss": 0.0032,
      "step": 15270
    },
    {
      "epoch": 1.3582222222222222,
      "grad_norm": 0.4836530387401581,
      "learning_rate": 4.151111111111111e-05,
      "loss": 0.0039,
      "step": 15280
    },
    {
      "epoch": 1.3591111111111112,
      "grad_norm": 0.1919509619474411,
      "learning_rate": 4.1505555555555556e-05,
      "loss": 0.0051,
      "step": 15290
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.44654467701911926,
      "learning_rate": 4.15e-05,
      "loss": 0.0027,
      "step": 15300
    },
    {
      "epoch": 1.3608888888888888,
      "grad_norm": 0.05638159438967705,
      "learning_rate": 4.149444444444445e-05,
      "loss": 0.0029,
      "step": 15310
    },
    {
      "epoch": 1.3617777777777778,
      "grad_norm": 0.5202998518943787,
      "learning_rate": 4.1488888888888886e-05,
      "loss": 0.0031,
      "step": 15320
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.2525879144668579,
      "learning_rate": 4.1483333333333337e-05,
      "loss": 0.0029,
      "step": 15330
    },
    {
      "epoch": 1.3635555555555556,
      "grad_norm": 0.0871206745505333,
      "learning_rate": 4.147777777777778e-05,
      "loss": 0.0029,
      "step": 15340
    },
    {
      "epoch": 1.3644444444444446,
      "grad_norm": 0.8057839274406433,
      "learning_rate": 4.1472222222222224e-05,
      "loss": 0.0027,
      "step": 15350
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 1.3106591701507568,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0029,
      "step": 15360
    },
    {
      "epoch": 1.3662222222222222,
      "grad_norm": 1.2812645435333252,
      "learning_rate": 4.146111111111111e-05,
      "loss": 0.0035,
      "step": 15370
    },
    {
      "epoch": 1.3671111111111112,
      "grad_norm": 1.0515625476837158,
      "learning_rate": 4.145555555555556e-05,
      "loss": 0.0042,
      "step": 15380
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.21933165192604065,
      "learning_rate": 4.145e-05,
      "loss": 0.0023,
      "step": 15390
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 1.135306715965271,
      "learning_rate": 4.144444444444445e-05,
      "loss": 0.0031,
      "step": 15400
    },
    {
      "epoch": 1.3697777777777778,
      "grad_norm": 1.0848567485809326,
      "learning_rate": 4.1438888888888885e-05,
      "loss": 0.0028,
      "step": 15410
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.48031124472618103,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0033,
      "step": 15420
    },
    {
      "epoch": 1.3715555555555556,
      "grad_norm": 0.41504016518592834,
      "learning_rate": 4.142777777777778e-05,
      "loss": 0.0033,
      "step": 15430
    },
    {
      "epoch": 1.3724444444444446,
      "grad_norm": 0.4645358622074127,
      "learning_rate": 4.142222222222222e-05,
      "loss": 0.0028,
      "step": 15440
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.33499205112457275,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.0034,
      "step": 15450
    },
    {
      "epoch": 1.3742222222222222,
      "grad_norm": 0.2831314504146576,
      "learning_rate": 4.141111111111111e-05,
      "loss": 0.0036,
      "step": 15460
    },
    {
      "epoch": 1.3751111111111112,
      "grad_norm": 0.10734343528747559,
      "learning_rate": 4.140555555555556e-05,
      "loss": 0.0031,
      "step": 15470
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.9379745721817017,
      "learning_rate": 4.14e-05,
      "loss": 0.0019,
      "step": 15480
    },
    {
      "epoch": 1.3768888888888888,
      "grad_norm": 1.256156086921692,
      "learning_rate": 4.139444444444445e-05,
      "loss": 0.003,
      "step": 15490
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.2268199622631073,
      "learning_rate": 4.138888888888889e-05,
      "loss": 0.0024,
      "step": 15500
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.21968671679496765,
      "learning_rate": 4.1383333333333334e-05,
      "loss": 0.0026,
      "step": 15510
    },
    {
      "epoch": 1.3795555555555556,
      "grad_norm": 0.9829030632972717,
      "learning_rate": 4.1377777777777784e-05,
      "loss": 0.0029,
      "step": 15520
    },
    {
      "epoch": 1.3804444444444444,
      "grad_norm": 0.39212602376937866,
      "learning_rate": 4.137222222222222e-05,
      "loss": 0.0037,
      "step": 15530
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.3525069057941437,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0031,
      "step": 15540
    },
    {
      "epoch": 1.3822222222222222,
      "grad_norm": 0.3170320987701416,
      "learning_rate": 4.136111111111111e-05,
      "loss": 0.0036,
      "step": 15550
    },
    {
      "epoch": 1.3831111111111112,
      "grad_norm": 0.27157002687454224,
      "learning_rate": 4.135555555555556e-05,
      "loss": 0.0036,
      "step": 15560
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.7559097409248352,
      "learning_rate": 4.135e-05,
      "loss": 0.003,
      "step": 15570
    },
    {
      "epoch": 1.3848888888888888,
      "grad_norm": 0.3229425847530365,
      "learning_rate": 4.1344444444444446e-05,
      "loss": 0.0027,
      "step": 15580
    },
    {
      "epoch": 1.3857777777777778,
      "grad_norm": 0.6583185791969299,
      "learning_rate": 4.133888888888889e-05,
      "loss": 0.0046,
      "step": 15590
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.3705199956893921,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0028,
      "step": 15600
    },
    {
      "epoch": 1.3875555555555557,
      "grad_norm": 1.3344968557357788,
      "learning_rate": 4.132777777777778e-05,
      "loss": 0.0025,
      "step": 15610
    },
    {
      "epoch": 1.3884444444444444,
      "grad_norm": 1.0616918802261353,
      "learning_rate": 4.132222222222222e-05,
      "loss": 0.0029,
      "step": 15620
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.7078817486763,
      "learning_rate": 4.131666666666667e-05,
      "loss": 0.0038,
      "step": 15630
    },
    {
      "epoch": 1.3902222222222222,
      "grad_norm": 0.7939676642417908,
      "learning_rate": 4.1311111111111114e-05,
      "loss": 0.0021,
      "step": 15640
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 0.07300028949975967,
      "learning_rate": 4.130555555555556e-05,
      "loss": 0.0028,
      "step": 15650
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.186158686876297,
      "learning_rate": 4.13e-05,
      "loss": 0.0027,
      "step": 15660
    },
    {
      "epoch": 1.3928888888888888,
      "grad_norm": 0.6235016584396362,
      "learning_rate": 4.1294444444444445e-05,
      "loss": 0.0052,
      "step": 15670
    },
    {
      "epoch": 1.3937777777777778,
      "grad_norm": 1.1180862188339233,
      "learning_rate": 4.1288888888888895e-05,
      "loss": 0.0041,
      "step": 15680
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.6050736308097839,
      "learning_rate": 4.128333333333333e-05,
      "loss": 0.0033,
      "step": 15690
    },
    {
      "epoch": 1.3955555555555557,
      "grad_norm": 0.4922751486301422,
      "learning_rate": 4.127777777777778e-05,
      "loss": 0.0018,
      "step": 15700
    },
    {
      "epoch": 1.3964444444444444,
      "grad_norm": 0.28603485226631165,
      "learning_rate": 4.1272222222222226e-05,
      "loss": 0.0027,
      "step": 15710
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.3759419322013855,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0025,
      "step": 15720
    },
    {
      "epoch": 1.3982222222222223,
      "grad_norm": 0.11410228908061981,
      "learning_rate": 4.126111111111111e-05,
      "loss": 0.0027,
      "step": 15730
    },
    {
      "epoch": 1.3991111111111112,
      "grad_norm": 0.37844547629356384,
      "learning_rate": 4.1255555555555556e-05,
      "loss": 0.0033,
      "step": 15740
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.6535040140151978,
      "learning_rate": 4.125e-05,
      "loss": 0.0042,
      "step": 15750
    },
    {
      "epoch": 1.4008888888888889,
      "grad_norm": 0.08543110638856888,
      "learning_rate": 4.124444444444444e-05,
      "loss": 0.0025,
      "step": 15760
    },
    {
      "epoch": 1.4017777777777778,
      "grad_norm": 0.432417094707489,
      "learning_rate": 4.1238888888888894e-05,
      "loss": 0.0024,
      "step": 15770
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.8860028982162476,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.0031,
      "step": 15780
    },
    {
      "epoch": 1.4035555555555557,
      "grad_norm": 0.5935617089271545,
      "learning_rate": 4.122777777777778e-05,
      "loss": 0.0033,
      "step": 15790
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 1.2914291620254517,
      "learning_rate": 4.1222222222222224e-05,
      "loss": 0.0025,
      "step": 15800
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.07229570299386978,
      "learning_rate": 4.121666666666667e-05,
      "loss": 0.0028,
      "step": 15810
    },
    {
      "epoch": 1.4062222222222223,
      "grad_norm": 0.5958621501922607,
      "learning_rate": 4.121111111111111e-05,
      "loss": 0.0022,
      "step": 15820
    },
    {
      "epoch": 1.407111111111111,
      "grad_norm": 1.081318736076355,
      "learning_rate": 4.1205555555555555e-05,
      "loss": 0.0023,
      "step": 15830
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.33511969447135925,
      "learning_rate": 4.12e-05,
      "loss": 0.0024,
      "step": 15840
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.5584641098976135,
      "learning_rate": 4.119444444444445e-05,
      "loss": 0.0029,
      "step": 15850
    },
    {
      "epoch": 1.4097777777777778,
      "grad_norm": 0.5918394327163696,
      "learning_rate": 4.118888888888889e-05,
      "loss": 0.004,
      "step": 15860
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.43970155715942383,
      "learning_rate": 4.1183333333333336e-05,
      "loss": 0.0046,
      "step": 15870
    },
    {
      "epoch": 1.4115555555555557,
      "grad_norm": 1.4328043460845947,
      "learning_rate": 4.117777777777778e-05,
      "loss": 0.0033,
      "step": 15880
    },
    {
      "epoch": 1.4124444444444444,
      "grad_norm": 0.1764252632856369,
      "learning_rate": 4.117222222222222e-05,
      "loss": 0.004,
      "step": 15890
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.09853223711252213,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0028,
      "step": 15900
    },
    {
      "epoch": 1.4142222222222223,
      "grad_norm": 0.22598020732402802,
      "learning_rate": 4.116111111111111e-05,
      "loss": 0.0029,
      "step": 15910
    },
    {
      "epoch": 1.415111111111111,
      "grad_norm": 0.2727571129798889,
      "learning_rate": 4.115555555555556e-05,
      "loss": 0.0022,
      "step": 15920
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.43378689885139465,
      "learning_rate": 4.115e-05,
      "loss": 0.0036,
      "step": 15930
    },
    {
      "epoch": 1.4168888888888889,
      "grad_norm": 0.6654370427131653,
      "learning_rate": 4.114444444444445e-05,
      "loss": 0.0034,
      "step": 15940
    },
    {
      "epoch": 1.4177777777777778,
      "grad_norm": 0.21218012273311615,
      "learning_rate": 4.113888888888889e-05,
      "loss": 0.002,
      "step": 15950
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.5668824911117554,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0033,
      "step": 15960
    },
    {
      "epoch": 1.4195555555555557,
      "grad_norm": 0.23259608447551727,
      "learning_rate": 4.1127777777777785e-05,
      "loss": 0.0025,
      "step": 15970
    },
    {
      "epoch": 1.4204444444444444,
      "grad_norm": 0.6722498536109924,
      "learning_rate": 4.112222222222222e-05,
      "loss": 0.0025,
      "step": 15980
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.1037738174200058,
      "learning_rate": 4.111666666666667e-05,
      "loss": 0.0019,
      "step": 15990
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.7126317024230957,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.0019,
      "step": 16000
    },
    {
      "epoch": 1.423111111111111,
      "grad_norm": 0.9413785338401794,
      "learning_rate": 4.110555555555556e-05,
      "loss": 0.0023,
      "step": 16010
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.42224690318107605,
      "learning_rate": 4.11e-05,
      "loss": 0.0023,
      "step": 16020
    },
    {
      "epoch": 1.4248888888888889,
      "grad_norm": 0.40671616792678833,
      "learning_rate": 4.1094444444444446e-05,
      "loss": 0.0044,
      "step": 16030
    },
    {
      "epoch": 1.4257777777777778,
      "grad_norm": 0.5048239231109619,
      "learning_rate": 4.10888888888889e-05,
      "loss": 0.0026,
      "step": 16040
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.5988914966583252,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.0033,
      "step": 16050
    },
    {
      "epoch": 1.4275555555555557,
      "grad_norm": 0.2925529181957245,
      "learning_rate": 4.1077777777777784e-05,
      "loss": 0.0033,
      "step": 16060
    },
    {
      "epoch": 1.4284444444444444,
      "grad_norm": 0.38574716448783875,
      "learning_rate": 4.107222222222222e-05,
      "loss": 0.0025,
      "step": 16070
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.20188601315021515,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0032,
      "step": 16080
    },
    {
      "epoch": 1.4302222222222223,
      "grad_norm": 0.4163062870502472,
      "learning_rate": 4.1061111111111115e-05,
      "loss": 0.0017,
      "step": 16090
    },
    {
      "epoch": 1.431111111111111,
      "grad_norm": 1.0277856588363647,
      "learning_rate": 4.105555555555556e-05,
      "loss": 0.0031,
      "step": 16100
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.22327502071857452,
      "learning_rate": 4.105e-05,
      "loss": 0.0034,
      "step": 16110
    },
    {
      "epoch": 1.4328888888888889,
      "grad_norm": 0.8633385300636292,
      "learning_rate": 4.1044444444444445e-05,
      "loss": 0.0033,
      "step": 16120
    },
    {
      "epoch": 1.4337777777777778,
      "grad_norm": 0.3440876603126526,
      "learning_rate": 4.1038888888888896e-05,
      "loss": 0.0032,
      "step": 16130
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.12514719367027283,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0031,
      "step": 16140
    },
    {
      "epoch": 1.4355555555555555,
      "grad_norm": 0.6898390054702759,
      "learning_rate": 4.102777777777778e-05,
      "loss": 0.0044,
      "step": 16150
    },
    {
      "epoch": 1.4364444444444444,
      "grad_norm": 0.518174946308136,
      "learning_rate": 4.1022222222222226e-05,
      "loss": 0.0031,
      "step": 16160
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.45017117261886597,
      "learning_rate": 4.101666666666667e-05,
      "loss": 0.0035,
      "step": 16170
    },
    {
      "epoch": 1.4382222222222223,
      "grad_norm": 0.5156142115592957,
      "learning_rate": 4.101111111111111e-05,
      "loss": 0.0027,
      "step": 16180
    },
    {
      "epoch": 1.439111111111111,
      "grad_norm": 0.35852867364883423,
      "learning_rate": 4.100555555555556e-05,
      "loss": 0.0035,
      "step": 16190
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.42636924982070923,
      "learning_rate": 4.1e-05,
      "loss": 0.0024,
      "step": 16200
    },
    {
      "epoch": 1.4408888888888889,
      "grad_norm": 1.0431452989578247,
      "learning_rate": 4.0994444444444444e-05,
      "loss": 0.0041,
      "step": 16210
    },
    {
      "epoch": 1.4417777777777778,
      "grad_norm": 0.7070963978767395,
      "learning_rate": 4.0988888888888894e-05,
      "loss": 0.0027,
      "step": 16220
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.40368539094924927,
      "learning_rate": 4.098333333333334e-05,
      "loss": 0.004,
      "step": 16230
    },
    {
      "epoch": 1.4435555555555555,
      "grad_norm": 0.6102766990661621,
      "learning_rate": 4.097777777777778e-05,
      "loss": 0.0042,
      "step": 16240
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.6903947591781616,
      "learning_rate": 4.0972222222222225e-05,
      "loss": 0.0024,
      "step": 16250
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.39829835295677185,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0025,
      "step": 16260
    },
    {
      "epoch": 1.4462222222222223,
      "grad_norm": 0.06396038085222244,
      "learning_rate": 4.096111111111111e-05,
      "loss": 0.0024,
      "step": 16270
    },
    {
      "epoch": 1.447111111111111,
      "grad_norm": 0.3335067331790924,
      "learning_rate": 4.0955555555555556e-05,
      "loss": 0.0023,
      "step": 16280
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.8144434094429016,
      "learning_rate": 4.095e-05,
      "loss": 0.0027,
      "step": 16290
    },
    {
      "epoch": 1.448888888888889,
      "grad_norm": 0.828961968421936,
      "learning_rate": 4.094444444444445e-05,
      "loss": 0.0035,
      "step": 16300
    },
    {
      "epoch": 1.4497777777777778,
      "grad_norm": 0.30262285470962524,
      "learning_rate": 4.093888888888889e-05,
      "loss": 0.0041,
      "step": 16310
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.6948181986808777,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0043,
      "step": 16320
    },
    {
      "epoch": 1.4515555555555555,
      "grad_norm": 0.46759456396102905,
      "learning_rate": 4.092777777777778e-05,
      "loss": 0.0033,
      "step": 16330
    },
    {
      "epoch": 1.4524444444444444,
      "grad_norm": 0.8037680983543396,
      "learning_rate": 4.0922222222222224e-05,
      "loss": 0.0033,
      "step": 16340
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.2736075818538666,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.0018,
      "step": 16350
    },
    {
      "epoch": 1.4542222222222223,
      "grad_norm": 0.406402587890625,
      "learning_rate": 4.091111111111111e-05,
      "loss": 0.0022,
      "step": 16360
    },
    {
      "epoch": 1.455111111111111,
      "grad_norm": 0.17024485766887665,
      "learning_rate": 4.090555555555556e-05,
      "loss": 0.0036,
      "step": 16370
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.42443951964378357,
      "learning_rate": 4.09e-05,
      "loss": 0.0038,
      "step": 16380
    },
    {
      "epoch": 1.456888888888889,
      "grad_norm": 0.48786723613739014,
      "learning_rate": 4.089444444444445e-05,
      "loss": 0.0032,
      "step": 16390
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 0.29668566584587097,
      "learning_rate": 4.088888888888889e-05,
      "loss": 0.0035,
      "step": 16400
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.7777371406555176,
      "learning_rate": 4.0883333333333335e-05,
      "loss": 0.004,
      "step": 16410
    },
    {
      "epoch": 1.4595555555555555,
      "grad_norm": 1.245330810546875,
      "learning_rate": 4.087777777777778e-05,
      "loss": 0.0032,
      "step": 16420
    },
    {
      "epoch": 1.4604444444444444,
      "grad_norm": 0.731345534324646,
      "learning_rate": 4.087222222222222e-05,
      "loss": 0.0031,
      "step": 16430
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 1.079762578010559,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0027,
      "step": 16440
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 0.7984296679496765,
      "learning_rate": 4.086111111111111e-05,
      "loss": 0.002,
      "step": 16450
    },
    {
      "epoch": 1.463111111111111,
      "grad_norm": 1.080061435699463,
      "learning_rate": 4.085555555555556e-05,
      "loss": 0.0023,
      "step": 16460
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.8400894999504089,
      "learning_rate": 4.085e-05,
      "loss": 0.0026,
      "step": 16470
    },
    {
      "epoch": 1.464888888888889,
      "grad_norm": 0.41490110754966736,
      "learning_rate": 4.084444444444445e-05,
      "loss": 0.0037,
      "step": 16480
    },
    {
      "epoch": 1.4657777777777778,
      "grad_norm": 0.23636484146118164,
      "learning_rate": 4.083888888888889e-05,
      "loss": 0.0027,
      "step": 16490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.17649991810321808,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0025,
      "step": 16500
    },
    {
      "epoch": 1.4675555555555555,
      "grad_norm": 0.16190193593502045,
      "learning_rate": 4.0827777777777785e-05,
      "loss": 0.0036,
      "step": 16510
    },
    {
      "epoch": 1.4684444444444444,
      "grad_norm": 1.084344744682312,
      "learning_rate": 4.082222222222222e-05,
      "loss": 0.0033,
      "step": 16520
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.7189328074455261,
      "learning_rate": 4.081666666666667e-05,
      "loss": 0.0032,
      "step": 16530
    },
    {
      "epoch": 1.470222222222222,
      "grad_norm": 0.09114299714565277,
      "learning_rate": 4.081111111111111e-05,
      "loss": 0.0024,
      "step": 16540
    },
    {
      "epoch": 1.471111111111111,
      "grad_norm": 0.7940609455108643,
      "learning_rate": 4.080555555555556e-05,
      "loss": 0.0027,
      "step": 16550
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.7712977528572083,
      "learning_rate": 4.08e-05,
      "loss": 0.0026,
      "step": 16560
    },
    {
      "epoch": 1.472888888888889,
      "grad_norm": 0.30115488171577454,
      "learning_rate": 4.0794444444444446e-05,
      "loss": 0.003,
      "step": 16570
    },
    {
      "epoch": 1.4737777777777779,
      "grad_norm": 0.7306933999061584,
      "learning_rate": 4.0788888888888896e-05,
      "loss": 0.0023,
      "step": 16580
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.08847583085298538,
      "learning_rate": 4.078333333333333e-05,
      "loss": 0.0033,
      "step": 16590
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 0.29896390438079834,
      "learning_rate": 4.0777777777777783e-05,
      "loss": 0.0021,
      "step": 16600
    },
    {
      "epoch": 1.4764444444444444,
      "grad_norm": 0.22692446410655975,
      "learning_rate": 4.077222222222222e-05,
      "loss": 0.0036,
      "step": 16610
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 1.2996766567230225,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.0036,
      "step": 16620
    },
    {
      "epoch": 1.478222222222222,
      "grad_norm": 0.8136079907417297,
      "learning_rate": 4.0761111111111114e-05,
      "loss": 0.0037,
      "step": 16630
    },
    {
      "epoch": 1.479111111111111,
      "grad_norm": 1.0977987051010132,
      "learning_rate": 4.075555555555556e-05,
      "loss": 0.0045,
      "step": 16640
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.4848375916481018,
      "learning_rate": 4.075e-05,
      "loss": 0.005,
      "step": 16650
    },
    {
      "epoch": 1.480888888888889,
      "grad_norm": 0.7137776613235474,
      "learning_rate": 4.0744444444444445e-05,
      "loss": 0.0037,
      "step": 16660
    },
    {
      "epoch": 1.4817777777777779,
      "grad_norm": 1.0452921390533447,
      "learning_rate": 4.0738888888888895e-05,
      "loss": 0.0038,
      "step": 16670
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.7143917083740234,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0028,
      "step": 16680
    },
    {
      "epoch": 1.4835555555555555,
      "grad_norm": 0.515571117401123,
      "learning_rate": 4.072777777777778e-05,
      "loss": 0.0027,
      "step": 16690
    },
    {
      "epoch": 1.4844444444444445,
      "grad_norm": 0.12860123813152313,
      "learning_rate": 4.0722222222222226e-05,
      "loss": 0.0034,
      "step": 16700
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.2499249428510666,
      "learning_rate": 4.071666666666667e-05,
      "loss": 0.0032,
      "step": 16710
    },
    {
      "epoch": 1.4862222222222221,
      "grad_norm": 1.0117915868759155,
      "learning_rate": 4.071111111111111e-05,
      "loss": 0.0025,
      "step": 16720
    },
    {
      "epoch": 1.487111111111111,
      "grad_norm": 0.35593512654304504,
      "learning_rate": 4.0705555555555556e-05,
      "loss": 0.0036,
      "step": 16730
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.3813953399658203,
      "learning_rate": 4.07e-05,
      "loss": 0.0043,
      "step": 16740
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.4005949795246124,
      "learning_rate": 4.0694444444444444e-05,
      "loss": 0.0025,
      "step": 16750
    },
    {
      "epoch": 1.4897777777777779,
      "grad_norm": 0.7138135433197021,
      "learning_rate": 4.0688888888888894e-05,
      "loss": 0.003,
      "step": 16760
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.48850178718566895,
      "learning_rate": 4.068333333333334e-05,
      "loss": 0.004,
      "step": 16770
    },
    {
      "epoch": 1.4915555555555555,
      "grad_norm": 0.6470478177070618,
      "learning_rate": 4.067777777777778e-05,
      "loss": 0.003,
      "step": 16780
    },
    {
      "epoch": 1.4924444444444445,
      "grad_norm": 1.1504918336868286,
      "learning_rate": 4.0672222222222225e-05,
      "loss": 0.0033,
      "step": 16790
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.690616250038147,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0036,
      "step": 16800
    },
    {
      "epoch": 1.4942222222222221,
      "grad_norm": 0.8900324106216431,
      "learning_rate": 4.066111111111111e-05,
      "loss": 0.0027,
      "step": 16810
    },
    {
      "epoch": 1.495111111111111,
      "grad_norm": 0.11012260615825653,
      "learning_rate": 4.0655555555555555e-05,
      "loss": 0.0026,
      "step": 16820
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.043747466057538986,
      "learning_rate": 4.065e-05,
      "loss": 0.0027,
      "step": 16830
    },
    {
      "epoch": 1.496888888888889,
      "grad_norm": 0.35149943828582764,
      "learning_rate": 4.064444444444445e-05,
      "loss": 0.0031,
      "step": 16840
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 0.23765258491039276,
      "learning_rate": 4.063888888888889e-05,
      "loss": 0.0028,
      "step": 16850
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.32281234860420227,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.0032,
      "step": 16860
    },
    {
      "epoch": 1.4995555555555555,
      "grad_norm": 0.487374871969223,
      "learning_rate": 4.062777777777778e-05,
      "loss": 0.003,
      "step": 16870
    },
    {
      "epoch": 1.5004444444444445,
      "grad_norm": 0.35526514053344727,
      "learning_rate": 4.062222222222222e-05,
      "loss": 0.0025,
      "step": 16880
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.7882209420204163,
      "learning_rate": 4.061666666666667e-05,
      "loss": 0.0025,
      "step": 16890
    },
    {
      "epoch": 1.5022222222222221,
      "grad_norm": 0.26150020956993103,
      "learning_rate": 4.061111111111111e-05,
      "loss": 0.0032,
      "step": 16900
    },
    {
      "epoch": 1.503111111111111,
      "grad_norm": 0.1475648283958435,
      "learning_rate": 4.060555555555556e-05,
      "loss": 0.0023,
      "step": 16910
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.7810356020927429,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0022,
      "step": 16920
    },
    {
      "epoch": 1.504888888888889,
      "grad_norm": 0.3123146593570709,
      "learning_rate": 4.059444444444445e-05,
      "loss": 0.0024,
      "step": 16930
    },
    {
      "epoch": 1.5057777777777779,
      "grad_norm": 1.025496006011963,
      "learning_rate": 4.058888888888889e-05,
      "loss": 0.0024,
      "step": 16940
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.7062022686004639,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.0024,
      "step": 16950
    },
    {
      "epoch": 1.5075555555555555,
      "grad_norm": 0.5563015341758728,
      "learning_rate": 4.057777777777778e-05,
      "loss": 0.0023,
      "step": 16960
    },
    {
      "epoch": 1.5084444444444445,
      "grad_norm": 0.3752623200416565,
      "learning_rate": 4.057222222222222e-05,
      "loss": 0.003,
      "step": 16970
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.11031222343444824,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.003,
      "step": 16980
    },
    {
      "epoch": 1.5102222222222221,
      "grad_norm": 1.2106454372406006,
      "learning_rate": 4.056111111111111e-05,
      "loss": 0.0028,
      "step": 16990
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 1.2029199600219727,
      "learning_rate": 4.055555555555556e-05,
      "loss": 0.0017,
      "step": 17000
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.6256759166717529,
      "learning_rate": 4.055e-05,
      "loss": 0.0028,
      "step": 17010
    },
    {
      "epoch": 1.512888888888889,
      "grad_norm": 0.29867398738861084,
      "learning_rate": 4.054444444444445e-05,
      "loss": 0.0032,
      "step": 17020
    },
    {
      "epoch": 1.5137777777777779,
      "grad_norm": 0.3370802104473114,
      "learning_rate": 4.053888888888889e-05,
      "loss": 0.003,
      "step": 17030
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.6947388052940369,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.003,
      "step": 17040
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.26440364122390747,
      "learning_rate": 4.0527777777777784e-05,
      "loss": 0.003,
      "step": 17050
    },
    {
      "epoch": 1.5164444444444445,
      "grad_norm": 0.6482186913490295,
      "learning_rate": 4.052222222222222e-05,
      "loss": 0.0034,
      "step": 17060
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 1.2218726873397827,
      "learning_rate": 4.051666666666667e-05,
      "loss": 0.0027,
      "step": 17070
    },
    {
      "epoch": 1.5182222222222221,
      "grad_norm": 0.04077110439538956,
      "learning_rate": 4.051111111111111e-05,
      "loss": 0.0036,
      "step": 17080
    },
    {
      "epoch": 1.519111111111111,
      "grad_norm": 0.8702486157417297,
      "learning_rate": 4.050555555555556e-05,
      "loss": 0.0023,
      "step": 17090
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.4540829360485077,
      "learning_rate": 4.05e-05,
      "loss": 0.0028,
      "step": 17100
    },
    {
      "epoch": 1.520888888888889,
      "grad_norm": 0.19994579255580902,
      "learning_rate": 4.0494444444444445e-05,
      "loss": 0.0028,
      "step": 17110
    },
    {
      "epoch": 1.521777777777778,
      "grad_norm": 0.5961747169494629,
      "learning_rate": 4.0488888888888896e-05,
      "loss": 0.0026,
      "step": 17120
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.5886286497116089,
      "learning_rate": 4.048333333333333e-05,
      "loss": 0.0018,
      "step": 17130
    },
    {
      "epoch": 1.5235555555555556,
      "grad_norm": 0.5398014783859253,
      "learning_rate": 4.047777777777778e-05,
      "loss": 0.0033,
      "step": 17140
    },
    {
      "epoch": 1.5244444444444445,
      "grad_norm": 0.46766728162765503,
      "learning_rate": 4.047222222222222e-05,
      "loss": 0.0024,
      "step": 17150
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.6110398173332214,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0028,
      "step": 17160
    },
    {
      "epoch": 1.5262222222222221,
      "grad_norm": 0.21358153223991394,
      "learning_rate": 4.0461111111111114e-05,
      "loss": 0.0039,
      "step": 17170
    },
    {
      "epoch": 1.527111111111111,
      "grad_norm": 1.0961238145828247,
      "learning_rate": 4.045555555555556e-05,
      "loss": 0.0031,
      "step": 17180
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.5356501340866089,
      "learning_rate": 4.045000000000001e-05,
      "loss": 0.0033,
      "step": 17190
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 0.09657292068004608,
      "learning_rate": 4.0444444444444444e-05,
      "loss": 0.0026,
      "step": 17200
    },
    {
      "epoch": 1.529777777777778,
      "grad_norm": 0.09373033791780472,
      "learning_rate": 4.0438888888888895e-05,
      "loss": 0.0028,
      "step": 17210
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.3246675431728363,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0029,
      "step": 17220
    },
    {
      "epoch": 1.5315555555555556,
      "grad_norm": 0.8261188268661499,
      "learning_rate": 4.042777777777778e-05,
      "loss": 0.0023,
      "step": 17230
    },
    {
      "epoch": 1.5324444444444445,
      "grad_norm": 0.2849988639354706,
      "learning_rate": 4.0422222222222225e-05,
      "loss": 0.0016,
      "step": 17240
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.23686635494232178,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.0024,
      "step": 17250
    },
    {
      "epoch": 1.5342222222222222,
      "grad_norm": 1.1837046146392822,
      "learning_rate": 4.041111111111111e-05,
      "loss": 0.0024,
      "step": 17260
    },
    {
      "epoch": 1.535111111111111,
      "grad_norm": 0.3493964672088623,
      "learning_rate": 4.0405555555555556e-05,
      "loss": 0.0026,
      "step": 17270
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.17454130947589874,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0032,
      "step": 17280
    },
    {
      "epoch": 1.536888888888889,
      "grad_norm": 0.07382965087890625,
      "learning_rate": 4.039444444444444e-05,
      "loss": 0.002,
      "step": 17290
    },
    {
      "epoch": 1.537777777777778,
      "grad_norm": 0.07728948444128036,
      "learning_rate": 4.038888888888889e-05,
      "loss": 0.002,
      "step": 17300
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.7563785910606384,
      "learning_rate": 4.038333333333334e-05,
      "loss": 0.0023,
      "step": 17310
    },
    {
      "epoch": 1.5395555555555556,
      "grad_norm": 0.8533180952072144,
      "learning_rate": 4.037777777777778e-05,
      "loss": 0.0043,
      "step": 17320
    },
    {
      "epoch": 1.5404444444444443,
      "grad_norm": 0.8055514097213745,
      "learning_rate": 4.0372222222222224e-05,
      "loss": 0.0034,
      "step": 17330
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.9104965329170227,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.0035,
      "step": 17340
    },
    {
      "epoch": 1.5422222222222222,
      "grad_norm": 0.3375476598739624,
      "learning_rate": 4.036111111111111e-05,
      "loss": 0.0024,
      "step": 17350
    },
    {
      "epoch": 1.543111111111111,
      "grad_norm": 0.8224537968635559,
      "learning_rate": 4.0355555555555555e-05,
      "loss": 0.0025,
      "step": 17360
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.42258313298225403,
      "learning_rate": 4.0350000000000005e-05,
      "loss": 0.0023,
      "step": 17370
    },
    {
      "epoch": 1.544888888888889,
      "grad_norm": 0.07331347465515137,
      "learning_rate": 4.034444444444445e-05,
      "loss": 0.0034,
      "step": 17380
    },
    {
      "epoch": 1.545777777777778,
      "grad_norm": 0.3632872700691223,
      "learning_rate": 4.033888888888889e-05,
      "loss": 0.0024,
      "step": 17390
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 1.0321749448776245,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0027,
      "step": 17400
    },
    {
      "epoch": 1.5475555555555556,
      "grad_norm": 0.7069326639175415,
      "learning_rate": 4.032777777777778e-05,
      "loss": 0.0017,
      "step": 17410
    },
    {
      "epoch": 1.5484444444444443,
      "grad_norm": 0.6608973741531372,
      "learning_rate": 4.032222222222222e-05,
      "loss": 0.0029,
      "step": 17420
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.3809528946876526,
      "learning_rate": 4.0316666666666666e-05,
      "loss": 0.0018,
      "step": 17430
    },
    {
      "epoch": 1.5502222222222222,
      "grad_norm": 0.41614997386932373,
      "learning_rate": 4.031111111111111e-05,
      "loss": 0.0022,
      "step": 17440
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 0.36504673957824707,
      "learning_rate": 4.030555555555556e-05,
      "loss": 0.0029,
      "step": 17450
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.47476157546043396,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0029,
      "step": 17460
    },
    {
      "epoch": 1.552888888888889,
      "grad_norm": 0.10492159426212311,
      "learning_rate": 4.029444444444445e-05,
      "loss": 0.0023,
      "step": 17470
    },
    {
      "epoch": 1.553777777777778,
      "grad_norm": 0.298006147146225,
      "learning_rate": 4.028888888888889e-05,
      "loss": 0.0028,
      "step": 17480
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.4292445182800293,
      "learning_rate": 4.0283333333333334e-05,
      "loss": 0.0029,
      "step": 17490
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.4966367781162262,
      "learning_rate": 4.027777777777778e-05,
      "loss": 0.0031,
      "step": 17500
    },
    {
      "epoch": 1.5564444444444443,
      "grad_norm": 1.1349432468414307,
      "learning_rate": 4.027222222222222e-05,
      "loss": 0.0019,
      "step": 17510
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 1.3012239933013916,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0022,
      "step": 17520
    },
    {
      "epoch": 1.5582222222222222,
      "grad_norm": 0.8247981071472168,
      "learning_rate": 4.026111111111111e-05,
      "loss": 0.0044,
      "step": 17530
    },
    {
      "epoch": 1.5591111111111111,
      "grad_norm": 0.3115827143192291,
      "learning_rate": 4.025555555555556e-05,
      "loss": 0.0029,
      "step": 17540
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.867195725440979,
      "learning_rate": 4.025e-05,
      "loss": 0.0029,
      "step": 17550
    },
    {
      "epoch": 1.560888888888889,
      "grad_norm": 0.5411784648895264,
      "learning_rate": 4.0244444444444446e-05,
      "loss": 0.0033,
      "step": 17560
    },
    {
      "epoch": 1.561777777777778,
      "grad_norm": 0.8320393562316895,
      "learning_rate": 4.023888888888889e-05,
      "loss": 0.0038,
      "step": 17570
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.7451862692832947,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.0018,
      "step": 17580
    },
    {
      "epoch": 1.5635555555555556,
      "grad_norm": 0.6138870120048523,
      "learning_rate": 4.0227777777777784e-05,
      "loss": 0.0034,
      "step": 17590
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 0.46136313676834106,
      "learning_rate": 4.022222222222222e-05,
      "loss": 0.0028,
      "step": 17600
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.6150339245796204,
      "learning_rate": 4.021666666666667e-05,
      "loss": 0.0027,
      "step": 17610
    },
    {
      "epoch": 1.5662222222222222,
      "grad_norm": 0.10101261734962463,
      "learning_rate": 4.021111111111111e-05,
      "loss": 0.0024,
      "step": 17620
    },
    {
      "epoch": 1.5671111111111111,
      "grad_norm": 0.4430280923843384,
      "learning_rate": 4.020555555555556e-05,
      "loss": 0.0028,
      "step": 17630
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.10007718205451965,
      "learning_rate": 4.02e-05,
      "loss": 0.0033,
      "step": 17640
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.1812380701303482,
      "learning_rate": 4.0194444444444445e-05,
      "loss": 0.0033,
      "step": 17650
    },
    {
      "epoch": 1.569777777777778,
      "grad_norm": 0.5525702834129333,
      "learning_rate": 4.0188888888888895e-05,
      "loss": 0.0024,
      "step": 17660
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.05994682013988495,
      "learning_rate": 4.018333333333333e-05,
      "loss": 0.0039,
      "step": 17670
    },
    {
      "epoch": 1.5715555555555556,
      "grad_norm": 0.18423506617546082,
      "learning_rate": 4.017777777777778e-05,
      "loss": 0.0026,
      "step": 17680
    },
    {
      "epoch": 1.5724444444444443,
      "grad_norm": 0.3748907148838043,
      "learning_rate": 4.017222222222222e-05,
      "loss": 0.0028,
      "step": 17690
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.7486913204193115,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0033,
      "step": 17700
    },
    {
      "epoch": 1.5742222222222222,
      "grad_norm": 0.5031577944755554,
      "learning_rate": 4.016111111111111e-05,
      "loss": 0.0019,
      "step": 17710
    },
    {
      "epoch": 1.5751111111111111,
      "grad_norm": 0.6298907399177551,
      "learning_rate": 4.0155555555555557e-05,
      "loss": 0.0029,
      "step": 17720
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.5742453336715698,
      "learning_rate": 4.015000000000001e-05,
      "loss": 0.0035,
      "step": 17730
    },
    {
      "epoch": 1.576888888888889,
      "grad_norm": 0.117799773812294,
      "learning_rate": 4.0144444444444444e-05,
      "loss": 0.003,
      "step": 17740
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.27431121468544006,
      "learning_rate": 4.0138888888888894e-05,
      "loss": 0.0048,
      "step": 17750
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.13654586672782898,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0039,
      "step": 17760
    },
    {
      "epoch": 1.5795555555555556,
      "grad_norm": 0.3277910649776459,
      "learning_rate": 4.012777777777778e-05,
      "loss": 0.0022,
      "step": 17770
    },
    {
      "epoch": 1.5804444444444443,
      "grad_norm": 0.32452458143234253,
      "learning_rate": 4.0122222222222225e-05,
      "loss": 0.0027,
      "step": 17780
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.6666136980056763,
      "learning_rate": 4.011666666666667e-05,
      "loss": 0.0034,
      "step": 17790
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 0.716171145439148,
      "learning_rate": 4.011111111111111e-05,
      "loss": 0.0028,
      "step": 17800
    },
    {
      "epoch": 1.5831111111111111,
      "grad_norm": 0.4286639988422394,
      "learning_rate": 4.0105555555555555e-05,
      "loss": 0.003,
      "step": 17810
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.7244263887405396,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.003,
      "step": 17820
    },
    {
      "epoch": 1.584888888888889,
      "grad_norm": 0.9346384406089783,
      "learning_rate": 4.009444444444444e-05,
      "loss": 0.0032,
      "step": 17830
    },
    {
      "epoch": 1.5857777777777777,
      "grad_norm": 0.8098475337028503,
      "learning_rate": 4.008888888888889e-05,
      "loss": 0.0031,
      "step": 17840
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.3824191391468048,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.0021,
      "step": 17850
    },
    {
      "epoch": 1.5875555555555556,
      "grad_norm": 0.7098982930183411,
      "learning_rate": 4.007777777777778e-05,
      "loss": 0.0017,
      "step": 17860
    },
    {
      "epoch": 1.5884444444444443,
      "grad_norm": 1.0801243782043457,
      "learning_rate": 4.0072222222222223e-05,
      "loss": 0.0021,
      "step": 17870
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.09328962862491608,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.004,
      "step": 17880
    },
    {
      "epoch": 1.5902222222222222,
      "grad_norm": 0.6998768448829651,
      "learning_rate": 4.006111111111111e-05,
      "loss": 0.0029,
      "step": 17890
    },
    {
      "epoch": 1.5911111111111111,
      "grad_norm": 1.3216694593429565,
      "learning_rate": 4.0055555555555554e-05,
      "loss": 0.004,
      "step": 17900
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.07042215764522552,
      "learning_rate": 4.0050000000000004e-05,
      "loss": 0.0041,
      "step": 17910
    },
    {
      "epoch": 1.592888888888889,
      "grad_norm": 0.6971957087516785,
      "learning_rate": 4.004444444444445e-05,
      "loss": 0.0044,
      "step": 17920
    },
    {
      "epoch": 1.5937777777777777,
      "grad_norm": 0.2781091332435608,
      "learning_rate": 4.003888888888889e-05,
      "loss": 0.0025,
      "step": 17930
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.19256830215454102,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0024,
      "step": 17940
    },
    {
      "epoch": 1.5955555555555554,
      "grad_norm": 1.0172016620635986,
      "learning_rate": 4.002777777777778e-05,
      "loss": 0.0044,
      "step": 17950
    },
    {
      "epoch": 1.5964444444444443,
      "grad_norm": 0.6688083410263062,
      "learning_rate": 4.002222222222222e-05,
      "loss": 0.0033,
      "step": 17960
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.24716825783252716,
      "learning_rate": 4.0016666666666666e-05,
      "loss": 0.0019,
      "step": 17970
    },
    {
      "epoch": 1.5982222222222222,
      "grad_norm": 0.6013524532318115,
      "learning_rate": 4.001111111111111e-05,
      "loss": 0.0023,
      "step": 17980
    },
    {
      "epoch": 1.5991111111111111,
      "grad_norm": 0.45094847679138184,
      "learning_rate": 4.000555555555556e-05,
      "loss": 0.0034,
      "step": 17990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.13092754781246185,
      "learning_rate": 4e-05,
      "loss": 0.0018,
      "step": 18000
    },
    {
      "epoch": 1.600888888888889,
      "grad_norm": 1.1047948598861694,
      "learning_rate": 3.999444444444445e-05,
      "loss": 0.0032,
      "step": 18010
    },
    {
      "epoch": 1.6017777777777777,
      "grad_norm": 0.0673191174864769,
      "learning_rate": 3.998888888888889e-05,
      "loss": 0.0027,
      "step": 18020
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.12368693947792053,
      "learning_rate": 3.9983333333333334e-05,
      "loss": 0.0037,
      "step": 18030
    },
    {
      "epoch": 1.6035555555555554,
      "grad_norm": 0.6352105736732483,
      "learning_rate": 3.997777777777778e-05,
      "loss": 0.0024,
      "step": 18040
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.7717753052711487,
      "learning_rate": 3.997222222222222e-05,
      "loss": 0.0034,
      "step": 18050
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.8826466202735901,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.0031,
      "step": 18060
    },
    {
      "epoch": 1.6062222222222222,
      "grad_norm": 0.21901820600032806,
      "learning_rate": 3.996111111111111e-05,
      "loss": 0.0036,
      "step": 18070
    },
    {
      "epoch": 1.6071111111111112,
      "grad_norm": 0.4364650547504425,
      "learning_rate": 3.995555555555556e-05,
      "loss": 0.0033,
      "step": 18080
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.06032818928360939,
      "learning_rate": 3.995e-05,
      "loss": 0.0023,
      "step": 18090
    },
    {
      "epoch": 1.608888888888889,
      "grad_norm": 0.24018514156341553,
      "learning_rate": 3.9944444444444446e-05,
      "loss": 0.0028,
      "step": 18100
    },
    {
      "epoch": 1.6097777777777778,
      "grad_norm": 1.3362520933151245,
      "learning_rate": 3.993888888888889e-05,
      "loss": 0.0037,
      "step": 18110
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.563424825668335,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0033,
      "step": 18120
    },
    {
      "epoch": 1.6115555555555554,
      "grad_norm": 0.07303053140640259,
      "learning_rate": 3.992777777777778e-05,
      "loss": 0.003,
      "step": 18130
    },
    {
      "epoch": 1.6124444444444443,
      "grad_norm": 0.4536483585834503,
      "learning_rate": 3.992222222222222e-05,
      "loss": 0.0019,
      "step": 18140
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.2145300805568695,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.0027,
      "step": 18150
    },
    {
      "epoch": 1.6142222222222222,
      "grad_norm": 0.5062196254730225,
      "learning_rate": 3.9911111111111114e-05,
      "loss": 0.0031,
      "step": 18160
    },
    {
      "epoch": 1.6151111111111112,
      "grad_norm": 0.271414577960968,
      "learning_rate": 3.990555555555556e-05,
      "loss": 0.003,
      "step": 18170
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.379410058259964,
      "learning_rate": 3.99e-05,
      "loss": 0.0026,
      "step": 18180
    },
    {
      "epoch": 1.616888888888889,
      "grad_norm": 0.776423990726471,
      "learning_rate": 3.9894444444444444e-05,
      "loss": 0.0031,
      "step": 18190
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 1.1663178205490112,
      "learning_rate": 3.9888888888888895e-05,
      "loss": 0.0019,
      "step": 18200
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.08767465502023697,
      "learning_rate": 3.988333333333333e-05,
      "loss": 0.0026,
      "step": 18210
    },
    {
      "epoch": 1.6195555555555554,
      "grad_norm": 0.09393346309661865,
      "learning_rate": 3.987777777777778e-05,
      "loss": 0.0021,
      "step": 18220
    },
    {
      "epoch": 1.6204444444444444,
      "grad_norm": 0.5649037957191467,
      "learning_rate": 3.987222222222222e-05,
      "loss": 0.0033,
      "step": 18230
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.767734944820404,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0022,
      "step": 18240
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.46923813223838806,
      "learning_rate": 3.986111111111111e-05,
      "loss": 0.0034,
      "step": 18250
    },
    {
      "epoch": 1.6231111111111112,
      "grad_norm": 0.3503512740135193,
      "learning_rate": 3.9855555555555556e-05,
      "loss": 0.0026,
      "step": 18260
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.8795632123947144,
      "learning_rate": 3.9850000000000006e-05,
      "loss": 0.0026,
      "step": 18270
    },
    {
      "epoch": 1.624888888888889,
      "grad_norm": 0.44728097319602966,
      "learning_rate": 3.984444444444444e-05,
      "loss": 0.0024,
      "step": 18280
    },
    {
      "epoch": 1.6257777777777778,
      "grad_norm": 0.07001430541276932,
      "learning_rate": 3.9838888888888894e-05,
      "loss": 0.0023,
      "step": 18290
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.4542403221130371,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0026,
      "step": 18300
    },
    {
      "epoch": 1.6275555555555554,
      "grad_norm": 0.804176926612854,
      "learning_rate": 3.982777777777778e-05,
      "loss": 0.0041,
      "step": 18310
    },
    {
      "epoch": 1.6284444444444444,
      "grad_norm": 0.4276893734931946,
      "learning_rate": 3.9822222222222224e-05,
      "loss": 0.0023,
      "step": 18320
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.4389817714691162,
      "learning_rate": 3.981666666666667e-05,
      "loss": 0.0025,
      "step": 18330
    },
    {
      "epoch": 1.6302222222222222,
      "grad_norm": 0.1186797171831131,
      "learning_rate": 3.981111111111112e-05,
      "loss": 0.0023,
      "step": 18340
    },
    {
      "epoch": 1.6311111111111112,
      "grad_norm": 0.6416916847229004,
      "learning_rate": 3.9805555555555555e-05,
      "loss": 0.0025,
      "step": 18350
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.27258485555648804,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0024,
      "step": 18360
    },
    {
      "epoch": 1.6328888888888888,
      "grad_norm": 0.5119271278381348,
      "learning_rate": 3.979444444444444e-05,
      "loss": 0.0029,
      "step": 18370
    },
    {
      "epoch": 1.6337777777777778,
      "grad_norm": 0.41143253445625305,
      "learning_rate": 3.978888888888889e-05,
      "loss": 0.0027,
      "step": 18380
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.059583067893981934,
      "learning_rate": 3.9783333333333336e-05,
      "loss": 0.002,
      "step": 18390
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 1.4227646589279175,
      "learning_rate": 3.977777777777778e-05,
      "loss": 0.0022,
      "step": 18400
    },
    {
      "epoch": 1.6364444444444444,
      "grad_norm": 0.9469703435897827,
      "learning_rate": 3.977222222222222e-05,
      "loss": 0.0031,
      "step": 18410
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.6415057182312012,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0025,
      "step": 18420
    },
    {
      "epoch": 1.6382222222222222,
      "grad_norm": 0.07443130016326904,
      "learning_rate": 3.976111111111112e-05,
      "loss": 0.003,
      "step": 18430
    },
    {
      "epoch": 1.6391111111111112,
      "grad_norm": 0.09577270597219467,
      "learning_rate": 3.9755555555555554e-05,
      "loss": 0.0027,
      "step": 18440
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.8019747734069824,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.004,
      "step": 18450
    },
    {
      "epoch": 1.6408888888888888,
      "grad_norm": 0.15286321938037872,
      "learning_rate": 3.974444444444445e-05,
      "loss": 0.0026,
      "step": 18460
    },
    {
      "epoch": 1.6417777777777778,
      "grad_norm": 0.060153014957904816,
      "learning_rate": 3.973888888888889e-05,
      "loss": 0.0022,
      "step": 18470
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.9239185452461243,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0027,
      "step": 18480
    },
    {
      "epoch": 1.6435555555555554,
      "grad_norm": 0.6514726877212524,
      "learning_rate": 3.972777777777778e-05,
      "loss": 0.003,
      "step": 18490
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 0.1435031294822693,
      "learning_rate": 3.972222222222222e-05,
      "loss": 0.004,
      "step": 18500
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.3045683801174164,
      "learning_rate": 3.9716666666666665e-05,
      "loss": 0.0026,
      "step": 18510
    },
    {
      "epoch": 1.6462222222222223,
      "grad_norm": 0.48658737540245056,
      "learning_rate": 3.9711111111111116e-05,
      "loss": 0.0023,
      "step": 18520
    },
    {
      "epoch": 1.6471111111111112,
      "grad_norm": 0.6883167028427124,
      "learning_rate": 3.970555555555556e-05,
      "loss": 0.0032,
      "step": 18530
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.5258743762969971,
      "learning_rate": 3.97e-05,
      "loss": 0.0027,
      "step": 18540
    },
    {
      "epoch": 1.6488888888888888,
      "grad_norm": 0.545211672782898,
      "learning_rate": 3.9694444444444446e-05,
      "loss": 0.003,
      "step": 18550
    },
    {
      "epoch": 1.6497777777777778,
      "grad_norm": 0.584794819355011,
      "learning_rate": 3.968888888888889e-05,
      "loss": 0.0024,
      "step": 18560
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.8045341968536377,
      "learning_rate": 3.9683333333333333e-05,
      "loss": 0.0027,
      "step": 18570
    },
    {
      "epoch": 1.6515555555555554,
      "grad_norm": 1.159622311592102,
      "learning_rate": 3.9677777777777784e-05,
      "loss": 0.0025,
      "step": 18580
    },
    {
      "epoch": 1.6524444444444444,
      "grad_norm": 0.514843225479126,
      "learning_rate": 3.967222222222222e-05,
      "loss": 0.0025,
      "step": 18590
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.2877831757068634,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0042,
      "step": 18600
    },
    {
      "epoch": 1.6542222222222223,
      "grad_norm": 0.26646941900253296,
      "learning_rate": 3.9661111111111114e-05,
      "loss": 0.0034,
      "step": 18610
    },
    {
      "epoch": 1.6551111111111112,
      "grad_norm": 0.2108604460954666,
      "learning_rate": 3.965555555555556e-05,
      "loss": 0.0029,
      "step": 18620
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.1107264906167984,
      "learning_rate": 3.965e-05,
      "loss": 0.0037,
      "step": 18630
    },
    {
      "epoch": 1.6568888888888889,
      "grad_norm": 0.07064510136842728,
      "learning_rate": 3.9644444444444445e-05,
      "loss": 0.0037,
      "step": 18640
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 0.6983156800270081,
      "learning_rate": 3.9638888888888895e-05,
      "loss": 0.003,
      "step": 18650
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.07018619775772095,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0022,
      "step": 18660
    },
    {
      "epoch": 1.6595555555555555,
      "grad_norm": 0.29012981057167053,
      "learning_rate": 3.962777777777778e-05,
      "loss": 0.0037,
      "step": 18670
    },
    {
      "epoch": 1.6604444444444444,
      "grad_norm": 0.16762246191501617,
      "learning_rate": 3.962222222222222e-05,
      "loss": 0.0031,
      "step": 18680
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.2549867033958435,
      "learning_rate": 3.961666666666667e-05,
      "loss": 0.0035,
      "step": 18690
    },
    {
      "epoch": 1.6622222222222223,
      "grad_norm": 0.1628318876028061,
      "learning_rate": 3.961111111111111e-05,
      "loss": 0.002,
      "step": 18700
    },
    {
      "epoch": 1.6631111111111112,
      "grad_norm": 0.4050178825855255,
      "learning_rate": 3.960555555555556e-05,
      "loss": 0.0033,
      "step": 18710
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.531338632106781,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0034,
      "step": 18720
    },
    {
      "epoch": 1.6648888888888889,
      "grad_norm": 0.9791548252105713,
      "learning_rate": 3.9594444444444444e-05,
      "loss": 0.0032,
      "step": 18730
    },
    {
      "epoch": 1.6657777777777778,
      "grad_norm": 0.09425117820501328,
      "learning_rate": 3.9588888888888894e-05,
      "loss": 0.0026,
      "step": 18740
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.23561632633209229,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.0023,
      "step": 18750
    },
    {
      "epoch": 1.6675555555555555,
      "grad_norm": 0.18171681463718414,
      "learning_rate": 3.957777777777778e-05,
      "loss": 0.0019,
      "step": 18760
    },
    {
      "epoch": 1.6684444444444444,
      "grad_norm": 0.7000830769538879,
      "learning_rate": 3.9572222222222225e-05,
      "loss": 0.0032,
      "step": 18770
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.19607926905155182,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.0032,
      "step": 18780
    },
    {
      "epoch": 1.6702222222222223,
      "grad_norm": 0.4380803108215332,
      "learning_rate": 3.956111111111112e-05,
      "loss": 0.0038,
      "step": 18790
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 1.1484626531600952,
      "learning_rate": 3.9555555555555556e-05,
      "loss": 0.0025,
      "step": 18800
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.9075176119804382,
      "learning_rate": 3.9550000000000006e-05,
      "loss": 0.0031,
      "step": 18810
    },
    {
      "epoch": 1.6728888888888889,
      "grad_norm": 0.47320616245269775,
      "learning_rate": 3.954444444444444e-05,
      "loss": 0.0036,
      "step": 18820
    },
    {
      "epoch": 1.6737777777777778,
      "grad_norm": 0.9277833700180054,
      "learning_rate": 3.953888888888889e-05,
      "loss": 0.004,
      "step": 18830
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.6621413826942444,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0036,
      "step": 18840
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 0.8224912285804749,
      "learning_rate": 3.952777777777778e-05,
      "loss": 0.0024,
      "step": 18850
    },
    {
      "epoch": 1.6764444444444444,
      "grad_norm": 0.13383224606513977,
      "learning_rate": 3.9522222222222224e-05,
      "loss": 0.0036,
      "step": 18860
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.7044506072998047,
      "learning_rate": 3.951666666666667e-05,
      "loss": 0.004,
      "step": 18870
    },
    {
      "epoch": 1.6782222222222223,
      "grad_norm": 0.6323705911636353,
      "learning_rate": 3.951111111111112e-05,
      "loss": 0.0042,
      "step": 18880
    },
    {
      "epoch": 1.6791111111111112,
      "grad_norm": 0.5338025689125061,
      "learning_rate": 3.9505555555555554e-05,
      "loss": 0.0035,
      "step": 18890
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.6821882128715515,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0026,
      "step": 18900
    },
    {
      "epoch": 1.6808888888888889,
      "grad_norm": 0.22573886811733246,
      "learning_rate": 3.949444444444445e-05,
      "loss": 0.0032,
      "step": 18910
    },
    {
      "epoch": 1.6817777777777778,
      "grad_norm": 0.42000317573547363,
      "learning_rate": 3.948888888888889e-05,
      "loss": 0.0021,
      "step": 18920
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.8102313280105591,
      "learning_rate": 3.9483333333333335e-05,
      "loss": 0.0026,
      "step": 18930
    },
    {
      "epoch": 1.6835555555555555,
      "grad_norm": 1.0972315073013306,
      "learning_rate": 3.947777777777778e-05,
      "loss": 0.0037,
      "step": 18940
    },
    {
      "epoch": 1.6844444444444444,
      "grad_norm": 0.5465729832649231,
      "learning_rate": 3.947222222222222e-05,
      "loss": 0.0034,
      "step": 18950
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.9843995571136475,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0025,
      "step": 18960
    },
    {
      "epoch": 1.6862222222222223,
      "grad_norm": 0.3633747100830078,
      "learning_rate": 3.9461111111111116e-05,
      "loss": 0.0027,
      "step": 18970
    },
    {
      "epoch": 1.6871111111111112,
      "grad_norm": 0.4461027681827545,
      "learning_rate": 3.945555555555556e-05,
      "loss": 0.004,
      "step": 18980
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.6938150525093079,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 0.003,
      "step": 18990
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.6096739768981934,
      "learning_rate": 3.944444444444445e-05,
      "loss": 0.0039,
      "step": 19000
    },
    {
      "epoch": 1.6897777777777778,
      "grad_norm": 0.8213498592376709,
      "learning_rate": 3.943888888888889e-05,
      "loss": 0.0045,
      "step": 19010
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.6145538687705994,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.0033,
      "step": 19020
    },
    {
      "epoch": 1.6915555555555555,
      "grad_norm": 0.19844622910022736,
      "learning_rate": 3.942777777777778e-05,
      "loss": 0.0028,
      "step": 19030
    },
    {
      "epoch": 1.6924444444444444,
      "grad_norm": 0.2066894918680191,
      "learning_rate": 3.942222222222222e-05,
      "loss": 0.0026,
      "step": 19040
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.7234973311424255,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.0026,
      "step": 19050
    },
    {
      "epoch": 1.6942222222222223,
      "grad_norm": 0.40347376465797424,
      "learning_rate": 3.9411111111111115e-05,
      "loss": 0.0025,
      "step": 19060
    },
    {
      "epoch": 1.6951111111111112,
      "grad_norm": 0.8412390947341919,
      "learning_rate": 3.940555555555556e-05,
      "loss": 0.003,
      "step": 19070
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.35411936044692993,
      "learning_rate": 3.94e-05,
      "loss": 0.0023,
      "step": 19080
    },
    {
      "epoch": 1.696888888888889,
      "grad_norm": 0.2659025192260742,
      "learning_rate": 3.9394444444444446e-05,
      "loss": 0.0038,
      "step": 19090
    },
    {
      "epoch": 1.6977777777777778,
      "grad_norm": 0.2561394274234772,
      "learning_rate": 3.938888888888889e-05,
      "loss": 0.0021,
      "step": 19100
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.5732905864715576,
      "learning_rate": 3.938333333333333e-05,
      "loss": 0.0021,
      "step": 19110
    },
    {
      "epoch": 1.6995555555555555,
      "grad_norm": 0.0763450637459755,
      "learning_rate": 3.937777777777778e-05,
      "loss": 0.002,
      "step": 19120
    },
    {
      "epoch": 1.7004444444444444,
      "grad_norm": 0.5356438755989075,
      "learning_rate": 3.937222222222222e-05,
      "loss": 0.0023,
      "step": 19130
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.21012915670871735,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.0029,
      "step": 19140
    },
    {
      "epoch": 1.7022222222222223,
      "grad_norm": 0.8819586634635925,
      "learning_rate": 3.9361111111111114e-05,
      "loss": 0.002,
      "step": 19150
    },
    {
      "epoch": 1.7031111111111112,
      "grad_norm": 0.2962215542793274,
      "learning_rate": 3.935555555555556e-05,
      "loss": 0.0025,
      "step": 19160
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.5019217133522034,
      "learning_rate": 3.935e-05,
      "loss": 0.0025,
      "step": 19170
    },
    {
      "epoch": 1.704888888888889,
      "grad_norm": 0.9947921633720398,
      "learning_rate": 3.9344444444444445e-05,
      "loss": 0.0031,
      "step": 19180
    },
    {
      "epoch": 1.7057777777777776,
      "grad_norm": 0.0872327983379364,
      "learning_rate": 3.9338888888888895e-05,
      "loss": 0.0037,
      "step": 19190
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 1.0288541316986084,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0034,
      "step": 19200
    },
    {
      "epoch": 1.7075555555555555,
      "grad_norm": 0.3504277765750885,
      "learning_rate": 3.932777777777778e-05,
      "loss": 0.0026,
      "step": 19210
    },
    {
      "epoch": 1.7084444444444444,
      "grad_norm": 0.08580509573221207,
      "learning_rate": 3.932222222222222e-05,
      "loss": 0.0026,
      "step": 19220
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.6617245674133301,
      "learning_rate": 3.931666666666667e-05,
      "loss": 0.0035,
      "step": 19230
    },
    {
      "epoch": 1.7102222222222223,
      "grad_norm": 0.4665943682193756,
      "learning_rate": 3.931111111111111e-05,
      "loss": 0.0016,
      "step": 19240
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.09586229175329208,
      "learning_rate": 3.9305555555555556e-05,
      "loss": 0.0025,
      "step": 19250
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.1339772492647171,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.0028,
      "step": 19260
    },
    {
      "epoch": 1.712888888888889,
      "grad_norm": 0.19627241790294647,
      "learning_rate": 3.929444444444444e-05,
      "loss": 0.0024,
      "step": 19270
    },
    {
      "epoch": 1.7137777777777776,
      "grad_norm": 0.6304749846458435,
      "learning_rate": 3.9288888888888894e-05,
      "loss": 0.0036,
      "step": 19280
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.6257396340370178,
      "learning_rate": 3.928333333333333e-05,
      "loss": 0.0032,
      "step": 19290
    },
    {
      "epoch": 1.7155555555555555,
      "grad_norm": 0.30855560302734375,
      "learning_rate": 3.927777777777778e-05,
      "loss": 0.0034,
      "step": 19300
    },
    {
      "epoch": 1.7164444444444444,
      "grad_norm": 0.5891593098640442,
      "learning_rate": 3.9272222222222224e-05,
      "loss": 0.0038,
      "step": 19310
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.36077743768692017,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0028,
      "step": 19320
    },
    {
      "epoch": 1.7182222222222223,
      "grad_norm": 1.115687608718872,
      "learning_rate": 3.926111111111112e-05,
      "loss": 0.0025,
      "step": 19330
    },
    {
      "epoch": 1.7191111111111113,
      "grad_norm": 0.08515238016843796,
      "learning_rate": 3.9255555555555555e-05,
      "loss": 0.0029,
      "step": 19340
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.49263647198677063,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.0031,
      "step": 19350
    },
    {
      "epoch": 1.720888888888889,
      "grad_norm": 0.40619340538978577,
      "learning_rate": 3.924444444444444e-05,
      "loss": 0.0029,
      "step": 19360
    },
    {
      "epoch": 1.7217777777777776,
      "grad_norm": 1.0647387504577637,
      "learning_rate": 3.923888888888889e-05,
      "loss": 0.0034,
      "step": 19370
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.5409652590751648,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0032,
      "step": 19380
    },
    {
      "epoch": 1.7235555555555555,
      "grad_norm": 0.15731996297836304,
      "learning_rate": 3.922777777777778e-05,
      "loss": 0.0031,
      "step": 19390
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 0.06904950737953186,
      "learning_rate": 3.922222222222223e-05,
      "loss": 0.0024,
      "step": 19400
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.09086893498897552,
      "learning_rate": 3.921666666666667e-05,
      "loss": 0.0027,
      "step": 19410
    },
    {
      "epoch": 1.7262222222222223,
      "grad_norm": 0.8394413590431213,
      "learning_rate": 3.921111111111112e-05,
      "loss": 0.0034,
      "step": 19420
    },
    {
      "epoch": 1.7271111111111113,
      "grad_norm": 0.6920890808105469,
      "learning_rate": 3.9205555555555554e-05,
      "loss": 0.0025,
      "step": 19430
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.2867964208126068,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0039,
      "step": 19440
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 0.8152486681938171,
      "learning_rate": 3.919444444444445e-05,
      "loss": 0.0028,
      "step": 19450
    },
    {
      "epoch": 1.7297777777777776,
      "grad_norm": 0.7101084589958191,
      "learning_rate": 3.918888888888889e-05,
      "loss": 0.0039,
      "step": 19460
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.7870397567749023,
      "learning_rate": 3.9183333333333335e-05,
      "loss": 0.0034,
      "step": 19470
    },
    {
      "epoch": 1.7315555555555555,
      "grad_norm": 0.5957415699958801,
      "learning_rate": 3.917777777777778e-05,
      "loss": 0.0028,
      "step": 19480
    },
    {
      "epoch": 1.7324444444444445,
      "grad_norm": 0.14843690395355225,
      "learning_rate": 3.917222222222223e-05,
      "loss": 0.0031,
      "step": 19490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.04569330811500549,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0026,
      "step": 19500
    },
    {
      "epoch": 1.7342222222222223,
      "grad_norm": 1.0672597885131836,
      "learning_rate": 3.9161111111111116e-05,
      "loss": 0.0028,
      "step": 19510
    },
    {
      "epoch": 1.7351111111111113,
      "grad_norm": 0.22117796540260315,
      "learning_rate": 3.915555555555556e-05,
      "loss": 0.0025,
      "step": 19520
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.30103200674057007,
      "learning_rate": 3.915e-05,
      "loss": 0.0021,
      "step": 19530
    },
    {
      "epoch": 1.736888888888889,
      "grad_norm": 0.3315656781196594,
      "learning_rate": 3.9144444444444446e-05,
      "loss": 0.0035,
      "step": 19540
    },
    {
      "epoch": 1.7377777777777776,
      "grad_norm": 0.9479209184646606,
      "learning_rate": 3.913888888888889e-05,
      "loss": 0.0024,
      "step": 19550
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.6722604632377625,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0017,
      "step": 19560
    },
    {
      "epoch": 1.7395555555555555,
      "grad_norm": 0.0538066029548645,
      "learning_rate": 3.912777777777778e-05,
      "loss": 0.0025,
      "step": 19570
    },
    {
      "epoch": 1.7404444444444445,
      "grad_norm": 0.7736749649047852,
      "learning_rate": 3.912222222222223e-05,
      "loss": 0.0037,
      "step": 19580
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.5482126474380493,
      "learning_rate": 3.911666666666667e-05,
      "loss": 0.0034,
      "step": 19590
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 0.2837845981121063,
      "learning_rate": 3.9111111111111115e-05,
      "loss": 0.0024,
      "step": 19600
    },
    {
      "epoch": 1.743111111111111,
      "grad_norm": 0.6686687469482422,
      "learning_rate": 3.910555555555556e-05,
      "loss": 0.0025,
      "step": 19610
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.9858134984970093,
      "learning_rate": 3.91e-05,
      "loss": 0.0032,
      "step": 19620
    },
    {
      "epoch": 1.744888888888889,
      "grad_norm": 0.7818857431411743,
      "learning_rate": 3.9094444444444445e-05,
      "loss": 0.0028,
      "step": 19630
    },
    {
      "epoch": 1.7457777777777777,
      "grad_norm": 0.4962243437767029,
      "learning_rate": 3.908888888888889e-05,
      "loss": 0.0032,
      "step": 19640
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 1.138633370399475,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.0028,
      "step": 19650
    },
    {
      "epoch": 1.7475555555555555,
      "grad_norm": 0.48718467354774475,
      "learning_rate": 3.907777777777778e-05,
      "loss": 0.0026,
      "step": 19660
    },
    {
      "epoch": 1.7484444444444445,
      "grad_norm": 0.9800884127616882,
      "learning_rate": 3.9072222222222226e-05,
      "loss": 0.0036,
      "step": 19670
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.12633438408374786,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0039,
      "step": 19680
    },
    {
      "epoch": 1.7502222222222223,
      "grad_norm": 0.6502202153205872,
      "learning_rate": 3.9061111111111113e-05,
      "loss": 0.0031,
      "step": 19690
    },
    {
      "epoch": 1.751111111111111,
      "grad_norm": 0.5055054426193237,
      "learning_rate": 3.905555555555556e-05,
      "loss": 0.0022,
      "step": 19700
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.5087365508079529,
      "learning_rate": 3.905e-05,
      "loss": 0.0029,
      "step": 19710
    },
    {
      "epoch": 1.752888888888889,
      "grad_norm": 1.0368326902389526,
      "learning_rate": 3.9044444444444444e-05,
      "loss": 0.0027,
      "step": 19720
    },
    {
      "epoch": 1.7537777777777777,
      "grad_norm": 0.4584604501724243,
      "learning_rate": 3.9038888888888894e-05,
      "loss": 0.0025,
      "step": 19730
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.24886178970336914,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0021,
      "step": 19740
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.46337372064590454,
      "learning_rate": 3.902777777777778e-05,
      "loss": 0.0026,
      "step": 19750
    },
    {
      "epoch": 1.7564444444444445,
      "grad_norm": 0.19266074895858765,
      "learning_rate": 3.9022222222222225e-05,
      "loss": 0.0037,
      "step": 19760
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.6197951436042786,
      "learning_rate": 3.901666666666667e-05,
      "loss": 0.0025,
      "step": 19770
    },
    {
      "epoch": 1.7582222222222224,
      "grad_norm": 0.6585773825645447,
      "learning_rate": 3.901111111111111e-05,
      "loss": 0.0022,
      "step": 19780
    },
    {
      "epoch": 1.759111111111111,
      "grad_norm": 0.7677445411682129,
      "learning_rate": 3.9005555555555556e-05,
      "loss": 0.0037,
      "step": 19790
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.27812084555625916,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0024,
      "step": 19800
    },
    {
      "epoch": 1.7608888888888887,
      "grad_norm": 0.8138808608055115,
      "learning_rate": 3.899444444444444e-05,
      "loss": 0.0028,
      "step": 19810
    },
    {
      "epoch": 1.7617777777777777,
      "grad_norm": 0.0807310938835144,
      "learning_rate": 3.898888888888889e-05,
      "loss": 0.0032,
      "step": 19820
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.2486518919467926,
      "learning_rate": 3.898333333333333e-05,
      "loss": 0.0034,
      "step": 19830
    },
    {
      "epoch": 1.7635555555555555,
      "grad_norm": 0.11541443318128586,
      "learning_rate": 3.897777777777778e-05,
      "loss": 0.0031,
      "step": 19840
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.4012141525745392,
      "learning_rate": 3.8972222222222224e-05,
      "loss": 0.0024,
      "step": 19850
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.2534642815589905,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.0033,
      "step": 19860
    },
    {
      "epoch": 1.7662222222222224,
      "grad_norm": 0.2854785919189453,
      "learning_rate": 3.896111111111112e-05,
      "loss": 0.0025,
      "step": 19870
    },
    {
      "epoch": 1.767111111111111,
      "grad_norm": 0.40414324402809143,
      "learning_rate": 3.8955555555555555e-05,
      "loss": 0.0017,
      "step": 19880
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.7935410141944885,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 0.0029,
      "step": 19890
    },
    {
      "epoch": 1.7688888888888887,
      "grad_norm": 0.49835801124572754,
      "learning_rate": 3.894444444444444e-05,
      "loss": 0.0024,
      "step": 19900
    },
    {
      "epoch": 1.7697777777777777,
      "grad_norm": 0.3531356751918793,
      "learning_rate": 3.893888888888889e-05,
      "loss": 0.0023,
      "step": 19910
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.6600006222724915,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0035,
      "step": 19920
    },
    {
      "epoch": 1.7715555555555556,
      "grad_norm": 0.13755294680595398,
      "learning_rate": 3.892777777777778e-05,
      "loss": 0.0023,
      "step": 19930
    },
    {
      "epoch": 1.7724444444444445,
      "grad_norm": 0.6168864369392395,
      "learning_rate": 3.892222222222223e-05,
      "loss": 0.0028,
      "step": 19940
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.2420092225074768,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.002,
      "step": 19950
    },
    {
      "epoch": 1.7742222222222224,
      "grad_norm": 0.269145131111145,
      "learning_rate": 3.8911111111111117e-05,
      "loss": 0.0025,
      "step": 19960
    },
    {
      "epoch": 1.775111111111111,
      "grad_norm": 0.44998419284820557,
      "learning_rate": 3.890555555555555e-05,
      "loss": 0.0028,
      "step": 19970
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.31292852759361267,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.0032,
      "step": 19980
    },
    {
      "epoch": 1.7768888888888887,
      "grad_norm": 0.3943708539009094,
      "learning_rate": 3.889444444444445e-05,
      "loss": 0.0026,
      "step": 19990
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.7679480314254761,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.0029,
      "step": 20000
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.859453558921814,
      "learning_rate": 3.8883333333333334e-05,
      "loss": 0.0017,
      "step": 20010
    },
    {
      "epoch": 1.7795555555555556,
      "grad_norm": 0.7526343464851379,
      "learning_rate": 3.887777777777778e-05,
      "loss": 0.003,
      "step": 20020
    },
    {
      "epoch": 1.7804444444444445,
      "grad_norm": 0.9253092408180237,
      "learning_rate": 3.887222222222223e-05,
      "loss": 0.0025,
      "step": 20030
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.7457359433174133,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0021,
      "step": 20040
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 0.08398372679948807,
      "learning_rate": 3.8861111111111115e-05,
      "loss": 0.004,
      "step": 20050
    },
    {
      "epoch": 1.783111111111111,
      "grad_norm": 0.30352693796157837,
      "learning_rate": 3.885555555555556e-05,
      "loss": 0.0033,
      "step": 20060
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.28210020065307617,
      "learning_rate": 3.885e-05,
      "loss": 0.0025,
      "step": 20070
    },
    {
      "epoch": 1.7848888888888887,
      "grad_norm": 0.2583859860897064,
      "learning_rate": 3.8844444444444446e-05,
      "loss": 0.0029,
      "step": 20080
    },
    {
      "epoch": 1.7857777777777777,
      "grad_norm": 0.18969914317131042,
      "learning_rate": 3.883888888888889e-05,
      "loss": 0.003,
      "step": 20090
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.5421042442321777,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0028,
      "step": 20100
    },
    {
      "epoch": 1.7875555555555556,
      "grad_norm": 0.3054184317588806,
      "learning_rate": 3.882777777777778e-05,
      "loss": 0.0024,
      "step": 20110
    },
    {
      "epoch": 1.7884444444444445,
      "grad_norm": 0.0892106294631958,
      "learning_rate": 3.882222222222223e-05,
      "loss": 0.0028,
      "step": 20120
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.2151060551404953,
      "learning_rate": 3.881666666666667e-05,
      "loss": 0.0023,
      "step": 20130
    },
    {
      "epoch": 1.7902222222222224,
      "grad_norm": 0.5587309002876282,
      "learning_rate": 3.8811111111111114e-05,
      "loss": 0.0022,
      "step": 20140
    },
    {
      "epoch": 1.791111111111111,
      "grad_norm": 1.0268335342407227,
      "learning_rate": 3.880555555555556e-05,
      "loss": 0.002,
      "step": 20150
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.5547610521316528,
      "learning_rate": 3.88e-05,
      "loss": 0.0025,
      "step": 20160
    },
    {
      "epoch": 1.7928888888888888,
      "grad_norm": 0.508693516254425,
      "learning_rate": 3.8794444444444445e-05,
      "loss": 0.0021,
      "step": 20170
    },
    {
      "epoch": 1.7937777777777777,
      "grad_norm": 0.15978315472602844,
      "learning_rate": 3.878888888888889e-05,
      "loss": 0.0025,
      "step": 20180
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.556144118309021,
      "learning_rate": 3.878333333333333e-05,
      "loss": 0.0036,
      "step": 20190
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 0.2880673408508301,
      "learning_rate": 3.877777777777778e-05,
      "loss": 0.003,
      "step": 20200
    },
    {
      "epoch": 1.7964444444444445,
      "grad_norm": 0.463779479265213,
      "learning_rate": 3.8772222222222226e-05,
      "loss": 0.002,
      "step": 20210
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.4497765898704529,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.0021,
      "step": 20220
    },
    {
      "epoch": 1.7982222222222224,
      "grad_norm": 0.6612105965614319,
      "learning_rate": 3.876111111111111e-05,
      "loss": 0.004,
      "step": 20230
    },
    {
      "epoch": 1.799111111111111,
      "grad_norm": 1.2225626707077026,
      "learning_rate": 3.8755555555555556e-05,
      "loss": 0.003,
      "step": 20240
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.2752310335636139,
      "learning_rate": 3.875e-05,
      "loss": 0.0022,
      "step": 20250
    },
    {
      "epoch": 1.8008888888888888,
      "grad_norm": 0.6682056188583374,
      "learning_rate": 3.8744444444444444e-05,
      "loss": 0.0027,
      "step": 20260
    },
    {
      "epoch": 1.8017777777777777,
      "grad_norm": 0.7358304262161255,
      "learning_rate": 3.8738888888888894e-05,
      "loss": 0.003,
      "step": 20270
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.6326385736465454,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.003,
      "step": 20280
    },
    {
      "epoch": 1.8035555555555556,
      "grad_norm": 0.44575589895248413,
      "learning_rate": 3.872777777777778e-05,
      "loss": 0.0023,
      "step": 20290
    },
    {
      "epoch": 1.8044444444444445,
      "grad_norm": 0.5298676490783691,
      "learning_rate": 3.8722222222222225e-05,
      "loss": 0.0031,
      "step": 20300
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.18025662004947662,
      "learning_rate": 3.871666666666667e-05,
      "loss": 0.002,
      "step": 20310
    },
    {
      "epoch": 1.8062222222222222,
      "grad_norm": 0.7262458801269531,
      "learning_rate": 3.871111111111111e-05,
      "loss": 0.0038,
      "step": 20320
    },
    {
      "epoch": 1.8071111111111111,
      "grad_norm": 0.5161333084106445,
      "learning_rate": 3.8705555555555555e-05,
      "loss": 0.0034,
      "step": 20330
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.4687649607658386,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0024,
      "step": 20340
    },
    {
      "epoch": 1.8088888888888888,
      "grad_norm": 0.7223092317581177,
      "learning_rate": 3.869444444444444e-05,
      "loss": 0.0029,
      "step": 20350
    },
    {
      "epoch": 1.8097777777777777,
      "grad_norm": 0.8517963290214539,
      "learning_rate": 3.868888888888889e-05,
      "loss": 0.0031,
      "step": 20360
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.14955294132232666,
      "learning_rate": 3.868333333333333e-05,
      "loss": 0.0026,
      "step": 20370
    },
    {
      "epoch": 1.8115555555555556,
      "grad_norm": 0.6013640761375427,
      "learning_rate": 3.867777777777778e-05,
      "loss": 0.0025,
      "step": 20380
    },
    {
      "epoch": 1.8124444444444445,
      "grad_norm": 0.42973822355270386,
      "learning_rate": 3.867222222222222e-05,
      "loss": 0.0029,
      "step": 20390
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.24232330918312073,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.003,
      "step": 20400
    },
    {
      "epoch": 1.8142222222222222,
      "grad_norm": 0.3370852470397949,
      "learning_rate": 3.866111111111112e-05,
      "loss": 0.0016,
      "step": 20410
    },
    {
      "epoch": 1.8151111111111111,
      "grad_norm": 0.22855247557163239,
      "learning_rate": 3.8655555555555554e-05,
      "loss": 0.0029,
      "step": 20420
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.34512096643447876,
      "learning_rate": 3.8650000000000004e-05,
      "loss": 0.0017,
      "step": 20430
    },
    {
      "epoch": 1.8168888888888888,
      "grad_norm": 0.5483200550079346,
      "learning_rate": 3.864444444444444e-05,
      "loss": 0.0023,
      "step": 20440
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 1.0369913578033447,
      "learning_rate": 3.863888888888889e-05,
      "loss": 0.0032,
      "step": 20450
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.5666232109069824,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.002,
      "step": 20460
    },
    {
      "epoch": 1.8195555555555556,
      "grad_norm": 0.7476512789726257,
      "learning_rate": 3.862777777777778e-05,
      "loss": 0.0036,
      "step": 20470
    },
    {
      "epoch": 1.8204444444444445,
      "grad_norm": 1.1220842599868774,
      "learning_rate": 3.862222222222223e-05,
      "loss": 0.0031,
      "step": 20480
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.8038340210914612,
      "learning_rate": 3.8616666666666666e-05,
      "loss": 0.0029,
      "step": 20490
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.8583658933639526,
      "learning_rate": 3.8611111111111116e-05,
      "loss": 0.0031,
      "step": 20500
    },
    {
      "epoch": 1.8231111111111111,
      "grad_norm": 0.5733579397201538,
      "learning_rate": 3.860555555555555e-05,
      "loss": 0.0028,
      "step": 20510
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.17243646085262299,
      "learning_rate": 3.86e-05,
      "loss": 0.0022,
      "step": 20520
    },
    {
      "epoch": 1.8248888888888888,
      "grad_norm": 0.13708576560020447,
      "learning_rate": 3.859444444444445e-05,
      "loss": 0.0026,
      "step": 20530
    },
    {
      "epoch": 1.8257777777777777,
      "grad_norm": 0.2718849778175354,
      "learning_rate": 3.858888888888889e-05,
      "loss": 0.0025,
      "step": 20540
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.30373185873031616,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.0017,
      "step": 20550
    },
    {
      "epoch": 1.8275555555555556,
      "grad_norm": 0.32684004306793213,
      "learning_rate": 3.857777777777778e-05,
      "loss": 0.0027,
      "step": 20560
    },
    {
      "epoch": 1.8284444444444445,
      "grad_norm": 0.3286686837673187,
      "learning_rate": 3.857222222222223e-05,
      "loss": 0.003,
      "step": 20570
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.20922648906707764,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.0035,
      "step": 20580
    },
    {
      "epoch": 1.8302222222222222,
      "grad_norm": 0.4982403516769409,
      "learning_rate": 3.8561111111111115e-05,
      "loss": 0.0023,
      "step": 20590
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 0.12721428275108337,
      "learning_rate": 3.855555555555556e-05,
      "loss": 0.0025,
      "step": 20600
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.4038909077644348,
      "learning_rate": 3.855e-05,
      "loss": 0.0022,
      "step": 20610
    },
    {
      "epoch": 1.8328888888888888,
      "grad_norm": 0.0914357528090477,
      "learning_rate": 3.8544444444444445e-05,
      "loss": 0.0022,
      "step": 20620
    },
    {
      "epoch": 1.8337777777777777,
      "grad_norm": 0.22895066440105438,
      "learning_rate": 3.853888888888889e-05,
      "loss": 0.0038,
      "step": 20630
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.33215656876564026,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0041,
      "step": 20640
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 0.92790287733078,
      "learning_rate": 3.8527777777777776e-05,
      "loss": 0.0042,
      "step": 20650
    },
    {
      "epoch": 1.8364444444444445,
      "grad_norm": 0.7054857015609741,
      "learning_rate": 3.8522222222222226e-05,
      "loss": 0.0034,
      "step": 20660
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.33479630947113037,
      "learning_rate": 3.851666666666667e-05,
      "loss": 0.003,
      "step": 20670
    },
    {
      "epoch": 1.8382222222222222,
      "grad_norm": 0.5652902722358704,
      "learning_rate": 3.8511111111111114e-05,
      "loss": 0.0033,
      "step": 20680
    },
    {
      "epoch": 1.8391111111111111,
      "grad_norm": 0.5058016180992126,
      "learning_rate": 3.850555555555556e-05,
      "loss": 0.002,
      "step": 20690
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.1917574405670166,
      "learning_rate": 3.85e-05,
      "loss": 0.0026,
      "step": 20700
    },
    {
      "epoch": 1.8408888888888888,
      "grad_norm": 0.14328771829605103,
      "learning_rate": 3.8494444444444444e-05,
      "loss": 0.0028,
      "step": 20710
    },
    {
      "epoch": 1.8417777777777777,
      "grad_norm": 1.1994035243988037,
      "learning_rate": 3.848888888888889e-05,
      "loss": 0.0036,
      "step": 20720
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.0721903070807457,
      "learning_rate": 3.848333333333334e-05,
      "loss": 0.0027,
      "step": 20730
    },
    {
      "epoch": 1.8435555555555556,
      "grad_norm": 0.6718570590019226,
      "learning_rate": 3.847777777777778e-05,
      "loss": 0.0034,
      "step": 20740
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 0.31877970695495605,
      "learning_rate": 3.8472222222222225e-05,
      "loss": 0.0028,
      "step": 20750
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.13984428346157074,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0031,
      "step": 20760
    },
    {
      "epoch": 1.8462222222222222,
      "grad_norm": 0.20039236545562744,
      "learning_rate": 3.846111111111111e-05,
      "loss": 0.0024,
      "step": 20770
    },
    {
      "epoch": 1.8471111111111111,
      "grad_norm": 0.722929835319519,
      "learning_rate": 3.8455555555555556e-05,
      "loss": 0.0029,
      "step": 20780
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.07637781649827957,
      "learning_rate": 3.845e-05,
      "loss": 0.0027,
      "step": 20790
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 0.5715761184692383,
      "learning_rate": 3.844444444444444e-05,
      "loss": 0.0025,
      "step": 20800
    },
    {
      "epoch": 1.8497777777777777,
      "grad_norm": 0.29344290494918823,
      "learning_rate": 3.843888888888889e-05,
      "loss": 0.0023,
      "step": 20810
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 1.2226468324661255,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.003,
      "step": 20820
    },
    {
      "epoch": 1.8515555555555556,
      "grad_norm": 0.6874884963035583,
      "learning_rate": 3.842777777777778e-05,
      "loss": 0.0028,
      "step": 20830
    },
    {
      "epoch": 1.8524444444444446,
      "grad_norm": 0.7085257768630981,
      "learning_rate": 3.8422222222222224e-05,
      "loss": 0.0018,
      "step": 20840
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.10138361155986786,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.002,
      "step": 20850
    },
    {
      "epoch": 1.8542222222222222,
      "grad_norm": 0.23435752093791962,
      "learning_rate": 3.841111111111111e-05,
      "loss": 0.0024,
      "step": 20860
    },
    {
      "epoch": 1.8551111111111112,
      "grad_norm": 1.143859624862671,
      "learning_rate": 3.8405555555555555e-05,
      "loss": 0.0028,
      "step": 20870
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.42759495973587036,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0034,
      "step": 20880
    },
    {
      "epoch": 1.8568888888888888,
      "grad_norm": 0.4057125747203827,
      "learning_rate": 3.839444444444444e-05,
      "loss": 0.0035,
      "step": 20890
    },
    {
      "epoch": 1.8577777777777778,
      "grad_norm": 0.20357954502105713,
      "learning_rate": 3.838888888888889e-05,
      "loss": 0.0026,
      "step": 20900
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.2497362494468689,
      "learning_rate": 3.8383333333333336e-05,
      "loss": 0.0032,
      "step": 20910
    },
    {
      "epoch": 1.8595555555555556,
      "grad_norm": 1.206766128540039,
      "learning_rate": 3.837777777777778e-05,
      "loss": 0.003,
      "step": 20920
    },
    {
      "epoch": 1.8604444444444446,
      "grad_norm": 0.06495555490255356,
      "learning_rate": 3.837222222222222e-05,
      "loss": 0.0025,
      "step": 20930
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.1787097454071045,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.0024,
      "step": 20940
    },
    {
      "epoch": 1.8622222222222222,
      "grad_norm": 0.9231168031692505,
      "learning_rate": 3.836111111111112e-05,
      "loss": 0.003,
      "step": 20950
    },
    {
      "epoch": 1.8631111111111112,
      "grad_norm": 0.6331977844238281,
      "learning_rate": 3.8355555555555553e-05,
      "loss": 0.002,
      "step": 20960
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.1536993533372879,
      "learning_rate": 3.8350000000000004e-05,
      "loss": 0.0023,
      "step": 20970
    },
    {
      "epoch": 1.8648888888888888,
      "grad_norm": 0.5782933831214905,
      "learning_rate": 3.834444444444444e-05,
      "loss": 0.0027,
      "step": 20980
    },
    {
      "epoch": 1.8657777777777778,
      "grad_norm": 0.37988340854644775,
      "learning_rate": 3.833888888888889e-05,
      "loss": 0.0041,
      "step": 20990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.09773265570402145,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0018,
      "step": 21000
    },
    {
      "epoch": 1.8675555555555556,
      "grad_norm": 0.6276535391807556,
      "learning_rate": 3.832777777777778e-05,
      "loss": 0.0027,
      "step": 21010
    },
    {
      "epoch": 1.8684444444444446,
      "grad_norm": 0.7628580331802368,
      "learning_rate": 3.832222222222223e-05,
      "loss": 0.0025,
      "step": 21020
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.28348681330680847,
      "learning_rate": 3.8316666666666665e-05,
      "loss": 0.0029,
      "step": 21030
    },
    {
      "epoch": 1.8702222222222222,
      "grad_norm": 0.7260929942131042,
      "learning_rate": 3.8311111111111115e-05,
      "loss": 0.0024,
      "step": 21040
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 0.6550984382629395,
      "learning_rate": 3.830555555555555e-05,
      "loss": 0.0032,
      "step": 21050
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.20766159892082214,
      "learning_rate": 3.83e-05,
      "loss": 0.0031,
      "step": 21060
    },
    {
      "epoch": 1.8728888888888888,
      "grad_norm": 0.21290966868400574,
      "learning_rate": 3.8294444444444446e-05,
      "loss": 0.0034,
      "step": 21070
    },
    {
      "epoch": 1.8737777777777778,
      "grad_norm": 0.07797568291425705,
      "learning_rate": 3.828888888888889e-05,
      "loss": 0.0027,
      "step": 21080
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.5315602421760559,
      "learning_rate": 3.828333333333334e-05,
      "loss": 0.0034,
      "step": 21090
    },
    {
      "epoch": 1.8755555555555556,
      "grad_norm": 0.2669377326965332,
      "learning_rate": 3.827777777777778e-05,
      "loss": 0.0028,
      "step": 21100
    },
    {
      "epoch": 1.8764444444444446,
      "grad_norm": 0.9324195981025696,
      "learning_rate": 3.827222222222223e-05,
      "loss": 0.0044,
      "step": 21110
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.22303995490074158,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0024,
      "step": 21120
    },
    {
      "epoch": 1.8782222222222222,
      "grad_norm": 0.3792979121208191,
      "learning_rate": 3.8261111111111114e-05,
      "loss": 0.0028,
      "step": 21130
    },
    {
      "epoch": 1.879111111111111,
      "grad_norm": 0.2778542935848236,
      "learning_rate": 3.825555555555556e-05,
      "loss": 0.0033,
      "step": 21140
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.2223639339208603,
      "learning_rate": 3.825e-05,
      "loss": 0.0016,
      "step": 21150
    },
    {
      "epoch": 1.8808888888888888,
      "grad_norm": 0.7713721990585327,
      "learning_rate": 3.8244444444444445e-05,
      "loss": 0.0023,
      "step": 21160
    },
    {
      "epoch": 1.8817777777777778,
      "grad_norm": 0.5216691493988037,
      "learning_rate": 3.823888888888889e-05,
      "loss": 0.0028,
      "step": 21170
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.2585545778274536,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.002,
      "step": 21180
    },
    {
      "epoch": 1.8835555555555556,
      "grad_norm": 0.06857051700353622,
      "learning_rate": 3.822777777777778e-05,
      "loss": 0.0015,
      "step": 21190
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 0.29211726784706116,
      "learning_rate": 3.8222222222222226e-05,
      "loss": 0.0023,
      "step": 21200
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.672982931137085,
      "learning_rate": 3.821666666666667e-05,
      "loss": 0.0028,
      "step": 21210
    },
    {
      "epoch": 1.8862222222222222,
      "grad_norm": 0.4535830616950989,
      "learning_rate": 3.821111111111111e-05,
      "loss": 0.002,
      "step": 21220
    },
    {
      "epoch": 1.887111111111111,
      "grad_norm": 0.4119339883327484,
      "learning_rate": 3.820555555555556e-05,
      "loss": 0.0021,
      "step": 21230
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.7346807718276978,
      "learning_rate": 3.82e-05,
      "loss": 0.0021,
      "step": 21240
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.12374043464660645,
      "learning_rate": 3.8194444444444444e-05,
      "loss": 0.0022,
      "step": 21250
    },
    {
      "epoch": 1.8897777777777778,
      "grad_norm": 0.49831199645996094,
      "learning_rate": 3.8188888888888894e-05,
      "loss": 0.0031,
      "step": 21260
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.2532663941383362,
      "learning_rate": 3.818333333333334e-05,
      "loss": 0.0021,
      "step": 21270
    },
    {
      "epoch": 1.8915555555555557,
      "grad_norm": 0.3397165536880493,
      "learning_rate": 3.817777777777778e-05,
      "loss": 0.002,
      "step": 21280
    },
    {
      "epoch": 1.8924444444444446,
      "grad_norm": 0.44544655084609985,
      "learning_rate": 3.8172222222222225e-05,
      "loss": 0.0031,
      "step": 21290
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.2847834825515747,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0017,
      "step": 21300
    },
    {
      "epoch": 1.8942222222222223,
      "grad_norm": 0.2724761664867401,
      "learning_rate": 3.816111111111111e-05,
      "loss": 0.0016,
      "step": 21310
    },
    {
      "epoch": 1.895111111111111,
      "grad_norm": 0.5392383933067322,
      "learning_rate": 3.8155555555555555e-05,
      "loss": 0.0036,
      "step": 21320
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.42936205863952637,
      "learning_rate": 3.8150000000000006e-05,
      "loss": 0.0027,
      "step": 21330
    },
    {
      "epoch": 1.8968888888888888,
      "grad_norm": 0.2003384530544281,
      "learning_rate": 3.814444444444444e-05,
      "loss": 0.004,
      "step": 21340
    },
    {
      "epoch": 1.8977777777777778,
      "grad_norm": 0.16662921011447906,
      "learning_rate": 3.813888888888889e-05,
      "loss": 0.0019,
      "step": 21350
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.12886637449264526,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0026,
      "step": 21360
    },
    {
      "epoch": 1.8995555555555557,
      "grad_norm": 0.4612993896007538,
      "learning_rate": 3.812777777777778e-05,
      "loss": 0.0026,
      "step": 21370
    },
    {
      "epoch": 1.9004444444444446,
      "grad_norm": 0.48019471764564514,
      "learning_rate": 3.8122222222222224e-05,
      "loss": 0.0033,
      "step": 21380
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.19050434231758118,
      "learning_rate": 3.811666666666667e-05,
      "loss": 0.0025,
      "step": 21390
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 0.6427842974662781,
      "learning_rate": 3.811111111111112e-05,
      "loss": 0.0022,
      "step": 21400
    },
    {
      "epoch": 1.903111111111111,
      "grad_norm": 0.7515749931335449,
      "learning_rate": 3.8105555555555554e-05,
      "loss": 0.0029,
      "step": 21410
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.4477149248123169,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.0024,
      "step": 21420
    },
    {
      "epoch": 1.9048888888888889,
      "grad_norm": 0.6567169427871704,
      "learning_rate": 3.809444444444444e-05,
      "loss": 0.0026,
      "step": 21430
    },
    {
      "epoch": 1.9057777777777778,
      "grad_norm": 0.7939158082008362,
      "learning_rate": 3.808888888888889e-05,
      "loss": 0.0027,
      "step": 21440
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.25804996490478516,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.0027,
      "step": 21450
    },
    {
      "epoch": 1.9075555555555557,
      "grad_norm": 0.7067450284957886,
      "learning_rate": 3.807777777777778e-05,
      "loss": 0.0017,
      "step": 21460
    },
    {
      "epoch": 1.9084444444444446,
      "grad_norm": 0.2576099932193756,
      "learning_rate": 3.807222222222223e-05,
      "loss": 0.0036,
      "step": 21470
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.04617412015795708,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0032,
      "step": 21480
    },
    {
      "epoch": 1.9102222222222223,
      "grad_norm": 0.2581614851951599,
      "learning_rate": 3.8061111111111116e-05,
      "loss": 0.0015,
      "step": 21490
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.10171770304441452,
      "learning_rate": 3.805555555555555e-05,
      "loss": 0.0028,
      "step": 21500
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.30896955728530884,
      "learning_rate": 3.805e-05,
      "loss": 0.0036,
      "step": 21510
    },
    {
      "epoch": 1.9128888888888889,
      "grad_norm": 1.0265625715255737,
      "learning_rate": 3.804444444444445e-05,
      "loss": 0.0027,
      "step": 21520
    },
    {
      "epoch": 1.9137777777777778,
      "grad_norm": 1.02034592628479,
      "learning_rate": 3.803888888888889e-05,
      "loss": 0.0026,
      "step": 21530
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.3527199625968933,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0034,
      "step": 21540
    },
    {
      "epoch": 1.9155555555555557,
      "grad_norm": 0.5368516445159912,
      "learning_rate": 3.802777777777778e-05,
      "loss": 0.0023,
      "step": 21550
    },
    {
      "epoch": 1.9164444444444444,
      "grad_norm": 0.8459900617599487,
      "learning_rate": 3.802222222222223e-05,
      "loss": 0.0026,
      "step": 21560
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.5692748427391052,
      "learning_rate": 3.8016666666666665e-05,
      "loss": 0.0021,
      "step": 21570
    },
    {
      "epoch": 1.9182222222222223,
      "grad_norm": 0.500357449054718,
      "learning_rate": 3.8011111111111115e-05,
      "loss": 0.0035,
      "step": 21580
    },
    {
      "epoch": 1.919111111111111,
      "grad_norm": 1.0065251588821411,
      "learning_rate": 3.800555555555556e-05,
      "loss": 0.0034,
      "step": 21590
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.11501120775938034,
      "learning_rate": 3.8e-05,
      "loss": 0.0028,
      "step": 21600
    },
    {
      "epoch": 1.9208888888888889,
      "grad_norm": 0.6168927550315857,
      "learning_rate": 3.7994444444444446e-05,
      "loss": 0.0016,
      "step": 21610
    },
    {
      "epoch": 1.9217777777777778,
      "grad_norm": 0.4226002097129822,
      "learning_rate": 3.798888888888889e-05,
      "loss": 0.003,
      "step": 21620
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.431471049785614,
      "learning_rate": 3.798333333333334e-05,
      "loss": 0.0023,
      "step": 21630
    },
    {
      "epoch": 1.9235555555555557,
      "grad_norm": 0.8233258724212646,
      "learning_rate": 3.7977777777777776e-05,
      "loss": 0.0032,
      "step": 21640
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 0.05779274180531502,
      "learning_rate": 3.797222222222223e-05,
      "loss": 0.0027,
      "step": 21650
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.9589300155639648,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.0035,
      "step": 21660
    },
    {
      "epoch": 1.926222222222222,
      "grad_norm": 0.3105130195617676,
      "learning_rate": 3.7961111111111114e-05,
      "loss": 0.0018,
      "step": 21670
    },
    {
      "epoch": 1.927111111111111,
      "grad_norm": 0.2284887284040451,
      "learning_rate": 3.795555555555556e-05,
      "loss": 0.0026,
      "step": 21680
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.28173741698265076,
      "learning_rate": 3.795e-05,
      "loss": 0.0025,
      "step": 21690
    },
    {
      "epoch": 1.9288888888888889,
      "grad_norm": 0.8268598914146423,
      "learning_rate": 3.7944444444444444e-05,
      "loss": 0.0025,
      "step": 21700
    },
    {
      "epoch": 1.9297777777777778,
      "grad_norm": 1.1981573104858398,
      "learning_rate": 3.793888888888889e-05,
      "loss": 0.0034,
      "step": 21710
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.50274658203125,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.002,
      "step": 21720
    },
    {
      "epoch": 1.9315555555555557,
      "grad_norm": 0.27159497141838074,
      "learning_rate": 3.792777777777778e-05,
      "loss": 0.0031,
      "step": 21730
    },
    {
      "epoch": 1.9324444444444444,
      "grad_norm": 0.30718740820884705,
      "learning_rate": 3.7922222222222225e-05,
      "loss": 0.0034,
      "step": 21740
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.6465480923652649,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.0021,
      "step": 21750
    },
    {
      "epoch": 1.934222222222222,
      "grad_norm": 0.1938294619321823,
      "learning_rate": 3.791111111111111e-05,
      "loss": 0.0018,
      "step": 21760
    },
    {
      "epoch": 1.935111111111111,
      "grad_norm": 0.19189362227916718,
      "learning_rate": 3.7905555555555556e-05,
      "loss": 0.0036,
      "step": 21770
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.8678401708602905,
      "learning_rate": 3.79e-05,
      "loss": 0.0039,
      "step": 21780
    },
    {
      "epoch": 1.9368888888888889,
      "grad_norm": 0.5008144974708557,
      "learning_rate": 3.789444444444444e-05,
      "loss": 0.0027,
      "step": 21790
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 0.22472360730171204,
      "learning_rate": 3.7888888888888894e-05,
      "loss": 0.0023,
      "step": 21800
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.7468168139457703,
      "learning_rate": 3.788333333333334e-05,
      "loss": 0.0024,
      "step": 21810
    },
    {
      "epoch": 1.9395555555555557,
      "grad_norm": 0.8361899852752686,
      "learning_rate": 3.787777777777778e-05,
      "loss": 0.0016,
      "step": 21820
    },
    {
      "epoch": 1.9404444444444444,
      "grad_norm": 0.9150069355964661,
      "learning_rate": 3.7872222222222224e-05,
      "loss": 0.002,
      "step": 21830
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.6432503461837769,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0034,
      "step": 21840
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 0.6476163864135742,
      "learning_rate": 3.786111111111111e-05,
      "loss": 0.0038,
      "step": 21850
    },
    {
      "epoch": 1.943111111111111,
      "grad_norm": 0.3857852816581726,
      "learning_rate": 3.7855555555555555e-05,
      "loss": 0.0024,
      "step": 21860
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.22395603358745575,
      "learning_rate": 3.7850000000000005e-05,
      "loss": 0.0022,
      "step": 21870
    },
    {
      "epoch": 1.944888888888889,
      "grad_norm": 0.36994171142578125,
      "learning_rate": 3.784444444444445e-05,
      "loss": 0.0021,
      "step": 21880
    },
    {
      "epoch": 1.9457777777777778,
      "grad_norm": 0.2848309874534607,
      "learning_rate": 3.783888888888889e-05,
      "loss": 0.0024,
      "step": 21890
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.4882255494594574,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0025,
      "step": 21900
    },
    {
      "epoch": 1.9475555555555557,
      "grad_norm": 0.25810685753822327,
      "learning_rate": 3.782777777777778e-05,
      "loss": 0.0019,
      "step": 21910
    },
    {
      "epoch": 1.9484444444444444,
      "grad_norm": 1.0918525457382202,
      "learning_rate": 3.782222222222222e-05,
      "loss": 0.0026,
      "step": 21920
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.20956069231033325,
      "learning_rate": 3.7816666666666667e-05,
      "loss": 0.0022,
      "step": 21930
    },
    {
      "epoch": 1.950222222222222,
      "grad_norm": 0.7140217423439026,
      "learning_rate": 3.781111111111112e-05,
      "loss": 0.0025,
      "step": 21940
    },
    {
      "epoch": 1.951111111111111,
      "grad_norm": 0.6200425028800964,
      "learning_rate": 3.7805555555555554e-05,
      "loss": 0.0026,
      "step": 21950
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.3809454143047333,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0023,
      "step": 21960
    },
    {
      "epoch": 1.952888888888889,
      "grad_norm": 1.1730332374572754,
      "learning_rate": 3.779444444444445e-05,
      "loss": 0.0031,
      "step": 21970
    },
    {
      "epoch": 1.9537777777777778,
      "grad_norm": 0.5368393659591675,
      "learning_rate": 3.778888888888889e-05,
      "loss": 0.003,
      "step": 21980
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.029470307752490044,
      "learning_rate": 3.7783333333333335e-05,
      "loss": 0.0021,
      "step": 21990
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.5407781600952148,
      "learning_rate": 3.777777777777778e-05,
      "loss": 0.002,
      "step": 22000
    },
    {
      "epoch": 1.9564444444444444,
      "grad_norm": 0.13788267970085144,
      "learning_rate": 3.777222222222223e-05,
      "loss": 0.0029,
      "step": 22010
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.3573956787586212,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0028,
      "step": 22020
    },
    {
      "epoch": 1.958222222222222,
      "grad_norm": 0.3946843147277832,
      "learning_rate": 3.7761111111111116e-05,
      "loss": 0.0025,
      "step": 22030
    },
    {
      "epoch": 1.959111111111111,
      "grad_norm": 0.4899592995643616,
      "learning_rate": 3.775555555555555e-05,
      "loss": 0.0021,
      "step": 22040
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.6522504687309265,
      "learning_rate": 3.775e-05,
      "loss": 0.0019,
      "step": 22050
    },
    {
      "epoch": 1.960888888888889,
      "grad_norm": 1.3087396621704102,
      "learning_rate": 3.7744444444444446e-05,
      "loss": 0.0035,
      "step": 22060
    },
    {
      "epoch": 1.9617777777777778,
      "grad_norm": 0.056661684066057205,
      "learning_rate": 3.773888888888889e-05,
      "loss": 0.0024,
      "step": 22070
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.28516480326652527,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0024,
      "step": 22080
    },
    {
      "epoch": 1.9635555555555557,
      "grad_norm": 0.1342923641204834,
      "learning_rate": 3.772777777777778e-05,
      "loss": 0.0034,
      "step": 22090
    },
    {
      "epoch": 1.9644444444444444,
      "grad_norm": 0.30414602160453796,
      "learning_rate": 3.772222222222223e-05,
      "loss": 0.0042,
      "step": 22100
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.6870657801628113,
      "learning_rate": 3.7716666666666664e-05,
      "loss": 0.0037,
      "step": 22110
    },
    {
      "epoch": 1.966222222222222,
      "grad_norm": 1.105155110359192,
      "learning_rate": 3.7711111111111114e-05,
      "loss": 0.0028,
      "step": 22120
    },
    {
      "epoch": 1.967111111111111,
      "grad_norm": 0.213579922914505,
      "learning_rate": 3.770555555555556e-05,
      "loss": 0.0031,
      "step": 22130
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.33640000224113464,
      "learning_rate": 3.77e-05,
      "loss": 0.0024,
      "step": 22140
    },
    {
      "epoch": 1.968888888888889,
      "grad_norm": 0.28515830636024475,
      "learning_rate": 3.769444444444445e-05,
      "loss": 0.0029,
      "step": 22150
    },
    {
      "epoch": 1.9697777777777778,
      "grad_norm": 0.45115166902542114,
      "learning_rate": 3.768888888888889e-05,
      "loss": 0.0027,
      "step": 22160
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.052703674882650375,
      "learning_rate": 3.768333333333334e-05,
      "loss": 0.002,
      "step": 22170
    },
    {
      "epoch": 1.9715555555555555,
      "grad_norm": 0.6877421736717224,
      "learning_rate": 3.7677777777777776e-05,
      "loss": 0.0024,
      "step": 22180
    },
    {
      "epoch": 1.9724444444444444,
      "grad_norm": 0.09408661723136902,
      "learning_rate": 3.7672222222222226e-05,
      "loss": 0.002,
      "step": 22190
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.6041784882545471,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0022,
      "step": 22200
    },
    {
      "epoch": 1.974222222222222,
      "grad_norm": 0.18459990620613098,
      "learning_rate": 3.766111111111111e-05,
      "loss": 0.0028,
      "step": 22210
    },
    {
      "epoch": 1.975111111111111,
      "grad_norm": 0.347671777009964,
      "learning_rate": 3.765555555555556e-05,
      "loss": 0.0022,
      "step": 22220
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.577767014503479,
      "learning_rate": 3.765e-05,
      "loss": 0.0021,
      "step": 22230
    },
    {
      "epoch": 1.976888888888889,
      "grad_norm": 0.34270182251930237,
      "learning_rate": 3.764444444444445e-05,
      "loss": 0.0035,
      "step": 22240
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.7942705154418945,
      "learning_rate": 3.763888888888889e-05,
      "loss": 0.0031,
      "step": 22250
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.32223185896873474,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0033,
      "step": 22260
    },
    {
      "epoch": 1.9795555555555555,
      "grad_norm": 0.3480151891708374,
      "learning_rate": 3.762777777777778e-05,
      "loss": 0.0023,
      "step": 22270
    },
    {
      "epoch": 1.9804444444444445,
      "grad_norm": 0.6673806309700012,
      "learning_rate": 3.7622222222222225e-05,
      "loss": 0.0032,
      "step": 22280
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.08739151060581207,
      "learning_rate": 3.761666666666667e-05,
      "loss": 0.0025,
      "step": 22290
    },
    {
      "epoch": 1.982222222222222,
      "grad_norm": 0.6865929961204529,
      "learning_rate": 3.761111111111111e-05,
      "loss": 0.002,
      "step": 22300
    },
    {
      "epoch": 1.983111111111111,
      "grad_norm": 0.29777923226356506,
      "learning_rate": 3.7605555555555556e-05,
      "loss": 0.0035,
      "step": 22310
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.3115342855453491,
      "learning_rate": 3.76e-05,
      "loss": 0.0034,
      "step": 22320
    },
    {
      "epoch": 1.984888888888889,
      "grad_norm": 0.26917582750320435,
      "learning_rate": 3.759444444444445e-05,
      "loss": 0.0022,
      "step": 22330
    },
    {
      "epoch": 1.9857777777777779,
      "grad_norm": 0.9926349520683289,
      "learning_rate": 3.758888888888889e-05,
      "loss": 0.0038,
      "step": 22340
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.511725902557373,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.003,
      "step": 22350
    },
    {
      "epoch": 1.9875555555555555,
      "grad_norm": 0.6593552827835083,
      "learning_rate": 3.757777777777778e-05,
      "loss": 0.0032,
      "step": 22360
    },
    {
      "epoch": 1.9884444444444445,
      "grad_norm": 0.6173770427703857,
      "learning_rate": 3.7572222222222224e-05,
      "loss": 0.0023,
      "step": 22370
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.8552441596984863,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.0022,
      "step": 22380
    },
    {
      "epoch": 1.9902222222222221,
      "grad_norm": 0.22931966185569763,
      "learning_rate": 3.756111111111111e-05,
      "loss": 0.0025,
      "step": 22390
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 0.5000203847885132,
      "learning_rate": 3.7555555555555554e-05,
      "loss": 0.0026,
      "step": 22400
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.3771000802516937,
      "learning_rate": 3.7550000000000005e-05,
      "loss": 0.0019,
      "step": 22410
    },
    {
      "epoch": 1.992888888888889,
      "grad_norm": 1.198878288269043,
      "learning_rate": 3.754444444444445e-05,
      "loss": 0.0035,
      "step": 22420
    },
    {
      "epoch": 1.9937777777777779,
      "grad_norm": 0.3999941349029541,
      "learning_rate": 3.753888888888889e-05,
      "loss": 0.0034,
      "step": 22430
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.11257079243659973,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0017,
      "step": 22440
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 0.8701841235160828,
      "learning_rate": 3.752777777777778e-05,
      "loss": 0.0025,
      "step": 22450
    },
    {
      "epoch": 1.9964444444444445,
      "grad_norm": 0.48980581760406494,
      "learning_rate": 3.752222222222222e-05,
      "loss": 0.0029,
      "step": 22460
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 1.0937246084213257,
      "learning_rate": 3.7516666666666666e-05,
      "loss": 0.0029,
      "step": 22470
    },
    {
      "epoch": 1.9982222222222221,
      "grad_norm": 0.5434542298316956,
      "learning_rate": 3.7511111111111116e-05,
      "loss": 0.0023,
      "step": 22480
    },
    {
      "epoch": 1.999111111111111,
      "grad_norm": 0.3664948344230652,
      "learning_rate": 3.750555555555555e-05,
      "loss": 0.0034,
      "step": 22490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.33426204323768616,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0021,
      "step": 22500
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.002742474665865302,
      "eval_runtime": 118.432,
      "eval_samples_per_second": 1266.55,
      "eval_steps_per_second": 31.664,
      "step": 22500
    },
    {
      "epoch": 2.000888888888889,
      "grad_norm": 0.9050413966178894,
      "learning_rate": 3.749444444444445e-05,
      "loss": 0.0036,
      "step": 22510
    },
    {
      "epoch": 2.001777777777778,
      "grad_norm": 1.2721004486083984,
      "learning_rate": 3.748888888888889e-05,
      "loss": 0.0031,
      "step": 22520
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.2412356287240982,
      "learning_rate": 3.7483333333333334e-05,
      "loss": 0.0026,
      "step": 22530
    },
    {
      "epoch": 2.0035555555555558,
      "grad_norm": 0.5303831100463867,
      "learning_rate": 3.747777777777778e-05,
      "loss": 0.0028,
      "step": 22540
    },
    {
      "epoch": 2.0044444444444443,
      "grad_norm": 0.5282881855964661,
      "learning_rate": 3.747222222222223e-05,
      "loss": 0.0017,
      "step": 22550
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.9545719623565674,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0032,
      "step": 22560
    },
    {
      "epoch": 2.006222222222222,
      "grad_norm": 0.8159158825874329,
      "learning_rate": 3.7461111111111115e-05,
      "loss": 0.0032,
      "step": 22570
    },
    {
      "epoch": 2.007111111111111,
      "grad_norm": 0.8041121363639832,
      "learning_rate": 3.745555555555555e-05,
      "loss": 0.0032,
      "step": 22580
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.9966155290603638,
      "learning_rate": 3.745e-05,
      "loss": 0.0033,
      "step": 22590
    },
    {
      "epoch": 2.008888888888889,
      "grad_norm": 1.2883564233779907,
      "learning_rate": 3.7444444444444446e-05,
      "loss": 0.0029,
      "step": 22600
    },
    {
      "epoch": 2.009777777777778,
      "grad_norm": 0.4108271896839142,
      "learning_rate": 3.743888888888889e-05,
      "loss": 0.0026,
      "step": 22610
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 1.0870851278305054,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.0027,
      "step": 22620
    },
    {
      "epoch": 2.0115555555555558,
      "grad_norm": 0.10968969762325287,
      "learning_rate": 3.7427777777777777e-05,
      "loss": 0.0028,
      "step": 22630
    },
    {
      "epoch": 2.0124444444444443,
      "grad_norm": 1.030285358428955,
      "learning_rate": 3.742222222222223e-05,
      "loss": 0.0024,
      "step": 22640
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.4279538691043854,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.0022,
      "step": 22650
    },
    {
      "epoch": 2.014222222222222,
      "grad_norm": 0.616909921169281,
      "learning_rate": 3.7411111111111114e-05,
      "loss": 0.0033,
      "step": 22660
    },
    {
      "epoch": 2.015111111111111,
      "grad_norm": 0.5030600428581238,
      "learning_rate": 3.740555555555556e-05,
      "loss": 0.0017,
      "step": 22670
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.5946738123893738,
      "learning_rate": 3.74e-05,
      "loss": 0.0027,
      "step": 22680
    },
    {
      "epoch": 2.016888888888889,
      "grad_norm": 0.8679803609848022,
      "learning_rate": 3.739444444444445e-05,
      "loss": 0.0031,
      "step": 22690
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 0.26447534561157227,
      "learning_rate": 3.738888888888889e-05,
      "loss": 0.0035,
      "step": 22700
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.2938590943813324,
      "learning_rate": 3.738333333333334e-05,
      "loss": 0.0028,
      "step": 22710
    },
    {
      "epoch": 2.0195555555555558,
      "grad_norm": 0.8039785027503967,
      "learning_rate": 3.7377777777777775e-05,
      "loss": 0.0037,
      "step": 22720
    },
    {
      "epoch": 2.0204444444444443,
      "grad_norm": 0.36994752287864685,
      "learning_rate": 3.7372222222222226e-05,
      "loss": 0.0022,
      "step": 22730
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.12559594213962555,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.0024,
      "step": 22740
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 0.733491837978363,
      "learning_rate": 3.736111111111111e-05,
      "loss": 0.0024,
      "step": 22750
    },
    {
      "epoch": 2.023111111111111,
      "grad_norm": 0.34939301013946533,
      "learning_rate": 3.7355555555555556e-05,
      "loss": 0.0024,
      "step": 22760
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.09171593189239502,
      "learning_rate": 3.735e-05,
      "loss": 0.0024,
      "step": 22770
    },
    {
      "epoch": 2.024888888888889,
      "grad_norm": 0.8163485527038574,
      "learning_rate": 3.734444444444445e-05,
      "loss": 0.0029,
      "step": 22780
    },
    {
      "epoch": 2.025777777777778,
      "grad_norm": 0.4186354875564575,
      "learning_rate": 3.733888888888889e-05,
      "loss": 0.0023,
      "step": 22790
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.4999479353427887,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0026,
      "step": 22800
    },
    {
      "epoch": 2.0275555555555558,
      "grad_norm": 0.8621141910552979,
      "learning_rate": 3.732777777777778e-05,
      "loss": 0.0018,
      "step": 22810
    },
    {
      "epoch": 2.0284444444444443,
      "grad_norm": 0.3757478594779968,
      "learning_rate": 3.7322222222222224e-05,
      "loss": 0.0039,
      "step": 22820
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 1.1004904508590698,
      "learning_rate": 3.731666666666667e-05,
      "loss": 0.0033,
      "step": 22830
    },
    {
      "epoch": 2.030222222222222,
      "grad_norm": 0.24442197382450104,
      "learning_rate": 3.731111111111111e-05,
      "loss": 0.002,
      "step": 22840
    },
    {
      "epoch": 2.031111111111111,
      "grad_norm": 0.5374336242675781,
      "learning_rate": 3.7305555555555555e-05,
      "loss": 0.0016,
      "step": 22850
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.7368274927139282,
      "learning_rate": 3.73e-05,
      "loss": 0.0025,
      "step": 22860
    },
    {
      "epoch": 2.032888888888889,
      "grad_norm": 0.4733479619026184,
      "learning_rate": 3.729444444444445e-05,
      "loss": 0.0026,
      "step": 22870
    },
    {
      "epoch": 2.033777777777778,
      "grad_norm": 1.1423051357269287,
      "learning_rate": 3.728888888888889e-05,
      "loss": 0.0023,
      "step": 22880
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.8758378624916077,
      "learning_rate": 3.7283333333333336e-05,
      "loss": 0.0025,
      "step": 22890
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 0.5584173798561096,
      "learning_rate": 3.727777777777778e-05,
      "loss": 0.0019,
      "step": 22900
    },
    {
      "epoch": 2.0364444444444443,
      "grad_norm": 0.5892882943153381,
      "learning_rate": 3.727222222222222e-05,
      "loss": 0.0019,
      "step": 22910
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 1.1078901290893555,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0032,
      "step": 22920
    },
    {
      "epoch": 2.038222222222222,
      "grad_norm": 0.8066695332527161,
      "learning_rate": 3.726111111111111e-05,
      "loss": 0.003,
      "step": 22930
    },
    {
      "epoch": 2.039111111111111,
      "grad_norm": 0.05242883414030075,
      "learning_rate": 3.7255555555555554e-05,
      "loss": 0.0039,
      "step": 22940
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.49501991271972656,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.0038,
      "step": 22950
    },
    {
      "epoch": 2.040888888888889,
      "grad_norm": 0.7214712500572205,
      "learning_rate": 3.724444444444445e-05,
      "loss": 0.003,
      "step": 22960
    },
    {
      "epoch": 2.041777777777778,
      "grad_norm": 0.46591895818710327,
      "learning_rate": 3.723888888888889e-05,
      "loss": 0.0027,
      "step": 22970
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.5841183662414551,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0015,
      "step": 22980
    },
    {
      "epoch": 2.0435555555555553,
      "grad_norm": 0.22234384715557098,
      "learning_rate": 3.722777777777778e-05,
      "loss": 0.0032,
      "step": 22990
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 0.4114040732383728,
      "learning_rate": 3.722222222222222e-05,
      "loss": 0.0027,
      "step": 23000
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.04885132610797882,
      "learning_rate": 3.7216666666666666e-05,
      "loss": 0.0027,
      "step": 23010
    },
    {
      "epoch": 2.046222222222222,
      "grad_norm": 0.34309977293014526,
      "learning_rate": 3.7211111111111116e-05,
      "loss": 0.0028,
      "step": 23020
    },
    {
      "epoch": 2.047111111111111,
      "grad_norm": 0.14361485838890076,
      "learning_rate": 3.720555555555555e-05,
      "loss": 0.0024,
      "step": 23030
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.5540025234222412,
      "learning_rate": 3.72e-05,
      "loss": 0.0027,
      "step": 23040
    },
    {
      "epoch": 2.048888888888889,
      "grad_norm": 0.09430084377527237,
      "learning_rate": 3.7194444444444447e-05,
      "loss": 0.003,
      "step": 23050
    },
    {
      "epoch": 2.049777777777778,
      "grad_norm": 0.4637313783168793,
      "learning_rate": 3.718888888888889e-05,
      "loss": 0.0034,
      "step": 23060
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.7051517963409424,
      "learning_rate": 3.7183333333333334e-05,
      "loss": 0.0016,
      "step": 23070
    },
    {
      "epoch": 2.0515555555555554,
      "grad_norm": 0.3135956823825836,
      "learning_rate": 3.717777777777778e-05,
      "loss": 0.0024,
      "step": 23080
    },
    {
      "epoch": 2.0524444444444443,
      "grad_norm": 0.7726172208786011,
      "learning_rate": 3.717222222222223e-05,
      "loss": 0.0028,
      "step": 23090
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.9753663539886475,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.0027,
      "step": 23100
    },
    {
      "epoch": 2.054222222222222,
      "grad_norm": 0.7973943948745728,
      "learning_rate": 3.7161111111111115e-05,
      "loss": 0.0026,
      "step": 23110
    },
    {
      "epoch": 2.055111111111111,
      "grad_norm": 0.32746538519859314,
      "learning_rate": 3.715555555555555e-05,
      "loss": 0.0034,
      "step": 23120
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.5316029787063599,
      "learning_rate": 3.715e-05,
      "loss": 0.0029,
      "step": 23130
    },
    {
      "epoch": 2.056888888888889,
      "grad_norm": 0.9652915596961975,
      "learning_rate": 3.7144444444444445e-05,
      "loss": 0.0037,
      "step": 23140
    },
    {
      "epoch": 2.057777777777778,
      "grad_norm": 0.5539360642433167,
      "learning_rate": 3.713888888888889e-05,
      "loss": 0.0021,
      "step": 23150
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.325335830450058,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0026,
      "step": 23160
    },
    {
      "epoch": 2.0595555555555554,
      "grad_norm": 0.11149570345878601,
      "learning_rate": 3.7127777777777776e-05,
      "loss": 0.0029,
      "step": 23170
    },
    {
      "epoch": 2.0604444444444443,
      "grad_norm": 0.1580130159854889,
      "learning_rate": 3.7122222222222226e-05,
      "loss": 0.0034,
      "step": 23180
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.7279734015464783,
      "learning_rate": 3.711666666666666e-05,
      "loss": 0.0023,
      "step": 23190
    },
    {
      "epoch": 2.062222222222222,
      "grad_norm": 0.44083842635154724,
      "learning_rate": 3.7111111111111113e-05,
      "loss": 0.0026,
      "step": 23200
    },
    {
      "epoch": 2.063111111111111,
      "grad_norm": 1.1394764184951782,
      "learning_rate": 3.710555555555556e-05,
      "loss": 0.003,
      "step": 23210
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.12739956378936768,
      "learning_rate": 3.71e-05,
      "loss": 0.0021,
      "step": 23220
    },
    {
      "epoch": 2.064888888888889,
      "grad_norm": 0.5771738290786743,
      "learning_rate": 3.709444444444445e-05,
      "loss": 0.0016,
      "step": 23230
    },
    {
      "epoch": 2.065777777777778,
      "grad_norm": 0.4088670015335083,
      "learning_rate": 3.708888888888889e-05,
      "loss": 0.0029,
      "step": 23240
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.6431403160095215,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.0023,
      "step": 23250
    },
    {
      "epoch": 2.0675555555555554,
      "grad_norm": 0.7599744200706482,
      "learning_rate": 3.7077777777777775e-05,
      "loss": 0.0038,
      "step": 23260
    },
    {
      "epoch": 2.0684444444444443,
      "grad_norm": 0.9871761798858643,
      "learning_rate": 3.7072222222222225e-05,
      "loss": 0.0029,
      "step": 23270
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.17576365172863007,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.002,
      "step": 23280
    },
    {
      "epoch": 2.070222222222222,
      "grad_norm": 0.08546184748411179,
      "learning_rate": 3.706111111111111e-05,
      "loss": 0.0027,
      "step": 23290
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 0.9034925103187561,
      "learning_rate": 3.705555555555556e-05,
      "loss": 0.0019,
      "step": 23300
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.2590636610984802,
      "learning_rate": 3.705e-05,
      "loss": 0.0024,
      "step": 23310
    },
    {
      "epoch": 2.072888888888889,
      "grad_norm": 0.7223719954490662,
      "learning_rate": 3.704444444444445e-05,
      "loss": 0.0024,
      "step": 23320
    },
    {
      "epoch": 2.073777777777778,
      "grad_norm": 0.9850878119468689,
      "learning_rate": 3.7038888888888886e-05,
      "loss": 0.0037,
      "step": 23330
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.955686092376709,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0051,
      "step": 23340
    },
    {
      "epoch": 2.0755555555555554,
      "grad_norm": 0.5589381456375122,
      "learning_rate": 3.702777777777778e-05,
      "loss": 0.0039,
      "step": 23350
    },
    {
      "epoch": 2.0764444444444443,
      "grad_norm": 0.47151342034339905,
      "learning_rate": 3.7022222222222224e-05,
      "loss": 0.0022,
      "step": 23360
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.7179276347160339,
      "learning_rate": 3.701666666666667e-05,
      "loss": 0.003,
      "step": 23370
    },
    {
      "epoch": 2.078222222222222,
      "grad_norm": 0.17425189912319183,
      "learning_rate": 3.701111111111111e-05,
      "loss": 0.0024,
      "step": 23380
    },
    {
      "epoch": 2.079111111111111,
      "grad_norm": 0.39759716391563416,
      "learning_rate": 3.700555555555556e-05,
      "loss": 0.0029,
      "step": 23390
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.3787776827812195,
      "learning_rate": 3.7e-05,
      "loss": 0.0042,
      "step": 23400
    },
    {
      "epoch": 2.080888888888889,
      "grad_norm": 0.3449510633945465,
      "learning_rate": 3.699444444444445e-05,
      "loss": 0.0029,
      "step": 23410
    },
    {
      "epoch": 2.081777777777778,
      "grad_norm": 0.18912304937839508,
      "learning_rate": 3.698888888888889e-05,
      "loss": 0.0028,
      "step": 23420
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.06366468966007233,
      "learning_rate": 3.6983333333333336e-05,
      "loss": 0.0032,
      "step": 23430
    },
    {
      "epoch": 2.0835555555555554,
      "grad_norm": 0.3432573080062866,
      "learning_rate": 3.697777777777778e-05,
      "loss": 0.0023,
      "step": 23440
    },
    {
      "epoch": 2.0844444444444443,
      "grad_norm": 0.5966477990150452,
      "learning_rate": 3.697222222222222e-05,
      "loss": 0.0026,
      "step": 23450
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.3274849057197571,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0023,
      "step": 23460
    },
    {
      "epoch": 2.086222222222222,
      "grad_norm": 0.2188878208398819,
      "learning_rate": 3.696111111111111e-05,
      "loss": 0.0017,
      "step": 23470
    },
    {
      "epoch": 2.087111111111111,
      "grad_norm": 0.5781814455986023,
      "learning_rate": 3.695555555555556e-05,
      "loss": 0.0016,
      "step": 23480
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.24338577687740326,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 0.0025,
      "step": 23490
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.5408337116241455,
      "learning_rate": 3.694444444444445e-05,
      "loss": 0.0017,
      "step": 23500
    },
    {
      "epoch": 2.089777777777778,
      "grad_norm": 1.0418603420257568,
      "learning_rate": 3.693888888888889e-05,
      "loss": 0.0027,
      "step": 23510
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.28783828020095825,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0029,
      "step": 23520
    },
    {
      "epoch": 2.0915555555555554,
      "grad_norm": 0.28433287143707275,
      "learning_rate": 3.692777777777778e-05,
      "loss": 0.0032,
      "step": 23530
    },
    {
      "epoch": 2.0924444444444443,
      "grad_norm": 0.24139238893985748,
      "learning_rate": 3.692222222222222e-05,
      "loss": 0.0034,
      "step": 23540
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.12197621911764145,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.0027,
      "step": 23550
    },
    {
      "epoch": 2.094222222222222,
      "grad_norm": 0.45675674080848694,
      "learning_rate": 3.6911111111111115e-05,
      "loss": 0.0021,
      "step": 23560
    },
    {
      "epoch": 2.095111111111111,
      "grad_norm": 0.33325666189193726,
      "learning_rate": 3.690555555555556e-05,
      "loss": 0.0022,
      "step": 23570
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.11921582370996475,
      "learning_rate": 3.69e-05,
      "loss": 0.0029,
      "step": 23580
    },
    {
      "epoch": 2.096888888888889,
      "grad_norm": 0.21051615476608276,
      "learning_rate": 3.6894444444444446e-05,
      "loss": 0.0037,
      "step": 23590
    },
    {
      "epoch": 2.097777777777778,
      "grad_norm": 0.16602997481822968,
      "learning_rate": 3.688888888888889e-05,
      "loss": 0.0032,
      "step": 23600
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.6573975086212158,
      "learning_rate": 3.688333333333333e-05,
      "loss": 0.0037,
      "step": 23610
    },
    {
      "epoch": 2.0995555555555554,
      "grad_norm": 0.2179357409477234,
      "learning_rate": 3.687777777777778e-05,
      "loss": 0.0031,
      "step": 23620
    },
    {
      "epoch": 2.1004444444444443,
      "grad_norm": 0.36282870173454285,
      "learning_rate": 3.687222222222223e-05,
      "loss": 0.0022,
      "step": 23630
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.8437220454216003,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0029,
      "step": 23640
    },
    {
      "epoch": 2.102222222222222,
      "grad_norm": 0.32774481177330017,
      "learning_rate": 3.6861111111111114e-05,
      "loss": 0.0022,
      "step": 23650
    },
    {
      "epoch": 2.103111111111111,
      "grad_norm": 0.41667434573173523,
      "learning_rate": 3.685555555555556e-05,
      "loss": 0.0032,
      "step": 23660
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.2806444764137268,
      "learning_rate": 3.685e-05,
      "loss": 0.0021,
      "step": 23670
    },
    {
      "epoch": 2.104888888888889,
      "grad_norm": 0.5361197590827942,
      "learning_rate": 3.6844444444444445e-05,
      "loss": 0.0024,
      "step": 23680
    },
    {
      "epoch": 2.105777777777778,
      "grad_norm": 0.6515559554100037,
      "learning_rate": 3.683888888888889e-05,
      "loss": 0.0032,
      "step": 23690
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.627843976020813,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.002,
      "step": 23700
    },
    {
      "epoch": 2.1075555555555554,
      "grad_norm": 0.15790648758411407,
      "learning_rate": 3.6827777777777775e-05,
      "loss": 0.0029,
      "step": 23710
    },
    {
      "epoch": 2.1084444444444443,
      "grad_norm": 0.4810793399810791,
      "learning_rate": 3.6822222222222226e-05,
      "loss": 0.0016,
      "step": 23720
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 1.1985617876052856,
      "learning_rate": 3.681666666666667e-05,
      "loss": 0.0037,
      "step": 23730
    },
    {
      "epoch": 2.110222222222222,
      "grad_norm": 0.3919142186641693,
      "learning_rate": 3.681111111111111e-05,
      "loss": 0.0022,
      "step": 23740
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.8894915580749512,
      "learning_rate": 3.6805555555555556e-05,
      "loss": 0.0019,
      "step": 23750
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.6313766837120056,
      "learning_rate": 3.68e-05,
      "loss": 0.0034,
      "step": 23760
    },
    {
      "epoch": 2.112888888888889,
      "grad_norm": 0.06416907906532288,
      "learning_rate": 3.679444444444445e-05,
      "loss": 0.0024,
      "step": 23770
    },
    {
      "epoch": 2.113777777777778,
      "grad_norm": 0.2642744183540344,
      "learning_rate": 3.678888888888889e-05,
      "loss": 0.0029,
      "step": 23780
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.19051235914230347,
      "learning_rate": 3.678333333333334e-05,
      "loss": 0.0034,
      "step": 23790
    },
    {
      "epoch": 2.1155555555555554,
      "grad_norm": 0.3032381236553192,
      "learning_rate": 3.677777777777778e-05,
      "loss": 0.0017,
      "step": 23800
    },
    {
      "epoch": 2.1164444444444444,
      "grad_norm": 0.6222659349441528,
      "learning_rate": 3.6772222222222225e-05,
      "loss": 0.0023,
      "step": 23810
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.09085360169410706,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.003,
      "step": 23820
    },
    {
      "epoch": 2.1182222222222222,
      "grad_norm": 0.18094836175441742,
      "learning_rate": 3.676111111111111e-05,
      "loss": 0.0033,
      "step": 23830
    },
    {
      "epoch": 2.119111111111111,
      "grad_norm": 0.380001962184906,
      "learning_rate": 3.675555555555556e-05,
      "loss": 0.0021,
      "step": 23840
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.6996925473213196,
      "learning_rate": 3.675e-05,
      "loss": 0.0026,
      "step": 23850
    },
    {
      "epoch": 2.120888888888889,
      "grad_norm": 0.4988420605659485,
      "learning_rate": 3.674444444444445e-05,
      "loss": 0.0025,
      "step": 23860
    },
    {
      "epoch": 2.121777777777778,
      "grad_norm": 0.8132673501968384,
      "learning_rate": 3.673888888888889e-05,
      "loss": 0.0022,
      "step": 23870
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.9455645084381104,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0034,
      "step": 23880
    },
    {
      "epoch": 2.1235555555555554,
      "grad_norm": 0.4737569987773895,
      "learning_rate": 3.672777777777778e-05,
      "loss": 0.0027,
      "step": 23890
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 0.596842348575592,
      "learning_rate": 3.672222222222222e-05,
      "loss": 0.0032,
      "step": 23900
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.41843435168266296,
      "learning_rate": 3.671666666666667e-05,
      "loss": 0.003,
      "step": 23910
    },
    {
      "epoch": 2.1262222222222222,
      "grad_norm": 0.3189578950405121,
      "learning_rate": 3.671111111111111e-05,
      "loss": 0.003,
      "step": 23920
    },
    {
      "epoch": 2.127111111111111,
      "grad_norm": 0.7306419610977173,
      "learning_rate": 3.670555555555556e-05,
      "loss": 0.0054,
      "step": 23930
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.2438022792339325,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0021,
      "step": 23940
    },
    {
      "epoch": 2.128888888888889,
      "grad_norm": 0.2544732987880707,
      "learning_rate": 3.669444444444445e-05,
      "loss": 0.0037,
      "step": 23950
    },
    {
      "epoch": 2.129777777777778,
      "grad_norm": 0.2154606580734253,
      "learning_rate": 3.668888888888889e-05,
      "loss": 0.0039,
      "step": 23960
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.2516312003135681,
      "learning_rate": 3.6683333333333335e-05,
      "loss": 0.003,
      "step": 23970
    },
    {
      "epoch": 2.1315555555555554,
      "grad_norm": 0.8483683466911316,
      "learning_rate": 3.667777777777778e-05,
      "loss": 0.0027,
      "step": 23980
    },
    {
      "epoch": 2.1324444444444444,
      "grad_norm": 0.134354829788208,
      "learning_rate": 3.667222222222222e-05,
      "loss": 0.0033,
      "step": 23990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.7719206809997559,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0023,
      "step": 24000
    },
    {
      "epoch": 2.1342222222222222,
      "grad_norm": 0.7071227431297302,
      "learning_rate": 3.6661111111111116e-05,
      "loss": 0.0027,
      "step": 24010
    },
    {
      "epoch": 2.135111111111111,
      "grad_norm": 0.6864117980003357,
      "learning_rate": 3.665555555555556e-05,
      "loss": 0.0031,
      "step": 24020
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.4949520230293274,
      "learning_rate": 3.665e-05,
      "loss": 0.0028,
      "step": 24030
    },
    {
      "epoch": 2.136888888888889,
      "grad_norm": 0.5286158919334412,
      "learning_rate": 3.664444444444445e-05,
      "loss": 0.002,
      "step": 24040
    },
    {
      "epoch": 2.137777777777778,
      "grad_norm": 0.8047966957092285,
      "learning_rate": 3.663888888888889e-05,
      "loss": 0.003,
      "step": 24050
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 1.1602118015289307,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.0021,
      "step": 24060
    },
    {
      "epoch": 2.1395555555555554,
      "grad_norm": 0.5703533887863159,
      "learning_rate": 3.662777777777778e-05,
      "loss": 0.0035,
      "step": 24070
    },
    {
      "epoch": 2.1404444444444444,
      "grad_norm": 0.5647214651107788,
      "learning_rate": 3.662222222222223e-05,
      "loss": 0.0021,
      "step": 24080
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.29203927516937256,
      "learning_rate": 3.6616666666666664e-05,
      "loss": 0.0023,
      "step": 24090
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 0.5914077758789062,
      "learning_rate": 3.6611111111111115e-05,
      "loss": 0.0021,
      "step": 24100
    },
    {
      "epoch": 2.143111111111111,
      "grad_norm": 0.4238341152667999,
      "learning_rate": 3.660555555555556e-05,
      "loss": 0.0028,
      "step": 24110
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.3840523362159729,
      "learning_rate": 3.66e-05,
      "loss": 0.003,
      "step": 24120
    },
    {
      "epoch": 2.144888888888889,
      "grad_norm": 0.13451328873634338,
      "learning_rate": 3.6594444444444446e-05,
      "loss": 0.0018,
      "step": 24130
    },
    {
      "epoch": 2.145777777777778,
      "grad_norm": 0.5804193019866943,
      "learning_rate": 3.658888888888889e-05,
      "loss": 0.0015,
      "step": 24140
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.8583154678344727,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.003,
      "step": 24150
    },
    {
      "epoch": 2.1475555555555554,
      "grad_norm": 0.7984210848808289,
      "learning_rate": 3.6577777777777776e-05,
      "loss": 0.0016,
      "step": 24160
    },
    {
      "epoch": 2.1484444444444444,
      "grad_norm": 0.8043835759162903,
      "learning_rate": 3.6572222222222227e-05,
      "loss": 0.0024,
      "step": 24170
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.5443800687789917,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.002,
      "step": 24180
    },
    {
      "epoch": 2.1502222222222223,
      "grad_norm": 0.4967724084854126,
      "learning_rate": 3.6561111111111114e-05,
      "loss": 0.003,
      "step": 24190
    },
    {
      "epoch": 2.151111111111111,
      "grad_norm": 0.40226051211357117,
      "learning_rate": 3.655555555555556e-05,
      "loss": 0.0025,
      "step": 24200
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.1781061440706253,
      "learning_rate": 3.655e-05,
      "loss": 0.0023,
      "step": 24210
    },
    {
      "epoch": 2.152888888888889,
      "grad_norm": 0.49587303400039673,
      "learning_rate": 3.654444444444445e-05,
      "loss": 0.0033,
      "step": 24220
    },
    {
      "epoch": 2.153777777777778,
      "grad_norm": 0.47536584734916687,
      "learning_rate": 3.653888888888889e-05,
      "loss": 0.0031,
      "step": 24230
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 1.282691478729248,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0024,
      "step": 24240
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 1.1704024076461792,
      "learning_rate": 3.6527777777777775e-05,
      "loss": 0.0038,
      "step": 24250
    },
    {
      "epoch": 2.1564444444444444,
      "grad_norm": 0.3411962389945984,
      "learning_rate": 3.6522222222222225e-05,
      "loss": 0.0031,
      "step": 24260
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.9122040867805481,
      "learning_rate": 3.651666666666667e-05,
      "loss": 0.0023,
      "step": 24270
    },
    {
      "epoch": 2.1582222222222223,
      "grad_norm": 1.0714982748031616,
      "learning_rate": 3.651111111111111e-05,
      "loss": 0.0033,
      "step": 24280
    },
    {
      "epoch": 2.159111111111111,
      "grad_norm": 0.7213619947433472,
      "learning_rate": 3.650555555555556e-05,
      "loss": 0.002,
      "step": 24290
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.9779262542724609,
      "learning_rate": 3.65e-05,
      "loss": 0.0031,
      "step": 24300
    },
    {
      "epoch": 2.160888888888889,
      "grad_norm": 0.9185335636138916,
      "learning_rate": 3.649444444444445e-05,
      "loss": 0.0029,
      "step": 24310
    },
    {
      "epoch": 2.1617777777777776,
      "grad_norm": 0.5365779995918274,
      "learning_rate": 3.648888888888889e-05,
      "loss": 0.0028,
      "step": 24320
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.6736693382263184,
      "learning_rate": 3.648333333333334e-05,
      "loss": 0.0029,
      "step": 24330
    },
    {
      "epoch": 2.1635555555555555,
      "grad_norm": 0.20282624661922455,
      "learning_rate": 3.647777777777778e-05,
      "loss": 0.0027,
      "step": 24340
    },
    {
      "epoch": 2.1644444444444444,
      "grad_norm": 0.7690795660018921,
      "learning_rate": 3.6472222222222224e-05,
      "loss": 0.0036,
      "step": 24350
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.6539345383644104,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0022,
      "step": 24360
    },
    {
      "epoch": 2.1662222222222223,
      "grad_norm": 0.20832310616970062,
      "learning_rate": 3.646111111111111e-05,
      "loss": 0.002,
      "step": 24370
    },
    {
      "epoch": 2.167111111111111,
      "grad_norm": 0.20914189517498016,
      "learning_rate": 3.645555555555556e-05,
      "loss": 0.0021,
      "step": 24380
    },
    {
      "epoch": 2.168,
      "grad_norm": 1.0013271570205688,
      "learning_rate": 3.645e-05,
      "loss": 0.0029,
      "step": 24390
    },
    {
      "epoch": 2.168888888888889,
      "grad_norm": 0.47682875394821167,
      "learning_rate": 3.644444444444445e-05,
      "loss": 0.0026,
      "step": 24400
    },
    {
      "epoch": 2.1697777777777776,
      "grad_norm": 0.41608771681785583,
      "learning_rate": 3.643888888888889e-05,
      "loss": 0.0016,
      "step": 24410
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.5708203911781311,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0018,
      "step": 24420
    },
    {
      "epoch": 2.1715555555555555,
      "grad_norm": 0.12350775301456451,
      "learning_rate": 3.642777777777778e-05,
      "loss": 0.0018,
      "step": 24430
    },
    {
      "epoch": 2.1724444444444444,
      "grad_norm": 0.784342885017395,
      "learning_rate": 3.642222222222222e-05,
      "loss": 0.0026,
      "step": 24440
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.6086679697036743,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.0024,
      "step": 24450
    },
    {
      "epoch": 2.1742222222222223,
      "grad_norm": 0.6754822134971619,
      "learning_rate": 3.641111111111111e-05,
      "loss": 0.0029,
      "step": 24460
    },
    {
      "epoch": 2.175111111111111,
      "grad_norm": 0.09118732810020447,
      "learning_rate": 3.640555555555556e-05,
      "loss": 0.0028,
      "step": 24470
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.06209763512015343,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.003,
      "step": 24480
    },
    {
      "epoch": 2.176888888888889,
      "grad_norm": 0.3979189097881317,
      "learning_rate": 3.639444444444445e-05,
      "loss": 0.0027,
      "step": 24490
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 1.1502830982208252,
      "learning_rate": 3.638888888888889e-05,
      "loss": 0.0019,
      "step": 24500
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.568958580493927,
      "learning_rate": 3.6383333333333335e-05,
      "loss": 0.0023,
      "step": 24510
    },
    {
      "epoch": 2.1795555555555555,
      "grad_norm": 0.09919101744890213,
      "learning_rate": 3.637777777777778e-05,
      "loss": 0.0029,
      "step": 24520
    },
    {
      "epoch": 2.1804444444444444,
      "grad_norm": 0.0345768928527832,
      "learning_rate": 3.637222222222222e-05,
      "loss": 0.0021,
      "step": 24530
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.18888571858406067,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.0032,
      "step": 24540
    },
    {
      "epoch": 2.1822222222222223,
      "grad_norm": 0.2011556625366211,
      "learning_rate": 3.6361111111111116e-05,
      "loss": 0.0033,
      "step": 24550
    },
    {
      "epoch": 2.1831111111111112,
      "grad_norm": 0.3450488746166229,
      "learning_rate": 3.635555555555556e-05,
      "loss": 0.0021,
      "step": 24560
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.1822700947523117,
      "learning_rate": 3.635e-05,
      "loss": 0.0032,
      "step": 24570
    },
    {
      "epoch": 2.1848888888888887,
      "grad_norm": 0.4565563201904297,
      "learning_rate": 3.6344444444444446e-05,
      "loss": 0.0027,
      "step": 24580
    },
    {
      "epoch": 2.1857777777777776,
      "grad_norm": 0.37304893136024475,
      "learning_rate": 3.633888888888889e-05,
      "loss": 0.003,
      "step": 24590
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.12510563433170319,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0031,
      "step": 24600
    },
    {
      "epoch": 2.1875555555555555,
      "grad_norm": 0.09951700270175934,
      "learning_rate": 3.632777777777778e-05,
      "loss": 0.0016,
      "step": 24610
    },
    {
      "epoch": 2.1884444444444444,
      "grad_norm": 0.30632132291793823,
      "learning_rate": 3.632222222222223e-05,
      "loss": 0.0023,
      "step": 24620
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.7142866849899292,
      "learning_rate": 3.631666666666667e-05,
      "loss": 0.0026,
      "step": 24630
    },
    {
      "epoch": 2.1902222222222223,
      "grad_norm": 0.1415138989686966,
      "learning_rate": 3.6311111111111114e-05,
      "loss": 0.0021,
      "step": 24640
    },
    {
      "epoch": 2.1911111111111112,
      "grad_norm": 0.4572678506374359,
      "learning_rate": 3.630555555555556e-05,
      "loss": 0.0018,
      "step": 24650
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.16004720330238342,
      "learning_rate": 3.63e-05,
      "loss": 0.0031,
      "step": 24660
    },
    {
      "epoch": 2.1928888888888887,
      "grad_norm": 0.28708600997924805,
      "learning_rate": 3.6294444444444445e-05,
      "loss": 0.0022,
      "step": 24670
    },
    {
      "epoch": 2.1937777777777776,
      "grad_norm": 0.1373656690120697,
      "learning_rate": 3.628888888888889e-05,
      "loss": 0.0026,
      "step": 24680
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.28592994809150696,
      "learning_rate": 3.628333333333334e-05,
      "loss": 0.0033,
      "step": 24690
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 0.6478468775749207,
      "learning_rate": 3.6277777777777776e-05,
      "loss": 0.0033,
      "step": 24700
    },
    {
      "epoch": 2.1964444444444444,
      "grad_norm": 0.15923543274402618,
      "learning_rate": 3.6272222222222226e-05,
      "loss": 0.0024,
      "step": 24710
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.5756456851959229,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0036,
      "step": 24720
    },
    {
      "epoch": 2.1982222222222223,
      "grad_norm": 0.09174072742462158,
      "learning_rate": 3.626111111111111e-05,
      "loss": 0.0013,
      "step": 24730
    },
    {
      "epoch": 2.1991111111111112,
      "grad_norm": 0.6703811883926392,
      "learning_rate": 3.625555555555556e-05,
      "loss": 0.0034,
      "step": 24740
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.1772310584783554,
      "learning_rate": 3.625e-05,
      "loss": 0.0022,
      "step": 24750
    },
    {
      "epoch": 2.2008888888888887,
      "grad_norm": 0.055887371301651,
      "learning_rate": 3.624444444444445e-05,
      "loss": 0.0029,
      "step": 24760
    },
    {
      "epoch": 2.2017777777777776,
      "grad_norm": 0.23938198387622833,
      "learning_rate": 3.623888888888889e-05,
      "loss": 0.0031,
      "step": 24770
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.30384451150894165,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.0028,
      "step": 24780
    },
    {
      "epoch": 2.2035555555555555,
      "grad_norm": 0.6562871932983398,
      "learning_rate": 3.6227777777777774e-05,
      "loss": 0.0021,
      "step": 24790
    },
    {
      "epoch": 2.2044444444444444,
      "grad_norm": 0.5288549661636353,
      "learning_rate": 3.6222222222222225e-05,
      "loss": 0.0038,
      "step": 24800
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.8079207539558411,
      "learning_rate": 3.621666666666667e-05,
      "loss": 0.0019,
      "step": 24810
    },
    {
      "epoch": 2.2062222222222223,
      "grad_norm": 0.11940010637044907,
      "learning_rate": 3.621111111111111e-05,
      "loss": 0.0034,
      "step": 24820
    },
    {
      "epoch": 2.2071111111111112,
      "grad_norm": 0.36211732029914856,
      "learning_rate": 3.620555555555556e-05,
      "loss": 0.0018,
      "step": 24830
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.05544965714216232,
      "learning_rate": 3.62e-05,
      "loss": 0.0026,
      "step": 24840
    },
    {
      "epoch": 2.2088888888888887,
      "grad_norm": 0.28428471088409424,
      "learning_rate": 3.619444444444445e-05,
      "loss": 0.0029,
      "step": 24850
    },
    {
      "epoch": 2.2097777777777776,
      "grad_norm": 0.21049091219902039,
      "learning_rate": 3.6188888888888886e-05,
      "loss": 0.0028,
      "step": 24860
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.051334723830223083,
      "learning_rate": 3.6183333333333336e-05,
      "loss": 0.0023,
      "step": 24870
    },
    {
      "epoch": 2.2115555555555555,
      "grad_norm": 0.8753555417060852,
      "learning_rate": 3.617777777777778e-05,
      "loss": 0.0029,
      "step": 24880
    },
    {
      "epoch": 2.2124444444444444,
      "grad_norm": 0.17601732909679413,
      "learning_rate": 3.6172222222222224e-05,
      "loss": 0.0029,
      "step": 24890
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.6708011031150818,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0031,
      "step": 24900
    },
    {
      "epoch": 2.2142222222222223,
      "grad_norm": 0.38624563813209534,
      "learning_rate": 3.616111111111111e-05,
      "loss": 0.0032,
      "step": 24910
    },
    {
      "epoch": 2.2151111111111113,
      "grad_norm": 0.6971450448036194,
      "learning_rate": 3.615555555555556e-05,
      "loss": 0.0032,
      "step": 24920
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.08598866313695908,
      "learning_rate": 3.615e-05,
      "loss": 0.0024,
      "step": 24930
    },
    {
      "epoch": 2.2168888888888887,
      "grad_norm": 0.10835133492946625,
      "learning_rate": 3.614444444444445e-05,
      "loss": 0.003,
      "step": 24940
    },
    {
      "epoch": 2.2177777777777776,
      "grad_norm": 0.11159893125295639,
      "learning_rate": 3.613888888888889e-05,
      "loss": 0.002,
      "step": 24950
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 1.1168360710144043,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0026,
      "step": 24960
    },
    {
      "epoch": 2.2195555555555555,
      "grad_norm": 0.41394656896591187,
      "learning_rate": 3.612777777777778e-05,
      "loss": 0.0031,
      "step": 24970
    },
    {
      "epoch": 2.2204444444444444,
      "grad_norm": 0.290613055229187,
      "learning_rate": 3.612222222222222e-05,
      "loss": 0.003,
      "step": 24980
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.17458122968673706,
      "learning_rate": 3.611666666666667e-05,
      "loss": 0.0028,
      "step": 24990
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.7600412368774414,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.0032,
      "step": 25000
    },
    {
      "epoch": 2.2231111111111113,
      "grad_norm": 0.5391988754272461,
      "learning_rate": 3.610555555555556e-05,
      "loss": 0.0036,
      "step": 25010
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.46586740016937256,
      "learning_rate": 3.61e-05,
      "loss": 0.0028,
      "step": 25020
    },
    {
      "epoch": 2.2248888888888887,
      "grad_norm": 0.532936692237854,
      "learning_rate": 3.609444444444445e-05,
      "loss": 0.0028,
      "step": 25030
    },
    {
      "epoch": 2.2257777777777776,
      "grad_norm": 0.06402582675218582,
      "learning_rate": 3.608888888888889e-05,
      "loss": 0.0023,
      "step": 25040
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 1.262252688407898,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.0029,
      "step": 25050
    },
    {
      "epoch": 2.2275555555555555,
      "grad_norm": 1.2301876544952393,
      "learning_rate": 3.607777777777778e-05,
      "loss": 0.0024,
      "step": 25060
    },
    {
      "epoch": 2.2284444444444444,
      "grad_norm": 0.4304761290550232,
      "learning_rate": 3.607222222222222e-05,
      "loss": 0.0016,
      "step": 25070
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.905113160610199,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0043,
      "step": 25080
    },
    {
      "epoch": 2.2302222222222223,
      "grad_norm": 0.1447533220052719,
      "learning_rate": 3.6061111111111115e-05,
      "loss": 0.0029,
      "step": 25090
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 0.11280673742294312,
      "learning_rate": 3.605555555555556e-05,
      "loss": 0.0032,
      "step": 25100
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.8063495755195618,
      "learning_rate": 3.605e-05,
      "loss": 0.0028,
      "step": 25110
    },
    {
      "epoch": 2.2328888888888887,
      "grad_norm": 0.37810152769088745,
      "learning_rate": 3.6044444444444446e-05,
      "loss": 0.0029,
      "step": 25120
    },
    {
      "epoch": 2.2337777777777776,
      "grad_norm": 0.13324013352394104,
      "learning_rate": 3.603888888888889e-05,
      "loss": 0.0024,
      "step": 25130
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.8838804960250854,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0029,
      "step": 25140
    },
    {
      "epoch": 2.2355555555555555,
      "grad_norm": 0.33824241161346436,
      "learning_rate": 3.6027777777777776e-05,
      "loss": 0.0017,
      "step": 25150
    },
    {
      "epoch": 2.2364444444444445,
      "grad_norm": 1.0033557415008545,
      "learning_rate": 3.602222222222223e-05,
      "loss": 0.0027,
      "step": 25160
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.3238223195075989,
      "learning_rate": 3.601666666666667e-05,
      "loss": 0.0036,
      "step": 25170
    },
    {
      "epoch": 2.2382222222222223,
      "grad_norm": 0.6734282374382019,
      "learning_rate": 3.6011111111111114e-05,
      "loss": 0.0025,
      "step": 25180
    },
    {
      "epoch": 2.2391111111111113,
      "grad_norm": 0.029109440743923187,
      "learning_rate": 3.600555555555556e-05,
      "loss": 0.0028,
      "step": 25190
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.43120837211608887,
      "learning_rate": 3.6e-05,
      "loss": 0.0024,
      "step": 25200
    },
    {
      "epoch": 2.2408888888888887,
      "grad_norm": 0.19881300628185272,
      "learning_rate": 3.5994444444444444e-05,
      "loss": 0.0035,
      "step": 25210
    },
    {
      "epoch": 2.2417777777777776,
      "grad_norm": 0.7877424359321594,
      "learning_rate": 3.598888888888889e-05,
      "loss": 0.0029,
      "step": 25220
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.37253284454345703,
      "learning_rate": 3.598333333333334e-05,
      "loss": 0.0018,
      "step": 25230
    },
    {
      "epoch": 2.2435555555555555,
      "grad_norm": 0.29711705446243286,
      "learning_rate": 3.5977777777777775e-05,
      "loss": 0.0022,
      "step": 25240
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 0.08684039115905762,
      "learning_rate": 3.5972222222222225e-05,
      "loss": 0.0026,
      "step": 25250
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.5342712998390198,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.003,
      "step": 25260
    },
    {
      "epoch": 2.2462222222222223,
      "grad_norm": 0.2844240665435791,
      "learning_rate": 3.596111111111111e-05,
      "loss": 0.0019,
      "step": 25270
    },
    {
      "epoch": 2.2471111111111113,
      "grad_norm": 0.6238488554954529,
      "learning_rate": 3.5955555555555556e-05,
      "loss": 0.0034,
      "step": 25280
    },
    {
      "epoch": 2.248,
      "grad_norm": 1.3555634021759033,
      "learning_rate": 3.595e-05,
      "loss": 0.0025,
      "step": 25290
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 0.6304701566696167,
      "learning_rate": 3.594444444444445e-05,
      "loss": 0.0021,
      "step": 25300
    },
    {
      "epoch": 2.2497777777777777,
      "grad_norm": 0.1327200084924698,
      "learning_rate": 3.593888888888889e-05,
      "loss": 0.0023,
      "step": 25310
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.5808179378509521,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0025,
      "step": 25320
    },
    {
      "epoch": 2.2515555555555555,
      "grad_norm": 0.38592880964279175,
      "learning_rate": 3.5927777777777774e-05,
      "loss": 0.0019,
      "step": 25330
    },
    {
      "epoch": 2.2524444444444445,
      "grad_norm": 0.5491580963134766,
      "learning_rate": 3.5922222222222224e-05,
      "loss": 0.0026,
      "step": 25340
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.4610476791858673,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.0022,
      "step": 25350
    },
    {
      "epoch": 2.2542222222222223,
      "grad_norm": 0.41155990958213806,
      "learning_rate": 3.591111111111111e-05,
      "loss": 0.0037,
      "step": 25360
    },
    {
      "epoch": 2.2551111111111113,
      "grad_norm": 0.4089524745941162,
      "learning_rate": 3.590555555555556e-05,
      "loss": 0.0023,
      "step": 25370
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.9288306832313538,
      "learning_rate": 3.59e-05,
      "loss": 0.0027,
      "step": 25380
    },
    {
      "epoch": 2.2568888888888887,
      "grad_norm": 0.4813559055328369,
      "learning_rate": 3.589444444444445e-05,
      "loss": 0.0025,
      "step": 25390
    },
    {
      "epoch": 2.2577777777777777,
      "grad_norm": 0.4239576756954193,
      "learning_rate": 3.5888888888888886e-05,
      "loss": 0.0028,
      "step": 25400
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.6347064971923828,
      "learning_rate": 3.5883333333333336e-05,
      "loss": 0.003,
      "step": 25410
    },
    {
      "epoch": 2.2595555555555555,
      "grad_norm": 0.7267765998840332,
      "learning_rate": 3.587777777777778e-05,
      "loss": 0.0025,
      "step": 25420
    },
    {
      "epoch": 2.2604444444444445,
      "grad_norm": 0.48295319080352783,
      "learning_rate": 3.587222222222222e-05,
      "loss": 0.0024,
      "step": 25430
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.25289860367774963,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0027,
      "step": 25440
    },
    {
      "epoch": 2.2622222222222224,
      "grad_norm": 0.5445278286933899,
      "learning_rate": 3.586111111111111e-05,
      "loss": 0.0029,
      "step": 25450
    },
    {
      "epoch": 2.2631111111111113,
      "grad_norm": 0.37409457564353943,
      "learning_rate": 3.585555555555556e-05,
      "loss": 0.003,
      "step": 25460
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 1.3061542510986328,
      "learning_rate": 3.585e-05,
      "loss": 0.0022,
      "step": 25470
    },
    {
      "epoch": 2.2648888888888887,
      "grad_norm": 0.48953354358673096,
      "learning_rate": 3.584444444444445e-05,
      "loss": 0.0019,
      "step": 25480
    },
    {
      "epoch": 2.2657777777777777,
      "grad_norm": 0.08102573454380035,
      "learning_rate": 3.583888888888889e-05,
      "loss": 0.0022,
      "step": 25490
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.08705224841833115,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0027,
      "step": 25500
    },
    {
      "epoch": 2.2675555555555555,
      "grad_norm": 0.8836865425109863,
      "learning_rate": 3.582777777777778e-05,
      "loss": 0.0023,
      "step": 25510
    },
    {
      "epoch": 2.2684444444444445,
      "grad_norm": 0.08244463056325912,
      "learning_rate": 3.582222222222222e-05,
      "loss": 0.0028,
      "step": 25520
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.2912721633911133,
      "learning_rate": 3.581666666666667e-05,
      "loss": 0.0018,
      "step": 25530
    },
    {
      "epoch": 2.2702222222222224,
      "grad_norm": 0.9617406129837036,
      "learning_rate": 3.581111111111111e-05,
      "loss": 0.0031,
      "step": 25540
    },
    {
      "epoch": 2.2711111111111113,
      "grad_norm": 0.7511245012283325,
      "learning_rate": 3.580555555555556e-05,
      "loss": 0.0018,
      "step": 25550
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.24290308356285095,
      "learning_rate": 3.58e-05,
      "loss": 0.0018,
      "step": 25560
    },
    {
      "epoch": 2.2728888888888887,
      "grad_norm": 0.27189815044403076,
      "learning_rate": 3.5794444444444446e-05,
      "loss": 0.0016,
      "step": 25570
    },
    {
      "epoch": 2.2737777777777777,
      "grad_norm": 0.7762494087219238,
      "learning_rate": 3.578888888888889e-05,
      "loss": 0.0036,
      "step": 25580
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.6022326350212097,
      "learning_rate": 3.5783333333333333e-05,
      "loss": 0.0035,
      "step": 25590
    },
    {
      "epoch": 2.2755555555555556,
      "grad_norm": 0.19282518327236176,
      "learning_rate": 3.577777777777778e-05,
      "loss": 0.0021,
      "step": 25600
    },
    {
      "epoch": 2.2764444444444445,
      "grad_norm": 0.4757285416126251,
      "learning_rate": 3.577222222222222e-05,
      "loss": 0.0026,
      "step": 25610
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.3946789503097534,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.0024,
      "step": 25620
    },
    {
      "epoch": 2.2782222222222224,
      "grad_norm": 0.6077274680137634,
      "learning_rate": 3.5761111111111114e-05,
      "loss": 0.0026,
      "step": 25630
    },
    {
      "epoch": 2.279111111111111,
      "grad_norm": 0.2864452600479126,
      "learning_rate": 3.575555555555556e-05,
      "loss": 0.0032,
      "step": 25640
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.5942386388778687,
      "learning_rate": 3.575e-05,
      "loss": 0.0033,
      "step": 25650
    },
    {
      "epoch": 2.2808888888888887,
      "grad_norm": 0.3088530898094177,
      "learning_rate": 3.5744444444444445e-05,
      "loss": 0.0024,
      "step": 25660
    },
    {
      "epoch": 2.2817777777777777,
      "grad_norm": 0.7764396667480469,
      "learning_rate": 3.573888888888889e-05,
      "loss": 0.002,
      "step": 25670
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.5313599705696106,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0026,
      "step": 25680
    },
    {
      "epoch": 2.2835555555555556,
      "grad_norm": 0.6791888475418091,
      "learning_rate": 3.572777777777778e-05,
      "loss": 0.0035,
      "step": 25690
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 0.5362343192100525,
      "learning_rate": 3.5722222222222226e-05,
      "loss": 0.0023,
      "step": 25700
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.4090351164340973,
      "learning_rate": 3.571666666666667e-05,
      "loss": 0.0023,
      "step": 25710
    },
    {
      "epoch": 2.2862222222222224,
      "grad_norm": 0.9853683710098267,
      "learning_rate": 3.571111111111111e-05,
      "loss": 0.0021,
      "step": 25720
    },
    {
      "epoch": 2.287111111111111,
      "grad_norm": 0.5326047539710999,
      "learning_rate": 3.570555555555556e-05,
      "loss": 0.0028,
      "step": 25730
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.688528299331665,
      "learning_rate": 3.57e-05,
      "loss": 0.0027,
      "step": 25740
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 0.17120778560638428,
      "learning_rate": 3.5694444444444444e-05,
      "loss": 0.0027,
      "step": 25750
    },
    {
      "epoch": 2.2897777777777777,
      "grad_norm": 1.144112229347229,
      "learning_rate": 3.568888888888889e-05,
      "loss": 0.002,
      "step": 25760
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.35389864444732666,
      "learning_rate": 3.568333333333334e-05,
      "loss": 0.0017,
      "step": 25770
    },
    {
      "epoch": 2.2915555555555556,
      "grad_norm": 0.49729815125465393,
      "learning_rate": 3.567777777777778e-05,
      "loss": 0.0021,
      "step": 25780
    },
    {
      "epoch": 2.2924444444444445,
      "grad_norm": 0.092332623898983,
      "learning_rate": 3.5672222222222225e-05,
      "loss": 0.0021,
      "step": 25790
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.1828306019306183,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0022,
      "step": 25800
    },
    {
      "epoch": 2.2942222222222224,
      "grad_norm": 0.5213722586631775,
      "learning_rate": 3.566111111111111e-05,
      "loss": 0.0025,
      "step": 25810
    },
    {
      "epoch": 2.295111111111111,
      "grad_norm": 0.31580373644828796,
      "learning_rate": 3.5655555555555556e-05,
      "loss": 0.002,
      "step": 25820
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.140790194272995,
      "learning_rate": 3.565e-05,
      "loss": 0.0022,
      "step": 25830
    },
    {
      "epoch": 2.2968888888888888,
      "grad_norm": 0.6508398056030273,
      "learning_rate": 3.564444444444445e-05,
      "loss": 0.0028,
      "step": 25840
    },
    {
      "epoch": 2.2977777777777777,
      "grad_norm": 0.08382748067378998,
      "learning_rate": 3.5638888888888886e-05,
      "loss": 0.0021,
      "step": 25850
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.37176457047462463,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0021,
      "step": 25860
    },
    {
      "epoch": 2.2995555555555556,
      "grad_norm": 0.11365033686161041,
      "learning_rate": 3.562777777777778e-05,
      "loss": 0.0028,
      "step": 25870
    },
    {
      "epoch": 2.3004444444444445,
      "grad_norm": 0.6472916603088379,
      "learning_rate": 3.5622222222222224e-05,
      "loss": 0.0025,
      "step": 25880
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.9540907144546509,
      "learning_rate": 3.561666666666667e-05,
      "loss": 0.0022,
      "step": 25890
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 0.41954144835472107,
      "learning_rate": 3.561111111111111e-05,
      "loss": 0.0016,
      "step": 25900
    },
    {
      "epoch": 2.303111111111111,
      "grad_norm": 0.9442383050918579,
      "learning_rate": 3.560555555555556e-05,
      "loss": 0.0026,
      "step": 25910
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.7745288014411926,
      "learning_rate": 3.56e-05,
      "loss": 0.0023,
      "step": 25920
    },
    {
      "epoch": 2.3048888888888888,
      "grad_norm": 0.507129967212677,
      "learning_rate": 3.559444444444445e-05,
      "loss": 0.0024,
      "step": 25930
    },
    {
      "epoch": 2.3057777777777777,
      "grad_norm": 0.3014155924320221,
      "learning_rate": 3.5588888888888885e-05,
      "loss": 0.0025,
      "step": 25940
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.18050943315029144,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.0017,
      "step": 25950
    },
    {
      "epoch": 2.3075555555555556,
      "grad_norm": 0.8940924406051636,
      "learning_rate": 3.557777777777778e-05,
      "loss": 0.0031,
      "step": 25960
    },
    {
      "epoch": 2.3084444444444445,
      "grad_norm": 0.6019841432571411,
      "learning_rate": 3.557222222222222e-05,
      "loss": 0.0026,
      "step": 25970
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.5950846672058105,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.0026,
      "step": 25980
    },
    {
      "epoch": 2.3102222222222224,
      "grad_norm": 0.16606301069259644,
      "learning_rate": 3.556111111111111e-05,
      "loss": 0.0022,
      "step": 25990
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.46422797441482544,
      "learning_rate": 3.555555555555556e-05,
      "loss": 0.0034,
      "step": 26000
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.2533133029937744,
      "learning_rate": 3.555e-05,
      "loss": 0.0023,
      "step": 26010
    },
    {
      "epoch": 2.3128888888888888,
      "grad_norm": 0.21540313959121704,
      "learning_rate": 3.554444444444445e-05,
      "loss": 0.0022,
      "step": 26020
    },
    {
      "epoch": 2.3137777777777777,
      "grad_norm": 0.989422082901001,
      "learning_rate": 3.553888888888889e-05,
      "loss": 0.0024,
      "step": 26030
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.6650714874267578,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0025,
      "step": 26040
    },
    {
      "epoch": 2.3155555555555556,
      "grad_norm": 0.574883222579956,
      "learning_rate": 3.5527777777777785e-05,
      "loss": 0.0025,
      "step": 26050
    },
    {
      "epoch": 2.3164444444444445,
      "grad_norm": 0.31149765849113464,
      "learning_rate": 3.552222222222222e-05,
      "loss": 0.0021,
      "step": 26060
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.3841905891895294,
      "learning_rate": 3.551666666666667e-05,
      "loss": 0.0026,
      "step": 26070
    },
    {
      "epoch": 2.3182222222222224,
      "grad_norm": 0.6843949556350708,
      "learning_rate": 3.551111111111111e-05,
      "loss": 0.002,
      "step": 26080
    },
    {
      "epoch": 2.319111111111111,
      "grad_norm": 0.7781202793121338,
      "learning_rate": 3.550555555555556e-05,
      "loss": 0.003,
      "step": 26090
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.0904059410095215,
      "learning_rate": 3.55e-05,
      "loss": 0.0022,
      "step": 26100
    },
    {
      "epoch": 2.320888888888889,
      "grad_norm": 0.45860186219215393,
      "learning_rate": 3.5494444444444446e-05,
      "loss": 0.0027,
      "step": 26110
    },
    {
      "epoch": 2.3217777777777777,
      "grad_norm": 0.31732362508773804,
      "learning_rate": 3.548888888888889e-05,
      "loss": 0.0034,
      "step": 26120
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.8221110105514526,
      "learning_rate": 3.548333333333333e-05,
      "loss": 0.0024,
      "step": 26130
    },
    {
      "epoch": 2.3235555555555556,
      "grad_norm": 1.022871732711792,
      "learning_rate": 3.547777777777778e-05,
      "loss": 0.0026,
      "step": 26140
    },
    {
      "epoch": 2.3244444444444445,
      "grad_norm": 0.540615439414978,
      "learning_rate": 3.547222222222222e-05,
      "loss": 0.003,
      "step": 26150
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.2454783022403717,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0034,
      "step": 26160
    },
    {
      "epoch": 2.3262222222222224,
      "grad_norm": 0.32185110449790955,
      "learning_rate": 3.5461111111111114e-05,
      "loss": 0.0024,
      "step": 26170
    },
    {
      "epoch": 2.327111111111111,
      "grad_norm": 0.4881393611431122,
      "learning_rate": 3.545555555555556e-05,
      "loss": 0.0026,
      "step": 26180
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.5760512351989746,
      "learning_rate": 3.545e-05,
      "loss": 0.0016,
      "step": 26190
    },
    {
      "epoch": 2.328888888888889,
      "grad_norm": 0.44412267208099365,
      "learning_rate": 3.5444444444444445e-05,
      "loss": 0.002,
      "step": 26200
    },
    {
      "epoch": 2.3297777777777777,
      "grad_norm": 0.08579560369253159,
      "learning_rate": 3.543888888888889e-05,
      "loss": 0.0026,
      "step": 26210
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.16476328670978546,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.0035,
      "step": 26220
    },
    {
      "epoch": 2.3315555555555556,
      "grad_norm": 0.8551386594772339,
      "learning_rate": 3.542777777777778e-05,
      "loss": 0.0023,
      "step": 26230
    },
    {
      "epoch": 2.3324444444444445,
      "grad_norm": 0.1774705946445465,
      "learning_rate": 3.5422222222222226e-05,
      "loss": 0.0026,
      "step": 26240
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.6529300808906555,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.0032,
      "step": 26250
    },
    {
      "epoch": 2.3342222222222224,
      "grad_norm": 0.8105959892272949,
      "learning_rate": 3.541111111111111e-05,
      "loss": 0.0023,
      "step": 26260
    },
    {
      "epoch": 2.335111111111111,
      "grad_norm": 0.17617307603359222,
      "learning_rate": 3.5405555555555556e-05,
      "loss": 0.0027,
      "step": 26270
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.4948074221611023,
      "learning_rate": 3.54e-05,
      "loss": 0.003,
      "step": 26280
    },
    {
      "epoch": 2.336888888888889,
      "grad_norm": 0.7688909769058228,
      "learning_rate": 3.5394444444444443e-05,
      "loss": 0.0024,
      "step": 26290
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 0.6071161031723022,
      "learning_rate": 3.538888888888889e-05,
      "loss": 0.0022,
      "step": 26300
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.3173976540565491,
      "learning_rate": 3.538333333333334e-05,
      "loss": 0.0027,
      "step": 26310
    },
    {
      "epoch": 2.3395555555555556,
      "grad_norm": 0.16917714476585388,
      "learning_rate": 3.537777777777778e-05,
      "loss": 0.003,
      "step": 26320
    },
    {
      "epoch": 2.3404444444444445,
      "grad_norm": 0.2724236249923706,
      "learning_rate": 3.5372222222222224e-05,
      "loss": 0.0024,
      "step": 26330
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.4708923399448395,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0028,
      "step": 26340
    },
    {
      "epoch": 2.3422222222222224,
      "grad_norm": 0.674128532409668,
      "learning_rate": 3.536111111111111e-05,
      "loss": 0.0032,
      "step": 26350
    },
    {
      "epoch": 2.343111111111111,
      "grad_norm": 0.243967205286026,
      "learning_rate": 3.5355555555555555e-05,
      "loss": 0.0023,
      "step": 26360
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.8628557920455933,
      "learning_rate": 3.535e-05,
      "loss": 0.0022,
      "step": 26370
    },
    {
      "epoch": 2.344888888888889,
      "grad_norm": 0.4695512354373932,
      "learning_rate": 3.534444444444445e-05,
      "loss": 0.0018,
      "step": 26380
    },
    {
      "epoch": 2.3457777777777777,
      "grad_norm": 0.8219414949417114,
      "learning_rate": 3.5338888888888886e-05,
      "loss": 0.004,
      "step": 26390
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.20445233583450317,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.003,
      "step": 26400
    },
    {
      "epoch": 2.3475555555555556,
      "grad_norm": 0.37230968475341797,
      "learning_rate": 3.532777777777778e-05,
      "loss": 0.0023,
      "step": 26410
    },
    {
      "epoch": 2.3484444444444446,
      "grad_norm": 0.4952593147754669,
      "learning_rate": 3.532222222222222e-05,
      "loss": 0.002,
      "step": 26420
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.9555950164794922,
      "learning_rate": 3.531666666666667e-05,
      "loss": 0.0019,
      "step": 26430
    },
    {
      "epoch": 2.3502222222222224,
      "grad_norm": 0.9670407772064209,
      "learning_rate": 3.531111111111111e-05,
      "loss": 0.002,
      "step": 26440
    },
    {
      "epoch": 2.351111111111111,
      "grad_norm": 0.3983702063560486,
      "learning_rate": 3.530555555555556e-05,
      "loss": 0.0027,
      "step": 26450
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.21891088783740997,
      "learning_rate": 3.53e-05,
      "loss": 0.0024,
      "step": 26460
    },
    {
      "epoch": 2.352888888888889,
      "grad_norm": 0.6353740096092224,
      "learning_rate": 3.529444444444445e-05,
      "loss": 0.0016,
      "step": 26470
    },
    {
      "epoch": 2.3537777777777777,
      "grad_norm": 0.5582025647163391,
      "learning_rate": 3.528888888888889e-05,
      "loss": 0.002,
      "step": 26480
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.2221251279115677,
      "learning_rate": 3.5283333333333335e-05,
      "loss": 0.0026,
      "step": 26490
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.10678553581237793,
      "learning_rate": 3.527777777777778e-05,
      "loss": 0.0016,
      "step": 26500
    },
    {
      "epoch": 2.3564444444444446,
      "grad_norm": 0.0714225247502327,
      "learning_rate": 3.527222222222222e-05,
      "loss": 0.0017,
      "step": 26510
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.47799307107925415,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0032,
      "step": 26520
    },
    {
      "epoch": 2.3582222222222224,
      "grad_norm": 0.29078319668769836,
      "learning_rate": 3.526111111111111e-05,
      "loss": 0.0023,
      "step": 26530
    },
    {
      "epoch": 2.359111111111111,
      "grad_norm": 0.08775701373815536,
      "learning_rate": 3.525555555555556e-05,
      "loss": 0.0023,
      "step": 26540
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.1467732191085815,
      "learning_rate": 3.525e-05,
      "loss": 0.002,
      "step": 26550
    },
    {
      "epoch": 2.360888888888889,
      "grad_norm": 1.2680655717849731,
      "learning_rate": 3.5244444444444447e-05,
      "loss": 0.0029,
      "step": 26560
    },
    {
      "epoch": 2.3617777777777778,
      "grad_norm": 0.6313455700874329,
      "learning_rate": 3.523888888888889e-05,
      "loss": 0.0032,
      "step": 26570
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.8992683291435242,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.003,
      "step": 26580
    },
    {
      "epoch": 2.3635555555555556,
      "grad_norm": 0.5548825860023499,
      "learning_rate": 3.5227777777777784e-05,
      "loss": 0.0022,
      "step": 26590
    },
    {
      "epoch": 2.3644444444444446,
      "grad_norm": 0.46347925066947937,
      "learning_rate": 3.522222222222222e-05,
      "loss": 0.002,
      "step": 26600
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.5991708636283875,
      "learning_rate": 3.521666666666667e-05,
      "loss": 0.0029,
      "step": 26610
    },
    {
      "epoch": 2.3662222222222224,
      "grad_norm": 0.8922024369239807,
      "learning_rate": 3.5211111111111115e-05,
      "loss": 0.0021,
      "step": 26620
    },
    {
      "epoch": 2.367111111111111,
      "grad_norm": 0.8344564437866211,
      "learning_rate": 3.520555555555556e-05,
      "loss": 0.0037,
      "step": 26630
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.5841671228408813,
      "learning_rate": 3.52e-05,
      "loss": 0.0018,
      "step": 26640
    },
    {
      "epoch": 2.368888888888889,
      "grad_norm": 0.9843143820762634,
      "learning_rate": 3.5194444444444445e-05,
      "loss": 0.0035,
      "step": 26650
    },
    {
      "epoch": 2.3697777777777778,
      "grad_norm": 0.3938450515270233,
      "learning_rate": 3.518888888888889e-05,
      "loss": 0.0018,
      "step": 26660
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.2838304936885834,
      "learning_rate": 3.518333333333333e-05,
      "loss": 0.0025,
      "step": 26670
    },
    {
      "epoch": 2.3715555555555556,
      "grad_norm": 0.27107667922973633,
      "learning_rate": 3.517777777777778e-05,
      "loss": 0.0016,
      "step": 26680
    },
    {
      "epoch": 2.3724444444444446,
      "grad_norm": 0.1287258267402649,
      "learning_rate": 3.5172222222222226e-05,
      "loss": 0.0019,
      "step": 26690
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.409498006105423,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0031,
      "step": 26700
    },
    {
      "epoch": 2.3742222222222225,
      "grad_norm": 0.11106536537408829,
      "learning_rate": 3.5161111111111113e-05,
      "loss": 0.0019,
      "step": 26710
    },
    {
      "epoch": 2.375111111111111,
      "grad_norm": 0.6726621985435486,
      "learning_rate": 3.515555555555556e-05,
      "loss": 0.0034,
      "step": 26720
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.519340991973877,
      "learning_rate": 3.515e-05,
      "loss": 0.0026,
      "step": 26730
    },
    {
      "epoch": 2.376888888888889,
      "grad_norm": 0.35183945298194885,
      "learning_rate": 3.5144444444444444e-05,
      "loss": 0.0025,
      "step": 26740
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 0.2879728376865387,
      "learning_rate": 3.513888888888889e-05,
      "loss": 0.0021,
      "step": 26750
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.7452712655067444,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0027,
      "step": 26760
    },
    {
      "epoch": 2.3795555555555556,
      "grad_norm": 0.18798750638961792,
      "learning_rate": 3.512777777777778e-05,
      "loss": 0.0018,
      "step": 26770
    },
    {
      "epoch": 2.3804444444444446,
      "grad_norm": 0.25862932205200195,
      "learning_rate": 3.5122222222222225e-05,
      "loss": 0.0016,
      "step": 26780
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.4195109009742737,
      "learning_rate": 3.511666666666667e-05,
      "loss": 0.0023,
      "step": 26790
    },
    {
      "epoch": 2.3822222222222225,
      "grad_norm": 0.058069128543138504,
      "learning_rate": 3.511111111111111e-05,
      "loss": 0.0025,
      "step": 26800
    },
    {
      "epoch": 2.383111111111111,
      "grad_norm": 0.5560081005096436,
      "learning_rate": 3.5105555555555556e-05,
      "loss": 0.0027,
      "step": 26810
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.13306355476379395,
      "learning_rate": 3.51e-05,
      "loss": 0.003,
      "step": 26820
    },
    {
      "epoch": 2.384888888888889,
      "grad_norm": 1.0593371391296387,
      "learning_rate": 3.509444444444445e-05,
      "loss": 0.0023,
      "step": 26830
    },
    {
      "epoch": 2.3857777777777778,
      "grad_norm": 0.3523918390274048,
      "learning_rate": 3.5088888888888886e-05,
      "loss": 0.0016,
      "step": 26840
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.437776654958725,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.002,
      "step": 26850
    },
    {
      "epoch": 2.3875555555555557,
      "grad_norm": 0.032811857759952545,
      "learning_rate": 3.507777777777778e-05,
      "loss": 0.0022,
      "step": 26860
    },
    {
      "epoch": 2.3884444444444446,
      "grad_norm": 0.16590262949466705,
      "learning_rate": 3.5072222222222224e-05,
      "loss": 0.002,
      "step": 26870
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.5253440141677856,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0022,
      "step": 26880
    },
    {
      "epoch": 2.3902222222222225,
      "grad_norm": 0.9886720776557922,
      "learning_rate": 3.506111111111111e-05,
      "loss": 0.0025,
      "step": 26890
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 1.4271024465560913,
      "learning_rate": 3.505555555555556e-05,
      "loss": 0.0026,
      "step": 26900
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.5069551467895508,
      "learning_rate": 3.505e-05,
      "loss": 0.002,
      "step": 26910
    },
    {
      "epoch": 2.392888888888889,
      "grad_norm": 0.22347716987133026,
      "learning_rate": 3.504444444444445e-05,
      "loss": 0.0027,
      "step": 26920
    },
    {
      "epoch": 2.393777777777778,
      "grad_norm": 0.6697896718978882,
      "learning_rate": 3.503888888888889e-05,
      "loss": 0.0024,
      "step": 26930
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.8369995951652527,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.0026,
      "step": 26940
    },
    {
      "epoch": 2.3955555555555557,
      "grad_norm": 0.2579675018787384,
      "learning_rate": 3.502777777777778e-05,
      "loss": 0.0023,
      "step": 26950
    },
    {
      "epoch": 2.3964444444444446,
      "grad_norm": 0.13128231465816498,
      "learning_rate": 3.502222222222222e-05,
      "loss": 0.002,
      "step": 26960
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.12691953778266907,
      "learning_rate": 3.501666666666667e-05,
      "loss": 0.0034,
      "step": 26970
    },
    {
      "epoch": 2.398222222222222,
      "grad_norm": 0.3265548348426819,
      "learning_rate": 3.501111111111111e-05,
      "loss": 0.0031,
      "step": 26980
    },
    {
      "epoch": 2.399111111111111,
      "grad_norm": 0.6049241423606873,
      "learning_rate": 3.500555555555556e-05,
      "loss": 0.002,
      "step": 26990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.4003966450691223,
      "learning_rate": 3.5e-05,
      "loss": 0.0033,
      "step": 27000
    },
    {
      "epoch": 2.400888888888889,
      "grad_norm": 0.4798155128955841,
      "learning_rate": 3.499444444444445e-05,
      "loss": 0.0025,
      "step": 27010
    },
    {
      "epoch": 2.401777777777778,
      "grad_norm": 0.3685644268989563,
      "learning_rate": 3.498888888888889e-05,
      "loss": 0.0023,
      "step": 27020
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.4351603090763092,
      "learning_rate": 3.4983333333333334e-05,
      "loss": 0.0022,
      "step": 27030
    },
    {
      "epoch": 2.4035555555555557,
      "grad_norm": 0.3851787745952606,
      "learning_rate": 3.4977777777777785e-05,
      "loss": 0.0023,
      "step": 27040
    },
    {
      "epoch": 2.4044444444444446,
      "grad_norm": 0.9452514052391052,
      "learning_rate": 3.497222222222222e-05,
      "loss": 0.0018,
      "step": 27050
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.9360339641571045,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0019,
      "step": 27060
    },
    {
      "epoch": 2.406222222222222,
      "grad_norm": 0.22110003232955933,
      "learning_rate": 3.496111111111111e-05,
      "loss": 0.0017,
      "step": 27070
    },
    {
      "epoch": 2.407111111111111,
      "grad_norm": 0.8824238777160645,
      "learning_rate": 3.495555555555556e-05,
      "loss": 0.0022,
      "step": 27080
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.5909814834594727,
      "learning_rate": 3.495e-05,
      "loss": 0.004,
      "step": 27090
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 0.5659312605857849,
      "learning_rate": 3.4944444444444446e-05,
      "loss": 0.0013,
      "step": 27100
    },
    {
      "epoch": 2.409777777777778,
      "grad_norm": 0.14127850532531738,
      "learning_rate": 3.4938888888888896e-05,
      "loss": 0.0028,
      "step": 27110
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.6191480159759521,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0015,
      "step": 27120
    },
    {
      "epoch": 2.4115555555555557,
      "grad_norm": 0.7820058465003967,
      "learning_rate": 3.4927777777777783e-05,
      "loss": 0.0017,
      "step": 27130
    },
    {
      "epoch": 2.4124444444444446,
      "grad_norm": 0.6451665163040161,
      "learning_rate": 3.492222222222222e-05,
      "loss": 0.002,
      "step": 27140
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.5595142841339111,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.0025,
      "step": 27150
    },
    {
      "epoch": 2.414222222222222,
      "grad_norm": 0.3306925892829895,
      "learning_rate": 3.4911111111111114e-05,
      "loss": 0.0027,
      "step": 27160
    },
    {
      "epoch": 2.415111111111111,
      "grad_norm": 0.6829439401626587,
      "learning_rate": 3.490555555555556e-05,
      "loss": 0.0033,
      "step": 27170
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.4506123661994934,
      "learning_rate": 3.49e-05,
      "loss": 0.0033,
      "step": 27180
    },
    {
      "epoch": 2.416888888888889,
      "grad_norm": 0.2871730327606201,
      "learning_rate": 3.4894444444444445e-05,
      "loss": 0.0019,
      "step": 27190
    },
    {
      "epoch": 2.417777777777778,
      "grad_norm": 0.3223070204257965,
      "learning_rate": 3.4888888888888895e-05,
      "loss": 0.0029,
      "step": 27200
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.6266412138938904,
      "learning_rate": 3.488333333333333e-05,
      "loss": 0.0017,
      "step": 27210
    },
    {
      "epoch": 2.4195555555555557,
      "grad_norm": 0.26009583473205566,
      "learning_rate": 3.487777777777778e-05,
      "loss": 0.0018,
      "step": 27220
    },
    {
      "epoch": 2.4204444444444446,
      "grad_norm": 0.41875147819519043,
      "learning_rate": 3.4872222222222226e-05,
      "loss": 0.0022,
      "step": 27230
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.38111382722854614,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0022,
      "step": 27240
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 0.07544776797294617,
      "learning_rate": 3.486111111111111e-05,
      "loss": 0.0019,
      "step": 27250
    },
    {
      "epoch": 2.423111111111111,
      "grad_norm": 0.6029061675071716,
      "learning_rate": 3.4855555555555557e-05,
      "loss": 0.0032,
      "step": 27260
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.7683882117271423,
      "learning_rate": 3.485e-05,
      "loss": 0.0014,
      "step": 27270
    },
    {
      "epoch": 2.424888888888889,
      "grad_norm": 0.6906382441520691,
      "learning_rate": 3.4844444444444444e-05,
      "loss": 0.0023,
      "step": 27280
    },
    {
      "epoch": 2.425777777777778,
      "grad_norm": 0.49518704414367676,
      "learning_rate": 3.4838888888888894e-05,
      "loss": 0.002,
      "step": 27290
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.8409715294837952,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0017,
      "step": 27300
    },
    {
      "epoch": 2.4275555555555557,
      "grad_norm": 0.24995285272598267,
      "learning_rate": 3.482777777777778e-05,
      "loss": 0.0018,
      "step": 27310
    },
    {
      "epoch": 2.4284444444444446,
      "grad_norm": 0.913221538066864,
      "learning_rate": 3.4822222222222225e-05,
      "loss": 0.0024,
      "step": 27320
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.2831592261791229,
      "learning_rate": 3.481666666666667e-05,
      "loss": 0.002,
      "step": 27330
    },
    {
      "epoch": 2.430222222222222,
      "grad_norm": 0.7632331848144531,
      "learning_rate": 3.481111111111111e-05,
      "loss": 0.0028,
      "step": 27340
    },
    {
      "epoch": 2.431111111111111,
      "grad_norm": 0.3253639042377472,
      "learning_rate": 3.4805555555555555e-05,
      "loss": 0.0024,
      "step": 27350
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.5286646485328674,
      "learning_rate": 3.48e-05,
      "loss": 0.002,
      "step": 27360
    },
    {
      "epoch": 2.432888888888889,
      "grad_norm": 0.3194611668586731,
      "learning_rate": 3.479444444444445e-05,
      "loss": 0.0024,
      "step": 27370
    },
    {
      "epoch": 2.433777777777778,
      "grad_norm": 0.7669044733047485,
      "learning_rate": 3.478888888888889e-05,
      "loss": 0.0021,
      "step": 27380
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.19211861491203308,
      "learning_rate": 3.4783333333333336e-05,
      "loss": 0.002,
      "step": 27390
    },
    {
      "epoch": 2.4355555555555557,
      "grad_norm": 0.14629997313022614,
      "learning_rate": 3.477777777777778e-05,
      "loss": 0.0023,
      "step": 27400
    },
    {
      "epoch": 2.4364444444444446,
      "grad_norm": 0.1328483670949936,
      "learning_rate": 3.4772222222222223e-05,
      "loss": 0.0015,
      "step": 27410
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.2855823040008545,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0017,
      "step": 27420
    },
    {
      "epoch": 2.438222222222222,
      "grad_norm": 0.7841634750366211,
      "learning_rate": 3.476111111111111e-05,
      "loss": 0.0021,
      "step": 27430
    },
    {
      "epoch": 2.439111111111111,
      "grad_norm": 1.0307981967926025,
      "learning_rate": 3.475555555555556e-05,
      "loss": 0.003,
      "step": 27440
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.4729728698730469,
      "learning_rate": 3.475e-05,
      "loss": 0.0017,
      "step": 27450
    },
    {
      "epoch": 2.440888888888889,
      "grad_norm": 0.37160032987594604,
      "learning_rate": 3.474444444444445e-05,
      "loss": 0.0023,
      "step": 27460
    },
    {
      "epoch": 2.441777777777778,
      "grad_norm": 0.4007793068885803,
      "learning_rate": 3.473888888888889e-05,
      "loss": 0.0024,
      "step": 27470
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.11436975002288818,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0027,
      "step": 27480
    },
    {
      "epoch": 2.4435555555555557,
      "grad_norm": 0.36547353863716125,
      "learning_rate": 3.472777777777778e-05,
      "loss": 0.0019,
      "step": 27490
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.27513930201530457,
      "learning_rate": 3.472222222222222e-05,
      "loss": 0.0014,
      "step": 27500
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.3604929745197296,
      "learning_rate": 3.471666666666667e-05,
      "loss": 0.002,
      "step": 27510
    },
    {
      "epoch": 2.446222222222222,
      "grad_norm": 0.5203403830528259,
      "learning_rate": 3.471111111111111e-05,
      "loss": 0.0021,
      "step": 27520
    },
    {
      "epoch": 2.447111111111111,
      "grad_norm": 0.8220989108085632,
      "learning_rate": 3.470555555555556e-05,
      "loss": 0.0024,
      "step": 27530
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.12691377103328705,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0034,
      "step": 27540
    },
    {
      "epoch": 2.448888888888889,
      "grad_norm": 0.3548600673675537,
      "learning_rate": 3.469444444444445e-05,
      "loss": 0.0025,
      "step": 27550
    },
    {
      "epoch": 2.449777777777778,
      "grad_norm": 0.1555689126253128,
      "learning_rate": 3.468888888888889e-05,
      "loss": 0.0037,
      "step": 27560
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.11793538928031921,
      "learning_rate": 3.4683333333333334e-05,
      "loss": 0.0034,
      "step": 27570
    },
    {
      "epoch": 2.4515555555555557,
      "grad_norm": 0.3150448203086853,
      "learning_rate": 3.4677777777777784e-05,
      "loss": 0.0034,
      "step": 27580
    },
    {
      "epoch": 2.4524444444444446,
      "grad_norm": 0.19825007021427155,
      "learning_rate": 3.467222222222222e-05,
      "loss": 0.0022,
      "step": 27590
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.9688475728034973,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0039,
      "step": 27600
    },
    {
      "epoch": 2.454222222222222,
      "grad_norm": 0.31380799412727356,
      "learning_rate": 3.466111111111111e-05,
      "loss": 0.0027,
      "step": 27610
    },
    {
      "epoch": 2.455111111111111,
      "grad_norm": 0.8174859881401062,
      "learning_rate": 3.465555555555556e-05,
      "loss": 0.0022,
      "step": 27620
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.24582615494728088,
      "learning_rate": 3.465e-05,
      "loss": 0.003,
      "step": 27630
    },
    {
      "epoch": 2.456888888888889,
      "grad_norm": 0.840013861656189,
      "learning_rate": 3.4644444444444446e-05,
      "loss": 0.0031,
      "step": 27640
    },
    {
      "epoch": 2.457777777777778,
      "grad_norm": 0.9589875340461731,
      "learning_rate": 3.4638888888888896e-05,
      "loss": 0.0027,
      "step": 27650
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.16610324382781982,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0032,
      "step": 27660
    },
    {
      "epoch": 2.4595555555555557,
      "grad_norm": 0.4931756556034088,
      "learning_rate": 3.462777777777778e-05,
      "loss": 0.0033,
      "step": 27670
    },
    {
      "epoch": 2.4604444444444447,
      "grad_norm": 0.39820781350135803,
      "learning_rate": 3.462222222222222e-05,
      "loss": 0.0031,
      "step": 27680
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.6011402606964111,
      "learning_rate": 3.461666666666667e-05,
      "loss": 0.0024,
      "step": 27690
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 0.32052433490753174,
      "learning_rate": 3.4611111111111114e-05,
      "loss": 0.0026,
      "step": 27700
    },
    {
      "epoch": 2.463111111111111,
      "grad_norm": 0.4062584638595581,
      "learning_rate": 3.460555555555556e-05,
      "loss": 0.003,
      "step": 27710
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.5721102952957153,
      "learning_rate": 3.46e-05,
      "loss": 0.0032,
      "step": 27720
    },
    {
      "epoch": 2.464888888888889,
      "grad_norm": 0.37194016575813293,
      "learning_rate": 3.4594444444444444e-05,
      "loss": 0.0039,
      "step": 27730
    },
    {
      "epoch": 2.465777777777778,
      "grad_norm": 0.46825286746025085,
      "learning_rate": 3.4588888888888895e-05,
      "loss": 0.0025,
      "step": 27740
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.485627681016922,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.0021,
      "step": 27750
    },
    {
      "epoch": 2.4675555555555557,
      "grad_norm": 0.9653117060661316,
      "learning_rate": 3.457777777777778e-05,
      "loss": 0.0017,
      "step": 27760
    },
    {
      "epoch": 2.4684444444444447,
      "grad_norm": 0.16888019442558289,
      "learning_rate": 3.4572222222222225e-05,
      "loss": 0.0027,
      "step": 27770
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.364641934633255,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0033,
      "step": 27780
    },
    {
      "epoch": 2.470222222222222,
      "grad_norm": 1.1515754461288452,
      "learning_rate": 3.456111111111111e-05,
      "loss": 0.0024,
      "step": 27790
    },
    {
      "epoch": 2.471111111111111,
      "grad_norm": 0.6677913665771484,
      "learning_rate": 3.4555555555555556e-05,
      "loss": 0.0023,
      "step": 27800
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.07667209208011627,
      "learning_rate": 3.455e-05,
      "loss": 0.0025,
      "step": 27810
    },
    {
      "epoch": 2.472888888888889,
      "grad_norm": 0.7288843393325806,
      "learning_rate": 3.454444444444444e-05,
      "loss": 0.0022,
      "step": 27820
    },
    {
      "epoch": 2.473777777777778,
      "grad_norm": 0.5648863315582275,
      "learning_rate": 3.4538888888888893e-05,
      "loss": 0.0022,
      "step": 27830
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.04649626091122627,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.003,
      "step": 27840
    },
    {
      "epoch": 2.4755555555555557,
      "grad_norm": 0.21596641838550568,
      "learning_rate": 3.452777777777778e-05,
      "loss": 0.0024,
      "step": 27850
    },
    {
      "epoch": 2.4764444444444447,
      "grad_norm": 0.24565225839614868,
      "learning_rate": 3.4522222222222224e-05,
      "loss": 0.003,
      "step": 27860
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.835236132144928,
      "learning_rate": 3.451666666666667e-05,
      "loss": 0.0019,
      "step": 27870
    },
    {
      "epoch": 2.478222222222222,
      "grad_norm": 0.09514320641756058,
      "learning_rate": 3.451111111111111e-05,
      "loss": 0.0016,
      "step": 27880
    },
    {
      "epoch": 2.479111111111111,
      "grad_norm": 0.37001192569732666,
      "learning_rate": 3.4505555555555555e-05,
      "loss": 0.0023,
      "step": 27890
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.42418378591537476,
      "learning_rate": 3.45e-05,
      "loss": 0.0025,
      "step": 27900
    },
    {
      "epoch": 2.480888888888889,
      "grad_norm": 0.4418700933456421,
      "learning_rate": 3.449444444444445e-05,
      "loss": 0.0024,
      "step": 27910
    },
    {
      "epoch": 2.481777777777778,
      "grad_norm": 0.0461287759244442,
      "learning_rate": 3.448888888888889e-05,
      "loss": 0.0023,
      "step": 27920
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.066718190908432,
      "learning_rate": 3.4483333333333336e-05,
      "loss": 0.0017,
      "step": 27930
    },
    {
      "epoch": 2.4835555555555557,
      "grad_norm": 0.053312283009290695,
      "learning_rate": 3.447777777777778e-05,
      "loss": 0.0025,
      "step": 27940
    },
    {
      "epoch": 2.4844444444444447,
      "grad_norm": 0.8447019457817078,
      "learning_rate": 3.447222222222222e-05,
      "loss": 0.0029,
      "step": 27950
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.8943308591842651,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0019,
      "step": 27960
    },
    {
      "epoch": 2.486222222222222,
      "grad_norm": 0.7509899735450745,
      "learning_rate": 3.446111111111111e-05,
      "loss": 0.0025,
      "step": 27970
    },
    {
      "epoch": 2.487111111111111,
      "grad_norm": 0.411906361579895,
      "learning_rate": 3.445555555555556e-05,
      "loss": 0.0021,
      "step": 27980
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.395824134349823,
      "learning_rate": 3.445e-05,
      "loss": 0.0022,
      "step": 27990
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 0.3346814215183258,
      "learning_rate": 3.444444444444445e-05,
      "loss": 0.0026,
      "step": 28000
    },
    {
      "epoch": 2.489777777777778,
      "grad_norm": 0.43372276425361633,
      "learning_rate": 3.443888888888889e-05,
      "loss": 0.0021,
      "step": 28010
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.8784082531929016,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0021,
      "step": 28020
    },
    {
      "epoch": 2.4915555555555557,
      "grad_norm": 0.39831534028053284,
      "learning_rate": 3.442777777777778e-05,
      "loss": 0.0019,
      "step": 28030
    },
    {
      "epoch": 2.4924444444444447,
      "grad_norm": 0.7298811078071594,
      "learning_rate": 3.442222222222222e-05,
      "loss": 0.0036,
      "step": 28040
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.7702329754829407,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.0019,
      "step": 28050
    },
    {
      "epoch": 2.494222222222222,
      "grad_norm": 0.344577819108963,
      "learning_rate": 3.441111111111111e-05,
      "loss": 0.0022,
      "step": 28060
    },
    {
      "epoch": 2.495111111111111,
      "grad_norm": 0.4857500493526459,
      "learning_rate": 3.440555555555556e-05,
      "loss": 0.002,
      "step": 28070
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.049355749040842056,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0023,
      "step": 28080
    },
    {
      "epoch": 2.496888888888889,
      "grad_norm": 0.813575804233551,
      "learning_rate": 3.4394444444444446e-05,
      "loss": 0.0025,
      "step": 28090
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 0.9008129835128784,
      "learning_rate": 3.438888888888889e-05,
      "loss": 0.0021,
      "step": 28100
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 1.034346103668213,
      "learning_rate": 3.438333333333333e-05,
      "loss": 0.0025,
      "step": 28110
    },
    {
      "epoch": 2.4995555555555553,
      "grad_norm": 0.5392261743545532,
      "learning_rate": 3.4377777777777784e-05,
      "loss": 0.0042,
      "step": 28120
    },
    {
      "epoch": 2.5004444444444447,
      "grad_norm": 0.6667331457138062,
      "learning_rate": 3.437222222222222e-05,
      "loss": 0.0021,
      "step": 28130
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.9332578778266907,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.0021,
      "step": 28140
    },
    {
      "epoch": 2.502222222222222,
      "grad_norm": 0.792788565158844,
      "learning_rate": 3.436111111111111e-05,
      "loss": 0.0025,
      "step": 28150
    },
    {
      "epoch": 2.503111111111111,
      "grad_norm": 0.6572220921516418,
      "learning_rate": 3.435555555555556e-05,
      "loss": 0.002,
      "step": 28160
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.6389933228492737,
      "learning_rate": 3.435e-05,
      "loss": 0.0023,
      "step": 28170
    },
    {
      "epoch": 2.504888888888889,
      "grad_norm": 0.20503106713294983,
      "learning_rate": 3.4344444444444445e-05,
      "loss": 0.0029,
      "step": 28180
    },
    {
      "epoch": 2.505777777777778,
      "grad_norm": 0.561052143573761,
      "learning_rate": 3.4338888888888895e-05,
      "loss": 0.0029,
      "step": 28190
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.5205896496772766,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0027,
      "step": 28200
    },
    {
      "epoch": 2.5075555555555553,
      "grad_norm": 1.0991557836532593,
      "learning_rate": 3.432777777777778e-05,
      "loss": 0.0024,
      "step": 28210
    },
    {
      "epoch": 2.5084444444444447,
      "grad_norm": 0.6080067157745361,
      "learning_rate": 3.432222222222222e-05,
      "loss": 0.0025,
      "step": 28220
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.2550901174545288,
      "learning_rate": 3.431666666666667e-05,
      "loss": 0.0037,
      "step": 28230
    },
    {
      "epoch": 2.510222222222222,
      "grad_norm": 0.13437335193157196,
      "learning_rate": 3.431111111111111e-05,
      "loss": 0.0029,
      "step": 28240
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 0.352131187915802,
      "learning_rate": 3.430555555555556e-05,
      "loss": 0.0023,
      "step": 28250
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.09656854718923569,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0023,
      "step": 28260
    },
    {
      "epoch": 2.512888888888889,
      "grad_norm": 0.3554595708847046,
      "learning_rate": 3.4294444444444444e-05,
      "loss": 0.0026,
      "step": 28270
    },
    {
      "epoch": 2.513777777777778,
      "grad_norm": 0.2531810998916626,
      "learning_rate": 3.4288888888888894e-05,
      "loss": 0.003,
      "step": 28280
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.21236492693424225,
      "learning_rate": 3.428333333333333e-05,
      "loss": 0.0032,
      "step": 28290
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 0.1648254096508026,
      "learning_rate": 3.427777777777778e-05,
      "loss": 0.0026,
      "step": 28300
    },
    {
      "epoch": 2.5164444444444447,
      "grad_norm": 0.2701777219772339,
      "learning_rate": 3.4272222222222225e-05,
      "loss": 0.0026,
      "step": 28310
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.5301327705383301,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0032,
      "step": 28320
    },
    {
      "epoch": 2.518222222222222,
      "grad_norm": 0.6125340461730957,
      "learning_rate": 3.426111111111111e-05,
      "loss": 0.0014,
      "step": 28330
    },
    {
      "epoch": 2.519111111111111,
      "grad_norm": 0.19058899581432343,
      "learning_rate": 3.4255555555555555e-05,
      "loss": 0.0025,
      "step": 28340
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.4717346429824829,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.0021,
      "step": 28350
    },
    {
      "epoch": 2.520888888888889,
      "grad_norm": 0.4658415913581848,
      "learning_rate": 3.424444444444444e-05,
      "loss": 0.0019,
      "step": 28360
    },
    {
      "epoch": 2.521777777777778,
      "grad_norm": 0.372690886259079,
      "learning_rate": 3.423888888888889e-05,
      "loss": 0.0026,
      "step": 28370
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.3062589466571808,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.0017,
      "step": 28380
    },
    {
      "epoch": 2.5235555555555553,
      "grad_norm": 0.420554518699646,
      "learning_rate": 3.422777777777778e-05,
      "loss": 0.0026,
      "step": 28390
    },
    {
      "epoch": 2.5244444444444447,
      "grad_norm": 0.16053295135498047,
      "learning_rate": 3.4222222222222224e-05,
      "loss": 0.0028,
      "step": 28400
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.27626925706863403,
      "learning_rate": 3.421666666666667e-05,
      "loss": 0.0018,
      "step": 28410
    },
    {
      "epoch": 2.526222222222222,
      "grad_norm": 0.3321644365787506,
      "learning_rate": 3.421111111111111e-05,
      "loss": 0.0027,
      "step": 28420
    },
    {
      "epoch": 2.527111111111111,
      "grad_norm": 0.5941082239151001,
      "learning_rate": 3.4205555555555554e-05,
      "loss": 0.0025,
      "step": 28430
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.46879300475120544,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0027,
      "step": 28440
    },
    {
      "epoch": 2.528888888888889,
      "grad_norm": 0.20103415846824646,
      "learning_rate": 3.419444444444445e-05,
      "loss": 0.0021,
      "step": 28450
    },
    {
      "epoch": 2.529777777777778,
      "grad_norm": 0.7604840397834778,
      "learning_rate": 3.418888888888889e-05,
      "loss": 0.0027,
      "step": 28460
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.6832479238510132,
      "learning_rate": 3.4183333333333335e-05,
      "loss": 0.0023,
      "step": 28470
    },
    {
      "epoch": 2.5315555555555553,
      "grad_norm": 0.3568335175514221,
      "learning_rate": 3.417777777777778e-05,
      "loss": 0.0028,
      "step": 28480
    },
    {
      "epoch": 2.5324444444444447,
      "grad_norm": 0.8759757280349731,
      "learning_rate": 3.417222222222222e-05,
      "loss": 0.0017,
      "step": 28490
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.3665562868118286,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0024,
      "step": 28500
    },
    {
      "epoch": 2.534222222222222,
      "grad_norm": 0.06754149496555328,
      "learning_rate": 3.416111111111111e-05,
      "loss": 0.0014,
      "step": 28510
    },
    {
      "epoch": 2.535111111111111,
      "grad_norm": 0.4030958414077759,
      "learning_rate": 3.415555555555556e-05,
      "loss": 0.0014,
      "step": 28520
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.7401310801506042,
      "learning_rate": 3.415e-05,
      "loss": 0.0024,
      "step": 28530
    },
    {
      "epoch": 2.536888888888889,
      "grad_norm": 0.9413635730743408,
      "learning_rate": 3.414444444444445e-05,
      "loss": 0.0016,
      "step": 28540
    },
    {
      "epoch": 2.537777777777778,
      "grad_norm": 0.46611642837524414,
      "learning_rate": 3.413888888888889e-05,
      "loss": 0.0023,
      "step": 28550
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.39393144845962524,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0023,
      "step": 28560
    },
    {
      "epoch": 2.5395555555555553,
      "grad_norm": 0.8369591236114502,
      "learning_rate": 3.412777777777778e-05,
      "loss": 0.0023,
      "step": 28570
    },
    {
      "epoch": 2.5404444444444443,
      "grad_norm": 0.41426393389701843,
      "learning_rate": 3.412222222222222e-05,
      "loss": 0.0015,
      "step": 28580
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.3241340219974518,
      "learning_rate": 3.411666666666667e-05,
      "loss": 0.0034,
      "step": 28590
    },
    {
      "epoch": 2.542222222222222,
      "grad_norm": 0.4208705723285675,
      "learning_rate": 3.411111111111111e-05,
      "loss": 0.0031,
      "step": 28600
    },
    {
      "epoch": 2.543111111111111,
      "grad_norm": 0.6333622932434082,
      "learning_rate": 3.410555555555556e-05,
      "loss": 0.0017,
      "step": 28610
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.23897890746593475,
      "learning_rate": 3.41e-05,
      "loss": 0.0021,
      "step": 28620
    },
    {
      "epoch": 2.544888888888889,
      "grad_norm": 0.31729254126548767,
      "learning_rate": 3.4094444444444446e-05,
      "loss": 0.0031,
      "step": 28630
    },
    {
      "epoch": 2.545777777777778,
      "grad_norm": 0.4642428755760193,
      "learning_rate": 3.408888888888889e-05,
      "loss": 0.0017,
      "step": 28640
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.3437844216823578,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.002,
      "step": 28650
    },
    {
      "epoch": 2.5475555555555554,
      "grad_norm": 0.3087756335735321,
      "learning_rate": 3.407777777777778e-05,
      "loss": 0.0031,
      "step": 28660
    },
    {
      "epoch": 2.5484444444444443,
      "grad_norm": 0.7538783550262451,
      "learning_rate": 3.407222222222222e-05,
      "loss": 0.003,
      "step": 28670
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.2547553479671478,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.002,
      "step": 28680
    },
    {
      "epoch": 2.550222222222222,
      "grad_norm": 0.20211954414844513,
      "learning_rate": 3.406111111111111e-05,
      "loss": 0.0036,
      "step": 28690
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 0.7355943918228149,
      "learning_rate": 3.405555555555556e-05,
      "loss": 0.0021,
      "step": 28700
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.5131437182426453,
      "learning_rate": 3.405e-05,
      "loss": 0.0015,
      "step": 28710
    },
    {
      "epoch": 2.552888888888889,
      "grad_norm": 0.39872032403945923,
      "learning_rate": 3.4044444444444445e-05,
      "loss": 0.0023,
      "step": 28720
    },
    {
      "epoch": 2.553777777777778,
      "grad_norm": 0.2885037660598755,
      "learning_rate": 3.4038888888888895e-05,
      "loss": 0.0022,
      "step": 28730
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.4554520547389984,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0029,
      "step": 28740
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.5648812055587769,
      "learning_rate": 3.402777777777778e-05,
      "loss": 0.0023,
      "step": 28750
    },
    {
      "epoch": 2.5564444444444443,
      "grad_norm": 0.4022546112537384,
      "learning_rate": 3.402222222222222e-05,
      "loss": 0.0029,
      "step": 28760
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.4420677721500397,
      "learning_rate": 3.401666666666667e-05,
      "loss": 0.0022,
      "step": 28770
    },
    {
      "epoch": 2.558222222222222,
      "grad_norm": 0.5674460530281067,
      "learning_rate": 3.401111111111111e-05,
      "loss": 0.0031,
      "step": 28780
    },
    {
      "epoch": 2.559111111111111,
      "grad_norm": 0.1638827621936798,
      "learning_rate": 3.4005555555555556e-05,
      "loss": 0.0026,
      "step": 28790
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.174891859292984,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0033,
      "step": 28800
    },
    {
      "epoch": 2.560888888888889,
      "grad_norm": 0.39056408405303955,
      "learning_rate": 3.399444444444444e-05,
      "loss": 0.0019,
      "step": 28810
    },
    {
      "epoch": 2.561777777777778,
      "grad_norm": 0.8226529955863953,
      "learning_rate": 3.3988888888888894e-05,
      "loss": 0.0015,
      "step": 28820
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.027703700587153435,
      "learning_rate": 3.398333333333333e-05,
      "loss": 0.0016,
      "step": 28830
    },
    {
      "epoch": 2.5635555555555554,
      "grad_norm": 0.7003198266029358,
      "learning_rate": 3.397777777777778e-05,
      "loss": 0.002,
      "step": 28840
    },
    {
      "epoch": 2.5644444444444443,
      "grad_norm": 0.8145462274551392,
      "learning_rate": 3.3972222222222224e-05,
      "loss": 0.0022,
      "step": 28850
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.23540543019771576,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.0034,
      "step": 28860
    },
    {
      "epoch": 2.566222222222222,
      "grad_norm": 0.3642362952232361,
      "learning_rate": 3.396111111111111e-05,
      "loss": 0.0035,
      "step": 28870
    },
    {
      "epoch": 2.567111111111111,
      "grad_norm": 0.3699345886707306,
      "learning_rate": 3.3955555555555555e-05,
      "loss": 0.002,
      "step": 28880
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.22595249116420746,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 0.002,
      "step": 28890
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 0.41692885756492615,
      "learning_rate": 3.394444444444444e-05,
      "loss": 0.0025,
      "step": 28900
    },
    {
      "epoch": 2.569777777777778,
      "grad_norm": 0.4567442536354065,
      "learning_rate": 3.393888888888889e-05,
      "loss": 0.0033,
      "step": 28910
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.5488905310630798,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0025,
      "step": 28920
    },
    {
      "epoch": 2.5715555555555554,
      "grad_norm": 0.7824094295501709,
      "learning_rate": 3.392777777777778e-05,
      "loss": 0.0029,
      "step": 28930
    },
    {
      "epoch": 2.5724444444444443,
      "grad_norm": 0.18079787492752075,
      "learning_rate": 3.392222222222222e-05,
      "loss": 0.0022,
      "step": 28940
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.5693182349205017,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.0028,
      "step": 28950
    },
    {
      "epoch": 2.574222222222222,
      "grad_norm": 0.8447777032852173,
      "learning_rate": 3.391111111111111e-05,
      "loss": 0.0019,
      "step": 28960
    },
    {
      "epoch": 2.575111111111111,
      "grad_norm": 0.177887961268425,
      "learning_rate": 3.3905555555555554e-05,
      "loss": 0.0037,
      "step": 28970
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.14883534610271454,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.0018,
      "step": 28980
    },
    {
      "epoch": 2.576888888888889,
      "grad_norm": 0.5491712689399719,
      "learning_rate": 3.389444444444445e-05,
      "loss": 0.0017,
      "step": 28990
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 0.37805941700935364,
      "learning_rate": 3.388888888888889e-05,
      "loss": 0.0015,
      "step": 29000
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.2536865472793579,
      "learning_rate": 3.3883333333333335e-05,
      "loss": 0.0023,
      "step": 29010
    },
    {
      "epoch": 2.5795555555555554,
      "grad_norm": 0.7077340483665466,
      "learning_rate": 3.387777777777778e-05,
      "loss": 0.0034,
      "step": 29020
    },
    {
      "epoch": 2.5804444444444443,
      "grad_norm": 0.6544398665428162,
      "learning_rate": 3.387222222222222e-05,
      "loss": 0.0016,
      "step": 29030
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.061932627111673355,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0027,
      "step": 29040
    },
    {
      "epoch": 2.582222222222222,
      "grad_norm": 0.1650814712047577,
      "learning_rate": 3.386111111111111e-05,
      "loss": 0.0019,
      "step": 29050
    },
    {
      "epoch": 2.583111111111111,
      "grad_norm": 0.4389287233352661,
      "learning_rate": 3.385555555555556e-05,
      "loss": 0.002,
      "step": 29060
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.6639138460159302,
      "learning_rate": 3.385e-05,
      "loss": 0.0017,
      "step": 29070
    },
    {
      "epoch": 2.584888888888889,
      "grad_norm": 0.08772048354148865,
      "learning_rate": 3.3844444444444446e-05,
      "loss": 0.0022,
      "step": 29080
    },
    {
      "epoch": 2.5857777777777775,
      "grad_norm": 0.4654632806777954,
      "learning_rate": 3.383888888888889e-05,
      "loss": 0.003,
      "step": 29090
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.4889490306377411,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0031,
      "step": 29100
    },
    {
      "epoch": 2.5875555555555554,
      "grad_norm": 0.5780754089355469,
      "learning_rate": 3.382777777777778e-05,
      "loss": 0.0029,
      "step": 29110
    },
    {
      "epoch": 2.5884444444444443,
      "grad_norm": 0.11457937955856323,
      "learning_rate": 3.382222222222222e-05,
      "loss": 0.0028,
      "step": 29120
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.14776606857776642,
      "learning_rate": 3.381666666666667e-05,
      "loss": 0.0028,
      "step": 29130
    },
    {
      "epoch": 2.590222222222222,
      "grad_norm": 0.4759984612464905,
      "learning_rate": 3.381111111111111e-05,
      "loss": 0.0017,
      "step": 29140
    },
    {
      "epoch": 2.591111111111111,
      "grad_norm": 0.35894668102264404,
      "learning_rate": 3.380555555555556e-05,
      "loss": 0.0026,
      "step": 29150
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.6297957301139832,
      "learning_rate": 3.38e-05,
      "loss": 0.0032,
      "step": 29160
    },
    {
      "epoch": 2.592888888888889,
      "grad_norm": 0.7986164093017578,
      "learning_rate": 3.3794444444444445e-05,
      "loss": 0.0019,
      "step": 29170
    },
    {
      "epoch": 2.5937777777777775,
      "grad_norm": 0.8223668932914734,
      "learning_rate": 3.378888888888889e-05,
      "loss": 0.0027,
      "step": 29180
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 1.1146647930145264,
      "learning_rate": 3.378333333333333e-05,
      "loss": 0.0024,
      "step": 29190
    },
    {
      "epoch": 2.5955555555555554,
      "grad_norm": 0.6179339289665222,
      "learning_rate": 3.377777777777778e-05,
      "loss": 0.0023,
      "step": 29200
    },
    {
      "epoch": 2.5964444444444443,
      "grad_norm": 0.5673720836639404,
      "learning_rate": 3.377222222222222e-05,
      "loss": 0.0021,
      "step": 29210
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.7599166631698608,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0025,
      "step": 29220
    },
    {
      "epoch": 2.598222222222222,
      "grad_norm": 0.24821189045906067,
      "learning_rate": 3.376111111111111e-05,
      "loss": 0.0031,
      "step": 29230
    },
    {
      "epoch": 2.599111111111111,
      "grad_norm": 0.8308858275413513,
      "learning_rate": 3.375555555555556e-05,
      "loss": 0.0026,
      "step": 29240
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.4424164593219757,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.0027,
      "step": 29250
    },
    {
      "epoch": 2.600888888888889,
      "grad_norm": 0.18374720215797424,
      "learning_rate": 3.3744444444444444e-05,
      "loss": 0.0018,
      "step": 29260
    },
    {
      "epoch": 2.6017777777777775,
      "grad_norm": 0.5064289569854736,
      "learning_rate": 3.3738888888888894e-05,
      "loss": 0.0026,
      "step": 29270
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 1.085768461227417,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0024,
      "step": 29280
    },
    {
      "epoch": 2.6035555555555554,
      "grad_norm": 1.2767975330352783,
      "learning_rate": 3.372777777777778e-05,
      "loss": 0.003,
      "step": 29290
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 0.46148067712783813,
      "learning_rate": 3.3722222222222225e-05,
      "loss": 0.0036,
      "step": 29300
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.7944685816764832,
      "learning_rate": 3.371666666666667e-05,
      "loss": 0.003,
      "step": 29310
    },
    {
      "epoch": 2.606222222222222,
      "grad_norm": 0.4486769139766693,
      "learning_rate": 3.371111111111111e-05,
      "loss": 0.0019,
      "step": 29320
    },
    {
      "epoch": 2.607111111111111,
      "grad_norm": 0.8613355755805969,
      "learning_rate": 3.3705555555555556e-05,
      "loss": 0.002,
      "step": 29330
    },
    {
      "epoch": 2.608,
      "grad_norm": 1.0622252225875854,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0026,
      "step": 29340
    },
    {
      "epoch": 2.608888888888889,
      "grad_norm": 1.1538097858428955,
      "learning_rate": 3.369444444444444e-05,
      "loss": 0.0021,
      "step": 29350
    },
    {
      "epoch": 2.6097777777777775,
      "grad_norm": 0.24541836977005005,
      "learning_rate": 3.368888888888889e-05,
      "loss": 0.0016,
      "step": 29360
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 1.0832618474960327,
      "learning_rate": 3.368333333333334e-05,
      "loss": 0.0033,
      "step": 29370
    },
    {
      "epoch": 2.6115555555555554,
      "grad_norm": 0.6200047731399536,
      "learning_rate": 3.367777777777778e-05,
      "loss": 0.0022,
      "step": 29380
    },
    {
      "epoch": 2.6124444444444443,
      "grad_norm": 0.547995388507843,
      "learning_rate": 3.3672222222222224e-05,
      "loss": 0.002,
      "step": 29390
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.5129825472831726,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0031,
      "step": 29400
    },
    {
      "epoch": 2.6142222222222222,
      "grad_norm": 0.06625784188508987,
      "learning_rate": 3.366111111111112e-05,
      "loss": 0.0028,
      "step": 29410
    },
    {
      "epoch": 2.615111111111111,
      "grad_norm": 0.6545724868774414,
      "learning_rate": 3.3655555555555554e-05,
      "loss": 0.0021,
      "step": 29420
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.7364770770072937,
      "learning_rate": 3.3650000000000005e-05,
      "loss": 0.0025,
      "step": 29430
    },
    {
      "epoch": 2.616888888888889,
      "grad_norm": 0.9225594997406006,
      "learning_rate": 3.364444444444445e-05,
      "loss": 0.0018,
      "step": 29440
    },
    {
      "epoch": 2.6177777777777775,
      "grad_norm": 1.2932579517364502,
      "learning_rate": 3.363888888888889e-05,
      "loss": 0.0018,
      "step": 29450
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.6163527369499207,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0027,
      "step": 29460
    },
    {
      "epoch": 2.6195555555555554,
      "grad_norm": 0.1953635960817337,
      "learning_rate": 3.362777777777778e-05,
      "loss": 0.0018,
      "step": 29470
    },
    {
      "epoch": 2.6204444444444444,
      "grad_norm": 0.4695698320865631,
      "learning_rate": 3.362222222222222e-05,
      "loss": 0.002,
      "step": 29480
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.1919395476579666,
      "learning_rate": 3.3616666666666666e-05,
      "loss": 0.0022,
      "step": 29490
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.20762115716934204,
      "learning_rate": 3.3611111111111116e-05,
      "loss": 0.0021,
      "step": 29500
    },
    {
      "epoch": 2.623111111111111,
      "grad_norm": 0.6870273351669312,
      "learning_rate": 3.360555555555556e-05,
      "loss": 0.0022,
      "step": 29510
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.39681774377822876,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0024,
      "step": 29520
    },
    {
      "epoch": 2.624888888888889,
      "grad_norm": 1.022382140159607,
      "learning_rate": 3.359444444444445e-05,
      "loss": 0.0022,
      "step": 29530
    },
    {
      "epoch": 2.6257777777777775,
      "grad_norm": 0.5831717252731323,
      "learning_rate": 3.358888888888889e-05,
      "loss": 0.0017,
      "step": 29540
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.5072121620178223,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.0019,
      "step": 29550
    },
    {
      "epoch": 2.6275555555555554,
      "grad_norm": 0.3115695118904114,
      "learning_rate": 3.357777777777778e-05,
      "loss": 0.0024,
      "step": 29560
    },
    {
      "epoch": 2.6284444444444444,
      "grad_norm": 0.5001634359359741,
      "learning_rate": 3.357222222222222e-05,
      "loss": 0.0022,
      "step": 29570
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 1.0388497114181519,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0025,
      "step": 29580
    },
    {
      "epoch": 2.6302222222222222,
      "grad_norm": 1.191946029663086,
      "learning_rate": 3.3561111111111115e-05,
      "loss": 0.0024,
      "step": 29590
    },
    {
      "epoch": 2.631111111111111,
      "grad_norm": 0.9414240121841431,
      "learning_rate": 3.355555555555556e-05,
      "loss": 0.0034,
      "step": 29600
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.1448981612920761,
      "learning_rate": 3.355e-05,
      "loss": 0.0019,
      "step": 29610
    },
    {
      "epoch": 2.632888888888889,
      "grad_norm": 0.0645461231470108,
      "learning_rate": 3.3544444444444446e-05,
      "loss": 0.0029,
      "step": 29620
    },
    {
      "epoch": 2.6337777777777776,
      "grad_norm": 0.47086626291275024,
      "learning_rate": 3.353888888888889e-05,
      "loss": 0.0028,
      "step": 29630
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.6965651512145996,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0031,
      "step": 29640
    },
    {
      "epoch": 2.6355555555555554,
      "grad_norm": 0.8781707286834717,
      "learning_rate": 3.352777777777778e-05,
      "loss": 0.0022,
      "step": 29650
    },
    {
      "epoch": 2.6364444444444444,
      "grad_norm": 0.6653184294700623,
      "learning_rate": 3.352222222222222e-05,
      "loss": 0.003,
      "step": 29660
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.29275748133659363,
      "learning_rate": 3.351666666666667e-05,
      "loss": 0.0022,
      "step": 29670
    },
    {
      "epoch": 2.6382222222222222,
      "grad_norm": 0.48856469988822937,
      "learning_rate": 3.3511111111111114e-05,
      "loss": 0.0017,
      "step": 29680
    },
    {
      "epoch": 2.639111111111111,
      "grad_norm": 0.1609528362751007,
      "learning_rate": 3.350555555555556e-05,
      "loss": 0.0027,
      "step": 29690
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.2719493806362152,
      "learning_rate": 3.35e-05,
      "loss": 0.0017,
      "step": 29700
    },
    {
      "epoch": 2.640888888888889,
      "grad_norm": 0.4372851550579071,
      "learning_rate": 3.3494444444444445e-05,
      "loss": 0.0028,
      "step": 29710
    },
    {
      "epoch": 2.6417777777777776,
      "grad_norm": 0.3070428967475891,
      "learning_rate": 3.3488888888888895e-05,
      "loss": 0.0017,
      "step": 29720
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.8275814652442932,
      "learning_rate": 3.348333333333333e-05,
      "loss": 0.0022,
      "step": 29730
    },
    {
      "epoch": 2.6435555555555554,
      "grad_norm": 0.16416017711162567,
      "learning_rate": 3.347777777777778e-05,
      "loss": 0.0019,
      "step": 29740
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 0.9011327624320984,
      "learning_rate": 3.347222222222222e-05,
      "loss": 0.0023,
      "step": 29750
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.23346106708049774,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.002,
      "step": 29760
    },
    {
      "epoch": 2.6462222222222223,
      "grad_norm": 0.20492549240589142,
      "learning_rate": 3.346111111111111e-05,
      "loss": 0.002,
      "step": 29770
    },
    {
      "epoch": 2.647111111111111,
      "grad_norm": 0.8334800601005554,
      "learning_rate": 3.3455555555555556e-05,
      "loss": 0.0016,
      "step": 29780
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.9557654857635498,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.002,
      "step": 29790
    },
    {
      "epoch": 2.648888888888889,
      "grad_norm": 0.4482618570327759,
      "learning_rate": 3.3444444444444443e-05,
      "loss": 0.0021,
      "step": 29800
    },
    {
      "epoch": 2.6497777777777776,
      "grad_norm": 0.06502720713615417,
      "learning_rate": 3.3438888888888894e-05,
      "loss": 0.003,
      "step": 29810
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.7632255554199219,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0022,
      "step": 29820
    },
    {
      "epoch": 2.6515555555555554,
      "grad_norm": 0.1582251787185669,
      "learning_rate": 3.342777777777778e-05,
      "loss": 0.0026,
      "step": 29830
    },
    {
      "epoch": 2.6524444444444444,
      "grad_norm": 0.3787616491317749,
      "learning_rate": 3.3422222222222224e-05,
      "loss": 0.0027,
      "step": 29840
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.11486861109733582,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.0026,
      "step": 29850
    },
    {
      "epoch": 2.6542222222222223,
      "grad_norm": 0.3884222209453583,
      "learning_rate": 3.341111111111112e-05,
      "loss": 0.0026,
      "step": 29860
    },
    {
      "epoch": 2.655111111111111,
      "grad_norm": 0.15195947885513306,
      "learning_rate": 3.3405555555555555e-05,
      "loss": 0.0026,
      "step": 29870
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.23708775639533997,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0017,
      "step": 29880
    },
    {
      "epoch": 2.656888888888889,
      "grad_norm": 0.5094687938690186,
      "learning_rate": 3.339444444444444e-05,
      "loss": 0.0027,
      "step": 29890
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 0.6558244824409485,
      "learning_rate": 3.338888888888889e-05,
      "loss": 0.0028,
      "step": 29900
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.6207109689712524,
      "learning_rate": 3.3383333333333336e-05,
      "loss": 0.0019,
      "step": 29910
    },
    {
      "epoch": 2.6595555555555555,
      "grad_norm": 0.772977888584137,
      "learning_rate": 3.337777777777778e-05,
      "loss": 0.0016,
      "step": 29920
    },
    {
      "epoch": 2.6604444444444444,
      "grad_norm": 0.26212257146835327,
      "learning_rate": 3.337222222222222e-05,
      "loss": 0.0024,
      "step": 29930
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.5843886733055115,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.0031,
      "step": 29940
    },
    {
      "epoch": 2.6622222222222223,
      "grad_norm": 0.23237939178943634,
      "learning_rate": 3.336111111111112e-05,
      "loss": 0.003,
      "step": 29950
    },
    {
      "epoch": 2.663111111111111,
      "grad_norm": 0.2306891828775406,
      "learning_rate": 3.3355555555555554e-05,
      "loss": 0.0016,
      "step": 29960
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.8667995929718018,
      "learning_rate": 3.3350000000000004e-05,
      "loss": 0.0026,
      "step": 29970
    },
    {
      "epoch": 2.664888888888889,
      "grad_norm": 0.2579137980937958,
      "learning_rate": 3.334444444444445e-05,
      "loss": 0.0025,
      "step": 29980
    },
    {
      "epoch": 2.6657777777777776,
      "grad_norm": 0.4217664301395416,
      "learning_rate": 3.333888888888889e-05,
      "loss": 0.0022,
      "step": 29990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.29220134019851685,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.002,
      "step": 30000
    },
    {
      "epoch": 2.6675555555555555,
      "grad_norm": 0.20894700288772583,
      "learning_rate": 3.332777777777778e-05,
      "loss": 0.0028,
      "step": 30010
    },
    {
      "epoch": 2.6684444444444444,
      "grad_norm": 0.5124140977859497,
      "learning_rate": 3.332222222222222e-05,
      "loss": 0.0033,
      "step": 30020
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.2547711730003357,
      "learning_rate": 3.3316666666666666e-05,
      "loss": 0.0021,
      "step": 30030
    },
    {
      "epoch": 2.6702222222222223,
      "grad_norm": 0.17496375739574432,
      "learning_rate": 3.3311111111111116e-05,
      "loss": 0.003,
      "step": 30040
    },
    {
      "epoch": 2.671111111111111,
      "grad_norm": 0.16856399178504944,
      "learning_rate": 3.330555555555556e-05,
      "loss": 0.0021,
      "step": 30050
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.603717029094696,
      "learning_rate": 3.33e-05,
      "loss": 0.0028,
      "step": 30060
    },
    {
      "epoch": 2.672888888888889,
      "grad_norm": 0.9308272004127502,
      "learning_rate": 3.3294444444444447e-05,
      "loss": 0.0026,
      "step": 30070
    },
    {
      "epoch": 2.6737777777777776,
      "grad_norm": 0.051981959491968155,
      "learning_rate": 3.328888888888889e-05,
      "loss": 0.0021,
      "step": 30080
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.5277704000473022,
      "learning_rate": 3.3283333333333334e-05,
      "loss": 0.0039,
      "step": 30090
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 0.24906080961227417,
      "learning_rate": 3.327777777777778e-05,
      "loss": 0.003,
      "step": 30100
    },
    {
      "epoch": 2.6764444444444444,
      "grad_norm": 0.31852760910987854,
      "learning_rate": 3.327222222222222e-05,
      "loss": 0.0019,
      "step": 30110
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.3894694745540619,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0023,
      "step": 30120
    },
    {
      "epoch": 2.6782222222222223,
      "grad_norm": 0.2912903130054474,
      "learning_rate": 3.3261111111111115e-05,
      "loss": 0.002,
      "step": 30130
    },
    {
      "epoch": 2.679111111111111,
      "grad_norm": 0.45666006207466125,
      "learning_rate": 3.325555555555556e-05,
      "loss": 0.0029,
      "step": 30140
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.15148548781871796,
      "learning_rate": 3.325e-05,
      "loss": 0.0026,
      "step": 30150
    },
    {
      "epoch": 2.680888888888889,
      "grad_norm": 0.30310481786727905,
      "learning_rate": 3.3244444444444445e-05,
      "loss": 0.0027,
      "step": 30160
    },
    {
      "epoch": 2.6817777777777776,
      "grad_norm": 0.23881462216377258,
      "learning_rate": 3.323888888888889e-05,
      "loss": 0.0023,
      "step": 30170
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.06508970260620117,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0037,
      "step": 30180
    },
    {
      "epoch": 2.6835555555555555,
      "grad_norm": 0.6688724160194397,
      "learning_rate": 3.322777777777778e-05,
      "loss": 0.0023,
      "step": 30190
    },
    {
      "epoch": 2.6844444444444444,
      "grad_norm": 0.20982174575328827,
      "learning_rate": 3.322222222222222e-05,
      "loss": 0.0034,
      "step": 30200
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.7104576230049133,
      "learning_rate": 3.321666666666667e-05,
      "loss": 0.0024,
      "step": 30210
    },
    {
      "epoch": 2.6862222222222223,
      "grad_norm": 0.4436163008213043,
      "learning_rate": 3.3211111111111114e-05,
      "loss": 0.0016,
      "step": 30220
    },
    {
      "epoch": 2.6871111111111112,
      "grad_norm": 0.6903302669525146,
      "learning_rate": 3.320555555555556e-05,
      "loss": 0.0026,
      "step": 30230
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.4111264944076538,
      "learning_rate": 3.32e-05,
      "loss": 0.0021,
      "step": 30240
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 0.8949745893478394,
      "learning_rate": 3.3194444444444444e-05,
      "loss": 0.0021,
      "step": 30250
    },
    {
      "epoch": 2.6897777777777776,
      "grad_norm": 0.3236440420150757,
      "learning_rate": 3.3188888888888895e-05,
      "loss": 0.0021,
      "step": 30260
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.6053745746612549,
      "learning_rate": 3.318333333333333e-05,
      "loss": 0.003,
      "step": 30270
    },
    {
      "epoch": 2.6915555555555555,
      "grad_norm": 0.638679027557373,
      "learning_rate": 3.317777777777778e-05,
      "loss": 0.0026,
      "step": 30280
    },
    {
      "epoch": 2.6924444444444444,
      "grad_norm": 0.9277963638305664,
      "learning_rate": 3.317222222222222e-05,
      "loss": 0.0029,
      "step": 30290
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.3243945240974426,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0024,
      "step": 30300
    },
    {
      "epoch": 2.6942222222222223,
      "grad_norm": 0.39370429515838623,
      "learning_rate": 3.316111111111111e-05,
      "loss": 0.0017,
      "step": 30310
    },
    {
      "epoch": 2.6951111111111112,
      "grad_norm": 0.15650533139705658,
      "learning_rate": 3.3155555555555556e-05,
      "loss": 0.0023,
      "step": 30320
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.5232114195823669,
      "learning_rate": 3.3150000000000006e-05,
      "loss": 0.0035,
      "step": 30330
    },
    {
      "epoch": 2.696888888888889,
      "grad_norm": 0.33062729239463806,
      "learning_rate": 3.314444444444444e-05,
      "loss": 0.0025,
      "step": 30340
    },
    {
      "epoch": 2.6977777777777776,
      "grad_norm": 0.07866120338439941,
      "learning_rate": 3.313888888888889e-05,
      "loss": 0.0026,
      "step": 30350
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.2483253926038742,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0026,
      "step": 30360
    },
    {
      "epoch": 2.6995555555555555,
      "grad_norm": 0.8821409344673157,
      "learning_rate": 3.312777777777778e-05,
      "loss": 0.0021,
      "step": 30370
    },
    {
      "epoch": 2.7004444444444444,
      "grad_norm": 0.4435872435569763,
      "learning_rate": 3.3122222222222224e-05,
      "loss": 0.0019,
      "step": 30380
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.40629684925079346,
      "learning_rate": 3.311666666666667e-05,
      "loss": 0.0029,
      "step": 30390
    },
    {
      "epoch": 2.7022222222222223,
      "grad_norm": 0.1477484405040741,
      "learning_rate": 3.311111111111112e-05,
      "loss": 0.0015,
      "step": 30400
    },
    {
      "epoch": 2.7031111111111112,
      "grad_norm": 0.3816154897212982,
      "learning_rate": 3.3105555555555555e-05,
      "loss": 0.0018,
      "step": 30410
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.2559383511543274,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.002,
      "step": 30420
    },
    {
      "epoch": 2.704888888888889,
      "grad_norm": 0.14642173051834106,
      "learning_rate": 3.309444444444444e-05,
      "loss": 0.0029,
      "step": 30430
    },
    {
      "epoch": 2.7057777777777776,
      "grad_norm": 0.06919564306735992,
      "learning_rate": 3.308888888888889e-05,
      "loss": 0.0022,
      "step": 30440
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.4887726902961731,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.0025,
      "step": 30450
    },
    {
      "epoch": 2.7075555555555555,
      "grad_norm": 0.35451552271842957,
      "learning_rate": 3.307777777777778e-05,
      "loss": 0.0026,
      "step": 30460
    },
    {
      "epoch": 2.7084444444444444,
      "grad_norm": 0.3181701898574829,
      "learning_rate": 3.307222222222222e-05,
      "loss": 0.0017,
      "step": 30470
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.5109812617301941,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0019,
      "step": 30480
    },
    {
      "epoch": 2.7102222222222223,
      "grad_norm": 0.08483577519655228,
      "learning_rate": 3.306111111111112e-05,
      "loss": 0.0023,
      "step": 30490
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.7747336626052856,
      "learning_rate": 3.3055555555555553e-05,
      "loss": 0.0016,
      "step": 30500
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.15693160891532898,
      "learning_rate": 3.3050000000000004e-05,
      "loss": 0.0022,
      "step": 30510
    },
    {
      "epoch": 2.712888888888889,
      "grad_norm": 1.0328433513641357,
      "learning_rate": 3.304444444444445e-05,
      "loss": 0.0021,
      "step": 30520
    },
    {
      "epoch": 2.7137777777777776,
      "grad_norm": 0.5121794939041138,
      "learning_rate": 3.303888888888889e-05,
      "loss": 0.0019,
      "step": 30530
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.6470869779586792,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.0018,
      "step": 30540
    },
    {
      "epoch": 2.7155555555555555,
      "grad_norm": 0.18506023287773132,
      "learning_rate": 3.302777777777778e-05,
      "loss": 0.0018,
      "step": 30550
    },
    {
      "epoch": 2.7164444444444444,
      "grad_norm": 0.9452977180480957,
      "learning_rate": 3.302222222222222e-05,
      "loss": 0.0026,
      "step": 30560
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.12716229259967804,
      "learning_rate": 3.3016666666666665e-05,
      "loss": 0.0029,
      "step": 30570
    },
    {
      "epoch": 2.7182222222222223,
      "grad_norm": 0.17675797641277313,
      "learning_rate": 3.3011111111111115e-05,
      "loss": 0.0025,
      "step": 30580
    },
    {
      "epoch": 2.7191111111111113,
      "grad_norm": 0.13592663407325745,
      "learning_rate": 3.300555555555556e-05,
      "loss": 0.002,
      "step": 30590
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.9608720541000366,
      "learning_rate": 3.3e-05,
      "loss": 0.0035,
      "step": 30600
    },
    {
      "epoch": 2.720888888888889,
      "grad_norm": 0.5251182317733765,
      "learning_rate": 3.2994444444444446e-05,
      "loss": 0.0022,
      "step": 30610
    },
    {
      "epoch": 2.7217777777777776,
      "grad_norm": 0.9293516278266907,
      "learning_rate": 3.298888888888889e-05,
      "loss": 0.0019,
      "step": 30620
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.45391741394996643,
      "learning_rate": 3.298333333333333e-05,
      "loss": 0.0023,
      "step": 30630
    },
    {
      "epoch": 2.7235555555555555,
      "grad_norm": 0.5986284017562866,
      "learning_rate": 3.297777777777778e-05,
      "loss": 0.0032,
      "step": 30640
    },
    {
      "epoch": 2.7244444444444444,
      "grad_norm": 0.05409287288784981,
      "learning_rate": 3.297222222222223e-05,
      "loss": 0.0025,
      "step": 30650
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.8201692700386047,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0021,
      "step": 30660
    },
    {
      "epoch": 2.7262222222222223,
      "grad_norm": 0.17110666632652283,
      "learning_rate": 3.2961111111111114e-05,
      "loss": 0.0019,
      "step": 30670
    },
    {
      "epoch": 2.7271111111111113,
      "grad_norm": 0.8802658319473267,
      "learning_rate": 3.295555555555556e-05,
      "loss": 0.0022,
      "step": 30680
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.8897795677185059,
      "learning_rate": 3.295e-05,
      "loss": 0.0024,
      "step": 30690
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 0.4814694821834564,
      "learning_rate": 3.2944444444444445e-05,
      "loss": 0.0017,
      "step": 30700
    },
    {
      "epoch": 2.7297777777777776,
      "grad_norm": 0.205091655254364,
      "learning_rate": 3.293888888888889e-05,
      "loss": 0.0044,
      "step": 30710
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.43337056040763855,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.003,
      "step": 30720
    },
    {
      "epoch": 2.7315555555555555,
      "grad_norm": 0.1618335247039795,
      "learning_rate": 3.292777777777778e-05,
      "loss": 0.002,
      "step": 30730
    },
    {
      "epoch": 2.7324444444444445,
      "grad_norm": 1.0279844999313354,
      "learning_rate": 3.2922222222222226e-05,
      "loss": 0.0026,
      "step": 30740
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.4231985807418823,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.0022,
      "step": 30750
    },
    {
      "epoch": 2.7342222222222223,
      "grad_norm": 0.27034714818000793,
      "learning_rate": 3.291111111111111e-05,
      "loss": 0.0027,
      "step": 30760
    },
    {
      "epoch": 2.7351111111111113,
      "grad_norm": 0.4416516125202179,
      "learning_rate": 3.2905555555555557e-05,
      "loss": 0.0019,
      "step": 30770
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.05305976793169975,
      "learning_rate": 3.29e-05,
      "loss": 0.0024,
      "step": 30780
    },
    {
      "epoch": 2.736888888888889,
      "grad_norm": 0.36752310395240784,
      "learning_rate": 3.2894444444444444e-05,
      "loss": 0.0018,
      "step": 30790
    },
    {
      "epoch": 2.7377777777777776,
      "grad_norm": 0.10035966336727142,
      "learning_rate": 3.2888888888888894e-05,
      "loss": 0.002,
      "step": 30800
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.43787896633148193,
      "learning_rate": 3.288333333333333e-05,
      "loss": 0.002,
      "step": 30810
    },
    {
      "epoch": 2.7395555555555555,
      "grad_norm": 0.3846684396266937,
      "learning_rate": 3.287777777777778e-05,
      "loss": 0.0025,
      "step": 30820
    },
    {
      "epoch": 2.7404444444444445,
      "grad_norm": 0.16220466792583466,
      "learning_rate": 3.2872222222222225e-05,
      "loss": 0.0026,
      "step": 30830
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.739020824432373,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.003,
      "step": 30840
    },
    {
      "epoch": 2.7422222222222223,
      "grad_norm": 0.09218467772006989,
      "learning_rate": 3.286111111111111e-05,
      "loss": 0.0021,
      "step": 30850
    },
    {
      "epoch": 2.7431111111111113,
      "grad_norm": 0.5382272005081177,
      "learning_rate": 3.2855555555555555e-05,
      "loss": 0.0024,
      "step": 30860
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.4795345664024353,
      "learning_rate": 3.2850000000000006e-05,
      "loss": 0.0026,
      "step": 30870
    },
    {
      "epoch": 2.744888888888889,
      "grad_norm": 0.5498571395874023,
      "learning_rate": 3.284444444444444e-05,
      "loss": 0.0021,
      "step": 30880
    },
    {
      "epoch": 2.7457777777777777,
      "grad_norm": 0.396279513835907,
      "learning_rate": 3.283888888888889e-05,
      "loss": 0.0027,
      "step": 30890
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.44591769576072693,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0024,
      "step": 30900
    },
    {
      "epoch": 2.7475555555555555,
      "grad_norm": 0.364694207906723,
      "learning_rate": 3.282777777777778e-05,
      "loss": 0.0037,
      "step": 30910
    },
    {
      "epoch": 2.7484444444444445,
      "grad_norm": 0.9806600213050842,
      "learning_rate": 3.2822222222222223e-05,
      "loss": 0.0028,
      "step": 30920
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.10464870929718018,
      "learning_rate": 3.281666666666667e-05,
      "loss": 0.0016,
      "step": 30930
    },
    {
      "epoch": 2.7502222222222223,
      "grad_norm": 0.20384393632411957,
      "learning_rate": 3.281111111111112e-05,
      "loss": 0.0022,
      "step": 30940
    },
    {
      "epoch": 2.7511111111111113,
      "grad_norm": 0.36863604187965393,
      "learning_rate": 3.2805555555555554e-05,
      "loss": 0.0035,
      "step": 30950
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.23717601597309113,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0022,
      "step": 30960
    },
    {
      "epoch": 2.752888888888889,
      "grad_norm": 0.09862146526575089,
      "learning_rate": 3.279444444444444e-05,
      "loss": 0.0024,
      "step": 30970
    },
    {
      "epoch": 2.7537777777777777,
      "grad_norm": 0.15996888279914856,
      "learning_rate": 3.278888888888889e-05,
      "loss": 0.0029,
      "step": 30980
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.3887880742549896,
      "learning_rate": 3.2783333333333335e-05,
      "loss": 0.0023,
      "step": 30990
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 1.1406787633895874,
      "learning_rate": 3.277777777777778e-05,
      "loss": 0.0024,
      "step": 31000
    },
    {
      "epoch": 2.7564444444444445,
      "grad_norm": 0.43898940086364746,
      "learning_rate": 3.277222222222223e-05,
      "loss": 0.0029,
      "step": 31010
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.3911413848400116,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.0022,
      "step": 31020
    },
    {
      "epoch": 2.7582222222222224,
      "grad_norm": 0.4266147017478943,
      "learning_rate": 3.2761111111111116e-05,
      "loss": 0.003,
      "step": 31030
    },
    {
      "epoch": 2.7591111111111113,
      "grad_norm": 0.7846307754516602,
      "learning_rate": 3.275555555555555e-05,
      "loss": 0.0018,
      "step": 31040
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.6500762104988098,
      "learning_rate": 3.275e-05,
      "loss": 0.0036,
      "step": 31050
    },
    {
      "epoch": 2.7608888888888887,
      "grad_norm": 0.5058670043945312,
      "learning_rate": 3.274444444444445e-05,
      "loss": 0.0024,
      "step": 31060
    },
    {
      "epoch": 2.7617777777777777,
      "grad_norm": 0.7720459699630737,
      "learning_rate": 3.273888888888889e-05,
      "loss": 0.0026,
      "step": 31070
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.11120252311229706,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0024,
      "step": 31080
    },
    {
      "epoch": 2.7635555555555555,
      "grad_norm": 0.7576864957809448,
      "learning_rate": 3.272777777777778e-05,
      "loss": 0.0038,
      "step": 31090
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 0.21345758438110352,
      "learning_rate": 3.272222222222223e-05,
      "loss": 0.0025,
      "step": 31100
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 1.115770697593689,
      "learning_rate": 3.2716666666666665e-05,
      "loss": 0.0026,
      "step": 31110
    },
    {
      "epoch": 2.7662222222222224,
      "grad_norm": 0.40854841470718384,
      "learning_rate": 3.2711111111111115e-05,
      "loss": 0.0018,
      "step": 31120
    },
    {
      "epoch": 2.7671111111111113,
      "grad_norm": 1.2414674758911133,
      "learning_rate": 3.270555555555556e-05,
      "loss": 0.0026,
      "step": 31130
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.13805001974105835,
      "learning_rate": 3.27e-05,
      "loss": 0.0023,
      "step": 31140
    },
    {
      "epoch": 2.7688888888888887,
      "grad_norm": 0.10678897798061371,
      "learning_rate": 3.2694444444444446e-05,
      "loss": 0.0026,
      "step": 31150
    },
    {
      "epoch": 2.7697777777777777,
      "grad_norm": 0.4861275553703308,
      "learning_rate": 3.268888888888889e-05,
      "loss": 0.002,
      "step": 31160
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.15857744216918945,
      "learning_rate": 3.268333333333333e-05,
      "loss": 0.0025,
      "step": 31170
    },
    {
      "epoch": 2.7715555555555556,
      "grad_norm": 0.5281383395195007,
      "learning_rate": 3.2677777777777776e-05,
      "loss": 0.0022,
      "step": 31180
    },
    {
      "epoch": 2.7724444444444445,
      "grad_norm": 0.2437126189470291,
      "learning_rate": 3.2672222222222227e-05,
      "loss": 0.0025,
      "step": 31190
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.6610784530639648,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0023,
      "step": 31200
    },
    {
      "epoch": 2.7742222222222224,
      "grad_norm": 0.1304560899734497,
      "learning_rate": 3.2661111111111114e-05,
      "loss": 0.0017,
      "step": 31210
    },
    {
      "epoch": 2.7751111111111113,
      "grad_norm": 0.3389318585395813,
      "learning_rate": 3.265555555555556e-05,
      "loss": 0.002,
      "step": 31220
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.3319726586341858,
      "learning_rate": 3.265e-05,
      "loss": 0.0035,
      "step": 31230
    },
    {
      "epoch": 2.7768888888888887,
      "grad_norm": 0.7512849569320679,
      "learning_rate": 3.2644444444444444e-05,
      "loss": 0.0022,
      "step": 31240
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.6410691738128662,
      "learning_rate": 3.263888888888889e-05,
      "loss": 0.0024,
      "step": 31250
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.06459888815879822,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.0016,
      "step": 31260
    },
    {
      "epoch": 2.7795555555555556,
      "grad_norm": 1.0213271379470825,
      "learning_rate": 3.262777777777778e-05,
      "loss": 0.0015,
      "step": 31270
    },
    {
      "epoch": 2.7804444444444445,
      "grad_norm": 0.15738002955913544,
      "learning_rate": 3.2622222222222225e-05,
      "loss": 0.0025,
      "step": 31280
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.8579073548316956,
      "learning_rate": 3.261666666666667e-05,
      "loss": 0.0028,
      "step": 31290
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 0.4838162362575531,
      "learning_rate": 3.261111111111111e-05,
      "loss": 0.0031,
      "step": 31300
    },
    {
      "epoch": 2.7831111111111113,
      "grad_norm": 0.16097590327262878,
      "learning_rate": 3.2605555555555556e-05,
      "loss": 0.0018,
      "step": 31310
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.3122735321521759,
      "learning_rate": 3.26e-05,
      "loss": 0.0016,
      "step": 31320
    },
    {
      "epoch": 2.7848888888888887,
      "grad_norm": 0.1677362024784088,
      "learning_rate": 3.259444444444444e-05,
      "loss": 0.0035,
      "step": 31330
    },
    {
      "epoch": 2.7857777777777777,
      "grad_norm": 0.0899699479341507,
      "learning_rate": 3.2588888888888893e-05,
      "loss": 0.0019,
      "step": 31340
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.675301194190979,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.0026,
      "step": 31350
    },
    {
      "epoch": 2.7875555555555556,
      "grad_norm": 0.9499197602272034,
      "learning_rate": 3.257777777777778e-05,
      "loss": 0.0022,
      "step": 31360
    },
    {
      "epoch": 2.7884444444444445,
      "grad_norm": 0.7895664572715759,
      "learning_rate": 3.2572222222222224e-05,
      "loss": 0.0023,
      "step": 31370
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.2803933024406433,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0028,
      "step": 31380
    },
    {
      "epoch": 2.7902222222222224,
      "grad_norm": 0.320092111825943,
      "learning_rate": 3.256111111111111e-05,
      "loss": 0.0031,
      "step": 31390
    },
    {
      "epoch": 2.7911111111111113,
      "grad_norm": 0.40543806552886963,
      "learning_rate": 3.2555555555555555e-05,
      "loss": 0.002,
      "step": 31400
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.745161771774292,
      "learning_rate": 3.2550000000000005e-05,
      "loss": 0.0023,
      "step": 31410
    },
    {
      "epoch": 2.7928888888888888,
      "grad_norm": 0.1083693578839302,
      "learning_rate": 3.254444444444444e-05,
      "loss": 0.0013,
      "step": 31420
    },
    {
      "epoch": 2.7937777777777777,
      "grad_norm": 0.42228177189826965,
      "learning_rate": 3.253888888888889e-05,
      "loss": 0.0018,
      "step": 31430
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.510621190071106,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0025,
      "step": 31440
    },
    {
      "epoch": 2.7955555555555556,
      "grad_norm": 0.1544313132762909,
      "learning_rate": 3.252777777777778e-05,
      "loss": 0.0021,
      "step": 31450
    },
    {
      "epoch": 2.7964444444444445,
      "grad_norm": 0.42937788367271423,
      "learning_rate": 3.252222222222222e-05,
      "loss": 0.0018,
      "step": 31460
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 1.0440776348114014,
      "learning_rate": 3.2516666666666666e-05,
      "loss": 0.0029,
      "step": 31470
    },
    {
      "epoch": 2.7982222222222224,
      "grad_norm": 0.42704904079437256,
      "learning_rate": 3.251111111111112e-05,
      "loss": 0.0028,
      "step": 31480
    },
    {
      "epoch": 2.7991111111111113,
      "grad_norm": 0.5165061354637146,
      "learning_rate": 3.2505555555555554e-05,
      "loss": 0.0035,
      "step": 31490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.6171871423721313,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0037,
      "step": 31500
    },
    {
      "epoch": 2.8008888888888888,
      "grad_norm": 0.3482654094696045,
      "learning_rate": 3.249444444444444e-05,
      "loss": 0.0028,
      "step": 31510
    },
    {
      "epoch": 2.8017777777777777,
      "grad_norm": 1.1072887182235718,
      "learning_rate": 3.248888888888889e-05,
      "loss": 0.0024,
      "step": 31520
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.2562863528728485,
      "learning_rate": 3.2483333333333335e-05,
      "loss": 0.0019,
      "step": 31530
    },
    {
      "epoch": 2.8035555555555556,
      "grad_norm": 0.2964383661746979,
      "learning_rate": 3.247777777777778e-05,
      "loss": 0.0023,
      "step": 31540
    },
    {
      "epoch": 2.8044444444444445,
      "grad_norm": 0.4369461238384247,
      "learning_rate": 3.247222222222223e-05,
      "loss": 0.0016,
      "step": 31550
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.11506199836730957,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0024,
      "step": 31560
    },
    {
      "epoch": 2.806222222222222,
      "grad_norm": 0.43722885847091675,
      "learning_rate": 3.2461111111111116e-05,
      "loss": 0.0027,
      "step": 31570
    },
    {
      "epoch": 2.8071111111111113,
      "grad_norm": 0.48985615372657776,
      "learning_rate": 3.245555555555555e-05,
      "loss": 0.0027,
      "step": 31580
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.5561854839324951,
      "learning_rate": 3.245e-05,
      "loss": 0.0034,
      "step": 31590
    },
    {
      "epoch": 2.8088888888888888,
      "grad_norm": 0.060148194432258606,
      "learning_rate": 3.2444444444444446e-05,
      "loss": 0.0032,
      "step": 31600
    },
    {
      "epoch": 2.8097777777777777,
      "grad_norm": 0.23989596962928772,
      "learning_rate": 3.243888888888889e-05,
      "loss": 0.0022,
      "step": 31610
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.6175039410591125,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0026,
      "step": 31620
    },
    {
      "epoch": 2.8115555555555556,
      "grad_norm": 0.40459486842155457,
      "learning_rate": 3.242777777777778e-05,
      "loss": 0.0036,
      "step": 31630
    },
    {
      "epoch": 2.8124444444444445,
      "grad_norm": 0.10756184160709381,
      "learning_rate": 3.242222222222223e-05,
      "loss": 0.0028,
      "step": 31640
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.13486041128635406,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.0033,
      "step": 31650
    },
    {
      "epoch": 2.814222222222222,
      "grad_norm": 0.5146287083625793,
      "learning_rate": 3.2411111111111114e-05,
      "loss": 0.0018,
      "step": 31660
    },
    {
      "epoch": 2.8151111111111113,
      "grad_norm": 0.12221136689186096,
      "learning_rate": 3.240555555555556e-05,
      "loss": 0.0022,
      "step": 31670
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.758795440196991,
      "learning_rate": 3.24e-05,
      "loss": 0.0019,
      "step": 31680
    },
    {
      "epoch": 2.8168888888888888,
      "grad_norm": 0.2754151523113251,
      "learning_rate": 3.2394444444444445e-05,
      "loss": 0.0022,
      "step": 31690
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 0.08002586662769318,
      "learning_rate": 3.238888888888889e-05,
      "loss": 0.0017,
      "step": 31700
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.5747573375701904,
      "learning_rate": 3.238333333333333e-05,
      "loss": 0.0024,
      "step": 31710
    },
    {
      "epoch": 2.8195555555555556,
      "grad_norm": 0.23665258288383484,
      "learning_rate": 3.2377777777777776e-05,
      "loss": 0.0028,
      "step": 31720
    },
    {
      "epoch": 2.8204444444444445,
      "grad_norm": 0.4869557321071625,
      "learning_rate": 3.2372222222222226e-05,
      "loss": 0.0024,
      "step": 31730
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.11282411962747574,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0019,
      "step": 31740
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 0.13183332979679108,
      "learning_rate": 3.236111111111111e-05,
      "loss": 0.0025,
      "step": 31750
    },
    {
      "epoch": 2.8231111111111113,
      "grad_norm": 0.3602510690689087,
      "learning_rate": 3.235555555555556e-05,
      "loss": 0.0028,
      "step": 31760
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.2513282597064972,
      "learning_rate": 3.235e-05,
      "loss": 0.0024,
      "step": 31770
    },
    {
      "epoch": 2.824888888888889,
      "grad_norm": 0.14005716145038605,
      "learning_rate": 3.2344444444444444e-05,
      "loss": 0.0022,
      "step": 31780
    },
    {
      "epoch": 2.8257777777777777,
      "grad_norm": 0.4590533375740051,
      "learning_rate": 3.2338888888888894e-05,
      "loss": 0.0022,
      "step": 31790
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.4088825285434723,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.003,
      "step": 31800
    },
    {
      "epoch": 2.8275555555555556,
      "grad_norm": 0.17438673973083496,
      "learning_rate": 3.232777777777778e-05,
      "loss": 0.0025,
      "step": 31810
    },
    {
      "epoch": 2.8284444444444445,
      "grad_norm": 0.4138258099555969,
      "learning_rate": 3.2322222222222225e-05,
      "loss": 0.0019,
      "step": 31820
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.3230733275413513,
      "learning_rate": 3.231666666666667e-05,
      "loss": 0.0035,
      "step": 31830
    },
    {
      "epoch": 2.830222222222222,
      "grad_norm": 0.9126800894737244,
      "learning_rate": 3.231111111111111e-05,
      "loss": 0.0031,
      "step": 31840
    },
    {
      "epoch": 2.8311111111111114,
      "grad_norm": 0.6694560050964355,
      "learning_rate": 3.2305555555555556e-05,
      "loss": 0.0024,
      "step": 31850
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.4241234362125397,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0019,
      "step": 31860
    },
    {
      "epoch": 2.832888888888889,
      "grad_norm": 0.28116002678871155,
      "learning_rate": 3.229444444444444e-05,
      "loss": 0.0026,
      "step": 31870
    },
    {
      "epoch": 2.8337777777777777,
      "grad_norm": 0.358232706785202,
      "learning_rate": 3.228888888888889e-05,
      "loss": 0.0025,
      "step": 31880
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.14173048734664917,
      "learning_rate": 3.2283333333333337e-05,
      "loss": 0.0018,
      "step": 31890
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 0.5443174242973328,
      "learning_rate": 3.227777777777778e-05,
      "loss": 0.0028,
      "step": 31900
    },
    {
      "epoch": 2.8364444444444445,
      "grad_norm": 0.1241271048784256,
      "learning_rate": 3.2272222222222224e-05,
      "loss": 0.0024,
      "step": 31910
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.0838896632194519,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0021,
      "step": 31920
    },
    {
      "epoch": 2.838222222222222,
      "grad_norm": 0.7819194793701172,
      "learning_rate": 3.226111111111112e-05,
      "loss": 0.0032,
      "step": 31930
    },
    {
      "epoch": 2.8391111111111114,
      "grad_norm": 0.8697465658187866,
      "learning_rate": 3.2255555555555554e-05,
      "loss": 0.0028,
      "step": 31940
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.5623388886451721,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.002,
      "step": 31950
    },
    {
      "epoch": 2.840888888888889,
      "grad_norm": 0.755890965461731,
      "learning_rate": 3.224444444444444e-05,
      "loss": 0.0029,
      "step": 31960
    },
    {
      "epoch": 2.8417777777777777,
      "grad_norm": 0.36534446477890015,
      "learning_rate": 3.223888888888889e-05,
      "loss": 0.0022,
      "step": 31970
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.8573216199874878,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.0026,
      "step": 31980
    },
    {
      "epoch": 2.8435555555555556,
      "grad_norm": 0.6721025705337524,
      "learning_rate": 3.222777777777778e-05,
      "loss": 0.002,
      "step": 31990
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 0.6118915677070618,
      "learning_rate": 3.222222222222223e-05,
      "loss": 0.0027,
      "step": 32000
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.24908919632434845,
      "learning_rate": 3.2216666666666666e-05,
      "loss": 0.0026,
      "step": 32010
    },
    {
      "epoch": 2.846222222222222,
      "grad_norm": 0.16341565549373627,
      "learning_rate": 3.2211111111111116e-05,
      "loss": 0.0017,
      "step": 32020
    },
    {
      "epoch": 2.8471111111111114,
      "grad_norm": 0.4124906659126282,
      "learning_rate": 3.220555555555555e-05,
      "loss": 0.0022,
      "step": 32030
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.40276798605918884,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0019,
      "step": 32040
    },
    {
      "epoch": 2.848888888888889,
      "grad_norm": 0.04050881415605545,
      "learning_rate": 3.219444444444445e-05,
      "loss": 0.0017,
      "step": 32050
    },
    {
      "epoch": 2.8497777777777777,
      "grad_norm": 0.15322622656822205,
      "learning_rate": 3.218888888888889e-05,
      "loss": 0.0027,
      "step": 32060
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.07113050669431686,
      "learning_rate": 3.218333333333334e-05,
      "loss": 0.0031,
      "step": 32070
    },
    {
      "epoch": 2.8515555555555556,
      "grad_norm": 0.2341330498456955,
      "learning_rate": 3.217777777777778e-05,
      "loss": 0.0019,
      "step": 32080
    },
    {
      "epoch": 2.8524444444444446,
      "grad_norm": 0.6448646187782288,
      "learning_rate": 3.217222222222223e-05,
      "loss": 0.0024,
      "step": 32090
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.6913020610809326,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.003,
      "step": 32100
    },
    {
      "epoch": 2.854222222222222,
      "grad_norm": 0.2548781633377075,
      "learning_rate": 3.2161111111111115e-05,
      "loss": 0.0016,
      "step": 32110
    },
    {
      "epoch": 2.8551111111111114,
      "grad_norm": 0.20900055766105652,
      "learning_rate": 3.215555555555556e-05,
      "loss": 0.003,
      "step": 32120
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.6970109939575195,
      "learning_rate": 3.215e-05,
      "loss": 0.0023,
      "step": 32130
    },
    {
      "epoch": 2.856888888888889,
      "grad_norm": 0.6433961391448975,
      "learning_rate": 3.2144444444444446e-05,
      "loss": 0.0037,
      "step": 32140
    },
    {
      "epoch": 2.8577777777777778,
      "grad_norm": 0.171245738863945,
      "learning_rate": 3.213888888888889e-05,
      "loss": 0.0022,
      "step": 32150
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.08612135797739029,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.002,
      "step": 32160
    },
    {
      "epoch": 2.8595555555555556,
      "grad_norm": 0.6768789291381836,
      "learning_rate": 3.2127777777777776e-05,
      "loss": 0.0021,
      "step": 32170
    },
    {
      "epoch": 2.8604444444444446,
      "grad_norm": 0.17754732072353363,
      "learning_rate": 3.212222222222223e-05,
      "loss": 0.0024,
      "step": 32180
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.8385208249092102,
      "learning_rate": 3.211666666666667e-05,
      "loss": 0.0026,
      "step": 32190
    },
    {
      "epoch": 2.862222222222222,
      "grad_norm": 0.7460935711860657,
      "learning_rate": 3.2111111111111114e-05,
      "loss": 0.0019,
      "step": 32200
    },
    {
      "epoch": 2.8631111111111114,
      "grad_norm": 0.3414788842201233,
      "learning_rate": 3.210555555555556e-05,
      "loss": 0.0017,
      "step": 32210
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.06814916431903839,
      "learning_rate": 3.21e-05,
      "loss": 0.0025,
      "step": 32220
    },
    {
      "epoch": 2.864888888888889,
      "grad_norm": 0.08487127721309662,
      "learning_rate": 3.2094444444444445e-05,
      "loss": 0.0019,
      "step": 32230
    },
    {
      "epoch": 2.8657777777777778,
      "grad_norm": 0.17338763177394867,
      "learning_rate": 3.208888888888889e-05,
      "loss": 0.0025,
      "step": 32240
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.5763376951217651,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.0019,
      "step": 32250
    },
    {
      "epoch": 2.8675555555555556,
      "grad_norm": 0.13190831243991852,
      "learning_rate": 3.207777777777778e-05,
      "loss": 0.0015,
      "step": 32260
    },
    {
      "epoch": 2.8684444444444446,
      "grad_norm": 0.3952066898345947,
      "learning_rate": 3.2072222222222226e-05,
      "loss": 0.0023,
      "step": 32270
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.39050766825675964,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.002,
      "step": 32280
    },
    {
      "epoch": 2.870222222222222,
      "grad_norm": 0.30745697021484375,
      "learning_rate": 3.206111111111111e-05,
      "loss": 0.0019,
      "step": 32290
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 0.17124463617801666,
      "learning_rate": 3.2055555555555556e-05,
      "loss": 0.0017,
      "step": 32300
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.13173839449882507,
      "learning_rate": 3.205e-05,
      "loss": 0.0031,
      "step": 32310
    },
    {
      "epoch": 2.872888888888889,
      "grad_norm": 1.067388892173767,
      "learning_rate": 3.204444444444444e-05,
      "loss": 0.0019,
      "step": 32320
    },
    {
      "epoch": 2.8737777777777778,
      "grad_norm": 0.20847922563552856,
      "learning_rate": 3.2038888888888894e-05,
      "loss": 0.0023,
      "step": 32330
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.7454936504364014,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0024,
      "step": 32340
    },
    {
      "epoch": 2.8755555555555556,
      "grad_norm": 0.10175883024930954,
      "learning_rate": 3.202777777777778e-05,
      "loss": 0.0024,
      "step": 32350
    },
    {
      "epoch": 2.8764444444444446,
      "grad_norm": 0.16832023859024048,
      "learning_rate": 3.2022222222222224e-05,
      "loss": 0.0021,
      "step": 32360
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.09862186014652252,
      "learning_rate": 3.201666666666667e-05,
      "loss": 0.0021,
      "step": 32370
    },
    {
      "epoch": 2.878222222222222,
      "grad_norm": 0.16375751793384552,
      "learning_rate": 3.201111111111111e-05,
      "loss": 0.0021,
      "step": 32380
    },
    {
      "epoch": 2.879111111111111,
      "grad_norm": 0.13506777584552765,
      "learning_rate": 3.2005555555555555e-05,
      "loss": 0.002,
      "step": 32390
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.3832825720310211,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0012,
      "step": 32400
    },
    {
      "epoch": 2.880888888888889,
      "grad_norm": 0.6131278872489929,
      "learning_rate": 3.199444444444444e-05,
      "loss": 0.0022,
      "step": 32410
    },
    {
      "epoch": 2.8817777777777778,
      "grad_norm": 0.15387967228889465,
      "learning_rate": 3.198888888888889e-05,
      "loss": 0.0021,
      "step": 32420
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.39231646060943604,
      "learning_rate": 3.1983333333333336e-05,
      "loss": 0.0016,
      "step": 32430
    },
    {
      "epoch": 2.8835555555555556,
      "grad_norm": 0.4484216272830963,
      "learning_rate": 3.197777777777778e-05,
      "loss": 0.0026,
      "step": 32440
    },
    {
      "epoch": 2.8844444444444446,
      "grad_norm": 0.16033338010311127,
      "learning_rate": 3.197222222222222e-05,
      "loss": 0.0022,
      "step": 32450
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.222440704703331,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.0025,
      "step": 32460
    },
    {
      "epoch": 2.886222222222222,
      "grad_norm": 0.6349915266036987,
      "learning_rate": 3.196111111111112e-05,
      "loss": 0.0016,
      "step": 32470
    },
    {
      "epoch": 2.887111111111111,
      "grad_norm": 0.3458060324192047,
      "learning_rate": 3.1955555555555554e-05,
      "loss": 0.002,
      "step": 32480
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.5890550017356873,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.0026,
      "step": 32490
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.27710023522377014,
      "learning_rate": 3.194444444444444e-05,
      "loss": 0.0018,
      "step": 32500
    },
    {
      "epoch": 2.889777777777778,
      "grad_norm": 0.42309340834617615,
      "learning_rate": 3.193888888888889e-05,
      "loss": 0.0021,
      "step": 32510
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.1574372947216034,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0023,
      "step": 32520
    },
    {
      "epoch": 2.8915555555555557,
      "grad_norm": 0.3622145652770996,
      "learning_rate": 3.192777777777778e-05,
      "loss": 0.0021,
      "step": 32530
    },
    {
      "epoch": 2.8924444444444446,
      "grad_norm": 0.3407309949398041,
      "learning_rate": 3.192222222222223e-05,
      "loss": 0.0021,
      "step": 32540
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.807201623916626,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.002,
      "step": 32550
    },
    {
      "epoch": 2.894222222222222,
      "grad_norm": 0.5093168020248413,
      "learning_rate": 3.1911111111111116e-05,
      "loss": 0.0022,
      "step": 32560
    },
    {
      "epoch": 2.895111111111111,
      "grad_norm": 0.2236029952764511,
      "learning_rate": 3.190555555555555e-05,
      "loss": 0.0018,
      "step": 32570
    },
    {
      "epoch": 2.896,
      "grad_norm": 1.1058372259140015,
      "learning_rate": 3.19e-05,
      "loss": 0.0024,
      "step": 32580
    },
    {
      "epoch": 2.896888888888889,
      "grad_norm": 0.35065221786499023,
      "learning_rate": 3.1894444444444446e-05,
      "loss": 0.0032,
      "step": 32590
    },
    {
      "epoch": 2.897777777777778,
      "grad_norm": 0.4717479646205902,
      "learning_rate": 3.188888888888889e-05,
      "loss": 0.0034,
      "step": 32600
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.7203307747840881,
      "learning_rate": 3.188333333333334e-05,
      "loss": 0.0023,
      "step": 32610
    },
    {
      "epoch": 2.8995555555555557,
      "grad_norm": 0.825383722782135,
      "learning_rate": 3.187777777777778e-05,
      "loss": 0.0015,
      "step": 32620
    },
    {
      "epoch": 2.9004444444444446,
      "grad_norm": 0.3027501404285431,
      "learning_rate": 3.187222222222223e-05,
      "loss": 0.0026,
      "step": 32630
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.6109611988067627,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.002,
      "step": 32640
    },
    {
      "epoch": 2.902222222222222,
      "grad_norm": 0.09418796747922897,
      "learning_rate": 3.1861111111111115e-05,
      "loss": 0.0017,
      "step": 32650
    },
    {
      "epoch": 2.903111111111111,
      "grad_norm": 0.6544305682182312,
      "learning_rate": 3.185555555555556e-05,
      "loss": 0.0019,
      "step": 32660
    },
    {
      "epoch": 2.904,
      "grad_norm": 1.3282086849212646,
      "learning_rate": 3.185e-05,
      "loss": 0.0028,
      "step": 32670
    },
    {
      "epoch": 2.904888888888889,
      "grad_norm": 0.26048120856285095,
      "learning_rate": 3.1844444444444445e-05,
      "loss": 0.0015,
      "step": 32680
    },
    {
      "epoch": 2.905777777777778,
      "grad_norm": 0.23447081446647644,
      "learning_rate": 3.183888888888889e-05,
      "loss": 0.0032,
      "step": 32690
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.5996513962745667,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0022,
      "step": 32700
    },
    {
      "epoch": 2.9075555555555557,
      "grad_norm": 0.24144865572452545,
      "learning_rate": 3.1827777777777776e-05,
      "loss": 0.0027,
      "step": 32710
    },
    {
      "epoch": 2.9084444444444446,
      "grad_norm": 0.06706023961305618,
      "learning_rate": 3.1822222222222226e-05,
      "loss": 0.0025,
      "step": 32720
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.12523655593395233,
      "learning_rate": 3.181666666666667e-05,
      "loss": 0.0013,
      "step": 32730
    },
    {
      "epoch": 2.910222222222222,
      "grad_norm": 0.1665782332420349,
      "learning_rate": 3.181111111111111e-05,
      "loss": 0.0029,
      "step": 32740
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 0.6955561637878418,
      "learning_rate": 3.180555555555556e-05,
      "loss": 0.0021,
      "step": 32750
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.4541148245334625,
      "learning_rate": 3.18e-05,
      "loss": 0.0014,
      "step": 32760
    },
    {
      "epoch": 2.912888888888889,
      "grad_norm": 0.8052834272384644,
      "learning_rate": 3.1794444444444444e-05,
      "loss": 0.002,
      "step": 32770
    },
    {
      "epoch": 2.913777777777778,
      "grad_norm": 0.40382644534111023,
      "learning_rate": 3.178888888888889e-05,
      "loss": 0.0021,
      "step": 32780
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.2430674433708191,
      "learning_rate": 3.178333333333334e-05,
      "loss": 0.0023,
      "step": 32790
    },
    {
      "epoch": 2.9155555555555557,
      "grad_norm": 0.04764258861541748,
      "learning_rate": 3.177777777777778e-05,
      "loss": 0.0021,
      "step": 32800
    },
    {
      "epoch": 2.916444444444444,
      "grad_norm": 0.1645725816488266,
      "learning_rate": 3.1772222222222225e-05,
      "loss": 0.0019,
      "step": 32810
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.5521950721740723,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.002,
      "step": 32820
    },
    {
      "epoch": 2.918222222222222,
      "grad_norm": 0.9660477042198181,
      "learning_rate": 3.176111111111111e-05,
      "loss": 0.0023,
      "step": 32830
    },
    {
      "epoch": 2.919111111111111,
      "grad_norm": 0.6723148822784424,
      "learning_rate": 3.1755555555555556e-05,
      "loss": 0.0025,
      "step": 32840
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.2052406519651413,
      "learning_rate": 3.175e-05,
      "loss": 0.0021,
      "step": 32850
    },
    {
      "epoch": 2.920888888888889,
      "grad_norm": 0.7326976656913757,
      "learning_rate": 3.174444444444444e-05,
      "loss": 0.0026,
      "step": 32860
    },
    {
      "epoch": 2.921777777777778,
      "grad_norm": 0.757653534412384,
      "learning_rate": 3.173888888888889e-05,
      "loss": 0.0016,
      "step": 32870
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.2324216514825821,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0023,
      "step": 32880
    },
    {
      "epoch": 2.9235555555555557,
      "grad_norm": 0.8356981873512268,
      "learning_rate": 3.172777777777778e-05,
      "loss": 0.0026,
      "step": 32890
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 0.3328835964202881,
      "learning_rate": 3.1722222222222224e-05,
      "loss": 0.0014,
      "step": 32900
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.0651141107082367,
      "learning_rate": 3.171666666666667e-05,
      "loss": 0.0017,
      "step": 32910
    },
    {
      "epoch": 2.926222222222222,
      "grad_norm": 0.10386571288108826,
      "learning_rate": 3.171111111111111e-05,
      "loss": 0.0019,
      "step": 32920
    },
    {
      "epoch": 2.927111111111111,
      "grad_norm": 0.07929771393537521,
      "learning_rate": 3.1705555555555554e-05,
      "loss": 0.0022,
      "step": 32930
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.44779056310653687,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0036,
      "step": 32940
    },
    {
      "epoch": 2.928888888888889,
      "grad_norm": 1.1056365966796875,
      "learning_rate": 3.169444444444444e-05,
      "loss": 0.0024,
      "step": 32950
    },
    {
      "epoch": 2.929777777777778,
      "grad_norm": 0.5115528106689453,
      "learning_rate": 3.168888888888889e-05,
      "loss": 0.0028,
      "step": 32960
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.3859430253505707,
      "learning_rate": 3.1683333333333335e-05,
      "loss": 0.0021,
      "step": 32970
    },
    {
      "epoch": 2.9315555555555557,
      "grad_norm": 1.0396562814712524,
      "learning_rate": 3.167777777777778e-05,
      "loss": 0.0022,
      "step": 32980
    },
    {
      "epoch": 2.932444444444444,
      "grad_norm": 0.4794978201389313,
      "learning_rate": 3.167222222222222e-05,
      "loss": 0.0021,
      "step": 32990
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.24136267602443695,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0023,
      "step": 33000
    },
    {
      "epoch": 2.934222222222222,
      "grad_norm": 0.9543818235397339,
      "learning_rate": 3.1661111111111116e-05,
      "loss": 0.0023,
      "step": 33010
    },
    {
      "epoch": 2.935111111111111,
      "grad_norm": 0.8836358785629272,
      "learning_rate": 3.165555555555555e-05,
      "loss": 0.0017,
      "step": 33020
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.46129265427589417,
      "learning_rate": 3.1650000000000004e-05,
      "loss": 0.0034,
      "step": 33030
    },
    {
      "epoch": 2.936888888888889,
      "grad_norm": 0.5098844170570374,
      "learning_rate": 3.164444444444444e-05,
      "loss": 0.004,
      "step": 33040
    },
    {
      "epoch": 2.937777777777778,
      "grad_norm": 0.706412672996521,
      "learning_rate": 3.163888888888889e-05,
      "loss": 0.0027,
      "step": 33050
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.18025338649749756,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.0022,
      "step": 33060
    },
    {
      "epoch": 2.9395555555555557,
      "grad_norm": 0.3015390634536743,
      "learning_rate": 3.162777777777778e-05,
      "loss": 0.0022,
      "step": 33070
    },
    {
      "epoch": 2.940444444444444,
      "grad_norm": 0.46977725625038147,
      "learning_rate": 3.162222222222223e-05,
      "loss": 0.0026,
      "step": 33080
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.2907058000564575,
      "learning_rate": 3.1616666666666665e-05,
      "loss": 0.0023,
      "step": 33090
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 0.13174134492874146,
      "learning_rate": 3.1611111111111115e-05,
      "loss": 0.0027,
      "step": 33100
    },
    {
      "epoch": 2.943111111111111,
      "grad_norm": 0.46110081672668457,
      "learning_rate": 3.160555555555555e-05,
      "loss": 0.0023,
      "step": 33110
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.2896299958229065,
      "learning_rate": 3.16e-05,
      "loss": 0.0021,
      "step": 33120
    },
    {
      "epoch": 2.944888888888889,
      "grad_norm": 0.09591199457645416,
      "learning_rate": 3.1594444444444446e-05,
      "loss": 0.0028,
      "step": 33130
    },
    {
      "epoch": 2.945777777777778,
      "grad_norm": 0.46970871090888977,
      "learning_rate": 3.158888888888889e-05,
      "loss": 0.0024,
      "step": 33140
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.1218823716044426,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.0022,
      "step": 33150
    },
    {
      "epoch": 2.9475555555555557,
      "grad_norm": 0.2378419190645218,
      "learning_rate": 3.1577777777777777e-05,
      "loss": 0.0022,
      "step": 33160
    },
    {
      "epoch": 2.948444444444444,
      "grad_norm": 0.5140465497970581,
      "learning_rate": 3.157222222222223e-05,
      "loss": 0.002,
      "step": 33170
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.626772940158844,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0022,
      "step": 33180
    },
    {
      "epoch": 2.950222222222222,
      "grad_norm": 0.6914528012275696,
      "learning_rate": 3.1561111111111114e-05,
      "loss": 0.0026,
      "step": 33190
    },
    {
      "epoch": 2.951111111111111,
      "grad_norm": 0.12200567871332169,
      "learning_rate": 3.155555555555556e-05,
      "loss": 0.0026,
      "step": 33200
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.45116716623306274,
      "learning_rate": 3.155e-05,
      "loss": 0.0026,
      "step": 33210
    },
    {
      "epoch": 2.952888888888889,
      "grad_norm": 0.5730186104774475,
      "learning_rate": 3.154444444444445e-05,
      "loss": 0.002,
      "step": 33220
    },
    {
      "epoch": 2.953777777777778,
      "grad_norm": 0.09573706984519958,
      "learning_rate": 3.153888888888889e-05,
      "loss": 0.0021,
      "step": 33230
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.5931366682052612,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0024,
      "step": 33240
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 0.39472702145576477,
      "learning_rate": 3.1527777777777775e-05,
      "loss": 0.0022,
      "step": 33250
    },
    {
      "epoch": 2.956444444444444,
      "grad_norm": 0.1672438234090805,
      "learning_rate": 3.1522222222222226e-05,
      "loss": 0.0035,
      "step": 33260
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.046196937561035156,
      "learning_rate": 3.151666666666667e-05,
      "loss": 0.0017,
      "step": 33270
    },
    {
      "epoch": 2.958222222222222,
      "grad_norm": 0.6237914562225342,
      "learning_rate": 3.151111111111111e-05,
      "loss": 0.0023,
      "step": 33280
    },
    {
      "epoch": 2.959111111111111,
      "grad_norm": 0.31493648886680603,
      "learning_rate": 3.1505555555555556e-05,
      "loss": 0.0033,
      "step": 33290
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.9308539628982544,
      "learning_rate": 3.15e-05,
      "loss": 0.0021,
      "step": 33300
    },
    {
      "epoch": 2.960888888888889,
      "grad_norm": 0.27980443835258484,
      "learning_rate": 3.149444444444445e-05,
      "loss": 0.002,
      "step": 33310
    },
    {
      "epoch": 2.961777777777778,
      "grad_norm": 0.19955827295780182,
      "learning_rate": 3.148888888888889e-05,
      "loss": 0.0018,
      "step": 33320
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.3130696415901184,
      "learning_rate": 3.148333333333334e-05,
      "loss": 0.0028,
      "step": 33330
    },
    {
      "epoch": 2.9635555555555557,
      "grad_norm": 0.30042406916618347,
      "learning_rate": 3.147777777777778e-05,
      "loss": 0.0022,
      "step": 33340
    },
    {
      "epoch": 2.964444444444444,
      "grad_norm": 0.12887197732925415,
      "learning_rate": 3.1472222222222225e-05,
      "loss": 0.0018,
      "step": 33350
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.13242273032665253,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0014,
      "step": 33360
    },
    {
      "epoch": 2.966222222222222,
      "grad_norm": 0.955998957157135,
      "learning_rate": 3.146111111111111e-05,
      "loss": 0.0014,
      "step": 33370
    },
    {
      "epoch": 2.967111111111111,
      "grad_norm": 0.7156793475151062,
      "learning_rate": 3.1455555555555555e-05,
      "loss": 0.0015,
      "step": 33380
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.183860182762146,
      "learning_rate": 3.145e-05,
      "loss": 0.0019,
      "step": 33390
    },
    {
      "epoch": 2.968888888888889,
      "grad_norm": 0.057343948632478714,
      "learning_rate": 3.144444444444445e-05,
      "loss": 0.0017,
      "step": 33400
    },
    {
      "epoch": 2.969777777777778,
      "grad_norm": 0.10109752416610718,
      "learning_rate": 3.143888888888889e-05,
      "loss": 0.0018,
      "step": 33410
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.5372494459152222,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.0024,
      "step": 33420
    },
    {
      "epoch": 2.9715555555555557,
      "grad_norm": 0.15900740027427673,
      "learning_rate": 3.142777777777778e-05,
      "loss": 0.0018,
      "step": 33430
    },
    {
      "epoch": 2.9724444444444442,
      "grad_norm": 0.17029732465744019,
      "learning_rate": 3.142222222222222e-05,
      "loss": 0.0037,
      "step": 33440
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.5801843404769897,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.0027,
      "step": 33450
    },
    {
      "epoch": 2.974222222222222,
      "grad_norm": 0.5789461731910706,
      "learning_rate": 3.141111111111111e-05,
      "loss": 0.0018,
      "step": 33460
    },
    {
      "epoch": 2.975111111111111,
      "grad_norm": 0.14831028878688812,
      "learning_rate": 3.1405555555555554e-05,
      "loss": 0.0015,
      "step": 33470
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.09796852618455887,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0021,
      "step": 33480
    },
    {
      "epoch": 2.976888888888889,
      "grad_norm": 0.739605724811554,
      "learning_rate": 3.139444444444445e-05,
      "loss": 0.0026,
      "step": 33490
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.5469443202018738,
      "learning_rate": 3.138888888888889e-05,
      "loss": 0.0026,
      "step": 33500
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.15232409536838531,
      "learning_rate": 3.1383333333333335e-05,
      "loss": 0.0033,
      "step": 33510
    },
    {
      "epoch": 2.9795555555555557,
      "grad_norm": 0.6446928381919861,
      "learning_rate": 3.137777777777778e-05,
      "loss": 0.0031,
      "step": 33520
    },
    {
      "epoch": 2.9804444444444442,
      "grad_norm": 0.15560001134872437,
      "learning_rate": 3.137222222222222e-05,
      "loss": 0.0023,
      "step": 33530
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.37347689270973206,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0029,
      "step": 33540
    },
    {
      "epoch": 2.982222222222222,
      "grad_norm": 0.36230212450027466,
      "learning_rate": 3.1361111111111116e-05,
      "loss": 0.0018,
      "step": 33550
    },
    {
      "epoch": 2.983111111111111,
      "grad_norm": 0.7722469568252563,
      "learning_rate": 3.135555555555555e-05,
      "loss": 0.0021,
      "step": 33560
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.5925558805465698,
      "learning_rate": 3.135e-05,
      "loss": 0.0017,
      "step": 33570
    },
    {
      "epoch": 2.984888888888889,
      "grad_norm": 0.26906174421310425,
      "learning_rate": 3.134444444444445e-05,
      "loss": 0.0014,
      "step": 33580
    },
    {
      "epoch": 2.985777777777778,
      "grad_norm": 0.7219260931015015,
      "learning_rate": 3.133888888888889e-05,
      "loss": 0.0018,
      "step": 33590
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.2628769874572754,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0017,
      "step": 33600
    },
    {
      "epoch": 2.9875555555555557,
      "grad_norm": 0.36106765270233154,
      "learning_rate": 3.132777777777778e-05,
      "loss": 0.0019,
      "step": 33610
    },
    {
      "epoch": 2.9884444444444442,
      "grad_norm": 0.12312696129083633,
      "learning_rate": 3.132222222222223e-05,
      "loss": 0.0022,
      "step": 33620
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.16002783179283142,
      "learning_rate": 3.1316666666666664e-05,
      "loss": 0.0017,
      "step": 33630
    },
    {
      "epoch": 2.990222222222222,
      "grad_norm": 0.8766500949859619,
      "learning_rate": 3.1311111111111115e-05,
      "loss": 0.0021,
      "step": 33640
    },
    {
      "epoch": 2.991111111111111,
      "grad_norm": 0.932104766368866,
      "learning_rate": 3.130555555555555e-05,
      "loss": 0.0017,
      "step": 33650
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.4442644715309143,
      "learning_rate": 3.13e-05,
      "loss": 0.0033,
      "step": 33660
    },
    {
      "epoch": 2.992888888888889,
      "grad_norm": 0.4624522924423218,
      "learning_rate": 3.1294444444444445e-05,
      "loss": 0.0021,
      "step": 33670
    },
    {
      "epoch": 2.993777777777778,
      "grad_norm": 0.9905769228935242,
      "learning_rate": 3.128888888888889e-05,
      "loss": 0.0016,
      "step": 33680
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.4744183123111725,
      "learning_rate": 3.128333333333334e-05,
      "loss": 0.0019,
      "step": 33690
    },
    {
      "epoch": 2.9955555555555557,
      "grad_norm": 0.2162017673254013,
      "learning_rate": 3.1277777777777776e-05,
      "loss": 0.0017,
      "step": 33700
    },
    {
      "epoch": 2.9964444444444442,
      "grad_norm": 0.09804671257734299,
      "learning_rate": 3.1272222222222226e-05,
      "loss": 0.0029,
      "step": 33710
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.772764265537262,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0019,
      "step": 33720
    },
    {
      "epoch": 2.998222222222222,
      "grad_norm": 0.3202047646045685,
      "learning_rate": 3.1261111111111114e-05,
      "loss": 0.0014,
      "step": 33730
    },
    {
      "epoch": 2.999111111111111,
      "grad_norm": 0.2461138516664505,
      "learning_rate": 3.125555555555556e-05,
      "loss": 0.0021,
      "step": 33740
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.7812191247940063,
      "learning_rate": 3.125e-05,
      "loss": 0.0024,
      "step": 33750
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.0023241499438881874,
      "eval_runtime": 100.6789,
      "eval_samples_per_second": 1489.886,
      "eval_steps_per_second": 37.247,
      "step": 33750
    },
    {
      "epoch": 3.000888888888889,
      "grad_norm": 0.06083826348185539,
      "learning_rate": 3.124444444444445e-05,
      "loss": 0.0028,
      "step": 33760
    },
    {
      "epoch": 3.001777777777778,
      "grad_norm": 0.27983158826828003,
      "learning_rate": 3.123888888888889e-05,
      "loss": 0.0018,
      "step": 33770
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.5555440783500671,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0015,
      "step": 33780
    },
    {
      "epoch": 3.0035555555555558,
      "grad_norm": 0.6739434003829956,
      "learning_rate": 3.1227777777777775e-05,
      "loss": 0.0017,
      "step": 33790
    },
    {
      "epoch": 3.0044444444444443,
      "grad_norm": 0.633080005645752,
      "learning_rate": 3.1222222222222225e-05,
      "loss": 0.0023,
      "step": 33800
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.05907021462917328,
      "learning_rate": 3.121666666666667e-05,
      "loss": 0.0022,
      "step": 33810
    },
    {
      "epoch": 3.006222222222222,
      "grad_norm": 0.16699478030204773,
      "learning_rate": 3.121111111111111e-05,
      "loss": 0.0022,
      "step": 33820
    },
    {
      "epoch": 3.007111111111111,
      "grad_norm": 0.3210543394088745,
      "learning_rate": 3.1205555555555556e-05,
      "loss": 0.0027,
      "step": 33830
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.41395777463912964,
      "learning_rate": 3.12e-05,
      "loss": 0.0015,
      "step": 33840
    },
    {
      "epoch": 3.008888888888889,
      "grad_norm": 0.17595966160297394,
      "learning_rate": 3.119444444444445e-05,
      "loss": 0.0022,
      "step": 33850
    },
    {
      "epoch": 3.009777777777778,
      "grad_norm": 0.7263662219047546,
      "learning_rate": 3.1188888888888887e-05,
      "loss": 0.004,
      "step": 33860
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.5159582495689392,
      "learning_rate": 3.118333333333334e-05,
      "loss": 0.0024,
      "step": 33870
    },
    {
      "epoch": 3.0115555555555558,
      "grad_norm": 0.05676240846514702,
      "learning_rate": 3.117777777777778e-05,
      "loss": 0.0028,
      "step": 33880
    },
    {
      "epoch": 3.0124444444444443,
      "grad_norm": 0.0879879891872406,
      "learning_rate": 3.1172222222222224e-05,
      "loss": 0.0026,
      "step": 33890
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.9042221307754517,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.0025,
      "step": 33900
    },
    {
      "epoch": 3.014222222222222,
      "grad_norm": 0.06870473921298981,
      "learning_rate": 3.116111111111111e-05,
      "loss": 0.0031,
      "step": 33910
    },
    {
      "epoch": 3.015111111111111,
      "grad_norm": 0.21397988498210907,
      "learning_rate": 3.1155555555555555e-05,
      "loss": 0.0023,
      "step": 33920
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.20282740890979767,
      "learning_rate": 3.115e-05,
      "loss": 0.0018,
      "step": 33930
    },
    {
      "epoch": 3.016888888888889,
      "grad_norm": 0.6460023522377014,
      "learning_rate": 3.114444444444445e-05,
      "loss": 0.0029,
      "step": 33940
    },
    {
      "epoch": 3.017777777777778,
      "grad_norm": 0.28411898016929626,
      "learning_rate": 3.113888888888889e-05,
      "loss": 0.0021,
      "step": 33950
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.07899651676416397,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0015,
      "step": 33960
    },
    {
      "epoch": 3.0195555555555558,
      "grad_norm": 0.29085564613342285,
      "learning_rate": 3.112777777777778e-05,
      "loss": 0.0021,
      "step": 33970
    },
    {
      "epoch": 3.0204444444444443,
      "grad_norm": 0.6130751967430115,
      "learning_rate": 3.112222222222222e-05,
      "loss": 0.0017,
      "step": 33980
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.035167429596185684,
      "learning_rate": 3.1116666666666666e-05,
      "loss": 0.0031,
      "step": 33990
    },
    {
      "epoch": 3.022222222222222,
      "grad_norm": 0.9095385074615479,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.0022,
      "step": 34000
    },
    {
      "epoch": 3.023111111111111,
      "grad_norm": 0.06162464991211891,
      "learning_rate": 3.1105555555555553e-05,
      "loss": 0.0022,
      "step": 34010
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.11784569174051285,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.002,
      "step": 34020
    },
    {
      "epoch": 3.024888888888889,
      "grad_norm": 0.7464066743850708,
      "learning_rate": 3.109444444444445e-05,
      "loss": 0.0015,
      "step": 34030
    },
    {
      "epoch": 3.025777777777778,
      "grad_norm": 0.5849815607070923,
      "learning_rate": 3.108888888888889e-05,
      "loss": 0.0019,
      "step": 34040
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.6193525791168213,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.0023,
      "step": 34050
    },
    {
      "epoch": 3.0275555555555558,
      "grad_norm": 0.47363775968551636,
      "learning_rate": 3.107777777777778e-05,
      "loss": 0.0028,
      "step": 34060
    },
    {
      "epoch": 3.0284444444444443,
      "grad_norm": 0.28216904401779175,
      "learning_rate": 3.107222222222222e-05,
      "loss": 0.0022,
      "step": 34070
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.6764408946037292,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0025,
      "step": 34080
    },
    {
      "epoch": 3.030222222222222,
      "grad_norm": 0.6121019124984741,
      "learning_rate": 3.1061111111111115e-05,
      "loss": 0.0035,
      "step": 34090
    },
    {
      "epoch": 3.031111111111111,
      "grad_norm": 0.24594759941101074,
      "learning_rate": 3.105555555555555e-05,
      "loss": 0.0019,
      "step": 34100
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.06273499876260757,
      "learning_rate": 3.105e-05,
      "loss": 0.0017,
      "step": 34110
    },
    {
      "epoch": 3.032888888888889,
      "grad_norm": 0.2514529824256897,
      "learning_rate": 3.1044444444444446e-05,
      "loss": 0.0033,
      "step": 34120
    },
    {
      "epoch": 3.033777777777778,
      "grad_norm": 0.48762431740760803,
      "learning_rate": 3.103888888888889e-05,
      "loss": 0.0026,
      "step": 34130
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.5547668933868408,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0028,
      "step": 34140
    },
    {
      "epoch": 3.0355555555555553,
      "grad_norm": 1.021084189414978,
      "learning_rate": 3.102777777777778e-05,
      "loss": 0.0021,
      "step": 34150
    },
    {
      "epoch": 3.0364444444444443,
      "grad_norm": 0.9780946373939514,
      "learning_rate": 3.102222222222223e-05,
      "loss": 0.0034,
      "step": 34160
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.5184774994850159,
      "learning_rate": 3.1016666666666664e-05,
      "loss": 0.0021,
      "step": 34170
    },
    {
      "epoch": 3.038222222222222,
      "grad_norm": 0.32785096764564514,
      "learning_rate": 3.1011111111111114e-05,
      "loss": 0.0029,
      "step": 34180
    },
    {
      "epoch": 3.039111111111111,
      "grad_norm": 1.0362143516540527,
      "learning_rate": 3.100555555555555e-05,
      "loss": 0.0023,
      "step": 34190
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.19605208933353424,
      "learning_rate": 3.1e-05,
      "loss": 0.0019,
      "step": 34200
    },
    {
      "epoch": 3.040888888888889,
      "grad_norm": 0.8098888397216797,
      "learning_rate": 3.0994444444444445e-05,
      "loss": 0.0025,
      "step": 34210
    },
    {
      "epoch": 3.041777777777778,
      "grad_norm": 0.5450016260147095,
      "learning_rate": 3.098888888888889e-05,
      "loss": 0.0034,
      "step": 34220
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.22443373501300812,
      "learning_rate": 3.098333333333334e-05,
      "loss": 0.0022,
      "step": 34230
    },
    {
      "epoch": 3.0435555555555553,
      "grad_norm": 0.3596384525299072,
      "learning_rate": 3.0977777777777776e-05,
      "loss": 0.0032,
      "step": 34240
    },
    {
      "epoch": 3.0444444444444443,
      "grad_norm": 0.40016287565231323,
      "learning_rate": 3.0972222222222226e-05,
      "loss": 0.0027,
      "step": 34250
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.1303441822528839,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.0026,
      "step": 34260
    },
    {
      "epoch": 3.046222222222222,
      "grad_norm": 0.38332924246788025,
      "learning_rate": 3.096111111111111e-05,
      "loss": 0.0026,
      "step": 34270
    },
    {
      "epoch": 3.047111111111111,
      "grad_norm": 1.0773943662643433,
      "learning_rate": 3.0955555555555557e-05,
      "loss": 0.0024,
      "step": 34280
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.16262182593345642,
      "learning_rate": 3.095e-05,
      "loss": 0.0021,
      "step": 34290
    },
    {
      "epoch": 3.048888888888889,
      "grad_norm": 0.8985744118690491,
      "learning_rate": 3.094444444444445e-05,
      "loss": 0.0023,
      "step": 34300
    },
    {
      "epoch": 3.049777777777778,
      "grad_norm": 1.0126186609268188,
      "learning_rate": 3.093888888888889e-05,
      "loss": 0.002,
      "step": 34310
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.09167136251926422,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0018,
      "step": 34320
    },
    {
      "epoch": 3.0515555555555554,
      "grad_norm": 0.20418573915958405,
      "learning_rate": 3.0927777777777774e-05,
      "loss": 0.0023,
      "step": 34330
    },
    {
      "epoch": 3.0524444444444443,
      "grad_norm": 0.24443942308425903,
      "learning_rate": 3.0922222222222225e-05,
      "loss": 0.0035,
      "step": 34340
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.5044204592704773,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.0025,
      "step": 34350
    },
    {
      "epoch": 3.054222222222222,
      "grad_norm": 0.7669423222541809,
      "learning_rate": 3.091111111111111e-05,
      "loss": 0.0025,
      "step": 34360
    },
    {
      "epoch": 3.055111111111111,
      "grad_norm": 0.06600527465343475,
      "learning_rate": 3.0905555555555555e-05,
      "loss": 0.0026,
      "step": 34370
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.19396737217903137,
      "learning_rate": 3.09e-05,
      "loss": 0.0037,
      "step": 34380
    },
    {
      "epoch": 3.056888888888889,
      "grad_norm": 1.004011869430542,
      "learning_rate": 3.089444444444445e-05,
      "loss": 0.003,
      "step": 34390
    },
    {
      "epoch": 3.057777777777778,
      "grad_norm": 0.5878859758377075,
      "learning_rate": 3.088888888888889e-05,
      "loss": 0.0017,
      "step": 34400
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.6173642873764038,
      "learning_rate": 3.0883333333333336e-05,
      "loss": 0.0019,
      "step": 34410
    },
    {
      "epoch": 3.0595555555555554,
      "grad_norm": 0.1697118580341339,
      "learning_rate": 3.087777777777778e-05,
      "loss": 0.0025,
      "step": 34420
    },
    {
      "epoch": 3.0604444444444443,
      "grad_norm": 0.1992805451154709,
      "learning_rate": 3.0872222222222223e-05,
      "loss": 0.0016,
      "step": 34430
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.6468165516853333,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0015,
      "step": 34440
    },
    {
      "epoch": 3.062222222222222,
      "grad_norm": 0.07041379064321518,
      "learning_rate": 3.086111111111111e-05,
      "loss": 0.0022,
      "step": 34450
    },
    {
      "epoch": 3.063111111111111,
      "grad_norm": 0.09435950964689255,
      "learning_rate": 3.085555555555556e-05,
      "loss": 0.0018,
      "step": 34460
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.08895668387413025,
      "learning_rate": 3.0850000000000004e-05,
      "loss": 0.0017,
      "step": 34470
    },
    {
      "epoch": 3.064888888888889,
      "grad_norm": 0.7491838932037354,
      "learning_rate": 3.084444444444445e-05,
      "loss": 0.002,
      "step": 34480
    },
    {
      "epoch": 3.065777777777778,
      "grad_norm": 0.5746809244155884,
      "learning_rate": 3.083888888888889e-05,
      "loss": 0.0023,
      "step": 34490
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.6223812103271484,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0021,
      "step": 34500
    },
    {
      "epoch": 3.0675555555555554,
      "grad_norm": 0.34501317143440247,
      "learning_rate": 3.082777777777778e-05,
      "loss": 0.0031,
      "step": 34510
    },
    {
      "epoch": 3.0684444444444443,
      "grad_norm": 0.20234042406082153,
      "learning_rate": 3.082222222222222e-05,
      "loss": 0.0021,
      "step": 34520
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.13083316385746002,
      "learning_rate": 3.0816666666666666e-05,
      "loss": 0.0018,
      "step": 34530
    },
    {
      "epoch": 3.070222222222222,
      "grad_norm": 1.1424589157104492,
      "learning_rate": 3.0811111111111116e-05,
      "loss": 0.002,
      "step": 34540
    },
    {
      "epoch": 3.071111111111111,
      "grad_norm": 0.6939051747322083,
      "learning_rate": 3.080555555555556e-05,
      "loss": 0.004,
      "step": 34550
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.5365064144134521,
      "learning_rate": 3.08e-05,
      "loss": 0.0027,
      "step": 34560
    },
    {
      "epoch": 3.072888888888889,
      "grad_norm": 0.2668645977973938,
      "learning_rate": 3.079444444444445e-05,
      "loss": 0.0033,
      "step": 34570
    },
    {
      "epoch": 3.073777777777778,
      "grad_norm": 0.23915377259254456,
      "learning_rate": 3.078888888888889e-05,
      "loss": 0.0023,
      "step": 34580
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.49261462688446045,
      "learning_rate": 3.0783333333333334e-05,
      "loss": 0.0018,
      "step": 34590
    },
    {
      "epoch": 3.0755555555555554,
      "grad_norm": 0.15278157591819763,
      "learning_rate": 3.077777777777778e-05,
      "loss": 0.003,
      "step": 34600
    },
    {
      "epoch": 3.0764444444444443,
      "grad_norm": 0.7118105888366699,
      "learning_rate": 3.077222222222223e-05,
      "loss": 0.0025,
      "step": 34610
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.36112821102142334,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.0015,
      "step": 34620
    },
    {
      "epoch": 3.078222222222222,
      "grad_norm": 0.2672152519226074,
      "learning_rate": 3.0761111111111115e-05,
      "loss": 0.003,
      "step": 34630
    },
    {
      "epoch": 3.079111111111111,
      "grad_norm": 0.3850756585597992,
      "learning_rate": 3.075555555555556e-05,
      "loss": 0.0019,
      "step": 34640
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.4022226333618164,
      "learning_rate": 3.075e-05,
      "loss": 0.0028,
      "step": 34650
    },
    {
      "epoch": 3.080888888888889,
      "grad_norm": 0.36546674370765686,
      "learning_rate": 3.0744444444444446e-05,
      "loss": 0.0014,
      "step": 34660
    },
    {
      "epoch": 3.081777777777778,
      "grad_norm": 0.20760135352611542,
      "learning_rate": 3.073888888888889e-05,
      "loss": 0.0021,
      "step": 34670
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.8344048261642456,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0016,
      "step": 34680
    },
    {
      "epoch": 3.0835555555555554,
      "grad_norm": 0.24125352501869202,
      "learning_rate": 3.0727777777777776e-05,
      "loss": 0.0018,
      "step": 34690
    },
    {
      "epoch": 3.0844444444444443,
      "grad_norm": 0.6433415412902832,
      "learning_rate": 3.0722222222222227e-05,
      "loss": 0.0028,
      "step": 34700
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.6403204202651978,
      "learning_rate": 3.0716666666666663e-05,
      "loss": 0.0028,
      "step": 34710
    },
    {
      "epoch": 3.086222222222222,
      "grad_norm": 0.30904486775398254,
      "learning_rate": 3.0711111111111114e-05,
      "loss": 0.002,
      "step": 34720
    },
    {
      "epoch": 3.087111111111111,
      "grad_norm": 0.5211820006370544,
      "learning_rate": 3.070555555555556e-05,
      "loss": 0.0024,
      "step": 34730
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.2908104658126831,
      "learning_rate": 3.07e-05,
      "loss": 0.0023,
      "step": 34740
    },
    {
      "epoch": 3.088888888888889,
      "grad_norm": 0.8071847558021545,
      "learning_rate": 3.069444444444445e-05,
      "loss": 0.0016,
      "step": 34750
    },
    {
      "epoch": 3.089777777777778,
      "grad_norm": 1.117161512374878,
      "learning_rate": 3.068888888888889e-05,
      "loss": 0.0028,
      "step": 34760
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.28868746757507324,
      "learning_rate": 3.068333333333334e-05,
      "loss": 0.0024,
      "step": 34770
    },
    {
      "epoch": 3.0915555555555554,
      "grad_norm": 0.7079663276672363,
      "learning_rate": 3.0677777777777775e-05,
      "loss": 0.0024,
      "step": 34780
    },
    {
      "epoch": 3.0924444444444443,
      "grad_norm": 0.819406270980835,
      "learning_rate": 3.0672222222222225e-05,
      "loss": 0.0023,
      "step": 34790
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.04328123852610588,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0018,
      "step": 34800
    },
    {
      "epoch": 3.094222222222222,
      "grad_norm": 0.25880274176597595,
      "learning_rate": 3.066111111111111e-05,
      "loss": 0.0019,
      "step": 34810
    },
    {
      "epoch": 3.095111111111111,
      "grad_norm": 0.4407176971435547,
      "learning_rate": 3.065555555555556e-05,
      "loss": 0.0026,
      "step": 34820
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.4985186457633972,
      "learning_rate": 3.065e-05,
      "loss": 0.0024,
      "step": 34830
    },
    {
      "epoch": 3.096888888888889,
      "grad_norm": 0.20060773193836212,
      "learning_rate": 3.064444444444445e-05,
      "loss": 0.0016,
      "step": 34840
    },
    {
      "epoch": 3.097777777777778,
      "grad_norm": 0.5483763217926025,
      "learning_rate": 3.063888888888889e-05,
      "loss": 0.0018,
      "step": 34850
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.20834924280643463,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0025,
      "step": 34860
    },
    {
      "epoch": 3.0995555555555554,
      "grad_norm": 0.7798454761505127,
      "learning_rate": 3.062777777777778e-05,
      "loss": 0.003,
      "step": 34870
    },
    {
      "epoch": 3.1004444444444443,
      "grad_norm": 0.2530936896800995,
      "learning_rate": 3.0622222222222224e-05,
      "loss": 0.0023,
      "step": 34880
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.2756894826889038,
      "learning_rate": 3.061666666666667e-05,
      "loss": 0.003,
      "step": 34890
    },
    {
      "epoch": 3.102222222222222,
      "grad_norm": 0.26527097821235657,
      "learning_rate": 3.061111111111111e-05,
      "loss": 0.0024,
      "step": 34900
    },
    {
      "epoch": 3.103111111111111,
      "grad_norm": 0.05104929953813553,
      "learning_rate": 3.060555555555556e-05,
      "loss": 0.0021,
      "step": 34910
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.7635166645050049,
      "learning_rate": 3.06e-05,
      "loss": 0.0024,
      "step": 34920
    },
    {
      "epoch": 3.104888888888889,
      "grad_norm": 0.8192038536071777,
      "learning_rate": 3.059444444444445e-05,
      "loss": 0.0027,
      "step": 34930
    },
    {
      "epoch": 3.105777777777778,
      "grad_norm": 0.3572048246860504,
      "learning_rate": 3.058888888888889e-05,
      "loss": 0.0021,
      "step": 34940
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.2769204080104828,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.0015,
      "step": 34950
    },
    {
      "epoch": 3.1075555555555554,
      "grad_norm": 0.5456631183624268,
      "learning_rate": 3.057777777777778e-05,
      "loss": 0.0014,
      "step": 34960
    },
    {
      "epoch": 3.1084444444444443,
      "grad_norm": 0.35791638493537903,
      "learning_rate": 3.057222222222222e-05,
      "loss": 0.0019,
      "step": 34970
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.4966651201248169,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0024,
      "step": 34980
    },
    {
      "epoch": 3.110222222222222,
      "grad_norm": 0.5852473378181458,
      "learning_rate": 3.056111111111111e-05,
      "loss": 0.0023,
      "step": 34990
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.19779284298419952,
      "learning_rate": 3.055555555555556e-05,
      "loss": 0.0028,
      "step": 35000
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.5088816285133362,
      "learning_rate": 3.0550000000000004e-05,
      "loss": 0.0025,
      "step": 35010
    },
    {
      "epoch": 3.112888888888889,
      "grad_norm": 0.35693275928497314,
      "learning_rate": 3.054444444444445e-05,
      "loss": 0.0027,
      "step": 35020
    },
    {
      "epoch": 3.113777777777778,
      "grad_norm": 0.27805766463279724,
      "learning_rate": 3.053888888888889e-05,
      "loss": 0.0025,
      "step": 35030
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.6957364678382874,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0018,
      "step": 35040
    },
    {
      "epoch": 3.1155555555555554,
      "grad_norm": 0.2494729459285736,
      "learning_rate": 3.052777777777778e-05,
      "loss": 0.0022,
      "step": 35050
    },
    {
      "epoch": 3.1164444444444444,
      "grad_norm": 0.9483948945999146,
      "learning_rate": 3.052222222222222e-05,
      "loss": 0.0022,
      "step": 35060
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.34074676036834717,
      "learning_rate": 3.0516666666666665e-05,
      "loss": 0.0024,
      "step": 35070
    },
    {
      "epoch": 3.1182222222222222,
      "grad_norm": 0.8575800061225891,
      "learning_rate": 3.0511111111111112e-05,
      "loss": 0.0033,
      "step": 35080
    },
    {
      "epoch": 3.119111111111111,
      "grad_norm": 1.0517170429229736,
      "learning_rate": 3.050555555555556e-05,
      "loss": 0.0025,
      "step": 35090
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.44360464811325073,
      "learning_rate": 3.05e-05,
      "loss": 0.0028,
      "step": 35100
    },
    {
      "epoch": 3.120888888888889,
      "grad_norm": 0.35153165459632874,
      "learning_rate": 3.0494444444444446e-05,
      "loss": 0.0021,
      "step": 35110
    },
    {
      "epoch": 3.121777777777778,
      "grad_norm": 0.09707982838153839,
      "learning_rate": 3.048888888888889e-05,
      "loss": 0.0014,
      "step": 35120
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.23805174231529236,
      "learning_rate": 3.0483333333333337e-05,
      "loss": 0.002,
      "step": 35130
    },
    {
      "epoch": 3.1235555555555554,
      "grad_norm": 0.6991055011749268,
      "learning_rate": 3.0477777777777777e-05,
      "loss": 0.003,
      "step": 35140
    },
    {
      "epoch": 3.1244444444444444,
      "grad_norm": 0.7513554096221924,
      "learning_rate": 3.0472222222222224e-05,
      "loss": 0.0018,
      "step": 35150
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.3158169686794281,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0038,
      "step": 35160
    },
    {
      "epoch": 3.1262222222222222,
      "grad_norm": 0.7560270428657532,
      "learning_rate": 3.046111111111111e-05,
      "loss": 0.0022,
      "step": 35170
    },
    {
      "epoch": 3.127111111111111,
      "grad_norm": 0.42735472321510315,
      "learning_rate": 3.0455555555555558e-05,
      "loss": 0.0022,
      "step": 35180
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.9105942249298096,
      "learning_rate": 3.045e-05,
      "loss": 0.0016,
      "step": 35190
    },
    {
      "epoch": 3.128888888888889,
      "grad_norm": 0.03644903004169464,
      "learning_rate": 3.044444444444445e-05,
      "loss": 0.0016,
      "step": 35200
    },
    {
      "epoch": 3.129777777777778,
      "grad_norm": 0.5610617399215698,
      "learning_rate": 3.043888888888889e-05,
      "loss": 0.0027,
      "step": 35210
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.8323925733566284,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.0024,
      "step": 35220
    },
    {
      "epoch": 3.1315555555555554,
      "grad_norm": 0.21147063374519348,
      "learning_rate": 3.0427777777777776e-05,
      "loss": 0.0025,
      "step": 35230
    },
    {
      "epoch": 3.1324444444444444,
      "grad_norm": 0.41951850056648254,
      "learning_rate": 3.0422222222222223e-05,
      "loss": 0.0028,
      "step": 35240
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.6942740678787231,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.0019,
      "step": 35250
    },
    {
      "epoch": 3.1342222222222222,
      "grad_norm": 0.0799839124083519,
      "learning_rate": 3.0411111111111113e-05,
      "loss": 0.0024,
      "step": 35260
    },
    {
      "epoch": 3.135111111111111,
      "grad_norm": 0.03767843917012215,
      "learning_rate": 3.040555555555556e-05,
      "loss": 0.003,
      "step": 35270
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.03751683607697487,
      "learning_rate": 3.04e-05,
      "loss": 0.0015,
      "step": 35280
    },
    {
      "epoch": 3.136888888888889,
      "grad_norm": 0.5370891690254211,
      "learning_rate": 3.0394444444444447e-05,
      "loss": 0.0026,
      "step": 35290
    },
    {
      "epoch": 3.137777777777778,
      "grad_norm": 0.7357520461082458,
      "learning_rate": 3.0388888888888887e-05,
      "loss": 0.002,
      "step": 35300
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.03412682190537453,
      "learning_rate": 3.0383333333333334e-05,
      "loss": 0.0017,
      "step": 35310
    },
    {
      "epoch": 3.1395555555555554,
      "grad_norm": 1.0207141637802124,
      "learning_rate": 3.0377777777777778e-05,
      "loss": 0.0018,
      "step": 35320
    },
    {
      "epoch": 3.1404444444444444,
      "grad_norm": 0.053876422345638275,
      "learning_rate": 3.0372222222222225e-05,
      "loss": 0.0026,
      "step": 35330
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.3915688395500183,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0024,
      "step": 35340
    },
    {
      "epoch": 3.1422222222222222,
      "grad_norm": 0.44051501154899597,
      "learning_rate": 3.0361111111111112e-05,
      "loss": 0.0024,
      "step": 35350
    },
    {
      "epoch": 3.143111111111111,
      "grad_norm": 0.7591952681541443,
      "learning_rate": 3.035555555555556e-05,
      "loss": 0.002,
      "step": 35360
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.4552140235900879,
      "learning_rate": 3.035e-05,
      "loss": 0.0026,
      "step": 35370
    },
    {
      "epoch": 3.144888888888889,
      "grad_norm": 0.4305512607097626,
      "learning_rate": 3.0344444444444446e-05,
      "loss": 0.002,
      "step": 35380
    },
    {
      "epoch": 3.145777777777778,
      "grad_norm": 0.3845084309577942,
      "learning_rate": 3.033888888888889e-05,
      "loss": 0.0035,
      "step": 35390
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.30745989084243774,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0025,
      "step": 35400
    },
    {
      "epoch": 3.1475555555555554,
      "grad_norm": 0.7244457006454468,
      "learning_rate": 3.0327777777777777e-05,
      "loss": 0.0019,
      "step": 35410
    },
    {
      "epoch": 3.1484444444444444,
      "grad_norm": 0.8064769506454468,
      "learning_rate": 3.0322222222222224e-05,
      "loss": 0.0021,
      "step": 35420
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.31403452157974243,
      "learning_rate": 3.0316666666666664e-05,
      "loss": 0.0029,
      "step": 35430
    },
    {
      "epoch": 3.1502222222222223,
      "grad_norm": 0.1002134159207344,
      "learning_rate": 3.031111111111111e-05,
      "loss": 0.0032,
      "step": 35440
    },
    {
      "epoch": 3.151111111111111,
      "grad_norm": 0.18440160155296326,
      "learning_rate": 3.0305555555555558e-05,
      "loss": 0.0034,
      "step": 35450
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.10626814514398575,
      "learning_rate": 3.03e-05,
      "loss": 0.0021,
      "step": 35460
    },
    {
      "epoch": 3.152888888888889,
      "grad_norm": 0.2742721438407898,
      "learning_rate": 3.0294444444444448e-05,
      "loss": 0.0015,
      "step": 35470
    },
    {
      "epoch": 3.153777777777778,
      "grad_norm": 0.1282041370868683,
      "learning_rate": 3.028888888888889e-05,
      "loss": 0.0019,
      "step": 35480
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.3878214955329895,
      "learning_rate": 3.0283333333333335e-05,
      "loss": 0.0026,
      "step": 35490
    },
    {
      "epoch": 3.1555555555555554,
      "grad_norm": 0.20229285955429077,
      "learning_rate": 3.0277777777777776e-05,
      "loss": 0.0028,
      "step": 35500
    },
    {
      "epoch": 3.1564444444444444,
      "grad_norm": 0.5408738851547241,
      "learning_rate": 3.0272222222222222e-05,
      "loss": 0.0023,
      "step": 35510
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.3103844225406647,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0022,
      "step": 35520
    },
    {
      "epoch": 3.1582222222222223,
      "grad_norm": 0.201391339302063,
      "learning_rate": 3.0261111111111113e-05,
      "loss": 0.0022,
      "step": 35530
    },
    {
      "epoch": 3.159111111111111,
      "grad_norm": 0.3889174163341522,
      "learning_rate": 3.025555555555556e-05,
      "loss": 0.0019,
      "step": 35540
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.3552398085594177,
      "learning_rate": 3.025e-05,
      "loss": 0.0013,
      "step": 35550
    },
    {
      "epoch": 3.160888888888889,
      "grad_norm": 0.6567063927650452,
      "learning_rate": 3.0244444444444447e-05,
      "loss": 0.0015,
      "step": 35560
    },
    {
      "epoch": 3.1617777777777776,
      "grad_norm": 0.5076449513435364,
      "learning_rate": 3.0238888888888887e-05,
      "loss": 0.0026,
      "step": 35570
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.6683419942855835,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.0025,
      "step": 35580
    },
    {
      "epoch": 3.1635555555555555,
      "grad_norm": 0.12865722179412842,
      "learning_rate": 3.0227777777777778e-05,
      "loss": 0.002,
      "step": 35590
    },
    {
      "epoch": 3.1644444444444444,
      "grad_norm": 0.2817780673503876,
      "learning_rate": 3.0222222222222225e-05,
      "loss": 0.0018,
      "step": 35600
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.5877983570098877,
      "learning_rate": 3.0216666666666665e-05,
      "loss": 0.0017,
      "step": 35610
    },
    {
      "epoch": 3.1662222222222223,
      "grad_norm": 0.9465717077255249,
      "learning_rate": 3.0211111111111112e-05,
      "loss": 0.0022,
      "step": 35620
    },
    {
      "epoch": 3.167111111111111,
      "grad_norm": 0.04693092778325081,
      "learning_rate": 3.020555555555556e-05,
      "loss": 0.0027,
      "step": 35630
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.048165373504161835,
      "learning_rate": 3.02e-05,
      "loss": 0.002,
      "step": 35640
    },
    {
      "epoch": 3.168888888888889,
      "grad_norm": 0.4621298611164093,
      "learning_rate": 3.0194444444444446e-05,
      "loss": 0.0031,
      "step": 35650
    },
    {
      "epoch": 3.1697777777777776,
      "grad_norm": 0.2752273976802826,
      "learning_rate": 3.018888888888889e-05,
      "loss": 0.0023,
      "step": 35660
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.3345644772052765,
      "learning_rate": 3.0183333333333336e-05,
      "loss": 0.0022,
      "step": 35670
    },
    {
      "epoch": 3.1715555555555555,
      "grad_norm": 0.047193579375743866,
      "learning_rate": 3.0177777777777776e-05,
      "loss": 0.0017,
      "step": 35680
    },
    {
      "epoch": 3.1724444444444444,
      "grad_norm": 0.541934072971344,
      "learning_rate": 3.0172222222222223e-05,
      "loss": 0.0022,
      "step": 35690
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.19498731195926666,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0029,
      "step": 35700
    },
    {
      "epoch": 3.1742222222222223,
      "grad_norm": 0.4075552523136139,
      "learning_rate": 3.016111111111111e-05,
      "loss": 0.0022,
      "step": 35710
    },
    {
      "epoch": 3.175111111111111,
      "grad_norm": 0.639228880405426,
      "learning_rate": 3.0155555555555557e-05,
      "loss": 0.0025,
      "step": 35720
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.2800346910953522,
      "learning_rate": 3.015e-05,
      "loss": 0.0021,
      "step": 35730
    },
    {
      "epoch": 3.176888888888889,
      "grad_norm": 0.042761243879795074,
      "learning_rate": 3.0144444444444448e-05,
      "loss": 0.0018,
      "step": 35740
    },
    {
      "epoch": 3.1777777777777776,
      "grad_norm": 0.2031576931476593,
      "learning_rate": 3.0138888888888888e-05,
      "loss": 0.0023,
      "step": 35750
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.4171868562698364,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0024,
      "step": 35760
    },
    {
      "epoch": 3.1795555555555555,
      "grad_norm": 0.4326517581939697,
      "learning_rate": 3.012777777777778e-05,
      "loss": 0.0031,
      "step": 35770
    },
    {
      "epoch": 3.1804444444444444,
      "grad_norm": 0.6178159117698669,
      "learning_rate": 3.0122222222222226e-05,
      "loss": 0.0015,
      "step": 35780
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.23079761862754822,
      "learning_rate": 3.011666666666667e-05,
      "loss": 0.0024,
      "step": 35790
    },
    {
      "epoch": 3.1822222222222223,
      "grad_norm": 0.6803212761878967,
      "learning_rate": 3.0111111111111113e-05,
      "loss": 0.0025,
      "step": 35800
    },
    {
      "epoch": 3.1831111111111112,
      "grad_norm": 0.5675433874130249,
      "learning_rate": 3.010555555555556e-05,
      "loss": 0.0019,
      "step": 35810
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.06064365804195404,
      "learning_rate": 3.01e-05,
      "loss": 0.0015,
      "step": 35820
    },
    {
      "epoch": 3.1848888888888887,
      "grad_norm": 0.41188183426856995,
      "learning_rate": 3.0094444444444447e-05,
      "loss": 0.002,
      "step": 35830
    },
    {
      "epoch": 3.1857777777777776,
      "grad_norm": 0.5501622557640076,
      "learning_rate": 3.008888888888889e-05,
      "loss": 0.002,
      "step": 35840
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.41388916969299316,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.0037,
      "step": 35850
    },
    {
      "epoch": 3.1875555555555555,
      "grad_norm": 0.5967299938201904,
      "learning_rate": 3.0077777777777777e-05,
      "loss": 0.0016,
      "step": 35860
    },
    {
      "epoch": 3.1884444444444444,
      "grad_norm": 0.46456295251846313,
      "learning_rate": 3.0072222222222224e-05,
      "loss": 0.0026,
      "step": 35870
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.6527701616287231,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0029,
      "step": 35880
    },
    {
      "epoch": 3.1902222222222223,
      "grad_norm": 0.4451547861099243,
      "learning_rate": 3.006111111111111e-05,
      "loss": 0.0027,
      "step": 35890
    },
    {
      "epoch": 3.1911111111111112,
      "grad_norm": 0.0786881074309349,
      "learning_rate": 3.005555555555556e-05,
      "loss": 0.0018,
      "step": 35900
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.4242161214351654,
      "learning_rate": 3.0050000000000002e-05,
      "loss": 0.0014,
      "step": 35910
    },
    {
      "epoch": 3.1928888888888887,
      "grad_norm": 0.08771125972270966,
      "learning_rate": 3.004444444444445e-05,
      "loss": 0.003,
      "step": 35920
    },
    {
      "epoch": 3.1937777777777776,
      "grad_norm": 0.2103928178548813,
      "learning_rate": 3.003888888888889e-05,
      "loss": 0.0021,
      "step": 35930
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.12228308618068695,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.002,
      "step": 35940
    },
    {
      "epoch": 3.1955555555555555,
      "grad_norm": 0.03886999562382698,
      "learning_rate": 3.0027777777777776e-05,
      "loss": 0.0033,
      "step": 35950
    },
    {
      "epoch": 3.1964444444444444,
      "grad_norm": 0.7015939354896545,
      "learning_rate": 3.0022222222222223e-05,
      "loss": 0.0024,
      "step": 35960
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.13232462108135223,
      "learning_rate": 3.001666666666667e-05,
      "loss": 0.0027,
      "step": 35970
    },
    {
      "epoch": 3.1982222222222223,
      "grad_norm": 0.2076338529586792,
      "learning_rate": 3.0011111111111114e-05,
      "loss": 0.0034,
      "step": 35980
    },
    {
      "epoch": 3.1991111111111112,
      "grad_norm": 0.6590197086334229,
      "learning_rate": 3.000555555555556e-05,
      "loss": 0.0025,
      "step": 35990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.10451190918684006,
      "learning_rate": 3e-05,
      "loss": 0.0035,
      "step": 36000
    },
    {
      "epoch": 3.2008888888888887,
      "grad_norm": 0.8306431770324707,
      "learning_rate": 2.9994444444444448e-05,
      "loss": 0.0014,
      "step": 36010
    },
    {
      "epoch": 3.2017777777777776,
      "grad_norm": 0.26777157187461853,
      "learning_rate": 2.9988888888888888e-05,
      "loss": 0.0018,
      "step": 36020
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.7324843406677246,
      "learning_rate": 2.9983333333333335e-05,
      "loss": 0.002,
      "step": 36030
    },
    {
      "epoch": 3.2035555555555555,
      "grad_norm": 0.4800449311733246,
      "learning_rate": 2.997777777777778e-05,
      "loss": 0.002,
      "step": 36040
    },
    {
      "epoch": 3.2044444444444444,
      "grad_norm": 0.702172577381134,
      "learning_rate": 2.9972222222222225e-05,
      "loss": 0.0031,
      "step": 36050
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.8748286366462708,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.0027,
      "step": 36060
    },
    {
      "epoch": 3.2062222222222223,
      "grad_norm": 0.20223240554332733,
      "learning_rate": 2.9961111111111112e-05,
      "loss": 0.0019,
      "step": 36070
    },
    {
      "epoch": 3.2071111111111112,
      "grad_norm": 0.48477160930633545,
      "learning_rate": 2.995555555555556e-05,
      "loss": 0.0017,
      "step": 36080
    },
    {
      "epoch": 3.208,
      "grad_norm": 1.2350062131881714,
      "learning_rate": 2.995e-05,
      "loss": 0.0016,
      "step": 36090
    },
    {
      "epoch": 3.2088888888888887,
      "grad_norm": 0.9090273976325989,
      "learning_rate": 2.9944444444444446e-05,
      "loss": 0.0027,
      "step": 36100
    },
    {
      "epoch": 3.2097777777777776,
      "grad_norm": 0.5981298685073853,
      "learning_rate": 2.993888888888889e-05,
      "loss": 0.0028,
      "step": 36110
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.4296503961086273,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0032,
      "step": 36120
    },
    {
      "epoch": 3.2115555555555555,
      "grad_norm": 0.5176160335540771,
      "learning_rate": 2.9927777777777777e-05,
      "loss": 0.0019,
      "step": 36130
    },
    {
      "epoch": 3.2124444444444444,
      "grad_norm": 0.25350475311279297,
      "learning_rate": 2.9922222222222224e-05,
      "loss": 0.0023,
      "step": 36140
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.1812836229801178,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.0018,
      "step": 36150
    },
    {
      "epoch": 3.2142222222222223,
      "grad_norm": 0.4996736943721771,
      "learning_rate": 2.991111111111111e-05,
      "loss": 0.0016,
      "step": 36160
    },
    {
      "epoch": 3.2151111111111113,
      "grad_norm": 0.15809616446495056,
      "learning_rate": 2.9905555555555558e-05,
      "loss": 0.0013,
      "step": 36170
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.5420989990234375,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0015,
      "step": 36180
    },
    {
      "epoch": 3.2168888888888887,
      "grad_norm": 0.09111049026250839,
      "learning_rate": 2.989444444444445e-05,
      "loss": 0.0019,
      "step": 36190
    },
    {
      "epoch": 3.2177777777777776,
      "grad_norm": 0.5106890201568604,
      "learning_rate": 2.988888888888889e-05,
      "loss": 0.0023,
      "step": 36200
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.08074747025966644,
      "learning_rate": 2.9883333333333336e-05,
      "loss": 0.0025,
      "step": 36210
    },
    {
      "epoch": 3.2195555555555555,
      "grad_norm": 0.37776607275009155,
      "learning_rate": 2.9877777777777776e-05,
      "loss": 0.0016,
      "step": 36220
    },
    {
      "epoch": 3.2204444444444444,
      "grad_norm": 0.1044577956199646,
      "learning_rate": 2.9872222222222223e-05,
      "loss": 0.0026,
      "step": 36230
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.21394741535186768,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0019,
      "step": 36240
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.3904503583908081,
      "learning_rate": 2.9861111111111113e-05,
      "loss": 0.0027,
      "step": 36250
    },
    {
      "epoch": 3.2231111111111113,
      "grad_norm": 0.3531353175640106,
      "learning_rate": 2.985555555555556e-05,
      "loss": 0.0015,
      "step": 36260
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.20691901445388794,
      "learning_rate": 2.985e-05,
      "loss": 0.0018,
      "step": 36270
    },
    {
      "epoch": 3.2248888888888887,
      "grad_norm": 0.18452788889408112,
      "learning_rate": 2.9844444444444447e-05,
      "loss": 0.0022,
      "step": 36280
    },
    {
      "epoch": 3.2257777777777776,
      "grad_norm": 0.31220272183418274,
      "learning_rate": 2.9838888888888888e-05,
      "loss": 0.0026,
      "step": 36290
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.6244315505027771,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0012,
      "step": 36300
    },
    {
      "epoch": 3.2275555555555555,
      "grad_norm": 0.2864437699317932,
      "learning_rate": 2.9827777777777778e-05,
      "loss": 0.0025,
      "step": 36310
    },
    {
      "epoch": 3.2284444444444444,
      "grad_norm": 0.5470638275146484,
      "learning_rate": 2.9822222222222225e-05,
      "loss": 0.0017,
      "step": 36320
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.5409306883811951,
      "learning_rate": 2.9816666666666672e-05,
      "loss": 0.0017,
      "step": 36330
    },
    {
      "epoch": 3.2302222222222223,
      "grad_norm": 0.2733791172504425,
      "learning_rate": 2.9811111111111112e-05,
      "loss": 0.0023,
      "step": 36340
    },
    {
      "epoch": 3.2311111111111113,
      "grad_norm": 0.18457558751106262,
      "learning_rate": 2.980555555555556e-05,
      "loss": 0.0025,
      "step": 36350
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.17384132742881775,
      "learning_rate": 2.98e-05,
      "loss": 0.0029,
      "step": 36360
    },
    {
      "epoch": 3.2328888888888887,
      "grad_norm": 0.1651778370141983,
      "learning_rate": 2.9794444444444446e-05,
      "loss": 0.0018,
      "step": 36370
    },
    {
      "epoch": 3.2337777777777776,
      "grad_norm": 0.5248735547065735,
      "learning_rate": 2.978888888888889e-05,
      "loss": 0.0024,
      "step": 36380
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.6572414040565491,
      "learning_rate": 2.9783333333333337e-05,
      "loss": 0.0028,
      "step": 36390
    },
    {
      "epoch": 3.2355555555555555,
      "grad_norm": 0.732438325881958,
      "learning_rate": 2.9777777777777777e-05,
      "loss": 0.0023,
      "step": 36400
    },
    {
      "epoch": 3.2364444444444445,
      "grad_norm": 0.18405362963676453,
      "learning_rate": 2.9772222222222224e-05,
      "loss": 0.0029,
      "step": 36410
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.4229637384414673,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.002,
      "step": 36420
    },
    {
      "epoch": 3.2382222222222223,
      "grad_norm": 0.17277474701404572,
      "learning_rate": 2.976111111111111e-05,
      "loss": 0.0028,
      "step": 36430
    },
    {
      "epoch": 3.2391111111111113,
      "grad_norm": 0.0647183209657669,
      "learning_rate": 2.9755555555555558e-05,
      "loss": 0.0028,
      "step": 36440
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.3017294704914093,
      "learning_rate": 2.975e-05,
      "loss": 0.0022,
      "step": 36450
    },
    {
      "epoch": 3.2408888888888887,
      "grad_norm": 0.26743757724761963,
      "learning_rate": 2.974444444444445e-05,
      "loss": 0.0024,
      "step": 36460
    },
    {
      "epoch": 3.2417777777777776,
      "grad_norm": 0.46213340759277344,
      "learning_rate": 2.973888888888889e-05,
      "loss": 0.0021,
      "step": 36470
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.26472756266593933,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0021,
      "step": 36480
    },
    {
      "epoch": 3.2435555555555555,
      "grad_norm": 0.7449454069137573,
      "learning_rate": 2.9727777777777776e-05,
      "loss": 0.0019,
      "step": 36490
    },
    {
      "epoch": 3.2444444444444445,
      "grad_norm": 0.110195592045784,
      "learning_rate": 2.9722222222222223e-05,
      "loss": 0.0017,
      "step": 36500
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.17146648466587067,
      "learning_rate": 2.971666666666667e-05,
      "loss": 0.0029,
      "step": 36510
    },
    {
      "epoch": 3.2462222222222223,
      "grad_norm": 0.3808414340019226,
      "learning_rate": 2.9711111111111113e-05,
      "loss": 0.0033,
      "step": 36520
    },
    {
      "epoch": 3.2471111111111113,
      "grad_norm": 0.6364067792892456,
      "learning_rate": 2.970555555555556e-05,
      "loss": 0.0022,
      "step": 36530
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.5848385691642761,
      "learning_rate": 2.97e-05,
      "loss": 0.0027,
      "step": 36540
    },
    {
      "epoch": 3.2488888888888887,
      "grad_norm": 0.2994800806045532,
      "learning_rate": 2.9694444444444447e-05,
      "loss": 0.0021,
      "step": 36550
    },
    {
      "epoch": 3.2497777777777777,
      "grad_norm": 0.4778938889503479,
      "learning_rate": 2.9688888888888887e-05,
      "loss": 0.0021,
      "step": 36560
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.3476782739162445,
      "learning_rate": 2.9683333333333334e-05,
      "loss": 0.0034,
      "step": 36570
    },
    {
      "epoch": 3.2515555555555555,
      "grad_norm": 0.2867357134819031,
      "learning_rate": 2.9677777777777778e-05,
      "loss": 0.0025,
      "step": 36580
    },
    {
      "epoch": 3.2524444444444445,
      "grad_norm": 0.23593582212924957,
      "learning_rate": 2.9672222222222225e-05,
      "loss": 0.002,
      "step": 36590
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.4551260471343994,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0016,
      "step": 36600
    },
    {
      "epoch": 3.2542222222222223,
      "grad_norm": 0.45180776715278625,
      "learning_rate": 2.9661111111111112e-05,
      "loss": 0.002,
      "step": 36610
    },
    {
      "epoch": 3.2551111111111113,
      "grad_norm": 0.7754374742507935,
      "learning_rate": 2.965555555555556e-05,
      "loss": 0.0036,
      "step": 36620
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.06807788461446762,
      "learning_rate": 2.965e-05,
      "loss": 0.0024,
      "step": 36630
    },
    {
      "epoch": 3.2568888888888887,
      "grad_norm": 0.21563436090946198,
      "learning_rate": 2.9644444444444446e-05,
      "loss": 0.0027,
      "step": 36640
    },
    {
      "epoch": 3.2577777777777777,
      "grad_norm": 0.047724101692438126,
      "learning_rate": 2.963888888888889e-05,
      "loss": 0.0025,
      "step": 36650
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.38722917437553406,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0022,
      "step": 36660
    },
    {
      "epoch": 3.2595555555555555,
      "grad_norm": 0.3841835558414459,
      "learning_rate": 2.9627777777777777e-05,
      "loss": 0.0033,
      "step": 36670
    },
    {
      "epoch": 3.2604444444444445,
      "grad_norm": 0.02903018891811371,
      "learning_rate": 2.9622222222222224e-05,
      "loss": 0.0019,
      "step": 36680
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.24175123870372772,
      "learning_rate": 2.961666666666667e-05,
      "loss": 0.0026,
      "step": 36690
    },
    {
      "epoch": 3.2622222222222224,
      "grad_norm": 0.40284404158592224,
      "learning_rate": 2.961111111111111e-05,
      "loss": 0.0026,
      "step": 36700
    },
    {
      "epoch": 3.2631111111111113,
      "grad_norm": 0.10370489954948425,
      "learning_rate": 2.9605555555555558e-05,
      "loss": 0.0027,
      "step": 36710
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.4409804046154022,
      "learning_rate": 2.96e-05,
      "loss": 0.0017,
      "step": 36720
    },
    {
      "epoch": 3.2648888888888887,
      "grad_norm": 0.33719080686569214,
      "learning_rate": 2.9594444444444448e-05,
      "loss": 0.0023,
      "step": 36730
    },
    {
      "epoch": 3.2657777777777777,
      "grad_norm": 0.23326970636844635,
      "learning_rate": 2.958888888888889e-05,
      "loss": 0.0019,
      "step": 36740
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.38365182280540466,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.0028,
      "step": 36750
    },
    {
      "epoch": 3.2675555555555555,
      "grad_norm": 0.10452965646982193,
      "learning_rate": 2.9577777777777775e-05,
      "loss": 0.0014,
      "step": 36760
    },
    {
      "epoch": 3.2684444444444445,
      "grad_norm": 0.49173861742019653,
      "learning_rate": 2.9572222222222222e-05,
      "loss": 0.004,
      "step": 36770
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.3394874930381775,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0022,
      "step": 36780
    },
    {
      "epoch": 3.2702222222222224,
      "grad_norm": 0.23738043010234833,
      "learning_rate": 2.9561111111111113e-05,
      "loss": 0.0018,
      "step": 36790
    },
    {
      "epoch": 3.2711111111111113,
      "grad_norm": 0.06241299584507942,
      "learning_rate": 2.955555555555556e-05,
      "loss": 0.0016,
      "step": 36800
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.07915305346250534,
      "learning_rate": 2.955e-05,
      "loss": 0.0025,
      "step": 36810
    },
    {
      "epoch": 3.2728888888888887,
      "grad_norm": 0.6007605791091919,
      "learning_rate": 2.9544444444444447e-05,
      "loss": 0.0019,
      "step": 36820
    },
    {
      "epoch": 3.2737777777777777,
      "grad_norm": 0.4509846866130829,
      "learning_rate": 2.9538888888888887e-05,
      "loss": 0.0021,
      "step": 36830
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.4180983006954193,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0037,
      "step": 36840
    },
    {
      "epoch": 3.2755555555555556,
      "grad_norm": 0.06044376641511917,
      "learning_rate": 2.9527777777777778e-05,
      "loss": 0.0016,
      "step": 36850
    },
    {
      "epoch": 3.2764444444444445,
      "grad_norm": 0.20395372807979584,
      "learning_rate": 2.9522222222222225e-05,
      "loss": 0.0031,
      "step": 36860
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.37044116854667664,
      "learning_rate": 2.951666666666667e-05,
      "loss": 0.0022,
      "step": 36870
    },
    {
      "epoch": 3.2782222222222224,
      "grad_norm": 0.31573814153671265,
      "learning_rate": 2.951111111111111e-05,
      "loss": 0.0023,
      "step": 36880
    },
    {
      "epoch": 3.279111111111111,
      "grad_norm": 0.2553866505622864,
      "learning_rate": 2.950555555555556e-05,
      "loss": 0.0034,
      "step": 36890
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.5627231597900391,
      "learning_rate": 2.95e-05,
      "loss": 0.0024,
      "step": 36900
    },
    {
      "epoch": 3.2808888888888887,
      "grad_norm": 0.30713945627212524,
      "learning_rate": 2.9494444444444446e-05,
      "loss": 0.0016,
      "step": 36910
    },
    {
      "epoch": 3.2817777777777777,
      "grad_norm": 0.10332006961107254,
      "learning_rate": 2.948888888888889e-05,
      "loss": 0.0021,
      "step": 36920
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.6170881390571594,
      "learning_rate": 2.9483333333333336e-05,
      "loss": 0.0019,
      "step": 36930
    },
    {
      "epoch": 3.2835555555555556,
      "grad_norm": 0.2929680347442627,
      "learning_rate": 2.9477777777777783e-05,
      "loss": 0.0028,
      "step": 36940
    },
    {
      "epoch": 3.2844444444444445,
      "grad_norm": 0.5172549486160278,
      "learning_rate": 2.9472222222222223e-05,
      "loss": 0.0016,
      "step": 36950
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.8814894556999207,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0031,
      "step": 36960
    },
    {
      "epoch": 3.2862222222222224,
      "grad_norm": 0.3037512004375458,
      "learning_rate": 2.946111111111111e-05,
      "loss": 0.0024,
      "step": 36970
    },
    {
      "epoch": 3.287111111111111,
      "grad_norm": 0.2507287561893463,
      "learning_rate": 2.9455555555555557e-05,
      "loss": 0.0019,
      "step": 36980
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.7276989221572876,
      "learning_rate": 2.945e-05,
      "loss": 0.0019,
      "step": 36990
    },
    {
      "epoch": 3.2888888888888888,
      "grad_norm": 0.15895727276802063,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 0.0018,
      "step": 37000
    },
    {
      "epoch": 3.2897777777777777,
      "grad_norm": 0.5049629807472229,
      "learning_rate": 2.9438888888888888e-05,
      "loss": 0.002,
      "step": 37010
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.5003144145011902,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0018,
      "step": 37020
    },
    {
      "epoch": 3.2915555555555556,
      "grad_norm": 0.4932844936847687,
      "learning_rate": 2.9427777777777782e-05,
      "loss": 0.0019,
      "step": 37030
    },
    {
      "epoch": 3.2924444444444445,
      "grad_norm": 0.49224305152893066,
      "learning_rate": 2.9422222222222222e-05,
      "loss": 0.0024,
      "step": 37040
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.32584309577941895,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.0022,
      "step": 37050
    },
    {
      "epoch": 3.2942222222222224,
      "grad_norm": 0.5227559804916382,
      "learning_rate": 2.9411111111111113e-05,
      "loss": 0.0025,
      "step": 37060
    },
    {
      "epoch": 3.295111111111111,
      "grad_norm": 0.5780108571052551,
      "learning_rate": 2.940555555555556e-05,
      "loss": 0.0019,
      "step": 37070
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.500251293182373,
      "learning_rate": 2.94e-05,
      "loss": 0.0023,
      "step": 37080
    },
    {
      "epoch": 3.2968888888888888,
      "grad_norm": 0.35580700635910034,
      "learning_rate": 2.9394444444444447e-05,
      "loss": 0.0023,
      "step": 37090
    },
    {
      "epoch": 3.2977777777777777,
      "grad_norm": 0.6639034748077393,
      "learning_rate": 2.9388888888888887e-05,
      "loss": 0.0027,
      "step": 37100
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.11327925324440002,
      "learning_rate": 2.9383333333333334e-05,
      "loss": 0.0015,
      "step": 37110
    },
    {
      "epoch": 3.2995555555555556,
      "grad_norm": 0.6587427258491516,
      "learning_rate": 2.937777777777778e-05,
      "loss": 0.0031,
      "step": 37120
    },
    {
      "epoch": 3.3004444444444445,
      "grad_norm": 0.34932658076286316,
      "learning_rate": 2.9372222222222224e-05,
      "loss": 0.0024,
      "step": 37130
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.19562804698944092,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.002,
      "step": 37140
    },
    {
      "epoch": 3.3022222222222224,
      "grad_norm": 0.9712945818901062,
      "learning_rate": 2.936111111111111e-05,
      "loss": 0.0028,
      "step": 37150
    },
    {
      "epoch": 3.303111111111111,
      "grad_norm": 0.658230721950531,
      "learning_rate": 2.935555555555556e-05,
      "loss": 0.0017,
      "step": 37160
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.9590544104576111,
      "learning_rate": 2.935e-05,
      "loss": 0.002,
      "step": 37170
    },
    {
      "epoch": 3.3048888888888888,
      "grad_norm": 0.04830537736415863,
      "learning_rate": 2.9344444444444445e-05,
      "loss": 0.0022,
      "step": 37180
    },
    {
      "epoch": 3.3057777777777777,
      "grad_norm": 0.7070627212524414,
      "learning_rate": 2.933888888888889e-05,
      "loss": 0.0019,
      "step": 37190
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.17031623423099518,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0026,
      "step": 37200
    },
    {
      "epoch": 3.3075555555555556,
      "grad_norm": 0.38149935007095337,
      "learning_rate": 2.9327777777777783e-05,
      "loss": 0.0035,
      "step": 37210
    },
    {
      "epoch": 3.3084444444444445,
      "grad_norm": 0.20741961896419525,
      "learning_rate": 2.9322222222222223e-05,
      "loss": 0.0028,
      "step": 37220
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.5842747688293457,
      "learning_rate": 2.931666666666667e-05,
      "loss": 0.0031,
      "step": 37230
    },
    {
      "epoch": 3.3102222222222224,
      "grad_norm": 0.565526008605957,
      "learning_rate": 2.931111111111111e-05,
      "loss": 0.0024,
      "step": 37240
    },
    {
      "epoch": 3.311111111111111,
      "grad_norm": 0.23281686007976532,
      "learning_rate": 2.9305555555555557e-05,
      "loss": 0.0024,
      "step": 37250
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.2797030508518219,
      "learning_rate": 2.93e-05,
      "loss": 0.0015,
      "step": 37260
    },
    {
      "epoch": 3.3128888888888888,
      "grad_norm": 0.2365216463804245,
      "learning_rate": 2.9294444444444448e-05,
      "loss": 0.0025,
      "step": 37270
    },
    {
      "epoch": 3.3137777777777777,
      "grad_norm": 0.25548040866851807,
      "learning_rate": 2.9288888888888888e-05,
      "loss": 0.0022,
      "step": 37280
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.8314955234527588,
      "learning_rate": 2.9283333333333335e-05,
      "loss": 0.0019,
      "step": 37290
    },
    {
      "epoch": 3.3155555555555556,
      "grad_norm": 0.2652861177921295,
      "learning_rate": 2.927777777777778e-05,
      "loss": 0.0014,
      "step": 37300
    },
    {
      "epoch": 3.3164444444444445,
      "grad_norm": 0.7300868630409241,
      "learning_rate": 2.9272222222222222e-05,
      "loss": 0.0021,
      "step": 37310
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.238729789853096,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0024,
      "step": 37320
    },
    {
      "epoch": 3.3182222222222224,
      "grad_norm": 0.5447247624397278,
      "learning_rate": 2.9261111111111112e-05,
      "loss": 0.0025,
      "step": 37330
    },
    {
      "epoch": 3.319111111111111,
      "grad_norm": 0.4763002097606659,
      "learning_rate": 2.925555555555556e-05,
      "loss": 0.0019,
      "step": 37340
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.3691506087779999,
      "learning_rate": 2.925e-05,
      "loss": 0.0022,
      "step": 37350
    },
    {
      "epoch": 3.320888888888889,
      "grad_norm": 0.044568322598934174,
      "learning_rate": 2.9244444444444446e-05,
      "loss": 0.0012,
      "step": 37360
    },
    {
      "epoch": 3.3217777777777777,
      "grad_norm": 0.4354068636894226,
      "learning_rate": 2.9238888888888887e-05,
      "loss": 0.0018,
      "step": 37370
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.5512217283248901,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.003,
      "step": 37380
    },
    {
      "epoch": 3.3235555555555556,
      "grad_norm": 0.3428153991699219,
      "learning_rate": 2.922777777777778e-05,
      "loss": 0.0019,
      "step": 37390
    },
    {
      "epoch": 3.3244444444444445,
      "grad_norm": 0.09962355345487595,
      "learning_rate": 2.9222222222222224e-05,
      "loss": 0.0017,
      "step": 37400
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.7676711678504944,
      "learning_rate": 2.921666666666667e-05,
      "loss": 0.0027,
      "step": 37410
    },
    {
      "epoch": 3.3262222222222224,
      "grad_norm": 0.33688533306121826,
      "learning_rate": 2.921111111111111e-05,
      "loss": 0.0028,
      "step": 37420
    },
    {
      "epoch": 3.327111111111111,
      "grad_norm": 0.15841932594776154,
      "learning_rate": 2.9205555555555558e-05,
      "loss": 0.0022,
      "step": 37430
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.16984674334526062,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0023,
      "step": 37440
    },
    {
      "epoch": 3.328888888888889,
      "grad_norm": 0.5061905384063721,
      "learning_rate": 2.9194444444444445e-05,
      "loss": 0.002,
      "step": 37450
    },
    {
      "epoch": 3.3297777777777777,
      "grad_norm": 0.9779623746871948,
      "learning_rate": 2.918888888888889e-05,
      "loss": 0.0014,
      "step": 37460
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.36089783906936646,
      "learning_rate": 2.9183333333333336e-05,
      "loss": 0.0014,
      "step": 37470
    },
    {
      "epoch": 3.3315555555555556,
      "grad_norm": 0.8474085927009583,
      "learning_rate": 2.9177777777777783e-05,
      "loss": 0.0024,
      "step": 37480
    },
    {
      "epoch": 3.3324444444444445,
      "grad_norm": 0.7352766990661621,
      "learning_rate": 2.9172222222222223e-05,
      "loss": 0.0021,
      "step": 37490
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.08046390116214752,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0032,
      "step": 37500
    },
    {
      "epoch": 3.3342222222222224,
      "grad_norm": 0.1937650442123413,
      "learning_rate": 2.916111111111111e-05,
      "loss": 0.0021,
      "step": 37510
    },
    {
      "epoch": 3.335111111111111,
      "grad_norm": 0.12960782647132874,
      "learning_rate": 2.9155555555555557e-05,
      "loss": 0.0016,
      "step": 37520
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.42149412631988525,
      "learning_rate": 2.915e-05,
      "loss": 0.0017,
      "step": 37530
    },
    {
      "epoch": 3.336888888888889,
      "grad_norm": 0.4599592685699463,
      "learning_rate": 2.9144444444444447e-05,
      "loss": 0.0024,
      "step": 37540
    },
    {
      "epoch": 3.3377777777777777,
      "grad_norm": 0.7334596514701843,
      "learning_rate": 2.9138888888888888e-05,
      "loss": 0.0017,
      "step": 37550
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.788754403591156,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0018,
      "step": 37560
    },
    {
      "epoch": 3.3395555555555556,
      "grad_norm": 0.22872740030288696,
      "learning_rate": 2.912777777777778e-05,
      "loss": 0.0016,
      "step": 37570
    },
    {
      "epoch": 3.3404444444444445,
      "grad_norm": 0.2602057456970215,
      "learning_rate": 2.912222222222222e-05,
      "loss": 0.003,
      "step": 37580
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.15958788990974426,
      "learning_rate": 2.911666666666667e-05,
      "loss": 0.0021,
      "step": 37590
    },
    {
      "epoch": 3.3422222222222224,
      "grad_norm": 0.12757210433483124,
      "learning_rate": 2.9111111111111112e-05,
      "loss": 0.0026,
      "step": 37600
    },
    {
      "epoch": 3.343111111111111,
      "grad_norm": 0.40397748351097107,
      "learning_rate": 2.910555555555556e-05,
      "loss": 0.0021,
      "step": 37610
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.6246262192726135,
      "learning_rate": 2.91e-05,
      "loss": 0.0031,
      "step": 37620
    },
    {
      "epoch": 3.344888888888889,
      "grad_norm": 0.5831766724586487,
      "learning_rate": 2.9094444444444446e-05,
      "loss": 0.0013,
      "step": 37630
    },
    {
      "epoch": 3.3457777777777777,
      "grad_norm": 0.16267123818397522,
      "learning_rate": 2.9088888888888886e-05,
      "loss": 0.0018,
      "step": 37640
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.3133065402507782,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.0018,
      "step": 37650
    },
    {
      "epoch": 3.3475555555555556,
      "grad_norm": 1.0229815244674683,
      "learning_rate": 2.907777777777778e-05,
      "loss": 0.0017,
      "step": 37660
    },
    {
      "epoch": 3.3484444444444446,
      "grad_norm": 0.02631966583430767,
      "learning_rate": 2.9072222222222224e-05,
      "loss": 0.0017,
      "step": 37670
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.30110058188438416,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0025,
      "step": 37680
    },
    {
      "epoch": 3.3502222222222224,
      "grad_norm": 0.22734853625297546,
      "learning_rate": 2.906111111111111e-05,
      "loss": 0.0013,
      "step": 37690
    },
    {
      "epoch": 3.351111111111111,
      "grad_norm": 0.6372784376144409,
      "learning_rate": 2.9055555555555558e-05,
      "loss": 0.0024,
      "step": 37700
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.5266140103340149,
      "learning_rate": 2.9049999999999998e-05,
      "loss": 0.0021,
      "step": 37710
    },
    {
      "epoch": 3.352888888888889,
      "grad_norm": 0.4793817102909088,
      "learning_rate": 2.9044444444444445e-05,
      "loss": 0.002,
      "step": 37720
    },
    {
      "epoch": 3.3537777777777777,
      "grad_norm": 0.708746075630188,
      "learning_rate": 2.903888888888889e-05,
      "loss": 0.0016,
      "step": 37730
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.7601487040519714,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0021,
      "step": 37740
    },
    {
      "epoch": 3.3555555555555556,
      "grad_norm": 0.9039430022239685,
      "learning_rate": 2.9027777777777782e-05,
      "loss": 0.0041,
      "step": 37750
    },
    {
      "epoch": 3.3564444444444446,
      "grad_norm": 0.48214250802993774,
      "learning_rate": 2.9022222222222223e-05,
      "loss": 0.0021,
      "step": 37760
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.36915123462677,
      "learning_rate": 2.901666666666667e-05,
      "loss": 0.0023,
      "step": 37770
    },
    {
      "epoch": 3.3582222222222224,
      "grad_norm": 0.26995474100112915,
      "learning_rate": 2.901111111111111e-05,
      "loss": 0.0017,
      "step": 37780
    },
    {
      "epoch": 3.359111111111111,
      "grad_norm": 0.6256856322288513,
      "learning_rate": 2.9005555555555557e-05,
      "loss": 0.003,
      "step": 37790
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.690690279006958,
      "learning_rate": 2.9e-05,
      "loss": 0.0018,
      "step": 37800
    },
    {
      "epoch": 3.360888888888889,
      "grad_norm": 0.36950910091400146,
      "learning_rate": 2.8994444444444447e-05,
      "loss": 0.0022,
      "step": 37810
    },
    {
      "epoch": 3.3617777777777778,
      "grad_norm": 0.31674501299858093,
      "learning_rate": 2.8988888888888887e-05,
      "loss": 0.002,
      "step": 37820
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.3714156448841095,
      "learning_rate": 2.8983333333333334e-05,
      "loss": 0.0034,
      "step": 37830
    },
    {
      "epoch": 3.3635555555555556,
      "grad_norm": 0.24942126870155334,
      "learning_rate": 2.897777777777778e-05,
      "loss": 0.0029,
      "step": 37840
    },
    {
      "epoch": 3.3644444444444446,
      "grad_norm": 0.27201902866363525,
      "learning_rate": 2.897222222222222e-05,
      "loss": 0.0017,
      "step": 37850
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.1633719652891159,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0016,
      "step": 37860
    },
    {
      "epoch": 3.3662222222222224,
      "grad_norm": 0.0956500694155693,
      "learning_rate": 2.8961111111111112e-05,
      "loss": 0.0015,
      "step": 37870
    },
    {
      "epoch": 3.367111111111111,
      "grad_norm": 0.7587730288505554,
      "learning_rate": 2.895555555555556e-05,
      "loss": 0.0021,
      "step": 37880
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.2059464156627655,
      "learning_rate": 2.895e-05,
      "loss": 0.0018,
      "step": 37890
    },
    {
      "epoch": 3.368888888888889,
      "grad_norm": 0.1268523782491684,
      "learning_rate": 2.8944444444444446e-05,
      "loss": 0.0018,
      "step": 37900
    },
    {
      "epoch": 3.3697777777777778,
      "grad_norm": 0.6452779173851013,
      "learning_rate": 2.8938888888888886e-05,
      "loss": 0.0021,
      "step": 37910
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.8382291197776794,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0028,
      "step": 37920
    },
    {
      "epoch": 3.3715555555555556,
      "grad_norm": 0.6928783059120178,
      "learning_rate": 2.892777777777778e-05,
      "loss": 0.0022,
      "step": 37930
    },
    {
      "epoch": 3.3724444444444446,
      "grad_norm": 0.40964868664741516,
      "learning_rate": 2.8922222222222224e-05,
      "loss": 0.0017,
      "step": 37940
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.09603458642959595,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.0018,
      "step": 37950
    },
    {
      "epoch": 3.3742222222222225,
      "grad_norm": 0.5318325757980347,
      "learning_rate": 2.891111111111111e-05,
      "loss": 0.0033,
      "step": 37960
    },
    {
      "epoch": 3.375111111111111,
      "grad_norm": 0.2291913777589798,
      "learning_rate": 2.8905555555555558e-05,
      "loss": 0.0019,
      "step": 37970
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.37237924337387085,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0025,
      "step": 37980
    },
    {
      "epoch": 3.376888888888889,
      "grad_norm": 0.1587648093700409,
      "learning_rate": 2.8894444444444445e-05,
      "loss": 0.0025,
      "step": 37990
    },
    {
      "epoch": 3.3777777777777778,
      "grad_norm": 0.13723061978816986,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 0.002,
      "step": 38000
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 1.247988224029541,
      "learning_rate": 2.8883333333333335e-05,
      "loss": 0.0029,
      "step": 38010
    },
    {
      "epoch": 3.3795555555555556,
      "grad_norm": 0.13749876618385315,
      "learning_rate": 2.8877777777777782e-05,
      "loss": 0.0027,
      "step": 38020
    },
    {
      "epoch": 3.3804444444444446,
      "grad_norm": 0.4408155381679535,
      "learning_rate": 2.8872222222222222e-05,
      "loss": 0.0023,
      "step": 38030
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.9341427087783813,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0018,
      "step": 38040
    },
    {
      "epoch": 3.3822222222222225,
      "grad_norm": 0.24615035951137543,
      "learning_rate": 2.886111111111111e-05,
      "loss": 0.002,
      "step": 38050
    },
    {
      "epoch": 3.383111111111111,
      "grad_norm": 0.6141672134399414,
      "learning_rate": 2.8855555555555556e-05,
      "loss": 0.0017,
      "step": 38060
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.1320822685956955,
      "learning_rate": 2.885e-05,
      "loss": 0.0018,
      "step": 38070
    },
    {
      "epoch": 3.384888888888889,
      "grad_norm": 0.2966903746128082,
      "learning_rate": 2.8844444444444447e-05,
      "loss": 0.0025,
      "step": 38080
    },
    {
      "epoch": 3.3857777777777778,
      "grad_norm": 0.5164536833763123,
      "learning_rate": 2.8838888888888887e-05,
      "loss": 0.002,
      "step": 38090
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.352539598941803,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0021,
      "step": 38100
    },
    {
      "epoch": 3.3875555555555557,
      "grad_norm": 0.2642826735973358,
      "learning_rate": 2.882777777777778e-05,
      "loss": 0.0016,
      "step": 38110
    },
    {
      "epoch": 3.3884444444444446,
      "grad_norm": 0.6182573437690735,
      "learning_rate": 2.882222222222222e-05,
      "loss": 0.002,
      "step": 38120
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 1.006237506866455,
      "learning_rate": 2.8816666666666668e-05,
      "loss": 0.0035,
      "step": 38130
    },
    {
      "epoch": 3.3902222222222225,
      "grad_norm": 0.4330216944217682,
      "learning_rate": 2.881111111111111e-05,
      "loss": 0.0016,
      "step": 38140
    },
    {
      "epoch": 3.391111111111111,
      "grad_norm": 0.6546164155006409,
      "learning_rate": 2.880555555555556e-05,
      "loss": 0.0015,
      "step": 38150
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.2781796157360077,
      "learning_rate": 2.88e-05,
      "loss": 0.0016,
      "step": 38160
    },
    {
      "epoch": 3.392888888888889,
      "grad_norm": 0.19148682057857513,
      "learning_rate": 2.8794444444444446e-05,
      "loss": 0.003,
      "step": 38170
    },
    {
      "epoch": 3.393777777777778,
      "grad_norm": 0.11296475678682327,
      "learning_rate": 2.8788888888888893e-05,
      "loss": 0.0016,
      "step": 38180
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.07931722700595856,
      "learning_rate": 2.8783333333333333e-05,
      "loss": 0.0028,
      "step": 38190
    },
    {
      "epoch": 3.3955555555555557,
      "grad_norm": 0.6990979313850403,
      "learning_rate": 2.877777777777778e-05,
      "loss": 0.0017,
      "step": 38200
    },
    {
      "epoch": 3.3964444444444446,
      "grad_norm": 0.1875336766242981,
      "learning_rate": 2.8772222222222223e-05,
      "loss": 0.0026,
      "step": 38210
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.15894918143749237,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.003,
      "step": 38220
    },
    {
      "epoch": 3.398222222222222,
      "grad_norm": 0.07006849348545074,
      "learning_rate": 2.876111111111111e-05,
      "loss": 0.0017,
      "step": 38230
    },
    {
      "epoch": 3.399111111111111,
      "grad_norm": 0.19832180440425873,
      "learning_rate": 2.8755555555555557e-05,
      "loss": 0.003,
      "step": 38240
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.32822898030281067,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.0028,
      "step": 38250
    },
    {
      "epoch": 3.400888888888889,
      "grad_norm": 0.23734474182128906,
      "learning_rate": 2.8744444444444444e-05,
      "loss": 0.0022,
      "step": 38260
    },
    {
      "epoch": 3.401777777777778,
      "grad_norm": 0.8462339639663696,
      "learning_rate": 2.873888888888889e-05,
      "loss": 0.0021,
      "step": 38270
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 1.018428921699524,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0015,
      "step": 38280
    },
    {
      "epoch": 3.4035555555555557,
      "grad_norm": 0.34995701909065247,
      "learning_rate": 2.8727777777777782e-05,
      "loss": 0.002,
      "step": 38290
    },
    {
      "epoch": 3.4044444444444446,
      "grad_norm": 0.3078268766403198,
      "learning_rate": 2.8722222222222222e-05,
      "loss": 0.003,
      "step": 38300
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.3762001097202301,
      "learning_rate": 2.871666666666667e-05,
      "loss": 0.0025,
      "step": 38310
    },
    {
      "epoch": 3.406222222222222,
      "grad_norm": 0.3215060234069824,
      "learning_rate": 2.8711111111111113e-05,
      "loss": 0.0021,
      "step": 38320
    },
    {
      "epoch": 3.407111111111111,
      "grad_norm": 0.4052478075027466,
      "learning_rate": 2.8705555555555556e-05,
      "loss": 0.0017,
      "step": 38330
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.15874134004116058,
      "learning_rate": 2.87e-05,
      "loss": 0.0018,
      "step": 38340
    },
    {
      "epoch": 3.408888888888889,
      "grad_norm": 0.20142784714698792,
      "learning_rate": 2.8694444444444447e-05,
      "loss": 0.0018,
      "step": 38350
    },
    {
      "epoch": 3.409777777777778,
      "grad_norm": 0.05130495876073837,
      "learning_rate": 2.8688888888888894e-05,
      "loss": 0.0031,
      "step": 38360
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.33865082263946533,
      "learning_rate": 2.8683333333333334e-05,
      "loss": 0.0027,
      "step": 38370
    },
    {
      "epoch": 3.4115555555555557,
      "grad_norm": 0.8097131848335266,
      "learning_rate": 2.867777777777778e-05,
      "loss": 0.0022,
      "step": 38380
    },
    {
      "epoch": 3.4124444444444446,
      "grad_norm": 1.1236742734909058,
      "learning_rate": 2.8672222222222224e-05,
      "loss": 0.0018,
      "step": 38390
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.58595210313797,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0017,
      "step": 38400
    },
    {
      "epoch": 3.414222222222222,
      "grad_norm": 0.38647064566612244,
      "learning_rate": 2.866111111111111e-05,
      "loss": 0.003,
      "step": 38410
    },
    {
      "epoch": 3.415111111111111,
      "grad_norm": 0.29148244857788086,
      "learning_rate": 2.8655555555555558e-05,
      "loss": 0.0016,
      "step": 38420
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.3015756905078888,
      "learning_rate": 2.865e-05,
      "loss": 0.0023,
      "step": 38430
    },
    {
      "epoch": 3.416888888888889,
      "grad_norm": 0.05035531893372536,
      "learning_rate": 2.8644444444444445e-05,
      "loss": 0.0022,
      "step": 38440
    },
    {
      "epoch": 3.417777777777778,
      "grad_norm": 0.07676773518323898,
      "learning_rate": 2.8638888888888892e-05,
      "loss": 0.0021,
      "step": 38450
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.16878175735473633,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.0015,
      "step": 38460
    },
    {
      "epoch": 3.4195555555555557,
      "grad_norm": 0.5820143818855286,
      "learning_rate": 2.8627777777777783e-05,
      "loss": 0.0037,
      "step": 38470
    },
    {
      "epoch": 3.4204444444444446,
      "grad_norm": 0.6097536683082581,
      "learning_rate": 2.8622222222222223e-05,
      "loss": 0.0015,
      "step": 38480
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.38525673747062683,
      "learning_rate": 2.861666666666667e-05,
      "loss": 0.0022,
      "step": 38490
    },
    {
      "epoch": 3.422222222222222,
      "grad_norm": 0.30632293224334717,
      "learning_rate": 2.861111111111111e-05,
      "loss": 0.0024,
      "step": 38500
    },
    {
      "epoch": 3.423111111111111,
      "grad_norm": 1.1674872636795044,
      "learning_rate": 2.8605555555555557e-05,
      "loss": 0.0035,
      "step": 38510
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.09341300278902054,
      "learning_rate": 2.86e-05,
      "loss": 0.0026,
      "step": 38520
    },
    {
      "epoch": 3.424888888888889,
      "grad_norm": 0.7801787853240967,
      "learning_rate": 2.8594444444444448e-05,
      "loss": 0.0029,
      "step": 38530
    },
    {
      "epoch": 3.425777777777778,
      "grad_norm": 0.8864457011222839,
      "learning_rate": 2.8588888888888895e-05,
      "loss": 0.0025,
      "step": 38540
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.4703039526939392,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.0027,
      "step": 38550
    },
    {
      "epoch": 3.4275555555555557,
      "grad_norm": 0.6127144694328308,
      "learning_rate": 2.857777777777778e-05,
      "loss": 0.0027,
      "step": 38560
    },
    {
      "epoch": 3.4284444444444446,
      "grad_norm": 0.7887618541717529,
      "learning_rate": 2.8572222222222222e-05,
      "loss": 0.0023,
      "step": 38570
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.35044801235198975,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0018,
      "step": 38580
    },
    {
      "epoch": 3.430222222222222,
      "grad_norm": 0.05041339620947838,
      "learning_rate": 2.8561111111111112e-05,
      "loss": 0.0015,
      "step": 38590
    },
    {
      "epoch": 3.431111111111111,
      "grad_norm": 0.04549427703022957,
      "learning_rate": 2.855555555555556e-05,
      "loss": 0.0024,
      "step": 38600
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.5530352592468262,
      "learning_rate": 2.855e-05,
      "loss": 0.0021,
      "step": 38610
    },
    {
      "epoch": 3.432888888888889,
      "grad_norm": 0.2884548008441925,
      "learning_rate": 2.8544444444444446e-05,
      "loss": 0.0023,
      "step": 38620
    },
    {
      "epoch": 3.433777777777778,
      "grad_norm": 0.22944822907447815,
      "learning_rate": 2.8538888888888893e-05,
      "loss": 0.0016,
      "step": 38630
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.233673095703125,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0022,
      "step": 38640
    },
    {
      "epoch": 3.4355555555555557,
      "grad_norm": 0.15538448095321655,
      "learning_rate": 2.852777777777778e-05,
      "loss": 0.0031,
      "step": 38650
    },
    {
      "epoch": 3.4364444444444446,
      "grad_norm": 0.04223364591598511,
      "learning_rate": 2.8522222222222224e-05,
      "loss": 0.0019,
      "step": 38660
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.652190089225769,
      "learning_rate": 2.851666666666667e-05,
      "loss": 0.0025,
      "step": 38670
    },
    {
      "epoch": 3.438222222222222,
      "grad_norm": 0.4152697026729584,
      "learning_rate": 2.851111111111111e-05,
      "loss": 0.0025,
      "step": 38680
    },
    {
      "epoch": 3.439111111111111,
      "grad_norm": 0.24302232265472412,
      "learning_rate": 2.8505555555555558e-05,
      "loss": 0.0017,
      "step": 38690
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.6398460268974304,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0025,
      "step": 38700
    },
    {
      "epoch": 3.440888888888889,
      "grad_norm": 0.7882983684539795,
      "learning_rate": 2.8494444444444445e-05,
      "loss": 0.0032,
      "step": 38710
    },
    {
      "epoch": 3.441777777777778,
      "grad_norm": 0.41292130947113037,
      "learning_rate": 2.8488888888888892e-05,
      "loss": 0.002,
      "step": 38720
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.6969169974327087,
      "learning_rate": 2.8483333333333336e-05,
      "loss": 0.0026,
      "step": 38730
    },
    {
      "epoch": 3.4435555555555557,
      "grad_norm": 0.06076311692595482,
      "learning_rate": 2.8477777777777783e-05,
      "loss": 0.0023,
      "step": 38740
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.4147406220436096,
      "learning_rate": 2.8472222222222223e-05,
      "loss": 0.0018,
      "step": 38750
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.20476220548152924,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0025,
      "step": 38760
    },
    {
      "epoch": 3.446222222222222,
      "grad_norm": 1.2630902528762817,
      "learning_rate": 2.846111111111111e-05,
      "loss": 0.0024,
      "step": 38770
    },
    {
      "epoch": 3.447111111111111,
      "grad_norm": 0.4198896586894989,
      "learning_rate": 2.8455555555555557e-05,
      "loss": 0.0018,
      "step": 38780
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.20282435417175293,
      "learning_rate": 2.845e-05,
      "loss": 0.0029,
      "step": 38790
    },
    {
      "epoch": 3.448888888888889,
      "grad_norm": 0.7097021341323853,
      "learning_rate": 2.8444444444444447e-05,
      "loss": 0.0027,
      "step": 38800
    },
    {
      "epoch": 3.449777777777778,
      "grad_norm": 0.45809876918792725,
      "learning_rate": 2.8438888888888894e-05,
      "loss": 0.0027,
      "step": 38810
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.4477294385433197,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.0022,
      "step": 38820
    },
    {
      "epoch": 3.4515555555555557,
      "grad_norm": 0.6755796670913696,
      "learning_rate": 2.842777777777778e-05,
      "loss": 0.0016,
      "step": 38830
    },
    {
      "epoch": 3.4524444444444446,
      "grad_norm": 0.30856117606163025,
      "learning_rate": 2.842222222222222e-05,
      "loss": 0.0029,
      "step": 38840
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.5659247636795044,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.0026,
      "step": 38850
    },
    {
      "epoch": 3.454222222222222,
      "grad_norm": 0.2288902997970581,
      "learning_rate": 2.8411111111111112e-05,
      "loss": 0.0019,
      "step": 38860
    },
    {
      "epoch": 3.455111111111111,
      "grad_norm": 0.5601564049720764,
      "learning_rate": 2.840555555555556e-05,
      "loss": 0.0026,
      "step": 38870
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.24717918038368225,
      "learning_rate": 2.84e-05,
      "loss": 0.0028,
      "step": 38880
    },
    {
      "epoch": 3.456888888888889,
      "grad_norm": 0.11667775362730026,
      "learning_rate": 2.8394444444444446e-05,
      "loss": 0.0026,
      "step": 38890
    },
    {
      "epoch": 3.457777777777778,
      "grad_norm": 0.2859271466732025,
      "learning_rate": 2.8388888888888893e-05,
      "loss": 0.0029,
      "step": 38900
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.3181322515010834,
      "learning_rate": 2.8383333333333333e-05,
      "loss": 0.0031,
      "step": 38910
    },
    {
      "epoch": 3.4595555555555557,
      "grad_norm": 0.1567096710205078,
      "learning_rate": 2.837777777777778e-05,
      "loss": 0.0022,
      "step": 38920
    },
    {
      "epoch": 3.4604444444444447,
      "grad_norm": 0.22496803104877472,
      "learning_rate": 2.8372222222222224e-05,
      "loss": 0.002,
      "step": 38930
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.7780003547668457,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.0015,
      "step": 38940
    },
    {
      "epoch": 3.462222222222222,
      "grad_norm": 0.0540887676179409,
      "learning_rate": 2.836111111111111e-05,
      "loss": 0.0022,
      "step": 38950
    },
    {
      "epoch": 3.463111111111111,
      "grad_norm": 0.2916813790798187,
      "learning_rate": 2.8355555555555558e-05,
      "loss": 0.0021,
      "step": 38960
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.13322113454341888,
      "learning_rate": 2.8349999999999998e-05,
      "loss": 0.002,
      "step": 38970
    },
    {
      "epoch": 3.464888888888889,
      "grad_norm": 0.19067083299160004,
      "learning_rate": 2.8344444444444445e-05,
      "loss": 0.0025,
      "step": 38980
    },
    {
      "epoch": 3.465777777777778,
      "grad_norm": 0.12698547542095184,
      "learning_rate": 2.8338888888888892e-05,
      "loss": 0.0021,
      "step": 38990
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.6142212748527527,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0024,
      "step": 39000
    },
    {
      "epoch": 3.4675555555555557,
      "grad_norm": 0.19504119455814362,
      "learning_rate": 2.8327777777777782e-05,
      "loss": 0.0019,
      "step": 39010
    },
    {
      "epoch": 3.4684444444444447,
      "grad_norm": 0.14580145478248596,
      "learning_rate": 2.8322222222222222e-05,
      "loss": 0.0026,
      "step": 39020
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.12726975977420807,
      "learning_rate": 2.831666666666667e-05,
      "loss": 0.0015,
      "step": 39030
    },
    {
      "epoch": 3.470222222222222,
      "grad_norm": 0.287888765335083,
      "learning_rate": 2.831111111111111e-05,
      "loss": 0.0022,
      "step": 39040
    },
    {
      "epoch": 3.471111111111111,
      "grad_norm": 0.6109957098960876,
      "learning_rate": 2.8305555555555557e-05,
      "loss": 0.0026,
      "step": 39050
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.50864577293396,
      "learning_rate": 2.83e-05,
      "loss": 0.0019,
      "step": 39060
    },
    {
      "epoch": 3.472888888888889,
      "grad_norm": 0.5716025829315186,
      "learning_rate": 2.8294444444444447e-05,
      "loss": 0.0025,
      "step": 39070
    },
    {
      "epoch": 3.473777777777778,
      "grad_norm": 1.1150026321411133,
      "learning_rate": 2.8288888888888894e-05,
      "loss": 0.0017,
      "step": 39080
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.607155442237854,
      "learning_rate": 2.8283333333333334e-05,
      "loss": 0.0017,
      "step": 39090
    },
    {
      "epoch": 3.4755555555555557,
      "grad_norm": 0.5544610023498535,
      "learning_rate": 2.827777777777778e-05,
      "loss": 0.0031,
      "step": 39100
    },
    {
      "epoch": 3.4764444444444447,
      "grad_norm": 0.4654025137424469,
      "learning_rate": 2.827222222222222e-05,
      "loss": 0.0023,
      "step": 39110
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.1995234489440918,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0019,
      "step": 39120
    },
    {
      "epoch": 3.478222222222222,
      "grad_norm": 0.9137670397758484,
      "learning_rate": 2.8261111111111112e-05,
      "loss": 0.0025,
      "step": 39130
    },
    {
      "epoch": 3.479111111111111,
      "grad_norm": 1.0075173377990723,
      "learning_rate": 2.825555555555556e-05,
      "loss": 0.0024,
      "step": 39140
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.15887773036956787,
      "learning_rate": 2.825e-05,
      "loss": 0.0031,
      "step": 39150
    },
    {
      "epoch": 3.480888888888889,
      "grad_norm": 0.9067733883857727,
      "learning_rate": 2.8244444444444446e-05,
      "loss": 0.0021,
      "step": 39160
    },
    {
      "epoch": 3.481777777777778,
      "grad_norm": 0.21827332675457,
      "learning_rate": 2.8238888888888893e-05,
      "loss": 0.0022,
      "step": 39170
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.2062937468290329,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0021,
      "step": 39180
    },
    {
      "epoch": 3.4835555555555557,
      "grad_norm": 0.1958369016647339,
      "learning_rate": 2.822777777777778e-05,
      "loss": 0.0024,
      "step": 39190
    },
    {
      "epoch": 3.4844444444444447,
      "grad_norm": 0.3167618215084076,
      "learning_rate": 2.8222222222222223e-05,
      "loss": 0.0034,
      "step": 39200
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.6795703768730164,
      "learning_rate": 2.821666666666667e-05,
      "loss": 0.0019,
      "step": 39210
    },
    {
      "epoch": 3.486222222222222,
      "grad_norm": 0.4517667591571808,
      "learning_rate": 2.821111111111111e-05,
      "loss": 0.0019,
      "step": 39220
    },
    {
      "epoch": 3.487111111111111,
      "grad_norm": 0.14562709629535675,
      "learning_rate": 2.8205555555555557e-05,
      "loss": 0.0014,
      "step": 39230
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.5720118880271912,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0028,
      "step": 39240
    },
    {
      "epoch": 3.488888888888889,
      "grad_norm": 0.1459445208311081,
      "learning_rate": 2.8194444444444445e-05,
      "loss": 0.0027,
      "step": 39250
    },
    {
      "epoch": 3.489777777777778,
      "grad_norm": 0.5087730288505554,
      "learning_rate": 2.818888888888889e-05,
      "loss": 0.0019,
      "step": 39260
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.7667133808135986,
      "learning_rate": 2.8183333333333335e-05,
      "loss": 0.0018,
      "step": 39270
    },
    {
      "epoch": 3.4915555555555557,
      "grad_norm": 0.8743724226951599,
      "learning_rate": 2.8177777777777782e-05,
      "loss": 0.0014,
      "step": 39280
    },
    {
      "epoch": 3.4924444444444447,
      "grad_norm": 0.2736111283302307,
      "learning_rate": 2.8172222222222222e-05,
      "loss": 0.0033,
      "step": 39290
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.996740460395813,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.0019,
      "step": 39300
    },
    {
      "epoch": 3.494222222222222,
      "grad_norm": 0.8320268988609314,
      "learning_rate": 2.816111111111111e-05,
      "loss": 0.0022,
      "step": 39310
    },
    {
      "epoch": 3.495111111111111,
      "grad_norm": 0.16928532719612122,
      "learning_rate": 2.8155555555555556e-05,
      "loss": 0.0034,
      "step": 39320
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.27654674649238586,
      "learning_rate": 2.815e-05,
      "loss": 0.0022,
      "step": 39330
    },
    {
      "epoch": 3.496888888888889,
      "grad_norm": 0.4320915639400482,
      "learning_rate": 2.8144444444444447e-05,
      "loss": 0.0027,
      "step": 39340
    },
    {
      "epoch": 3.497777777777778,
      "grad_norm": 0.12481897324323654,
      "learning_rate": 2.8138888888888894e-05,
      "loss": 0.0024,
      "step": 39350
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.43589258193969727,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0026,
      "step": 39360
    },
    {
      "epoch": 3.4995555555555553,
      "grad_norm": 0.37615397572517395,
      "learning_rate": 2.812777777777778e-05,
      "loss": 0.0022,
      "step": 39370
    },
    {
      "epoch": 3.5004444444444447,
      "grad_norm": 0.6189135909080505,
      "learning_rate": 2.812222222222222e-05,
      "loss": 0.0025,
      "step": 39380
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.0877109244465828,
      "learning_rate": 2.8116666666666668e-05,
      "loss": 0.0024,
      "step": 39390
    },
    {
      "epoch": 3.502222222222222,
      "grad_norm": 0.2847958505153656,
      "learning_rate": 2.811111111111111e-05,
      "loss": 0.0024,
      "step": 39400
    },
    {
      "epoch": 3.503111111111111,
      "grad_norm": 0.8203290700912476,
      "learning_rate": 2.810555555555556e-05,
      "loss": 0.0021,
      "step": 39410
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.4121186435222626,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.0015,
      "step": 39420
    },
    {
      "epoch": 3.504888888888889,
      "grad_norm": 0.11062037199735641,
      "learning_rate": 2.8094444444444446e-05,
      "loss": 0.0023,
      "step": 39430
    },
    {
      "epoch": 3.505777777777778,
      "grad_norm": 0.6664412617683411,
      "learning_rate": 2.8088888888888893e-05,
      "loss": 0.0025,
      "step": 39440
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.34540024399757385,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.0029,
      "step": 39450
    },
    {
      "epoch": 3.5075555555555553,
      "grad_norm": 0.6769710779190063,
      "learning_rate": 2.807777777777778e-05,
      "loss": 0.0026,
      "step": 39460
    },
    {
      "epoch": 3.5084444444444447,
      "grad_norm": 0.7162660956382751,
      "learning_rate": 2.8072222222222223e-05,
      "loss": 0.0023,
      "step": 39470
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.12162229418754578,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0019,
      "step": 39480
    },
    {
      "epoch": 3.510222222222222,
      "grad_norm": 0.461684912443161,
      "learning_rate": 2.806111111111111e-05,
      "loss": 0.0024,
      "step": 39490
    },
    {
      "epoch": 3.511111111111111,
      "grad_norm": 0.22337394952774048,
      "learning_rate": 2.8055555555555557e-05,
      "loss": 0.0029,
      "step": 39500
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.4260781705379486,
      "learning_rate": 2.8050000000000004e-05,
      "loss": 0.0031,
      "step": 39510
    },
    {
      "epoch": 3.512888888888889,
      "grad_norm": 0.09156684577465057,
      "learning_rate": 2.8044444444444444e-05,
      "loss": 0.002,
      "step": 39520
    },
    {
      "epoch": 3.513777777777778,
      "grad_norm": 0.8403440117835999,
      "learning_rate": 2.803888888888889e-05,
      "loss": 0.0018,
      "step": 39530
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.23856593668460846,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0024,
      "step": 39540
    },
    {
      "epoch": 3.5155555555555553,
      "grad_norm": 0.1469457894563675,
      "learning_rate": 2.8027777777777782e-05,
      "loss": 0.0017,
      "step": 39550
    },
    {
      "epoch": 3.5164444444444447,
      "grad_norm": 0.09937810897827148,
      "learning_rate": 2.8022222222222222e-05,
      "loss": 0.0017,
      "step": 39560
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.523557722568512,
      "learning_rate": 2.801666666666667e-05,
      "loss": 0.0029,
      "step": 39570
    },
    {
      "epoch": 3.518222222222222,
      "grad_norm": 0.27390503883361816,
      "learning_rate": 2.801111111111111e-05,
      "loss": 0.002,
      "step": 39580
    },
    {
      "epoch": 3.519111111111111,
      "grad_norm": 0.7958449125289917,
      "learning_rate": 2.8005555555555556e-05,
      "loss": 0.0015,
      "step": 39590
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.46429145336151123,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0028,
      "step": 39600
    },
    {
      "epoch": 3.520888888888889,
      "grad_norm": 0.15720701217651367,
      "learning_rate": 2.7994444444444447e-05,
      "loss": 0.0023,
      "step": 39610
    },
    {
      "epoch": 3.521777777777778,
      "grad_norm": 0.8100720047950745,
      "learning_rate": 2.7988888888888893e-05,
      "loss": 0.0019,
      "step": 39620
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.06293840706348419,
      "learning_rate": 2.7983333333333334e-05,
      "loss": 0.0018,
      "step": 39630
    },
    {
      "epoch": 3.5235555555555553,
      "grad_norm": 0.5126376748085022,
      "learning_rate": 2.797777777777778e-05,
      "loss": 0.002,
      "step": 39640
    },
    {
      "epoch": 3.5244444444444447,
      "grad_norm": 0.2725611627101898,
      "learning_rate": 2.797222222222222e-05,
      "loss": 0.0023,
      "step": 39650
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.4172731041908264,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.002,
      "step": 39660
    },
    {
      "epoch": 3.526222222222222,
      "grad_norm": 0.1604176163673401,
      "learning_rate": 2.796111111111111e-05,
      "loss": 0.0021,
      "step": 39670
    },
    {
      "epoch": 3.527111111111111,
      "grad_norm": 0.7031282186508179,
      "learning_rate": 2.7955555555555558e-05,
      "loss": 0.0027,
      "step": 39680
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.47285303473472595,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 0.0019,
      "step": 39690
    },
    {
      "epoch": 3.528888888888889,
      "grad_norm": 0.389870822429657,
      "learning_rate": 2.7944444444444445e-05,
      "loss": 0.0022,
      "step": 39700
    },
    {
      "epoch": 3.529777777777778,
      "grad_norm": 0.9196484684944153,
      "learning_rate": 2.7938888888888892e-05,
      "loss": 0.002,
      "step": 39710
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.5433636903762817,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0019,
      "step": 39720
    },
    {
      "epoch": 3.5315555555555553,
      "grad_norm": 0.6799933910369873,
      "learning_rate": 2.792777777777778e-05,
      "loss": 0.0014,
      "step": 39730
    },
    {
      "epoch": 3.5324444444444447,
      "grad_norm": 0.11999618262052536,
      "learning_rate": 2.7922222222222223e-05,
      "loss": 0.0028,
      "step": 39740
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.6566646099090576,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.0017,
      "step": 39750
    },
    {
      "epoch": 3.534222222222222,
      "grad_norm": 0.05328899621963501,
      "learning_rate": 2.791111111111111e-05,
      "loss": 0.0027,
      "step": 39760
    },
    {
      "epoch": 3.535111111111111,
      "grad_norm": 0.39682093262672424,
      "learning_rate": 2.7905555555555557e-05,
      "loss": 0.0026,
      "step": 39770
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.4271785020828247,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0016,
      "step": 39780
    },
    {
      "epoch": 3.536888888888889,
      "grad_norm": 0.05791015923023224,
      "learning_rate": 2.7894444444444444e-05,
      "loss": 0.0027,
      "step": 39790
    },
    {
      "epoch": 3.537777777777778,
      "grad_norm": 0.22623877227306366,
      "learning_rate": 2.788888888888889e-05,
      "loss": 0.0022,
      "step": 39800
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.27415111660957336,
      "learning_rate": 2.7883333333333335e-05,
      "loss": 0.0022,
      "step": 39810
    },
    {
      "epoch": 3.5395555555555553,
      "grad_norm": 0.22127915918827057,
      "learning_rate": 2.787777777777778e-05,
      "loss": 0.002,
      "step": 39820
    },
    {
      "epoch": 3.5404444444444443,
      "grad_norm": 0.4805825352668762,
      "learning_rate": 2.7872222222222222e-05,
      "loss": 0.0019,
      "step": 39830
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.3953213691711426,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0018,
      "step": 39840
    },
    {
      "epoch": 3.542222222222222,
      "grad_norm": 0.19334770739078522,
      "learning_rate": 2.786111111111111e-05,
      "loss": 0.0016,
      "step": 39850
    },
    {
      "epoch": 3.543111111111111,
      "grad_norm": 0.7041588425636292,
      "learning_rate": 2.7855555555555556e-05,
      "loss": 0.0025,
      "step": 39860
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.7289652824401855,
      "learning_rate": 2.7850000000000003e-05,
      "loss": 0.0021,
      "step": 39870
    },
    {
      "epoch": 3.544888888888889,
      "grad_norm": 0.5726199150085449,
      "learning_rate": 2.7844444444444446e-05,
      "loss": 0.0027,
      "step": 39880
    },
    {
      "epoch": 3.545777777777778,
      "grad_norm": 0.2723049521446228,
      "learning_rate": 2.7838888888888893e-05,
      "loss": 0.0014,
      "step": 39890
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.41404789686203003,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0015,
      "step": 39900
    },
    {
      "epoch": 3.5475555555555554,
      "grad_norm": 0.1557118445634842,
      "learning_rate": 2.782777777777778e-05,
      "loss": 0.0022,
      "step": 39910
    },
    {
      "epoch": 3.5484444444444443,
      "grad_norm": 0.645211935043335,
      "learning_rate": 2.782222222222222e-05,
      "loss": 0.002,
      "step": 39920
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.10307856649160385,
      "learning_rate": 2.7816666666666667e-05,
      "loss": 0.0017,
      "step": 39930
    },
    {
      "epoch": 3.550222222222222,
      "grad_norm": 0.5312041640281677,
      "learning_rate": 2.781111111111111e-05,
      "loss": 0.0034,
      "step": 39940
    },
    {
      "epoch": 3.551111111111111,
      "grad_norm": 0.7198655009269714,
      "learning_rate": 2.7805555555555558e-05,
      "loss": 0.0033,
      "step": 39950
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.5972058176994324,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.002,
      "step": 39960
    },
    {
      "epoch": 3.552888888888889,
      "grad_norm": 0.1531629115343094,
      "learning_rate": 2.7794444444444445e-05,
      "loss": 0.0021,
      "step": 39970
    },
    {
      "epoch": 3.553777777777778,
      "grad_norm": 0.5682170391082764,
      "learning_rate": 2.7788888888888892e-05,
      "loss": 0.0018,
      "step": 39980
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.19290639460086823,
      "learning_rate": 2.7783333333333332e-05,
      "loss": 0.0016,
      "step": 39990
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.3048347234725952,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.0025,
      "step": 40000
    },
    {
      "epoch": 3.5564444444444443,
      "grad_norm": 0.08479497581720352,
      "learning_rate": 2.7772222222222223e-05,
      "loss": 0.0021,
      "step": 40010
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.2753700613975525,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.0025,
      "step": 40020
    },
    {
      "epoch": 3.558222222222222,
      "grad_norm": 0.2729968726634979,
      "learning_rate": 2.776111111111111e-05,
      "loss": 0.0024,
      "step": 40030
    },
    {
      "epoch": 3.559111111111111,
      "grad_norm": 0.3520858883857727,
      "learning_rate": 2.7755555555555557e-05,
      "loss": 0.0021,
      "step": 40040
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.5018543004989624,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.0019,
      "step": 40050
    },
    {
      "epoch": 3.560888888888889,
      "grad_norm": 0.10522225499153137,
      "learning_rate": 2.7744444444444444e-05,
      "loss": 0.0017,
      "step": 40060
    },
    {
      "epoch": 3.561777777777778,
      "grad_norm": 0.6097683310508728,
      "learning_rate": 2.773888888888889e-05,
      "loss": 0.0017,
      "step": 40070
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.15462300181388855,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0021,
      "step": 40080
    },
    {
      "epoch": 3.5635555555555554,
      "grad_norm": 0.41490960121154785,
      "learning_rate": 2.772777777777778e-05,
      "loss": 0.0017,
      "step": 40090
    },
    {
      "epoch": 3.5644444444444443,
      "grad_norm": 0.2701592743396759,
      "learning_rate": 2.772222222222222e-05,
      "loss": 0.0019,
      "step": 40100
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 1.2016626596450806,
      "learning_rate": 2.771666666666667e-05,
      "loss": 0.0028,
      "step": 40110
    },
    {
      "epoch": 3.566222222222222,
      "grad_norm": 0.43517664074897766,
      "learning_rate": 2.771111111111111e-05,
      "loss": 0.0028,
      "step": 40120
    },
    {
      "epoch": 3.567111111111111,
      "grad_norm": 0.08426371961832047,
      "learning_rate": 2.7705555555555556e-05,
      "loss": 0.0027,
      "step": 40130
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.6100203990936279,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.0025,
      "step": 40140
    },
    {
      "epoch": 3.568888888888889,
      "grad_norm": 0.7723084688186646,
      "learning_rate": 2.7694444444444446e-05,
      "loss": 0.0023,
      "step": 40150
    },
    {
      "epoch": 3.569777777777778,
      "grad_norm": 0.484873503446579,
      "learning_rate": 2.7688888888888893e-05,
      "loss": 0.0024,
      "step": 40160
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 1.0585546493530273,
      "learning_rate": 2.7683333333333333e-05,
      "loss": 0.0031,
      "step": 40170
    },
    {
      "epoch": 3.5715555555555554,
      "grad_norm": 0.5649101734161377,
      "learning_rate": 2.767777777777778e-05,
      "loss": 0.0012,
      "step": 40180
    },
    {
      "epoch": 3.5724444444444443,
      "grad_norm": 0.29088398814201355,
      "learning_rate": 2.767222222222222e-05,
      "loss": 0.0019,
      "step": 40190
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.05240321904420853,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0031,
      "step": 40200
    },
    {
      "epoch": 3.574222222222222,
      "grad_norm": 0.3388753831386566,
      "learning_rate": 2.766111111111111e-05,
      "loss": 0.0029,
      "step": 40210
    },
    {
      "epoch": 3.575111111111111,
      "grad_norm": 0.4529334008693695,
      "learning_rate": 2.7655555555555558e-05,
      "loss": 0.0026,
      "step": 40220
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.6285266280174255,
      "learning_rate": 2.7650000000000005e-05,
      "loss": 0.0017,
      "step": 40230
    },
    {
      "epoch": 3.576888888888889,
      "grad_norm": 0.16035039722919464,
      "learning_rate": 2.7644444444444445e-05,
      "loss": 0.002,
      "step": 40240
    },
    {
      "epoch": 3.5777777777777775,
      "grad_norm": 0.9468935132026672,
      "learning_rate": 2.7638888888888892e-05,
      "loss": 0.002,
      "step": 40250
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.9887710809707642,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0024,
      "step": 40260
    },
    {
      "epoch": 3.5795555555555554,
      "grad_norm": 0.4967651069164276,
      "learning_rate": 2.762777777777778e-05,
      "loss": 0.0027,
      "step": 40270
    },
    {
      "epoch": 3.5804444444444443,
      "grad_norm": 0.18066678941249847,
      "learning_rate": 2.7622222222222222e-05,
      "loss": 0.0025,
      "step": 40280
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.6918247938156128,
      "learning_rate": 2.761666666666667e-05,
      "loss": 0.0018,
      "step": 40290
    },
    {
      "epoch": 3.582222222222222,
      "grad_norm": 0.6506427526473999,
      "learning_rate": 2.761111111111111e-05,
      "loss": 0.0021,
      "step": 40300
    },
    {
      "epoch": 3.583111111111111,
      "grad_norm": 0.0696801021695137,
      "learning_rate": 2.7605555555555556e-05,
      "loss": 0.0025,
      "step": 40310
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.1866939663887024,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0029,
      "step": 40320
    },
    {
      "epoch": 3.584888888888889,
      "grad_norm": 0.46447399258613586,
      "learning_rate": 2.7594444444444444e-05,
      "loss": 0.003,
      "step": 40330
    },
    {
      "epoch": 3.5857777777777775,
      "grad_norm": 0.11976664513349533,
      "learning_rate": 2.758888888888889e-05,
      "loss": 0.0022,
      "step": 40340
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.6449840664863586,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.0027,
      "step": 40350
    },
    {
      "epoch": 3.5875555555555554,
      "grad_norm": 0.7811645865440369,
      "learning_rate": 2.757777777777778e-05,
      "loss": 0.0026,
      "step": 40360
    },
    {
      "epoch": 3.5884444444444443,
      "grad_norm": 0.1699775904417038,
      "learning_rate": 2.757222222222222e-05,
      "loss": 0.0023,
      "step": 40370
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.7714086174964905,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0027,
      "step": 40380
    },
    {
      "epoch": 3.590222222222222,
      "grad_norm": 0.31873056292533875,
      "learning_rate": 2.7561111111111108e-05,
      "loss": 0.002,
      "step": 40390
    },
    {
      "epoch": 3.591111111111111,
      "grad_norm": 0.9786316752433777,
      "learning_rate": 2.7555555555555555e-05,
      "loss": 0.002,
      "step": 40400
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.27300095558166504,
      "learning_rate": 2.7550000000000002e-05,
      "loss": 0.0025,
      "step": 40410
    },
    {
      "epoch": 3.592888888888889,
      "grad_norm": 0.15566430985927582,
      "learning_rate": 2.7544444444444446e-05,
      "loss": 0.0033,
      "step": 40420
    },
    {
      "epoch": 3.5937777777777775,
      "grad_norm": 0.5280154347419739,
      "learning_rate": 2.7538888888888893e-05,
      "loss": 0.0019,
      "step": 40430
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.3621308505535126,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0013,
      "step": 40440
    },
    {
      "epoch": 3.5955555555555554,
      "grad_norm": 0.5464867949485779,
      "learning_rate": 2.752777777777778e-05,
      "loss": 0.0028,
      "step": 40450
    },
    {
      "epoch": 3.5964444444444443,
      "grad_norm": 0.926807165145874,
      "learning_rate": 2.752222222222222e-05,
      "loss": 0.0029,
      "step": 40460
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.27501413226127625,
      "learning_rate": 2.7516666666666667e-05,
      "loss": 0.0013,
      "step": 40470
    },
    {
      "epoch": 3.598222222222222,
      "grad_norm": 0.5685246586799622,
      "learning_rate": 2.751111111111111e-05,
      "loss": 0.002,
      "step": 40480
    },
    {
      "epoch": 3.599111111111111,
      "grad_norm": 0.6998217105865479,
      "learning_rate": 2.7505555555555557e-05,
      "loss": 0.0025,
      "step": 40490
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.188119575381279,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0019,
      "step": 40500
    },
    {
      "epoch": 3.600888888888889,
      "grad_norm": 0.12472403049468994,
      "learning_rate": 2.7494444444444445e-05,
      "loss": 0.002,
      "step": 40510
    },
    {
      "epoch": 3.6017777777777775,
      "grad_norm": 0.24664461612701416,
      "learning_rate": 2.748888888888889e-05,
      "loss": 0.0021,
      "step": 40520
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.4755226969718933,
      "learning_rate": 2.748333333333333e-05,
      "loss": 0.0018,
      "step": 40530
    },
    {
      "epoch": 3.6035555555555554,
      "grad_norm": 0.5781978964805603,
      "learning_rate": 2.747777777777778e-05,
      "loss": 0.0031,
      "step": 40540
    },
    {
      "epoch": 3.6044444444444443,
      "grad_norm": 0.25959286093711853,
      "learning_rate": 2.7472222222222222e-05,
      "loss": 0.0029,
      "step": 40550
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.6341091990470886,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0024,
      "step": 40560
    },
    {
      "epoch": 3.606222222222222,
      "grad_norm": 0.09783662110567093,
      "learning_rate": 2.746111111111111e-05,
      "loss": 0.0016,
      "step": 40570
    },
    {
      "epoch": 3.607111111111111,
      "grad_norm": 0.6121963262557983,
      "learning_rate": 2.7455555555555556e-05,
      "loss": 0.0019,
      "step": 40580
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.24864532053470612,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 0.0014,
      "step": 40590
    },
    {
      "epoch": 3.608888888888889,
      "grad_norm": 0.15221188962459564,
      "learning_rate": 2.7444444444444443e-05,
      "loss": 0.0019,
      "step": 40600
    },
    {
      "epoch": 3.6097777777777775,
      "grad_norm": 0.12579205632209778,
      "learning_rate": 2.743888888888889e-05,
      "loss": 0.0021,
      "step": 40610
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.3515918552875519,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0024,
      "step": 40620
    },
    {
      "epoch": 3.6115555555555554,
      "grad_norm": 0.44132232666015625,
      "learning_rate": 2.742777777777778e-05,
      "loss": 0.0025,
      "step": 40630
    },
    {
      "epoch": 3.6124444444444443,
      "grad_norm": 0.30952849984169006,
      "learning_rate": 2.742222222222222e-05,
      "loss": 0.0026,
      "step": 40640
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.2113489955663681,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.0015,
      "step": 40650
    },
    {
      "epoch": 3.6142222222222222,
      "grad_norm": 0.319559782743454,
      "learning_rate": 2.7411111111111115e-05,
      "loss": 0.0024,
      "step": 40660
    },
    {
      "epoch": 3.615111111111111,
      "grad_norm": 0.30624058842658997,
      "learning_rate": 2.7405555555555555e-05,
      "loss": 0.0017,
      "step": 40670
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.8884785771369934,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0021,
      "step": 40680
    },
    {
      "epoch": 3.616888888888889,
      "grad_norm": 0.9938759207725525,
      "learning_rate": 2.7394444444444445e-05,
      "loss": 0.0018,
      "step": 40690
    },
    {
      "epoch": 3.6177777777777775,
      "grad_norm": 0.8227257132530212,
      "learning_rate": 2.7388888888888892e-05,
      "loss": 0.0026,
      "step": 40700
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.09068629145622253,
      "learning_rate": 2.7383333333333333e-05,
      "loss": 0.0023,
      "step": 40710
    },
    {
      "epoch": 3.6195555555555554,
      "grad_norm": 0.07725053280591965,
      "learning_rate": 2.737777777777778e-05,
      "loss": 0.002,
      "step": 40720
    },
    {
      "epoch": 3.6204444444444444,
      "grad_norm": 0.08133411407470703,
      "learning_rate": 2.737222222222222e-05,
      "loss": 0.002,
      "step": 40730
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.31213700771331787,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0019,
      "step": 40740
    },
    {
      "epoch": 3.6222222222222222,
      "grad_norm": 0.08740817755460739,
      "learning_rate": 2.7361111111111114e-05,
      "loss": 0.0038,
      "step": 40750
    },
    {
      "epoch": 3.623111111111111,
      "grad_norm": 0.3886319100856781,
      "learning_rate": 2.7355555555555557e-05,
      "loss": 0.0021,
      "step": 40760
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.4618518054485321,
      "learning_rate": 2.7350000000000004e-05,
      "loss": 0.0016,
      "step": 40770
    },
    {
      "epoch": 3.624888888888889,
      "grad_norm": 0.3127157986164093,
      "learning_rate": 2.7344444444444444e-05,
      "loss": 0.0025,
      "step": 40780
    },
    {
      "epoch": 3.6257777777777775,
      "grad_norm": 0.5008050799369812,
      "learning_rate": 2.733888888888889e-05,
      "loss": 0.0022,
      "step": 40790
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.501648485660553,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0014,
      "step": 40800
    },
    {
      "epoch": 3.6275555555555554,
      "grad_norm": 0.04418012499809265,
      "learning_rate": 2.732777777777778e-05,
      "loss": 0.0019,
      "step": 40810
    },
    {
      "epoch": 3.6284444444444444,
      "grad_norm": 0.530290961265564,
      "learning_rate": 2.7322222222222222e-05,
      "loss": 0.0024,
      "step": 40820
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.2795858383178711,
      "learning_rate": 2.731666666666667e-05,
      "loss": 0.0025,
      "step": 40830
    },
    {
      "epoch": 3.6302222222222222,
      "grad_norm": 0.3577222228050232,
      "learning_rate": 2.7311111111111116e-05,
      "loss": 0.0018,
      "step": 40840
    },
    {
      "epoch": 3.631111111111111,
      "grad_norm": 0.8937566876411438,
      "learning_rate": 2.7305555555555556e-05,
      "loss": 0.0027,
      "step": 40850
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.7797966599464417,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0019,
      "step": 40860
    },
    {
      "epoch": 3.632888888888889,
      "grad_norm": 0.34177201986312866,
      "learning_rate": 2.7294444444444443e-05,
      "loss": 0.0019,
      "step": 40870
    },
    {
      "epoch": 3.6337777777777776,
      "grad_norm": 0.5774669051170349,
      "learning_rate": 2.728888888888889e-05,
      "loss": 0.0025,
      "step": 40880
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.4946058988571167,
      "learning_rate": 2.7283333333333334e-05,
      "loss": 0.0026,
      "step": 40890
    },
    {
      "epoch": 3.6355555555555554,
      "grad_norm": 0.2723202109336853,
      "learning_rate": 2.727777777777778e-05,
      "loss": 0.0029,
      "step": 40900
    },
    {
      "epoch": 3.6364444444444444,
      "grad_norm": 0.18877308070659637,
      "learning_rate": 2.727222222222222e-05,
      "loss": 0.0021,
      "step": 40910
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.3570457398891449,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.002,
      "step": 40920
    },
    {
      "epoch": 3.6382222222222222,
      "grad_norm": 0.7469529509544373,
      "learning_rate": 2.7261111111111115e-05,
      "loss": 0.002,
      "step": 40930
    },
    {
      "epoch": 3.639111111111111,
      "grad_norm": 0.799496591091156,
      "learning_rate": 2.7255555555555555e-05,
      "loss": 0.0022,
      "step": 40940
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.26579511165618896,
      "learning_rate": 2.725e-05,
      "loss": 0.0021,
      "step": 40950
    },
    {
      "epoch": 3.640888888888889,
      "grad_norm": 0.13410355150699615,
      "learning_rate": 2.7244444444444445e-05,
      "loss": 0.0013,
      "step": 40960
    },
    {
      "epoch": 3.6417777777777776,
      "grad_norm": 0.08563478291034698,
      "learning_rate": 2.7238888888888892e-05,
      "loss": 0.002,
      "step": 40970
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.5930461287498474,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0021,
      "step": 40980
    },
    {
      "epoch": 3.6435555555555554,
      "grad_norm": 0.7093754410743713,
      "learning_rate": 2.722777777777778e-05,
      "loss": 0.0026,
      "step": 40990
    },
    {
      "epoch": 3.6444444444444444,
      "grad_norm": 0.3841288685798645,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 0.0016,
      "step": 41000
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.26589131355285645,
      "learning_rate": 2.7216666666666666e-05,
      "loss": 0.0024,
      "step": 41010
    },
    {
      "epoch": 3.6462222222222223,
      "grad_norm": 0.04753955081105232,
      "learning_rate": 2.7211111111111113e-05,
      "loss": 0.0028,
      "step": 41020
    },
    {
      "epoch": 3.647111111111111,
      "grad_norm": 0.22615140676498413,
      "learning_rate": 2.7205555555555557e-05,
      "loss": 0.0028,
      "step": 41030
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.08511780947446823,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0024,
      "step": 41040
    },
    {
      "epoch": 3.648888888888889,
      "grad_norm": 0.20445452630519867,
      "learning_rate": 2.7194444444444444e-05,
      "loss": 0.0022,
      "step": 41050
    },
    {
      "epoch": 3.6497777777777776,
      "grad_norm": 0.16006438434123993,
      "learning_rate": 2.718888888888889e-05,
      "loss": 0.0022,
      "step": 41060
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.06419770419597626,
      "learning_rate": 2.7183333333333335e-05,
      "loss": 0.0022,
      "step": 41070
    },
    {
      "epoch": 3.6515555555555554,
      "grad_norm": 0.7283705472946167,
      "learning_rate": 2.717777777777778e-05,
      "loss": 0.0033,
      "step": 41080
    },
    {
      "epoch": 3.6524444444444444,
      "grad_norm": 0.05737851932644844,
      "learning_rate": 2.717222222222222e-05,
      "loss": 0.0021,
      "step": 41090
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.18924571573734283,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0023,
      "step": 41100
    },
    {
      "epoch": 3.6542222222222223,
      "grad_norm": 0.23546406626701355,
      "learning_rate": 2.7161111111111116e-05,
      "loss": 0.0018,
      "step": 41110
    },
    {
      "epoch": 3.655111111111111,
      "grad_norm": 0.5627383589744568,
      "learning_rate": 2.7155555555555556e-05,
      "loss": 0.0019,
      "step": 41120
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.2115117609500885,
      "learning_rate": 2.7150000000000003e-05,
      "loss": 0.0024,
      "step": 41130
    },
    {
      "epoch": 3.656888888888889,
      "grad_norm": 0.39296624064445496,
      "learning_rate": 2.7144444444444446e-05,
      "loss": 0.0018,
      "step": 41140
    },
    {
      "epoch": 3.6577777777777776,
      "grad_norm": 0.23803217709064484,
      "learning_rate": 2.7138888888888893e-05,
      "loss": 0.0021,
      "step": 41150
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.646727979183197,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0016,
      "step": 41160
    },
    {
      "epoch": 3.6595555555555555,
      "grad_norm": 0.9974475502967834,
      "learning_rate": 2.712777777777778e-05,
      "loss": 0.0031,
      "step": 41170
    },
    {
      "epoch": 3.6604444444444444,
      "grad_norm": 0.7504596710205078,
      "learning_rate": 2.712222222222222e-05,
      "loss": 0.0021,
      "step": 41180
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.23074911534786224,
      "learning_rate": 2.7116666666666667e-05,
      "loss": 0.0027,
      "step": 41190
    },
    {
      "epoch": 3.6622222222222223,
      "grad_norm": 0.2717294692993164,
      "learning_rate": 2.7111111111111114e-05,
      "loss": 0.0022,
      "step": 41200
    },
    {
      "epoch": 3.663111111111111,
      "grad_norm": 0.5495620965957642,
      "learning_rate": 2.7105555555555558e-05,
      "loss": 0.0028,
      "step": 41210
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.8125519156455994,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0017,
      "step": 41220
    },
    {
      "epoch": 3.664888888888889,
      "grad_norm": 0.04241672158241272,
      "learning_rate": 2.7094444444444445e-05,
      "loss": 0.0022,
      "step": 41230
    },
    {
      "epoch": 3.6657777777777776,
      "grad_norm": 0.47001737356185913,
      "learning_rate": 2.7088888888888892e-05,
      "loss": 0.0025,
      "step": 41240
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.1947721391916275,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.0023,
      "step": 41250
    },
    {
      "epoch": 3.6675555555555555,
      "grad_norm": 0.8932181596755981,
      "learning_rate": 2.707777777777778e-05,
      "loss": 0.0023,
      "step": 41260
    },
    {
      "epoch": 3.6684444444444444,
      "grad_norm": 0.11740801483392715,
      "learning_rate": 2.7072222222222223e-05,
      "loss": 0.0022,
      "step": 41270
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.28387749195098877,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0028,
      "step": 41280
    },
    {
      "epoch": 3.6702222222222223,
      "grad_norm": 0.37524962425231934,
      "learning_rate": 2.7061111111111116e-05,
      "loss": 0.0018,
      "step": 41290
    },
    {
      "epoch": 3.671111111111111,
      "grad_norm": 0.12448449432849884,
      "learning_rate": 2.7055555555555557e-05,
      "loss": 0.0025,
      "step": 41300
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.5816538333892822,
      "learning_rate": 2.7050000000000004e-05,
      "loss": 0.002,
      "step": 41310
    },
    {
      "epoch": 3.672888888888889,
      "grad_norm": 0.2658792734146118,
      "learning_rate": 2.7044444444444444e-05,
      "loss": 0.0027,
      "step": 41320
    },
    {
      "epoch": 3.6737777777777776,
      "grad_norm": 0.6308520436286926,
      "learning_rate": 2.703888888888889e-05,
      "loss": 0.0017,
      "step": 41330
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.2678307592868805,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.0014,
      "step": 41340
    },
    {
      "epoch": 3.6755555555555555,
      "grad_norm": 0.06276542693376541,
      "learning_rate": 2.702777777777778e-05,
      "loss": 0.002,
      "step": 41350
    },
    {
      "epoch": 3.6764444444444444,
      "grad_norm": 0.48437583446502686,
      "learning_rate": 2.702222222222222e-05,
      "loss": 0.0018,
      "step": 41360
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.42651498317718506,
      "learning_rate": 2.701666666666667e-05,
      "loss": 0.0022,
      "step": 41370
    },
    {
      "epoch": 3.6782222222222223,
      "grad_norm": 0.3477165997028351,
      "learning_rate": 2.7011111111111115e-05,
      "loss": 0.0027,
      "step": 41380
    },
    {
      "epoch": 3.679111111111111,
      "grad_norm": 0.3792915642261505,
      "learning_rate": 2.7005555555555555e-05,
      "loss": 0.0017,
      "step": 41390
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.7828200459480286,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0017,
      "step": 41400
    },
    {
      "epoch": 3.680888888888889,
      "grad_norm": 0.22366656363010406,
      "learning_rate": 2.6994444444444446e-05,
      "loss": 0.0033,
      "step": 41410
    },
    {
      "epoch": 3.6817777777777776,
      "grad_norm": 0.4268920123577118,
      "learning_rate": 2.6988888888888893e-05,
      "loss": 0.002,
      "step": 41420
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.19886243343353271,
      "learning_rate": 2.6983333333333333e-05,
      "loss": 0.0035,
      "step": 41430
    },
    {
      "epoch": 3.6835555555555555,
      "grad_norm": 0.5725252032279968,
      "learning_rate": 2.697777777777778e-05,
      "loss": 0.0015,
      "step": 41440
    },
    {
      "epoch": 3.6844444444444444,
      "grad_norm": 0.6925212144851685,
      "learning_rate": 2.697222222222222e-05,
      "loss": 0.0019,
      "step": 41450
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.5246751308441162,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.0019,
      "step": 41460
    },
    {
      "epoch": 3.6862222222222223,
      "grad_norm": 0.3504992425441742,
      "learning_rate": 2.6961111111111114e-05,
      "loss": 0.0032,
      "step": 41470
    },
    {
      "epoch": 3.6871111111111112,
      "grad_norm": 0.23742873966693878,
      "learning_rate": 2.6955555555555558e-05,
      "loss": 0.003,
      "step": 41480
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.30098244547843933,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 0.0018,
      "step": 41490
    },
    {
      "epoch": 3.688888888888889,
      "grad_norm": 0.391170859336853,
      "learning_rate": 2.6944444444444445e-05,
      "loss": 0.003,
      "step": 41500
    },
    {
      "epoch": 3.6897777777777776,
      "grad_norm": 0.6446248292922974,
      "learning_rate": 2.693888888888889e-05,
      "loss": 0.0024,
      "step": 41510
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.2014121562242508,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0015,
      "step": 41520
    },
    {
      "epoch": 3.6915555555555555,
      "grad_norm": 0.535527229309082,
      "learning_rate": 2.692777777777778e-05,
      "loss": 0.0019,
      "step": 41530
    },
    {
      "epoch": 3.6924444444444444,
      "grad_norm": 0.49956122040748596,
      "learning_rate": 2.6922222222222222e-05,
      "loss": 0.0025,
      "step": 41540
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.5826792120933533,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.0031,
      "step": 41550
    },
    {
      "epoch": 3.6942222222222223,
      "grad_norm": 0.36266642808914185,
      "learning_rate": 2.6911111111111116e-05,
      "loss": 0.002,
      "step": 41560
    },
    {
      "epoch": 3.6951111111111112,
      "grad_norm": 0.5200098156929016,
      "learning_rate": 2.6905555555555556e-05,
      "loss": 0.0018,
      "step": 41570
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.38464340567588806,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.0019,
      "step": 41580
    },
    {
      "epoch": 3.696888888888889,
      "grad_norm": 0.5526762008666992,
      "learning_rate": 2.6894444444444444e-05,
      "loss": 0.0017,
      "step": 41590
    },
    {
      "epoch": 3.6977777777777776,
      "grad_norm": 0.6077972650527954,
      "learning_rate": 2.688888888888889e-05,
      "loss": 0.0034,
      "step": 41600
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.7162360548973083,
      "learning_rate": 2.6883333333333334e-05,
      "loss": 0.0018,
      "step": 41610
    },
    {
      "epoch": 3.6995555555555555,
      "grad_norm": 0.45519620180130005,
      "learning_rate": 2.687777777777778e-05,
      "loss": 0.0019,
      "step": 41620
    },
    {
      "epoch": 3.7004444444444444,
      "grad_norm": 0.18714697659015656,
      "learning_rate": 2.687222222222222e-05,
      "loss": 0.0027,
      "step": 41630
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.5762364268302917,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.002,
      "step": 41640
    },
    {
      "epoch": 3.7022222222222223,
      "grad_norm": 0.5366289615631104,
      "learning_rate": 2.6861111111111115e-05,
      "loss": 0.0022,
      "step": 41650
    },
    {
      "epoch": 3.7031111111111112,
      "grad_norm": 0.12197496742010117,
      "learning_rate": 2.6855555555555555e-05,
      "loss": 0.0015,
      "step": 41660
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.5422421097755432,
      "learning_rate": 2.6850000000000002e-05,
      "loss": 0.0016,
      "step": 41670
    },
    {
      "epoch": 3.704888888888889,
      "grad_norm": 0.28044813871383667,
      "learning_rate": 2.6844444444444446e-05,
      "loss": 0.0022,
      "step": 41680
    },
    {
      "epoch": 3.7057777777777776,
      "grad_norm": 0.15507115423679352,
      "learning_rate": 2.6838888888888893e-05,
      "loss": 0.0019,
      "step": 41690
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.05873706564307213,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0016,
      "step": 41700
    },
    {
      "epoch": 3.7075555555555555,
      "grad_norm": 0.8787029385566711,
      "learning_rate": 2.682777777777778e-05,
      "loss": 0.0018,
      "step": 41710
    },
    {
      "epoch": 3.7084444444444444,
      "grad_norm": 0.1968577355146408,
      "learning_rate": 2.682222222222222e-05,
      "loss": 0.0016,
      "step": 41720
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.26787638664245605,
      "learning_rate": 2.6816666666666667e-05,
      "loss": 0.003,
      "step": 41730
    },
    {
      "epoch": 3.7102222222222223,
      "grad_norm": 0.5539459586143494,
      "learning_rate": 2.6811111111111114e-05,
      "loss": 0.0035,
      "step": 41740
    },
    {
      "epoch": 3.7111111111111112,
      "grad_norm": 0.4389461278915405,
      "learning_rate": 2.6805555555555557e-05,
      "loss": 0.0021,
      "step": 41750
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.275242418050766,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0019,
      "step": 41760
    },
    {
      "epoch": 3.712888888888889,
      "grad_norm": 0.8328312635421753,
      "learning_rate": 2.6794444444444444e-05,
      "loss": 0.0028,
      "step": 41770
    },
    {
      "epoch": 3.7137777777777776,
      "grad_norm": 0.03334113582968712,
      "learning_rate": 2.678888888888889e-05,
      "loss": 0.0024,
      "step": 41780
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.3809579312801361,
      "learning_rate": 2.678333333333333e-05,
      "loss": 0.0034,
      "step": 41790
    },
    {
      "epoch": 3.7155555555555555,
      "grad_norm": 0.36661380529403687,
      "learning_rate": 2.677777777777778e-05,
      "loss": 0.0021,
      "step": 41800
    },
    {
      "epoch": 3.7164444444444444,
      "grad_norm": 0.30267831683158875,
      "learning_rate": 2.6772222222222222e-05,
      "loss": 0.0014,
      "step": 41810
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.30600035190582275,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.0014,
      "step": 41820
    },
    {
      "epoch": 3.7182222222222223,
      "grad_norm": 0.5490806102752686,
      "learning_rate": 2.6761111111111116e-05,
      "loss": 0.0029,
      "step": 41830
    },
    {
      "epoch": 3.7191111111111113,
      "grad_norm": 0.3513321578502655,
      "learning_rate": 2.6755555555555556e-05,
      "loss": 0.0016,
      "step": 41840
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.5537921190261841,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.0022,
      "step": 41850
    },
    {
      "epoch": 3.720888888888889,
      "grad_norm": 0.7356363534927368,
      "learning_rate": 2.6744444444444443e-05,
      "loss": 0.0022,
      "step": 41860
    },
    {
      "epoch": 3.7217777777777776,
      "grad_norm": 0.612607479095459,
      "learning_rate": 2.673888888888889e-05,
      "loss": 0.002,
      "step": 41870
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.18546487390995026,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0026,
      "step": 41880
    },
    {
      "epoch": 3.7235555555555555,
      "grad_norm": 0.16604956984519958,
      "learning_rate": 2.672777777777778e-05,
      "loss": 0.0026,
      "step": 41890
    },
    {
      "epoch": 3.7244444444444444,
      "grad_norm": 0.37666529417037964,
      "learning_rate": 2.6722222222222228e-05,
      "loss": 0.0036,
      "step": 41900
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.1679973602294922,
      "learning_rate": 2.6716666666666668e-05,
      "loss": 0.0027,
      "step": 41910
    },
    {
      "epoch": 3.7262222222222223,
      "grad_norm": 0.24242931604385376,
      "learning_rate": 2.6711111111111115e-05,
      "loss": 0.0021,
      "step": 41920
    },
    {
      "epoch": 3.7271111111111113,
      "grad_norm": 0.22363777458667755,
      "learning_rate": 2.6705555555555555e-05,
      "loss": 0.002,
      "step": 41930
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.12222205102443695,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0025,
      "step": 41940
    },
    {
      "epoch": 3.728888888888889,
      "grad_norm": 0.34941673278808594,
      "learning_rate": 2.6694444444444445e-05,
      "loss": 0.0024,
      "step": 41950
    },
    {
      "epoch": 3.7297777777777776,
      "grad_norm": 0.9880447387695312,
      "learning_rate": 2.6688888888888892e-05,
      "loss": 0.0019,
      "step": 41960
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.19318535923957825,
      "learning_rate": 2.6683333333333333e-05,
      "loss": 0.0016,
      "step": 41970
    },
    {
      "epoch": 3.7315555555555555,
      "grad_norm": 0.4731905460357666,
      "learning_rate": 2.667777777777778e-05,
      "loss": 0.0022,
      "step": 41980
    },
    {
      "epoch": 3.7324444444444445,
      "grad_norm": 0.8020426630973816,
      "learning_rate": 2.6672222222222226e-05,
      "loss": 0.0018,
      "step": 41990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.23884381353855133,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0021,
      "step": 42000
    },
    {
      "epoch": 3.7342222222222223,
      "grad_norm": 0.214024618268013,
      "learning_rate": 2.6661111111111114e-05,
      "loss": 0.0019,
      "step": 42010
    },
    {
      "epoch": 3.7351111111111113,
      "grad_norm": 0.0821794867515564,
      "learning_rate": 2.6655555555555557e-05,
      "loss": 0.0018,
      "step": 42020
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.4052296578884125,
      "learning_rate": 2.6650000000000004e-05,
      "loss": 0.0026,
      "step": 42030
    },
    {
      "epoch": 3.736888888888889,
      "grad_norm": 0.4103541076183319,
      "learning_rate": 2.6644444444444444e-05,
      "loss": 0.0023,
      "step": 42040
    },
    {
      "epoch": 3.7377777777777776,
      "grad_norm": 0.7596079111099243,
      "learning_rate": 2.663888888888889e-05,
      "loss": 0.0019,
      "step": 42050
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.3210464119911194,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.0016,
      "step": 42060
    },
    {
      "epoch": 3.7395555555555555,
      "grad_norm": 0.28103047609329224,
      "learning_rate": 2.6627777777777778e-05,
      "loss": 0.0022,
      "step": 42070
    },
    {
      "epoch": 3.7404444444444445,
      "grad_norm": 0.09080944955348969,
      "learning_rate": 2.6622222222222225e-05,
      "loss": 0.0017,
      "step": 42080
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.3453681468963623,
      "learning_rate": 2.661666666666667e-05,
      "loss": 0.0016,
      "step": 42090
    },
    {
      "epoch": 3.7422222222222223,
      "grad_norm": 0.23617498576641083,
      "learning_rate": 2.6611111111111116e-05,
      "loss": 0.002,
      "step": 42100
    },
    {
      "epoch": 3.7431111111111113,
      "grad_norm": 0.040511325001716614,
      "learning_rate": 2.6605555555555556e-05,
      "loss": 0.0033,
      "step": 42110
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.42064157128334045,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0027,
      "step": 42120
    },
    {
      "epoch": 3.744888888888889,
      "grad_norm": 0.3515864908695221,
      "learning_rate": 2.6594444444444443e-05,
      "loss": 0.0021,
      "step": 42130
    },
    {
      "epoch": 3.7457777777777777,
      "grad_norm": 0.36176174879074097,
      "learning_rate": 2.658888888888889e-05,
      "loss": 0.0018,
      "step": 42140
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.07667607814073563,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.0026,
      "step": 42150
    },
    {
      "epoch": 3.7475555555555555,
      "grad_norm": 0.07763199508190155,
      "learning_rate": 2.657777777777778e-05,
      "loss": 0.0022,
      "step": 42160
    },
    {
      "epoch": 3.7484444444444445,
      "grad_norm": 0.5667991042137146,
      "learning_rate": 2.6572222222222227e-05,
      "loss": 0.0021,
      "step": 42170
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.6327059268951416,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0019,
      "step": 42180
    },
    {
      "epoch": 3.7502222222222223,
      "grad_norm": 0.28697094321250916,
      "learning_rate": 2.6561111111111114e-05,
      "loss": 0.0022,
      "step": 42190
    },
    {
      "epoch": 3.7511111111111113,
      "grad_norm": 0.29978054761886597,
      "learning_rate": 2.6555555555555555e-05,
      "loss": 0.0018,
      "step": 42200
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.07091804593801498,
      "learning_rate": 2.655e-05,
      "loss": 0.0026,
      "step": 42210
    },
    {
      "epoch": 3.752888888888889,
      "grad_norm": 0.662080705165863,
      "learning_rate": 2.6544444444444445e-05,
      "loss": 0.0018,
      "step": 42220
    },
    {
      "epoch": 3.7537777777777777,
      "grad_norm": 0.11807654798030853,
      "learning_rate": 2.6538888888888892e-05,
      "loss": 0.0016,
      "step": 42230
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.19494451582431793,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0018,
      "step": 42240
    },
    {
      "epoch": 3.7555555555555555,
      "grad_norm": 0.29998543858528137,
      "learning_rate": 2.652777777777778e-05,
      "loss": 0.0021,
      "step": 42250
    },
    {
      "epoch": 3.7564444444444445,
      "grad_norm": 0.3172226548194885,
      "learning_rate": 2.6522222222222226e-05,
      "loss": 0.0025,
      "step": 42260
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 1.0169202089309692,
      "learning_rate": 2.6516666666666666e-05,
      "loss": 0.0024,
      "step": 42270
    },
    {
      "epoch": 3.7582222222222224,
      "grad_norm": 0.5831893682479858,
      "learning_rate": 2.6511111111111113e-05,
      "loss": 0.0033,
      "step": 42280
    },
    {
      "epoch": 3.7591111111111113,
      "grad_norm": 0.18713238835334778,
      "learning_rate": 2.6505555555555557e-05,
      "loss": 0.0016,
      "step": 42290
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.31287717819213867,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0017,
      "step": 42300
    },
    {
      "epoch": 3.7608888888888887,
      "grad_norm": 0.22661590576171875,
      "learning_rate": 2.6494444444444444e-05,
      "loss": 0.0016,
      "step": 42310
    },
    {
      "epoch": 3.7617777777777777,
      "grad_norm": 0.2273617833852768,
      "learning_rate": 2.648888888888889e-05,
      "loss": 0.0022,
      "step": 42320
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.04691129922866821,
      "learning_rate": 2.648333333333333e-05,
      "loss": 0.0026,
      "step": 42330
    },
    {
      "epoch": 3.7635555555555555,
      "grad_norm": 0.4685320258140564,
      "learning_rate": 2.6477777777777778e-05,
      "loss": 0.0029,
      "step": 42340
    },
    {
      "epoch": 3.7644444444444445,
      "grad_norm": 0.3022899627685547,
      "learning_rate": 2.6472222222222225e-05,
      "loss": 0.0016,
      "step": 42350
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.632262647151947,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0015,
      "step": 42360
    },
    {
      "epoch": 3.7662222222222224,
      "grad_norm": 0.2476436048746109,
      "learning_rate": 2.6461111111111115e-05,
      "loss": 0.0023,
      "step": 42370
    },
    {
      "epoch": 3.7671111111111113,
      "grad_norm": 0.5051478147506714,
      "learning_rate": 2.6455555555555556e-05,
      "loss": 0.0018,
      "step": 42380
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.26977336406707764,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 0.0026,
      "step": 42390
    },
    {
      "epoch": 3.7688888888888887,
      "grad_norm": 1.0505753755569458,
      "learning_rate": 2.6444444444444443e-05,
      "loss": 0.0019,
      "step": 42400
    },
    {
      "epoch": 3.7697777777777777,
      "grad_norm": 1.2232959270477295,
      "learning_rate": 2.643888888888889e-05,
      "loss": 0.0025,
      "step": 42410
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.7216209769248962,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.0026,
      "step": 42420
    },
    {
      "epoch": 3.7715555555555556,
      "grad_norm": 0.4160119593143463,
      "learning_rate": 2.642777777777778e-05,
      "loss": 0.0015,
      "step": 42430
    },
    {
      "epoch": 3.7724444444444445,
      "grad_norm": 0.8005767464637756,
      "learning_rate": 2.6422222222222227e-05,
      "loss": 0.0015,
      "step": 42440
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.2944673001766205,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.0019,
      "step": 42450
    },
    {
      "epoch": 3.7742222222222224,
      "grad_norm": 0.16181764006614685,
      "learning_rate": 2.6411111111111114e-05,
      "loss": 0.0022,
      "step": 42460
    },
    {
      "epoch": 3.7751111111111113,
      "grad_norm": 0.6146343350410461,
      "learning_rate": 2.6405555555555554e-05,
      "loss": 0.0025,
      "step": 42470
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.1597103327512741,
      "learning_rate": 2.64e-05,
      "loss": 0.0021,
      "step": 42480
    },
    {
      "epoch": 3.7768888888888887,
      "grad_norm": 0.41658276319503784,
      "learning_rate": 2.6394444444444445e-05,
      "loss": 0.0017,
      "step": 42490
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.2722662389278412,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 0.0024,
      "step": 42500
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 1.05321204662323,
      "learning_rate": 2.6383333333333332e-05,
      "loss": 0.0021,
      "step": 42510
    },
    {
      "epoch": 3.7795555555555556,
      "grad_norm": 0.8059711456298828,
      "learning_rate": 2.637777777777778e-05,
      "loss": 0.002,
      "step": 42520
    },
    {
      "epoch": 3.7804444444444445,
      "grad_norm": 0.6431699991226196,
      "learning_rate": 2.6372222222222226e-05,
      "loss": 0.0027,
      "step": 42530
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.6724867820739746,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.0018,
      "step": 42540
    },
    {
      "epoch": 3.7822222222222224,
      "grad_norm": 0.19675619900226593,
      "learning_rate": 2.6361111111111113e-05,
      "loss": 0.0031,
      "step": 42550
    },
    {
      "epoch": 3.7831111111111113,
      "grad_norm": 0.4430381655693054,
      "learning_rate": 2.6355555555555557e-05,
      "loss": 0.0015,
      "step": 42560
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.3937862515449524,
      "learning_rate": 2.6350000000000004e-05,
      "loss": 0.0025,
      "step": 42570
    },
    {
      "epoch": 3.7848888888888887,
      "grad_norm": 0.8310245871543884,
      "learning_rate": 2.6344444444444444e-05,
      "loss": 0.0024,
      "step": 42580
    },
    {
      "epoch": 3.7857777777777777,
      "grad_norm": 0.3085876703262329,
      "learning_rate": 2.633888888888889e-05,
      "loss": 0.0025,
      "step": 42590
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.04799632728099823,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0017,
      "step": 42600
    },
    {
      "epoch": 3.7875555555555556,
      "grad_norm": 0.47601959109306335,
      "learning_rate": 2.6327777777777778e-05,
      "loss": 0.0017,
      "step": 42610
    },
    {
      "epoch": 3.7884444444444445,
      "grad_norm": 0.3857475221157074,
      "learning_rate": 2.6322222222222225e-05,
      "loss": 0.0017,
      "step": 42620
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.051859769970178604,
      "learning_rate": 2.6316666666666668e-05,
      "loss": 0.0016,
      "step": 42630
    },
    {
      "epoch": 3.7902222222222224,
      "grad_norm": 0.2250819057226181,
      "learning_rate": 2.6311111111111115e-05,
      "loss": 0.0019,
      "step": 42640
    },
    {
      "epoch": 3.7911111111111113,
      "grad_norm": 0.2279701679944992,
      "learning_rate": 2.6305555555555555e-05,
      "loss": 0.0026,
      "step": 42650
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.18638095259666443,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0021,
      "step": 42660
    },
    {
      "epoch": 3.7928888888888888,
      "grad_norm": 0.4215982258319855,
      "learning_rate": 2.6294444444444442e-05,
      "loss": 0.0022,
      "step": 42670
    },
    {
      "epoch": 3.7937777777777777,
      "grad_norm": 0.15239067375659943,
      "learning_rate": 2.628888888888889e-05,
      "loss": 0.002,
      "step": 42680
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.08183389902114868,
      "learning_rate": 2.6283333333333333e-05,
      "loss": 0.0032,
      "step": 42690
    },
    {
      "epoch": 3.7955555555555556,
      "grad_norm": 0.18183410167694092,
      "learning_rate": 2.627777777777778e-05,
      "loss": 0.0017,
      "step": 42700
    },
    {
      "epoch": 3.7964444444444445,
      "grad_norm": 0.19311796128749847,
      "learning_rate": 2.6272222222222227e-05,
      "loss": 0.0024,
      "step": 42710
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.17702360451221466,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0017,
      "step": 42720
    },
    {
      "epoch": 3.7982222222222224,
      "grad_norm": 0.4170936942100525,
      "learning_rate": 2.6261111111111114e-05,
      "loss": 0.0023,
      "step": 42730
    },
    {
      "epoch": 3.7991111111111113,
      "grad_norm": 0.06576769053936005,
      "learning_rate": 2.6255555555555554e-05,
      "loss": 0.0017,
      "step": 42740
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.4196355640888214,
      "learning_rate": 2.625e-05,
      "loss": 0.0018,
      "step": 42750
    },
    {
      "epoch": 3.8008888888888888,
      "grad_norm": 0.35354265570640564,
      "learning_rate": 2.6244444444444445e-05,
      "loss": 0.002,
      "step": 42760
    },
    {
      "epoch": 3.8017777777777777,
      "grad_norm": 0.11074785888195038,
      "learning_rate": 2.623888888888889e-05,
      "loss": 0.0014,
      "step": 42770
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.47006726264953613,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.0022,
      "step": 42780
    },
    {
      "epoch": 3.8035555555555556,
      "grad_norm": 0.40401098132133484,
      "learning_rate": 2.622777777777778e-05,
      "loss": 0.0027,
      "step": 42790
    },
    {
      "epoch": 3.8044444444444445,
      "grad_norm": 0.22400793433189392,
      "learning_rate": 2.6222222222222226e-05,
      "loss": 0.0017,
      "step": 42800
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.18999050557613373,
      "learning_rate": 2.6216666666666666e-05,
      "loss": 0.0018,
      "step": 42810
    },
    {
      "epoch": 3.806222222222222,
      "grad_norm": 0.17442558705806732,
      "learning_rate": 2.6211111111111113e-05,
      "loss": 0.0022,
      "step": 42820
    },
    {
      "epoch": 3.8071111111111113,
      "grad_norm": 0.04886256530880928,
      "learning_rate": 2.6205555555555556e-05,
      "loss": 0.0028,
      "step": 42830
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.10044164210557938,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0017,
      "step": 42840
    },
    {
      "epoch": 3.8088888888888888,
      "grad_norm": 0.12320541590452194,
      "learning_rate": 2.6194444444444443e-05,
      "loss": 0.0018,
      "step": 42850
    },
    {
      "epoch": 3.8097777777777777,
      "grad_norm": 0.15953770279884338,
      "learning_rate": 2.618888888888889e-05,
      "loss": 0.0016,
      "step": 42860
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.08499602973461151,
      "learning_rate": 2.618333333333333e-05,
      "loss": 0.0021,
      "step": 42870
    },
    {
      "epoch": 3.8115555555555556,
      "grad_norm": 0.531768262386322,
      "learning_rate": 2.6177777777777777e-05,
      "loss": 0.0019,
      "step": 42880
    },
    {
      "epoch": 3.8124444444444445,
      "grad_norm": 0.3581773042678833,
      "learning_rate": 2.6172222222222224e-05,
      "loss": 0.0022,
      "step": 42890
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.41232389211654663,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0026,
      "step": 42900
    },
    {
      "epoch": 3.814222222222222,
      "grad_norm": 1.0354044437408447,
      "learning_rate": 2.6161111111111115e-05,
      "loss": 0.0025,
      "step": 42910
    },
    {
      "epoch": 3.8151111111111113,
      "grad_norm": 0.730709969997406,
      "learning_rate": 2.6155555555555555e-05,
      "loss": 0.0028,
      "step": 42920
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.06573674827814102,
      "learning_rate": 2.6150000000000002e-05,
      "loss": 0.0027,
      "step": 42930
    },
    {
      "epoch": 3.8168888888888888,
      "grad_norm": 0.170003741979599,
      "learning_rate": 2.6144444444444442e-05,
      "loss": 0.0023,
      "step": 42940
    },
    {
      "epoch": 3.8177777777777777,
      "grad_norm": 0.2660486400127411,
      "learning_rate": 2.613888888888889e-05,
      "loss": 0.0023,
      "step": 42950
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.6462741494178772,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0031,
      "step": 42960
    },
    {
      "epoch": 3.8195555555555556,
      "grad_norm": 0.2243947833776474,
      "learning_rate": 2.612777777777778e-05,
      "loss": 0.0021,
      "step": 42970
    },
    {
      "epoch": 3.8204444444444445,
      "grad_norm": 0.4540902376174927,
      "learning_rate": 2.6122222222222227e-05,
      "loss": 0.0034,
      "step": 42980
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.3194163739681244,
      "learning_rate": 2.6116666666666667e-05,
      "loss": 0.0026,
      "step": 42990
    },
    {
      "epoch": 3.822222222222222,
      "grad_norm": 0.26979756355285645,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 0.0025,
      "step": 43000
    },
    {
      "epoch": 3.8231111111111113,
      "grad_norm": 0.2261778861284256,
      "learning_rate": 2.6105555555555554e-05,
      "loss": 0.0019,
      "step": 43010
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.17427940666675568,
      "learning_rate": 2.61e-05,
      "loss": 0.002,
      "step": 43020
    },
    {
      "epoch": 3.824888888888889,
      "grad_norm": 0.2253132164478302,
      "learning_rate": 2.6094444444444444e-05,
      "loss": 0.0022,
      "step": 43030
    },
    {
      "epoch": 3.8257777777777777,
      "grad_norm": 0.5489749908447266,
      "learning_rate": 2.608888888888889e-05,
      "loss": 0.0018,
      "step": 43040
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.852185845375061,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.0021,
      "step": 43050
    },
    {
      "epoch": 3.8275555555555556,
      "grad_norm": 0.2247946560382843,
      "learning_rate": 2.607777777777778e-05,
      "loss": 0.0019,
      "step": 43060
    },
    {
      "epoch": 3.8284444444444445,
      "grad_norm": 0.12840823829174042,
      "learning_rate": 2.6072222222222225e-05,
      "loss": 0.0021,
      "step": 43070
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.761309802532196,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0024,
      "step": 43080
    },
    {
      "epoch": 3.830222222222222,
      "grad_norm": 0.7025703191757202,
      "learning_rate": 2.6061111111111113e-05,
      "loss": 0.002,
      "step": 43090
    },
    {
      "epoch": 3.8311111111111114,
      "grad_norm": 0.25227490067481995,
      "learning_rate": 2.6055555555555556e-05,
      "loss": 0.0026,
      "step": 43100
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.768474280834198,
      "learning_rate": 2.6050000000000003e-05,
      "loss": 0.0035,
      "step": 43110
    },
    {
      "epoch": 3.832888888888889,
      "grad_norm": 0.22064591944217682,
      "learning_rate": 2.6044444444444443e-05,
      "loss": 0.0018,
      "step": 43120
    },
    {
      "epoch": 3.8337777777777777,
      "grad_norm": 0.7933527231216431,
      "learning_rate": 2.603888888888889e-05,
      "loss": 0.0015,
      "step": 43130
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.08962792903184891,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.0021,
      "step": 43140
    },
    {
      "epoch": 3.8355555555555556,
      "grad_norm": 0.31455984711647034,
      "learning_rate": 2.6027777777777777e-05,
      "loss": 0.0015,
      "step": 43150
    },
    {
      "epoch": 3.8364444444444445,
      "grad_norm": 0.1167265772819519,
      "learning_rate": 2.6022222222222224e-05,
      "loss": 0.0027,
      "step": 43160
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.67840576171875,
      "learning_rate": 2.6016666666666668e-05,
      "loss": 0.0028,
      "step": 43170
    },
    {
      "epoch": 3.838222222222222,
      "grad_norm": 0.3715471029281616,
      "learning_rate": 2.6011111111111115e-05,
      "loss": 0.0025,
      "step": 43180
    },
    {
      "epoch": 3.8391111111111114,
      "grad_norm": 0.26408571004867554,
      "learning_rate": 2.6005555555555555e-05,
      "loss": 0.0021,
      "step": 43190
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.259151428937912,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0022,
      "step": 43200
    },
    {
      "epoch": 3.840888888888889,
      "grad_norm": 0.3016480803489685,
      "learning_rate": 2.5994444444444442e-05,
      "loss": 0.0018,
      "step": 43210
    },
    {
      "epoch": 3.8417777777777777,
      "grad_norm": 0.37579602003097534,
      "learning_rate": 2.598888888888889e-05,
      "loss": 0.0016,
      "step": 43220
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.1137317344546318,
      "learning_rate": 2.5983333333333336e-05,
      "loss": 0.0026,
      "step": 43230
    },
    {
      "epoch": 3.8435555555555556,
      "grad_norm": 0.2638489603996277,
      "learning_rate": 2.597777777777778e-05,
      "loss": 0.0018,
      "step": 43240
    },
    {
      "epoch": 3.8444444444444446,
      "grad_norm": 0.2820616364479065,
      "learning_rate": 2.5972222222222226e-05,
      "loss": 0.0029,
      "step": 43250
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.6457975506782532,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.0021,
      "step": 43260
    },
    {
      "epoch": 3.846222222222222,
      "grad_norm": 0.26214274764060974,
      "learning_rate": 2.5961111111111113e-05,
      "loss": 0.0017,
      "step": 43270
    },
    {
      "epoch": 3.8471111111111114,
      "grad_norm": 0.45006299018859863,
      "learning_rate": 2.5955555555555554e-05,
      "loss": 0.0024,
      "step": 43280
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.08156264573335648,
      "learning_rate": 2.595e-05,
      "loss": 0.0016,
      "step": 43290
    },
    {
      "epoch": 3.848888888888889,
      "grad_norm": 0.6031906008720398,
      "learning_rate": 2.5944444444444444e-05,
      "loss": 0.0021,
      "step": 43300
    },
    {
      "epoch": 3.8497777777777777,
      "grad_norm": 0.6589142680168152,
      "learning_rate": 2.593888888888889e-05,
      "loss": 0.0031,
      "step": 43310
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 1.1269187927246094,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0017,
      "step": 43320
    },
    {
      "epoch": 3.8515555555555556,
      "grad_norm": 0.46575772762298584,
      "learning_rate": 2.5927777777777778e-05,
      "loss": 0.0026,
      "step": 43330
    },
    {
      "epoch": 3.8524444444444446,
      "grad_norm": 0.6805841326713562,
      "learning_rate": 2.5922222222222225e-05,
      "loss": 0.0016,
      "step": 43340
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.27225443720817566,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.0014,
      "step": 43350
    },
    {
      "epoch": 3.854222222222222,
      "grad_norm": 0.7973151206970215,
      "learning_rate": 2.5911111111111112e-05,
      "loss": 0.002,
      "step": 43360
    },
    {
      "epoch": 3.8551111111111114,
      "grad_norm": 0.7381387948989868,
      "learning_rate": 2.5905555555555556e-05,
      "loss": 0.0029,
      "step": 43370
    },
    {
      "epoch": 3.856,
      "grad_norm": 1.049187421798706,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0023,
      "step": 43380
    },
    {
      "epoch": 3.856888888888889,
      "grad_norm": 0.6373479962348938,
      "learning_rate": 2.5894444444444443e-05,
      "loss": 0.0027,
      "step": 43390
    },
    {
      "epoch": 3.8577777777777778,
      "grad_norm": 0.708145797252655,
      "learning_rate": 2.588888888888889e-05,
      "loss": 0.0025,
      "step": 43400
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.39041146636009216,
      "learning_rate": 2.5883333333333337e-05,
      "loss": 0.0031,
      "step": 43410
    },
    {
      "epoch": 3.8595555555555556,
      "grad_norm": 0.962024986743927,
      "learning_rate": 2.5877777777777777e-05,
      "loss": 0.0024,
      "step": 43420
    },
    {
      "epoch": 3.8604444444444446,
      "grad_norm": 0.5423195362091064,
      "learning_rate": 2.5872222222222224e-05,
      "loss": 0.0016,
      "step": 43430
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.08057160675525665,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0019,
      "step": 43440
    },
    {
      "epoch": 3.862222222222222,
      "grad_norm": 0.15481460094451904,
      "learning_rate": 2.5861111111111114e-05,
      "loss": 0.002,
      "step": 43450
    },
    {
      "epoch": 3.8631111111111114,
      "grad_norm": 0.20397049188613892,
      "learning_rate": 2.5855555555555555e-05,
      "loss": 0.0022,
      "step": 43460
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.18826903402805328,
      "learning_rate": 2.585e-05,
      "loss": 0.0025,
      "step": 43470
    },
    {
      "epoch": 3.864888888888889,
      "grad_norm": 0.3926200270652771,
      "learning_rate": 2.5844444444444442e-05,
      "loss": 0.0016,
      "step": 43480
    },
    {
      "epoch": 3.8657777777777778,
      "grad_norm": 0.6244266629219055,
      "learning_rate": 2.583888888888889e-05,
      "loss": 0.0023,
      "step": 43490
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.6784720420837402,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.003,
      "step": 43500
    },
    {
      "epoch": 3.8675555555555556,
      "grad_norm": 0.46127909421920776,
      "learning_rate": 2.582777777777778e-05,
      "loss": 0.0031,
      "step": 43510
    },
    {
      "epoch": 3.8684444444444446,
      "grad_norm": 0.47198501229286194,
      "learning_rate": 2.5822222222222226e-05,
      "loss": 0.0013,
      "step": 43520
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.5737856030464172,
      "learning_rate": 2.5816666666666666e-05,
      "loss": 0.0017,
      "step": 43530
    },
    {
      "epoch": 3.870222222222222,
      "grad_norm": 0.1119450032711029,
      "learning_rate": 2.5811111111111113e-05,
      "loss": 0.0018,
      "step": 43540
    },
    {
      "epoch": 3.871111111111111,
      "grad_norm": 0.12180740386247635,
      "learning_rate": 2.5805555555555553e-05,
      "loss": 0.0019,
      "step": 43550
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.959384560585022,
      "learning_rate": 2.58e-05,
      "loss": 0.0017,
      "step": 43560
    },
    {
      "epoch": 3.872888888888889,
      "grad_norm": 0.055658970028162,
      "learning_rate": 2.5794444444444444e-05,
      "loss": 0.0013,
      "step": 43570
    },
    {
      "epoch": 3.8737777777777778,
      "grad_norm": 0.434218168258667,
      "learning_rate": 2.578888888888889e-05,
      "loss": 0.0024,
      "step": 43580
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.668999195098877,
      "learning_rate": 2.5783333333333338e-05,
      "loss": 0.0024,
      "step": 43590
    },
    {
      "epoch": 3.8755555555555556,
      "grad_norm": 0.9353699684143066,
      "learning_rate": 2.5777777777777778e-05,
      "loss": 0.0016,
      "step": 43600
    },
    {
      "epoch": 3.8764444444444446,
      "grad_norm": 0.435613751411438,
      "learning_rate": 2.5772222222222225e-05,
      "loss": 0.0028,
      "step": 43610
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.16658878326416016,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.0025,
      "step": 43620
    },
    {
      "epoch": 3.878222222222222,
      "grad_norm": 0.05787898600101471,
      "learning_rate": 2.5761111111111112e-05,
      "loss": 0.0018,
      "step": 43630
    },
    {
      "epoch": 3.879111111111111,
      "grad_norm": 0.20166803896427155,
      "learning_rate": 2.5755555555555556e-05,
      "loss": 0.0017,
      "step": 43640
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.22274179756641388,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.002,
      "step": 43650
    },
    {
      "epoch": 3.880888888888889,
      "grad_norm": 0.4764581620693207,
      "learning_rate": 2.5744444444444443e-05,
      "loss": 0.0027,
      "step": 43660
    },
    {
      "epoch": 3.8817777777777778,
      "grad_norm": 0.1795574575662613,
      "learning_rate": 2.573888888888889e-05,
      "loss": 0.0027,
      "step": 43670
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.28891459107398987,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0022,
      "step": 43680
    },
    {
      "epoch": 3.8835555555555556,
      "grad_norm": 0.07781918346881866,
      "learning_rate": 2.572777777777778e-05,
      "loss": 0.0024,
      "step": 43690
    },
    {
      "epoch": 3.8844444444444446,
      "grad_norm": 0.1851803958415985,
      "learning_rate": 2.5722222222222224e-05,
      "loss": 0.0021,
      "step": 43700
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.11943793296813965,
      "learning_rate": 2.5716666666666667e-05,
      "loss": 0.0014,
      "step": 43710
    },
    {
      "epoch": 3.886222222222222,
      "grad_norm": 0.22985951602458954,
      "learning_rate": 2.5711111111111114e-05,
      "loss": 0.0021,
      "step": 43720
    },
    {
      "epoch": 3.887111111111111,
      "grad_norm": 0.4661197364330292,
      "learning_rate": 2.5705555555555554e-05,
      "loss": 0.0014,
      "step": 43730
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.9370821118354797,
      "learning_rate": 2.57e-05,
      "loss": 0.0025,
      "step": 43740
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.9988468885421753,
      "learning_rate": 2.5694444444444445e-05,
      "loss": 0.0031,
      "step": 43750
    },
    {
      "epoch": 3.889777777777778,
      "grad_norm": 0.33845680952072144,
      "learning_rate": 2.5688888888888892e-05,
      "loss": 0.0029,
      "step": 43760
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.8287537097930908,
      "learning_rate": 2.5683333333333335e-05,
      "loss": 0.0022,
      "step": 43770
    },
    {
      "epoch": 3.8915555555555557,
      "grad_norm": 0.1265139877796173,
      "learning_rate": 2.567777777777778e-05,
      "loss": 0.002,
      "step": 43780
    },
    {
      "epoch": 3.8924444444444446,
      "grad_norm": 0.9948139190673828,
      "learning_rate": 2.5672222222222226e-05,
      "loss": 0.002,
      "step": 43790
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.5350431799888611,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0024,
      "step": 43800
    },
    {
      "epoch": 3.894222222222222,
      "grad_norm": 0.21849371492862701,
      "learning_rate": 2.5661111111111113e-05,
      "loss": 0.0024,
      "step": 43810
    },
    {
      "epoch": 3.895111111111111,
      "grad_norm": 0.13506127893924713,
      "learning_rate": 2.5655555555555557e-05,
      "loss": 0.0021,
      "step": 43820
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.046559013426303864,
      "learning_rate": 2.5650000000000003e-05,
      "loss": 0.0021,
      "step": 43830
    },
    {
      "epoch": 3.896888888888889,
      "grad_norm": 0.19923534989356995,
      "learning_rate": 2.5644444444444444e-05,
      "loss": 0.0024,
      "step": 43840
    },
    {
      "epoch": 3.897777777777778,
      "grad_norm": 0.02269083261489868,
      "learning_rate": 2.563888888888889e-05,
      "loss": 0.0014,
      "step": 43850
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.09830106794834137,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0022,
      "step": 43860
    },
    {
      "epoch": 3.8995555555555557,
      "grad_norm": 0.41913631558418274,
      "learning_rate": 2.5627777777777778e-05,
      "loss": 0.0022,
      "step": 43870
    },
    {
      "epoch": 3.9004444444444446,
      "grad_norm": 0.6670308113098145,
      "learning_rate": 2.5622222222222225e-05,
      "loss": 0.0021,
      "step": 43880
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.6823206543922424,
      "learning_rate": 2.5616666666666668e-05,
      "loss": 0.0017,
      "step": 43890
    },
    {
      "epoch": 3.902222222222222,
      "grad_norm": 0.16907595098018646,
      "learning_rate": 2.5611111111111115e-05,
      "loss": 0.0015,
      "step": 43900
    },
    {
      "epoch": 3.903111111111111,
      "grad_norm": 0.38016772270202637,
      "learning_rate": 2.5605555555555555e-05,
      "loss": 0.0025,
      "step": 43910
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.9338623881340027,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0016,
      "step": 43920
    },
    {
      "epoch": 3.904888888888889,
      "grad_norm": 0.9284594058990479,
      "learning_rate": 2.5594444444444442e-05,
      "loss": 0.0016,
      "step": 43930
    },
    {
      "epoch": 3.905777777777778,
      "grad_norm": 0.6662449240684509,
      "learning_rate": 2.558888888888889e-05,
      "loss": 0.0018,
      "step": 43940
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.4956223964691162,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.0032,
      "step": 43950
    },
    {
      "epoch": 3.9075555555555557,
      "grad_norm": 0.337777704000473,
      "learning_rate": 2.557777777777778e-05,
      "loss": 0.0024,
      "step": 43960
    },
    {
      "epoch": 3.9084444444444446,
      "grad_norm": 0.6001914143562317,
      "learning_rate": 2.5572222222222227e-05,
      "loss": 0.0015,
      "step": 43970
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.5992769598960876,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0015,
      "step": 43980
    },
    {
      "epoch": 3.910222222222222,
      "grad_norm": 0.3398977220058441,
      "learning_rate": 2.5561111111111114e-05,
      "loss": 0.0029,
      "step": 43990
    },
    {
      "epoch": 3.911111111111111,
      "grad_norm": 0.6952391862869263,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 0.003,
      "step": 44000
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.4603491425514221,
      "learning_rate": 2.555e-05,
      "loss": 0.0021,
      "step": 44010
    },
    {
      "epoch": 3.912888888888889,
      "grad_norm": 0.38681313395500183,
      "learning_rate": 2.5544444444444445e-05,
      "loss": 0.0014,
      "step": 44020
    },
    {
      "epoch": 3.913777777777778,
      "grad_norm": 0.5088033676147461,
      "learning_rate": 2.553888888888889e-05,
      "loss": 0.0016,
      "step": 44030
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.3796781897544861,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0023,
      "step": 44040
    },
    {
      "epoch": 3.9155555555555557,
      "grad_norm": 0.38588467240333557,
      "learning_rate": 2.552777777777778e-05,
      "loss": 0.0018,
      "step": 44050
    },
    {
      "epoch": 3.916444444444444,
      "grad_norm": 0.39206835627555847,
      "learning_rate": 2.5522222222222226e-05,
      "loss": 0.0015,
      "step": 44060
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.0960613265633583,
      "learning_rate": 2.5516666666666666e-05,
      "loss": 0.0016,
      "step": 44070
    },
    {
      "epoch": 3.918222222222222,
      "grad_norm": 0.7498427629470825,
      "learning_rate": 2.5511111111111113e-05,
      "loss": 0.0029,
      "step": 44080
    },
    {
      "epoch": 3.919111111111111,
      "grad_norm": 0.08382533490657806,
      "learning_rate": 2.5505555555555556e-05,
      "loss": 0.0014,
      "step": 44090
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.27692240476608276,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0017,
      "step": 44100
    },
    {
      "epoch": 3.920888888888889,
      "grad_norm": 0.20098918676376343,
      "learning_rate": 2.5494444444444443e-05,
      "loss": 0.0016,
      "step": 44110
    },
    {
      "epoch": 3.921777777777778,
      "grad_norm": 0.19856426119804382,
      "learning_rate": 2.548888888888889e-05,
      "loss": 0.0017,
      "step": 44120
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.49528226256370544,
      "learning_rate": 2.5483333333333337e-05,
      "loss": 0.0024,
      "step": 44130
    },
    {
      "epoch": 3.9235555555555557,
      "grad_norm": 0.5338907241821289,
      "learning_rate": 2.5477777777777777e-05,
      "loss": 0.0023,
      "step": 44140
    },
    {
      "epoch": 3.924444444444444,
      "grad_norm": 0.12239909917116165,
      "learning_rate": 2.5472222222222224e-05,
      "loss": 0.0017,
      "step": 44150
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.3953315317630768,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0015,
      "step": 44160
    },
    {
      "epoch": 3.926222222222222,
      "grad_norm": 0.29880577325820923,
      "learning_rate": 2.5461111111111115e-05,
      "loss": 0.0027,
      "step": 44170
    },
    {
      "epoch": 3.927111111111111,
      "grad_norm": 0.7194185256958008,
      "learning_rate": 2.5455555555555555e-05,
      "loss": 0.002,
      "step": 44180
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.1263398826122284,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 0.0016,
      "step": 44190
    },
    {
      "epoch": 3.928888888888889,
      "grad_norm": 0.8706075549125671,
      "learning_rate": 2.5444444444444442e-05,
      "loss": 0.0027,
      "step": 44200
    },
    {
      "epoch": 3.929777777777778,
      "grad_norm": 0.4541274607181549,
      "learning_rate": 2.543888888888889e-05,
      "loss": 0.0016,
      "step": 44210
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.042698636651039124,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0021,
      "step": 44220
    },
    {
      "epoch": 3.9315555555555557,
      "grad_norm": 0.3697742819786072,
      "learning_rate": 2.542777777777778e-05,
      "loss": 0.0018,
      "step": 44230
    },
    {
      "epoch": 3.932444444444444,
      "grad_norm": 0.19264966249465942,
      "learning_rate": 2.5422222222222227e-05,
      "loss": 0.0021,
      "step": 44240
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.15239910781383514,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.0023,
      "step": 44250
    },
    {
      "epoch": 3.934222222222222,
      "grad_norm": 0.3847261071205139,
      "learning_rate": 2.5411111111111114e-05,
      "loss": 0.0018,
      "step": 44260
    },
    {
      "epoch": 3.935111111111111,
      "grad_norm": 0.6361899971961975,
      "learning_rate": 2.5405555555555554e-05,
      "loss": 0.0016,
      "step": 44270
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.3571307063102722,
      "learning_rate": 2.54e-05,
      "loss": 0.0034,
      "step": 44280
    },
    {
      "epoch": 3.936888888888889,
      "grad_norm": 0.5656397342681885,
      "learning_rate": 2.5394444444444444e-05,
      "loss": 0.0016,
      "step": 44290
    },
    {
      "epoch": 3.937777777777778,
      "grad_norm": 0.8881824016571045,
      "learning_rate": 2.538888888888889e-05,
      "loss": 0.0025,
      "step": 44300
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.27862903475761414,
      "learning_rate": 2.5383333333333338e-05,
      "loss": 0.0025,
      "step": 44310
    },
    {
      "epoch": 3.9395555555555557,
      "grad_norm": 0.26508569717407227,
      "learning_rate": 2.537777777777778e-05,
      "loss": 0.0018,
      "step": 44320
    },
    {
      "epoch": 3.940444444444444,
      "grad_norm": 0.8977441787719727,
      "learning_rate": 2.5372222222222225e-05,
      "loss": 0.0019,
      "step": 44330
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.046566423028707504,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0022,
      "step": 44340
    },
    {
      "epoch": 3.942222222222222,
      "grad_norm": 0.43330979347229004,
      "learning_rate": 2.5361111111111112e-05,
      "loss": 0.0021,
      "step": 44350
    },
    {
      "epoch": 3.943111111111111,
      "grad_norm": 0.26438358426094055,
      "learning_rate": 2.5355555555555556e-05,
      "loss": 0.003,
      "step": 44360
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.09548582136631012,
      "learning_rate": 2.5350000000000003e-05,
      "loss": 0.0021,
      "step": 44370
    },
    {
      "epoch": 3.944888888888889,
      "grad_norm": 0.5809574127197266,
      "learning_rate": 2.534444444444445e-05,
      "loss": 0.0023,
      "step": 44380
    },
    {
      "epoch": 3.945777777777778,
      "grad_norm": 0.811899721622467,
      "learning_rate": 2.533888888888889e-05,
      "loss": 0.0022,
      "step": 44390
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.3506629467010498,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0016,
      "step": 44400
    },
    {
      "epoch": 3.9475555555555557,
      "grad_norm": 0.6030062437057495,
      "learning_rate": 2.5327777777777777e-05,
      "loss": 0.0017,
      "step": 44410
    },
    {
      "epoch": 3.948444444444444,
      "grad_norm": 0.6371366381645203,
      "learning_rate": 2.5322222222222224e-05,
      "loss": 0.0027,
      "step": 44420
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.6144458055496216,
      "learning_rate": 2.5316666666666668e-05,
      "loss": 0.0024,
      "step": 44430
    },
    {
      "epoch": 3.950222222222222,
      "grad_norm": 0.5242959260940552,
      "learning_rate": 2.5311111111111115e-05,
      "loss": 0.0022,
      "step": 44440
    },
    {
      "epoch": 3.951111111111111,
      "grad_norm": 0.19201360642910004,
      "learning_rate": 2.5305555555555555e-05,
      "loss": 0.0031,
      "step": 44450
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.5239659547805786,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.0023,
      "step": 44460
    },
    {
      "epoch": 3.952888888888889,
      "grad_norm": 0.08136618137359619,
      "learning_rate": 2.529444444444445e-05,
      "loss": 0.0015,
      "step": 44470
    },
    {
      "epoch": 3.953777777777778,
      "grad_norm": 0.05341876670718193,
      "learning_rate": 2.528888888888889e-05,
      "loss": 0.0014,
      "step": 44480
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.9093087315559387,
      "learning_rate": 2.5283333333333336e-05,
      "loss": 0.0027,
      "step": 44490
    },
    {
      "epoch": 3.9555555555555557,
      "grad_norm": 0.27039214968681335,
      "learning_rate": 2.527777777777778e-05,
      "loss": 0.0017,
      "step": 44500
    },
    {
      "epoch": 3.956444444444444,
      "grad_norm": 0.07944092154502869,
      "learning_rate": 2.5272222222222226e-05,
      "loss": 0.0017,
      "step": 44510
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.5748375058174133,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0014,
      "step": 44520
    },
    {
      "epoch": 3.958222222222222,
      "grad_norm": 0.5672595500946045,
      "learning_rate": 2.5261111111111113e-05,
      "loss": 0.0033,
      "step": 44530
    },
    {
      "epoch": 3.959111111111111,
      "grad_norm": 0.31787437200546265,
      "learning_rate": 2.5255555555555554e-05,
      "loss": 0.0023,
      "step": 44540
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.2353440225124359,
      "learning_rate": 2.525e-05,
      "loss": 0.0027,
      "step": 44550
    },
    {
      "epoch": 3.960888888888889,
      "grad_norm": 0.18868693709373474,
      "learning_rate": 2.5244444444444447e-05,
      "loss": 0.0023,
      "step": 44560
    },
    {
      "epoch": 3.961777777777778,
      "grad_norm": 0.22022369503974915,
      "learning_rate": 2.523888888888889e-05,
      "loss": 0.0023,
      "step": 44570
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.3421764671802521,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0018,
      "step": 44580
    },
    {
      "epoch": 3.9635555555555557,
      "grad_norm": 0.1949453204870224,
      "learning_rate": 2.5227777777777778e-05,
      "loss": 0.0017,
      "step": 44590
    },
    {
      "epoch": 3.964444444444444,
      "grad_norm": 0.28840160369873047,
      "learning_rate": 2.5222222222222225e-05,
      "loss": 0.0023,
      "step": 44600
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.4241947829723358,
      "learning_rate": 2.5216666666666665e-05,
      "loss": 0.0021,
      "step": 44610
    },
    {
      "epoch": 3.966222222222222,
      "grad_norm": 0.9682043194770813,
      "learning_rate": 2.5211111111111112e-05,
      "loss": 0.0024,
      "step": 44620
    },
    {
      "epoch": 3.967111111111111,
      "grad_norm": 0.7364553213119507,
      "learning_rate": 2.5205555555555556e-05,
      "loss": 0.0017,
      "step": 44630
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.305777370929718,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0025,
      "step": 44640
    },
    {
      "epoch": 3.968888888888889,
      "grad_norm": 0.1558758020401001,
      "learning_rate": 2.519444444444445e-05,
      "loss": 0.0026,
      "step": 44650
    },
    {
      "epoch": 3.969777777777778,
      "grad_norm": 0.09174376726150513,
      "learning_rate": 2.518888888888889e-05,
      "loss": 0.0021,
      "step": 44660
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.04235042631626129,
      "learning_rate": 2.5183333333333337e-05,
      "loss": 0.0027,
      "step": 44670
    },
    {
      "epoch": 3.9715555555555557,
      "grad_norm": 0.6434504985809326,
      "learning_rate": 2.5177777777777777e-05,
      "loss": 0.0022,
      "step": 44680
    },
    {
      "epoch": 3.9724444444444442,
      "grad_norm": 0.5210513472557068,
      "learning_rate": 2.5172222222222224e-05,
      "loss": 0.0022,
      "step": 44690
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.1105409562587738,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.002,
      "step": 44700
    },
    {
      "epoch": 3.974222222222222,
      "grad_norm": 0.29741036891937256,
      "learning_rate": 2.5161111111111114e-05,
      "loss": 0.0015,
      "step": 44710
    },
    {
      "epoch": 3.975111111111111,
      "grad_norm": 0.20438550412654877,
      "learning_rate": 2.5155555555555555e-05,
      "loss": 0.0014,
      "step": 44720
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.5408371686935425,
      "learning_rate": 2.515e-05,
      "loss": 0.0014,
      "step": 44730
    },
    {
      "epoch": 3.976888888888889,
      "grad_norm": 0.13965457677841187,
      "learning_rate": 2.514444444444445e-05,
      "loss": 0.0019,
      "step": 44740
    },
    {
      "epoch": 3.977777777777778,
      "grad_norm": 0.126736119389534,
      "learning_rate": 2.513888888888889e-05,
      "loss": 0.002,
      "step": 44750
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.6738258600234985,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0023,
      "step": 44760
    },
    {
      "epoch": 3.9795555555555557,
      "grad_norm": 0.20012642443180084,
      "learning_rate": 2.512777777777778e-05,
      "loss": 0.0028,
      "step": 44770
    },
    {
      "epoch": 3.9804444444444442,
      "grad_norm": 0.9168828129768372,
      "learning_rate": 2.5122222222222226e-05,
      "loss": 0.0033,
      "step": 44780
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.5802211165428162,
      "learning_rate": 2.5116666666666666e-05,
      "loss": 0.0027,
      "step": 44790
    },
    {
      "epoch": 3.982222222222222,
      "grad_norm": 0.0839894562959671,
      "learning_rate": 2.5111111111111113e-05,
      "loss": 0.0023,
      "step": 44800
    },
    {
      "epoch": 3.983111111111111,
      "grad_norm": 0.26833802461624146,
      "learning_rate": 2.5105555555555553e-05,
      "loss": 0.002,
      "step": 44810
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.5559408664703369,
      "learning_rate": 2.51e-05,
      "loss": 0.0028,
      "step": 44820
    },
    {
      "epoch": 3.984888888888889,
      "grad_norm": 0.4098890721797943,
      "learning_rate": 2.5094444444444447e-05,
      "loss": 0.0022,
      "step": 44830
    },
    {
      "epoch": 3.985777777777778,
      "grad_norm": 0.3018019497394562,
      "learning_rate": 2.508888888888889e-05,
      "loss": 0.0017,
      "step": 44840
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.33640408515930176,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.0016,
      "step": 44850
    },
    {
      "epoch": 3.9875555555555557,
      "grad_norm": 0.35776951909065247,
      "learning_rate": 2.5077777777777778e-05,
      "loss": 0.0029,
      "step": 44860
    },
    {
      "epoch": 3.9884444444444442,
      "grad_norm": 0.37217429280281067,
      "learning_rate": 2.5072222222222225e-05,
      "loss": 0.0018,
      "step": 44870
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.3840535283088684,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0016,
      "step": 44880
    },
    {
      "epoch": 3.990222222222222,
      "grad_norm": 0.9613855481147766,
      "learning_rate": 2.5061111111111112e-05,
      "loss": 0.0021,
      "step": 44890
    },
    {
      "epoch": 3.991111111111111,
      "grad_norm": 1.1951336860656738,
      "learning_rate": 2.5055555555555555e-05,
      "loss": 0.003,
      "step": 44900
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.21493226289749146,
      "learning_rate": 2.5050000000000002e-05,
      "loss": 0.0023,
      "step": 44910
    },
    {
      "epoch": 3.992888888888889,
      "grad_norm": 0.042405866086483,
      "learning_rate": 2.504444444444445e-05,
      "loss": 0.0034,
      "step": 44920
    },
    {
      "epoch": 3.993777777777778,
      "grad_norm": 0.42690160870552063,
      "learning_rate": 2.503888888888889e-05,
      "loss": 0.0013,
      "step": 44930
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.026616454124450684,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.0025,
      "step": 44940
    },
    {
      "epoch": 3.9955555555555557,
      "grad_norm": 0.6483412384986877,
      "learning_rate": 2.5027777777777777e-05,
      "loss": 0.0016,
      "step": 44950
    },
    {
      "epoch": 3.9964444444444442,
      "grad_norm": 0.24865800142288208,
      "learning_rate": 2.5022222222222224e-05,
      "loss": 0.0023,
      "step": 44960
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.3875846564769745,
      "learning_rate": 2.5016666666666667e-05,
      "loss": 0.0019,
      "step": 44970
    },
    {
      "epoch": 3.998222222222222,
      "grad_norm": 0.31608566641807556,
      "learning_rate": 2.5011111111111114e-05,
      "loss": 0.0034,
      "step": 44980
    },
    {
      "epoch": 3.999111111111111,
      "grad_norm": 0.3356882631778717,
      "learning_rate": 2.5005555555555554e-05,
      "loss": 0.0024,
      "step": 44990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.17794550955295563,
      "learning_rate": 2.5e-05,
      "loss": 0.0024,
      "step": 45000
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.002090669935569167,
      "eval_runtime": 105.4741,
      "eval_samples_per_second": 1422.151,
      "eval_steps_per_second": 35.554,
      "step": 45000
    },
    {
      "epoch": 4.0008888888888885,
      "grad_norm": 0.19840475916862488,
      "learning_rate": 2.4994444444444445e-05,
      "loss": 0.0019,
      "step": 45010
    },
    {
      "epoch": 4.001777777777778,
      "grad_norm": 0.31774240732192993,
      "learning_rate": 2.498888888888889e-05,
      "loss": 0.0019,
      "step": 45020
    },
    {
      "epoch": 4.002666666666666,
      "grad_norm": 0.09938789159059525,
      "learning_rate": 2.4983333333333335e-05,
      "loss": 0.002,
      "step": 45030
    },
    {
      "epoch": 4.003555555555556,
      "grad_norm": 0.16036869585514069,
      "learning_rate": 2.497777777777778e-05,
      "loss": 0.0026,
      "step": 45040
    },
    {
      "epoch": 4.004444444444444,
      "grad_norm": 0.06424534320831299,
      "learning_rate": 2.4972222222222226e-05,
      "loss": 0.0024,
      "step": 45050
    },
    {
      "epoch": 4.005333333333334,
      "grad_norm": 0.1970643550157547,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0017,
      "step": 45060
    },
    {
      "epoch": 4.006222222222222,
      "grad_norm": 0.41205528378486633,
      "learning_rate": 2.4961111111111113e-05,
      "loss": 0.0021,
      "step": 45070
    },
    {
      "epoch": 4.0071111111111115,
      "grad_norm": 0.2884626090526581,
      "learning_rate": 2.4955555555555556e-05,
      "loss": 0.002,
      "step": 45080
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.6203818917274475,
      "learning_rate": 2.495e-05,
      "loss": 0.0014,
      "step": 45090
    },
    {
      "epoch": 4.0088888888888885,
      "grad_norm": 0.5903937816619873,
      "learning_rate": 2.4944444444444447e-05,
      "loss": 0.0023,
      "step": 45100
    },
    {
      "epoch": 4.009777777777778,
      "grad_norm": 0.1668342500925064,
      "learning_rate": 2.493888888888889e-05,
      "loss": 0.002,
      "step": 45110
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.24047885835170746,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0019,
      "step": 45120
    },
    {
      "epoch": 4.011555555555556,
      "grad_norm": 0.09939485043287277,
      "learning_rate": 2.4927777777777778e-05,
      "loss": 0.0032,
      "step": 45130
    },
    {
      "epoch": 4.012444444444444,
      "grad_norm": 0.41575658321380615,
      "learning_rate": 2.4922222222222225e-05,
      "loss": 0.0018,
      "step": 45140
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.06210174411535263,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.0028,
      "step": 45150
    },
    {
      "epoch": 4.014222222222222,
      "grad_norm": 0.4494372606277466,
      "learning_rate": 2.491111111111111e-05,
      "loss": 0.0022,
      "step": 45160
    },
    {
      "epoch": 4.0151111111111115,
      "grad_norm": 0.34996190667152405,
      "learning_rate": 2.490555555555556e-05,
      "loss": 0.0023,
      "step": 45170
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.5799482464790344,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0018,
      "step": 45180
    },
    {
      "epoch": 4.0168888888888885,
      "grad_norm": 0.5496668815612793,
      "learning_rate": 2.4894444444444446e-05,
      "loss": 0.0023,
      "step": 45190
    },
    {
      "epoch": 4.017777777777778,
      "grad_norm": 0.4968991279602051,
      "learning_rate": 2.488888888888889e-05,
      "loss": 0.0024,
      "step": 45200
    },
    {
      "epoch": 4.018666666666666,
      "grad_norm": 0.5618623495101929,
      "learning_rate": 2.4883333333333333e-05,
      "loss": 0.0029,
      "step": 45210
    },
    {
      "epoch": 4.019555555555556,
      "grad_norm": 0.38866519927978516,
      "learning_rate": 2.4877777777777776e-05,
      "loss": 0.0022,
      "step": 45220
    },
    {
      "epoch": 4.020444444444444,
      "grad_norm": 0.5208715200424194,
      "learning_rate": 2.4872222222222223e-05,
      "loss": 0.0027,
      "step": 45230
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.05623582378029823,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0017,
      "step": 45240
    },
    {
      "epoch": 4.022222222222222,
      "grad_norm": 0.19844593107700348,
      "learning_rate": 2.4861111111111114e-05,
      "loss": 0.002,
      "step": 45250
    },
    {
      "epoch": 4.0231111111111115,
      "grad_norm": 0.7135992050170898,
      "learning_rate": 2.4855555555555557e-05,
      "loss": 0.0018,
      "step": 45260
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.08830289542675018,
      "learning_rate": 2.485e-05,
      "loss": 0.0023,
      "step": 45270
    },
    {
      "epoch": 4.0248888888888885,
      "grad_norm": 0.3769823908805847,
      "learning_rate": 2.4844444444444444e-05,
      "loss": 0.002,
      "step": 45280
    },
    {
      "epoch": 4.025777777777778,
      "grad_norm": 0.18793542683124542,
      "learning_rate": 2.4838888888888888e-05,
      "loss": 0.0023,
      "step": 45290
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.712320864200592,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0018,
      "step": 45300
    },
    {
      "epoch": 4.027555555555556,
      "grad_norm": 1.0666812658309937,
      "learning_rate": 2.482777777777778e-05,
      "loss": 0.0022,
      "step": 45310
    },
    {
      "epoch": 4.028444444444444,
      "grad_norm": 0.06023912504315376,
      "learning_rate": 2.4822222222222225e-05,
      "loss": 0.0026,
      "step": 45320
    },
    {
      "epoch": 4.029333333333334,
      "grad_norm": 0.5376054048538208,
      "learning_rate": 2.481666666666667e-05,
      "loss": 0.0019,
      "step": 45330
    },
    {
      "epoch": 4.030222222222222,
      "grad_norm": 0.42101922631263733,
      "learning_rate": 2.4811111111111113e-05,
      "loss": 0.0018,
      "step": 45340
    },
    {
      "epoch": 4.0311111111111115,
      "grad_norm": 0.5606591105461121,
      "learning_rate": 2.4805555555555556e-05,
      "loss": 0.0023,
      "step": 45350
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.3550322949886322,
      "learning_rate": 2.48e-05,
      "loss": 0.0038,
      "step": 45360
    },
    {
      "epoch": 4.0328888888888885,
      "grad_norm": 0.18968692421913147,
      "learning_rate": 2.4794444444444447e-05,
      "loss": 0.0028,
      "step": 45370
    },
    {
      "epoch": 4.033777777777778,
      "grad_norm": 0.1585494875907898,
      "learning_rate": 2.478888888888889e-05,
      "loss": 0.0018,
      "step": 45380
    },
    {
      "epoch": 4.034666666666666,
      "grad_norm": 0.10731971263885498,
      "learning_rate": 2.4783333333333334e-05,
      "loss": 0.0024,
      "step": 45390
    },
    {
      "epoch": 4.035555555555556,
      "grad_norm": 0.39734387397766113,
      "learning_rate": 2.477777777777778e-05,
      "loss": 0.0036,
      "step": 45400
    },
    {
      "epoch": 4.036444444444444,
      "grad_norm": 0.15695181488990784,
      "learning_rate": 2.4772222222222224e-05,
      "loss": 0.0027,
      "step": 45410
    },
    {
      "epoch": 4.037333333333334,
      "grad_norm": 0.40554341673851013,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0022,
      "step": 45420
    },
    {
      "epoch": 4.038222222222222,
      "grad_norm": 0.11776239424943924,
      "learning_rate": 2.476111111111111e-05,
      "loss": 0.0015,
      "step": 45430
    },
    {
      "epoch": 4.0391111111111115,
      "grad_norm": 0.35109445452690125,
      "learning_rate": 2.475555555555556e-05,
      "loss": 0.0027,
      "step": 45440
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.464429646730423,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.0015,
      "step": 45450
    },
    {
      "epoch": 4.0408888888888885,
      "grad_norm": 0.21570605039596558,
      "learning_rate": 2.4744444444444445e-05,
      "loss": 0.0021,
      "step": 45460
    },
    {
      "epoch": 4.041777777777778,
      "grad_norm": 0.26307183504104614,
      "learning_rate": 2.473888888888889e-05,
      "loss": 0.0017,
      "step": 45470
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.3716288208961487,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0019,
      "step": 45480
    },
    {
      "epoch": 4.043555555555556,
      "grad_norm": 0.30916038155555725,
      "learning_rate": 2.472777777777778e-05,
      "loss": 0.002,
      "step": 45490
    },
    {
      "epoch": 4.044444444444444,
      "grad_norm": 0.3618062138557434,
      "learning_rate": 2.4722222222222223e-05,
      "loss": 0.0025,
      "step": 45500
    },
    {
      "epoch": 4.045333333333334,
      "grad_norm": 0.572924792766571,
      "learning_rate": 2.471666666666667e-05,
      "loss": 0.0026,
      "step": 45510
    },
    {
      "epoch": 4.046222222222222,
      "grad_norm": 0.4359497129917145,
      "learning_rate": 2.4711111111111114e-05,
      "loss": 0.0017,
      "step": 45520
    },
    {
      "epoch": 4.0471111111111115,
      "grad_norm": 0.05799146369099617,
      "learning_rate": 2.4705555555555557e-05,
      "loss": 0.002,
      "step": 45530
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.8976823091506958,
      "learning_rate": 2.47e-05,
      "loss": 0.0026,
      "step": 45540
    },
    {
      "epoch": 4.0488888888888885,
      "grad_norm": 0.25925779342651367,
      "learning_rate": 2.4694444444444444e-05,
      "loss": 0.0017,
      "step": 45550
    },
    {
      "epoch": 4.049777777777778,
      "grad_norm": 0.27219104766845703,
      "learning_rate": 2.4688888888888888e-05,
      "loss": 0.0032,
      "step": 45560
    },
    {
      "epoch": 4.050666666666666,
      "grad_norm": 0.1714949756860733,
      "learning_rate": 2.4683333333333335e-05,
      "loss": 0.0023,
      "step": 45570
    },
    {
      "epoch": 4.051555555555556,
      "grad_norm": 0.27546167373657227,
      "learning_rate": 2.467777777777778e-05,
      "loss": 0.0024,
      "step": 45580
    },
    {
      "epoch": 4.052444444444444,
      "grad_norm": 0.5441623330116272,
      "learning_rate": 2.4672222222222225e-05,
      "loss": 0.0015,
      "step": 45590
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.35587117075920105,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0027,
      "step": 45600
    },
    {
      "epoch": 4.054222222222222,
      "grad_norm": 0.1490008383989334,
      "learning_rate": 2.4661111111111112e-05,
      "loss": 0.0027,
      "step": 45610
    },
    {
      "epoch": 4.0551111111111116,
      "grad_norm": 0.6642298102378845,
      "learning_rate": 2.4655555555555556e-05,
      "loss": 0.0021,
      "step": 45620
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.22866669297218323,
      "learning_rate": 2.465e-05,
      "loss": 0.0021,
      "step": 45630
    },
    {
      "epoch": 4.0568888888888885,
      "grad_norm": 0.03862936422228813,
      "learning_rate": 2.4644444444444446e-05,
      "loss": 0.002,
      "step": 45640
    },
    {
      "epoch": 4.057777777777778,
      "grad_norm": 0.7679418921470642,
      "learning_rate": 2.463888888888889e-05,
      "loss": 0.0024,
      "step": 45650
    },
    {
      "epoch": 4.058666666666666,
      "grad_norm": 0.7744045853614807,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.0019,
      "step": 45660
    },
    {
      "epoch": 4.059555555555556,
      "grad_norm": 0.07988210022449493,
      "learning_rate": 2.462777777777778e-05,
      "loss": 0.0024,
      "step": 45670
    },
    {
      "epoch": 4.060444444444444,
      "grad_norm": 0.17157019674777985,
      "learning_rate": 2.4622222222222224e-05,
      "loss": 0.0017,
      "step": 45680
    },
    {
      "epoch": 4.061333333333334,
      "grad_norm": 0.34100866317749023,
      "learning_rate": 2.4616666666666668e-05,
      "loss": 0.002,
      "step": 45690
    },
    {
      "epoch": 4.062222222222222,
      "grad_norm": 0.19071674346923828,
      "learning_rate": 2.461111111111111e-05,
      "loss": 0.0018,
      "step": 45700
    },
    {
      "epoch": 4.063111111111111,
      "grad_norm": 0.2746843993663788,
      "learning_rate": 2.4605555555555558e-05,
      "loss": 0.0028,
      "step": 45710
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.19400811195373535,
      "learning_rate": 2.46e-05,
      "loss": 0.0021,
      "step": 45720
    },
    {
      "epoch": 4.0648888888888886,
      "grad_norm": 0.3118475079536438,
      "learning_rate": 2.4594444444444445e-05,
      "loss": 0.0027,
      "step": 45730
    },
    {
      "epoch": 4.065777777777778,
      "grad_norm": 0.16831310093402863,
      "learning_rate": 2.458888888888889e-05,
      "loss": 0.0018,
      "step": 45740
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.5173312425613403,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.0021,
      "step": 45750
    },
    {
      "epoch": 4.067555555555556,
      "grad_norm": 0.04295472055673599,
      "learning_rate": 2.457777777777778e-05,
      "loss": 0.0016,
      "step": 45760
    },
    {
      "epoch": 4.068444444444444,
      "grad_norm": 0.27782270312309265,
      "learning_rate": 2.4572222222222223e-05,
      "loss": 0.0024,
      "step": 45770
    },
    {
      "epoch": 4.069333333333334,
      "grad_norm": 0.23337365686893463,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0022,
      "step": 45780
    },
    {
      "epoch": 4.070222222222222,
      "grad_norm": 0.5255124568939209,
      "learning_rate": 2.4561111111111113e-05,
      "loss": 0.0024,
      "step": 45790
    },
    {
      "epoch": 4.071111111111111,
      "grad_norm": 0.5070792436599731,
      "learning_rate": 2.4555555555555557e-05,
      "loss": 0.0016,
      "step": 45800
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.5077335834503174,
      "learning_rate": 2.455e-05,
      "loss": 0.0023,
      "step": 45810
    },
    {
      "epoch": 4.072888888888889,
      "grad_norm": 0.09103946387767792,
      "learning_rate": 2.4544444444444444e-05,
      "loss": 0.002,
      "step": 45820
    },
    {
      "epoch": 4.073777777777778,
      "grad_norm": 0.187495157122612,
      "learning_rate": 2.4538888888888888e-05,
      "loss": 0.0023,
      "step": 45830
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.07916262745857239,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0022,
      "step": 45840
    },
    {
      "epoch": 4.075555555555556,
      "grad_norm": 0.6009195446968079,
      "learning_rate": 2.452777777777778e-05,
      "loss": 0.0021,
      "step": 45850
    },
    {
      "epoch": 4.076444444444444,
      "grad_norm": 0.5944992899894714,
      "learning_rate": 2.4522222222222225e-05,
      "loss": 0.0025,
      "step": 45860
    },
    {
      "epoch": 4.077333333333334,
      "grad_norm": 0.26683399081230164,
      "learning_rate": 2.451666666666667e-05,
      "loss": 0.0019,
      "step": 45870
    },
    {
      "epoch": 4.078222222222222,
      "grad_norm": 0.41291704773902893,
      "learning_rate": 2.4511111111111112e-05,
      "loss": 0.002,
      "step": 45880
    },
    {
      "epoch": 4.079111111111111,
      "grad_norm": 0.1290285587310791,
      "learning_rate": 2.4505555555555556e-05,
      "loss": 0.0016,
      "step": 45890
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.341093510389328,
      "learning_rate": 2.45e-05,
      "loss": 0.0023,
      "step": 45900
    },
    {
      "epoch": 4.080888888888889,
      "grad_norm": 0.22098839282989502,
      "learning_rate": 2.4494444444444446e-05,
      "loss": 0.0021,
      "step": 45910
    },
    {
      "epoch": 4.081777777777778,
      "grad_norm": 0.42775261402130127,
      "learning_rate": 2.448888888888889e-05,
      "loss": 0.0033,
      "step": 45920
    },
    {
      "epoch": 4.082666666666666,
      "grad_norm": 0.1173143982887268,
      "learning_rate": 2.4483333333333333e-05,
      "loss": 0.0018,
      "step": 45930
    },
    {
      "epoch": 4.083555555555556,
      "grad_norm": 0.5378626585006714,
      "learning_rate": 2.447777777777778e-05,
      "loss": 0.0023,
      "step": 45940
    },
    {
      "epoch": 4.084444444444444,
      "grad_norm": 0.23585547506809235,
      "learning_rate": 2.4472222222222224e-05,
      "loss": 0.003,
      "step": 45950
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.42582687735557556,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0018,
      "step": 45960
    },
    {
      "epoch": 4.086222222222222,
      "grad_norm": 0.40977686643600464,
      "learning_rate": 2.446111111111111e-05,
      "loss": 0.0014,
      "step": 45970
    },
    {
      "epoch": 4.087111111111111,
      "grad_norm": 0.3347458243370056,
      "learning_rate": 2.4455555555555558e-05,
      "loss": 0.0027,
      "step": 45980
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.48990434408187866,
      "learning_rate": 2.445e-05,
      "loss": 0.0015,
      "step": 45990
    },
    {
      "epoch": 4.088888888888889,
      "grad_norm": 0.19115249812602997,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.0025,
      "step": 46000
    },
    {
      "epoch": 4.089777777777778,
      "grad_norm": 0.46743783354759216,
      "learning_rate": 2.443888888888889e-05,
      "loss": 0.0023,
      "step": 46010
    },
    {
      "epoch": 4.0906666666666665,
      "grad_norm": 0.1792895793914795,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0021,
      "step": 46020
    },
    {
      "epoch": 4.091555555555556,
      "grad_norm": 0.25669923424720764,
      "learning_rate": 2.442777777777778e-05,
      "loss": 0.0018,
      "step": 46030
    },
    {
      "epoch": 4.092444444444444,
      "grad_norm": 0.05716438964009285,
      "learning_rate": 2.4422222222222223e-05,
      "loss": 0.0016,
      "step": 46040
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.2852041721343994,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.002,
      "step": 46050
    },
    {
      "epoch": 4.094222222222222,
      "grad_norm": 0.22453410923480988,
      "learning_rate": 2.4411111111111113e-05,
      "loss": 0.0018,
      "step": 46060
    },
    {
      "epoch": 4.095111111111111,
      "grad_norm": 0.37086060643196106,
      "learning_rate": 2.4405555555555557e-05,
      "loss": 0.0018,
      "step": 46070
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.2516094744205475,
      "learning_rate": 2.44e-05,
      "loss": 0.0027,
      "step": 46080
    },
    {
      "epoch": 4.096888888888889,
      "grad_norm": 0.8952831625938416,
      "learning_rate": 2.4394444444444444e-05,
      "loss": 0.0021,
      "step": 46090
    },
    {
      "epoch": 4.097777777777778,
      "grad_norm": 0.5747898817062378,
      "learning_rate": 2.4388888888888887e-05,
      "loss": 0.0025,
      "step": 46100
    },
    {
      "epoch": 4.0986666666666665,
      "grad_norm": 0.18451856076717377,
      "learning_rate": 2.4383333333333334e-05,
      "loss": 0.0021,
      "step": 46110
    },
    {
      "epoch": 4.099555555555556,
      "grad_norm": 0.1261616200208664,
      "learning_rate": 2.437777777777778e-05,
      "loss": 0.0013,
      "step": 46120
    },
    {
      "epoch": 4.100444444444444,
      "grad_norm": 0.5484322309494019,
      "learning_rate": 2.4372222222222225e-05,
      "loss": 0.0026,
      "step": 46130
    },
    {
      "epoch": 4.101333333333334,
      "grad_norm": 0.2679015100002289,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.0022,
      "step": 46140
    },
    {
      "epoch": 4.102222222222222,
      "grad_norm": 0.19589193165302277,
      "learning_rate": 2.4361111111111112e-05,
      "loss": 0.0023,
      "step": 46150
    },
    {
      "epoch": 4.103111111111111,
      "grad_norm": 0.05078611150383949,
      "learning_rate": 2.4355555555555555e-05,
      "loss": 0.0027,
      "step": 46160
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.5312234163284302,
      "learning_rate": 2.435e-05,
      "loss": 0.0022,
      "step": 46170
    },
    {
      "epoch": 4.104888888888889,
      "grad_norm": 0.7167138457298279,
      "learning_rate": 2.4344444444444446e-05,
      "loss": 0.0016,
      "step": 46180
    },
    {
      "epoch": 4.105777777777778,
      "grad_norm": 0.6907919645309448,
      "learning_rate": 2.433888888888889e-05,
      "loss": 0.0025,
      "step": 46190
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.3025161325931549,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0039,
      "step": 46200
    },
    {
      "epoch": 4.107555555555556,
      "grad_norm": 0.16458186507225037,
      "learning_rate": 2.432777777777778e-05,
      "loss": 0.0028,
      "step": 46210
    },
    {
      "epoch": 4.108444444444444,
      "grad_norm": 0.2725423276424408,
      "learning_rate": 2.4322222222222224e-05,
      "loss": 0.0026,
      "step": 46220
    },
    {
      "epoch": 4.109333333333334,
      "grad_norm": 0.23084521293640137,
      "learning_rate": 2.4316666666666667e-05,
      "loss": 0.0029,
      "step": 46230
    },
    {
      "epoch": 4.110222222222222,
      "grad_norm": 0.29215267300605774,
      "learning_rate": 2.431111111111111e-05,
      "loss": 0.0021,
      "step": 46240
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.10505550354719162,
      "learning_rate": 2.4305555555555558e-05,
      "loss": 0.0037,
      "step": 46250
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.06301648169755936,
      "learning_rate": 2.43e-05,
      "loss": 0.0019,
      "step": 46260
    },
    {
      "epoch": 4.112888888888889,
      "grad_norm": 0.3597151041030884,
      "learning_rate": 2.4294444444444445e-05,
      "loss": 0.0028,
      "step": 46270
    },
    {
      "epoch": 4.113777777777778,
      "grad_norm": 0.36871084570884705,
      "learning_rate": 2.4288888888888888e-05,
      "loss": 0.003,
      "step": 46280
    },
    {
      "epoch": 4.1146666666666665,
      "grad_norm": 0.1698710173368454,
      "learning_rate": 2.4283333333333335e-05,
      "loss": 0.0019,
      "step": 46290
    },
    {
      "epoch": 4.115555555555556,
      "grad_norm": 0.4893629848957062,
      "learning_rate": 2.427777777777778e-05,
      "loss": 0.0014,
      "step": 46300
    },
    {
      "epoch": 4.116444444444444,
      "grad_norm": 0.6372860074043274,
      "learning_rate": 2.4272222222222222e-05,
      "loss": 0.0025,
      "step": 46310
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.20061177015304565,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0028,
      "step": 46320
    },
    {
      "epoch": 4.118222222222222,
      "grad_norm": 0.5286858677864075,
      "learning_rate": 2.4261111111111113e-05,
      "loss": 0.0016,
      "step": 46330
    },
    {
      "epoch": 4.119111111111111,
      "grad_norm": 0.10085516422986984,
      "learning_rate": 2.4255555555555556e-05,
      "loss": 0.0018,
      "step": 46340
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.30979809165000916,
      "learning_rate": 2.425e-05,
      "loss": 0.002,
      "step": 46350
    },
    {
      "epoch": 4.120888888888889,
      "grad_norm": 0.2035818248987198,
      "learning_rate": 2.4244444444444443e-05,
      "loss": 0.0024,
      "step": 46360
    },
    {
      "epoch": 4.121777777777778,
      "grad_norm": 0.8328729867935181,
      "learning_rate": 2.423888888888889e-05,
      "loss": 0.0024,
      "step": 46370
    },
    {
      "epoch": 4.1226666666666665,
      "grad_norm": 0.4123920500278473,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0028,
      "step": 46380
    },
    {
      "epoch": 4.123555555555556,
      "grad_norm": 0.08166348189115524,
      "learning_rate": 2.422777777777778e-05,
      "loss": 0.0016,
      "step": 46390
    },
    {
      "epoch": 4.124444444444444,
      "grad_norm": 0.28463155031204224,
      "learning_rate": 2.4222222222222224e-05,
      "loss": 0.0019,
      "step": 46400
    },
    {
      "epoch": 4.125333333333334,
      "grad_norm": 0.5580652952194214,
      "learning_rate": 2.4216666666666668e-05,
      "loss": 0.002,
      "step": 46410
    },
    {
      "epoch": 4.126222222222222,
      "grad_norm": 0.5173506140708923,
      "learning_rate": 2.421111111111111e-05,
      "loss": 0.0017,
      "step": 46420
    },
    {
      "epoch": 4.127111111111111,
      "grad_norm": 0.07866605371236801,
      "learning_rate": 2.4205555555555555e-05,
      "loss": 0.0027,
      "step": 46430
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.05028954893350601,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.002,
      "step": 46440
    },
    {
      "epoch": 4.128888888888889,
      "grad_norm": 0.3892258107662201,
      "learning_rate": 2.4194444444444446e-05,
      "loss": 0.0015,
      "step": 46450
    },
    {
      "epoch": 4.129777777777778,
      "grad_norm": 0.2644950747489929,
      "learning_rate": 2.418888888888889e-05,
      "loss": 0.0024,
      "step": 46460
    },
    {
      "epoch": 4.1306666666666665,
      "grad_norm": 0.11480624228715897,
      "learning_rate": 2.4183333333333336e-05,
      "loss": 0.002,
      "step": 46470
    },
    {
      "epoch": 4.131555555555556,
      "grad_norm": 0.43049150705337524,
      "learning_rate": 2.417777777777778e-05,
      "loss": 0.0024,
      "step": 46480
    },
    {
      "epoch": 4.132444444444444,
      "grad_norm": 0.14787094295024872,
      "learning_rate": 2.4172222222222223e-05,
      "loss": 0.0025,
      "step": 46490
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.32946091890335083,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0018,
      "step": 46500
    },
    {
      "epoch": 4.134222222222222,
      "grad_norm": 0.3683483898639679,
      "learning_rate": 2.4161111111111114e-05,
      "loss": 0.0017,
      "step": 46510
    },
    {
      "epoch": 4.135111111111111,
      "grad_norm": 0.2366374284029007,
      "learning_rate": 2.4155555555555557e-05,
      "loss": 0.0016,
      "step": 46520
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.47609537839889526,
      "learning_rate": 2.415e-05,
      "loss": 0.0021,
      "step": 46530
    },
    {
      "epoch": 4.136888888888889,
      "grad_norm": 0.41606032848358154,
      "learning_rate": 2.4144444444444444e-05,
      "loss": 0.0024,
      "step": 46540
    },
    {
      "epoch": 4.137777777777778,
      "grad_norm": 0.48310741782188416,
      "learning_rate": 2.4138888888888888e-05,
      "loss": 0.0021,
      "step": 46550
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.7007115483283997,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0019,
      "step": 46560
    },
    {
      "epoch": 4.139555555555556,
      "grad_norm": 0.8247460126876831,
      "learning_rate": 2.412777777777778e-05,
      "loss": 0.0021,
      "step": 46570
    },
    {
      "epoch": 4.140444444444444,
      "grad_norm": 0.35235434770584106,
      "learning_rate": 2.4122222222222225e-05,
      "loss": 0.0017,
      "step": 46580
    },
    {
      "epoch": 4.141333333333334,
      "grad_norm": 0.2795322835445404,
      "learning_rate": 2.411666666666667e-05,
      "loss": 0.002,
      "step": 46590
    },
    {
      "epoch": 4.142222222222222,
      "grad_norm": 0.4129998981952667,
      "learning_rate": 2.4111111111111113e-05,
      "loss": 0.0014,
      "step": 46600
    },
    {
      "epoch": 4.143111111111111,
      "grad_norm": 1.1017557382583618,
      "learning_rate": 2.4105555555555556e-05,
      "loss": 0.0026,
      "step": 46610
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.48643994331359863,
      "learning_rate": 2.41e-05,
      "loss": 0.0022,
      "step": 46620
    },
    {
      "epoch": 4.144888888888889,
      "grad_norm": 0.6428787112236023,
      "learning_rate": 2.4094444444444443e-05,
      "loss": 0.0019,
      "step": 46630
    },
    {
      "epoch": 4.145777777777778,
      "grad_norm": 0.262446790933609,
      "learning_rate": 2.408888888888889e-05,
      "loss": 0.0013,
      "step": 46640
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.62725430727005,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.0025,
      "step": 46650
    },
    {
      "epoch": 4.147555555555556,
      "grad_norm": 0.3806087076663971,
      "learning_rate": 2.407777777777778e-05,
      "loss": 0.0035,
      "step": 46660
    },
    {
      "epoch": 4.148444444444444,
      "grad_norm": 0.03373028710484505,
      "learning_rate": 2.4072222222222224e-05,
      "loss": 0.0023,
      "step": 46670
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.19172179698944092,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0013,
      "step": 46680
    },
    {
      "epoch": 4.150222222222222,
      "grad_norm": 0.04264175891876221,
      "learning_rate": 2.406111111111111e-05,
      "loss": 0.0019,
      "step": 46690
    },
    {
      "epoch": 4.151111111111111,
      "grad_norm": 0.8435454368591309,
      "learning_rate": 2.4055555555555555e-05,
      "loss": 0.0016,
      "step": 46700
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.10445498675107956,
      "learning_rate": 2.4050000000000002e-05,
      "loss": 0.0024,
      "step": 46710
    },
    {
      "epoch": 4.152888888888889,
      "grad_norm": 0.43917879462242126,
      "learning_rate": 2.4044444444444445e-05,
      "loss": 0.002,
      "step": 46720
    },
    {
      "epoch": 4.153777777777778,
      "grad_norm": 0.6025636196136475,
      "learning_rate": 2.4038888888888892e-05,
      "loss": 0.0019,
      "step": 46730
    },
    {
      "epoch": 4.1546666666666665,
      "grad_norm": 0.18421664834022522,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0022,
      "step": 46740
    },
    {
      "epoch": 4.155555555555556,
      "grad_norm": 0.06234537810087204,
      "learning_rate": 2.402777777777778e-05,
      "loss": 0.002,
      "step": 46750
    },
    {
      "epoch": 4.156444444444444,
      "grad_norm": 0.200001522898674,
      "learning_rate": 2.4022222222222223e-05,
      "loss": 0.0027,
      "step": 46760
    },
    {
      "epoch": 4.157333333333334,
      "grad_norm": 0.15263709425926208,
      "learning_rate": 2.4016666666666667e-05,
      "loss": 0.0025,
      "step": 46770
    },
    {
      "epoch": 4.158222222222222,
      "grad_norm": 0.4328269064426422,
      "learning_rate": 2.4011111111111113e-05,
      "loss": 0.0019,
      "step": 46780
    },
    {
      "epoch": 4.159111111111111,
      "grad_norm": 0.04713166505098343,
      "learning_rate": 2.4005555555555557e-05,
      "loss": 0.0023,
      "step": 46790
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.48504742980003357,
      "learning_rate": 2.4e-05,
      "loss": 0.0019,
      "step": 46800
    },
    {
      "epoch": 4.160888888888889,
      "grad_norm": 0.6093401312828064,
      "learning_rate": 2.3994444444444444e-05,
      "loss": 0.0014,
      "step": 46810
    },
    {
      "epoch": 4.161777777777778,
      "grad_norm": 0.4077356457710266,
      "learning_rate": 2.398888888888889e-05,
      "loss": 0.0016,
      "step": 46820
    },
    {
      "epoch": 4.1626666666666665,
      "grad_norm": 0.26040709018707275,
      "learning_rate": 2.3983333333333335e-05,
      "loss": 0.0017,
      "step": 46830
    },
    {
      "epoch": 4.163555555555556,
      "grad_norm": 0.5970165133476257,
      "learning_rate": 2.3977777777777778e-05,
      "loss": 0.0024,
      "step": 46840
    },
    {
      "epoch": 4.164444444444444,
      "grad_norm": 0.15139314532279968,
      "learning_rate": 2.3972222222222225e-05,
      "loss": 0.0016,
      "step": 46850
    },
    {
      "epoch": 4.165333333333333,
      "grad_norm": 0.16118966042995453,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0023,
      "step": 46860
    },
    {
      "epoch": 4.166222222222222,
      "grad_norm": 0.13069778680801392,
      "learning_rate": 2.3961111111111112e-05,
      "loss": 0.0021,
      "step": 46870
    },
    {
      "epoch": 4.167111111111111,
      "grad_norm": 0.267872154712677,
      "learning_rate": 2.3955555555555556e-05,
      "loss": 0.0023,
      "step": 46880
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.049553729593753815,
      "learning_rate": 2.395e-05,
      "loss": 0.0013,
      "step": 46890
    },
    {
      "epoch": 4.168888888888889,
      "grad_norm": 0.5704399347305298,
      "learning_rate": 2.3944444444444443e-05,
      "loss": 0.0021,
      "step": 46900
    },
    {
      "epoch": 4.169777777777778,
      "grad_norm": 0.05272866040468216,
      "learning_rate": 2.393888888888889e-05,
      "loss": 0.0025,
      "step": 46910
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.0441768579185009,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0026,
      "step": 46920
    },
    {
      "epoch": 4.171555555555556,
      "grad_norm": 0.13845862448215485,
      "learning_rate": 2.392777777777778e-05,
      "loss": 0.002,
      "step": 46930
    },
    {
      "epoch": 4.172444444444444,
      "grad_norm": 0.0783776044845581,
      "learning_rate": 2.3922222222222224e-05,
      "loss": 0.0021,
      "step": 46940
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.6003516316413879,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.0018,
      "step": 46950
    },
    {
      "epoch": 4.174222222222222,
      "grad_norm": 0.07989975064992905,
      "learning_rate": 2.391111111111111e-05,
      "loss": 0.0017,
      "step": 46960
    },
    {
      "epoch": 4.175111111111111,
      "grad_norm": 0.7539678812026978,
      "learning_rate": 2.3905555555555555e-05,
      "loss": 0.0017,
      "step": 46970
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.7487562298774719,
      "learning_rate": 2.39e-05,
      "loss": 0.0019,
      "step": 46980
    },
    {
      "epoch": 4.176888888888889,
      "grad_norm": 0.2168848216533661,
      "learning_rate": 2.3894444444444445e-05,
      "loss": 0.0025,
      "step": 46990
    },
    {
      "epoch": 4.177777777777778,
      "grad_norm": 0.27355167269706726,
      "learning_rate": 2.3888888888888892e-05,
      "loss": 0.002,
      "step": 47000
    },
    {
      "epoch": 4.1786666666666665,
      "grad_norm": 0.36948657035827637,
      "learning_rate": 2.3883333333333336e-05,
      "loss": 0.0019,
      "step": 47010
    },
    {
      "epoch": 4.179555555555556,
      "grad_norm": 0.1814420223236084,
      "learning_rate": 2.387777777777778e-05,
      "loss": 0.0013,
      "step": 47020
    },
    {
      "epoch": 4.180444444444444,
      "grad_norm": 0.8275425434112549,
      "learning_rate": 2.3872222222222223e-05,
      "loss": 0.0025,
      "step": 47030
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.044618476182222366,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0024,
      "step": 47040
    },
    {
      "epoch": 4.182222222222222,
      "grad_norm": 0.2615605592727661,
      "learning_rate": 2.3861111111111113e-05,
      "loss": 0.0019,
      "step": 47050
    },
    {
      "epoch": 4.183111111111111,
      "grad_norm": 0.2225060611963272,
      "learning_rate": 2.3855555555555557e-05,
      "loss": 0.0029,
      "step": 47060
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.22366167604923248,
      "learning_rate": 2.385e-05,
      "loss": 0.0017,
      "step": 47070
    },
    {
      "epoch": 4.184888888888889,
      "grad_norm": 0.2611437439918518,
      "learning_rate": 2.3844444444444444e-05,
      "loss": 0.0021,
      "step": 47080
    },
    {
      "epoch": 4.185777777777778,
      "grad_norm": 0.4907490909099579,
      "learning_rate": 2.383888888888889e-05,
      "loss": 0.0021,
      "step": 47090
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.5162781476974487,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.002,
      "step": 47100
    },
    {
      "epoch": 4.187555555555556,
      "grad_norm": 0.5747088193893433,
      "learning_rate": 2.3827777777777778e-05,
      "loss": 0.0017,
      "step": 47110
    },
    {
      "epoch": 4.188444444444444,
      "grad_norm": 0.6553569436073303,
      "learning_rate": 2.3822222222222225e-05,
      "loss": 0.0019,
      "step": 47120
    },
    {
      "epoch": 4.189333333333333,
      "grad_norm": 0.7428122758865356,
      "learning_rate": 2.381666666666667e-05,
      "loss": 0.0031,
      "step": 47130
    },
    {
      "epoch": 4.190222222222222,
      "grad_norm": 0.0543300025165081,
      "learning_rate": 2.3811111111111112e-05,
      "loss": 0.0021,
      "step": 47140
    },
    {
      "epoch": 4.191111111111111,
      "grad_norm": 0.4949686527252197,
      "learning_rate": 2.3805555555555556e-05,
      "loss": 0.0019,
      "step": 47150
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.12240967899560928,
      "learning_rate": 2.38e-05,
      "loss": 0.0027,
      "step": 47160
    },
    {
      "epoch": 4.192888888888889,
      "grad_norm": 0.12133436650037766,
      "learning_rate": 2.3794444444444443e-05,
      "loss": 0.0017,
      "step": 47170
    },
    {
      "epoch": 4.193777777777778,
      "grad_norm": 0.37290287017822266,
      "learning_rate": 2.378888888888889e-05,
      "loss": 0.0016,
      "step": 47180
    },
    {
      "epoch": 4.1946666666666665,
      "grad_norm": 0.24156679213047028,
      "learning_rate": 2.3783333333333337e-05,
      "loss": 0.0023,
      "step": 47190
    },
    {
      "epoch": 4.195555555555556,
      "grad_norm": 0.4197899103164673,
      "learning_rate": 2.377777777777778e-05,
      "loss": 0.002,
      "step": 47200
    },
    {
      "epoch": 4.196444444444444,
      "grad_norm": 0.61537766456604,
      "learning_rate": 2.3772222222222224e-05,
      "loss": 0.0017,
      "step": 47210
    },
    {
      "epoch": 4.197333333333333,
      "grad_norm": 0.7580690979957581,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0032,
      "step": 47220
    },
    {
      "epoch": 4.198222222222222,
      "grad_norm": 0.8012025356292725,
      "learning_rate": 2.376111111111111e-05,
      "loss": 0.0018,
      "step": 47230
    },
    {
      "epoch": 4.199111111111111,
      "grad_norm": 0.19552618265151978,
      "learning_rate": 2.3755555555555554e-05,
      "loss": 0.002,
      "step": 47240
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.4374561905860901,
      "learning_rate": 2.375e-05,
      "loss": 0.0022,
      "step": 47250
    },
    {
      "epoch": 4.200888888888889,
      "grad_norm": 0.06133020669221878,
      "learning_rate": 2.3744444444444448e-05,
      "loss": 0.0023,
      "step": 47260
    },
    {
      "epoch": 4.201777777777778,
      "grad_norm": 0.4418788254261017,
      "learning_rate": 2.3738888888888892e-05,
      "loss": 0.0023,
      "step": 47270
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.029634295031428337,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0014,
      "step": 47280
    },
    {
      "epoch": 4.203555555555556,
      "grad_norm": 0.8375145196914673,
      "learning_rate": 2.372777777777778e-05,
      "loss": 0.0024,
      "step": 47290
    },
    {
      "epoch": 4.204444444444444,
      "grad_norm": 0.7572157979011536,
      "learning_rate": 2.3722222222222222e-05,
      "loss": 0.0023,
      "step": 47300
    },
    {
      "epoch": 4.205333333333333,
      "grad_norm": 0.6057729125022888,
      "learning_rate": 2.3716666666666666e-05,
      "loss": 0.0018,
      "step": 47310
    },
    {
      "epoch": 4.206222222222222,
      "grad_norm": 0.3076271116733551,
      "learning_rate": 2.3711111111111113e-05,
      "loss": 0.002,
      "step": 47320
    },
    {
      "epoch": 4.207111111111111,
      "grad_norm": 0.6400957703590393,
      "learning_rate": 2.3705555555555557e-05,
      "loss": 0.0018,
      "step": 47330
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.5364211797714233,
      "learning_rate": 2.37e-05,
      "loss": 0.0017,
      "step": 47340
    },
    {
      "epoch": 4.208888888888889,
      "grad_norm": 0.13689853250980377,
      "learning_rate": 2.3694444444444447e-05,
      "loss": 0.0015,
      "step": 47350
    },
    {
      "epoch": 4.209777777777778,
      "grad_norm": 0.41733434796333313,
      "learning_rate": 2.368888888888889e-05,
      "loss": 0.0023,
      "step": 47360
    },
    {
      "epoch": 4.210666666666667,
      "grad_norm": 0.5145089626312256,
      "learning_rate": 2.3683333333333334e-05,
      "loss": 0.0015,
      "step": 47370
    },
    {
      "epoch": 4.211555555555556,
      "grad_norm": 0.44215723872184753,
      "learning_rate": 2.3677777777777778e-05,
      "loss": 0.0019,
      "step": 47380
    },
    {
      "epoch": 4.212444444444444,
      "grad_norm": 0.3416666090488434,
      "learning_rate": 2.3672222222222225e-05,
      "loss": 0.002,
      "step": 47390
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.9021990895271301,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0024,
      "step": 47400
    },
    {
      "epoch": 4.214222222222222,
      "grad_norm": 0.9086925387382507,
      "learning_rate": 2.3661111111111112e-05,
      "loss": 0.0018,
      "step": 47410
    },
    {
      "epoch": 4.215111111111111,
      "grad_norm": 0.3566575050354004,
      "learning_rate": 2.3655555555555555e-05,
      "loss": 0.0016,
      "step": 47420
    },
    {
      "epoch": 4.216,
      "grad_norm": 1.0153011083602905,
      "learning_rate": 2.365e-05,
      "loss": 0.0015,
      "step": 47430
    },
    {
      "epoch": 4.216888888888889,
      "grad_norm": 0.4128383994102478,
      "learning_rate": 2.3644444444444446e-05,
      "loss": 0.0026,
      "step": 47440
    },
    {
      "epoch": 4.217777777777778,
      "grad_norm": 0.4586005210876465,
      "learning_rate": 2.363888888888889e-05,
      "loss": 0.0018,
      "step": 47450
    },
    {
      "epoch": 4.218666666666667,
      "grad_norm": 0.5590723156929016,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0019,
      "step": 47460
    },
    {
      "epoch": 4.219555555555556,
      "grad_norm": 0.4399585425853729,
      "learning_rate": 2.362777777777778e-05,
      "loss": 0.002,
      "step": 47470
    },
    {
      "epoch": 4.220444444444444,
      "grad_norm": 0.24425746500492096,
      "learning_rate": 2.3622222222222223e-05,
      "loss": 0.0016,
      "step": 47480
    },
    {
      "epoch": 4.221333333333333,
      "grad_norm": 0.2450522631406784,
      "learning_rate": 2.3616666666666667e-05,
      "loss": 0.0017,
      "step": 47490
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.4550423324108124,
      "learning_rate": 2.361111111111111e-05,
      "loss": 0.0026,
      "step": 47500
    },
    {
      "epoch": 4.223111111111111,
      "grad_norm": 0.7560431957244873,
      "learning_rate": 2.3605555555555554e-05,
      "loss": 0.0021,
      "step": 47510
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.33931660652160645,
      "learning_rate": 2.36e-05,
      "loss": 0.0019,
      "step": 47520
    },
    {
      "epoch": 4.224888888888889,
      "grad_norm": 0.08873030543327332,
      "learning_rate": 2.3594444444444448e-05,
      "loss": 0.0025,
      "step": 47530
    },
    {
      "epoch": 4.225777777777778,
      "grad_norm": 0.37902674078941345,
      "learning_rate": 2.358888888888889e-05,
      "loss": 0.0017,
      "step": 47540
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.12599493563175201,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.0028,
      "step": 47550
    },
    {
      "epoch": 4.227555555555556,
      "grad_norm": 0.26697927713394165,
      "learning_rate": 2.357777777777778e-05,
      "loss": 0.002,
      "step": 47560
    },
    {
      "epoch": 4.2284444444444444,
      "grad_norm": 0.12482013553380966,
      "learning_rate": 2.3572222222222222e-05,
      "loss": 0.0029,
      "step": 47570
    },
    {
      "epoch": 4.229333333333333,
      "grad_norm": 0.1560896933078766,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0016,
      "step": 47580
    },
    {
      "epoch": 4.230222222222222,
      "grad_norm": 0.13054250180721283,
      "learning_rate": 2.3561111111111113e-05,
      "loss": 0.0023,
      "step": 47590
    },
    {
      "epoch": 4.231111111111111,
      "grad_norm": 0.23873598873615265,
      "learning_rate": 2.3555555555555556e-05,
      "loss": 0.0013,
      "step": 47600
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.5098546147346497,
      "learning_rate": 2.355e-05,
      "loss": 0.0013,
      "step": 47610
    },
    {
      "epoch": 4.232888888888889,
      "grad_norm": 0.3051348328590393,
      "learning_rate": 2.3544444444444447e-05,
      "loss": 0.0016,
      "step": 47620
    },
    {
      "epoch": 4.233777777777778,
      "grad_norm": 0.6384455561637878,
      "learning_rate": 2.353888888888889e-05,
      "loss": 0.0024,
      "step": 47630
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.11193963140249252,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0019,
      "step": 47640
    },
    {
      "epoch": 4.235555555555556,
      "grad_norm": 0.1467355489730835,
      "learning_rate": 2.3527777777777777e-05,
      "loss": 0.0019,
      "step": 47650
    },
    {
      "epoch": 4.2364444444444445,
      "grad_norm": 0.14574752748012543,
      "learning_rate": 2.3522222222222224e-05,
      "loss": 0.002,
      "step": 47660
    },
    {
      "epoch": 4.237333333333333,
      "grad_norm": 0.16866853833198547,
      "learning_rate": 2.3516666666666668e-05,
      "loss": 0.0028,
      "step": 47670
    },
    {
      "epoch": 4.238222222222222,
      "grad_norm": 0.593793511390686,
      "learning_rate": 2.351111111111111e-05,
      "loss": 0.0023,
      "step": 47680
    },
    {
      "epoch": 4.239111111111111,
      "grad_norm": 0.6024667024612427,
      "learning_rate": 2.3505555555555555e-05,
      "loss": 0.0026,
      "step": 47690
    },
    {
      "epoch": 4.24,
      "grad_norm": 1.0680137872695923,
      "learning_rate": 2.35e-05,
      "loss": 0.0022,
      "step": 47700
    },
    {
      "epoch": 4.240888888888889,
      "grad_norm": 0.9944496750831604,
      "learning_rate": 2.3494444444444446e-05,
      "loss": 0.002,
      "step": 47710
    },
    {
      "epoch": 4.241777777777778,
      "grad_norm": 0.5601052045822144,
      "learning_rate": 2.3488888888888893e-05,
      "loss": 0.0022,
      "step": 47720
    },
    {
      "epoch": 4.242666666666667,
      "grad_norm": 0.1231953501701355,
      "learning_rate": 2.3483333333333336e-05,
      "loss": 0.0023,
      "step": 47730
    },
    {
      "epoch": 4.243555555555556,
      "grad_norm": 0.7096362113952637,
      "learning_rate": 2.347777777777778e-05,
      "loss": 0.003,
      "step": 47740
    },
    {
      "epoch": 4.2444444444444445,
      "grad_norm": 0.4214480221271515,
      "learning_rate": 2.3472222222222223e-05,
      "loss": 0.0027,
      "step": 47750
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.056181635707616806,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.002,
      "step": 47760
    },
    {
      "epoch": 4.246222222222222,
      "grad_norm": 0.30282577872276306,
      "learning_rate": 2.346111111111111e-05,
      "loss": 0.0019,
      "step": 47770
    },
    {
      "epoch": 4.247111111111111,
      "grad_norm": 0.28616949915885925,
      "learning_rate": 2.3455555555555557e-05,
      "loss": 0.0014,
      "step": 47780
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.923215925693512,
      "learning_rate": 2.345e-05,
      "loss": 0.0025,
      "step": 47790
    },
    {
      "epoch": 4.248888888888889,
      "grad_norm": 0.4741399884223938,
      "learning_rate": 2.3444444444444448e-05,
      "loss": 0.0017,
      "step": 47800
    },
    {
      "epoch": 4.249777777777778,
      "grad_norm": 0.7641400694847107,
      "learning_rate": 2.343888888888889e-05,
      "loss": 0.0018,
      "step": 47810
    },
    {
      "epoch": 4.250666666666667,
      "grad_norm": 0.09613101184368134,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.0017,
      "step": 47820
    },
    {
      "epoch": 4.251555555555555,
      "grad_norm": 0.3968115746974945,
      "learning_rate": 2.342777777777778e-05,
      "loss": 0.0018,
      "step": 47830
    },
    {
      "epoch": 4.2524444444444445,
      "grad_norm": 0.7358272671699524,
      "learning_rate": 2.3422222222222222e-05,
      "loss": 0.0036,
      "step": 47840
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.3185963034629822,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.0024,
      "step": 47850
    },
    {
      "epoch": 4.254222222222222,
      "grad_norm": 0.07743296027183533,
      "learning_rate": 2.3411111111111112e-05,
      "loss": 0.002,
      "step": 47860
    },
    {
      "epoch": 4.255111111111111,
      "grad_norm": 0.13175061345100403,
      "learning_rate": 2.3405555555555556e-05,
      "loss": 0.0028,
      "step": 47870
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.1320347636938095,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.002,
      "step": 47880
    },
    {
      "epoch": 4.256888888888889,
      "grad_norm": 0.5456562638282776,
      "learning_rate": 2.3394444444444447e-05,
      "loss": 0.0022,
      "step": 47890
    },
    {
      "epoch": 4.257777777777778,
      "grad_norm": 0.28546011447906494,
      "learning_rate": 2.338888888888889e-05,
      "loss": 0.0015,
      "step": 47900
    },
    {
      "epoch": 4.258666666666667,
      "grad_norm": 0.8106252551078796,
      "learning_rate": 2.3383333333333334e-05,
      "loss": 0.0025,
      "step": 47910
    },
    {
      "epoch": 4.259555555555556,
      "grad_norm": 0.2991158366203308,
      "learning_rate": 2.337777777777778e-05,
      "loss": 0.003,
      "step": 47920
    },
    {
      "epoch": 4.2604444444444445,
      "grad_norm": 0.5325307846069336,
      "learning_rate": 2.3372222222222224e-05,
      "loss": 0.0027,
      "step": 47930
    },
    {
      "epoch": 4.261333333333333,
      "grad_norm": 0.6751923561096191,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0023,
      "step": 47940
    },
    {
      "epoch": 4.262222222222222,
      "grad_norm": 1.1110843420028687,
      "learning_rate": 2.336111111111111e-05,
      "loss": 0.0017,
      "step": 47950
    },
    {
      "epoch": 4.263111111111111,
      "grad_norm": 0.6635252833366394,
      "learning_rate": 2.3355555555555555e-05,
      "loss": 0.0023,
      "step": 47960
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.21035051345825195,
      "learning_rate": 2.3350000000000002e-05,
      "loss": 0.0027,
      "step": 47970
    },
    {
      "epoch": 4.264888888888889,
      "grad_norm": 0.14603351056575775,
      "learning_rate": 2.3344444444444445e-05,
      "loss": 0.002,
      "step": 47980
    },
    {
      "epoch": 4.265777777777778,
      "grad_norm": 0.31344670057296753,
      "learning_rate": 2.3338888888888892e-05,
      "loss": 0.0018,
      "step": 47990
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.809140682220459,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0016,
      "step": 48000
    },
    {
      "epoch": 4.267555555555555,
      "grad_norm": 0.30906379222869873,
      "learning_rate": 2.332777777777778e-05,
      "loss": 0.0023,
      "step": 48010
    },
    {
      "epoch": 4.2684444444444445,
      "grad_norm": 0.123397096991539,
      "learning_rate": 2.3322222222222223e-05,
      "loss": 0.0013,
      "step": 48020
    },
    {
      "epoch": 4.269333333333333,
      "grad_norm": 0.2599700093269348,
      "learning_rate": 2.3316666666666666e-05,
      "loss": 0.0019,
      "step": 48030
    },
    {
      "epoch": 4.270222222222222,
      "grad_norm": 0.3479021191596985,
      "learning_rate": 2.331111111111111e-05,
      "loss": 0.0033,
      "step": 48040
    },
    {
      "epoch": 4.271111111111111,
      "grad_norm": 0.09562171995639801,
      "learning_rate": 2.3305555555555557e-05,
      "loss": 0.0015,
      "step": 48050
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.09514645487070084,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0026,
      "step": 48060
    },
    {
      "epoch": 4.272888888888889,
      "grad_norm": 0.33889225125312805,
      "learning_rate": 2.3294444444444447e-05,
      "loss": 0.0023,
      "step": 48070
    },
    {
      "epoch": 4.273777777777778,
      "grad_norm": 0.494062602519989,
      "learning_rate": 2.328888888888889e-05,
      "loss": 0.002,
      "step": 48080
    },
    {
      "epoch": 4.274666666666667,
      "grad_norm": 0.23123328387737274,
      "learning_rate": 2.3283333333333335e-05,
      "loss": 0.0018,
      "step": 48090
    },
    {
      "epoch": 4.275555555555556,
      "grad_norm": 0.32823169231414795,
      "learning_rate": 2.3277777777777778e-05,
      "loss": 0.0016,
      "step": 48100
    },
    {
      "epoch": 4.2764444444444445,
      "grad_norm": 0.04396795853972435,
      "learning_rate": 2.3272222222222222e-05,
      "loss": 0.0016,
      "step": 48110
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.42203596234321594,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0014,
      "step": 48120
    },
    {
      "epoch": 4.278222222222222,
      "grad_norm": 0.28629299998283386,
      "learning_rate": 2.3261111111111112e-05,
      "loss": 0.002,
      "step": 48130
    },
    {
      "epoch": 4.279111111111111,
      "grad_norm": 0.3008740544319153,
      "learning_rate": 2.3255555555555556e-05,
      "loss": 0.003,
      "step": 48140
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.7062819600105286,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.0025,
      "step": 48150
    },
    {
      "epoch": 4.280888888888889,
      "grad_norm": 0.22576864063739777,
      "learning_rate": 2.3244444444444446e-05,
      "loss": 0.0024,
      "step": 48160
    },
    {
      "epoch": 4.281777777777778,
      "grad_norm": 0.3218928873538971,
      "learning_rate": 2.323888888888889e-05,
      "loss": 0.0034,
      "step": 48170
    },
    {
      "epoch": 4.282666666666667,
      "grad_norm": 0.2285316437482834,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.0026,
      "step": 48180
    },
    {
      "epoch": 4.283555555555555,
      "grad_norm": 0.1367451250553131,
      "learning_rate": 2.322777777777778e-05,
      "loss": 0.0018,
      "step": 48190
    },
    {
      "epoch": 4.2844444444444445,
      "grad_norm": 0.02418132685124874,
      "learning_rate": 2.3222222222222224e-05,
      "loss": 0.0025,
      "step": 48200
    },
    {
      "epoch": 4.285333333333333,
      "grad_norm": 0.3925839066505432,
      "learning_rate": 2.3216666666666667e-05,
      "loss": 0.0017,
      "step": 48210
    },
    {
      "epoch": 4.286222222222222,
      "grad_norm": 0.5953721404075623,
      "learning_rate": 2.321111111111111e-05,
      "loss": 0.0024,
      "step": 48220
    },
    {
      "epoch": 4.287111111111111,
      "grad_norm": 0.4556695520877838,
      "learning_rate": 2.3205555555555555e-05,
      "loss": 0.0032,
      "step": 48230
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.17909997701644897,
      "learning_rate": 2.32e-05,
      "loss": 0.003,
      "step": 48240
    },
    {
      "epoch": 4.288888888888889,
      "grad_norm": 0.4570397734642029,
      "learning_rate": 2.3194444444444445e-05,
      "loss": 0.0025,
      "step": 48250
    },
    {
      "epoch": 4.289777777777778,
      "grad_norm": 0.25497370958328247,
      "learning_rate": 2.3188888888888892e-05,
      "loss": 0.0022,
      "step": 48260
    },
    {
      "epoch": 4.290666666666667,
      "grad_norm": 0.20425714552402496,
      "learning_rate": 2.3183333333333336e-05,
      "loss": 0.0023,
      "step": 48270
    },
    {
      "epoch": 4.291555555555556,
      "grad_norm": 0.3022099435329437,
      "learning_rate": 2.317777777777778e-05,
      "loss": 0.0014,
      "step": 48280
    },
    {
      "epoch": 4.2924444444444445,
      "grad_norm": 0.38298240303993225,
      "learning_rate": 2.3172222222222223e-05,
      "loss": 0.0026,
      "step": 48290
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.234069362282753,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.002,
      "step": 48300
    },
    {
      "epoch": 4.294222222222222,
      "grad_norm": 0.3447645306587219,
      "learning_rate": 2.316111111111111e-05,
      "loss": 0.0022,
      "step": 48310
    },
    {
      "epoch": 4.295111111111111,
      "grad_norm": 0.34023940563201904,
      "learning_rate": 2.3155555555555557e-05,
      "loss": 0.0021,
      "step": 48320
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.4302569627761841,
      "learning_rate": 2.3150000000000004e-05,
      "loss": 0.0019,
      "step": 48330
    },
    {
      "epoch": 4.296888888888889,
      "grad_norm": 0.6501352190971375,
      "learning_rate": 2.3144444444444447e-05,
      "loss": 0.0024,
      "step": 48340
    },
    {
      "epoch": 4.297777777777778,
      "grad_norm": 0.6364150047302246,
      "learning_rate": 2.313888888888889e-05,
      "loss": 0.0019,
      "step": 48350
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.49959796667099,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0018,
      "step": 48360
    },
    {
      "epoch": 4.299555555555555,
      "grad_norm": 0.24583369493484497,
      "learning_rate": 2.3127777777777778e-05,
      "loss": 0.0019,
      "step": 48370
    },
    {
      "epoch": 4.3004444444444445,
      "grad_norm": 0.44389572739601135,
      "learning_rate": 2.312222222222222e-05,
      "loss": 0.0018,
      "step": 48380
    },
    {
      "epoch": 4.301333333333333,
      "grad_norm": 0.26496946811676025,
      "learning_rate": 2.311666666666667e-05,
      "loss": 0.0019,
      "step": 48390
    },
    {
      "epoch": 4.302222222222222,
      "grad_norm": 0.16599689424037933,
      "learning_rate": 2.3111111111111112e-05,
      "loss": 0.0016,
      "step": 48400
    },
    {
      "epoch": 4.303111111111111,
      "grad_norm": 0.40917614102363586,
      "learning_rate": 2.3105555555555556e-05,
      "loss": 0.0016,
      "step": 48410
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.04212099313735962,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0016,
      "step": 48420
    },
    {
      "epoch": 4.304888888888889,
      "grad_norm": 0.1564243882894516,
      "learning_rate": 2.3094444444444446e-05,
      "loss": 0.003,
      "step": 48430
    },
    {
      "epoch": 4.305777777777778,
      "grad_norm": 0.042018044739961624,
      "learning_rate": 2.308888888888889e-05,
      "loss": 0.0021,
      "step": 48440
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.11880173534154892,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.0023,
      "step": 48450
    },
    {
      "epoch": 4.307555555555556,
      "grad_norm": 0.16003626585006714,
      "learning_rate": 2.307777777777778e-05,
      "loss": 0.0026,
      "step": 48460
    },
    {
      "epoch": 4.3084444444444445,
      "grad_norm": 0.44197744131088257,
      "learning_rate": 2.3072222222222224e-05,
      "loss": 0.0014,
      "step": 48470
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.14595448970794678,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0029,
      "step": 48480
    },
    {
      "epoch": 4.310222222222222,
      "grad_norm": 1.7594412565231323,
      "learning_rate": 2.306111111111111e-05,
      "loss": 0.0023,
      "step": 48490
    },
    {
      "epoch": 4.311111111111111,
      "grad_norm": 0.2716015577316284,
      "learning_rate": 2.3055555555555558e-05,
      "loss": 0.0023,
      "step": 48500
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.37673264741897583,
      "learning_rate": 2.305e-05,
      "loss": 0.0018,
      "step": 48510
    },
    {
      "epoch": 4.312888888888889,
      "grad_norm": 0.0851668044924736,
      "learning_rate": 2.3044444444444445e-05,
      "loss": 0.0032,
      "step": 48520
    },
    {
      "epoch": 4.313777777777778,
      "grad_norm": 0.351985365152359,
      "learning_rate": 2.3038888888888892e-05,
      "loss": 0.0024,
      "step": 48530
    },
    {
      "epoch": 4.314666666666667,
      "grad_norm": 0.4983821511268616,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0024,
      "step": 48540
    },
    {
      "epoch": 4.315555555555555,
      "grad_norm": 0.41377493739128113,
      "learning_rate": 2.302777777777778e-05,
      "loss": 0.0022,
      "step": 48550
    },
    {
      "epoch": 4.3164444444444445,
      "grad_norm": 0.04876861721277237,
      "learning_rate": 2.3022222222222222e-05,
      "loss": 0.0023,
      "step": 48560
    },
    {
      "epoch": 4.317333333333333,
      "grad_norm": 0.09136587381362915,
      "learning_rate": 2.3016666666666666e-05,
      "loss": 0.0033,
      "step": 48570
    },
    {
      "epoch": 4.318222222222222,
      "grad_norm": 0.22747543454170227,
      "learning_rate": 2.301111111111111e-05,
      "loss": 0.0015,
      "step": 48580
    },
    {
      "epoch": 4.319111111111111,
      "grad_norm": 0.04255029931664467,
      "learning_rate": 2.3005555555555556e-05,
      "loss": 0.0015,
      "step": 48590
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.34402742981910706,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0015,
      "step": 48600
    },
    {
      "epoch": 4.320888888888889,
      "grad_norm": 0.13410770893096924,
      "learning_rate": 2.2994444444444447e-05,
      "loss": 0.0015,
      "step": 48610
    },
    {
      "epoch": 4.321777777777778,
      "grad_norm": 0.38393810391426086,
      "learning_rate": 2.298888888888889e-05,
      "loss": 0.0014,
      "step": 48620
    },
    {
      "epoch": 4.322666666666667,
      "grad_norm": 0.04061368107795715,
      "learning_rate": 2.2983333333333334e-05,
      "loss": 0.0025,
      "step": 48630
    },
    {
      "epoch": 4.323555555555555,
      "grad_norm": 0.5250400900840759,
      "learning_rate": 2.2977777777777778e-05,
      "loss": 0.0028,
      "step": 48640
    },
    {
      "epoch": 4.3244444444444445,
      "grad_norm": 0.1358470618724823,
      "learning_rate": 2.297222222222222e-05,
      "loss": 0.0024,
      "step": 48650
    },
    {
      "epoch": 4.325333333333333,
      "grad_norm": 0.1452895700931549,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.002,
      "step": 48660
    },
    {
      "epoch": 4.326222222222222,
      "grad_norm": 0.8325917720794678,
      "learning_rate": 2.296111111111111e-05,
      "loss": 0.0017,
      "step": 48670
    },
    {
      "epoch": 4.327111111111111,
      "grad_norm": 0.9021747708320618,
      "learning_rate": 2.295555555555556e-05,
      "loss": 0.0016,
      "step": 48680
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.7062963843345642,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 0.0019,
      "step": 48690
    },
    {
      "epoch": 4.328888888888889,
      "grad_norm": 0.8270812630653381,
      "learning_rate": 2.2944444444444446e-05,
      "loss": 0.0016,
      "step": 48700
    },
    {
      "epoch": 4.329777777777778,
      "grad_norm": 0.3393123745918274,
      "learning_rate": 2.293888888888889e-05,
      "loss": 0.0015,
      "step": 48710
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 1.032133936882019,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0014,
      "step": 48720
    },
    {
      "epoch": 4.331555555555555,
      "grad_norm": 0.5009041428565979,
      "learning_rate": 2.292777777777778e-05,
      "loss": 0.0021,
      "step": 48730
    },
    {
      "epoch": 4.3324444444444445,
      "grad_norm": 0.31608453392982483,
      "learning_rate": 2.2922222222222223e-05,
      "loss": 0.002,
      "step": 48740
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.44719383120536804,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.003,
      "step": 48750
    },
    {
      "epoch": 4.334222222222222,
      "grad_norm": 0.1084786206483841,
      "learning_rate": 2.291111111111111e-05,
      "loss": 0.0027,
      "step": 48760
    },
    {
      "epoch": 4.335111111111111,
      "grad_norm": 0.4436855614185333,
      "learning_rate": 2.2905555555555557e-05,
      "loss": 0.0023,
      "step": 48770
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.772676408290863,
      "learning_rate": 2.29e-05,
      "loss": 0.0029,
      "step": 48780
    },
    {
      "epoch": 4.336888888888889,
      "grad_norm": 0.15830866992473602,
      "learning_rate": 2.2894444444444445e-05,
      "loss": 0.0016,
      "step": 48790
    },
    {
      "epoch": 4.337777777777778,
      "grad_norm": 0.31428349018096924,
      "learning_rate": 2.288888888888889e-05,
      "loss": 0.0025,
      "step": 48800
    },
    {
      "epoch": 4.338666666666667,
      "grad_norm": 0.3404465615749359,
      "learning_rate": 2.2883333333333335e-05,
      "loss": 0.0024,
      "step": 48810
    },
    {
      "epoch": 4.339555555555555,
      "grad_norm": 0.0852849930524826,
      "learning_rate": 2.287777777777778e-05,
      "loss": 0.0016,
      "step": 48820
    },
    {
      "epoch": 4.3404444444444445,
      "grad_norm": 0.3406635522842407,
      "learning_rate": 2.2872222222222222e-05,
      "loss": 0.0016,
      "step": 48830
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.5613110661506653,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0018,
      "step": 48840
    },
    {
      "epoch": 4.342222222222222,
      "grad_norm": 0.39941897988319397,
      "learning_rate": 2.286111111111111e-05,
      "loss": 0.0022,
      "step": 48850
    },
    {
      "epoch": 4.343111111111111,
      "grad_norm": 0.23623719811439514,
      "learning_rate": 2.2855555555555556e-05,
      "loss": 0.0027,
      "step": 48860
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.47308966517448425,
      "learning_rate": 2.2850000000000003e-05,
      "loss": 0.0027,
      "step": 48870
    },
    {
      "epoch": 4.344888888888889,
      "grad_norm": 0.4727502167224884,
      "learning_rate": 2.2844444444444447e-05,
      "loss": 0.0017,
      "step": 48880
    },
    {
      "epoch": 4.345777777777778,
      "grad_norm": 0.3040771186351776,
      "learning_rate": 2.283888888888889e-05,
      "loss": 0.0032,
      "step": 48890
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.6313782930374146,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0026,
      "step": 48900
    },
    {
      "epoch": 4.347555555555555,
      "grad_norm": 0.20499424636363983,
      "learning_rate": 2.2827777777777777e-05,
      "loss": 0.0017,
      "step": 48910
    },
    {
      "epoch": 4.348444444444445,
      "grad_norm": 0.5607568621635437,
      "learning_rate": 2.282222222222222e-05,
      "loss": 0.0023,
      "step": 48920
    },
    {
      "epoch": 4.349333333333333,
      "grad_norm": 0.09750581532716751,
      "learning_rate": 2.2816666666666668e-05,
      "loss": 0.0011,
      "step": 48930
    },
    {
      "epoch": 4.350222222222222,
      "grad_norm": 0.4676102101802826,
      "learning_rate": 2.281111111111111e-05,
      "loss": 0.0015,
      "step": 48940
    },
    {
      "epoch": 4.351111111111111,
      "grad_norm": 0.34467408061027527,
      "learning_rate": 2.280555555555556e-05,
      "loss": 0.0025,
      "step": 48950
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.27285391092300415,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0018,
      "step": 48960
    },
    {
      "epoch": 4.352888888888889,
      "grad_norm": 0.15671256184577942,
      "learning_rate": 2.2794444444444445e-05,
      "loss": 0.0018,
      "step": 48970
    },
    {
      "epoch": 4.353777777777778,
      "grad_norm": 0.4832291007041931,
      "learning_rate": 2.278888888888889e-05,
      "loss": 0.0024,
      "step": 48980
    },
    {
      "epoch": 4.354666666666667,
      "grad_norm": 0.5790584087371826,
      "learning_rate": 2.2783333333333336e-05,
      "loss": 0.0029,
      "step": 48990
    },
    {
      "epoch": 4.355555555555555,
      "grad_norm": 0.13544611632823944,
      "learning_rate": 2.277777777777778e-05,
      "loss": 0.002,
      "step": 49000
    },
    {
      "epoch": 4.356444444444445,
      "grad_norm": 0.11465923488140106,
      "learning_rate": 2.2772222222222223e-05,
      "loss": 0.0014,
      "step": 49010
    },
    {
      "epoch": 4.357333333333333,
      "grad_norm": 0.34330224990844727,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.003,
      "step": 49020
    },
    {
      "epoch": 4.358222222222222,
      "grad_norm": 0.34621816873550415,
      "learning_rate": 2.276111111111111e-05,
      "loss": 0.0031,
      "step": 49030
    },
    {
      "epoch": 4.359111111111111,
      "grad_norm": 0.489412397146225,
      "learning_rate": 2.2755555555555557e-05,
      "loss": 0.0023,
      "step": 49040
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.45614078640937805,
      "learning_rate": 2.275e-05,
      "loss": 0.0032,
      "step": 49050
    },
    {
      "epoch": 4.360888888888889,
      "grad_norm": 0.8351799249649048,
      "learning_rate": 2.2744444444444448e-05,
      "loss": 0.0024,
      "step": 49060
    },
    {
      "epoch": 4.361777777777778,
      "grad_norm": 0.23254622519016266,
      "learning_rate": 2.273888888888889e-05,
      "loss": 0.0024,
      "step": 49070
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.11313384771347046,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0024,
      "step": 49080
    },
    {
      "epoch": 4.363555555555555,
      "grad_norm": 0.49025392532348633,
      "learning_rate": 2.272777777777778e-05,
      "loss": 0.0021,
      "step": 49090
    },
    {
      "epoch": 4.364444444444445,
      "grad_norm": 0.46078914403915405,
      "learning_rate": 2.2722222222222222e-05,
      "loss": 0.0024,
      "step": 49100
    },
    {
      "epoch": 4.365333333333333,
      "grad_norm": 0.567507803440094,
      "learning_rate": 2.2716666666666665e-05,
      "loss": 0.0026,
      "step": 49110
    },
    {
      "epoch": 4.3662222222222224,
      "grad_norm": 0.6323089599609375,
      "learning_rate": 2.2711111111111112e-05,
      "loss": 0.0031,
      "step": 49120
    },
    {
      "epoch": 4.367111111111111,
      "grad_norm": 0.6316489577293396,
      "learning_rate": 2.270555555555556e-05,
      "loss": 0.0031,
      "step": 49130
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.3659438192844391,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.002,
      "step": 49140
    },
    {
      "epoch": 4.368888888888889,
      "grad_norm": 0.24279916286468506,
      "learning_rate": 2.2694444444444446e-05,
      "loss": 0.0023,
      "step": 49150
    },
    {
      "epoch": 4.369777777777777,
      "grad_norm": 0.07912822812795639,
      "learning_rate": 2.268888888888889e-05,
      "loss": 0.0027,
      "step": 49160
    },
    {
      "epoch": 4.370666666666667,
      "grad_norm": 0.45017680525779724,
      "learning_rate": 2.2683333333333334e-05,
      "loss": 0.0022,
      "step": 49170
    },
    {
      "epoch": 4.371555555555555,
      "grad_norm": 0.33935463428497314,
      "learning_rate": 2.2677777777777777e-05,
      "loss": 0.0031,
      "step": 49180
    },
    {
      "epoch": 4.372444444444445,
      "grad_norm": 0.2586847245693207,
      "learning_rate": 2.2672222222222224e-05,
      "loss": 0.0022,
      "step": 49190
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.224382221698761,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0016,
      "step": 49200
    },
    {
      "epoch": 4.3742222222222225,
      "grad_norm": 0.2571142613887787,
      "learning_rate": 2.2661111111111115e-05,
      "loss": 0.0022,
      "step": 49210
    },
    {
      "epoch": 4.375111111111111,
      "grad_norm": 0.5898950099945068,
      "learning_rate": 2.2655555555555558e-05,
      "loss": 0.0027,
      "step": 49220
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.23557397723197937,
      "learning_rate": 2.265e-05,
      "loss": 0.0014,
      "step": 49230
    },
    {
      "epoch": 4.376888888888889,
      "grad_norm": 0.19920910894870758,
      "learning_rate": 2.2644444444444445e-05,
      "loss": 0.0029,
      "step": 49240
    },
    {
      "epoch": 4.377777777777778,
      "grad_norm": 0.5193884372711182,
      "learning_rate": 2.263888888888889e-05,
      "loss": 0.0018,
      "step": 49250
    },
    {
      "epoch": 4.378666666666667,
      "grad_norm": 0.5298824906349182,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.0023,
      "step": 49260
    },
    {
      "epoch": 4.379555555555555,
      "grad_norm": 0.627851665019989,
      "learning_rate": 2.262777777777778e-05,
      "loss": 0.0023,
      "step": 49270
    },
    {
      "epoch": 4.380444444444445,
      "grad_norm": 0.9665120840072632,
      "learning_rate": 2.2622222222222223e-05,
      "loss": 0.0016,
      "step": 49280
    },
    {
      "epoch": 4.381333333333333,
      "grad_norm": 0.6623792052268982,
      "learning_rate": 2.2616666666666666e-05,
      "loss": 0.0025,
      "step": 49290
    },
    {
      "epoch": 4.3822222222222225,
      "grad_norm": 0.6080554127693176,
      "learning_rate": 2.2611111111111113e-05,
      "loss": 0.002,
      "step": 49300
    },
    {
      "epoch": 4.383111111111111,
      "grad_norm": 0.515647292137146,
      "learning_rate": 2.2605555555555557e-05,
      "loss": 0.0013,
      "step": 49310
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.44341790676116943,
      "learning_rate": 2.26e-05,
      "loss": 0.0016,
      "step": 49320
    },
    {
      "epoch": 4.384888888888889,
      "grad_norm": 0.15302681922912598,
      "learning_rate": 2.2594444444444447e-05,
      "loss": 0.0032,
      "step": 49330
    },
    {
      "epoch": 4.385777777777777,
      "grad_norm": 0.3785758912563324,
      "learning_rate": 2.258888888888889e-05,
      "loss": 0.0021,
      "step": 49340
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.08810179680585861,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.0032,
      "step": 49350
    },
    {
      "epoch": 4.387555555555555,
      "grad_norm": 0.27583104372024536,
      "learning_rate": 2.2577777777777778e-05,
      "loss": 0.002,
      "step": 49360
    },
    {
      "epoch": 4.388444444444445,
      "grad_norm": 0.5164918899536133,
      "learning_rate": 2.257222222222222e-05,
      "loss": 0.0018,
      "step": 49370
    },
    {
      "epoch": 4.389333333333333,
      "grad_norm": 0.7149345278739929,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0025,
      "step": 49380
    },
    {
      "epoch": 4.3902222222222225,
      "grad_norm": 0.1594136357307434,
      "learning_rate": 2.2561111111111112e-05,
      "loss": 0.0022,
      "step": 49390
    },
    {
      "epoch": 4.391111111111111,
      "grad_norm": 0.5206487774848938,
      "learning_rate": 2.255555555555556e-05,
      "loss": 0.0027,
      "step": 49400
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.4192570149898529,
      "learning_rate": 2.2550000000000003e-05,
      "loss": 0.0015,
      "step": 49410
    },
    {
      "epoch": 4.392888888888889,
      "grad_norm": 0.3598005771636963,
      "learning_rate": 2.2544444444444446e-05,
      "loss": 0.0023,
      "step": 49420
    },
    {
      "epoch": 4.393777777777778,
      "grad_norm": 0.6015896201133728,
      "learning_rate": 2.253888888888889e-05,
      "loss": 0.0021,
      "step": 49430
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.8854796886444092,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0014,
      "step": 49440
    },
    {
      "epoch": 4.395555555555555,
      "grad_norm": 0.7619242668151855,
      "learning_rate": 2.2527777777777777e-05,
      "loss": 0.0024,
      "step": 49450
    },
    {
      "epoch": 4.396444444444445,
      "grad_norm": 0.9347496628761292,
      "learning_rate": 2.2522222222222224e-05,
      "loss": 0.0014,
      "step": 49460
    },
    {
      "epoch": 4.397333333333333,
      "grad_norm": 0.6103164553642273,
      "learning_rate": 2.2516666666666667e-05,
      "loss": 0.0018,
      "step": 49470
    },
    {
      "epoch": 4.3982222222222225,
      "grad_norm": 0.9732666015625,
      "learning_rate": 2.2511111111111114e-05,
      "loss": 0.0024,
      "step": 49480
    },
    {
      "epoch": 4.399111111111111,
      "grad_norm": 0.044497955590486526,
      "learning_rate": 2.2505555555555558e-05,
      "loss": 0.0021,
      "step": 49490
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.13636255264282227,
      "learning_rate": 2.25e-05,
      "loss": 0.0015,
      "step": 49500
    },
    {
      "epoch": 4.400888888888889,
      "grad_norm": 0.43045786023139954,
      "learning_rate": 2.2494444444444445e-05,
      "loss": 0.0022,
      "step": 49510
    },
    {
      "epoch": 4.401777777777777,
      "grad_norm": 0.11449724435806274,
      "learning_rate": 2.248888888888889e-05,
      "loss": 0.0021,
      "step": 49520
    },
    {
      "epoch": 4.402666666666667,
      "grad_norm": 0.19916801154613495,
      "learning_rate": 2.2483333333333335e-05,
      "loss": 0.0021,
      "step": 49530
    },
    {
      "epoch": 4.403555555555555,
      "grad_norm": 0.18629351258277893,
      "learning_rate": 2.247777777777778e-05,
      "loss": 0.0027,
      "step": 49540
    },
    {
      "epoch": 4.404444444444445,
      "grad_norm": 0.1917673647403717,
      "learning_rate": 2.2472222222222223e-05,
      "loss": 0.0013,
      "step": 49550
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.4436357319355011,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0026,
      "step": 49560
    },
    {
      "epoch": 4.4062222222222225,
      "grad_norm": 0.5704349875450134,
      "learning_rate": 2.2461111111111113e-05,
      "loss": 0.0025,
      "step": 49570
    },
    {
      "epoch": 4.407111111111111,
      "grad_norm": 0.35207563638687134,
      "learning_rate": 2.2455555555555557e-05,
      "loss": 0.0016,
      "step": 49580
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.29320332407951355,
      "learning_rate": 2.245e-05,
      "loss": 0.002,
      "step": 49590
    },
    {
      "epoch": 4.408888888888889,
      "grad_norm": 0.2678486108779907,
      "learning_rate": 2.2444444444444447e-05,
      "loss": 0.0015,
      "step": 49600
    },
    {
      "epoch": 4.409777777777778,
      "grad_norm": 0.06412893533706665,
      "learning_rate": 2.243888888888889e-05,
      "loss": 0.0024,
      "step": 49610
    },
    {
      "epoch": 4.410666666666667,
      "grad_norm": 0.03286216780543327,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.0027,
      "step": 49620
    },
    {
      "epoch": 4.411555555555555,
      "grad_norm": 0.2608509361743927,
      "learning_rate": 2.2427777777777778e-05,
      "loss": 0.002,
      "step": 49630
    },
    {
      "epoch": 4.412444444444445,
      "grad_norm": 0.03193376958370209,
      "learning_rate": 2.242222222222222e-05,
      "loss": 0.0019,
      "step": 49640
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.2106250822544098,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.0018,
      "step": 49650
    },
    {
      "epoch": 4.4142222222222225,
      "grad_norm": 0.6410848498344421,
      "learning_rate": 2.2411111111111112e-05,
      "loss": 0.0031,
      "step": 49660
    },
    {
      "epoch": 4.415111111111111,
      "grad_norm": 0.5165103673934937,
      "learning_rate": 2.240555555555556e-05,
      "loss": 0.0027,
      "step": 49670
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.1981794536113739,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0013,
      "step": 49680
    },
    {
      "epoch": 4.416888888888889,
      "grad_norm": 0.3840564489364624,
      "learning_rate": 2.2394444444444446e-05,
      "loss": 0.0015,
      "step": 49690
    },
    {
      "epoch": 4.417777777777777,
      "grad_norm": 0.06852696090936661,
      "learning_rate": 2.238888888888889e-05,
      "loss": 0.0017,
      "step": 49700
    },
    {
      "epoch": 4.418666666666667,
      "grad_norm": 0.11217496544122696,
      "learning_rate": 2.2383333333333333e-05,
      "loss": 0.0015,
      "step": 49710
    },
    {
      "epoch": 4.419555555555555,
      "grad_norm": 0.48898154497146606,
      "learning_rate": 2.2377777777777777e-05,
      "loss": 0.0016,
      "step": 49720
    },
    {
      "epoch": 4.420444444444445,
      "grad_norm": 0.6837679147720337,
      "learning_rate": 2.2372222222222224e-05,
      "loss": 0.0027,
      "step": 49730
    },
    {
      "epoch": 4.421333333333333,
      "grad_norm": 0.2231340855360031,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.0015,
      "step": 49740
    },
    {
      "epoch": 4.4222222222222225,
      "grad_norm": 0.22600388526916504,
      "learning_rate": 2.2361111111111114e-05,
      "loss": 0.0018,
      "step": 49750
    },
    {
      "epoch": 4.423111111111111,
      "grad_norm": 0.5286784172058105,
      "learning_rate": 2.2355555555555558e-05,
      "loss": 0.003,
      "step": 49760
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.08661848306655884,
      "learning_rate": 2.235e-05,
      "loss": 0.0013,
      "step": 49770
    },
    {
      "epoch": 4.424888888888889,
      "grad_norm": 0.8319764733314514,
      "learning_rate": 2.2344444444444445e-05,
      "loss": 0.0016,
      "step": 49780
    },
    {
      "epoch": 4.425777777777777,
      "grad_norm": 0.9427724480628967,
      "learning_rate": 2.2338888888888888e-05,
      "loss": 0.002,
      "step": 49790
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.4668959081172943,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0017,
      "step": 49800
    },
    {
      "epoch": 4.427555555555555,
      "grad_norm": 0.03914542868733406,
      "learning_rate": 2.232777777777778e-05,
      "loss": 0.0019,
      "step": 49810
    },
    {
      "epoch": 4.428444444444445,
      "grad_norm": 0.04977184161543846,
      "learning_rate": 2.2322222222222222e-05,
      "loss": 0.0016,
      "step": 49820
    },
    {
      "epoch": 4.429333333333333,
      "grad_norm": 0.7251390218734741,
      "learning_rate": 2.231666666666667e-05,
      "loss": 0.0029,
      "step": 49830
    },
    {
      "epoch": 4.4302222222222225,
      "grad_norm": 0.15231457352638245,
      "learning_rate": 2.2311111111111113e-05,
      "loss": 0.002,
      "step": 49840
    },
    {
      "epoch": 4.431111111111111,
      "grad_norm": 0.08237152546644211,
      "learning_rate": 2.2305555555555556e-05,
      "loss": 0.002,
      "step": 49850
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.7247278690338135,
      "learning_rate": 2.23e-05,
      "loss": 0.0014,
      "step": 49860
    },
    {
      "epoch": 4.432888888888889,
      "grad_norm": 0.11040738224983215,
      "learning_rate": 2.2294444444444447e-05,
      "loss": 0.0018,
      "step": 49870
    },
    {
      "epoch": 4.433777777777777,
      "grad_norm": 0.35211697220802307,
      "learning_rate": 2.228888888888889e-05,
      "loss": 0.0016,
      "step": 49880
    },
    {
      "epoch": 4.434666666666667,
      "grad_norm": 0.12480971217155457,
      "learning_rate": 2.2283333333333334e-05,
      "loss": 0.0019,
      "step": 49890
    },
    {
      "epoch": 4.435555555555555,
      "grad_norm": 0.9085556268692017,
      "learning_rate": 2.2277777777777778e-05,
      "loss": 0.0018,
      "step": 49900
    },
    {
      "epoch": 4.436444444444445,
      "grad_norm": 0.15692546963691711,
      "learning_rate": 2.227222222222222e-05,
      "loss": 0.0022,
      "step": 49910
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.6511464715003967,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0017,
      "step": 49920
    },
    {
      "epoch": 4.4382222222222225,
      "grad_norm": 0.24428994953632355,
      "learning_rate": 2.226111111111111e-05,
      "loss": 0.0012,
      "step": 49930
    },
    {
      "epoch": 4.439111111111111,
      "grad_norm": 0.8228595852851868,
      "learning_rate": 2.225555555555556e-05,
      "loss": 0.0022,
      "step": 49940
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.5805887579917908,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.0025,
      "step": 49950
    },
    {
      "epoch": 4.440888888888889,
      "grad_norm": 0.6621120572090149,
      "learning_rate": 2.2244444444444446e-05,
      "loss": 0.0014,
      "step": 49960
    },
    {
      "epoch": 4.441777777777777,
      "grad_norm": 0.11351671814918518,
      "learning_rate": 2.223888888888889e-05,
      "loss": 0.0017,
      "step": 49970
    },
    {
      "epoch": 4.442666666666667,
      "grad_norm": 0.022954095155000687,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0024,
      "step": 49980
    },
    {
      "epoch": 4.443555555555555,
      "grad_norm": 0.18417669832706451,
      "learning_rate": 2.2227777777777776e-05,
      "loss": 0.0025,
      "step": 49990
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.45711424946784973,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.0021,
      "step": 50000
    },
    {
      "epoch": 4.445333333333333,
      "grad_norm": 0.6551417112350464,
      "learning_rate": 2.221666666666667e-05,
      "loss": 0.0026,
      "step": 50010
    },
    {
      "epoch": 4.4462222222222225,
      "grad_norm": 1.1726884841918945,
      "learning_rate": 2.2211111111111114e-05,
      "loss": 0.002,
      "step": 50020
    },
    {
      "epoch": 4.447111111111111,
      "grad_norm": 0.48974400758743286,
      "learning_rate": 2.2205555555555557e-05,
      "loss": 0.0024,
      "step": 50030
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.46355852484703064,
      "learning_rate": 2.22e-05,
      "loss": 0.0025,
      "step": 50040
    },
    {
      "epoch": 4.448888888888889,
      "grad_norm": 0.6414856314659119,
      "learning_rate": 2.2194444444444444e-05,
      "loss": 0.0019,
      "step": 50050
    },
    {
      "epoch": 4.449777777777777,
      "grad_norm": 0.2633740305900574,
      "learning_rate": 2.2188888888888888e-05,
      "loss": 0.0015,
      "step": 50060
    },
    {
      "epoch": 4.450666666666667,
      "grad_norm": 0.1797042042016983,
      "learning_rate": 2.2183333333333335e-05,
      "loss": 0.0029,
      "step": 50070
    },
    {
      "epoch": 4.451555555555555,
      "grad_norm": 0.7157520055770874,
      "learning_rate": 2.217777777777778e-05,
      "loss": 0.002,
      "step": 50080
    },
    {
      "epoch": 4.452444444444445,
      "grad_norm": 0.606148898601532,
      "learning_rate": 2.2172222222222222e-05,
      "loss": 0.0022,
      "step": 50090
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.3604739308357239,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.002,
      "step": 50100
    },
    {
      "epoch": 4.4542222222222225,
      "grad_norm": 0.06637880951166153,
      "learning_rate": 2.2161111111111113e-05,
      "loss": 0.0023,
      "step": 50110
    },
    {
      "epoch": 4.455111111111111,
      "grad_norm": 0.09475952386856079,
      "learning_rate": 2.2155555555555556e-05,
      "loss": 0.0019,
      "step": 50120
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.6193023920059204,
      "learning_rate": 2.215e-05,
      "loss": 0.002,
      "step": 50130
    },
    {
      "epoch": 4.456888888888889,
      "grad_norm": 0.2681851387023926,
      "learning_rate": 2.2144444444444447e-05,
      "loss": 0.0021,
      "step": 50140
    },
    {
      "epoch": 4.457777777777777,
      "grad_norm": 0.6703834533691406,
      "learning_rate": 2.213888888888889e-05,
      "loss": 0.0027,
      "step": 50150
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.06451340019702911,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0019,
      "step": 50160
    },
    {
      "epoch": 4.459555555555555,
      "grad_norm": 0.39960262179374695,
      "learning_rate": 2.2127777777777777e-05,
      "loss": 0.0018,
      "step": 50170
    },
    {
      "epoch": 4.460444444444445,
      "grad_norm": 0.07401521503925323,
      "learning_rate": 2.212222222222222e-05,
      "loss": 0.0021,
      "step": 50180
    },
    {
      "epoch": 4.461333333333333,
      "grad_norm": 0.5717136263847351,
      "learning_rate": 2.2116666666666668e-05,
      "loss": 0.0018,
      "step": 50190
    },
    {
      "epoch": 4.4622222222222225,
      "grad_norm": 0.5389741659164429,
      "learning_rate": 2.211111111111111e-05,
      "loss": 0.0019,
      "step": 50200
    },
    {
      "epoch": 4.463111111111111,
      "grad_norm": 0.4805822968482971,
      "learning_rate": 2.2105555555555558e-05,
      "loss": 0.0018,
      "step": 50210
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.21431060135364532,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0022,
      "step": 50220
    },
    {
      "epoch": 4.464888888888889,
      "grad_norm": 0.1669791340827942,
      "learning_rate": 2.2094444444444445e-05,
      "loss": 0.0031,
      "step": 50230
    },
    {
      "epoch": 4.465777777777777,
      "grad_norm": 0.0507642924785614,
      "learning_rate": 2.208888888888889e-05,
      "loss": 0.0017,
      "step": 50240
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 1.0377216339111328,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.002,
      "step": 50250
    },
    {
      "epoch": 4.467555555555555,
      "grad_norm": 0.45025748014450073,
      "learning_rate": 2.207777777777778e-05,
      "loss": 0.0017,
      "step": 50260
    },
    {
      "epoch": 4.468444444444445,
      "grad_norm": 0.15169066190719604,
      "learning_rate": 2.2072222222222223e-05,
      "loss": 0.0021,
      "step": 50270
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.4381049871444702,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0029,
      "step": 50280
    },
    {
      "epoch": 4.4702222222222225,
      "grad_norm": 0.1276959627866745,
      "learning_rate": 2.2061111111111114e-05,
      "loss": 0.0025,
      "step": 50290
    },
    {
      "epoch": 4.471111111111111,
      "grad_norm": 0.11231586337089539,
      "learning_rate": 2.2055555555555557e-05,
      "loss": 0.002,
      "step": 50300
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.34297290444374084,
      "learning_rate": 2.205e-05,
      "loss": 0.0016,
      "step": 50310
    },
    {
      "epoch": 4.472888888888889,
      "grad_norm": 0.4325990080833435,
      "learning_rate": 2.2044444444444444e-05,
      "loss": 0.0022,
      "step": 50320
    },
    {
      "epoch": 4.473777777777777,
      "grad_norm": 0.4851195812225342,
      "learning_rate": 2.203888888888889e-05,
      "loss": 0.002,
      "step": 50330
    },
    {
      "epoch": 4.474666666666667,
      "grad_norm": 0.1227463111281395,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0021,
      "step": 50340
    },
    {
      "epoch": 4.475555555555555,
      "grad_norm": 0.8709459900856018,
      "learning_rate": 2.2027777777777778e-05,
      "loss": 0.0018,
      "step": 50350
    },
    {
      "epoch": 4.476444444444445,
      "grad_norm": 0.08505062758922577,
      "learning_rate": 2.2022222222222225e-05,
      "loss": 0.0022,
      "step": 50360
    },
    {
      "epoch": 4.477333333333333,
      "grad_norm": 0.059768691658973694,
      "learning_rate": 2.201666666666667e-05,
      "loss": 0.0017,
      "step": 50370
    },
    {
      "epoch": 4.4782222222222225,
      "grad_norm": 0.4110429286956787,
      "learning_rate": 2.2011111111111112e-05,
      "loss": 0.0021,
      "step": 50380
    },
    {
      "epoch": 4.479111111111111,
      "grad_norm": 0.11451330035924911,
      "learning_rate": 2.2005555555555556e-05,
      "loss": 0.0019,
      "step": 50390
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.33561190962791443,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0018,
      "step": 50400
    },
    {
      "epoch": 4.480888888888889,
      "grad_norm": 0.10137700289487839,
      "learning_rate": 2.1994444444444446e-05,
      "loss": 0.0021,
      "step": 50410
    },
    {
      "epoch": 4.481777777777777,
      "grad_norm": 0.6570439338684082,
      "learning_rate": 2.198888888888889e-05,
      "loss": 0.0029,
      "step": 50420
    },
    {
      "epoch": 4.482666666666667,
      "grad_norm": 0.2613283693790436,
      "learning_rate": 2.1983333333333333e-05,
      "loss": 0.003,
      "step": 50430
    },
    {
      "epoch": 4.483555555555555,
      "grad_norm": 0.3475192189216614,
      "learning_rate": 2.1977777777777777e-05,
      "loss": 0.0023,
      "step": 50440
    },
    {
      "epoch": 4.484444444444445,
      "grad_norm": 0.39090782403945923,
      "learning_rate": 2.1972222222222224e-05,
      "loss": 0.0019,
      "step": 50450
    },
    {
      "epoch": 4.485333333333333,
      "grad_norm": 0.824916660785675,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.0023,
      "step": 50460
    },
    {
      "epoch": 4.4862222222222226,
      "grad_norm": 0.38104772567749023,
      "learning_rate": 2.1961111111111114e-05,
      "loss": 0.002,
      "step": 50470
    },
    {
      "epoch": 4.487111111111111,
      "grad_norm": 0.4469088912010193,
      "learning_rate": 2.1955555555555558e-05,
      "loss": 0.0015,
      "step": 50480
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.3134021461009979,
      "learning_rate": 2.195e-05,
      "loss": 0.0018,
      "step": 50490
    },
    {
      "epoch": 4.488888888888889,
      "grad_norm": 0.36389902234077454,
      "learning_rate": 2.1944444444444445e-05,
      "loss": 0.0019,
      "step": 50500
    },
    {
      "epoch": 4.489777777777777,
      "grad_norm": 0.11793708801269531,
      "learning_rate": 2.193888888888889e-05,
      "loss": 0.0018,
      "step": 50510
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.23279523849487305,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0016,
      "step": 50520
    },
    {
      "epoch": 4.491555555555555,
      "grad_norm": 0.12855109572410583,
      "learning_rate": 2.192777777777778e-05,
      "loss": 0.0022,
      "step": 50530
    },
    {
      "epoch": 4.492444444444445,
      "grad_norm": 0.08400225639343262,
      "learning_rate": 2.1922222222222226e-05,
      "loss": 0.0021,
      "step": 50540
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.5245740413665771,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.0019,
      "step": 50550
    },
    {
      "epoch": 4.494222222222223,
      "grad_norm": 0.13061369955539703,
      "learning_rate": 2.1911111111111113e-05,
      "loss": 0.0022,
      "step": 50560
    },
    {
      "epoch": 4.495111111111111,
      "grad_norm": 0.41819390654563904,
      "learning_rate": 2.1905555555555557e-05,
      "loss": 0.0019,
      "step": 50570
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.28234484791755676,
      "learning_rate": 2.19e-05,
      "loss": 0.0026,
      "step": 50580
    },
    {
      "epoch": 4.496888888888889,
      "grad_norm": 0.25395673513412476,
      "learning_rate": 2.1894444444444444e-05,
      "loss": 0.0018,
      "step": 50590
    },
    {
      "epoch": 4.497777777777777,
      "grad_norm": 0.07468371838331223,
      "learning_rate": 2.188888888888889e-05,
      "loss": 0.0026,
      "step": 50600
    },
    {
      "epoch": 4.498666666666667,
      "grad_norm": 0.49756065011024475,
      "learning_rate": 2.1883333333333334e-05,
      "loss": 0.0023,
      "step": 50610
    },
    {
      "epoch": 4.499555555555555,
      "grad_norm": 0.223123237490654,
      "learning_rate": 2.1877777777777778e-05,
      "loss": 0.0027,
      "step": 50620
    },
    {
      "epoch": 4.500444444444445,
      "grad_norm": 0.37854889035224915,
      "learning_rate": 2.1872222222222225e-05,
      "loss": 0.002,
      "step": 50630
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.3823757469654083,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0016,
      "step": 50640
    },
    {
      "epoch": 4.502222222222223,
      "grad_norm": 0.5326998829841614,
      "learning_rate": 2.1861111111111112e-05,
      "loss": 0.002,
      "step": 50650
    },
    {
      "epoch": 4.503111111111111,
      "grad_norm": 0.34329602122306824,
      "learning_rate": 2.1855555555555556e-05,
      "loss": 0.0018,
      "step": 50660
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.30277732014656067,
      "learning_rate": 2.1850000000000003e-05,
      "loss": 0.003,
      "step": 50670
    },
    {
      "epoch": 4.504888888888889,
      "grad_norm": 0.20762167870998383,
      "learning_rate": 2.1844444444444446e-05,
      "loss": 0.0022,
      "step": 50680
    },
    {
      "epoch": 4.505777777777777,
      "grad_norm": 0.4627375304698944,
      "learning_rate": 2.183888888888889e-05,
      "loss": 0.0026,
      "step": 50690
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.8517150282859802,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0028,
      "step": 50700
    },
    {
      "epoch": 4.507555555555555,
      "grad_norm": 0.4229358732700348,
      "learning_rate": 2.1827777777777777e-05,
      "loss": 0.0023,
      "step": 50710
    },
    {
      "epoch": 4.508444444444445,
      "grad_norm": 0.07271187007427216,
      "learning_rate": 2.1822222222222224e-05,
      "loss": 0.0019,
      "step": 50720
    },
    {
      "epoch": 4.509333333333333,
      "grad_norm": 0.2662869691848755,
      "learning_rate": 2.1816666666666667e-05,
      "loss": 0.0021,
      "step": 50730
    },
    {
      "epoch": 4.510222222222223,
      "grad_norm": 0.3250778913497925,
      "learning_rate": 2.1811111111111114e-05,
      "loss": 0.0019,
      "step": 50740
    },
    {
      "epoch": 4.511111111111111,
      "grad_norm": 0.19093140959739685,
      "learning_rate": 2.1805555555555558e-05,
      "loss": 0.0019,
      "step": 50750
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.5028490424156189,
      "learning_rate": 2.18e-05,
      "loss": 0.0016,
      "step": 50760
    },
    {
      "epoch": 4.512888888888889,
      "grad_norm": 0.3061544895172119,
      "learning_rate": 2.1794444444444445e-05,
      "loss": 0.0015,
      "step": 50770
    },
    {
      "epoch": 4.5137777777777774,
      "grad_norm": 0.3686320185661316,
      "learning_rate": 2.178888888888889e-05,
      "loss": 0.0027,
      "step": 50780
    },
    {
      "epoch": 4.514666666666667,
      "grad_norm": 0.15191146731376648,
      "learning_rate": 2.1783333333333332e-05,
      "loss": 0.002,
      "step": 50790
    },
    {
      "epoch": 4.515555555555555,
      "grad_norm": 0.2747294306755066,
      "learning_rate": 2.177777777777778e-05,
      "loss": 0.0014,
      "step": 50800
    },
    {
      "epoch": 4.516444444444445,
      "grad_norm": 0.2644781470298767,
      "learning_rate": 2.1772222222222226e-05,
      "loss": 0.0018,
      "step": 50810
    },
    {
      "epoch": 4.517333333333333,
      "grad_norm": 0.18647751212120056,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0018,
      "step": 50820
    },
    {
      "epoch": 4.518222222222223,
      "grad_norm": 0.1539071947336197,
      "learning_rate": 2.1761111111111113e-05,
      "loss": 0.0013,
      "step": 50830
    },
    {
      "epoch": 4.519111111111111,
      "grad_norm": 0.6014145016670227,
      "learning_rate": 2.1755555555555557e-05,
      "loss": 0.0031,
      "step": 50840
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.48926636576652527,
      "learning_rate": 2.175e-05,
      "loss": 0.0016,
      "step": 50850
    },
    {
      "epoch": 4.520888888888889,
      "grad_norm": 0.15522463619709015,
      "learning_rate": 2.1744444444444444e-05,
      "loss": 0.002,
      "step": 50860
    },
    {
      "epoch": 4.5217777777777775,
      "grad_norm": 0.10713666677474976,
      "learning_rate": 2.173888888888889e-05,
      "loss": 0.0016,
      "step": 50870
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.05896732211112976,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0017,
      "step": 50880
    },
    {
      "epoch": 4.523555555555555,
      "grad_norm": 0.5100424885749817,
      "learning_rate": 2.1727777777777778e-05,
      "loss": 0.0029,
      "step": 50890
    },
    {
      "epoch": 4.524444444444445,
      "grad_norm": 0.07368451356887817,
      "learning_rate": 2.1722222222222225e-05,
      "loss": 0.0013,
      "step": 50900
    },
    {
      "epoch": 4.525333333333333,
      "grad_norm": 0.40627309679985046,
      "learning_rate": 2.1716666666666668e-05,
      "loss": 0.0016,
      "step": 50910
    },
    {
      "epoch": 4.526222222222223,
      "grad_norm": 0.8088863492012024,
      "learning_rate": 2.1711111111111112e-05,
      "loss": 0.0016,
      "step": 50920
    },
    {
      "epoch": 4.527111111111111,
      "grad_norm": 0.4471323788166046,
      "learning_rate": 2.1705555555555555e-05,
      "loss": 0.0016,
      "step": 50930
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.26542308926582336,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.0019,
      "step": 50940
    },
    {
      "epoch": 4.528888888888889,
      "grad_norm": 0.43591567873954773,
      "learning_rate": 2.1694444444444446e-05,
      "loss": 0.0016,
      "step": 50950
    },
    {
      "epoch": 4.5297777777777775,
      "grad_norm": 0.8522652983665466,
      "learning_rate": 2.168888888888889e-05,
      "loss": 0.0022,
      "step": 50960
    },
    {
      "epoch": 4.530666666666667,
      "grad_norm": 0.37743180990219116,
      "learning_rate": 2.1683333333333333e-05,
      "loss": 0.0016,
      "step": 50970
    },
    {
      "epoch": 4.531555555555555,
      "grad_norm": 0.11353591084480286,
      "learning_rate": 2.167777777777778e-05,
      "loss": 0.0021,
      "step": 50980
    },
    {
      "epoch": 4.532444444444445,
      "grad_norm": 0.059662267565727234,
      "learning_rate": 2.1672222222222223e-05,
      "loss": 0.0019,
      "step": 50990
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.6923481822013855,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0024,
      "step": 51000
    },
    {
      "epoch": 4.534222222222223,
      "grad_norm": 0.15397892892360687,
      "learning_rate": 2.1661111111111114e-05,
      "loss": 0.0013,
      "step": 51010
    },
    {
      "epoch": 4.535111111111111,
      "grad_norm": 0.15243515372276306,
      "learning_rate": 2.1655555555555558e-05,
      "loss": 0.0016,
      "step": 51020
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.07722193002700806,
      "learning_rate": 2.165e-05,
      "loss": 0.0019,
      "step": 51030
    },
    {
      "epoch": 4.536888888888889,
      "grad_norm": 0.47973543405532837,
      "learning_rate": 2.1644444444444445e-05,
      "loss": 0.0015,
      "step": 51040
    },
    {
      "epoch": 4.5377777777777775,
      "grad_norm": 0.26381993293762207,
      "learning_rate": 2.1638888888888888e-05,
      "loss": 0.0026,
      "step": 51050
    },
    {
      "epoch": 4.538666666666667,
      "grad_norm": 0.6659427285194397,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0027,
      "step": 51060
    },
    {
      "epoch": 4.539555555555555,
      "grad_norm": 0.12644977867603302,
      "learning_rate": 2.162777777777778e-05,
      "loss": 0.002,
      "step": 51070
    },
    {
      "epoch": 4.540444444444445,
      "grad_norm": 0.11723347753286362,
      "learning_rate": 2.1622222222222226e-05,
      "loss": 0.001,
      "step": 51080
    },
    {
      "epoch": 4.541333333333333,
      "grad_norm": 0.4426576793193817,
      "learning_rate": 2.161666666666667e-05,
      "loss": 0.0023,
      "step": 51090
    },
    {
      "epoch": 4.542222222222223,
      "grad_norm": 0.4984496533870697,
      "learning_rate": 2.1611111111111113e-05,
      "loss": 0.0016,
      "step": 51100
    },
    {
      "epoch": 4.543111111111111,
      "grad_norm": 0.1578822284936905,
      "learning_rate": 2.1605555555555556e-05,
      "loss": 0.0016,
      "step": 51110
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.0492352694272995,
      "learning_rate": 2.16e-05,
      "loss": 0.0017,
      "step": 51120
    },
    {
      "epoch": 4.544888888888889,
      "grad_norm": 0.40792644023895264,
      "learning_rate": 2.1594444444444443e-05,
      "loss": 0.0025,
      "step": 51130
    },
    {
      "epoch": 4.5457777777777775,
      "grad_norm": 0.5049899816513062,
      "learning_rate": 2.158888888888889e-05,
      "loss": 0.0028,
      "step": 51140
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.41376516222953796,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.0024,
      "step": 51150
    },
    {
      "epoch": 4.547555555555555,
      "grad_norm": 0.12389001995325089,
      "learning_rate": 2.157777777777778e-05,
      "loss": 0.0022,
      "step": 51160
    },
    {
      "epoch": 4.548444444444445,
      "grad_norm": 0.7131590247154236,
      "learning_rate": 2.1572222222222224e-05,
      "loss": 0.0016,
      "step": 51170
    },
    {
      "epoch": 4.549333333333333,
      "grad_norm": 0.08197809010744095,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.002,
      "step": 51180
    },
    {
      "epoch": 4.550222222222223,
      "grad_norm": 0.08081133663654327,
      "learning_rate": 2.156111111111111e-05,
      "loss": 0.0022,
      "step": 51190
    },
    {
      "epoch": 4.551111111111111,
      "grad_norm": 0.5874525308609009,
      "learning_rate": 2.1555555555555555e-05,
      "loss": 0.0022,
      "step": 51200
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.22086724638938904,
      "learning_rate": 2.1550000000000002e-05,
      "loss": 0.002,
      "step": 51210
    },
    {
      "epoch": 4.552888888888889,
      "grad_norm": 0.11305305361747742,
      "learning_rate": 2.1544444444444446e-05,
      "loss": 0.0027,
      "step": 51220
    },
    {
      "epoch": 4.5537777777777775,
      "grad_norm": 0.7832844257354736,
      "learning_rate": 2.153888888888889e-05,
      "loss": 0.0016,
      "step": 51230
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.513455331325531,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0018,
      "step": 51240
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.5939642786979675,
      "learning_rate": 2.152777777777778e-05,
      "loss": 0.003,
      "step": 51250
    },
    {
      "epoch": 4.556444444444445,
      "grad_norm": 0.05991094559431076,
      "learning_rate": 2.1522222222222223e-05,
      "loss": 0.0027,
      "step": 51260
    },
    {
      "epoch": 4.557333333333333,
      "grad_norm": 0.4397616386413574,
      "learning_rate": 2.1516666666666667e-05,
      "loss": 0.0016,
      "step": 51270
    },
    {
      "epoch": 4.558222222222222,
      "grad_norm": 0.223276749253273,
      "learning_rate": 2.1511111111111114e-05,
      "loss": 0.0025,
      "step": 51280
    },
    {
      "epoch": 4.559111111111111,
      "grad_norm": 0.3306960463523865,
      "learning_rate": 2.1505555555555557e-05,
      "loss": 0.0019,
      "step": 51290
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.630180299282074,
      "learning_rate": 2.15e-05,
      "loss": 0.002,
      "step": 51300
    },
    {
      "epoch": 4.560888888888889,
      "grad_norm": 0.6038733124732971,
      "learning_rate": 2.1494444444444444e-05,
      "loss": 0.0016,
      "step": 51310
    },
    {
      "epoch": 4.5617777777777775,
      "grad_norm": 0.06994766741991043,
      "learning_rate": 2.1488888888888888e-05,
      "loss": 0.0018,
      "step": 51320
    },
    {
      "epoch": 4.562666666666667,
      "grad_norm": 0.2633468210697174,
      "learning_rate": 2.148333333333333e-05,
      "loss": 0.0018,
      "step": 51330
    },
    {
      "epoch": 4.563555555555555,
      "grad_norm": 0.21938581764698029,
      "learning_rate": 2.147777777777778e-05,
      "loss": 0.0024,
      "step": 51340
    },
    {
      "epoch": 4.564444444444445,
      "grad_norm": 0.21692170202732086,
      "learning_rate": 2.1472222222222225e-05,
      "loss": 0.0018,
      "step": 51350
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.08172543346881866,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0014,
      "step": 51360
    },
    {
      "epoch": 4.566222222222223,
      "grad_norm": 0.382633775472641,
      "learning_rate": 2.1461111111111112e-05,
      "loss": 0.0013,
      "step": 51370
    },
    {
      "epoch": 4.567111111111111,
      "grad_norm": 0.1286284476518631,
      "learning_rate": 2.1455555555555556e-05,
      "loss": 0.0017,
      "step": 51380
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.20901541411876678,
      "learning_rate": 2.145e-05,
      "loss": 0.0019,
      "step": 51390
    },
    {
      "epoch": 4.568888888888889,
      "grad_norm": 0.08808781951665878,
      "learning_rate": 2.1444444444444443e-05,
      "loss": 0.0023,
      "step": 51400
    },
    {
      "epoch": 4.5697777777777775,
      "grad_norm": 0.5257804989814758,
      "learning_rate": 2.143888888888889e-05,
      "loss": 0.0027,
      "step": 51410
    },
    {
      "epoch": 4.570666666666667,
      "grad_norm": 0.4464107155799866,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.0022,
      "step": 51420
    },
    {
      "epoch": 4.571555555555555,
      "grad_norm": 0.8168364763259888,
      "learning_rate": 2.142777777777778e-05,
      "loss": 0.0015,
      "step": 51430
    },
    {
      "epoch": 4.572444444444445,
      "grad_norm": 0.3477766215801239,
      "learning_rate": 2.1422222222222224e-05,
      "loss": 0.002,
      "step": 51440
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.08617927879095078,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.0021,
      "step": 51450
    },
    {
      "epoch": 4.574222222222222,
      "grad_norm": 0.37503907084465027,
      "learning_rate": 2.141111111111111e-05,
      "loss": 0.0021,
      "step": 51460
    },
    {
      "epoch": 4.575111111111111,
      "grad_norm": 0.46422654390335083,
      "learning_rate": 2.1405555555555555e-05,
      "loss": 0.0018,
      "step": 51470
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.5883841514587402,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0018,
      "step": 51480
    },
    {
      "epoch": 4.576888888888889,
      "grad_norm": 0.7877354025840759,
      "learning_rate": 2.1394444444444445e-05,
      "loss": 0.002,
      "step": 51490
    },
    {
      "epoch": 4.5777777777777775,
      "grad_norm": 0.36349621415138245,
      "learning_rate": 2.138888888888889e-05,
      "loss": 0.0033,
      "step": 51500
    },
    {
      "epoch": 4.578666666666667,
      "grad_norm": 0.4364564120769501,
      "learning_rate": 2.1383333333333332e-05,
      "loss": 0.0022,
      "step": 51510
    },
    {
      "epoch": 4.579555555555555,
      "grad_norm": 0.4727902114391327,
      "learning_rate": 2.137777777777778e-05,
      "loss": 0.0021,
      "step": 51520
    },
    {
      "epoch": 4.580444444444445,
      "grad_norm": 0.23571746051311493,
      "learning_rate": 2.1372222222222223e-05,
      "loss": 0.0024,
      "step": 51530
    },
    {
      "epoch": 4.581333333333333,
      "grad_norm": 0.043511949479579926,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.002,
      "step": 51540
    },
    {
      "epoch": 4.582222222222223,
      "grad_norm": 0.33588895201683044,
      "learning_rate": 2.1361111111111113e-05,
      "loss": 0.0018,
      "step": 51550
    },
    {
      "epoch": 4.583111111111111,
      "grad_norm": 0.7035983800888062,
      "learning_rate": 2.1355555555555557e-05,
      "loss": 0.0022,
      "step": 51560
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.20057590305805206,
      "learning_rate": 2.135e-05,
      "loss": 0.0017,
      "step": 51570
    },
    {
      "epoch": 4.584888888888889,
      "grad_norm": 0.33682942390441895,
      "learning_rate": 2.1344444444444444e-05,
      "loss": 0.0019,
      "step": 51580
    },
    {
      "epoch": 4.5857777777777775,
      "grad_norm": 0.11272323131561279,
      "learning_rate": 2.1338888888888888e-05,
      "loss": 0.0016,
      "step": 51590
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.34413477778434753,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0019,
      "step": 51600
    },
    {
      "epoch": 4.587555555555555,
      "grad_norm": 0.09021689742803574,
      "learning_rate": 2.1327777777777778e-05,
      "loss": 0.0021,
      "step": 51610
    },
    {
      "epoch": 4.588444444444445,
      "grad_norm": 0.07657098770141602,
      "learning_rate": 2.1322222222222225e-05,
      "loss": 0.0021,
      "step": 51620
    },
    {
      "epoch": 4.589333333333333,
      "grad_norm": 0.2249188870191574,
      "learning_rate": 2.131666666666667e-05,
      "loss": 0.0022,
      "step": 51630
    },
    {
      "epoch": 4.590222222222222,
      "grad_norm": 0.1661452054977417,
      "learning_rate": 2.1311111111111112e-05,
      "loss": 0.0018,
      "step": 51640
    },
    {
      "epoch": 4.591111111111111,
      "grad_norm": 0.4111296534538269,
      "learning_rate": 2.1305555555555556e-05,
      "loss": 0.0017,
      "step": 51650
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.8872688412666321,
      "learning_rate": 2.13e-05,
      "loss": 0.0023,
      "step": 51660
    },
    {
      "epoch": 4.592888888888889,
      "grad_norm": 0.6275395750999451,
      "learning_rate": 2.1294444444444446e-05,
      "loss": 0.0016,
      "step": 51670
    },
    {
      "epoch": 4.5937777777777775,
      "grad_norm": 0.5561653971672058,
      "learning_rate": 2.128888888888889e-05,
      "loss": 0.0014,
      "step": 51680
    },
    {
      "epoch": 4.594666666666667,
      "grad_norm": 0.3425968587398529,
      "learning_rate": 2.1283333333333337e-05,
      "loss": 0.0023,
      "step": 51690
    },
    {
      "epoch": 4.595555555555555,
      "grad_norm": 0.36508095264434814,
      "learning_rate": 2.127777777777778e-05,
      "loss": 0.0017,
      "step": 51700
    },
    {
      "epoch": 4.596444444444445,
      "grad_norm": 0.11986739188432693,
      "learning_rate": 2.1272222222222224e-05,
      "loss": 0.0019,
      "step": 51710
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.4847363531589508,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0019,
      "step": 51720
    },
    {
      "epoch": 4.598222222222223,
      "grad_norm": 0.14507241547107697,
      "learning_rate": 2.126111111111111e-05,
      "loss": 0.0026,
      "step": 51730
    },
    {
      "epoch": 4.599111111111111,
      "grad_norm": 0.11528978496789932,
      "learning_rate": 2.1255555555555558e-05,
      "loss": 0.0017,
      "step": 51740
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.23392103612422943,
      "learning_rate": 2.125e-05,
      "loss": 0.0013,
      "step": 51750
    },
    {
      "epoch": 4.600888888888889,
      "grad_norm": 0.4112463593482971,
      "learning_rate": 2.1244444444444445e-05,
      "loss": 0.0014,
      "step": 51760
    },
    {
      "epoch": 4.6017777777777775,
      "grad_norm": 0.6241416931152344,
      "learning_rate": 2.123888888888889e-05,
      "loss": 0.0022,
      "step": 51770
    },
    {
      "epoch": 4.602666666666667,
      "grad_norm": 0.46239450573921204,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0027,
      "step": 51780
    },
    {
      "epoch": 4.603555555555555,
      "grad_norm": 0.7819933891296387,
      "learning_rate": 2.122777777777778e-05,
      "loss": 0.0019,
      "step": 51790
    },
    {
      "epoch": 4.604444444444445,
      "grad_norm": 0.023844145238399506,
      "learning_rate": 2.1222222222222223e-05,
      "loss": 0.0021,
      "step": 51800
    },
    {
      "epoch": 4.605333333333333,
      "grad_norm": 0.03666487708687782,
      "learning_rate": 2.121666666666667e-05,
      "loss": 0.0028,
      "step": 51810
    },
    {
      "epoch": 4.606222222222222,
      "grad_norm": 0.4429404139518738,
      "learning_rate": 2.1211111111111113e-05,
      "loss": 0.0011,
      "step": 51820
    },
    {
      "epoch": 4.607111111111111,
      "grad_norm": 0.26062750816345215,
      "learning_rate": 2.1205555555555557e-05,
      "loss": 0.0015,
      "step": 51830
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.34475576877593994,
      "learning_rate": 2.12e-05,
      "loss": 0.0014,
      "step": 51840
    },
    {
      "epoch": 4.608888888888889,
      "grad_norm": 0.2971302270889282,
      "learning_rate": 2.1194444444444444e-05,
      "loss": 0.0026,
      "step": 51850
    },
    {
      "epoch": 4.6097777777777775,
      "grad_norm": 0.5977447628974915,
      "learning_rate": 2.1188888888888887e-05,
      "loss": 0.0022,
      "step": 51860
    },
    {
      "epoch": 4.610666666666667,
      "grad_norm": 0.043080344796180725,
      "learning_rate": 2.1183333333333334e-05,
      "loss": 0.002,
      "step": 51870
    },
    {
      "epoch": 4.611555555555555,
      "grad_norm": 0.0516798235476017,
      "learning_rate": 2.117777777777778e-05,
      "loss": 0.0025,
      "step": 51880
    },
    {
      "epoch": 4.612444444444445,
      "grad_norm": 0.2980603575706482,
      "learning_rate": 2.1172222222222225e-05,
      "loss": 0.0016,
      "step": 51890
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.3836061358451843,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.003,
      "step": 51900
    },
    {
      "epoch": 4.614222222222223,
      "grad_norm": 0.5837014317512512,
      "learning_rate": 2.1161111111111112e-05,
      "loss": 0.0015,
      "step": 51910
    },
    {
      "epoch": 4.615111111111111,
      "grad_norm": 0.37731215357780457,
      "learning_rate": 2.1155555555555556e-05,
      "loss": 0.0016,
      "step": 51920
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.2665199041366577,
      "learning_rate": 2.115e-05,
      "loss": 0.0018,
      "step": 51930
    },
    {
      "epoch": 4.616888888888889,
      "grad_norm": 0.05056242272257805,
      "learning_rate": 2.1144444444444446e-05,
      "loss": 0.0024,
      "step": 51940
    },
    {
      "epoch": 4.6177777777777775,
      "grad_norm": 0.17029204964637756,
      "learning_rate": 2.113888888888889e-05,
      "loss": 0.0023,
      "step": 51950
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.5743028521537781,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0022,
      "step": 51960
    },
    {
      "epoch": 4.619555555555555,
      "grad_norm": 0.4889218509197235,
      "learning_rate": 2.112777777777778e-05,
      "loss": 0.0021,
      "step": 51970
    },
    {
      "epoch": 4.620444444444445,
      "grad_norm": 0.5296997427940369,
      "learning_rate": 2.1122222222222224e-05,
      "loss": 0.0013,
      "step": 51980
    },
    {
      "epoch": 4.621333333333333,
      "grad_norm": 0.27768686413764954,
      "learning_rate": 2.1116666666666667e-05,
      "loss": 0.0024,
      "step": 51990
    },
    {
      "epoch": 4.622222222222222,
      "grad_norm": 0.04858706519007683,
      "learning_rate": 2.111111111111111e-05,
      "loss": 0.0014,
      "step": 52000
    },
    {
      "epoch": 4.623111111111111,
      "grad_norm": 1.0891004800796509,
      "learning_rate": 2.1105555555555558e-05,
      "loss": 0.0022,
      "step": 52010
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.7809621095657349,
      "learning_rate": 2.11e-05,
      "loss": 0.0018,
      "step": 52020
    },
    {
      "epoch": 4.624888888888889,
      "grad_norm": 0.15398721396923065,
      "learning_rate": 2.1094444444444445e-05,
      "loss": 0.0015,
      "step": 52030
    },
    {
      "epoch": 4.6257777777777775,
      "grad_norm": 0.38239628076553345,
      "learning_rate": 2.108888888888889e-05,
      "loss": 0.0029,
      "step": 52040
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.5178632140159607,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.0029,
      "step": 52050
    },
    {
      "epoch": 4.627555555555555,
      "grad_norm": 0.5212499499320984,
      "learning_rate": 2.107777777777778e-05,
      "loss": 0.0014,
      "step": 52060
    },
    {
      "epoch": 4.628444444444445,
      "grad_norm": 0.164276123046875,
      "learning_rate": 2.1072222222222222e-05,
      "loss": 0.0016,
      "step": 52070
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.37385430932044983,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0028,
      "step": 52080
    },
    {
      "epoch": 4.630222222222223,
      "grad_norm": 0.2567359507083893,
      "learning_rate": 2.1061111111111113e-05,
      "loss": 0.0014,
      "step": 52090
    },
    {
      "epoch": 4.631111111111111,
      "grad_norm": 0.855781614780426,
      "learning_rate": 2.1055555555555556e-05,
      "loss": 0.0019,
      "step": 52100
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.09552118927240372,
      "learning_rate": 2.105e-05,
      "loss": 0.0032,
      "step": 52110
    },
    {
      "epoch": 4.632888888888889,
      "grad_norm": 0.46986931562423706,
      "learning_rate": 2.1044444444444444e-05,
      "loss": 0.0018,
      "step": 52120
    },
    {
      "epoch": 4.6337777777777776,
      "grad_norm": 0.6082994937896729,
      "learning_rate": 2.1038888888888887e-05,
      "loss": 0.0013,
      "step": 52130
    },
    {
      "epoch": 4.634666666666667,
      "grad_norm": 0.22908645868301392,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0023,
      "step": 52140
    },
    {
      "epoch": 4.635555555555555,
      "grad_norm": 0.1879235953092575,
      "learning_rate": 2.102777777777778e-05,
      "loss": 0.0023,
      "step": 52150
    },
    {
      "epoch": 4.636444444444445,
      "grad_norm": 0.6214584708213806,
      "learning_rate": 2.1022222222222225e-05,
      "loss": 0.0016,
      "step": 52160
    },
    {
      "epoch": 4.637333333333333,
      "grad_norm": 0.43922334909439087,
      "learning_rate": 2.1016666666666668e-05,
      "loss": 0.0021,
      "step": 52170
    },
    {
      "epoch": 4.638222222222222,
      "grad_norm": 0.295056015253067,
      "learning_rate": 2.1011111111111112e-05,
      "loss": 0.0011,
      "step": 52180
    },
    {
      "epoch": 4.639111111111111,
      "grad_norm": 0.14987319707870483,
      "learning_rate": 2.1005555555555555e-05,
      "loss": 0.002,
      "step": 52190
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.7806349992752075,
      "learning_rate": 2.1e-05,
      "loss": 0.0015,
      "step": 52200
    },
    {
      "epoch": 4.640888888888889,
      "grad_norm": 0.2628158926963806,
      "learning_rate": 2.0994444444444446e-05,
      "loss": 0.0014,
      "step": 52210
    },
    {
      "epoch": 4.641777777777778,
      "grad_norm": 0.05027557164430618,
      "learning_rate": 2.0988888888888893e-05,
      "loss": 0.0017,
      "step": 52220
    },
    {
      "epoch": 4.642666666666667,
      "grad_norm": 0.4337300956249237,
      "learning_rate": 2.0983333333333336e-05,
      "loss": 0.003,
      "step": 52230
    },
    {
      "epoch": 4.643555555555555,
      "grad_norm": 0.12606962025165558,
      "learning_rate": 2.097777777777778e-05,
      "loss": 0.0027,
      "step": 52240
    },
    {
      "epoch": 4.644444444444445,
      "grad_norm": 0.3734186589717865,
      "learning_rate": 2.0972222222222223e-05,
      "loss": 0.0018,
      "step": 52250
    },
    {
      "epoch": 4.645333333333333,
      "grad_norm": 0.45754343271255493,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0021,
      "step": 52260
    },
    {
      "epoch": 4.646222222222223,
      "grad_norm": 0.7793126702308655,
      "learning_rate": 2.096111111111111e-05,
      "loss": 0.0015,
      "step": 52270
    },
    {
      "epoch": 4.647111111111111,
      "grad_norm": 0.6117860078811646,
      "learning_rate": 2.0955555555555557e-05,
      "loss": 0.0025,
      "step": 52280
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.7163184881210327,
      "learning_rate": 2.095e-05,
      "loss": 0.0027,
      "step": 52290
    },
    {
      "epoch": 4.648888888888889,
      "grad_norm": 0.3671286404132843,
      "learning_rate": 2.0944444444444445e-05,
      "loss": 0.0025,
      "step": 52300
    },
    {
      "epoch": 4.649777777777778,
      "grad_norm": 0.6780605912208557,
      "learning_rate": 2.093888888888889e-05,
      "loss": 0.0022,
      "step": 52310
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.48654600977897644,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0031,
      "step": 52320
    },
    {
      "epoch": 4.651555555555555,
      "grad_norm": 0.7346788048744202,
      "learning_rate": 2.092777777777778e-05,
      "loss": 0.0027,
      "step": 52330
    },
    {
      "epoch": 4.652444444444445,
      "grad_norm": 0.44499775767326355,
      "learning_rate": 2.0922222222222222e-05,
      "loss": 0.0024,
      "step": 52340
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.2562597990036011,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.0026,
      "step": 52350
    },
    {
      "epoch": 4.654222222222222,
      "grad_norm": 0.07922136038541794,
      "learning_rate": 2.0911111111111113e-05,
      "loss": 0.0028,
      "step": 52360
    },
    {
      "epoch": 4.655111111111111,
      "grad_norm": 0.24178533256053925,
      "learning_rate": 2.0905555555555556e-05,
      "loss": 0.0019,
      "step": 52370
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.6740322709083557,
      "learning_rate": 2.09e-05,
      "loss": 0.0027,
      "step": 52380
    },
    {
      "epoch": 4.656888888888889,
      "grad_norm": 0.4278448522090912,
      "learning_rate": 2.0894444444444443e-05,
      "loss": 0.002,
      "step": 52390
    },
    {
      "epoch": 4.657777777777778,
      "grad_norm": 0.07701988518238068,
      "learning_rate": 2.088888888888889e-05,
      "loss": 0.0018,
      "step": 52400
    },
    {
      "epoch": 4.658666666666667,
      "grad_norm": 0.04939728602766991,
      "learning_rate": 2.0883333333333334e-05,
      "loss": 0.002,
      "step": 52410
    },
    {
      "epoch": 4.6595555555555555,
      "grad_norm": 0.09435973316431046,
      "learning_rate": 2.087777777777778e-05,
      "loss": 0.0018,
      "step": 52420
    },
    {
      "epoch": 4.660444444444444,
      "grad_norm": 0.13503798842430115,
      "learning_rate": 2.0872222222222224e-05,
      "loss": 0.0032,
      "step": 52430
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.2882474660873413,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0017,
      "step": 52440
    },
    {
      "epoch": 4.662222222222223,
      "grad_norm": 0.22788572311401367,
      "learning_rate": 2.086111111111111e-05,
      "loss": 0.0025,
      "step": 52450
    },
    {
      "epoch": 4.663111111111111,
      "grad_norm": 0.059843819588422775,
      "learning_rate": 2.0855555555555555e-05,
      "loss": 0.0019,
      "step": 52460
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.07522901147603989,
      "learning_rate": 2.085e-05,
      "loss": 0.0023,
      "step": 52470
    },
    {
      "epoch": 4.664888888888889,
      "grad_norm": 0.09275523573160172,
      "learning_rate": 2.0844444444444446e-05,
      "loss": 0.002,
      "step": 52480
    },
    {
      "epoch": 4.665777777777778,
      "grad_norm": 0.4565128982067108,
      "learning_rate": 2.0838888888888892e-05,
      "loss": 0.0027,
      "step": 52490
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.29719477891921997,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.003,
      "step": 52500
    },
    {
      "epoch": 4.6675555555555555,
      "grad_norm": 0.33621591329574585,
      "learning_rate": 2.082777777777778e-05,
      "loss": 0.0017,
      "step": 52510
    },
    {
      "epoch": 4.668444444444445,
      "grad_norm": 0.6925986409187317,
      "learning_rate": 2.0822222222222223e-05,
      "loss": 0.002,
      "step": 52520
    },
    {
      "epoch": 4.669333333333333,
      "grad_norm": 0.2201746702194214,
      "learning_rate": 2.0816666666666667e-05,
      "loss": 0.003,
      "step": 52530
    },
    {
      "epoch": 4.670222222222222,
      "grad_norm": 0.20142753422260284,
      "learning_rate": 2.081111111111111e-05,
      "loss": 0.0018,
      "step": 52540
    },
    {
      "epoch": 4.671111111111111,
      "grad_norm": 0.45304372906684875,
      "learning_rate": 2.0805555555555557e-05,
      "loss": 0.0016,
      "step": 52550
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.3180229365825653,
      "learning_rate": 2.08e-05,
      "loss": 0.0023,
      "step": 52560
    },
    {
      "epoch": 4.672888888888889,
      "grad_norm": 0.6257461905479431,
      "learning_rate": 2.0794444444444444e-05,
      "loss": 0.0021,
      "step": 52570
    },
    {
      "epoch": 4.673777777777778,
      "grad_norm": 0.4265667498111725,
      "learning_rate": 2.078888888888889e-05,
      "loss": 0.0022,
      "step": 52580
    },
    {
      "epoch": 4.674666666666667,
      "grad_norm": 0.1066100150346756,
      "learning_rate": 2.0783333333333335e-05,
      "loss": 0.0025,
      "step": 52590
    },
    {
      "epoch": 4.6755555555555555,
      "grad_norm": 0.15231457352638245,
      "learning_rate": 2.077777777777778e-05,
      "loss": 0.0018,
      "step": 52600
    },
    {
      "epoch": 4.676444444444444,
      "grad_norm": 0.23365452885627747,
      "learning_rate": 2.0772222222222222e-05,
      "loss": 0.0017,
      "step": 52610
    },
    {
      "epoch": 4.677333333333333,
      "grad_norm": 0.035795196890830994,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.0012,
      "step": 52620
    },
    {
      "epoch": 4.678222222222222,
      "grad_norm": 0.08526121824979782,
      "learning_rate": 2.0761111111111112e-05,
      "loss": 0.0015,
      "step": 52630
    },
    {
      "epoch": 4.679111111111111,
      "grad_norm": 0.23342770338058472,
      "learning_rate": 2.0755555555555556e-05,
      "loss": 0.0016,
      "step": 52640
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.1577529013156891,
      "learning_rate": 2.075e-05,
      "loss": 0.0026,
      "step": 52650
    },
    {
      "epoch": 4.680888888888889,
      "grad_norm": 0.23074086010456085,
      "learning_rate": 2.0744444444444443e-05,
      "loss": 0.0022,
      "step": 52660
    },
    {
      "epoch": 4.681777777777778,
      "grad_norm": 0.6655169129371643,
      "learning_rate": 2.073888888888889e-05,
      "loss": 0.0017,
      "step": 52670
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.05422072112560272,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0017,
      "step": 52680
    },
    {
      "epoch": 4.6835555555555555,
      "grad_norm": 0.1566261351108551,
      "learning_rate": 2.072777777777778e-05,
      "loss": 0.0021,
      "step": 52690
    },
    {
      "epoch": 4.684444444444445,
      "grad_norm": 0.10195530205965042,
      "learning_rate": 2.0722222222222224e-05,
      "loss": 0.0027,
      "step": 52700
    },
    {
      "epoch": 4.685333333333333,
      "grad_norm": 0.11336066573858261,
      "learning_rate": 2.0716666666666668e-05,
      "loss": 0.0025,
      "step": 52710
    },
    {
      "epoch": 4.686222222222222,
      "grad_norm": 0.7052640914916992,
      "learning_rate": 2.071111111111111e-05,
      "loss": 0.0018,
      "step": 52720
    },
    {
      "epoch": 4.687111111111111,
      "grad_norm": 0.47345516085624695,
      "learning_rate": 2.0705555555555555e-05,
      "loss": 0.0023,
      "step": 52730
    },
    {
      "epoch": 4.688,
      "grad_norm": 1.0826284885406494,
      "learning_rate": 2.07e-05,
      "loss": 0.0022,
      "step": 52740
    },
    {
      "epoch": 4.688888888888889,
      "grad_norm": 0.7640681266784668,
      "learning_rate": 2.0694444444444445e-05,
      "loss": 0.0017,
      "step": 52750
    },
    {
      "epoch": 4.689777777777778,
      "grad_norm": 0.5388242602348328,
      "learning_rate": 2.0688888888888892e-05,
      "loss": 0.0024,
      "step": 52760
    },
    {
      "epoch": 4.690666666666667,
      "grad_norm": 0.5976336002349854,
      "learning_rate": 2.0683333333333336e-05,
      "loss": 0.0019,
      "step": 52770
    },
    {
      "epoch": 4.6915555555555555,
      "grad_norm": 0.2713552415370941,
      "learning_rate": 2.067777777777778e-05,
      "loss": 0.0018,
      "step": 52780
    },
    {
      "epoch": 4.692444444444444,
      "grad_norm": 0.29090920090675354,
      "learning_rate": 2.0672222222222223e-05,
      "loss": 0.0041,
      "step": 52790
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.42125117778778076,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0026,
      "step": 52800
    },
    {
      "epoch": 4.694222222222222,
      "grad_norm": 0.15741406381130219,
      "learning_rate": 2.066111111111111e-05,
      "loss": 0.002,
      "step": 52810
    },
    {
      "epoch": 4.695111111111111,
      "grad_norm": 0.6884933114051819,
      "learning_rate": 2.0655555555555557e-05,
      "loss": 0.0027,
      "step": 52820
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.18665741384029388,
      "learning_rate": 2.065e-05,
      "loss": 0.0019,
      "step": 52830
    },
    {
      "epoch": 4.696888888888889,
      "grad_norm": 0.26604411005973816,
      "learning_rate": 2.0644444444444447e-05,
      "loss": 0.002,
      "step": 52840
    },
    {
      "epoch": 4.697777777777778,
      "grad_norm": 0.6581094861030579,
      "learning_rate": 2.063888888888889e-05,
      "loss": 0.0023,
      "step": 52850
    },
    {
      "epoch": 4.698666666666667,
      "grad_norm": 0.5588610768318176,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.0021,
      "step": 52860
    },
    {
      "epoch": 4.6995555555555555,
      "grad_norm": 0.48220905661582947,
      "learning_rate": 2.0627777777777778e-05,
      "loss": 0.0024,
      "step": 52870
    },
    {
      "epoch": 4.700444444444445,
      "grad_norm": 0.1698712855577469,
      "learning_rate": 2.062222222222222e-05,
      "loss": 0.0021,
      "step": 52880
    },
    {
      "epoch": 4.701333333333333,
      "grad_norm": 0.19248594343662262,
      "learning_rate": 2.061666666666667e-05,
      "loss": 0.0026,
      "step": 52890
    },
    {
      "epoch": 4.702222222222222,
      "grad_norm": 0.4962577819824219,
      "learning_rate": 2.0611111111111112e-05,
      "loss": 0.0017,
      "step": 52900
    },
    {
      "epoch": 4.703111111111111,
      "grad_norm": 0.2911710739135742,
      "learning_rate": 2.0605555555555556e-05,
      "loss": 0.0021,
      "step": 52910
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.2242031693458557,
      "learning_rate": 2.06e-05,
      "loss": 0.0014,
      "step": 52920
    },
    {
      "epoch": 4.704888888888889,
      "grad_norm": 0.22797931730747223,
      "learning_rate": 2.0594444444444446e-05,
      "loss": 0.0022,
      "step": 52930
    },
    {
      "epoch": 4.705777777777778,
      "grad_norm": 0.6220492124557495,
      "learning_rate": 2.058888888888889e-05,
      "loss": 0.0021,
      "step": 52940
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.3389417231082916,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.0023,
      "step": 52950
    },
    {
      "epoch": 4.7075555555555555,
      "grad_norm": 0.2205602526664734,
      "learning_rate": 2.057777777777778e-05,
      "loss": 0.0019,
      "step": 52960
    },
    {
      "epoch": 4.708444444444444,
      "grad_norm": 0.17871803045272827,
      "learning_rate": 2.0572222222222224e-05,
      "loss": 0.0024,
      "step": 52970
    },
    {
      "epoch": 4.709333333333333,
      "grad_norm": 0.12785744667053223,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0018,
      "step": 52980
    },
    {
      "epoch": 4.710222222222222,
      "grad_norm": 0.11649719625711441,
      "learning_rate": 2.056111111111111e-05,
      "loss": 0.0022,
      "step": 52990
    },
    {
      "epoch": 4.711111111111111,
      "grad_norm": 0.30298393964767456,
      "learning_rate": 2.0555555555555555e-05,
      "loss": 0.0021,
      "step": 53000
    },
    {
      "epoch": 4.712,
      "grad_norm": 1.1174322366714478,
      "learning_rate": 2.055e-05,
      "loss": 0.0022,
      "step": 53010
    },
    {
      "epoch": 4.712888888888889,
      "grad_norm": 0.24409538507461548,
      "learning_rate": 2.054444444444445e-05,
      "loss": 0.0026,
      "step": 53020
    },
    {
      "epoch": 4.713777777777778,
      "grad_norm": 0.1327640861272812,
      "learning_rate": 2.0538888888888892e-05,
      "loss": 0.0023,
      "step": 53030
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.18417607247829437,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0012,
      "step": 53040
    },
    {
      "epoch": 4.7155555555555555,
      "grad_norm": 0.3140568435192108,
      "learning_rate": 2.052777777777778e-05,
      "loss": 0.0023,
      "step": 53050
    },
    {
      "epoch": 4.716444444444445,
      "grad_norm": 0.11649707704782486,
      "learning_rate": 2.0522222222222223e-05,
      "loss": 0.0024,
      "step": 53060
    },
    {
      "epoch": 4.717333333333333,
      "grad_norm": 0.3059178590774536,
      "learning_rate": 2.0516666666666666e-05,
      "loss": 0.0021,
      "step": 53070
    },
    {
      "epoch": 4.718222222222222,
      "grad_norm": 0.20300118625164032,
      "learning_rate": 2.0511111111111113e-05,
      "loss": 0.0023,
      "step": 53080
    },
    {
      "epoch": 4.719111111111111,
      "grad_norm": 0.10462985187768936,
      "learning_rate": 2.0505555555555557e-05,
      "loss": 0.002,
      "step": 53090
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.8238074779510498,
      "learning_rate": 2.05e-05,
      "loss": 0.0015,
      "step": 53100
    },
    {
      "epoch": 4.720888888888889,
      "grad_norm": 0.22089244425296783,
      "learning_rate": 2.0494444444444447e-05,
      "loss": 0.0026,
      "step": 53110
    },
    {
      "epoch": 4.721777777777778,
      "grad_norm": 0.2860661447048187,
      "learning_rate": 2.048888888888889e-05,
      "loss": 0.0018,
      "step": 53120
    },
    {
      "epoch": 4.722666666666667,
      "grad_norm": 0.24877993762493134,
      "learning_rate": 2.0483333333333334e-05,
      "loss": 0.002,
      "step": 53130
    },
    {
      "epoch": 4.7235555555555555,
      "grad_norm": 0.3174726068973541,
      "learning_rate": 2.0477777777777778e-05,
      "loss": 0.0023,
      "step": 53140
    },
    {
      "epoch": 4.724444444444444,
      "grad_norm": 0.5947439074516296,
      "learning_rate": 2.0472222222222225e-05,
      "loss": 0.0017,
      "step": 53150
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.9294384717941284,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0015,
      "step": 53160
    },
    {
      "epoch": 4.726222222222222,
      "grad_norm": 0.46530425548553467,
      "learning_rate": 2.0461111111111112e-05,
      "loss": 0.0021,
      "step": 53170
    },
    {
      "epoch": 4.727111111111111,
      "grad_norm": 0.12449460476636887,
      "learning_rate": 2.0455555555555555e-05,
      "loss": 0.0014,
      "step": 53180
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.2547681927680969,
      "learning_rate": 2.045e-05,
      "loss": 0.0024,
      "step": 53190
    },
    {
      "epoch": 4.728888888888889,
      "grad_norm": 0.6218032240867615,
      "learning_rate": 2.0444444444444446e-05,
      "loss": 0.0021,
      "step": 53200
    },
    {
      "epoch": 4.729777777777778,
      "grad_norm": 0.16619491577148438,
      "learning_rate": 2.043888888888889e-05,
      "loss": 0.0019,
      "step": 53210
    },
    {
      "epoch": 4.730666666666667,
      "grad_norm": 0.1161310151219368,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.0018,
      "step": 53220
    },
    {
      "epoch": 4.7315555555555555,
      "grad_norm": 0.0504591204226017,
      "learning_rate": 2.042777777777778e-05,
      "loss": 0.0025,
      "step": 53230
    },
    {
      "epoch": 4.732444444444445,
      "grad_norm": 0.29661038517951965,
      "learning_rate": 2.0422222222222224e-05,
      "loss": 0.0021,
      "step": 53240
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.1237882524728775,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.0018,
      "step": 53250
    },
    {
      "epoch": 4.734222222222222,
      "grad_norm": 0.6101939082145691,
      "learning_rate": 2.041111111111111e-05,
      "loss": 0.0016,
      "step": 53260
    },
    {
      "epoch": 4.735111111111111,
      "grad_norm": 0.19053485989570618,
      "learning_rate": 2.0405555555555554e-05,
      "loss": 0.0027,
      "step": 53270
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.37411370873451233,
      "learning_rate": 2.04e-05,
      "loss": 0.002,
      "step": 53280
    },
    {
      "epoch": 4.736888888888889,
      "grad_norm": 0.4539380669593811,
      "learning_rate": 2.0394444444444448e-05,
      "loss": 0.0019,
      "step": 53290
    },
    {
      "epoch": 4.737777777777778,
      "grad_norm": 0.07847239077091217,
      "learning_rate": 2.0388888888888892e-05,
      "loss": 0.0015,
      "step": 53300
    },
    {
      "epoch": 4.738666666666667,
      "grad_norm": 0.6740939617156982,
      "learning_rate": 2.0383333333333335e-05,
      "loss": 0.0017,
      "step": 53310
    },
    {
      "epoch": 4.7395555555555555,
      "grad_norm": 0.15440858900547028,
      "learning_rate": 2.037777777777778e-05,
      "loss": 0.0026,
      "step": 53320
    },
    {
      "epoch": 4.740444444444444,
      "grad_norm": 0.082066111266613,
      "learning_rate": 2.0372222222222222e-05,
      "loss": 0.0029,
      "step": 53330
    },
    {
      "epoch": 4.741333333333333,
      "grad_norm": 0.20528975129127502,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0018,
      "step": 53340
    },
    {
      "epoch": 4.742222222222222,
      "grad_norm": 0.07544339448213577,
      "learning_rate": 2.0361111111111113e-05,
      "loss": 0.0021,
      "step": 53350
    },
    {
      "epoch": 4.743111111111111,
      "grad_norm": 0.11461852490901947,
      "learning_rate": 2.0355555555555556e-05,
      "loss": 0.0027,
      "step": 53360
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.297373503446579,
      "learning_rate": 2.035e-05,
      "loss": 0.0023,
      "step": 53370
    },
    {
      "epoch": 4.744888888888889,
      "grad_norm": 0.5999830365180969,
      "learning_rate": 2.0344444444444447e-05,
      "loss": 0.0014,
      "step": 53380
    },
    {
      "epoch": 4.745777777777778,
      "grad_norm": 0.8170697689056396,
      "learning_rate": 2.033888888888889e-05,
      "loss": 0.0021,
      "step": 53390
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.5180659890174866,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0017,
      "step": 53400
    },
    {
      "epoch": 4.7475555555555555,
      "grad_norm": 0.17165684700012207,
      "learning_rate": 2.0327777777777778e-05,
      "loss": 0.0022,
      "step": 53410
    },
    {
      "epoch": 4.748444444444445,
      "grad_norm": 0.5615894794464111,
      "learning_rate": 2.0322222222222225e-05,
      "loss": 0.0021,
      "step": 53420
    },
    {
      "epoch": 4.749333333333333,
      "grad_norm": 0.9480522274971008,
      "learning_rate": 2.0316666666666668e-05,
      "loss": 0.0024,
      "step": 53430
    },
    {
      "epoch": 4.750222222222222,
      "grad_norm": 0.5384154915809631,
      "learning_rate": 2.031111111111111e-05,
      "loss": 0.0019,
      "step": 53440
    },
    {
      "epoch": 4.751111111111111,
      "grad_norm": 0.320361465215683,
      "learning_rate": 2.0305555555555555e-05,
      "loss": 0.0019,
      "step": 53450
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.6045504212379456,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0016,
      "step": 53460
    },
    {
      "epoch": 4.752888888888889,
      "grad_norm": 0.5294865965843201,
      "learning_rate": 2.0294444444444446e-05,
      "loss": 0.0022,
      "step": 53470
    },
    {
      "epoch": 4.753777777777778,
      "grad_norm": 0.14554396271705627,
      "learning_rate": 2.028888888888889e-05,
      "loss": 0.0022,
      "step": 53480
    },
    {
      "epoch": 4.754666666666667,
      "grad_norm": 0.11181008815765381,
      "learning_rate": 2.0283333333333336e-05,
      "loss": 0.0021,
      "step": 53490
    },
    {
      "epoch": 4.7555555555555555,
      "grad_norm": 0.3711530268192291,
      "learning_rate": 2.027777777777778e-05,
      "loss": 0.002,
      "step": 53500
    },
    {
      "epoch": 4.756444444444444,
      "grad_norm": 0.07577399909496307,
      "learning_rate": 2.0272222222222223e-05,
      "loss": 0.0014,
      "step": 53510
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.3099450469017029,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0023,
      "step": 53520
    },
    {
      "epoch": 4.758222222222222,
      "grad_norm": 0.6198700070381165,
      "learning_rate": 2.026111111111111e-05,
      "loss": 0.0016,
      "step": 53530
    },
    {
      "epoch": 4.759111111111111,
      "grad_norm": 0.14865858852863312,
      "learning_rate": 2.0255555555555554e-05,
      "loss": 0.0015,
      "step": 53540
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.685849130153656,
      "learning_rate": 2.025e-05,
      "loss": 0.003,
      "step": 53550
    },
    {
      "epoch": 4.760888888888889,
      "grad_norm": 0.43626365065574646,
      "learning_rate": 2.0244444444444448e-05,
      "loss": 0.002,
      "step": 53560
    },
    {
      "epoch": 4.761777777777778,
      "grad_norm": 0.6213109493255615,
      "learning_rate": 2.023888888888889e-05,
      "loss": 0.0024,
      "step": 53570
    },
    {
      "epoch": 4.762666666666667,
      "grad_norm": 0.31674888730049133,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.0026,
      "step": 53580
    },
    {
      "epoch": 4.7635555555555555,
      "grad_norm": 0.22262641787528992,
      "learning_rate": 2.022777777777778e-05,
      "loss": 0.0022,
      "step": 53590
    },
    {
      "epoch": 4.764444444444445,
      "grad_norm": 0.21829088032245636,
      "learning_rate": 2.0222222222222222e-05,
      "loss": 0.0017,
      "step": 53600
    },
    {
      "epoch": 4.765333333333333,
      "grad_norm": 0.4114004373550415,
      "learning_rate": 2.0216666666666666e-05,
      "loss": 0.0021,
      "step": 53610
    },
    {
      "epoch": 4.766222222222222,
      "grad_norm": 0.3975580632686615,
      "learning_rate": 2.0211111111111113e-05,
      "loss": 0.002,
      "step": 53620
    },
    {
      "epoch": 4.767111111111111,
      "grad_norm": 0.2616519033908844,
      "learning_rate": 2.0205555555555556e-05,
      "loss": 0.0018,
      "step": 53630
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.29885753989219666,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0022,
      "step": 53640
    },
    {
      "epoch": 4.768888888888889,
      "grad_norm": 0.38466453552246094,
      "learning_rate": 2.0194444444444447e-05,
      "loss": 0.0024,
      "step": 53650
    },
    {
      "epoch": 4.769777777777778,
      "grad_norm": 0.09675901383161545,
      "learning_rate": 2.018888888888889e-05,
      "loss": 0.0018,
      "step": 53660
    },
    {
      "epoch": 4.770666666666667,
      "grad_norm": 0.28104567527770996,
      "learning_rate": 2.0183333333333334e-05,
      "loss": 0.0023,
      "step": 53670
    },
    {
      "epoch": 4.7715555555555556,
      "grad_norm": 0.21006591618061066,
      "learning_rate": 2.0177777777777777e-05,
      "loss": 0.0016,
      "step": 53680
    },
    {
      "epoch": 4.772444444444444,
      "grad_norm": 0.05177466198801994,
      "learning_rate": 2.0172222222222224e-05,
      "loss": 0.0016,
      "step": 53690
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.06737600266933441,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0019,
      "step": 53700
    },
    {
      "epoch": 4.774222222222222,
      "grad_norm": 0.6815713047981262,
      "learning_rate": 2.016111111111111e-05,
      "loss": 0.002,
      "step": 53710
    },
    {
      "epoch": 4.775111111111111,
      "grad_norm": 0.7832688093185425,
      "learning_rate": 2.0155555555555555e-05,
      "loss": 0.0021,
      "step": 53720
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.19877289235591888,
      "learning_rate": 2.0150000000000002e-05,
      "loss": 0.0015,
      "step": 53730
    },
    {
      "epoch": 4.776888888888889,
      "grad_norm": 0.21372422575950623,
      "learning_rate": 2.0144444444444445e-05,
      "loss": 0.0026,
      "step": 53740
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.10336542129516602,
      "learning_rate": 2.013888888888889e-05,
      "loss": 0.002,
      "step": 53750
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.10711032897233963,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0027,
      "step": 53760
    },
    {
      "epoch": 4.779555555555556,
      "grad_norm": 0.49455639719963074,
      "learning_rate": 2.012777777777778e-05,
      "loss": 0.0026,
      "step": 53770
    },
    {
      "epoch": 4.780444444444445,
      "grad_norm": 0.3744238018989563,
      "learning_rate": 2.0122222222222223e-05,
      "loss": 0.0021,
      "step": 53780
    },
    {
      "epoch": 4.781333333333333,
      "grad_norm": 0.09527018666267395,
      "learning_rate": 2.0116666666666667e-05,
      "loss": 0.0013,
      "step": 53790
    },
    {
      "epoch": 4.782222222222222,
      "grad_norm": 0.4828241169452667,
      "learning_rate": 2.011111111111111e-05,
      "loss": 0.0027,
      "step": 53800
    },
    {
      "epoch": 4.783111111111111,
      "grad_norm": 0.3140363097190857,
      "learning_rate": 2.0105555555555554e-05,
      "loss": 0.0016,
      "step": 53810
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.19354718923568726,
      "learning_rate": 2.01e-05,
      "loss": 0.0014,
      "step": 53820
    },
    {
      "epoch": 4.784888888888889,
      "grad_norm": 0.595469057559967,
      "learning_rate": 2.0094444444444448e-05,
      "loss": 0.0026,
      "step": 53830
    },
    {
      "epoch": 4.785777777777778,
      "grad_norm": 0.09194251894950867,
      "learning_rate": 2.008888888888889e-05,
      "loss": 0.0018,
      "step": 53840
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.34026336669921875,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.0022,
      "step": 53850
    },
    {
      "epoch": 4.787555555555556,
      "grad_norm": 0.5926381945610046,
      "learning_rate": 2.0077777777777778e-05,
      "loss": 0.0017,
      "step": 53860
    },
    {
      "epoch": 4.788444444444444,
      "grad_norm": 0.8217204213142395,
      "learning_rate": 2.0072222222222222e-05,
      "loss": 0.0023,
      "step": 53870
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 0.3026900589466095,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.002,
      "step": 53880
    },
    {
      "epoch": 4.790222222222222,
      "grad_norm": 0.5784695148468018,
      "learning_rate": 2.0061111111111112e-05,
      "loss": 0.002,
      "step": 53890
    },
    {
      "epoch": 4.791111111111111,
      "grad_norm": 0.4131774604320526,
      "learning_rate": 2.0055555555555556e-05,
      "loss": 0.0021,
      "step": 53900
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.3947767913341522,
      "learning_rate": 2.0050000000000003e-05,
      "loss": 0.0016,
      "step": 53910
    },
    {
      "epoch": 4.792888888888889,
      "grad_norm": 0.22160041332244873,
      "learning_rate": 2.0044444444444446e-05,
      "loss": 0.0023,
      "step": 53920
    },
    {
      "epoch": 4.793777777777778,
      "grad_norm": 0.5406295657157898,
      "learning_rate": 2.003888888888889e-05,
      "loss": 0.0022,
      "step": 53930
    },
    {
      "epoch": 4.794666666666666,
      "grad_norm": 0.5680975317955017,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0026,
      "step": 53940
    },
    {
      "epoch": 4.795555555555556,
      "grad_norm": 0.10471397638320923,
      "learning_rate": 2.0027777777777777e-05,
      "loss": 0.0023,
      "step": 53950
    },
    {
      "epoch": 4.796444444444444,
      "grad_norm": 0.17948018014431,
      "learning_rate": 2.0022222222222224e-05,
      "loss": 0.0019,
      "step": 53960
    },
    {
      "epoch": 4.7973333333333334,
      "grad_norm": 0.45326200127601624,
      "learning_rate": 2.0016666666666668e-05,
      "loss": 0.0019,
      "step": 53970
    },
    {
      "epoch": 4.798222222222222,
      "grad_norm": 0.2578381896018982,
      "learning_rate": 2.001111111111111e-05,
      "loss": 0.0019,
      "step": 53980
    },
    {
      "epoch": 4.799111111111111,
      "grad_norm": 0.18718767166137695,
      "learning_rate": 2.0005555555555555e-05,
      "loss": 0.0016,
      "step": 53990
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.29857075214385986,
      "learning_rate": 2e-05,
      "loss": 0.0021,
      "step": 54000
    },
    {
      "epoch": 4.800888888888889,
      "grad_norm": 0.10474523901939392,
      "learning_rate": 1.9994444444444445e-05,
      "loss": 0.0018,
      "step": 54010
    },
    {
      "epoch": 4.801777777777778,
      "grad_norm": 0.38789641857147217,
      "learning_rate": 1.998888888888889e-05,
      "loss": 0.0023,
      "step": 54020
    },
    {
      "epoch": 4.802666666666667,
      "grad_norm": 0.31223681569099426,
      "learning_rate": 1.9983333333333336e-05,
      "loss": 0.0025,
      "step": 54030
    },
    {
      "epoch": 4.803555555555556,
      "grad_norm": 0.6072167754173279,
      "learning_rate": 1.997777777777778e-05,
      "loss": 0.0028,
      "step": 54040
    },
    {
      "epoch": 4.804444444444444,
      "grad_norm": 0.6552776098251343,
      "learning_rate": 1.9972222222222223e-05,
      "loss": 0.0017,
      "step": 54050
    },
    {
      "epoch": 4.8053333333333335,
      "grad_norm": 0.3658958673477173,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0021,
      "step": 54060
    },
    {
      "epoch": 4.806222222222222,
      "grad_norm": 0.036688245832920074,
      "learning_rate": 1.996111111111111e-05,
      "loss": 0.0021,
      "step": 54070
    },
    {
      "epoch": 4.807111111111111,
      "grad_norm": 0.22031012177467346,
      "learning_rate": 1.9955555555555557e-05,
      "loss": 0.0025,
      "step": 54080
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.7397217750549316,
      "learning_rate": 1.995e-05,
      "loss": 0.0021,
      "step": 54090
    },
    {
      "epoch": 4.808888888888889,
      "grad_norm": 0.17016719281673431,
      "learning_rate": 1.9944444444444447e-05,
      "loss": 0.0016,
      "step": 54100
    },
    {
      "epoch": 4.809777777777778,
      "grad_norm": 0.25530773401260376,
      "learning_rate": 1.993888888888889e-05,
      "loss": 0.0021,
      "step": 54110
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.07382207363843918,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0018,
      "step": 54120
    },
    {
      "epoch": 4.811555555555556,
      "grad_norm": 0.5241609215736389,
      "learning_rate": 1.9927777777777778e-05,
      "loss": 0.0016,
      "step": 54130
    },
    {
      "epoch": 4.812444444444444,
      "grad_norm": 0.15519170463085175,
      "learning_rate": 1.992222222222222e-05,
      "loss": 0.0037,
      "step": 54140
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.18548621237277985,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.002,
      "step": 54150
    },
    {
      "epoch": 4.814222222222222,
      "grad_norm": 0.5654303431510925,
      "learning_rate": 1.9911111111111112e-05,
      "loss": 0.002,
      "step": 54160
    },
    {
      "epoch": 4.815111111111111,
      "grad_norm": 0.7430959939956665,
      "learning_rate": 1.990555555555556e-05,
      "loss": 0.0022,
      "step": 54170
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.10455071926116943,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0019,
      "step": 54180
    },
    {
      "epoch": 4.816888888888889,
      "grad_norm": 0.15599828958511353,
      "learning_rate": 1.9894444444444446e-05,
      "loss": 0.0027,
      "step": 54190
    },
    {
      "epoch": 4.817777777777778,
      "grad_norm": 0.6333506107330322,
      "learning_rate": 1.988888888888889e-05,
      "loss": 0.0021,
      "step": 54200
    },
    {
      "epoch": 4.818666666666667,
      "grad_norm": 0.6953315138816833,
      "learning_rate": 1.9883333333333333e-05,
      "loss": 0.0025,
      "step": 54210
    },
    {
      "epoch": 4.819555555555556,
      "grad_norm": 0.03688569366931915,
      "learning_rate": 1.9877777777777777e-05,
      "loss": 0.0022,
      "step": 54220
    },
    {
      "epoch": 4.820444444444444,
      "grad_norm": 0.5564420819282532,
      "learning_rate": 1.9872222222222224e-05,
      "loss": 0.0027,
      "step": 54230
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.36695414781570435,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0017,
      "step": 54240
    },
    {
      "epoch": 4.822222222222222,
      "grad_norm": 0.40650781989097595,
      "learning_rate": 1.986111111111111e-05,
      "loss": 0.0026,
      "step": 54250
    },
    {
      "epoch": 4.823111111111111,
      "grad_norm": 0.15708263218402863,
      "learning_rate": 1.9855555555555558e-05,
      "loss": 0.0022,
      "step": 54260
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.5250118970870972,
      "learning_rate": 1.985e-05,
      "loss": 0.0026,
      "step": 54270
    },
    {
      "epoch": 4.824888888888889,
      "grad_norm": 0.1820143312215805,
      "learning_rate": 1.9844444444444445e-05,
      "loss": 0.0021,
      "step": 54280
    },
    {
      "epoch": 4.825777777777778,
      "grad_norm": 0.11592311412096024,
      "learning_rate": 1.9838888888888892e-05,
      "loss": 0.002,
      "step": 54290
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.4890947639942169,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0014,
      "step": 54300
    },
    {
      "epoch": 4.827555555555556,
      "grad_norm": 0.669295608997345,
      "learning_rate": 1.982777777777778e-05,
      "loss": 0.0027,
      "step": 54310
    },
    {
      "epoch": 4.828444444444444,
      "grad_norm": 0.5872040390968323,
      "learning_rate": 1.9822222222222223e-05,
      "loss": 0.0019,
      "step": 54320
    },
    {
      "epoch": 4.8293333333333335,
      "grad_norm": 0.7060453295707703,
      "learning_rate": 1.9816666666666666e-05,
      "loss": 0.0024,
      "step": 54330
    },
    {
      "epoch": 4.830222222222222,
      "grad_norm": 0.5585336685180664,
      "learning_rate": 1.981111111111111e-05,
      "loss": 0.0022,
      "step": 54340
    },
    {
      "epoch": 4.831111111111111,
      "grad_norm": 0.5185506343841553,
      "learning_rate": 1.9805555555555557e-05,
      "loss": 0.0012,
      "step": 54350
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.11528603732585907,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0016,
      "step": 54360
    },
    {
      "epoch": 4.832888888888889,
      "grad_norm": 0.6367849707603455,
      "learning_rate": 1.9794444444444447e-05,
      "loss": 0.002,
      "step": 54370
    },
    {
      "epoch": 4.833777777777778,
      "grad_norm": 0.17264027893543243,
      "learning_rate": 1.978888888888889e-05,
      "loss": 0.0016,
      "step": 54380
    },
    {
      "epoch": 4.834666666666667,
      "grad_norm": 0.23194526135921478,
      "learning_rate": 1.9783333333333334e-05,
      "loss": 0.0018,
      "step": 54390
    },
    {
      "epoch": 4.835555555555556,
      "grad_norm": 0.817379355430603,
      "learning_rate": 1.9777777777777778e-05,
      "loss": 0.002,
      "step": 54400
    },
    {
      "epoch": 4.836444444444444,
      "grad_norm": 0.015452496707439423,
      "learning_rate": 1.977222222222222e-05,
      "loss": 0.0022,
      "step": 54410
    },
    {
      "epoch": 4.8373333333333335,
      "grad_norm": 0.4670490026473999,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0014,
      "step": 54420
    },
    {
      "epoch": 4.838222222222222,
      "grad_norm": 0.22540347278118134,
      "learning_rate": 1.9761111111111112e-05,
      "loss": 0.0014,
      "step": 54430
    },
    {
      "epoch": 4.839111111111111,
      "grad_norm": 0.08724906295537949,
      "learning_rate": 1.975555555555556e-05,
      "loss": 0.0018,
      "step": 54440
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.5704194903373718,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.0026,
      "step": 54450
    },
    {
      "epoch": 4.840888888888889,
      "grad_norm": 0.3834817707538605,
      "learning_rate": 1.9744444444444446e-05,
      "loss": 0.0026,
      "step": 54460
    },
    {
      "epoch": 4.841777777777778,
      "grad_norm": 0.26220637559890747,
      "learning_rate": 1.973888888888889e-05,
      "loss": 0.0016,
      "step": 54470
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.1232670247554779,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0027,
      "step": 54480
    },
    {
      "epoch": 4.843555555555556,
      "grad_norm": 0.0782947689294815,
      "learning_rate": 1.972777777777778e-05,
      "loss": 0.0017,
      "step": 54490
    },
    {
      "epoch": 4.844444444444444,
      "grad_norm": 0.18249772489070892,
      "learning_rate": 1.9722222222222224e-05,
      "loss": 0.0022,
      "step": 54500
    },
    {
      "epoch": 4.8453333333333335,
      "grad_norm": 0.4759889543056488,
      "learning_rate": 1.9716666666666667e-05,
      "loss": 0.0015,
      "step": 54510
    },
    {
      "epoch": 4.846222222222222,
      "grad_norm": 0.4556199014186859,
      "learning_rate": 1.971111111111111e-05,
      "loss": 0.0021,
      "step": 54520
    },
    {
      "epoch": 4.847111111111111,
      "grad_norm": 0.2953621745109558,
      "learning_rate": 1.9705555555555558e-05,
      "loss": 0.0017,
      "step": 54530
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.15759162604808807,
      "learning_rate": 1.97e-05,
      "loss": 0.0019,
      "step": 54540
    },
    {
      "epoch": 4.848888888888889,
      "grad_norm": 0.5941594243049622,
      "learning_rate": 1.9694444444444445e-05,
      "loss": 0.0025,
      "step": 54550
    },
    {
      "epoch": 4.849777777777778,
      "grad_norm": 0.206289604306221,
      "learning_rate": 1.968888888888889e-05,
      "loss": 0.0015,
      "step": 54560
    },
    {
      "epoch": 4.850666666666667,
      "grad_norm": 0.2840289771556854,
      "learning_rate": 1.9683333333333335e-05,
      "loss": 0.0016,
      "step": 54570
    },
    {
      "epoch": 4.851555555555556,
      "grad_norm": 0.6893191337585449,
      "learning_rate": 1.967777777777778e-05,
      "loss": 0.0013,
      "step": 54580
    },
    {
      "epoch": 4.852444444444444,
      "grad_norm": 0.3411217927932739,
      "learning_rate": 1.9672222222222222e-05,
      "loss": 0.0017,
      "step": 54590
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.45646950602531433,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0022,
      "step": 54600
    },
    {
      "epoch": 4.854222222222222,
      "grad_norm": 0.30159634351730347,
      "learning_rate": 1.966111111111111e-05,
      "loss": 0.0023,
      "step": 54610
    },
    {
      "epoch": 4.855111111111111,
      "grad_norm": 0.08059641718864441,
      "learning_rate": 1.9655555555555556e-05,
      "loss": 0.0021,
      "step": 54620
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.6675370335578918,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 0.002,
      "step": 54630
    },
    {
      "epoch": 4.856888888888889,
      "grad_norm": 0.07246208190917969,
      "learning_rate": 1.9644444444444447e-05,
      "loss": 0.0027,
      "step": 54640
    },
    {
      "epoch": 4.857777777777778,
      "grad_norm": 0.5551676750183105,
      "learning_rate": 1.963888888888889e-05,
      "loss": 0.0016,
      "step": 54650
    },
    {
      "epoch": 4.858666666666666,
      "grad_norm": 0.15098559856414795,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.0029,
      "step": 54660
    },
    {
      "epoch": 4.859555555555556,
      "grad_norm": 0.9066075086593628,
      "learning_rate": 1.9627777777777778e-05,
      "loss": 0.0017,
      "step": 54670
    },
    {
      "epoch": 4.860444444444444,
      "grad_norm": 0.39945316314697266,
      "learning_rate": 1.962222222222222e-05,
      "loss": 0.0025,
      "step": 54680
    },
    {
      "epoch": 4.8613333333333335,
      "grad_norm": 0.030964132398366928,
      "learning_rate": 1.9616666666666668e-05,
      "loss": 0.0023,
      "step": 54690
    },
    {
      "epoch": 4.862222222222222,
      "grad_norm": 0.05873407796025276,
      "learning_rate": 1.9611111111111115e-05,
      "loss": 0.0019,
      "step": 54700
    },
    {
      "epoch": 4.863111111111111,
      "grad_norm": 0.3379391133785248,
      "learning_rate": 1.960555555555556e-05,
      "loss": 0.0019,
      "step": 54710
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.6453153491020203,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0023,
      "step": 54720
    },
    {
      "epoch": 4.864888888888889,
      "grad_norm": 0.34654372930526733,
      "learning_rate": 1.9594444444444446e-05,
      "loss": 0.0016,
      "step": 54730
    },
    {
      "epoch": 4.865777777777778,
      "grad_norm": 0.3559224009513855,
      "learning_rate": 1.958888888888889e-05,
      "loss": 0.0018,
      "step": 54740
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.35317766666412354,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.002,
      "step": 54750
    },
    {
      "epoch": 4.867555555555556,
      "grad_norm": 0.2963695228099823,
      "learning_rate": 1.957777777777778e-05,
      "loss": 0.0023,
      "step": 54760
    },
    {
      "epoch": 4.868444444444444,
      "grad_norm": 0.29468461871147156,
      "learning_rate": 1.9572222222222223e-05,
      "loss": 0.0018,
      "step": 54770
    },
    {
      "epoch": 4.8693333333333335,
      "grad_norm": 0.22310008108615875,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.0026,
      "step": 54780
    },
    {
      "epoch": 4.870222222222222,
      "grad_norm": 0.4778566360473633,
      "learning_rate": 1.9561111111111114e-05,
      "loss": 0.0034,
      "step": 54790
    },
    {
      "epoch": 4.871111111111111,
      "grad_norm": 0.6397270560264587,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 0.0019,
      "step": 54800
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.10803280770778656,
      "learning_rate": 1.955e-05,
      "loss": 0.0019,
      "step": 54810
    },
    {
      "epoch": 4.872888888888889,
      "grad_norm": 0.308586448431015,
      "learning_rate": 1.9544444444444444e-05,
      "loss": 0.0024,
      "step": 54820
    },
    {
      "epoch": 4.873777777777778,
      "grad_norm": 0.7113333344459534,
      "learning_rate": 1.953888888888889e-05,
      "loss": 0.0022,
      "step": 54830
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.23597513139247894,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0017,
      "step": 54840
    },
    {
      "epoch": 4.875555555555556,
      "grad_norm": 0.19198620319366455,
      "learning_rate": 1.952777777777778e-05,
      "loss": 0.0018,
      "step": 54850
    },
    {
      "epoch": 4.876444444444444,
      "grad_norm": 0.1722848117351532,
      "learning_rate": 1.9522222222222222e-05,
      "loss": 0.0019,
      "step": 54860
    },
    {
      "epoch": 4.8773333333333335,
      "grad_norm": 0.05125558376312256,
      "learning_rate": 1.9516666666666666e-05,
      "loss": 0.002,
      "step": 54870
    },
    {
      "epoch": 4.878222222222222,
      "grad_norm": 0.6506649851799011,
      "learning_rate": 1.9511111111111113e-05,
      "loss": 0.0023,
      "step": 54880
    },
    {
      "epoch": 4.879111111111111,
      "grad_norm": 0.3749152719974518,
      "learning_rate": 1.9505555555555556e-05,
      "loss": 0.0018,
      "step": 54890
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.29957225918769836,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.0017,
      "step": 54900
    },
    {
      "epoch": 4.880888888888889,
      "grad_norm": 0.38327643275260925,
      "learning_rate": 1.9494444444444447e-05,
      "loss": 0.0022,
      "step": 54910
    },
    {
      "epoch": 4.881777777777778,
      "grad_norm": 0.12634976208209991,
      "learning_rate": 1.948888888888889e-05,
      "loss": 0.0019,
      "step": 54920
    },
    {
      "epoch": 4.882666666666667,
      "grad_norm": 0.2976032793521881,
      "learning_rate": 1.9483333333333334e-05,
      "loss": 0.0018,
      "step": 54930
    },
    {
      "epoch": 4.883555555555556,
      "grad_norm": 0.6421277523040771,
      "learning_rate": 1.9477777777777777e-05,
      "loss": 0.0017,
      "step": 54940
    },
    {
      "epoch": 4.884444444444444,
      "grad_norm": 0.12853501737117767,
      "learning_rate": 1.947222222222222e-05,
      "loss": 0.002,
      "step": 54950
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.043703652918338776,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0026,
      "step": 54960
    },
    {
      "epoch": 4.886222222222222,
      "grad_norm": 0.23770806193351746,
      "learning_rate": 1.9461111111111115e-05,
      "loss": 0.0013,
      "step": 54970
    },
    {
      "epoch": 4.887111111111111,
      "grad_norm": 0.19636669754981995,
      "learning_rate": 1.9455555555555558e-05,
      "loss": 0.0019,
      "step": 54980
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.2707720398902893,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 0.0017,
      "step": 54990
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.0863000750541687,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 0.0019,
      "step": 55000
    },
    {
      "epoch": 4.889777777777778,
      "grad_norm": 0.1531044840812683,
      "learning_rate": 1.943888888888889e-05,
      "loss": 0.0014,
      "step": 55010
    },
    {
      "epoch": 4.890666666666666,
      "grad_norm": 0.2987799346446991,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0016,
      "step": 55020
    },
    {
      "epoch": 4.891555555555556,
      "grad_norm": 0.32935893535614014,
      "learning_rate": 1.942777777777778e-05,
      "loss": 0.0017,
      "step": 55030
    },
    {
      "epoch": 4.892444444444444,
      "grad_norm": 0.41434112191200256,
      "learning_rate": 1.9422222222222223e-05,
      "loss": 0.0019,
      "step": 55040
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.4050697088241577,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.0026,
      "step": 55050
    },
    {
      "epoch": 4.894222222222222,
      "grad_norm": 0.12463846802711487,
      "learning_rate": 1.9411111111111113e-05,
      "loss": 0.0012,
      "step": 55060
    },
    {
      "epoch": 4.895111111111111,
      "grad_norm": 0.37173038721084595,
      "learning_rate": 1.9405555555555557e-05,
      "loss": 0.0021,
      "step": 55070
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.385997474193573,
      "learning_rate": 1.94e-05,
      "loss": 0.0025,
      "step": 55080
    },
    {
      "epoch": 4.896888888888888,
      "grad_norm": 0.7578327059745789,
      "learning_rate": 1.9394444444444444e-05,
      "loss": 0.0019,
      "step": 55090
    },
    {
      "epoch": 4.897777777777778,
      "grad_norm": 0.6741397976875305,
      "learning_rate": 1.938888888888889e-05,
      "loss": 0.0027,
      "step": 55100
    },
    {
      "epoch": 4.898666666666666,
      "grad_norm": 0.8922213315963745,
      "learning_rate": 1.9383333333333335e-05,
      "loss": 0.0024,
      "step": 55110
    },
    {
      "epoch": 4.899555555555556,
      "grad_norm": 0.4942944347858429,
      "learning_rate": 1.9377777777777778e-05,
      "loss": 0.0017,
      "step": 55120
    },
    {
      "epoch": 4.900444444444444,
      "grad_norm": 0.8366018533706665,
      "learning_rate": 1.9372222222222222e-05,
      "loss": 0.0019,
      "step": 55130
    },
    {
      "epoch": 4.9013333333333335,
      "grad_norm": 0.22623854875564575,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.0018,
      "step": 55140
    },
    {
      "epoch": 4.902222222222222,
      "grad_norm": 0.563130795955658,
      "learning_rate": 1.9361111111111112e-05,
      "loss": 0.0021,
      "step": 55150
    },
    {
      "epoch": 4.903111111111111,
      "grad_norm": 0.20468857884407043,
      "learning_rate": 1.9355555555555556e-05,
      "loss": 0.0023,
      "step": 55160
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.47966673970222473,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 0.0017,
      "step": 55170
    },
    {
      "epoch": 4.904888888888889,
      "grad_norm": 0.16854752600193024,
      "learning_rate": 1.9344444444444446e-05,
      "loss": 0.002,
      "step": 55180
    },
    {
      "epoch": 4.905777777777778,
      "grad_norm": 0.16584044694900513,
      "learning_rate": 1.933888888888889e-05,
      "loss": 0.0021,
      "step": 55190
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.5757966637611389,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0012,
      "step": 55200
    },
    {
      "epoch": 4.907555555555556,
      "grad_norm": 0.47005903720855713,
      "learning_rate": 1.9327777777777777e-05,
      "loss": 0.002,
      "step": 55210
    },
    {
      "epoch": 4.908444444444444,
      "grad_norm": 0.6457914113998413,
      "learning_rate": 1.932222222222222e-05,
      "loss": 0.0023,
      "step": 55220
    },
    {
      "epoch": 4.9093333333333335,
      "grad_norm": 0.38049444556236267,
      "learning_rate": 1.9316666666666668e-05,
      "loss": 0.002,
      "step": 55230
    },
    {
      "epoch": 4.910222222222222,
      "grad_norm": 0.2428291141986847,
      "learning_rate": 1.9311111111111114e-05,
      "loss": 0.0021,
      "step": 55240
    },
    {
      "epoch": 4.911111111111111,
      "grad_norm": 0.22860482335090637,
      "learning_rate": 1.9305555555555558e-05,
      "loss": 0.0013,
      "step": 55250
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.3891471326351166,
      "learning_rate": 1.93e-05,
      "loss": 0.0031,
      "step": 55260
    },
    {
      "epoch": 4.912888888888888,
      "grad_norm": 0.5139918327331543,
      "learning_rate": 1.9294444444444445e-05,
      "loss": 0.0023,
      "step": 55270
    },
    {
      "epoch": 4.913777777777778,
      "grad_norm": 0.1855301856994629,
      "learning_rate": 1.928888888888889e-05,
      "loss": 0.0022,
      "step": 55280
    },
    {
      "epoch": 4.914666666666666,
      "grad_norm": 0.36051395535469055,
      "learning_rate": 1.9283333333333332e-05,
      "loss": 0.0025,
      "step": 55290
    },
    {
      "epoch": 4.915555555555556,
      "grad_norm": 0.6945427656173706,
      "learning_rate": 1.927777777777778e-05,
      "loss": 0.0023,
      "step": 55300
    },
    {
      "epoch": 4.916444444444444,
      "grad_norm": 0.11072257906198502,
      "learning_rate": 1.9272222222222223e-05,
      "loss": 0.0019,
      "step": 55310
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 0.11109940707683563,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0023,
      "step": 55320
    },
    {
      "epoch": 4.918222222222222,
      "grad_norm": 0.2203879952430725,
      "learning_rate": 1.9261111111111113e-05,
      "loss": 0.0023,
      "step": 55330
    },
    {
      "epoch": 4.919111111111111,
      "grad_norm": 0.3046762943267822,
      "learning_rate": 1.9255555555555557e-05,
      "loss": 0.0017,
      "step": 55340
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.6381035447120667,
      "learning_rate": 1.925e-05,
      "loss": 0.0019,
      "step": 55350
    },
    {
      "epoch": 4.920888888888889,
      "grad_norm": 1.1172038316726685,
      "learning_rate": 1.9244444444444444e-05,
      "loss": 0.0035,
      "step": 55360
    },
    {
      "epoch": 4.921777777777778,
      "grad_norm": 0.2022968828678131,
      "learning_rate": 1.923888888888889e-05,
      "loss": 0.0019,
      "step": 55370
    },
    {
      "epoch": 4.922666666666666,
      "grad_norm": 0.08799682557582855,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0022,
      "step": 55380
    },
    {
      "epoch": 4.923555555555556,
      "grad_norm": 0.32486993074417114,
      "learning_rate": 1.9227777777777778e-05,
      "loss": 0.0017,
      "step": 55390
    },
    {
      "epoch": 4.924444444444444,
      "grad_norm": 0.04195320978760719,
      "learning_rate": 1.922222222222222e-05,
      "loss": 0.0015,
      "step": 55400
    },
    {
      "epoch": 4.925333333333334,
      "grad_norm": 0.07547233998775482,
      "learning_rate": 1.921666666666667e-05,
      "loss": 0.0017,
      "step": 55410
    },
    {
      "epoch": 4.926222222222222,
      "grad_norm": 0.4453426003456116,
      "learning_rate": 1.9211111111111112e-05,
      "loss": 0.0022,
      "step": 55420
    },
    {
      "epoch": 4.927111111111111,
      "grad_norm": 0.23107048869132996,
      "learning_rate": 1.9205555555555556e-05,
      "loss": 0.0018,
      "step": 55430
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.10544160008430481,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0021,
      "step": 55440
    },
    {
      "epoch": 4.928888888888888,
      "grad_norm": 0.1638132482767105,
      "learning_rate": 1.9194444444444446e-05,
      "loss": 0.002,
      "step": 55450
    },
    {
      "epoch": 4.929777777777778,
      "grad_norm": 0.08026475459337234,
      "learning_rate": 1.918888888888889e-05,
      "loss": 0.0028,
      "step": 55460
    },
    {
      "epoch": 4.930666666666666,
      "grad_norm": 0.07237287610769272,
      "learning_rate": 1.9183333333333333e-05,
      "loss": 0.0018,
      "step": 55470
    },
    {
      "epoch": 4.931555555555556,
      "grad_norm": 1.0700874328613281,
      "learning_rate": 1.9177777777777777e-05,
      "loss": 0.0022,
      "step": 55480
    },
    {
      "epoch": 4.932444444444444,
      "grad_norm": 0.49047935009002686,
      "learning_rate": 1.917222222222222e-05,
      "loss": 0.0022,
      "step": 55490
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.056798629462718964,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0016,
      "step": 55500
    },
    {
      "epoch": 4.934222222222222,
      "grad_norm": 0.07359171658754349,
      "learning_rate": 1.9161111111111114e-05,
      "loss": 0.0022,
      "step": 55510
    },
    {
      "epoch": 4.9351111111111114,
      "grad_norm": 0.2971506118774414,
      "learning_rate": 1.9155555555555558e-05,
      "loss": 0.0016,
      "step": 55520
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.25506484508514404,
      "learning_rate": 1.915e-05,
      "loss": 0.0014,
      "step": 55530
    },
    {
      "epoch": 4.936888888888889,
      "grad_norm": 0.2688066065311432,
      "learning_rate": 1.9144444444444445e-05,
      "loss": 0.0018,
      "step": 55540
    },
    {
      "epoch": 4.937777777777778,
      "grad_norm": 0.5178488492965698,
      "learning_rate": 1.913888888888889e-05,
      "loss": 0.0014,
      "step": 55550
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.23240749537944794,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0017,
      "step": 55560
    },
    {
      "epoch": 4.939555555555556,
      "grad_norm": 0.9755510687828064,
      "learning_rate": 1.912777777777778e-05,
      "loss": 0.0032,
      "step": 55570
    },
    {
      "epoch": 4.940444444444444,
      "grad_norm": 1.0309557914733887,
      "learning_rate": 1.9122222222222222e-05,
      "loss": 0.0026,
      "step": 55580
    },
    {
      "epoch": 4.941333333333334,
      "grad_norm": 0.48673340678215027,
      "learning_rate": 1.911666666666667e-05,
      "loss": 0.0026,
      "step": 55590
    },
    {
      "epoch": 4.942222222222222,
      "grad_norm": 0.3040764033794403,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.0018,
      "step": 55600
    },
    {
      "epoch": 4.9431111111111115,
      "grad_norm": 0.2276875227689743,
      "learning_rate": 1.9105555555555557e-05,
      "loss": 0.0025,
      "step": 55610
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.37796762585639954,
      "learning_rate": 1.91e-05,
      "loss": 0.0025,
      "step": 55620
    },
    {
      "epoch": 4.9448888888888884,
      "grad_norm": 0.3974892795085907,
      "learning_rate": 1.9094444444444447e-05,
      "loss": 0.0016,
      "step": 55630
    },
    {
      "epoch": 4.945777777777778,
      "grad_norm": 0.5981866121292114,
      "learning_rate": 1.908888888888889e-05,
      "loss": 0.0017,
      "step": 55640
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.04976722225546837,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.0012,
      "step": 55650
    },
    {
      "epoch": 4.947555555555556,
      "grad_norm": 0.4893352687358856,
      "learning_rate": 1.9077777777777778e-05,
      "loss": 0.0018,
      "step": 55660
    },
    {
      "epoch": 4.948444444444444,
      "grad_norm": 0.46645355224609375,
      "learning_rate": 1.907222222222222e-05,
      "loss": 0.002,
      "step": 55670
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.3045477271080017,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.002,
      "step": 55680
    },
    {
      "epoch": 4.950222222222222,
      "grad_norm": 0.20216742157936096,
      "learning_rate": 1.9061111111111112e-05,
      "loss": 0.0027,
      "step": 55690
    },
    {
      "epoch": 4.9511111111111115,
      "grad_norm": 0.5176572799682617,
      "learning_rate": 1.905555555555556e-05,
      "loss": 0.0024,
      "step": 55700
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.5379770398139954,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 0.0023,
      "step": 55710
    },
    {
      "epoch": 4.952888888888889,
      "grad_norm": 0.09467630088329315,
      "learning_rate": 1.9044444444444446e-05,
      "loss": 0.0034,
      "step": 55720
    },
    {
      "epoch": 4.953777777777778,
      "grad_norm": 0.1497778296470642,
      "learning_rate": 1.903888888888889e-05,
      "loss": 0.0019,
      "step": 55730
    },
    {
      "epoch": 4.954666666666666,
      "grad_norm": 0.07343807071447372,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0014,
      "step": 55740
    },
    {
      "epoch": 4.955555555555556,
      "grad_norm": 0.300243079662323,
      "learning_rate": 1.9027777777777776e-05,
      "loss": 0.0018,
      "step": 55750
    },
    {
      "epoch": 4.956444444444444,
      "grad_norm": 0.3318890631198883,
      "learning_rate": 1.9022222222222223e-05,
      "loss": 0.002,
      "step": 55760
    },
    {
      "epoch": 4.957333333333334,
      "grad_norm": 0.3117086887359619,
      "learning_rate": 1.901666666666667e-05,
      "loss": 0.0023,
      "step": 55770
    },
    {
      "epoch": 4.958222222222222,
      "grad_norm": 0.5250355005264282,
      "learning_rate": 1.9011111111111114e-05,
      "loss": 0.0018,
      "step": 55780
    },
    {
      "epoch": 4.9591111111111115,
      "grad_norm": 0.1901395469903946,
      "learning_rate": 1.9005555555555557e-05,
      "loss": 0.0018,
      "step": 55790
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.5906252861022949,
      "learning_rate": 1.9e-05,
      "loss": 0.0024,
      "step": 55800
    },
    {
      "epoch": 4.9608888888888885,
      "grad_norm": 0.6077434420585632,
      "learning_rate": 1.8994444444444445e-05,
      "loss": 0.0028,
      "step": 55810
    },
    {
      "epoch": 4.961777777777778,
      "grad_norm": 0.5559659600257874,
      "learning_rate": 1.8988888888888888e-05,
      "loss": 0.0023,
      "step": 55820
    },
    {
      "epoch": 4.962666666666666,
      "grad_norm": 0.26571282744407654,
      "learning_rate": 1.8983333333333335e-05,
      "loss": 0.0016,
      "step": 55830
    },
    {
      "epoch": 4.963555555555556,
      "grad_norm": 0.4448867440223694,
      "learning_rate": 1.897777777777778e-05,
      "loss": 0.0022,
      "step": 55840
    },
    {
      "epoch": 4.964444444444444,
      "grad_norm": 0.5519059896469116,
      "learning_rate": 1.8972222222222222e-05,
      "loss": 0.0018,
      "step": 55850
    },
    {
      "epoch": 4.965333333333334,
      "grad_norm": 0.1531941443681717,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0022,
      "step": 55860
    },
    {
      "epoch": 4.966222222222222,
      "grad_norm": 0.4465808570384979,
      "learning_rate": 1.8961111111111113e-05,
      "loss": 0.0019,
      "step": 55870
    },
    {
      "epoch": 4.9671111111111115,
      "grad_norm": 0.4909704327583313,
      "learning_rate": 1.8955555555555556e-05,
      "loss": 0.0015,
      "step": 55880
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.5466005206108093,
      "learning_rate": 1.895e-05,
      "loss": 0.0026,
      "step": 55890
    },
    {
      "epoch": 4.968888888888889,
      "grad_norm": 0.1662045568227768,
      "learning_rate": 1.8944444444444447e-05,
      "loss": 0.002,
      "step": 55900
    },
    {
      "epoch": 4.969777777777778,
      "grad_norm": 0.11396162956953049,
      "learning_rate": 1.893888888888889e-05,
      "loss": 0.0019,
      "step": 55910
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.1915833204984665,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0027,
      "step": 55920
    },
    {
      "epoch": 4.971555555555556,
      "grad_norm": 0.05251326784491539,
      "learning_rate": 1.8927777777777777e-05,
      "loss": 0.0016,
      "step": 55930
    },
    {
      "epoch": 4.972444444444444,
      "grad_norm": 0.2130192071199417,
      "learning_rate": 1.8922222222222224e-05,
      "loss": 0.0014,
      "step": 55940
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.6017286777496338,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.0022,
      "step": 55950
    },
    {
      "epoch": 4.974222222222222,
      "grad_norm": 0.7096030116081238,
      "learning_rate": 1.891111111111111e-05,
      "loss": 0.0023,
      "step": 55960
    },
    {
      "epoch": 4.9751111111111115,
      "grad_norm": 0.12539196014404297,
      "learning_rate": 1.890555555555556e-05,
      "loss": 0.0025,
      "step": 55970
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.0934637039899826,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.0026,
      "step": 55980
    },
    {
      "epoch": 4.9768888888888885,
      "grad_norm": 0.027767376974225044,
      "learning_rate": 1.8894444444444446e-05,
      "loss": 0.0019,
      "step": 55990
    },
    {
      "epoch": 4.977777777777778,
      "grad_norm": 0.07989118248224258,
      "learning_rate": 1.888888888888889e-05,
      "loss": 0.0017,
      "step": 56000
    },
    {
      "epoch": 4.978666666666666,
      "grad_norm": 0.5555583834648132,
      "learning_rate": 1.8883333333333333e-05,
      "loss": 0.0017,
      "step": 56010
    },
    {
      "epoch": 4.979555555555556,
      "grad_norm": 0.36846548318862915,
      "learning_rate": 1.8877777777777776e-05,
      "loss": 0.0021,
      "step": 56020
    },
    {
      "epoch": 4.980444444444444,
      "grad_norm": 0.38925331830978394,
      "learning_rate": 1.8872222222222223e-05,
      "loss": 0.0012,
      "step": 56030
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.26252123713493347,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0019,
      "step": 56040
    },
    {
      "epoch": 4.982222222222222,
      "grad_norm": 0.26076740026474,
      "learning_rate": 1.8861111111111114e-05,
      "loss": 0.0029,
      "step": 56050
    },
    {
      "epoch": 4.9831111111111115,
      "grad_norm": 0.2620939314365387,
      "learning_rate": 1.8855555555555557e-05,
      "loss": 0.002,
      "step": 56060
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.09162535518407822,
      "learning_rate": 1.885e-05,
      "loss": 0.0017,
      "step": 56070
    },
    {
      "epoch": 4.984888888888889,
      "grad_norm": 0.625619649887085,
      "learning_rate": 1.8844444444444444e-05,
      "loss": 0.0016,
      "step": 56080
    },
    {
      "epoch": 4.985777777777778,
      "grad_norm": 0.6490468382835388,
      "learning_rate": 1.8838888888888888e-05,
      "loss": 0.0016,
      "step": 56090
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.7885020971298218,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0022,
      "step": 56100
    },
    {
      "epoch": 4.987555555555556,
      "grad_norm": 0.7520166039466858,
      "learning_rate": 1.882777777777778e-05,
      "loss": 0.0016,
      "step": 56110
    },
    {
      "epoch": 4.988444444444444,
      "grad_norm": 0.8468149304389954,
      "learning_rate": 1.8822222222222225e-05,
      "loss": 0.0025,
      "step": 56120
    },
    {
      "epoch": 4.989333333333334,
      "grad_norm": 0.270907461643219,
      "learning_rate": 1.881666666666667e-05,
      "loss": 0.0017,
      "step": 56130
    },
    {
      "epoch": 4.990222222222222,
      "grad_norm": 0.11784414201974869,
      "learning_rate": 1.8811111111111112e-05,
      "loss": 0.0014,
      "step": 56140
    },
    {
      "epoch": 4.9911111111111115,
      "grad_norm": 0.5098391771316528,
      "learning_rate": 1.8805555555555556e-05,
      "loss": 0.002,
      "step": 56150
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.0869021788239479,
      "learning_rate": 1.88e-05,
      "loss": 0.0016,
      "step": 56160
    },
    {
      "epoch": 4.9928888888888885,
      "grad_norm": 0.30503177642822266,
      "learning_rate": 1.8794444444444447e-05,
      "loss": 0.0024,
      "step": 56170
    },
    {
      "epoch": 4.993777777777778,
      "grad_norm": 0.060238998383283615,
      "learning_rate": 1.878888888888889e-05,
      "loss": 0.002,
      "step": 56180
    },
    {
      "epoch": 4.994666666666666,
      "grad_norm": 0.41223081946372986,
      "learning_rate": 1.8783333333333334e-05,
      "loss": 0.0023,
      "step": 56190
    },
    {
      "epoch": 4.995555555555556,
      "grad_norm": 0.03074282594025135,
      "learning_rate": 1.8777777777777777e-05,
      "loss": 0.0015,
      "step": 56200
    },
    {
      "epoch": 4.996444444444444,
      "grad_norm": 0.4516370892524719,
      "learning_rate": 1.8772222222222224e-05,
      "loss": 0.002,
      "step": 56210
    },
    {
      "epoch": 4.997333333333334,
      "grad_norm": 0.542712926864624,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.0022,
      "step": 56220
    },
    {
      "epoch": 4.998222222222222,
      "grad_norm": 0.4936857521533966,
      "learning_rate": 1.876111111111111e-05,
      "loss": 0.0018,
      "step": 56230
    },
    {
      "epoch": 4.999111111111111,
      "grad_norm": 0.15621371567249298,
      "learning_rate": 1.8755555555555558e-05,
      "loss": 0.0027,
      "step": 56240
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.3175191879272461,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.0018,
      "step": 56250
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.002182460855692625,
      "eval_runtime": 102.708,
      "eval_samples_per_second": 1460.451,
      "eval_steps_per_second": 36.511,
      "step": 56250
    },
    {
      "epoch": 5.0008888888888885,
      "grad_norm": 0.08265965431928635,
      "learning_rate": 1.8744444444444445e-05,
      "loss": 0.0022,
      "step": 56260
    },
    {
      "epoch": 5.001777777777778,
      "grad_norm": 0.44450074434280396,
      "learning_rate": 1.873888888888889e-05,
      "loss": 0.0019,
      "step": 56270
    },
    {
      "epoch": 5.002666666666666,
      "grad_norm": 0.42097941040992737,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0019,
      "step": 56280
    },
    {
      "epoch": 5.003555555555556,
      "grad_norm": 0.2950162887573242,
      "learning_rate": 1.8727777777777776e-05,
      "loss": 0.0017,
      "step": 56290
    },
    {
      "epoch": 5.004444444444444,
      "grad_norm": 0.47222280502319336,
      "learning_rate": 1.8722222222222223e-05,
      "loss": 0.0022,
      "step": 56300
    },
    {
      "epoch": 5.005333333333334,
      "grad_norm": 0.6016523838043213,
      "learning_rate": 1.871666666666667e-05,
      "loss": 0.0024,
      "step": 56310
    },
    {
      "epoch": 5.006222222222222,
      "grad_norm": 0.06530019640922546,
      "learning_rate": 1.8711111111111113e-05,
      "loss": 0.0023,
      "step": 56320
    },
    {
      "epoch": 5.0071111111111115,
      "grad_norm": 0.44213056564331055,
      "learning_rate": 1.8705555555555557e-05,
      "loss": 0.0018,
      "step": 56330
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.18323059380054474,
      "learning_rate": 1.87e-05,
      "loss": 0.0017,
      "step": 56340
    },
    {
      "epoch": 5.0088888888888885,
      "grad_norm": 0.6286596655845642,
      "learning_rate": 1.8694444444444444e-05,
      "loss": 0.0016,
      "step": 56350
    },
    {
      "epoch": 5.009777777777778,
      "grad_norm": 0.6335253715515137,
      "learning_rate": 1.8688888888888888e-05,
      "loss": 0.0015,
      "step": 56360
    },
    {
      "epoch": 5.010666666666666,
      "grad_norm": 0.3049300014972687,
      "learning_rate": 1.8683333333333335e-05,
      "loss": 0.0025,
      "step": 56370
    },
    {
      "epoch": 5.011555555555556,
      "grad_norm": 0.6005687713623047,
      "learning_rate": 1.8677777777777778e-05,
      "loss": 0.0025,
      "step": 56380
    },
    {
      "epoch": 5.012444444444444,
      "grad_norm": 0.6296294331550598,
      "learning_rate": 1.8672222222222225e-05,
      "loss": 0.0033,
      "step": 56390
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 0.5205356478691101,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0012,
      "step": 56400
    },
    {
      "epoch": 5.014222222222222,
      "grad_norm": 0.35101068019866943,
      "learning_rate": 1.8661111111111112e-05,
      "loss": 0.0024,
      "step": 56410
    },
    {
      "epoch": 5.0151111111111115,
      "grad_norm": 0.6242713332176208,
      "learning_rate": 1.8655555555555556e-05,
      "loss": 0.0021,
      "step": 56420
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.44353994727134705,
      "learning_rate": 1.865e-05,
      "loss": 0.0025,
      "step": 56430
    },
    {
      "epoch": 5.0168888888888885,
      "grad_norm": 0.04684915766119957,
      "learning_rate": 1.8644444444444446e-05,
      "loss": 0.0032,
      "step": 56440
    },
    {
      "epoch": 5.017777777777778,
      "grad_norm": 0.2323565036058426,
      "learning_rate": 1.863888888888889e-05,
      "loss": 0.0032,
      "step": 56450
    },
    {
      "epoch": 5.018666666666666,
      "grad_norm": 0.44178974628448486,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.0015,
      "step": 56460
    },
    {
      "epoch": 5.019555555555556,
      "grad_norm": 0.05977628752589226,
      "learning_rate": 1.8627777777777777e-05,
      "loss": 0.002,
      "step": 56470
    },
    {
      "epoch": 5.020444444444444,
      "grad_norm": 0.8698191046714783,
      "learning_rate": 1.8622222222222224e-05,
      "loss": 0.0024,
      "step": 56480
    },
    {
      "epoch": 5.021333333333334,
      "grad_norm": 0.4139724671840668,
      "learning_rate": 1.8616666666666667e-05,
      "loss": 0.0015,
      "step": 56490
    },
    {
      "epoch": 5.022222222222222,
      "grad_norm": 0.47708094120025635,
      "learning_rate": 1.861111111111111e-05,
      "loss": 0.0024,
      "step": 56500
    },
    {
      "epoch": 5.0231111111111115,
      "grad_norm": 0.24831953644752502,
      "learning_rate": 1.8605555555555558e-05,
      "loss": 0.0017,
      "step": 56510
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.15735368430614471,
      "learning_rate": 1.86e-05,
      "loss": 0.0017,
      "step": 56520
    },
    {
      "epoch": 5.0248888888888885,
      "grad_norm": 0.6207275986671448,
      "learning_rate": 1.8594444444444445e-05,
      "loss": 0.0022,
      "step": 56530
    },
    {
      "epoch": 5.025777777777778,
      "grad_norm": 0.2979869842529297,
      "learning_rate": 1.858888888888889e-05,
      "loss": 0.0019,
      "step": 56540
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 0.4752638041973114,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.0026,
      "step": 56550
    },
    {
      "epoch": 5.027555555555556,
      "grad_norm": 0.08223988115787506,
      "learning_rate": 1.8577777777777776e-05,
      "loss": 0.0016,
      "step": 56560
    },
    {
      "epoch": 5.028444444444444,
      "grad_norm": 0.1136879026889801,
      "learning_rate": 1.8572222222222223e-05,
      "loss": 0.0014,
      "step": 56570
    },
    {
      "epoch": 5.029333333333334,
      "grad_norm": 0.10351084917783737,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0022,
      "step": 56580
    },
    {
      "epoch": 5.030222222222222,
      "grad_norm": 0.260359525680542,
      "learning_rate": 1.8561111111111113e-05,
      "loss": 0.0025,
      "step": 56590
    },
    {
      "epoch": 5.0311111111111115,
      "grad_norm": 0.6260846257209778,
      "learning_rate": 1.8555555555555557e-05,
      "loss": 0.0019,
      "step": 56600
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.1485898643732071,
      "learning_rate": 1.855e-05,
      "loss": 0.0017,
      "step": 56610
    },
    {
      "epoch": 5.0328888888888885,
      "grad_norm": 0.4918009638786316,
      "learning_rate": 1.8544444444444444e-05,
      "loss": 0.0014,
      "step": 56620
    },
    {
      "epoch": 5.033777777777778,
      "grad_norm": 0.40512222051620483,
      "learning_rate": 1.8538888888888887e-05,
      "loss": 0.0019,
      "step": 56630
    },
    {
      "epoch": 5.034666666666666,
      "grad_norm": 0.2665919363498688,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0016,
      "step": 56640
    },
    {
      "epoch": 5.035555555555556,
      "grad_norm": 0.2178766280412674,
      "learning_rate": 1.852777777777778e-05,
      "loss": 0.0025,
      "step": 56650
    },
    {
      "epoch": 5.036444444444444,
      "grad_norm": 0.24420639872550964,
      "learning_rate": 1.8522222222222225e-05,
      "loss": 0.0017,
      "step": 56660
    },
    {
      "epoch": 5.037333333333334,
      "grad_norm": 0.04920895770192146,
      "learning_rate": 1.851666666666667e-05,
      "loss": 0.0018,
      "step": 56670
    },
    {
      "epoch": 5.038222222222222,
      "grad_norm": 0.3030947744846344,
      "learning_rate": 1.8511111111111112e-05,
      "loss": 0.0018,
      "step": 56680
    },
    {
      "epoch": 5.0391111111111115,
      "grad_norm": 0.4621840715408325,
      "learning_rate": 1.8505555555555556e-05,
      "loss": 0.002,
      "step": 56690
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.7993845343589783,
      "learning_rate": 1.85e-05,
      "loss": 0.0025,
      "step": 56700
    },
    {
      "epoch": 5.0408888888888885,
      "grad_norm": 0.15325947105884552,
      "learning_rate": 1.8494444444444446e-05,
      "loss": 0.0016,
      "step": 56710
    },
    {
      "epoch": 5.041777777777778,
      "grad_norm": 0.15629222989082336,
      "learning_rate": 1.848888888888889e-05,
      "loss": 0.0023,
      "step": 56720
    },
    {
      "epoch": 5.042666666666666,
      "grad_norm": 0.7448059916496277,
      "learning_rate": 1.8483333333333333e-05,
      "loss": 0.0019,
      "step": 56730
    },
    {
      "epoch": 5.043555555555556,
      "grad_norm": 0.36282262206077576,
      "learning_rate": 1.847777777777778e-05,
      "loss": 0.002,
      "step": 56740
    },
    {
      "epoch": 5.044444444444444,
      "grad_norm": 0.07795551419258118,
      "learning_rate": 1.8472222222222224e-05,
      "loss": 0.0022,
      "step": 56750
    },
    {
      "epoch": 5.045333333333334,
      "grad_norm": 0.21902897953987122,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0012,
      "step": 56760
    },
    {
      "epoch": 5.046222222222222,
      "grad_norm": 0.4198904037475586,
      "learning_rate": 1.846111111111111e-05,
      "loss": 0.0018,
      "step": 56770
    },
    {
      "epoch": 5.0471111111111115,
      "grad_norm": 0.5199686288833618,
      "learning_rate": 1.8455555555555558e-05,
      "loss": 0.0018,
      "step": 56780
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.08578744530677795,
      "learning_rate": 1.845e-05,
      "loss": 0.002,
      "step": 56790
    },
    {
      "epoch": 5.0488888888888885,
      "grad_norm": 0.5536898374557495,
      "learning_rate": 1.8444444444444445e-05,
      "loss": 0.0028,
      "step": 56800
    },
    {
      "epoch": 5.049777777777778,
      "grad_norm": 0.5994203090667725,
      "learning_rate": 1.843888888888889e-05,
      "loss": 0.0025,
      "step": 56810
    },
    {
      "epoch": 5.050666666666666,
      "grad_norm": 0.12212452292442322,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.0015,
      "step": 56820
    },
    {
      "epoch": 5.051555555555556,
      "grad_norm": 0.09270702302455902,
      "learning_rate": 1.842777777777778e-05,
      "loss": 0.0019,
      "step": 56830
    },
    {
      "epoch": 5.052444444444444,
      "grad_norm": 0.37815532088279724,
      "learning_rate": 1.8422222222222222e-05,
      "loss": 0.002,
      "step": 56840
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.2931600511074066,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.0018,
      "step": 56850
    },
    {
      "epoch": 5.054222222222222,
      "grad_norm": 0.2929629981517792,
      "learning_rate": 1.8411111111111113e-05,
      "loss": 0.0017,
      "step": 56860
    },
    {
      "epoch": 5.0551111111111116,
      "grad_norm": 0.1518867164850235,
      "learning_rate": 1.8405555555555556e-05,
      "loss": 0.0019,
      "step": 56870
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.4911907911300659,
      "learning_rate": 1.84e-05,
      "loss": 0.0018,
      "step": 56880
    },
    {
      "epoch": 5.0568888888888885,
      "grad_norm": 0.18094246089458466,
      "learning_rate": 1.8394444444444444e-05,
      "loss": 0.0029,
      "step": 56890
    },
    {
      "epoch": 5.057777777777778,
      "grad_norm": 0.08160167932510376,
      "learning_rate": 1.838888888888889e-05,
      "loss": 0.0033,
      "step": 56900
    },
    {
      "epoch": 5.058666666666666,
      "grad_norm": 0.1484730839729309,
      "learning_rate": 1.8383333333333334e-05,
      "loss": 0.0019,
      "step": 56910
    },
    {
      "epoch": 5.059555555555556,
      "grad_norm": 0.08556964248418808,
      "learning_rate": 1.837777777777778e-05,
      "loss": 0.0027,
      "step": 56920
    },
    {
      "epoch": 5.060444444444444,
      "grad_norm": 0.2392621636390686,
      "learning_rate": 1.8372222222222225e-05,
      "loss": 0.002,
      "step": 56930
    },
    {
      "epoch": 5.061333333333334,
      "grad_norm": 0.3655528724193573,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.0023,
      "step": 56940
    },
    {
      "epoch": 5.062222222222222,
      "grad_norm": 0.3348238468170166,
      "learning_rate": 1.836111111111111e-05,
      "loss": 0.0025,
      "step": 56950
    },
    {
      "epoch": 5.063111111111111,
      "grad_norm": 0.4496465027332306,
      "learning_rate": 1.8355555555555555e-05,
      "loss": 0.0016,
      "step": 56960
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.855306088924408,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 0.0026,
      "step": 56970
    },
    {
      "epoch": 5.0648888888888886,
      "grad_norm": 0.5531388521194458,
      "learning_rate": 1.8344444444444446e-05,
      "loss": 0.0023,
      "step": 56980
    },
    {
      "epoch": 5.065777777777778,
      "grad_norm": 0.06867432594299316,
      "learning_rate": 1.833888888888889e-05,
      "loss": 0.0022,
      "step": 56990
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.46195265650749207,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.002,
      "step": 57000
    },
    {
      "epoch": 5.067555555555556,
      "grad_norm": 0.1893942803144455,
      "learning_rate": 1.832777777777778e-05,
      "loss": 0.0014,
      "step": 57010
    },
    {
      "epoch": 5.068444444444444,
      "grad_norm": 0.0576956532895565,
      "learning_rate": 1.8322222222222223e-05,
      "loss": 0.0019,
      "step": 57020
    },
    {
      "epoch": 5.069333333333334,
      "grad_norm": 0.23384594917297363,
      "learning_rate": 1.8316666666666667e-05,
      "loss": 0.0015,
      "step": 57030
    },
    {
      "epoch": 5.070222222222222,
      "grad_norm": 0.3614877164363861,
      "learning_rate": 1.8311111111111114e-05,
      "loss": 0.0023,
      "step": 57040
    },
    {
      "epoch": 5.071111111111111,
      "grad_norm": 0.4927006959915161,
      "learning_rate": 1.8305555555555557e-05,
      "loss": 0.0016,
      "step": 57050
    },
    {
      "epoch": 5.072,
      "grad_norm": 0.21919187903404236,
      "learning_rate": 1.83e-05,
      "loss": 0.0016,
      "step": 57060
    },
    {
      "epoch": 5.072888888888889,
      "grad_norm": 0.037340279668569565,
      "learning_rate": 1.8294444444444445e-05,
      "loss": 0.002,
      "step": 57070
    },
    {
      "epoch": 5.073777777777778,
      "grad_norm": 0.4561753571033478,
      "learning_rate": 1.8288888888888888e-05,
      "loss": 0.0024,
      "step": 57080
    },
    {
      "epoch": 5.074666666666666,
      "grad_norm": 0.11514590680599213,
      "learning_rate": 1.828333333333333e-05,
      "loss": 0.0024,
      "step": 57090
    },
    {
      "epoch": 5.075555555555556,
      "grad_norm": 0.5627409219741821,
      "learning_rate": 1.827777777777778e-05,
      "loss": 0.0024,
      "step": 57100
    },
    {
      "epoch": 5.076444444444444,
      "grad_norm": 0.4752436578273773,
      "learning_rate": 1.8272222222222226e-05,
      "loss": 0.0038,
      "step": 57110
    },
    {
      "epoch": 5.077333333333334,
      "grad_norm": 0.24994681775569916,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.002,
      "step": 57120
    },
    {
      "epoch": 5.078222222222222,
      "grad_norm": 0.4457416236400604,
      "learning_rate": 1.8261111111111113e-05,
      "loss": 0.002,
      "step": 57130
    },
    {
      "epoch": 5.079111111111111,
      "grad_norm": 0.5647453665733337,
      "learning_rate": 1.8255555555555556e-05,
      "loss": 0.0025,
      "step": 57140
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.14786666631698608,
      "learning_rate": 1.825e-05,
      "loss": 0.0021,
      "step": 57150
    },
    {
      "epoch": 5.080888888888889,
      "grad_norm": 0.23779559135437012,
      "learning_rate": 1.8244444444444443e-05,
      "loss": 0.0021,
      "step": 57160
    },
    {
      "epoch": 5.081777777777778,
      "grad_norm": 0.30066096782684326,
      "learning_rate": 1.823888888888889e-05,
      "loss": 0.002,
      "step": 57170
    },
    {
      "epoch": 5.082666666666666,
      "grad_norm": 0.08758625388145447,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.0014,
      "step": 57180
    },
    {
      "epoch": 5.083555555555556,
      "grad_norm": 0.05508820712566376,
      "learning_rate": 1.822777777777778e-05,
      "loss": 0.0017,
      "step": 57190
    },
    {
      "epoch": 5.084444444444444,
      "grad_norm": 0.5226123332977295,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 0.0014,
      "step": 57200
    },
    {
      "epoch": 5.085333333333334,
      "grad_norm": 0.2662905752658844,
      "learning_rate": 1.8216666666666668e-05,
      "loss": 0.0023,
      "step": 57210
    },
    {
      "epoch": 5.086222222222222,
      "grad_norm": 0.812030017375946,
      "learning_rate": 1.821111111111111e-05,
      "loss": 0.0033,
      "step": 57220
    },
    {
      "epoch": 5.087111111111111,
      "grad_norm": 0.40937766432762146,
      "learning_rate": 1.8205555555555555e-05,
      "loss": 0.0021,
      "step": 57230
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.2654225826263428,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0013,
      "step": 57240
    },
    {
      "epoch": 5.088888888888889,
      "grad_norm": 0.12744469940662384,
      "learning_rate": 1.8194444444444445e-05,
      "loss": 0.002,
      "step": 57250
    },
    {
      "epoch": 5.089777777777778,
      "grad_norm": 0.9899637699127197,
      "learning_rate": 1.818888888888889e-05,
      "loss": 0.003,
      "step": 57260
    },
    {
      "epoch": 5.0906666666666665,
      "grad_norm": 0.14692223072052002,
      "learning_rate": 1.8183333333333336e-05,
      "loss": 0.0023,
      "step": 57270
    },
    {
      "epoch": 5.091555555555556,
      "grad_norm": 0.368285208940506,
      "learning_rate": 1.817777777777778e-05,
      "loss": 0.0017,
      "step": 57280
    },
    {
      "epoch": 5.092444444444444,
      "grad_norm": 0.08538820594549179,
      "learning_rate": 1.8172222222222223e-05,
      "loss": 0.002,
      "step": 57290
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 0.4379116892814636,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0015,
      "step": 57300
    },
    {
      "epoch": 5.094222222222222,
      "grad_norm": 0.1091499924659729,
      "learning_rate": 1.8161111111111114e-05,
      "loss": 0.0017,
      "step": 57310
    },
    {
      "epoch": 5.095111111111111,
      "grad_norm": 0.42196333408355713,
      "learning_rate": 1.8155555555555557e-05,
      "loss": 0.002,
      "step": 57320
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.43779319524765015,
      "learning_rate": 1.815e-05,
      "loss": 0.0024,
      "step": 57330
    },
    {
      "epoch": 5.096888888888889,
      "grad_norm": 0.157554030418396,
      "learning_rate": 1.8144444444444444e-05,
      "loss": 0.0026,
      "step": 57340
    },
    {
      "epoch": 5.097777777777778,
      "grad_norm": 0.2629232108592987,
      "learning_rate": 1.8138888888888888e-05,
      "loss": 0.0026,
      "step": 57350
    },
    {
      "epoch": 5.0986666666666665,
      "grad_norm": 0.13494247198104858,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0021,
      "step": 57360
    },
    {
      "epoch": 5.099555555555556,
      "grad_norm": 0.7544485926628113,
      "learning_rate": 1.812777777777778e-05,
      "loss": 0.0018,
      "step": 57370
    },
    {
      "epoch": 5.100444444444444,
      "grad_norm": 0.5542150735855103,
      "learning_rate": 1.8122222222222225e-05,
      "loss": 0.0025,
      "step": 57380
    },
    {
      "epoch": 5.101333333333334,
      "grad_norm": 0.6576215624809265,
      "learning_rate": 1.811666666666667e-05,
      "loss": 0.0022,
      "step": 57390
    },
    {
      "epoch": 5.102222222222222,
      "grad_norm": 0.14613163471221924,
      "learning_rate": 1.8111111111111112e-05,
      "loss": 0.0012,
      "step": 57400
    },
    {
      "epoch": 5.103111111111111,
      "grad_norm": 0.7649209499359131,
      "learning_rate": 1.8105555555555556e-05,
      "loss": 0.0024,
      "step": 57410
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.2916390895843506,
      "learning_rate": 1.81e-05,
      "loss": 0.0022,
      "step": 57420
    },
    {
      "epoch": 5.104888888888889,
      "grad_norm": 0.1737995743751526,
      "learning_rate": 1.8094444444444443e-05,
      "loss": 0.0022,
      "step": 57430
    },
    {
      "epoch": 5.105777777777778,
      "grad_norm": 0.8821501731872559,
      "learning_rate": 1.808888888888889e-05,
      "loss": 0.0014,
      "step": 57440
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 0.7335758805274963,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.0019,
      "step": 57450
    },
    {
      "epoch": 5.107555555555556,
      "grad_norm": 0.5571598410606384,
      "learning_rate": 1.807777777777778e-05,
      "loss": 0.0013,
      "step": 57460
    },
    {
      "epoch": 5.108444444444444,
      "grad_norm": 0.039290670305490494,
      "learning_rate": 1.8072222222222224e-05,
      "loss": 0.0023,
      "step": 57470
    },
    {
      "epoch": 5.109333333333334,
      "grad_norm": 0.37211060523986816,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0026,
      "step": 57480
    },
    {
      "epoch": 5.110222222222222,
      "grad_norm": 0.4835144579410553,
      "learning_rate": 1.806111111111111e-05,
      "loss": 0.0017,
      "step": 57490
    },
    {
      "epoch": 5.111111111111111,
      "grad_norm": 0.2782892882823944,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 0.0016,
      "step": 57500
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.12032217532396317,
      "learning_rate": 1.805e-05,
      "loss": 0.002,
      "step": 57510
    },
    {
      "epoch": 5.112888888888889,
      "grad_norm": 0.3944458067417145,
      "learning_rate": 1.8044444444444445e-05,
      "loss": 0.0016,
      "step": 57520
    },
    {
      "epoch": 5.113777777777778,
      "grad_norm": 0.4588179588317871,
      "learning_rate": 1.803888888888889e-05,
      "loss": 0.0027,
      "step": 57530
    },
    {
      "epoch": 5.1146666666666665,
      "grad_norm": 0.2994987666606903,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0017,
      "step": 57540
    },
    {
      "epoch": 5.115555555555556,
      "grad_norm": 0.26086074113845825,
      "learning_rate": 1.802777777777778e-05,
      "loss": 0.0014,
      "step": 57550
    },
    {
      "epoch": 5.116444444444444,
      "grad_norm": 0.8156671524047852,
      "learning_rate": 1.8022222222222223e-05,
      "loss": 0.0019,
      "step": 57560
    },
    {
      "epoch": 5.117333333333334,
      "grad_norm": 0.4069313704967499,
      "learning_rate": 1.8016666666666666e-05,
      "loss": 0.003,
      "step": 57570
    },
    {
      "epoch": 5.118222222222222,
      "grad_norm": 0.33589059114456177,
      "learning_rate": 1.8011111111111113e-05,
      "loss": 0.0024,
      "step": 57580
    },
    {
      "epoch": 5.119111111111111,
      "grad_norm": 0.4795602262020111,
      "learning_rate": 1.8005555555555557e-05,
      "loss": 0.0016,
      "step": 57590
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.19805361330509186,
      "learning_rate": 1.8e-05,
      "loss": 0.0015,
      "step": 57600
    },
    {
      "epoch": 5.120888888888889,
      "grad_norm": 0.27990344166755676,
      "learning_rate": 1.7994444444444444e-05,
      "loss": 0.0023,
      "step": 57610
    },
    {
      "epoch": 5.121777777777778,
      "grad_norm": 0.5335345268249512,
      "learning_rate": 1.7988888888888888e-05,
      "loss": 0.0013,
      "step": 57620
    },
    {
      "epoch": 5.1226666666666665,
      "grad_norm": 0.511879563331604,
      "learning_rate": 1.7983333333333335e-05,
      "loss": 0.0015,
      "step": 57630
    },
    {
      "epoch": 5.123555555555556,
      "grad_norm": 0.7399057149887085,
      "learning_rate": 1.7977777777777778e-05,
      "loss": 0.0018,
      "step": 57640
    },
    {
      "epoch": 5.124444444444444,
      "grad_norm": 0.08349568396806717,
      "learning_rate": 1.7972222222222225e-05,
      "loss": 0.002,
      "step": 57650
    },
    {
      "epoch": 5.125333333333334,
      "grad_norm": 0.24571533501148224,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.0027,
      "step": 57660
    },
    {
      "epoch": 5.126222222222222,
      "grad_norm": 0.5121333003044128,
      "learning_rate": 1.7961111111111112e-05,
      "loss": 0.0024,
      "step": 57670
    },
    {
      "epoch": 5.127111111111111,
      "grad_norm": 0.22306358814239502,
      "learning_rate": 1.7955555555555556e-05,
      "loss": 0.0022,
      "step": 57680
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.3207779824733734,
      "learning_rate": 1.795e-05,
      "loss": 0.0024,
      "step": 57690
    },
    {
      "epoch": 5.128888888888889,
      "grad_norm": 0.150199756026268,
      "learning_rate": 1.7944444444444443e-05,
      "loss": 0.0019,
      "step": 57700
    },
    {
      "epoch": 5.129777777777778,
      "grad_norm": 0.7287279367446899,
      "learning_rate": 1.793888888888889e-05,
      "loss": 0.0013,
      "step": 57710
    },
    {
      "epoch": 5.1306666666666665,
      "grad_norm": 0.1769697219133377,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0021,
      "step": 57720
    },
    {
      "epoch": 5.131555555555556,
      "grad_norm": 0.22180353105068207,
      "learning_rate": 1.792777777777778e-05,
      "loss": 0.0017,
      "step": 57730
    },
    {
      "epoch": 5.132444444444444,
      "grad_norm": 0.7339778542518616,
      "learning_rate": 1.7922222222222224e-05,
      "loss": 0.0024,
      "step": 57740
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.09842143952846527,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.0021,
      "step": 57750
    },
    {
      "epoch": 5.134222222222222,
      "grad_norm": 0.3295929431915283,
      "learning_rate": 1.791111111111111e-05,
      "loss": 0.0012,
      "step": 57760
    },
    {
      "epoch": 5.135111111111111,
      "grad_norm": 0.22566142678260803,
      "learning_rate": 1.7905555555555554e-05,
      "loss": 0.0013,
      "step": 57770
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.3648429214954376,
      "learning_rate": 1.79e-05,
      "loss": 0.0017,
      "step": 57780
    },
    {
      "epoch": 5.136888888888889,
      "grad_norm": 0.572766900062561,
      "learning_rate": 1.7894444444444445e-05,
      "loss": 0.0017,
      "step": 57790
    },
    {
      "epoch": 5.137777777777778,
      "grad_norm": 0.11949335038661957,
      "learning_rate": 1.788888888888889e-05,
      "loss": 0.0016,
      "step": 57800
    },
    {
      "epoch": 5.1386666666666665,
      "grad_norm": 0.3040315508842468,
      "learning_rate": 1.7883333333333335e-05,
      "loss": 0.0022,
      "step": 57810
    },
    {
      "epoch": 5.139555555555556,
      "grad_norm": 0.1498546153306961,
      "learning_rate": 1.787777777777778e-05,
      "loss": 0.0013,
      "step": 57820
    },
    {
      "epoch": 5.140444444444444,
      "grad_norm": 0.18426278233528137,
      "learning_rate": 1.7872222222222223e-05,
      "loss": 0.0019,
      "step": 57830
    },
    {
      "epoch": 5.141333333333334,
      "grad_norm": 0.12378022819757462,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0021,
      "step": 57840
    },
    {
      "epoch": 5.142222222222222,
      "grad_norm": 0.4434422552585602,
      "learning_rate": 1.7861111111111113e-05,
      "loss": 0.0017,
      "step": 57850
    },
    {
      "epoch": 5.143111111111111,
      "grad_norm": 0.05082305520772934,
      "learning_rate": 1.7855555555555557e-05,
      "loss": 0.0018,
      "step": 57860
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.11895552277565002,
      "learning_rate": 1.785e-05,
      "loss": 0.002,
      "step": 57870
    },
    {
      "epoch": 5.144888888888889,
      "grad_norm": 0.2954575717449188,
      "learning_rate": 1.7844444444444444e-05,
      "loss": 0.0017,
      "step": 57880
    },
    {
      "epoch": 5.145777777777778,
      "grad_norm": 0.31221991777420044,
      "learning_rate": 1.783888888888889e-05,
      "loss": 0.0025,
      "step": 57890
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 0.44831717014312744,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0023,
      "step": 57900
    },
    {
      "epoch": 5.147555555555556,
      "grad_norm": 0.5526113510131836,
      "learning_rate": 1.7827777777777778e-05,
      "loss": 0.0016,
      "step": 57910
    },
    {
      "epoch": 5.148444444444444,
      "grad_norm": 0.566275954246521,
      "learning_rate": 1.7822222222222225e-05,
      "loss": 0.0032,
      "step": 57920
    },
    {
      "epoch": 5.149333333333334,
      "grad_norm": 0.543728768825531,
      "learning_rate": 1.781666666666667e-05,
      "loss": 0.0029,
      "step": 57930
    },
    {
      "epoch": 5.150222222222222,
      "grad_norm": 0.8085642457008362,
      "learning_rate": 1.7811111111111112e-05,
      "loss": 0.0029,
      "step": 57940
    },
    {
      "epoch": 5.151111111111111,
      "grad_norm": 0.7021660804748535,
      "learning_rate": 1.7805555555555555e-05,
      "loss": 0.0023,
      "step": 57950
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.29338881373405457,
      "learning_rate": 1.78e-05,
      "loss": 0.0019,
      "step": 57960
    },
    {
      "epoch": 5.152888888888889,
      "grad_norm": 0.9025830030441284,
      "learning_rate": 1.7794444444444443e-05,
      "loss": 0.0022,
      "step": 57970
    },
    {
      "epoch": 5.153777777777778,
      "grad_norm": 0.7323896288871765,
      "learning_rate": 1.778888888888889e-05,
      "loss": 0.0027,
      "step": 57980
    },
    {
      "epoch": 5.1546666666666665,
      "grad_norm": 0.09850182384252548,
      "learning_rate": 1.7783333333333336e-05,
      "loss": 0.0017,
      "step": 57990
    },
    {
      "epoch": 5.155555555555556,
      "grad_norm": 0.25539734959602356,
      "learning_rate": 1.777777777777778e-05,
      "loss": 0.0017,
      "step": 58000
    },
    {
      "epoch": 5.156444444444444,
      "grad_norm": 0.08788228034973145,
      "learning_rate": 1.7772222222222224e-05,
      "loss": 0.0018,
      "step": 58010
    },
    {
      "epoch": 5.157333333333334,
      "grad_norm": 0.02527623623609543,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0023,
      "step": 58020
    },
    {
      "epoch": 5.158222222222222,
      "grad_norm": 0.05377214029431343,
      "learning_rate": 1.776111111111111e-05,
      "loss": 0.0026,
      "step": 58030
    },
    {
      "epoch": 5.159111111111111,
      "grad_norm": 0.034348372370004654,
      "learning_rate": 1.7755555555555554e-05,
      "loss": 0.002,
      "step": 58040
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.19038519263267517,
      "learning_rate": 1.775e-05,
      "loss": 0.0017,
      "step": 58050
    },
    {
      "epoch": 5.160888888888889,
      "grad_norm": 0.2652484178543091,
      "learning_rate": 1.7744444444444445e-05,
      "loss": 0.0024,
      "step": 58060
    },
    {
      "epoch": 5.161777777777778,
      "grad_norm": 0.3692292273044586,
      "learning_rate": 1.773888888888889e-05,
      "loss": 0.0019,
      "step": 58070
    },
    {
      "epoch": 5.1626666666666665,
      "grad_norm": 0.373232364654541,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0023,
      "step": 58080
    },
    {
      "epoch": 5.163555555555556,
      "grad_norm": 0.37068116664886475,
      "learning_rate": 1.772777777777778e-05,
      "loss": 0.0034,
      "step": 58090
    },
    {
      "epoch": 5.164444444444444,
      "grad_norm": 0.46572113037109375,
      "learning_rate": 1.7722222222222222e-05,
      "loss": 0.0016,
      "step": 58100
    },
    {
      "epoch": 5.165333333333333,
      "grad_norm": 0.6238166689872742,
      "learning_rate": 1.7716666666666666e-05,
      "loss": 0.0029,
      "step": 58110
    },
    {
      "epoch": 5.166222222222222,
      "grad_norm": 0.29051393270492554,
      "learning_rate": 1.7711111111111113e-05,
      "loss": 0.0023,
      "step": 58120
    },
    {
      "epoch": 5.167111111111111,
      "grad_norm": 0.6344432830810547,
      "learning_rate": 1.7705555555555556e-05,
      "loss": 0.0021,
      "step": 58130
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.30051031708717346,
      "learning_rate": 1.77e-05,
      "loss": 0.0017,
      "step": 58140
    },
    {
      "epoch": 5.168888888888889,
      "grad_norm": 0.06963574886322021,
      "learning_rate": 1.7694444444444443e-05,
      "loss": 0.0017,
      "step": 58150
    },
    {
      "epoch": 5.169777777777778,
      "grad_norm": 0.07977993041276932,
      "learning_rate": 1.768888888888889e-05,
      "loss": 0.0026,
      "step": 58160
    },
    {
      "epoch": 5.1706666666666665,
      "grad_norm": 0.4756588041782379,
      "learning_rate": 1.7683333333333334e-05,
      "loss": 0.0015,
      "step": 58170
    },
    {
      "epoch": 5.171555555555556,
      "grad_norm": 0.0871228575706482,
      "learning_rate": 1.7677777777777778e-05,
      "loss": 0.0014,
      "step": 58180
    },
    {
      "epoch": 5.172444444444444,
      "grad_norm": 0.37825265526771545,
      "learning_rate": 1.7672222222222224e-05,
      "loss": 0.002,
      "step": 58190
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.36361822485923767,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0019,
      "step": 58200
    },
    {
      "epoch": 5.174222222222222,
      "grad_norm": 0.4339849650859833,
      "learning_rate": 1.766111111111111e-05,
      "loss": 0.002,
      "step": 58210
    },
    {
      "epoch": 5.175111111111111,
      "grad_norm": 0.03777643293142319,
      "learning_rate": 1.7655555555555555e-05,
      "loss": 0.0016,
      "step": 58220
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.1481771171092987,
      "learning_rate": 1.765e-05,
      "loss": 0.0023,
      "step": 58230
    },
    {
      "epoch": 5.176888888888889,
      "grad_norm": 0.5166723132133484,
      "learning_rate": 1.7644444444444446e-05,
      "loss": 0.0016,
      "step": 58240
    },
    {
      "epoch": 5.177777777777778,
      "grad_norm": 0.4052088260650635,
      "learning_rate": 1.763888888888889e-05,
      "loss": 0.0032,
      "step": 58250
    },
    {
      "epoch": 5.1786666666666665,
      "grad_norm": 0.11170549690723419,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.002,
      "step": 58260
    },
    {
      "epoch": 5.179555555555556,
      "grad_norm": 0.05881212651729584,
      "learning_rate": 1.762777777777778e-05,
      "loss": 0.0021,
      "step": 58270
    },
    {
      "epoch": 5.180444444444444,
      "grad_norm": 0.14653657376766205,
      "learning_rate": 1.7622222222222223e-05,
      "loss": 0.0013,
      "step": 58280
    },
    {
      "epoch": 5.181333333333333,
      "grad_norm": 0.3463907241821289,
      "learning_rate": 1.7616666666666667e-05,
      "loss": 0.0012,
      "step": 58290
    },
    {
      "epoch": 5.182222222222222,
      "grad_norm": 0.49865907430648804,
      "learning_rate": 1.761111111111111e-05,
      "loss": 0.0021,
      "step": 58300
    },
    {
      "epoch": 5.183111111111111,
      "grad_norm": 0.406220018863678,
      "learning_rate": 1.7605555555555557e-05,
      "loss": 0.0021,
      "step": 58310
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.5898284912109375,
      "learning_rate": 1.76e-05,
      "loss": 0.0017,
      "step": 58320
    },
    {
      "epoch": 5.184888888888889,
      "grad_norm": 0.6311353445053101,
      "learning_rate": 1.7594444444444444e-05,
      "loss": 0.0015,
      "step": 58330
    },
    {
      "epoch": 5.185777777777778,
      "grad_norm": 0.19198597967624664,
      "learning_rate": 1.758888888888889e-05,
      "loss": 0.0014,
      "step": 58340
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 0.30140167474746704,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.0015,
      "step": 58350
    },
    {
      "epoch": 5.187555555555556,
      "grad_norm": 0.46273598074913025,
      "learning_rate": 1.757777777777778e-05,
      "loss": 0.0022,
      "step": 58360
    },
    {
      "epoch": 5.188444444444444,
      "grad_norm": 0.1479167342185974,
      "learning_rate": 1.7572222222222222e-05,
      "loss": 0.0019,
      "step": 58370
    },
    {
      "epoch": 5.189333333333333,
      "grad_norm": 0.5269100666046143,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.0021,
      "step": 58380
    },
    {
      "epoch": 5.190222222222222,
      "grad_norm": 0.4956323504447937,
      "learning_rate": 1.7561111111111113e-05,
      "loss": 0.0015,
      "step": 58390
    },
    {
      "epoch": 5.191111111111111,
      "grad_norm": 0.4663134515285492,
      "learning_rate": 1.7555555555555556e-05,
      "loss": 0.0012,
      "step": 58400
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.4139636754989624,
      "learning_rate": 1.755e-05,
      "loss": 0.0033,
      "step": 58410
    },
    {
      "epoch": 5.192888888888889,
      "grad_norm": 0.3837983310222626,
      "learning_rate": 1.7544444444444443e-05,
      "loss": 0.0023,
      "step": 58420
    },
    {
      "epoch": 5.193777777777778,
      "grad_norm": 1.2175912857055664,
      "learning_rate": 1.753888888888889e-05,
      "loss": 0.0017,
      "step": 58430
    },
    {
      "epoch": 5.1946666666666665,
      "grad_norm": 0.759635329246521,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0019,
      "step": 58440
    },
    {
      "epoch": 5.195555555555556,
      "grad_norm": 0.2993300259113312,
      "learning_rate": 1.752777777777778e-05,
      "loss": 0.0019,
      "step": 58450
    },
    {
      "epoch": 5.196444444444444,
      "grad_norm": 0.3295299708843231,
      "learning_rate": 1.7522222222222224e-05,
      "loss": 0.0022,
      "step": 58460
    },
    {
      "epoch": 5.197333333333333,
      "grad_norm": 0.30867040157318115,
      "learning_rate": 1.7516666666666668e-05,
      "loss": 0.0015,
      "step": 58470
    },
    {
      "epoch": 5.198222222222222,
      "grad_norm": 0.14008662104606628,
      "learning_rate": 1.751111111111111e-05,
      "loss": 0.0025,
      "step": 58480
    },
    {
      "epoch": 5.199111111111111,
      "grad_norm": 0.7599433660507202,
      "learning_rate": 1.7505555555555555e-05,
      "loss": 0.0021,
      "step": 58490
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.557567834854126,
      "learning_rate": 1.75e-05,
      "loss": 0.0017,
      "step": 58500
    },
    {
      "epoch": 5.200888888888889,
      "grad_norm": 0.5256307125091553,
      "learning_rate": 1.7494444444444445e-05,
      "loss": 0.0018,
      "step": 58510
    },
    {
      "epoch": 5.201777777777778,
      "grad_norm": 0.23324257135391235,
      "learning_rate": 1.7488888888888892e-05,
      "loss": 0.0015,
      "step": 58520
    },
    {
      "epoch": 5.2026666666666666,
      "grad_norm": 0.2765265107154846,
      "learning_rate": 1.7483333333333336e-05,
      "loss": 0.0025,
      "step": 58530
    },
    {
      "epoch": 5.203555555555556,
      "grad_norm": 0.08094271272420883,
      "learning_rate": 1.747777777777778e-05,
      "loss": 0.0017,
      "step": 58540
    },
    {
      "epoch": 5.204444444444444,
      "grad_norm": 0.1613738089799881,
      "learning_rate": 1.7472222222222223e-05,
      "loss": 0.0015,
      "step": 58550
    },
    {
      "epoch": 5.205333333333333,
      "grad_norm": 0.5563792586326599,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0013,
      "step": 58560
    },
    {
      "epoch": 5.206222222222222,
      "grad_norm": 0.05415287986397743,
      "learning_rate": 1.746111111111111e-05,
      "loss": 0.0018,
      "step": 58570
    },
    {
      "epoch": 5.207111111111111,
      "grad_norm": 0.46770966053009033,
      "learning_rate": 1.7455555555555557e-05,
      "loss": 0.0014,
      "step": 58580
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.15299421548843384,
      "learning_rate": 1.745e-05,
      "loss": 0.0017,
      "step": 58590
    },
    {
      "epoch": 5.208888888888889,
      "grad_norm": 0.3989948630332947,
      "learning_rate": 1.7444444444444448e-05,
      "loss": 0.002,
      "step": 58600
    },
    {
      "epoch": 5.209777777777778,
      "grad_norm": 0.49533209204673767,
      "learning_rate": 1.743888888888889e-05,
      "loss": 0.0016,
      "step": 58610
    },
    {
      "epoch": 5.210666666666667,
      "grad_norm": 0.8098663091659546,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0015,
      "step": 58620
    },
    {
      "epoch": 5.211555555555556,
      "grad_norm": 1.0524494647979736,
      "learning_rate": 1.7427777777777778e-05,
      "loss": 0.0019,
      "step": 58630
    },
    {
      "epoch": 5.212444444444444,
      "grad_norm": 0.32818111777305603,
      "learning_rate": 1.7422222222222222e-05,
      "loss": 0.0017,
      "step": 58640
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 0.25852394104003906,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.0016,
      "step": 58650
    },
    {
      "epoch": 5.214222222222222,
      "grad_norm": 0.2273186892271042,
      "learning_rate": 1.7411111111111112e-05,
      "loss": 0.0025,
      "step": 58660
    },
    {
      "epoch": 5.215111111111111,
      "grad_norm": 0.43301767110824585,
      "learning_rate": 1.7405555555555556e-05,
      "loss": 0.0022,
      "step": 58670
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.1280234456062317,
      "learning_rate": 1.74e-05,
      "loss": 0.002,
      "step": 58680
    },
    {
      "epoch": 5.216888888888889,
      "grad_norm": 0.15183410048484802,
      "learning_rate": 1.7394444444444446e-05,
      "loss": 0.0015,
      "step": 58690
    },
    {
      "epoch": 5.217777777777778,
      "grad_norm": 0.36317455768585205,
      "learning_rate": 1.738888888888889e-05,
      "loss": 0.0024,
      "step": 58700
    },
    {
      "epoch": 5.218666666666667,
      "grad_norm": 0.15991835296154022,
      "learning_rate": 1.7383333333333333e-05,
      "loss": 0.0013,
      "step": 58710
    },
    {
      "epoch": 5.219555555555556,
      "grad_norm": 0.3769424855709076,
      "learning_rate": 1.737777777777778e-05,
      "loss": 0.0022,
      "step": 58720
    },
    {
      "epoch": 5.220444444444444,
      "grad_norm": 0.37385839223861694,
      "learning_rate": 1.7372222222222224e-05,
      "loss": 0.0029,
      "step": 58730
    },
    {
      "epoch": 5.221333333333333,
      "grad_norm": 0.37579774856567383,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0023,
      "step": 58740
    },
    {
      "epoch": 5.222222222222222,
      "grad_norm": 0.15933701395988464,
      "learning_rate": 1.736111111111111e-05,
      "loss": 0.0022,
      "step": 58750
    },
    {
      "epoch": 5.223111111111111,
      "grad_norm": 0.5267916917800903,
      "learning_rate": 1.7355555555555555e-05,
      "loss": 0.0026,
      "step": 58760
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.8001903295516968,
      "learning_rate": 1.7349999999999998e-05,
      "loss": 0.0026,
      "step": 58770
    },
    {
      "epoch": 5.224888888888889,
      "grad_norm": 0.11732612550258636,
      "learning_rate": 1.7344444444444445e-05,
      "loss": 0.002,
      "step": 58780
    },
    {
      "epoch": 5.225777777777778,
      "grad_norm": 0.45374128222465515,
      "learning_rate": 1.7338888888888892e-05,
      "loss": 0.0013,
      "step": 58790
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.18720833957195282,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.002,
      "step": 58800
    },
    {
      "epoch": 5.227555555555556,
      "grad_norm": 0.297367125749588,
      "learning_rate": 1.732777777777778e-05,
      "loss": 0.0017,
      "step": 58810
    },
    {
      "epoch": 5.2284444444444444,
      "grad_norm": 0.5833582282066345,
      "learning_rate": 1.7322222222222223e-05,
      "loss": 0.0014,
      "step": 58820
    },
    {
      "epoch": 5.229333333333333,
      "grad_norm": 0.3424488604068756,
      "learning_rate": 1.7316666666666666e-05,
      "loss": 0.0013,
      "step": 58830
    },
    {
      "epoch": 5.230222222222222,
      "grad_norm": 0.11160348355770111,
      "learning_rate": 1.731111111111111e-05,
      "loss": 0.0015,
      "step": 58840
    },
    {
      "epoch": 5.231111111111111,
      "grad_norm": 0.39825427532196045,
      "learning_rate": 1.7305555555555557e-05,
      "loss": 0.0018,
      "step": 58850
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.19031251966953278,
      "learning_rate": 1.73e-05,
      "loss": 0.0016,
      "step": 58860
    },
    {
      "epoch": 5.232888888888889,
      "grad_norm": 0.29961562156677246,
      "learning_rate": 1.7294444444444447e-05,
      "loss": 0.0019,
      "step": 58870
    },
    {
      "epoch": 5.233777777777778,
      "grad_norm": 0.43854981660842896,
      "learning_rate": 1.728888888888889e-05,
      "loss": 0.0023,
      "step": 58880
    },
    {
      "epoch": 5.234666666666667,
      "grad_norm": 0.5067139267921448,
      "learning_rate": 1.7283333333333334e-05,
      "loss": 0.0017,
      "step": 58890
    },
    {
      "epoch": 5.235555555555556,
      "grad_norm": 0.2594282329082489,
      "learning_rate": 1.7277777777777778e-05,
      "loss": 0.0019,
      "step": 58900
    },
    {
      "epoch": 5.2364444444444445,
      "grad_norm": 0.672169029712677,
      "learning_rate": 1.727222222222222e-05,
      "loss": 0.002,
      "step": 58910
    },
    {
      "epoch": 5.237333333333333,
      "grad_norm": 0.8871020078659058,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0016,
      "step": 58920
    },
    {
      "epoch": 5.238222222222222,
      "grad_norm": 0.058305222541093826,
      "learning_rate": 1.7261111111111112e-05,
      "loss": 0.0031,
      "step": 58930
    },
    {
      "epoch": 5.239111111111111,
      "grad_norm": 0.19305063784122467,
      "learning_rate": 1.7255555555555556e-05,
      "loss": 0.0033,
      "step": 58940
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.35765960812568665,
      "learning_rate": 1.725e-05,
      "loss": 0.0017,
      "step": 58950
    },
    {
      "epoch": 5.240888888888889,
      "grad_norm": 0.9170339703559875,
      "learning_rate": 1.7244444444444446e-05,
      "loss": 0.0028,
      "step": 58960
    },
    {
      "epoch": 5.241777777777778,
      "grad_norm": 0.07455220818519592,
      "learning_rate": 1.723888888888889e-05,
      "loss": 0.0024,
      "step": 58970
    },
    {
      "epoch": 5.242666666666667,
      "grad_norm": 0.1704971194267273,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0017,
      "step": 58980
    },
    {
      "epoch": 5.243555555555556,
      "grad_norm": 0.10206954926252365,
      "learning_rate": 1.722777777777778e-05,
      "loss": 0.0019,
      "step": 58990
    },
    {
      "epoch": 5.2444444444444445,
      "grad_norm": 0.18391187489032745,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 0.0023,
      "step": 59000
    },
    {
      "epoch": 5.245333333333333,
      "grad_norm": 0.07623730599880219,
      "learning_rate": 1.7216666666666667e-05,
      "loss": 0.0029,
      "step": 59010
    },
    {
      "epoch": 5.246222222222222,
      "grad_norm": 0.10606708377599716,
      "learning_rate": 1.721111111111111e-05,
      "loss": 0.0024,
      "step": 59020
    },
    {
      "epoch": 5.247111111111111,
      "grad_norm": 0.1956000179052353,
      "learning_rate": 1.7205555555555554e-05,
      "loss": 0.0013,
      "step": 59030
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.05382475629448891,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0012,
      "step": 59040
    },
    {
      "epoch": 5.248888888888889,
      "grad_norm": 0.22945590317249298,
      "learning_rate": 1.7194444444444445e-05,
      "loss": 0.0015,
      "step": 59050
    },
    {
      "epoch": 5.249777777777778,
      "grad_norm": 0.3637944757938385,
      "learning_rate": 1.7188888888888892e-05,
      "loss": 0.0014,
      "step": 59060
    },
    {
      "epoch": 5.250666666666667,
      "grad_norm": 0.5489708781242371,
      "learning_rate": 1.7183333333333335e-05,
      "loss": 0.0017,
      "step": 59070
    },
    {
      "epoch": 5.251555555555555,
      "grad_norm": 0.02711622230708599,
      "learning_rate": 1.717777777777778e-05,
      "loss": 0.0014,
      "step": 59080
    },
    {
      "epoch": 5.2524444444444445,
      "grad_norm": 0.4100760221481323,
      "learning_rate": 1.7172222222222223e-05,
      "loss": 0.0017,
      "step": 59090
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 0.45028671622276306,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.0038,
      "step": 59100
    },
    {
      "epoch": 5.254222222222222,
      "grad_norm": 0.08693653345108032,
      "learning_rate": 1.716111111111111e-05,
      "loss": 0.0014,
      "step": 59110
    },
    {
      "epoch": 5.255111111111111,
      "grad_norm": 0.32960274815559387,
      "learning_rate": 1.7155555555555557e-05,
      "loss": 0.0019,
      "step": 59120
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.5479087829589844,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.0014,
      "step": 59130
    },
    {
      "epoch": 5.256888888888889,
      "grad_norm": 0.12016352266073227,
      "learning_rate": 1.7144444444444447e-05,
      "loss": 0.0012,
      "step": 59140
    },
    {
      "epoch": 5.257777777777778,
      "grad_norm": 0.27998828887939453,
      "learning_rate": 1.713888888888889e-05,
      "loss": 0.0023,
      "step": 59150
    },
    {
      "epoch": 5.258666666666667,
      "grad_norm": 0.04797960817813873,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0026,
      "step": 59160
    },
    {
      "epoch": 5.259555555555556,
      "grad_norm": 0.18449801206588745,
      "learning_rate": 1.7127777777777778e-05,
      "loss": 0.0023,
      "step": 59170
    },
    {
      "epoch": 5.2604444444444445,
      "grad_norm": 0.2528308629989624,
      "learning_rate": 1.712222222222222e-05,
      "loss": 0.0019,
      "step": 59180
    },
    {
      "epoch": 5.261333333333333,
      "grad_norm": 0.5305545926094055,
      "learning_rate": 1.7116666666666668e-05,
      "loss": 0.0021,
      "step": 59190
    },
    {
      "epoch": 5.262222222222222,
      "grad_norm": 0.3472251296043396,
      "learning_rate": 1.7111111111111112e-05,
      "loss": 0.0017,
      "step": 59200
    },
    {
      "epoch": 5.263111111111111,
      "grad_norm": 0.18311387300491333,
      "learning_rate": 1.7105555555555555e-05,
      "loss": 0.0021,
      "step": 59210
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.32449233531951904,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.0021,
      "step": 59220
    },
    {
      "epoch": 5.264888888888889,
      "grad_norm": 0.34409546852111816,
      "learning_rate": 1.7094444444444446e-05,
      "loss": 0.0019,
      "step": 59230
    },
    {
      "epoch": 5.265777777777778,
      "grad_norm": 0.6453892588615417,
      "learning_rate": 1.708888888888889e-05,
      "loss": 0.0013,
      "step": 59240
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.7562631368637085,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.0027,
      "step": 59250
    },
    {
      "epoch": 5.267555555555555,
      "grad_norm": 0.6615743637084961,
      "learning_rate": 1.707777777777778e-05,
      "loss": 0.002,
      "step": 59260
    },
    {
      "epoch": 5.2684444444444445,
      "grad_norm": 0.1028897762298584,
      "learning_rate": 1.7072222222222223e-05,
      "loss": 0.0039,
      "step": 59270
    },
    {
      "epoch": 5.269333333333333,
      "grad_norm": 0.21342557668685913,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.002,
      "step": 59280
    },
    {
      "epoch": 5.270222222222222,
      "grad_norm": 0.23838554322719574,
      "learning_rate": 1.706111111111111e-05,
      "loss": 0.0025,
      "step": 59290
    },
    {
      "epoch": 5.271111111111111,
      "grad_norm": 0.19331912696361542,
      "learning_rate": 1.7055555555555554e-05,
      "loss": 0.0017,
      "step": 59300
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.12237469851970673,
      "learning_rate": 1.705e-05,
      "loss": 0.0023,
      "step": 59310
    },
    {
      "epoch": 5.272888888888889,
      "grad_norm": 0.2404608577489853,
      "learning_rate": 1.7044444444444445e-05,
      "loss": 0.0022,
      "step": 59320
    },
    {
      "epoch": 5.273777777777778,
      "grad_norm": 0.24996736645698547,
      "learning_rate": 1.703888888888889e-05,
      "loss": 0.0019,
      "step": 59330
    },
    {
      "epoch": 5.274666666666667,
      "grad_norm": 0.542745053768158,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0017,
      "step": 59340
    },
    {
      "epoch": 5.275555555555556,
      "grad_norm": 0.5996257662773132,
      "learning_rate": 1.702777777777778e-05,
      "loss": 0.0023,
      "step": 59350
    },
    {
      "epoch": 5.2764444444444445,
      "grad_norm": 0.3349122703075409,
      "learning_rate": 1.7022222222222222e-05,
      "loss": 0.0015,
      "step": 59360
    },
    {
      "epoch": 5.277333333333333,
      "grad_norm": 0.15919604897499084,
      "learning_rate": 1.7016666666666666e-05,
      "loss": 0.0019,
      "step": 59370
    },
    {
      "epoch": 5.278222222222222,
      "grad_norm": 0.06259013712406158,
      "learning_rate": 1.701111111111111e-05,
      "loss": 0.0015,
      "step": 59380
    },
    {
      "epoch": 5.279111111111111,
      "grad_norm": 0.4780007600784302,
      "learning_rate": 1.7005555555555556e-05,
      "loss": 0.0026,
      "step": 59390
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.8027055263519287,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0024,
      "step": 59400
    },
    {
      "epoch": 5.280888888888889,
      "grad_norm": 0.6361148953437805,
      "learning_rate": 1.6994444444444447e-05,
      "loss": 0.0019,
      "step": 59410
    },
    {
      "epoch": 5.281777777777778,
      "grad_norm": 1.0373115539550781,
      "learning_rate": 1.698888888888889e-05,
      "loss": 0.0033,
      "step": 59420
    },
    {
      "epoch": 5.282666666666667,
      "grad_norm": 0.20285353064537048,
      "learning_rate": 1.6983333333333334e-05,
      "loss": 0.002,
      "step": 59430
    },
    {
      "epoch": 5.283555555555555,
      "grad_norm": 0.3703184127807617,
      "learning_rate": 1.6977777777777777e-05,
      "loss": 0.0019,
      "step": 59440
    },
    {
      "epoch": 5.2844444444444445,
      "grad_norm": 0.07377389818429947,
      "learning_rate": 1.697222222222222e-05,
      "loss": 0.0022,
      "step": 59450
    },
    {
      "epoch": 5.285333333333333,
      "grad_norm": 0.3440153896808624,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0026,
      "step": 59460
    },
    {
      "epoch": 5.286222222222222,
      "grad_norm": 0.4100344777107239,
      "learning_rate": 1.696111111111111e-05,
      "loss": 0.0029,
      "step": 59470
    },
    {
      "epoch": 5.287111111111111,
      "grad_norm": 0.316021203994751,
      "learning_rate": 1.6955555555555555e-05,
      "loss": 0.0018,
      "step": 59480
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.11779095232486725,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.0021,
      "step": 59490
    },
    {
      "epoch": 5.288888888888889,
      "grad_norm": 0.5216803550720215,
      "learning_rate": 1.6944444444444446e-05,
      "loss": 0.0017,
      "step": 59500
    },
    {
      "epoch": 5.289777777777778,
      "grad_norm": 0.0639517530798912,
      "learning_rate": 1.693888888888889e-05,
      "loss": 0.0025,
      "step": 59510
    },
    {
      "epoch": 5.290666666666667,
      "grad_norm": 0.11631061881780624,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0017,
      "step": 59520
    },
    {
      "epoch": 5.291555555555556,
      "grad_norm": 0.3629075884819031,
      "learning_rate": 1.692777777777778e-05,
      "loss": 0.0024,
      "step": 59530
    },
    {
      "epoch": 5.2924444444444445,
      "grad_norm": 0.051608435809612274,
      "learning_rate": 1.6922222222222223e-05,
      "loss": 0.0019,
      "step": 59540
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 0.26287394762039185,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.0019,
      "step": 59550
    },
    {
      "epoch": 5.294222222222222,
      "grad_norm": 0.10757136344909668,
      "learning_rate": 1.691111111111111e-05,
      "loss": 0.0017,
      "step": 59560
    },
    {
      "epoch": 5.295111111111111,
      "grad_norm": 0.144344761967659,
      "learning_rate": 1.6905555555555554e-05,
      "loss": 0.0025,
      "step": 59570
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.6591891050338745,
      "learning_rate": 1.69e-05,
      "loss": 0.0021,
      "step": 59580
    },
    {
      "epoch": 5.296888888888889,
      "grad_norm": 0.30070552229881287,
      "learning_rate": 1.6894444444444444e-05,
      "loss": 0.0025,
      "step": 59590
    },
    {
      "epoch": 5.297777777777778,
      "grad_norm": 0.04177749902009964,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.0033,
      "step": 59600
    },
    {
      "epoch": 5.298666666666667,
      "grad_norm": 0.3589935898780823,
      "learning_rate": 1.6883333333333335e-05,
      "loss": 0.0017,
      "step": 59610
    },
    {
      "epoch": 5.299555555555555,
      "grad_norm": 0.4398854672908783,
      "learning_rate": 1.687777777777778e-05,
      "loss": 0.0024,
      "step": 59620
    },
    {
      "epoch": 5.3004444444444445,
      "grad_norm": 0.4639679491519928,
      "learning_rate": 1.6872222222222222e-05,
      "loss": 0.0018,
      "step": 59630
    },
    {
      "epoch": 5.301333333333333,
      "grad_norm": 0.05782462656497955,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0017,
      "step": 59640
    },
    {
      "epoch": 5.302222222222222,
      "grad_norm": 0.38675281405448914,
      "learning_rate": 1.6861111111111112e-05,
      "loss": 0.0018,
      "step": 59650
    },
    {
      "epoch": 5.303111111111111,
      "grad_norm": 0.3974326252937317,
      "learning_rate": 1.6855555555555556e-05,
      "loss": 0.0021,
      "step": 59660
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.12428944557905197,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 0.002,
      "step": 59670
    },
    {
      "epoch": 5.304888888888889,
      "grad_norm": 0.11627350747585297,
      "learning_rate": 1.6844444444444447e-05,
      "loss": 0.0018,
      "step": 59680
    },
    {
      "epoch": 5.305777777777778,
      "grad_norm": 0.4017042815685272,
      "learning_rate": 1.683888888888889e-05,
      "loss": 0.002,
      "step": 59690
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 0.11928214877843857,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0025,
      "step": 59700
    },
    {
      "epoch": 5.307555555555556,
      "grad_norm": 0.02980133704841137,
      "learning_rate": 1.6827777777777777e-05,
      "loss": 0.0022,
      "step": 59710
    },
    {
      "epoch": 5.3084444444444445,
      "grad_norm": 0.6697218418121338,
      "learning_rate": 1.6822222222222224e-05,
      "loss": 0.0019,
      "step": 59720
    },
    {
      "epoch": 5.309333333333333,
      "grad_norm": 0.118203304708004,
      "learning_rate": 1.6816666666666668e-05,
      "loss": 0.0016,
      "step": 59730
    },
    {
      "epoch": 5.310222222222222,
      "grad_norm": 0.11516621708869934,
      "learning_rate": 1.681111111111111e-05,
      "loss": 0.002,
      "step": 59740
    },
    {
      "epoch": 5.311111111111111,
      "grad_norm": 0.06679140031337738,
      "learning_rate": 1.6805555555555558e-05,
      "loss": 0.0017,
      "step": 59750
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.10252567380666733,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.002,
      "step": 59760
    },
    {
      "epoch": 5.312888888888889,
      "grad_norm": 0.15324725210666656,
      "learning_rate": 1.6794444444444445e-05,
      "loss": 0.0022,
      "step": 59770
    },
    {
      "epoch": 5.313777777777778,
      "grad_norm": 0.3184400200843811,
      "learning_rate": 1.678888888888889e-05,
      "loss": 0.0016,
      "step": 59780
    },
    {
      "epoch": 5.314666666666667,
      "grad_norm": 0.34568217396736145,
      "learning_rate": 1.6783333333333336e-05,
      "loss": 0.0023,
      "step": 59790
    },
    {
      "epoch": 5.315555555555555,
      "grad_norm": 0.3637191355228424,
      "learning_rate": 1.677777777777778e-05,
      "loss": 0.0016,
      "step": 59800
    },
    {
      "epoch": 5.3164444444444445,
      "grad_norm": 0.11569257080554962,
      "learning_rate": 1.6772222222222223e-05,
      "loss": 0.0017,
      "step": 59810
    },
    {
      "epoch": 5.317333333333333,
      "grad_norm": 0.24209406971931458,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0015,
      "step": 59820
    },
    {
      "epoch": 5.318222222222222,
      "grad_norm": 0.12488812208175659,
      "learning_rate": 1.676111111111111e-05,
      "loss": 0.0017,
      "step": 59830
    },
    {
      "epoch": 5.319111111111111,
      "grad_norm": 0.4459400177001953,
      "learning_rate": 1.6755555555555557e-05,
      "loss": 0.0025,
      "step": 59840
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.6235782504081726,
      "learning_rate": 1.675e-05,
      "loss": 0.0035,
      "step": 59850
    },
    {
      "epoch": 5.320888888888889,
      "grad_norm": 0.4146961271762848,
      "learning_rate": 1.6744444444444448e-05,
      "loss": 0.0019,
      "step": 59860
    },
    {
      "epoch": 5.321777777777778,
      "grad_norm": 0.4020223617553711,
      "learning_rate": 1.673888888888889e-05,
      "loss": 0.0016,
      "step": 59870
    },
    {
      "epoch": 5.322666666666667,
      "grad_norm": 0.17089344561100006,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.003,
      "step": 59880
    },
    {
      "epoch": 5.323555555555555,
      "grad_norm": 0.15395019948482513,
      "learning_rate": 1.6727777777777778e-05,
      "loss": 0.0019,
      "step": 59890
    },
    {
      "epoch": 5.3244444444444445,
      "grad_norm": 0.08023704588413239,
      "learning_rate": 1.6722222222222222e-05,
      "loss": 0.0019,
      "step": 59900
    },
    {
      "epoch": 5.325333333333333,
      "grad_norm": 0.3100826144218445,
      "learning_rate": 1.6716666666666665e-05,
      "loss": 0.0016,
      "step": 59910
    },
    {
      "epoch": 5.326222222222222,
      "grad_norm": 0.09767311066389084,
      "learning_rate": 1.6711111111111112e-05,
      "loss": 0.0017,
      "step": 59920
    },
    {
      "epoch": 5.327111111111111,
      "grad_norm": 0.1777537316083908,
      "learning_rate": 1.670555555555556e-05,
      "loss": 0.0018,
      "step": 59930
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.6411991119384766,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0018,
      "step": 59940
    },
    {
      "epoch": 5.328888888888889,
      "grad_norm": 0.25154563784599304,
      "learning_rate": 1.6694444444444446e-05,
      "loss": 0.0014,
      "step": 59950
    },
    {
      "epoch": 5.329777777777778,
      "grad_norm": 0.03778361156582832,
      "learning_rate": 1.668888888888889e-05,
      "loss": 0.0024,
      "step": 59960
    },
    {
      "epoch": 5.330666666666667,
      "grad_norm": 0.052792470902204514,
      "learning_rate": 1.6683333333333333e-05,
      "loss": 0.0012,
      "step": 59970
    },
    {
      "epoch": 5.331555555555555,
      "grad_norm": 0.30709391832351685,
      "learning_rate": 1.6677777777777777e-05,
      "loss": 0.0024,
      "step": 59980
    },
    {
      "epoch": 5.3324444444444445,
      "grad_norm": 0.5071002244949341,
      "learning_rate": 1.6672222222222224e-05,
      "loss": 0.0021,
      "step": 59990
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.7689773440361023,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0022,
      "step": 60000
    },
    {
      "epoch": 5.334222222222222,
      "grad_norm": 0.4797326922416687,
      "learning_rate": 1.666111111111111e-05,
      "loss": 0.0016,
      "step": 60010
    },
    {
      "epoch": 5.335111111111111,
      "grad_norm": 0.08450046181678772,
      "learning_rate": 1.6655555555555558e-05,
      "loss": 0.0022,
      "step": 60020
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.349627822637558,
      "learning_rate": 1.665e-05,
      "loss": 0.0017,
      "step": 60030
    },
    {
      "epoch": 5.336888888888889,
      "grad_norm": 0.17467030882835388,
      "learning_rate": 1.6644444444444445e-05,
      "loss": 0.0017,
      "step": 60040
    },
    {
      "epoch": 5.337777777777778,
      "grad_norm": 0.3343852460384369,
      "learning_rate": 1.663888888888889e-05,
      "loss": 0.0016,
      "step": 60050
    },
    {
      "epoch": 5.338666666666667,
      "grad_norm": 0.3643663823604584,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0017,
      "step": 60060
    },
    {
      "epoch": 5.339555555555555,
      "grad_norm": 0.29482579231262207,
      "learning_rate": 1.662777777777778e-05,
      "loss": 0.0023,
      "step": 60070
    },
    {
      "epoch": 5.3404444444444445,
      "grad_norm": 0.6098430156707764,
      "learning_rate": 1.6622222222222223e-05,
      "loss": 0.0017,
      "step": 60080
    },
    {
      "epoch": 5.341333333333333,
      "grad_norm": 0.08405090123414993,
      "learning_rate": 1.6616666666666666e-05,
      "loss": 0.0016,
      "step": 60090
    },
    {
      "epoch": 5.342222222222222,
      "grad_norm": 0.5866997241973877,
      "learning_rate": 1.661111111111111e-05,
      "loss": 0.0025,
      "step": 60100
    },
    {
      "epoch": 5.343111111111111,
      "grad_norm": 0.2598593235015869,
      "learning_rate": 1.6605555555555557e-05,
      "loss": 0.0023,
      "step": 60110
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.10736652463674545,
      "learning_rate": 1.66e-05,
      "loss": 0.0027,
      "step": 60120
    },
    {
      "epoch": 5.344888888888889,
      "grad_norm": 0.4353371560573578,
      "learning_rate": 1.6594444444444447e-05,
      "loss": 0.0017,
      "step": 60130
    },
    {
      "epoch": 5.345777777777778,
      "grad_norm": 0.5239979028701782,
      "learning_rate": 1.658888888888889e-05,
      "loss": 0.0019,
      "step": 60140
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 0.04105597361922264,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.0019,
      "step": 60150
    },
    {
      "epoch": 5.347555555555555,
      "grad_norm": 0.08426228910684586,
      "learning_rate": 1.6577777777777778e-05,
      "loss": 0.0014,
      "step": 60160
    },
    {
      "epoch": 5.348444444444445,
      "grad_norm": 0.22804315388202667,
      "learning_rate": 1.657222222222222e-05,
      "loss": 0.002,
      "step": 60170
    },
    {
      "epoch": 5.349333333333333,
      "grad_norm": 0.29756593704223633,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0017,
      "step": 60180
    },
    {
      "epoch": 5.350222222222222,
      "grad_norm": 0.062442597001791,
      "learning_rate": 1.6561111111111112e-05,
      "loss": 0.0029,
      "step": 60190
    },
    {
      "epoch": 5.351111111111111,
      "grad_norm": 0.045790426433086395,
      "learning_rate": 1.655555555555556e-05,
      "loss": 0.0014,
      "step": 60200
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.15994355082511902,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 0.002,
      "step": 60210
    },
    {
      "epoch": 5.352888888888889,
      "grad_norm": 0.1964045614004135,
      "learning_rate": 1.6544444444444446e-05,
      "loss": 0.0028,
      "step": 60220
    },
    {
      "epoch": 5.353777777777778,
      "grad_norm": 0.6924227476119995,
      "learning_rate": 1.653888888888889e-05,
      "loss": 0.0022,
      "step": 60230
    },
    {
      "epoch": 5.354666666666667,
      "grad_norm": 0.1489333212375641,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0024,
      "step": 60240
    },
    {
      "epoch": 5.355555555555555,
      "grad_norm": 0.39719852805137634,
      "learning_rate": 1.6527777777777777e-05,
      "loss": 0.0026,
      "step": 60250
    },
    {
      "epoch": 5.356444444444445,
      "grad_norm": 0.1821223795413971,
      "learning_rate": 1.6522222222222224e-05,
      "loss": 0.0014,
      "step": 60260
    },
    {
      "epoch": 5.357333333333333,
      "grad_norm": 0.24701321125030518,
      "learning_rate": 1.6516666666666667e-05,
      "loss": 0.0019,
      "step": 60270
    },
    {
      "epoch": 5.358222222222222,
      "grad_norm": 0.22819390892982483,
      "learning_rate": 1.651111111111111e-05,
      "loss": 0.002,
      "step": 60280
    },
    {
      "epoch": 5.359111111111111,
      "grad_norm": 0.11845412105321884,
      "learning_rate": 1.6505555555555558e-05,
      "loss": 0.0018,
      "step": 60290
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.18705785274505615,
      "learning_rate": 1.65e-05,
      "loss": 0.0026,
      "step": 60300
    },
    {
      "epoch": 5.360888888888889,
      "grad_norm": 0.6977856159210205,
      "learning_rate": 1.6494444444444445e-05,
      "loss": 0.0027,
      "step": 60310
    },
    {
      "epoch": 5.361777777777778,
      "grad_norm": 0.44479840993881226,
      "learning_rate": 1.648888888888889e-05,
      "loss": 0.0015,
      "step": 60320
    },
    {
      "epoch": 5.362666666666667,
      "grad_norm": 0.17174603044986725,
      "learning_rate": 1.6483333333333335e-05,
      "loss": 0.0018,
      "step": 60330
    },
    {
      "epoch": 5.363555555555555,
      "grad_norm": 0.12399716675281525,
      "learning_rate": 1.647777777777778e-05,
      "loss": 0.002,
      "step": 60340
    },
    {
      "epoch": 5.364444444444445,
      "grad_norm": 0.7215689420700073,
      "learning_rate": 1.6472222222222222e-05,
      "loss": 0.002,
      "step": 60350
    },
    {
      "epoch": 5.365333333333333,
      "grad_norm": 0.3738340735435486,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.002,
      "step": 60360
    },
    {
      "epoch": 5.3662222222222224,
      "grad_norm": 0.11549783498048782,
      "learning_rate": 1.6461111111111113e-05,
      "loss": 0.0016,
      "step": 60370
    },
    {
      "epoch": 5.367111111111111,
      "grad_norm": 0.2300843894481659,
      "learning_rate": 1.6455555555555556e-05,
      "loss": 0.0028,
      "step": 60380
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.03860001638531685,
      "learning_rate": 1.645e-05,
      "loss": 0.0016,
      "step": 60390
    },
    {
      "epoch": 5.368888888888889,
      "grad_norm": 0.19535985589027405,
      "learning_rate": 1.6444444444444447e-05,
      "loss": 0.0015,
      "step": 60400
    },
    {
      "epoch": 5.369777777777777,
      "grad_norm": 0.029815196990966797,
      "learning_rate": 1.643888888888889e-05,
      "loss": 0.0018,
      "step": 60410
    },
    {
      "epoch": 5.370666666666667,
      "grad_norm": 0.20057837665081024,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.0025,
      "step": 60420
    },
    {
      "epoch": 5.371555555555555,
      "grad_norm": 0.057849232107400894,
      "learning_rate": 1.6427777777777778e-05,
      "loss": 0.0021,
      "step": 60430
    },
    {
      "epoch": 5.372444444444445,
      "grad_norm": 0.3770982027053833,
      "learning_rate": 1.642222222222222e-05,
      "loss": 0.0018,
      "step": 60440
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 0.9679749011993408,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.0023,
      "step": 60450
    },
    {
      "epoch": 5.3742222222222225,
      "grad_norm": 0.1253998875617981,
      "learning_rate": 1.6411111111111112e-05,
      "loss": 0.0022,
      "step": 60460
    },
    {
      "epoch": 5.375111111111111,
      "grad_norm": 0.05268990993499756,
      "learning_rate": 1.640555555555556e-05,
      "loss": 0.0024,
      "step": 60470
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.5768216848373413,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.002,
      "step": 60480
    },
    {
      "epoch": 5.376888888888889,
      "grad_norm": 0.6002169847488403,
      "learning_rate": 1.6394444444444446e-05,
      "loss": 0.0024,
      "step": 60490
    },
    {
      "epoch": 5.377777777777778,
      "grad_norm": 0.22432298958301544,
      "learning_rate": 1.638888888888889e-05,
      "loss": 0.0019,
      "step": 60500
    },
    {
      "epoch": 5.378666666666667,
      "grad_norm": 0.41096919775009155,
      "learning_rate": 1.6383333333333333e-05,
      "loss": 0.0021,
      "step": 60510
    },
    {
      "epoch": 5.379555555555555,
      "grad_norm": 0.21382464468479156,
      "learning_rate": 1.6377777777777776e-05,
      "loss": 0.0029,
      "step": 60520
    },
    {
      "epoch": 5.380444444444445,
      "grad_norm": 0.15237700939178467,
      "learning_rate": 1.6372222222222223e-05,
      "loss": 0.0014,
      "step": 60530
    },
    {
      "epoch": 5.381333333333333,
      "grad_norm": 0.32327574491500854,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0027,
      "step": 60540
    },
    {
      "epoch": 5.3822222222222225,
      "grad_norm": 0.49269044399261475,
      "learning_rate": 1.6361111111111114e-05,
      "loss": 0.0015,
      "step": 60550
    },
    {
      "epoch": 5.383111111111111,
      "grad_norm": 0.21758298575878143,
      "learning_rate": 1.6355555555555557e-05,
      "loss": 0.0021,
      "step": 60560
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.19490185379981995,
      "learning_rate": 1.635e-05,
      "loss": 0.0017,
      "step": 60570
    },
    {
      "epoch": 5.384888888888889,
      "grad_norm": 0.21682532131671906,
      "learning_rate": 1.6344444444444445e-05,
      "loss": 0.0022,
      "step": 60580
    },
    {
      "epoch": 5.385777777777777,
      "grad_norm": 0.15877573192119598,
      "learning_rate": 1.6338888888888888e-05,
      "loss": 0.0015,
      "step": 60590
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.5106988549232483,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0022,
      "step": 60600
    },
    {
      "epoch": 5.387555555555555,
      "grad_norm": 0.33402881026268005,
      "learning_rate": 1.632777777777778e-05,
      "loss": 0.0015,
      "step": 60610
    },
    {
      "epoch": 5.388444444444445,
      "grad_norm": 0.09675747901201248,
      "learning_rate": 1.6322222222222222e-05,
      "loss": 0.0016,
      "step": 60620
    },
    {
      "epoch": 5.389333333333333,
      "grad_norm": 0.5567681193351746,
      "learning_rate": 1.6316666666666666e-05,
      "loss": 0.0016,
      "step": 60630
    },
    {
      "epoch": 5.3902222222222225,
      "grad_norm": 0.15420134365558624,
      "learning_rate": 1.6311111111111113e-05,
      "loss": 0.0016,
      "step": 60640
    },
    {
      "epoch": 5.391111111111111,
      "grad_norm": 0.18819385766983032,
      "learning_rate": 1.6305555555555556e-05,
      "loss": 0.0027,
      "step": 60650
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.5757738351821899,
      "learning_rate": 1.63e-05,
      "loss": 0.0026,
      "step": 60660
    },
    {
      "epoch": 5.392888888888889,
      "grad_norm": 0.4041765332221985,
      "learning_rate": 1.6294444444444447e-05,
      "loss": 0.0023,
      "step": 60670
    },
    {
      "epoch": 5.393777777777778,
      "grad_norm": 0.1100856214761734,
      "learning_rate": 1.628888888888889e-05,
      "loss": 0.002,
      "step": 60680
    },
    {
      "epoch": 5.394666666666667,
      "grad_norm": 0.4956789016723633,
      "learning_rate": 1.6283333333333334e-05,
      "loss": 0.0015,
      "step": 60690
    },
    {
      "epoch": 5.395555555555555,
      "grad_norm": 0.047559741884469986,
      "learning_rate": 1.6277777777777777e-05,
      "loss": 0.0014,
      "step": 60700
    },
    {
      "epoch": 5.396444444444445,
      "grad_norm": 0.5801475644111633,
      "learning_rate": 1.627222222222222e-05,
      "loss": 0.0024,
      "step": 60710
    },
    {
      "epoch": 5.397333333333333,
      "grad_norm": 0.4507785141468048,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0017,
      "step": 60720
    },
    {
      "epoch": 5.3982222222222225,
      "grad_norm": 0.188153475522995,
      "learning_rate": 1.626111111111111e-05,
      "loss": 0.0014,
      "step": 60730
    },
    {
      "epoch": 5.399111111111111,
      "grad_norm": 0.4204087257385254,
      "learning_rate": 1.625555555555556e-05,
      "loss": 0.0026,
      "step": 60740
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.22982129454612732,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.002,
      "step": 60750
    },
    {
      "epoch": 5.400888888888889,
      "grad_norm": 0.4077843427658081,
      "learning_rate": 1.6244444444444446e-05,
      "loss": 0.0016,
      "step": 60760
    },
    {
      "epoch": 5.401777777777777,
      "grad_norm": 0.2589520514011383,
      "learning_rate": 1.623888888888889e-05,
      "loss": 0.0017,
      "step": 60770
    },
    {
      "epoch": 5.402666666666667,
      "grad_norm": 0.515424907207489,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.0023,
      "step": 60780
    },
    {
      "epoch": 5.403555555555555,
      "grad_norm": 0.6030315160751343,
      "learning_rate": 1.6227777777777776e-05,
      "loss": 0.0019,
      "step": 60790
    },
    {
      "epoch": 5.404444444444445,
      "grad_norm": 0.1199432983994484,
      "learning_rate": 1.6222222222222223e-05,
      "loss": 0.0022,
      "step": 60800
    },
    {
      "epoch": 5.405333333333333,
      "grad_norm": 0.06258125603199005,
      "learning_rate": 1.6216666666666667e-05,
      "loss": 0.0023,
      "step": 60810
    },
    {
      "epoch": 5.4062222222222225,
      "grad_norm": 0.4713529646396637,
      "learning_rate": 1.6211111111111114e-05,
      "loss": 0.002,
      "step": 60820
    },
    {
      "epoch": 5.407111111111111,
      "grad_norm": 0.14085906744003296,
      "learning_rate": 1.6205555555555557e-05,
      "loss": 0.0019,
      "step": 60830
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.0850706398487091,
      "learning_rate": 1.62e-05,
      "loss": 0.0015,
      "step": 60840
    },
    {
      "epoch": 5.408888888888889,
      "grad_norm": 0.35744187235832214,
      "learning_rate": 1.6194444444444444e-05,
      "loss": 0.0014,
      "step": 60850
    },
    {
      "epoch": 5.409777777777778,
      "grad_norm": 0.16481904685497284,
      "learning_rate": 1.6188888888888888e-05,
      "loss": 0.0016,
      "step": 60860
    },
    {
      "epoch": 5.410666666666667,
      "grad_norm": 0.19087032973766327,
      "learning_rate": 1.6183333333333335e-05,
      "loss": 0.0022,
      "step": 60870
    },
    {
      "epoch": 5.411555555555555,
      "grad_norm": 0.3314055800437927,
      "learning_rate": 1.617777777777778e-05,
      "loss": 0.0015,
      "step": 60880
    },
    {
      "epoch": 5.412444444444445,
      "grad_norm": 1.136209487915039,
      "learning_rate": 1.6172222222222222e-05,
      "loss": 0.0014,
      "step": 60890
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 0.14218109846115112,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0019,
      "step": 60900
    },
    {
      "epoch": 5.4142222222222225,
      "grad_norm": 0.0429450161755085,
      "learning_rate": 1.6161111111111112e-05,
      "loss": 0.0026,
      "step": 60910
    },
    {
      "epoch": 5.415111111111111,
      "grad_norm": 0.6564057469367981,
      "learning_rate": 1.6155555555555556e-05,
      "loss": 0.0015,
      "step": 60920
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.1615895926952362,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.0019,
      "step": 60930
    },
    {
      "epoch": 5.416888888888889,
      "grad_norm": 0.3531358540058136,
      "learning_rate": 1.6144444444444446e-05,
      "loss": 0.0016,
      "step": 60940
    },
    {
      "epoch": 5.417777777777777,
      "grad_norm": 0.2462083250284195,
      "learning_rate": 1.613888888888889e-05,
      "loss": 0.002,
      "step": 60950
    },
    {
      "epoch": 5.418666666666667,
      "grad_norm": 0.4428763687610626,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0021,
      "step": 60960
    },
    {
      "epoch": 5.419555555555555,
      "grad_norm": 0.24401965737342834,
      "learning_rate": 1.6127777777777777e-05,
      "loss": 0.0013,
      "step": 60970
    },
    {
      "epoch": 5.420444444444445,
      "grad_norm": 0.23277722299098969,
      "learning_rate": 1.612222222222222e-05,
      "loss": 0.0015,
      "step": 60980
    },
    {
      "epoch": 5.421333333333333,
      "grad_norm": 0.04418057203292847,
      "learning_rate": 1.6116666666666668e-05,
      "loss": 0.0025,
      "step": 60990
    },
    {
      "epoch": 5.4222222222222225,
      "grad_norm": 0.14515089988708496,
      "learning_rate": 1.6111111111111115e-05,
      "loss": 0.0018,
      "step": 61000
    },
    {
      "epoch": 5.423111111111111,
      "grad_norm": 0.2244872748851776,
      "learning_rate": 1.6105555555555558e-05,
      "loss": 0.002,
      "step": 61010
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.31376543641090393,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0028,
      "step": 61020
    },
    {
      "epoch": 5.424888888888889,
      "grad_norm": 0.41450250148773193,
      "learning_rate": 1.6094444444444445e-05,
      "loss": 0.0019,
      "step": 61030
    },
    {
      "epoch": 5.425777777777777,
      "grad_norm": 0.16099439561367035,
      "learning_rate": 1.608888888888889e-05,
      "loss": 0.002,
      "step": 61040
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.3390049636363983,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.0017,
      "step": 61050
    },
    {
      "epoch": 5.427555555555555,
      "grad_norm": 0.49341195821762085,
      "learning_rate": 1.607777777777778e-05,
      "loss": 0.0012,
      "step": 61060
    },
    {
      "epoch": 5.428444444444445,
      "grad_norm": 0.4756416082382202,
      "learning_rate": 1.6072222222222223e-05,
      "loss": 0.0015,
      "step": 61070
    },
    {
      "epoch": 5.429333333333333,
      "grad_norm": 0.09003963321447372,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0025,
      "step": 61080
    },
    {
      "epoch": 5.4302222222222225,
      "grad_norm": 0.29095277190208435,
      "learning_rate": 1.6061111111111113e-05,
      "loss": 0.0016,
      "step": 61090
    },
    {
      "epoch": 5.431111111111111,
      "grad_norm": 0.26536548137664795,
      "learning_rate": 1.6055555555555557e-05,
      "loss": 0.0024,
      "step": 61100
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.11943445354700089,
      "learning_rate": 1.605e-05,
      "loss": 0.0018,
      "step": 61110
    },
    {
      "epoch": 5.432888888888889,
      "grad_norm": 0.0695161446928978,
      "learning_rate": 1.6044444444444444e-05,
      "loss": 0.0026,
      "step": 61120
    },
    {
      "epoch": 5.433777777777777,
      "grad_norm": 0.06413247436285019,
      "learning_rate": 1.603888888888889e-05,
      "loss": 0.0019,
      "step": 61130
    },
    {
      "epoch": 5.434666666666667,
      "grad_norm": 0.23252888023853302,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.003,
      "step": 61140
    },
    {
      "epoch": 5.435555555555555,
      "grad_norm": 0.6466708779335022,
      "learning_rate": 1.6027777777777778e-05,
      "loss": 0.0019,
      "step": 61150
    },
    {
      "epoch": 5.436444444444445,
      "grad_norm": 0.05942778289318085,
      "learning_rate": 1.602222222222222e-05,
      "loss": 0.002,
      "step": 61160
    },
    {
      "epoch": 5.437333333333333,
      "grad_norm": 0.6969907879829407,
      "learning_rate": 1.601666666666667e-05,
      "loss": 0.0015,
      "step": 61170
    },
    {
      "epoch": 5.4382222222222225,
      "grad_norm": 0.39874619245529175,
      "learning_rate": 1.6011111111111112e-05,
      "loss": 0.0019,
      "step": 61180
    },
    {
      "epoch": 5.439111111111111,
      "grad_norm": 0.2264498621225357,
      "learning_rate": 1.6005555555555556e-05,
      "loss": 0.0021,
      "step": 61190
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.5212928056716919,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0025,
      "step": 61200
    },
    {
      "epoch": 5.440888888888889,
      "grad_norm": 0.3321231007575989,
      "learning_rate": 1.5994444444444446e-05,
      "loss": 0.002,
      "step": 61210
    },
    {
      "epoch": 5.441777777777777,
      "grad_norm": 0.4863717555999756,
      "learning_rate": 1.598888888888889e-05,
      "loss": 0.0021,
      "step": 61220
    },
    {
      "epoch": 5.442666666666667,
      "grad_norm": 0.4368651509284973,
      "learning_rate": 1.5983333333333333e-05,
      "loss": 0.0025,
      "step": 61230
    },
    {
      "epoch": 5.443555555555555,
      "grad_norm": 0.0802355483174324,
      "learning_rate": 1.5977777777777777e-05,
      "loss": 0.0026,
      "step": 61240
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 0.3263151943683624,
      "learning_rate": 1.597222222222222e-05,
      "loss": 0.0025,
      "step": 61250
    },
    {
      "epoch": 5.445333333333333,
      "grad_norm": 0.05325436592102051,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0022,
      "step": 61260
    },
    {
      "epoch": 5.4462222222222225,
      "grad_norm": 0.2775047719478607,
      "learning_rate": 1.5961111111111114e-05,
      "loss": 0.0018,
      "step": 61270
    },
    {
      "epoch": 5.447111111111111,
      "grad_norm": 0.47547486424446106,
      "learning_rate": 1.5955555555555558e-05,
      "loss": 0.0025,
      "step": 61280
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.5245694518089294,
      "learning_rate": 1.595e-05,
      "loss": 0.0017,
      "step": 61290
    },
    {
      "epoch": 5.448888888888889,
      "grad_norm": 0.23043733835220337,
      "learning_rate": 1.5944444444444445e-05,
      "loss": 0.0019,
      "step": 61300
    },
    {
      "epoch": 5.449777777777777,
      "grad_norm": 0.8726315498352051,
      "learning_rate": 1.593888888888889e-05,
      "loss": 0.0015,
      "step": 61310
    },
    {
      "epoch": 5.450666666666667,
      "grad_norm": 0.0882205218076706,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0017,
      "step": 61320
    },
    {
      "epoch": 5.451555555555555,
      "grad_norm": 0.21246600151062012,
      "learning_rate": 1.592777777777778e-05,
      "loss": 0.0023,
      "step": 61330
    },
    {
      "epoch": 5.452444444444445,
      "grad_norm": 0.057803358882665634,
      "learning_rate": 1.5922222222222223e-05,
      "loss": 0.0015,
      "step": 61340
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.18186162412166595,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.0025,
      "step": 61350
    },
    {
      "epoch": 5.4542222222222225,
      "grad_norm": 0.11496371030807495,
      "learning_rate": 1.5911111111111113e-05,
      "loss": 0.0014,
      "step": 61360
    },
    {
      "epoch": 5.455111111111111,
      "grad_norm": 0.11358531564474106,
      "learning_rate": 1.5905555555555557e-05,
      "loss": 0.0018,
      "step": 61370
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.4497644901275635,
      "learning_rate": 1.59e-05,
      "loss": 0.0012,
      "step": 61380
    },
    {
      "epoch": 5.456888888888889,
      "grad_norm": 0.1085299402475357,
      "learning_rate": 1.5894444444444444e-05,
      "loss": 0.0026,
      "step": 61390
    },
    {
      "epoch": 5.457777777777777,
      "grad_norm": 0.43330660462379456,
      "learning_rate": 1.588888888888889e-05,
      "loss": 0.0017,
      "step": 61400
    },
    {
      "epoch": 5.458666666666667,
      "grad_norm": 0.19163385033607483,
      "learning_rate": 1.5883333333333334e-05,
      "loss": 0.0017,
      "step": 61410
    },
    {
      "epoch": 5.459555555555555,
      "grad_norm": 0.3987995684146881,
      "learning_rate": 1.5877777777777778e-05,
      "loss": 0.0019,
      "step": 61420
    },
    {
      "epoch": 5.460444444444445,
      "grad_norm": 0.11190712451934814,
      "learning_rate": 1.587222222222222e-05,
      "loss": 0.0016,
      "step": 61430
    },
    {
      "epoch": 5.461333333333333,
      "grad_norm": 0.030677881091833115,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0016,
      "step": 61440
    },
    {
      "epoch": 5.4622222222222225,
      "grad_norm": 0.18343357741832733,
      "learning_rate": 1.5861111111111112e-05,
      "loss": 0.0024,
      "step": 61450
    },
    {
      "epoch": 5.463111111111111,
      "grad_norm": 0.2524499297142029,
      "learning_rate": 1.5855555555555555e-05,
      "loss": 0.0031,
      "step": 61460
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.08601053804159164,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.0014,
      "step": 61470
    },
    {
      "epoch": 5.464888888888889,
      "grad_norm": 0.32817524671554565,
      "learning_rate": 1.5844444444444446e-05,
      "loss": 0.0024,
      "step": 61480
    },
    {
      "epoch": 5.465777777777777,
      "grad_norm": 0.3373763859272003,
      "learning_rate": 1.583888888888889e-05,
      "loss": 0.0029,
      "step": 61490
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.296716570854187,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.0024,
      "step": 61500
    },
    {
      "epoch": 5.467555555555555,
      "grad_norm": 0.36753523349761963,
      "learning_rate": 1.5827777777777777e-05,
      "loss": 0.0021,
      "step": 61510
    },
    {
      "epoch": 5.468444444444445,
      "grad_norm": 0.39993780851364136,
      "learning_rate": 1.582222222222222e-05,
      "loss": 0.0024,
      "step": 61520
    },
    {
      "epoch": 5.469333333333333,
      "grad_norm": 0.290785014629364,
      "learning_rate": 1.5816666666666667e-05,
      "loss": 0.0023,
      "step": 61530
    },
    {
      "epoch": 5.4702222222222225,
      "grad_norm": 0.5632781386375427,
      "learning_rate": 1.5811111111111114e-05,
      "loss": 0.0021,
      "step": 61540
    },
    {
      "epoch": 5.471111111111111,
      "grad_norm": 0.04637230932712555,
      "learning_rate": 1.5805555555555558e-05,
      "loss": 0.0028,
      "step": 61550
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.437133252620697,
      "learning_rate": 1.58e-05,
      "loss": 0.0029,
      "step": 61560
    },
    {
      "epoch": 5.472888888888889,
      "grad_norm": 0.08828587830066681,
      "learning_rate": 1.5794444444444445e-05,
      "loss": 0.0024,
      "step": 61570
    },
    {
      "epoch": 5.473777777777777,
      "grad_norm": 0.5168095827102661,
      "learning_rate": 1.5788888888888888e-05,
      "loss": 0.0013,
      "step": 61580
    },
    {
      "epoch": 5.474666666666667,
      "grad_norm": 0.45214807987213135,
      "learning_rate": 1.5783333333333332e-05,
      "loss": 0.0014,
      "step": 61590
    },
    {
      "epoch": 5.475555555555555,
      "grad_norm": 0.5995627045631409,
      "learning_rate": 1.577777777777778e-05,
      "loss": 0.0022,
      "step": 61600
    },
    {
      "epoch": 5.476444444444445,
      "grad_norm": 0.3206521272659302,
      "learning_rate": 1.5772222222222226e-05,
      "loss": 0.0029,
      "step": 61610
    },
    {
      "epoch": 5.477333333333333,
      "grad_norm": 0.1491466462612152,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0022,
      "step": 61620
    },
    {
      "epoch": 5.4782222222222225,
      "grad_norm": 0.15189415216445923,
      "learning_rate": 1.5761111111111113e-05,
      "loss": 0.0017,
      "step": 61630
    },
    {
      "epoch": 5.479111111111111,
      "grad_norm": 0.22062133252620697,
      "learning_rate": 1.5755555555555556e-05,
      "loss": 0.0018,
      "step": 61640
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.12394518405199051,
      "learning_rate": 1.575e-05,
      "loss": 0.002,
      "step": 61650
    },
    {
      "epoch": 5.480888888888889,
      "grad_norm": 0.048149749636650085,
      "learning_rate": 1.5744444444444444e-05,
      "loss": 0.002,
      "step": 61660
    },
    {
      "epoch": 5.481777777777777,
      "grad_norm": 0.29344046115875244,
      "learning_rate": 1.573888888888889e-05,
      "loss": 0.0023,
      "step": 61670
    },
    {
      "epoch": 5.482666666666667,
      "grad_norm": 0.3057185113430023,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0019,
      "step": 61680
    },
    {
      "epoch": 5.483555555555555,
      "grad_norm": 0.06675763428211212,
      "learning_rate": 1.5727777777777778e-05,
      "loss": 0.0021,
      "step": 61690
    },
    {
      "epoch": 5.484444444444445,
      "grad_norm": 0.18995851278305054,
      "learning_rate": 1.5722222222222225e-05,
      "loss": 0.0019,
      "step": 61700
    },
    {
      "epoch": 5.485333333333333,
      "grad_norm": 0.4688432216644287,
      "learning_rate": 1.5716666666666668e-05,
      "loss": 0.0023,
      "step": 61710
    },
    {
      "epoch": 5.4862222222222226,
      "grad_norm": 0.5232115983963013,
      "learning_rate": 1.571111111111111e-05,
      "loss": 0.0016,
      "step": 61720
    },
    {
      "epoch": 5.487111111111111,
      "grad_norm": 0.0841750055551529,
      "learning_rate": 1.5705555555555555e-05,
      "loss": 0.0019,
      "step": 61730
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.28423455357551575,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.0022,
      "step": 61740
    },
    {
      "epoch": 5.488888888888889,
      "grad_norm": 0.3035569489002228,
      "learning_rate": 1.5694444444444446e-05,
      "loss": 0.0023,
      "step": 61750
    },
    {
      "epoch": 5.489777777777777,
      "grad_norm": 0.2901148498058319,
      "learning_rate": 1.568888888888889e-05,
      "loss": 0.0019,
      "step": 61760
    },
    {
      "epoch": 5.490666666666667,
      "grad_norm": 0.07775705307722092,
      "learning_rate": 1.5683333333333333e-05,
      "loss": 0.0016,
      "step": 61770
    },
    {
      "epoch": 5.491555555555555,
      "grad_norm": 0.051137350499629974,
      "learning_rate": 1.5677777777777776e-05,
      "loss": 0.0015,
      "step": 61780
    },
    {
      "epoch": 5.492444444444445,
      "grad_norm": 0.3866749107837677,
      "learning_rate": 1.5672222222222223e-05,
      "loss": 0.002,
      "step": 61790
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.12865138053894043,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0017,
      "step": 61800
    },
    {
      "epoch": 5.494222222222223,
      "grad_norm": 0.7996885180473328,
      "learning_rate": 1.5661111111111114e-05,
      "loss": 0.0018,
      "step": 61810
    },
    {
      "epoch": 5.495111111111111,
      "grad_norm": 0.3486427366733551,
      "learning_rate": 1.5655555555555557e-05,
      "loss": 0.0026,
      "step": 61820
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.40625283122062683,
      "learning_rate": 1.565e-05,
      "loss": 0.0016,
      "step": 61830
    },
    {
      "epoch": 5.496888888888889,
      "grad_norm": 0.4659389853477478,
      "learning_rate": 1.5644444444444444e-05,
      "loss": 0.0024,
      "step": 61840
    },
    {
      "epoch": 5.497777777777777,
      "grad_norm": 0.7641052603721619,
      "learning_rate": 1.5638888888888888e-05,
      "loss": 0.0018,
      "step": 61850
    },
    {
      "epoch": 5.498666666666667,
      "grad_norm": 0.5036793947219849,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0018,
      "step": 61860
    },
    {
      "epoch": 5.499555555555555,
      "grad_norm": 0.9426710605621338,
      "learning_rate": 1.562777777777778e-05,
      "loss": 0.0015,
      "step": 61870
    },
    {
      "epoch": 5.500444444444445,
      "grad_norm": 0.448051393032074,
      "learning_rate": 1.5622222222222225e-05,
      "loss": 0.0016,
      "step": 61880
    },
    {
      "epoch": 5.501333333333333,
      "grad_norm": 0.11061224341392517,
      "learning_rate": 1.561666666666667e-05,
      "loss": 0.002,
      "step": 61890
    },
    {
      "epoch": 5.502222222222223,
      "grad_norm": 0.2709594964981079,
      "learning_rate": 1.5611111111111113e-05,
      "loss": 0.0017,
      "step": 61900
    },
    {
      "epoch": 5.503111111111111,
      "grad_norm": 0.29519370198249817,
      "learning_rate": 1.5605555555555556e-05,
      "loss": 0.0019,
      "step": 61910
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.43982264399528503,
      "learning_rate": 1.56e-05,
      "loss": 0.0021,
      "step": 61920
    },
    {
      "epoch": 5.504888888888889,
      "grad_norm": 0.08179454505443573,
      "learning_rate": 1.5594444444444443e-05,
      "loss": 0.0015,
      "step": 61930
    },
    {
      "epoch": 5.505777777777777,
      "grad_norm": 0.41987913846969604,
      "learning_rate": 1.558888888888889e-05,
      "loss": 0.0021,
      "step": 61940
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 0.1948544979095459,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.0019,
      "step": 61950
    },
    {
      "epoch": 5.507555555555555,
      "grad_norm": 0.11996842920780182,
      "learning_rate": 1.5577777777777777e-05,
      "loss": 0.0016,
      "step": 61960
    },
    {
      "epoch": 5.508444444444445,
      "grad_norm": 0.10354557633399963,
      "learning_rate": 1.5572222222222224e-05,
      "loss": 0.0021,
      "step": 61970
    },
    {
      "epoch": 5.509333333333333,
      "grad_norm": 0.15113465487957,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0024,
      "step": 61980
    },
    {
      "epoch": 5.510222222222223,
      "grad_norm": 0.6963642835617065,
      "learning_rate": 1.556111111111111e-05,
      "loss": 0.0018,
      "step": 61990
    },
    {
      "epoch": 5.511111111111111,
      "grad_norm": 0.054228972643613815,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 0.0022,
      "step": 62000
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.5208681225776672,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 0.0016,
      "step": 62010
    },
    {
      "epoch": 5.512888888888889,
      "grad_norm": 0.19060476124286652,
      "learning_rate": 1.5544444444444445e-05,
      "loss": 0.0022,
      "step": 62020
    },
    {
      "epoch": 5.5137777777777774,
      "grad_norm": 0.10255086421966553,
      "learning_rate": 1.553888888888889e-05,
      "loss": 0.003,
      "step": 62030
    },
    {
      "epoch": 5.514666666666667,
      "grad_norm": 0.22849464416503906,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0017,
      "step": 62040
    },
    {
      "epoch": 5.515555555555555,
      "grad_norm": 0.2873745858669281,
      "learning_rate": 1.5527777777777776e-05,
      "loss": 0.0014,
      "step": 62050
    },
    {
      "epoch": 5.516444444444445,
      "grad_norm": 0.7498847842216492,
      "learning_rate": 1.5522222222222223e-05,
      "loss": 0.0018,
      "step": 62060
    },
    {
      "epoch": 5.517333333333333,
      "grad_norm": 0.0806262418627739,
      "learning_rate": 1.5516666666666667e-05,
      "loss": 0.0026,
      "step": 62070
    },
    {
      "epoch": 5.518222222222223,
      "grad_norm": 0.4112032353878021,
      "learning_rate": 1.5511111111111114e-05,
      "loss": 0.0017,
      "step": 62080
    },
    {
      "epoch": 5.519111111111111,
      "grad_norm": 0.5920819640159607,
      "learning_rate": 1.5505555555555557e-05,
      "loss": 0.0014,
      "step": 62090
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.578411340713501,
      "learning_rate": 1.55e-05,
      "loss": 0.0025,
      "step": 62100
    },
    {
      "epoch": 5.520888888888889,
      "grad_norm": 0.2879796326160431,
      "learning_rate": 1.5494444444444444e-05,
      "loss": 0.0014,
      "step": 62110
    },
    {
      "epoch": 5.5217777777777775,
      "grad_norm": 0.6823955178260803,
      "learning_rate": 1.5488888888888888e-05,
      "loss": 0.0021,
      "step": 62120
    },
    {
      "epoch": 5.522666666666667,
      "grad_norm": 0.3014087975025177,
      "learning_rate": 1.548333333333333e-05,
      "loss": 0.0018,
      "step": 62130
    },
    {
      "epoch": 5.523555555555555,
      "grad_norm": 0.25860849022865295,
      "learning_rate": 1.5477777777777778e-05,
      "loss": 0.0019,
      "step": 62140
    },
    {
      "epoch": 5.524444444444445,
      "grad_norm": 0.159180149435997,
      "learning_rate": 1.5472222222222225e-05,
      "loss": 0.0024,
      "step": 62150
    },
    {
      "epoch": 5.525333333333333,
      "grad_norm": 0.16172082722187042,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0023,
      "step": 62160
    },
    {
      "epoch": 5.526222222222223,
      "grad_norm": 0.2261791229248047,
      "learning_rate": 1.5461111111111112e-05,
      "loss": 0.0019,
      "step": 62170
    },
    {
      "epoch": 5.527111111111111,
      "grad_norm": 0.3240954875946045,
      "learning_rate": 1.5455555555555556e-05,
      "loss": 0.002,
      "step": 62180
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.7806233167648315,
      "learning_rate": 1.545e-05,
      "loss": 0.0017,
      "step": 62190
    },
    {
      "epoch": 5.528888888888889,
      "grad_norm": 0.44758865237236023,
      "learning_rate": 1.5444444444444446e-05,
      "loss": 0.0027,
      "step": 62200
    },
    {
      "epoch": 5.5297777777777775,
      "grad_norm": 0.2994442284107208,
      "learning_rate": 1.543888888888889e-05,
      "loss": 0.0016,
      "step": 62210
    },
    {
      "epoch": 5.530666666666667,
      "grad_norm": 0.5055379271507263,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.002,
      "step": 62220
    },
    {
      "epoch": 5.531555555555555,
      "grad_norm": 0.15129372477531433,
      "learning_rate": 1.542777777777778e-05,
      "loss": 0.0022,
      "step": 62230
    },
    {
      "epoch": 5.532444444444445,
      "grad_norm": 0.576433539390564,
      "learning_rate": 1.5422222222222224e-05,
      "loss": 0.0018,
      "step": 62240
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.36997175216674805,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.0015,
      "step": 62250
    },
    {
      "epoch": 5.534222222222223,
      "grad_norm": 0.3283352553844452,
      "learning_rate": 1.541111111111111e-05,
      "loss": 0.0019,
      "step": 62260
    },
    {
      "epoch": 5.535111111111111,
      "grad_norm": 0.253639817237854,
      "learning_rate": 1.5405555555555558e-05,
      "loss": 0.0022,
      "step": 62270
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.12921461462974548,
      "learning_rate": 1.54e-05,
      "loss": 0.0024,
      "step": 62280
    },
    {
      "epoch": 5.536888888888889,
      "grad_norm": 0.6791915893554688,
      "learning_rate": 1.5394444444444445e-05,
      "loss": 0.0018,
      "step": 62290
    },
    {
      "epoch": 5.5377777777777775,
      "grad_norm": 0.25877588987350464,
      "learning_rate": 1.538888888888889e-05,
      "loss": 0.0018,
      "step": 62300
    },
    {
      "epoch": 5.538666666666667,
      "grad_norm": 0.5958826541900635,
      "learning_rate": 1.5383333333333332e-05,
      "loss": 0.0019,
      "step": 62310
    },
    {
      "epoch": 5.539555555555555,
      "grad_norm": 0.8200523853302002,
      "learning_rate": 1.537777777777778e-05,
      "loss": 0.0022,
      "step": 62320
    },
    {
      "epoch": 5.540444444444445,
      "grad_norm": 0.4689105749130249,
      "learning_rate": 1.5372222222222223e-05,
      "loss": 0.0015,
      "step": 62330
    },
    {
      "epoch": 5.541333333333333,
      "grad_norm": 0.07648808509111404,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0019,
      "step": 62340
    },
    {
      "epoch": 5.542222222222223,
      "grad_norm": 0.39268186688423157,
      "learning_rate": 1.5361111111111113e-05,
      "loss": 0.0019,
      "step": 62350
    },
    {
      "epoch": 5.543111111111111,
      "grad_norm": 0.38884061574935913,
      "learning_rate": 1.5355555555555557e-05,
      "loss": 0.0015,
      "step": 62360
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.2985072135925293,
      "learning_rate": 1.535e-05,
      "loss": 0.0018,
      "step": 62370
    },
    {
      "epoch": 5.544888888888889,
      "grad_norm": 0.3383352756500244,
      "learning_rate": 1.5344444444444444e-05,
      "loss": 0.0017,
      "step": 62380
    },
    {
      "epoch": 5.5457777777777775,
      "grad_norm": 0.5178643465042114,
      "learning_rate": 1.5338888888888888e-05,
      "loss": 0.0016,
      "step": 62390
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.6215932965278625,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0022,
      "step": 62400
    },
    {
      "epoch": 5.547555555555555,
      "grad_norm": 0.027376551181077957,
      "learning_rate": 1.532777777777778e-05,
      "loss": 0.002,
      "step": 62410
    },
    {
      "epoch": 5.548444444444445,
      "grad_norm": 1.0280976295471191,
      "learning_rate": 1.5322222222222225e-05,
      "loss": 0.0021,
      "step": 62420
    },
    {
      "epoch": 5.549333333333333,
      "grad_norm": 0.49934473633766174,
      "learning_rate": 1.531666666666667e-05,
      "loss": 0.003,
      "step": 62430
    },
    {
      "epoch": 5.550222222222223,
      "grad_norm": 0.3536461293697357,
      "learning_rate": 1.5311111111111112e-05,
      "loss": 0.0024,
      "step": 62440
    },
    {
      "epoch": 5.551111111111111,
      "grad_norm": 0.054133545607328415,
      "learning_rate": 1.5305555555555556e-05,
      "loss": 0.0017,
      "step": 62450
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.5565789937973022,
      "learning_rate": 1.53e-05,
      "loss": 0.003,
      "step": 62460
    },
    {
      "epoch": 5.552888888888889,
      "grad_norm": 0.29246586561203003,
      "learning_rate": 1.5294444444444446e-05,
      "loss": 0.0026,
      "step": 62470
    },
    {
      "epoch": 5.5537777777777775,
      "grad_norm": 0.049946244806051254,
      "learning_rate": 1.528888888888889e-05,
      "loss": 0.0018,
      "step": 62480
    },
    {
      "epoch": 5.554666666666667,
      "grad_norm": 0.8568009734153748,
      "learning_rate": 1.5283333333333333e-05,
      "loss": 0.0013,
      "step": 62490
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 0.7204519510269165,
      "learning_rate": 1.527777777777778e-05,
      "loss": 0.0015,
      "step": 62500
    },
    {
      "epoch": 5.556444444444445,
      "grad_norm": 0.7830493450164795,
      "learning_rate": 1.5272222222222224e-05,
      "loss": 0.002,
      "step": 62510
    },
    {
      "epoch": 5.557333333333333,
      "grad_norm": 0.07956534624099731,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0019,
      "step": 62520
    },
    {
      "epoch": 5.558222222222222,
      "grad_norm": 0.22148099541664124,
      "learning_rate": 1.526111111111111e-05,
      "loss": 0.0014,
      "step": 62530
    },
    {
      "epoch": 5.559111111111111,
      "grad_norm": 0.48682382702827454,
      "learning_rate": 1.5255555555555556e-05,
      "loss": 0.0027,
      "step": 62540
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.2996315360069275,
      "learning_rate": 1.525e-05,
      "loss": 0.0025,
      "step": 62550
    },
    {
      "epoch": 5.560888888888889,
      "grad_norm": 0.7045242190361023,
      "learning_rate": 1.5244444444444445e-05,
      "loss": 0.0016,
      "step": 62560
    },
    {
      "epoch": 5.5617777777777775,
      "grad_norm": 0.1430744230747223,
      "learning_rate": 1.5238888888888888e-05,
      "loss": 0.0015,
      "step": 62570
    },
    {
      "epoch": 5.562666666666667,
      "grad_norm": 0.9286412596702576,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0024,
      "step": 62580
    },
    {
      "epoch": 5.563555555555555,
      "grad_norm": 0.4791610836982727,
      "learning_rate": 1.5227777777777779e-05,
      "loss": 0.0019,
      "step": 62590
    },
    {
      "epoch": 5.564444444444445,
      "grad_norm": 0.15249283611774445,
      "learning_rate": 1.5222222222222224e-05,
      "loss": 0.0024,
      "step": 62600
    },
    {
      "epoch": 5.565333333333333,
      "grad_norm": 0.46952664852142334,
      "learning_rate": 1.5216666666666668e-05,
      "loss": 0.0012,
      "step": 62610
    },
    {
      "epoch": 5.566222222222223,
      "grad_norm": 0.04887458682060242,
      "learning_rate": 1.5211111111111111e-05,
      "loss": 0.0019,
      "step": 62620
    },
    {
      "epoch": 5.567111111111111,
      "grad_norm": 0.31554940342903137,
      "learning_rate": 1.5205555555555557e-05,
      "loss": 0.0018,
      "step": 62630
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.2874399721622467,
      "learning_rate": 1.52e-05,
      "loss": 0.0019,
      "step": 62640
    },
    {
      "epoch": 5.568888888888889,
      "grad_norm": 0.11865542083978653,
      "learning_rate": 1.5194444444444444e-05,
      "loss": 0.0021,
      "step": 62650
    },
    {
      "epoch": 5.5697777777777775,
      "grad_norm": 0.2931109368801117,
      "learning_rate": 1.5188888888888889e-05,
      "loss": 0.0024,
      "step": 62660
    },
    {
      "epoch": 5.570666666666667,
      "grad_norm": 0.3875563144683838,
      "learning_rate": 1.5183333333333333e-05,
      "loss": 0.002,
      "step": 62670
    },
    {
      "epoch": 5.571555555555555,
      "grad_norm": 0.08351472020149231,
      "learning_rate": 1.517777777777778e-05,
      "loss": 0.0017,
      "step": 62680
    },
    {
      "epoch": 5.572444444444445,
      "grad_norm": 0.36736947298049927,
      "learning_rate": 1.5172222222222223e-05,
      "loss": 0.0031,
      "step": 62690
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 0.3017520010471344,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0023,
      "step": 62700
    },
    {
      "epoch": 5.574222222222222,
      "grad_norm": 0.0642106682062149,
      "learning_rate": 1.5161111111111112e-05,
      "loss": 0.0035,
      "step": 62710
    },
    {
      "epoch": 5.575111111111111,
      "grad_norm": 0.2698964476585388,
      "learning_rate": 1.5155555555555555e-05,
      "loss": 0.0017,
      "step": 62720
    },
    {
      "epoch": 5.576,
      "grad_norm": 0.1729724407196045,
      "learning_rate": 1.515e-05,
      "loss": 0.0019,
      "step": 62730
    },
    {
      "epoch": 5.576888888888889,
      "grad_norm": 0.7639781832695007,
      "learning_rate": 1.5144444444444444e-05,
      "loss": 0.0022,
      "step": 62740
    },
    {
      "epoch": 5.5777777777777775,
      "grad_norm": 0.49858200550079346,
      "learning_rate": 1.5138888888888888e-05,
      "loss": 0.0018,
      "step": 62750
    },
    {
      "epoch": 5.578666666666667,
      "grad_norm": 0.34206998348236084,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0031,
      "step": 62760
    },
    {
      "epoch": 5.579555555555555,
      "grad_norm": 0.3337354362010956,
      "learning_rate": 1.512777777777778e-05,
      "loss": 0.0016,
      "step": 62770
    },
    {
      "epoch": 5.580444444444445,
      "grad_norm": 0.47972917556762695,
      "learning_rate": 1.5122222222222224e-05,
      "loss": 0.0021,
      "step": 62780
    },
    {
      "epoch": 5.581333333333333,
      "grad_norm": 0.34509754180908203,
      "learning_rate": 1.5116666666666667e-05,
      "loss": 0.0018,
      "step": 62790
    },
    {
      "epoch": 5.582222222222223,
      "grad_norm": 0.12851136922836304,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 0.0014,
      "step": 62800
    },
    {
      "epoch": 5.583111111111111,
      "grad_norm": 0.24792827665805817,
      "learning_rate": 1.5105555555555556e-05,
      "loss": 0.0019,
      "step": 62810
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.061162762343883514,
      "learning_rate": 1.51e-05,
      "loss": 0.002,
      "step": 62820
    },
    {
      "epoch": 5.584888888888889,
      "grad_norm": 0.5460609793663025,
      "learning_rate": 1.5094444444444445e-05,
      "loss": 0.0016,
      "step": 62830
    },
    {
      "epoch": 5.5857777777777775,
      "grad_norm": 0.6446511149406433,
      "learning_rate": 1.5088888888888888e-05,
      "loss": 0.002,
      "step": 62840
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 0.1218896359205246,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.0016,
      "step": 62850
    },
    {
      "epoch": 5.587555555555555,
      "grad_norm": 0.26901841163635254,
      "learning_rate": 1.5077777777777779e-05,
      "loss": 0.0022,
      "step": 62860
    },
    {
      "epoch": 5.588444444444445,
      "grad_norm": 0.2567288279533386,
      "learning_rate": 1.5072222222222224e-05,
      "loss": 0.002,
      "step": 62870
    },
    {
      "epoch": 5.589333333333333,
      "grad_norm": 0.5560571551322937,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0026,
      "step": 62880
    },
    {
      "epoch": 5.590222222222222,
      "grad_norm": 0.30328473448753357,
      "learning_rate": 1.5061111111111113e-05,
      "loss": 0.0028,
      "step": 62890
    },
    {
      "epoch": 5.591111111111111,
      "grad_norm": 0.3620411157608032,
      "learning_rate": 1.5055555555555556e-05,
      "loss": 0.0019,
      "step": 62900
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.5301085710525513,
      "learning_rate": 1.505e-05,
      "loss": 0.0018,
      "step": 62910
    },
    {
      "epoch": 5.592888888888889,
      "grad_norm": 0.25796833634376526,
      "learning_rate": 1.5044444444444445e-05,
      "loss": 0.0015,
      "step": 62920
    },
    {
      "epoch": 5.5937777777777775,
      "grad_norm": 0.09443726390600204,
      "learning_rate": 1.5038888888888889e-05,
      "loss": 0.0014,
      "step": 62930
    },
    {
      "epoch": 5.594666666666667,
      "grad_norm": 0.2988409399986267,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0027,
      "step": 62940
    },
    {
      "epoch": 5.595555555555555,
      "grad_norm": 0.32499390840530396,
      "learning_rate": 1.502777777777778e-05,
      "loss": 0.0011,
      "step": 62950
    },
    {
      "epoch": 5.596444444444445,
      "grad_norm": 0.340711385011673,
      "learning_rate": 1.5022222222222224e-05,
      "loss": 0.0012,
      "step": 62960
    },
    {
      "epoch": 5.597333333333333,
      "grad_norm": 0.22605854272842407,
      "learning_rate": 1.5016666666666668e-05,
      "loss": 0.0015,
      "step": 62970
    },
    {
      "epoch": 5.598222222222223,
      "grad_norm": 0.2972045838832855,
      "learning_rate": 1.5011111111111112e-05,
      "loss": 0.0017,
      "step": 62980
    },
    {
      "epoch": 5.599111111111111,
      "grad_norm": 0.371838241815567,
      "learning_rate": 1.5005555555555557e-05,
      "loss": 0.0022,
      "step": 62990
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.494454026222229,
      "learning_rate": 1.5e-05,
      "loss": 0.0021,
      "step": 63000
    },
    {
      "epoch": 5.600888888888889,
      "grad_norm": 0.15179942548274994,
      "learning_rate": 1.4994444444444444e-05,
      "loss": 0.0024,
      "step": 63010
    },
    {
      "epoch": 5.6017777777777775,
      "grad_norm": 0.29962754249572754,
      "learning_rate": 1.498888888888889e-05,
      "loss": 0.0014,
      "step": 63020
    },
    {
      "epoch": 5.602666666666667,
      "grad_norm": 0.31755387783050537,
      "learning_rate": 1.4983333333333336e-05,
      "loss": 0.002,
      "step": 63030
    },
    {
      "epoch": 5.603555555555555,
      "grad_norm": 0.2159220576286316,
      "learning_rate": 1.497777777777778e-05,
      "loss": 0.002,
      "step": 63040
    },
    {
      "epoch": 5.604444444444445,
      "grad_norm": 0.2001916766166687,
      "learning_rate": 1.4972222222222223e-05,
      "loss": 0.0015,
      "step": 63050
    },
    {
      "epoch": 5.605333333333333,
      "grad_norm": 0.09232120215892792,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0022,
      "step": 63060
    },
    {
      "epoch": 5.606222222222222,
      "grad_norm": 0.10854949057102203,
      "learning_rate": 1.4961111111111112e-05,
      "loss": 0.0022,
      "step": 63070
    },
    {
      "epoch": 5.607111111111111,
      "grad_norm": 0.41182756423950195,
      "learning_rate": 1.4955555555555556e-05,
      "loss": 0.0019,
      "step": 63080
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.27434080839157104,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 0.0024,
      "step": 63090
    },
    {
      "epoch": 5.608888888888889,
      "grad_norm": 0.441302090883255,
      "learning_rate": 1.4944444444444444e-05,
      "loss": 0.0018,
      "step": 63100
    },
    {
      "epoch": 5.6097777777777775,
      "grad_norm": 0.2451767474412918,
      "learning_rate": 1.4938888888888888e-05,
      "loss": 0.0018,
      "step": 63110
    },
    {
      "epoch": 5.610666666666667,
      "grad_norm": 0.7866429686546326,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.002,
      "step": 63120
    },
    {
      "epoch": 5.611555555555555,
      "grad_norm": 0.2932840883731842,
      "learning_rate": 1.492777777777778e-05,
      "loss": 0.0022,
      "step": 63130
    },
    {
      "epoch": 5.612444444444445,
      "grad_norm": 0.1657991111278534,
      "learning_rate": 1.4922222222222224e-05,
      "loss": 0.0015,
      "step": 63140
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 0.10714008659124374,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.0015,
      "step": 63150
    },
    {
      "epoch": 5.614222222222223,
      "grad_norm": 0.4305398762226105,
      "learning_rate": 1.4911111111111113e-05,
      "loss": 0.0018,
      "step": 63160
    },
    {
      "epoch": 5.615111111111111,
      "grad_norm": 0.1104138046503067,
      "learning_rate": 1.4905555555555556e-05,
      "loss": 0.0019,
      "step": 63170
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.4294973313808441,
      "learning_rate": 1.49e-05,
      "loss": 0.0026,
      "step": 63180
    },
    {
      "epoch": 5.616888888888889,
      "grad_norm": 0.6746378540992737,
      "learning_rate": 1.4894444444444445e-05,
      "loss": 0.0017,
      "step": 63190
    },
    {
      "epoch": 5.6177777777777775,
      "grad_norm": 0.4257912337779999,
      "learning_rate": 1.4888888888888888e-05,
      "loss": 0.0018,
      "step": 63200
    },
    {
      "epoch": 5.618666666666667,
      "grad_norm": 0.32628440856933594,
      "learning_rate": 1.4883333333333335e-05,
      "loss": 0.0023,
      "step": 63210
    },
    {
      "epoch": 5.619555555555555,
      "grad_norm": 0.15759451687335968,
      "learning_rate": 1.4877777777777779e-05,
      "loss": 0.0015,
      "step": 63220
    },
    {
      "epoch": 5.620444444444445,
      "grad_norm": 0.2348122000694275,
      "learning_rate": 1.4872222222222224e-05,
      "loss": 0.0013,
      "step": 63230
    },
    {
      "epoch": 5.621333333333333,
      "grad_norm": 0.2103443592786789,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0028,
      "step": 63240
    },
    {
      "epoch": 5.622222222222222,
      "grad_norm": 0.37052106857299805,
      "learning_rate": 1.4861111111111111e-05,
      "loss": 0.0017,
      "step": 63250
    },
    {
      "epoch": 5.623111111111111,
      "grad_norm": 0.2925489842891693,
      "learning_rate": 1.4855555555555557e-05,
      "loss": 0.002,
      "step": 63260
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.6101825833320618,
      "learning_rate": 1.485e-05,
      "loss": 0.0021,
      "step": 63270
    },
    {
      "epoch": 5.624888888888889,
      "grad_norm": 0.2513635754585266,
      "learning_rate": 1.4844444444444444e-05,
      "loss": 0.0018,
      "step": 63280
    },
    {
      "epoch": 5.6257777777777775,
      "grad_norm": 0.46988436579704285,
      "learning_rate": 1.4838888888888889e-05,
      "loss": 0.0027,
      "step": 63290
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 0.46715137362480164,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0017,
      "step": 63300
    },
    {
      "epoch": 5.627555555555555,
      "grad_norm": 0.632605791091919,
      "learning_rate": 1.482777777777778e-05,
      "loss": 0.0018,
      "step": 63310
    },
    {
      "epoch": 5.628444444444445,
      "grad_norm": 0.05828706547617912,
      "learning_rate": 1.4822222222222223e-05,
      "loss": 0.0027,
      "step": 63320
    },
    {
      "epoch": 5.629333333333333,
      "grad_norm": 0.3198649287223816,
      "learning_rate": 1.4816666666666668e-05,
      "loss": 0.0013,
      "step": 63330
    },
    {
      "epoch": 5.630222222222223,
      "grad_norm": 0.5281069874763489,
      "learning_rate": 1.4811111111111112e-05,
      "loss": 0.0019,
      "step": 63340
    },
    {
      "epoch": 5.631111111111111,
      "grad_norm": 0.22178800404071808,
      "learning_rate": 1.4805555555555555e-05,
      "loss": 0.0024,
      "step": 63350
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.29945138096809387,
      "learning_rate": 1.48e-05,
      "loss": 0.0024,
      "step": 63360
    },
    {
      "epoch": 5.632888888888889,
      "grad_norm": 0.13081678748130798,
      "learning_rate": 1.4794444444444444e-05,
      "loss": 0.0013,
      "step": 63370
    },
    {
      "epoch": 5.6337777777777776,
      "grad_norm": 0.22331488132476807,
      "learning_rate": 1.4788888888888888e-05,
      "loss": 0.0017,
      "step": 63380
    },
    {
      "epoch": 5.634666666666667,
      "grad_norm": 0.18166020512580872,
      "learning_rate": 1.4783333333333335e-05,
      "loss": 0.002,
      "step": 63390
    },
    {
      "epoch": 5.635555555555555,
      "grad_norm": 0.06137002632021904,
      "learning_rate": 1.477777777777778e-05,
      "loss": 0.0015,
      "step": 63400
    },
    {
      "epoch": 5.636444444444445,
      "grad_norm": 0.8190882802009583,
      "learning_rate": 1.4772222222222223e-05,
      "loss": 0.0019,
      "step": 63410
    },
    {
      "epoch": 5.637333333333333,
      "grad_norm": 0.6357715725898743,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.0019,
      "step": 63420
    },
    {
      "epoch": 5.638222222222222,
      "grad_norm": 0.26836079359054565,
      "learning_rate": 1.4761111111111112e-05,
      "loss": 0.0015,
      "step": 63430
    },
    {
      "epoch": 5.639111111111111,
      "grad_norm": 0.2646524906158447,
      "learning_rate": 1.4755555555555556e-05,
      "loss": 0.0017,
      "step": 63440
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.40572798252105713,
      "learning_rate": 1.475e-05,
      "loss": 0.0017,
      "step": 63450
    },
    {
      "epoch": 5.640888888888889,
      "grad_norm": 0.0839548259973526,
      "learning_rate": 1.4744444444444445e-05,
      "loss": 0.0018,
      "step": 63460
    },
    {
      "epoch": 5.641777777777778,
      "grad_norm": 0.44446393847465515,
      "learning_rate": 1.4738888888888892e-05,
      "loss": 0.0023,
      "step": 63470
    },
    {
      "epoch": 5.642666666666667,
      "grad_norm": 0.15487982332706451,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0017,
      "step": 63480
    },
    {
      "epoch": 5.643555555555555,
      "grad_norm": 0.4104209840297699,
      "learning_rate": 1.4727777777777779e-05,
      "loss": 0.0018,
      "step": 63490
    },
    {
      "epoch": 5.644444444444445,
      "grad_norm": 0.5525225400924683,
      "learning_rate": 1.4722222222222224e-05,
      "loss": 0.0017,
      "step": 63500
    },
    {
      "epoch": 5.645333333333333,
      "grad_norm": 0.8638779520988464,
      "learning_rate": 1.4716666666666668e-05,
      "loss": 0.0019,
      "step": 63510
    },
    {
      "epoch": 5.646222222222223,
      "grad_norm": 0.11495553702116013,
      "learning_rate": 1.4711111111111111e-05,
      "loss": 0.0023,
      "step": 63520
    },
    {
      "epoch": 5.647111111111111,
      "grad_norm": 0.04744311794638634,
      "learning_rate": 1.4705555555555556e-05,
      "loss": 0.0014,
      "step": 63530
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.2609027028083801,
      "learning_rate": 1.47e-05,
      "loss": 0.0017,
      "step": 63540
    },
    {
      "epoch": 5.648888888888889,
      "grad_norm": 0.11946515738964081,
      "learning_rate": 1.4694444444444443e-05,
      "loss": 0.0023,
      "step": 63550
    },
    {
      "epoch": 5.649777777777778,
      "grad_norm": 0.5884084701538086,
      "learning_rate": 1.468888888888889e-05,
      "loss": 0.0017,
      "step": 63560
    },
    {
      "epoch": 5.650666666666667,
      "grad_norm": 0.14928452670574188,
      "learning_rate": 1.4683333333333336e-05,
      "loss": 0.0017,
      "step": 63570
    },
    {
      "epoch": 5.651555555555555,
      "grad_norm": 0.04890245199203491,
      "learning_rate": 1.467777777777778e-05,
      "loss": 0.0017,
      "step": 63580
    },
    {
      "epoch": 5.652444444444445,
      "grad_norm": 0.07029830664396286,
      "learning_rate": 1.4672222222222223e-05,
      "loss": 0.0015,
      "step": 63590
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.5537644624710083,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0021,
      "step": 63600
    },
    {
      "epoch": 5.654222222222222,
      "grad_norm": 0.4806267321109772,
      "learning_rate": 1.4661111111111112e-05,
      "loss": 0.0015,
      "step": 63610
    },
    {
      "epoch": 5.655111111111111,
      "grad_norm": 0.524773120880127,
      "learning_rate": 1.4655555555555555e-05,
      "loss": 0.0021,
      "step": 63620
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.1517973691225052,
      "learning_rate": 1.465e-05,
      "loss": 0.0016,
      "step": 63630
    },
    {
      "epoch": 5.656888888888889,
      "grad_norm": 0.6721698641777039,
      "learning_rate": 1.4644444444444444e-05,
      "loss": 0.0021,
      "step": 63640
    },
    {
      "epoch": 5.657777777777778,
      "grad_norm": 0.15641739964485168,
      "learning_rate": 1.463888888888889e-05,
      "loss": 0.0021,
      "step": 63650
    },
    {
      "epoch": 5.658666666666667,
      "grad_norm": 0.08634352684020996,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0018,
      "step": 63660
    },
    {
      "epoch": 5.6595555555555555,
      "grad_norm": 0.26854103803634644,
      "learning_rate": 1.462777777777778e-05,
      "loss": 0.0029,
      "step": 63670
    },
    {
      "epoch": 5.660444444444444,
      "grad_norm": 0.44079887866973877,
      "learning_rate": 1.4622222222222223e-05,
      "loss": 0.0017,
      "step": 63680
    },
    {
      "epoch": 5.661333333333333,
      "grad_norm": 0.3033871352672577,
      "learning_rate": 1.4616666666666667e-05,
      "loss": 0.0019,
      "step": 63690
    },
    {
      "epoch": 5.662222222222223,
      "grad_norm": 0.06137841194868088,
      "learning_rate": 1.4611111111111112e-05,
      "loss": 0.0018,
      "step": 63700
    },
    {
      "epoch": 5.663111111111111,
      "grad_norm": 1.0379178524017334,
      "learning_rate": 1.4605555555555556e-05,
      "loss": 0.0014,
      "step": 63710
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.512883722782135,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0014,
      "step": 63720
    },
    {
      "epoch": 5.664888888888889,
      "grad_norm": 0.08293599635362625,
      "learning_rate": 1.4594444444444444e-05,
      "loss": 0.002,
      "step": 63730
    },
    {
      "epoch": 5.665777777777778,
      "grad_norm": 0.2201889157295227,
      "learning_rate": 1.4588888888888891e-05,
      "loss": 0.0017,
      "step": 63740
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.29593557119369507,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.0015,
      "step": 63750
    },
    {
      "epoch": 5.6675555555555555,
      "grad_norm": 0.052531447261571884,
      "learning_rate": 1.4577777777777778e-05,
      "loss": 0.0019,
      "step": 63760
    },
    {
      "epoch": 5.668444444444445,
      "grad_norm": 0.2919788956642151,
      "learning_rate": 1.4572222222222224e-05,
      "loss": 0.0019,
      "step": 63770
    },
    {
      "epoch": 5.669333333333333,
      "grad_norm": 0.26210010051727295,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0019,
      "step": 63780
    },
    {
      "epoch": 5.670222222222222,
      "grad_norm": 0.5819613933563232,
      "learning_rate": 1.456111111111111e-05,
      "loss": 0.0017,
      "step": 63790
    },
    {
      "epoch": 5.671111111111111,
      "grad_norm": 0.2097064107656479,
      "learning_rate": 1.4555555555555556e-05,
      "loss": 0.002,
      "step": 63800
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.3230762183666229,
      "learning_rate": 1.455e-05,
      "loss": 0.0018,
      "step": 63810
    },
    {
      "epoch": 5.672888888888889,
      "grad_norm": 0.04461527615785599,
      "learning_rate": 1.4544444444444443e-05,
      "loss": 0.0014,
      "step": 63820
    },
    {
      "epoch": 5.673777777777778,
      "grad_norm": 0.07833918184041977,
      "learning_rate": 1.453888888888889e-05,
      "loss": 0.0023,
      "step": 63830
    },
    {
      "epoch": 5.674666666666667,
      "grad_norm": 0.11699949949979782,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0023,
      "step": 63840
    },
    {
      "epoch": 5.6755555555555555,
      "grad_norm": 0.051721565425395966,
      "learning_rate": 1.4527777777777779e-05,
      "loss": 0.0013,
      "step": 63850
    },
    {
      "epoch": 5.676444444444444,
      "grad_norm": 0.4053504467010498,
      "learning_rate": 1.4522222222222222e-05,
      "loss": 0.0031,
      "step": 63860
    },
    {
      "epoch": 5.677333333333333,
      "grad_norm": 0.19864973425865173,
      "learning_rate": 1.4516666666666668e-05,
      "loss": 0.0013,
      "step": 63870
    },
    {
      "epoch": 5.678222222222222,
      "grad_norm": 0.26047125458717346,
      "learning_rate": 1.4511111111111111e-05,
      "loss": 0.0019,
      "step": 63880
    },
    {
      "epoch": 5.679111111111111,
      "grad_norm": 0.29137134552001953,
      "learning_rate": 1.4505555555555555e-05,
      "loss": 0.0014,
      "step": 63890
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.15916819870471954,
      "learning_rate": 1.45e-05,
      "loss": 0.0018,
      "step": 63900
    },
    {
      "epoch": 5.680888888888889,
      "grad_norm": 1.0915840864181519,
      "learning_rate": 1.4494444444444444e-05,
      "loss": 0.0022,
      "step": 63910
    },
    {
      "epoch": 5.681777777777778,
      "grad_norm": 0.20395436882972717,
      "learning_rate": 1.448888888888889e-05,
      "loss": 0.0016,
      "step": 63920
    },
    {
      "epoch": 5.682666666666667,
      "grad_norm": 0.5005035400390625,
      "learning_rate": 1.4483333333333334e-05,
      "loss": 0.0028,
      "step": 63930
    },
    {
      "epoch": 5.6835555555555555,
      "grad_norm": 0.4929652810096741,
      "learning_rate": 1.447777777777778e-05,
      "loss": 0.0018,
      "step": 63940
    },
    {
      "epoch": 5.684444444444445,
      "grad_norm": 0.7313690185546875,
      "learning_rate": 1.4472222222222223e-05,
      "loss": 0.0017,
      "step": 63950
    },
    {
      "epoch": 5.685333333333333,
      "grad_norm": 0.1027766540646553,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0023,
      "step": 63960
    },
    {
      "epoch": 5.686222222222222,
      "grad_norm": 0.5191370844841003,
      "learning_rate": 1.4461111111111112e-05,
      "loss": 0.0031,
      "step": 63970
    },
    {
      "epoch": 5.687111111111111,
      "grad_norm": 0.8629887104034424,
      "learning_rate": 1.4455555555555555e-05,
      "loss": 0.0029,
      "step": 63980
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.2890966832637787,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 0.002,
      "step": 63990
    },
    {
      "epoch": 5.688888888888889,
      "grad_norm": 0.3613179922103882,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 0.0024,
      "step": 64000
    },
    {
      "epoch": 5.689777777777778,
      "grad_norm": 0.2950369119644165,
      "learning_rate": 1.4438888888888891e-05,
      "loss": 0.0019,
      "step": 64010
    },
    {
      "epoch": 5.690666666666667,
      "grad_norm": 0.29441729187965393,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0023,
      "step": 64020
    },
    {
      "epoch": 5.6915555555555555,
      "grad_norm": 0.17458102107048035,
      "learning_rate": 1.4427777777777778e-05,
      "loss": 0.0024,
      "step": 64030
    },
    {
      "epoch": 5.692444444444444,
      "grad_norm": 0.18053556978702545,
      "learning_rate": 1.4422222222222223e-05,
      "loss": 0.0017,
      "step": 64040
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 0.22221118211746216,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.0022,
      "step": 64050
    },
    {
      "epoch": 5.694222222222222,
      "grad_norm": 0.14612098038196564,
      "learning_rate": 1.441111111111111e-05,
      "loss": 0.0018,
      "step": 64060
    },
    {
      "epoch": 5.695111111111111,
      "grad_norm": 0.31477484107017517,
      "learning_rate": 1.4405555555555556e-05,
      "loss": 0.0015,
      "step": 64070
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.16111940145492554,
      "learning_rate": 1.44e-05,
      "loss": 0.0016,
      "step": 64080
    },
    {
      "epoch": 5.696888888888889,
      "grad_norm": 0.5907279849052429,
      "learning_rate": 1.4394444444444446e-05,
      "loss": 0.0019,
      "step": 64090
    },
    {
      "epoch": 5.697777777777778,
      "grad_norm": 0.6081017851829529,
      "learning_rate": 1.438888888888889e-05,
      "loss": 0.0028,
      "step": 64100
    },
    {
      "epoch": 5.698666666666667,
      "grad_norm": 0.22525088489055634,
      "learning_rate": 1.4383333333333335e-05,
      "loss": 0.0018,
      "step": 64110
    },
    {
      "epoch": 5.6995555555555555,
      "grad_norm": 0.034126073122024536,
      "learning_rate": 1.4377777777777779e-05,
      "loss": 0.0012,
      "step": 64120
    },
    {
      "epoch": 5.700444444444445,
      "grad_norm": 0.2502385675907135,
      "learning_rate": 1.4372222222222222e-05,
      "loss": 0.0022,
      "step": 64130
    },
    {
      "epoch": 5.701333333333333,
      "grad_norm": 0.1899207979440689,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0013,
      "step": 64140
    },
    {
      "epoch": 5.702222222222222,
      "grad_norm": 0.18713437020778656,
      "learning_rate": 1.4361111111111111e-05,
      "loss": 0.0015,
      "step": 64150
    },
    {
      "epoch": 5.703111111111111,
      "grad_norm": 0.07953222095966339,
      "learning_rate": 1.4355555555555556e-05,
      "loss": 0.002,
      "step": 64160
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.3818134665489197,
      "learning_rate": 1.435e-05,
      "loss": 0.0013,
      "step": 64170
    },
    {
      "epoch": 5.704888888888889,
      "grad_norm": 0.14897827804088593,
      "learning_rate": 1.4344444444444447e-05,
      "loss": 0.0017,
      "step": 64180
    },
    {
      "epoch": 5.705777777777778,
      "grad_norm": 0.7713796496391296,
      "learning_rate": 1.433888888888889e-05,
      "loss": 0.0028,
      "step": 64190
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.2553425133228302,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0025,
      "step": 64200
    },
    {
      "epoch": 5.7075555555555555,
      "grad_norm": 0.195989727973938,
      "learning_rate": 1.4327777777777779e-05,
      "loss": 0.0017,
      "step": 64210
    },
    {
      "epoch": 5.708444444444444,
      "grad_norm": 0.3795205354690552,
      "learning_rate": 1.4322222222222223e-05,
      "loss": 0.0022,
      "step": 64220
    },
    {
      "epoch": 5.709333333333333,
      "grad_norm": 0.4737870991230011,
      "learning_rate": 1.4316666666666668e-05,
      "loss": 0.0025,
      "step": 64230
    },
    {
      "epoch": 5.710222222222222,
      "grad_norm": 0.05066956579685211,
      "learning_rate": 1.4311111111111111e-05,
      "loss": 0.0022,
      "step": 64240
    },
    {
      "epoch": 5.711111111111111,
      "grad_norm": 0.05518575385212898,
      "learning_rate": 1.4305555555555555e-05,
      "loss": 0.0022,
      "step": 64250
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.5156518816947937,
      "learning_rate": 1.43e-05,
      "loss": 0.0013,
      "step": 64260
    },
    {
      "epoch": 5.712888888888889,
      "grad_norm": 0.5892210602760315,
      "learning_rate": 1.4294444444444447e-05,
      "loss": 0.0019,
      "step": 64270
    },
    {
      "epoch": 5.713777777777778,
      "grad_norm": 0.1885731965303421,
      "learning_rate": 1.428888888888889e-05,
      "loss": 0.0019,
      "step": 64280
    },
    {
      "epoch": 5.714666666666667,
      "grad_norm": 0.4410075545310974,
      "learning_rate": 1.4283333333333334e-05,
      "loss": 0.0018,
      "step": 64290
    },
    {
      "epoch": 5.7155555555555555,
      "grad_norm": 0.1112389788031578,
      "learning_rate": 1.427777777777778e-05,
      "loss": 0.0018,
      "step": 64300
    },
    {
      "epoch": 5.716444444444445,
      "grad_norm": 0.04931855946779251,
      "learning_rate": 1.4272222222222223e-05,
      "loss": 0.0022,
      "step": 64310
    },
    {
      "epoch": 5.717333333333333,
      "grad_norm": 0.23575003445148468,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0036,
      "step": 64320
    },
    {
      "epoch": 5.718222222222222,
      "grad_norm": 0.2393890917301178,
      "learning_rate": 1.4261111111111112e-05,
      "loss": 0.0015,
      "step": 64330
    },
    {
      "epoch": 5.719111111111111,
      "grad_norm": 0.6000799536705017,
      "learning_rate": 1.4255555555555556e-05,
      "loss": 0.002,
      "step": 64340
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.0542944073677063,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.0014,
      "step": 64350
    },
    {
      "epoch": 5.720888888888889,
      "grad_norm": 0.5597456693649292,
      "learning_rate": 1.4244444444444446e-05,
      "loss": 0.0027,
      "step": 64360
    },
    {
      "epoch": 5.721777777777778,
      "grad_norm": 0.3817705810070038,
      "learning_rate": 1.4238888888888891e-05,
      "loss": 0.0023,
      "step": 64370
    },
    {
      "epoch": 5.722666666666667,
      "grad_norm": 0.44011470675468445,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.0021,
      "step": 64380
    },
    {
      "epoch": 5.7235555555555555,
      "grad_norm": 0.4367707073688507,
      "learning_rate": 1.4227777777777778e-05,
      "loss": 0.0016,
      "step": 64390
    },
    {
      "epoch": 5.724444444444444,
      "grad_norm": 0.0537496842443943,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 0.0021,
      "step": 64400
    },
    {
      "epoch": 5.725333333333333,
      "grad_norm": 0.9607140421867371,
      "learning_rate": 1.4216666666666667e-05,
      "loss": 0.0013,
      "step": 64410
    },
    {
      "epoch": 5.726222222222222,
      "grad_norm": 0.07896225899457932,
      "learning_rate": 1.421111111111111e-05,
      "loss": 0.0016,
      "step": 64420
    },
    {
      "epoch": 5.727111111111111,
      "grad_norm": 0.3733504116535187,
      "learning_rate": 1.4205555555555556e-05,
      "loss": 0.0025,
      "step": 64430
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.4015406668186188,
      "learning_rate": 1.42e-05,
      "loss": 0.0019,
      "step": 64440
    },
    {
      "epoch": 5.728888888888889,
      "grad_norm": 0.05175120756030083,
      "learning_rate": 1.4194444444444447e-05,
      "loss": 0.0021,
      "step": 64450
    },
    {
      "epoch": 5.729777777777778,
      "grad_norm": 0.039759233593940735,
      "learning_rate": 1.418888888888889e-05,
      "loss": 0.0014,
      "step": 64460
    },
    {
      "epoch": 5.730666666666667,
      "grad_norm": 0.7785287499427795,
      "learning_rate": 1.4183333333333335e-05,
      "loss": 0.0021,
      "step": 64470
    },
    {
      "epoch": 5.7315555555555555,
      "grad_norm": 0.4490727186203003,
      "learning_rate": 1.4177777777777779e-05,
      "loss": 0.0012,
      "step": 64480
    },
    {
      "epoch": 5.732444444444445,
      "grad_norm": 0.6203471422195435,
      "learning_rate": 1.4172222222222222e-05,
      "loss": 0.0029,
      "step": 64490
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.18172146379947662,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0021,
      "step": 64500
    },
    {
      "epoch": 5.734222222222222,
      "grad_norm": 0.1487942636013031,
      "learning_rate": 1.4161111111111111e-05,
      "loss": 0.0013,
      "step": 64510
    },
    {
      "epoch": 5.735111111111111,
      "grad_norm": 0.7557313442230225,
      "learning_rate": 1.4155555555555555e-05,
      "loss": 0.0019,
      "step": 64520
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.3719389736652374,
      "learning_rate": 1.415e-05,
      "loss": 0.0013,
      "step": 64530
    },
    {
      "epoch": 5.736888888888889,
      "grad_norm": 0.5995364189147949,
      "learning_rate": 1.4144444444444447e-05,
      "loss": 0.002,
      "step": 64540
    },
    {
      "epoch": 5.737777777777778,
      "grad_norm": 0.32822203636169434,
      "learning_rate": 1.413888888888889e-05,
      "loss": 0.0025,
      "step": 64550
    },
    {
      "epoch": 5.738666666666667,
      "grad_norm": 0.49253687262535095,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0017,
      "step": 64560
    },
    {
      "epoch": 5.7395555555555555,
      "grad_norm": 0.5887622833251953,
      "learning_rate": 1.412777777777778e-05,
      "loss": 0.0019,
      "step": 64570
    },
    {
      "epoch": 5.740444444444444,
      "grad_norm": 0.7256158590316772,
      "learning_rate": 1.4122222222222223e-05,
      "loss": 0.0021,
      "step": 64580
    },
    {
      "epoch": 5.741333333333333,
      "grad_norm": 0.37788325548171997,
      "learning_rate": 1.4116666666666666e-05,
      "loss": 0.0014,
      "step": 64590
    },
    {
      "epoch": 5.742222222222222,
      "grad_norm": 0.5342072248458862,
      "learning_rate": 1.4111111111111112e-05,
      "loss": 0.0018,
      "step": 64600
    },
    {
      "epoch": 5.743111111111111,
      "grad_norm": 0.29065436124801636,
      "learning_rate": 1.4105555555555555e-05,
      "loss": 0.0021,
      "step": 64610
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.30291569232940674,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0026,
      "step": 64620
    },
    {
      "epoch": 5.744888888888889,
      "grad_norm": 0.07861975580453873,
      "learning_rate": 1.4094444444444446e-05,
      "loss": 0.0021,
      "step": 64630
    },
    {
      "epoch": 5.745777777777778,
      "grad_norm": 0.6576657891273499,
      "learning_rate": 1.4088888888888891e-05,
      "loss": 0.0016,
      "step": 64640
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.0822281539440155,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.0014,
      "step": 64650
    },
    {
      "epoch": 5.7475555555555555,
      "grad_norm": 0.8117067217826843,
      "learning_rate": 1.4077777777777778e-05,
      "loss": 0.0021,
      "step": 64660
    },
    {
      "epoch": 5.748444444444445,
      "grad_norm": 0.32504162192344666,
      "learning_rate": 1.4072222222222223e-05,
      "loss": 0.0022,
      "step": 64670
    },
    {
      "epoch": 5.749333333333333,
      "grad_norm": 0.29830849170684814,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0015,
      "step": 64680
    },
    {
      "epoch": 5.750222222222222,
      "grad_norm": 0.6908629536628723,
      "learning_rate": 1.406111111111111e-05,
      "loss": 0.0013,
      "step": 64690
    },
    {
      "epoch": 5.751111111111111,
      "grad_norm": 0.20009168982505798,
      "learning_rate": 1.4055555555555556e-05,
      "loss": 0.0014,
      "step": 64700
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.6996802687644958,
      "learning_rate": 1.4050000000000003e-05,
      "loss": 0.0019,
      "step": 64710
    },
    {
      "epoch": 5.752888888888889,
      "grad_norm": 0.1259431540966034,
      "learning_rate": 1.4044444444444446e-05,
      "loss": 0.0016,
      "step": 64720
    },
    {
      "epoch": 5.753777777777778,
      "grad_norm": 0.21446219086647034,
      "learning_rate": 1.403888888888889e-05,
      "loss": 0.0014,
      "step": 64730
    },
    {
      "epoch": 5.754666666666667,
      "grad_norm": 0.47400814294815063,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0017,
      "step": 64740
    },
    {
      "epoch": 5.7555555555555555,
      "grad_norm": 0.10567209124565125,
      "learning_rate": 1.4027777777777779e-05,
      "loss": 0.0023,
      "step": 64750
    },
    {
      "epoch": 5.756444444444444,
      "grad_norm": 0.07996678352355957,
      "learning_rate": 1.4022222222222222e-05,
      "loss": 0.0016,
      "step": 64760
    },
    {
      "epoch": 5.757333333333333,
      "grad_norm": 0.29946649074554443,
      "learning_rate": 1.4016666666666667e-05,
      "loss": 0.002,
      "step": 64770
    },
    {
      "epoch": 5.758222222222222,
      "grad_norm": 0.27184778451919556,
      "learning_rate": 1.4011111111111111e-05,
      "loss": 0.0022,
      "step": 64780
    },
    {
      "epoch": 5.759111111111111,
      "grad_norm": 0.21310904622077942,
      "learning_rate": 1.4005555555555555e-05,
      "loss": 0.0021,
      "step": 64790
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.13101579248905182,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0019,
      "step": 64800
    },
    {
      "epoch": 5.760888888888889,
      "grad_norm": 0.6947858929634094,
      "learning_rate": 1.3994444444444447e-05,
      "loss": 0.0022,
      "step": 64810
    },
    {
      "epoch": 5.761777777777778,
      "grad_norm": 0.15432873368263245,
      "learning_rate": 1.398888888888889e-05,
      "loss": 0.0016,
      "step": 64820
    },
    {
      "epoch": 5.762666666666667,
      "grad_norm": 0.14025866985321045,
      "learning_rate": 1.3983333333333334e-05,
      "loss": 0.0024,
      "step": 64830
    },
    {
      "epoch": 5.7635555555555555,
      "grad_norm": 0.6138917207717896,
      "learning_rate": 1.3977777777777779e-05,
      "loss": 0.0014,
      "step": 64840
    },
    {
      "epoch": 5.764444444444445,
      "grad_norm": 0.020726097747683525,
      "learning_rate": 1.3972222222222223e-05,
      "loss": 0.0015,
      "step": 64850
    },
    {
      "epoch": 5.765333333333333,
      "grad_norm": 0.5594692230224609,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.0024,
      "step": 64860
    },
    {
      "epoch": 5.766222222222222,
      "grad_norm": 0.08768943697214127,
      "learning_rate": 1.3961111111111111e-05,
      "loss": 0.0018,
      "step": 64870
    },
    {
      "epoch": 5.767111111111111,
      "grad_norm": 0.13233472406864166,
      "learning_rate": 1.3955555555555555e-05,
      "loss": 0.0017,
      "step": 64880
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.084502212703228,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 0.0012,
      "step": 64890
    },
    {
      "epoch": 5.768888888888889,
      "grad_norm": 0.05794800445437431,
      "learning_rate": 1.3944444444444446e-05,
      "loss": 0.002,
      "step": 64900
    },
    {
      "epoch": 5.769777777777778,
      "grad_norm": 0.1867561638355255,
      "learning_rate": 1.393888888888889e-05,
      "loss": 0.0017,
      "step": 64910
    },
    {
      "epoch": 5.770666666666667,
      "grad_norm": 0.20109835267066956,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0017,
      "step": 64920
    },
    {
      "epoch": 5.7715555555555556,
      "grad_norm": 0.08405999839305878,
      "learning_rate": 1.3927777777777778e-05,
      "loss": 0.0025,
      "step": 64930
    },
    {
      "epoch": 5.772444444444444,
      "grad_norm": 0.10129165649414062,
      "learning_rate": 1.3922222222222223e-05,
      "loss": 0.0018,
      "step": 64940
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 0.7633286714553833,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.0021,
      "step": 64950
    },
    {
      "epoch": 5.774222222222222,
      "grad_norm": 0.4146750569343567,
      "learning_rate": 1.391111111111111e-05,
      "loss": 0.0023,
      "step": 64960
    },
    {
      "epoch": 5.775111111111111,
      "grad_norm": 0.22447670996189117,
      "learning_rate": 1.3905555555555555e-05,
      "loss": 0.0014,
      "step": 64970
    },
    {
      "epoch": 5.776,
      "grad_norm": 0.0435912162065506,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0026,
      "step": 64980
    },
    {
      "epoch": 5.776888888888889,
      "grad_norm": 0.16817617416381836,
      "learning_rate": 1.3894444444444446e-05,
      "loss": 0.0017,
      "step": 64990
    },
    {
      "epoch": 5.777777777777778,
      "grad_norm": 0.33528146147727966,
      "learning_rate": 1.388888888888889e-05,
      "loss": 0.0016,
      "step": 65000
    },
    {
      "epoch": 5.778666666666666,
      "grad_norm": 0.10944017022848129,
      "learning_rate": 1.3883333333333335e-05,
      "loss": 0.0017,
      "step": 65010
    },
    {
      "epoch": 5.779555555555556,
      "grad_norm": 0.48221901059150696,
      "learning_rate": 1.3877777777777778e-05,
      "loss": 0.0021,
      "step": 65020
    },
    {
      "epoch": 5.780444444444445,
      "grad_norm": 0.16195866465568542,
      "learning_rate": 1.3872222222222222e-05,
      "loss": 0.0018,
      "step": 65030
    },
    {
      "epoch": 5.781333333333333,
      "grad_norm": 0.37554308772087097,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0017,
      "step": 65040
    },
    {
      "epoch": 5.782222222222222,
      "grad_norm": 0.18995319306850433,
      "learning_rate": 1.386111111111111e-05,
      "loss": 0.002,
      "step": 65050
    },
    {
      "epoch": 5.783111111111111,
      "grad_norm": 0.4770771563053131,
      "learning_rate": 1.3855555555555554e-05,
      "loss": 0.0017,
      "step": 65060
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.2639302611351013,
      "learning_rate": 1.3850000000000001e-05,
      "loss": 0.0015,
      "step": 65070
    },
    {
      "epoch": 5.784888888888889,
      "grad_norm": 0.7015540599822998,
      "learning_rate": 1.3844444444444446e-05,
      "loss": 0.0024,
      "step": 65080
    },
    {
      "epoch": 5.785777777777778,
      "grad_norm": 0.2241581380367279,
      "learning_rate": 1.383888888888889e-05,
      "loss": 0.0018,
      "step": 65090
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 0.3998447060585022,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.002,
      "step": 65100
    },
    {
      "epoch": 5.787555555555556,
      "grad_norm": 0.33293065428733826,
      "learning_rate": 1.3827777777777779e-05,
      "loss": 0.0019,
      "step": 65110
    },
    {
      "epoch": 5.788444444444444,
      "grad_norm": 0.3239805996417999,
      "learning_rate": 1.3822222222222222e-05,
      "loss": 0.0024,
      "step": 65120
    },
    {
      "epoch": 5.789333333333333,
      "grad_norm": 0.632146418094635,
      "learning_rate": 1.3816666666666666e-05,
      "loss": 0.0017,
      "step": 65130
    },
    {
      "epoch": 5.790222222222222,
      "grad_norm": 0.5643959045410156,
      "learning_rate": 1.3811111111111111e-05,
      "loss": 0.0024,
      "step": 65140
    },
    {
      "epoch": 5.791111111111111,
      "grad_norm": 0.29898083209991455,
      "learning_rate": 1.3805555555555555e-05,
      "loss": 0.0019,
      "step": 65150
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.2667980194091797,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0026,
      "step": 65160
    },
    {
      "epoch": 5.792888888888889,
      "grad_norm": 0.2570374608039856,
      "learning_rate": 1.3794444444444445e-05,
      "loss": 0.0027,
      "step": 65170
    },
    {
      "epoch": 5.793777777777778,
      "grad_norm": 0.5119991302490234,
      "learning_rate": 1.378888888888889e-05,
      "loss": 0.0023,
      "step": 65180
    },
    {
      "epoch": 5.794666666666666,
      "grad_norm": 0.14541450142860413,
      "learning_rate": 1.3783333333333334e-05,
      "loss": 0.0023,
      "step": 65190
    },
    {
      "epoch": 5.795555555555556,
      "grad_norm": 0.04474782943725586,
      "learning_rate": 1.3777777777777778e-05,
      "loss": 0.0012,
      "step": 65200
    },
    {
      "epoch": 5.796444444444444,
      "grad_norm": 0.11067624390125275,
      "learning_rate": 1.3772222222222223e-05,
      "loss": 0.0022,
      "step": 65210
    },
    {
      "epoch": 5.7973333333333334,
      "grad_norm": 0.25449517369270325,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.0022,
      "step": 65220
    },
    {
      "epoch": 5.798222222222222,
      "grad_norm": 0.29165446758270264,
      "learning_rate": 1.376111111111111e-05,
      "loss": 0.0022,
      "step": 65230
    },
    {
      "epoch": 5.799111111111111,
      "grad_norm": 0.2231941968202591,
      "learning_rate": 1.3755555555555555e-05,
      "loss": 0.0021,
      "step": 65240
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.21174786984920502,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.0021,
      "step": 65250
    },
    {
      "epoch": 5.800888888888889,
      "grad_norm": 0.5654646158218384,
      "learning_rate": 1.3744444444444446e-05,
      "loss": 0.0024,
      "step": 65260
    },
    {
      "epoch": 5.801777777777778,
      "grad_norm": 0.8122046589851379,
      "learning_rate": 1.373888888888889e-05,
      "loss": 0.0022,
      "step": 65270
    },
    {
      "epoch": 5.802666666666667,
      "grad_norm": 0.4621337354183197,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0032,
      "step": 65280
    },
    {
      "epoch": 5.803555555555556,
      "grad_norm": 0.06670801341533661,
      "learning_rate": 1.3727777777777778e-05,
      "loss": 0.0025,
      "step": 65290
    },
    {
      "epoch": 5.804444444444444,
      "grad_norm": 0.6094725131988525,
      "learning_rate": 1.3722222222222222e-05,
      "loss": 0.0017,
      "step": 65300
    },
    {
      "epoch": 5.8053333333333335,
      "grad_norm": 0.5123988389968872,
      "learning_rate": 1.3716666666666667e-05,
      "loss": 0.0019,
      "step": 65310
    },
    {
      "epoch": 5.806222222222222,
      "grad_norm": 0.055436890572309494,
      "learning_rate": 1.371111111111111e-05,
      "loss": 0.0026,
      "step": 65320
    },
    {
      "epoch": 5.807111111111111,
      "grad_norm": 0.4084621071815491,
      "learning_rate": 1.3705555555555557e-05,
      "loss": 0.0018,
      "step": 65330
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.12770302593708038,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.002,
      "step": 65340
    },
    {
      "epoch": 5.808888888888889,
      "grad_norm": 0.06384941190481186,
      "learning_rate": 1.3694444444444446e-05,
      "loss": 0.0015,
      "step": 65350
    },
    {
      "epoch": 5.809777777777778,
      "grad_norm": 0.15755918622016907,
      "learning_rate": 1.368888888888889e-05,
      "loss": 0.0016,
      "step": 65360
    },
    {
      "epoch": 5.810666666666666,
      "grad_norm": 0.11310597509145737,
      "learning_rate": 1.3683333333333333e-05,
      "loss": 0.0017,
      "step": 65370
    },
    {
      "epoch": 5.811555555555556,
      "grad_norm": 0.05851885303854942,
      "learning_rate": 1.3677777777777779e-05,
      "loss": 0.002,
      "step": 65380
    },
    {
      "epoch": 5.812444444444444,
      "grad_norm": 0.0959157645702362,
      "learning_rate": 1.3672222222222222e-05,
      "loss": 0.0024,
      "step": 65390
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.31335440278053284,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0025,
      "step": 65400
    },
    {
      "epoch": 5.814222222222222,
      "grad_norm": 0.07759816199541092,
      "learning_rate": 1.3661111111111111e-05,
      "loss": 0.0019,
      "step": 65410
    },
    {
      "epoch": 5.815111111111111,
      "grad_norm": 0.23313763737678528,
      "learning_rate": 1.3655555555555558e-05,
      "loss": 0.0015,
      "step": 65420
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.4120388925075531,
      "learning_rate": 1.3650000000000001e-05,
      "loss": 0.0025,
      "step": 65430
    },
    {
      "epoch": 5.816888888888889,
      "grad_norm": 0.06738869845867157,
      "learning_rate": 1.3644444444444445e-05,
      "loss": 0.0018,
      "step": 65440
    },
    {
      "epoch": 5.817777777777778,
      "grad_norm": 0.47037938237190247,
      "learning_rate": 1.363888888888889e-05,
      "loss": 0.0036,
      "step": 65450
    },
    {
      "epoch": 5.818666666666667,
      "grad_norm": 0.3239849805831909,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0013,
      "step": 65460
    },
    {
      "epoch": 5.819555555555556,
      "grad_norm": 0.16184218227863312,
      "learning_rate": 1.3627777777777777e-05,
      "loss": 0.0021,
      "step": 65470
    },
    {
      "epoch": 5.820444444444444,
      "grad_norm": 0.37951067090034485,
      "learning_rate": 1.3622222222222223e-05,
      "loss": 0.0013,
      "step": 65480
    },
    {
      "epoch": 5.8213333333333335,
      "grad_norm": 0.17924660444259644,
      "learning_rate": 1.3616666666666666e-05,
      "loss": 0.0016,
      "step": 65490
    },
    {
      "epoch": 5.822222222222222,
      "grad_norm": 0.4474281370639801,
      "learning_rate": 1.3611111111111111e-05,
      "loss": 0.0017,
      "step": 65500
    },
    {
      "epoch": 5.823111111111111,
      "grad_norm": 0.398589164018631,
      "learning_rate": 1.3605555555555557e-05,
      "loss": 0.0021,
      "step": 65510
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.6185479760169983,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.003,
      "step": 65520
    },
    {
      "epoch": 5.824888888888889,
      "grad_norm": 0.04821692407131195,
      "learning_rate": 1.3594444444444445e-05,
      "loss": 0.0017,
      "step": 65530
    },
    {
      "epoch": 5.825777777777778,
      "grad_norm": 0.18063874542713165,
      "learning_rate": 1.358888888888889e-05,
      "loss": 0.0025,
      "step": 65540
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.589999258518219,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.0025,
      "step": 65550
    },
    {
      "epoch": 5.827555555555556,
      "grad_norm": 0.09369204193353653,
      "learning_rate": 1.3577777777777778e-05,
      "loss": 0.0016,
      "step": 65560
    },
    {
      "epoch": 5.828444444444444,
      "grad_norm": 0.2946610450744629,
      "learning_rate": 1.3572222222222223e-05,
      "loss": 0.0024,
      "step": 65570
    },
    {
      "epoch": 5.8293333333333335,
      "grad_norm": 0.4133615493774414,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0026,
      "step": 65580
    },
    {
      "epoch": 5.830222222222222,
      "grad_norm": 0.49590930342674255,
      "learning_rate": 1.356111111111111e-05,
      "loss": 0.0019,
      "step": 65590
    },
    {
      "epoch": 5.831111111111111,
      "grad_norm": 0.18240076303482056,
      "learning_rate": 1.3555555555555557e-05,
      "loss": 0.0018,
      "step": 65600
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.2278165966272354,
      "learning_rate": 1.3550000000000002e-05,
      "loss": 0.0017,
      "step": 65610
    },
    {
      "epoch": 5.832888888888889,
      "grad_norm": 0.5592437386512756,
      "learning_rate": 1.3544444444444446e-05,
      "loss": 0.0019,
      "step": 65620
    },
    {
      "epoch": 5.833777777777778,
      "grad_norm": 0.5911137461662292,
      "learning_rate": 1.353888888888889e-05,
      "loss": 0.0018,
      "step": 65630
    },
    {
      "epoch": 5.834666666666667,
      "grad_norm": 0.4531663954257965,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.002,
      "step": 65640
    },
    {
      "epoch": 5.835555555555556,
      "grad_norm": 0.4827558398246765,
      "learning_rate": 1.3527777777777778e-05,
      "loss": 0.0024,
      "step": 65650
    },
    {
      "epoch": 5.836444444444444,
      "grad_norm": 0.10917270928621292,
      "learning_rate": 1.3522222222222222e-05,
      "loss": 0.0027,
      "step": 65660
    },
    {
      "epoch": 5.8373333333333335,
      "grad_norm": 0.03535543754696846,
      "learning_rate": 1.3516666666666667e-05,
      "loss": 0.0023,
      "step": 65670
    },
    {
      "epoch": 5.838222222222222,
      "grad_norm": 0.1893608123064041,
      "learning_rate": 1.351111111111111e-05,
      "loss": 0.0021,
      "step": 65680
    },
    {
      "epoch": 5.839111111111111,
      "grad_norm": 0.11169838160276413,
      "learning_rate": 1.3505555555555558e-05,
      "loss": 0.0016,
      "step": 65690
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.17175693809986115,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0027,
      "step": 65700
    },
    {
      "epoch": 5.840888888888889,
      "grad_norm": 0.41510918736457825,
      "learning_rate": 1.3494444444444446e-05,
      "loss": 0.0024,
      "step": 65710
    },
    {
      "epoch": 5.841777777777778,
      "grad_norm": 0.4545251131057739,
      "learning_rate": 1.348888888888889e-05,
      "loss": 0.0013,
      "step": 65720
    },
    {
      "epoch": 5.842666666666666,
      "grad_norm": 0.06831669807434082,
      "learning_rate": 1.3483333333333334e-05,
      "loss": 0.0018,
      "step": 65730
    },
    {
      "epoch": 5.843555555555556,
      "grad_norm": 0.044028349220752716,
      "learning_rate": 1.3477777777777779e-05,
      "loss": 0.0014,
      "step": 65740
    },
    {
      "epoch": 5.844444444444444,
      "grad_norm": 0.33301448822021484,
      "learning_rate": 1.3472222222222222e-05,
      "loss": 0.0021,
      "step": 65750
    },
    {
      "epoch": 5.8453333333333335,
      "grad_norm": 0.11682659387588501,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0018,
      "step": 65760
    },
    {
      "epoch": 5.846222222222222,
      "grad_norm": 0.5850594639778137,
      "learning_rate": 1.3461111111111111e-05,
      "loss": 0.002,
      "step": 65770
    },
    {
      "epoch": 5.847111111111111,
      "grad_norm": 0.08820585161447525,
      "learning_rate": 1.3455555555555558e-05,
      "loss": 0.002,
      "step": 65780
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.5899313688278198,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 0.002,
      "step": 65790
    },
    {
      "epoch": 5.848888888888889,
      "grad_norm": 0.05401994287967682,
      "learning_rate": 1.3444444444444445e-05,
      "loss": 0.0019,
      "step": 65800
    },
    {
      "epoch": 5.849777777777778,
      "grad_norm": 0.07717899978160858,
      "learning_rate": 1.343888888888889e-05,
      "loss": 0.002,
      "step": 65810
    },
    {
      "epoch": 5.850666666666667,
      "grad_norm": 0.22513984143733978,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.0013,
      "step": 65820
    },
    {
      "epoch": 5.851555555555556,
      "grad_norm": 0.5151497721672058,
      "learning_rate": 1.3427777777777778e-05,
      "loss": 0.002,
      "step": 65830
    },
    {
      "epoch": 5.852444444444444,
      "grad_norm": 0.18710725009441376,
      "learning_rate": 1.3422222222222223e-05,
      "loss": 0.0021,
      "step": 65840
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 0.37198615074157715,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.0018,
      "step": 65850
    },
    {
      "epoch": 5.854222222222222,
      "grad_norm": 0.22793731093406677,
      "learning_rate": 1.341111111111111e-05,
      "loss": 0.0017,
      "step": 65860
    },
    {
      "epoch": 5.855111111111111,
      "grad_norm": 0.5867854952812195,
      "learning_rate": 1.3405555555555557e-05,
      "loss": 0.0012,
      "step": 65870
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.157521054148674,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0019,
      "step": 65880
    },
    {
      "epoch": 5.856888888888889,
      "grad_norm": 0.14503438770771027,
      "learning_rate": 1.3394444444444446e-05,
      "loss": 0.0021,
      "step": 65890
    },
    {
      "epoch": 5.857777777777778,
      "grad_norm": 0.039644017815589905,
      "learning_rate": 1.338888888888889e-05,
      "loss": 0.0018,
      "step": 65900
    },
    {
      "epoch": 5.858666666666666,
      "grad_norm": 0.07475987821817398,
      "learning_rate": 1.3383333333333335e-05,
      "loss": 0.0017,
      "step": 65910
    },
    {
      "epoch": 5.859555555555556,
      "grad_norm": 0.07891301810741425,
      "learning_rate": 1.3377777777777778e-05,
      "loss": 0.0018,
      "step": 65920
    },
    {
      "epoch": 5.860444444444444,
      "grad_norm": 0.2265709489583969,
      "learning_rate": 1.3372222222222222e-05,
      "loss": 0.0017,
      "step": 65930
    },
    {
      "epoch": 5.8613333333333335,
      "grad_norm": 0.11611006408929825,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.0018,
      "step": 65940
    },
    {
      "epoch": 5.862222222222222,
      "grad_norm": 0.4113132953643799,
      "learning_rate": 1.3361111111111114e-05,
      "loss": 0.002,
      "step": 65950
    },
    {
      "epoch": 5.863111111111111,
      "grad_norm": 0.4379477798938751,
      "learning_rate": 1.3355555555555557e-05,
      "loss": 0.0015,
      "step": 65960
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.6264517903327942,
      "learning_rate": 1.3350000000000001e-05,
      "loss": 0.003,
      "step": 65970
    },
    {
      "epoch": 5.864888888888889,
      "grad_norm": 0.4407172203063965,
      "learning_rate": 1.3344444444444446e-05,
      "loss": 0.0013,
      "step": 65980
    },
    {
      "epoch": 5.865777777777778,
      "grad_norm": 0.10781806707382202,
      "learning_rate": 1.333888888888889e-05,
      "loss": 0.0028,
      "step": 65990
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.1044403612613678,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.002,
      "step": 66000
    },
    {
      "epoch": 5.867555555555556,
      "grad_norm": 0.12380150705575943,
      "learning_rate": 1.3327777777777779e-05,
      "loss": 0.0018,
      "step": 66010
    },
    {
      "epoch": 5.868444444444444,
      "grad_norm": 0.5819835066795349,
      "learning_rate": 1.3322222222222222e-05,
      "loss": 0.0024,
      "step": 66020
    },
    {
      "epoch": 5.8693333333333335,
      "grad_norm": 0.0629899799823761,
      "learning_rate": 1.3316666666666666e-05,
      "loss": 0.0017,
      "step": 66030
    },
    {
      "epoch": 5.870222222222222,
      "grad_norm": 0.5937970280647278,
      "learning_rate": 1.3311111111111113e-05,
      "loss": 0.0019,
      "step": 66040
    },
    {
      "epoch": 5.871111111111111,
      "grad_norm": 0.2548086643218994,
      "learning_rate": 1.3305555555555558e-05,
      "loss": 0.0018,
      "step": 66050
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.26601168513298035,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.0018,
      "step": 66060
    },
    {
      "epoch": 5.872888888888889,
      "grad_norm": 0.6230033040046692,
      "learning_rate": 1.3294444444444445e-05,
      "loss": 0.0021,
      "step": 66070
    },
    {
      "epoch": 5.873777777777778,
      "grad_norm": 0.5278245210647583,
      "learning_rate": 1.328888888888889e-05,
      "loss": 0.0026,
      "step": 66080
    },
    {
      "epoch": 5.874666666666666,
      "grad_norm": 0.09646657854318619,
      "learning_rate": 1.3283333333333334e-05,
      "loss": 0.0022,
      "step": 66090
    },
    {
      "epoch": 5.875555555555556,
      "grad_norm": 0.7925050854682922,
      "learning_rate": 1.3277777777777777e-05,
      "loss": 0.0045,
      "step": 66100
    },
    {
      "epoch": 5.876444444444444,
      "grad_norm": 0.7627785205841064,
      "learning_rate": 1.3272222222222223e-05,
      "loss": 0.0034,
      "step": 66110
    },
    {
      "epoch": 5.8773333333333335,
      "grad_norm": 0.07726889103651047,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.002,
      "step": 66120
    },
    {
      "epoch": 5.878222222222222,
      "grad_norm": 0.46428096294403076,
      "learning_rate": 1.3261111111111113e-05,
      "loss": 0.0024,
      "step": 66130
    },
    {
      "epoch": 5.879111111111111,
      "grad_norm": 0.043083131313323975,
      "learning_rate": 1.3255555555555557e-05,
      "loss": 0.0021,
      "step": 66140
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.20124442875385284,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.0016,
      "step": 66150
    },
    {
      "epoch": 5.880888888888889,
      "grad_norm": 0.18798968195915222,
      "learning_rate": 1.3244444444444445e-05,
      "loss": 0.0026,
      "step": 66160
    },
    {
      "epoch": 5.881777777777778,
      "grad_norm": 0.33140215277671814,
      "learning_rate": 1.3238888888888889e-05,
      "loss": 0.0014,
      "step": 66170
    },
    {
      "epoch": 5.882666666666667,
      "grad_norm": 0.49524563550949097,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.002,
      "step": 66180
    },
    {
      "epoch": 5.883555555555556,
      "grad_norm": 0.6429422497749329,
      "learning_rate": 1.3227777777777778e-05,
      "loss": 0.0014,
      "step": 66190
    },
    {
      "epoch": 5.884444444444444,
      "grad_norm": 0.12964285910129547,
      "learning_rate": 1.3222222222222221e-05,
      "loss": 0.0017,
      "step": 66200
    },
    {
      "epoch": 5.8853333333333335,
      "grad_norm": 0.4743178188800812,
      "learning_rate": 1.3216666666666667e-05,
      "loss": 0.0018,
      "step": 66210
    },
    {
      "epoch": 5.886222222222222,
      "grad_norm": 0.37167128920555115,
      "learning_rate": 1.3211111111111114e-05,
      "loss": 0.0015,
      "step": 66220
    },
    {
      "epoch": 5.887111111111111,
      "grad_norm": 0.4874645173549652,
      "learning_rate": 1.3205555555555557e-05,
      "loss": 0.0014,
      "step": 66230
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.3685222566127777,
      "learning_rate": 1.32e-05,
      "loss": 0.0014,
      "step": 66240
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 0.44095852971076965,
      "learning_rate": 1.3194444444444446e-05,
      "loss": 0.0018,
      "step": 66250
    },
    {
      "epoch": 5.889777777777778,
      "grad_norm": 0.26609864830970764,
      "learning_rate": 1.318888888888889e-05,
      "loss": 0.0026,
      "step": 66260
    },
    {
      "epoch": 5.890666666666666,
      "grad_norm": 0.5329532027244568,
      "learning_rate": 1.3183333333333333e-05,
      "loss": 0.0026,
      "step": 66270
    },
    {
      "epoch": 5.891555555555556,
      "grad_norm": 0.08291647583246231,
      "learning_rate": 1.3177777777777778e-05,
      "loss": 0.0023,
      "step": 66280
    },
    {
      "epoch": 5.892444444444444,
      "grad_norm": 0.09680839627981186,
      "learning_rate": 1.3172222222222222e-05,
      "loss": 0.0016,
      "step": 66290
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 0.11530528217554092,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.0013,
      "step": 66300
    },
    {
      "epoch": 5.894222222222222,
      "grad_norm": 0.40979692339897156,
      "learning_rate": 1.3161111111111112e-05,
      "loss": 0.0025,
      "step": 66310
    },
    {
      "epoch": 5.895111111111111,
      "grad_norm": 0.10683692991733551,
      "learning_rate": 1.3155555555555558e-05,
      "loss": 0.0019,
      "step": 66320
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.3749675154685974,
      "learning_rate": 1.3150000000000001e-05,
      "loss": 0.0016,
      "step": 66330
    },
    {
      "epoch": 5.896888888888888,
      "grad_norm": 0.4354829490184784,
      "learning_rate": 1.3144444444444445e-05,
      "loss": 0.0016,
      "step": 66340
    },
    {
      "epoch": 5.897777777777778,
      "grad_norm": 0.09125305712223053,
      "learning_rate": 1.313888888888889e-05,
      "loss": 0.0022,
      "step": 66350
    },
    {
      "epoch": 5.898666666666666,
      "grad_norm": 0.04193396493792534,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0018,
      "step": 66360
    },
    {
      "epoch": 5.899555555555556,
      "grad_norm": 0.04675908014178276,
      "learning_rate": 1.3127777777777777e-05,
      "loss": 0.0013,
      "step": 66370
    },
    {
      "epoch": 5.900444444444444,
      "grad_norm": 0.6765829920768738,
      "learning_rate": 1.3122222222222222e-05,
      "loss": 0.0014,
      "step": 66380
    },
    {
      "epoch": 5.9013333333333335,
      "grad_norm": 0.4298785924911499,
      "learning_rate": 1.3116666666666666e-05,
      "loss": 0.0022,
      "step": 66390
    },
    {
      "epoch": 5.902222222222222,
      "grad_norm": 0.26800236105918884,
      "learning_rate": 1.3111111111111113e-05,
      "loss": 0.0022,
      "step": 66400
    },
    {
      "epoch": 5.903111111111111,
      "grad_norm": 0.08132393658161163,
      "learning_rate": 1.3105555555555556e-05,
      "loss": 0.0016,
      "step": 66410
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.18348528444766998,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.002,
      "step": 66420
    },
    {
      "epoch": 5.904888888888889,
      "grad_norm": 0.2156670242547989,
      "learning_rate": 1.3094444444444445e-05,
      "loss": 0.0026,
      "step": 66430
    },
    {
      "epoch": 5.905777777777778,
      "grad_norm": 0.41086718440055847,
      "learning_rate": 1.3088888888888889e-05,
      "loss": 0.0018,
      "step": 66440
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 0.6205447912216187,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.0026,
      "step": 66450
    },
    {
      "epoch": 5.907555555555556,
      "grad_norm": 0.18677325546741486,
      "learning_rate": 1.3077777777777778e-05,
      "loss": 0.0017,
      "step": 66460
    },
    {
      "epoch": 5.908444444444444,
      "grad_norm": 0.07477255165576935,
      "learning_rate": 1.3072222222222221e-05,
      "loss": 0.0013,
      "step": 66470
    },
    {
      "epoch": 5.9093333333333335,
      "grad_norm": 0.07028133422136307,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0015,
      "step": 66480
    },
    {
      "epoch": 5.910222222222222,
      "grad_norm": 0.2598927915096283,
      "learning_rate": 1.3061111111111113e-05,
      "loss": 0.0017,
      "step": 66490
    },
    {
      "epoch": 5.911111111111111,
      "grad_norm": 0.06669078767299652,
      "learning_rate": 1.3055555555555557e-05,
      "loss": 0.0031,
      "step": 66500
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.1486532986164093,
      "learning_rate": 1.305e-05,
      "loss": 0.0016,
      "step": 66510
    },
    {
      "epoch": 5.912888888888888,
      "grad_norm": 0.1557912975549698,
      "learning_rate": 1.3044444444444446e-05,
      "loss": 0.0018,
      "step": 66520
    },
    {
      "epoch": 5.913777777777778,
      "grad_norm": 0.2942773699760437,
      "learning_rate": 1.303888888888889e-05,
      "loss": 0.0023,
      "step": 66530
    },
    {
      "epoch": 5.914666666666666,
      "grad_norm": 0.4666183590888977,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.003,
      "step": 66540
    },
    {
      "epoch": 5.915555555555556,
      "grad_norm": 0.11782858520746231,
      "learning_rate": 1.3027777777777778e-05,
      "loss": 0.0016,
      "step": 66550
    },
    {
      "epoch": 5.916444444444444,
      "grad_norm": 0.4462287127971649,
      "learning_rate": 1.3022222222222222e-05,
      "loss": 0.0026,
      "step": 66560
    },
    {
      "epoch": 5.917333333333334,
      "grad_norm": 0.4835793375968933,
      "learning_rate": 1.3016666666666669e-05,
      "loss": 0.0015,
      "step": 66570
    },
    {
      "epoch": 5.918222222222222,
      "grad_norm": 0.33525538444519043,
      "learning_rate": 1.3011111111111112e-05,
      "loss": 0.0025,
      "step": 66580
    },
    {
      "epoch": 5.919111111111111,
      "grad_norm": 0.300864040851593,
      "learning_rate": 1.3005555555555557e-05,
      "loss": 0.0025,
      "step": 66590
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.2992597222328186,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0019,
      "step": 66600
    },
    {
      "epoch": 5.920888888888889,
      "grad_norm": 0.2201978862285614,
      "learning_rate": 1.2994444444444444e-05,
      "loss": 0.0017,
      "step": 66610
    },
    {
      "epoch": 5.921777777777778,
      "grad_norm": 0.04625987634062767,
      "learning_rate": 1.298888888888889e-05,
      "loss": 0.0027,
      "step": 66620
    },
    {
      "epoch": 5.922666666666666,
      "grad_norm": 0.07943138480186462,
      "learning_rate": 1.2983333333333333e-05,
      "loss": 0.0013,
      "step": 66630
    },
    {
      "epoch": 5.923555555555556,
      "grad_norm": 0.2958800792694092,
      "learning_rate": 1.2977777777777777e-05,
      "loss": 0.0014,
      "step": 66640
    },
    {
      "epoch": 5.924444444444444,
      "grad_norm": 0.39957669377326965,
      "learning_rate": 1.2972222222222222e-05,
      "loss": 0.002,
      "step": 66650
    },
    {
      "epoch": 5.925333333333334,
      "grad_norm": 0.5685258507728577,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.002,
      "step": 66660
    },
    {
      "epoch": 5.926222222222222,
      "grad_norm": 0.03561960905790329,
      "learning_rate": 1.2961111111111113e-05,
      "loss": 0.0015,
      "step": 66670
    },
    {
      "epoch": 5.927111111111111,
      "grad_norm": 0.44813790917396545,
      "learning_rate": 1.2955555555555556e-05,
      "loss": 0.0015,
      "step": 66680
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.5492437481880188,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 0.002,
      "step": 66690
    },
    {
      "epoch": 5.928888888888888,
      "grad_norm": 0.4799206554889679,
      "learning_rate": 1.2944444444444445e-05,
      "loss": 0.0024,
      "step": 66700
    },
    {
      "epoch": 5.929777777777778,
      "grad_norm": 0.5647614002227783,
      "learning_rate": 1.2938888888888888e-05,
      "loss": 0.0023,
      "step": 66710
    },
    {
      "epoch": 5.930666666666666,
      "grad_norm": 0.620341956615448,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0029,
      "step": 66720
    },
    {
      "epoch": 5.931555555555556,
      "grad_norm": 0.4223427176475525,
      "learning_rate": 1.2927777777777777e-05,
      "loss": 0.0032,
      "step": 66730
    },
    {
      "epoch": 5.932444444444444,
      "grad_norm": 0.7517690658569336,
      "learning_rate": 1.2922222222222221e-05,
      "loss": 0.0025,
      "step": 66740
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.11990255117416382,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0021,
      "step": 66750
    },
    {
      "epoch": 5.934222222222222,
      "grad_norm": 0.19725288450717926,
      "learning_rate": 1.2911111111111113e-05,
      "loss": 0.0019,
      "step": 66760
    },
    {
      "epoch": 5.9351111111111114,
      "grad_norm": 0.32209452986717224,
      "learning_rate": 1.2905555555555557e-05,
      "loss": 0.0017,
      "step": 66770
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.3992266356945038,
      "learning_rate": 1.29e-05,
      "loss": 0.0028,
      "step": 66780
    },
    {
      "epoch": 5.936888888888889,
      "grad_norm": 0.08166433870792389,
      "learning_rate": 1.2894444444444445e-05,
      "loss": 0.0028,
      "step": 66790
    },
    {
      "epoch": 5.937777777777778,
      "grad_norm": 0.11904001235961914,
      "learning_rate": 1.2888888888888889e-05,
      "loss": 0.0013,
      "step": 66800
    },
    {
      "epoch": 5.938666666666666,
      "grad_norm": 0.21919965744018555,
      "learning_rate": 1.2883333333333333e-05,
      "loss": 0.002,
      "step": 66810
    },
    {
      "epoch": 5.939555555555556,
      "grad_norm": 0.23896826803684235,
      "learning_rate": 1.2877777777777778e-05,
      "loss": 0.0023,
      "step": 66820
    },
    {
      "epoch": 5.940444444444444,
      "grad_norm": 0.5672979950904846,
      "learning_rate": 1.2872222222222221e-05,
      "loss": 0.0032,
      "step": 66830
    },
    {
      "epoch": 5.941333333333334,
      "grad_norm": 0.059400368481874466,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0019,
      "step": 66840
    },
    {
      "epoch": 5.942222222222222,
      "grad_norm": 0.33532220125198364,
      "learning_rate": 1.2861111111111112e-05,
      "loss": 0.0017,
      "step": 66850
    },
    {
      "epoch": 5.9431111111111115,
      "grad_norm": 0.45707160234451294,
      "learning_rate": 1.2855555555555557e-05,
      "loss": 0.0022,
      "step": 66860
    },
    {
      "epoch": 5.944,
      "grad_norm": 1.041011929512024,
      "learning_rate": 1.285e-05,
      "loss": 0.0014,
      "step": 66870
    },
    {
      "epoch": 5.9448888888888884,
      "grad_norm": 0.12785853445529938,
      "learning_rate": 1.2844444444444446e-05,
      "loss": 0.0015,
      "step": 66880
    },
    {
      "epoch": 5.945777777777778,
      "grad_norm": 0.41675758361816406,
      "learning_rate": 1.283888888888889e-05,
      "loss": 0.0017,
      "step": 66890
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 0.1568136364221573,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0028,
      "step": 66900
    },
    {
      "epoch": 5.947555555555556,
      "grad_norm": 0.23762229084968567,
      "learning_rate": 1.2827777777777778e-05,
      "loss": 0.0025,
      "step": 66910
    },
    {
      "epoch": 5.948444444444444,
      "grad_norm": 0.10740896314382553,
      "learning_rate": 1.2822222222222222e-05,
      "loss": 0.0013,
      "step": 66920
    },
    {
      "epoch": 5.949333333333334,
      "grad_norm": 0.26854997873306274,
      "learning_rate": 1.2816666666666669e-05,
      "loss": 0.0029,
      "step": 66930
    },
    {
      "epoch": 5.950222222222222,
      "grad_norm": 0.5158859491348267,
      "learning_rate": 1.2811111111111112e-05,
      "loss": 0.0018,
      "step": 66940
    },
    {
      "epoch": 5.9511111111111115,
      "grad_norm": 0.8335167765617371,
      "learning_rate": 1.2805555555555558e-05,
      "loss": 0.0017,
      "step": 66950
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.2476910650730133,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0029,
      "step": 66960
    },
    {
      "epoch": 5.952888888888889,
      "grad_norm": 0.6601358652114868,
      "learning_rate": 1.2794444444444445e-05,
      "loss": 0.0012,
      "step": 66970
    },
    {
      "epoch": 5.953777777777778,
      "grad_norm": 0.18474867939949036,
      "learning_rate": 1.278888888888889e-05,
      "loss": 0.002,
      "step": 66980
    },
    {
      "epoch": 5.954666666666666,
      "grad_norm": 0.1125236377120018,
      "learning_rate": 1.2783333333333333e-05,
      "loss": 0.0018,
      "step": 66990
    },
    {
      "epoch": 5.955555555555556,
      "grad_norm": 0.16466820240020752,
      "learning_rate": 1.2777777777777777e-05,
      "loss": 0.0021,
      "step": 67000
    },
    {
      "epoch": 5.956444444444444,
      "grad_norm": 0.09260114282369614,
      "learning_rate": 1.2772222222222222e-05,
      "loss": 0.0021,
      "step": 67010
    },
    {
      "epoch": 5.957333333333334,
      "grad_norm": 0.13255548477172852,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0015,
      "step": 67020
    },
    {
      "epoch": 5.958222222222222,
      "grad_norm": 0.18997082114219666,
      "learning_rate": 1.2761111111111113e-05,
      "loss": 0.0016,
      "step": 67030
    },
    {
      "epoch": 5.9591111111111115,
      "grad_norm": 0.37932291626930237,
      "learning_rate": 1.2755555555555556e-05,
      "loss": 0.0019,
      "step": 67040
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.24718037247657776,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.002,
      "step": 67050
    },
    {
      "epoch": 5.9608888888888885,
      "grad_norm": 0.43430808186531067,
      "learning_rate": 1.2744444444444445e-05,
      "loss": 0.0016,
      "step": 67060
    },
    {
      "epoch": 5.961777777777778,
      "grad_norm": 0.2200864553451538,
      "learning_rate": 1.2738888888888889e-05,
      "loss": 0.0023,
      "step": 67070
    },
    {
      "epoch": 5.962666666666666,
      "grad_norm": 0.4546527862548828,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0012,
      "step": 67080
    },
    {
      "epoch": 5.963555555555556,
      "grad_norm": 0.33174774050712585,
      "learning_rate": 1.2727777777777778e-05,
      "loss": 0.0014,
      "step": 67090
    },
    {
      "epoch": 5.964444444444444,
      "grad_norm": 0.5707431435585022,
      "learning_rate": 1.2722222222222221e-05,
      "loss": 0.0019,
      "step": 67100
    },
    {
      "epoch": 5.965333333333334,
      "grad_norm": 0.10593456029891968,
      "learning_rate": 1.2716666666666668e-05,
      "loss": 0.0018,
      "step": 67110
    },
    {
      "epoch": 5.966222222222222,
      "grad_norm": 0.5951894521713257,
      "learning_rate": 1.2711111111111113e-05,
      "loss": 0.0014,
      "step": 67120
    },
    {
      "epoch": 5.9671111111111115,
      "grad_norm": 0.32546964287757874,
      "learning_rate": 1.2705555555555557e-05,
      "loss": 0.0018,
      "step": 67130
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.5484262108802795,
      "learning_rate": 1.27e-05,
      "loss": 0.0015,
      "step": 67140
    },
    {
      "epoch": 5.968888888888889,
      "grad_norm": 0.7639986276626587,
      "learning_rate": 1.2694444444444446e-05,
      "loss": 0.002,
      "step": 67150
    },
    {
      "epoch": 5.969777777777778,
      "grad_norm": 0.4206622540950775,
      "learning_rate": 1.268888888888889e-05,
      "loss": 0.0017,
      "step": 67160
    },
    {
      "epoch": 5.970666666666666,
      "grad_norm": 0.40465474128723145,
      "learning_rate": 1.2683333333333333e-05,
      "loss": 0.0018,
      "step": 67170
    },
    {
      "epoch": 5.971555555555556,
      "grad_norm": 0.4989718496799469,
      "learning_rate": 1.2677777777777778e-05,
      "loss": 0.0018,
      "step": 67180
    },
    {
      "epoch": 5.972444444444444,
      "grad_norm": 0.4097800850868225,
      "learning_rate": 1.2672222222222225e-05,
      "loss": 0.0028,
      "step": 67190
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.22332996129989624,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0019,
      "step": 67200
    },
    {
      "epoch": 5.974222222222222,
      "grad_norm": 0.21442484855651855,
      "learning_rate": 1.2661111111111112e-05,
      "loss": 0.0018,
      "step": 67210
    },
    {
      "epoch": 5.9751111111111115,
      "grad_norm": 0.15785008668899536,
      "learning_rate": 1.2655555555555557e-05,
      "loss": 0.0025,
      "step": 67220
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.41860294342041016,
      "learning_rate": 1.2650000000000001e-05,
      "loss": 0.0019,
      "step": 67230
    },
    {
      "epoch": 5.9768888888888885,
      "grad_norm": 0.33976325392723083,
      "learning_rate": 1.2644444444444444e-05,
      "loss": 0.0025,
      "step": 67240
    },
    {
      "epoch": 5.977777777777778,
      "grad_norm": 0.7273333072662354,
      "learning_rate": 1.263888888888889e-05,
      "loss": 0.0018,
      "step": 67250
    },
    {
      "epoch": 5.978666666666666,
      "grad_norm": 0.34422051906585693,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.0019,
      "step": 67260
    },
    {
      "epoch": 5.979555555555556,
      "grad_norm": 0.11703658849000931,
      "learning_rate": 1.2627777777777777e-05,
      "loss": 0.0013,
      "step": 67270
    },
    {
      "epoch": 5.980444444444444,
      "grad_norm": 0.5741568207740784,
      "learning_rate": 1.2622222222222224e-05,
      "loss": 0.002,
      "step": 67280
    },
    {
      "epoch": 5.981333333333334,
      "grad_norm": 0.6821024417877197,
      "learning_rate": 1.2616666666666669e-05,
      "loss": 0.0018,
      "step": 67290
    },
    {
      "epoch": 5.982222222222222,
      "grad_norm": 0.8675774335861206,
      "learning_rate": 1.2611111111111113e-05,
      "loss": 0.0019,
      "step": 67300
    },
    {
      "epoch": 5.9831111111111115,
      "grad_norm": 0.1252027153968811,
      "learning_rate": 1.2605555555555556e-05,
      "loss": 0.0017,
      "step": 67310
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.18499229848384857,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.002,
      "step": 67320
    },
    {
      "epoch": 5.984888888888889,
      "grad_norm": 0.2970111668109894,
      "learning_rate": 1.2594444444444445e-05,
      "loss": 0.0014,
      "step": 67330
    },
    {
      "epoch": 5.985777777777778,
      "grad_norm": 0.05645236000418663,
      "learning_rate": 1.2588888888888888e-05,
      "loss": 0.0013,
      "step": 67340
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 0.18532420694828033,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.0016,
      "step": 67350
    },
    {
      "epoch": 5.987555555555556,
      "grad_norm": 0.04454527050256729,
      "learning_rate": 1.2577777777777777e-05,
      "loss": 0.0013,
      "step": 67360
    },
    {
      "epoch": 5.988444444444444,
      "grad_norm": 0.3626333177089691,
      "learning_rate": 1.2572222222222224e-05,
      "loss": 0.0024,
      "step": 67370
    },
    {
      "epoch": 5.989333333333334,
      "grad_norm": 0.19273653626441956,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0014,
      "step": 67380
    },
    {
      "epoch": 5.990222222222222,
      "grad_norm": 0.1502094268798828,
      "learning_rate": 1.2561111111111113e-05,
      "loss": 0.0015,
      "step": 67390
    },
    {
      "epoch": 5.9911111111111115,
      "grad_norm": 0.7494476437568665,
      "learning_rate": 1.2555555555555557e-05,
      "loss": 0.0017,
      "step": 67400
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.6646739840507507,
      "learning_rate": 1.255e-05,
      "loss": 0.0013,
      "step": 67410
    },
    {
      "epoch": 5.9928888888888885,
      "grad_norm": 0.12328323721885681,
      "learning_rate": 1.2544444444444445e-05,
      "loss": 0.0016,
      "step": 67420
    },
    {
      "epoch": 5.993777777777778,
      "grad_norm": 0.2526659667491913,
      "learning_rate": 1.2538888888888889e-05,
      "loss": 0.0031,
      "step": 67430
    },
    {
      "epoch": 5.994666666666666,
      "grad_norm": 0.47364407777786255,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0018,
      "step": 67440
    },
    {
      "epoch": 5.995555555555556,
      "grad_norm": 0.23526781797409058,
      "learning_rate": 1.2527777777777778e-05,
      "loss": 0.0015,
      "step": 67450
    },
    {
      "epoch": 5.996444444444444,
      "grad_norm": 0.09294980764389038,
      "learning_rate": 1.2522222222222225e-05,
      "loss": 0.0014,
      "step": 67460
    },
    {
      "epoch": 5.997333333333334,
      "grad_norm": 0.7121139764785767,
      "learning_rate": 1.2516666666666668e-05,
      "loss": 0.0015,
      "step": 67470
    },
    {
      "epoch": 5.998222222222222,
      "grad_norm": 0.5042707920074463,
      "learning_rate": 1.2511111111111112e-05,
      "loss": 0.0021,
      "step": 67480
    },
    {
      "epoch": 5.999111111111111,
      "grad_norm": 0.8090952634811401,
      "learning_rate": 1.2505555555555557e-05,
      "loss": 0.0019,
      "step": 67490
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.3028709590435028,
      "learning_rate": 1.25e-05,
      "loss": 0.0022,
      "step": 67500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.0020108153112232685,
      "eval_runtime": 100.6887,
      "eval_samples_per_second": 1489.741,
      "eval_steps_per_second": 37.244,
      "step": 67500
    },
    {
      "epoch": 6.0008888888888885,
      "grad_norm": 0.4729565680027008,
      "learning_rate": 1.2494444444444444e-05,
      "loss": 0.0024,
      "step": 67510
    },
    {
      "epoch": 6.001777777777778,
      "grad_norm": 0.8049489855766296,
      "learning_rate": 1.248888888888889e-05,
      "loss": 0.0015,
      "step": 67520
    },
    {
      "epoch": 6.002666666666666,
      "grad_norm": 0.46995797753334045,
      "learning_rate": 1.2483333333333335e-05,
      "loss": 0.0027,
      "step": 67530
    },
    {
      "epoch": 6.003555555555556,
      "grad_norm": 0.020832939073443413,
      "learning_rate": 1.2477777777777778e-05,
      "loss": 0.0024,
      "step": 67540
    },
    {
      "epoch": 6.004444444444444,
      "grad_norm": 0.25781679153442383,
      "learning_rate": 1.2472222222222223e-05,
      "loss": 0.0027,
      "step": 67550
    },
    {
      "epoch": 6.005333333333334,
      "grad_norm": 0.22286035120487213,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0018,
      "step": 67560
    },
    {
      "epoch": 6.006222222222222,
      "grad_norm": 0.08771196007728577,
      "learning_rate": 1.2461111111111112e-05,
      "loss": 0.0018,
      "step": 67570
    },
    {
      "epoch": 6.0071111111111115,
      "grad_norm": 0.36315056681632996,
      "learning_rate": 1.2455555555555556e-05,
      "loss": 0.0019,
      "step": 67580
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.4043940603733063,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 0.0019,
      "step": 67590
    },
    {
      "epoch": 6.0088888888888885,
      "grad_norm": 0.21730206906795502,
      "learning_rate": 1.2444444444444445e-05,
      "loss": 0.0018,
      "step": 67600
    },
    {
      "epoch": 6.009777777777778,
      "grad_norm": 0.22871965169906616,
      "learning_rate": 1.2438888888888888e-05,
      "loss": 0.0017,
      "step": 67610
    },
    {
      "epoch": 6.010666666666666,
      "grad_norm": 0.4152054786682129,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0029,
      "step": 67620
    },
    {
      "epoch": 6.011555555555556,
      "grad_norm": 0.14805644750595093,
      "learning_rate": 1.2427777777777779e-05,
      "loss": 0.0016,
      "step": 67630
    },
    {
      "epoch": 6.012444444444444,
      "grad_norm": 0.27053746581077576,
      "learning_rate": 1.2422222222222222e-05,
      "loss": 0.0025,
      "step": 67640
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 0.18738114833831787,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.0016,
      "step": 67650
    },
    {
      "epoch": 6.014222222222222,
      "grad_norm": 0.21902324259281158,
      "learning_rate": 1.2411111111111113e-05,
      "loss": 0.0034,
      "step": 67660
    },
    {
      "epoch": 6.0151111111111115,
      "grad_norm": 0.05377552658319473,
      "learning_rate": 1.2405555555555556e-05,
      "loss": 0.0025,
      "step": 67670
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.09520351886749268,
      "learning_rate": 1.24e-05,
      "loss": 0.0015,
      "step": 67680
    },
    {
      "epoch": 6.0168888888888885,
      "grad_norm": 0.05275602638721466,
      "learning_rate": 1.2394444444444445e-05,
      "loss": 0.0015,
      "step": 67690
    },
    {
      "epoch": 6.017777777777778,
      "grad_norm": 0.2640810012817383,
      "learning_rate": 1.238888888888889e-05,
      "loss": 0.0014,
      "step": 67700
    },
    {
      "epoch": 6.018666666666666,
      "grad_norm": 0.5794054269790649,
      "learning_rate": 1.2383333333333334e-05,
      "loss": 0.0018,
      "step": 67710
    },
    {
      "epoch": 6.019555555555556,
      "grad_norm": 0.06173768267035484,
      "learning_rate": 1.237777777777778e-05,
      "loss": 0.0018,
      "step": 67720
    },
    {
      "epoch": 6.020444444444444,
      "grad_norm": 0.24685534834861755,
      "learning_rate": 1.2372222222222223e-05,
      "loss": 0.0024,
      "step": 67730
    },
    {
      "epoch": 6.021333333333334,
      "grad_norm": 0.08881892263889313,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0019,
      "step": 67740
    },
    {
      "epoch": 6.022222222222222,
      "grad_norm": 0.29644501209259033,
      "learning_rate": 1.2361111111111112e-05,
      "loss": 0.0014,
      "step": 67750
    },
    {
      "epoch": 6.0231111111111115,
      "grad_norm": 0.21786965429782867,
      "learning_rate": 1.2355555555555557e-05,
      "loss": 0.0022,
      "step": 67760
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.29459694027900696,
      "learning_rate": 1.235e-05,
      "loss": 0.0021,
      "step": 67770
    },
    {
      "epoch": 6.0248888888888885,
      "grad_norm": 0.2915055453777313,
      "learning_rate": 1.2344444444444444e-05,
      "loss": 0.0026,
      "step": 67780
    },
    {
      "epoch": 6.025777777777778,
      "grad_norm": 0.3250539004802704,
      "learning_rate": 1.233888888888889e-05,
      "loss": 0.002,
      "step": 67790
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.14655040204524994,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0015,
      "step": 67800
    },
    {
      "epoch": 6.027555555555556,
      "grad_norm": 0.7575530409812927,
      "learning_rate": 1.2327777777777778e-05,
      "loss": 0.0015,
      "step": 67810
    },
    {
      "epoch": 6.028444444444444,
      "grad_norm": 0.4591287076473236,
      "learning_rate": 1.2322222222222223e-05,
      "loss": 0.0031,
      "step": 67820
    },
    {
      "epoch": 6.029333333333334,
      "grad_norm": 0.545335054397583,
      "learning_rate": 1.2316666666666667e-05,
      "loss": 0.0014,
      "step": 67830
    },
    {
      "epoch": 6.030222222222222,
      "grad_norm": 0.5467392802238464,
      "learning_rate": 1.2311111111111112e-05,
      "loss": 0.0014,
      "step": 67840
    },
    {
      "epoch": 6.0311111111111115,
      "grad_norm": 0.2550640404224396,
      "learning_rate": 1.2305555555555556e-05,
      "loss": 0.0019,
      "step": 67850
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.05966714024543762,
      "learning_rate": 1.23e-05,
      "loss": 0.0021,
      "step": 67860
    },
    {
      "epoch": 6.0328888888888885,
      "grad_norm": 0.42131078243255615,
      "learning_rate": 1.2294444444444444e-05,
      "loss": 0.0019,
      "step": 67870
    },
    {
      "epoch": 6.033777777777778,
      "grad_norm": 0.490631639957428,
      "learning_rate": 1.228888888888889e-05,
      "loss": 0.0027,
      "step": 67880
    },
    {
      "epoch": 6.034666666666666,
      "grad_norm": 0.6239032745361328,
      "learning_rate": 1.2283333333333335e-05,
      "loss": 0.0029,
      "step": 67890
    },
    {
      "epoch": 6.035555555555556,
      "grad_norm": 0.2917988896369934,
      "learning_rate": 1.2277777777777778e-05,
      "loss": 0.0028,
      "step": 67900
    },
    {
      "epoch": 6.036444444444444,
      "grad_norm": 0.2912641167640686,
      "learning_rate": 1.2272222222222222e-05,
      "loss": 0.0014,
      "step": 67910
    },
    {
      "epoch": 6.037333333333334,
      "grad_norm": 0.44756004214286804,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0017,
      "step": 67920
    },
    {
      "epoch": 6.038222222222222,
      "grad_norm": 0.29233279824256897,
      "learning_rate": 1.2261111111111112e-05,
      "loss": 0.0023,
      "step": 67930
    },
    {
      "epoch": 6.0391111111111115,
      "grad_norm": 0.5607393383979797,
      "learning_rate": 1.2255555555555556e-05,
      "loss": 0.0024,
      "step": 67940
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.052612513303756714,
      "learning_rate": 1.225e-05,
      "loss": 0.0016,
      "step": 67950
    },
    {
      "epoch": 6.0408888888888885,
      "grad_norm": 0.25954416394233704,
      "learning_rate": 1.2244444444444445e-05,
      "loss": 0.0021,
      "step": 67960
    },
    {
      "epoch": 6.041777777777778,
      "grad_norm": 0.33548614382743835,
      "learning_rate": 1.223888888888889e-05,
      "loss": 0.0018,
      "step": 67970
    },
    {
      "epoch": 6.042666666666666,
      "grad_norm": 0.4644722640514374,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0014,
      "step": 67980
    },
    {
      "epoch": 6.043555555555556,
      "grad_norm": 0.05933312326669693,
      "learning_rate": 1.2227777777777779e-05,
      "loss": 0.0018,
      "step": 67990
    },
    {
      "epoch": 6.044444444444444,
      "grad_norm": 0.05490602180361748,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 0.0017,
      "step": 68000
    },
    {
      "epoch": 6.045333333333334,
      "grad_norm": 0.06197187677025795,
      "learning_rate": 1.2216666666666668e-05,
      "loss": 0.0018,
      "step": 68010
    },
    {
      "epoch": 6.046222222222222,
      "grad_norm": 0.5647609829902649,
      "learning_rate": 1.2211111111111111e-05,
      "loss": 0.004,
      "step": 68020
    },
    {
      "epoch": 6.0471111111111115,
      "grad_norm": 0.5102171897888184,
      "learning_rate": 1.2205555555555557e-05,
      "loss": 0.0015,
      "step": 68030
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.29577478766441345,
      "learning_rate": 1.22e-05,
      "loss": 0.0013,
      "step": 68040
    },
    {
      "epoch": 6.0488888888888885,
      "grad_norm": 0.43372654914855957,
      "learning_rate": 1.2194444444444444e-05,
      "loss": 0.0018,
      "step": 68050
    },
    {
      "epoch": 6.049777777777778,
      "grad_norm": 0.08847455680370331,
      "learning_rate": 1.218888888888889e-05,
      "loss": 0.0017,
      "step": 68060
    },
    {
      "epoch": 6.050666666666666,
      "grad_norm": 0.11544638872146606,
      "learning_rate": 1.2183333333333334e-05,
      "loss": 0.0017,
      "step": 68070
    },
    {
      "epoch": 6.051555555555556,
      "grad_norm": 0.2992684841156006,
      "learning_rate": 1.2177777777777778e-05,
      "loss": 0.0021,
      "step": 68080
    },
    {
      "epoch": 6.052444444444444,
      "grad_norm": 0.30227524042129517,
      "learning_rate": 1.2172222222222223e-05,
      "loss": 0.0015,
      "step": 68090
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 0.06077493354678154,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0018,
      "step": 68100
    },
    {
      "epoch": 6.054222222222222,
      "grad_norm": 0.3647405505180359,
      "learning_rate": 1.2161111111111112e-05,
      "loss": 0.0028,
      "step": 68110
    },
    {
      "epoch": 6.0551111111111116,
      "grad_norm": 0.08798196166753769,
      "learning_rate": 1.2155555555555555e-05,
      "loss": 0.0018,
      "step": 68120
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.056703343987464905,
      "learning_rate": 1.215e-05,
      "loss": 0.0016,
      "step": 68130
    },
    {
      "epoch": 6.0568888888888885,
      "grad_norm": 0.928317666053772,
      "learning_rate": 1.2144444444444444e-05,
      "loss": 0.0032,
      "step": 68140
    },
    {
      "epoch": 6.057777777777778,
      "grad_norm": 0.5522840619087219,
      "learning_rate": 1.213888888888889e-05,
      "loss": 0.0024,
      "step": 68150
    },
    {
      "epoch": 6.058666666666666,
      "grad_norm": 0.07544092833995819,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0017,
      "step": 68160
    },
    {
      "epoch": 6.059555555555556,
      "grad_norm": 0.5898310542106628,
      "learning_rate": 1.2127777777777778e-05,
      "loss": 0.0025,
      "step": 68170
    },
    {
      "epoch": 6.060444444444444,
      "grad_norm": 0.438975065946579,
      "learning_rate": 1.2122222222222222e-05,
      "loss": 0.0017,
      "step": 68180
    },
    {
      "epoch": 6.061333333333334,
      "grad_norm": 0.28759488463401794,
      "learning_rate": 1.2116666666666669e-05,
      "loss": 0.0019,
      "step": 68190
    },
    {
      "epoch": 6.062222222222222,
      "grad_norm": 0.14685282111167908,
      "learning_rate": 1.2111111111111112e-05,
      "loss": 0.0015,
      "step": 68200
    },
    {
      "epoch": 6.063111111111111,
      "grad_norm": 0.43270066380500793,
      "learning_rate": 1.2105555555555556e-05,
      "loss": 0.0017,
      "step": 68210
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.5493742227554321,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.0024,
      "step": 68220
    },
    {
      "epoch": 6.0648888888888886,
      "grad_norm": 0.23067036271095276,
      "learning_rate": 1.2094444444444445e-05,
      "loss": 0.0015,
      "step": 68230
    },
    {
      "epoch": 6.065777777777778,
      "grad_norm": 0.5493888258934021,
      "learning_rate": 1.208888888888889e-05,
      "loss": 0.0013,
      "step": 68240
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.18418501317501068,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.0027,
      "step": 68250
    },
    {
      "epoch": 6.067555555555556,
      "grad_norm": 0.08343919366598129,
      "learning_rate": 1.2077777777777779e-05,
      "loss": 0.0018,
      "step": 68260
    },
    {
      "epoch": 6.068444444444444,
      "grad_norm": 0.2863238751888275,
      "learning_rate": 1.2072222222222222e-05,
      "loss": 0.0022,
      "step": 68270
    },
    {
      "epoch": 6.069333333333334,
      "grad_norm": 0.30351248383522034,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0014,
      "step": 68280
    },
    {
      "epoch": 6.070222222222222,
      "grad_norm": 0.9959118366241455,
      "learning_rate": 1.2061111111111113e-05,
      "loss": 0.0019,
      "step": 68290
    },
    {
      "epoch": 6.071111111111111,
      "grad_norm": 0.22012314200401306,
      "learning_rate": 1.2055555555555556e-05,
      "loss": 0.0017,
      "step": 68300
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.1715250164270401,
      "learning_rate": 1.205e-05,
      "loss": 0.0019,
      "step": 68310
    },
    {
      "epoch": 6.072888888888889,
      "grad_norm": 0.40190866589546204,
      "learning_rate": 1.2044444444444445e-05,
      "loss": 0.0019,
      "step": 68320
    },
    {
      "epoch": 6.073777777777778,
      "grad_norm": 0.39901068806648254,
      "learning_rate": 1.203888888888889e-05,
      "loss": 0.0022,
      "step": 68330
    },
    {
      "epoch": 6.074666666666666,
      "grad_norm": 0.46458497643470764,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0023,
      "step": 68340
    },
    {
      "epoch": 6.075555555555556,
      "grad_norm": 0.34148022532463074,
      "learning_rate": 1.2027777777777777e-05,
      "loss": 0.002,
      "step": 68350
    },
    {
      "epoch": 6.076444444444444,
      "grad_norm": 0.4011504352092743,
      "learning_rate": 1.2022222222222223e-05,
      "loss": 0.0023,
      "step": 68360
    },
    {
      "epoch": 6.077333333333334,
      "grad_norm": 0.39019638299942017,
      "learning_rate": 1.2016666666666668e-05,
      "loss": 0.0023,
      "step": 68370
    },
    {
      "epoch": 6.078222222222222,
      "grad_norm": 0.4369036853313446,
      "learning_rate": 1.2011111111111111e-05,
      "loss": 0.0014,
      "step": 68380
    },
    {
      "epoch": 6.079111111111111,
      "grad_norm": 0.799985945224762,
      "learning_rate": 1.2005555555555557e-05,
      "loss": 0.0016,
      "step": 68390
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.0737600326538086,
      "learning_rate": 1.2e-05,
      "loss": 0.0028,
      "step": 68400
    },
    {
      "epoch": 6.080888888888889,
      "grad_norm": 0.36569225788116455,
      "learning_rate": 1.1994444444444446e-05,
      "loss": 0.0014,
      "step": 68410
    },
    {
      "epoch": 6.081777777777778,
      "grad_norm": 0.6533135175704956,
      "learning_rate": 1.1988888888888889e-05,
      "loss": 0.002,
      "step": 68420
    },
    {
      "epoch": 6.082666666666666,
      "grad_norm": 0.15361574292182922,
      "learning_rate": 1.1983333333333334e-05,
      "loss": 0.0015,
      "step": 68430
    },
    {
      "epoch": 6.083555555555556,
      "grad_norm": 0.11880876123905182,
      "learning_rate": 1.1977777777777778e-05,
      "loss": 0.0018,
      "step": 68440
    },
    {
      "epoch": 6.084444444444444,
      "grad_norm": 0.3253580331802368,
      "learning_rate": 1.1972222222222221e-05,
      "loss": 0.0021,
      "step": 68450
    },
    {
      "epoch": 6.085333333333334,
      "grad_norm": 0.2724774479866028,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0015,
      "step": 68460
    },
    {
      "epoch": 6.086222222222222,
      "grad_norm": 0.5450320839881897,
      "learning_rate": 1.1961111111111112e-05,
      "loss": 0.002,
      "step": 68470
    },
    {
      "epoch": 6.087111111111111,
      "grad_norm": 0.21694591641426086,
      "learning_rate": 1.1955555555555556e-05,
      "loss": 0.002,
      "step": 68480
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.40805789828300476,
      "learning_rate": 1.195e-05,
      "loss": 0.0021,
      "step": 68490
    },
    {
      "epoch": 6.088888888888889,
      "grad_norm": 0.11350075900554657,
      "learning_rate": 1.1944444444444446e-05,
      "loss": 0.002,
      "step": 68500
    },
    {
      "epoch": 6.089777777777778,
      "grad_norm": 0.08368644118309021,
      "learning_rate": 1.193888888888889e-05,
      "loss": 0.0017,
      "step": 68510
    },
    {
      "epoch": 6.0906666666666665,
      "grad_norm": 0.5098209381103516,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0024,
      "step": 68520
    },
    {
      "epoch": 6.091555555555556,
      "grad_norm": 0.3690873086452484,
      "learning_rate": 1.1927777777777778e-05,
      "loss": 0.0015,
      "step": 68530
    },
    {
      "epoch": 6.092444444444444,
      "grad_norm": 0.36802491545677185,
      "learning_rate": 1.1922222222222222e-05,
      "loss": 0.0017,
      "step": 68540
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 0.4362567663192749,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.0027,
      "step": 68550
    },
    {
      "epoch": 6.094222222222222,
      "grad_norm": 0.2609122395515442,
      "learning_rate": 1.1911111111111112e-05,
      "loss": 0.0019,
      "step": 68560
    },
    {
      "epoch": 6.095111111111111,
      "grad_norm": 0.6495662331581116,
      "learning_rate": 1.1905555555555556e-05,
      "loss": 0.0018,
      "step": 68570
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.16414713859558105,
      "learning_rate": 1.19e-05,
      "loss": 0.002,
      "step": 68580
    },
    {
      "epoch": 6.096888888888889,
      "grad_norm": 0.21238212287425995,
      "learning_rate": 1.1894444444444445e-05,
      "loss": 0.0017,
      "step": 68590
    },
    {
      "epoch": 6.097777777777778,
      "grad_norm": 0.24808818101882935,
      "learning_rate": 1.188888888888889e-05,
      "loss": 0.0023,
      "step": 68600
    },
    {
      "epoch": 6.0986666666666665,
      "grad_norm": 0.18146516382694244,
      "learning_rate": 1.1883333333333334e-05,
      "loss": 0.0028,
      "step": 68610
    },
    {
      "epoch": 6.099555555555556,
      "grad_norm": 0.0950508713722229,
      "learning_rate": 1.1877777777777777e-05,
      "loss": 0.0014,
      "step": 68620
    },
    {
      "epoch": 6.100444444444444,
      "grad_norm": 0.4404328465461731,
      "learning_rate": 1.1872222222222224e-05,
      "loss": 0.002,
      "step": 68630
    },
    {
      "epoch": 6.101333333333334,
      "grad_norm": 0.07835857570171356,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0018,
      "step": 68640
    },
    {
      "epoch": 6.102222222222222,
      "grad_norm": 0.136447012424469,
      "learning_rate": 1.1861111111111111e-05,
      "loss": 0.0025,
      "step": 68650
    },
    {
      "epoch": 6.103111111111111,
      "grad_norm": 0.04429495707154274,
      "learning_rate": 1.1855555555555556e-05,
      "loss": 0.0023,
      "step": 68660
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.10893785208463669,
      "learning_rate": 1.185e-05,
      "loss": 0.0019,
      "step": 68670
    },
    {
      "epoch": 6.104888888888889,
      "grad_norm": 0.3264598548412323,
      "learning_rate": 1.1844444444444445e-05,
      "loss": 0.0017,
      "step": 68680
    },
    {
      "epoch": 6.105777777777778,
      "grad_norm": 0.3042374551296234,
      "learning_rate": 1.1838888888888889e-05,
      "loss": 0.0015,
      "step": 68690
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 0.18846029043197632,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.002,
      "step": 68700
    },
    {
      "epoch": 6.107555555555556,
      "grad_norm": 0.41565316915512085,
      "learning_rate": 1.1827777777777778e-05,
      "loss": 0.0015,
      "step": 68710
    },
    {
      "epoch": 6.108444444444444,
      "grad_norm": 0.2625901699066162,
      "learning_rate": 1.1822222222222223e-05,
      "loss": 0.0021,
      "step": 68720
    },
    {
      "epoch": 6.109333333333334,
      "grad_norm": 0.18890953063964844,
      "learning_rate": 1.1816666666666668e-05,
      "loss": 0.0022,
      "step": 68730
    },
    {
      "epoch": 6.110222222222222,
      "grad_norm": 0.20803099870681763,
      "learning_rate": 1.1811111111111112e-05,
      "loss": 0.0021,
      "step": 68740
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 0.11010712385177612,
      "learning_rate": 1.1805555555555555e-05,
      "loss": 0.0019,
      "step": 68750
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.3976399600505829,
      "learning_rate": 1.18e-05,
      "loss": 0.0013,
      "step": 68760
    },
    {
      "epoch": 6.112888888888889,
      "grad_norm": 0.23340541124343872,
      "learning_rate": 1.1794444444444446e-05,
      "loss": 0.0013,
      "step": 68770
    },
    {
      "epoch": 6.113777777777778,
      "grad_norm": 0.27878493070602417,
      "learning_rate": 1.178888888888889e-05,
      "loss": 0.0017,
      "step": 68780
    },
    {
      "epoch": 6.1146666666666665,
      "grad_norm": 0.11390721797943115,
      "learning_rate": 1.1783333333333333e-05,
      "loss": 0.002,
      "step": 68790
    },
    {
      "epoch": 6.115555555555556,
      "grad_norm": 0.027841342613101006,
      "learning_rate": 1.1777777777777778e-05,
      "loss": 0.0018,
      "step": 68800
    },
    {
      "epoch": 6.116444444444444,
      "grad_norm": 0.5569970607757568,
      "learning_rate": 1.1772222222222223e-05,
      "loss": 0.0018,
      "step": 68810
    },
    {
      "epoch": 6.117333333333334,
      "grad_norm": 0.7843533754348755,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0024,
      "step": 68820
    },
    {
      "epoch": 6.118222222222222,
      "grad_norm": 0.27072954177856445,
      "learning_rate": 1.1761111111111112e-05,
      "loss": 0.0024,
      "step": 68830
    },
    {
      "epoch": 6.119111111111111,
      "grad_norm": 0.7924778461456299,
      "learning_rate": 1.1755555555555556e-05,
      "loss": 0.0015,
      "step": 68840
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.11508253961801529,
      "learning_rate": 1.175e-05,
      "loss": 0.0028,
      "step": 68850
    },
    {
      "epoch": 6.120888888888889,
      "grad_norm": 0.470992773771286,
      "learning_rate": 1.1744444444444446e-05,
      "loss": 0.0018,
      "step": 68860
    },
    {
      "epoch": 6.121777777777778,
      "grad_norm": 0.7169849872589111,
      "learning_rate": 1.173888888888889e-05,
      "loss": 0.0017,
      "step": 68870
    },
    {
      "epoch": 6.1226666666666665,
      "grad_norm": 0.587701141834259,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0021,
      "step": 68880
    },
    {
      "epoch": 6.123555555555556,
      "grad_norm": 0.10815446823835373,
      "learning_rate": 1.1727777777777779e-05,
      "loss": 0.0021,
      "step": 68890
    },
    {
      "epoch": 6.124444444444444,
      "grad_norm": 0.13171860575675964,
      "learning_rate": 1.1722222222222224e-05,
      "loss": 0.0015,
      "step": 68900
    },
    {
      "epoch": 6.125333333333334,
      "grad_norm": 0.3321918249130249,
      "learning_rate": 1.1716666666666667e-05,
      "loss": 0.0016,
      "step": 68910
    },
    {
      "epoch": 6.126222222222222,
      "grad_norm": 0.4592725336551666,
      "learning_rate": 1.1711111111111111e-05,
      "loss": 0.0026,
      "step": 68920
    },
    {
      "epoch": 6.127111111111111,
      "grad_norm": 0.07642427831888199,
      "learning_rate": 1.1705555555555556e-05,
      "loss": 0.0011,
      "step": 68930
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.11610351502895355,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.0022,
      "step": 68940
    },
    {
      "epoch": 6.128888888888889,
      "grad_norm": 0.28739070892333984,
      "learning_rate": 1.1694444444444445e-05,
      "loss": 0.003,
      "step": 68950
    },
    {
      "epoch": 6.129777777777778,
      "grad_norm": 0.20388971269130707,
      "learning_rate": 1.168888888888889e-05,
      "loss": 0.0025,
      "step": 68960
    },
    {
      "epoch": 6.1306666666666665,
      "grad_norm": 0.1968165934085846,
      "learning_rate": 1.1683333333333334e-05,
      "loss": 0.0022,
      "step": 68970
    },
    {
      "epoch": 6.131555555555556,
      "grad_norm": 0.29396313428878784,
      "learning_rate": 1.1677777777777777e-05,
      "loss": 0.0019,
      "step": 68980
    },
    {
      "epoch": 6.132444444444444,
      "grad_norm": 0.7839969992637634,
      "learning_rate": 1.1672222222222223e-05,
      "loss": 0.0014,
      "step": 68990
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.1035328134894371,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0019,
      "step": 69000
    },
    {
      "epoch": 6.134222222222222,
      "grad_norm": 0.22816994786262512,
      "learning_rate": 1.1661111111111111e-05,
      "loss": 0.0015,
      "step": 69010
    },
    {
      "epoch": 6.135111111111111,
      "grad_norm": 0.2337532490491867,
      "learning_rate": 1.1655555555555555e-05,
      "loss": 0.0022,
      "step": 69020
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.0715550109744072,
      "learning_rate": 1.1650000000000002e-05,
      "loss": 0.0023,
      "step": 69030
    },
    {
      "epoch": 6.136888888888889,
      "grad_norm": 0.2368939071893692,
      "learning_rate": 1.1644444444444446e-05,
      "loss": 0.0018,
      "step": 69040
    },
    {
      "epoch": 6.137777777777778,
      "grad_norm": 0.12465821206569672,
      "learning_rate": 1.1638888888888889e-05,
      "loss": 0.0024,
      "step": 69050
    },
    {
      "epoch": 6.1386666666666665,
      "grad_norm": 0.04410265386104584,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0017,
      "step": 69060
    },
    {
      "epoch": 6.139555555555556,
      "grad_norm": 0.835864245891571,
      "learning_rate": 1.1627777777777778e-05,
      "loss": 0.0017,
      "step": 69070
    },
    {
      "epoch": 6.140444444444444,
      "grad_norm": 0.16096794605255127,
      "learning_rate": 1.1622222222222223e-05,
      "loss": 0.0023,
      "step": 69080
    },
    {
      "epoch": 6.141333333333334,
      "grad_norm": 0.22958698868751526,
      "learning_rate": 1.1616666666666667e-05,
      "loss": 0.0018,
      "step": 69090
    },
    {
      "epoch": 6.142222222222222,
      "grad_norm": 0.15245486795902252,
      "learning_rate": 1.1611111111111112e-05,
      "loss": 0.0032,
      "step": 69100
    },
    {
      "epoch": 6.143111111111111,
      "grad_norm": 0.45416855812072754,
      "learning_rate": 1.1605555555555555e-05,
      "loss": 0.0018,
      "step": 69110
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.5639573931694031,
      "learning_rate": 1.16e-05,
      "loss": 0.0026,
      "step": 69120
    },
    {
      "epoch": 6.144888888888889,
      "grad_norm": 0.08749515563249588,
      "learning_rate": 1.1594444444444446e-05,
      "loss": 0.0017,
      "step": 69130
    },
    {
      "epoch": 6.145777777777778,
      "grad_norm": 0.18481898307800293,
      "learning_rate": 1.158888888888889e-05,
      "loss": 0.0022,
      "step": 69140
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 0.23256677389144897,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.0029,
      "step": 69150
    },
    {
      "epoch": 6.147555555555556,
      "grad_norm": 0.8299806714057922,
      "learning_rate": 1.1577777777777778e-05,
      "loss": 0.0017,
      "step": 69160
    },
    {
      "epoch": 6.148444444444444,
      "grad_norm": 0.25560736656188965,
      "learning_rate": 1.1572222222222224e-05,
      "loss": 0.0017,
      "step": 69170
    },
    {
      "epoch": 6.149333333333334,
      "grad_norm": 0.337350457906723,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.0022,
      "step": 69180
    },
    {
      "epoch": 6.150222222222222,
      "grad_norm": 0.3708324730396271,
      "learning_rate": 1.156111111111111e-05,
      "loss": 0.0034,
      "step": 69190
    },
    {
      "epoch": 6.151111111111111,
      "grad_norm": 0.6202163100242615,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 0.0017,
      "step": 69200
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.36238494515419006,
      "learning_rate": 1.1550000000000001e-05,
      "loss": 0.0022,
      "step": 69210
    },
    {
      "epoch": 6.152888888888889,
      "grad_norm": 0.084883913397789,
      "learning_rate": 1.1544444444444445e-05,
      "loss": 0.002,
      "step": 69220
    },
    {
      "epoch": 6.153777777777778,
      "grad_norm": 0.5324185490608215,
      "learning_rate": 1.153888888888889e-05,
      "loss": 0.0017,
      "step": 69230
    },
    {
      "epoch": 6.1546666666666665,
      "grad_norm": 0.12129592150449753,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0018,
      "step": 69240
    },
    {
      "epoch": 6.155555555555556,
      "grad_norm": 0.30254682898521423,
      "learning_rate": 1.1527777777777779e-05,
      "loss": 0.0023,
      "step": 69250
    },
    {
      "epoch": 6.156444444444444,
      "grad_norm": 0.6308673620223999,
      "learning_rate": 1.1522222222222222e-05,
      "loss": 0.0035,
      "step": 69260
    },
    {
      "epoch": 6.157333333333334,
      "grad_norm": 0.5361896753311157,
      "learning_rate": 1.1516666666666668e-05,
      "loss": 0.0016,
      "step": 69270
    },
    {
      "epoch": 6.158222222222222,
      "grad_norm": 0.05086739361286163,
      "learning_rate": 1.1511111111111111e-05,
      "loss": 0.0014,
      "step": 69280
    },
    {
      "epoch": 6.159111111111111,
      "grad_norm": 0.08609174191951752,
      "learning_rate": 1.1505555555555555e-05,
      "loss": 0.0019,
      "step": 69290
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.11626479774713516,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0018,
      "step": 69300
    },
    {
      "epoch": 6.160888888888889,
      "grad_norm": 0.26233676075935364,
      "learning_rate": 1.1494444444444445e-05,
      "loss": 0.0015,
      "step": 69310
    },
    {
      "epoch": 6.161777777777778,
      "grad_norm": 0.2541567385196686,
      "learning_rate": 1.1488888888888889e-05,
      "loss": 0.0021,
      "step": 69320
    },
    {
      "epoch": 6.1626666666666665,
      "grad_norm": 0.40036308765411377,
      "learning_rate": 1.1483333333333334e-05,
      "loss": 0.0013,
      "step": 69330
    },
    {
      "epoch": 6.163555555555556,
      "grad_norm": 0.23501957952976227,
      "learning_rate": 1.147777777777778e-05,
      "loss": 0.002,
      "step": 69340
    },
    {
      "epoch": 6.164444444444444,
      "grad_norm": 0.6106244921684265,
      "learning_rate": 1.1472222222222223e-05,
      "loss": 0.0016,
      "step": 69350
    },
    {
      "epoch": 6.165333333333333,
      "grad_norm": 0.11631342023611069,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0019,
      "step": 69360
    },
    {
      "epoch": 6.166222222222222,
      "grad_norm": 0.5425634980201721,
      "learning_rate": 1.1461111111111112e-05,
      "loss": 0.003,
      "step": 69370
    },
    {
      "epoch": 6.167111111111111,
      "grad_norm": 0.5061380863189697,
      "learning_rate": 1.1455555555555555e-05,
      "loss": 0.0017,
      "step": 69380
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.5676637887954712,
      "learning_rate": 1.145e-05,
      "loss": 0.0017,
      "step": 69390
    },
    {
      "epoch": 6.168888888888889,
      "grad_norm": 0.25875455141067505,
      "learning_rate": 1.1444444444444446e-05,
      "loss": 0.0013,
      "step": 69400
    },
    {
      "epoch": 6.169777777777778,
      "grad_norm": 0.7340524792671204,
      "learning_rate": 1.143888888888889e-05,
      "loss": 0.0026,
      "step": 69410
    },
    {
      "epoch": 6.1706666666666665,
      "grad_norm": 0.08320073783397675,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.0018,
      "step": 69420
    },
    {
      "epoch": 6.171555555555556,
      "grad_norm": 0.05976875498890877,
      "learning_rate": 1.1427777777777778e-05,
      "loss": 0.0028,
      "step": 69430
    },
    {
      "epoch": 6.172444444444444,
      "grad_norm": 0.42917007207870483,
      "learning_rate": 1.1422222222222223e-05,
      "loss": 0.0022,
      "step": 69440
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 0.5559511184692383,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.0015,
      "step": 69450
    },
    {
      "epoch": 6.174222222222222,
      "grad_norm": 0.1847517043352127,
      "learning_rate": 1.141111111111111e-05,
      "loss": 0.0021,
      "step": 69460
    },
    {
      "epoch": 6.175111111111111,
      "grad_norm": 0.5795626640319824,
      "learning_rate": 1.1405555555555556e-05,
      "loss": 0.0015,
      "step": 69470
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.36110976338386536,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0016,
      "step": 69480
    },
    {
      "epoch": 6.176888888888889,
      "grad_norm": 0.1490355134010315,
      "learning_rate": 1.1394444444444445e-05,
      "loss": 0.0021,
      "step": 69490
    },
    {
      "epoch": 6.177777777777778,
      "grad_norm": 0.18537838757038116,
      "learning_rate": 1.138888888888889e-05,
      "loss": 0.0019,
      "step": 69500
    },
    {
      "epoch": 6.1786666666666665,
      "grad_norm": 0.3151599168777466,
      "learning_rate": 1.1383333333333333e-05,
      "loss": 0.0019,
      "step": 69510
    },
    {
      "epoch": 6.179555555555556,
      "grad_norm": 0.2925674021244049,
      "learning_rate": 1.1377777777777779e-05,
      "loss": 0.0014,
      "step": 69520
    },
    {
      "epoch": 6.180444444444444,
      "grad_norm": 0.4819461703300476,
      "learning_rate": 1.1372222222222224e-05,
      "loss": 0.0018,
      "step": 69530
    },
    {
      "epoch": 6.181333333333333,
      "grad_norm": 0.21853020787239075,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0021,
      "step": 69540
    },
    {
      "epoch": 6.182222222222222,
      "grad_norm": 0.32460659742355347,
      "learning_rate": 1.1361111111111111e-05,
      "loss": 0.0018,
      "step": 69550
    },
    {
      "epoch": 6.183111111111111,
      "grad_norm": 0.40715017914772034,
      "learning_rate": 1.1355555555555556e-05,
      "loss": 0.0014,
      "step": 69560
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.16272827982902527,
      "learning_rate": 1.1350000000000001e-05,
      "loss": 0.0021,
      "step": 69570
    },
    {
      "epoch": 6.184888888888889,
      "grad_norm": 0.3708595037460327,
      "learning_rate": 1.1344444444444445e-05,
      "loss": 0.0013,
      "step": 69580
    },
    {
      "epoch": 6.185777777777778,
      "grad_norm": 0.4114401340484619,
      "learning_rate": 1.1338888888888889e-05,
      "loss": 0.0017,
      "step": 69590
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.599584698677063,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0024,
      "step": 69600
    },
    {
      "epoch": 6.187555555555556,
      "grad_norm": 0.11183716356754303,
      "learning_rate": 1.1327777777777779e-05,
      "loss": 0.0013,
      "step": 69610
    },
    {
      "epoch": 6.188444444444444,
      "grad_norm": 0.1519090086221695,
      "learning_rate": 1.1322222222222223e-05,
      "loss": 0.0017,
      "step": 69620
    },
    {
      "epoch": 6.189333333333333,
      "grad_norm": 0.1301126629114151,
      "learning_rate": 1.1316666666666668e-05,
      "loss": 0.0013,
      "step": 69630
    },
    {
      "epoch": 6.190222222222222,
      "grad_norm": 0.42411327362060547,
      "learning_rate": 1.1311111111111111e-05,
      "loss": 0.0028,
      "step": 69640
    },
    {
      "epoch": 6.191111111111111,
      "grad_norm": 0.23390701413154602,
      "learning_rate": 1.1305555555555557e-05,
      "loss": 0.0019,
      "step": 69650
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.41325005888938904,
      "learning_rate": 1.13e-05,
      "loss": 0.0016,
      "step": 69660
    },
    {
      "epoch": 6.192888888888889,
      "grad_norm": 0.08456721901893616,
      "learning_rate": 1.1294444444444445e-05,
      "loss": 0.0014,
      "step": 69670
    },
    {
      "epoch": 6.193777777777778,
      "grad_norm": 0.1101803183555603,
      "learning_rate": 1.1288888888888889e-05,
      "loss": 0.002,
      "step": 69680
    },
    {
      "epoch": 6.1946666666666665,
      "grad_norm": 0.2882111370563507,
      "learning_rate": 1.1283333333333333e-05,
      "loss": 0.0015,
      "step": 69690
    },
    {
      "epoch": 6.195555555555556,
      "grad_norm": 0.20539310574531555,
      "learning_rate": 1.127777777777778e-05,
      "loss": 0.0028,
      "step": 69700
    },
    {
      "epoch": 6.196444444444444,
      "grad_norm": 0.04483072832226753,
      "learning_rate": 1.1272222222222223e-05,
      "loss": 0.0017,
      "step": 69710
    },
    {
      "epoch": 6.197333333333333,
      "grad_norm": 0.16052177548408508,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0022,
      "step": 69720
    },
    {
      "epoch": 6.198222222222222,
      "grad_norm": 0.47353076934814453,
      "learning_rate": 1.1261111111111112e-05,
      "loss": 0.0016,
      "step": 69730
    },
    {
      "epoch": 6.199111111111111,
      "grad_norm": 0.2586221396923065,
      "learning_rate": 1.1255555555555557e-05,
      "loss": 0.0016,
      "step": 69740
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.4709375202655792,
      "learning_rate": 1.125e-05,
      "loss": 0.0019,
      "step": 69750
    },
    {
      "epoch": 6.200888888888889,
      "grad_norm": 0.05250686779618263,
      "learning_rate": 1.1244444444444444e-05,
      "loss": 0.0025,
      "step": 69760
    },
    {
      "epoch": 6.201777777777778,
      "grad_norm": 0.11371942609548569,
      "learning_rate": 1.123888888888889e-05,
      "loss": 0.0025,
      "step": 69770
    },
    {
      "epoch": 6.2026666666666666,
      "grad_norm": 0.30302518606185913,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0014,
      "step": 69780
    },
    {
      "epoch": 6.203555555555556,
      "grad_norm": 0.0678076446056366,
      "learning_rate": 1.1227777777777778e-05,
      "loss": 0.0022,
      "step": 69790
    },
    {
      "epoch": 6.204444444444444,
      "grad_norm": 0.18793098628520966,
      "learning_rate": 1.1222222222222224e-05,
      "loss": 0.002,
      "step": 69800
    },
    {
      "epoch": 6.205333333333333,
      "grad_norm": 0.1444147378206253,
      "learning_rate": 1.1216666666666667e-05,
      "loss": 0.0025,
      "step": 69810
    },
    {
      "epoch": 6.206222222222222,
      "grad_norm": 0.4185822606086731,
      "learning_rate": 1.121111111111111e-05,
      "loss": 0.0016,
      "step": 69820
    },
    {
      "epoch": 6.207111111111111,
      "grad_norm": 0.2107553780078888,
      "learning_rate": 1.1205555555555556e-05,
      "loss": 0.0017,
      "step": 69830
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.04522088170051575,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.002,
      "step": 69840
    },
    {
      "epoch": 6.208888888888889,
      "grad_norm": 0.22943994402885437,
      "learning_rate": 1.1194444444444445e-05,
      "loss": 0.002,
      "step": 69850
    },
    {
      "epoch": 6.209777777777778,
      "grad_norm": 0.2629767656326294,
      "learning_rate": 1.1188888888888888e-05,
      "loss": 0.0024,
      "step": 69860
    },
    {
      "epoch": 6.210666666666667,
      "grad_norm": 0.7194433212280273,
      "learning_rate": 1.1183333333333335e-05,
      "loss": 0.0014,
      "step": 69870
    },
    {
      "epoch": 6.211555555555556,
      "grad_norm": 0.07473734021186829,
      "learning_rate": 1.1177777777777779e-05,
      "loss": 0.0018,
      "step": 69880
    },
    {
      "epoch": 6.212444444444444,
      "grad_norm": 0.2989397943019867,
      "learning_rate": 1.1172222222222222e-05,
      "loss": 0.0013,
      "step": 69890
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.05840490013360977,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0015,
      "step": 69900
    },
    {
      "epoch": 6.214222222222222,
      "grad_norm": 0.48592931032180786,
      "learning_rate": 1.1161111111111111e-05,
      "loss": 0.002,
      "step": 69910
    },
    {
      "epoch": 6.215111111111111,
      "grad_norm": 0.1500566452741623,
      "learning_rate": 1.1155555555555556e-05,
      "loss": 0.0016,
      "step": 69920
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.4529232978820801,
      "learning_rate": 1.115e-05,
      "loss": 0.0018,
      "step": 69930
    },
    {
      "epoch": 6.216888888888889,
      "grad_norm": 0.06975818425416946,
      "learning_rate": 1.1144444444444445e-05,
      "loss": 0.0018,
      "step": 69940
    },
    {
      "epoch": 6.217777777777778,
      "grad_norm": 0.1963902711868286,
      "learning_rate": 1.1138888888888889e-05,
      "loss": 0.0021,
      "step": 69950
    },
    {
      "epoch": 6.218666666666667,
      "grad_norm": 0.12117625027894974,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0021,
      "step": 69960
    },
    {
      "epoch": 6.219555555555556,
      "grad_norm": 0.29175251722335815,
      "learning_rate": 1.112777777777778e-05,
      "loss": 0.0017,
      "step": 69970
    },
    {
      "epoch": 6.220444444444444,
      "grad_norm": 0.10693776607513428,
      "learning_rate": 1.1122222222222223e-05,
      "loss": 0.0015,
      "step": 69980
    },
    {
      "epoch": 6.221333333333333,
      "grad_norm": 0.07890699058771133,
      "learning_rate": 1.1116666666666666e-05,
      "loss": 0.0028,
      "step": 69990
    },
    {
      "epoch": 6.222222222222222,
      "grad_norm": 0.7750740647315979,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.0028,
      "step": 70000
    },
    {
      "epoch": 6.223111111111111,
      "grad_norm": 0.1899138242006302,
      "learning_rate": 1.1105555555555557e-05,
      "loss": 0.0015,
      "step": 70010
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.5688183903694153,
      "learning_rate": 1.11e-05,
      "loss": 0.0019,
      "step": 70020
    },
    {
      "epoch": 6.224888888888889,
      "grad_norm": 0.32395026087760925,
      "learning_rate": 1.1094444444444444e-05,
      "loss": 0.0016,
      "step": 70030
    },
    {
      "epoch": 6.225777777777778,
      "grad_norm": 0.36297258734703064,
      "learning_rate": 1.108888888888889e-05,
      "loss": 0.0026,
      "step": 70040
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 0.4438299834728241,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.0023,
      "step": 70050
    },
    {
      "epoch": 6.227555555555556,
      "grad_norm": 0.3414877653121948,
      "learning_rate": 1.1077777777777778e-05,
      "loss": 0.0014,
      "step": 70060
    },
    {
      "epoch": 6.2284444444444444,
      "grad_norm": 0.69637531042099,
      "learning_rate": 1.1072222222222223e-05,
      "loss": 0.0025,
      "step": 70070
    },
    {
      "epoch": 6.229333333333333,
      "grad_norm": 0.13609634339809418,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0018,
      "step": 70080
    },
    {
      "epoch": 6.230222222222222,
      "grad_norm": 0.09720625728368759,
      "learning_rate": 1.106111111111111e-05,
      "loss": 0.0017,
      "step": 70090
    },
    {
      "epoch": 6.231111111111111,
      "grad_norm": 0.0913681760430336,
      "learning_rate": 1.1055555555555556e-05,
      "loss": 0.0022,
      "step": 70100
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.5814598202705383,
      "learning_rate": 1.1050000000000001e-05,
      "loss": 0.0023,
      "step": 70110
    },
    {
      "epoch": 6.232888888888889,
      "grad_norm": 0.22078855335712433,
      "learning_rate": 1.1044444444444444e-05,
      "loss": 0.0019,
      "step": 70120
    },
    {
      "epoch": 6.233777777777778,
      "grad_norm": 0.11265154927968979,
      "learning_rate": 1.103888888888889e-05,
      "loss": 0.0018,
      "step": 70130
    },
    {
      "epoch": 6.234666666666667,
      "grad_norm": 0.4900023341178894,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.0021,
      "step": 70140
    },
    {
      "epoch": 6.235555555555556,
      "grad_norm": 0.040337610989809036,
      "learning_rate": 1.1027777777777779e-05,
      "loss": 0.0023,
      "step": 70150
    },
    {
      "epoch": 6.2364444444444445,
      "grad_norm": 0.21891914308071136,
      "learning_rate": 1.1022222222222222e-05,
      "loss": 0.0018,
      "step": 70160
    },
    {
      "epoch": 6.237333333333333,
      "grad_norm": 0.09216688573360443,
      "learning_rate": 1.1016666666666667e-05,
      "loss": 0.0017,
      "step": 70170
    },
    {
      "epoch": 6.238222222222222,
      "grad_norm": 0.5005451440811157,
      "learning_rate": 1.1011111111111113e-05,
      "loss": 0.0022,
      "step": 70180
    },
    {
      "epoch": 6.239111111111111,
      "grad_norm": 0.18492728471755981,
      "learning_rate": 1.1005555555555556e-05,
      "loss": 0.0016,
      "step": 70190
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.33246684074401855,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0015,
      "step": 70200
    },
    {
      "epoch": 6.240888888888889,
      "grad_norm": 0.1247977614402771,
      "learning_rate": 1.0994444444444445e-05,
      "loss": 0.0027,
      "step": 70210
    },
    {
      "epoch": 6.241777777777778,
      "grad_norm": 0.07609739899635315,
      "learning_rate": 1.0988888888888889e-05,
      "loss": 0.0024,
      "step": 70220
    },
    {
      "epoch": 6.242666666666667,
      "grad_norm": 0.12423577159643173,
      "learning_rate": 1.0983333333333334e-05,
      "loss": 0.0018,
      "step": 70230
    },
    {
      "epoch": 6.243555555555556,
      "grad_norm": 0.2502814531326294,
      "learning_rate": 1.0977777777777779e-05,
      "loss": 0.0014,
      "step": 70240
    },
    {
      "epoch": 6.2444444444444445,
      "grad_norm": 0.06293725222349167,
      "learning_rate": 1.0972222222222223e-05,
      "loss": 0.0016,
      "step": 70250
    },
    {
      "epoch": 6.245333333333333,
      "grad_norm": 0.07273966073989868,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.002,
      "step": 70260
    },
    {
      "epoch": 6.246222222222222,
      "grad_norm": 0.16140520572662354,
      "learning_rate": 1.0961111111111113e-05,
      "loss": 0.0015,
      "step": 70270
    },
    {
      "epoch": 6.247111111111111,
      "grad_norm": 0.31873056292533875,
      "learning_rate": 1.0955555555555557e-05,
      "loss": 0.0018,
      "step": 70280
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.27474313974380493,
      "learning_rate": 1.095e-05,
      "loss": 0.0016,
      "step": 70290
    },
    {
      "epoch": 6.248888888888889,
      "grad_norm": 0.42599499225616455,
      "learning_rate": 1.0944444444444445e-05,
      "loss": 0.0023,
      "step": 70300
    },
    {
      "epoch": 6.249777777777778,
      "grad_norm": 0.7634211778640747,
      "learning_rate": 1.0938888888888889e-05,
      "loss": 0.0026,
      "step": 70310
    },
    {
      "epoch": 6.250666666666667,
      "grad_norm": 0.06904104351997375,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0015,
      "step": 70320
    },
    {
      "epoch": 6.251555555555555,
      "grad_norm": 0.5975050926208496,
      "learning_rate": 1.0927777777777778e-05,
      "loss": 0.0022,
      "step": 70330
    },
    {
      "epoch": 6.2524444444444445,
      "grad_norm": 0.05586830526590347,
      "learning_rate": 1.0922222222222223e-05,
      "loss": 0.0016,
      "step": 70340
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 0.6518166065216064,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.0022,
      "step": 70350
    },
    {
      "epoch": 6.254222222222222,
      "grad_norm": 0.2614125609397888,
      "learning_rate": 1.0911111111111112e-05,
      "loss": 0.0021,
      "step": 70360
    },
    {
      "epoch": 6.255111111111111,
      "grad_norm": 0.4241420030593872,
      "learning_rate": 1.0905555555555557e-05,
      "loss": 0.0033,
      "step": 70370
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.43367522954940796,
      "learning_rate": 1.09e-05,
      "loss": 0.0016,
      "step": 70380
    },
    {
      "epoch": 6.256888888888889,
      "grad_norm": 0.23862400650978088,
      "learning_rate": 1.0894444444444444e-05,
      "loss": 0.0021,
      "step": 70390
    },
    {
      "epoch": 6.257777777777778,
      "grad_norm": 0.5813581943511963,
      "learning_rate": 1.088888888888889e-05,
      "loss": 0.0024,
      "step": 70400
    },
    {
      "epoch": 6.258666666666667,
      "grad_norm": 0.1856972724199295,
      "learning_rate": 1.0883333333333335e-05,
      "loss": 0.0017,
      "step": 70410
    },
    {
      "epoch": 6.259555555555556,
      "grad_norm": 0.3407810628414154,
      "learning_rate": 1.0877777777777778e-05,
      "loss": 0.0014,
      "step": 70420
    },
    {
      "epoch": 6.2604444444444445,
      "grad_norm": 0.08509233593940735,
      "learning_rate": 1.0872222222222222e-05,
      "loss": 0.0025,
      "step": 70430
    },
    {
      "epoch": 6.261333333333333,
      "grad_norm": 0.07694044709205627,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0018,
      "step": 70440
    },
    {
      "epoch": 6.262222222222222,
      "grad_norm": 0.19192995131015778,
      "learning_rate": 1.0861111111111112e-05,
      "loss": 0.0017,
      "step": 70450
    },
    {
      "epoch": 6.263111111111111,
      "grad_norm": 0.7760410904884338,
      "learning_rate": 1.0855555555555556e-05,
      "loss": 0.0019,
      "step": 70460
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.48285889625549316,
      "learning_rate": 1.0850000000000001e-05,
      "loss": 0.0019,
      "step": 70470
    },
    {
      "epoch": 6.264888888888889,
      "grad_norm": 0.12368849664926529,
      "learning_rate": 1.0844444444444445e-05,
      "loss": 0.0016,
      "step": 70480
    },
    {
      "epoch": 6.265777777777778,
      "grad_norm": 0.27382269501686096,
      "learning_rate": 1.083888888888889e-05,
      "loss": 0.002,
      "step": 70490
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.7329930067062378,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0024,
      "step": 70500
    },
    {
      "epoch": 6.267555555555555,
      "grad_norm": 0.3421189785003662,
      "learning_rate": 1.0827777777777779e-05,
      "loss": 0.0024,
      "step": 70510
    },
    {
      "epoch": 6.2684444444444445,
      "grad_norm": 0.31942272186279297,
      "learning_rate": 1.0822222222222222e-05,
      "loss": 0.003,
      "step": 70520
    },
    {
      "epoch": 6.269333333333333,
      "grad_norm": 0.07697910815477371,
      "learning_rate": 1.0816666666666666e-05,
      "loss": 0.0016,
      "step": 70530
    },
    {
      "epoch": 6.270222222222222,
      "grad_norm": 0.9376077055931091,
      "learning_rate": 1.0811111111111113e-05,
      "loss": 0.0022,
      "step": 70540
    },
    {
      "epoch": 6.271111111111111,
      "grad_norm": 0.08063241094350815,
      "learning_rate": 1.0805555555555556e-05,
      "loss": 0.0017,
      "step": 70550
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.41041380167007446,
      "learning_rate": 1.08e-05,
      "loss": 0.0012,
      "step": 70560
    },
    {
      "epoch": 6.272888888888889,
      "grad_norm": 0.3280641734600067,
      "learning_rate": 1.0794444444444445e-05,
      "loss": 0.0015,
      "step": 70570
    },
    {
      "epoch": 6.273777777777778,
      "grad_norm": 0.04420601949095726,
      "learning_rate": 1.078888888888889e-05,
      "loss": 0.0015,
      "step": 70580
    },
    {
      "epoch": 6.274666666666667,
      "grad_norm": 0.07937315106391907,
      "learning_rate": 1.0783333333333334e-05,
      "loss": 0.0015,
      "step": 70590
    },
    {
      "epoch": 6.275555555555556,
      "grad_norm": 0.12189216911792755,
      "learning_rate": 1.0777777777777778e-05,
      "loss": 0.0013,
      "step": 70600
    },
    {
      "epoch": 6.2764444444444445,
      "grad_norm": 0.33457207679748535,
      "learning_rate": 1.0772222222222223e-05,
      "loss": 0.0019,
      "step": 70610
    },
    {
      "epoch": 6.277333333333333,
      "grad_norm": 0.06500383466482162,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0021,
      "step": 70620
    },
    {
      "epoch": 6.278222222222222,
      "grad_norm": 0.5914854407310486,
      "learning_rate": 1.0761111111111112e-05,
      "loss": 0.0016,
      "step": 70630
    },
    {
      "epoch": 6.279111111111111,
      "grad_norm": 0.15696066617965698,
      "learning_rate": 1.0755555555555557e-05,
      "loss": 0.0014,
      "step": 70640
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.30028435587882996,
      "learning_rate": 1.075e-05,
      "loss": 0.0025,
      "step": 70650
    },
    {
      "epoch": 6.280888888888889,
      "grad_norm": 0.26235446333885193,
      "learning_rate": 1.0744444444444444e-05,
      "loss": 0.0017,
      "step": 70660
    },
    {
      "epoch": 6.281777777777778,
      "grad_norm": 0.6344789266586304,
      "learning_rate": 1.073888888888889e-05,
      "loss": 0.0026,
      "step": 70670
    },
    {
      "epoch": 6.282666666666667,
      "grad_norm": 0.22182299196720123,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0018,
      "step": 70680
    },
    {
      "epoch": 6.283555555555555,
      "grad_norm": 0.5609740614891052,
      "learning_rate": 1.0727777777777778e-05,
      "loss": 0.002,
      "step": 70690
    },
    {
      "epoch": 6.2844444444444445,
      "grad_norm": 0.047024086117744446,
      "learning_rate": 1.0722222222222222e-05,
      "loss": 0.0018,
      "step": 70700
    },
    {
      "epoch": 6.285333333333333,
      "grad_norm": 0.5698752999305725,
      "learning_rate": 1.0716666666666667e-05,
      "loss": 0.002,
      "step": 70710
    },
    {
      "epoch": 6.286222222222222,
      "grad_norm": 0.11581742018461227,
      "learning_rate": 1.0711111111111112e-05,
      "loss": 0.0017,
      "step": 70720
    },
    {
      "epoch": 6.287111111111111,
      "grad_norm": 0.32302701473236084,
      "learning_rate": 1.0705555555555556e-05,
      "loss": 0.0014,
      "step": 70730
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.21820931136608124,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.0022,
      "step": 70740
    },
    {
      "epoch": 6.288888888888889,
      "grad_norm": 0.044224388897418976,
      "learning_rate": 1.0694444444444444e-05,
      "loss": 0.0016,
      "step": 70750
    },
    {
      "epoch": 6.289777777777778,
      "grad_norm": 0.07372117787599564,
      "learning_rate": 1.068888888888889e-05,
      "loss": 0.0015,
      "step": 70760
    },
    {
      "epoch": 6.290666666666667,
      "grad_norm": 0.5750836730003357,
      "learning_rate": 1.0683333333333333e-05,
      "loss": 0.003,
      "step": 70770
    },
    {
      "epoch": 6.291555555555556,
      "grad_norm": 0.3398571014404297,
      "learning_rate": 1.0677777777777779e-05,
      "loss": 0.0025,
      "step": 70780
    },
    {
      "epoch": 6.2924444444444445,
      "grad_norm": 0.47323012351989746,
      "learning_rate": 1.0672222222222222e-05,
      "loss": 0.0013,
      "step": 70790
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 0.422433078289032,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0013,
      "step": 70800
    },
    {
      "epoch": 6.294222222222222,
      "grad_norm": 0.26238831877708435,
      "learning_rate": 1.0661111111111113e-05,
      "loss": 0.0024,
      "step": 70810
    },
    {
      "epoch": 6.295111111111111,
      "grad_norm": 0.5048730373382568,
      "learning_rate": 1.0655555555555556e-05,
      "loss": 0.0018,
      "step": 70820
    },
    {
      "epoch": 6.296,
      "grad_norm": 0.4480729103088379,
      "learning_rate": 1.065e-05,
      "loss": 0.0028,
      "step": 70830
    },
    {
      "epoch": 6.296888888888889,
      "grad_norm": 0.40175437927246094,
      "learning_rate": 1.0644444444444445e-05,
      "loss": 0.002,
      "step": 70840
    },
    {
      "epoch": 6.297777777777778,
      "grad_norm": 0.10907121002674103,
      "learning_rate": 1.063888888888889e-05,
      "loss": 0.0015,
      "step": 70850
    },
    {
      "epoch": 6.298666666666667,
      "grad_norm": 0.34931480884552,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0017,
      "step": 70860
    },
    {
      "epoch": 6.299555555555555,
      "grad_norm": 0.5485802888870239,
      "learning_rate": 1.0627777777777779e-05,
      "loss": 0.0031,
      "step": 70870
    },
    {
      "epoch": 6.3004444444444445,
      "grad_norm": 0.11032749712467194,
      "learning_rate": 1.0622222222222223e-05,
      "loss": 0.0022,
      "step": 70880
    },
    {
      "epoch": 6.301333333333333,
      "grad_norm": 0.18448881804943085,
      "learning_rate": 1.0616666666666668e-05,
      "loss": 0.002,
      "step": 70890
    },
    {
      "epoch": 6.302222222222222,
      "grad_norm": 0.04725704342126846,
      "learning_rate": 1.0611111111111111e-05,
      "loss": 0.0017,
      "step": 70900
    },
    {
      "epoch": 6.303111111111111,
      "grad_norm": 0.5393497347831726,
      "learning_rate": 1.0605555555555557e-05,
      "loss": 0.002,
      "step": 70910
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.39979714155197144,
      "learning_rate": 1.06e-05,
      "loss": 0.0022,
      "step": 70920
    },
    {
      "epoch": 6.304888888888889,
      "grad_norm": 0.23544266819953918,
      "learning_rate": 1.0594444444444444e-05,
      "loss": 0.002,
      "step": 70930
    },
    {
      "epoch": 6.305777777777778,
      "grad_norm": 0.15617740154266357,
      "learning_rate": 1.058888888888889e-05,
      "loss": 0.0016,
      "step": 70940
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 0.1564953327178955,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.0027,
      "step": 70950
    },
    {
      "epoch": 6.307555555555556,
      "grad_norm": 0.46507057547569275,
      "learning_rate": 1.0577777777777778e-05,
      "loss": 0.0028,
      "step": 70960
    },
    {
      "epoch": 6.3084444444444445,
      "grad_norm": 0.25409820675849915,
      "learning_rate": 1.0572222222222223e-05,
      "loss": 0.0019,
      "step": 70970
    },
    {
      "epoch": 6.309333333333333,
      "grad_norm": 0.08788994699716568,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0023,
      "step": 70980
    },
    {
      "epoch": 6.310222222222222,
      "grad_norm": 0.486684650182724,
      "learning_rate": 1.0561111111111112e-05,
      "loss": 0.0019,
      "step": 70990
    },
    {
      "epoch": 6.311111111111111,
      "grad_norm": 0.25997260212898254,
      "learning_rate": 1.0555555555555555e-05,
      "loss": 0.0014,
      "step": 71000
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.24667559564113617,
      "learning_rate": 1.055e-05,
      "loss": 0.0015,
      "step": 71010
    },
    {
      "epoch": 6.312888888888889,
      "grad_norm": 0.04631219804286957,
      "learning_rate": 1.0544444444444444e-05,
      "loss": 0.0016,
      "step": 71020
    },
    {
      "epoch": 6.313777777777778,
      "grad_norm": 0.10725478082895279,
      "learning_rate": 1.053888888888889e-05,
      "loss": 0.0021,
      "step": 71030
    },
    {
      "epoch": 6.314666666666667,
      "grad_norm": 0.6269978284835815,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0021,
      "step": 71040
    },
    {
      "epoch": 6.315555555555555,
      "grad_norm": 0.053209781646728516,
      "learning_rate": 1.0527777777777778e-05,
      "loss": 0.003,
      "step": 71050
    },
    {
      "epoch": 6.3164444444444445,
      "grad_norm": 0.3645418882369995,
      "learning_rate": 1.0522222222222222e-05,
      "loss": 0.0015,
      "step": 71060
    },
    {
      "epoch": 6.317333333333333,
      "grad_norm": 0.2203652709722519,
      "learning_rate": 1.0516666666666667e-05,
      "loss": 0.0022,
      "step": 71070
    },
    {
      "epoch": 6.318222222222222,
      "grad_norm": 0.1810941994190216,
      "learning_rate": 1.0511111111111112e-05,
      "loss": 0.0013,
      "step": 71080
    },
    {
      "epoch": 6.319111111111111,
      "grad_norm": 0.5348168611526489,
      "learning_rate": 1.0505555555555556e-05,
      "loss": 0.0021,
      "step": 71090
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.2145465463399887,
      "learning_rate": 1.05e-05,
      "loss": 0.0016,
      "step": 71100
    },
    {
      "epoch": 6.320888888888889,
      "grad_norm": 0.2241772562265396,
      "learning_rate": 1.0494444444444446e-05,
      "loss": 0.002,
      "step": 71110
    },
    {
      "epoch": 6.321777777777778,
      "grad_norm": 0.04975347965955734,
      "learning_rate": 1.048888888888889e-05,
      "loss": 0.0015,
      "step": 71120
    },
    {
      "epoch": 6.322666666666667,
      "grad_norm": 0.05730881169438362,
      "learning_rate": 1.0483333333333333e-05,
      "loss": 0.0023,
      "step": 71130
    },
    {
      "epoch": 6.323555555555555,
      "grad_norm": 0.2247735857963562,
      "learning_rate": 1.0477777777777779e-05,
      "loss": 0.0019,
      "step": 71140
    },
    {
      "epoch": 6.3244444444444445,
      "grad_norm": 0.1269037276506424,
      "learning_rate": 1.0472222222222222e-05,
      "loss": 0.0015,
      "step": 71150
    },
    {
      "epoch": 6.325333333333333,
      "grad_norm": 0.3551409840583801,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0019,
      "step": 71160
    },
    {
      "epoch": 6.326222222222222,
      "grad_norm": 0.3487052917480469,
      "learning_rate": 1.0461111111111111e-05,
      "loss": 0.0026,
      "step": 71170
    },
    {
      "epoch": 6.327111111111111,
      "grad_norm": 0.07577960193157196,
      "learning_rate": 1.0455555555555556e-05,
      "loss": 0.0018,
      "step": 71180
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.3593757748603821,
      "learning_rate": 1.045e-05,
      "loss": 0.0028,
      "step": 71190
    },
    {
      "epoch": 6.328888888888889,
      "grad_norm": 0.03557391092181206,
      "learning_rate": 1.0444444444444445e-05,
      "loss": 0.0017,
      "step": 71200
    },
    {
      "epoch": 6.329777777777778,
      "grad_norm": 0.4862273037433624,
      "learning_rate": 1.043888888888889e-05,
      "loss": 0.0015,
      "step": 71210
    },
    {
      "epoch": 6.330666666666667,
      "grad_norm": 0.1090453714132309,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.0017,
      "step": 71220
    },
    {
      "epoch": 6.331555555555555,
      "grad_norm": 0.49286648631095886,
      "learning_rate": 1.0427777777777778e-05,
      "loss": 0.0019,
      "step": 71230
    },
    {
      "epoch": 6.3324444444444445,
      "grad_norm": 0.2526019811630249,
      "learning_rate": 1.0422222222222223e-05,
      "loss": 0.002,
      "step": 71240
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.06986507773399353,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.0025,
      "step": 71250
    },
    {
      "epoch": 6.334222222222222,
      "grad_norm": 0.22099868953227997,
      "learning_rate": 1.0411111111111112e-05,
      "loss": 0.0015,
      "step": 71260
    },
    {
      "epoch": 6.335111111111111,
      "grad_norm": 0.6945913434028625,
      "learning_rate": 1.0405555555555555e-05,
      "loss": 0.0033,
      "step": 71270
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.3696534037590027,
      "learning_rate": 1.04e-05,
      "loss": 0.0012,
      "step": 71280
    },
    {
      "epoch": 6.336888888888889,
      "grad_norm": 0.47179943323135376,
      "learning_rate": 1.0394444444444446e-05,
      "loss": 0.0019,
      "step": 71290
    },
    {
      "epoch": 6.337777777777778,
      "grad_norm": 0.22790315747261047,
      "learning_rate": 1.038888888888889e-05,
      "loss": 0.0014,
      "step": 71300
    },
    {
      "epoch": 6.338666666666667,
      "grad_norm": 0.6219941973686218,
      "learning_rate": 1.0383333333333334e-05,
      "loss": 0.0019,
      "step": 71310
    },
    {
      "epoch": 6.339555555555555,
      "grad_norm": 0.17672914266586304,
      "learning_rate": 1.0377777777777778e-05,
      "loss": 0.0015,
      "step": 71320
    },
    {
      "epoch": 6.3404444444444445,
      "grad_norm": 0.3260224461555481,
      "learning_rate": 1.0372222222222222e-05,
      "loss": 0.0013,
      "step": 71330
    },
    {
      "epoch": 6.341333333333333,
      "grad_norm": 0.684295117855072,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0025,
      "step": 71340
    },
    {
      "epoch": 6.342222222222222,
      "grad_norm": 0.1892888993024826,
      "learning_rate": 1.0361111111111112e-05,
      "loss": 0.002,
      "step": 71350
    },
    {
      "epoch": 6.343111111111111,
      "grad_norm": 0.3526937663555145,
      "learning_rate": 1.0355555555555556e-05,
      "loss": 0.0018,
      "step": 71360
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.19051694869995117,
      "learning_rate": 1.035e-05,
      "loss": 0.0013,
      "step": 71370
    },
    {
      "epoch": 6.344888888888889,
      "grad_norm": 0.6132447123527527,
      "learning_rate": 1.0344444444444446e-05,
      "loss": 0.0013,
      "step": 71380
    },
    {
      "epoch": 6.345777777777778,
      "grad_norm": 0.562326192855835,
      "learning_rate": 1.033888888888889e-05,
      "loss": 0.001,
      "step": 71390
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.11147277057170868,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0022,
      "step": 71400
    },
    {
      "epoch": 6.347555555555555,
      "grad_norm": 0.2211003601551056,
      "learning_rate": 1.0327777777777778e-05,
      "loss": 0.0018,
      "step": 71410
    },
    {
      "epoch": 6.348444444444445,
      "grad_norm": 0.6698670983314514,
      "learning_rate": 1.0322222222222224e-05,
      "loss": 0.0017,
      "step": 71420
    },
    {
      "epoch": 6.349333333333333,
      "grad_norm": 0.4708887040615082,
      "learning_rate": 1.0316666666666667e-05,
      "loss": 0.0013,
      "step": 71430
    },
    {
      "epoch": 6.350222222222222,
      "grad_norm": 0.14943630993366241,
      "learning_rate": 1.031111111111111e-05,
      "loss": 0.0016,
      "step": 71440
    },
    {
      "epoch": 6.351111111111111,
      "grad_norm": 0.2500249147415161,
      "learning_rate": 1.0305555555555556e-05,
      "loss": 0.0026,
      "step": 71450
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.12159852683544159,
      "learning_rate": 1.03e-05,
      "loss": 0.0021,
      "step": 71460
    },
    {
      "epoch": 6.352888888888889,
      "grad_norm": 0.08062794804573059,
      "learning_rate": 1.0294444444444445e-05,
      "loss": 0.0013,
      "step": 71470
    },
    {
      "epoch": 6.353777777777778,
      "grad_norm": 0.13881249725818634,
      "learning_rate": 1.028888888888889e-05,
      "loss": 0.0032,
      "step": 71480
    },
    {
      "epoch": 6.354666666666667,
      "grad_norm": 0.10938385128974915,
      "learning_rate": 1.0283333333333334e-05,
      "loss": 0.0017,
      "step": 71490
    },
    {
      "epoch": 6.355555555555555,
      "grad_norm": 0.9040973782539368,
      "learning_rate": 1.0277777777777777e-05,
      "loss": 0.0028,
      "step": 71500
    },
    {
      "epoch": 6.356444444444445,
      "grad_norm": 0.23546956479549408,
      "learning_rate": 1.0272222222222224e-05,
      "loss": 0.0015,
      "step": 71510
    },
    {
      "epoch": 6.357333333333333,
      "grad_norm": 0.29169008135795593,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0014,
      "step": 71520
    },
    {
      "epoch": 6.358222222222222,
      "grad_norm": 0.2850584089756012,
      "learning_rate": 1.0261111111111111e-05,
      "loss": 0.0026,
      "step": 71530
    },
    {
      "epoch": 6.359111111111111,
      "grad_norm": 0.33047470450401306,
      "learning_rate": 1.0255555555555557e-05,
      "loss": 0.0016,
      "step": 71540
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.455708384513855,
      "learning_rate": 1.025e-05,
      "loss": 0.0015,
      "step": 71550
    },
    {
      "epoch": 6.360888888888889,
      "grad_norm": 0.4758164584636688,
      "learning_rate": 1.0244444444444445e-05,
      "loss": 0.0011,
      "step": 71560
    },
    {
      "epoch": 6.361777777777778,
      "grad_norm": 0.257241427898407,
      "learning_rate": 1.0238888888888889e-05,
      "loss": 0.0026,
      "step": 71570
    },
    {
      "epoch": 6.362666666666667,
      "grad_norm": 0.12140075117349625,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.0021,
      "step": 71580
    },
    {
      "epoch": 6.363555555555555,
      "grad_norm": 0.35726895928382874,
      "learning_rate": 1.0227777777777778e-05,
      "loss": 0.002,
      "step": 71590
    },
    {
      "epoch": 6.364444444444445,
      "grad_norm": 0.11057519912719727,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 0.0025,
      "step": 71600
    },
    {
      "epoch": 6.365333333333333,
      "grad_norm": 0.36188289523124695,
      "learning_rate": 1.0216666666666668e-05,
      "loss": 0.0024,
      "step": 71610
    },
    {
      "epoch": 6.3662222222222224,
      "grad_norm": 0.4413723647594452,
      "learning_rate": 1.0211111111111112e-05,
      "loss": 0.0017,
      "step": 71620
    },
    {
      "epoch": 6.367111111111111,
      "grad_norm": 0.4865477681159973,
      "learning_rate": 1.0205555555555555e-05,
      "loss": 0.0019,
      "step": 71630
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.18798458576202393,
      "learning_rate": 1.02e-05,
      "loss": 0.0013,
      "step": 71640
    },
    {
      "epoch": 6.368888888888889,
      "grad_norm": 0.10990516096353531,
      "learning_rate": 1.0194444444444446e-05,
      "loss": 0.002,
      "step": 71650
    },
    {
      "epoch": 6.369777777777777,
      "grad_norm": 0.32950741052627563,
      "learning_rate": 1.018888888888889e-05,
      "loss": 0.0018,
      "step": 71660
    },
    {
      "epoch": 6.370666666666667,
      "grad_norm": 0.1451384574174881,
      "learning_rate": 1.0183333333333333e-05,
      "loss": 0.0018,
      "step": 71670
    },
    {
      "epoch": 6.371555555555555,
      "grad_norm": 0.1253172904253006,
      "learning_rate": 1.0177777777777778e-05,
      "loss": 0.0023,
      "step": 71680
    },
    {
      "epoch": 6.372444444444445,
      "grad_norm": 0.4831003248691559,
      "learning_rate": 1.0172222222222223e-05,
      "loss": 0.0014,
      "step": 71690
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 0.40028807520866394,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0015,
      "step": 71700
    },
    {
      "epoch": 6.3742222222222225,
      "grad_norm": 0.17670132219791412,
      "learning_rate": 1.0161111111111112e-05,
      "loss": 0.0019,
      "step": 71710
    },
    {
      "epoch": 6.375111111111111,
      "grad_norm": 0.16476285457611084,
      "learning_rate": 1.0155555555555556e-05,
      "loss": 0.0018,
      "step": 71720
    },
    {
      "epoch": 6.376,
      "grad_norm": 0.048933327198028564,
      "learning_rate": 1.0150000000000001e-05,
      "loss": 0.0018,
      "step": 71730
    },
    {
      "epoch": 6.376888888888889,
      "grad_norm": 0.6938608288764954,
      "learning_rate": 1.0144444444444445e-05,
      "loss": 0.0014,
      "step": 71740
    },
    {
      "epoch": 6.377777777777778,
      "grad_norm": 0.04140642657876015,
      "learning_rate": 1.013888888888889e-05,
      "loss": 0.0033,
      "step": 71750
    },
    {
      "epoch": 6.378666666666667,
      "grad_norm": 0.08360672742128372,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0022,
      "step": 71760
    },
    {
      "epoch": 6.379555555555555,
      "grad_norm": 0.058625563979148865,
      "learning_rate": 1.0127777777777777e-05,
      "loss": 0.0021,
      "step": 71770
    },
    {
      "epoch": 6.380444444444445,
      "grad_norm": 0.10866446048021317,
      "learning_rate": 1.0122222222222224e-05,
      "loss": 0.002,
      "step": 71780
    },
    {
      "epoch": 6.381333333333333,
      "grad_norm": 0.6535828113555908,
      "learning_rate": 1.0116666666666667e-05,
      "loss": 0.0013,
      "step": 71790
    },
    {
      "epoch": 6.3822222222222225,
      "grad_norm": 0.066413015127182,
      "learning_rate": 1.0111111111111111e-05,
      "loss": 0.0015,
      "step": 71800
    },
    {
      "epoch": 6.383111111111111,
      "grad_norm": 0.2885386645793915,
      "learning_rate": 1.0105555555555556e-05,
      "loss": 0.0019,
      "step": 71810
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.27425265312194824,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.0013,
      "step": 71820
    },
    {
      "epoch": 6.384888888888889,
      "grad_norm": 0.5415934920310974,
      "learning_rate": 1.0094444444444445e-05,
      "loss": 0.002,
      "step": 71830
    },
    {
      "epoch": 6.385777777777777,
      "grad_norm": 0.11102594435214996,
      "learning_rate": 1.0088888888888889e-05,
      "loss": 0.0017,
      "step": 71840
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 0.4068553149700165,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.0017,
      "step": 71850
    },
    {
      "epoch": 6.387555555555555,
      "grad_norm": 0.37113216519355774,
      "learning_rate": 1.0077777777777777e-05,
      "loss": 0.002,
      "step": 71860
    },
    {
      "epoch": 6.388444444444445,
      "grad_norm": 0.5784034729003906,
      "learning_rate": 1.0072222222222223e-05,
      "loss": 0.0028,
      "step": 71870
    },
    {
      "epoch": 6.389333333333333,
      "grad_norm": 0.21116448938846588,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0022,
      "step": 71880
    },
    {
      "epoch": 6.3902222222222225,
      "grad_norm": 0.6468400359153748,
      "learning_rate": 1.0061111111111112e-05,
      "loss": 0.0017,
      "step": 71890
    },
    {
      "epoch": 6.391111111111111,
      "grad_norm": 0.1556236445903778,
      "learning_rate": 1.0055555555555555e-05,
      "loss": 0.0019,
      "step": 71900
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.6918982267379761,
      "learning_rate": 1.005e-05,
      "loss": 0.0017,
      "step": 71910
    },
    {
      "epoch": 6.392888888888889,
      "grad_norm": 0.43630966544151306,
      "learning_rate": 1.0044444444444446e-05,
      "loss": 0.0016,
      "step": 71920
    },
    {
      "epoch": 6.393777777777778,
      "grad_norm": 0.0489828959107399,
      "learning_rate": 1.0038888888888889e-05,
      "loss": 0.0015,
      "step": 71930
    },
    {
      "epoch": 6.394666666666667,
      "grad_norm": 0.4553925693035126,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0022,
      "step": 71940
    },
    {
      "epoch": 6.395555555555555,
      "grad_norm": 0.18540897965431213,
      "learning_rate": 1.0027777777777778e-05,
      "loss": 0.002,
      "step": 71950
    },
    {
      "epoch": 6.396444444444445,
      "grad_norm": 0.47660326957702637,
      "learning_rate": 1.0022222222222223e-05,
      "loss": 0.0016,
      "step": 71960
    },
    {
      "epoch": 6.397333333333333,
      "grad_norm": 0.14027172327041626,
      "learning_rate": 1.0016666666666667e-05,
      "loss": 0.0017,
      "step": 71970
    },
    {
      "epoch": 6.3982222222222225,
      "grad_norm": 0.04459919407963753,
      "learning_rate": 1.0011111111111112e-05,
      "loss": 0.0015,
      "step": 71980
    },
    {
      "epoch": 6.399111111111111,
      "grad_norm": 0.3943879306316376,
      "learning_rate": 1.0005555555555556e-05,
      "loss": 0.0019,
      "step": 71990
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.4033862352371216,
      "learning_rate": 1e-05,
      "loss": 0.002,
      "step": 72000
    },
    {
      "epoch": 6.400888888888889,
      "grad_norm": 0.12371759861707687,
      "learning_rate": 9.994444444444444e-06,
      "loss": 0.0026,
      "step": 72010
    },
    {
      "epoch": 6.401777777777777,
      "grad_norm": 0.10829417407512665,
      "learning_rate": 9.98888888888889e-06,
      "loss": 0.0018,
      "step": 72020
    },
    {
      "epoch": 6.402666666666667,
      "grad_norm": 0.0930626317858696,
      "learning_rate": 9.983333333333333e-06,
      "loss": 0.0023,
      "step": 72030
    },
    {
      "epoch": 6.403555555555555,
      "grad_norm": 0.06375595182180405,
      "learning_rate": 9.977777777777778e-06,
      "loss": 0.0023,
      "step": 72040
    },
    {
      "epoch": 6.404444444444445,
      "grad_norm": 0.2875044345855713,
      "learning_rate": 9.972222222222224e-06,
      "loss": 0.002,
      "step": 72050
    },
    {
      "epoch": 6.405333333333333,
      "grad_norm": 0.12489885091781616,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0022,
      "step": 72060
    },
    {
      "epoch": 6.4062222222222225,
      "grad_norm": 0.493629515171051,
      "learning_rate": 9.96111111111111e-06,
      "loss": 0.0018,
      "step": 72070
    },
    {
      "epoch": 6.407111111111111,
      "grad_norm": 0.36782345175743103,
      "learning_rate": 9.955555555555556e-06,
      "loss": 0.0017,
      "step": 72080
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.547227144241333,
      "learning_rate": 9.950000000000001e-06,
      "loss": 0.0023,
      "step": 72090
    },
    {
      "epoch": 6.408888888888889,
      "grad_norm": 0.5064249038696289,
      "learning_rate": 9.944444444444445e-06,
      "loss": 0.0019,
      "step": 72100
    },
    {
      "epoch": 6.409777777777778,
      "grad_norm": 0.23045215010643005,
      "learning_rate": 9.938888888888888e-06,
      "loss": 0.0029,
      "step": 72110
    },
    {
      "epoch": 6.410666666666667,
      "grad_norm": 0.29199129343032837,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0021,
      "step": 72120
    },
    {
      "epoch": 6.411555555555555,
      "grad_norm": 0.2288881242275238,
      "learning_rate": 9.927777777777779e-06,
      "loss": 0.0023,
      "step": 72130
    },
    {
      "epoch": 6.412444444444445,
      "grad_norm": 0.21421808004379272,
      "learning_rate": 9.922222222222222e-06,
      "loss": 0.0018,
      "step": 72140
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 0.2873242497444153,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.002,
      "step": 72150
    },
    {
      "epoch": 6.4142222222222225,
      "grad_norm": 0.36365142464637756,
      "learning_rate": 9.911111111111111e-06,
      "loss": 0.0015,
      "step": 72160
    },
    {
      "epoch": 6.415111111111111,
      "grad_norm": 0.3101496994495392,
      "learning_rate": 9.905555555555555e-06,
      "loss": 0.0018,
      "step": 72170
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.5550037026405334,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0019,
      "step": 72180
    },
    {
      "epoch": 6.416888888888889,
      "grad_norm": 0.16113097965717316,
      "learning_rate": 9.894444444444445e-06,
      "loss": 0.0018,
      "step": 72190
    },
    {
      "epoch": 6.417777777777777,
      "grad_norm": 0.10846427083015442,
      "learning_rate": 9.888888888888889e-06,
      "loss": 0.0015,
      "step": 72200
    },
    {
      "epoch": 6.418666666666667,
      "grad_norm": 0.2618846297264099,
      "learning_rate": 9.883333333333334e-06,
      "loss": 0.002,
      "step": 72210
    },
    {
      "epoch": 6.419555555555555,
      "grad_norm": 0.051911089569330215,
      "learning_rate": 9.87777777777778e-06,
      "loss": 0.0018,
      "step": 72220
    },
    {
      "epoch": 6.420444444444445,
      "grad_norm": 0.14710116386413574,
      "learning_rate": 9.872222222222223e-06,
      "loss": 0.0017,
      "step": 72230
    },
    {
      "epoch": 6.421333333333333,
      "grad_norm": 0.3209684491157532,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0026,
      "step": 72240
    },
    {
      "epoch": 6.4222222222222225,
      "grad_norm": 0.33024147152900696,
      "learning_rate": 9.861111111111112e-06,
      "loss": 0.0013,
      "step": 72250
    },
    {
      "epoch": 6.423111111111111,
      "grad_norm": 0.08142214268445969,
      "learning_rate": 9.855555555555555e-06,
      "loss": 0.0015,
      "step": 72260
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.6408662796020508,
      "learning_rate": 9.85e-06,
      "loss": 0.0022,
      "step": 72270
    },
    {
      "epoch": 6.424888888888889,
      "grad_norm": 0.46211713552474976,
      "learning_rate": 9.844444444444446e-06,
      "loss": 0.0013,
      "step": 72280
    },
    {
      "epoch": 6.425777777777777,
      "grad_norm": 0.21788908541202545,
      "learning_rate": 9.83888888888889e-06,
      "loss": 0.0018,
      "step": 72290
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 0.5057681798934937,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0024,
      "step": 72300
    },
    {
      "epoch": 6.427555555555555,
      "grad_norm": 0.5114859938621521,
      "learning_rate": 9.827777777777778e-06,
      "loss": 0.0013,
      "step": 72310
    },
    {
      "epoch": 6.428444444444445,
      "grad_norm": 0.8002081513404846,
      "learning_rate": 9.822222222222223e-06,
      "loss": 0.0022,
      "step": 72320
    },
    {
      "epoch": 6.429333333333333,
      "grad_norm": 0.13848476111888885,
      "learning_rate": 9.816666666666667e-06,
      "loss": 0.0019,
      "step": 72330
    },
    {
      "epoch": 6.4302222222222225,
      "grad_norm": 0.29661908745765686,
      "learning_rate": 9.81111111111111e-06,
      "loss": 0.0015,
      "step": 72340
    },
    {
      "epoch": 6.431111111111111,
      "grad_norm": 0.17838455736637115,
      "learning_rate": 9.805555555555557e-06,
      "loss": 0.0012,
      "step": 72350
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.08565232157707214,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0021,
      "step": 72360
    },
    {
      "epoch": 6.432888888888889,
      "grad_norm": 0.18788877129554749,
      "learning_rate": 9.794444444444445e-06,
      "loss": 0.002,
      "step": 72370
    },
    {
      "epoch": 6.433777777777777,
      "grad_norm": 0.3980529010295868,
      "learning_rate": 9.78888888888889e-06,
      "loss": 0.0011,
      "step": 72380
    },
    {
      "epoch": 6.434666666666667,
      "grad_norm": 0.22495141625404358,
      "learning_rate": 9.783333333333333e-06,
      "loss": 0.0019,
      "step": 72390
    },
    {
      "epoch": 6.435555555555555,
      "grad_norm": 0.6210152506828308,
      "learning_rate": 9.777777777777779e-06,
      "loss": 0.0013,
      "step": 72400
    },
    {
      "epoch": 6.436444444444445,
      "grad_norm": 0.14399102330207825,
      "learning_rate": 9.772222222222222e-06,
      "loss": 0.0017,
      "step": 72410
    },
    {
      "epoch": 6.437333333333333,
      "grad_norm": 0.34226804971694946,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.0012,
      "step": 72420
    },
    {
      "epoch": 6.4382222222222225,
      "grad_norm": 0.04227449744939804,
      "learning_rate": 9.761111111111111e-06,
      "loss": 0.0016,
      "step": 72430
    },
    {
      "epoch": 6.439111111111111,
      "grad_norm": 0.4731861650943756,
      "learning_rate": 9.755555555555556e-06,
      "loss": 0.0018,
      "step": 72440
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.06366456300020218,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.0025,
      "step": 72450
    },
    {
      "epoch": 6.440888888888889,
      "grad_norm": 0.5702425241470337,
      "learning_rate": 9.744444444444445e-06,
      "loss": 0.0016,
      "step": 72460
    },
    {
      "epoch": 6.441777777777777,
      "grad_norm": 0.4387678802013397,
      "learning_rate": 9.738888888888889e-06,
      "loss": 0.0028,
      "step": 72470
    },
    {
      "epoch": 6.442666666666667,
      "grad_norm": 0.07945898175239563,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0021,
      "step": 72480
    },
    {
      "epoch": 6.443555555555555,
      "grad_norm": 0.7158785462379456,
      "learning_rate": 9.727777777777779e-06,
      "loss": 0.0014,
      "step": 72490
    },
    {
      "epoch": 6.444444444444445,
      "grad_norm": 0.1483892947435379,
      "learning_rate": 9.722222222222223e-06,
      "loss": 0.0018,
      "step": 72500
    },
    {
      "epoch": 6.445333333333333,
      "grad_norm": 0.04583403840661049,
      "learning_rate": 9.716666666666666e-06,
      "loss": 0.0017,
      "step": 72510
    },
    {
      "epoch": 6.4462222222222225,
      "grad_norm": 0.04885141924023628,
      "learning_rate": 9.711111111111111e-06,
      "loss": 0.0023,
      "step": 72520
    },
    {
      "epoch": 6.447111111111111,
      "grad_norm": 0.25305452942848206,
      "learning_rate": 9.705555555555557e-06,
      "loss": 0.002,
      "step": 72530
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.5342888832092285,
      "learning_rate": 9.7e-06,
      "loss": 0.0015,
      "step": 72540
    },
    {
      "epoch": 6.448888888888889,
      "grad_norm": 0.16450950503349304,
      "learning_rate": 9.694444444444446e-06,
      "loss": 0.0017,
      "step": 72550
    },
    {
      "epoch": 6.449777777777777,
      "grad_norm": 0.31716620922088623,
      "learning_rate": 9.688888888888889e-06,
      "loss": 0.0017,
      "step": 72560
    },
    {
      "epoch": 6.450666666666667,
      "grad_norm": 0.3611156642436981,
      "learning_rate": 9.683333333333333e-06,
      "loss": 0.0024,
      "step": 72570
    },
    {
      "epoch": 6.451555555555555,
      "grad_norm": 0.4767892360687256,
      "learning_rate": 9.677777777777778e-06,
      "loss": 0.0021,
      "step": 72580
    },
    {
      "epoch": 6.452444444444445,
      "grad_norm": 0.6827444434165955,
      "learning_rate": 9.672222222222223e-06,
      "loss": 0.0023,
      "step": 72590
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.2165241539478302,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0026,
      "step": 72600
    },
    {
      "epoch": 6.4542222222222225,
      "grad_norm": 0.14359921216964722,
      "learning_rate": 9.66111111111111e-06,
      "loss": 0.0018,
      "step": 72610
    },
    {
      "epoch": 6.455111111111111,
      "grad_norm": 0.6588262319564819,
      "learning_rate": 9.655555555555557e-06,
      "loss": 0.0016,
      "step": 72620
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.3328861594200134,
      "learning_rate": 9.65e-06,
      "loss": 0.0025,
      "step": 72630
    },
    {
      "epoch": 6.456888888888889,
      "grad_norm": 0.12153176963329315,
      "learning_rate": 9.644444444444444e-06,
      "loss": 0.002,
      "step": 72640
    },
    {
      "epoch": 6.457777777777777,
      "grad_norm": 0.04097183048725128,
      "learning_rate": 9.63888888888889e-06,
      "loss": 0.0022,
      "step": 72650
    },
    {
      "epoch": 6.458666666666667,
      "grad_norm": 0.08587275445461273,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0013,
      "step": 72660
    },
    {
      "epoch": 6.459555555555555,
      "grad_norm": 0.06940571963787079,
      "learning_rate": 9.627777777777778e-06,
      "loss": 0.0021,
      "step": 72670
    },
    {
      "epoch": 6.460444444444445,
      "grad_norm": 0.22893255949020386,
      "learning_rate": 9.622222222222222e-06,
      "loss": 0.0018,
      "step": 72680
    },
    {
      "epoch": 6.461333333333333,
      "grad_norm": 0.42108187079429626,
      "learning_rate": 9.616666666666667e-06,
      "loss": 0.0019,
      "step": 72690
    },
    {
      "epoch": 6.4622222222222225,
      "grad_norm": 0.08717048168182373,
      "learning_rate": 9.61111111111111e-06,
      "loss": 0.0021,
      "step": 72700
    },
    {
      "epoch": 6.463111111111111,
      "grad_norm": 0.47275274991989136,
      "learning_rate": 9.605555555555556e-06,
      "loss": 0.002,
      "step": 72710
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.19271685183048248,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0017,
      "step": 72720
    },
    {
      "epoch": 6.464888888888889,
      "grad_norm": 0.07155406475067139,
      "learning_rate": 9.594444444444445e-06,
      "loss": 0.0021,
      "step": 72730
    },
    {
      "epoch": 6.465777777777777,
      "grad_norm": 0.3950446844100952,
      "learning_rate": 9.588888888888888e-06,
      "loss": 0.0021,
      "step": 72740
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.28582751750946045,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.0021,
      "step": 72750
    },
    {
      "epoch": 6.467555555555555,
      "grad_norm": 0.4341561794281006,
      "learning_rate": 9.577777777777779e-06,
      "loss": 0.0025,
      "step": 72760
    },
    {
      "epoch": 6.468444444444445,
      "grad_norm": 0.5007993578910828,
      "learning_rate": 9.572222222222222e-06,
      "loss": 0.0018,
      "step": 72770
    },
    {
      "epoch": 6.469333333333333,
      "grad_norm": 0.4691463112831116,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0015,
      "step": 72780
    },
    {
      "epoch": 6.4702222222222225,
      "grad_norm": 0.23121093213558197,
      "learning_rate": 9.561111111111111e-06,
      "loss": 0.0019,
      "step": 72790
    },
    {
      "epoch": 6.471111111111111,
      "grad_norm": 0.1269780397415161,
      "learning_rate": 9.555555555555556e-06,
      "loss": 0.0021,
      "step": 72800
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.8454278707504272,
      "learning_rate": 9.55e-06,
      "loss": 0.0021,
      "step": 72810
    },
    {
      "epoch": 6.472888888888889,
      "grad_norm": 0.09261737763881683,
      "learning_rate": 9.544444444444445e-06,
      "loss": 0.0029,
      "step": 72820
    },
    {
      "epoch": 6.473777777777777,
      "grad_norm": 0.5686041712760925,
      "learning_rate": 9.538888888888889e-06,
      "loss": 0.0022,
      "step": 72830
    },
    {
      "epoch": 6.474666666666667,
      "grad_norm": 0.07322359830141068,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0014,
      "step": 72840
    },
    {
      "epoch": 6.475555555555555,
      "grad_norm": 0.331804096698761,
      "learning_rate": 9.52777777777778e-06,
      "loss": 0.0016,
      "step": 72850
    },
    {
      "epoch": 6.476444444444445,
      "grad_norm": 0.7824488878250122,
      "learning_rate": 9.522222222222223e-06,
      "loss": 0.0015,
      "step": 72860
    },
    {
      "epoch": 6.477333333333333,
      "grad_norm": 0.5036841630935669,
      "learning_rate": 9.516666666666666e-06,
      "loss": 0.0021,
      "step": 72870
    },
    {
      "epoch": 6.4782222222222225,
      "grad_norm": 0.08325587958097458,
      "learning_rate": 9.511111111111112e-06,
      "loss": 0.0023,
      "step": 72880
    },
    {
      "epoch": 6.479111111111111,
      "grad_norm": 0.41844576597213745,
      "learning_rate": 9.505555555555557e-06,
      "loss": 0.0031,
      "step": 72890
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.4572340250015259,
      "learning_rate": 9.5e-06,
      "loss": 0.002,
      "step": 72900
    },
    {
      "epoch": 6.480888888888889,
      "grad_norm": 0.11851135641336441,
      "learning_rate": 9.494444444444444e-06,
      "loss": 0.0022,
      "step": 72910
    },
    {
      "epoch": 6.481777777777777,
      "grad_norm": 0.5597885251045227,
      "learning_rate": 9.48888888888889e-06,
      "loss": 0.002,
      "step": 72920
    },
    {
      "epoch": 6.482666666666667,
      "grad_norm": 0.14304332435131073,
      "learning_rate": 9.483333333333335e-06,
      "loss": 0.0019,
      "step": 72930
    },
    {
      "epoch": 6.483555555555555,
      "grad_norm": 0.44189468026161194,
      "learning_rate": 9.477777777777778e-06,
      "loss": 0.0018,
      "step": 72940
    },
    {
      "epoch": 6.484444444444445,
      "grad_norm": 0.17740067839622498,
      "learning_rate": 9.472222222222223e-06,
      "loss": 0.0019,
      "step": 72950
    },
    {
      "epoch": 6.485333333333333,
      "grad_norm": 0.22367772459983826,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.002,
      "step": 72960
    },
    {
      "epoch": 6.4862222222222226,
      "grad_norm": 0.09249231219291687,
      "learning_rate": 9.461111111111112e-06,
      "loss": 0.0017,
      "step": 72970
    },
    {
      "epoch": 6.487111111111111,
      "grad_norm": 0.13666817545890808,
      "learning_rate": 9.455555555555556e-06,
      "loss": 0.0018,
      "step": 72980
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.061734940856695175,
      "learning_rate": 9.450000000000001e-06,
      "loss": 0.0023,
      "step": 72990
    },
    {
      "epoch": 6.488888888888889,
      "grad_norm": 0.03975740447640419,
      "learning_rate": 9.444444444444445e-06,
      "loss": 0.002,
      "step": 73000
    },
    {
      "epoch": 6.489777777777777,
      "grad_norm": 0.023838447406888008,
      "learning_rate": 9.438888888888888e-06,
      "loss": 0.0016,
      "step": 73010
    },
    {
      "epoch": 6.490666666666667,
      "grad_norm": 0.22826442122459412,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0025,
      "step": 73020
    },
    {
      "epoch": 6.491555555555555,
      "grad_norm": 0.5558551549911499,
      "learning_rate": 9.427777777777779e-06,
      "loss": 0.0018,
      "step": 73030
    },
    {
      "epoch": 6.492444444444445,
      "grad_norm": 0.18583181500434875,
      "learning_rate": 9.422222222222222e-06,
      "loss": 0.0013,
      "step": 73040
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 0.6204272508621216,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.0013,
      "step": 73050
    },
    {
      "epoch": 6.494222222222223,
      "grad_norm": 0.17756333947181702,
      "learning_rate": 9.411111111111113e-06,
      "loss": 0.0012,
      "step": 73060
    },
    {
      "epoch": 6.495111111111111,
      "grad_norm": 0.4684433341026306,
      "learning_rate": 9.405555555555556e-06,
      "loss": 0.0016,
      "step": 73070
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.7393338680267334,
      "learning_rate": 9.4e-06,
      "loss": 0.0023,
      "step": 73080
    },
    {
      "epoch": 6.496888888888889,
      "grad_norm": 0.25981763005256653,
      "learning_rate": 9.394444444444445e-06,
      "loss": 0.0019,
      "step": 73090
    },
    {
      "epoch": 6.497777777777777,
      "grad_norm": 0.051573511213064194,
      "learning_rate": 9.388888888888889e-06,
      "loss": 0.0029,
      "step": 73100
    },
    {
      "epoch": 6.498666666666667,
      "grad_norm": 0.5031315088272095,
      "learning_rate": 9.383333333333334e-06,
      "loss": 0.0025,
      "step": 73110
    },
    {
      "epoch": 6.499555555555555,
      "grad_norm": 0.5466633439064026,
      "learning_rate": 9.377777777777779e-06,
      "loss": 0.0019,
      "step": 73120
    },
    {
      "epoch": 6.500444444444445,
      "grad_norm": 0.07677401602268219,
      "learning_rate": 9.372222222222223e-06,
      "loss": 0.0017,
      "step": 73130
    },
    {
      "epoch": 6.501333333333333,
      "grad_norm": 0.20302285254001617,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0017,
      "step": 73140
    },
    {
      "epoch": 6.502222222222223,
      "grad_norm": 0.157740518450737,
      "learning_rate": 9.361111111111111e-06,
      "loss": 0.0024,
      "step": 73150
    },
    {
      "epoch": 6.503111111111111,
      "grad_norm": 0.2459993213415146,
      "learning_rate": 9.355555555555557e-06,
      "loss": 0.0018,
      "step": 73160
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.5176937580108643,
      "learning_rate": 9.35e-06,
      "loss": 0.0012,
      "step": 73170
    },
    {
      "epoch": 6.504888888888889,
      "grad_norm": 0.29770028591156006,
      "learning_rate": 9.344444444444444e-06,
      "loss": 0.0016,
      "step": 73180
    },
    {
      "epoch": 6.505777777777777,
      "grad_norm": 0.4750370383262634,
      "learning_rate": 9.338888888888889e-06,
      "loss": 0.0021,
      "step": 73190
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.3213988244533539,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0018,
      "step": 73200
    },
    {
      "epoch": 6.507555555555555,
      "grad_norm": 0.05156166851520538,
      "learning_rate": 9.327777777777778e-06,
      "loss": 0.0033,
      "step": 73210
    },
    {
      "epoch": 6.508444444444445,
      "grad_norm": 0.5035539865493774,
      "learning_rate": 9.322222222222223e-06,
      "loss": 0.0018,
      "step": 73220
    },
    {
      "epoch": 6.509333333333333,
      "grad_norm": 0.5693592429161072,
      "learning_rate": 9.316666666666667e-06,
      "loss": 0.0028,
      "step": 73230
    },
    {
      "epoch": 6.510222222222223,
      "grad_norm": 0.18177324533462524,
      "learning_rate": 9.311111111111112e-06,
      "loss": 0.002,
      "step": 73240
    },
    {
      "epoch": 6.511111111111111,
      "grad_norm": 0.14577126502990723,
      "learning_rate": 9.305555555555555e-06,
      "loss": 0.002,
      "step": 73250
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.2166360467672348,
      "learning_rate": 9.3e-06,
      "loss": 0.0019,
      "step": 73260
    },
    {
      "epoch": 6.512888888888889,
      "grad_norm": 0.2223867028951645,
      "learning_rate": 9.294444444444444e-06,
      "loss": 0.0015,
      "step": 73270
    },
    {
      "epoch": 6.5137777777777774,
      "grad_norm": 0.4664698541164398,
      "learning_rate": 9.288888888888888e-06,
      "loss": 0.0015,
      "step": 73280
    },
    {
      "epoch": 6.514666666666667,
      "grad_norm": 0.23565013706684113,
      "learning_rate": 9.283333333333335e-06,
      "loss": 0.0018,
      "step": 73290
    },
    {
      "epoch": 6.515555555555555,
      "grad_norm": 0.28574851155281067,
      "learning_rate": 9.277777777777778e-06,
      "loss": 0.0019,
      "step": 73300
    },
    {
      "epoch": 6.516444444444445,
      "grad_norm": 0.22154712677001953,
      "learning_rate": 9.272222222222222e-06,
      "loss": 0.002,
      "step": 73310
    },
    {
      "epoch": 6.517333333333333,
      "grad_norm": 0.4233132302761078,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0018,
      "step": 73320
    },
    {
      "epoch": 6.518222222222223,
      "grad_norm": 0.2880215644836426,
      "learning_rate": 9.261111111111112e-06,
      "loss": 0.0021,
      "step": 73330
    },
    {
      "epoch": 6.519111111111111,
      "grad_norm": 0.4692467153072357,
      "learning_rate": 9.255555555555556e-06,
      "loss": 0.0022,
      "step": 73340
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.5820936560630798,
      "learning_rate": 9.25e-06,
      "loss": 0.0018,
      "step": 73350
    },
    {
      "epoch": 6.520888888888889,
      "grad_norm": 0.16186991333961487,
      "learning_rate": 9.244444444444445e-06,
      "loss": 0.0016,
      "step": 73360
    },
    {
      "epoch": 6.5217777777777775,
      "grad_norm": 0.29102760553359985,
      "learning_rate": 9.23888888888889e-06,
      "loss": 0.0018,
      "step": 73370
    },
    {
      "epoch": 6.522666666666667,
      "grad_norm": 0.11345905810594559,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0014,
      "step": 73380
    },
    {
      "epoch": 6.523555555555555,
      "grad_norm": 0.2105593979358673,
      "learning_rate": 9.227777777777779e-06,
      "loss": 0.0016,
      "step": 73390
    },
    {
      "epoch": 6.524444444444445,
      "grad_norm": 0.30938372015953064,
      "learning_rate": 9.222222222222222e-06,
      "loss": 0.0017,
      "step": 73400
    },
    {
      "epoch": 6.525333333333333,
      "grad_norm": 0.2855861186981201,
      "learning_rate": 9.216666666666666e-06,
      "loss": 0.0012,
      "step": 73410
    },
    {
      "epoch": 6.526222222222223,
      "grad_norm": 0.1438388079404831,
      "learning_rate": 9.211111111111111e-06,
      "loss": 0.0019,
      "step": 73420
    },
    {
      "epoch": 6.527111111111111,
      "grad_norm": 0.05818469449877739,
      "learning_rate": 9.205555555555556e-06,
      "loss": 0.0019,
      "step": 73430
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.3905794620513916,
      "learning_rate": 9.2e-06,
      "loss": 0.0028,
      "step": 73440
    },
    {
      "epoch": 6.528888888888889,
      "grad_norm": 0.19586779177188873,
      "learning_rate": 9.194444444444445e-06,
      "loss": 0.0018,
      "step": 73450
    },
    {
      "epoch": 6.5297777777777775,
      "grad_norm": 0.503153920173645,
      "learning_rate": 9.18888888888889e-06,
      "loss": 0.0022,
      "step": 73460
    },
    {
      "epoch": 6.530666666666667,
      "grad_norm": 0.28463196754455566,
      "learning_rate": 9.183333333333334e-06,
      "loss": 0.002,
      "step": 73470
    },
    {
      "epoch": 6.531555555555555,
      "grad_norm": 0.35317355394363403,
      "learning_rate": 9.177777777777778e-06,
      "loss": 0.0017,
      "step": 73480
    },
    {
      "epoch": 6.532444444444445,
      "grad_norm": 0.2888455390930176,
      "learning_rate": 9.172222222222223e-06,
      "loss": 0.0021,
      "step": 73490
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.3271659314632416,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0024,
      "step": 73500
    },
    {
      "epoch": 6.534222222222223,
      "grad_norm": 0.04443265497684479,
      "learning_rate": 9.161111111111112e-06,
      "loss": 0.0014,
      "step": 73510
    },
    {
      "epoch": 6.535111111111111,
      "grad_norm": 0.24626395106315613,
      "learning_rate": 9.155555555555557e-06,
      "loss": 0.0014,
      "step": 73520
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.1853765845298767,
      "learning_rate": 9.15e-06,
      "loss": 0.0012,
      "step": 73530
    },
    {
      "epoch": 6.536888888888889,
      "grad_norm": 0.03695334866642952,
      "learning_rate": 9.144444444444444e-06,
      "loss": 0.0017,
      "step": 73540
    },
    {
      "epoch": 6.5377777777777775,
      "grad_norm": 0.3909774720668793,
      "learning_rate": 9.13888888888889e-06,
      "loss": 0.0017,
      "step": 73550
    },
    {
      "epoch": 6.538666666666667,
      "grad_norm": 0.4699862599372864,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0027,
      "step": 73560
    },
    {
      "epoch": 6.539555555555555,
      "grad_norm": 0.9730610251426697,
      "learning_rate": 9.127777777777778e-06,
      "loss": 0.002,
      "step": 73570
    },
    {
      "epoch": 6.540444444444445,
      "grad_norm": 0.6143330335617065,
      "learning_rate": 9.122222222222222e-06,
      "loss": 0.0021,
      "step": 73580
    },
    {
      "epoch": 6.541333333333333,
      "grad_norm": 0.6543189883232117,
      "learning_rate": 9.116666666666667e-06,
      "loss": 0.0018,
      "step": 73590
    },
    {
      "epoch": 6.542222222222223,
      "grad_norm": 0.09314646571874619,
      "learning_rate": 9.111111111111112e-06,
      "loss": 0.0018,
      "step": 73600
    },
    {
      "epoch": 6.543111111111111,
      "grad_norm": 0.2930290102958679,
      "learning_rate": 9.105555555555556e-06,
      "loss": 0.0019,
      "step": 73610
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.2927304804325104,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.0028,
      "step": 73620
    },
    {
      "epoch": 6.544888888888889,
      "grad_norm": 0.2770196795463562,
      "learning_rate": 9.094444444444445e-06,
      "loss": 0.0019,
      "step": 73630
    },
    {
      "epoch": 6.5457777777777775,
      "grad_norm": 0.39707884192466736,
      "learning_rate": 9.08888888888889e-06,
      "loss": 0.0014,
      "step": 73640
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 0.26041287183761597,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.0017,
      "step": 73650
    },
    {
      "epoch": 6.547555555555555,
      "grad_norm": 0.17728640139102936,
      "learning_rate": 9.077777777777779e-06,
      "loss": 0.0018,
      "step": 73660
    },
    {
      "epoch": 6.548444444444445,
      "grad_norm": 0.07906793802976608,
      "learning_rate": 9.072222222222222e-06,
      "loss": 0.0013,
      "step": 73670
    },
    {
      "epoch": 6.549333333333333,
      "grad_norm": 0.5686226487159729,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0021,
      "step": 73680
    },
    {
      "epoch": 6.550222222222223,
      "grad_norm": 0.5716689825057983,
      "learning_rate": 9.061111111111113e-06,
      "loss": 0.0015,
      "step": 73690
    },
    {
      "epoch": 6.551111111111111,
      "grad_norm": 0.18124467134475708,
      "learning_rate": 9.055555555555556e-06,
      "loss": 0.0023,
      "step": 73700
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.20034971833229065,
      "learning_rate": 9.05e-06,
      "loss": 0.0014,
      "step": 73710
    },
    {
      "epoch": 6.552888888888889,
      "grad_norm": 0.33315223455429077,
      "learning_rate": 9.044444444444445e-06,
      "loss": 0.0017,
      "step": 73720
    },
    {
      "epoch": 6.5537777777777775,
      "grad_norm": 0.3008519411087036,
      "learning_rate": 9.03888888888889e-06,
      "loss": 0.0032,
      "step": 73730
    },
    {
      "epoch": 6.554666666666667,
      "grad_norm": 0.15024912357330322,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0022,
      "step": 73740
    },
    {
      "epoch": 6.555555555555555,
      "grad_norm": 0.05234057828783989,
      "learning_rate": 9.027777777777777e-06,
      "loss": 0.0022,
      "step": 73750
    },
    {
      "epoch": 6.556444444444445,
      "grad_norm": 0.6637037992477417,
      "learning_rate": 9.022222222222223e-06,
      "loss": 0.002,
      "step": 73760
    },
    {
      "epoch": 6.557333333333333,
      "grad_norm": 0.2814826965332031,
      "learning_rate": 9.016666666666668e-06,
      "loss": 0.0015,
      "step": 73770
    },
    {
      "epoch": 6.558222222222222,
      "grad_norm": 0.15023575723171234,
      "learning_rate": 9.011111111111111e-06,
      "loss": 0.0013,
      "step": 73780
    },
    {
      "epoch": 6.559111111111111,
      "grad_norm": 0.10726060718297958,
      "learning_rate": 9.005555555555557e-06,
      "loss": 0.0014,
      "step": 73790
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.6189854741096497,
      "learning_rate": 9e-06,
      "loss": 0.0032,
      "step": 73800
    },
    {
      "epoch": 6.560888888888889,
      "grad_norm": 0.9689550995826721,
      "learning_rate": 8.994444444444444e-06,
      "loss": 0.0025,
      "step": 73810
    },
    {
      "epoch": 6.5617777777777775,
      "grad_norm": 0.21833831071853638,
      "learning_rate": 8.988888888888889e-06,
      "loss": 0.0022,
      "step": 73820
    },
    {
      "epoch": 6.562666666666667,
      "grad_norm": 0.05335584282875061,
      "learning_rate": 8.983333333333334e-06,
      "loss": 0.002,
      "step": 73830
    },
    {
      "epoch": 6.563555555555555,
      "grad_norm": 0.2135407030582428,
      "learning_rate": 8.977777777777778e-06,
      "loss": 0.0023,
      "step": 73840
    },
    {
      "epoch": 6.564444444444445,
      "grad_norm": 0.4164324402809143,
      "learning_rate": 8.972222222222221e-06,
      "loss": 0.0025,
      "step": 73850
    },
    {
      "epoch": 6.565333333333333,
      "grad_norm": 0.4028089642524719,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0016,
      "step": 73860
    },
    {
      "epoch": 6.566222222222223,
      "grad_norm": 0.20013415813446045,
      "learning_rate": 8.961111111111112e-06,
      "loss": 0.0021,
      "step": 73870
    },
    {
      "epoch": 6.567111111111111,
      "grad_norm": 0.6809081435203552,
      "learning_rate": 8.955555555555555e-06,
      "loss": 0.0027,
      "step": 73880
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.7262071371078491,
      "learning_rate": 8.95e-06,
      "loss": 0.0021,
      "step": 73890
    },
    {
      "epoch": 6.568888888888889,
      "grad_norm": 0.26267024874687195,
      "learning_rate": 8.944444444444444e-06,
      "loss": 0.0022,
      "step": 73900
    },
    {
      "epoch": 6.5697777777777775,
      "grad_norm": 0.2187425047159195,
      "learning_rate": 8.93888888888889e-06,
      "loss": 0.0017,
      "step": 73910
    },
    {
      "epoch": 6.570666666666667,
      "grad_norm": 0.5685309767723083,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0016,
      "step": 73920
    },
    {
      "epoch": 6.571555555555555,
      "grad_norm": 0.2186330109834671,
      "learning_rate": 8.927777777777778e-06,
      "loss": 0.0016,
      "step": 73930
    },
    {
      "epoch": 6.572444444444445,
      "grad_norm": 0.6626477837562561,
      "learning_rate": 8.922222222222222e-06,
      "loss": 0.0014,
      "step": 73940
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 0.21553552150726318,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.0012,
      "step": 73950
    },
    {
      "epoch": 6.574222222222222,
      "grad_norm": 0.12100566178560257,
      "learning_rate": 8.911111111111112e-06,
      "loss": 0.0015,
      "step": 73960
    },
    {
      "epoch": 6.575111111111111,
      "grad_norm": 0.32782822847366333,
      "learning_rate": 8.905555555555556e-06,
      "loss": 0.0013,
      "step": 73970
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.11531472951173782,
      "learning_rate": 8.9e-06,
      "loss": 0.0023,
      "step": 73980
    },
    {
      "epoch": 6.576888888888889,
      "grad_norm": 0.5715411901473999,
      "learning_rate": 8.894444444444445e-06,
      "loss": 0.0018,
      "step": 73990
    },
    {
      "epoch": 6.5777777777777775,
      "grad_norm": 0.21632708609104156,
      "learning_rate": 8.88888888888889e-06,
      "loss": 0.0015,
      "step": 74000
    },
    {
      "epoch": 6.578666666666667,
      "grad_norm": 0.10486801713705063,
      "learning_rate": 8.883333333333334e-06,
      "loss": 0.0023,
      "step": 74010
    },
    {
      "epoch": 6.579555555555555,
      "grad_norm": 0.06234325096011162,
      "learning_rate": 8.877777777777777e-06,
      "loss": 0.0018,
      "step": 74020
    },
    {
      "epoch": 6.580444444444445,
      "grad_norm": 0.4466687738895416,
      "learning_rate": 8.872222222222222e-06,
      "loss": 0.002,
      "step": 74030
    },
    {
      "epoch": 6.581333333333333,
      "grad_norm": 0.6342918276786804,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0018,
      "step": 74040
    },
    {
      "epoch": 6.582222222222223,
      "grad_norm": 0.13252712786197662,
      "learning_rate": 8.861111111111111e-06,
      "loss": 0.0016,
      "step": 74050
    },
    {
      "epoch": 6.583111111111111,
      "grad_norm": 0.11883620917797089,
      "learning_rate": 8.855555555555556e-06,
      "loss": 0.0016,
      "step": 74060
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.21745522320270538,
      "learning_rate": 8.85e-06,
      "loss": 0.0021,
      "step": 74070
    },
    {
      "epoch": 6.584888888888889,
      "grad_norm": 0.36275753378868103,
      "learning_rate": 8.844444444444445e-06,
      "loss": 0.0012,
      "step": 74080
    },
    {
      "epoch": 6.5857777777777775,
      "grad_norm": 0.1488870531320572,
      "learning_rate": 8.838888888888889e-06,
      "loss": 0.0017,
      "step": 74090
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 0.28435397148132324,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0019,
      "step": 74100
    },
    {
      "epoch": 6.587555555555555,
      "grad_norm": 0.26771101355552673,
      "learning_rate": 8.827777777777778e-06,
      "loss": 0.0018,
      "step": 74110
    },
    {
      "epoch": 6.588444444444445,
      "grad_norm": 0.46906256675720215,
      "learning_rate": 8.822222222222223e-06,
      "loss": 0.0014,
      "step": 74120
    },
    {
      "epoch": 6.589333333333333,
      "grad_norm": 0.6095225811004639,
      "learning_rate": 8.816666666666668e-06,
      "loss": 0.0017,
      "step": 74130
    },
    {
      "epoch": 6.590222222222222,
      "grad_norm": 0.16349980235099792,
      "learning_rate": 8.811111111111112e-06,
      "loss": 0.0021,
      "step": 74140
    },
    {
      "epoch": 6.591111111111111,
      "grad_norm": 0.033599093556404114,
      "learning_rate": 8.805555555555555e-06,
      "loss": 0.0021,
      "step": 74150
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.506710410118103,
      "learning_rate": 8.8e-06,
      "loss": 0.0016,
      "step": 74160
    },
    {
      "epoch": 6.592888888888889,
      "grad_norm": 0.08997657895088196,
      "learning_rate": 8.794444444444446e-06,
      "loss": 0.002,
      "step": 74170
    },
    {
      "epoch": 6.5937777777777775,
      "grad_norm": 0.05629712715744972,
      "learning_rate": 8.78888888888889e-06,
      "loss": 0.0021,
      "step": 74180
    },
    {
      "epoch": 6.594666666666667,
      "grad_norm": 0.46041086316108704,
      "learning_rate": 8.783333333333335e-06,
      "loss": 0.0034,
      "step": 74190
    },
    {
      "epoch": 6.595555555555555,
      "grad_norm": 0.6121811270713806,
      "learning_rate": 8.777777777777778e-06,
      "loss": 0.0026,
      "step": 74200
    },
    {
      "epoch": 6.596444444444445,
      "grad_norm": 0.541199266910553,
      "learning_rate": 8.772222222222222e-06,
      "loss": 0.0013,
      "step": 74210
    },
    {
      "epoch": 6.597333333333333,
      "grad_norm": 0.3763978183269501,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.0026,
      "step": 74220
    },
    {
      "epoch": 6.598222222222223,
      "grad_norm": 0.6756362318992615,
      "learning_rate": 8.761111111111112e-06,
      "loss": 0.0021,
      "step": 74230
    },
    {
      "epoch": 6.599111111111111,
      "grad_norm": 0.19768862426280975,
      "learning_rate": 8.755555555555556e-06,
      "loss": 0.0019,
      "step": 74240
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.7549811601638794,
      "learning_rate": 8.75e-06,
      "loss": 0.0016,
      "step": 74250
    },
    {
      "epoch": 6.600888888888889,
      "grad_norm": 0.2941935956478119,
      "learning_rate": 8.744444444444446e-06,
      "loss": 0.0017,
      "step": 74260
    },
    {
      "epoch": 6.6017777777777775,
      "grad_norm": 0.44976183772087097,
      "learning_rate": 8.73888888888889e-06,
      "loss": 0.0016,
      "step": 74270
    },
    {
      "epoch": 6.602666666666667,
      "grad_norm": 0.1686955839395523,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0017,
      "step": 74280
    },
    {
      "epoch": 6.603555555555555,
      "grad_norm": 0.3228292465209961,
      "learning_rate": 8.727777777777779e-06,
      "loss": 0.002,
      "step": 74290
    },
    {
      "epoch": 6.604444444444445,
      "grad_norm": 0.5411036610603333,
      "learning_rate": 8.722222222222224e-06,
      "loss": 0.0018,
      "step": 74300
    },
    {
      "epoch": 6.605333333333333,
      "grad_norm": 0.4756311774253845,
      "learning_rate": 8.716666666666667e-06,
      "loss": 0.002,
      "step": 74310
    },
    {
      "epoch": 6.606222222222222,
      "grad_norm": 0.06140720844268799,
      "learning_rate": 8.711111111111111e-06,
      "loss": 0.0014,
      "step": 74320
    },
    {
      "epoch": 6.607111111111111,
      "grad_norm": 0.18442922830581665,
      "learning_rate": 8.705555555555556e-06,
      "loss": 0.002,
      "step": 74330
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.3840741813182831,
      "learning_rate": 8.7e-06,
      "loss": 0.0014,
      "step": 74340
    },
    {
      "epoch": 6.608888888888889,
      "grad_norm": 0.15655554831027985,
      "learning_rate": 8.694444444444445e-06,
      "loss": 0.0016,
      "step": 74350
    },
    {
      "epoch": 6.6097777777777775,
      "grad_norm": 0.2966312766075134,
      "learning_rate": 8.68888888888889e-06,
      "loss": 0.0018,
      "step": 74360
    },
    {
      "epoch": 6.610666666666667,
      "grad_norm": 0.19824986159801483,
      "learning_rate": 8.683333333333334e-06,
      "loss": 0.0015,
      "step": 74370
    },
    {
      "epoch": 6.611555555555555,
      "grad_norm": 0.2614752948284149,
      "learning_rate": 8.677777777777777e-06,
      "loss": 0.0013,
      "step": 74380
    },
    {
      "epoch": 6.612444444444445,
      "grad_norm": 0.3657863140106201,
      "learning_rate": 8.672222222222223e-06,
      "loss": 0.0016,
      "step": 74390
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.5310876965522766,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0021,
      "step": 74400
    },
    {
      "epoch": 6.614222222222223,
      "grad_norm": 0.36115387082099915,
      "learning_rate": 8.661111111111111e-06,
      "loss": 0.0026,
      "step": 74410
    },
    {
      "epoch": 6.615111111111111,
      "grad_norm": 0.49107012152671814,
      "learning_rate": 8.655555555555555e-06,
      "loss": 0.0014,
      "step": 74420
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.08216048777103424,
      "learning_rate": 8.65e-06,
      "loss": 0.002,
      "step": 74430
    },
    {
      "epoch": 6.616888888888889,
      "grad_norm": 0.365601122379303,
      "learning_rate": 8.644444444444445e-06,
      "loss": 0.0015,
      "step": 74440
    },
    {
      "epoch": 6.6177777777777775,
      "grad_norm": 0.05459141731262207,
      "learning_rate": 8.638888888888889e-06,
      "loss": 0.0015,
      "step": 74450
    },
    {
      "epoch": 6.618666666666667,
      "grad_norm": 0.25518661737442017,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.0027,
      "step": 74460
    },
    {
      "epoch": 6.619555555555555,
      "grad_norm": 0.22207513451576233,
      "learning_rate": 8.627777777777778e-06,
      "loss": 0.0025,
      "step": 74470
    },
    {
      "epoch": 6.620444444444445,
      "grad_norm": 0.07554884999990463,
      "learning_rate": 8.622222222222223e-06,
      "loss": 0.0019,
      "step": 74480
    },
    {
      "epoch": 6.621333333333333,
      "grad_norm": 0.20947717130184174,
      "learning_rate": 8.616666666666667e-06,
      "loss": 0.0013,
      "step": 74490
    },
    {
      "epoch": 6.622222222222222,
      "grad_norm": 0.05894395709037781,
      "learning_rate": 8.611111111111112e-06,
      "loss": 0.0014,
      "step": 74500
    },
    {
      "epoch": 6.623111111111111,
      "grad_norm": 0.3018840253353119,
      "learning_rate": 8.605555555555555e-06,
      "loss": 0.0021,
      "step": 74510
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.2600473165512085,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0021,
      "step": 74520
    },
    {
      "epoch": 6.624888888888889,
      "grad_norm": 0.1938290297985077,
      "learning_rate": 8.594444444444446e-06,
      "loss": 0.0022,
      "step": 74530
    },
    {
      "epoch": 6.6257777777777775,
      "grad_norm": 0.36492347717285156,
      "learning_rate": 8.58888888888889e-06,
      "loss": 0.0013,
      "step": 74540
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.2813529670238495,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.0017,
      "step": 74550
    },
    {
      "epoch": 6.627555555555555,
      "grad_norm": 0.6793195009231567,
      "learning_rate": 8.577777777777778e-06,
      "loss": 0.0016,
      "step": 74560
    },
    {
      "epoch": 6.628444444444445,
      "grad_norm": 0.21580344438552856,
      "learning_rate": 8.572222222222224e-06,
      "loss": 0.0016,
      "step": 74570
    },
    {
      "epoch": 6.629333333333333,
      "grad_norm": 0.48036277294158936,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0024,
      "step": 74580
    },
    {
      "epoch": 6.630222222222223,
      "grad_norm": 0.15445846319198608,
      "learning_rate": 8.56111111111111e-06,
      "loss": 0.0014,
      "step": 74590
    },
    {
      "epoch": 6.631111111111111,
      "grad_norm": 0.5236170887947083,
      "learning_rate": 8.555555555555556e-06,
      "loss": 0.0023,
      "step": 74600
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.6673071980476379,
      "learning_rate": 8.550000000000001e-06,
      "loss": 0.003,
      "step": 74610
    },
    {
      "epoch": 6.632888888888889,
      "grad_norm": 0.4406417906284332,
      "learning_rate": 8.544444444444445e-06,
      "loss": 0.0015,
      "step": 74620
    },
    {
      "epoch": 6.6337777777777776,
      "grad_norm": 0.05794365331530571,
      "learning_rate": 8.53888888888889e-06,
      "loss": 0.0026,
      "step": 74630
    },
    {
      "epoch": 6.634666666666667,
      "grad_norm": 0.10417512059211731,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0023,
      "step": 74640
    },
    {
      "epoch": 6.635555555555555,
      "grad_norm": 0.0422283411026001,
      "learning_rate": 8.527777777777777e-06,
      "loss": 0.0016,
      "step": 74650
    },
    {
      "epoch": 6.636444444444445,
      "grad_norm": 0.26483839750289917,
      "learning_rate": 8.522222222222222e-06,
      "loss": 0.002,
      "step": 74660
    },
    {
      "epoch": 6.637333333333333,
      "grad_norm": 0.34822750091552734,
      "learning_rate": 8.516666666666668e-06,
      "loss": 0.0023,
      "step": 74670
    },
    {
      "epoch": 6.638222222222222,
      "grad_norm": 0.32041826844215393,
      "learning_rate": 8.511111111111111e-06,
      "loss": 0.0017,
      "step": 74680
    },
    {
      "epoch": 6.639111111111111,
      "grad_norm": 0.8141930103302002,
      "learning_rate": 8.505555555555555e-06,
      "loss": 0.0026,
      "step": 74690
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.041898999363183975,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0019,
      "step": 74700
    },
    {
      "epoch": 6.640888888888889,
      "grad_norm": 0.06047496944665909,
      "learning_rate": 8.494444444444445e-06,
      "loss": 0.002,
      "step": 74710
    },
    {
      "epoch": 6.641777777777778,
      "grad_norm": 0.4097653925418854,
      "learning_rate": 8.488888888888889e-06,
      "loss": 0.0016,
      "step": 74720
    },
    {
      "epoch": 6.642666666666667,
      "grad_norm": 0.22066135704517365,
      "learning_rate": 8.483333333333334e-06,
      "loss": 0.0013,
      "step": 74730
    },
    {
      "epoch": 6.643555555555555,
      "grad_norm": 0.18850135803222656,
      "learning_rate": 8.477777777777778e-06,
      "loss": 0.0014,
      "step": 74740
    },
    {
      "epoch": 6.644444444444445,
      "grad_norm": 0.31015825271606445,
      "learning_rate": 8.472222222222223e-06,
      "loss": 0.0013,
      "step": 74750
    },
    {
      "epoch": 6.645333333333333,
      "grad_norm": 0.6754782795906067,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0021,
      "step": 74760
    },
    {
      "epoch": 6.646222222222223,
      "grad_norm": 0.21855680644512177,
      "learning_rate": 8.461111111111112e-06,
      "loss": 0.0021,
      "step": 74770
    },
    {
      "epoch": 6.647111111111111,
      "grad_norm": 0.03532996401190758,
      "learning_rate": 8.455555555555555e-06,
      "loss": 0.0015,
      "step": 74780
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.19846861064434052,
      "learning_rate": 8.45e-06,
      "loss": 0.0014,
      "step": 74790
    },
    {
      "epoch": 6.648888888888889,
      "grad_norm": 0.5414878129959106,
      "learning_rate": 8.444444444444446e-06,
      "loss": 0.0017,
      "step": 74800
    },
    {
      "epoch": 6.649777777777778,
      "grad_norm": 0.4374292492866516,
      "learning_rate": 8.43888888888889e-06,
      "loss": 0.0017,
      "step": 74810
    },
    {
      "epoch": 6.650666666666667,
      "grad_norm": 0.4367823004722595,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0015,
      "step": 74820
    },
    {
      "epoch": 6.651555555555555,
      "grad_norm": 0.11428400129079819,
      "learning_rate": 8.427777777777778e-06,
      "loss": 0.0021,
      "step": 74830
    },
    {
      "epoch": 6.652444444444445,
      "grad_norm": 0.36480316519737244,
      "learning_rate": 8.422222222222223e-06,
      "loss": 0.0019,
      "step": 74840
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 0.12268813699483871,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.0013,
      "step": 74850
    },
    {
      "epoch": 6.654222222222222,
      "grad_norm": 0.7957795858383179,
      "learning_rate": 8.411111111111112e-06,
      "loss": 0.002,
      "step": 74860
    },
    {
      "epoch": 6.655111111111111,
      "grad_norm": 0.458046555519104,
      "learning_rate": 8.405555555555556e-06,
      "loss": 0.003,
      "step": 74870
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.2316346913576126,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.003,
      "step": 74880
    },
    {
      "epoch": 6.656888888888889,
      "grad_norm": 0.3406916558742523,
      "learning_rate": 8.394444444444444e-06,
      "loss": 0.0021,
      "step": 74890
    },
    {
      "epoch": 6.657777777777778,
      "grad_norm": 0.2871643006801605,
      "learning_rate": 8.38888888888889e-06,
      "loss": 0.0016,
      "step": 74900
    },
    {
      "epoch": 6.658666666666667,
      "grad_norm": 0.15625447034835815,
      "learning_rate": 8.383333333333333e-06,
      "loss": 0.0015,
      "step": 74910
    },
    {
      "epoch": 6.6595555555555555,
      "grad_norm": 0.059487484395504,
      "learning_rate": 8.377777777777779e-06,
      "loss": 0.0014,
      "step": 74920
    },
    {
      "epoch": 6.660444444444444,
      "grad_norm": 0.05125024914741516,
      "learning_rate": 8.372222222222224e-06,
      "loss": 0.0016,
      "step": 74930
    },
    {
      "epoch": 6.661333333333333,
      "grad_norm": 0.24239905178546906,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0018,
      "step": 74940
    },
    {
      "epoch": 6.662222222222223,
      "grad_norm": 0.15119951963424683,
      "learning_rate": 8.361111111111111e-06,
      "loss": 0.0025,
      "step": 74950
    },
    {
      "epoch": 6.663111111111111,
      "grad_norm": 0.7243037819862366,
      "learning_rate": 8.355555555555556e-06,
      "loss": 0.0013,
      "step": 74960
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.39680415391921997,
      "learning_rate": 8.350000000000001e-06,
      "loss": 0.0019,
      "step": 74970
    },
    {
      "epoch": 6.664888888888889,
      "grad_norm": 0.2972639203071594,
      "learning_rate": 8.344444444444445e-06,
      "loss": 0.0016,
      "step": 74980
    },
    {
      "epoch": 6.665777777777778,
      "grad_norm": 0.17327064275741577,
      "learning_rate": 8.338888888888888e-06,
      "loss": 0.0015,
      "step": 74990
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.07393830269575119,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0018,
      "step": 75000
    },
    {
      "epoch": 6.6675555555555555,
      "grad_norm": 0.07811906933784485,
      "learning_rate": 8.327777777777779e-06,
      "loss": 0.0031,
      "step": 75010
    },
    {
      "epoch": 6.668444444444445,
      "grad_norm": 0.05940887704491615,
      "learning_rate": 8.322222222222223e-06,
      "loss": 0.0018,
      "step": 75020
    },
    {
      "epoch": 6.669333333333333,
      "grad_norm": 0.24234764277935028,
      "learning_rate": 8.316666666666668e-06,
      "loss": 0.0017,
      "step": 75030
    },
    {
      "epoch": 6.670222222222222,
      "grad_norm": 0.46443936228752136,
      "learning_rate": 8.311111111111111e-06,
      "loss": 0.0017,
      "step": 75040
    },
    {
      "epoch": 6.671111111111111,
      "grad_norm": 0.220261812210083,
      "learning_rate": 8.305555555555555e-06,
      "loss": 0.002,
      "step": 75050
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.04957795515656471,
      "learning_rate": 8.3e-06,
      "loss": 0.0017,
      "step": 75060
    },
    {
      "epoch": 6.672888888888889,
      "grad_norm": 0.07958894968032837,
      "learning_rate": 8.294444444444445e-06,
      "loss": 0.0018,
      "step": 75070
    },
    {
      "epoch": 6.673777777777778,
      "grad_norm": 0.06580106914043427,
      "learning_rate": 8.288888888888889e-06,
      "loss": 0.0013,
      "step": 75080
    },
    {
      "epoch": 6.674666666666667,
      "grad_norm": 0.8631419539451599,
      "learning_rate": 8.283333333333333e-06,
      "loss": 0.0037,
      "step": 75090
    },
    {
      "epoch": 6.6755555555555555,
      "grad_norm": 0.3631449043750763,
      "learning_rate": 8.27777777777778e-06,
      "loss": 0.0021,
      "step": 75100
    },
    {
      "epoch": 6.676444444444444,
      "grad_norm": 0.4346238970756531,
      "learning_rate": 8.272222222222223e-06,
      "loss": 0.0019,
      "step": 75110
    },
    {
      "epoch": 6.677333333333333,
      "grad_norm": 0.07964851707220078,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0017,
      "step": 75120
    },
    {
      "epoch": 6.678222222222222,
      "grad_norm": 0.2870883345603943,
      "learning_rate": 8.261111111111112e-06,
      "loss": 0.0017,
      "step": 75130
    },
    {
      "epoch": 6.679111111111111,
      "grad_norm": 0.42261967062950134,
      "learning_rate": 8.255555555555555e-06,
      "loss": 0.0015,
      "step": 75140
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.28262200951576233,
      "learning_rate": 8.25e-06,
      "loss": 0.0012,
      "step": 75150
    },
    {
      "epoch": 6.680888888888889,
      "grad_norm": 0.2590424418449402,
      "learning_rate": 8.244444444444444e-06,
      "loss": 0.0015,
      "step": 75160
    },
    {
      "epoch": 6.681777777777778,
      "grad_norm": 0.6758895516395569,
      "learning_rate": 8.23888888888889e-06,
      "loss": 0.0018,
      "step": 75170
    },
    {
      "epoch": 6.682666666666667,
      "grad_norm": 0.5419069528579712,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.0021,
      "step": 75180
    },
    {
      "epoch": 6.6835555555555555,
      "grad_norm": 0.22554881870746613,
      "learning_rate": 8.227777777777778e-06,
      "loss": 0.0014,
      "step": 75190
    },
    {
      "epoch": 6.684444444444445,
      "grad_norm": 0.057195089757442474,
      "learning_rate": 8.222222222222223e-06,
      "loss": 0.0017,
      "step": 75200
    },
    {
      "epoch": 6.685333333333333,
      "grad_norm": 0.5070576667785645,
      "learning_rate": 8.216666666666667e-06,
      "loss": 0.0015,
      "step": 75210
    },
    {
      "epoch": 6.686222222222222,
      "grad_norm": 0.07552936673164368,
      "learning_rate": 8.21111111111111e-06,
      "loss": 0.0013,
      "step": 75220
    },
    {
      "epoch": 6.687111111111111,
      "grad_norm": 0.26272091269493103,
      "learning_rate": 8.205555555555556e-06,
      "loss": 0.0015,
      "step": 75230
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.18121346831321716,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0016,
      "step": 75240
    },
    {
      "epoch": 6.688888888888889,
      "grad_norm": 0.1796019822359085,
      "learning_rate": 8.194444444444445e-06,
      "loss": 0.0022,
      "step": 75250
    },
    {
      "epoch": 6.689777777777778,
      "grad_norm": 0.05270301178097725,
      "learning_rate": 8.188888888888888e-06,
      "loss": 0.0021,
      "step": 75260
    },
    {
      "epoch": 6.690666666666667,
      "grad_norm": 0.11314500123262405,
      "learning_rate": 8.183333333333333e-06,
      "loss": 0.0036,
      "step": 75270
    },
    {
      "epoch": 6.6915555555555555,
      "grad_norm": 0.4288642704486847,
      "learning_rate": 8.177777777777779e-06,
      "loss": 0.0021,
      "step": 75280
    },
    {
      "epoch": 6.692444444444444,
      "grad_norm": 0.49885469675064087,
      "learning_rate": 8.172222222222222e-06,
      "loss": 0.0026,
      "step": 75290
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 0.3247448801994324,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.0019,
      "step": 75300
    },
    {
      "epoch": 6.694222222222222,
      "grad_norm": 0.2246810495853424,
      "learning_rate": 8.161111111111111e-06,
      "loss": 0.002,
      "step": 75310
    },
    {
      "epoch": 6.695111111111111,
      "grad_norm": 0.12058398872613907,
      "learning_rate": 8.155555555555556e-06,
      "loss": 0.0019,
      "step": 75320
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.18893185257911682,
      "learning_rate": 8.15e-06,
      "loss": 0.0012,
      "step": 75330
    },
    {
      "epoch": 6.696888888888889,
      "grad_norm": 0.6744188666343689,
      "learning_rate": 8.144444444444445e-06,
      "loss": 0.0025,
      "step": 75340
    },
    {
      "epoch": 6.697777777777778,
      "grad_norm": 0.49655023217201233,
      "learning_rate": 8.138888888888889e-06,
      "loss": 0.0017,
      "step": 75350
    },
    {
      "epoch": 6.698666666666667,
      "grad_norm": 0.28788796067237854,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0017,
      "step": 75360
    },
    {
      "epoch": 6.6995555555555555,
      "grad_norm": 0.39270198345184326,
      "learning_rate": 8.12777777777778e-06,
      "loss": 0.002,
      "step": 75370
    },
    {
      "epoch": 6.700444444444445,
      "grad_norm": 0.14055390655994415,
      "learning_rate": 8.122222222222223e-06,
      "loss": 0.0016,
      "step": 75380
    },
    {
      "epoch": 6.701333333333333,
      "grad_norm": 0.11112219840288162,
      "learning_rate": 8.116666666666666e-06,
      "loss": 0.0013,
      "step": 75390
    },
    {
      "epoch": 6.702222222222222,
      "grad_norm": 0.31735825538635254,
      "learning_rate": 8.111111111111112e-06,
      "loss": 0.0013,
      "step": 75400
    },
    {
      "epoch": 6.703111111111111,
      "grad_norm": 0.1803300529718399,
      "learning_rate": 8.105555555555557e-06,
      "loss": 0.0016,
      "step": 75410
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.260285884141922,
      "learning_rate": 8.1e-06,
      "loss": 0.002,
      "step": 75420
    },
    {
      "epoch": 6.704888888888889,
      "grad_norm": 0.7744823098182678,
      "learning_rate": 8.094444444444444e-06,
      "loss": 0.0013,
      "step": 75430
    },
    {
      "epoch": 6.705777777777778,
      "grad_norm": 0.19368989765644073,
      "learning_rate": 8.08888888888889e-06,
      "loss": 0.0018,
      "step": 75440
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 0.45997804403305054,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.0023,
      "step": 75450
    },
    {
      "epoch": 6.7075555555555555,
      "grad_norm": 0.28438764810562134,
      "learning_rate": 8.077777777777778e-06,
      "loss": 0.0018,
      "step": 75460
    },
    {
      "epoch": 6.708444444444444,
      "grad_norm": 0.4265231788158417,
      "learning_rate": 8.072222222222223e-06,
      "loss": 0.0016,
      "step": 75470
    },
    {
      "epoch": 6.709333333333333,
      "grad_norm": 0.08049581199884415,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0021,
      "step": 75480
    },
    {
      "epoch": 6.710222222222222,
      "grad_norm": 0.0746460035443306,
      "learning_rate": 8.06111111111111e-06,
      "loss": 0.0019,
      "step": 75490
    },
    {
      "epoch": 6.711111111111111,
      "grad_norm": 0.19987604022026062,
      "learning_rate": 8.055555555555557e-06,
      "loss": 0.002,
      "step": 75500
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.30531322956085205,
      "learning_rate": 8.050000000000001e-06,
      "loss": 0.0017,
      "step": 75510
    },
    {
      "epoch": 6.712888888888889,
      "grad_norm": 0.49440547823905945,
      "learning_rate": 8.044444444444444e-06,
      "loss": 0.0023,
      "step": 75520
    },
    {
      "epoch": 6.713777777777778,
      "grad_norm": 0.4878517985343933,
      "learning_rate": 8.03888888888889e-06,
      "loss": 0.0014,
      "step": 75530
    },
    {
      "epoch": 6.714666666666667,
      "grad_norm": 0.0844036266207695,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0028,
      "step": 75540
    },
    {
      "epoch": 6.7155555555555555,
      "grad_norm": 0.155642569065094,
      "learning_rate": 8.027777777777778e-06,
      "loss": 0.0019,
      "step": 75550
    },
    {
      "epoch": 6.716444444444445,
      "grad_norm": 0.21827925741672516,
      "learning_rate": 8.022222222222222e-06,
      "loss": 0.0016,
      "step": 75560
    },
    {
      "epoch": 6.717333333333333,
      "grad_norm": 0.14860732853412628,
      "learning_rate": 8.016666666666667e-06,
      "loss": 0.002,
      "step": 75570
    },
    {
      "epoch": 6.718222222222222,
      "grad_norm": 0.4115382134914398,
      "learning_rate": 8.01111111111111e-06,
      "loss": 0.0021,
      "step": 75580
    },
    {
      "epoch": 6.719111111111111,
      "grad_norm": 0.6104561686515808,
      "learning_rate": 8.005555555555556e-06,
      "loss": 0.0029,
      "step": 75590
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.46319398283958435,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0011,
      "step": 75600
    },
    {
      "epoch": 6.720888888888889,
      "grad_norm": 0.048250410705804825,
      "learning_rate": 7.994444444444445e-06,
      "loss": 0.0022,
      "step": 75610
    },
    {
      "epoch": 6.721777777777778,
      "grad_norm": 0.21534831821918488,
      "learning_rate": 7.988888888888888e-06,
      "loss": 0.0026,
      "step": 75620
    },
    {
      "epoch": 6.722666666666667,
      "grad_norm": 0.12004265189170837,
      "learning_rate": 7.983333333333334e-06,
      "loss": 0.0012,
      "step": 75630
    },
    {
      "epoch": 6.7235555555555555,
      "grad_norm": 0.16705456376075745,
      "learning_rate": 7.977777777777779e-06,
      "loss": 0.0017,
      "step": 75640
    },
    {
      "epoch": 6.724444444444444,
      "grad_norm": 0.045772988349199295,
      "learning_rate": 7.972222222222223e-06,
      "loss": 0.0014,
      "step": 75650
    },
    {
      "epoch": 6.725333333333333,
      "grad_norm": 0.6990137696266174,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0018,
      "step": 75660
    },
    {
      "epoch": 6.726222222222222,
      "grad_norm": 0.5905676484107971,
      "learning_rate": 7.961111111111111e-06,
      "loss": 0.002,
      "step": 75670
    },
    {
      "epoch": 6.727111111111111,
      "grad_norm": 0.2639597952365875,
      "learning_rate": 7.955555555555557e-06,
      "loss": 0.0015,
      "step": 75680
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.08740324527025223,
      "learning_rate": 7.95e-06,
      "loss": 0.0015,
      "step": 75690
    },
    {
      "epoch": 6.728888888888889,
      "grad_norm": 0.04163534194231033,
      "learning_rate": 7.944444444444445e-06,
      "loss": 0.002,
      "step": 75700
    },
    {
      "epoch": 6.729777777777778,
      "grad_norm": 0.38646993041038513,
      "learning_rate": 7.938888888888889e-06,
      "loss": 0.002,
      "step": 75710
    },
    {
      "epoch": 6.730666666666667,
      "grad_norm": 0.12559981644153595,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0013,
      "step": 75720
    },
    {
      "epoch": 6.7315555555555555,
      "grad_norm": 0.30503007769584656,
      "learning_rate": 7.927777777777778e-06,
      "loss": 0.0023,
      "step": 75730
    },
    {
      "epoch": 6.732444444444445,
      "grad_norm": 0.1858498603105545,
      "learning_rate": 7.922222222222223e-06,
      "loss": 0.0011,
      "step": 75740
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.09182146191596985,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.002,
      "step": 75750
    },
    {
      "epoch": 6.734222222222222,
      "grad_norm": 0.2403213381767273,
      "learning_rate": 7.91111111111111e-06,
      "loss": 0.0018,
      "step": 75760
    },
    {
      "epoch": 6.735111111111111,
      "grad_norm": 0.11317262798547745,
      "learning_rate": 7.905555555555557e-06,
      "loss": 0.0021,
      "step": 75770
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.14466139674186707,
      "learning_rate": 7.9e-06,
      "loss": 0.0021,
      "step": 75780
    },
    {
      "epoch": 6.736888888888889,
      "grad_norm": 0.04799303412437439,
      "learning_rate": 7.894444444444444e-06,
      "loss": 0.002,
      "step": 75790
    },
    {
      "epoch": 6.737777777777778,
      "grad_norm": 0.1367337554693222,
      "learning_rate": 7.88888888888889e-06,
      "loss": 0.0019,
      "step": 75800
    },
    {
      "epoch": 6.738666666666667,
      "grad_norm": 0.09596168249845505,
      "learning_rate": 7.883333333333335e-06,
      "loss": 0.0019,
      "step": 75810
    },
    {
      "epoch": 6.7395555555555555,
      "grad_norm": 0.04492712393403053,
      "learning_rate": 7.877777777777778e-06,
      "loss": 0.0026,
      "step": 75820
    },
    {
      "epoch": 6.740444444444444,
      "grad_norm": 0.1659577339887619,
      "learning_rate": 7.872222222222222e-06,
      "loss": 0.0018,
      "step": 75830
    },
    {
      "epoch": 6.741333333333333,
      "grad_norm": 0.21921789646148682,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0013,
      "step": 75840
    },
    {
      "epoch": 6.742222222222222,
      "grad_norm": 0.43062472343444824,
      "learning_rate": 7.861111111111112e-06,
      "loss": 0.0017,
      "step": 75850
    },
    {
      "epoch": 6.743111111111111,
      "grad_norm": 0.14342617988586426,
      "learning_rate": 7.855555555555556e-06,
      "loss": 0.0013,
      "step": 75860
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.29364025592803955,
      "learning_rate": 7.850000000000001e-06,
      "loss": 0.0024,
      "step": 75870
    },
    {
      "epoch": 6.744888888888889,
      "grad_norm": 0.5481348633766174,
      "learning_rate": 7.844444444444445e-06,
      "loss": 0.0021,
      "step": 75880
    },
    {
      "epoch": 6.745777777777778,
      "grad_norm": 0.2616499960422516,
      "learning_rate": 7.838888888888888e-06,
      "loss": 0.0015,
      "step": 75890
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.31866973638534546,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.0013,
      "step": 75900
    },
    {
      "epoch": 6.7475555555555555,
      "grad_norm": 0.14483694732189178,
      "learning_rate": 7.827777777777779e-06,
      "loss": 0.0018,
      "step": 75910
    },
    {
      "epoch": 6.748444444444445,
      "grad_norm": 0.18339556455612183,
      "learning_rate": 7.822222222222222e-06,
      "loss": 0.0016,
      "step": 75920
    },
    {
      "epoch": 6.749333333333333,
      "grad_norm": 0.06211615353822708,
      "learning_rate": 7.816666666666666e-06,
      "loss": 0.0014,
      "step": 75930
    },
    {
      "epoch": 6.750222222222222,
      "grad_norm": 0.3486117422580719,
      "learning_rate": 7.811111111111113e-06,
      "loss": 0.0019,
      "step": 75940
    },
    {
      "epoch": 6.751111111111111,
      "grad_norm": 0.09072510898113251,
      "learning_rate": 7.805555555555556e-06,
      "loss": 0.0013,
      "step": 75950
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.4688791334629059,
      "learning_rate": 7.8e-06,
      "loss": 0.0018,
      "step": 75960
    },
    {
      "epoch": 6.752888888888889,
      "grad_norm": 0.24718669056892395,
      "learning_rate": 7.794444444444445e-06,
      "loss": 0.0025,
      "step": 75970
    },
    {
      "epoch": 6.753777777777778,
      "grad_norm": 0.2148921936750412,
      "learning_rate": 7.788888888888889e-06,
      "loss": 0.0021,
      "step": 75980
    },
    {
      "epoch": 6.754666666666667,
      "grad_norm": 0.06023108959197998,
      "learning_rate": 7.783333333333334e-06,
      "loss": 0.0023,
      "step": 75990
    },
    {
      "epoch": 6.7555555555555555,
      "grad_norm": 0.08014635741710663,
      "learning_rate": 7.777777777777777e-06,
      "loss": 0.0017,
      "step": 76000
    },
    {
      "epoch": 6.756444444444444,
      "grad_norm": 0.5784754753112793,
      "learning_rate": 7.772222222222223e-06,
      "loss": 0.0022,
      "step": 76010
    },
    {
      "epoch": 6.757333333333333,
      "grad_norm": 0.1858770102262497,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0016,
      "step": 76020
    },
    {
      "epoch": 6.758222222222222,
      "grad_norm": 0.14574545621871948,
      "learning_rate": 7.761111111111112e-06,
      "loss": 0.0024,
      "step": 76030
    },
    {
      "epoch": 6.759111111111111,
      "grad_norm": 0.11818516254425049,
      "learning_rate": 7.755555555555557e-06,
      "loss": 0.0024,
      "step": 76040
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.5150976777076721,
      "learning_rate": 7.75e-06,
      "loss": 0.0023,
      "step": 76050
    },
    {
      "epoch": 6.760888888888889,
      "grad_norm": 0.4664882719516754,
      "learning_rate": 7.744444444444444e-06,
      "loss": 0.0019,
      "step": 76060
    },
    {
      "epoch": 6.761777777777778,
      "grad_norm": 0.21686014533042908,
      "learning_rate": 7.738888888888889e-06,
      "loss": 0.0023,
      "step": 76070
    },
    {
      "epoch": 6.762666666666667,
      "grad_norm": 0.08979878574609756,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0021,
      "step": 76080
    },
    {
      "epoch": 6.7635555555555555,
      "grad_norm": 0.47919484972953796,
      "learning_rate": 7.727777777777778e-06,
      "loss": 0.0017,
      "step": 76090
    },
    {
      "epoch": 6.764444444444445,
      "grad_norm": 0.5031543970108032,
      "learning_rate": 7.722222222222223e-06,
      "loss": 0.0016,
      "step": 76100
    },
    {
      "epoch": 6.765333333333333,
      "grad_norm": 0.32303133606910706,
      "learning_rate": 7.716666666666667e-06,
      "loss": 0.0021,
      "step": 76110
    },
    {
      "epoch": 6.766222222222222,
      "grad_norm": 0.10876167565584183,
      "learning_rate": 7.711111111111112e-06,
      "loss": 0.0018,
      "step": 76120
    },
    {
      "epoch": 6.767111111111111,
      "grad_norm": 0.17673029005527496,
      "learning_rate": 7.705555555555556e-06,
      "loss": 0.0019,
      "step": 76130
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.3613450229167938,
      "learning_rate": 7.7e-06,
      "loss": 0.0033,
      "step": 76140
    },
    {
      "epoch": 6.768888888888889,
      "grad_norm": 0.4118926525115967,
      "learning_rate": 7.694444444444444e-06,
      "loss": 0.0028,
      "step": 76150
    },
    {
      "epoch": 6.769777777777778,
      "grad_norm": 0.3419075310230255,
      "learning_rate": 7.68888888888889e-06,
      "loss": 0.0016,
      "step": 76160
    },
    {
      "epoch": 6.770666666666667,
      "grad_norm": 0.08925393223762512,
      "learning_rate": 7.683333333333335e-06,
      "loss": 0.0015,
      "step": 76170
    },
    {
      "epoch": 6.7715555555555556,
      "grad_norm": 0.4134766161441803,
      "learning_rate": 7.677777777777778e-06,
      "loss": 0.0012,
      "step": 76180
    },
    {
      "epoch": 6.772444444444444,
      "grad_norm": 0.32480186223983765,
      "learning_rate": 7.672222222222222e-06,
      "loss": 0.0016,
      "step": 76190
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.42785802483558655,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0015,
      "step": 76200
    },
    {
      "epoch": 6.774222222222222,
      "grad_norm": 0.04860898479819298,
      "learning_rate": 7.661111111111112e-06,
      "loss": 0.002,
      "step": 76210
    },
    {
      "epoch": 6.775111111111111,
      "grad_norm": 0.14975662529468536,
      "learning_rate": 7.655555555555556e-06,
      "loss": 0.0018,
      "step": 76220
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.26448842883110046,
      "learning_rate": 7.65e-06,
      "loss": 0.0024,
      "step": 76230
    },
    {
      "epoch": 6.776888888888889,
      "grad_norm": 0.3714534342288971,
      "learning_rate": 7.644444444444445e-06,
      "loss": 0.0022,
      "step": 76240
    },
    {
      "epoch": 6.777777777777778,
      "grad_norm": 0.21631914377212524,
      "learning_rate": 7.63888888888889e-06,
      "loss": 0.0017,
      "step": 76250
    },
    {
      "epoch": 6.778666666666666,
      "grad_norm": 0.5143804550170898,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.0017,
      "step": 76260
    },
    {
      "epoch": 6.779555555555556,
      "grad_norm": 0.08050025999546051,
      "learning_rate": 7.627777777777778e-06,
      "loss": 0.0023,
      "step": 76270
    },
    {
      "epoch": 6.780444444444445,
      "grad_norm": 0.39846450090408325,
      "learning_rate": 7.6222222222222225e-06,
      "loss": 0.0013,
      "step": 76280
    },
    {
      "epoch": 6.781333333333333,
      "grad_norm": 0.2803467810153961,
      "learning_rate": 7.616666666666666e-06,
      "loss": 0.0016,
      "step": 76290
    },
    {
      "epoch": 6.782222222222222,
      "grad_norm": 0.4744868278503418,
      "learning_rate": 7.611111111111112e-06,
      "loss": 0.0023,
      "step": 76300
    },
    {
      "epoch": 6.783111111111111,
      "grad_norm": 0.2959143817424774,
      "learning_rate": 7.605555555555556e-06,
      "loss": 0.0039,
      "step": 76310
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.3317521810531616,
      "learning_rate": 7.6e-06,
      "loss": 0.0015,
      "step": 76320
    },
    {
      "epoch": 6.784888888888889,
      "grad_norm": 0.1517898440361023,
      "learning_rate": 7.5944444444444445e-06,
      "loss": 0.0016,
      "step": 76330
    },
    {
      "epoch": 6.785777777777778,
      "grad_norm": 0.2345016598701477,
      "learning_rate": 7.58888888888889e-06,
      "loss": 0.0016,
      "step": 76340
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.5044629573822021,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.0012,
      "step": 76350
    },
    {
      "epoch": 6.787555555555556,
      "grad_norm": 0.4561709463596344,
      "learning_rate": 7.577777777777778e-06,
      "loss": 0.0024,
      "step": 76360
    },
    {
      "epoch": 6.788444444444444,
      "grad_norm": 0.3983560800552368,
      "learning_rate": 7.572222222222222e-06,
      "loss": 0.0025,
      "step": 76370
    },
    {
      "epoch": 6.789333333333333,
      "grad_norm": 0.16321557760238647,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0025,
      "step": 76380
    },
    {
      "epoch": 6.790222222222222,
      "grad_norm": 0.09803539514541626,
      "learning_rate": 7.561111111111112e-06,
      "loss": 0.0022,
      "step": 76390
    },
    {
      "epoch": 6.791111111111111,
      "grad_norm": 0.11321482062339783,
      "learning_rate": 7.555555555555556e-06,
      "loss": 0.0017,
      "step": 76400
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.07350935786962509,
      "learning_rate": 7.55e-06,
      "loss": 0.0019,
      "step": 76410
    },
    {
      "epoch": 6.792888888888889,
      "grad_norm": 0.2940131425857544,
      "learning_rate": 7.544444444444444e-06,
      "loss": 0.0015,
      "step": 76420
    },
    {
      "epoch": 6.793777777777778,
      "grad_norm": 0.16985294222831726,
      "learning_rate": 7.538888888888889e-06,
      "loss": 0.0016,
      "step": 76430
    },
    {
      "epoch": 6.794666666666666,
      "grad_norm": 0.06951296329498291,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0012,
      "step": 76440
    },
    {
      "epoch": 6.795555555555556,
      "grad_norm": 0.11314884573221207,
      "learning_rate": 7.527777777777778e-06,
      "loss": 0.0022,
      "step": 76450
    },
    {
      "epoch": 6.796444444444444,
      "grad_norm": 0.11915557086467743,
      "learning_rate": 7.5222222222222226e-06,
      "loss": 0.0019,
      "step": 76460
    },
    {
      "epoch": 6.7973333333333334,
      "grad_norm": 0.4020770788192749,
      "learning_rate": 7.516666666666668e-06,
      "loss": 0.0022,
      "step": 76470
    },
    {
      "epoch": 6.798222222222222,
      "grad_norm": 0.1404663622379303,
      "learning_rate": 7.511111111111112e-06,
      "loss": 0.002,
      "step": 76480
    },
    {
      "epoch": 6.799111111111111,
      "grad_norm": 0.6052148342132568,
      "learning_rate": 7.505555555555556e-06,
      "loss": 0.0015,
      "step": 76490
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.5477671027183533,
      "learning_rate": 7.5e-06,
      "loss": 0.0024,
      "step": 76500
    },
    {
      "epoch": 6.800888888888889,
      "grad_norm": 0.1136029064655304,
      "learning_rate": 7.494444444444445e-06,
      "loss": 0.0024,
      "step": 76510
    },
    {
      "epoch": 6.801777777777778,
      "grad_norm": 0.4387085735797882,
      "learning_rate": 7.48888888888889e-06,
      "loss": 0.0017,
      "step": 76520
    },
    {
      "epoch": 6.802666666666667,
      "grad_norm": 0.07360953092575073,
      "learning_rate": 7.483333333333334e-06,
      "loss": 0.0017,
      "step": 76530
    },
    {
      "epoch": 6.803555555555556,
      "grad_norm": 0.3745638430118561,
      "learning_rate": 7.477777777777778e-06,
      "loss": 0.0018,
      "step": 76540
    },
    {
      "epoch": 6.804444444444444,
      "grad_norm": 0.20759132504463196,
      "learning_rate": 7.472222222222222e-06,
      "loss": 0.0019,
      "step": 76550
    },
    {
      "epoch": 6.8053333333333335,
      "grad_norm": 0.043868955224752426,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0026,
      "step": 76560
    },
    {
      "epoch": 6.806222222222222,
      "grad_norm": 0.2916249930858612,
      "learning_rate": 7.461111111111112e-06,
      "loss": 0.0023,
      "step": 76570
    },
    {
      "epoch": 6.807111111111111,
      "grad_norm": 0.11614067107439041,
      "learning_rate": 7.455555555555556e-06,
      "loss": 0.0014,
      "step": 76580
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.07585922628641129,
      "learning_rate": 7.45e-06,
      "loss": 0.0014,
      "step": 76590
    },
    {
      "epoch": 6.808888888888889,
      "grad_norm": 0.03551730141043663,
      "learning_rate": 7.444444444444444e-06,
      "loss": 0.0017,
      "step": 76600
    },
    {
      "epoch": 6.809777777777778,
      "grad_norm": 0.29416003823280334,
      "learning_rate": 7.4388888888888895e-06,
      "loss": 0.0016,
      "step": 76610
    },
    {
      "epoch": 6.810666666666666,
      "grad_norm": 0.17643164098262787,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.0022,
      "step": 76620
    },
    {
      "epoch": 6.811555555555556,
      "grad_norm": 0.08150111138820648,
      "learning_rate": 7.427777777777778e-06,
      "loss": 0.0018,
      "step": 76630
    },
    {
      "epoch": 6.812444444444444,
      "grad_norm": 0.1499137282371521,
      "learning_rate": 7.422222222222222e-06,
      "loss": 0.0012,
      "step": 76640
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 0.3323296308517456,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.0033,
      "step": 76650
    },
    {
      "epoch": 6.814222222222222,
      "grad_norm": 0.29683029651641846,
      "learning_rate": 7.4111111111111115e-06,
      "loss": 0.0017,
      "step": 76660
    },
    {
      "epoch": 6.815111111111111,
      "grad_norm": 0.3035498559474945,
      "learning_rate": 7.405555555555556e-06,
      "loss": 0.0016,
      "step": 76670
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.10205747932195663,
      "learning_rate": 7.4e-06,
      "loss": 0.0019,
      "step": 76680
    },
    {
      "epoch": 6.816888888888889,
      "grad_norm": 0.43785592913627625,
      "learning_rate": 7.394444444444444e-06,
      "loss": 0.002,
      "step": 76690
    },
    {
      "epoch": 6.817777777777778,
      "grad_norm": 0.3848467767238617,
      "learning_rate": 7.38888888888889e-06,
      "loss": 0.0015,
      "step": 76700
    },
    {
      "epoch": 6.818666666666667,
      "grad_norm": 0.17917411029338837,
      "learning_rate": 7.3833333333333335e-06,
      "loss": 0.0022,
      "step": 76710
    },
    {
      "epoch": 6.819555555555556,
      "grad_norm": 0.06164627894759178,
      "learning_rate": 7.377777777777778e-06,
      "loss": 0.0029,
      "step": 76720
    },
    {
      "epoch": 6.820444444444444,
      "grad_norm": 0.2133598029613495,
      "learning_rate": 7.372222222222222e-06,
      "loss": 0.0024,
      "step": 76730
    },
    {
      "epoch": 6.8213333333333335,
      "grad_norm": 0.061289116740226746,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0019,
      "step": 76740
    },
    {
      "epoch": 6.822222222222222,
      "grad_norm": 0.5192750096321106,
      "learning_rate": 7.361111111111112e-06,
      "loss": 0.0026,
      "step": 76750
    },
    {
      "epoch": 6.823111111111111,
      "grad_norm": 0.14351539313793182,
      "learning_rate": 7.3555555555555555e-06,
      "loss": 0.0014,
      "step": 76760
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.46574658155441284,
      "learning_rate": 7.35e-06,
      "loss": 0.002,
      "step": 76770
    },
    {
      "epoch": 6.824888888888889,
      "grad_norm": 0.3611427843570709,
      "learning_rate": 7.344444444444445e-06,
      "loss": 0.0021,
      "step": 76780
    },
    {
      "epoch": 6.825777777777778,
      "grad_norm": 0.20222070813179016,
      "learning_rate": 7.33888888888889e-06,
      "loss": 0.0015,
      "step": 76790
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.47729989886283875,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0015,
      "step": 76800
    },
    {
      "epoch": 6.827555555555556,
      "grad_norm": 0.11797256767749786,
      "learning_rate": 7.3277777777777775e-06,
      "loss": 0.0019,
      "step": 76810
    },
    {
      "epoch": 6.828444444444444,
      "grad_norm": 0.12026834487915039,
      "learning_rate": 7.322222222222222e-06,
      "loss": 0.0013,
      "step": 76820
    },
    {
      "epoch": 6.8293333333333335,
      "grad_norm": 0.4380146861076355,
      "learning_rate": 7.316666666666667e-06,
      "loss": 0.0013,
      "step": 76830
    },
    {
      "epoch": 6.830222222222222,
      "grad_norm": 0.3370363414287567,
      "learning_rate": 7.311111111111112e-06,
      "loss": 0.0017,
      "step": 76840
    },
    {
      "epoch": 6.831111111111111,
      "grad_norm": 0.43409600853919983,
      "learning_rate": 7.305555555555556e-06,
      "loss": 0.0019,
      "step": 76850
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.04919886961579323,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0021,
      "step": 76860
    },
    {
      "epoch": 6.832888888888889,
      "grad_norm": 0.25600573420524597,
      "learning_rate": 7.294444444444446e-06,
      "loss": 0.0017,
      "step": 76870
    },
    {
      "epoch": 6.833777777777778,
      "grad_norm": 0.11450769752264023,
      "learning_rate": 7.288888888888889e-06,
      "loss": 0.0019,
      "step": 76880
    },
    {
      "epoch": 6.834666666666667,
      "grad_norm": 0.08613551408052444,
      "learning_rate": 7.283333333333334e-06,
      "loss": 0.0015,
      "step": 76890
    },
    {
      "epoch": 6.835555555555556,
      "grad_norm": 0.2556709349155426,
      "learning_rate": 7.277777777777778e-06,
      "loss": 0.0022,
      "step": 76900
    },
    {
      "epoch": 6.836444444444444,
      "grad_norm": 0.2950129210948944,
      "learning_rate": 7.272222222222222e-06,
      "loss": 0.0013,
      "step": 76910
    },
    {
      "epoch": 6.8373333333333335,
      "grad_norm": 0.031073270365595818,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0024,
      "step": 76920
    },
    {
      "epoch": 6.838222222222222,
      "grad_norm": 0.3862099349498749,
      "learning_rate": 7.261111111111111e-06,
      "loss": 0.0017,
      "step": 76930
    },
    {
      "epoch": 6.839111111111111,
      "grad_norm": 0.04834858328104019,
      "learning_rate": 7.255555555555556e-06,
      "loss": 0.0015,
      "step": 76940
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.3574518859386444,
      "learning_rate": 7.25e-06,
      "loss": 0.0013,
      "step": 76950
    },
    {
      "epoch": 6.840888888888889,
      "grad_norm": 0.3569954037666321,
      "learning_rate": 7.244444444444445e-06,
      "loss": 0.0023,
      "step": 76960
    },
    {
      "epoch": 6.841777777777778,
      "grad_norm": 0.11344140022993088,
      "learning_rate": 7.23888888888889e-06,
      "loss": 0.0031,
      "step": 76970
    },
    {
      "epoch": 6.842666666666666,
      "grad_norm": 0.10690998286008835,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0014,
      "step": 76980
    },
    {
      "epoch": 6.843555555555556,
      "grad_norm": 0.48975664377212524,
      "learning_rate": 7.227777777777778e-06,
      "loss": 0.0023,
      "step": 76990
    },
    {
      "epoch": 6.844444444444444,
      "grad_norm": 0.09458838403224945,
      "learning_rate": 7.222222222222222e-06,
      "loss": 0.0016,
      "step": 77000
    },
    {
      "epoch": 6.8453333333333335,
      "grad_norm": 0.210614413022995,
      "learning_rate": 7.216666666666667e-06,
      "loss": 0.0015,
      "step": 77010
    },
    {
      "epoch": 6.846222222222222,
      "grad_norm": 0.04666265845298767,
      "learning_rate": 7.211111111111112e-06,
      "loss": 0.0019,
      "step": 77020
    },
    {
      "epoch": 6.847111111111111,
      "grad_norm": 0.5305455923080444,
      "learning_rate": 7.205555555555555e-06,
      "loss": 0.0022,
      "step": 77030
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.722514271736145,
      "learning_rate": 7.2e-06,
      "loss": 0.0015,
      "step": 77040
    },
    {
      "epoch": 6.848888888888889,
      "grad_norm": 0.257436603307724,
      "learning_rate": 7.194444444444445e-06,
      "loss": 0.0015,
      "step": 77050
    },
    {
      "epoch": 6.849777777777778,
      "grad_norm": 0.39779871702194214,
      "learning_rate": 7.188888888888889e-06,
      "loss": 0.0029,
      "step": 77060
    },
    {
      "epoch": 6.850666666666667,
      "grad_norm": 0.05876511335372925,
      "learning_rate": 7.183333333333334e-06,
      "loss": 0.0016,
      "step": 77070
    },
    {
      "epoch": 6.851555555555556,
      "grad_norm": 0.28839629888534546,
      "learning_rate": 7.177777777777778e-06,
      "loss": 0.0022,
      "step": 77080
    },
    {
      "epoch": 6.852444444444444,
      "grad_norm": 0.22284376621246338,
      "learning_rate": 7.172222222222223e-06,
      "loss": 0.0012,
      "step": 77090
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 0.10447035729885101,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.002,
      "step": 77100
    },
    {
      "epoch": 6.854222222222222,
      "grad_norm": 0.43202829360961914,
      "learning_rate": 7.161111111111111e-06,
      "loss": 0.0019,
      "step": 77110
    },
    {
      "epoch": 6.855111111111111,
      "grad_norm": 0.5740693807601929,
      "learning_rate": 7.155555555555556e-06,
      "loss": 0.002,
      "step": 77120
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.217631533741951,
      "learning_rate": 7.15e-06,
      "loss": 0.0013,
      "step": 77130
    },
    {
      "epoch": 6.856888888888889,
      "grad_norm": 0.07385718822479248,
      "learning_rate": 7.144444444444445e-06,
      "loss": 0.0015,
      "step": 77140
    },
    {
      "epoch": 6.857777777777778,
      "grad_norm": 0.24964724481105804,
      "learning_rate": 7.13888888888889e-06,
      "loss": 0.002,
      "step": 77150
    },
    {
      "epoch": 6.858666666666666,
      "grad_norm": 0.15298409759998322,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0015,
      "step": 77160
    },
    {
      "epoch": 6.859555555555556,
      "grad_norm": 0.19504857063293457,
      "learning_rate": 7.127777777777778e-06,
      "loss": 0.003,
      "step": 77170
    },
    {
      "epoch": 6.860444444444444,
      "grad_norm": 0.5233719348907471,
      "learning_rate": 7.122222222222223e-06,
      "loss": 0.0016,
      "step": 77180
    },
    {
      "epoch": 6.8613333333333335,
      "grad_norm": 0.18714967370033264,
      "learning_rate": 7.116666666666667e-06,
      "loss": 0.0015,
      "step": 77190
    },
    {
      "epoch": 6.862222222222222,
      "grad_norm": 0.15030239522457123,
      "learning_rate": 7.111111111111112e-06,
      "loss": 0.0016,
      "step": 77200
    },
    {
      "epoch": 6.863111111111111,
      "grad_norm": 0.49785783886909485,
      "learning_rate": 7.105555555555555e-06,
      "loss": 0.0016,
      "step": 77210
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.18101033568382263,
      "learning_rate": 7.1e-06,
      "loss": 0.002,
      "step": 77220
    },
    {
      "epoch": 6.864888888888889,
      "grad_norm": 0.2228994071483612,
      "learning_rate": 7.094444444444445e-06,
      "loss": 0.002,
      "step": 77230
    },
    {
      "epoch": 6.865777777777778,
      "grad_norm": 0.10977564007043839,
      "learning_rate": 7.0888888888888894e-06,
      "loss": 0.0019,
      "step": 77240
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.03246583417057991,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.002,
      "step": 77250
    },
    {
      "epoch": 6.867555555555556,
      "grad_norm": 0.4733707010746002,
      "learning_rate": 7.077777777777777e-06,
      "loss": 0.0017,
      "step": 77260
    },
    {
      "epoch": 6.868444444444444,
      "grad_norm": 0.1537383496761322,
      "learning_rate": 7.0722222222222235e-06,
      "loss": 0.0022,
      "step": 77270
    },
    {
      "epoch": 6.8693333333333335,
      "grad_norm": 0.28895309567451477,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0012,
      "step": 77280
    },
    {
      "epoch": 6.870222222222222,
      "grad_norm": 0.8241864442825317,
      "learning_rate": 7.0611111111111115e-06,
      "loss": 0.0024,
      "step": 77290
    },
    {
      "epoch": 6.871111111111111,
      "grad_norm": 0.399137943983078,
      "learning_rate": 7.055555555555556e-06,
      "loss": 0.0013,
      "step": 77300
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.19987262785434723,
      "learning_rate": 7.049999999999999e-06,
      "loss": 0.0015,
      "step": 77310
    },
    {
      "epoch": 6.872888888888889,
      "grad_norm": 0.6704530715942383,
      "learning_rate": 7.0444444444444455e-06,
      "loss": 0.0021,
      "step": 77320
    },
    {
      "epoch": 6.873777777777778,
      "grad_norm": 0.370944619178772,
      "learning_rate": 7.038888888888889e-06,
      "loss": 0.0019,
      "step": 77330
    },
    {
      "epoch": 6.874666666666666,
      "grad_norm": 0.6125752329826355,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.002,
      "step": 77340
    },
    {
      "epoch": 6.875555555555556,
      "grad_norm": 0.2277059257030487,
      "learning_rate": 7.027777777777778e-06,
      "loss": 0.002,
      "step": 77350
    },
    {
      "epoch": 6.876444444444444,
      "grad_norm": 0.2851937413215637,
      "learning_rate": 7.022222222222223e-06,
      "loss": 0.0014,
      "step": 77360
    },
    {
      "epoch": 6.8773333333333335,
      "grad_norm": 0.31839099526405334,
      "learning_rate": 7.0166666666666675e-06,
      "loss": 0.0016,
      "step": 77370
    },
    {
      "epoch": 6.878222222222222,
      "grad_norm": 0.6055728793144226,
      "learning_rate": 7.011111111111111e-06,
      "loss": 0.0014,
      "step": 77380
    },
    {
      "epoch": 6.879111111111111,
      "grad_norm": 0.18477556109428406,
      "learning_rate": 7.0055555555555555e-06,
      "loss": 0.0018,
      "step": 77390
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.05898624658584595,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0013,
      "step": 77400
    },
    {
      "epoch": 6.880888888888889,
      "grad_norm": 0.08703165501356125,
      "learning_rate": 6.994444444444445e-06,
      "loss": 0.0024,
      "step": 77410
    },
    {
      "epoch": 6.881777777777778,
      "grad_norm": 0.28947892785072327,
      "learning_rate": 6.9888888888888895e-06,
      "loss": 0.002,
      "step": 77420
    },
    {
      "epoch": 6.882666666666667,
      "grad_norm": 0.3329348862171173,
      "learning_rate": 6.983333333333333e-06,
      "loss": 0.0017,
      "step": 77430
    },
    {
      "epoch": 6.883555555555556,
      "grad_norm": 0.2881629168987274,
      "learning_rate": 6.9777777777777775e-06,
      "loss": 0.0018,
      "step": 77440
    },
    {
      "epoch": 6.884444444444444,
      "grad_norm": 0.372135728597641,
      "learning_rate": 6.972222222222223e-06,
      "loss": 0.0019,
      "step": 77450
    },
    {
      "epoch": 6.8853333333333335,
      "grad_norm": 0.4644182324409485,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0018,
      "step": 77460
    },
    {
      "epoch": 6.886222222222222,
      "grad_norm": 0.39998871088027954,
      "learning_rate": 6.9611111111111116e-06,
      "loss": 0.0014,
      "step": 77470
    },
    {
      "epoch": 6.887111111111111,
      "grad_norm": 0.27649179100990295,
      "learning_rate": 6.955555555555555e-06,
      "loss": 0.0018,
      "step": 77480
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.06482751667499542,
      "learning_rate": 6.950000000000001e-06,
      "loss": 0.002,
      "step": 77490
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 0.4054391384124756,
      "learning_rate": 6.944444444444445e-06,
      "loss": 0.0022,
      "step": 77500
    },
    {
      "epoch": 6.889777777777778,
      "grad_norm": 0.0862719938158989,
      "learning_rate": 6.938888888888889e-06,
      "loss": 0.0018,
      "step": 77510
    },
    {
      "epoch": 6.890666666666666,
      "grad_norm": 0.1603364646434784,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0016,
      "step": 77520
    },
    {
      "epoch": 6.891555555555556,
      "grad_norm": 0.07351110130548477,
      "learning_rate": 6.927777777777777e-06,
      "loss": 0.0016,
      "step": 77530
    },
    {
      "epoch": 6.892444444444444,
      "grad_norm": 0.28164413571357727,
      "learning_rate": 6.922222222222223e-06,
      "loss": 0.0014,
      "step": 77540
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 0.09363357722759247,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.0013,
      "step": 77550
    },
    {
      "epoch": 6.894222222222222,
      "grad_norm": 0.43324434757232666,
      "learning_rate": 6.911111111111111e-06,
      "loss": 0.0012,
      "step": 77560
    },
    {
      "epoch": 6.895111111111111,
      "grad_norm": 0.05359343811869621,
      "learning_rate": 6.905555555555556e-06,
      "loss": 0.0018,
      "step": 77570
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.28651902079582214,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.002,
      "step": 77580
    },
    {
      "epoch": 6.896888888888888,
      "grad_norm": 0.05163810774683952,
      "learning_rate": 6.894444444444445e-06,
      "loss": 0.002,
      "step": 77590
    },
    {
      "epoch": 6.897777777777778,
      "grad_norm": 0.4974203407764435,
      "learning_rate": 6.888888888888889e-06,
      "loss": 0.002,
      "step": 77600
    },
    {
      "epoch": 6.898666666666666,
      "grad_norm": 0.21939650177955627,
      "learning_rate": 6.883333333333333e-06,
      "loss": 0.0029,
      "step": 77610
    },
    {
      "epoch": 6.899555555555556,
      "grad_norm": 0.352392315864563,
      "learning_rate": 6.877777777777778e-06,
      "loss": 0.0014,
      "step": 77620
    },
    {
      "epoch": 6.900444444444444,
      "grad_norm": 0.3180251121520996,
      "learning_rate": 6.872222222222223e-06,
      "loss": 0.0017,
      "step": 77630
    },
    {
      "epoch": 6.9013333333333335,
      "grad_norm": 0.14089123904705048,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0023,
      "step": 77640
    },
    {
      "epoch": 6.902222222222222,
      "grad_norm": 0.7757255434989929,
      "learning_rate": 6.861111111111111e-06,
      "loss": 0.0024,
      "step": 77650
    },
    {
      "epoch": 6.903111111111111,
      "grad_norm": 0.24697794020175934,
      "learning_rate": 6.855555555555555e-06,
      "loss": 0.0016,
      "step": 77660
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.13776077330112457,
      "learning_rate": 6.8500000000000005e-06,
      "loss": 0.0015,
      "step": 77670
    },
    {
      "epoch": 6.904888888888889,
      "grad_norm": 0.4803132712841034,
      "learning_rate": 6.844444444444445e-06,
      "loss": 0.0019,
      "step": 77680
    },
    {
      "epoch": 6.905777777777778,
      "grad_norm": 0.08231741189956665,
      "learning_rate": 6.838888888888889e-06,
      "loss": 0.0023,
      "step": 77690
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 0.07352596521377563,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0022,
      "step": 77700
    },
    {
      "epoch": 6.907555555555556,
      "grad_norm": 0.5481918454170227,
      "learning_rate": 6.827777777777779e-06,
      "loss": 0.0017,
      "step": 77710
    },
    {
      "epoch": 6.908444444444444,
      "grad_norm": 0.05904553085565567,
      "learning_rate": 6.8222222222222225e-06,
      "loss": 0.0023,
      "step": 77720
    },
    {
      "epoch": 6.9093333333333335,
      "grad_norm": 0.719100296497345,
      "learning_rate": 6.816666666666667e-06,
      "loss": 0.002,
      "step": 77730
    },
    {
      "epoch": 6.910222222222222,
      "grad_norm": 0.4449295103549957,
      "learning_rate": 6.811111111111111e-06,
      "loss": 0.0017,
      "step": 77740
    },
    {
      "epoch": 6.911111111111111,
      "grad_norm": 0.2574372887611389,
      "learning_rate": 6.805555555555556e-06,
      "loss": 0.0019,
      "step": 77750
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.0736285150051117,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0019,
      "step": 77760
    },
    {
      "epoch": 6.912888888888888,
      "grad_norm": 0.1506539136171341,
      "learning_rate": 6.794444444444445e-06,
      "loss": 0.0019,
      "step": 77770
    },
    {
      "epoch": 6.913777777777778,
      "grad_norm": 0.18318822979927063,
      "learning_rate": 6.788888888888889e-06,
      "loss": 0.0023,
      "step": 77780
    },
    {
      "epoch": 6.914666666666666,
      "grad_norm": 0.1381121128797531,
      "learning_rate": 6.783333333333333e-06,
      "loss": 0.0016,
      "step": 77790
    },
    {
      "epoch": 6.915555555555556,
      "grad_norm": 0.14093032479286194,
      "learning_rate": 6.777777777777779e-06,
      "loss": 0.0017,
      "step": 77800
    },
    {
      "epoch": 6.916444444444444,
      "grad_norm": 0.05225106701254845,
      "learning_rate": 6.772222222222223e-06,
      "loss": 0.0026,
      "step": 77810
    },
    {
      "epoch": 6.917333333333334,
      "grad_norm": 0.5772625803947449,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.0024,
      "step": 77820
    },
    {
      "epoch": 6.918222222222222,
      "grad_norm": 0.25841522216796875,
      "learning_rate": 6.761111111111111e-06,
      "loss": 0.0013,
      "step": 77830
    },
    {
      "epoch": 6.919111111111111,
      "grad_norm": 0.40152859687805176,
      "learning_rate": 6.755555555555555e-06,
      "loss": 0.0014,
      "step": 77840
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.27163875102996826,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.0016,
      "step": 77850
    },
    {
      "epoch": 6.920888888888889,
      "grad_norm": 0.607481062412262,
      "learning_rate": 6.744444444444445e-06,
      "loss": 0.0015,
      "step": 77860
    },
    {
      "epoch": 6.921777777777778,
      "grad_norm": 0.04820455238223076,
      "learning_rate": 6.738888888888889e-06,
      "loss": 0.0012,
      "step": 77870
    },
    {
      "epoch": 6.922666666666666,
      "grad_norm": 0.17991095781326294,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0016,
      "step": 77880
    },
    {
      "epoch": 6.923555555555556,
      "grad_norm": 0.22731705009937286,
      "learning_rate": 6.727777777777779e-06,
      "loss": 0.0014,
      "step": 77890
    },
    {
      "epoch": 6.924444444444444,
      "grad_norm": 0.11828023195266724,
      "learning_rate": 6.722222222222223e-06,
      "loss": 0.0014,
      "step": 77900
    },
    {
      "epoch": 6.925333333333334,
      "grad_norm": 0.11362158507108688,
      "learning_rate": 6.716666666666667e-06,
      "loss": 0.002,
      "step": 77910
    },
    {
      "epoch": 6.926222222222222,
      "grad_norm": 0.08240292221307755,
      "learning_rate": 6.711111111111111e-06,
      "loss": 0.0021,
      "step": 77920
    },
    {
      "epoch": 6.927111111111111,
      "grad_norm": 0.32064345479011536,
      "learning_rate": 6.705555555555555e-06,
      "loss": 0.0019,
      "step": 77930
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.34913790225982666,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0018,
      "step": 77940
    },
    {
      "epoch": 6.928888888888888,
      "grad_norm": 0.1789919137954712,
      "learning_rate": 6.694444444444445e-06,
      "loss": 0.0017,
      "step": 77950
    },
    {
      "epoch": 6.929777777777778,
      "grad_norm": 0.2597544193267822,
      "learning_rate": 6.688888888888889e-06,
      "loss": 0.0019,
      "step": 77960
    },
    {
      "epoch": 6.930666666666666,
      "grad_norm": 0.2573738992214203,
      "learning_rate": 6.6833333333333334e-06,
      "loss": 0.0015,
      "step": 77970
    },
    {
      "epoch": 6.931555555555556,
      "grad_norm": 0.41464298963546753,
      "learning_rate": 6.677777777777779e-06,
      "loss": 0.0019,
      "step": 77980
    },
    {
      "epoch": 6.932444444444444,
      "grad_norm": 0.15928424894809723,
      "learning_rate": 6.672222222222223e-06,
      "loss": 0.002,
      "step": 77990
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.08923894166946411,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0016,
      "step": 78000
    },
    {
      "epoch": 6.934222222222222,
      "grad_norm": 0.4726773798465729,
      "learning_rate": 6.661111111111111e-06,
      "loss": 0.0012,
      "step": 78010
    },
    {
      "epoch": 6.9351111111111114,
      "grad_norm": 0.8959109783172607,
      "learning_rate": 6.655555555555556e-06,
      "loss": 0.0016,
      "step": 78020
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.29905492067337036,
      "learning_rate": 6.650000000000001e-06,
      "loss": 0.0019,
      "step": 78030
    },
    {
      "epoch": 6.936888888888889,
      "grad_norm": 0.052867453545331955,
      "learning_rate": 6.644444444444445e-06,
      "loss": 0.0011,
      "step": 78040
    },
    {
      "epoch": 6.937777777777778,
      "grad_norm": 0.12011023610830307,
      "learning_rate": 6.638888888888889e-06,
      "loss": 0.0024,
      "step": 78050
    },
    {
      "epoch": 6.938666666666666,
      "grad_norm": 0.0804368183016777,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.0022,
      "step": 78060
    },
    {
      "epoch": 6.939555555555556,
      "grad_norm": 0.04194016754627228,
      "learning_rate": 6.627777777777778e-06,
      "loss": 0.0014,
      "step": 78070
    },
    {
      "epoch": 6.940444444444444,
      "grad_norm": 0.2219412922859192,
      "learning_rate": 6.622222222222223e-06,
      "loss": 0.0024,
      "step": 78080
    },
    {
      "epoch": 6.941333333333334,
      "grad_norm": 0.12152576446533203,
      "learning_rate": 6.616666666666667e-06,
      "loss": 0.002,
      "step": 78090
    },
    {
      "epoch": 6.942222222222222,
      "grad_norm": 0.2351865917444229,
      "learning_rate": 6.611111111111111e-06,
      "loss": 0.0012,
      "step": 78100
    },
    {
      "epoch": 6.9431111111111115,
      "grad_norm": 0.8245082497596741,
      "learning_rate": 6.605555555555557e-06,
      "loss": 0.0018,
      "step": 78110
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.33625781536102295,
      "learning_rate": 6.6e-06,
      "loss": 0.002,
      "step": 78120
    },
    {
      "epoch": 6.9448888888888884,
      "grad_norm": 0.10350548475980759,
      "learning_rate": 6.594444444444445e-06,
      "loss": 0.0014,
      "step": 78130
    },
    {
      "epoch": 6.945777777777778,
      "grad_norm": 0.6434758901596069,
      "learning_rate": 6.588888888888889e-06,
      "loss": 0.0022,
      "step": 78140
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 0.29630205035209656,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.0017,
      "step": 78150
    },
    {
      "epoch": 6.947555555555556,
      "grad_norm": 0.6535805463790894,
      "learning_rate": 6.577777777777779e-06,
      "loss": 0.0016,
      "step": 78160
    },
    {
      "epoch": 6.948444444444444,
      "grad_norm": 0.47127199172973633,
      "learning_rate": 6.572222222222222e-06,
      "loss": 0.0027,
      "step": 78170
    },
    {
      "epoch": 6.949333333333334,
      "grad_norm": 0.11995162814855576,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0017,
      "step": 78180
    },
    {
      "epoch": 6.950222222222222,
      "grad_norm": 0.27833622694015503,
      "learning_rate": 6.561111111111111e-06,
      "loss": 0.0017,
      "step": 78190
    },
    {
      "epoch": 6.9511111111111115,
      "grad_norm": 0.10655611008405685,
      "learning_rate": 6.555555555555556e-06,
      "loss": 0.0021,
      "step": 78200
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.053316377103328705,
      "learning_rate": 6.550000000000001e-06,
      "loss": 0.0034,
      "step": 78210
    },
    {
      "epoch": 6.952888888888889,
      "grad_norm": 0.08172743022441864,
      "learning_rate": 6.544444444444444e-06,
      "loss": 0.0021,
      "step": 78220
    },
    {
      "epoch": 6.953777777777778,
      "grad_norm": 0.4028131365776062,
      "learning_rate": 6.538888888888889e-06,
      "loss": 0.0018,
      "step": 78230
    },
    {
      "epoch": 6.954666666666666,
      "grad_norm": 0.49526509642601013,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0018,
      "step": 78240
    },
    {
      "epoch": 6.955555555555556,
      "grad_norm": 0.39918074011802673,
      "learning_rate": 6.5277777777777784e-06,
      "loss": 0.0015,
      "step": 78250
    },
    {
      "epoch": 6.956444444444444,
      "grad_norm": 0.5904229879379272,
      "learning_rate": 6.522222222222223e-06,
      "loss": 0.003,
      "step": 78260
    },
    {
      "epoch": 6.957333333333334,
      "grad_norm": 0.39632266759872437,
      "learning_rate": 6.516666666666666e-06,
      "loss": 0.0017,
      "step": 78270
    },
    {
      "epoch": 6.958222222222222,
      "grad_norm": 0.544524073600769,
      "learning_rate": 6.511111111111111e-06,
      "loss": 0.0017,
      "step": 78280
    },
    {
      "epoch": 6.9591111111111115,
      "grad_norm": 0.14541342854499817,
      "learning_rate": 6.505555555555556e-06,
      "loss": 0.0024,
      "step": 78290
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.18705275654792786,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0026,
      "step": 78300
    },
    {
      "epoch": 6.9608888888888885,
      "grad_norm": 0.6503212451934814,
      "learning_rate": 6.494444444444445e-06,
      "loss": 0.0025,
      "step": 78310
    },
    {
      "epoch": 6.961777777777778,
      "grad_norm": 0.03241050988435745,
      "learning_rate": 6.488888888888888e-06,
      "loss": 0.0017,
      "step": 78320
    },
    {
      "epoch": 6.962666666666666,
      "grad_norm": 0.1332581788301468,
      "learning_rate": 6.4833333333333345e-06,
      "loss": 0.0017,
      "step": 78330
    },
    {
      "epoch": 6.963555555555556,
      "grad_norm": 0.30212295055389404,
      "learning_rate": 6.477777777777778e-06,
      "loss": 0.0015,
      "step": 78340
    },
    {
      "epoch": 6.964444444444444,
      "grad_norm": 0.038894280791282654,
      "learning_rate": 6.4722222222222225e-06,
      "loss": 0.0022,
      "step": 78350
    },
    {
      "epoch": 6.965333333333334,
      "grad_norm": 0.0390542596578598,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.002,
      "step": 78360
    },
    {
      "epoch": 6.966222222222222,
      "grad_norm": 0.6080048084259033,
      "learning_rate": 6.4611111111111104e-06,
      "loss": 0.0029,
      "step": 78370
    },
    {
      "epoch": 6.9671111111111115,
      "grad_norm": 0.24846304953098297,
      "learning_rate": 6.4555555555555565e-06,
      "loss": 0.0019,
      "step": 78380
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.7305612564086914,
      "learning_rate": 6.45e-06,
      "loss": 0.0015,
      "step": 78390
    },
    {
      "epoch": 6.968888888888889,
      "grad_norm": 0.33788684010505676,
      "learning_rate": 6.4444444444444445e-06,
      "loss": 0.0018,
      "step": 78400
    },
    {
      "epoch": 6.969777777777778,
      "grad_norm": 0.5320742130279541,
      "learning_rate": 6.438888888888889e-06,
      "loss": 0.0018,
      "step": 78410
    },
    {
      "epoch": 6.970666666666666,
      "grad_norm": 0.5072079300880432,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.0017,
      "step": 78420
    },
    {
      "epoch": 6.971555555555556,
      "grad_norm": 0.22533665597438812,
      "learning_rate": 6.4277777777777785e-06,
      "loss": 0.0015,
      "step": 78430
    },
    {
      "epoch": 6.972444444444444,
      "grad_norm": 0.16140446066856384,
      "learning_rate": 6.422222222222223e-06,
      "loss": 0.0015,
      "step": 78440
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 0.1768105924129486,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.002,
      "step": 78450
    },
    {
      "epoch": 6.974222222222222,
      "grad_norm": 0.4131937325000763,
      "learning_rate": 6.411111111111111e-06,
      "loss": 0.0013,
      "step": 78460
    },
    {
      "epoch": 6.9751111111111115,
      "grad_norm": 0.3744874596595764,
      "learning_rate": 6.405555555555556e-06,
      "loss": 0.002,
      "step": 78470
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.04012022912502289,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0016,
      "step": 78480
    },
    {
      "epoch": 6.9768888888888885,
      "grad_norm": 0.098106749355793,
      "learning_rate": 6.394444444444445e-06,
      "loss": 0.0021,
      "step": 78490
    },
    {
      "epoch": 6.977777777777778,
      "grad_norm": 0.12438812851905823,
      "learning_rate": 6.3888888888888885e-06,
      "loss": 0.0012,
      "step": 78500
    },
    {
      "epoch": 6.978666666666666,
      "grad_norm": 0.18335852026939392,
      "learning_rate": 6.383333333333335e-06,
      "loss": 0.0014,
      "step": 78510
    },
    {
      "epoch": 6.979555555555556,
      "grad_norm": 0.053002312779426575,
      "learning_rate": 6.377777777777778e-06,
      "loss": 0.0032,
      "step": 78520
    },
    {
      "epoch": 6.980444444444444,
      "grad_norm": 0.31962624192237854,
      "learning_rate": 6.3722222222222226e-06,
      "loss": 0.0017,
      "step": 78530
    },
    {
      "epoch": 6.981333333333334,
      "grad_norm": 0.3636440932750702,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0017,
      "step": 78540
    },
    {
      "epoch": 6.982222222222222,
      "grad_norm": 0.5400809049606323,
      "learning_rate": 6.3611111111111105e-06,
      "loss": 0.0022,
      "step": 78550
    },
    {
      "epoch": 6.9831111111111115,
      "grad_norm": 0.10821576416492462,
      "learning_rate": 6.355555555555557e-06,
      "loss": 0.0025,
      "step": 78560
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.1462639421224594,
      "learning_rate": 6.35e-06,
      "loss": 0.0017,
      "step": 78570
    },
    {
      "epoch": 6.984888888888889,
      "grad_norm": 0.35691535472869873,
      "learning_rate": 6.344444444444445e-06,
      "loss": 0.0027,
      "step": 78580
    },
    {
      "epoch": 6.985777777777778,
      "grad_norm": 0.07492741197347641,
      "learning_rate": 6.338888888888889e-06,
      "loss": 0.0016,
      "step": 78590
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.2916240096092224,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0014,
      "step": 78600
    },
    {
      "epoch": 6.987555555555556,
      "grad_norm": 0.567588746547699,
      "learning_rate": 6.327777777777779e-06,
      "loss": 0.0022,
      "step": 78610
    },
    {
      "epoch": 6.988444444444444,
      "grad_norm": 0.2672205865383148,
      "learning_rate": 6.322222222222222e-06,
      "loss": 0.0015,
      "step": 78620
    },
    {
      "epoch": 6.989333333333334,
      "grad_norm": 0.09673099964857101,
      "learning_rate": 6.316666666666667e-06,
      "loss": 0.0017,
      "step": 78630
    },
    {
      "epoch": 6.990222222222222,
      "grad_norm": 0.07880624383687973,
      "learning_rate": 6.311111111111112e-06,
      "loss": 0.0013,
      "step": 78640
    },
    {
      "epoch": 6.9911111111111115,
      "grad_norm": 0.6388967633247375,
      "learning_rate": 6.305555555555556e-06,
      "loss": 0.0013,
      "step": 78650
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.27532389760017395,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0023,
      "step": 78660
    },
    {
      "epoch": 6.9928888888888885,
      "grad_norm": 0.3588413894176483,
      "learning_rate": 6.294444444444444e-06,
      "loss": 0.0013,
      "step": 78670
    },
    {
      "epoch": 6.993777777777778,
      "grad_norm": 0.36003151535987854,
      "learning_rate": 6.288888888888889e-06,
      "loss": 0.0018,
      "step": 78680
    },
    {
      "epoch": 6.994666666666666,
      "grad_norm": 0.24454273283481598,
      "learning_rate": 6.283333333333334e-06,
      "loss": 0.0015,
      "step": 78690
    },
    {
      "epoch": 6.995555555555556,
      "grad_norm": 0.544805109500885,
      "learning_rate": 6.277777777777778e-06,
      "loss": 0.0017,
      "step": 78700
    },
    {
      "epoch": 6.996444444444444,
      "grad_norm": 0.07382409274578094,
      "learning_rate": 6.272222222222223e-06,
      "loss": 0.0019,
      "step": 78710
    },
    {
      "epoch": 6.997333333333334,
      "grad_norm": 0.18413527309894562,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0018,
      "step": 78720
    },
    {
      "epoch": 6.998222222222222,
      "grad_norm": 0.37706759572029114,
      "learning_rate": 6.261111111111112e-06,
      "loss": 0.0012,
      "step": 78730
    },
    {
      "epoch": 6.999111111111111,
      "grad_norm": 0.26389896869659424,
      "learning_rate": 6.255555555555556e-06,
      "loss": 0.0024,
      "step": 78740
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.1572147160768509,
      "learning_rate": 6.25e-06,
      "loss": 0.0016,
      "step": 78750
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.0018865688471123576,
      "eval_runtime": 101.4806,
      "eval_samples_per_second": 1478.115,
      "eval_steps_per_second": 36.953,
      "step": 78750
    },
    {
      "epoch": 7.0008888888888885,
      "grad_norm": 0.14758527278900146,
      "learning_rate": 6.244444444444445e-06,
      "loss": 0.0013,
      "step": 78760
    },
    {
      "epoch": 7.001777777777778,
      "grad_norm": 0.3185129463672638,
      "learning_rate": 6.238888888888889e-06,
      "loss": 0.0015,
      "step": 78770
    },
    {
      "epoch": 7.002666666666666,
      "grad_norm": 0.3369065523147583,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0017,
      "step": 78780
    },
    {
      "epoch": 7.003555555555556,
      "grad_norm": 0.2164132297039032,
      "learning_rate": 6.227777777777778e-06,
      "loss": 0.0018,
      "step": 78790
    },
    {
      "epoch": 7.004444444444444,
      "grad_norm": 0.21614325046539307,
      "learning_rate": 6.222222222222222e-06,
      "loss": 0.0018,
      "step": 78800
    },
    {
      "epoch": 7.005333333333334,
      "grad_norm": 0.3296782970428467,
      "learning_rate": 6.2166666666666676e-06,
      "loss": 0.0028,
      "step": 78810
    },
    {
      "epoch": 7.006222222222222,
      "grad_norm": 0.05614008754491806,
      "learning_rate": 6.211111111111111e-06,
      "loss": 0.0017,
      "step": 78820
    },
    {
      "epoch": 7.0071111111111115,
      "grad_norm": 0.08442460745573044,
      "learning_rate": 6.205555555555556e-06,
      "loss": 0.002,
      "step": 78830
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.3471870720386505,
      "learning_rate": 6.2e-06,
      "loss": 0.0017,
      "step": 78840
    },
    {
      "epoch": 7.0088888888888885,
      "grad_norm": 0.15272612869739532,
      "learning_rate": 6.194444444444445e-06,
      "loss": 0.0025,
      "step": 78850
    },
    {
      "epoch": 7.009777777777778,
      "grad_norm": 0.3185301423072815,
      "learning_rate": 6.18888888888889e-06,
      "loss": 0.0026,
      "step": 78860
    },
    {
      "epoch": 7.010666666666666,
      "grad_norm": 0.0465107262134552,
      "learning_rate": 6.183333333333333e-06,
      "loss": 0.0021,
      "step": 78870
    },
    {
      "epoch": 7.011555555555556,
      "grad_norm": 0.14765961468219757,
      "learning_rate": 6.177777777777778e-06,
      "loss": 0.0013,
      "step": 78880
    },
    {
      "epoch": 7.012444444444444,
      "grad_norm": 0.14162857830524445,
      "learning_rate": 6.172222222222222e-06,
      "loss": 0.0017,
      "step": 78890
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 0.3048135042190552,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0021,
      "step": 78900
    },
    {
      "epoch": 7.014222222222222,
      "grad_norm": 0.5744247436523438,
      "learning_rate": 6.161111111111112e-06,
      "loss": 0.0014,
      "step": 78910
    },
    {
      "epoch": 7.0151111111111115,
      "grad_norm": 0.6738372445106506,
      "learning_rate": 6.155555555555556e-06,
      "loss": 0.0022,
      "step": 78920
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.18728815019130707,
      "learning_rate": 6.15e-06,
      "loss": 0.0022,
      "step": 78930
    },
    {
      "epoch": 7.0168888888888885,
      "grad_norm": 0.08532077074050903,
      "learning_rate": 6.144444444444445e-06,
      "loss": 0.0026,
      "step": 78940
    },
    {
      "epoch": 7.017777777777778,
      "grad_norm": 0.4408211410045624,
      "learning_rate": 6.138888888888889e-06,
      "loss": 0.0018,
      "step": 78950
    },
    {
      "epoch": 7.018666666666666,
      "grad_norm": 0.21983066201210022,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0017,
      "step": 78960
    },
    {
      "epoch": 7.019555555555556,
      "grad_norm": 0.19131208956241608,
      "learning_rate": 6.127777777777778e-06,
      "loss": 0.0019,
      "step": 78970
    },
    {
      "epoch": 7.020444444444444,
      "grad_norm": 0.1816341131925583,
      "learning_rate": 6.1222222222222224e-06,
      "loss": 0.002,
      "step": 78980
    },
    {
      "epoch": 7.021333333333334,
      "grad_norm": 0.46237480640411377,
      "learning_rate": 6.116666666666667e-06,
      "loss": 0.0015,
      "step": 78990
    },
    {
      "epoch": 7.022222222222222,
      "grad_norm": 0.5391745567321777,
      "learning_rate": 6.111111111111111e-06,
      "loss": 0.0016,
      "step": 79000
    },
    {
      "epoch": 7.0231111111111115,
      "grad_norm": 0.16043993830680847,
      "learning_rate": 6.105555555555556e-06,
      "loss": 0.0027,
      "step": 79010
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.11344605684280396,
      "learning_rate": 6.1e-06,
      "loss": 0.0013,
      "step": 79020
    },
    {
      "epoch": 7.0248888888888885,
      "grad_norm": 0.1501646190881729,
      "learning_rate": 6.094444444444445e-06,
      "loss": 0.0017,
      "step": 79030
    },
    {
      "epoch": 7.025777777777778,
      "grad_norm": 0.640934944152832,
      "learning_rate": 6.088888888888889e-06,
      "loss": 0.0012,
      "step": 79040
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 0.39684146642684937,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.0016,
      "step": 79050
    },
    {
      "epoch": 7.027555555555556,
      "grad_norm": 0.5689534544944763,
      "learning_rate": 6.077777777777778e-06,
      "loss": 0.0024,
      "step": 79060
    },
    {
      "epoch": 7.028444444444444,
      "grad_norm": 0.03798926621675491,
      "learning_rate": 6.072222222222222e-06,
      "loss": 0.0015,
      "step": 79070
    },
    {
      "epoch": 7.029333333333334,
      "grad_norm": 0.21836118400096893,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0015,
      "step": 79080
    },
    {
      "epoch": 7.030222222222222,
      "grad_norm": 0.1551511138677597,
      "learning_rate": 6.061111111111111e-06,
      "loss": 0.0026,
      "step": 79090
    },
    {
      "epoch": 7.0311111111111115,
      "grad_norm": 0.3262406587600708,
      "learning_rate": 6.055555555555556e-06,
      "loss": 0.0024,
      "step": 79100
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.07875313609838486,
      "learning_rate": 6.0500000000000005e-06,
      "loss": 0.0021,
      "step": 79110
    },
    {
      "epoch": 7.0328888888888885,
      "grad_norm": 0.8834728598594666,
      "learning_rate": 6.044444444444445e-06,
      "loss": 0.0031,
      "step": 79120
    },
    {
      "epoch": 7.033777777777778,
      "grad_norm": 0.32203495502471924,
      "learning_rate": 6.038888888888889e-06,
      "loss": 0.0019,
      "step": 79130
    },
    {
      "epoch": 7.034666666666666,
      "grad_norm": 0.0829695388674736,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0023,
      "step": 79140
    },
    {
      "epoch": 7.035555555555556,
      "grad_norm": 0.4825649857521057,
      "learning_rate": 6.027777777777778e-06,
      "loss": 0.0014,
      "step": 79150
    },
    {
      "epoch": 7.036444444444444,
      "grad_norm": 0.13766519725322723,
      "learning_rate": 6.0222222222222225e-06,
      "loss": 0.0019,
      "step": 79160
    },
    {
      "epoch": 7.037333333333334,
      "grad_norm": 0.5653437376022339,
      "learning_rate": 6.016666666666667e-06,
      "loss": 0.0021,
      "step": 79170
    },
    {
      "epoch": 7.038222222222222,
      "grad_norm": 0.37964677810668945,
      "learning_rate": 6.011111111111111e-06,
      "loss": 0.0014,
      "step": 79180
    },
    {
      "epoch": 7.0391111111111115,
      "grad_norm": 0.1329592913389206,
      "learning_rate": 6.005555555555556e-06,
      "loss": 0.0016,
      "step": 79190
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.07999252527952194,
      "learning_rate": 6e-06,
      "loss": 0.0013,
      "step": 79200
    },
    {
      "epoch": 7.0408888888888885,
      "grad_norm": 0.18880969285964966,
      "learning_rate": 5.9944444444444446e-06,
      "loss": 0.0021,
      "step": 79210
    },
    {
      "epoch": 7.041777777777778,
      "grad_norm": 0.047801755368709564,
      "learning_rate": 5.988888888888889e-06,
      "loss": 0.0015,
      "step": 79220
    },
    {
      "epoch": 7.042666666666666,
      "grad_norm": 0.5848755836486816,
      "learning_rate": 5.983333333333334e-06,
      "loss": 0.0021,
      "step": 79230
    },
    {
      "epoch": 7.043555555555556,
      "grad_norm": 0.3605031967163086,
      "learning_rate": 5.977777777777778e-06,
      "loss": 0.0021,
      "step": 79240
    },
    {
      "epoch": 7.044444444444444,
      "grad_norm": 0.26063039898872375,
      "learning_rate": 5.972222222222223e-06,
      "loss": 0.0022,
      "step": 79250
    },
    {
      "epoch": 7.045333333333334,
      "grad_norm": 0.3285958766937256,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0014,
      "step": 79260
    },
    {
      "epoch": 7.046222222222222,
      "grad_norm": 0.46419569849967957,
      "learning_rate": 5.961111111111111e-06,
      "loss": 0.0017,
      "step": 79270
    },
    {
      "epoch": 7.0471111111111115,
      "grad_norm": 0.21539965271949768,
      "learning_rate": 5.955555555555556e-06,
      "loss": 0.0022,
      "step": 79280
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.14722613990306854,
      "learning_rate": 5.95e-06,
      "loss": 0.0017,
      "step": 79290
    },
    {
      "epoch": 7.0488888888888885,
      "grad_norm": 0.21582992374897003,
      "learning_rate": 5.944444444444445e-06,
      "loss": 0.0021,
      "step": 79300
    },
    {
      "epoch": 7.049777777777778,
      "grad_norm": 0.09234928339719772,
      "learning_rate": 5.938888888888889e-06,
      "loss": 0.0018,
      "step": 79310
    },
    {
      "epoch": 7.050666666666666,
      "grad_norm": 0.29303738474845886,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0014,
      "step": 79320
    },
    {
      "epoch": 7.051555555555556,
      "grad_norm": 0.442978173494339,
      "learning_rate": 5.927777777777778e-06,
      "loss": 0.0012,
      "step": 79330
    },
    {
      "epoch": 7.052444444444444,
      "grad_norm": 0.23936042189598083,
      "learning_rate": 5.922222222222223e-06,
      "loss": 0.0021,
      "step": 79340
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 0.17962385714054108,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.0016,
      "step": 79350
    },
    {
      "epoch": 7.054222222222222,
      "grad_norm": 0.21348929405212402,
      "learning_rate": 5.9111111111111115e-06,
      "loss": 0.0016,
      "step": 79360
    },
    {
      "epoch": 7.0551111111111116,
      "grad_norm": 0.21995380520820618,
      "learning_rate": 5.905555555555556e-06,
      "loss": 0.0018,
      "step": 79370
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.28579065203666687,
      "learning_rate": 5.9e-06,
      "loss": 0.0018,
      "step": 79380
    },
    {
      "epoch": 7.0568888888888885,
      "grad_norm": 0.18671341240406036,
      "learning_rate": 5.894444444444445e-06,
      "loss": 0.0014,
      "step": 79390
    },
    {
      "epoch": 7.057777777777778,
      "grad_norm": 0.6884050965309143,
      "learning_rate": 5.888888888888889e-06,
      "loss": 0.0023,
      "step": 79400
    },
    {
      "epoch": 7.058666666666666,
      "grad_norm": 0.25853508710861206,
      "learning_rate": 5.8833333333333335e-06,
      "loss": 0.0035,
      "step": 79410
    },
    {
      "epoch": 7.059555555555556,
      "grad_norm": 0.48452937602996826,
      "learning_rate": 5.877777777777778e-06,
      "loss": 0.0018,
      "step": 79420
    },
    {
      "epoch": 7.060444444444444,
      "grad_norm": 0.4725005626678467,
      "learning_rate": 5.872222222222223e-06,
      "loss": 0.002,
      "step": 79430
    },
    {
      "epoch": 7.061333333333334,
      "grad_norm": 0.08222725242376328,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0021,
      "step": 79440
    },
    {
      "epoch": 7.062222222222222,
      "grad_norm": 0.3638990521430969,
      "learning_rate": 5.861111111111112e-06,
      "loss": 0.0013,
      "step": 79450
    },
    {
      "epoch": 7.063111111111111,
      "grad_norm": 0.14912676811218262,
      "learning_rate": 5.8555555555555555e-06,
      "loss": 0.0014,
      "step": 79460
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.3605135977268219,
      "learning_rate": 5.850000000000001e-06,
      "loss": 0.0019,
      "step": 79470
    },
    {
      "epoch": 7.0648888888888886,
      "grad_norm": 0.488174706697464,
      "learning_rate": 5.844444444444445e-06,
      "loss": 0.0015,
      "step": 79480
    },
    {
      "epoch": 7.065777777777778,
      "grad_norm": 0.3540448248386383,
      "learning_rate": 5.838888888888889e-06,
      "loss": 0.0025,
      "step": 79490
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.34929168224334717,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0022,
      "step": 79500
    },
    {
      "epoch": 7.067555555555556,
      "grad_norm": 0.1921425461769104,
      "learning_rate": 5.8277777777777775e-06,
      "loss": 0.0016,
      "step": 79510
    },
    {
      "epoch": 7.068444444444444,
      "grad_norm": 0.07316266745328903,
      "learning_rate": 5.822222222222223e-06,
      "loss": 0.0021,
      "step": 79520
    },
    {
      "epoch": 7.069333333333334,
      "grad_norm": 0.3237050771713257,
      "learning_rate": 5.816666666666667e-06,
      "loss": 0.0028,
      "step": 79530
    },
    {
      "epoch": 7.070222222222222,
      "grad_norm": 0.46929845213890076,
      "learning_rate": 5.8111111111111116e-06,
      "loss": 0.0019,
      "step": 79540
    },
    {
      "epoch": 7.071111111111111,
      "grad_norm": 0.43339353799819946,
      "learning_rate": 5.805555555555556e-06,
      "loss": 0.002,
      "step": 79550
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.1592438966035843,
      "learning_rate": 5.8e-06,
      "loss": 0.0021,
      "step": 79560
    },
    {
      "epoch": 7.072888888888889,
      "grad_norm": 0.47432857751846313,
      "learning_rate": 5.794444444444445e-06,
      "loss": 0.0016,
      "step": 79570
    },
    {
      "epoch": 7.073777777777778,
      "grad_norm": 0.14728201925754547,
      "learning_rate": 5.788888888888889e-06,
      "loss": 0.0017,
      "step": 79580
    },
    {
      "epoch": 7.074666666666666,
      "grad_norm": 0.2087581902742386,
      "learning_rate": 5.783333333333334e-06,
      "loss": 0.0014,
      "step": 79590
    },
    {
      "epoch": 7.075555555555556,
      "grad_norm": 0.09222472459077835,
      "learning_rate": 5.777777777777778e-06,
      "loss": 0.0024,
      "step": 79600
    },
    {
      "epoch": 7.076444444444444,
      "grad_norm": 0.3439721465110779,
      "learning_rate": 5.772222222222222e-06,
      "loss": 0.0017,
      "step": 79610
    },
    {
      "epoch": 7.077333333333334,
      "grad_norm": 0.3653002679347992,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0014,
      "step": 79620
    },
    {
      "epoch": 7.078222222222222,
      "grad_norm": 0.04656131565570831,
      "learning_rate": 5.761111111111111e-06,
      "loss": 0.001,
      "step": 79630
    },
    {
      "epoch": 7.079111111111111,
      "grad_norm": 0.5001686811447144,
      "learning_rate": 5.755555555555556e-06,
      "loss": 0.0014,
      "step": 79640
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.2805296778678894,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.002,
      "step": 79650
    },
    {
      "epoch": 7.080888888888889,
      "grad_norm": 0.39543259143829346,
      "learning_rate": 5.744444444444444e-06,
      "loss": 0.0032,
      "step": 79660
    },
    {
      "epoch": 7.081777777777778,
      "grad_norm": 0.6148264408111572,
      "learning_rate": 5.73888888888889e-06,
      "loss": 0.0017,
      "step": 79670
    },
    {
      "epoch": 7.082666666666666,
      "grad_norm": 0.330365389585495,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.002,
      "step": 79680
    },
    {
      "epoch": 7.083555555555556,
      "grad_norm": 0.24479232728481293,
      "learning_rate": 5.727777777777778e-06,
      "loss": 0.0015,
      "step": 79690
    },
    {
      "epoch": 7.084444444444444,
      "grad_norm": 0.07114189118146896,
      "learning_rate": 5.722222222222223e-06,
      "loss": 0.0012,
      "step": 79700
    },
    {
      "epoch": 7.085333333333334,
      "grad_norm": 0.8488591909408569,
      "learning_rate": 5.7166666666666664e-06,
      "loss": 0.0016,
      "step": 79710
    },
    {
      "epoch": 7.086222222222222,
      "grad_norm": 0.07874790579080582,
      "learning_rate": 5.711111111111112e-06,
      "loss": 0.0026,
      "step": 79720
    },
    {
      "epoch": 7.087111111111111,
      "grad_norm": 0.5421090126037598,
      "learning_rate": 5.705555555555555e-06,
      "loss": 0.0016,
      "step": 79730
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.3553347587585449,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0016,
      "step": 79740
    },
    {
      "epoch": 7.088888888888889,
      "grad_norm": 0.2566770613193512,
      "learning_rate": 5.694444444444445e-06,
      "loss": 0.0019,
      "step": 79750
    },
    {
      "epoch": 7.089777777777778,
      "grad_norm": 0.4282025098800659,
      "learning_rate": 5.688888888888889e-06,
      "loss": 0.0015,
      "step": 79760
    },
    {
      "epoch": 7.0906666666666665,
      "grad_norm": 0.2608139216899872,
      "learning_rate": 5.683333333333334e-06,
      "loss": 0.0022,
      "step": 79770
    },
    {
      "epoch": 7.091555555555556,
      "grad_norm": 0.4963613748550415,
      "learning_rate": 5.677777777777778e-06,
      "loss": 0.0016,
      "step": 79780
    },
    {
      "epoch": 7.092444444444444,
      "grad_norm": 0.08559837192296982,
      "learning_rate": 5.6722222222222225e-06,
      "loss": 0.0015,
      "step": 79790
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.10708992928266525,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0012,
      "step": 79800
    },
    {
      "epoch": 7.094222222222222,
      "grad_norm": 0.42614781856536865,
      "learning_rate": 5.661111111111111e-06,
      "loss": 0.0017,
      "step": 79810
    },
    {
      "epoch": 7.095111111111111,
      "grad_norm": 0.11741222441196442,
      "learning_rate": 5.655555555555556e-06,
      "loss": 0.0017,
      "step": 79820
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.04995868727564812,
      "learning_rate": 5.65e-06,
      "loss": 0.0015,
      "step": 79830
    },
    {
      "epoch": 7.096888888888889,
      "grad_norm": 0.053654544055461884,
      "learning_rate": 5.6444444444444445e-06,
      "loss": 0.0027,
      "step": 79840
    },
    {
      "epoch": 7.097777777777778,
      "grad_norm": 0.410898357629776,
      "learning_rate": 5.63888888888889e-06,
      "loss": 0.0015,
      "step": 79850
    },
    {
      "epoch": 7.0986666666666665,
      "grad_norm": 0.22202397882938385,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0022,
      "step": 79860
    },
    {
      "epoch": 7.099555555555556,
      "grad_norm": 0.22346706688404083,
      "learning_rate": 5.6277777777777786e-06,
      "loss": 0.002,
      "step": 79870
    },
    {
      "epoch": 7.100444444444444,
      "grad_norm": 0.04814092814922333,
      "learning_rate": 5.622222222222222e-06,
      "loss": 0.0019,
      "step": 79880
    },
    {
      "epoch": 7.101333333333334,
      "grad_norm": 0.2559181749820709,
      "learning_rate": 5.6166666666666665e-06,
      "loss": 0.0014,
      "step": 79890
    },
    {
      "epoch": 7.102222222222222,
      "grad_norm": 0.2835308015346527,
      "learning_rate": 5.611111111111112e-06,
      "loss": 0.0018,
      "step": 79900
    },
    {
      "epoch": 7.103111111111111,
      "grad_norm": 0.08569176495075226,
      "learning_rate": 5.605555555555555e-06,
      "loss": 0.0022,
      "step": 79910
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.08426743745803833,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0025,
      "step": 79920
    },
    {
      "epoch": 7.104888888888889,
      "grad_norm": 0.36910563707351685,
      "learning_rate": 5.594444444444444e-06,
      "loss": 0.0014,
      "step": 79930
    },
    {
      "epoch": 7.105777777777778,
      "grad_norm": 0.060445547103881836,
      "learning_rate": 5.588888888888889e-06,
      "loss": 0.0012,
      "step": 79940
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 0.3222452700138092,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.0014,
      "step": 79950
    },
    {
      "epoch": 7.107555555555556,
      "grad_norm": 0.13911005854606628,
      "learning_rate": 5.577777777777778e-06,
      "loss": 0.0024,
      "step": 79960
    },
    {
      "epoch": 7.108444444444444,
      "grad_norm": 0.054695792496204376,
      "learning_rate": 5.572222222222223e-06,
      "loss": 0.0019,
      "step": 79970
    },
    {
      "epoch": 7.109333333333334,
      "grad_norm": 0.14545463025569916,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0022,
      "step": 79980
    },
    {
      "epoch": 7.110222222222222,
      "grad_norm": 0.21081219613552094,
      "learning_rate": 5.561111111111111e-06,
      "loss": 0.0015,
      "step": 79990
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 0.09279312938451767,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.0022,
      "step": 80000
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.3047797977924347,
      "learning_rate": 5.55e-06,
      "loss": 0.0018,
      "step": 80010
    },
    {
      "epoch": 7.112888888888889,
      "grad_norm": 0.368974894285202,
      "learning_rate": 5.544444444444445e-06,
      "loss": 0.0019,
      "step": 80020
    },
    {
      "epoch": 7.113777777777778,
      "grad_norm": 0.5079101324081421,
      "learning_rate": 5.538888888888889e-06,
      "loss": 0.0018,
      "step": 80030
    },
    {
      "epoch": 7.1146666666666665,
      "grad_norm": 0.6404064297676086,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0019,
      "step": 80040
    },
    {
      "epoch": 7.115555555555556,
      "grad_norm": 0.4744478464126587,
      "learning_rate": 5.527777777777778e-06,
      "loss": 0.0019,
      "step": 80050
    },
    {
      "epoch": 7.116444444444444,
      "grad_norm": 0.2658804953098297,
      "learning_rate": 5.522222222222222e-06,
      "loss": 0.0023,
      "step": 80060
    },
    {
      "epoch": 7.117333333333334,
      "grad_norm": 0.14223289489746094,
      "learning_rate": 5.5166666666666675e-06,
      "loss": 0.0021,
      "step": 80070
    },
    {
      "epoch": 7.118222222222222,
      "grad_norm": 0.3220584988594055,
      "learning_rate": 5.511111111111111e-06,
      "loss": 0.0016,
      "step": 80080
    },
    {
      "epoch": 7.119111111111111,
      "grad_norm": 0.39765310287475586,
      "learning_rate": 5.505555555555556e-06,
      "loss": 0.0019,
      "step": 80090
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.3599669635295868,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.002,
      "step": 80100
    },
    {
      "epoch": 7.120888888888889,
      "grad_norm": 0.3419334888458252,
      "learning_rate": 5.494444444444444e-06,
      "loss": 0.0014,
      "step": 80110
    },
    {
      "epoch": 7.121777777777778,
      "grad_norm": 0.16332584619522095,
      "learning_rate": 5.4888888888888895e-06,
      "loss": 0.0014,
      "step": 80120
    },
    {
      "epoch": 7.1226666666666665,
      "grad_norm": 0.06515536457300186,
      "learning_rate": 5.483333333333333e-06,
      "loss": 0.0017,
      "step": 80130
    },
    {
      "epoch": 7.123555555555556,
      "grad_norm": 0.11861451715230942,
      "learning_rate": 5.477777777777778e-06,
      "loss": 0.0018,
      "step": 80140
    },
    {
      "epoch": 7.124444444444444,
      "grad_norm": 0.31528574228286743,
      "learning_rate": 5.472222222222223e-06,
      "loss": 0.0014,
      "step": 80150
    },
    {
      "epoch": 7.125333333333334,
      "grad_norm": 0.28781965374946594,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0013,
      "step": 80160
    },
    {
      "epoch": 7.126222222222222,
      "grad_norm": 0.29204022884368896,
      "learning_rate": 5.4611111111111115e-06,
      "loss": 0.0022,
      "step": 80170
    },
    {
      "epoch": 7.127111111111111,
      "grad_norm": 0.21677619218826294,
      "learning_rate": 5.455555555555556e-06,
      "loss": 0.002,
      "step": 80180
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.4428127408027649,
      "learning_rate": 5.45e-06,
      "loss": 0.0021,
      "step": 80190
    },
    {
      "epoch": 7.128888888888889,
      "grad_norm": 0.5889549255371094,
      "learning_rate": 5.444444444444445e-06,
      "loss": 0.0022,
      "step": 80200
    },
    {
      "epoch": 7.129777777777778,
      "grad_norm": 0.19571124017238617,
      "learning_rate": 5.438888888888889e-06,
      "loss": 0.0013,
      "step": 80210
    },
    {
      "epoch": 7.1306666666666665,
      "grad_norm": 0.20917466282844543,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.0025,
      "step": 80220
    },
    {
      "epoch": 7.131555555555556,
      "grad_norm": 0.05752107873558998,
      "learning_rate": 5.427777777777778e-06,
      "loss": 0.0023,
      "step": 80230
    },
    {
      "epoch": 7.132444444444444,
      "grad_norm": 0.2123361974954605,
      "learning_rate": 5.422222222222222e-06,
      "loss": 0.0018,
      "step": 80240
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.5391719341278076,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.0019,
      "step": 80250
    },
    {
      "epoch": 7.134222222222222,
      "grad_norm": 0.37702274322509766,
      "learning_rate": 5.411111111111111e-06,
      "loss": 0.0018,
      "step": 80260
    },
    {
      "epoch": 7.135111111111111,
      "grad_norm": 0.055782757699489594,
      "learning_rate": 5.405555555555556e-06,
      "loss": 0.0014,
      "step": 80270
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.6647716760635376,
      "learning_rate": 5.4e-06,
      "loss": 0.0018,
      "step": 80280
    },
    {
      "epoch": 7.136888888888889,
      "grad_norm": 0.31670230627059937,
      "learning_rate": 5.394444444444445e-06,
      "loss": 0.0015,
      "step": 80290
    },
    {
      "epoch": 7.137777777777778,
      "grad_norm": 0.3742266595363617,
      "learning_rate": 5.388888888888889e-06,
      "loss": 0.0014,
      "step": 80300
    },
    {
      "epoch": 7.1386666666666665,
      "grad_norm": 0.14683784544467926,
      "learning_rate": 5.383333333333333e-06,
      "loss": 0.0014,
      "step": 80310
    },
    {
      "epoch": 7.139555555555556,
      "grad_norm": 0.28809404373168945,
      "learning_rate": 5.3777777777777784e-06,
      "loss": 0.0018,
      "step": 80320
    },
    {
      "epoch": 7.140444444444444,
      "grad_norm": 0.2505277097225189,
      "learning_rate": 5.372222222222222e-06,
      "loss": 0.0013,
      "step": 80330
    },
    {
      "epoch": 7.141333333333334,
      "grad_norm": 0.634981632232666,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.0023,
      "step": 80340
    },
    {
      "epoch": 7.142222222222222,
      "grad_norm": 0.18222852051258087,
      "learning_rate": 5.361111111111111e-06,
      "loss": 0.0021,
      "step": 80350
    },
    {
      "epoch": 7.143111111111111,
      "grad_norm": 0.25349652767181396,
      "learning_rate": 5.355555555555556e-06,
      "loss": 0.0015,
      "step": 80360
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.4854915738105774,
      "learning_rate": 5.3500000000000004e-06,
      "loss": 0.0017,
      "step": 80370
    },
    {
      "epoch": 7.144888888888889,
      "grad_norm": 0.6351547241210938,
      "learning_rate": 5.344444444444445e-06,
      "loss": 0.0015,
      "step": 80380
    },
    {
      "epoch": 7.145777777777778,
      "grad_norm": 0.07697968930006027,
      "learning_rate": 5.338888888888889e-06,
      "loss": 0.0014,
      "step": 80390
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.15400929749011993,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0023,
      "step": 80400
    },
    {
      "epoch": 7.147555555555556,
      "grad_norm": 0.03323754295706749,
      "learning_rate": 5.327777777777778e-06,
      "loss": 0.0015,
      "step": 80410
    },
    {
      "epoch": 7.148444444444444,
      "grad_norm": 0.42221304774284363,
      "learning_rate": 5.3222222222222225e-06,
      "loss": 0.0013,
      "step": 80420
    },
    {
      "epoch": 7.149333333333334,
      "grad_norm": 0.5280741453170776,
      "learning_rate": 5.316666666666667e-06,
      "loss": 0.0018,
      "step": 80430
    },
    {
      "epoch": 7.150222222222222,
      "grad_norm": 0.10778502374887466,
      "learning_rate": 5.311111111111111e-06,
      "loss": 0.0016,
      "step": 80440
    },
    {
      "epoch": 7.151111111111111,
      "grad_norm": 0.05214516445994377,
      "learning_rate": 5.305555555555556e-06,
      "loss": 0.0013,
      "step": 80450
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.321123868227005,
      "learning_rate": 5.3e-06,
      "loss": 0.0025,
      "step": 80460
    },
    {
      "epoch": 7.152888888888889,
      "grad_norm": 0.17164234817028046,
      "learning_rate": 5.294444444444445e-06,
      "loss": 0.0015,
      "step": 80470
    },
    {
      "epoch": 7.153777777777778,
      "grad_norm": 0.19273580610752106,
      "learning_rate": 5.288888888888889e-06,
      "loss": 0.0027,
      "step": 80480
    },
    {
      "epoch": 7.1546666666666665,
      "grad_norm": 0.45274263620376587,
      "learning_rate": 5.283333333333334e-06,
      "loss": 0.0018,
      "step": 80490
    },
    {
      "epoch": 7.155555555555556,
      "grad_norm": 0.3690692186355591,
      "learning_rate": 5.277777777777778e-06,
      "loss": 0.002,
      "step": 80500
    },
    {
      "epoch": 7.156444444444444,
      "grad_norm": 0.08023528754711151,
      "learning_rate": 5.272222222222222e-06,
      "loss": 0.0015,
      "step": 80510
    },
    {
      "epoch": 7.157333333333334,
      "grad_norm": 0.6782466769218445,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0015,
      "step": 80520
    },
    {
      "epoch": 7.158222222222222,
      "grad_norm": 0.505254864692688,
      "learning_rate": 5.261111111111111e-06,
      "loss": 0.0021,
      "step": 80530
    },
    {
      "epoch": 7.159111111111111,
      "grad_norm": 0.18552854657173157,
      "learning_rate": 5.255555555555556e-06,
      "loss": 0.0011,
      "step": 80540
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.19245536625385284,
      "learning_rate": 5.25e-06,
      "loss": 0.0018,
      "step": 80550
    },
    {
      "epoch": 7.160888888888889,
      "grad_norm": 0.407319575548172,
      "learning_rate": 5.244444444444445e-06,
      "loss": 0.0026,
      "step": 80560
    },
    {
      "epoch": 7.161777777777778,
      "grad_norm": 0.4482592046260834,
      "learning_rate": 5.238888888888889e-06,
      "loss": 0.0017,
      "step": 80570
    },
    {
      "epoch": 7.1626666666666665,
      "grad_norm": 0.5045740008354187,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.0017,
      "step": 80580
    },
    {
      "epoch": 7.163555555555556,
      "grad_norm": 0.07931724935770035,
      "learning_rate": 5.227777777777778e-06,
      "loss": 0.0015,
      "step": 80590
    },
    {
      "epoch": 7.164444444444444,
      "grad_norm": 0.06207921728491783,
      "learning_rate": 5.2222222222222226e-06,
      "loss": 0.0019,
      "step": 80600
    },
    {
      "epoch": 7.165333333333333,
      "grad_norm": 0.22085386514663696,
      "learning_rate": 5.216666666666667e-06,
      "loss": 0.0015,
      "step": 80610
    },
    {
      "epoch": 7.166222222222222,
      "grad_norm": 0.09182381629943848,
      "learning_rate": 5.211111111111111e-06,
      "loss": 0.0015,
      "step": 80620
    },
    {
      "epoch": 7.167111111111111,
      "grad_norm": 0.16157467663288116,
      "learning_rate": 5.205555555555556e-06,
      "loss": 0.0015,
      "step": 80630
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.1777319759130478,
      "learning_rate": 5.2e-06,
      "loss": 0.0015,
      "step": 80640
    },
    {
      "epoch": 7.168888888888889,
      "grad_norm": 0.10126574337482452,
      "learning_rate": 5.194444444444445e-06,
      "loss": 0.0014,
      "step": 80650
    },
    {
      "epoch": 7.169777777777778,
      "grad_norm": 0.5448656678199768,
      "learning_rate": 5.188888888888889e-06,
      "loss": 0.0025,
      "step": 80660
    },
    {
      "epoch": 7.1706666666666665,
      "grad_norm": 0.06641076505184174,
      "learning_rate": 5.183333333333333e-06,
      "loss": 0.0026,
      "step": 80670
    },
    {
      "epoch": 7.171555555555556,
      "grad_norm": 0.11316011846065521,
      "learning_rate": 5.177777777777778e-06,
      "loss": 0.0016,
      "step": 80680
    },
    {
      "epoch": 7.172444444444444,
      "grad_norm": 0.289054274559021,
      "learning_rate": 5.172222222222223e-06,
      "loss": 0.0012,
      "step": 80690
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 0.3926369547843933,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.002,
      "step": 80700
    },
    {
      "epoch": 7.174222222222222,
      "grad_norm": 0.15206637978553772,
      "learning_rate": 5.161111111111112e-06,
      "loss": 0.0017,
      "step": 80710
    },
    {
      "epoch": 7.175111111111111,
      "grad_norm": 0.058542318642139435,
      "learning_rate": 5.155555555555555e-06,
      "loss": 0.0017,
      "step": 80720
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.04917626455426216,
      "learning_rate": 5.15e-06,
      "loss": 0.0025,
      "step": 80730
    },
    {
      "epoch": 7.176888888888889,
      "grad_norm": 0.11298158764839172,
      "learning_rate": 5.144444444444445e-06,
      "loss": 0.002,
      "step": 80740
    },
    {
      "epoch": 7.177777777777778,
      "grad_norm": 0.5021360516548157,
      "learning_rate": 5.138888888888889e-06,
      "loss": 0.0017,
      "step": 80750
    },
    {
      "epoch": 7.1786666666666665,
      "grad_norm": 0.05998481065034866,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0019,
      "step": 80760
    },
    {
      "epoch": 7.179555555555556,
      "grad_norm": 0.2303084433078766,
      "learning_rate": 5.127777777777778e-06,
      "loss": 0.0015,
      "step": 80770
    },
    {
      "epoch": 7.180444444444444,
      "grad_norm": 0.0655781477689743,
      "learning_rate": 5.122222222222223e-06,
      "loss": 0.0015,
      "step": 80780
    },
    {
      "epoch": 7.181333333333333,
      "grad_norm": 0.22855961322784424,
      "learning_rate": 5.116666666666667e-06,
      "loss": 0.0035,
      "step": 80790
    },
    {
      "epoch": 7.182222222222222,
      "grad_norm": 0.43637293577194214,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 0.002,
      "step": 80800
    },
    {
      "epoch": 7.183111111111111,
      "grad_norm": 0.0819549709558487,
      "learning_rate": 5.105555555555556e-06,
      "loss": 0.0016,
      "step": 80810
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.4050149619579315,
      "learning_rate": 5.1e-06,
      "loss": 0.0023,
      "step": 80820
    },
    {
      "epoch": 7.184888888888889,
      "grad_norm": 0.26742246747016907,
      "learning_rate": 5.094444444444445e-06,
      "loss": 0.0014,
      "step": 80830
    },
    {
      "epoch": 7.185777777777778,
      "grad_norm": 0.3153945207595825,
      "learning_rate": 5.088888888888889e-06,
      "loss": 0.0023,
      "step": 80840
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 0.07698715478181839,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.0015,
      "step": 80850
    },
    {
      "epoch": 7.187555555555556,
      "grad_norm": 0.1285439282655716,
      "learning_rate": 5.077777777777778e-06,
      "loss": 0.0019,
      "step": 80860
    },
    {
      "epoch": 7.188444444444444,
      "grad_norm": 0.24391518533229828,
      "learning_rate": 5.072222222222222e-06,
      "loss": 0.0014,
      "step": 80870
    },
    {
      "epoch": 7.189333333333333,
      "grad_norm": 0.24721847474575043,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0015,
      "step": 80880
    },
    {
      "epoch": 7.190222222222222,
      "grad_norm": 0.33385372161865234,
      "learning_rate": 5.061111111111112e-06,
      "loss": 0.0015,
      "step": 80890
    },
    {
      "epoch": 7.191111111111111,
      "grad_norm": 0.4629080295562744,
      "learning_rate": 5.0555555555555555e-06,
      "loss": 0.0013,
      "step": 80900
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.5774562954902649,
      "learning_rate": 5.050000000000001e-06,
      "loss": 0.0019,
      "step": 80910
    },
    {
      "epoch": 7.192888888888889,
      "grad_norm": 0.14648260176181793,
      "learning_rate": 5.044444444444444e-06,
      "loss": 0.0016,
      "step": 80920
    },
    {
      "epoch": 7.193777777777778,
      "grad_norm": 0.04556416720151901,
      "learning_rate": 5.038888888888889e-06,
      "loss": 0.0016,
      "step": 80930
    },
    {
      "epoch": 7.1946666666666665,
      "grad_norm": 0.17709246277809143,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0021,
      "step": 80940
    },
    {
      "epoch": 7.195555555555556,
      "grad_norm": 1.2828744649887085,
      "learning_rate": 5.0277777777777775e-06,
      "loss": 0.0025,
      "step": 80950
    },
    {
      "epoch": 7.196444444444444,
      "grad_norm": 0.5812056064605713,
      "learning_rate": 5.022222222222223e-06,
      "loss": 0.0024,
      "step": 80960
    },
    {
      "epoch": 7.197333333333333,
      "grad_norm": 0.49988123774528503,
      "learning_rate": 5.016666666666666e-06,
      "loss": 0.0012,
      "step": 80970
    },
    {
      "epoch": 7.198222222222222,
      "grad_norm": 0.5402584075927734,
      "learning_rate": 5.011111111111112e-06,
      "loss": 0.003,
      "step": 80980
    },
    {
      "epoch": 7.199111111111111,
      "grad_norm": 0.3260551691055298,
      "learning_rate": 5.005555555555556e-06,
      "loss": 0.0025,
      "step": 80990
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.4204806983470917,
      "learning_rate": 5e-06,
      "loss": 0.0018,
      "step": 81000
    },
    {
      "epoch": 7.200888888888889,
      "grad_norm": 0.38850486278533936,
      "learning_rate": 4.994444444444445e-06,
      "loss": 0.0026,
      "step": 81010
    },
    {
      "epoch": 7.201777777777778,
      "grad_norm": 0.40079718828201294,
      "learning_rate": 4.988888888888889e-06,
      "loss": 0.0017,
      "step": 81020
    },
    {
      "epoch": 7.2026666666666666,
      "grad_norm": 0.1803489476442337,
      "learning_rate": 4.983333333333334e-06,
      "loss": 0.0025,
      "step": 81030
    },
    {
      "epoch": 7.203555555555556,
      "grad_norm": 0.3942249119281769,
      "learning_rate": 4.977777777777778e-06,
      "loss": 0.0015,
      "step": 81040
    },
    {
      "epoch": 7.204444444444444,
      "grad_norm": 0.06887748092412949,
      "learning_rate": 4.9722222222222224e-06,
      "loss": 0.0019,
      "step": 81050
    },
    {
      "epoch": 7.205333333333333,
      "grad_norm": 0.3239874839782715,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0016,
      "step": 81060
    },
    {
      "epoch": 7.206222222222222,
      "grad_norm": 0.5402988195419312,
      "learning_rate": 4.961111111111111e-06,
      "loss": 0.0024,
      "step": 81070
    },
    {
      "epoch": 7.207111111111111,
      "grad_norm": 0.057802435010671616,
      "learning_rate": 4.955555555555556e-06,
      "loss": 0.0015,
      "step": 81080
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.08724289387464523,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.0023,
      "step": 81090
    },
    {
      "epoch": 7.208888888888889,
      "grad_norm": 0.2707867920398712,
      "learning_rate": 4.9444444444444444e-06,
      "loss": 0.0015,
      "step": 81100
    },
    {
      "epoch": 7.209777777777778,
      "grad_norm": 0.08562736213207245,
      "learning_rate": 4.93888888888889e-06,
      "loss": 0.002,
      "step": 81110
    },
    {
      "epoch": 7.210666666666667,
      "grad_norm": 0.5530757904052734,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0021,
      "step": 81120
    },
    {
      "epoch": 7.211555555555556,
      "grad_norm": 0.05611984059214592,
      "learning_rate": 4.927777777777778e-06,
      "loss": 0.0019,
      "step": 81130
    },
    {
      "epoch": 7.212444444444444,
      "grad_norm": 0.05456934869289398,
      "learning_rate": 4.922222222222223e-06,
      "loss": 0.0019,
      "step": 81140
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 0.43049561977386475,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.0014,
      "step": 81150
    },
    {
      "epoch": 7.214222222222222,
      "grad_norm": 0.5333346724510193,
      "learning_rate": 4.911111111111112e-06,
      "loss": 0.0017,
      "step": 81160
    },
    {
      "epoch": 7.215111111111111,
      "grad_norm": 0.08011253923177719,
      "learning_rate": 4.905555555555555e-06,
      "loss": 0.0015,
      "step": 81170
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.19315899908542633,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0014,
      "step": 81180
    },
    {
      "epoch": 7.216888888888889,
      "grad_norm": 0.08434408158063889,
      "learning_rate": 4.894444444444445e-06,
      "loss": 0.0025,
      "step": 81190
    },
    {
      "epoch": 7.217777777777778,
      "grad_norm": 0.3529222011566162,
      "learning_rate": 4.888888888888889e-06,
      "loss": 0.0023,
      "step": 81200
    },
    {
      "epoch": 7.218666666666667,
      "grad_norm": 0.39390870928764343,
      "learning_rate": 4.883333333333334e-06,
      "loss": 0.0022,
      "step": 81210
    },
    {
      "epoch": 7.219555555555556,
      "grad_norm": 0.39898642897605896,
      "learning_rate": 4.877777777777778e-06,
      "loss": 0.0014,
      "step": 81220
    },
    {
      "epoch": 7.220444444444444,
      "grad_norm": 0.3948320746421814,
      "learning_rate": 4.8722222222222225e-06,
      "loss": 0.0017,
      "step": 81230
    },
    {
      "epoch": 7.221333333333333,
      "grad_norm": 0.19120417535305023,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0021,
      "step": 81240
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 0.2584078013896942,
      "learning_rate": 4.861111111111111e-06,
      "loss": 0.0033,
      "step": 81250
    },
    {
      "epoch": 7.223111111111111,
      "grad_norm": 0.0578724704682827,
      "learning_rate": 4.855555555555556e-06,
      "loss": 0.0015,
      "step": 81260
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.36630353331565857,
      "learning_rate": 4.85e-06,
      "loss": 0.0016,
      "step": 81270
    },
    {
      "epoch": 7.224888888888889,
      "grad_norm": 0.15516354143619537,
      "learning_rate": 4.8444444444444446e-06,
      "loss": 0.0023,
      "step": 81280
    },
    {
      "epoch": 7.225777777777778,
      "grad_norm": 0.5287888646125793,
      "learning_rate": 4.838888888888889e-06,
      "loss": 0.002,
      "step": 81290
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 0.32386040687561035,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0012,
      "step": 81300
    },
    {
      "epoch": 7.227555555555556,
      "grad_norm": 0.11617297679185867,
      "learning_rate": 4.827777777777779e-06,
      "loss": 0.002,
      "step": 81310
    },
    {
      "epoch": 7.2284444444444444,
      "grad_norm": 0.18147775530815125,
      "learning_rate": 4.822222222222222e-06,
      "loss": 0.0026,
      "step": 81320
    },
    {
      "epoch": 7.229333333333333,
      "grad_norm": 0.5773342847824097,
      "learning_rate": 4.816666666666667e-06,
      "loss": 0.0013,
      "step": 81330
    },
    {
      "epoch": 7.230222222222222,
      "grad_norm": 0.3548545837402344,
      "learning_rate": 4.811111111111111e-06,
      "loss": 0.0018,
      "step": 81340
    },
    {
      "epoch": 7.231111111111111,
      "grad_norm": 0.19337788224220276,
      "learning_rate": 4.805555555555555e-06,
      "loss": 0.0021,
      "step": 81350
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.07600593566894531,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0018,
      "step": 81360
    },
    {
      "epoch": 7.232888888888889,
      "grad_norm": 0.16237132251262665,
      "learning_rate": 4.794444444444444e-06,
      "loss": 0.0016,
      "step": 81370
    },
    {
      "epoch": 7.233777777777778,
      "grad_norm": 0.088541679084301,
      "learning_rate": 4.7888888888888894e-06,
      "loss": 0.0019,
      "step": 81380
    },
    {
      "epoch": 7.234666666666667,
      "grad_norm": 0.036945804953575134,
      "learning_rate": 4.783333333333333e-06,
      "loss": 0.0023,
      "step": 81390
    },
    {
      "epoch": 7.235555555555556,
      "grad_norm": 0.14828917384147644,
      "learning_rate": 4.777777777777778e-06,
      "loss": 0.002,
      "step": 81400
    },
    {
      "epoch": 7.2364444444444445,
      "grad_norm": 0.2172280251979828,
      "learning_rate": 4.772222222222223e-06,
      "loss": 0.0017,
      "step": 81410
    },
    {
      "epoch": 7.237333333333333,
      "grad_norm": 0.43578940629959106,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.0019,
      "step": 81420
    },
    {
      "epoch": 7.238222222222222,
      "grad_norm": 0.5264625549316406,
      "learning_rate": 4.7611111111111115e-06,
      "loss": 0.0027,
      "step": 81430
    },
    {
      "epoch": 7.239111111111111,
      "grad_norm": 0.32508233189582825,
      "learning_rate": 4.755555555555556e-06,
      "loss": 0.0015,
      "step": 81440
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.15081672370433807,
      "learning_rate": 4.75e-06,
      "loss": 0.0017,
      "step": 81450
    },
    {
      "epoch": 7.240888888888889,
      "grad_norm": 0.08589960634708405,
      "learning_rate": 4.744444444444445e-06,
      "loss": 0.0015,
      "step": 81460
    },
    {
      "epoch": 7.241777777777778,
      "grad_norm": 0.0794210359454155,
      "learning_rate": 4.738888888888889e-06,
      "loss": 0.0022,
      "step": 81470
    },
    {
      "epoch": 7.242666666666667,
      "grad_norm": 0.21610736846923828,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0012,
      "step": 81480
    },
    {
      "epoch": 7.243555555555556,
      "grad_norm": 0.04296189174056053,
      "learning_rate": 4.727777777777778e-06,
      "loss": 0.0013,
      "step": 81490
    },
    {
      "epoch": 7.2444444444444445,
      "grad_norm": 0.44282370805740356,
      "learning_rate": 4.722222222222222e-06,
      "loss": 0.0023,
      "step": 81500
    },
    {
      "epoch": 7.245333333333333,
      "grad_norm": 0.5269917845726013,
      "learning_rate": 4.7166666666666675e-06,
      "loss": 0.0023,
      "step": 81510
    },
    {
      "epoch": 7.246222222222222,
      "grad_norm": 0.4242382347583771,
      "learning_rate": 4.711111111111111e-06,
      "loss": 0.0014,
      "step": 81520
    },
    {
      "epoch": 7.247111111111111,
      "grad_norm": 0.5355993509292603,
      "learning_rate": 4.705555555555556e-06,
      "loss": 0.0024,
      "step": 81530
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.2862902879714966,
      "learning_rate": 4.7e-06,
      "loss": 0.0013,
      "step": 81540
    },
    {
      "epoch": 7.248888888888889,
      "grad_norm": 0.447380393743515,
      "learning_rate": 4.694444444444444e-06,
      "loss": 0.0014,
      "step": 81550
    },
    {
      "epoch": 7.249777777777778,
      "grad_norm": 0.7443172335624695,
      "learning_rate": 4.6888888888888895e-06,
      "loss": 0.002,
      "step": 81560
    },
    {
      "epoch": 7.250666666666667,
      "grad_norm": 0.4251624643802643,
      "learning_rate": 4.683333333333333e-06,
      "loss": 0.0022,
      "step": 81570
    },
    {
      "epoch": 7.251555555555555,
      "grad_norm": 0.253858745098114,
      "learning_rate": 4.677777777777778e-06,
      "loss": 0.0014,
      "step": 81580
    },
    {
      "epoch": 7.2524444444444445,
      "grad_norm": 0.1794721782207489,
      "learning_rate": 4.672222222222222e-06,
      "loss": 0.0015,
      "step": 81590
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.2821568548679352,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0016,
      "step": 81600
    },
    {
      "epoch": 7.254222222222222,
      "grad_norm": 0.302511066198349,
      "learning_rate": 4.6611111111111116e-06,
      "loss": 0.0014,
      "step": 81610
    },
    {
      "epoch": 7.255111111111111,
      "grad_norm": 0.18122945725917816,
      "learning_rate": 4.655555555555556e-06,
      "loss": 0.0012,
      "step": 81620
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.09265219420194626,
      "learning_rate": 4.65e-06,
      "loss": 0.0015,
      "step": 81630
    },
    {
      "epoch": 7.256888888888889,
      "grad_norm": 0.8100346326828003,
      "learning_rate": 4.644444444444444e-06,
      "loss": 0.0021,
      "step": 81640
    },
    {
      "epoch": 7.257777777777778,
      "grad_norm": 0.2081737071275711,
      "learning_rate": 4.638888888888889e-06,
      "loss": 0.0019,
      "step": 81650
    },
    {
      "epoch": 7.258666666666667,
      "grad_norm": 0.3348618149757385,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0016,
      "step": 81660
    },
    {
      "epoch": 7.259555555555556,
      "grad_norm": 0.28984537720680237,
      "learning_rate": 4.627777777777778e-06,
      "loss": 0.002,
      "step": 81670
    },
    {
      "epoch": 7.2604444444444445,
      "grad_norm": 0.39334216713905334,
      "learning_rate": 4.622222222222222e-06,
      "loss": 0.0013,
      "step": 81680
    },
    {
      "epoch": 7.261333333333333,
      "grad_norm": 0.5806493759155273,
      "learning_rate": 4.616666666666667e-06,
      "loss": 0.0021,
      "step": 81690
    },
    {
      "epoch": 7.262222222222222,
      "grad_norm": 0.17888911068439484,
      "learning_rate": 4.611111111111111e-06,
      "loss": 0.0012,
      "step": 81700
    },
    {
      "epoch": 7.263111111111111,
      "grad_norm": 0.15352343022823334,
      "learning_rate": 4.605555555555556e-06,
      "loss": 0.0016,
      "step": 81710
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.17919819056987762,
      "learning_rate": 4.6e-06,
      "loss": 0.0014,
      "step": 81720
    },
    {
      "epoch": 7.264888888888889,
      "grad_norm": 0.1528007686138153,
      "learning_rate": 4.594444444444445e-06,
      "loss": 0.0014,
      "step": 81730
    },
    {
      "epoch": 7.265777777777778,
      "grad_norm": 0.3487904369831085,
      "learning_rate": 4.588888888888889e-06,
      "loss": 0.0029,
      "step": 81740
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.04212285205721855,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.0012,
      "step": 81750
    },
    {
      "epoch": 7.267555555555555,
      "grad_norm": 0.3910848796367645,
      "learning_rate": 4.5777777777777785e-06,
      "loss": 0.0019,
      "step": 81760
    },
    {
      "epoch": 7.2684444444444445,
      "grad_norm": 0.18839991092681885,
      "learning_rate": 4.572222222222222e-06,
      "loss": 0.0026,
      "step": 81770
    },
    {
      "epoch": 7.269333333333333,
      "grad_norm": 0.05167756229639053,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0013,
      "step": 81780
    },
    {
      "epoch": 7.270222222222222,
      "grad_norm": 0.6060375571250916,
      "learning_rate": 4.561111111111111e-06,
      "loss": 0.0022,
      "step": 81790
    },
    {
      "epoch": 7.271111111111111,
      "grad_norm": 0.2906237542629242,
      "learning_rate": 4.555555555555556e-06,
      "loss": 0.0027,
      "step": 81800
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.6225639581680298,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 0.0016,
      "step": 81810
    },
    {
      "epoch": 7.272888888888889,
      "grad_norm": 0.18782155215740204,
      "learning_rate": 4.544444444444445e-06,
      "loss": 0.002,
      "step": 81820
    },
    {
      "epoch": 7.273777777777778,
      "grad_norm": 0.06020188704133034,
      "learning_rate": 4.538888888888889e-06,
      "loss": 0.002,
      "step": 81830
    },
    {
      "epoch": 7.274666666666667,
      "grad_norm": 0.3589032292366028,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0014,
      "step": 81840
    },
    {
      "epoch": 7.275555555555556,
      "grad_norm": 0.17760704457759857,
      "learning_rate": 4.527777777777778e-06,
      "loss": 0.0019,
      "step": 81850
    },
    {
      "epoch": 7.2764444444444445,
      "grad_norm": 0.6023664474487305,
      "learning_rate": 4.5222222222222225e-06,
      "loss": 0.0018,
      "step": 81860
    },
    {
      "epoch": 7.277333333333333,
      "grad_norm": 0.42663079500198364,
      "learning_rate": 4.516666666666667e-06,
      "loss": 0.0014,
      "step": 81870
    },
    {
      "epoch": 7.278222222222222,
      "grad_norm": 0.11422780156135559,
      "learning_rate": 4.511111111111111e-06,
      "loss": 0.0013,
      "step": 81880
    },
    {
      "epoch": 7.279111111111111,
      "grad_norm": 0.2220379263162613,
      "learning_rate": 4.505555555555556e-06,
      "loss": 0.0019,
      "step": 81890
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.24126271903514862,
      "learning_rate": 4.5e-06,
      "loss": 0.0017,
      "step": 81900
    },
    {
      "epoch": 7.280888888888889,
      "grad_norm": 0.06270190328359604,
      "learning_rate": 4.4944444444444445e-06,
      "loss": 0.0019,
      "step": 81910
    },
    {
      "epoch": 7.281777777777778,
      "grad_norm": 0.08099342882633209,
      "learning_rate": 4.488888888888889e-06,
      "loss": 0.0018,
      "step": 81920
    },
    {
      "epoch": 7.282666666666667,
      "grad_norm": 0.04336150735616684,
      "learning_rate": 4.483333333333334e-06,
      "loss": 0.0013,
      "step": 81930
    },
    {
      "epoch": 7.283555555555555,
      "grad_norm": 0.15371614694595337,
      "learning_rate": 4.477777777777778e-06,
      "loss": 0.0014,
      "step": 81940
    },
    {
      "epoch": 7.2844444444444445,
      "grad_norm": 0.11106279492378235,
      "learning_rate": 4.472222222222222e-06,
      "loss": 0.002,
      "step": 81950
    },
    {
      "epoch": 7.285333333333333,
      "grad_norm": 0.11190734803676605,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.002,
      "step": 81960
    },
    {
      "epoch": 7.286222222222222,
      "grad_norm": 0.21722783148288727,
      "learning_rate": 4.461111111111111e-06,
      "loss": 0.0017,
      "step": 81970
    },
    {
      "epoch": 7.287111111111111,
      "grad_norm": 0.28931280970573425,
      "learning_rate": 4.455555555555556e-06,
      "loss": 0.0018,
      "step": 81980
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.31542348861694336,
      "learning_rate": 4.45e-06,
      "loss": 0.0019,
      "step": 81990
    },
    {
      "epoch": 7.288888888888889,
      "grad_norm": 0.4148820638656616,
      "learning_rate": 4.444444444444445e-06,
      "loss": 0.0014,
      "step": 82000
    },
    {
      "epoch": 7.289777777777778,
      "grad_norm": 0.3267250657081604,
      "learning_rate": 4.4388888888888886e-06,
      "loss": 0.0016,
      "step": 82010
    },
    {
      "epoch": 7.290666666666667,
      "grad_norm": 0.291745126247406,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0017,
      "step": 82020
    },
    {
      "epoch": 7.291555555555556,
      "grad_norm": 0.42430612444877625,
      "learning_rate": 4.427777777777778e-06,
      "loss": 0.002,
      "step": 82030
    },
    {
      "epoch": 7.2924444444444445,
      "grad_norm": 0.5986407995223999,
      "learning_rate": 4.422222222222223e-06,
      "loss": 0.0013,
      "step": 82040
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 0.26483872532844543,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.0022,
      "step": 82050
    },
    {
      "epoch": 7.294222222222222,
      "grad_norm": 0.08859966695308685,
      "learning_rate": 4.411111111111111e-06,
      "loss": 0.0026,
      "step": 82060
    },
    {
      "epoch": 7.295111111111111,
      "grad_norm": 0.3287132978439331,
      "learning_rate": 4.405555555555556e-06,
      "loss": 0.0018,
      "step": 82070
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.2884417474269867,
      "learning_rate": 4.4e-06,
      "loss": 0.0029,
      "step": 82080
    },
    {
      "epoch": 7.296888888888889,
      "grad_norm": 0.11751525104045868,
      "learning_rate": 4.394444444444445e-06,
      "loss": 0.0019,
      "step": 82090
    },
    {
      "epoch": 7.297777777777778,
      "grad_norm": 0.2177266776561737,
      "learning_rate": 4.388888888888889e-06,
      "loss": 0.0015,
      "step": 82100
    },
    {
      "epoch": 7.298666666666667,
      "grad_norm": 0.23000794649124146,
      "learning_rate": 4.3833333333333334e-06,
      "loss": 0.002,
      "step": 82110
    },
    {
      "epoch": 7.299555555555555,
      "grad_norm": 0.07914946228265762,
      "learning_rate": 4.377777777777778e-06,
      "loss": 0.0012,
      "step": 82120
    },
    {
      "epoch": 7.3004444444444445,
      "grad_norm": 0.19053517282009125,
      "learning_rate": 4.372222222222223e-06,
      "loss": 0.0014,
      "step": 82130
    },
    {
      "epoch": 7.301333333333333,
      "grad_norm": 0.04889237508177757,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.0018,
      "step": 82140
    },
    {
      "epoch": 7.302222222222222,
      "grad_norm": 0.443369060754776,
      "learning_rate": 4.361111111111112e-06,
      "loss": 0.0013,
      "step": 82150
    },
    {
      "epoch": 7.303111111111111,
      "grad_norm": 0.4149843454360962,
      "learning_rate": 4.3555555555555555e-06,
      "loss": 0.0021,
      "step": 82160
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.36235618591308594,
      "learning_rate": 4.35e-06,
      "loss": 0.002,
      "step": 82170
    },
    {
      "epoch": 7.304888888888889,
      "grad_norm": 0.06478291749954224,
      "learning_rate": 4.344444444444445e-06,
      "loss": 0.0013,
      "step": 82180
    },
    {
      "epoch": 7.305777777777778,
      "grad_norm": 0.268733412027359,
      "learning_rate": 4.338888888888889e-06,
      "loss": 0.0019,
      "step": 82190
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 0.0716790035367012,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0019,
      "step": 82200
    },
    {
      "epoch": 7.307555555555556,
      "grad_norm": 0.08283648639917374,
      "learning_rate": 4.3277777777777775e-06,
      "loss": 0.0021,
      "step": 82210
    },
    {
      "epoch": 7.3084444444444445,
      "grad_norm": 0.06505638360977173,
      "learning_rate": 4.322222222222223e-06,
      "loss": 0.0018,
      "step": 82220
    },
    {
      "epoch": 7.309333333333333,
      "grad_norm": 0.19187341630458832,
      "learning_rate": 4.316666666666667e-06,
      "loss": 0.0013,
      "step": 82230
    },
    {
      "epoch": 7.310222222222222,
      "grad_norm": 0.18100515007972717,
      "learning_rate": 4.3111111111111115e-06,
      "loss": 0.0027,
      "step": 82240
    },
    {
      "epoch": 7.311111111111111,
      "grad_norm": 0.11026155203580856,
      "learning_rate": 4.305555555555556e-06,
      "loss": 0.0019,
      "step": 82250
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.054007887840270996,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0017,
      "step": 82260
    },
    {
      "epoch": 7.312888888888889,
      "grad_norm": 0.2887120246887207,
      "learning_rate": 4.294444444444445e-06,
      "loss": 0.0013,
      "step": 82270
    },
    {
      "epoch": 7.313777777777778,
      "grad_norm": 0.508161187171936,
      "learning_rate": 4.288888888888889e-06,
      "loss": 0.002,
      "step": 82280
    },
    {
      "epoch": 7.314666666666667,
      "grad_norm": 0.4968443214893341,
      "learning_rate": 4.2833333333333335e-06,
      "loss": 0.0022,
      "step": 82290
    },
    {
      "epoch": 7.315555555555555,
      "grad_norm": 0.1488901525735855,
      "learning_rate": 4.277777777777778e-06,
      "loss": 0.0031,
      "step": 82300
    },
    {
      "epoch": 7.3164444444444445,
      "grad_norm": 0.3589872121810913,
      "learning_rate": 4.272222222222222e-06,
      "loss": 0.0014,
      "step": 82310
    },
    {
      "epoch": 7.317333333333333,
      "grad_norm": 0.25665268301963806,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0015,
      "step": 82320
    },
    {
      "epoch": 7.318222222222222,
      "grad_norm": 0.14298610389232635,
      "learning_rate": 4.261111111111111e-06,
      "loss": 0.0021,
      "step": 82330
    },
    {
      "epoch": 7.319111111111111,
      "grad_norm": 0.3190259337425232,
      "learning_rate": 4.2555555555555556e-06,
      "loss": 0.0017,
      "step": 82340
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.12013515830039978,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.0011,
      "step": 82350
    },
    {
      "epoch": 7.320888888888889,
      "grad_norm": 0.10777030885219574,
      "learning_rate": 4.244444444444444e-06,
      "loss": 0.0015,
      "step": 82360
    },
    {
      "epoch": 7.321777777777778,
      "grad_norm": 0.10935711115598679,
      "learning_rate": 4.238888888888889e-06,
      "loss": 0.0028,
      "step": 82370
    },
    {
      "epoch": 7.322666666666667,
      "grad_norm": 0.15092124044895172,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.002,
      "step": 82380
    },
    {
      "epoch": 7.323555555555555,
      "grad_norm": 0.5371081829071045,
      "learning_rate": 4.227777777777778e-06,
      "loss": 0.0015,
      "step": 82390
    },
    {
      "epoch": 7.3244444444444445,
      "grad_norm": 0.3302704095840454,
      "learning_rate": 4.222222222222223e-06,
      "loss": 0.0022,
      "step": 82400
    },
    {
      "epoch": 7.325333333333333,
      "grad_norm": 0.33381104469299316,
      "learning_rate": 4.216666666666666e-06,
      "loss": 0.0023,
      "step": 82410
    },
    {
      "epoch": 7.326222222222222,
      "grad_norm": 0.04590662568807602,
      "learning_rate": 4.211111111111112e-06,
      "loss": 0.0015,
      "step": 82420
    },
    {
      "epoch": 7.327111111111111,
      "grad_norm": 0.0745810866355896,
      "learning_rate": 4.205555555555556e-06,
      "loss": 0.0024,
      "step": 82430
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.7767544984817505,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0014,
      "step": 82440
    },
    {
      "epoch": 7.328888888888889,
      "grad_norm": 0.1508520543575287,
      "learning_rate": 4.194444444444445e-06,
      "loss": 0.0014,
      "step": 82450
    },
    {
      "epoch": 7.329777777777778,
      "grad_norm": 0.49750247597694397,
      "learning_rate": 4.188888888888889e-06,
      "loss": 0.0016,
      "step": 82460
    },
    {
      "epoch": 7.330666666666667,
      "grad_norm": 0.08925694227218628,
      "learning_rate": 4.183333333333334e-06,
      "loss": 0.0015,
      "step": 82470
    },
    {
      "epoch": 7.331555555555555,
      "grad_norm": 0.06168929114937782,
      "learning_rate": 4.177777777777778e-06,
      "loss": 0.002,
      "step": 82480
    },
    {
      "epoch": 7.3324444444444445,
      "grad_norm": 0.11206404864788055,
      "learning_rate": 4.1722222222222225e-06,
      "loss": 0.0019,
      "step": 82490
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.10963981598615646,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0017,
      "step": 82500
    },
    {
      "epoch": 7.334222222222222,
      "grad_norm": 0.47095611691474915,
      "learning_rate": 4.161111111111111e-06,
      "loss": 0.0014,
      "step": 82510
    },
    {
      "epoch": 7.335111111111111,
      "grad_norm": 0.600918710231781,
      "learning_rate": 4.155555555555556e-06,
      "loss": 0.0016,
      "step": 82520
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.08370760828256607,
      "learning_rate": 4.15e-06,
      "loss": 0.0021,
      "step": 82530
    },
    {
      "epoch": 7.336888888888889,
      "grad_norm": 0.3318544626235962,
      "learning_rate": 4.1444444444444445e-06,
      "loss": 0.0025,
      "step": 82540
    },
    {
      "epoch": 7.337777777777778,
      "grad_norm": 0.3239823579788208,
      "learning_rate": 4.13888888888889e-06,
      "loss": 0.0022,
      "step": 82550
    },
    {
      "epoch": 7.338666666666667,
      "grad_norm": 0.34694844484329224,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.002,
      "step": 82560
    },
    {
      "epoch": 7.339555555555555,
      "grad_norm": 0.5013797283172607,
      "learning_rate": 4.127777777777778e-06,
      "loss": 0.0017,
      "step": 82570
    },
    {
      "epoch": 7.3404444444444445,
      "grad_norm": 0.11289939284324646,
      "learning_rate": 4.122222222222222e-06,
      "loss": 0.0013,
      "step": 82580
    },
    {
      "epoch": 7.341333333333333,
      "grad_norm": 0.46432220935821533,
      "learning_rate": 4.1166666666666665e-06,
      "loss": 0.002,
      "step": 82590
    },
    {
      "epoch": 7.342222222222222,
      "grad_norm": 0.4370829463005066,
      "learning_rate": 4.111111111111112e-06,
      "loss": 0.0022,
      "step": 82600
    },
    {
      "epoch": 7.343111111111111,
      "grad_norm": 0.24640223383903503,
      "learning_rate": 4.105555555555555e-06,
      "loss": 0.002,
      "step": 82610
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.32561105489730835,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.0022,
      "step": 82620
    },
    {
      "epoch": 7.344888888888889,
      "grad_norm": 0.1255873590707779,
      "learning_rate": 4.094444444444444e-06,
      "loss": 0.0021,
      "step": 82630
    },
    {
      "epoch": 7.345777777777778,
      "grad_norm": 0.24288403987884521,
      "learning_rate": 4.088888888888889e-06,
      "loss": 0.0018,
      "step": 82640
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 0.1723679155111313,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.0011,
      "step": 82650
    },
    {
      "epoch": 7.347555555555555,
      "grad_norm": 0.08566738665103912,
      "learning_rate": 4.077777777777778e-06,
      "loss": 0.0018,
      "step": 82660
    },
    {
      "epoch": 7.348444444444445,
      "grad_norm": 0.2964157462120056,
      "learning_rate": 4.0722222222222226e-06,
      "loss": 0.0019,
      "step": 82670
    },
    {
      "epoch": 7.349333333333333,
      "grad_norm": 0.23657335340976715,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0017,
      "step": 82680
    },
    {
      "epoch": 7.350222222222222,
      "grad_norm": 0.0437968485057354,
      "learning_rate": 4.061111111111111e-06,
      "loss": 0.0023,
      "step": 82690
    },
    {
      "epoch": 7.351111111111111,
      "grad_norm": 0.3252843916416168,
      "learning_rate": 4.055555555555556e-06,
      "loss": 0.0019,
      "step": 82700
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.4596101939678192,
      "learning_rate": 4.05e-06,
      "loss": 0.0016,
      "step": 82710
    },
    {
      "epoch": 7.352888888888889,
      "grad_norm": 0.2865172326564789,
      "learning_rate": 4.044444444444445e-06,
      "loss": 0.0023,
      "step": 82720
    },
    {
      "epoch": 7.353777777777778,
      "grad_norm": 0.0824839174747467,
      "learning_rate": 4.038888888888889e-06,
      "loss": 0.0013,
      "step": 82730
    },
    {
      "epoch": 7.354666666666667,
      "grad_norm": 0.25220242142677307,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0023,
      "step": 82740
    },
    {
      "epoch": 7.355555555555555,
      "grad_norm": 0.1046595424413681,
      "learning_rate": 4.027777777777779e-06,
      "loss": 0.0017,
      "step": 82750
    },
    {
      "epoch": 7.356444444444445,
      "grad_norm": 0.04304607957601547,
      "learning_rate": 4.022222222222222e-06,
      "loss": 0.0014,
      "step": 82760
    },
    {
      "epoch": 7.357333333333333,
      "grad_norm": 0.12506386637687683,
      "learning_rate": 4.0166666666666675e-06,
      "loss": 0.0019,
      "step": 82770
    },
    {
      "epoch": 7.358222222222222,
      "grad_norm": 0.42725661396980286,
      "learning_rate": 4.011111111111111e-06,
      "loss": 0.0021,
      "step": 82780
    },
    {
      "epoch": 7.359111111111111,
      "grad_norm": 0.2130049467086792,
      "learning_rate": 4.005555555555555e-06,
      "loss": 0.0019,
      "step": 82790
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.2520240545272827,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0018,
      "step": 82800
    },
    {
      "epoch": 7.360888888888889,
      "grad_norm": 0.08008139580488205,
      "learning_rate": 3.994444444444444e-06,
      "loss": 0.0028,
      "step": 82810
    },
    {
      "epoch": 7.361777777777778,
      "grad_norm": 0.11580604314804077,
      "learning_rate": 3.9888888888888895e-06,
      "loss": 0.0025,
      "step": 82820
    },
    {
      "epoch": 7.362666666666667,
      "grad_norm": 0.04713458567857742,
      "learning_rate": 3.983333333333333e-06,
      "loss": 0.0018,
      "step": 82830
    },
    {
      "epoch": 7.363555555555555,
      "grad_norm": 0.18285472691059113,
      "learning_rate": 3.977777777777778e-06,
      "loss": 0.0023,
      "step": 82840
    },
    {
      "epoch": 7.364444444444445,
      "grad_norm": 0.2568000853061676,
      "learning_rate": 3.972222222222223e-06,
      "loss": 0.0015,
      "step": 82850
    },
    {
      "epoch": 7.365333333333333,
      "grad_norm": 0.21658937633037567,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.0015,
      "step": 82860
    },
    {
      "epoch": 7.3662222222222224,
      "grad_norm": 0.9335120916366577,
      "learning_rate": 3.9611111111111115e-06,
      "loss": 0.002,
      "step": 82870
    },
    {
      "epoch": 7.367111111111111,
      "grad_norm": 0.04957379400730133,
      "learning_rate": 3.955555555555555e-06,
      "loss": 0.0014,
      "step": 82880
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.21352079510688782,
      "learning_rate": 3.95e-06,
      "loss": 0.0019,
      "step": 82890
    },
    {
      "epoch": 7.368888888888889,
      "grad_norm": 0.2541026771068573,
      "learning_rate": 3.944444444444445e-06,
      "loss": 0.0014,
      "step": 82900
    },
    {
      "epoch": 7.369777777777777,
      "grad_norm": 0.32993608713150024,
      "learning_rate": 3.938888888888889e-06,
      "loss": 0.0027,
      "step": 82910
    },
    {
      "epoch": 7.370666666666667,
      "grad_norm": 0.057004302740097046,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.003,
      "step": 82920
    },
    {
      "epoch": 7.371555555555555,
      "grad_norm": 0.06541455537080765,
      "learning_rate": 3.927777777777778e-06,
      "loss": 0.0016,
      "step": 82930
    },
    {
      "epoch": 7.372444444444445,
      "grad_norm": 0.0806732326745987,
      "learning_rate": 3.922222222222222e-06,
      "loss": 0.0019,
      "step": 82940
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 0.22647489607334137,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.0022,
      "step": 82950
    },
    {
      "epoch": 7.3742222222222225,
      "grad_norm": 0.28579607605934143,
      "learning_rate": 3.911111111111111e-06,
      "loss": 0.0024,
      "step": 82960
    },
    {
      "epoch": 7.375111111111111,
      "grad_norm": 0.09567661583423615,
      "learning_rate": 3.905555555555556e-06,
      "loss": 0.0018,
      "step": 82970
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.1821550875902176,
      "learning_rate": 3.9e-06,
      "loss": 0.0016,
      "step": 82980
    },
    {
      "epoch": 7.376888888888889,
      "grad_norm": 0.4756159484386444,
      "learning_rate": 3.894444444444444e-06,
      "loss": 0.0023,
      "step": 82990
    },
    {
      "epoch": 7.377777777777778,
      "grad_norm": 0.31997060775756836,
      "learning_rate": 3.888888888888889e-06,
      "loss": 0.0017,
      "step": 83000
    },
    {
      "epoch": 7.378666666666667,
      "grad_norm": 0.8100947141647339,
      "learning_rate": 3.883333333333333e-06,
      "loss": 0.0019,
      "step": 83010
    },
    {
      "epoch": 7.379555555555555,
      "grad_norm": 0.14981205761432648,
      "learning_rate": 3.877777777777778e-06,
      "loss": 0.0015,
      "step": 83020
    },
    {
      "epoch": 7.380444444444445,
      "grad_norm": 0.08581312745809555,
      "learning_rate": 3.872222222222222e-06,
      "loss": 0.0013,
      "step": 83030
    },
    {
      "epoch": 7.381333333333333,
      "grad_norm": 0.11328917741775513,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0016,
      "step": 83040
    },
    {
      "epoch": 7.3822222222222225,
      "grad_norm": 0.060868747532367706,
      "learning_rate": 3.861111111111112e-06,
      "loss": 0.002,
      "step": 83050
    },
    {
      "epoch": 7.383111111111111,
      "grad_norm": 0.43666431307792664,
      "learning_rate": 3.855555555555556e-06,
      "loss": 0.0014,
      "step": 83060
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.2392437905073166,
      "learning_rate": 3.85e-06,
      "loss": 0.0021,
      "step": 83070
    },
    {
      "epoch": 7.384888888888889,
      "grad_norm": 0.16327661275863647,
      "learning_rate": 3.844444444444445e-06,
      "loss": 0.0024,
      "step": 83080
    },
    {
      "epoch": 7.385777777777777,
      "grad_norm": 0.44095349311828613,
      "learning_rate": 3.838888888888889e-06,
      "loss": 0.0016,
      "step": 83090
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.11358465254306793,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.002,
      "step": 83100
    },
    {
      "epoch": 7.387555555555555,
      "grad_norm": 0.112772636115551,
      "learning_rate": 3.827777777777778e-06,
      "loss": 0.0017,
      "step": 83110
    },
    {
      "epoch": 7.388444444444445,
      "grad_norm": 0.27059173583984375,
      "learning_rate": 3.8222222222222224e-06,
      "loss": 0.002,
      "step": 83120
    },
    {
      "epoch": 7.389333333333333,
      "grad_norm": 0.0694127157330513,
      "learning_rate": 3.816666666666667e-06,
      "loss": 0.002,
      "step": 83130
    },
    {
      "epoch": 7.3902222222222225,
      "grad_norm": 0.20027336478233337,
      "learning_rate": 3.8111111111111112e-06,
      "loss": 0.0017,
      "step": 83140
    },
    {
      "epoch": 7.391111111111111,
      "grad_norm": 0.5211607813835144,
      "learning_rate": 3.805555555555556e-06,
      "loss": 0.0017,
      "step": 83150
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.18012474477291107,
      "learning_rate": 3.8e-06,
      "loss": 0.0019,
      "step": 83160
    },
    {
      "epoch": 7.392888888888889,
      "grad_norm": 0.25168824195861816,
      "learning_rate": 3.794444444444445e-06,
      "loss": 0.0018,
      "step": 83170
    },
    {
      "epoch": 7.393777777777778,
      "grad_norm": 0.3040444254875183,
      "learning_rate": 3.788888888888889e-06,
      "loss": 0.0017,
      "step": 83180
    },
    {
      "epoch": 7.394666666666667,
      "grad_norm": 0.10321348160505295,
      "learning_rate": 3.7833333333333333e-06,
      "loss": 0.0024,
      "step": 83190
    },
    {
      "epoch": 7.395555555555555,
      "grad_norm": 0.0777861624956131,
      "learning_rate": 3.777777777777778e-06,
      "loss": 0.0018,
      "step": 83200
    },
    {
      "epoch": 7.396444444444445,
      "grad_norm": 0.12321141362190247,
      "learning_rate": 3.772222222222222e-06,
      "loss": 0.0029,
      "step": 83210
    },
    {
      "epoch": 7.397333333333333,
      "grad_norm": 0.11525185406208038,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0016,
      "step": 83220
    },
    {
      "epoch": 7.3982222222222225,
      "grad_norm": 0.11258455365896225,
      "learning_rate": 3.7611111111111113e-06,
      "loss": 0.002,
      "step": 83230
    },
    {
      "epoch": 7.399111111111111,
      "grad_norm": 0.08814366161823273,
      "learning_rate": 3.755555555555556e-06,
      "loss": 0.0019,
      "step": 83240
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.4349587857723236,
      "learning_rate": 3.75e-06,
      "loss": 0.0015,
      "step": 83250
    },
    {
      "epoch": 7.400888888888889,
      "grad_norm": 0.2466641664505005,
      "learning_rate": 3.744444444444445e-06,
      "loss": 0.0015,
      "step": 83260
    },
    {
      "epoch": 7.401777777777777,
      "grad_norm": 0.08763373643159866,
      "learning_rate": 3.738888888888889e-06,
      "loss": 0.0016,
      "step": 83270
    },
    {
      "epoch": 7.402666666666667,
      "grad_norm": 0.4746609330177307,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0015,
      "step": 83280
    },
    {
      "epoch": 7.403555555555555,
      "grad_norm": 0.37482038140296936,
      "learning_rate": 3.727777777777778e-06,
      "loss": 0.0022,
      "step": 83290
    },
    {
      "epoch": 7.404444444444445,
      "grad_norm": 0.30764564871788025,
      "learning_rate": 3.722222222222222e-06,
      "loss": 0.0018,
      "step": 83300
    },
    {
      "epoch": 7.405333333333333,
      "grad_norm": 0.4667297601699829,
      "learning_rate": 3.716666666666667e-06,
      "loss": 0.0019,
      "step": 83310
    },
    {
      "epoch": 7.4062222222222225,
      "grad_norm": 0.13412722945213318,
      "learning_rate": 3.711111111111111e-06,
      "loss": 0.0023,
      "step": 83320
    },
    {
      "epoch": 7.407111111111111,
      "grad_norm": 0.49356406927108765,
      "learning_rate": 3.7055555555555557e-06,
      "loss": 0.003,
      "step": 83330
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.27920931577682495,
      "learning_rate": 3.7e-06,
      "loss": 0.0014,
      "step": 83340
    },
    {
      "epoch": 7.408888888888889,
      "grad_norm": 0.3279722332954407,
      "learning_rate": 3.694444444444445e-06,
      "loss": 0.0018,
      "step": 83350
    },
    {
      "epoch": 7.409777777777778,
      "grad_norm": 0.22283609211444855,
      "learning_rate": 3.688888888888889e-06,
      "loss": 0.0017,
      "step": 83360
    },
    {
      "epoch": 7.410666666666667,
      "grad_norm": 0.28900715708732605,
      "learning_rate": 3.6833333333333338e-06,
      "loss": 0.0016,
      "step": 83370
    },
    {
      "epoch": 7.411555555555555,
      "grad_norm": 0.5027898550033569,
      "learning_rate": 3.6777777777777778e-06,
      "loss": 0.0024,
      "step": 83380
    },
    {
      "epoch": 7.412444444444445,
      "grad_norm": 0.33709073066711426,
      "learning_rate": 3.6722222222222226e-06,
      "loss": 0.0019,
      "step": 83390
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.21730080246925354,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0014,
      "step": 83400
    },
    {
      "epoch": 7.4142222222222225,
      "grad_norm": 0.6036474108695984,
      "learning_rate": 3.661111111111111e-06,
      "loss": 0.0026,
      "step": 83410
    },
    {
      "epoch": 7.415111111111111,
      "grad_norm": 0.14522713422775269,
      "learning_rate": 3.655555555555556e-06,
      "loss": 0.0012,
      "step": 83420
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.12424451857805252,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 0.0015,
      "step": 83430
    },
    {
      "epoch": 7.416888888888889,
      "grad_norm": 0.7168741822242737,
      "learning_rate": 3.6444444444444446e-06,
      "loss": 0.0015,
      "step": 83440
    },
    {
      "epoch": 7.417777777777777,
      "grad_norm": 0.4371904730796814,
      "learning_rate": 3.638888888888889e-06,
      "loss": 0.0013,
      "step": 83450
    },
    {
      "epoch": 7.418666666666667,
      "grad_norm": 0.2148045003414154,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.0024,
      "step": 83460
    },
    {
      "epoch": 7.419555555555555,
      "grad_norm": 0.37875065207481384,
      "learning_rate": 3.627777777777778e-06,
      "loss": 0.0022,
      "step": 83470
    },
    {
      "epoch": 7.420444444444445,
      "grad_norm": 0.05723216384649277,
      "learning_rate": 3.6222222222222226e-06,
      "loss": 0.0018,
      "step": 83480
    },
    {
      "epoch": 7.421333333333333,
      "grad_norm": 0.11158998310565948,
      "learning_rate": 3.6166666666666666e-06,
      "loss": 0.0022,
      "step": 83490
    },
    {
      "epoch": 7.4222222222222225,
      "grad_norm": 0.049056876450777054,
      "learning_rate": 3.611111111111111e-06,
      "loss": 0.0016,
      "step": 83500
    },
    {
      "epoch": 7.423111111111111,
      "grad_norm": 0.17951011657714844,
      "learning_rate": 3.605555555555556e-06,
      "loss": 0.0015,
      "step": 83510
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.2452486902475357,
      "learning_rate": 3.6e-06,
      "loss": 0.0014,
      "step": 83520
    },
    {
      "epoch": 7.424888888888889,
      "grad_norm": 0.05272166058421135,
      "learning_rate": 3.5944444444444447e-06,
      "loss": 0.002,
      "step": 83530
    },
    {
      "epoch": 7.425777777777777,
      "grad_norm": 0.1834161877632141,
      "learning_rate": 3.588888888888889e-06,
      "loss": 0.0011,
      "step": 83540
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 0.05704613775014877,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.0018,
      "step": 83550
    },
    {
      "epoch": 7.427555555555555,
      "grad_norm": 0.28304916620254517,
      "learning_rate": 3.577777777777778e-06,
      "loss": 0.002,
      "step": 83560
    },
    {
      "epoch": 7.428444444444445,
      "grad_norm": 0.24453699588775635,
      "learning_rate": 3.5722222222222227e-06,
      "loss": 0.0021,
      "step": 83570
    },
    {
      "epoch": 7.429333333333333,
      "grad_norm": 0.20957490801811218,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.0018,
      "step": 83580
    },
    {
      "epoch": 7.4302222222222225,
      "grad_norm": 0.1842339038848877,
      "learning_rate": 3.5611111111111115e-06,
      "loss": 0.0018,
      "step": 83590
    },
    {
      "epoch": 7.431111111111111,
      "grad_norm": 0.41530248522758484,
      "learning_rate": 3.555555555555556e-06,
      "loss": 0.0022,
      "step": 83600
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.08906804770231247,
      "learning_rate": 3.55e-06,
      "loss": 0.0033,
      "step": 83610
    },
    {
      "epoch": 7.432888888888889,
      "grad_norm": 0.04536229372024536,
      "learning_rate": 3.5444444444444447e-06,
      "loss": 0.0026,
      "step": 83620
    },
    {
      "epoch": 7.433777777777777,
      "grad_norm": 0.4423336684703827,
      "learning_rate": 3.5388888888888887e-06,
      "loss": 0.0016,
      "step": 83630
    },
    {
      "epoch": 7.434666666666667,
      "grad_norm": 0.05622164160013199,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0014,
      "step": 83640
    },
    {
      "epoch": 7.435555555555555,
      "grad_norm": 0.11681561172008514,
      "learning_rate": 3.527777777777778e-06,
      "loss": 0.0023,
      "step": 83650
    },
    {
      "epoch": 7.436444444444445,
      "grad_norm": 0.5443926453590393,
      "learning_rate": 3.5222222222222228e-06,
      "loss": 0.0014,
      "step": 83660
    },
    {
      "epoch": 7.437333333333333,
      "grad_norm": 0.064921535551548,
      "learning_rate": 3.5166666666666667e-06,
      "loss": 0.0016,
      "step": 83670
    },
    {
      "epoch": 7.4382222222222225,
      "grad_norm": 0.6433870196342468,
      "learning_rate": 3.5111111111111116e-06,
      "loss": 0.0018,
      "step": 83680
    },
    {
      "epoch": 7.439111111111111,
      "grad_norm": 0.17280402779579163,
      "learning_rate": 3.5055555555555555e-06,
      "loss": 0.0028,
      "step": 83690
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.04743776097893715,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0019,
      "step": 83700
    },
    {
      "epoch": 7.440888888888889,
      "grad_norm": 0.08190404623746872,
      "learning_rate": 3.4944444444444448e-06,
      "loss": 0.0016,
      "step": 83710
    },
    {
      "epoch": 7.441777777777777,
      "grad_norm": 0.5113045573234558,
      "learning_rate": 3.4888888888888888e-06,
      "loss": 0.0016,
      "step": 83720
    },
    {
      "epoch": 7.442666666666667,
      "grad_norm": 0.0850815549492836,
      "learning_rate": 3.4833333333333336e-06,
      "loss": 0.0012,
      "step": 83730
    },
    {
      "epoch": 7.443555555555555,
      "grad_norm": 0.2483963519334793,
      "learning_rate": 3.4777777777777776e-06,
      "loss": 0.002,
      "step": 83740
    },
    {
      "epoch": 7.444444444444445,
      "grad_norm": 0.3697495460510254,
      "learning_rate": 3.4722222222222224e-06,
      "loss": 0.0017,
      "step": 83750
    },
    {
      "epoch": 7.445333333333333,
      "grad_norm": 0.5088515281677246,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0015,
      "step": 83760
    },
    {
      "epoch": 7.4462222222222225,
      "grad_norm": 0.3682536482810974,
      "learning_rate": 3.4611111111111116e-06,
      "loss": 0.0025,
      "step": 83770
    },
    {
      "epoch": 7.447111111111111,
      "grad_norm": 0.45553523302078247,
      "learning_rate": 3.4555555555555556e-06,
      "loss": 0.0014,
      "step": 83780
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.5761685371398926,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 0.0013,
      "step": 83790
    },
    {
      "epoch": 7.448888888888889,
      "grad_norm": 0.20229990780353546,
      "learning_rate": 3.4444444444444444e-06,
      "loss": 0.0029,
      "step": 83800
    },
    {
      "epoch": 7.449777777777777,
      "grad_norm": 0.05500735342502594,
      "learning_rate": 3.438888888888889e-06,
      "loss": 0.0016,
      "step": 83810
    },
    {
      "epoch": 7.450666666666667,
      "grad_norm": 0.12889401614665985,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0021,
      "step": 83820
    },
    {
      "epoch": 7.451555555555555,
      "grad_norm": 0.06002843752503395,
      "learning_rate": 3.4277777777777776e-06,
      "loss": 0.0027,
      "step": 83830
    },
    {
      "epoch": 7.452444444444445,
      "grad_norm": 0.4413284659385681,
      "learning_rate": 3.4222222222222224e-06,
      "loss": 0.0016,
      "step": 83840
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 0.2521837055683136,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.0012,
      "step": 83850
    },
    {
      "epoch": 7.4542222222222225,
      "grad_norm": 0.20821967720985413,
      "learning_rate": 3.4111111111111113e-06,
      "loss": 0.0016,
      "step": 83860
    },
    {
      "epoch": 7.455111111111111,
      "grad_norm": 0.4029548168182373,
      "learning_rate": 3.4055555555555557e-06,
      "loss": 0.0028,
      "step": 83870
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.32897496223449707,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0015,
      "step": 83880
    },
    {
      "epoch": 7.456888888888889,
      "grad_norm": 0.21594595909118652,
      "learning_rate": 3.3944444444444445e-06,
      "loss": 0.0016,
      "step": 83890
    },
    {
      "epoch": 7.457777777777777,
      "grad_norm": 0.43145889043807983,
      "learning_rate": 3.3888888888888893e-06,
      "loss": 0.002,
      "step": 83900
    },
    {
      "epoch": 7.458666666666667,
      "grad_norm": 0.2622973322868347,
      "learning_rate": 3.3833333333333337e-06,
      "loss": 0.002,
      "step": 83910
    },
    {
      "epoch": 7.459555555555555,
      "grad_norm": 0.28683558106422424,
      "learning_rate": 3.3777777777777777e-06,
      "loss": 0.0013,
      "step": 83920
    },
    {
      "epoch": 7.460444444444445,
      "grad_norm": 0.324952632188797,
      "learning_rate": 3.3722222222222225e-06,
      "loss": 0.0025,
      "step": 83930
    },
    {
      "epoch": 7.461333333333333,
      "grad_norm": 0.19994671642780304,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.0026,
      "step": 83940
    },
    {
      "epoch": 7.4622222222222225,
      "grad_norm": 0.12323017418384552,
      "learning_rate": 3.3611111111111113e-06,
      "loss": 0.0014,
      "step": 83950
    },
    {
      "epoch": 7.463111111111111,
      "grad_norm": 0.22305573523044586,
      "learning_rate": 3.3555555555555557e-06,
      "loss": 0.0012,
      "step": 83960
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.38950905203819275,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 0.0021,
      "step": 83970
    },
    {
      "epoch": 7.464888888888889,
      "grad_norm": 0.5655978918075562,
      "learning_rate": 3.3444444444444445e-06,
      "loss": 0.0016,
      "step": 83980
    },
    {
      "epoch": 7.465777777777777,
      "grad_norm": 0.28119346499443054,
      "learning_rate": 3.3388888888888893e-06,
      "loss": 0.0015,
      "step": 83990
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.2559066116809845,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0014,
      "step": 84000
    },
    {
      "epoch": 7.467555555555555,
      "grad_norm": 0.06406837701797485,
      "learning_rate": 3.327777777777778e-06,
      "loss": 0.0013,
      "step": 84010
    },
    {
      "epoch": 7.468444444444445,
      "grad_norm": 0.2869829535484314,
      "learning_rate": 3.3222222222222226e-06,
      "loss": 0.0024,
      "step": 84020
    },
    {
      "epoch": 7.469333333333333,
      "grad_norm": 0.14928805828094482,
      "learning_rate": 3.3166666666666665e-06,
      "loss": 0.0014,
      "step": 84030
    },
    {
      "epoch": 7.4702222222222225,
      "grad_norm": 0.32747402787208557,
      "learning_rate": 3.3111111111111114e-06,
      "loss": 0.0014,
      "step": 84040
    },
    {
      "epoch": 7.471111111111111,
      "grad_norm": 0.3570702075958252,
      "learning_rate": 3.3055555555555553e-06,
      "loss": 0.0014,
      "step": 84050
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.3893679082393646,
      "learning_rate": 3.3e-06,
      "loss": 0.0016,
      "step": 84060
    },
    {
      "epoch": 7.472888888888889,
      "grad_norm": 0.2797316908836365,
      "learning_rate": 3.2944444444444446e-06,
      "loss": 0.0021,
      "step": 84070
    },
    {
      "epoch": 7.473777777777777,
      "grad_norm": 0.19091713428497314,
      "learning_rate": 3.2888888888888894e-06,
      "loss": 0.0012,
      "step": 84080
    },
    {
      "epoch": 7.474666666666667,
      "grad_norm": 0.11473438888788223,
      "learning_rate": 3.2833333333333334e-06,
      "loss": 0.0022,
      "step": 84090
    },
    {
      "epoch": 7.475555555555555,
      "grad_norm": 0.1534755825996399,
      "learning_rate": 3.277777777777778e-06,
      "loss": 0.002,
      "step": 84100
    },
    {
      "epoch": 7.476444444444445,
      "grad_norm": 0.14608615636825562,
      "learning_rate": 3.272222222222222e-06,
      "loss": 0.0013,
      "step": 84110
    },
    {
      "epoch": 7.477333333333333,
      "grad_norm": 0.08078819513320923,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0013,
      "step": 84120
    },
    {
      "epoch": 7.4782222222222225,
      "grad_norm": 0.29006239771842957,
      "learning_rate": 3.2611111111111114e-06,
      "loss": 0.0018,
      "step": 84130
    },
    {
      "epoch": 7.479111111111111,
      "grad_norm": 0.22587640583515167,
      "learning_rate": 3.2555555555555554e-06,
      "loss": 0.0019,
      "step": 84140
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.23188748955726624,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.0013,
      "step": 84150
    },
    {
      "epoch": 7.480888888888889,
      "grad_norm": 0.3001329302787781,
      "learning_rate": 3.244444444444444e-06,
      "loss": 0.002,
      "step": 84160
    },
    {
      "epoch": 7.481777777777777,
      "grad_norm": 0.2269611656665802,
      "learning_rate": 3.238888888888889e-06,
      "loss": 0.0018,
      "step": 84170
    },
    {
      "epoch": 7.482666666666667,
      "grad_norm": 0.21981889009475708,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0019,
      "step": 84180
    },
    {
      "epoch": 7.483555555555555,
      "grad_norm": 0.4106617271900177,
      "learning_rate": 3.2277777777777783e-06,
      "loss": 0.0022,
      "step": 84190
    },
    {
      "epoch": 7.484444444444445,
      "grad_norm": 0.392599880695343,
      "learning_rate": 3.2222222222222222e-06,
      "loss": 0.0014,
      "step": 84200
    },
    {
      "epoch": 7.485333333333333,
      "grad_norm": 0.2810245156288147,
      "learning_rate": 3.216666666666667e-06,
      "loss": 0.0016,
      "step": 84210
    },
    {
      "epoch": 7.4862222222222226,
      "grad_norm": 0.3556972146034241,
      "learning_rate": 3.2111111111111115e-06,
      "loss": 0.0021,
      "step": 84220
    },
    {
      "epoch": 7.487111111111111,
      "grad_norm": 0.24893370270729065,
      "learning_rate": 3.2055555555555555e-06,
      "loss": 0.0014,
      "step": 84230
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.32308435440063477,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0013,
      "step": 84240
    },
    {
      "epoch": 7.488888888888889,
      "grad_norm": 0.32312142848968506,
      "learning_rate": 3.1944444444444443e-06,
      "loss": 0.0027,
      "step": 84250
    },
    {
      "epoch": 7.489777777777777,
      "grad_norm": 0.3880368173122406,
      "learning_rate": 3.188888888888889e-06,
      "loss": 0.0017,
      "step": 84260
    },
    {
      "epoch": 7.490666666666667,
      "grad_norm": 0.23353175818920135,
      "learning_rate": 3.1833333333333335e-06,
      "loss": 0.0024,
      "step": 84270
    },
    {
      "epoch": 7.491555555555555,
      "grad_norm": 0.2207116335630417,
      "learning_rate": 3.1777777777777783e-06,
      "loss": 0.0012,
      "step": 84280
    },
    {
      "epoch": 7.492444444444445,
      "grad_norm": 0.04130608215928078,
      "learning_rate": 3.1722222222222223e-06,
      "loss": 0.0019,
      "step": 84290
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 0.33472228050231934,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0016,
      "step": 84300
    },
    {
      "epoch": 7.494222222222223,
      "grad_norm": 0.08428864181041718,
      "learning_rate": 3.161111111111111e-06,
      "loss": 0.0012,
      "step": 84310
    },
    {
      "epoch": 7.495111111111111,
      "grad_norm": 0.3812043368816376,
      "learning_rate": 3.155555555555556e-06,
      "loss": 0.0014,
      "step": 84320
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.06061868369579315,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 0.0021,
      "step": 84330
    },
    {
      "epoch": 7.496888888888889,
      "grad_norm": 0.2340175360441208,
      "learning_rate": 3.1444444444444443e-06,
      "loss": 0.0017,
      "step": 84340
    },
    {
      "epoch": 7.497777777777777,
      "grad_norm": 0.2486380785703659,
      "learning_rate": 3.138888888888889e-06,
      "loss": 0.0026,
      "step": 84350
    },
    {
      "epoch": 7.498666666666667,
      "grad_norm": 0.45618879795074463,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0013,
      "step": 84360
    },
    {
      "epoch": 7.499555555555555,
      "grad_norm": 0.2958337068557739,
      "learning_rate": 3.127777777777778e-06,
      "loss": 0.0023,
      "step": 84370
    },
    {
      "epoch": 7.500444444444445,
      "grad_norm": 0.5680010914802551,
      "learning_rate": 3.1222222222222224e-06,
      "loss": 0.0022,
      "step": 84380
    },
    {
      "epoch": 7.501333333333333,
      "grad_norm": 0.1500179022550583,
      "learning_rate": 3.1166666666666668e-06,
      "loss": 0.0014,
      "step": 84390
    },
    {
      "epoch": 7.502222222222223,
      "grad_norm": 0.11011295020580292,
      "learning_rate": 3.111111111111111e-06,
      "loss": 0.0012,
      "step": 84400
    },
    {
      "epoch": 7.503111111111111,
      "grad_norm": 0.5195779800415039,
      "learning_rate": 3.1055555555555556e-06,
      "loss": 0.0021,
      "step": 84410
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.4297553300857544,
      "learning_rate": 3.1e-06,
      "loss": 0.0024,
      "step": 84420
    },
    {
      "epoch": 7.504888888888889,
      "grad_norm": 0.535377562046051,
      "learning_rate": 3.094444444444445e-06,
      "loss": 0.0014,
      "step": 84430
    },
    {
      "epoch": 7.505777777777777,
      "grad_norm": 0.42950448393821716,
      "learning_rate": 3.088888888888889e-06,
      "loss": 0.0014,
      "step": 84440
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 0.10979214310646057,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.0011,
      "step": 84450
    },
    {
      "epoch": 7.507555555555555,
      "grad_norm": 0.07969429343938828,
      "learning_rate": 3.077777777777778e-06,
      "loss": 0.0014,
      "step": 84460
    },
    {
      "epoch": 7.508444444444445,
      "grad_norm": 0.24478042125701904,
      "learning_rate": 3.0722222222222224e-06,
      "loss": 0.0019,
      "step": 84470
    },
    {
      "epoch": 7.509333333333333,
      "grad_norm": 0.5456252098083496,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0019,
      "step": 84480
    },
    {
      "epoch": 7.510222222222223,
      "grad_norm": 0.3602871596813202,
      "learning_rate": 3.0611111111111112e-06,
      "loss": 0.0013,
      "step": 84490
    },
    {
      "epoch": 7.511111111111111,
      "grad_norm": 0.11249541491270065,
      "learning_rate": 3.0555555555555556e-06,
      "loss": 0.0024,
      "step": 84500
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.181773841381073,
      "learning_rate": 3.05e-06,
      "loss": 0.0017,
      "step": 84510
    },
    {
      "epoch": 7.512888888888889,
      "grad_norm": 0.499345600605011,
      "learning_rate": 3.0444444444444444e-06,
      "loss": 0.0014,
      "step": 84520
    },
    {
      "epoch": 7.5137777777777774,
      "grad_norm": 0.10726071149110794,
      "learning_rate": 3.038888888888889e-06,
      "loss": 0.0018,
      "step": 84530
    },
    {
      "epoch": 7.514666666666667,
      "grad_norm": 0.4283416271209717,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.0013,
      "step": 84540
    },
    {
      "epoch": 7.515555555555555,
      "grad_norm": 0.21715210378170013,
      "learning_rate": 3.027777777777778e-06,
      "loss": 0.0016,
      "step": 84550
    },
    {
      "epoch": 7.516444444444445,
      "grad_norm": 0.22424812614917755,
      "learning_rate": 3.0222222222222225e-06,
      "loss": 0.0016,
      "step": 84560
    },
    {
      "epoch": 7.517333333333333,
      "grad_norm": 0.33728209137916565,
      "learning_rate": 3.016666666666667e-06,
      "loss": 0.0025,
      "step": 84570
    },
    {
      "epoch": 7.518222222222223,
      "grad_norm": 0.3584127724170685,
      "learning_rate": 3.0111111111111113e-06,
      "loss": 0.0023,
      "step": 84580
    },
    {
      "epoch": 7.519111111111111,
      "grad_norm": 0.08504883199930191,
      "learning_rate": 3.0055555555555557e-06,
      "loss": 0.0018,
      "step": 84590
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.04454801231622696,
      "learning_rate": 3e-06,
      "loss": 0.0011,
      "step": 84600
    },
    {
      "epoch": 7.520888888888889,
      "grad_norm": 0.31967636942863464,
      "learning_rate": 2.9944444444444445e-06,
      "loss": 0.0018,
      "step": 84610
    },
    {
      "epoch": 7.5217777777777775,
      "grad_norm": 0.4917560815811157,
      "learning_rate": 2.988888888888889e-06,
      "loss": 0.002,
      "step": 84620
    },
    {
      "epoch": 7.522666666666667,
      "grad_norm": 0.2520187795162201,
      "learning_rate": 2.9833333333333333e-06,
      "loss": 0.0014,
      "step": 84630
    },
    {
      "epoch": 7.523555555555555,
      "grad_norm": 0.17965944111347198,
      "learning_rate": 2.977777777777778e-06,
      "loss": 0.0013,
      "step": 84640
    },
    {
      "epoch": 7.524444444444445,
      "grad_norm": 0.4739338159561157,
      "learning_rate": 2.9722222222222225e-06,
      "loss": 0.0021,
      "step": 84650
    },
    {
      "epoch": 7.525333333333333,
      "grad_norm": 0.7952215671539307,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0022,
      "step": 84660
    },
    {
      "epoch": 7.526222222222223,
      "grad_norm": 0.5723994374275208,
      "learning_rate": 2.9611111111111113e-06,
      "loss": 0.0016,
      "step": 84670
    },
    {
      "epoch": 7.527111111111111,
      "grad_norm": 0.2608520984649658,
      "learning_rate": 2.9555555555555557e-06,
      "loss": 0.0011,
      "step": 84680
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.11706988513469696,
      "learning_rate": 2.95e-06,
      "loss": 0.0026,
      "step": 84690
    },
    {
      "epoch": 7.528888888888889,
      "grad_norm": 0.18247757852077484,
      "learning_rate": 2.9444444444444445e-06,
      "loss": 0.0032,
      "step": 84700
    },
    {
      "epoch": 7.5297777777777775,
      "grad_norm": 0.363162100315094,
      "learning_rate": 2.938888888888889e-06,
      "loss": 0.0018,
      "step": 84710
    },
    {
      "epoch": 7.530666666666667,
      "grad_norm": 0.08969299495220184,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0014,
      "step": 84720
    },
    {
      "epoch": 7.531555555555555,
      "grad_norm": 0.16583076119422913,
      "learning_rate": 2.9277777777777777e-06,
      "loss": 0.0021,
      "step": 84730
    },
    {
      "epoch": 7.532444444444445,
      "grad_norm": 0.2834594249725342,
      "learning_rate": 2.9222222222222226e-06,
      "loss": 0.0018,
      "step": 84740
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.053629353642463684,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0016,
      "step": 84750
    },
    {
      "epoch": 7.534222222222223,
      "grad_norm": 0.3673185706138611,
      "learning_rate": 2.9111111111111114e-06,
      "loss": 0.0016,
      "step": 84760
    },
    {
      "epoch": 7.535111111111111,
      "grad_norm": 0.39626964926719666,
      "learning_rate": 2.9055555555555558e-06,
      "loss": 0.0015,
      "step": 84770
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.1438826471567154,
      "learning_rate": 2.9e-06,
      "loss": 0.0018,
      "step": 84780
    },
    {
      "epoch": 7.536888888888889,
      "grad_norm": 0.02663005329668522,
      "learning_rate": 2.8944444444444446e-06,
      "loss": 0.0016,
      "step": 84790
    },
    {
      "epoch": 7.5377777777777775,
      "grad_norm": 0.2842249870300293,
      "learning_rate": 2.888888888888889e-06,
      "loss": 0.0015,
      "step": 84800
    },
    {
      "epoch": 7.538666666666667,
      "grad_norm": 0.14216671884059906,
      "learning_rate": 2.8833333333333334e-06,
      "loss": 0.0015,
      "step": 84810
    },
    {
      "epoch": 7.539555555555555,
      "grad_norm": 0.1337936669588089,
      "learning_rate": 2.877777777777778e-06,
      "loss": 0.0015,
      "step": 84820
    },
    {
      "epoch": 7.540444444444445,
      "grad_norm": 0.5777706503868103,
      "learning_rate": 2.872222222222222e-06,
      "loss": 0.0012,
      "step": 84830
    },
    {
      "epoch": 7.541333333333333,
      "grad_norm": 0.4079534113407135,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0022,
      "step": 84840
    },
    {
      "epoch": 7.542222222222223,
      "grad_norm": 0.2646174132823944,
      "learning_rate": 2.8611111111111114e-06,
      "loss": 0.0019,
      "step": 84850
    },
    {
      "epoch": 7.543111111111111,
      "grad_norm": 0.03068101964890957,
      "learning_rate": 2.855555555555556e-06,
      "loss": 0.0024,
      "step": 84860
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.1921837478876114,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 0.0025,
      "step": 84870
    },
    {
      "epoch": 7.544888888888889,
      "grad_norm": 0.4670808017253876,
      "learning_rate": 2.8444444444444446e-06,
      "loss": 0.0018,
      "step": 84880
    },
    {
      "epoch": 7.5457777777777775,
      "grad_norm": 0.7498783469200134,
      "learning_rate": 2.838888888888889e-06,
      "loss": 0.0018,
      "step": 84890
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 0.18614184856414795,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.0013,
      "step": 84900
    },
    {
      "epoch": 7.547555555555555,
      "grad_norm": 0.09779317677021027,
      "learning_rate": 2.827777777777778e-06,
      "loss": 0.0021,
      "step": 84910
    },
    {
      "epoch": 7.548444444444445,
      "grad_norm": 0.2566137909889221,
      "learning_rate": 2.8222222222222223e-06,
      "loss": 0.0025,
      "step": 84920
    },
    {
      "epoch": 7.549333333333333,
      "grad_norm": 0.41258135437965393,
      "learning_rate": 2.8166666666666667e-06,
      "loss": 0.0016,
      "step": 84930
    },
    {
      "epoch": 7.550222222222223,
      "grad_norm": 0.18591411411762238,
      "learning_rate": 2.811111111111111e-06,
      "loss": 0.0014,
      "step": 84940
    },
    {
      "epoch": 7.551111111111111,
      "grad_norm": 0.39207208156585693,
      "learning_rate": 2.805555555555556e-06,
      "loss": 0.0017,
      "step": 84950
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.25439316034317017,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0022,
      "step": 84960
    },
    {
      "epoch": 7.552888888888889,
      "grad_norm": 0.09937643259763718,
      "learning_rate": 2.7944444444444447e-06,
      "loss": 0.0018,
      "step": 84970
    },
    {
      "epoch": 7.5537777777777775,
      "grad_norm": 0.05560602992773056,
      "learning_rate": 2.788888888888889e-06,
      "loss": 0.0014,
      "step": 84980
    },
    {
      "epoch": 7.554666666666667,
      "grad_norm": 0.42563706636428833,
      "learning_rate": 2.7833333333333335e-06,
      "loss": 0.0019,
      "step": 84990
    },
    {
      "epoch": 7.555555555555555,
      "grad_norm": 0.2828170657157898,
      "learning_rate": 2.777777777777778e-06,
      "loss": 0.0017,
      "step": 85000
    },
    {
      "epoch": 7.556444444444445,
      "grad_norm": 0.10420035570859909,
      "learning_rate": 2.7722222222222223e-06,
      "loss": 0.0016,
      "step": 85010
    },
    {
      "epoch": 7.557333333333333,
      "grad_norm": 0.18324871361255646,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.0016,
      "step": 85020
    },
    {
      "epoch": 7.558222222222222,
      "grad_norm": 0.1540878415107727,
      "learning_rate": 2.761111111111111e-06,
      "loss": 0.0027,
      "step": 85030
    },
    {
      "epoch": 7.559111111111111,
      "grad_norm": 0.15048837661743164,
      "learning_rate": 2.7555555555555555e-06,
      "loss": 0.0021,
      "step": 85040
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.365854948759079,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.0017,
      "step": 85050
    },
    {
      "epoch": 7.560888888888889,
      "grad_norm": 0.44665977358818054,
      "learning_rate": 2.7444444444444448e-06,
      "loss": 0.0019,
      "step": 85060
    },
    {
      "epoch": 7.5617777777777775,
      "grad_norm": 0.32360905408859253,
      "learning_rate": 2.738888888888889e-06,
      "loss": 0.0019,
      "step": 85070
    },
    {
      "epoch": 7.562666666666667,
      "grad_norm": 0.5233296751976013,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0018,
      "step": 85080
    },
    {
      "epoch": 7.563555555555555,
      "grad_norm": 0.46346044540405273,
      "learning_rate": 2.727777777777778e-06,
      "loss": 0.0019,
      "step": 85090
    },
    {
      "epoch": 7.564444444444445,
      "grad_norm": 0.09007346630096436,
      "learning_rate": 2.7222222222222224e-06,
      "loss": 0.0027,
      "step": 85100
    },
    {
      "epoch": 7.565333333333333,
      "grad_norm": 0.5505374073982239,
      "learning_rate": 2.7166666666666668e-06,
      "loss": 0.0015,
      "step": 85110
    },
    {
      "epoch": 7.566222222222223,
      "grad_norm": 0.11109035462141037,
      "learning_rate": 2.711111111111111e-06,
      "loss": 0.0025,
      "step": 85120
    },
    {
      "epoch": 7.567111111111111,
      "grad_norm": 0.05121728032827377,
      "learning_rate": 2.7055555555555556e-06,
      "loss": 0.0027,
      "step": 85130
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.42236122488975525,
      "learning_rate": 2.7e-06,
      "loss": 0.0024,
      "step": 85140
    },
    {
      "epoch": 7.568888888888889,
      "grad_norm": 0.24767951667308807,
      "learning_rate": 2.6944444444444444e-06,
      "loss": 0.0015,
      "step": 85150
    },
    {
      "epoch": 7.5697777777777775,
      "grad_norm": 0.35608747601509094,
      "learning_rate": 2.6888888888888892e-06,
      "loss": 0.0016,
      "step": 85160
    },
    {
      "epoch": 7.570666666666667,
      "grad_norm": 0.10659099370241165,
      "learning_rate": 2.6833333333333336e-06,
      "loss": 0.0024,
      "step": 85170
    },
    {
      "epoch": 7.571555555555555,
      "grad_norm": 0.1412818729877472,
      "learning_rate": 2.677777777777778e-06,
      "loss": 0.0027,
      "step": 85180
    },
    {
      "epoch": 7.572444444444445,
      "grad_norm": 0.3105626106262207,
      "learning_rate": 2.6722222222222224e-06,
      "loss": 0.0019,
      "step": 85190
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.16816797852516174,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0013,
      "step": 85200
    },
    {
      "epoch": 7.574222222222222,
      "grad_norm": 0.11550737917423248,
      "learning_rate": 2.6611111111111112e-06,
      "loss": 0.0013,
      "step": 85210
    },
    {
      "epoch": 7.575111111111111,
      "grad_norm": 0.0460914745926857,
      "learning_rate": 2.6555555555555556e-06,
      "loss": 0.0018,
      "step": 85220
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.21288034319877625,
      "learning_rate": 2.65e-06,
      "loss": 0.0016,
      "step": 85230
    },
    {
      "epoch": 7.576888888888889,
      "grad_norm": 0.549021303653717,
      "learning_rate": 2.6444444444444444e-06,
      "loss": 0.0016,
      "step": 85240
    },
    {
      "epoch": 7.5777777777777775,
      "grad_norm": 0.41304656863212585,
      "learning_rate": 2.638888888888889e-06,
      "loss": 0.0013,
      "step": 85250
    },
    {
      "epoch": 7.578666666666667,
      "grad_norm": 0.2509068548679352,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.0025,
      "step": 85260
    },
    {
      "epoch": 7.579555555555555,
      "grad_norm": 0.06025530770421028,
      "learning_rate": 2.627777777777778e-06,
      "loss": 0.0018,
      "step": 85270
    },
    {
      "epoch": 7.580444444444445,
      "grad_norm": 0.15365098416805267,
      "learning_rate": 2.6222222222222225e-06,
      "loss": 0.0016,
      "step": 85280
    },
    {
      "epoch": 7.581333333333333,
      "grad_norm": 0.2905120253562927,
      "learning_rate": 2.616666666666667e-06,
      "loss": 0.0022,
      "step": 85290
    },
    {
      "epoch": 7.582222222222223,
      "grad_norm": 0.2899458110332489,
      "learning_rate": 2.6111111111111113e-06,
      "loss": 0.0015,
      "step": 85300
    },
    {
      "epoch": 7.583111111111111,
      "grad_norm": 0.3315727114677429,
      "learning_rate": 2.6055555555555557e-06,
      "loss": 0.002,
      "step": 85310
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.2856921851634979,
      "learning_rate": 2.6e-06,
      "loss": 0.0018,
      "step": 85320
    },
    {
      "epoch": 7.584888888888889,
      "grad_norm": 0.6091713905334473,
      "learning_rate": 2.5944444444444445e-06,
      "loss": 0.0028,
      "step": 85330
    },
    {
      "epoch": 7.5857777777777775,
      "grad_norm": 0.14968222379684448,
      "learning_rate": 2.588888888888889e-06,
      "loss": 0.0013,
      "step": 85340
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 0.18095020949840546,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.0021,
      "step": 85350
    },
    {
      "epoch": 7.587555555555555,
      "grad_norm": 0.2443353533744812,
      "learning_rate": 2.5777777777777777e-06,
      "loss": 0.0017,
      "step": 85360
    },
    {
      "epoch": 7.588444444444445,
      "grad_norm": 0.14364974200725555,
      "learning_rate": 2.5722222222222225e-06,
      "loss": 0.0014,
      "step": 85370
    },
    {
      "epoch": 7.589333333333333,
      "grad_norm": 0.29706084728240967,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.002,
      "step": 85380
    },
    {
      "epoch": 7.590222222222222,
      "grad_norm": 0.30233511328697205,
      "learning_rate": 2.5611111111111113e-06,
      "loss": 0.0028,
      "step": 85390
    },
    {
      "epoch": 7.591111111111111,
      "grad_norm": 0.10074906051158905,
      "learning_rate": 2.5555555555555557e-06,
      "loss": 0.0016,
      "step": 85400
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.10317627340555191,
      "learning_rate": 2.55e-06,
      "loss": 0.0013,
      "step": 85410
    },
    {
      "epoch": 7.592888888888889,
      "grad_norm": 0.3249109387397766,
      "learning_rate": 2.5444444444444446e-06,
      "loss": 0.0023,
      "step": 85420
    },
    {
      "epoch": 7.5937777777777775,
      "grad_norm": 0.2533802390098572,
      "learning_rate": 2.538888888888889e-06,
      "loss": 0.0013,
      "step": 85430
    },
    {
      "epoch": 7.594666666666667,
      "grad_norm": 0.028481757268309593,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0018,
      "step": 85440
    },
    {
      "epoch": 7.595555555555555,
      "grad_norm": 0.26240894198417664,
      "learning_rate": 2.5277777777777778e-06,
      "loss": 0.0011,
      "step": 85450
    },
    {
      "epoch": 7.596444444444445,
      "grad_norm": 0.23455318808555603,
      "learning_rate": 2.522222222222222e-06,
      "loss": 0.0011,
      "step": 85460
    },
    {
      "epoch": 7.597333333333333,
      "grad_norm": 0.2512069642543793,
      "learning_rate": 2.516666666666667e-06,
      "loss": 0.002,
      "step": 85470
    },
    {
      "epoch": 7.598222222222223,
      "grad_norm": 0.32664069533348083,
      "learning_rate": 2.5111111111111114e-06,
      "loss": 0.0016,
      "step": 85480
    },
    {
      "epoch": 7.599111111111111,
      "grad_norm": 0.543957531452179,
      "learning_rate": 2.505555555555556e-06,
      "loss": 0.0016,
      "step": 85490
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.18370747566223145,
      "learning_rate": 2.5e-06,
      "loss": 0.0011,
      "step": 85500
    },
    {
      "epoch": 7.600888888888889,
      "grad_norm": 0.21466350555419922,
      "learning_rate": 2.4944444444444446e-06,
      "loss": 0.0016,
      "step": 85510
    },
    {
      "epoch": 7.6017777777777775,
      "grad_norm": 0.42885592579841614,
      "learning_rate": 2.488888888888889e-06,
      "loss": 0.0023,
      "step": 85520
    },
    {
      "epoch": 7.602666666666667,
      "grad_norm": 0.3386324346065521,
      "learning_rate": 2.4833333333333334e-06,
      "loss": 0.002,
      "step": 85530
    },
    {
      "epoch": 7.603555555555555,
      "grad_norm": 0.5246946215629578,
      "learning_rate": 2.477777777777778e-06,
      "loss": 0.0015,
      "step": 85540
    },
    {
      "epoch": 7.604444444444445,
      "grad_norm": 0.11509920656681061,
      "learning_rate": 2.4722222222222222e-06,
      "loss": 0.0014,
      "step": 85550
    },
    {
      "epoch": 7.605333333333333,
      "grad_norm": 0.17895646393299103,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0014,
      "step": 85560
    },
    {
      "epoch": 7.606222222222222,
      "grad_norm": 0.25645291805267334,
      "learning_rate": 2.4611111111111115e-06,
      "loss": 0.003,
      "step": 85570
    },
    {
      "epoch": 7.607111111111111,
      "grad_norm": 0.3177891671657562,
      "learning_rate": 2.455555555555556e-06,
      "loss": 0.0018,
      "step": 85580
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.07196025550365448,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 0.0023,
      "step": 85590
    },
    {
      "epoch": 7.608888888888889,
      "grad_norm": 0.209384486079216,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 0.0017,
      "step": 85600
    },
    {
      "epoch": 7.6097777777777775,
      "grad_norm": 0.35719430446624756,
      "learning_rate": 2.438888888888889e-06,
      "loss": 0.0017,
      "step": 85610
    },
    {
      "epoch": 7.610666666666667,
      "grad_norm": 0.4497722089290619,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0019,
      "step": 85620
    },
    {
      "epoch": 7.611555555555555,
      "grad_norm": 0.28422603011131287,
      "learning_rate": 2.427777777777778e-06,
      "loss": 0.0016,
      "step": 85630
    },
    {
      "epoch": 7.612444444444445,
      "grad_norm": 0.2545841932296753,
      "learning_rate": 2.4222222222222223e-06,
      "loss": 0.003,
      "step": 85640
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 0.1536659449338913,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.0023,
      "step": 85650
    },
    {
      "epoch": 7.614222222222223,
      "grad_norm": 0.37346363067626953,
      "learning_rate": 2.411111111111111e-06,
      "loss": 0.0012,
      "step": 85660
    },
    {
      "epoch": 7.615111111111111,
      "grad_norm": 0.4085775315761566,
      "learning_rate": 2.4055555555555555e-06,
      "loss": 0.0011,
      "step": 85670
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.154519721865654,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0014,
      "step": 85680
    },
    {
      "epoch": 7.616888888888889,
      "grad_norm": 0.18918655812740326,
      "learning_rate": 2.3944444444444447e-06,
      "loss": 0.0016,
      "step": 85690
    },
    {
      "epoch": 7.6177777777777775,
      "grad_norm": 0.2656753659248352,
      "learning_rate": 2.388888888888889e-06,
      "loss": 0.0013,
      "step": 85700
    },
    {
      "epoch": 7.618666666666667,
      "grad_norm": 0.21593476831912994,
      "learning_rate": 2.3833333333333335e-06,
      "loss": 0.0018,
      "step": 85710
    },
    {
      "epoch": 7.619555555555555,
      "grad_norm": 0.11363223940134048,
      "learning_rate": 2.377777777777778e-06,
      "loss": 0.0014,
      "step": 85720
    },
    {
      "epoch": 7.620444444444445,
      "grad_norm": 0.6415873765945435,
      "learning_rate": 2.3722222222222223e-06,
      "loss": 0.0012,
      "step": 85730
    },
    {
      "epoch": 7.621333333333333,
      "grad_norm": 0.5683037638664246,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.0018,
      "step": 85740
    },
    {
      "epoch": 7.622222222222222,
      "grad_norm": 0.3253623843193054,
      "learning_rate": 2.361111111111111e-06,
      "loss": 0.003,
      "step": 85750
    },
    {
      "epoch": 7.623111111111111,
      "grad_norm": 0.056045517325401306,
      "learning_rate": 2.3555555555555555e-06,
      "loss": 0.0015,
      "step": 85760
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.4661136269569397,
      "learning_rate": 2.35e-06,
      "loss": 0.0017,
      "step": 85770
    },
    {
      "epoch": 7.624888888888889,
      "grad_norm": 0.3576762080192566,
      "learning_rate": 2.3444444444444448e-06,
      "loss": 0.0014,
      "step": 85780
    },
    {
      "epoch": 7.6257777777777775,
      "grad_norm": 0.4887099266052246,
      "learning_rate": 2.338888888888889e-06,
      "loss": 0.0017,
      "step": 85790
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.04651101306080818,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0022,
      "step": 85800
    },
    {
      "epoch": 7.627555555555555,
      "grad_norm": 0.39234066009521484,
      "learning_rate": 2.327777777777778e-06,
      "loss": 0.0018,
      "step": 85810
    },
    {
      "epoch": 7.628444444444445,
      "grad_norm": 0.4152088165283203,
      "learning_rate": 2.322222222222222e-06,
      "loss": 0.0019,
      "step": 85820
    },
    {
      "epoch": 7.629333333333333,
      "grad_norm": 0.21566545963287354,
      "learning_rate": 2.316666666666667e-06,
      "loss": 0.0024,
      "step": 85830
    },
    {
      "epoch": 7.630222222222223,
      "grad_norm": 0.5458531975746155,
      "learning_rate": 2.311111111111111e-06,
      "loss": 0.0021,
      "step": 85840
    },
    {
      "epoch": 7.631111111111111,
      "grad_norm": 0.10751707851886749,
      "learning_rate": 2.3055555555555556e-06,
      "loss": 0.0017,
      "step": 85850
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.1727059930562973,
      "learning_rate": 2.3e-06,
      "loss": 0.0023,
      "step": 85860
    },
    {
      "epoch": 7.632888888888889,
      "grad_norm": 0.6040167212486267,
      "learning_rate": 2.2944444444444444e-06,
      "loss": 0.0014,
      "step": 85870
    },
    {
      "epoch": 7.6337777777777776,
      "grad_norm": 0.21545475721359253,
      "learning_rate": 2.2888888888888892e-06,
      "loss": 0.0014,
      "step": 85880
    },
    {
      "epoch": 7.634666666666667,
      "grad_norm": 0.15090176463127136,
      "learning_rate": 2.2833333333333336e-06,
      "loss": 0.0023,
      "step": 85890
    },
    {
      "epoch": 7.635555555555555,
      "grad_norm": 0.12290410697460175,
      "learning_rate": 2.277777777777778e-06,
      "loss": 0.0019,
      "step": 85900
    },
    {
      "epoch": 7.636444444444445,
      "grad_norm": 0.361361563205719,
      "learning_rate": 2.2722222222222224e-06,
      "loss": 0.0017,
      "step": 85910
    },
    {
      "epoch": 7.637333333333333,
      "grad_norm": 0.18427594006061554,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0022,
      "step": 85920
    },
    {
      "epoch": 7.638222222222222,
      "grad_norm": 0.07267972081899643,
      "learning_rate": 2.2611111111111112e-06,
      "loss": 0.0021,
      "step": 85930
    },
    {
      "epoch": 7.639111111111111,
      "grad_norm": 0.5280659794807434,
      "learning_rate": 2.2555555555555557e-06,
      "loss": 0.0015,
      "step": 85940
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.21862652897834778,
      "learning_rate": 2.25e-06,
      "loss": 0.0012,
      "step": 85950
    },
    {
      "epoch": 7.640888888888889,
      "grad_norm": 0.22899192571640015,
      "learning_rate": 2.2444444444444445e-06,
      "loss": 0.0024,
      "step": 85960
    },
    {
      "epoch": 7.641777777777778,
      "grad_norm": 0.18004421889781952,
      "learning_rate": 2.238888888888889e-06,
      "loss": 0.0014,
      "step": 85970
    },
    {
      "epoch": 7.642666666666667,
      "grad_norm": 0.24889543652534485,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0015,
      "step": 85980
    },
    {
      "epoch": 7.643555555555555,
      "grad_norm": 0.31232380867004395,
      "learning_rate": 2.227777777777778e-06,
      "loss": 0.0019,
      "step": 85990
    },
    {
      "epoch": 7.644444444444445,
      "grad_norm": 0.7518879175186157,
      "learning_rate": 2.2222222222222225e-06,
      "loss": 0.0016,
      "step": 86000
    },
    {
      "epoch": 7.645333333333333,
      "grad_norm": 0.2472548633813858,
      "learning_rate": 2.216666666666667e-06,
      "loss": 0.0015,
      "step": 86010
    },
    {
      "epoch": 7.646222222222223,
      "grad_norm": 0.10402978956699371,
      "learning_rate": 2.2111111111111113e-06,
      "loss": 0.0026,
      "step": 86020
    },
    {
      "epoch": 7.647111111111111,
      "grad_norm": 0.5368260145187378,
      "learning_rate": 2.2055555555555557e-06,
      "loss": 0.0014,
      "step": 86030
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.06954622268676758,
      "learning_rate": 2.2e-06,
      "loss": 0.0017,
      "step": 86040
    },
    {
      "epoch": 7.648888888888889,
      "grad_norm": 0.33996257185935974,
      "learning_rate": 2.1944444444444445e-06,
      "loss": 0.0023,
      "step": 86050
    },
    {
      "epoch": 7.649777777777778,
      "grad_norm": 0.18713560700416565,
      "learning_rate": 2.188888888888889e-06,
      "loss": 0.0017,
      "step": 86060
    },
    {
      "epoch": 7.650666666666667,
      "grad_norm": 0.24717602133750916,
      "learning_rate": 2.1833333333333333e-06,
      "loss": 0.0012,
      "step": 86070
    },
    {
      "epoch": 7.651555555555555,
      "grad_norm": 0.38640061020851135,
      "learning_rate": 2.1777777777777777e-06,
      "loss": 0.0015,
      "step": 86080
    },
    {
      "epoch": 7.652444444444445,
      "grad_norm": 0.2819121778011322,
      "learning_rate": 2.1722222222222226e-06,
      "loss": 0.0017,
      "step": 86090
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 0.07035002112388611,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.0014,
      "step": 86100
    },
    {
      "epoch": 7.654222222222222,
      "grad_norm": 0.35850462317466736,
      "learning_rate": 2.1611111111111114e-06,
      "loss": 0.0017,
      "step": 86110
    },
    {
      "epoch": 7.655111111111111,
      "grad_norm": 0.4054333567619324,
      "learning_rate": 2.1555555555555558e-06,
      "loss": 0.0025,
      "step": 86120
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.07441967725753784,
      "learning_rate": 2.1499999999999997e-06,
      "loss": 0.0019,
      "step": 86130
    },
    {
      "epoch": 7.656888888888889,
      "grad_norm": 0.08356914669275284,
      "learning_rate": 2.1444444444444446e-06,
      "loss": 0.0018,
      "step": 86140
    },
    {
      "epoch": 7.657777777777778,
      "grad_norm": 0.17527185380458832,
      "learning_rate": 2.138888888888889e-06,
      "loss": 0.0029,
      "step": 86150
    },
    {
      "epoch": 7.658666666666667,
      "grad_norm": 0.18171337246894836,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0019,
      "step": 86160
    },
    {
      "epoch": 7.6595555555555555,
      "grad_norm": 0.22329233586788177,
      "learning_rate": 2.1277777777777778e-06,
      "loss": 0.0018,
      "step": 86170
    },
    {
      "epoch": 7.660444444444444,
      "grad_norm": 0.09654278308153152,
      "learning_rate": 2.122222222222222e-06,
      "loss": 0.0033,
      "step": 86180
    },
    {
      "epoch": 7.661333333333333,
      "grad_norm": 0.12131176143884659,
      "learning_rate": 2.1166666666666666e-06,
      "loss": 0.0024,
      "step": 86190
    },
    {
      "epoch": 7.662222222222223,
      "grad_norm": 0.11888839304447174,
      "learning_rate": 2.1111111111111114e-06,
      "loss": 0.0018,
      "step": 86200
    },
    {
      "epoch": 7.663111111111111,
      "grad_norm": 0.1942346841096878,
      "learning_rate": 2.105555555555556e-06,
      "loss": 0.0018,
      "step": 86210
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.2474886029958725,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.0029,
      "step": 86220
    },
    {
      "epoch": 7.664888888888889,
      "grad_norm": 0.2831500172615051,
      "learning_rate": 2.0944444444444446e-06,
      "loss": 0.0012,
      "step": 86230
    },
    {
      "epoch": 7.665777777777778,
      "grad_norm": 0.07761970907449722,
      "learning_rate": 2.088888888888889e-06,
      "loss": 0.0013,
      "step": 86240
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.28200647234916687,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.0023,
      "step": 86250
    },
    {
      "epoch": 7.6675555555555555,
      "grad_norm": 0.16663163900375366,
      "learning_rate": 2.077777777777778e-06,
      "loss": 0.0023,
      "step": 86260
    },
    {
      "epoch": 7.668444444444445,
      "grad_norm": 0.3169388771057129,
      "learning_rate": 2.0722222222222222e-06,
      "loss": 0.0023,
      "step": 86270
    },
    {
      "epoch": 7.669333333333333,
      "grad_norm": 0.20929287374019623,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0013,
      "step": 86280
    },
    {
      "epoch": 7.670222222222222,
      "grad_norm": 0.05607035383582115,
      "learning_rate": 2.061111111111111e-06,
      "loss": 0.0018,
      "step": 86290
    },
    {
      "epoch": 7.671111111111111,
      "grad_norm": 0.06505142152309418,
      "learning_rate": 2.055555555555556e-06,
      "loss": 0.0013,
      "step": 86300
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.1907871961593628,
      "learning_rate": 2.0500000000000003e-06,
      "loss": 0.0016,
      "step": 86310
    },
    {
      "epoch": 7.672888888888889,
      "grad_norm": 0.49185964465141296,
      "learning_rate": 2.0444444444444447e-06,
      "loss": 0.0012,
      "step": 86320
    },
    {
      "epoch": 7.673777777777778,
      "grad_norm": 0.25206613540649414,
      "learning_rate": 2.038888888888889e-06,
      "loss": 0.0016,
      "step": 86330
    },
    {
      "epoch": 7.674666666666667,
      "grad_norm": 0.06851133704185486,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.0017,
      "step": 86340
    },
    {
      "epoch": 7.6755555555555555,
      "grad_norm": 0.0809742659330368,
      "learning_rate": 2.027777777777778e-06,
      "loss": 0.0029,
      "step": 86350
    },
    {
      "epoch": 7.676444444444444,
      "grad_norm": 0.28566670417785645,
      "learning_rate": 2.0222222222222223e-06,
      "loss": 0.0016,
      "step": 86360
    },
    {
      "epoch": 7.677333333333333,
      "grad_norm": 0.5424061417579651,
      "learning_rate": 2.0166666666666667e-06,
      "loss": 0.0019,
      "step": 86370
    },
    {
      "epoch": 7.678222222222222,
      "grad_norm": 0.0672256350517273,
      "learning_rate": 2.011111111111111e-06,
      "loss": 0.0021,
      "step": 86380
    },
    {
      "epoch": 7.679111111111111,
      "grad_norm": 0.17831110954284668,
      "learning_rate": 2.0055555555555555e-06,
      "loss": 0.0019,
      "step": 86390
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.5796358585357666,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0014,
      "step": 86400
    },
    {
      "epoch": 7.680888888888889,
      "grad_norm": 0.5652831196784973,
      "learning_rate": 1.9944444444444447e-06,
      "loss": 0.0021,
      "step": 86410
    },
    {
      "epoch": 7.681777777777778,
      "grad_norm": 0.06333018094301224,
      "learning_rate": 1.988888888888889e-06,
      "loss": 0.0018,
      "step": 86420
    },
    {
      "epoch": 7.682666666666667,
      "grad_norm": 0.3075313866138458,
      "learning_rate": 1.9833333333333335e-06,
      "loss": 0.002,
      "step": 86430
    },
    {
      "epoch": 7.6835555555555555,
      "grad_norm": 0.25436729192733765,
      "learning_rate": 1.9777777777777775e-06,
      "loss": 0.0034,
      "step": 86440
    },
    {
      "epoch": 7.684444444444445,
      "grad_norm": 0.35035768151283264,
      "learning_rate": 1.9722222222222224e-06,
      "loss": 0.0019,
      "step": 86450
    },
    {
      "epoch": 7.685333333333333,
      "grad_norm": 0.2519551217556,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.0017,
      "step": 86460
    },
    {
      "epoch": 7.686222222222222,
      "grad_norm": 0.5296295285224915,
      "learning_rate": 1.961111111111111e-06,
      "loss": 0.0011,
      "step": 86470
    },
    {
      "epoch": 7.687111111111111,
      "grad_norm": 0.5091774463653564,
      "learning_rate": 1.9555555555555556e-06,
      "loss": 0.0017,
      "step": 86480
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.3418968617916107,
      "learning_rate": 1.95e-06,
      "loss": 0.0014,
      "step": 86490
    },
    {
      "epoch": 7.688888888888889,
      "grad_norm": 0.26364725828170776,
      "learning_rate": 1.9444444444444444e-06,
      "loss": 0.0016,
      "step": 86500
    },
    {
      "epoch": 7.689777777777778,
      "grad_norm": 0.5601029992103577,
      "learning_rate": 1.938888888888889e-06,
      "loss": 0.0012,
      "step": 86510
    },
    {
      "epoch": 7.690666666666667,
      "grad_norm": 0.6702536344528198,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0012,
      "step": 86520
    },
    {
      "epoch": 7.6915555555555555,
      "grad_norm": 0.10727639496326447,
      "learning_rate": 1.927777777777778e-06,
      "loss": 0.0016,
      "step": 86530
    },
    {
      "epoch": 7.692444444444444,
      "grad_norm": 0.07582987844944,
      "learning_rate": 1.9222222222222224e-06,
      "loss": 0.0023,
      "step": 86540
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.2494225651025772,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.0021,
      "step": 86550
    },
    {
      "epoch": 7.694222222222222,
      "grad_norm": 0.30283698439598083,
      "learning_rate": 1.9111111111111112e-06,
      "loss": 0.0017,
      "step": 86560
    },
    {
      "epoch": 7.695111111111111,
      "grad_norm": 0.5299203991889954,
      "learning_rate": 1.9055555555555556e-06,
      "loss": 0.0023,
      "step": 86570
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.1440242975950241,
      "learning_rate": 1.9e-06,
      "loss": 0.0012,
      "step": 86580
    },
    {
      "epoch": 7.696888888888889,
      "grad_norm": 0.14335013926029205,
      "learning_rate": 1.8944444444444444e-06,
      "loss": 0.0022,
      "step": 86590
    },
    {
      "epoch": 7.697777777777778,
      "grad_norm": 0.2524927258491516,
      "learning_rate": 1.888888888888889e-06,
      "loss": 0.0019,
      "step": 86600
    },
    {
      "epoch": 7.698666666666667,
      "grad_norm": 0.4488143026828766,
      "learning_rate": 1.8833333333333334e-06,
      "loss": 0.0019,
      "step": 86610
    },
    {
      "epoch": 7.6995555555555555,
      "grad_norm": 0.14383761584758759,
      "learning_rate": 1.877777777777778e-06,
      "loss": 0.0013,
      "step": 86620
    },
    {
      "epoch": 7.700444444444445,
      "grad_norm": 0.2462727278470993,
      "learning_rate": 1.8722222222222225e-06,
      "loss": 0.0016,
      "step": 86630
    },
    {
      "epoch": 7.701333333333333,
      "grad_norm": 0.4345400035381317,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0017,
      "step": 86640
    },
    {
      "epoch": 7.702222222222222,
      "grad_norm": 0.31711289286613464,
      "learning_rate": 1.861111111111111e-06,
      "loss": 0.0032,
      "step": 86650
    },
    {
      "epoch": 7.703111111111111,
      "grad_norm": 0.06684501469135284,
      "learning_rate": 1.8555555555555555e-06,
      "loss": 0.0019,
      "step": 86660
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.05736332759261131,
      "learning_rate": 1.85e-06,
      "loss": 0.0024,
      "step": 86670
    },
    {
      "epoch": 7.704888888888889,
      "grad_norm": 0.04652459919452667,
      "learning_rate": 1.8444444444444445e-06,
      "loss": 0.0013,
      "step": 86680
    },
    {
      "epoch": 7.705777777777778,
      "grad_norm": 0.30792102217674255,
      "learning_rate": 1.8388888888888889e-06,
      "loss": 0.0016,
      "step": 86690
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.35427767038345337,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0016,
      "step": 86700
    },
    {
      "epoch": 7.7075555555555555,
      "grad_norm": 0.07563769817352295,
      "learning_rate": 1.827777777777778e-06,
      "loss": 0.0015,
      "step": 86710
    },
    {
      "epoch": 7.708444444444444,
      "grad_norm": 0.11368498206138611,
      "learning_rate": 1.8222222222222223e-06,
      "loss": 0.002,
      "step": 86720
    },
    {
      "epoch": 7.709333333333333,
      "grad_norm": 0.18308191001415253,
      "learning_rate": 1.816666666666667e-06,
      "loss": 0.0019,
      "step": 86730
    },
    {
      "epoch": 7.710222222222222,
      "grad_norm": 0.056478407233953476,
      "learning_rate": 1.8111111111111113e-06,
      "loss": 0.0011,
      "step": 86740
    },
    {
      "epoch": 7.711111111111111,
      "grad_norm": 0.09923319518566132,
      "learning_rate": 1.8055555555555555e-06,
      "loss": 0.0022,
      "step": 86750
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.14955967664718628,
      "learning_rate": 1.8e-06,
      "loss": 0.0014,
      "step": 86760
    },
    {
      "epoch": 7.712888888888889,
      "grad_norm": 0.4285530745983124,
      "learning_rate": 1.7944444444444445e-06,
      "loss": 0.0018,
      "step": 86770
    },
    {
      "epoch": 7.713777777777778,
      "grad_norm": 0.4929525852203369,
      "learning_rate": 1.788888888888889e-06,
      "loss": 0.0014,
      "step": 86780
    },
    {
      "epoch": 7.714666666666667,
      "grad_norm": 0.39164575934410095,
      "learning_rate": 1.7833333333333333e-06,
      "loss": 0.0018,
      "step": 86790
    },
    {
      "epoch": 7.7155555555555555,
      "grad_norm": 0.0484551265835762,
      "learning_rate": 1.777777777777778e-06,
      "loss": 0.0022,
      "step": 86800
    },
    {
      "epoch": 7.716444444444445,
      "grad_norm": 0.15979531407356262,
      "learning_rate": 1.7722222222222224e-06,
      "loss": 0.0014,
      "step": 86810
    },
    {
      "epoch": 7.717333333333333,
      "grad_norm": 0.14124412834644318,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0013,
      "step": 86820
    },
    {
      "epoch": 7.718222222222222,
      "grad_norm": 0.07209385931491852,
      "learning_rate": 1.7611111111111114e-06,
      "loss": 0.0019,
      "step": 86830
    },
    {
      "epoch": 7.719111111111111,
      "grad_norm": 0.25590574741363525,
      "learning_rate": 1.7555555555555558e-06,
      "loss": 0.002,
      "step": 86840
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.28493982553482056,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.0017,
      "step": 86850
    },
    {
      "epoch": 7.720888888888889,
      "grad_norm": 0.3174597918987274,
      "learning_rate": 1.7444444444444444e-06,
      "loss": 0.0021,
      "step": 86860
    },
    {
      "epoch": 7.721777777777778,
      "grad_norm": 0.15128371119499207,
      "learning_rate": 1.7388888888888888e-06,
      "loss": 0.0014,
      "step": 86870
    },
    {
      "epoch": 7.722666666666667,
      "grad_norm": 0.28040653467178345,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0024,
      "step": 86880
    },
    {
      "epoch": 7.7235555555555555,
      "grad_norm": 0.3213734030723572,
      "learning_rate": 1.7277777777777778e-06,
      "loss": 0.0016,
      "step": 86890
    },
    {
      "epoch": 7.724444444444444,
      "grad_norm": 0.4071885943412781,
      "learning_rate": 1.7222222222222222e-06,
      "loss": 0.002,
      "step": 86900
    },
    {
      "epoch": 7.725333333333333,
      "grad_norm": 0.05531500279903412,
      "learning_rate": 1.7166666666666668e-06,
      "loss": 0.0014,
      "step": 86910
    },
    {
      "epoch": 7.726222222222222,
      "grad_norm": 0.1184707060456276,
      "learning_rate": 1.7111111111111112e-06,
      "loss": 0.0023,
      "step": 86920
    },
    {
      "epoch": 7.727111111111111,
      "grad_norm": 0.3011947274208069,
      "learning_rate": 1.7055555555555556e-06,
      "loss": 0.0015,
      "step": 86930
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.08869040757417679,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.0013,
      "step": 86940
    },
    {
      "epoch": 7.728888888888889,
      "grad_norm": 0.2135256975889206,
      "learning_rate": 1.6944444444444446e-06,
      "loss": 0.0019,
      "step": 86950
    },
    {
      "epoch": 7.729777777777778,
      "grad_norm": 0.11349812150001526,
      "learning_rate": 1.6888888888888888e-06,
      "loss": 0.002,
      "step": 86960
    },
    {
      "epoch": 7.730666666666667,
      "grad_norm": 0.1787714660167694,
      "learning_rate": 1.6833333333333332e-06,
      "loss": 0.0031,
      "step": 86970
    },
    {
      "epoch": 7.7315555555555555,
      "grad_norm": 0.18433520197868347,
      "learning_rate": 1.6777777777777779e-06,
      "loss": 0.0014,
      "step": 86980
    },
    {
      "epoch": 7.732444444444445,
      "grad_norm": 0.2478536069393158,
      "learning_rate": 1.6722222222222223e-06,
      "loss": 0.0017,
      "step": 86990
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.0552966482937336,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0019,
      "step": 87000
    },
    {
      "epoch": 7.734222222222222,
      "grad_norm": 0.09639901667833328,
      "learning_rate": 1.6611111111111113e-06,
      "loss": 0.002,
      "step": 87010
    },
    {
      "epoch": 7.735111111111111,
      "grad_norm": 0.33679813146591187,
      "learning_rate": 1.6555555555555557e-06,
      "loss": 0.0022,
      "step": 87020
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.07296235859394073,
      "learning_rate": 1.65e-06,
      "loss": 0.0017,
      "step": 87030
    },
    {
      "epoch": 7.736888888888889,
      "grad_norm": 0.11739036440849304,
      "learning_rate": 1.6444444444444447e-06,
      "loss": 0.0027,
      "step": 87040
    },
    {
      "epoch": 7.737777777777778,
      "grad_norm": 0.04181014746427536,
      "learning_rate": 1.638888888888889e-06,
      "loss": 0.0014,
      "step": 87050
    },
    {
      "epoch": 7.738666666666667,
      "grad_norm": 0.39141103625297546,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0014,
      "step": 87060
    },
    {
      "epoch": 7.7395555555555555,
      "grad_norm": 0.11468072235584259,
      "learning_rate": 1.6277777777777777e-06,
      "loss": 0.0014,
      "step": 87070
    },
    {
      "epoch": 7.740444444444444,
      "grad_norm": 0.03935963287949562,
      "learning_rate": 1.622222222222222e-06,
      "loss": 0.0018,
      "step": 87080
    },
    {
      "epoch": 7.741333333333333,
      "grad_norm": 0.13168926537036896,
      "learning_rate": 1.6166666666666667e-06,
      "loss": 0.0017,
      "step": 87090
    },
    {
      "epoch": 7.742222222222222,
      "grad_norm": 0.21493148803710938,
      "learning_rate": 1.6111111111111111e-06,
      "loss": 0.0017,
      "step": 87100
    },
    {
      "epoch": 7.743111111111111,
      "grad_norm": 0.467507004737854,
      "learning_rate": 1.6055555555555557e-06,
      "loss": 0.0022,
      "step": 87110
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.11817590892314911,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0014,
      "step": 87120
    },
    {
      "epoch": 7.744888888888889,
      "grad_norm": 0.11479394137859344,
      "learning_rate": 1.5944444444444445e-06,
      "loss": 0.0016,
      "step": 87130
    },
    {
      "epoch": 7.745777777777778,
      "grad_norm": 0.3117755651473999,
      "learning_rate": 1.5888888888888892e-06,
      "loss": 0.002,
      "step": 87140
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.43623676896095276,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.0015,
      "step": 87150
    },
    {
      "epoch": 7.7475555555555555,
      "grad_norm": 0.2115398496389389,
      "learning_rate": 1.577777777777778e-06,
      "loss": 0.0015,
      "step": 87160
    },
    {
      "epoch": 7.748444444444445,
      "grad_norm": 0.0736270546913147,
      "learning_rate": 1.5722222222222222e-06,
      "loss": 0.0015,
      "step": 87170
    },
    {
      "epoch": 7.749333333333333,
      "grad_norm": 0.2940845489501953,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0016,
      "step": 87180
    },
    {
      "epoch": 7.750222222222222,
      "grad_norm": 0.603417694568634,
      "learning_rate": 1.5611111111111112e-06,
      "loss": 0.0017,
      "step": 87190
    },
    {
      "epoch": 7.751111111111111,
      "grad_norm": 0.1769193857908249,
      "learning_rate": 1.5555555555555556e-06,
      "loss": 0.0015,
      "step": 87200
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.3917078375816345,
      "learning_rate": 1.55e-06,
      "loss": 0.0021,
      "step": 87210
    },
    {
      "epoch": 7.752888888888889,
      "grad_norm": 0.05478864535689354,
      "learning_rate": 1.5444444444444446e-06,
      "loss": 0.0011,
      "step": 87220
    },
    {
      "epoch": 7.753777777777778,
      "grad_norm": 0.1435903012752533,
      "learning_rate": 1.538888888888889e-06,
      "loss": 0.0016,
      "step": 87230
    },
    {
      "epoch": 7.754666666666667,
      "grad_norm": 0.21577464044094086,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0017,
      "step": 87240
    },
    {
      "epoch": 7.7555555555555555,
      "grad_norm": 0.08836662024259567,
      "learning_rate": 1.5277777777777778e-06,
      "loss": 0.0019,
      "step": 87250
    },
    {
      "epoch": 7.756444444444444,
      "grad_norm": 0.23138122260570526,
      "learning_rate": 1.5222222222222222e-06,
      "loss": 0.0015,
      "step": 87260
    },
    {
      "epoch": 7.757333333333333,
      "grad_norm": 0.07080920040607452,
      "learning_rate": 1.5166666666666668e-06,
      "loss": 0.0018,
      "step": 87270
    },
    {
      "epoch": 7.758222222222222,
      "grad_norm": 0.47092539072036743,
      "learning_rate": 1.5111111111111112e-06,
      "loss": 0.0017,
      "step": 87280
    },
    {
      "epoch": 7.759111111111111,
      "grad_norm": 0.5619087815284729,
      "learning_rate": 1.5055555555555556e-06,
      "loss": 0.0032,
      "step": 87290
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.22539854049682617,
      "learning_rate": 1.5e-06,
      "loss": 0.0024,
      "step": 87300
    },
    {
      "epoch": 7.760888888888889,
      "grad_norm": 0.22221976518630981,
      "learning_rate": 1.4944444444444444e-06,
      "loss": 0.0021,
      "step": 87310
    },
    {
      "epoch": 7.761777777777778,
      "grad_norm": 0.18713665008544922,
      "learning_rate": 1.488888888888889e-06,
      "loss": 0.0016,
      "step": 87320
    },
    {
      "epoch": 7.762666666666667,
      "grad_norm": 0.5637705326080322,
      "learning_rate": 1.4833333333333335e-06,
      "loss": 0.0015,
      "step": 87330
    },
    {
      "epoch": 7.7635555555555555,
      "grad_norm": 0.2881152331829071,
      "learning_rate": 1.4777777777777779e-06,
      "loss": 0.0015,
      "step": 87340
    },
    {
      "epoch": 7.764444444444445,
      "grad_norm": 0.2829524278640747,
      "learning_rate": 1.4722222222222223e-06,
      "loss": 0.0011,
      "step": 87350
    },
    {
      "epoch": 7.765333333333333,
      "grad_norm": 0.0774611160159111,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0021,
      "step": 87360
    },
    {
      "epoch": 7.766222222222222,
      "grad_norm": 0.44761303067207336,
      "learning_rate": 1.4611111111111113e-06,
      "loss": 0.0018,
      "step": 87370
    },
    {
      "epoch": 7.767111111111111,
      "grad_norm": 0.4230153262615204,
      "learning_rate": 1.4555555555555557e-06,
      "loss": 0.002,
      "step": 87380
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.3936370015144348,
      "learning_rate": 1.45e-06,
      "loss": 0.0014,
      "step": 87390
    },
    {
      "epoch": 7.768888888888889,
      "grad_norm": 0.3156391382217407,
      "learning_rate": 1.4444444444444445e-06,
      "loss": 0.002,
      "step": 87400
    },
    {
      "epoch": 7.769777777777778,
      "grad_norm": 0.08176729828119278,
      "learning_rate": 1.438888888888889e-06,
      "loss": 0.0014,
      "step": 87410
    },
    {
      "epoch": 7.770666666666667,
      "grad_norm": 0.09902311116456985,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.0015,
      "step": 87420
    },
    {
      "epoch": 7.7715555555555556,
      "grad_norm": 0.2883929908275604,
      "learning_rate": 1.427777777777778e-06,
      "loss": 0.0013,
      "step": 87430
    },
    {
      "epoch": 7.772444444444444,
      "grad_norm": 0.5096123814582825,
      "learning_rate": 1.4222222222222223e-06,
      "loss": 0.0021,
      "step": 87440
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 0.11125964671373367,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.0029,
      "step": 87450
    },
    {
      "epoch": 7.774222222222222,
      "grad_norm": 0.155068501830101,
      "learning_rate": 1.4111111111111111e-06,
      "loss": 0.0016,
      "step": 87460
    },
    {
      "epoch": 7.775111111111111,
      "grad_norm": 0.12347687780857086,
      "learning_rate": 1.4055555555555555e-06,
      "loss": 0.0017,
      "step": 87470
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.08402141183614731,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0022,
      "step": 87480
    },
    {
      "epoch": 7.776888888888889,
      "grad_norm": 0.14942023158073425,
      "learning_rate": 1.3944444444444446e-06,
      "loss": 0.0013,
      "step": 87490
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 0.18046368658542633,
      "learning_rate": 1.388888888888889e-06,
      "loss": 0.0032,
      "step": 87500
    },
    {
      "epoch": 7.778666666666666,
      "grad_norm": 0.3941955864429474,
      "learning_rate": 1.3833333333333334e-06,
      "loss": 0.0018,
      "step": 87510
    },
    {
      "epoch": 7.779555555555556,
      "grad_norm": 0.07359069585800171,
      "learning_rate": 1.3777777777777778e-06,
      "loss": 0.0019,
      "step": 87520
    },
    {
      "epoch": 7.780444444444445,
      "grad_norm": 0.03412691876292229,
      "learning_rate": 1.3722222222222224e-06,
      "loss": 0.0021,
      "step": 87530
    },
    {
      "epoch": 7.781333333333333,
      "grad_norm": 0.18010948598384857,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0013,
      "step": 87540
    },
    {
      "epoch": 7.782222222222222,
      "grad_norm": 0.32143986225128174,
      "learning_rate": 1.3611111111111112e-06,
      "loss": 0.0016,
      "step": 87550
    },
    {
      "epoch": 7.783111111111111,
      "grad_norm": 0.42119869589805603,
      "learning_rate": 1.3555555555555556e-06,
      "loss": 0.0027,
      "step": 87560
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.2841137647628784,
      "learning_rate": 1.35e-06,
      "loss": 0.0016,
      "step": 87570
    },
    {
      "epoch": 7.784888888888889,
      "grad_norm": 0.4612257778644562,
      "learning_rate": 1.3444444444444446e-06,
      "loss": 0.0013,
      "step": 87580
    },
    {
      "epoch": 7.785777777777778,
      "grad_norm": 0.15034961700439453,
      "learning_rate": 1.338888888888889e-06,
      "loss": 0.0025,
      "step": 87590
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.2986762523651123,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0013,
      "step": 87600
    },
    {
      "epoch": 7.787555555555556,
      "grad_norm": 0.12913204729557037,
      "learning_rate": 1.3277777777777778e-06,
      "loss": 0.0015,
      "step": 87610
    },
    {
      "epoch": 7.788444444444444,
      "grad_norm": 0.1099836528301239,
      "learning_rate": 1.3222222222222222e-06,
      "loss": 0.0026,
      "step": 87620
    },
    {
      "epoch": 7.789333333333333,
      "grad_norm": 0.044581688940525055,
      "learning_rate": 1.3166666666666668e-06,
      "loss": 0.0013,
      "step": 87630
    },
    {
      "epoch": 7.790222222222222,
      "grad_norm": 0.05276210978627205,
      "learning_rate": 1.3111111111111112e-06,
      "loss": 0.0012,
      "step": 87640
    },
    {
      "epoch": 7.791111111111111,
      "grad_norm": 0.4604416787624359,
      "learning_rate": 1.3055555555555556e-06,
      "loss": 0.0017,
      "step": 87650
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.052332039922475815,
      "learning_rate": 1.3e-06,
      "loss": 0.0016,
      "step": 87660
    },
    {
      "epoch": 7.792888888888889,
      "grad_norm": 0.058594245463609695,
      "learning_rate": 1.2944444444444445e-06,
      "loss": 0.002,
      "step": 87670
    },
    {
      "epoch": 7.793777777777778,
      "grad_norm": 0.14773589372634888,
      "learning_rate": 1.2888888888888889e-06,
      "loss": 0.0023,
      "step": 87680
    },
    {
      "epoch": 7.794666666666666,
      "grad_norm": 0.07809203118085861,
      "learning_rate": 1.2833333333333335e-06,
      "loss": 0.0021,
      "step": 87690
    },
    {
      "epoch": 7.795555555555556,
      "grad_norm": 0.603018581867218,
      "learning_rate": 1.2777777777777779e-06,
      "loss": 0.0015,
      "step": 87700
    },
    {
      "epoch": 7.796444444444444,
      "grad_norm": 0.17896388471126556,
      "learning_rate": 1.2722222222222223e-06,
      "loss": 0.0024,
      "step": 87710
    },
    {
      "epoch": 7.7973333333333334,
      "grad_norm": 0.22569213807582855,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0016,
      "step": 87720
    },
    {
      "epoch": 7.798222222222222,
      "grad_norm": 0.07307148724794388,
      "learning_rate": 1.261111111111111e-06,
      "loss": 0.0022,
      "step": 87730
    },
    {
      "epoch": 7.799111111111111,
      "grad_norm": 0.11898454278707504,
      "learning_rate": 1.2555555555555557e-06,
      "loss": 0.0013,
      "step": 87740
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.25286081433296204,
      "learning_rate": 1.25e-06,
      "loss": 0.0015,
      "step": 87750
    },
    {
      "epoch": 7.800888888888889,
      "grad_norm": 0.15901057422161102,
      "learning_rate": 1.2444444444444445e-06,
      "loss": 0.0016,
      "step": 87760
    },
    {
      "epoch": 7.801777777777778,
      "grad_norm": 0.0554506741464138,
      "learning_rate": 1.238888888888889e-06,
      "loss": 0.0018,
      "step": 87770
    },
    {
      "epoch": 7.802666666666667,
      "grad_norm": 0.36733880639076233,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0037,
      "step": 87780
    },
    {
      "epoch": 7.803555555555556,
      "grad_norm": 0.2159499078989029,
      "learning_rate": 1.227777777777778e-06,
      "loss": 0.0015,
      "step": 87790
    },
    {
      "epoch": 7.804444444444444,
      "grad_norm": 0.45464158058166504,
      "learning_rate": 1.2222222222222223e-06,
      "loss": 0.0019,
      "step": 87800
    },
    {
      "epoch": 7.8053333333333335,
      "grad_norm": 0.09077122807502747,
      "learning_rate": 1.2166666666666667e-06,
      "loss": 0.0012,
      "step": 87810
    },
    {
      "epoch": 7.806222222222222,
      "grad_norm": 0.6610590815544128,
      "learning_rate": 1.2111111111111111e-06,
      "loss": 0.0022,
      "step": 87820
    },
    {
      "epoch": 7.807111111111111,
      "grad_norm": 0.14202000200748444,
      "learning_rate": 1.2055555555555555e-06,
      "loss": 0.0027,
      "step": 87830
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.2544977068901062,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0019,
      "step": 87840
    },
    {
      "epoch": 7.808888888888889,
      "grad_norm": 0.3239971995353699,
      "learning_rate": 1.1944444444444446e-06,
      "loss": 0.0023,
      "step": 87850
    },
    {
      "epoch": 7.809777777777778,
      "grad_norm": 0.14596939086914062,
      "learning_rate": 1.188888888888889e-06,
      "loss": 0.0015,
      "step": 87860
    },
    {
      "epoch": 7.810666666666666,
      "grad_norm": 0.5271111726760864,
      "learning_rate": 1.1833333333333334e-06,
      "loss": 0.0016,
      "step": 87870
    },
    {
      "epoch": 7.811555555555556,
      "grad_norm": 0.15978123247623444,
      "learning_rate": 1.1777777777777778e-06,
      "loss": 0.0016,
      "step": 87880
    },
    {
      "epoch": 7.812444444444444,
      "grad_norm": 0.36840590834617615,
      "learning_rate": 1.1722222222222224e-06,
      "loss": 0.0017,
      "step": 87890
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.05245909467339516,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0014,
      "step": 87900
    },
    {
      "epoch": 7.814222222222222,
      "grad_norm": 0.12305619567632675,
      "learning_rate": 1.161111111111111e-06,
      "loss": 0.0024,
      "step": 87910
    },
    {
      "epoch": 7.815111111111111,
      "grad_norm": 0.3069061040878296,
      "learning_rate": 1.1555555555555556e-06,
      "loss": 0.0021,
      "step": 87920
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.3876512050628662,
      "learning_rate": 1.15e-06,
      "loss": 0.0015,
      "step": 87930
    },
    {
      "epoch": 7.816888888888889,
      "grad_norm": 0.046767327934503555,
      "learning_rate": 1.1444444444444446e-06,
      "loss": 0.0026,
      "step": 87940
    },
    {
      "epoch": 7.817777777777778,
      "grad_norm": 0.09317401051521301,
      "learning_rate": 1.138888888888889e-06,
      "loss": 0.0015,
      "step": 87950
    },
    {
      "epoch": 7.818666666666667,
      "grad_norm": 0.10051233321428299,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.002,
      "step": 87960
    },
    {
      "epoch": 7.819555555555556,
      "grad_norm": 0.07884304225444794,
      "learning_rate": 1.1277777777777778e-06,
      "loss": 0.0025,
      "step": 87970
    },
    {
      "epoch": 7.820444444444444,
      "grad_norm": 0.08560922741889954,
      "learning_rate": 1.1222222222222222e-06,
      "loss": 0.0027,
      "step": 87980
    },
    {
      "epoch": 7.8213333333333335,
      "grad_norm": 0.3195143938064575,
      "learning_rate": 1.1166666666666666e-06,
      "loss": 0.0019,
      "step": 87990
    },
    {
      "epoch": 7.822222222222222,
      "grad_norm": 0.424832820892334,
      "learning_rate": 1.1111111111111112e-06,
      "loss": 0.002,
      "step": 88000
    },
    {
      "epoch": 7.823111111111111,
      "grad_norm": 0.12437959760427475,
      "learning_rate": 1.1055555555555557e-06,
      "loss": 0.002,
      "step": 88010
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.2133582979440689,
      "learning_rate": 1.1e-06,
      "loss": 0.0015,
      "step": 88020
    },
    {
      "epoch": 7.824888888888889,
      "grad_norm": 0.32455992698669434,
      "learning_rate": 1.0944444444444445e-06,
      "loss": 0.0013,
      "step": 88030
    },
    {
      "epoch": 7.825777777777778,
      "grad_norm": 0.2888827323913574,
      "learning_rate": 1.0888888888888889e-06,
      "loss": 0.0025,
      "step": 88040
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 0.2514911890029907,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.0023,
      "step": 88050
    },
    {
      "epoch": 7.827555555555556,
      "grad_norm": 0.2515745460987091,
      "learning_rate": 1.0777777777777779e-06,
      "loss": 0.0012,
      "step": 88060
    },
    {
      "epoch": 7.828444444444444,
      "grad_norm": 0.2533297538757324,
      "learning_rate": 1.0722222222222223e-06,
      "loss": 0.0013,
      "step": 88070
    },
    {
      "epoch": 7.8293333333333335,
      "grad_norm": 0.18754220008850098,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0025,
      "step": 88080
    },
    {
      "epoch": 7.830222222222222,
      "grad_norm": 0.17132137715816498,
      "learning_rate": 1.061111111111111e-06,
      "loss": 0.0021,
      "step": 88090
    },
    {
      "epoch": 7.831111111111111,
      "grad_norm": 0.2510664761066437,
      "learning_rate": 1.0555555555555557e-06,
      "loss": 0.0016,
      "step": 88100
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.21699686348438263,
      "learning_rate": 1.0500000000000001e-06,
      "loss": 0.0012,
      "step": 88110
    },
    {
      "epoch": 7.832888888888889,
      "grad_norm": 0.25886690616607666,
      "learning_rate": 1.0444444444444445e-06,
      "loss": 0.0019,
      "step": 88120
    },
    {
      "epoch": 7.833777777777778,
      "grad_norm": 0.8006283044815063,
      "learning_rate": 1.038888888888889e-06,
      "loss": 0.0022,
      "step": 88130
    },
    {
      "epoch": 7.834666666666667,
      "grad_norm": 0.1143157109618187,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.0019,
      "step": 88140
    },
    {
      "epoch": 7.835555555555556,
      "grad_norm": 0.09370914846658707,
      "learning_rate": 1.027777777777778e-06,
      "loss": 0.0013,
      "step": 88150
    },
    {
      "epoch": 7.836444444444444,
      "grad_norm": 0.3256039023399353,
      "learning_rate": 1.0222222222222223e-06,
      "loss": 0.0022,
      "step": 88160
    },
    {
      "epoch": 7.8373333333333335,
      "grad_norm": 0.23822250962257385,
      "learning_rate": 1.0166666666666665e-06,
      "loss": 0.0029,
      "step": 88170
    },
    {
      "epoch": 7.838222222222222,
      "grad_norm": 0.09806856513023376,
      "learning_rate": 1.0111111111111111e-06,
      "loss": 0.0021,
      "step": 88180
    },
    {
      "epoch": 7.839111111111111,
      "grad_norm": 0.2169860303401947,
      "learning_rate": 1.0055555555555556e-06,
      "loss": 0.002,
      "step": 88190
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.18197257816791534,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0014,
      "step": 88200
    },
    {
      "epoch": 7.840888888888889,
      "grad_norm": 0.29605022072792053,
      "learning_rate": 9.944444444444446e-07,
      "loss": 0.0012,
      "step": 88210
    },
    {
      "epoch": 7.841777777777778,
      "grad_norm": 0.23898884654045105,
      "learning_rate": 9.888888888888888e-07,
      "loss": 0.0015,
      "step": 88220
    },
    {
      "epoch": 7.842666666666666,
      "grad_norm": 0.2191728800535202,
      "learning_rate": 9.833333333333334e-07,
      "loss": 0.0019,
      "step": 88230
    },
    {
      "epoch": 7.843555555555556,
      "grad_norm": 0.3943396806716919,
      "learning_rate": 9.777777777777778e-07,
      "loss": 0.0021,
      "step": 88240
    },
    {
      "epoch": 7.844444444444444,
      "grad_norm": 0.09432011097669601,
      "learning_rate": 9.722222222222222e-07,
      "loss": 0.0019,
      "step": 88250
    },
    {
      "epoch": 7.8453333333333335,
      "grad_norm": 0.16852563619613647,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.002,
      "step": 88260
    },
    {
      "epoch": 7.846222222222222,
      "grad_norm": 0.050207965075969696,
      "learning_rate": 9.611111111111112e-07,
      "loss": 0.0028,
      "step": 88270
    },
    {
      "epoch": 7.847111111111111,
      "grad_norm": 0.15439820289611816,
      "learning_rate": 9.555555555555556e-07,
      "loss": 0.0016,
      "step": 88280
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.10207071155309677,
      "learning_rate": 9.5e-07,
      "loss": 0.0024,
      "step": 88290
    },
    {
      "epoch": 7.848888888888889,
      "grad_norm": 0.4284478425979614,
      "learning_rate": 9.444444444444445e-07,
      "loss": 0.0019,
      "step": 88300
    },
    {
      "epoch": 7.849777777777778,
      "grad_norm": 0.46151965856552124,
      "learning_rate": 9.38888888888889e-07,
      "loss": 0.002,
      "step": 88310
    },
    {
      "epoch": 7.850666666666667,
      "grad_norm": 0.33259671926498413,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0014,
      "step": 88320
    },
    {
      "epoch": 7.851555555555556,
      "grad_norm": 0.39637184143066406,
      "learning_rate": 9.277777777777777e-07,
      "loss": 0.0014,
      "step": 88330
    },
    {
      "epoch": 7.852444444444444,
      "grad_norm": 0.2830088138580322,
      "learning_rate": 9.222222222222222e-07,
      "loss": 0.0018,
      "step": 88340
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.1214623749256134,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.0026,
      "step": 88350
    },
    {
      "epoch": 7.854222222222222,
      "grad_norm": 0.13501346111297607,
      "learning_rate": 9.111111111111112e-07,
      "loss": 0.0023,
      "step": 88360
    },
    {
      "epoch": 7.855111111111111,
      "grad_norm": 0.22045966982841492,
      "learning_rate": 9.055555555555557e-07,
      "loss": 0.0015,
      "step": 88370
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.07512111216783524,
      "learning_rate": 9e-07,
      "loss": 0.0013,
      "step": 88380
    },
    {
      "epoch": 7.856888888888889,
      "grad_norm": 0.24666538834571838,
      "learning_rate": 8.944444444444445e-07,
      "loss": 0.0013,
      "step": 88390
    },
    {
      "epoch": 7.857777777777778,
      "grad_norm": 0.27249789237976074,
      "learning_rate": 8.88888888888889e-07,
      "loss": 0.0015,
      "step": 88400
    },
    {
      "epoch": 7.858666666666666,
      "grad_norm": 0.12024097144603729,
      "learning_rate": 8.833333333333334e-07,
      "loss": 0.0021,
      "step": 88410
    },
    {
      "epoch": 7.859555555555556,
      "grad_norm": 0.12175706028938293,
      "learning_rate": 8.777777777777779e-07,
      "loss": 0.0019,
      "step": 88420
    },
    {
      "epoch": 7.860444444444444,
      "grad_norm": 0.11002770066261292,
      "learning_rate": 8.722222222222222e-07,
      "loss": 0.0013,
      "step": 88430
    },
    {
      "epoch": 7.8613333333333335,
      "grad_norm": 0.35380658507347107,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0013,
      "step": 88440
    },
    {
      "epoch": 7.862222222222222,
      "grad_norm": 0.1187446191906929,
      "learning_rate": 8.611111111111111e-07,
      "loss": 0.0013,
      "step": 88450
    },
    {
      "epoch": 7.863111111111111,
      "grad_norm": 0.20854182541370392,
      "learning_rate": 8.555555555555556e-07,
      "loss": 0.0017,
      "step": 88460
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.46330970525741577,
      "learning_rate": 8.500000000000001e-07,
      "loss": 0.0017,
      "step": 88470
    },
    {
      "epoch": 7.864888888888889,
      "grad_norm": 0.4163475036621094,
      "learning_rate": 8.444444444444444e-07,
      "loss": 0.0024,
      "step": 88480
    },
    {
      "epoch": 7.865777777777778,
      "grad_norm": 0.31715112924575806,
      "learning_rate": 8.388888888888889e-07,
      "loss": 0.0017,
      "step": 88490
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.3954435884952545,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0017,
      "step": 88500
    },
    {
      "epoch": 7.867555555555556,
      "grad_norm": 0.1508173644542694,
      "learning_rate": 8.277777777777778e-07,
      "loss": 0.0017,
      "step": 88510
    },
    {
      "epoch": 7.868444444444444,
      "grad_norm": 0.10641676932573318,
      "learning_rate": 8.222222222222223e-07,
      "loss": 0.0018,
      "step": 88520
    },
    {
      "epoch": 7.8693333333333335,
      "grad_norm": 0.4073057770729065,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0013,
      "step": 88530
    },
    {
      "epoch": 7.870222222222222,
      "grad_norm": 0.10826148837804794,
      "learning_rate": 8.11111111111111e-07,
      "loss": 0.0014,
      "step": 88540
    },
    {
      "epoch": 7.871111111111111,
      "grad_norm": 0.4647202789783478,
      "learning_rate": 8.055555555555556e-07,
      "loss": 0.0023,
      "step": 88550
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.2633236348628998,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0018,
      "step": 88560
    },
    {
      "epoch": 7.872888888888889,
      "grad_norm": 0.10595428198575974,
      "learning_rate": 7.944444444444446e-07,
      "loss": 0.0016,
      "step": 88570
    },
    {
      "epoch": 7.873777777777778,
      "grad_norm": 0.659462034702301,
      "learning_rate": 7.88888888888889e-07,
      "loss": 0.0014,
      "step": 88580
    },
    {
      "epoch": 7.874666666666666,
      "grad_norm": 0.25782492756843567,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.002,
      "step": 88590
    },
    {
      "epoch": 7.875555555555556,
      "grad_norm": 0.3215942680835724,
      "learning_rate": 7.777777777777778e-07,
      "loss": 0.0018,
      "step": 88600
    },
    {
      "epoch": 7.876444444444444,
      "grad_norm": 0.05219879746437073,
      "learning_rate": 7.722222222222223e-07,
      "loss": 0.0027,
      "step": 88610
    },
    {
      "epoch": 7.8773333333333335,
      "grad_norm": 0.17805132269859314,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.002,
      "step": 88620
    },
    {
      "epoch": 7.878222222222222,
      "grad_norm": 0.0534723624587059,
      "learning_rate": 7.611111111111111e-07,
      "loss": 0.002,
      "step": 88630
    },
    {
      "epoch": 7.879111111111111,
      "grad_norm": 0.182876318693161,
      "learning_rate": 7.555555555555556e-07,
      "loss": 0.0022,
      "step": 88640
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.3863937556743622,
      "learning_rate": 7.5e-07,
      "loss": 0.002,
      "step": 88650
    },
    {
      "epoch": 7.880888888888889,
      "grad_norm": 0.32502996921539307,
      "learning_rate": 7.444444444444445e-07,
      "loss": 0.0018,
      "step": 88660
    },
    {
      "epoch": 7.881777777777778,
      "grad_norm": 0.42481139302253723,
      "learning_rate": 7.388888888888889e-07,
      "loss": 0.0016,
      "step": 88670
    },
    {
      "epoch": 7.882666666666667,
      "grad_norm": 0.39869195222854614,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0017,
      "step": 88680
    },
    {
      "epoch": 7.883555555555556,
      "grad_norm": 0.31634005904197693,
      "learning_rate": 7.277777777777778e-07,
      "loss": 0.0016,
      "step": 88690
    },
    {
      "epoch": 7.884444444444444,
      "grad_norm": 0.6074786186218262,
      "learning_rate": 7.222222222222222e-07,
      "loss": 0.0022,
      "step": 88700
    },
    {
      "epoch": 7.8853333333333335,
      "grad_norm": 0.2612394094467163,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.0012,
      "step": 88710
    },
    {
      "epoch": 7.886222222222222,
      "grad_norm": 0.053469669073820114,
      "learning_rate": 7.111111111111112e-07,
      "loss": 0.0018,
      "step": 88720
    },
    {
      "epoch": 7.887111111111111,
      "grad_norm": 0.3902745246887207,
      "learning_rate": 7.055555555555556e-07,
      "loss": 0.0019,
      "step": 88730
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.05667857453227043,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0021,
      "step": 88740
    },
    {
      "epoch": 7.888888888888889,
      "grad_norm": 0.05220590904355049,
      "learning_rate": 6.944444444444445e-07,
      "loss": 0.0017,
      "step": 88750
    },
    {
      "epoch": 7.889777777777778,
      "grad_norm": 0.21827608346939087,
      "learning_rate": 6.888888888888889e-07,
      "loss": 0.0014,
      "step": 88760
    },
    {
      "epoch": 7.890666666666666,
      "grad_norm": 0.06987932324409485,
      "learning_rate": 6.833333333333334e-07,
      "loss": 0.002,
      "step": 88770
    },
    {
      "epoch": 7.891555555555556,
      "grad_norm": 0.23651286959648132,
      "learning_rate": 6.777777777777778e-07,
      "loss": 0.0028,
      "step": 88780
    },
    {
      "epoch": 7.892444444444444,
      "grad_norm": 0.18848058581352234,
      "learning_rate": 6.722222222222223e-07,
      "loss": 0.0018,
      "step": 88790
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.362068772315979,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0029,
      "step": 88800
    },
    {
      "epoch": 7.894222222222222,
      "grad_norm": 0.33646437525749207,
      "learning_rate": 6.611111111111111e-07,
      "loss": 0.0017,
      "step": 88810
    },
    {
      "epoch": 7.895111111111111,
      "grad_norm": 0.25940874218940735,
      "learning_rate": 6.555555555555556e-07,
      "loss": 0.0022,
      "step": 88820
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.2563628852367401,
      "learning_rate": 6.5e-07,
      "loss": 0.0015,
      "step": 88830
    },
    {
      "epoch": 7.896888888888888,
      "grad_norm": 0.4535563588142395,
      "learning_rate": 6.444444444444444e-07,
      "loss": 0.0014,
      "step": 88840
    },
    {
      "epoch": 7.897777777777778,
      "grad_norm": 0.05860697105526924,
      "learning_rate": 6.388888888888889e-07,
      "loss": 0.0017,
      "step": 88850
    },
    {
      "epoch": 7.898666666666666,
      "grad_norm": 0.05334319546818733,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.0025,
      "step": 88860
    },
    {
      "epoch": 7.899555555555556,
      "grad_norm": 0.1238759383559227,
      "learning_rate": 6.277777777777778e-07,
      "loss": 0.002,
      "step": 88870
    },
    {
      "epoch": 7.900444444444444,
      "grad_norm": 0.08494700491428375,
      "learning_rate": 6.222222222222223e-07,
      "loss": 0.0021,
      "step": 88880
    },
    {
      "epoch": 7.9013333333333335,
      "grad_norm": 0.1876244693994522,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.0015,
      "step": 88890
    },
    {
      "epoch": 7.902222222222222,
      "grad_norm": 0.13996432721614838,
      "learning_rate": 6.111111111111112e-07,
      "loss": 0.0018,
      "step": 88900
    },
    {
      "epoch": 7.903111111111111,
      "grad_norm": 0.11444342136383057,
      "learning_rate": 6.055555555555556e-07,
      "loss": 0.0018,
      "step": 88910
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.23435649275779724,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.002,
      "step": 88920
    },
    {
      "epoch": 7.904888888888889,
      "grad_norm": 0.3603232502937317,
      "learning_rate": 5.944444444444445e-07,
      "loss": 0.0018,
      "step": 88930
    },
    {
      "epoch": 7.905777777777778,
      "grad_norm": 0.07604344189167023,
      "learning_rate": 5.888888888888889e-07,
      "loss": 0.0015,
      "step": 88940
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.4283205568790436,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.002,
      "step": 88950
    },
    {
      "epoch": 7.907555555555556,
      "grad_norm": 0.5985042452812195,
      "learning_rate": 5.777777777777778e-07,
      "loss": 0.0019,
      "step": 88960
    },
    {
      "epoch": 7.908444444444444,
      "grad_norm": 0.14863333106040955,
      "learning_rate": 5.722222222222223e-07,
      "loss": 0.0018,
      "step": 88970
    },
    {
      "epoch": 7.9093333333333335,
      "grad_norm": 0.21436689794063568,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.002,
      "step": 88980
    },
    {
      "epoch": 7.910222222222222,
      "grad_norm": 0.14290405809879303,
      "learning_rate": 5.611111111111111e-07,
      "loss": 0.002,
      "step": 88990
    },
    {
      "epoch": 7.911111111111111,
      "grad_norm": 0.15503019094467163,
      "learning_rate": 5.555555555555556e-07,
      "loss": 0.0022,
      "step": 89000
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.14965496957302094,
      "learning_rate": 5.5e-07,
      "loss": 0.0019,
      "step": 89010
    },
    {
      "epoch": 7.912888888888888,
      "grad_norm": 0.03189454600214958,
      "learning_rate": 5.444444444444444e-07,
      "loss": 0.0015,
      "step": 89020
    },
    {
      "epoch": 7.913777777777778,
      "grad_norm": 0.0591956228017807,
      "learning_rate": 5.388888888888889e-07,
      "loss": 0.0019,
      "step": 89030
    },
    {
      "epoch": 7.914666666666666,
      "grad_norm": 0.1793559491634369,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0023,
      "step": 89040
    },
    {
      "epoch": 7.915555555555556,
      "grad_norm": 0.054446518421173096,
      "learning_rate": 5.277777777777779e-07,
      "loss": 0.0021,
      "step": 89050
    },
    {
      "epoch": 7.916444444444444,
      "grad_norm": 0.3299872577190399,
      "learning_rate": 5.222222222222223e-07,
      "loss": 0.0028,
      "step": 89060
    },
    {
      "epoch": 7.917333333333334,
      "grad_norm": 0.25269103050231934,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.0016,
      "step": 89070
    },
    {
      "epoch": 7.918222222222222,
      "grad_norm": 0.2744778096675873,
      "learning_rate": 5.111111111111112e-07,
      "loss": 0.0016,
      "step": 89080
    },
    {
      "epoch": 7.919111111111111,
      "grad_norm": 0.22851020097732544,
      "learning_rate": 5.055555555555556e-07,
      "loss": 0.0019,
      "step": 89090
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.31254810094833374,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0017,
      "step": 89100
    },
    {
      "epoch": 7.920888888888889,
      "grad_norm": 0.28960904479026794,
      "learning_rate": 4.944444444444444e-07,
      "loss": 0.0014,
      "step": 89110
    },
    {
      "epoch": 7.921777777777778,
      "grad_norm": 0.1193806380033493,
      "learning_rate": 4.888888888888889e-07,
      "loss": 0.0019,
      "step": 89120
    },
    {
      "epoch": 7.922666666666666,
      "grad_norm": 0.1471252292394638,
      "learning_rate": 4.833333333333334e-07,
      "loss": 0.0015,
      "step": 89130
    },
    {
      "epoch": 7.923555555555556,
      "grad_norm": 0.05998812988400459,
      "learning_rate": 4.777777777777778e-07,
      "loss": 0.0025,
      "step": 89140
    },
    {
      "epoch": 7.924444444444444,
      "grad_norm": 0.44456279277801514,
      "learning_rate": 4.7222222222222226e-07,
      "loss": 0.0016,
      "step": 89150
    },
    {
      "epoch": 7.925333333333334,
      "grad_norm": 0.3469730317592621,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.0016,
      "step": 89160
    },
    {
      "epoch": 7.926222222222222,
      "grad_norm": 0.04578566551208496,
      "learning_rate": 4.611111111111111e-07,
      "loss": 0.0011,
      "step": 89170
    },
    {
      "epoch": 7.927111111111111,
      "grad_norm": 0.0835912898182869,
      "learning_rate": 4.555555555555556e-07,
      "loss": 0.0016,
      "step": 89180
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.08352172374725342,
      "learning_rate": 4.5e-07,
      "loss": 0.0023,
      "step": 89190
    },
    {
      "epoch": 7.928888888888888,
      "grad_norm": 0.16315679252147675,
      "learning_rate": 4.444444444444445e-07,
      "loss": 0.0023,
      "step": 89200
    },
    {
      "epoch": 7.929777777777778,
      "grad_norm": 0.4123733937740326,
      "learning_rate": 4.3888888888888895e-07,
      "loss": 0.0018,
      "step": 89210
    },
    {
      "epoch": 7.930666666666666,
      "grad_norm": 0.040808163583278656,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.0019,
      "step": 89220
    },
    {
      "epoch": 7.931555555555556,
      "grad_norm": 0.17347858846187592,
      "learning_rate": 4.277777777777778e-07,
      "loss": 0.0021,
      "step": 89230
    },
    {
      "epoch": 7.932444444444444,
      "grad_norm": 0.4714706540107727,
      "learning_rate": 4.222222222222222e-07,
      "loss": 0.0024,
      "step": 89240
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.34176433086395264,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.0019,
      "step": 89250
    },
    {
      "epoch": 7.934222222222222,
      "grad_norm": 0.2303619682788849,
      "learning_rate": 4.111111111111112e-07,
      "loss": 0.0019,
      "step": 89260
    },
    {
      "epoch": 7.9351111111111114,
      "grad_norm": 0.049173753708601,
      "learning_rate": 4.055555555555555e-07,
      "loss": 0.0014,
      "step": 89270
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.08370399475097656,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.0017,
      "step": 89280
    },
    {
      "epoch": 7.936888888888889,
      "grad_norm": 0.3198668360710144,
      "learning_rate": 3.944444444444445e-07,
      "loss": 0.0018,
      "step": 89290
    },
    {
      "epoch": 7.937777777777778,
      "grad_norm": 0.42685940861701965,
      "learning_rate": 3.888888888888889e-07,
      "loss": 0.0019,
      "step": 89300
    },
    {
      "epoch": 7.938666666666666,
      "grad_norm": 0.18529687821865082,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 0.0021,
      "step": 89310
    },
    {
      "epoch": 7.939555555555556,
      "grad_norm": 0.07005888968706131,
      "learning_rate": 3.777777777777778e-07,
      "loss": 0.0015,
      "step": 89320
    },
    {
      "epoch": 7.940444444444444,
      "grad_norm": 0.292444109916687,
      "learning_rate": 3.7222222222222226e-07,
      "loss": 0.0019,
      "step": 89330
    },
    {
      "epoch": 7.941333333333334,
      "grad_norm": 0.06786776334047318,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.0012,
      "step": 89340
    },
    {
      "epoch": 7.942222222222222,
      "grad_norm": 0.045106638222932816,
      "learning_rate": 3.611111111111111e-07,
      "loss": 0.0016,
      "step": 89350
    },
    {
      "epoch": 7.9431111111111115,
      "grad_norm": 0.21383677423000336,
      "learning_rate": 3.555555555555556e-07,
      "loss": 0.0021,
      "step": 89360
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.14813640713691711,
      "learning_rate": 3.5000000000000004e-07,
      "loss": 0.0023,
      "step": 89370
    },
    {
      "epoch": 7.9448888888888884,
      "grad_norm": 0.17098060250282288,
      "learning_rate": 3.4444444444444444e-07,
      "loss": 0.002,
      "step": 89380
    },
    {
      "epoch": 7.945777777777778,
      "grad_norm": 0.430976539850235,
      "learning_rate": 3.388888888888889e-07,
      "loss": 0.0027,
      "step": 89390
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.22181202471256256,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.0019,
      "step": 89400
    },
    {
      "epoch": 7.947555555555556,
      "grad_norm": 0.2318728119134903,
      "learning_rate": 3.277777777777778e-07,
      "loss": 0.0021,
      "step": 89410
    },
    {
      "epoch": 7.948444444444444,
      "grad_norm": 0.08637044578790665,
      "learning_rate": 3.222222222222222e-07,
      "loss": 0.0029,
      "step": 89420
    },
    {
      "epoch": 7.949333333333334,
      "grad_norm": 0.1121748760342598,
      "learning_rate": 3.1666666666666667e-07,
      "loss": 0.0022,
      "step": 89430
    },
    {
      "epoch": 7.950222222222222,
      "grad_norm": 0.04984840750694275,
      "learning_rate": 3.111111111111111e-07,
      "loss": 0.0012,
      "step": 89440
    },
    {
      "epoch": 7.9511111111111115,
      "grad_norm": 0.12073008716106415,
      "learning_rate": 3.055555555555556e-07,
      "loss": 0.0021,
      "step": 89450
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.12716087698936462,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.0019,
      "step": 89460
    },
    {
      "epoch": 7.952888888888889,
      "grad_norm": 0.7514102458953857,
      "learning_rate": 2.9444444444444444e-07,
      "loss": 0.0027,
      "step": 89470
    },
    {
      "epoch": 7.953777777777778,
      "grad_norm": 0.3924252986907959,
      "learning_rate": 2.888888888888889e-07,
      "loss": 0.0019,
      "step": 89480
    },
    {
      "epoch": 7.954666666666666,
      "grad_norm": 0.358801931142807,
      "learning_rate": 2.8333333333333336e-07,
      "loss": 0.0013,
      "step": 89490
    },
    {
      "epoch": 7.955555555555556,
      "grad_norm": 0.1501152664422989,
      "learning_rate": 2.777777777777778e-07,
      "loss": 0.0012,
      "step": 89500
    },
    {
      "epoch": 7.956444444444444,
      "grad_norm": 0.1824948936700821,
      "learning_rate": 2.722222222222222e-07,
      "loss": 0.0015,
      "step": 89510
    },
    {
      "epoch": 7.957333333333334,
      "grad_norm": 0.0841454491019249,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.0021,
      "step": 89520
    },
    {
      "epoch": 7.958222222222222,
      "grad_norm": 0.36740928888320923,
      "learning_rate": 2.6111111111111113e-07,
      "loss": 0.0024,
      "step": 89530
    },
    {
      "epoch": 7.9591111111111115,
      "grad_norm": 0.5353924632072449,
      "learning_rate": 2.555555555555556e-07,
      "loss": 0.0014,
      "step": 89540
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.22048388421535492,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.0018,
      "step": 89550
    },
    {
      "epoch": 7.9608888888888885,
      "grad_norm": 0.05317632481455803,
      "learning_rate": 2.4444444444444445e-07,
      "loss": 0.0021,
      "step": 89560
    },
    {
      "epoch": 7.961777777777778,
      "grad_norm": 0.1614379584789276,
      "learning_rate": 2.388888888888889e-07,
      "loss": 0.0017,
      "step": 89570
    },
    {
      "epoch": 7.962666666666666,
      "grad_norm": 0.5387891530990601,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 0.0025,
      "step": 89580
    },
    {
      "epoch": 7.963555555555556,
      "grad_norm": 0.2319464087486267,
      "learning_rate": 2.277777777777778e-07,
      "loss": 0.0016,
      "step": 89590
    },
    {
      "epoch": 7.964444444444444,
      "grad_norm": 0.09005040675401688,
      "learning_rate": 2.2222222222222224e-07,
      "loss": 0.0021,
      "step": 89600
    },
    {
      "epoch": 7.965333333333334,
      "grad_norm": 0.17454074323177338,
      "learning_rate": 2.1666666666666667e-07,
      "loss": 0.0022,
      "step": 89610
    },
    {
      "epoch": 7.966222222222222,
      "grad_norm": 0.07587078213691711,
      "learning_rate": 2.111111111111111e-07,
      "loss": 0.0014,
      "step": 89620
    },
    {
      "epoch": 7.9671111111111115,
      "grad_norm": 0.08072689175605774,
      "learning_rate": 2.055555555555556e-07,
      "loss": 0.0012,
      "step": 89630
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.20988290011882782,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.0028,
      "step": 89640
    },
    {
      "epoch": 7.968888888888889,
      "grad_norm": 0.1716195046901703,
      "learning_rate": 1.9444444444444445e-07,
      "loss": 0.0013,
      "step": 89650
    },
    {
      "epoch": 7.969777777777778,
      "grad_norm": 0.32757800817489624,
      "learning_rate": 1.888888888888889e-07,
      "loss": 0.0017,
      "step": 89660
    },
    {
      "epoch": 7.970666666666666,
      "grad_norm": 0.2905227541923523,
      "learning_rate": 1.8333333333333333e-07,
      "loss": 0.0019,
      "step": 89670
    },
    {
      "epoch": 7.971555555555556,
      "grad_norm": 0.2967540919780731,
      "learning_rate": 1.777777777777778e-07,
      "loss": 0.0018,
      "step": 89680
    },
    {
      "epoch": 7.972444444444444,
      "grad_norm": 0.5942422747612,
      "learning_rate": 1.7222222222222222e-07,
      "loss": 0.0016,
      "step": 89690
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 0.1889774352312088,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.0013,
      "step": 89700
    },
    {
      "epoch": 7.974222222222222,
      "grad_norm": 0.08032016456127167,
      "learning_rate": 1.611111111111111e-07,
      "loss": 0.0019,
      "step": 89710
    },
    {
      "epoch": 7.9751111111111115,
      "grad_norm": 0.1245838925242424,
      "learning_rate": 1.5555555555555556e-07,
      "loss": 0.0014,
      "step": 89720
    },
    {
      "epoch": 7.976,
      "grad_norm": 0.3111528158187866,
      "learning_rate": 1.5000000000000002e-07,
      "loss": 0.0018,
      "step": 89730
    },
    {
      "epoch": 7.9768888888888885,
      "grad_norm": 0.25615376234054565,
      "learning_rate": 1.4444444444444445e-07,
      "loss": 0.0017,
      "step": 89740
    },
    {
      "epoch": 7.977777777777778,
      "grad_norm": 0.2192145437002182,
      "learning_rate": 1.388888888888889e-07,
      "loss": 0.0017,
      "step": 89750
    },
    {
      "epoch": 7.978666666666666,
      "grad_norm": 0.0899282917380333,
      "learning_rate": 1.3333333333333334e-07,
      "loss": 0.0014,
      "step": 89760
    },
    {
      "epoch": 7.979555555555556,
      "grad_norm": 0.14510250091552734,
      "learning_rate": 1.277777777777778e-07,
      "loss": 0.0024,
      "step": 89770
    },
    {
      "epoch": 7.980444444444444,
      "grad_norm": 0.25343868136405945,
      "learning_rate": 1.2222222222222222e-07,
      "loss": 0.0026,
      "step": 89780
    },
    {
      "epoch": 7.981333333333334,
      "grad_norm": 0.12050870060920715,
      "learning_rate": 1.1666666666666668e-07,
      "loss": 0.0024,
      "step": 89790
    },
    {
      "epoch": 7.982222222222222,
      "grad_norm": 0.2565152645111084,
      "learning_rate": 1.1111111111111112e-07,
      "loss": 0.0019,
      "step": 89800
    },
    {
      "epoch": 7.9831111111111115,
      "grad_norm": 0.06958062201738358,
      "learning_rate": 1.0555555555555555e-07,
      "loss": 0.0019,
      "step": 89810
    },
    {
      "epoch": 7.984,
      "grad_norm": 0.35447630286216736,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 0.002,
      "step": 89820
    },
    {
      "epoch": 7.984888888888889,
      "grad_norm": 0.05563442409038544,
      "learning_rate": 9.444444444444445e-08,
      "loss": 0.0026,
      "step": 89830
    },
    {
      "epoch": 7.985777777777778,
      "grad_norm": 0.39835283160209656,
      "learning_rate": 8.88888888888889e-08,
      "loss": 0.0023,
      "step": 89840
    },
    {
      "epoch": 7.986666666666666,
      "grad_norm": 0.36783847212791443,
      "learning_rate": 8.333333333333334e-08,
      "loss": 0.0015,
      "step": 89850
    },
    {
      "epoch": 7.987555555555556,
      "grad_norm": 0.060561131685972214,
      "learning_rate": 7.777777777777778e-08,
      "loss": 0.002,
      "step": 89860
    },
    {
      "epoch": 7.988444444444444,
      "grad_norm": 0.142059788107872,
      "learning_rate": 7.222222222222222e-08,
      "loss": 0.0013,
      "step": 89870
    },
    {
      "epoch": 7.989333333333334,
      "grad_norm": 0.05091579258441925,
      "learning_rate": 6.666666666666667e-08,
      "loss": 0.0015,
      "step": 89880
    },
    {
      "epoch": 7.990222222222222,
      "grad_norm": 0.09565597772598267,
      "learning_rate": 6.111111111111111e-08,
      "loss": 0.002,
      "step": 89890
    },
    {
      "epoch": 7.9911111111111115,
      "grad_norm": 0.29308709502220154,
      "learning_rate": 5.555555555555556e-08,
      "loss": 0.0021,
      "step": 89900
    },
    {
      "epoch": 7.992,
      "grad_norm": 0.3955780267715454,
      "learning_rate": 5.0000000000000004e-08,
      "loss": 0.0014,
      "step": 89910
    },
    {
      "epoch": 7.9928888888888885,
      "grad_norm": 0.05645591393113136,
      "learning_rate": 4.444444444444445e-08,
      "loss": 0.0017,
      "step": 89920
    },
    {
      "epoch": 7.993777777777778,
      "grad_norm": 0.2171630859375,
      "learning_rate": 3.888888888888889e-08,
      "loss": 0.0014,
      "step": 89930
    },
    {
      "epoch": 7.994666666666666,
      "grad_norm": 0.21888931095600128,
      "learning_rate": 3.3333333333333334e-08,
      "loss": 0.0011,
      "step": 89940
    },
    {
      "epoch": 7.995555555555556,
      "grad_norm": 0.21396753191947937,
      "learning_rate": 2.777777777777778e-08,
      "loss": 0.002,
      "step": 89950
    },
    {
      "epoch": 7.996444444444444,
      "grad_norm": 0.3409470319747925,
      "learning_rate": 2.2222222222222224e-08,
      "loss": 0.0027,
      "step": 89960
    },
    {
      "epoch": 7.997333333333334,
      "grad_norm": 0.1533193290233612,
      "learning_rate": 1.6666666666666667e-08,
      "loss": 0.0014,
      "step": 89970
    },
    {
      "epoch": 7.998222222222222,
      "grad_norm": 0.4287640452384949,
      "learning_rate": 1.1111111111111112e-08,
      "loss": 0.0013,
      "step": 89980
    },
    {
      "epoch": 7.999111111111111,
      "grad_norm": 0.05299896374344826,
      "learning_rate": 5.555555555555556e-09,
      "loss": 0.0014,
      "step": 89990
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.11506443470716476,
      "learning_rate": 0.0,
      "loss": 0.0021,
      "step": 90000
    }
  ],
  "logging_steps": 10,
  "max_steps": 90000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 27000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007407407407407407,
      "grad_norm": 1.0621947050094604,
      "learning_rate": 4.998148148148148e-05,
      "loss": 0.0311,
      "step": 10
    },
    {
      "epoch": 0.0014814814814814814,
      "grad_norm": 0.04312507063150406,
      "learning_rate": 4.9962962962962964e-05,
      "loss": 0.0055,
      "step": 20
    },
    {
      "epoch": 0.0022222222222222222,
      "grad_norm": 0.0983368307352066,
      "learning_rate": 4.994444444444445e-05,
      "loss": 0.0043,
      "step": 30
    },
    {
      "epoch": 0.002962962962962963,
      "grad_norm": 0.13424156606197357,
      "learning_rate": 4.9925925925925926e-05,
      "loss": 0.0044,
      "step": 40
    },
    {
      "epoch": 0.003703703703703704,
      "grad_norm": 0.02360651269555092,
      "learning_rate": 4.9907407407407406e-05,
      "loss": 0.0043,
      "step": 50
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 0.05419936031103134,
      "learning_rate": 4.9888888888888894e-05,
      "loss": 0.0031,
      "step": 60
    },
    {
      "epoch": 0.005185185185185185,
      "grad_norm": 0.09930488467216492,
      "learning_rate": 4.9870370370370375e-05,
      "loss": 0.0027,
      "step": 70
    },
    {
      "epoch": 0.005925925925925926,
      "grad_norm": 0.08526338636875153,
      "learning_rate": 4.9851851851851855e-05,
      "loss": 0.0029,
      "step": 80
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.05405111610889435,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0047,
      "step": 90
    },
    {
      "epoch": 0.007407407407407408,
      "grad_norm": 0.18778060376644135,
      "learning_rate": 4.981481481481482e-05,
      "loss": 0.0041,
      "step": 100
    },
    {
      "epoch": 0.008148148148148147,
      "grad_norm": 0.015005144290626049,
      "learning_rate": 4.97962962962963e-05,
      "loss": 0.0036,
      "step": 110
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 0.0847867876291275,
      "learning_rate": 4.977777777777778e-05,
      "loss": 0.0031,
      "step": 120
    },
    {
      "epoch": 0.00962962962962963,
      "grad_norm": 0.04632267728447914,
      "learning_rate": 4.975925925925926e-05,
      "loss": 0.0025,
      "step": 130
    },
    {
      "epoch": 0.01037037037037037,
      "grad_norm": 0.13064289093017578,
      "learning_rate": 4.974074074074074e-05,
      "loss": 0.0033,
      "step": 140
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 0.05551200732588768,
      "learning_rate": 4.972222222222223e-05,
      "loss": 0.0038,
      "step": 150
    },
    {
      "epoch": 0.011851851851851851,
      "grad_norm": 0.026392299681901932,
      "learning_rate": 4.970370370370371e-05,
      "loss": 0.004,
      "step": 160
    },
    {
      "epoch": 0.012592592592592593,
      "grad_norm": 0.21809254586696625,
      "learning_rate": 4.968518518518519e-05,
      "loss": 0.0038,
      "step": 170
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.06821439415216446,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0034,
      "step": 180
    },
    {
      "epoch": 0.014074074074074074,
      "grad_norm": 0.060036901384592056,
      "learning_rate": 4.964814814814815e-05,
      "loss": 0.0037,
      "step": 190
    },
    {
      "epoch": 0.014814814814814815,
      "grad_norm": 0.14895844459533691,
      "learning_rate": 4.962962962962963e-05,
      "loss": 0.004,
      "step": 200
    },
    {
      "epoch": 0.015555555555555555,
      "grad_norm": 0.10727564990520477,
      "learning_rate": 4.961111111111111e-05,
      "loss": 0.0038,
      "step": 210
    },
    {
      "epoch": 0.016296296296296295,
      "grad_norm": 0.1792684644460678,
      "learning_rate": 4.959259259259259e-05,
      "loss": 0.0029,
      "step": 220
    },
    {
      "epoch": 0.017037037037037038,
      "grad_norm": 0.10053537786006927,
      "learning_rate": 4.957407407407408e-05,
      "loss": 0.0036,
      "step": 230
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 0.032060422003269196,
      "learning_rate": 4.955555555555556e-05,
      "loss": 0.003,
      "step": 240
    },
    {
      "epoch": 0.018518518518518517,
      "grad_norm": 0.04122897982597351,
      "learning_rate": 4.9537037037037035e-05,
      "loss": 0.0032,
      "step": 250
    },
    {
      "epoch": 0.01925925925925926,
      "grad_norm": 0.13031156361103058,
      "learning_rate": 4.951851851851852e-05,
      "loss": 0.0039,
      "step": 260
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.1481645703315735,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.0031,
      "step": 270
    },
    {
      "epoch": 0.02074074074074074,
      "grad_norm": 0.09755195677280426,
      "learning_rate": 4.9481481481481485e-05,
      "loss": 0.0034,
      "step": 280
    },
    {
      "epoch": 0.02148148148148148,
      "grad_norm": 0.15833114087581635,
      "learning_rate": 4.9462962962962965e-05,
      "loss": 0.0033,
      "step": 290
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 0.06528786569833755,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 0.0031,
      "step": 300
    },
    {
      "epoch": 0.022962962962962963,
      "grad_norm": 0.017971577122807503,
      "learning_rate": 4.942592592592593e-05,
      "loss": 0.0035,
      "step": 310
    },
    {
      "epoch": 0.023703703703703703,
      "grad_norm": 0.17144787311553955,
      "learning_rate": 4.940740740740741e-05,
      "loss": 0.0017,
      "step": 320
    },
    {
      "epoch": 0.024444444444444446,
      "grad_norm": 0.16605722904205322,
      "learning_rate": 4.938888888888889e-05,
      "loss": 0.003,
      "step": 330
    },
    {
      "epoch": 0.025185185185185185,
      "grad_norm": 0.11621765047311783,
      "learning_rate": 4.937037037037037e-05,
      "loss": 0.0033,
      "step": 340
    },
    {
      "epoch": 0.025925925925925925,
      "grad_norm": 0.04034261777997017,
      "learning_rate": 4.935185185185186e-05,
      "loss": 0.0038,
      "step": 350
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.10263218730688095,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0036,
      "step": 360
    },
    {
      "epoch": 0.027407407407407408,
      "grad_norm": 0.08378059417009354,
      "learning_rate": 4.931481481481482e-05,
      "loss": 0.0026,
      "step": 370
    },
    {
      "epoch": 0.028148148148148148,
      "grad_norm": 0.09031001478433609,
      "learning_rate": 4.92962962962963e-05,
      "loss": 0.0034,
      "step": 380
    },
    {
      "epoch": 0.028888888888888888,
      "grad_norm": 0.05143090337514877,
      "learning_rate": 4.927777777777778e-05,
      "loss": 0.0029,
      "step": 390
    },
    {
      "epoch": 0.02962962962962963,
      "grad_norm": 0.15190181136131287,
      "learning_rate": 4.925925925925926e-05,
      "loss": 0.0032,
      "step": 400
    },
    {
      "epoch": 0.03037037037037037,
      "grad_norm": 0.08542677760124207,
      "learning_rate": 4.924074074074074e-05,
      "loss": 0.0033,
      "step": 410
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 0.07765830308198929,
      "learning_rate": 4.922222222222222e-05,
      "loss": 0.0022,
      "step": 420
    },
    {
      "epoch": 0.03185185185185185,
      "grad_norm": 0.04959217086434364,
      "learning_rate": 4.920370370370371e-05,
      "loss": 0.002,
      "step": 430
    },
    {
      "epoch": 0.03259259259259259,
      "grad_norm": 0.04966644197702408,
      "learning_rate": 4.918518518518519e-05,
      "loss": 0.0038,
      "step": 440
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.053773920983076096,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.002,
      "step": 450
    },
    {
      "epoch": 0.034074074074074076,
      "grad_norm": 0.08227040618658066,
      "learning_rate": 4.9148148148148145e-05,
      "loss": 0.0034,
      "step": 460
    },
    {
      "epoch": 0.03481481481481481,
      "grad_norm": 0.12431260943412781,
      "learning_rate": 4.912962962962963e-05,
      "loss": 0.0034,
      "step": 470
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 0.03717341646552086,
      "learning_rate": 4.9111111111111114e-05,
      "loss": 0.003,
      "step": 480
    },
    {
      "epoch": 0.0362962962962963,
      "grad_norm": 0.05398702248930931,
      "learning_rate": 4.9092592592592595e-05,
      "loss": 0.0028,
      "step": 490
    },
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 0.31811976432800293,
      "learning_rate": 4.9074074074074075e-05,
      "loss": 0.0027,
      "step": 500
    },
    {
      "epoch": 0.03777777777777778,
      "grad_norm": 0.06057588383555412,
      "learning_rate": 4.905555555555556e-05,
      "loss": 0.0031,
      "step": 510
    },
    {
      "epoch": 0.03851851851851852,
      "grad_norm": 0.08680570125579834,
      "learning_rate": 4.903703703703704e-05,
      "loss": 0.0028,
      "step": 520
    },
    {
      "epoch": 0.03925925925925926,
      "grad_norm": 0.045105211436748505,
      "learning_rate": 4.901851851851852e-05,
      "loss": 0.0028,
      "step": 530
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.043209388852119446,
      "learning_rate": 4.9e-05,
      "loss": 0.0023,
      "step": 540
    },
    {
      "epoch": 0.040740740740740744,
      "grad_norm": 0.05576493218541145,
      "learning_rate": 4.8981481481481486e-05,
      "loss": 0.0037,
      "step": 550
    },
    {
      "epoch": 0.04148148148148148,
      "grad_norm": 0.042426012456417084,
      "learning_rate": 4.896296296296297e-05,
      "loss": 0.0022,
      "step": 560
    },
    {
      "epoch": 0.042222222222222223,
      "grad_norm": 0.0868595764040947,
      "learning_rate": 4.894444444444445e-05,
      "loss": 0.0027,
      "step": 570
    },
    {
      "epoch": 0.04296296296296296,
      "grad_norm": 0.1870046705007553,
      "learning_rate": 4.892592592592593e-05,
      "loss": 0.0029,
      "step": 580
    },
    {
      "epoch": 0.0437037037037037,
      "grad_norm": 0.07943614572286606,
      "learning_rate": 4.890740740740741e-05,
      "loss": 0.0023,
      "step": 590
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.04563817381858826,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.0029,
      "step": 600
    },
    {
      "epoch": 0.04518518518518518,
      "grad_norm": 0.04340513050556183,
      "learning_rate": 4.887037037037037e-05,
      "loss": 0.003,
      "step": 610
    },
    {
      "epoch": 0.045925925925925926,
      "grad_norm": 0.050175223499536514,
      "learning_rate": 4.885185185185185e-05,
      "loss": 0.0042,
      "step": 620
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.046199094504117966,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0036,
      "step": 630
    },
    {
      "epoch": 0.047407407407407405,
      "grad_norm": 0.016545629128813744,
      "learning_rate": 4.881481481481482e-05,
      "loss": 0.0036,
      "step": 640
    },
    {
      "epoch": 0.04814814814814815,
      "grad_norm": 0.07013747096061707,
      "learning_rate": 4.87962962962963e-05,
      "loss": 0.0033,
      "step": 650
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 0.2021632045507431,
      "learning_rate": 4.8777777777777775e-05,
      "loss": 0.0023,
      "step": 660
    },
    {
      "epoch": 0.04962962962962963,
      "grad_norm": 0.09281324595212936,
      "learning_rate": 4.875925925925926e-05,
      "loss": 0.0046,
      "step": 670
    },
    {
      "epoch": 0.05037037037037037,
      "grad_norm": 0.07706275582313538,
      "learning_rate": 4.874074074074074e-05,
      "loss": 0.0035,
      "step": 680
    },
    {
      "epoch": 0.051111111111111114,
      "grad_norm": 0.09992849081754684,
      "learning_rate": 4.8722222222222224e-05,
      "loss": 0.0038,
      "step": 690
    },
    {
      "epoch": 0.05185185185185185,
      "grad_norm": 0.020283203572034836,
      "learning_rate": 4.8703703703703704e-05,
      "loss": 0.0028,
      "step": 700
    },
    {
      "epoch": 0.052592592592592594,
      "grad_norm": 0.047011662274599075,
      "learning_rate": 4.868518518518519e-05,
      "loss": 0.0024,
      "step": 710
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.05714607238769531,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0024,
      "step": 720
    },
    {
      "epoch": 0.05407407407407407,
      "grad_norm": 0.08747423440217972,
      "learning_rate": 4.864814814814815e-05,
      "loss": 0.0024,
      "step": 730
    },
    {
      "epoch": 0.054814814814814816,
      "grad_norm": 0.1524917185306549,
      "learning_rate": 4.862962962962963e-05,
      "loss": 0.0028,
      "step": 740
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.04791216924786568,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 0.002,
      "step": 750
    },
    {
      "epoch": 0.056296296296296296,
      "grad_norm": 0.0,
      "learning_rate": 4.8592592592592596e-05,
      "loss": 0.0018,
      "step": 760
    },
    {
      "epoch": 0.05703703703703704,
      "grad_norm": 0.21528145670890808,
      "learning_rate": 4.857407407407408e-05,
      "loss": 0.0024,
      "step": 770
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 0.06199989095330238,
      "learning_rate": 4.855555555555556e-05,
      "loss": 0.0028,
      "step": 780
    },
    {
      "epoch": 0.05851851851851852,
      "grad_norm": 0.06753972172737122,
      "learning_rate": 4.8537037037037045e-05,
      "loss": 0.0035,
      "step": 790
    },
    {
      "epoch": 0.05925925925925926,
      "grad_norm": 0.058951523154973984,
      "learning_rate": 4.851851851851852e-05,
      "loss": 0.0026,
      "step": 800
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.21415124833583832,
      "learning_rate": 4.85e-05,
      "loss": 0.0027,
      "step": 810
    },
    {
      "epoch": 0.06074074074074074,
      "grad_norm": 0.06171103194355965,
      "learning_rate": 4.848148148148148e-05,
      "loss": 0.0025,
      "step": 820
    },
    {
      "epoch": 0.061481481481481484,
      "grad_norm": 0.04954806715250015,
      "learning_rate": 4.846296296296297e-05,
      "loss": 0.0021,
      "step": 830
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 0.2637726664543152,
      "learning_rate": 4.844444444444445e-05,
      "loss": 0.0047,
      "step": 840
    },
    {
      "epoch": 0.06296296296296296,
      "grad_norm": 0.09741733968257904,
      "learning_rate": 4.842592592592593e-05,
      "loss": 0.002,
      "step": 850
    },
    {
      "epoch": 0.0637037037037037,
      "grad_norm": 0.20925574004650116,
      "learning_rate": 4.840740740740741e-05,
      "loss": 0.0028,
      "step": 860
    },
    {
      "epoch": 0.06444444444444444,
      "grad_norm": 0.05841084197163582,
      "learning_rate": 4.838888888888889e-05,
      "loss": 0.003,
      "step": 870
    },
    {
      "epoch": 0.06518518518518518,
      "grad_norm": 0.09125075489282608,
      "learning_rate": 4.837037037037037e-05,
      "loss": 0.0028,
      "step": 880
    },
    {
      "epoch": 0.06592592592592593,
      "grad_norm": 0.08101820945739746,
      "learning_rate": 4.835185185185185e-05,
      "loss": 0.003,
      "step": 890
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0028,
      "step": 900
    },
    {
      "epoch": 0.0674074074074074,
      "grad_norm": 0.12192858755588531,
      "learning_rate": 4.831481481481482e-05,
      "loss": 0.0028,
      "step": 910
    },
    {
      "epoch": 0.06814814814814815,
      "grad_norm": 0.14205265045166016,
      "learning_rate": 4.82962962962963e-05,
      "loss": 0.0025,
      "step": 920
    },
    {
      "epoch": 0.06888888888888889,
      "grad_norm": 0.13129447400569916,
      "learning_rate": 4.8277777777777776e-05,
      "loss": 0.0043,
      "step": 930
    },
    {
      "epoch": 0.06962962962962962,
      "grad_norm": 0.08943746984004974,
      "learning_rate": 4.825925925925926e-05,
      "loss": 0.0022,
      "step": 940
    },
    {
      "epoch": 0.07037037037037037,
      "grad_norm": 0.09360519796609879,
      "learning_rate": 4.8240740740740744e-05,
      "loss": 0.002,
      "step": 950
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.026193199679255486,
      "learning_rate": 4.8222222222222225e-05,
      "loss": 0.0027,
      "step": 960
    },
    {
      "epoch": 0.07185185185185185,
      "grad_norm": 0.0936441421508789,
      "learning_rate": 4.8203703703703706e-05,
      "loss": 0.0025,
      "step": 970
    },
    {
      "epoch": 0.0725925925925926,
      "grad_norm": 0.04133135825395584,
      "learning_rate": 4.818518518518519e-05,
      "loss": 0.0038,
      "step": 980
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.08578087389469147,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.0021,
      "step": 990
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 0.09141383320093155,
      "learning_rate": 4.814814814814815e-05,
      "loss": 0.003,
      "step": 1000
    },
    {
      "epoch": 0.07481481481481482,
      "grad_norm": 0.0,
      "learning_rate": 4.812962962962963e-05,
      "loss": 0.0023,
      "step": 1010
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 0.1242988184094429,
      "learning_rate": 4.811111111111111e-05,
      "loss": 0.0029,
      "step": 1020
    },
    {
      "epoch": 0.07629629629629629,
      "grad_norm": 0.08419875055551529,
      "learning_rate": 4.80925925925926e-05,
      "loss": 0.0021,
      "step": 1030
    },
    {
      "epoch": 0.07703703703703704,
      "grad_norm": 0.04023372381925583,
      "learning_rate": 4.807407407407408e-05,
      "loss": 0.003,
      "step": 1040
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 0.11748348921537399,
      "learning_rate": 4.805555555555556e-05,
      "loss": 0.0029,
      "step": 1050
    },
    {
      "epoch": 0.07851851851851852,
      "grad_norm": 0.04987208545207977,
      "learning_rate": 4.803703703703704e-05,
      "loss": 0.0014,
      "step": 1060
    },
    {
      "epoch": 0.07925925925925927,
      "grad_norm": 0.08689255267381668,
      "learning_rate": 4.801851851851852e-05,
      "loss": 0.0025,
      "step": 1070
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.33834025263786316,
      "learning_rate": 4.8e-05,
      "loss": 0.0025,
      "step": 1080
    },
    {
      "epoch": 0.08074074074074074,
      "grad_norm": 0.17585840821266174,
      "learning_rate": 4.798148148148148e-05,
      "loss": 0.0031,
      "step": 1090
    },
    {
      "epoch": 0.08148148148148149,
      "grad_norm": 0.16360528767108917,
      "learning_rate": 4.796296296296296e-05,
      "loss": 0.003,
      "step": 1100
    },
    {
      "epoch": 0.08222222222222222,
      "grad_norm": 0.07497485727071762,
      "learning_rate": 4.794444444444445e-05,
      "loss": 0.0039,
      "step": 1110
    },
    {
      "epoch": 0.08296296296296296,
      "grad_norm": 0.051666259765625,
      "learning_rate": 4.792592592592593e-05,
      "loss": 0.0029,
      "step": 1120
    },
    {
      "epoch": 0.0837037037037037,
      "grad_norm": 0.1369030922651291,
      "learning_rate": 4.790740740740741e-05,
      "loss": 0.0028,
      "step": 1130
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 0.043797336518764496,
      "learning_rate": 4.7888888888888886e-05,
      "loss": 0.0019,
      "step": 1140
    },
    {
      "epoch": 0.08518518518518518,
      "grad_norm": 0.13334165513515472,
      "learning_rate": 4.7870370370370373e-05,
      "loss": 0.0023,
      "step": 1150
    },
    {
      "epoch": 0.08592592592592592,
      "grad_norm": 0.12127745151519775,
      "learning_rate": 4.7851851851851854e-05,
      "loss": 0.0035,
      "step": 1160
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.1763235479593277,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0028,
      "step": 1170
    },
    {
      "epoch": 0.0874074074074074,
      "grad_norm": 0.11939484626054764,
      "learning_rate": 4.7814814814814816e-05,
      "loss": 0.0026,
      "step": 1180
    },
    {
      "epoch": 0.08814814814814814,
      "grad_norm": 0.04600785672664642,
      "learning_rate": 4.77962962962963e-05,
      "loss": 0.0035,
      "step": 1190
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.04660511016845703,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 0.0029,
      "step": 1200
    },
    {
      "epoch": 0.08962962962962963,
      "grad_norm": 0.12427963316440582,
      "learning_rate": 4.775925925925926e-05,
      "loss": 0.0024,
      "step": 1210
    },
    {
      "epoch": 0.09037037037037036,
      "grad_norm": 0.09602472931146622,
      "learning_rate": 4.774074074074074e-05,
      "loss": 0.0032,
      "step": 1220
    },
    {
      "epoch": 0.09111111111111111,
      "grad_norm": 0.0893794447183609,
      "learning_rate": 4.7722222222222226e-05,
      "loss": 0.0017,
      "step": 1230
    },
    {
      "epoch": 0.09185185185185185,
      "grad_norm": 0.039619080722332,
      "learning_rate": 4.770370370370371e-05,
      "loss": 0.0024,
      "step": 1240
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 0.05189099907875061,
      "learning_rate": 4.768518518518519e-05,
      "loss": 0.0026,
      "step": 1250
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.08504381030797958,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0031,
      "step": 1260
    },
    {
      "epoch": 0.09407407407407407,
      "grad_norm": 0.10898146033287048,
      "learning_rate": 4.764814814814815e-05,
      "loss": 0.0019,
      "step": 1270
    },
    {
      "epoch": 0.09481481481481481,
      "grad_norm": 0.09397821873426437,
      "learning_rate": 4.762962962962963e-05,
      "loss": 0.0028,
      "step": 1280
    },
    {
      "epoch": 0.09555555555555556,
      "grad_norm": 0.05934514105319977,
      "learning_rate": 4.761111111111111e-05,
      "loss": 0.0021,
      "step": 1290
    },
    {
      "epoch": 0.0962962962962963,
      "grad_norm": 0.054852455854415894,
      "learning_rate": 4.759259259259259e-05,
      "loss": 0.0021,
      "step": 1300
    },
    {
      "epoch": 0.09703703703703703,
      "grad_norm": 0.06985477358102798,
      "learning_rate": 4.757407407407408e-05,
      "loss": 0.0024,
      "step": 1310
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 0.05390821397304535,
      "learning_rate": 4.755555555555556e-05,
      "loss": 0.0015,
      "step": 1320
    },
    {
      "epoch": 0.09851851851851852,
      "grad_norm": 0.21776169538497925,
      "learning_rate": 4.753703703703704e-05,
      "loss": 0.0022,
      "step": 1330
    },
    {
      "epoch": 0.09925925925925926,
      "grad_norm": 0.08571924269199371,
      "learning_rate": 4.751851851851852e-05,
      "loss": 0.0028,
      "step": 1340
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.11423932760953903,
      "learning_rate": 4.75e-05,
      "loss": 0.0025,
      "step": 1350
    },
    {
      "epoch": 0.10074074074074074,
      "grad_norm": 0.05990113690495491,
      "learning_rate": 4.7481481481481483e-05,
      "loss": 0.0021,
      "step": 1360
    },
    {
      "epoch": 0.10148148148148148,
      "grad_norm": 0.20790782570838928,
      "learning_rate": 4.7462962962962964e-05,
      "loss": 0.0032,
      "step": 1370
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 0.039220601320266724,
      "learning_rate": 4.7444444444444445e-05,
      "loss": 0.0027,
      "step": 1380
    },
    {
      "epoch": 0.10296296296296296,
      "grad_norm": 0.12980221211910248,
      "learning_rate": 4.742592592592593e-05,
      "loss": 0.003,
      "step": 1390
    },
    {
      "epoch": 0.1037037037037037,
      "grad_norm": 0.0329231396317482,
      "learning_rate": 4.740740740740741e-05,
      "loss": 0.0027,
      "step": 1400
    },
    {
      "epoch": 0.10444444444444445,
      "grad_norm": 0.04328430816531181,
      "learning_rate": 4.7388888888888894e-05,
      "loss": 0.0035,
      "step": 1410
    },
    {
      "epoch": 0.10518518518518519,
      "grad_norm": 0.10480746626853943,
      "learning_rate": 4.737037037037037e-05,
      "loss": 0.0023,
      "step": 1420
    },
    {
      "epoch": 0.10592592592592592,
      "grad_norm": 0.08011861145496368,
      "learning_rate": 4.7351851851851856e-05,
      "loss": 0.0028,
      "step": 1430
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.060676209628582,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0027,
      "step": 1440
    },
    {
      "epoch": 0.10740740740740741,
      "grad_norm": 0.08044400066137314,
      "learning_rate": 4.731481481481482e-05,
      "loss": 0.0026,
      "step": 1450
    },
    {
      "epoch": 0.10814814814814815,
      "grad_norm": 0.09229402989149094,
      "learning_rate": 4.72962962962963e-05,
      "loss": 0.0042,
      "step": 1460
    },
    {
      "epoch": 0.10888888888888888,
      "grad_norm": 0.07942812144756317,
      "learning_rate": 4.727777777777778e-05,
      "loss": 0.0033,
      "step": 1470
    },
    {
      "epoch": 0.10962962962962963,
      "grad_norm": 0.19520795345306396,
      "learning_rate": 4.7259259259259266e-05,
      "loss": 0.0035,
      "step": 1480
    },
    {
      "epoch": 0.11037037037037037,
      "grad_norm": 0.09530926495790482,
      "learning_rate": 4.724074074074074e-05,
      "loss": 0.0027,
      "step": 1490
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.041531600058078766,
      "learning_rate": 4.722222222222222e-05,
      "loss": 0.0022,
      "step": 1500
    },
    {
      "epoch": 0.11185185185185186,
      "grad_norm": 0.09354711323976517,
      "learning_rate": 4.720370370370371e-05,
      "loss": 0.0031,
      "step": 1510
    },
    {
      "epoch": 0.11259259259259259,
      "grad_norm": 0.015249049291014671,
      "learning_rate": 4.718518518518519e-05,
      "loss": 0.0021,
      "step": 1520
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.14057371020317078,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0016,
      "step": 1530
    },
    {
      "epoch": 0.11407407407407408,
      "grad_norm": 0.06204140931367874,
      "learning_rate": 4.714814814814815e-05,
      "loss": 0.003,
      "step": 1540
    },
    {
      "epoch": 0.11481481481481481,
      "grad_norm": 0.056193962693214417,
      "learning_rate": 4.712962962962963e-05,
      "loss": 0.0029,
      "step": 1550
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 0.14054705202579498,
      "learning_rate": 4.711111111111111e-05,
      "loss": 0.0028,
      "step": 1560
    },
    {
      "epoch": 0.1162962962962963,
      "grad_norm": 0.049636173993349075,
      "learning_rate": 4.709259259259259e-05,
      "loss": 0.0029,
      "step": 1570
    },
    {
      "epoch": 0.11703703703703704,
      "grad_norm": 0.08518318831920624,
      "learning_rate": 4.7074074074074074e-05,
      "loss": 0.0034,
      "step": 1580
    },
    {
      "epoch": 0.11777777777777777,
      "grad_norm": 0.052974212914705276,
      "learning_rate": 4.7055555555555555e-05,
      "loss": 0.0024,
      "step": 1590
    },
    {
      "epoch": 0.11851851851851852,
      "grad_norm": 0.04647089168429375,
      "learning_rate": 4.703703703703704e-05,
      "loss": 0.002,
      "step": 1600
    },
    {
      "epoch": 0.11925925925925926,
      "grad_norm": 0.024117663502693176,
      "learning_rate": 4.701851851851852e-05,
      "loss": 0.003,
      "step": 1610
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.029840581119060516,
      "learning_rate": 4.7e-05,
      "loss": 0.0015,
      "step": 1620
    },
    {
      "epoch": 0.12074074074074075,
      "grad_norm": 0.06063346937298775,
      "learning_rate": 4.6981481481481485e-05,
      "loss": 0.0025,
      "step": 1630
    },
    {
      "epoch": 0.12148148148148148,
      "grad_norm": 0.04310402274131775,
      "learning_rate": 4.6962962962962966e-05,
      "loss": 0.0024,
      "step": 1640
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 0.07492917776107788,
      "learning_rate": 4.6944444444444446e-05,
      "loss": 0.0034,
      "step": 1650
    },
    {
      "epoch": 0.12296296296296297,
      "grad_norm": 0.05073215439915657,
      "learning_rate": 4.692592592592593e-05,
      "loss": 0.0027,
      "step": 1660
    },
    {
      "epoch": 0.1237037037037037,
      "grad_norm": 0.09678127616643906,
      "learning_rate": 4.690740740740741e-05,
      "loss": 0.0033,
      "step": 1670
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 0.08574672788381577,
      "learning_rate": 4.6888888888888895e-05,
      "loss": 0.0018,
      "step": 1680
    },
    {
      "epoch": 0.12518518518518518,
      "grad_norm": 0.11848647147417068,
      "learning_rate": 4.687037037037037e-05,
      "loss": 0.0025,
      "step": 1690
    },
    {
      "epoch": 0.1259259259259259,
      "grad_norm": 0.1243029311299324,
      "learning_rate": 4.685185185185185e-05,
      "loss": 0.0021,
      "step": 1700
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.11475828289985657,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0029,
      "step": 1710
    },
    {
      "epoch": 0.1274074074074074,
      "grad_norm": 0.13017776608467102,
      "learning_rate": 4.681481481481482e-05,
      "loss": 0.0023,
      "step": 1720
    },
    {
      "epoch": 0.12814814814814815,
      "grad_norm": 0.10495833307504654,
      "learning_rate": 4.67962962962963e-05,
      "loss": 0.0015,
      "step": 1730
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 0.17055676877498627,
      "learning_rate": 4.677777777777778e-05,
      "loss": 0.0026,
      "step": 1740
    },
    {
      "epoch": 0.12962962962962962,
      "grad_norm": 0.20574504137039185,
      "learning_rate": 4.675925925925926e-05,
      "loss": 0.0027,
      "step": 1750
    },
    {
      "epoch": 0.13037037037037036,
      "grad_norm": 0.0402553416788578,
      "learning_rate": 4.674074074074074e-05,
      "loss": 0.0021,
      "step": 1760
    },
    {
      "epoch": 0.13111111111111112,
      "grad_norm": 0.06339021027088165,
      "learning_rate": 4.672222222222222e-05,
      "loss": 0.0016,
      "step": 1770
    },
    {
      "epoch": 0.13185185185185186,
      "grad_norm": 0.09006064385175705,
      "learning_rate": 4.67037037037037e-05,
      "loss": 0.0028,
      "step": 1780
    },
    {
      "epoch": 0.1325925925925926,
      "grad_norm": 0.0899997130036354,
      "learning_rate": 4.6685185185185184e-05,
      "loss": 0.0023,
      "step": 1790
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.18426552414894104,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0028,
      "step": 1800
    },
    {
      "epoch": 0.13407407407407407,
      "grad_norm": 0.046257857233285904,
      "learning_rate": 4.664814814814815e-05,
      "loss": 0.0033,
      "step": 1810
    },
    {
      "epoch": 0.1348148148148148,
      "grad_norm": 0.0,
      "learning_rate": 4.662962962962963e-05,
      "loss": 0.0022,
      "step": 1820
    },
    {
      "epoch": 0.13555555555555557,
      "grad_norm": 0.07181093841791153,
      "learning_rate": 4.6611111111111114e-05,
      "loss": 0.0036,
      "step": 1830
    },
    {
      "epoch": 0.1362962962962963,
      "grad_norm": 0.30466726422309875,
      "learning_rate": 4.6592592592592595e-05,
      "loss": 0.0035,
      "step": 1840
    },
    {
      "epoch": 0.13703703703703704,
      "grad_norm": 0.1248699203133583,
      "learning_rate": 4.6574074074074076e-05,
      "loss": 0.0035,
      "step": 1850
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 0.12764529883861542,
      "learning_rate": 4.6555555555555556e-05,
      "loss": 0.0031,
      "step": 1860
    },
    {
      "epoch": 0.1385185185185185,
      "grad_norm": 0.04738713428378105,
      "learning_rate": 4.653703703703704e-05,
      "loss": 0.0028,
      "step": 1870
    },
    {
      "epoch": 0.13925925925925925,
      "grad_norm": 0.225871741771698,
      "learning_rate": 4.6518518518518525e-05,
      "loss": 0.0022,
      "step": 1880
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.05052432045340538,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0021,
      "step": 1890
    },
    {
      "epoch": 0.14074074074074075,
      "grad_norm": 0.12702716886997223,
      "learning_rate": 4.648148148148148e-05,
      "loss": 0.0025,
      "step": 1900
    },
    {
      "epoch": 0.14148148148148149,
      "grad_norm": 0.1252049207687378,
      "learning_rate": 4.646296296296297e-05,
      "loss": 0.0027,
      "step": 1910
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 0.07390584796667099,
      "learning_rate": 4.644444444444445e-05,
      "loss": 0.0029,
      "step": 1920
    },
    {
      "epoch": 0.14296296296296296,
      "grad_norm": 0.09269377589225769,
      "learning_rate": 4.642592592592593e-05,
      "loss": 0.0031,
      "step": 1930
    },
    {
      "epoch": 0.1437037037037037,
      "grad_norm": 0.16937845945358276,
      "learning_rate": 4.640740740740741e-05,
      "loss": 0.0033,
      "step": 1940
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 0.07106626778841019,
      "learning_rate": 4.638888888888889e-05,
      "loss": 0.002,
      "step": 1950
    },
    {
      "epoch": 0.1451851851851852,
      "grad_norm": 0.16860564053058624,
      "learning_rate": 4.637037037037038e-05,
      "loss": 0.0027,
      "step": 1960
    },
    {
      "epoch": 0.14592592592592593,
      "grad_norm": 0.0,
      "learning_rate": 4.635185185185185e-05,
      "loss": 0.0015,
      "step": 1970
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.08721191436052322,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0033,
      "step": 1980
    },
    {
      "epoch": 0.1474074074074074,
      "grad_norm": 0.12418351322412491,
      "learning_rate": 4.631481481481481e-05,
      "loss": 0.002,
      "step": 1990
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 0.018729347735643387,
      "learning_rate": 4.62962962962963e-05,
      "loss": 0.0028,
      "step": 2000
    },
    {
      "epoch": 0.14888888888888888,
      "grad_norm": 0.0,
      "learning_rate": 4.627777777777778e-05,
      "loss": 0.0026,
      "step": 2010
    },
    {
      "epoch": 0.14962962962962964,
      "grad_norm": 0.04302620515227318,
      "learning_rate": 4.625925925925926e-05,
      "loss": 0.0012,
      "step": 2020
    },
    {
      "epoch": 0.15037037037037038,
      "grad_norm": 0.10289425402879715,
      "learning_rate": 4.624074074074074e-05,
      "loss": 0.0014,
      "step": 2030
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 0.14385299384593964,
      "learning_rate": 4.6222222222222224e-05,
      "loss": 0.0034,
      "step": 2040
    },
    {
      "epoch": 0.15185185185185185,
      "grad_norm": 0.057309411466121674,
      "learning_rate": 4.6203703703703705e-05,
      "loss": 0.0021,
      "step": 2050
    },
    {
      "epoch": 0.15259259259259259,
      "grad_norm": 0.04774174094200134,
      "learning_rate": 4.6185185185185185e-05,
      "loss": 0.0031,
      "step": 2060
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.16572584211826324,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0027,
      "step": 2070
    },
    {
      "epoch": 0.15407407407407409,
      "grad_norm": 0.09932391345500946,
      "learning_rate": 4.6148148148148154e-05,
      "loss": 0.0023,
      "step": 2080
    },
    {
      "epoch": 0.15481481481481482,
      "grad_norm": 0.1719670295715332,
      "learning_rate": 4.6129629629629635e-05,
      "loss": 0.0024,
      "step": 2090
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.09178222715854645,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 0.0025,
      "step": 2100
    },
    {
      "epoch": 0.1562962962962963,
      "grad_norm": 0.06795790046453476,
      "learning_rate": 4.6092592592592596e-05,
      "loss": 0.0031,
      "step": 2110
    },
    {
      "epoch": 0.15703703703703703,
      "grad_norm": 0.08878733217716217,
      "learning_rate": 4.607407407407408e-05,
      "loss": 0.0033,
      "step": 2120
    },
    {
      "epoch": 0.15777777777777777,
      "grad_norm": 0.10163723677396774,
      "learning_rate": 4.605555555555556e-05,
      "loss": 0.0039,
      "step": 2130
    },
    {
      "epoch": 0.15851851851851853,
      "grad_norm": 0.10791518539190292,
      "learning_rate": 4.603703703703704e-05,
      "loss": 0.0016,
      "step": 2140
    },
    {
      "epoch": 0.15925925925925927,
      "grad_norm": 0.08118367195129395,
      "learning_rate": 4.601851851851852e-05,
      "loss": 0.0028,
      "step": 2150
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.06833569705486298,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0015,
      "step": 2160
    },
    {
      "epoch": 0.16074074074074074,
      "grad_norm": 0.04220752418041229,
      "learning_rate": 4.598148148148148e-05,
      "loss": 0.0024,
      "step": 2170
    },
    {
      "epoch": 0.16148148148148148,
      "grad_norm": 0.05921061336994171,
      "learning_rate": 4.596296296296296e-05,
      "loss": 0.0021,
      "step": 2180
    },
    {
      "epoch": 0.1622222222222222,
      "grad_norm": 0.09542216360569,
      "learning_rate": 4.594444444444444e-05,
      "loss": 0.0026,
      "step": 2190
    },
    {
      "epoch": 0.16296296296296298,
      "grad_norm": 0.1326884627342224,
      "learning_rate": 4.592592592592593e-05,
      "loss": 0.0024,
      "step": 2200
    },
    {
      "epoch": 0.1637037037037037,
      "grad_norm": 0.043841488659381866,
      "learning_rate": 4.590740740740741e-05,
      "loss": 0.0029,
      "step": 2210
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 0.07517928630113602,
      "learning_rate": 4.588888888888889e-05,
      "loss": 0.0019,
      "step": 2220
    },
    {
      "epoch": 0.16518518518518518,
      "grad_norm": 0.0,
      "learning_rate": 4.587037037037037e-05,
      "loss": 0.0026,
      "step": 2230
    },
    {
      "epoch": 0.16592592592592592,
      "grad_norm": 0.06509502977132797,
      "learning_rate": 4.585185185185185e-05,
      "loss": 0.0026,
      "step": 2240
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.08695656061172485,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0024,
      "step": 2250
    },
    {
      "epoch": 0.1674074074074074,
      "grad_norm": 0.0379226878285408,
      "learning_rate": 4.5814814814814815e-05,
      "loss": 0.0023,
      "step": 2260
    },
    {
      "epoch": 0.16814814814814816,
      "grad_norm": 0.11937090009450912,
      "learning_rate": 4.5796296296296295e-05,
      "loss": 0.0024,
      "step": 2270
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 0.07658923417329788,
      "learning_rate": 4.577777777777778e-05,
      "loss": 0.0026,
      "step": 2280
    },
    {
      "epoch": 0.16962962962962963,
      "grad_norm": 0.16190187633037567,
      "learning_rate": 4.5759259259259264e-05,
      "loss": 0.0027,
      "step": 2290
    },
    {
      "epoch": 0.17037037037037037,
      "grad_norm": 0.10430192202329636,
      "learning_rate": 4.5740740740740745e-05,
      "loss": 0.0027,
      "step": 2300
    },
    {
      "epoch": 0.1711111111111111,
      "grad_norm": 0.1470772922039032,
      "learning_rate": 4.572222222222222e-05,
      "loss": 0.0032,
      "step": 2310
    },
    {
      "epoch": 0.17185185185185184,
      "grad_norm": 0.08243715018033981,
      "learning_rate": 4.5703703703703706e-05,
      "loss": 0.0038,
      "step": 2320
    },
    {
      "epoch": 0.1725925925925926,
      "grad_norm": 0.031089473515748978,
      "learning_rate": 4.568518518518519e-05,
      "loss": 0.0025,
      "step": 2330
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.04702726751565933,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0015,
      "step": 2340
    },
    {
      "epoch": 0.17407407407407408,
      "grad_norm": 0.030323052778840065,
      "learning_rate": 4.564814814814815e-05,
      "loss": 0.002,
      "step": 2350
    },
    {
      "epoch": 0.1748148148148148,
      "grad_norm": 0.14919482171535492,
      "learning_rate": 4.5629629629629636e-05,
      "loss": 0.0032,
      "step": 2360
    },
    {
      "epoch": 0.17555555555555555,
      "grad_norm": 0.04358111694455147,
      "learning_rate": 4.561111111111112e-05,
      "loss": 0.0025,
      "step": 2370
    },
    {
      "epoch": 0.17629629629629628,
      "grad_norm": 0.039030853658914566,
      "learning_rate": 4.559259259259259e-05,
      "loss": 0.0024,
      "step": 2380
    },
    {
      "epoch": 0.17703703703703705,
      "grad_norm": 0.07857657968997955,
      "learning_rate": 4.557407407407407e-05,
      "loss": 0.002,
      "step": 2390
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.02774251438677311,
      "learning_rate": 4.555555555555556e-05,
      "loss": 0.0028,
      "step": 2400
    },
    {
      "epoch": 0.17851851851851852,
      "grad_norm": 0.24266879260540009,
      "learning_rate": 4.553703703703704e-05,
      "loss": 0.0029,
      "step": 2410
    },
    {
      "epoch": 0.17925925925925926,
      "grad_norm": 0.0845075398683548,
      "learning_rate": 4.551851851851852e-05,
      "loss": 0.003,
      "step": 2420
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.20624694228172302,
      "learning_rate": 4.55e-05,
      "loss": 0.0025,
      "step": 2430
    },
    {
      "epoch": 0.18074074074074073,
      "grad_norm": 0.06591681391000748,
      "learning_rate": 4.548148148148149e-05,
      "loss": 0.0026,
      "step": 2440
    },
    {
      "epoch": 0.1814814814814815,
      "grad_norm": 0.0,
      "learning_rate": 4.546296296296296e-05,
      "loss": 0.0024,
      "step": 2450
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 0.05589396879076958,
      "learning_rate": 4.5444444444444444e-05,
      "loss": 0.0029,
      "step": 2460
    },
    {
      "epoch": 0.18296296296296297,
      "grad_norm": 0.08835284411907196,
      "learning_rate": 4.5425925925925925e-05,
      "loss": 0.0025,
      "step": 2470
    },
    {
      "epoch": 0.1837037037037037,
      "grad_norm": 0.1321207731962204,
      "learning_rate": 4.540740740740741e-05,
      "loss": 0.0023,
      "step": 2480
    },
    {
      "epoch": 0.18444444444444444,
      "grad_norm": 0.03897181525826454,
      "learning_rate": 4.538888888888889e-05,
      "loss": 0.0025,
      "step": 2490
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 0.026319993659853935,
      "learning_rate": 4.5370370370370374e-05,
      "loss": 0.0019,
      "step": 2500
    },
    {
      "epoch": 0.18592592592592594,
      "grad_norm": 0.08492919057607651,
      "learning_rate": 4.5351851851851854e-05,
      "loss": 0.0025,
      "step": 2510
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.05173380672931671,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0024,
      "step": 2520
    },
    {
      "epoch": 0.1874074074074074,
      "grad_norm": 0.06271718442440033,
      "learning_rate": 4.5314814814814816e-05,
      "loss": 0.0018,
      "step": 2530
    },
    {
      "epoch": 0.18814814814814815,
      "grad_norm": 0.01873413659632206,
      "learning_rate": 4.52962962962963e-05,
      "loss": 0.0019,
      "step": 2540
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.06312686204910278,
      "learning_rate": 4.527777777777778e-05,
      "loss": 0.0036,
      "step": 2550
    },
    {
      "epoch": 0.18962962962962962,
      "grad_norm": 0.043109357357025146,
      "learning_rate": 4.5259259259259265e-05,
      "loss": 0.0025,
      "step": 2560
    },
    {
      "epoch": 0.19037037037037038,
      "grad_norm": 0.14468050003051758,
      "learning_rate": 4.5240740740740746e-05,
      "loss": 0.0022,
      "step": 2570
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 0.2815225124359131,
      "learning_rate": 4.522222222222223e-05,
      "loss": 0.0025,
      "step": 2580
    },
    {
      "epoch": 0.19185185185185186,
      "grad_norm": 0.04966111481189728,
      "learning_rate": 4.52037037037037e-05,
      "loss": 0.0017,
      "step": 2590
    },
    {
      "epoch": 0.1925925925925926,
      "grad_norm": 0.15960180759429932,
      "learning_rate": 4.518518518518519e-05,
      "loss": 0.0023,
      "step": 2600
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.09731166064739227,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0026,
      "step": 2610
    },
    {
      "epoch": 0.19407407407407407,
      "grad_norm": 0.04160312935709953,
      "learning_rate": 4.514814814814815e-05,
      "loss": 0.0028,
      "step": 2620
    },
    {
      "epoch": 0.1948148148148148,
      "grad_norm": 0.0961521565914154,
      "learning_rate": 4.512962962962963e-05,
      "loss": 0.0029,
      "step": 2630
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 0.073761485517025,
      "learning_rate": 4.511111111111112e-05,
      "loss": 0.0022,
      "step": 2640
    },
    {
      "epoch": 0.1962962962962963,
      "grad_norm": 0.10250090807676315,
      "learning_rate": 4.50925925925926e-05,
      "loss": 0.0017,
      "step": 2650
    },
    {
      "epoch": 0.19703703703703704,
      "grad_norm": 0.07291768491268158,
      "learning_rate": 4.507407407407407e-05,
      "loss": 0.0023,
      "step": 2660
    },
    {
      "epoch": 0.19777777777777777,
      "grad_norm": 0.09396142512559891,
      "learning_rate": 4.5055555555555554e-05,
      "loss": 0.003,
      "step": 2670
    },
    {
      "epoch": 0.1985185185185185,
      "grad_norm": 0.05370761826634407,
      "learning_rate": 4.503703703703704e-05,
      "loss": 0.0022,
      "step": 2680
    },
    {
      "epoch": 0.19925925925925925,
      "grad_norm": 0.08939486742019653,
      "learning_rate": 4.501851851851852e-05,
      "loss": 0.003,
      "step": 2690
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.0,
      "learning_rate": 4.5e-05,
      "loss": 0.0024,
      "step": 2700
    },
    {
      "epoch": 0.20074074074074075,
      "grad_norm": 0.0406847819685936,
      "learning_rate": 4.4981481481481484e-05,
      "loss": 0.0027,
      "step": 2710
    },
    {
      "epoch": 0.20148148148148148,
      "grad_norm": 0.09373480826616287,
      "learning_rate": 4.496296296296297e-05,
      "loss": 0.0033,
      "step": 2720
    },
    {
      "epoch": 0.20222222222222222,
      "grad_norm": 0.05767297372221947,
      "learning_rate": 4.4944444444444445e-05,
      "loss": 0.0016,
      "step": 2730
    },
    {
      "epoch": 0.20296296296296296,
      "grad_norm": 0.03551909327507019,
      "learning_rate": 4.4925925925925926e-05,
      "loss": 0.0025,
      "step": 2740
    },
    {
      "epoch": 0.2037037037037037,
      "grad_norm": 0.06357701867818832,
      "learning_rate": 4.490740740740741e-05,
      "loss": 0.0028,
      "step": 2750
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 0.06669807434082031,
      "learning_rate": 4.4888888888888894e-05,
      "loss": 0.0028,
      "step": 2760
    },
    {
      "epoch": 0.2051851851851852,
      "grad_norm": 0.058783482760190964,
      "learning_rate": 4.4870370370370375e-05,
      "loss": 0.002,
      "step": 2770
    },
    {
      "epoch": 0.20592592592592593,
      "grad_norm": 0.17484630644321442,
      "learning_rate": 4.4851851851851856e-05,
      "loss": 0.0021,
      "step": 2780
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.09161616861820221,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0027,
      "step": 2790
    },
    {
      "epoch": 0.2074074074074074,
      "grad_norm": 0.04817768186330795,
      "learning_rate": 4.481481481481482e-05,
      "loss": 0.0026,
      "step": 2800
    },
    {
      "epoch": 0.20814814814814814,
      "grad_norm": 0.06459225714206696,
      "learning_rate": 4.47962962962963e-05,
      "loss": 0.0028,
      "step": 2810
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 0.09652663767337799,
      "learning_rate": 4.477777777777778e-05,
      "loss": 0.0024,
      "step": 2820
    },
    {
      "epoch": 0.20962962962962964,
      "grad_norm": 0.07945074886083603,
      "learning_rate": 4.475925925925926e-05,
      "loss": 0.0025,
      "step": 2830
    },
    {
      "epoch": 0.21037037037037037,
      "grad_norm": 0.06406951695680618,
      "learning_rate": 4.474074074074075e-05,
      "loss": 0.0022,
      "step": 2840
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 0.05651864781975746,
      "learning_rate": 4.472222222222223e-05,
      "loss": 0.0027,
      "step": 2850
    },
    {
      "epoch": 0.21185185185185185,
      "grad_norm": 0.04418627545237541,
      "learning_rate": 4.47037037037037e-05,
      "loss": 0.0028,
      "step": 2860
    },
    {
      "epoch": 0.21259259259259258,
      "grad_norm": 0.10371290892362595,
      "learning_rate": 4.468518518518518e-05,
      "loss": 0.0033,
      "step": 2870
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.18449987471103668,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0023,
      "step": 2880
    },
    {
      "epoch": 0.21407407407407408,
      "grad_norm": 0.11588773131370544,
      "learning_rate": 4.464814814814815e-05,
      "loss": 0.0022,
      "step": 2890
    },
    {
      "epoch": 0.21481481481481482,
      "grad_norm": 0.048278868198394775,
      "learning_rate": 4.462962962962963e-05,
      "loss": 0.0031,
      "step": 2900
    },
    {
      "epoch": 0.21555555555555556,
      "grad_norm": 0.09085285663604736,
      "learning_rate": 4.461111111111111e-05,
      "loss": 0.0041,
      "step": 2910
    },
    {
      "epoch": 0.2162962962962963,
      "grad_norm": 0.08275603502988815,
      "learning_rate": 4.4592592592592594e-05,
      "loss": 0.0023,
      "step": 2920
    },
    {
      "epoch": 0.21703703703703703,
      "grad_norm": 0.11517457664012909,
      "learning_rate": 4.4574074074074074e-05,
      "loss": 0.0036,
      "step": 2930
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 0.1230233684182167,
      "learning_rate": 4.4555555555555555e-05,
      "loss": 0.0029,
      "step": 2940
    },
    {
      "epoch": 0.21851851851851853,
      "grad_norm": 0.10213372856378555,
      "learning_rate": 4.4537037037037036e-05,
      "loss": 0.0025,
      "step": 2950
    },
    {
      "epoch": 0.21925925925925926,
      "grad_norm": 0.05413934215903282,
      "learning_rate": 4.4518518518518523e-05,
      "loss": 0.0015,
      "step": 2960
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.0406310074031353,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.0031,
      "step": 2970
    },
    {
      "epoch": 0.22074074074074074,
      "grad_norm": 0.08671161532402039,
      "learning_rate": 4.4481481481481485e-05,
      "loss": 0.0027,
      "step": 2980
    },
    {
      "epoch": 0.22148148148148147,
      "grad_norm": 0.14598199725151062,
      "learning_rate": 4.4462962962962966e-05,
      "loss": 0.0029,
      "step": 2990
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.15449856221675873,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.0028,
      "step": 3000
    },
    {
      "epoch": 0.22296296296296297,
      "grad_norm": 0.13708126544952393,
      "learning_rate": 4.442592592592593e-05,
      "loss": 0.0025,
      "step": 3010
    },
    {
      "epoch": 0.2237037037037037,
      "grad_norm": 0.056534770876169205,
      "learning_rate": 4.440740740740741e-05,
      "loss": 0.003,
      "step": 3020
    },
    {
      "epoch": 0.22444444444444445,
      "grad_norm": 0.06464273482561111,
      "learning_rate": 4.438888888888889e-05,
      "loss": 0.004,
      "step": 3030
    },
    {
      "epoch": 0.22518518518518518,
      "grad_norm": 0.08207279443740845,
      "learning_rate": 4.4370370370370376e-05,
      "loss": 0.0037,
      "step": 3040
    },
    {
      "epoch": 0.22592592592592592,
      "grad_norm": 0.0,
      "learning_rate": 4.435185185185186e-05,
      "loss": 0.0023,
      "step": 3050
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.27404049038887024,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0026,
      "step": 3060
    },
    {
      "epoch": 0.22740740740740742,
      "grad_norm": 0.025012044236063957,
      "learning_rate": 4.431481481481481e-05,
      "loss": 0.0024,
      "step": 3070
    },
    {
      "epoch": 0.22814814814814816,
      "grad_norm": 0.08250413835048676,
      "learning_rate": 4.42962962962963e-05,
      "loss": 0.0012,
      "step": 3080
    },
    {
      "epoch": 0.2288888888888889,
      "grad_norm": 0.04036347568035126,
      "learning_rate": 4.427777777777778e-05,
      "loss": 0.0035,
      "step": 3090
    },
    {
      "epoch": 0.22962962962962963,
      "grad_norm": 0.046617381274700165,
      "learning_rate": 4.425925925925926e-05,
      "loss": 0.0027,
      "step": 3100
    },
    {
      "epoch": 0.23037037037037036,
      "grad_norm": 0.061099883168935776,
      "learning_rate": 4.424074074074074e-05,
      "loss": 0.0034,
      "step": 3110
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.11261987686157227,
      "learning_rate": 4.422222222222222e-05,
      "loss": 0.0019,
      "step": 3120
    },
    {
      "epoch": 0.23185185185185186,
      "grad_norm": 0.06082623824477196,
      "learning_rate": 4.420370370370371e-05,
      "loss": 0.0022,
      "step": 3130
    },
    {
      "epoch": 0.2325925925925926,
      "grad_norm": 0.04675083979964256,
      "learning_rate": 4.4185185185185184e-05,
      "loss": 0.0032,
      "step": 3140
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.058637380599975586,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0021,
      "step": 3150
    },
    {
      "epoch": 0.23407407407407407,
      "grad_norm": 0.14482088387012482,
      "learning_rate": 4.414814814814815e-05,
      "loss": 0.0018,
      "step": 3160
    },
    {
      "epoch": 0.2348148148148148,
      "grad_norm": 0.1818980723619461,
      "learning_rate": 4.4129629629629633e-05,
      "loss": 0.003,
      "step": 3170
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 0.11861071735620499,
      "learning_rate": 4.4111111111111114e-05,
      "loss": 0.0031,
      "step": 3180
    },
    {
      "epoch": 0.2362962962962963,
      "grad_norm": 0.06748911738395691,
      "learning_rate": 4.4092592592592595e-05,
      "loss": 0.002,
      "step": 3190
    },
    {
      "epoch": 0.23703703703703705,
      "grad_norm": 0.05053545534610748,
      "learning_rate": 4.4074074074074076e-05,
      "loss": 0.0027,
      "step": 3200
    },
    {
      "epoch": 0.23777777777777778,
      "grad_norm": 0.08781743049621582,
      "learning_rate": 4.4055555555555557e-05,
      "loss": 0.0029,
      "step": 3210
    },
    {
      "epoch": 0.23851851851851852,
      "grad_norm": 0.0950016975402832,
      "learning_rate": 4.403703703703704e-05,
      "loss": 0.0021,
      "step": 3220
    },
    {
      "epoch": 0.23925925925925925,
      "grad_norm": 0.09154273569583893,
      "learning_rate": 4.401851851851852e-05,
      "loss": 0.0017,
      "step": 3230
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.1330372393131256,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0028,
      "step": 3240
    },
    {
      "epoch": 0.24074074074074073,
      "grad_norm": 0.0,
      "learning_rate": 4.3981481481481486e-05,
      "loss": 0.0029,
      "step": 3250
    },
    {
      "epoch": 0.2414814814814815,
      "grad_norm": 0.0,
      "learning_rate": 4.396296296296297e-05,
      "loss": 0.0021,
      "step": 3260
    },
    {
      "epoch": 0.24222222222222223,
      "grad_norm": 0.04249167442321777,
      "learning_rate": 4.394444444444445e-05,
      "loss": 0.0022,
      "step": 3270
    },
    {
      "epoch": 0.24296296296296296,
      "grad_norm": 0.03848861902952194,
      "learning_rate": 4.392592592592593e-05,
      "loss": 0.0019,
      "step": 3280
    },
    {
      "epoch": 0.2437037037037037,
      "grad_norm": 0.06679625809192657,
      "learning_rate": 4.390740740740741e-05,
      "loss": 0.0024,
      "step": 3290
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.04287354648113251,
      "learning_rate": 4.388888888888889e-05,
      "loss": 0.0019,
      "step": 3300
    },
    {
      "epoch": 0.24518518518518517,
      "grad_norm": 0.23568059504032135,
      "learning_rate": 4.387037037037037e-05,
      "loss": 0.0021,
      "step": 3310
    },
    {
      "epoch": 0.24592592592592594,
      "grad_norm": 0.17786267399787903,
      "learning_rate": 4.385185185185185e-05,
      "loss": 0.0028,
      "step": 3320
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.03549616411328316,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.0026,
      "step": 3330
    },
    {
      "epoch": 0.2474074074074074,
      "grad_norm": 0.09656865149736404,
      "learning_rate": 4.381481481481482e-05,
      "loss": 0.0015,
      "step": 3340
    },
    {
      "epoch": 0.24814814814814815,
      "grad_norm": 0.02719046361744404,
      "learning_rate": 4.3796296296296294e-05,
      "loss": 0.0024,
      "step": 3350
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.05185336619615555,
      "learning_rate": 4.377777777777778e-05,
      "loss": 0.0027,
      "step": 3360
    },
    {
      "epoch": 0.24962962962962962,
      "grad_norm": 0.0619446262717247,
      "learning_rate": 4.375925925925926e-05,
      "loss": 0.0026,
      "step": 3370
    },
    {
      "epoch": 0.25037037037037035,
      "grad_norm": 0.037543125450611115,
      "learning_rate": 4.374074074074074e-05,
      "loss": 0.0017,
      "step": 3380
    },
    {
      "epoch": 0.2511111111111111,
      "grad_norm": 0.10942983627319336,
      "learning_rate": 4.3722222222222224e-05,
      "loss": 0.0023,
      "step": 3390
    },
    {
      "epoch": 0.2518518518518518,
      "grad_norm": 0.04826812818646431,
      "learning_rate": 4.3703703703703705e-05,
      "loss": 0.0021,
      "step": 3400
    },
    {
      "epoch": 0.2525925925925926,
      "grad_norm": 0.10795717686414719,
      "learning_rate": 4.3685185185185186e-05,
      "loss": 0.0018,
      "step": 3410
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.11047771573066711,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0024,
      "step": 3420
    },
    {
      "epoch": 0.25407407407407406,
      "grad_norm": 0.0746849775314331,
      "learning_rate": 4.364814814814815e-05,
      "loss": 0.0021,
      "step": 3430
    },
    {
      "epoch": 0.2548148148148148,
      "grad_norm": 0.013639291748404503,
      "learning_rate": 4.3629629629629635e-05,
      "loss": 0.0029,
      "step": 3440
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 0.14858171343803406,
      "learning_rate": 4.3611111111111116e-05,
      "loss": 0.0021,
      "step": 3450
    },
    {
      "epoch": 0.2562962962962963,
      "grad_norm": 0.046283263713121414,
      "learning_rate": 4.3592592592592596e-05,
      "loss": 0.0015,
      "step": 3460
    },
    {
      "epoch": 0.25703703703703706,
      "grad_norm": 0.18560051918029785,
      "learning_rate": 4.357407407407408e-05,
      "loss": 0.0029,
      "step": 3470
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 0.04638107866048813,
      "learning_rate": 4.355555555555556e-05,
      "loss": 0.0036,
      "step": 3480
    },
    {
      "epoch": 0.25851851851851854,
      "grad_norm": 0.07691789418458939,
      "learning_rate": 4.353703703703704e-05,
      "loss": 0.0025,
      "step": 3490
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 0.049922168254852295,
      "learning_rate": 4.351851851851852e-05,
      "loss": 0.0028,
      "step": 3500
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.02896738611161709,
      "learning_rate": 4.35e-05,
      "loss": 0.0016,
      "step": 3510
    },
    {
      "epoch": 0.2607407407407407,
      "grad_norm": 0.06270558387041092,
      "learning_rate": 4.348148148148148e-05,
      "loss": 0.0014,
      "step": 3520
    },
    {
      "epoch": 0.2614814814814815,
      "grad_norm": 0.050019752234220505,
      "learning_rate": 4.346296296296297e-05,
      "loss": 0.0021,
      "step": 3530
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 0.060563694685697556,
      "learning_rate": 4.344444444444445e-05,
      "loss": 0.0018,
      "step": 3540
    },
    {
      "epoch": 0.26296296296296295,
      "grad_norm": 0.12929502129554749,
      "learning_rate": 4.342592592592592e-05,
      "loss": 0.0018,
      "step": 3550
    },
    {
      "epoch": 0.2637037037037037,
      "grad_norm": 0.04265311360359192,
      "learning_rate": 4.340740740740741e-05,
      "loss": 0.0032,
      "step": 3560
    },
    {
      "epoch": 0.2644444444444444,
      "grad_norm": 0.06247110664844513,
      "learning_rate": 4.338888888888889e-05,
      "loss": 0.0018,
      "step": 3570
    },
    {
      "epoch": 0.2651851851851852,
      "grad_norm": 0.06092532351613045,
      "learning_rate": 4.337037037037037e-05,
      "loss": 0.0022,
      "step": 3580
    },
    {
      "epoch": 0.2659259259259259,
      "grad_norm": 0.046349622309207916,
      "learning_rate": 4.335185185185185e-05,
      "loss": 0.0023,
      "step": 3590
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.2077057957649231,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0028,
      "step": 3600
    },
    {
      "epoch": 0.2674074074074074,
      "grad_norm": 0.18181884288787842,
      "learning_rate": 4.331481481481482e-05,
      "loss": 0.0025,
      "step": 3610
    },
    {
      "epoch": 0.26814814814814814,
      "grad_norm": 0.011289098300039768,
      "learning_rate": 4.3296296296296296e-05,
      "loss": 0.0033,
      "step": 3620
    },
    {
      "epoch": 0.2688888888888889,
      "grad_norm": 0.08275839686393738,
      "learning_rate": 4.3277777777777776e-05,
      "loss": 0.0022,
      "step": 3630
    },
    {
      "epoch": 0.2696296296296296,
      "grad_norm": 0.09638991206884384,
      "learning_rate": 4.325925925925926e-05,
      "loss": 0.0031,
      "step": 3640
    },
    {
      "epoch": 0.27037037037037037,
      "grad_norm": 0.06270086020231247,
      "learning_rate": 4.3240740740740745e-05,
      "loss": 0.0025,
      "step": 3650
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 0.04079771414399147,
      "learning_rate": 4.3222222222222226e-05,
      "loss": 0.0021,
      "step": 3660
    },
    {
      "epoch": 0.27185185185185184,
      "grad_norm": 0.13429827988147736,
      "learning_rate": 4.3203703703703706e-05,
      "loss": 0.0025,
      "step": 3670
    },
    {
      "epoch": 0.2725925925925926,
      "grad_norm": 0.04040931910276413,
      "learning_rate": 4.318518518518519e-05,
      "loss": 0.0016,
      "step": 3680
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0014,
      "step": 3690
    },
    {
      "epoch": 0.2740740740740741,
      "grad_norm": 0.045497242361307144,
      "learning_rate": 4.314814814814815e-05,
      "loss": 0.0013,
      "step": 3700
    },
    {
      "epoch": 0.2748148148148148,
      "grad_norm": 0.0,
      "learning_rate": 4.312962962962963e-05,
      "loss": 0.0019,
      "step": 3710
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 0.054280515760183334,
      "learning_rate": 4.311111111111111e-05,
      "loss": 0.0026,
      "step": 3720
    },
    {
      "epoch": 0.2762962962962963,
      "grad_norm": 0.21256829798221588,
      "learning_rate": 4.30925925925926e-05,
      "loss": 0.0025,
      "step": 3730
    },
    {
      "epoch": 0.277037037037037,
      "grad_norm": 0.11814828217029572,
      "learning_rate": 4.307407407407408e-05,
      "loss": 0.0024,
      "step": 3740
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.0,
      "learning_rate": 4.305555555555556e-05,
      "loss": 0.0016,
      "step": 3750
    },
    {
      "epoch": 0.2785185185185185,
      "grad_norm": 0.08726684749126434,
      "learning_rate": 4.303703703703704e-05,
      "loss": 0.0014,
      "step": 3760
    },
    {
      "epoch": 0.27925925925925926,
      "grad_norm": 0.04246718809008598,
      "learning_rate": 4.301851851851852e-05,
      "loss": 0.0022,
      "step": 3770
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.11046432703733444,
      "learning_rate": 4.3e-05,
      "loss": 0.0025,
      "step": 3780
    },
    {
      "epoch": 0.28074074074074074,
      "grad_norm": 0.04762161150574684,
      "learning_rate": 4.298148148148148e-05,
      "loss": 0.0021,
      "step": 3790
    },
    {
      "epoch": 0.2814814814814815,
      "grad_norm": 0.1141480803489685,
      "learning_rate": 4.296296296296296e-05,
      "loss": 0.0017,
      "step": 3800
    },
    {
      "epoch": 0.2822222222222222,
      "grad_norm": 0.08570105582475662,
      "learning_rate": 4.294444444444445e-05,
      "loss": 0.002,
      "step": 3810
    },
    {
      "epoch": 0.28296296296296297,
      "grad_norm": 0.16680949926376343,
      "learning_rate": 4.292592592592593e-05,
      "loss": 0.0026,
      "step": 3820
    },
    {
      "epoch": 0.2837037037037037,
      "grad_norm": 0.12881483137607574,
      "learning_rate": 4.2907407407407406e-05,
      "loss": 0.0022,
      "step": 3830
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.25590330362319946,
      "learning_rate": 4.2888888888888886e-05,
      "loss": 0.0028,
      "step": 3840
    },
    {
      "epoch": 0.2851851851851852,
      "grad_norm": 0.08821405470371246,
      "learning_rate": 4.2870370370370374e-05,
      "loss": 0.0042,
      "step": 3850
    },
    {
      "epoch": 0.2859259259259259,
      "grad_norm": 0.11034594476222992,
      "learning_rate": 4.2851851851851855e-05,
      "loss": 0.0037,
      "step": 3860
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.04273317754268646,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0019,
      "step": 3870
    },
    {
      "epoch": 0.2874074074074074,
      "grad_norm": 0.08773926645517349,
      "learning_rate": 4.2814814814814816e-05,
      "loss": 0.0021,
      "step": 3880
    },
    {
      "epoch": 0.28814814814814815,
      "grad_norm": 0.078011654317379,
      "learning_rate": 4.2796296296296304e-05,
      "loss": 0.0027,
      "step": 3890
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.17493405938148499,
      "learning_rate": 4.277777777777778e-05,
      "loss": 0.0029,
      "step": 3900
    },
    {
      "epoch": 0.2896296296296296,
      "grad_norm": 0.12335392087697983,
      "learning_rate": 4.275925925925926e-05,
      "loss": 0.0022,
      "step": 3910
    },
    {
      "epoch": 0.2903703703703704,
      "grad_norm": 0.17346051335334778,
      "learning_rate": 4.274074074074074e-05,
      "loss": 0.0022,
      "step": 3920
    },
    {
      "epoch": 0.2911111111111111,
      "grad_norm": 0.08017574995756149,
      "learning_rate": 4.272222222222223e-05,
      "loss": 0.0018,
      "step": 3930
    },
    {
      "epoch": 0.29185185185185186,
      "grad_norm": 0.09269402176141739,
      "learning_rate": 4.270370370370371e-05,
      "loss": 0.0022,
      "step": 3940
    },
    {
      "epoch": 0.29259259259259257,
      "grad_norm": 0.0,
      "learning_rate": 4.268518518518519e-05,
      "loss": 0.0012,
      "step": 3950
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.09187097102403641,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0019,
      "step": 3960
    },
    {
      "epoch": 0.2940740740740741,
      "grad_norm": 0.13489405810832977,
      "learning_rate": 4.264814814814815e-05,
      "loss": 0.0035,
      "step": 3970
    },
    {
      "epoch": 0.2948148148148148,
      "grad_norm": 0.21824222803115845,
      "learning_rate": 4.262962962962963e-05,
      "loss": 0.0025,
      "step": 3980
    },
    {
      "epoch": 0.29555555555555557,
      "grad_norm": 0.0888594314455986,
      "learning_rate": 4.261111111111111e-05,
      "loss": 0.0027,
      "step": 3990
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 0.10514486581087112,
      "learning_rate": 4.259259259259259e-05,
      "loss": 0.003,
      "step": 4000
    },
    {
      "epoch": 0.29703703703703704,
      "grad_norm": 0.062546007335186,
      "learning_rate": 4.257407407407408e-05,
      "loss": 0.0025,
      "step": 4010
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 0.07221278548240662,
      "learning_rate": 4.255555555555556e-05,
      "loss": 0.0017,
      "step": 4020
    },
    {
      "epoch": 0.2985185185185185,
      "grad_norm": 0.09780263155698776,
      "learning_rate": 4.2537037037037035e-05,
      "loss": 0.0028,
      "step": 4030
    },
    {
      "epoch": 0.2992592592592593,
      "grad_norm": 0.03424564376473427,
      "learning_rate": 4.2518518518518515e-05,
      "loss": 0.0022,
      "step": 4040
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.14154690504074097,
      "learning_rate": 4.25e-05,
      "loss": 0.0025,
      "step": 4050
    },
    {
      "epoch": 0.30074074074074075,
      "grad_norm": 0.17107297480106354,
      "learning_rate": 4.2481481481481484e-05,
      "loss": 0.0031,
      "step": 4060
    },
    {
      "epoch": 0.30148148148148146,
      "grad_norm": 0.10392661392688751,
      "learning_rate": 4.2462962962962965e-05,
      "loss": 0.0016,
      "step": 4070
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 0.10685129463672638,
      "learning_rate": 4.2444444444444445e-05,
      "loss": 0.0021,
      "step": 4080
    },
    {
      "epoch": 0.302962962962963,
      "grad_norm": 0.12461866438388824,
      "learning_rate": 4.242592592592593e-05,
      "loss": 0.0038,
      "step": 4090
    },
    {
      "epoch": 0.3037037037037037,
      "grad_norm": 0.04588298499584198,
      "learning_rate": 4.240740740740741e-05,
      "loss": 0.0024,
      "step": 4100
    },
    {
      "epoch": 0.30444444444444446,
      "grad_norm": 0.04318735748529434,
      "learning_rate": 4.238888888888889e-05,
      "loss": 0.0022,
      "step": 4110
    },
    {
      "epoch": 0.30518518518518517,
      "grad_norm": 0.0,
      "learning_rate": 4.237037037037037e-05,
      "loss": 0.0019,
      "step": 4120
    },
    {
      "epoch": 0.30592592592592593,
      "grad_norm": 0.09009382873773575,
      "learning_rate": 4.2351851851851856e-05,
      "loss": 0.0032,
      "step": 4130
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.0413505993783474,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0033,
      "step": 4140
    },
    {
      "epoch": 0.3074074074074074,
      "grad_norm": 0.08912684768438339,
      "learning_rate": 4.231481481481482e-05,
      "loss": 0.0014,
      "step": 4150
    },
    {
      "epoch": 0.30814814814814817,
      "grad_norm": 0.07690800726413727,
      "learning_rate": 4.22962962962963e-05,
      "loss": 0.0021,
      "step": 4160
    },
    {
      "epoch": 0.3088888888888889,
      "grad_norm": 0.14064285159111023,
      "learning_rate": 4.227777777777778e-05,
      "loss": 0.0013,
      "step": 4170
    },
    {
      "epoch": 0.30962962962962964,
      "grad_norm": 0.10333159565925598,
      "learning_rate": 4.225925925925926e-05,
      "loss": 0.0026,
      "step": 4180
    },
    {
      "epoch": 0.31037037037037035,
      "grad_norm": 0.10379035025835037,
      "learning_rate": 4.224074074074074e-05,
      "loss": 0.0032,
      "step": 4190
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.06377566605806351,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.0023,
      "step": 4200
    },
    {
      "epoch": 0.3118518518518518,
      "grad_norm": 0.0,
      "learning_rate": 4.220370370370371e-05,
      "loss": 0.0028,
      "step": 4210
    },
    {
      "epoch": 0.3125925925925926,
      "grad_norm": 0.039422716945409775,
      "learning_rate": 4.218518518518519e-05,
      "loss": 0.0022,
      "step": 4220
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.037563685327768326,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0019,
      "step": 4230
    },
    {
      "epoch": 0.31407407407407406,
      "grad_norm": 0.1649092435836792,
      "learning_rate": 4.2148148148148145e-05,
      "loss": 0.003,
      "step": 4240
    },
    {
      "epoch": 0.3148148148148148,
      "grad_norm": 0.198151633143425,
      "learning_rate": 4.212962962962963e-05,
      "loss": 0.0026,
      "step": 4250
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 0.07581096142530441,
      "learning_rate": 4.211111111111111e-05,
      "loss": 0.0014,
      "step": 4260
    },
    {
      "epoch": 0.3162962962962963,
      "grad_norm": 0.061464834958314896,
      "learning_rate": 4.2092592592592594e-05,
      "loss": 0.003,
      "step": 4270
    },
    {
      "epoch": 0.31703703703703706,
      "grad_norm": 0.0743354856967926,
      "learning_rate": 4.2074074074074075e-05,
      "loss": 0.0017,
      "step": 4280
    },
    {
      "epoch": 0.31777777777777777,
      "grad_norm": 0.07391434162855148,
      "learning_rate": 4.205555555555556e-05,
      "loss": 0.0023,
      "step": 4290
    },
    {
      "epoch": 0.31851851851851853,
      "grad_norm": 0.2169138789176941,
      "learning_rate": 4.203703703703704e-05,
      "loss": 0.0016,
      "step": 4300
    },
    {
      "epoch": 0.31925925925925924,
      "grad_norm": 0.0,
      "learning_rate": 4.201851851851852e-05,
      "loss": 0.0032,
      "step": 4310
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.09638632833957672,
      "learning_rate": 4.2e-05,
      "loss": 0.0031,
      "step": 4320
    },
    {
      "epoch": 0.3207407407407407,
      "grad_norm": 0.13560515642166138,
      "learning_rate": 4.1981481481481485e-05,
      "loss": 0.0028,
      "step": 4330
    },
    {
      "epoch": 0.3214814814814815,
      "grad_norm": 0.04929178208112717,
      "learning_rate": 4.1962962962962966e-05,
      "loss": 0.0024,
      "step": 4340
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 0.04142693057656288,
      "learning_rate": 4.194444444444445e-05,
      "loss": 0.0013,
      "step": 4350
    },
    {
      "epoch": 0.32296296296296295,
      "grad_norm": 0.1428038775920868,
      "learning_rate": 4.192592592592593e-05,
      "loss": 0.0024,
      "step": 4360
    },
    {
      "epoch": 0.3237037037037037,
      "grad_norm": 0.07941544055938721,
      "learning_rate": 4.1907407407407415e-05,
      "loss": 0.0014,
      "step": 4370
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 0.038028448820114136,
      "learning_rate": 4.188888888888889e-05,
      "loss": 0.0016,
      "step": 4380
    },
    {
      "epoch": 0.3251851851851852,
      "grad_norm": 0.0,
      "learning_rate": 4.187037037037037e-05,
      "loss": 0.0014,
      "step": 4390
    },
    {
      "epoch": 0.32592592592592595,
      "grad_norm": 0.12936264276504517,
      "learning_rate": 4.185185185185185e-05,
      "loss": 0.0022,
      "step": 4400
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0008,
      "step": 4410
    },
    {
      "epoch": 0.3274074074074074,
      "grad_norm": 0.0,
      "learning_rate": 4.181481481481482e-05,
      "loss": 0.0029,
      "step": 4420
    },
    {
      "epoch": 0.32814814814814813,
      "grad_norm": 0.13462527096271515,
      "learning_rate": 4.17962962962963e-05,
      "loss": 0.0022,
      "step": 4430
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 0.146444633603096,
      "learning_rate": 4.177777777777778e-05,
      "loss": 0.0008,
      "step": 4440
    },
    {
      "epoch": 0.3296296296296296,
      "grad_norm": 0.0,
      "learning_rate": 4.175925925925926e-05,
      "loss": 0.0015,
      "step": 4450
    },
    {
      "epoch": 0.33037037037037037,
      "grad_norm": 0.058704961091279984,
      "learning_rate": 4.174074074074074e-05,
      "loss": 0.002,
      "step": 4460
    },
    {
      "epoch": 0.33111111111111113,
      "grad_norm": 0.03872029483318329,
      "learning_rate": 4.172222222222222e-05,
      "loss": 0.0013,
      "step": 4470
    },
    {
      "epoch": 0.33185185185185184,
      "grad_norm": 0.08463206887245178,
      "learning_rate": 4.1703703703703704e-05,
      "loss": 0.0016,
      "step": 4480
    },
    {
      "epoch": 0.3325925925925926,
      "grad_norm": 0.12729664146900177,
      "learning_rate": 4.168518518518519e-05,
      "loss": 0.0034,
      "step": 4490
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.04161616042256355,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0015,
      "step": 4500
    },
    {
      "epoch": 0.3340740740740741,
      "grad_norm": 0.0,
      "learning_rate": 4.164814814814815e-05,
      "loss": 0.0019,
      "step": 4510
    },
    {
      "epoch": 0.3348148148148148,
      "grad_norm": 0.07111728191375732,
      "learning_rate": 4.162962962962963e-05,
      "loss": 0.0017,
      "step": 4520
    },
    {
      "epoch": 0.33555555555555555,
      "grad_norm": 0.07856187969446182,
      "learning_rate": 4.1611111111111114e-05,
      "loss": 0.0012,
      "step": 4530
    },
    {
      "epoch": 0.3362962962962963,
      "grad_norm": 0.09752345830202103,
      "learning_rate": 4.1592592592592595e-05,
      "loss": 0.0021,
      "step": 4540
    },
    {
      "epoch": 0.337037037037037,
      "grad_norm": 0.03683239966630936,
      "learning_rate": 4.1574074074074076e-05,
      "loss": 0.0019,
      "step": 4550
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.049548644572496414,
      "learning_rate": 4.155555555555556e-05,
      "loss": 0.0027,
      "step": 4560
    },
    {
      "epoch": 0.3385185185185185,
      "grad_norm": 0.050306227058172226,
      "learning_rate": 4.1537037037037044e-05,
      "loss": 0.0024,
      "step": 4570
    },
    {
      "epoch": 0.33925925925925926,
      "grad_norm": 0.11582869291305542,
      "learning_rate": 4.1518518518518525e-05,
      "loss": 0.0025,
      "step": 4580
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.0,
      "learning_rate": 4.15e-05,
      "loss": 0.0019,
      "step": 4590
    },
    {
      "epoch": 0.34074074074074073,
      "grad_norm": 0.07632101327180862,
      "learning_rate": 4.148148148148148e-05,
      "loss": 0.0026,
      "step": 4600
    },
    {
      "epoch": 0.3414814814814815,
      "grad_norm": 0.20412756502628326,
      "learning_rate": 4.146296296296297e-05,
      "loss": 0.0021,
      "step": 4610
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 0.0977431982755661,
      "learning_rate": 4.144444444444445e-05,
      "loss": 0.0021,
      "step": 4620
    },
    {
      "epoch": 0.34296296296296297,
      "grad_norm": 0.04329560324549675,
      "learning_rate": 4.142592592592593e-05,
      "loss": 0.0018,
      "step": 4630
    },
    {
      "epoch": 0.3437037037037037,
      "grad_norm": 0.1770617812871933,
      "learning_rate": 4.140740740740741e-05,
      "loss": 0.0025,
      "step": 4640
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 0.07326729595661163,
      "learning_rate": 4.138888888888889e-05,
      "loss": 0.0012,
      "step": 4650
    },
    {
      "epoch": 0.3451851851851852,
      "grad_norm": 0.0,
      "learning_rate": 4.137037037037037e-05,
      "loss": 0.0022,
      "step": 4660
    },
    {
      "epoch": 0.3459259259259259,
      "grad_norm": 0.0774490237236023,
      "learning_rate": 4.135185185185185e-05,
      "loss": 0.0016,
      "step": 4670
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.04131457582116127,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0013,
      "step": 4680
    },
    {
      "epoch": 0.3474074074074074,
      "grad_norm": 0.016849281266331673,
      "learning_rate": 4.131481481481482e-05,
      "loss": 0.0027,
      "step": 4690
    },
    {
      "epoch": 0.34814814814814815,
      "grad_norm": 0.11952745169401169,
      "learning_rate": 4.12962962962963e-05,
      "loss": 0.0021,
      "step": 4700
    },
    {
      "epoch": 0.3488888888888889,
      "grad_norm": 0.09077290445566177,
      "learning_rate": 4.127777777777778e-05,
      "loss": 0.0024,
      "step": 4710
    },
    {
      "epoch": 0.3496296296296296,
      "grad_norm": 0.04665263742208481,
      "learning_rate": 4.1259259259259256e-05,
      "loss": 0.0026,
      "step": 4720
    },
    {
      "epoch": 0.3503703703703704,
      "grad_norm": 0.053372573107481,
      "learning_rate": 4.1240740740740744e-05,
      "loss": 0.0017,
      "step": 4730
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 0.11218122392892838,
      "learning_rate": 4.1222222222222224e-05,
      "loss": 0.0027,
      "step": 4740
    },
    {
      "epoch": 0.35185185185185186,
      "grad_norm": 0.05262690782546997,
      "learning_rate": 4.1203703703703705e-05,
      "loss": 0.0024,
      "step": 4750
    },
    {
      "epoch": 0.35259259259259257,
      "grad_norm": 0.10216853767633438,
      "learning_rate": 4.1185185185185186e-05,
      "loss": 0.0023,
      "step": 4760
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.09864062070846558,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0023,
      "step": 4770
    },
    {
      "epoch": 0.3540740740740741,
      "grad_norm": 0.04494526609778404,
      "learning_rate": 4.1148148148148154e-05,
      "loss": 0.0033,
      "step": 4780
    },
    {
      "epoch": 0.3548148148148148,
      "grad_norm": 0.10231714695692062,
      "learning_rate": 4.112962962962963e-05,
      "loss": 0.002,
      "step": 4790
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.11765193194150925,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.0022,
      "step": 4800
    },
    {
      "epoch": 0.3562962962962963,
      "grad_norm": 0.09244869649410248,
      "learning_rate": 4.1092592592592597e-05,
      "loss": 0.0016,
      "step": 4810
    },
    {
      "epoch": 0.35703703703703704,
      "grad_norm": 0.05278419330716133,
      "learning_rate": 4.107407407407408e-05,
      "loss": 0.0014,
      "step": 4820
    },
    {
      "epoch": 0.35777777777777775,
      "grad_norm": 0.12588509917259216,
      "learning_rate": 4.105555555555556e-05,
      "loss": 0.0021,
      "step": 4830
    },
    {
      "epoch": 0.3585185185185185,
      "grad_norm": 0.09701916575431824,
      "learning_rate": 4.103703703703704e-05,
      "loss": 0.0027,
      "step": 4840
    },
    {
      "epoch": 0.3592592592592593,
      "grad_norm": 0.06650646775960922,
      "learning_rate": 4.101851851851852e-05,
      "loss": 0.0022,
      "step": 4850
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.054440200328826904,
      "learning_rate": 4.1e-05,
      "loss": 0.0022,
      "step": 4860
    },
    {
      "epoch": 0.36074074074074075,
      "grad_norm": 0.09133587032556534,
      "learning_rate": 4.098148148148148e-05,
      "loss": 0.0026,
      "step": 4870
    },
    {
      "epoch": 0.36148148148148146,
      "grad_norm": 0.041488777846097946,
      "learning_rate": 4.096296296296296e-05,
      "loss": 0.0025,
      "step": 4880
    },
    {
      "epoch": 0.3622222222222222,
      "grad_norm": 0.10318529605865479,
      "learning_rate": 4.094444444444445e-05,
      "loss": 0.0026,
      "step": 4890
    },
    {
      "epoch": 0.362962962962963,
      "grad_norm": 0.036484044045209885,
      "learning_rate": 4.092592592592593e-05,
      "loss": 0.0022,
      "step": 4900
    },
    {
      "epoch": 0.3637037037037037,
      "grad_norm": 0.04075542464852333,
      "learning_rate": 4.090740740740741e-05,
      "loss": 0.003,
      "step": 4910
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 0.0961461141705513,
      "learning_rate": 4.088888888888889e-05,
      "loss": 0.0018,
      "step": 4920
    },
    {
      "epoch": 0.36518518518518517,
      "grad_norm": 0.12302851676940918,
      "learning_rate": 4.087037037037037e-05,
      "loss": 0.0024,
      "step": 4930
    },
    {
      "epoch": 0.36592592592592593,
      "grad_norm": 0.03889625519514084,
      "learning_rate": 4.0851851851851853e-05,
      "loss": 0.0019,
      "step": 4940
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.05697094649076462,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0017,
      "step": 4950
    },
    {
      "epoch": 0.3674074074074074,
      "grad_norm": 0.11450619250535965,
      "learning_rate": 4.0814814814814815e-05,
      "loss": 0.0034,
      "step": 4960
    },
    {
      "epoch": 0.36814814814814817,
      "grad_norm": 0.1437927633523941,
      "learning_rate": 4.0796296296296296e-05,
      "loss": 0.0019,
      "step": 4970
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 0.05871695280075073,
      "learning_rate": 4.0777777777777783e-05,
      "loss": 0.0034,
      "step": 4980
    },
    {
      "epoch": 0.36962962962962964,
      "grad_norm": 0.12559764087200165,
      "learning_rate": 4.0759259259259264e-05,
      "loss": 0.0021,
      "step": 4990
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 0.03251791372895241,
      "learning_rate": 4.074074074074074e-05,
      "loss": 0.0019,
      "step": 5000
    },
    {
      "epoch": 0.3711111111111111,
      "grad_norm": 0.117031030356884,
      "learning_rate": 4.0722222222222226e-05,
      "loss": 0.002,
      "step": 5010
    },
    {
      "epoch": 0.3718518518518519,
      "grad_norm": 0.04091675952076912,
      "learning_rate": 4.0703703703703707e-05,
      "loss": 0.0022,
      "step": 5020
    },
    {
      "epoch": 0.3725925925925926,
      "grad_norm": 0.09798718988895416,
      "learning_rate": 4.068518518518519e-05,
      "loss": 0.002,
      "step": 5030
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.1861611306667328,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0024,
      "step": 5040
    },
    {
      "epoch": 0.37407407407407406,
      "grad_norm": 0.06644555926322937,
      "learning_rate": 4.064814814814815e-05,
      "loss": 0.0014,
      "step": 5050
    },
    {
      "epoch": 0.3748148148148148,
      "grad_norm": 0.05354584753513336,
      "learning_rate": 4.0629629629629636e-05,
      "loss": 0.0028,
      "step": 5060
    },
    {
      "epoch": 0.37555555555555553,
      "grad_norm": 0.05269743874669075,
      "learning_rate": 4.061111111111111e-05,
      "loss": 0.0015,
      "step": 5070
    },
    {
      "epoch": 0.3762962962962963,
      "grad_norm": 0.07631958276033401,
      "learning_rate": 4.059259259259259e-05,
      "loss": 0.0016,
      "step": 5080
    },
    {
      "epoch": 0.37703703703703706,
      "grad_norm": 0.03106258250772953,
      "learning_rate": 4.057407407407408e-05,
      "loss": 0.0019,
      "step": 5090
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 0.1403704136610031,
      "learning_rate": 4.055555555555556e-05,
      "loss": 0.003,
      "step": 5100
    },
    {
      "epoch": 0.37851851851851853,
      "grad_norm": 0.13095252215862274,
      "learning_rate": 4.053703703703704e-05,
      "loss": 0.0022,
      "step": 5110
    },
    {
      "epoch": 0.37925925925925924,
      "grad_norm": 0.047703251242637634,
      "learning_rate": 4.051851851851852e-05,
      "loss": 0.002,
      "step": 5120
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.07701116800308228,
      "learning_rate": 4.05e-05,
      "loss": 0.0024,
      "step": 5130
    },
    {
      "epoch": 0.38074074074074077,
      "grad_norm": 0.07928872853517532,
      "learning_rate": 4.048148148148148e-05,
      "loss": 0.0027,
      "step": 5140
    },
    {
      "epoch": 0.3814814814814815,
      "grad_norm": 0.04591009393334389,
      "learning_rate": 4.0462962962962963e-05,
      "loss": 0.0025,
      "step": 5150
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 0.07722090184688568,
      "learning_rate": 4.0444444444444444e-05,
      "loss": 0.0031,
      "step": 5160
    },
    {
      "epoch": 0.38296296296296295,
      "grad_norm": 0.112368144094944,
      "learning_rate": 4.0425925925925925e-05,
      "loss": 0.0017,
      "step": 5170
    },
    {
      "epoch": 0.3837037037037037,
      "grad_norm": 0.0,
      "learning_rate": 4.040740740740741e-05,
      "loss": 0.0012,
      "step": 5180
    },
    {
      "epoch": 0.3844444444444444,
      "grad_norm": 0.0,
      "learning_rate": 4.038888888888889e-05,
      "loss": 0.0024,
      "step": 5190
    },
    {
      "epoch": 0.3851851851851852,
      "grad_norm": 0.07815878838300705,
      "learning_rate": 4.0370370370370374e-05,
      "loss": 0.0021,
      "step": 5200
    },
    {
      "epoch": 0.38592592592592595,
      "grad_norm": 0.1006552055478096,
      "learning_rate": 4.0351851851851855e-05,
      "loss": 0.0018,
      "step": 5210
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.10841355472803116,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0012,
      "step": 5220
    },
    {
      "epoch": 0.3874074074074074,
      "grad_norm": 0.14043331146240234,
      "learning_rate": 4.0314814814814816e-05,
      "loss": 0.0024,
      "step": 5230
    },
    {
      "epoch": 0.38814814814814813,
      "grad_norm": 0.05793832615017891,
      "learning_rate": 4.02962962962963e-05,
      "loss": 0.002,
      "step": 5240
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.053961608558893204,
      "learning_rate": 4.027777777777778e-05,
      "loss": 0.0024,
      "step": 5250
    },
    {
      "epoch": 0.3896296296296296,
      "grad_norm": 0.053033407777547836,
      "learning_rate": 4.0259259259259266e-05,
      "loss": 0.0018,
      "step": 5260
    },
    {
      "epoch": 0.39037037037037037,
      "grad_norm": 0.04171517863869667,
      "learning_rate": 4.024074074074074e-05,
      "loss": 0.0013,
      "step": 5270
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 0.233023539185524,
      "learning_rate": 4.022222222222222e-05,
      "loss": 0.0034,
      "step": 5280
    },
    {
      "epoch": 0.39185185185185184,
      "grad_norm": 0.13380208611488342,
      "learning_rate": 4.020370370370371e-05,
      "loss": 0.0016,
      "step": 5290
    },
    {
      "epoch": 0.3925925925925926,
      "grad_norm": 0.0,
      "learning_rate": 4.018518518518519e-05,
      "loss": 0.0014,
      "step": 5300
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.22175782918930054,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0018,
      "step": 5310
    },
    {
      "epoch": 0.3940740740740741,
      "grad_norm": 0.06822919100522995,
      "learning_rate": 4.014814814814815e-05,
      "loss": 0.0017,
      "step": 5320
    },
    {
      "epoch": 0.39481481481481484,
      "grad_norm": 0.0,
      "learning_rate": 4.012962962962963e-05,
      "loss": 0.002,
      "step": 5330
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 0.02798272855579853,
      "learning_rate": 4.011111111111111e-05,
      "loss": 0.0023,
      "step": 5340
    },
    {
      "epoch": 0.3962962962962963,
      "grad_norm": 0.09983497112989426,
      "learning_rate": 4.009259259259259e-05,
      "loss": 0.0007,
      "step": 5350
    },
    {
      "epoch": 0.397037037037037,
      "grad_norm": 0.0622597374022007,
      "learning_rate": 4.007407407407407e-05,
      "loss": 0.0022,
      "step": 5360
    },
    {
      "epoch": 0.3977777777777778,
      "grad_norm": 0.0,
      "learning_rate": 4.0055555555555554e-05,
      "loss": 0.0018,
      "step": 5370
    },
    {
      "epoch": 0.3985185185185185,
      "grad_norm": 0.05182868614792824,
      "learning_rate": 4.003703703703704e-05,
      "loss": 0.0017,
      "step": 5380
    },
    {
      "epoch": 0.39925925925925926,
      "grad_norm": 0.03977837786078453,
      "learning_rate": 4.001851851851852e-05,
      "loss": 0.0014,
      "step": 5390
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.0,
      "learning_rate": 4e-05,
      "loss": 0.0011,
      "step": 5400
    },
    {
      "epoch": 0.40074074074074073,
      "grad_norm": 0.10655047744512558,
      "learning_rate": 3.9981481481481484e-05,
      "loss": 0.001,
      "step": 5410
    },
    {
      "epoch": 0.4014814814814815,
      "grad_norm": 0.07744965702295303,
      "learning_rate": 3.9962962962962965e-05,
      "loss": 0.0013,
      "step": 5420
    },
    {
      "epoch": 0.4022222222222222,
      "grad_norm": 0.13362322747707367,
      "learning_rate": 3.9944444444444446e-05,
      "loss": 0.0022,
      "step": 5430
    },
    {
      "epoch": 0.40296296296296297,
      "grad_norm": 0.08227792382240295,
      "learning_rate": 3.9925925925925926e-05,
      "loss": 0.0026,
      "step": 5440
    },
    {
      "epoch": 0.40370370370370373,
      "grad_norm": 0.06323977559804916,
      "learning_rate": 3.990740740740741e-05,
      "loss": 0.0015,
      "step": 5450
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 0.06370086967945099,
      "learning_rate": 3.9888888888888895e-05,
      "loss": 0.0023,
      "step": 5460
    },
    {
      "epoch": 0.4051851851851852,
      "grad_norm": 0.0454166941344738,
      "learning_rate": 3.9870370370370376e-05,
      "loss": 0.0023,
      "step": 5470
    },
    {
      "epoch": 0.4059259259259259,
      "grad_norm": 0.0,
      "learning_rate": 3.985185185185185e-05,
      "loss": 0.0016,
      "step": 5480
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.05785452947020531,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0022,
      "step": 5490
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 0.07259178906679153,
      "learning_rate": 3.981481481481482e-05,
      "loss": 0.0018,
      "step": 5500
    },
    {
      "epoch": 0.40814814814814815,
      "grad_norm": 0.037658415734767914,
      "learning_rate": 3.97962962962963e-05,
      "loss": 0.0015,
      "step": 5510
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.06571541726589203,
      "learning_rate": 3.977777777777778e-05,
      "loss": 0.0029,
      "step": 5520
    },
    {
      "epoch": 0.4096296296296296,
      "grad_norm": 0.05026443675160408,
      "learning_rate": 3.975925925925926e-05,
      "loss": 0.0023,
      "step": 5530
    },
    {
      "epoch": 0.4103703703703704,
      "grad_norm": 0.09496310353279114,
      "learning_rate": 3.974074074074075e-05,
      "loss": 0.0018,
      "step": 5540
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 0.03789568319916725,
      "learning_rate": 3.972222222222222e-05,
      "loss": 0.002,
      "step": 5550
    },
    {
      "epoch": 0.41185185185185186,
      "grad_norm": 0.08796070516109467,
      "learning_rate": 3.97037037037037e-05,
      "loss": 0.0019,
      "step": 5560
    },
    {
      "epoch": 0.41259259259259257,
      "grad_norm": 0.09187287837266922,
      "learning_rate": 3.968518518518518e-05,
      "loss": 0.0017,
      "step": 5570
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.15464311838150024,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0016,
      "step": 5580
    },
    {
      "epoch": 0.4140740740740741,
      "grad_norm": 0.04325234144926071,
      "learning_rate": 3.964814814814815e-05,
      "loss": 0.001,
      "step": 5590
    },
    {
      "epoch": 0.4148148148148148,
      "grad_norm": 0.03864053636789322,
      "learning_rate": 3.962962962962963e-05,
      "loss": 0.0012,
      "step": 5600
    },
    {
      "epoch": 0.41555555555555557,
      "grad_norm": 0.019325975328683853,
      "learning_rate": 3.961111111111111e-05,
      "loss": 0.0013,
      "step": 5610
    },
    {
      "epoch": 0.4162962962962963,
      "grad_norm": 0.09519827365875244,
      "learning_rate": 3.9592592592592594e-05,
      "loss": 0.0023,
      "step": 5620
    },
    {
      "epoch": 0.41703703703703704,
      "grad_norm": 0.18771564960479736,
      "learning_rate": 3.9574074074074075e-05,
      "loss": 0.0028,
      "step": 5630
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 0.11413867771625519,
      "learning_rate": 3.9555555555555556e-05,
      "loss": 0.0017,
      "step": 5640
    },
    {
      "epoch": 0.4185185185185185,
      "grad_norm": 0.08112897723913193,
      "learning_rate": 3.9537037037037036e-05,
      "loss": 0.0024,
      "step": 5650
    },
    {
      "epoch": 0.4192592592592593,
      "grad_norm": 0.06497018039226532,
      "learning_rate": 3.9518518518518524e-05,
      "loss": 0.0015,
      "step": 5660
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.0827711820602417,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0016,
      "step": 5670
    },
    {
      "epoch": 0.42074074074074075,
      "grad_norm": 0.12261538207530975,
      "learning_rate": 3.9481481481481485e-05,
      "loss": 0.0014,
      "step": 5680
    },
    {
      "epoch": 0.42148148148148146,
      "grad_norm": 0.037372417747974396,
      "learning_rate": 3.946296296296296e-05,
      "loss": 0.0028,
      "step": 5690
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 0.07416566461324692,
      "learning_rate": 3.944444444444445e-05,
      "loss": 0.0023,
      "step": 5700
    },
    {
      "epoch": 0.422962962962963,
      "grad_norm": 0.09721130132675171,
      "learning_rate": 3.942592592592593e-05,
      "loss": 0.0021,
      "step": 5710
    },
    {
      "epoch": 0.4237037037037037,
      "grad_norm": 0.05917319282889366,
      "learning_rate": 3.940740740740741e-05,
      "loss": 0.0016,
      "step": 5720
    },
    {
      "epoch": 0.42444444444444446,
      "grad_norm": 0.020204998552799225,
      "learning_rate": 3.938888888888889e-05,
      "loss": 0.0019,
      "step": 5730
    },
    {
      "epoch": 0.42518518518518517,
      "grad_norm": 0.10205704718828201,
      "learning_rate": 3.937037037037038e-05,
      "loss": 0.0017,
      "step": 5740
    },
    {
      "epoch": 0.42592592592592593,
      "grad_norm": 0.0752367377281189,
      "learning_rate": 3.935185185185186e-05,
      "loss": 0.0017,
      "step": 5750
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.08930928260087967,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0016,
      "step": 5760
    },
    {
      "epoch": 0.4274074074074074,
      "grad_norm": 0.043749820441007614,
      "learning_rate": 3.931481481481481e-05,
      "loss": 0.0016,
      "step": 5770
    },
    {
      "epoch": 0.42814814814814817,
      "grad_norm": 0.03887803480029106,
      "learning_rate": 3.92962962962963e-05,
      "loss": 0.0023,
      "step": 5780
    },
    {
      "epoch": 0.4288888888888889,
      "grad_norm": 0.10561532527208328,
      "learning_rate": 3.927777777777778e-05,
      "loss": 0.0013,
      "step": 5790
    },
    {
      "epoch": 0.42962962962962964,
      "grad_norm": 0.10141720622777939,
      "learning_rate": 3.925925925925926e-05,
      "loss": 0.0025,
      "step": 5800
    },
    {
      "epoch": 0.43037037037037035,
      "grad_norm": 0.047497253865003586,
      "learning_rate": 3.924074074074074e-05,
      "loss": 0.0017,
      "step": 5810
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 0.0836775153875351,
      "learning_rate": 3.922222222222223e-05,
      "loss": 0.0024,
      "step": 5820
    },
    {
      "epoch": 0.4318518518518519,
      "grad_norm": 0.0,
      "learning_rate": 3.9203703703703704e-05,
      "loss": 0.0026,
      "step": 5830
    },
    {
      "epoch": 0.4325925925925926,
      "grad_norm": 0.12597401440143585,
      "learning_rate": 3.9185185185185185e-05,
      "loss": 0.0017,
      "step": 5840
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.09277304261922836,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0026,
      "step": 5850
    },
    {
      "epoch": 0.43407407407407406,
      "grad_norm": 0.12318603694438934,
      "learning_rate": 3.914814814814815e-05,
      "loss": 0.0016,
      "step": 5860
    },
    {
      "epoch": 0.4348148148148148,
      "grad_norm": 0.04283794388175011,
      "learning_rate": 3.9129629629629634e-05,
      "loss": 0.0015,
      "step": 5870
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 0.08459673076868057,
      "learning_rate": 3.9111111111111115e-05,
      "loss": 0.0016,
      "step": 5880
    },
    {
      "epoch": 0.4362962962962963,
      "grad_norm": 0.22751978039741516,
      "learning_rate": 3.909259259259259e-05,
      "loss": 0.0016,
      "step": 5890
    },
    {
      "epoch": 0.43703703703703706,
      "grad_norm": 0.1575736254453659,
      "learning_rate": 3.9074074074074076e-05,
      "loss": 0.0023,
      "step": 5900
    },
    {
      "epoch": 0.43777777777777777,
      "grad_norm": 0.20167291164398193,
      "learning_rate": 3.905555555555556e-05,
      "loss": 0.0014,
      "step": 5910
    },
    {
      "epoch": 0.43851851851851853,
      "grad_norm": 0.05669965595006943,
      "learning_rate": 3.903703703703704e-05,
      "loss": 0.0025,
      "step": 5920
    },
    {
      "epoch": 0.43925925925925924,
      "grad_norm": 0.046036750078201294,
      "learning_rate": 3.901851851851852e-05,
      "loss": 0.002,
      "step": 5930
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.07778926193714142,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0015,
      "step": 5940
    },
    {
      "epoch": 0.44074074074074077,
      "grad_norm": 0.07657461613416672,
      "learning_rate": 3.898148148148149e-05,
      "loss": 0.0027,
      "step": 5950
    },
    {
      "epoch": 0.4414814814814815,
      "grad_norm": 0.1344759166240692,
      "learning_rate": 3.896296296296296e-05,
      "loss": 0.0015,
      "step": 5960
    },
    {
      "epoch": 0.44222222222222224,
      "grad_norm": 0.0877566859126091,
      "learning_rate": 3.894444444444444e-05,
      "loss": 0.0022,
      "step": 5970
    },
    {
      "epoch": 0.44296296296296295,
      "grad_norm": 0.045610349625349045,
      "learning_rate": 3.892592592592593e-05,
      "loss": 0.0021,
      "step": 5980
    },
    {
      "epoch": 0.4437037037037037,
      "grad_norm": 0.055925145745277405,
      "learning_rate": 3.890740740740741e-05,
      "loss": 0.002,
      "step": 5990
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.039084091782569885,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.0015,
      "step": 6000
    },
    {
      "epoch": 0.4451851851851852,
      "grad_norm": 0.042511217296123505,
      "learning_rate": 3.887037037037037e-05,
      "loss": 0.0015,
      "step": 6010
    },
    {
      "epoch": 0.44592592592592595,
      "grad_norm": 0.07502143085002899,
      "learning_rate": 3.885185185185186e-05,
      "loss": 0.0013,
      "step": 6020
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.14188635349273682,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0022,
      "step": 6030
    },
    {
      "epoch": 0.4474074074074074,
      "grad_norm": 0.0,
      "learning_rate": 3.8814814814814814e-05,
      "loss": 0.0012,
      "step": 6040
    },
    {
      "epoch": 0.44814814814814813,
      "grad_norm": 0.13714087009429932,
      "learning_rate": 3.8796296296296295e-05,
      "loss": 0.0023,
      "step": 6050
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 0.08672310411930084,
      "learning_rate": 3.877777777777778e-05,
      "loss": 0.0023,
      "step": 6060
    },
    {
      "epoch": 0.44962962962962966,
      "grad_norm": 0.07300227880477905,
      "learning_rate": 3.875925925925926e-05,
      "loss": 0.0028,
      "step": 6070
    },
    {
      "epoch": 0.45037037037037037,
      "grad_norm": 0.20645150542259216,
      "learning_rate": 3.8740740740740744e-05,
      "loss": 0.0024,
      "step": 6080
    },
    {
      "epoch": 0.45111111111111113,
      "grad_norm": 0.10696475207805634,
      "learning_rate": 3.8722222222222225e-05,
      "loss": 0.0023,
      "step": 6090
    },
    {
      "epoch": 0.45185185185185184,
      "grad_norm": 0.048489030450582504,
      "learning_rate": 3.8703703703703705e-05,
      "loss": 0.0026,
      "step": 6100
    },
    {
      "epoch": 0.4525925925925926,
      "grad_norm": 0.19148996472358704,
      "learning_rate": 3.8685185185185186e-05,
      "loss": 0.0014,
      "step": 6110
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.04821036010980606,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0021,
      "step": 6120
    },
    {
      "epoch": 0.4540740740740741,
      "grad_norm": 0.07295646518468857,
      "learning_rate": 3.864814814814815e-05,
      "loss": 0.0022,
      "step": 6130
    },
    {
      "epoch": 0.45481481481481484,
      "grad_norm": 0.07074843347072601,
      "learning_rate": 3.8629629629629635e-05,
      "loss": 0.0018,
      "step": 6140
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 0.0,
      "learning_rate": 3.8611111111111116e-05,
      "loss": 0.0023,
      "step": 6150
    },
    {
      "epoch": 0.4562962962962963,
      "grad_norm": 0.16563743352890015,
      "learning_rate": 3.85925925925926e-05,
      "loss": 0.0015,
      "step": 6160
    },
    {
      "epoch": 0.457037037037037,
      "grad_norm": 0.06232218071818352,
      "learning_rate": 3.857407407407407e-05,
      "loss": 0.0028,
      "step": 6170
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 0.2656420171260834,
      "learning_rate": 3.855555555555556e-05,
      "loss": 0.0025,
      "step": 6180
    },
    {
      "epoch": 0.4585185185185185,
      "grad_norm": 0.055509742349386215,
      "learning_rate": 3.853703703703704e-05,
      "loss": 0.0019,
      "step": 6190
    },
    {
      "epoch": 0.45925925925925926,
      "grad_norm": 0.0,
      "learning_rate": 3.851851851851852e-05,
      "loss": 0.0016,
      "step": 6200
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.05901969224214554,
      "learning_rate": 3.85e-05,
      "loss": 0.0015,
      "step": 6210
    },
    {
      "epoch": 0.46074074074074073,
      "grad_norm": 0.0,
      "learning_rate": 3.848148148148149e-05,
      "loss": 0.0023,
      "step": 6220
    },
    {
      "epoch": 0.4614814814814815,
      "grad_norm": 0.12431911379098892,
      "learning_rate": 3.846296296296297e-05,
      "loss": 0.0021,
      "step": 6230
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.07854417711496353,
      "learning_rate": 3.844444444444444e-05,
      "loss": 0.0034,
      "step": 6240
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 0.0902918204665184,
      "learning_rate": 3.8425925925925924e-05,
      "loss": 0.0028,
      "step": 6250
    },
    {
      "epoch": 0.46370370370370373,
      "grad_norm": 0.05359220132231712,
      "learning_rate": 3.840740740740741e-05,
      "loss": 0.0019,
      "step": 6260
    },
    {
      "epoch": 0.46444444444444444,
      "grad_norm": 0.15247033536434174,
      "learning_rate": 3.838888888888889e-05,
      "loss": 0.0016,
      "step": 6270
    },
    {
      "epoch": 0.4651851851851852,
      "grad_norm": 0.0783434808254242,
      "learning_rate": 3.837037037037037e-05,
      "loss": 0.0019,
      "step": 6280
    },
    {
      "epoch": 0.4659259259259259,
      "grad_norm": 0.09545601159334183,
      "learning_rate": 3.8351851851851854e-05,
      "loss": 0.0023,
      "step": 6290
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.04033931344747543,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0016,
      "step": 6300
    },
    {
      "epoch": 0.4674074074074074,
      "grad_norm": 0.049416765570640564,
      "learning_rate": 3.8314814814814815e-05,
      "loss": 0.0024,
      "step": 6310
    },
    {
      "epoch": 0.46814814814814815,
      "grad_norm": 0.0764327198266983,
      "learning_rate": 3.8296296296296296e-05,
      "loss": 0.0023,
      "step": 6320
    },
    {
      "epoch": 0.4688888888888889,
      "grad_norm": 0.08960233628749847,
      "learning_rate": 3.827777777777778e-05,
      "loss": 0.0026,
      "step": 6330
    },
    {
      "epoch": 0.4696296296296296,
      "grad_norm": 0.08195247501134872,
      "learning_rate": 3.8259259259259264e-05,
      "loss": 0.0012,
      "step": 6340
    },
    {
      "epoch": 0.4703703703703704,
      "grad_norm": 0.0892571434378624,
      "learning_rate": 3.8240740740740745e-05,
      "loss": 0.0014,
      "step": 6350
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 0.08241575956344604,
      "learning_rate": 3.8222222222222226e-05,
      "loss": 0.0012,
      "step": 6360
    },
    {
      "epoch": 0.47185185185185186,
      "grad_norm": 0.11957311630249023,
      "learning_rate": 3.820370370370371e-05,
      "loss": 0.0021,
      "step": 6370
    },
    {
      "epoch": 0.4725925925925926,
      "grad_norm": 0.08451991528272629,
      "learning_rate": 3.818518518518519e-05,
      "loss": 0.0025,
      "step": 6380
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.06113172695040703,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0015,
      "step": 6390
    },
    {
      "epoch": 0.4740740740740741,
      "grad_norm": 0.0,
      "learning_rate": 3.814814814814815e-05,
      "loss": 0.0023,
      "step": 6400
    },
    {
      "epoch": 0.4748148148148148,
      "grad_norm": 0.046232376247644424,
      "learning_rate": 3.812962962962963e-05,
      "loss": 0.0024,
      "step": 6410
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 0.05282333120703697,
      "learning_rate": 3.811111111111112e-05,
      "loss": 0.0013,
      "step": 6420
    },
    {
      "epoch": 0.4762962962962963,
      "grad_norm": 0.04201902449131012,
      "learning_rate": 3.80925925925926e-05,
      "loss": 0.0022,
      "step": 6430
    },
    {
      "epoch": 0.47703703703703704,
      "grad_norm": 0.16737301647663116,
      "learning_rate": 3.807407407407408e-05,
      "loss": 0.0022,
      "step": 6440
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 0.09451396763324738,
      "learning_rate": 3.805555555555555e-05,
      "loss": 0.001,
      "step": 6450
    },
    {
      "epoch": 0.4785185185185185,
      "grad_norm": 0.20827028155326843,
      "learning_rate": 3.803703703703704e-05,
      "loss": 0.0023,
      "step": 6460
    },
    {
      "epoch": 0.4792592592592593,
      "grad_norm": 0.10716980695724487,
      "learning_rate": 3.801851851851852e-05,
      "loss": 0.002,
      "step": 6470
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.04063688591122627,
      "learning_rate": 3.8e-05,
      "loss": 0.0021,
      "step": 6480
    },
    {
      "epoch": 0.48074074074074075,
      "grad_norm": 0.10710681229829788,
      "learning_rate": 3.798148148148148e-05,
      "loss": 0.0024,
      "step": 6490
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 0.03877585381269455,
      "learning_rate": 3.7962962962962964e-05,
      "loss": 0.0022,
      "step": 6500
    },
    {
      "epoch": 0.4822222222222222,
      "grad_norm": 0.07687890529632568,
      "learning_rate": 3.7944444444444444e-05,
      "loss": 0.002,
      "step": 6510
    },
    {
      "epoch": 0.482962962962963,
      "grad_norm": 0.04636048898100853,
      "learning_rate": 3.7925925925925925e-05,
      "loss": 0.0013,
      "step": 6520
    },
    {
      "epoch": 0.4837037037037037,
      "grad_norm": 0.09407269954681396,
      "learning_rate": 3.7907407407407406e-05,
      "loss": 0.0018,
      "step": 6530
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 0.06959167122840881,
      "learning_rate": 3.7888888888888894e-05,
      "loss": 0.003,
      "step": 6540
    },
    {
      "epoch": 0.48518518518518516,
      "grad_norm": 0.09414374828338623,
      "learning_rate": 3.7870370370370374e-05,
      "loss": 0.0015,
      "step": 6550
    },
    {
      "epoch": 0.48592592592592593,
      "grad_norm": 0.0,
      "learning_rate": 3.7851851851851855e-05,
      "loss": 0.002,
      "step": 6560
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.03728316351771355,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0021,
      "step": 6570
    },
    {
      "epoch": 0.4874074074074074,
      "grad_norm": 0.09159982949495316,
      "learning_rate": 3.781481481481482e-05,
      "loss": 0.0019,
      "step": 6580
    },
    {
      "epoch": 0.48814814814814816,
      "grad_norm": 0.10003712028265,
      "learning_rate": 3.77962962962963e-05,
      "loss": 0.0014,
      "step": 6590
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.051738590002059937,
      "learning_rate": 3.777777777777778e-05,
      "loss": 0.0018,
      "step": 6600
    },
    {
      "epoch": 0.48962962962962964,
      "grad_norm": 0.0,
      "learning_rate": 3.775925925925926e-05,
      "loss": 0.0018,
      "step": 6610
    },
    {
      "epoch": 0.49037037037037035,
      "grad_norm": 0.0,
      "learning_rate": 3.774074074074074e-05,
      "loss": 0.001,
      "step": 6620
    },
    {
      "epoch": 0.4911111111111111,
      "grad_norm": 0.0601322203874588,
      "learning_rate": 3.772222222222223e-05,
      "loss": 0.0013,
      "step": 6630
    },
    {
      "epoch": 0.4918518518518519,
      "grad_norm": 0.0603024959564209,
      "learning_rate": 3.770370370370371e-05,
      "loss": 0.0018,
      "step": 6640
    },
    {
      "epoch": 0.4925925925925926,
      "grad_norm": 0.03751223534345627,
      "learning_rate": 3.768518518518518e-05,
      "loss": 0.0024,
      "step": 6650
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.0,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0013,
      "step": 6660
    },
    {
      "epoch": 0.49407407407407405,
      "grad_norm": 0.08750531077384949,
      "learning_rate": 3.764814814814815e-05,
      "loss": 0.0019,
      "step": 6670
    },
    {
      "epoch": 0.4948148148148148,
      "grad_norm": 0.15322712063789368,
      "learning_rate": 3.762962962962963e-05,
      "loss": 0.0016,
      "step": 6680
    },
    {
      "epoch": 0.4955555555555556,
      "grad_norm": 0.13718393445014954,
      "learning_rate": 3.761111111111111e-05,
      "loss": 0.0015,
      "step": 6690
    },
    {
      "epoch": 0.4962962962962963,
      "grad_norm": 0.04917283356189728,
      "learning_rate": 3.759259259259259e-05,
      "loss": 0.0018,
      "step": 6700
    },
    {
      "epoch": 0.49703703703703705,
      "grad_norm": 0.04313913732767105,
      "learning_rate": 3.757407407407408e-05,
      "loss": 0.0016,
      "step": 6710
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.16094233095645905,
      "learning_rate": 3.7555555555555554e-05,
      "loss": 0.0018,
      "step": 6720
    },
    {
      "epoch": 0.4985185185185185,
      "grad_norm": 0.0827351063489914,
      "learning_rate": 3.7537037037037035e-05,
      "loss": 0.0034,
      "step": 6730
    },
    {
      "epoch": 0.49925925925925924,
      "grad_norm": 0.037351831793785095,
      "learning_rate": 3.751851851851852e-05,
      "loss": 0.0022,
      "step": 6740
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.07845157384872437,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0029,
      "step": 6750
    },
    {
      "epoch": 0.5007407407407407,
      "grad_norm": 0.08654603362083435,
      "learning_rate": 3.7481481481481484e-05,
      "loss": 0.0013,
      "step": 6760
    },
    {
      "epoch": 0.5014814814814815,
      "grad_norm": 0.04778173565864563,
      "learning_rate": 3.7462962962962965e-05,
      "loss": 0.0023,
      "step": 6770
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 0.10859476774930954,
      "learning_rate": 3.7444444444444446e-05,
      "loss": 0.0022,
      "step": 6780
    },
    {
      "epoch": 0.502962962962963,
      "grad_norm": 0.08809752017259598,
      "learning_rate": 3.742592592592593e-05,
      "loss": 0.0014,
      "step": 6790
    },
    {
      "epoch": 0.5037037037037037,
      "grad_norm": 0.10051323473453522,
      "learning_rate": 3.740740740740741e-05,
      "loss": 0.0021,
      "step": 6800
    },
    {
      "epoch": 0.5044444444444445,
      "grad_norm": 0.07285679876804352,
      "learning_rate": 3.738888888888889e-05,
      "loss": 0.0016,
      "step": 6810
    },
    {
      "epoch": 0.5051851851851852,
      "grad_norm": 0.13500764966011047,
      "learning_rate": 3.737037037037037e-05,
      "loss": 0.0023,
      "step": 6820
    },
    {
      "epoch": 0.5059259259259259,
      "grad_norm": 0.042104631662368774,
      "learning_rate": 3.7351851851851857e-05,
      "loss": 0.0028,
      "step": 6830
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.051588140428066254,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0012,
      "step": 6840
    },
    {
      "epoch": 0.5074074074074074,
      "grad_norm": 0.0877237394452095,
      "learning_rate": 3.731481481481482e-05,
      "loss": 0.0019,
      "step": 6850
    },
    {
      "epoch": 0.5081481481481481,
      "grad_norm": 0.28186675906181335,
      "learning_rate": 3.72962962962963e-05,
      "loss": 0.002,
      "step": 6860
    },
    {
      "epoch": 0.5088888888888888,
      "grad_norm": 0.12364017218351364,
      "learning_rate": 3.727777777777778e-05,
      "loss": 0.0015,
      "step": 6870
    },
    {
      "epoch": 0.5096296296296297,
      "grad_norm": 0.05988943949341774,
      "learning_rate": 3.725925925925926e-05,
      "loss": 0.0017,
      "step": 6880
    },
    {
      "epoch": 0.5103703703703704,
      "grad_norm": 0.06647549569606781,
      "learning_rate": 3.724074074074074e-05,
      "loss": 0.0012,
      "step": 6890
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 0.05205158144235611,
      "learning_rate": 3.722222222222222e-05,
      "loss": 0.0026,
      "step": 6900
    },
    {
      "epoch": 0.5118518518518519,
      "grad_norm": 0.07789245992898941,
      "learning_rate": 3.720370370370371e-05,
      "loss": 0.0018,
      "step": 6910
    },
    {
      "epoch": 0.5125925925925926,
      "grad_norm": 0.0650419220328331,
      "learning_rate": 3.718518518518519e-05,
      "loss": 0.0011,
      "step": 6920
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.0013,
      "step": 6930
    },
    {
      "epoch": 0.5140740740740741,
      "grad_norm": 0.20542162656784058,
      "learning_rate": 3.714814814814815e-05,
      "loss": 0.0024,
      "step": 6940
    },
    {
      "epoch": 0.5148148148148148,
      "grad_norm": 0.040649011731147766,
      "learning_rate": 3.712962962962963e-05,
      "loss": 0.0012,
      "step": 6950
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.0,
      "learning_rate": 3.7111111111111113e-05,
      "loss": 0.0016,
      "step": 6960
    },
    {
      "epoch": 0.5162962962962963,
      "grad_norm": 0.3014606535434723,
      "learning_rate": 3.7092592592592594e-05,
      "loss": 0.0018,
      "step": 6970
    },
    {
      "epoch": 0.5170370370370371,
      "grad_norm": 0.07249227911233902,
      "learning_rate": 3.7074074074074075e-05,
      "loss": 0.0039,
      "step": 6980
    },
    {
      "epoch": 0.5177777777777778,
      "grad_norm": 0.13226984441280365,
      "learning_rate": 3.705555555555556e-05,
      "loss": 0.0033,
      "step": 6990
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 0.07034707814455032,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.0031,
      "step": 7000
    },
    {
      "epoch": 0.5192592592592593,
      "grad_norm": 0.08384161442518234,
      "learning_rate": 3.701851851851852e-05,
      "loss": 0.0019,
      "step": 7010
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.040712952613830566,
      "learning_rate": 3.7e-05,
      "loss": 0.0011,
      "step": 7020
    },
    {
      "epoch": 0.5207407407407407,
      "grad_norm": 0.06384403258562088,
      "learning_rate": 3.6981481481481486e-05,
      "loss": 0.0021,
      "step": 7030
    },
    {
      "epoch": 0.5214814814814814,
      "grad_norm": 0.12976796925067902,
      "learning_rate": 3.6962962962962966e-05,
      "loss": 0.0019,
      "step": 7040
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 0.1668524146080017,
      "learning_rate": 3.694444444444445e-05,
      "loss": 0.0024,
      "step": 7050
    },
    {
      "epoch": 0.522962962962963,
      "grad_norm": 0.15972404181957245,
      "learning_rate": 3.692592592592593e-05,
      "loss": 0.0015,
      "step": 7060
    },
    {
      "epoch": 0.5237037037037037,
      "grad_norm": 0.04760812222957611,
      "learning_rate": 3.690740740740741e-05,
      "loss": 0.0025,
      "step": 7070
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 0.08487643301486969,
      "learning_rate": 3.688888888888889e-05,
      "loss": 0.0019,
      "step": 7080
    },
    {
      "epoch": 0.5251851851851852,
      "grad_norm": 0.16280744969844818,
      "learning_rate": 3.687037037037037e-05,
      "loss": 0.002,
      "step": 7090
    },
    {
      "epoch": 0.5259259259259259,
      "grad_norm": 0.08398812264204025,
      "learning_rate": 3.685185185185185e-05,
      "loss": 0.002,
      "step": 7100
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.17701289057731628,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0023,
      "step": 7110
    },
    {
      "epoch": 0.5274074074074074,
      "grad_norm": 0.05112844333052635,
      "learning_rate": 3.681481481481482e-05,
      "loss": 0.0017,
      "step": 7120
    },
    {
      "epoch": 0.5281481481481481,
      "grad_norm": 0.0,
      "learning_rate": 3.6796296296296293e-05,
      "loss": 0.0022,
      "step": 7130
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 0.08168654143810272,
      "learning_rate": 3.677777777777778e-05,
      "loss": 0.0025,
      "step": 7140
    },
    {
      "epoch": 0.5296296296296297,
      "grad_norm": 0.13857267796993256,
      "learning_rate": 3.675925925925926e-05,
      "loss": 0.0027,
      "step": 7150
    },
    {
      "epoch": 0.5303703703703704,
      "grad_norm": 0.12511149048805237,
      "learning_rate": 3.674074074074074e-05,
      "loss": 0.002,
      "step": 7160
    },
    {
      "epoch": 0.5311111111111111,
      "grad_norm": 0.16620713472366333,
      "learning_rate": 3.672222222222222e-05,
      "loss": 0.003,
      "step": 7170
    },
    {
      "epoch": 0.5318518518518518,
      "grad_norm": 0.10695456713438034,
      "learning_rate": 3.6703703703703704e-05,
      "loss": 0.0027,
      "step": 7180
    },
    {
      "epoch": 0.5325925925925926,
      "grad_norm": 0.030734751373529434,
      "learning_rate": 3.668518518518519e-05,
      "loss": 0.0019,
      "step": 7190
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0021,
      "step": 7200
    },
    {
      "epoch": 0.534074074074074,
      "grad_norm": 0.09766467660665512,
      "learning_rate": 3.6648148148148146e-05,
      "loss": 0.0016,
      "step": 7210
    },
    {
      "epoch": 0.5348148148148149,
      "grad_norm": 0.18568553030490875,
      "learning_rate": 3.662962962962963e-05,
      "loss": 0.0021,
      "step": 7220
    },
    {
      "epoch": 0.5355555555555556,
      "grad_norm": 0.046755675226449966,
      "learning_rate": 3.6611111111111115e-05,
      "loss": 0.0022,
      "step": 7230
    },
    {
      "epoch": 0.5362962962962963,
      "grad_norm": 0.08886979520320892,
      "learning_rate": 3.6592592592592596e-05,
      "loss": 0.002,
      "step": 7240
    },
    {
      "epoch": 0.5370370370370371,
      "grad_norm": 0.0,
      "learning_rate": 3.6574074074074076e-05,
      "loss": 0.0018,
      "step": 7250
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 0.049787525087594986,
      "learning_rate": 3.655555555555556e-05,
      "loss": 0.0014,
      "step": 7260
    },
    {
      "epoch": 0.5385185185185185,
      "grad_norm": 0.0819544568657875,
      "learning_rate": 3.653703703703704e-05,
      "loss": 0.0024,
      "step": 7270
    },
    {
      "epoch": 0.5392592592592592,
      "grad_norm": 0.03902090713381767,
      "learning_rate": 3.651851851851852e-05,
      "loss": 0.0012,
      "step": 7280
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.10680468380451202,
      "learning_rate": 3.65e-05,
      "loss": 0.0023,
      "step": 7290
    },
    {
      "epoch": 0.5407407407407407,
      "grad_norm": 0.05921167507767677,
      "learning_rate": 3.648148148148148e-05,
      "loss": 0.0018,
      "step": 7300
    },
    {
      "epoch": 0.5414814814814815,
      "grad_norm": 0.10197222232818604,
      "learning_rate": 3.646296296296297e-05,
      "loss": 0.0019,
      "step": 7310
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 0.07676292955875397,
      "learning_rate": 3.644444444444445e-05,
      "loss": 0.0029,
      "step": 7320
    },
    {
      "epoch": 0.542962962962963,
      "grad_norm": 0.11485622078180313,
      "learning_rate": 3.642592592592593e-05,
      "loss": 0.0025,
      "step": 7330
    },
    {
      "epoch": 0.5437037037037037,
      "grad_norm": 0.02990511804819107,
      "learning_rate": 3.6407407407407403e-05,
      "loss": 0.0015,
      "step": 7340
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.0,
      "learning_rate": 3.638888888888889e-05,
      "loss": 0.0015,
      "step": 7350
    },
    {
      "epoch": 0.5451851851851852,
      "grad_norm": 0.19022880494594574,
      "learning_rate": 3.637037037037037e-05,
      "loss": 0.0019,
      "step": 7360
    },
    {
      "epoch": 0.5459259259259259,
      "grad_norm": 0.07063187658786774,
      "learning_rate": 3.635185185185185e-05,
      "loss": 0.0016,
      "step": 7370
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.04002095386385918,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0018,
      "step": 7380
    },
    {
      "epoch": 0.5474074074074075,
      "grad_norm": 0.1282252073287964,
      "learning_rate": 3.631481481481482e-05,
      "loss": 0.0033,
      "step": 7390
    },
    {
      "epoch": 0.5481481481481482,
      "grad_norm": 0.10300909727811813,
      "learning_rate": 3.62962962962963e-05,
      "loss": 0.0016,
      "step": 7400
    },
    {
      "epoch": 0.5488888888888889,
      "grad_norm": 0.10009167343378067,
      "learning_rate": 3.6277777777777776e-05,
      "loss": 0.0019,
      "step": 7410
    },
    {
      "epoch": 0.5496296296296296,
      "grad_norm": 0.08269477635622025,
      "learning_rate": 3.6259259259259256e-05,
      "loss": 0.0019,
      "step": 7420
    },
    {
      "epoch": 0.5503703703703704,
      "grad_norm": 0.08536659926176071,
      "learning_rate": 3.6240740740740744e-05,
      "loss": 0.0017,
      "step": 7430
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.1150914803147316,
      "learning_rate": 3.6222222222222225e-05,
      "loss": 0.0018,
      "step": 7440
    },
    {
      "epoch": 0.5518518518518518,
      "grad_norm": 0.07885454595088959,
      "learning_rate": 3.6203703703703706e-05,
      "loss": 0.0026,
      "step": 7450
    },
    {
      "epoch": 0.5525925925925926,
      "grad_norm": 0.139926940202713,
      "learning_rate": 3.6185185185185186e-05,
      "loss": 0.0019,
      "step": 7460
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.20213629305362701,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0013,
      "step": 7470
    },
    {
      "epoch": 0.554074074074074,
      "grad_norm": 0.04054293781518936,
      "learning_rate": 3.614814814814815e-05,
      "loss": 0.0029,
      "step": 7480
    },
    {
      "epoch": 0.5548148148148148,
      "grad_norm": 0.08144916594028473,
      "learning_rate": 3.612962962962963e-05,
      "loss": 0.002,
      "step": 7490
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.08117591589689255,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.0017,
      "step": 7500
    },
    {
      "epoch": 0.5562962962962963,
      "grad_norm": 0.13186615705490112,
      "learning_rate": 3.60925925925926e-05,
      "loss": 0.0024,
      "step": 7510
    },
    {
      "epoch": 0.557037037037037,
      "grad_norm": 0.06974319368600845,
      "learning_rate": 3.607407407407408e-05,
      "loss": 0.0023,
      "step": 7520
    },
    {
      "epoch": 0.5577777777777778,
      "grad_norm": 0.10591844469308853,
      "learning_rate": 3.605555555555556e-05,
      "loss": 0.0027,
      "step": 7530
    },
    {
      "epoch": 0.5585185185185185,
      "grad_norm": 0.10264512151479721,
      "learning_rate": 3.603703703703704e-05,
      "loss": 0.003,
      "step": 7540
    },
    {
      "epoch": 0.5592592592592592,
      "grad_norm": 0.11057880520820618,
      "learning_rate": 3.601851851851852e-05,
      "loss": 0.0013,
      "step": 7550
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.07659811526536942,
      "learning_rate": 3.6e-05,
      "loss": 0.0007,
      "step": 7560
    },
    {
      "epoch": 0.5607407407407408,
      "grad_norm": 0.11656804382801056,
      "learning_rate": 3.598148148148148e-05,
      "loss": 0.0021,
      "step": 7570
    },
    {
      "epoch": 0.5614814814814815,
      "grad_norm": 0.04932292923331261,
      "learning_rate": 3.596296296296296e-05,
      "loss": 0.0019,
      "step": 7580
    },
    {
      "epoch": 0.5622222222222222,
      "grad_norm": 0.061095427721738815,
      "learning_rate": 3.594444444444445e-05,
      "loss": 0.0025,
      "step": 7590
    },
    {
      "epoch": 0.562962962962963,
      "grad_norm": 0.07529859244823456,
      "learning_rate": 3.592592592592593e-05,
      "loss": 0.0014,
      "step": 7600
    },
    {
      "epoch": 0.5637037037037037,
      "grad_norm": 0.056553732603788376,
      "learning_rate": 3.590740740740741e-05,
      "loss": 0.0019,
      "step": 7610
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 0.08386512100696564,
      "learning_rate": 3.5888888888888886e-05,
      "loss": 0.0014,
      "step": 7620
    },
    {
      "epoch": 0.5651851851851852,
      "grad_norm": 0.10320080071687698,
      "learning_rate": 3.587037037037037e-05,
      "loss": 0.0022,
      "step": 7630
    },
    {
      "epoch": 0.5659259259259259,
      "grad_norm": 0.12758386135101318,
      "learning_rate": 3.5851851851851854e-05,
      "loss": 0.0025,
      "step": 7640
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.06719586253166199,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0015,
      "step": 7650
    },
    {
      "epoch": 0.5674074074074074,
      "grad_norm": 0.12993094325065613,
      "learning_rate": 3.5814814814814815e-05,
      "loss": 0.0014,
      "step": 7660
    },
    {
      "epoch": 0.5681481481481482,
      "grad_norm": 0.0,
      "learning_rate": 3.57962962962963e-05,
      "loss": 0.0016,
      "step": 7670
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.11403097212314606,
      "learning_rate": 3.577777777777778e-05,
      "loss": 0.0011,
      "step": 7680
    },
    {
      "epoch": 0.5696296296296296,
      "grad_norm": 0.03905937820672989,
      "learning_rate": 3.575925925925926e-05,
      "loss": 0.0019,
      "step": 7690
    },
    {
      "epoch": 0.5703703703703704,
      "grad_norm": 0.10602312535047531,
      "learning_rate": 3.574074074074074e-05,
      "loss": 0.0037,
      "step": 7700
    },
    {
      "epoch": 0.5711111111111111,
      "grad_norm": 0.05356815829873085,
      "learning_rate": 3.5722222222222226e-05,
      "loss": 0.0024,
      "step": 7710
    },
    {
      "epoch": 0.5718518518518518,
      "grad_norm": 0.2972460091114044,
      "learning_rate": 3.570370370370371e-05,
      "loss": 0.0024,
      "step": 7720
    },
    {
      "epoch": 0.5725925925925925,
      "grad_norm": 0.20968233048915863,
      "learning_rate": 3.568518518518519e-05,
      "loss": 0.0014,
      "step": 7730
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.10660702735185623,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0021,
      "step": 7740
    },
    {
      "epoch": 0.5740740740740741,
      "grad_norm": 0.07477398961782455,
      "learning_rate": 3.564814814814815e-05,
      "loss": 0.0016,
      "step": 7750
    },
    {
      "epoch": 0.5748148148148148,
      "grad_norm": 0.06591226905584335,
      "learning_rate": 3.562962962962963e-05,
      "loss": 0.0028,
      "step": 7760
    },
    {
      "epoch": 0.5755555555555556,
      "grad_norm": 0.059814803302288055,
      "learning_rate": 3.561111111111111e-05,
      "loss": 0.0012,
      "step": 7770
    },
    {
      "epoch": 0.5762962962962963,
      "grad_norm": 0.08131717890501022,
      "learning_rate": 3.559259259259259e-05,
      "loss": 0.0019,
      "step": 7780
    },
    {
      "epoch": 0.577037037037037,
      "grad_norm": 0.043419793248176575,
      "learning_rate": 3.557407407407408e-05,
      "loss": 0.001,
      "step": 7790
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.08631189912557602,
      "learning_rate": 3.555555555555556e-05,
      "loss": 0.0022,
      "step": 7800
    },
    {
      "epoch": 0.5785185185185185,
      "grad_norm": 0.0,
      "learning_rate": 3.553703703703704e-05,
      "loss": 0.0018,
      "step": 7810
    },
    {
      "epoch": 0.5792592592592593,
      "grad_norm": 0.09494435042142868,
      "learning_rate": 3.5518518518518515e-05,
      "loss": 0.0027,
      "step": 7820
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.15535347163677216,
      "learning_rate": 3.55e-05,
      "loss": 0.0023,
      "step": 7830
    },
    {
      "epoch": 0.5807407407407408,
      "grad_norm": 0.13008414208889008,
      "learning_rate": 3.548148148148148e-05,
      "loss": 0.0022,
      "step": 7840
    },
    {
      "epoch": 0.5814814814814815,
      "grad_norm": 0.04057174548506737,
      "learning_rate": 3.5462962962962964e-05,
      "loss": 0.0022,
      "step": 7850
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 0.14879055321216583,
      "learning_rate": 3.5444444444444445e-05,
      "loss": 0.0016,
      "step": 7860
    },
    {
      "epoch": 0.582962962962963,
      "grad_norm": 0.022144682705402374,
      "learning_rate": 3.542592592592593e-05,
      "loss": 0.0014,
      "step": 7870
    },
    {
      "epoch": 0.5837037037037037,
      "grad_norm": 0.062173064798116684,
      "learning_rate": 3.540740740740741e-05,
      "loss": 0.0022,
      "step": 7880
    },
    {
      "epoch": 0.5844444444444444,
      "grad_norm": 0.17670096457004547,
      "learning_rate": 3.538888888888889e-05,
      "loss": 0.0032,
      "step": 7890
    },
    {
      "epoch": 0.5851851851851851,
      "grad_norm": 0.0,
      "learning_rate": 3.537037037037037e-05,
      "loss": 0.0015,
      "step": 7900
    },
    {
      "epoch": 0.585925925925926,
      "grad_norm": 0.03791976720094681,
      "learning_rate": 3.5351851851851855e-05,
      "loss": 0.0017,
      "step": 7910
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.04960733652114868,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0021,
      "step": 7920
    },
    {
      "epoch": 0.5874074074074074,
      "grad_norm": 0.09776584804058075,
      "learning_rate": 3.531481481481482e-05,
      "loss": 0.0008,
      "step": 7930
    },
    {
      "epoch": 0.5881481481481482,
      "grad_norm": 0.11293929815292358,
      "learning_rate": 3.52962962962963e-05,
      "loss": 0.0023,
      "step": 7940
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.16809728741645813,
      "learning_rate": 3.527777777777778e-05,
      "loss": 0.0021,
      "step": 7950
    },
    {
      "epoch": 0.5896296296296296,
      "grad_norm": 0.08741439878940582,
      "learning_rate": 3.525925925925926e-05,
      "loss": 0.0015,
      "step": 7960
    },
    {
      "epoch": 0.5903703703703703,
      "grad_norm": 0.11485820263624191,
      "learning_rate": 3.524074074074074e-05,
      "loss": 0.0012,
      "step": 7970
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 0.03869311511516571,
      "learning_rate": 3.522222222222222e-05,
      "loss": 0.0019,
      "step": 7980
    },
    {
      "epoch": 0.5918518518518519,
      "grad_norm": 0.03439151123166084,
      "learning_rate": 3.520370370370371e-05,
      "loss": 0.0015,
      "step": 7990
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 0.05087394267320633,
      "learning_rate": 3.518518518518519e-05,
      "loss": 0.0027,
      "step": 8000
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.0711955800652504,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0007,
      "step": 8010
    },
    {
      "epoch": 0.5940740740740741,
      "grad_norm": 0.12468228489160538,
      "learning_rate": 3.514814814814815e-05,
      "loss": 0.0016,
      "step": 8020
    },
    {
      "epoch": 0.5948148148148148,
      "grad_norm": 0.037429261952638626,
      "learning_rate": 3.512962962962963e-05,
      "loss": 0.002,
      "step": 8030
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 0.1676630973815918,
      "learning_rate": 3.511111111111111e-05,
      "loss": 0.0022,
      "step": 8040
    },
    {
      "epoch": 0.5962962962962963,
      "grad_norm": 0.18820910155773163,
      "learning_rate": 3.509259259259259e-05,
      "loss": 0.0029,
      "step": 8050
    },
    {
      "epoch": 0.597037037037037,
      "grad_norm": 0.1488705575466156,
      "learning_rate": 3.5074074074074074e-05,
      "loss": 0.0023,
      "step": 8060
    },
    {
      "epoch": 0.5977777777777777,
      "grad_norm": 0.10684984922409058,
      "learning_rate": 3.505555555555556e-05,
      "loss": 0.0018,
      "step": 8070
    },
    {
      "epoch": 0.5985185185185186,
      "grad_norm": 0.04850718379020691,
      "learning_rate": 3.503703703703704e-05,
      "loss": 0.0018,
      "step": 8080
    },
    {
      "epoch": 0.5992592592592593,
      "grad_norm": 0.06678395718336105,
      "learning_rate": 3.501851851851852e-05,
      "loss": 0.0021,
      "step": 8090
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.06298414617776871,
      "learning_rate": 3.5e-05,
      "loss": 0.0019,
      "step": 8100
    },
    {
      "epoch": 0.6007407407407407,
      "grad_norm": 0.1337163746356964,
      "learning_rate": 3.4981481481481484e-05,
      "loss": 0.0036,
      "step": 8110
    },
    {
      "epoch": 0.6014814814814815,
      "grad_norm": 0.07564101368188858,
      "learning_rate": 3.4962962962962965e-05,
      "loss": 0.002,
      "step": 8120
    },
    {
      "epoch": 0.6022222222222222,
      "grad_norm": 0.03733048588037491,
      "learning_rate": 3.4944444444444446e-05,
      "loss": 0.0017,
      "step": 8130
    },
    {
      "epoch": 0.6029629629629629,
      "grad_norm": 0.09501165896654129,
      "learning_rate": 3.492592592592593e-05,
      "loss": 0.0021,
      "step": 8140
    },
    {
      "epoch": 0.6037037037037037,
      "grad_norm": 0.1492968499660492,
      "learning_rate": 3.490740740740741e-05,
      "loss": 0.0009,
      "step": 8150
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.029317814856767654,
      "learning_rate": 3.4888888888888895e-05,
      "loss": 0.0016,
      "step": 8160
    },
    {
      "epoch": 0.6051851851851852,
      "grad_norm": 0.12184583395719528,
      "learning_rate": 3.487037037037037e-05,
      "loss": 0.0025,
      "step": 8170
    },
    {
      "epoch": 0.605925925925926,
      "grad_norm": 0.0548294372856617,
      "learning_rate": 3.485185185185185e-05,
      "loss": 0.0019,
      "step": 8180
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.033409349620342255,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0017,
      "step": 8190
    },
    {
      "epoch": 0.6074074074074074,
      "grad_norm": 0.0987647995352745,
      "learning_rate": 3.481481481481482e-05,
      "loss": 0.0014,
      "step": 8200
    },
    {
      "epoch": 0.6081481481481481,
      "grad_norm": 0.08024904131889343,
      "learning_rate": 3.47962962962963e-05,
      "loss": 0.0017,
      "step": 8210
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 0.0,
      "learning_rate": 3.477777777777778e-05,
      "loss": 0.0026,
      "step": 8220
    },
    {
      "epoch": 0.6096296296296296,
      "grad_norm": 0.03800887614488602,
      "learning_rate": 3.475925925925926e-05,
      "loss": 0.0009,
      "step": 8230
    },
    {
      "epoch": 0.6103703703703703,
      "grad_norm": 0.3359304368495941,
      "learning_rate": 3.474074074074074e-05,
      "loss": 0.0023,
      "step": 8240
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.0,
      "learning_rate": 3.472222222222222e-05,
      "loss": 0.0025,
      "step": 8250
    },
    {
      "epoch": 0.6118518518518519,
      "grad_norm": 0.16444534063339233,
      "learning_rate": 3.47037037037037e-05,
      "loss": 0.0015,
      "step": 8260
    },
    {
      "epoch": 0.6125925925925926,
      "grad_norm": 0.13277976214885712,
      "learning_rate": 3.468518518518519e-05,
      "loss": 0.0021,
      "step": 8270
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.11563830822706223,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0019,
      "step": 8280
    },
    {
      "epoch": 0.6140740740740741,
      "grad_norm": 0.0,
      "learning_rate": 3.464814814814815e-05,
      "loss": 0.0015,
      "step": 8290
    },
    {
      "epoch": 0.6148148148148148,
      "grad_norm": 0.0399918369948864,
      "learning_rate": 3.4629629629629626e-05,
      "loss": 0.0014,
      "step": 8300
    },
    {
      "epoch": 0.6155555555555555,
      "grad_norm": 0.0961679071187973,
      "learning_rate": 3.4611111111111114e-05,
      "loss": 0.0023,
      "step": 8310
    },
    {
      "epoch": 0.6162962962962963,
      "grad_norm": 0.0908140242099762,
      "learning_rate": 3.4592592592592594e-05,
      "loss": 0.0019,
      "step": 8320
    },
    {
      "epoch": 0.617037037037037,
      "grad_norm": 0.04766520857810974,
      "learning_rate": 3.4574074074074075e-05,
      "loss": 0.0012,
      "step": 8330
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 0.0,
      "learning_rate": 3.4555555555555556e-05,
      "loss": 0.0021,
      "step": 8340
    },
    {
      "epoch": 0.6185185185185185,
      "grad_norm": 0.09660204499959946,
      "learning_rate": 3.453703703703704e-05,
      "loss": 0.0026,
      "step": 8350
    },
    {
      "epoch": 0.6192592592592593,
      "grad_norm": 0.10568957775831223,
      "learning_rate": 3.4518518518518524e-05,
      "loss": 0.0018,
      "step": 8360
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.0,
      "learning_rate": 3.45e-05,
      "loss": 0.0016,
      "step": 8370
    },
    {
      "epoch": 0.6207407407407407,
      "grad_norm": 0.0,
      "learning_rate": 3.448148148148148e-05,
      "loss": 0.0025,
      "step": 8380
    },
    {
      "epoch": 0.6214814814814815,
      "grad_norm": 0.212227001786232,
      "learning_rate": 3.446296296296297e-05,
      "loss": 0.0022,
      "step": 8390
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.11446893960237503,
      "learning_rate": 3.444444444444445e-05,
      "loss": 0.0026,
      "step": 8400
    },
    {
      "epoch": 0.6229629629629629,
      "grad_norm": 0.056970108300447464,
      "learning_rate": 3.442592592592593e-05,
      "loss": 0.0016,
      "step": 8410
    },
    {
      "epoch": 0.6237037037037036,
      "grad_norm": 0.08990003913640976,
      "learning_rate": 3.440740740740741e-05,
      "loss": 0.003,
      "step": 8420
    },
    {
      "epoch": 0.6244444444444445,
      "grad_norm": 0.0,
      "learning_rate": 3.438888888888889e-05,
      "loss": 0.002,
      "step": 8430
    },
    {
      "epoch": 0.6251851851851852,
      "grad_norm": 0.12178277224302292,
      "learning_rate": 3.437037037037037e-05,
      "loss": 0.0014,
      "step": 8440
    },
    {
      "epoch": 0.6259259259259259,
      "grad_norm": 0.19126297533512115,
      "learning_rate": 3.435185185185185e-05,
      "loss": 0.002,
      "step": 8450
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.07276052981615067,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0018,
      "step": 8460
    },
    {
      "epoch": 0.6274074074074074,
      "grad_norm": 0.0,
      "learning_rate": 3.431481481481482e-05,
      "loss": 0.0027,
      "step": 8470
    },
    {
      "epoch": 0.6281481481481481,
      "grad_norm": 0.07863111793994904,
      "learning_rate": 3.42962962962963e-05,
      "loss": 0.0009,
      "step": 8480
    },
    {
      "epoch": 0.6288888888888889,
      "grad_norm": 0.124623142182827,
      "learning_rate": 3.427777777777778e-05,
      "loss": 0.0012,
      "step": 8490
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 0.10267224162817001,
      "learning_rate": 3.425925925925926e-05,
      "loss": 0.0018,
      "step": 8500
    },
    {
      "epoch": 0.6303703703703704,
      "grad_norm": 0.11509829759597778,
      "learning_rate": 3.424074074074074e-05,
      "loss": 0.0024,
      "step": 8510
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 0.2258942425251007,
      "learning_rate": 3.4222222222222224e-05,
      "loss": 0.0026,
      "step": 8520
    },
    {
      "epoch": 0.6318518518518519,
      "grad_norm": 0.0,
      "learning_rate": 3.4203703703703704e-05,
      "loss": 0.0017,
      "step": 8530
    },
    {
      "epoch": 0.6325925925925926,
      "grad_norm": 0.05712028220295906,
      "learning_rate": 3.4185185185185185e-05,
      "loss": 0.0028,
      "step": 8540
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.02060505375266075,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0022,
      "step": 8550
    },
    {
      "epoch": 0.6340740740740741,
      "grad_norm": 0.03765479475259781,
      "learning_rate": 3.4148148148148153e-05,
      "loss": 0.0011,
      "step": 8560
    },
    {
      "epoch": 0.6348148148148148,
      "grad_norm": 0.07807961106300354,
      "learning_rate": 3.4129629629629634e-05,
      "loss": 0.001,
      "step": 8570
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 0.039037059992551804,
      "learning_rate": 3.411111111111111e-05,
      "loss": 0.0024,
      "step": 8580
    },
    {
      "epoch": 0.6362962962962962,
      "grad_norm": 0.10716810822486877,
      "learning_rate": 3.4092592592592596e-05,
      "loss": 0.0024,
      "step": 8590
    },
    {
      "epoch": 0.6370370370370371,
      "grad_norm": 0.06841163337230682,
      "learning_rate": 3.4074074074074077e-05,
      "loss": 0.0024,
      "step": 8600
    },
    {
      "epoch": 0.6377777777777778,
      "grad_norm": 0.07958020269870758,
      "learning_rate": 3.405555555555556e-05,
      "loss": 0.0016,
      "step": 8610
    },
    {
      "epoch": 0.6385185185185185,
      "grad_norm": 0.08345764875411987,
      "learning_rate": 3.403703703703704e-05,
      "loss": 0.0014,
      "step": 8620
    },
    {
      "epoch": 0.6392592592592593,
      "grad_norm": 0.0743102878332138,
      "learning_rate": 3.401851851851852e-05,
      "loss": 0.0022,
      "step": 8630
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.0,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.001,
      "step": 8640
    },
    {
      "epoch": 0.6407407407407407,
      "grad_norm": 0.15736700594425201,
      "learning_rate": 3.398148148148148e-05,
      "loss": 0.0016,
      "step": 8650
    },
    {
      "epoch": 0.6414814814814814,
      "grad_norm": 0.08519674092531204,
      "learning_rate": 3.396296296296296e-05,
      "loss": 0.0021,
      "step": 8660
    },
    {
      "epoch": 0.6422222222222222,
      "grad_norm": 0.060690850019454956,
      "learning_rate": 3.394444444444444e-05,
      "loss": 0.0017,
      "step": 8670
    },
    {
      "epoch": 0.642962962962963,
      "grad_norm": 0.10462476313114166,
      "learning_rate": 3.392592592592593e-05,
      "loss": 0.0007,
      "step": 8680
    },
    {
      "epoch": 0.6437037037037037,
      "grad_norm": 0.042704083025455475,
      "learning_rate": 3.390740740740741e-05,
      "loss": 0.0016,
      "step": 8690
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.2801455557346344,
      "learning_rate": 3.388888888888889e-05,
      "loss": 0.0023,
      "step": 8700
    },
    {
      "epoch": 0.6451851851851852,
      "grad_norm": 0.08974599838256836,
      "learning_rate": 3.387037037037037e-05,
      "loss": 0.0016,
      "step": 8710
    },
    {
      "epoch": 0.6459259259259259,
      "grad_norm": 0.21787714958190918,
      "learning_rate": 3.385185185185185e-05,
      "loss": 0.0019,
      "step": 8720
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.03882515802979469,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0024,
      "step": 8730
    },
    {
      "epoch": 0.6474074074074074,
      "grad_norm": 0.08362053334712982,
      "learning_rate": 3.3814814814814814e-05,
      "loss": 0.0022,
      "step": 8740
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 0.049639541655778885,
      "learning_rate": 3.3796296296296295e-05,
      "loss": 0.0015,
      "step": 8750
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 0.1130957081913948,
      "learning_rate": 3.377777777777778e-05,
      "loss": 0.001,
      "step": 8760
    },
    {
      "epoch": 0.6496296296296297,
      "grad_norm": 0.13348162174224854,
      "learning_rate": 3.3759259259259263e-05,
      "loss": 0.0025,
      "step": 8770
    },
    {
      "epoch": 0.6503703703703704,
      "grad_norm": 0.0,
      "learning_rate": 3.3740740740740744e-05,
      "loss": 0.0014,
      "step": 8780
    },
    {
      "epoch": 0.6511111111111111,
      "grad_norm": 0.07915893197059631,
      "learning_rate": 3.3722222222222225e-05,
      "loss": 0.0021,
      "step": 8790
    },
    {
      "epoch": 0.6518518518518519,
      "grad_norm": 0.03800652176141739,
      "learning_rate": 3.3703703703703706e-05,
      "loss": 0.0012,
      "step": 8800
    },
    {
      "epoch": 0.6525925925925926,
      "grad_norm": 0.042877446860075,
      "learning_rate": 3.3685185185185187e-05,
      "loss": 0.001,
      "step": 8810
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.07313546538352966,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.002,
      "step": 8820
    },
    {
      "epoch": 0.654074074074074,
      "grad_norm": 0.14898358285427094,
      "learning_rate": 3.364814814814815e-05,
      "loss": 0.0017,
      "step": 8830
    },
    {
      "epoch": 0.6548148148148148,
      "grad_norm": 0.06383707374334335,
      "learning_rate": 3.3629629629629636e-05,
      "loss": 0.0021,
      "step": 8840
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 0.0813927948474884,
      "learning_rate": 3.3611111111111116e-05,
      "loss": 0.0015,
      "step": 8850
    },
    {
      "epoch": 0.6562962962962963,
      "grad_norm": 0.0,
      "learning_rate": 3.359259259259259e-05,
      "loss": 0.0021,
      "step": 8860
    },
    {
      "epoch": 0.6570370370370371,
      "grad_norm": 0.05411583185195923,
      "learning_rate": 3.357407407407407e-05,
      "loss": 0.0016,
      "step": 8870
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 0.06414784491062164,
      "learning_rate": 3.355555555555556e-05,
      "loss": 0.0021,
      "step": 8880
    },
    {
      "epoch": 0.6585185185185185,
      "grad_norm": 0.16851527988910675,
      "learning_rate": 3.353703703703704e-05,
      "loss": 0.0029,
      "step": 8890
    },
    {
      "epoch": 0.6592592592592592,
      "grad_norm": 0.12720999121665955,
      "learning_rate": 3.351851851851852e-05,
      "loss": 0.0025,
      "step": 8900
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.0,
      "learning_rate": 3.35e-05,
      "loss": 0.0014,
      "step": 8910
    },
    {
      "epoch": 0.6607407407407407,
      "grad_norm": 0.10525476187467575,
      "learning_rate": 3.348148148148148e-05,
      "loss": 0.0016,
      "step": 8920
    },
    {
      "epoch": 0.6614814814814814,
      "grad_norm": 0.0726739913225174,
      "learning_rate": 3.346296296296296e-05,
      "loss": 0.0029,
      "step": 8930
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 0.041908834129571915,
      "learning_rate": 3.3444444444444443e-05,
      "loss": 0.0011,
      "step": 8940
    },
    {
      "epoch": 0.662962962962963,
      "grad_norm": 0.12662456929683685,
      "learning_rate": 3.3425925925925924e-05,
      "loss": 0.0029,
      "step": 8950
    },
    {
      "epoch": 0.6637037037037037,
      "grad_norm": 0.13167597353458405,
      "learning_rate": 3.340740740740741e-05,
      "loss": 0.0019,
      "step": 8960
    },
    {
      "epoch": 0.6644444444444444,
      "grad_norm": 0.12219306826591492,
      "learning_rate": 3.338888888888889e-05,
      "loss": 0.0011,
      "step": 8970
    },
    {
      "epoch": 0.6651851851851852,
      "grad_norm": 0.06768729537725449,
      "learning_rate": 3.337037037037037e-05,
      "loss": 0.0019,
      "step": 8980
    },
    {
      "epoch": 0.6659259259259259,
      "grad_norm": 0.09631425887346268,
      "learning_rate": 3.3351851851851854e-05,
      "loss": 0.0016,
      "step": 8990
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.0521572083234787,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0017,
      "step": 9000
    },
    {
      "epoch": 0.6674074074074074,
      "grad_norm": 0.10097520798444748,
      "learning_rate": 3.3314814814814816e-05,
      "loss": 0.0014,
      "step": 9010
    },
    {
      "epoch": 0.6681481481481482,
      "grad_norm": 0.08544322103261948,
      "learning_rate": 3.3296296296296296e-05,
      "loss": 0.0013,
      "step": 9020
    },
    {
      "epoch": 0.6688888888888889,
      "grad_norm": 0.016550621017813683,
      "learning_rate": 3.327777777777778e-05,
      "loss": 0.0013,
      "step": 9030
    },
    {
      "epoch": 0.6696296296296296,
      "grad_norm": 0.157827228307724,
      "learning_rate": 3.3259259259259265e-05,
      "loss": 0.0015,
      "step": 9040
    },
    {
      "epoch": 0.6703703703703704,
      "grad_norm": 0.0818653553724289,
      "learning_rate": 3.3240740740740746e-05,
      "loss": 0.0017,
      "step": 9050
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 0.13805052638053894,
      "learning_rate": 3.322222222222222e-05,
      "loss": 0.0022,
      "step": 9060
    },
    {
      "epoch": 0.6718518518518518,
      "grad_norm": 0.08862762898206711,
      "learning_rate": 3.32037037037037e-05,
      "loss": 0.0014,
      "step": 9070
    },
    {
      "epoch": 0.6725925925925926,
      "grad_norm": 0.052644941955804825,
      "learning_rate": 3.318518518518519e-05,
      "loss": 0.0019,
      "step": 9080
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.02243432030081749,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0014,
      "step": 9090
    },
    {
      "epoch": 0.674074074074074,
      "grad_norm": 0.11256306618452072,
      "learning_rate": 3.314814814814815e-05,
      "loss": 0.0016,
      "step": 9100
    },
    {
      "epoch": 0.6748148148148149,
      "grad_norm": 0.02962227538228035,
      "learning_rate": 3.312962962962963e-05,
      "loss": 0.0018,
      "step": 9110
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.08689352124929428,
      "learning_rate": 3.311111111111112e-05,
      "loss": 0.0013,
      "step": 9120
    },
    {
      "epoch": 0.6762962962962963,
      "grad_norm": 0.07223101705312729,
      "learning_rate": 3.309259259259259e-05,
      "loss": 0.0023,
      "step": 9130
    },
    {
      "epoch": 0.677037037037037,
      "grad_norm": 0.038178451359272,
      "learning_rate": 3.307407407407407e-05,
      "loss": 0.0022,
      "step": 9140
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 0.06475120037794113,
      "learning_rate": 3.3055555555555553e-05,
      "loss": 0.0014,
      "step": 9150
    },
    {
      "epoch": 0.6785185185185185,
      "grad_norm": 0.047876693308353424,
      "learning_rate": 3.303703703703704e-05,
      "loss": 0.002,
      "step": 9160
    },
    {
      "epoch": 0.6792592592592592,
      "grad_norm": 0.0,
      "learning_rate": 3.301851851851852e-05,
      "loss": 0.0018,
      "step": 9170
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.06375068426132202,
      "learning_rate": 3.3e-05,
      "loss": 0.0019,
      "step": 9180
    },
    {
      "epoch": 0.6807407407407408,
      "grad_norm": 0.09944939613342285,
      "learning_rate": 3.298148148148148e-05,
      "loss": 0.0016,
      "step": 9190
    },
    {
      "epoch": 0.6814814814814815,
      "grad_norm": 0.08796363323926926,
      "learning_rate": 3.2962962962962964e-05,
      "loss": 0.0018,
      "step": 9200
    },
    {
      "epoch": 0.6822222222222222,
      "grad_norm": 0.08447514474391937,
      "learning_rate": 3.2944444444444445e-05,
      "loss": 0.0028,
      "step": 9210
    },
    {
      "epoch": 0.682962962962963,
      "grad_norm": 0.11536169797182083,
      "learning_rate": 3.2925925925925926e-05,
      "loss": 0.0011,
      "step": 9220
    },
    {
      "epoch": 0.6837037037037037,
      "grad_norm": 0.06621982902288437,
      "learning_rate": 3.2907407407407406e-05,
      "loss": 0.0019,
      "step": 9230
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 0.13757343590259552,
      "learning_rate": 3.2888888888888894e-05,
      "loss": 0.0012,
      "step": 9240
    },
    {
      "epoch": 0.6851851851851852,
      "grad_norm": 0.0,
      "learning_rate": 3.2870370370370375e-05,
      "loss": 0.0012,
      "step": 9250
    },
    {
      "epoch": 0.6859259259259259,
      "grad_norm": 0.048734404146671295,
      "learning_rate": 3.2851851851851856e-05,
      "loss": 0.0026,
      "step": 9260
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.07074245810508728,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0022,
      "step": 9270
    },
    {
      "epoch": 0.6874074074074074,
      "grad_norm": 0.04462311044335365,
      "learning_rate": 3.281481481481482e-05,
      "loss": 0.0015,
      "step": 9280
    },
    {
      "epoch": 0.6881481481481482,
      "grad_norm": 0.10030808299779892,
      "learning_rate": 3.27962962962963e-05,
      "loss": 0.002,
      "step": 9290
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.12205629050731659,
      "learning_rate": 3.277777777777778e-05,
      "loss": 0.002,
      "step": 9300
    },
    {
      "epoch": 0.6896296296296296,
      "grad_norm": 0.0,
      "learning_rate": 3.275925925925926e-05,
      "loss": 0.0014,
      "step": 9310
    },
    {
      "epoch": 0.6903703703703704,
      "grad_norm": 0.0633920431137085,
      "learning_rate": 3.274074074074075e-05,
      "loss": 0.0024,
      "step": 9320
    },
    {
      "epoch": 0.6911111111111111,
      "grad_norm": 0.16137787699699402,
      "learning_rate": 3.272222222222223e-05,
      "loss": 0.002,
      "step": 9330
    },
    {
      "epoch": 0.6918518518518518,
      "grad_norm": 0.061931952834129333,
      "learning_rate": 3.27037037037037e-05,
      "loss": 0.0016,
      "step": 9340
    },
    {
      "epoch": 0.6925925925925925,
      "grad_norm": 0.03242435306310654,
      "learning_rate": 3.268518518518518e-05,
      "loss": 0.0027,
      "step": 9350
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.0909435823559761,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0014,
      "step": 9360
    },
    {
      "epoch": 0.6940740740740741,
      "grad_norm": 0.07940047234296799,
      "learning_rate": 3.264814814814815e-05,
      "loss": 0.0015,
      "step": 9370
    },
    {
      "epoch": 0.6948148148148148,
      "grad_norm": 0.1247800961136818,
      "learning_rate": 3.262962962962963e-05,
      "loss": 0.0004,
      "step": 9380
    },
    {
      "epoch": 0.6955555555555556,
      "grad_norm": 0.021984266117215157,
      "learning_rate": 3.261111111111111e-05,
      "loss": 0.0026,
      "step": 9390
    },
    {
      "epoch": 0.6962962962962963,
      "grad_norm": 0.0,
      "learning_rate": 3.25925925925926e-05,
      "loss": 0.0013,
      "step": 9400
    },
    {
      "epoch": 0.697037037037037,
      "grad_norm": 0.059738099575042725,
      "learning_rate": 3.2574074074074074e-05,
      "loss": 0.0025,
      "step": 9410
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 0.10987145453691483,
      "learning_rate": 3.2555555555555555e-05,
      "loss": 0.0018,
      "step": 9420
    },
    {
      "epoch": 0.6985185185185185,
      "grad_norm": 0.08818875998258591,
      "learning_rate": 3.2537037037037036e-05,
      "loss": 0.0014,
      "step": 9430
    },
    {
      "epoch": 0.6992592592592592,
      "grad_norm": 0.11784958094358444,
      "learning_rate": 3.251851851851852e-05,
      "loss": 0.0011,
      "step": 9440
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.08250182121992111,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0015,
      "step": 9450
    },
    {
      "epoch": 0.7007407407407408,
      "grad_norm": 0.0663231611251831,
      "learning_rate": 3.2481481481481485e-05,
      "loss": 0.0024,
      "step": 9460
    },
    {
      "epoch": 0.7014814814814815,
      "grad_norm": 0.10223354399204254,
      "learning_rate": 3.2462962962962965e-05,
      "loss": 0.0007,
      "step": 9470
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 0.0987958014011383,
      "learning_rate": 3.2444444444444446e-05,
      "loss": 0.0013,
      "step": 9480
    },
    {
      "epoch": 0.702962962962963,
      "grad_norm": 0.08121650665998459,
      "learning_rate": 3.242592592592593e-05,
      "loss": 0.0013,
      "step": 9490
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 0.07717819511890411,
      "learning_rate": 3.240740740740741e-05,
      "loss": 0.0028,
      "step": 9500
    },
    {
      "epoch": 0.7044444444444444,
      "grad_norm": 0.0,
      "learning_rate": 3.238888888888889e-05,
      "loss": 0.0013,
      "step": 9510
    },
    {
      "epoch": 0.7051851851851851,
      "grad_norm": 0.07203135639429092,
      "learning_rate": 3.2370370370370376e-05,
      "loss": 0.0017,
      "step": 9520
    },
    {
      "epoch": 0.705925925925926,
      "grad_norm": 0.04513271898031235,
      "learning_rate": 3.235185185185186e-05,
      "loss": 0.0015,
      "step": 9530
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.11282334476709366,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0014,
      "step": 9540
    },
    {
      "epoch": 0.7074074074074074,
      "grad_norm": 0.13470609486103058,
      "learning_rate": 3.231481481481481e-05,
      "loss": 0.0024,
      "step": 9550
    },
    {
      "epoch": 0.7081481481481482,
      "grad_norm": 0.1528242975473404,
      "learning_rate": 3.22962962962963e-05,
      "loss": 0.0019,
      "step": 9560
    },
    {
      "epoch": 0.7088888888888889,
      "grad_norm": 0.07378645986318588,
      "learning_rate": 3.227777777777778e-05,
      "loss": 0.0023,
      "step": 9570
    },
    {
      "epoch": 0.7096296296296296,
      "grad_norm": 0.0,
      "learning_rate": 3.225925925925926e-05,
      "loss": 0.0028,
      "step": 9580
    },
    {
      "epoch": 0.7103703703703703,
      "grad_norm": 0.0,
      "learning_rate": 3.224074074074074e-05,
      "loss": 0.0016,
      "step": 9590
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.08136799931526184,
      "learning_rate": 3.222222222222223e-05,
      "loss": 0.0019,
      "step": 9600
    },
    {
      "epoch": 0.7118518518518518,
      "grad_norm": 0.04564492776989937,
      "learning_rate": 3.22037037037037e-05,
      "loss": 0.0009,
      "step": 9610
    },
    {
      "epoch": 0.7125925925925926,
      "grad_norm": 0.0,
      "learning_rate": 3.2185185185185184e-05,
      "loss": 0.0018,
      "step": 9620
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.06363843381404877,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0034,
      "step": 9630
    },
    {
      "epoch": 0.7140740740740741,
      "grad_norm": 0.04964650049805641,
      "learning_rate": 3.214814814814815e-05,
      "loss": 0.0015,
      "step": 9640
    },
    {
      "epoch": 0.7148148148148148,
      "grad_norm": 0.08587872236967087,
      "learning_rate": 3.212962962962963e-05,
      "loss": 0.0021,
      "step": 9650
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 0.0631299540400505,
      "learning_rate": 3.2111111111111114e-05,
      "loss": 0.0022,
      "step": 9660
    },
    {
      "epoch": 0.7162962962962963,
      "grad_norm": 0.14534766972064972,
      "learning_rate": 3.2092592592592595e-05,
      "loss": 0.0019,
      "step": 9670
    },
    {
      "epoch": 0.717037037037037,
      "grad_norm": 0.07103626430034637,
      "learning_rate": 3.2074074074074075e-05,
      "loss": 0.0028,
      "step": 9680
    },
    {
      "epoch": 0.7177777777777777,
      "grad_norm": 0.03981830179691315,
      "learning_rate": 3.2055555555555556e-05,
      "loss": 0.002,
      "step": 9690
    },
    {
      "epoch": 0.7185185185185186,
      "grad_norm": 0.13508403301239014,
      "learning_rate": 3.203703703703704e-05,
      "loss": 0.0023,
      "step": 9700
    },
    {
      "epoch": 0.7192592592592593,
      "grad_norm": 0.06479174643754959,
      "learning_rate": 3.201851851851852e-05,
      "loss": 0.0017,
      "step": 9710
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.15487845242023468,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0014,
      "step": 9720
    },
    {
      "epoch": 0.7207407407407408,
      "grad_norm": 0.05773238092660904,
      "learning_rate": 3.1981481481481486e-05,
      "loss": 0.0023,
      "step": 9730
    },
    {
      "epoch": 0.7214814814814815,
      "grad_norm": 0.08473730087280273,
      "learning_rate": 3.196296296296297e-05,
      "loss": 0.0022,
      "step": 9740
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.13166838884353638,
      "learning_rate": 3.194444444444444e-05,
      "loss": 0.0022,
      "step": 9750
    },
    {
      "epoch": 0.7229629629629629,
      "grad_norm": 0.07923419773578644,
      "learning_rate": 3.192592592592593e-05,
      "loss": 0.0019,
      "step": 9760
    },
    {
      "epoch": 0.7237037037037037,
      "grad_norm": 0.06497623026371002,
      "learning_rate": 3.190740740740741e-05,
      "loss": 0.0023,
      "step": 9770
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 0.10614457726478577,
      "learning_rate": 3.188888888888889e-05,
      "loss": 0.0016,
      "step": 9780
    },
    {
      "epoch": 0.7251851851851852,
      "grad_norm": 0.0,
      "learning_rate": 3.187037037037037e-05,
      "loss": 0.0017,
      "step": 9790
    },
    {
      "epoch": 0.725925925925926,
      "grad_norm": 0.11089883744716644,
      "learning_rate": 3.185185185185185e-05,
      "loss": 0.0015,
      "step": 9800
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.09222665429115295,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0016,
      "step": 9810
    },
    {
      "epoch": 0.7274074074074074,
      "grad_norm": 0.0402727872133255,
      "learning_rate": 3.181481481481481e-05,
      "loss": 0.0015,
      "step": 9820
    },
    {
      "epoch": 0.7281481481481481,
      "grad_norm": 0.06392984837293625,
      "learning_rate": 3.1796296296296294e-05,
      "loss": 0.0019,
      "step": 9830
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.18576164543628693,
      "learning_rate": 3.177777777777778e-05,
      "loss": 0.0022,
      "step": 9840
    },
    {
      "epoch": 0.7296296296296296,
      "grad_norm": 0.03341060131788254,
      "learning_rate": 3.175925925925926e-05,
      "loss": 0.002,
      "step": 9850
    },
    {
      "epoch": 0.7303703703703703,
      "grad_norm": 0.05612551420927048,
      "learning_rate": 3.174074074074074e-05,
      "loss": 0.0017,
      "step": 9860
    },
    {
      "epoch": 0.7311111111111112,
      "grad_norm": 0.15086765587329865,
      "learning_rate": 3.1722222222222224e-05,
      "loss": 0.0023,
      "step": 9870
    },
    {
      "epoch": 0.7318518518518519,
      "grad_norm": 0.051575589925050735,
      "learning_rate": 3.1703703703703705e-05,
      "loss": 0.002,
      "step": 9880
    },
    {
      "epoch": 0.7325925925925926,
      "grad_norm": 0.0834362581372261,
      "learning_rate": 3.1685185185185185e-05,
      "loss": 0.0031,
      "step": 9890
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0024,
      "step": 9900
    },
    {
      "epoch": 0.7340740740740741,
      "grad_norm": 0.13469859957695007,
      "learning_rate": 3.164814814814815e-05,
      "loss": 0.0023,
      "step": 9910
    },
    {
      "epoch": 0.7348148148148148,
      "grad_norm": 0.22482997179031372,
      "learning_rate": 3.1629629629629634e-05,
      "loss": 0.0011,
      "step": 9920
    },
    {
      "epoch": 0.7355555555555555,
      "grad_norm": 0.17926445603370667,
      "learning_rate": 3.1611111111111115e-05,
      "loss": 0.0017,
      "step": 9930
    },
    {
      "epoch": 0.7362962962962963,
      "grad_norm": 0.10699556767940521,
      "learning_rate": 3.1592592592592596e-05,
      "loss": 0.0031,
      "step": 9940
    },
    {
      "epoch": 0.737037037037037,
      "grad_norm": 0.10339143872261047,
      "learning_rate": 3.157407407407408e-05,
      "loss": 0.0023,
      "step": 9950
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 0.051510583609342575,
      "learning_rate": 3.155555555555556e-05,
      "loss": 0.0018,
      "step": 9960
    },
    {
      "epoch": 0.7385185185185185,
      "grad_norm": 0.0,
      "learning_rate": 3.153703703703704e-05,
      "loss": 0.0012,
      "step": 9970
    },
    {
      "epoch": 0.7392592592592593,
      "grad_norm": 0.05298493430018425,
      "learning_rate": 3.151851851851852e-05,
      "loss": 0.0013,
      "step": 9980
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.04505148530006409,
      "learning_rate": 3.15e-05,
      "loss": 0.0014,
      "step": 9990
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.10356267541646957,
      "learning_rate": 3.148148148148148e-05,
      "loss": 0.0018,
      "step": 10000
    },
    {
      "epoch": 0.7414814814814815,
      "grad_norm": 0.038660306483507156,
      "learning_rate": 3.146296296296297e-05,
      "loss": 0.0016,
      "step": 10010
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 0.12388648092746735,
      "learning_rate": 3.144444444444445e-05,
      "loss": 0.0018,
      "step": 10020
    },
    {
      "epoch": 0.7429629629629629,
      "grad_norm": 0.12985795736312866,
      "learning_rate": 3.142592592592592e-05,
      "loss": 0.0015,
      "step": 10030
    },
    {
      "epoch": 0.7437037037037038,
      "grad_norm": 0.03911145403981209,
      "learning_rate": 3.140740740740741e-05,
      "loss": 0.0012,
      "step": 10040
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.2107485979795456,
      "learning_rate": 3.138888888888889e-05,
      "loss": 0.0013,
      "step": 10050
    },
    {
      "epoch": 0.7451851851851852,
      "grad_norm": 0.12867951393127441,
      "learning_rate": 3.137037037037037e-05,
      "loss": 0.0019,
      "step": 10060
    },
    {
      "epoch": 0.7459259259259259,
      "grad_norm": 0.14450517296791077,
      "learning_rate": 3.135185185185185e-05,
      "loss": 0.0018,
      "step": 10070
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.038325436413288116,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0018,
      "step": 10080
    },
    {
      "epoch": 0.7474074074074074,
      "grad_norm": 0.08529426157474518,
      "learning_rate": 3.131481481481482e-05,
      "loss": 0.0015,
      "step": 10090
    },
    {
      "epoch": 0.7481481481481481,
      "grad_norm": 0.0394621416926384,
      "learning_rate": 3.1296296296296295e-05,
      "loss": 0.0016,
      "step": 10100
    },
    {
      "epoch": 0.7488888888888889,
      "grad_norm": 0.08580677211284637,
      "learning_rate": 3.1277777777777776e-05,
      "loss": 0.0018,
      "step": 10110
    },
    {
      "epoch": 0.7496296296296296,
      "grad_norm": 0.17784938216209412,
      "learning_rate": 3.1259259259259264e-05,
      "loss": 0.0015,
      "step": 10120
    },
    {
      "epoch": 0.7503703703703704,
      "grad_norm": 0.04215800762176514,
      "learning_rate": 3.1240740740740744e-05,
      "loss": 0.0024,
      "step": 10130
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 0.08601289987564087,
      "learning_rate": 3.1222222222222225e-05,
      "loss": 0.0023,
      "step": 10140
    },
    {
      "epoch": 0.7518518518518519,
      "grad_norm": 0.04302030801773071,
      "learning_rate": 3.1203703703703706e-05,
      "loss": 0.0011,
      "step": 10150
    },
    {
      "epoch": 0.7525925925925926,
      "grad_norm": 0.04471816122531891,
      "learning_rate": 3.118518518518519e-05,
      "loss": 0.0024,
      "step": 10160
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.0009,
      "step": 10170
    },
    {
      "epoch": 0.7540740740740741,
      "grad_norm": 0.0925067588686943,
      "learning_rate": 3.114814814814815e-05,
      "loss": 0.0012,
      "step": 10180
    },
    {
      "epoch": 0.7548148148148148,
      "grad_norm": 0.1146765947341919,
      "learning_rate": 3.112962962962963e-05,
      "loss": 0.0024,
      "step": 10190
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.05356047675013542,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.0012,
      "step": 10200
    },
    {
      "epoch": 0.7562962962962962,
      "grad_norm": 0.13669846951961517,
      "learning_rate": 3.10925925925926e-05,
      "loss": 0.0019,
      "step": 10210
    },
    {
      "epoch": 0.7570370370370371,
      "grad_norm": 0.18790766596794128,
      "learning_rate": 3.107407407407408e-05,
      "loss": 0.0015,
      "step": 10220
    },
    {
      "epoch": 0.7577777777777778,
      "grad_norm": 0.04328431189060211,
      "learning_rate": 3.105555555555555e-05,
      "loss": 0.0017,
      "step": 10230
    },
    {
      "epoch": 0.7585185185185185,
      "grad_norm": 0.09209778159856796,
      "learning_rate": 3.103703703703704e-05,
      "loss": 0.0018,
      "step": 10240
    },
    {
      "epoch": 0.7592592592592593,
      "grad_norm": 0.14996884763240814,
      "learning_rate": 3.101851851851852e-05,
      "loss": 0.0012,
      "step": 10250
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.04913832247257233,
      "learning_rate": 3.1e-05,
      "loss": 0.0016,
      "step": 10260
    },
    {
      "epoch": 0.7607407407407407,
      "grad_norm": 0.0916769877076149,
      "learning_rate": 3.098148148148148e-05,
      "loss": 0.0014,
      "step": 10270
    },
    {
      "epoch": 0.7614814814814815,
      "grad_norm": 0.11917825788259506,
      "learning_rate": 3.096296296296296e-05,
      "loss": 0.0019,
      "step": 10280
    },
    {
      "epoch": 0.7622222222222222,
      "grad_norm": 0.08997827768325806,
      "learning_rate": 3.094444444444445e-05,
      "loss": 0.0014,
      "step": 10290
    },
    {
      "epoch": 0.762962962962963,
      "grad_norm": 0.029563136398792267,
      "learning_rate": 3.0925925925925924e-05,
      "loss": 0.0013,
      "step": 10300
    },
    {
      "epoch": 0.7637037037037037,
      "grad_norm": 0.12262416630983353,
      "learning_rate": 3.0907407407407405e-05,
      "loss": 0.0019,
      "step": 10310
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.09646856784820557,
      "learning_rate": 3.088888888888889e-05,
      "loss": 0.0026,
      "step": 10320
    },
    {
      "epoch": 0.7651851851851852,
      "grad_norm": 0.03884175047278404,
      "learning_rate": 3.0870370370370374e-05,
      "loss": 0.0018,
      "step": 10330
    },
    {
      "epoch": 0.7659259259259259,
      "grad_norm": 0.041465334594249725,
      "learning_rate": 3.0851851851851854e-05,
      "loss": 0.0018,
      "step": 10340
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.12329611927270889,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0023,
      "step": 10350
    },
    {
      "epoch": 0.7674074074074074,
      "grad_norm": 0.14671723544597626,
      "learning_rate": 3.0814814814814816e-05,
      "loss": 0.0014,
      "step": 10360
    },
    {
      "epoch": 0.7681481481481481,
      "grad_norm": 0.09388435631990433,
      "learning_rate": 3.07962962962963e-05,
      "loss": 0.0018,
      "step": 10370
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 0.11752380430698395,
      "learning_rate": 3.077777777777778e-05,
      "loss": 0.0015,
      "step": 10380
    },
    {
      "epoch": 0.7696296296296297,
      "grad_norm": 0.03916410729289055,
      "learning_rate": 3.075925925925926e-05,
      "loss": 0.0018,
      "step": 10390
    },
    {
      "epoch": 0.7703703703703704,
      "grad_norm": 0.10387309640645981,
      "learning_rate": 3.074074074074074e-05,
      "loss": 0.0028,
      "step": 10400
    },
    {
      "epoch": 0.7711111111111111,
      "grad_norm": 0.10432647913694382,
      "learning_rate": 3.0722222222222227e-05,
      "loss": 0.0017,
      "step": 10410
    },
    {
      "epoch": 0.7718518518518519,
      "grad_norm": 0.0,
      "learning_rate": 3.070370370370371e-05,
      "loss": 0.0018,
      "step": 10420
    },
    {
      "epoch": 0.7725925925925926,
      "grad_norm": 0.1095549687743187,
      "learning_rate": 3.068518518518519e-05,
      "loss": 0.0027,
      "step": 10430
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.10186834633350372,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0019,
      "step": 10440
    },
    {
      "epoch": 0.774074074074074,
      "grad_norm": 0.11087960004806519,
      "learning_rate": 3.064814814814815e-05,
      "loss": 0.0021,
      "step": 10450
    },
    {
      "epoch": 0.7748148148148148,
      "grad_norm": 0.02960227243602276,
      "learning_rate": 3.062962962962963e-05,
      "loss": 0.0021,
      "step": 10460
    },
    {
      "epoch": 0.7755555555555556,
      "grad_norm": 0.12683328986167908,
      "learning_rate": 3.061111111111111e-05,
      "loss": 0.0034,
      "step": 10470
    },
    {
      "epoch": 0.7762962962962963,
      "grad_norm": 0.09772080183029175,
      "learning_rate": 3.059259259259259e-05,
      "loss": 0.0014,
      "step": 10480
    },
    {
      "epoch": 0.7770370370370371,
      "grad_norm": 0.06346326321363449,
      "learning_rate": 3.057407407407408e-05,
      "loss": 0.0018,
      "step": 10490
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.03148863464593887,
      "learning_rate": 3.055555555555556e-05,
      "loss": 0.002,
      "step": 10500
    },
    {
      "epoch": 0.7785185185185185,
      "grad_norm": 0.11102771013975143,
      "learning_rate": 3.0537037037037034e-05,
      "loss": 0.0019,
      "step": 10510
    },
    {
      "epoch": 0.7792592592592592,
      "grad_norm": 0.04988690838217735,
      "learning_rate": 3.0518518518518515e-05,
      "loss": 0.0011,
      "step": 10520
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.11538480222225189,
      "learning_rate": 3.05e-05,
      "loss": 0.0012,
      "step": 10530
    },
    {
      "epoch": 0.7807407407407407,
      "grad_norm": 0.0,
      "learning_rate": 3.0481481481481484e-05,
      "loss": 0.0021,
      "step": 10540
    },
    {
      "epoch": 0.7814814814814814,
      "grad_norm": 0.062322601675987244,
      "learning_rate": 3.0462962962962964e-05,
      "loss": 0.0018,
      "step": 10550
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 0.041748061776161194,
      "learning_rate": 3.044444444444445e-05,
      "loss": 0.001,
      "step": 10560
    },
    {
      "epoch": 0.782962962962963,
      "grad_norm": 0.015727661550045013,
      "learning_rate": 3.042592592592593e-05,
      "loss": 0.0022,
      "step": 10570
    },
    {
      "epoch": 0.7837037037037037,
      "grad_norm": 0.07229194045066833,
      "learning_rate": 3.0407407407407407e-05,
      "loss": 0.0021,
      "step": 10580
    },
    {
      "epoch": 0.7844444444444445,
      "grad_norm": 0.1567687839269638,
      "learning_rate": 3.0388888888888887e-05,
      "loss": 0.0016,
      "step": 10590
    },
    {
      "epoch": 0.7851851851851852,
      "grad_norm": 0.04393496736884117,
      "learning_rate": 3.037037037037037e-05,
      "loss": 0.0015,
      "step": 10600
    },
    {
      "epoch": 0.7859259259259259,
      "grad_norm": 0.11178141832351685,
      "learning_rate": 3.0351851851851852e-05,
      "loss": 0.0017,
      "step": 10610
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.04346437379717827,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0019,
      "step": 10620
    },
    {
      "epoch": 0.7874074074074074,
      "grad_norm": 0.08232478052377701,
      "learning_rate": 3.0314814814814817e-05,
      "loss": 0.002,
      "step": 10630
    },
    {
      "epoch": 0.7881481481481482,
      "grad_norm": 0.09849714487791061,
      "learning_rate": 3.02962962962963e-05,
      "loss": 0.0015,
      "step": 10640
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 0.04095526784658432,
      "learning_rate": 3.0277777777777776e-05,
      "loss": 0.002,
      "step": 10650
    },
    {
      "epoch": 0.7896296296296297,
      "grad_norm": 0.11654984951019287,
      "learning_rate": 3.025925925925926e-05,
      "loss": 0.001,
      "step": 10660
    },
    {
      "epoch": 0.7903703703703704,
      "grad_norm": 0.128110870718956,
      "learning_rate": 3.024074074074074e-05,
      "loss": 0.0007,
      "step": 10670
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 0.053094007074832916,
      "learning_rate": 3.0222222222222225e-05,
      "loss": 0.0017,
      "step": 10680
    },
    {
      "epoch": 0.7918518518518518,
      "grad_norm": 0.04215141385793686,
      "learning_rate": 3.0203703703703705e-05,
      "loss": 0.0015,
      "step": 10690
    },
    {
      "epoch": 0.7925925925925926,
      "grad_norm": 0.04167579114437103,
      "learning_rate": 3.018518518518519e-05,
      "loss": 0.0022,
      "step": 10700
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.08315622806549072,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.001,
      "step": 10710
    },
    {
      "epoch": 0.794074074074074,
      "grad_norm": 0.0989854633808136,
      "learning_rate": 3.0148148148148148e-05,
      "loss": 0.0015,
      "step": 10720
    },
    {
      "epoch": 0.7948148148148149,
      "grad_norm": 0.10597208142280579,
      "learning_rate": 3.012962962962963e-05,
      "loss": 0.0009,
      "step": 10730
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 0.03196300193667412,
      "learning_rate": 3.0111111111111113e-05,
      "loss": 0.0014,
      "step": 10740
    },
    {
      "epoch": 0.7962962962962963,
      "grad_norm": 0.09039642661809921,
      "learning_rate": 3.0092592592592593e-05,
      "loss": 0.0012,
      "step": 10750
    },
    {
      "epoch": 0.797037037037037,
      "grad_norm": 0.13382995128631592,
      "learning_rate": 3.0074074074074078e-05,
      "loss": 0.0018,
      "step": 10760
    },
    {
      "epoch": 0.7977777777777778,
      "grad_norm": 0.12628233432769775,
      "learning_rate": 3.005555555555556e-05,
      "loss": 0.0015,
      "step": 10770
    },
    {
      "epoch": 0.7985185185185185,
      "grad_norm": 0.12040696293115616,
      "learning_rate": 3.0037037037037036e-05,
      "loss": 0.0019,
      "step": 10780
    },
    {
      "epoch": 0.7992592592592592,
      "grad_norm": 0.01909182220697403,
      "learning_rate": 3.0018518518518517e-05,
      "loss": 0.0025,
      "step": 10790
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.05649648979306221,
      "learning_rate": 3e-05,
      "loss": 0.0016,
      "step": 10800
    },
    {
      "epoch": 0.8007407407407408,
      "grad_norm": 0.0,
      "learning_rate": 2.998148148148148e-05,
      "loss": 0.0016,
      "step": 10810
    },
    {
      "epoch": 0.8014814814814815,
      "grad_norm": 0.11164405196905136,
      "learning_rate": 2.9962962962962966e-05,
      "loss": 0.0016,
      "step": 10820
    },
    {
      "epoch": 0.8022222222222222,
      "grad_norm": 0.05016244575381279,
      "learning_rate": 2.9944444444444446e-05,
      "loss": 0.0013,
      "step": 10830
    },
    {
      "epoch": 0.802962962962963,
      "grad_norm": 0.06849566102027893,
      "learning_rate": 2.992592592592593e-05,
      "loss": 0.0011,
      "step": 10840
    },
    {
      "epoch": 0.8037037037037037,
      "grad_norm": 0.0,
      "learning_rate": 2.9907407407407405e-05,
      "loss": 0.0018,
      "step": 10850
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 0.07637795060873032,
      "learning_rate": 2.988888888888889e-05,
      "loss": 0.0018,
      "step": 10860
    },
    {
      "epoch": 0.8051851851851852,
      "grad_norm": 0.10098350793123245,
      "learning_rate": 2.987037037037037e-05,
      "loss": 0.0016,
      "step": 10870
    },
    {
      "epoch": 0.8059259259259259,
      "grad_norm": 0.06460008770227432,
      "learning_rate": 2.9851851851851854e-05,
      "loss": 0.002,
      "step": 10880
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.1368410885334015,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0013,
      "step": 10890
    },
    {
      "epoch": 0.8074074074074075,
      "grad_norm": 0.086648128926754,
      "learning_rate": 2.981481481481482e-05,
      "loss": 0.0019,
      "step": 10900
    },
    {
      "epoch": 0.8081481481481482,
      "grad_norm": 0.0,
      "learning_rate": 2.97962962962963e-05,
      "loss": 0.0018,
      "step": 10910
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 0.105465367436409,
      "learning_rate": 2.9777777777777777e-05,
      "loss": 0.0014,
      "step": 10920
    },
    {
      "epoch": 0.8096296296296296,
      "grad_norm": 0.059908170253038406,
      "learning_rate": 2.9759259259259258e-05,
      "loss": 0.0021,
      "step": 10930
    },
    {
      "epoch": 0.8103703703703704,
      "grad_norm": 0.15445344150066376,
      "learning_rate": 2.9740740740740742e-05,
      "loss": 0.0019,
      "step": 10940
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.05362967774271965,
      "learning_rate": 2.9722222222222223e-05,
      "loss": 0.0024,
      "step": 10950
    },
    {
      "epoch": 0.8118518518518518,
      "grad_norm": 0.07243139296770096,
      "learning_rate": 2.9703703703703707e-05,
      "loss": 0.0013,
      "step": 10960
    },
    {
      "epoch": 0.8125925925925926,
      "grad_norm": 0.13489055633544922,
      "learning_rate": 2.9685185185185188e-05,
      "loss": 0.0015,
      "step": 10970
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0015,
      "step": 10980
    },
    {
      "epoch": 0.8140740740740741,
      "grad_norm": 0.0,
      "learning_rate": 2.9648148148148146e-05,
      "loss": 0.002,
      "step": 10990
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 0.08447127789258957,
      "learning_rate": 2.962962962962963e-05,
      "loss": 0.0017,
      "step": 11000
    },
    {
      "epoch": 0.8155555555555556,
      "grad_norm": 0.0,
      "learning_rate": 2.961111111111111e-05,
      "loss": 0.0009,
      "step": 11010
    },
    {
      "epoch": 0.8162962962962963,
      "grad_norm": 0.08086921274662018,
      "learning_rate": 2.9592592592592595e-05,
      "loss": 0.0013,
      "step": 11020
    },
    {
      "epoch": 0.817037037037037,
      "grad_norm": 0.08103223145008087,
      "learning_rate": 2.9574074074074076e-05,
      "loss": 0.0013,
      "step": 11030
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.0,
      "learning_rate": 2.955555555555556e-05,
      "loss": 0.0018,
      "step": 11040
    },
    {
      "epoch": 0.8185185185185185,
      "grad_norm": 0.0,
      "learning_rate": 2.953703703703704e-05,
      "loss": 0.0017,
      "step": 11050
    },
    {
      "epoch": 0.8192592592592592,
      "grad_norm": 0.04816068708896637,
      "learning_rate": 2.9518518518518518e-05,
      "loss": 0.0016,
      "step": 11060
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.12895043194293976,
      "learning_rate": 2.95e-05,
      "loss": 0.0017,
      "step": 11070
    },
    {
      "epoch": 0.8207407407407408,
      "grad_norm": 0.0749814510345459,
      "learning_rate": 2.9481481481481483e-05,
      "loss": 0.0024,
      "step": 11080
    },
    {
      "epoch": 0.8214814814814815,
      "grad_norm": 0.0876973494887352,
      "learning_rate": 2.9462962962962964e-05,
      "loss": 0.0026,
      "step": 11090
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.06734879314899445,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 0.0015,
      "step": 11100
    },
    {
      "epoch": 0.822962962962963,
      "grad_norm": 0.046856753528118134,
      "learning_rate": 2.942592592592593e-05,
      "loss": 0.0027,
      "step": 11110
    },
    {
      "epoch": 0.8237037037037037,
      "grad_norm": 0.062002018094062805,
      "learning_rate": 2.9407407407407413e-05,
      "loss": 0.0015,
      "step": 11120
    },
    {
      "epoch": 0.8244444444444444,
      "grad_norm": 0.09770021587610245,
      "learning_rate": 2.9388888888888887e-05,
      "loss": 0.0012,
      "step": 11130
    },
    {
      "epoch": 0.8251851851851851,
      "grad_norm": 0.1480105221271515,
      "learning_rate": 2.937037037037037e-05,
      "loss": 0.0022,
      "step": 11140
    },
    {
      "epoch": 0.825925925925926,
      "grad_norm": 0.15725825726985931,
      "learning_rate": 2.9351851851851852e-05,
      "loss": 0.0016,
      "step": 11150
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.04441588744521141,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0036,
      "step": 11160
    },
    {
      "epoch": 0.8274074074074074,
      "grad_norm": 0.04058437794446945,
      "learning_rate": 2.9314814814814817e-05,
      "loss": 0.0026,
      "step": 11170
    },
    {
      "epoch": 0.8281481481481482,
      "grad_norm": 0.12283939868211746,
      "learning_rate": 2.92962962962963e-05,
      "loss": 0.0008,
      "step": 11180
    },
    {
      "epoch": 0.8288888888888889,
      "grad_norm": 0.08050613850355148,
      "learning_rate": 2.927777777777778e-05,
      "loss": 0.0013,
      "step": 11190
    },
    {
      "epoch": 0.8296296296296296,
      "grad_norm": 0.10679090023040771,
      "learning_rate": 2.925925925925926e-05,
      "loss": 0.0014,
      "step": 11200
    },
    {
      "epoch": 0.8303703703703704,
      "grad_norm": 0.08779311180114746,
      "learning_rate": 2.924074074074074e-05,
      "loss": 0.0028,
      "step": 11210
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 0.04451306164264679,
      "learning_rate": 2.9222222222222224e-05,
      "loss": 0.0013,
      "step": 11220
    },
    {
      "epoch": 0.8318518518518518,
      "grad_norm": 0.08015315234661102,
      "learning_rate": 2.9203703703703705e-05,
      "loss": 0.0011,
      "step": 11230
    },
    {
      "epoch": 0.8325925925925926,
      "grad_norm": 0.07929610460996628,
      "learning_rate": 2.918518518518519e-05,
      "loss": 0.0013,
      "step": 11240
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.06393676996231079,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0026,
      "step": 11250
    },
    {
      "epoch": 0.8340740740740741,
      "grad_norm": 0.07881536334753036,
      "learning_rate": 2.914814814814815e-05,
      "loss": 0.0017,
      "step": 11260
    },
    {
      "epoch": 0.8348148148148148,
      "grad_norm": 0.1023220494389534,
      "learning_rate": 2.9129629629629628e-05,
      "loss": 0.0023,
      "step": 11270
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.03798317909240723,
      "learning_rate": 2.9111111111111112e-05,
      "loss": 0.0017,
      "step": 11280
    },
    {
      "epoch": 0.8362962962962963,
      "grad_norm": 0.04892709106206894,
      "learning_rate": 2.9092592592592593e-05,
      "loss": 0.0019,
      "step": 11290
    },
    {
      "epoch": 0.837037037037037,
      "grad_norm": 0.09731701761484146,
      "learning_rate": 2.9074074074074077e-05,
      "loss": 0.0017,
      "step": 11300
    },
    {
      "epoch": 0.8377777777777777,
      "grad_norm": 0.07961992919445038,
      "learning_rate": 2.9055555555555558e-05,
      "loss": 0.0018,
      "step": 11310
    },
    {
      "epoch": 0.8385185185185186,
      "grad_norm": 0.102146215736866,
      "learning_rate": 2.9037037037037042e-05,
      "loss": 0.0018,
      "step": 11320
    },
    {
      "epoch": 0.8392592592592593,
      "grad_norm": 0.060024771839380264,
      "learning_rate": 2.9018518518518523e-05,
      "loss": 0.0013,
      "step": 11330
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.042890891432762146,
      "learning_rate": 2.9e-05,
      "loss": 0.0015,
      "step": 11340
    },
    {
      "epoch": 0.8407407407407408,
      "grad_norm": 0.07923351228237152,
      "learning_rate": 2.898148148148148e-05,
      "loss": 0.0007,
      "step": 11350
    },
    {
      "epoch": 0.8414814814814815,
      "grad_norm": 0.11815525591373444,
      "learning_rate": 2.8962962962962965e-05,
      "loss": 0.0012,
      "step": 11360
    },
    {
      "epoch": 0.8422222222222222,
      "grad_norm": 0.0,
      "learning_rate": 2.8944444444444446e-05,
      "loss": 0.0008,
      "step": 11370
    },
    {
      "epoch": 0.8429629629629629,
      "grad_norm": 0.18961460888385773,
      "learning_rate": 2.892592592592593e-05,
      "loss": 0.0023,
      "step": 11380
    },
    {
      "epoch": 0.8437037037037037,
      "grad_norm": 0.06525018811225891,
      "learning_rate": 2.890740740740741e-05,
      "loss": 0.0019,
      "step": 11390
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.0,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 0.0013,
      "step": 11400
    },
    {
      "epoch": 0.8451851851851852,
      "grad_norm": 0.12628839910030365,
      "learning_rate": 2.887037037037037e-05,
      "loss": 0.0031,
      "step": 11410
    },
    {
      "epoch": 0.845925925925926,
      "grad_norm": 0.10207217186689377,
      "learning_rate": 2.8851851851851853e-05,
      "loss": 0.0014,
      "step": 11420
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.04739801213145256,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0016,
      "step": 11430
    },
    {
      "epoch": 0.8474074074074074,
      "grad_norm": 0.12740737199783325,
      "learning_rate": 2.8814814814814818e-05,
      "loss": 0.0014,
      "step": 11440
    },
    {
      "epoch": 0.8481481481481481,
      "grad_norm": 0.04851103201508522,
      "learning_rate": 2.87962962962963e-05,
      "loss": 0.0027,
      "step": 11450
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 0.06623625755310059,
      "learning_rate": 2.877777777777778e-05,
      "loss": 0.0011,
      "step": 11460
    },
    {
      "epoch": 0.8496296296296296,
      "grad_norm": 0.16888807713985443,
      "learning_rate": 2.8759259259259257e-05,
      "loss": 0.0015,
      "step": 11470
    },
    {
      "epoch": 0.8503703703703703,
      "grad_norm": 0.12237159162759781,
      "learning_rate": 2.874074074074074e-05,
      "loss": 0.002,
      "step": 11480
    },
    {
      "epoch": 0.8511111111111112,
      "grad_norm": 0.1983315646648407,
      "learning_rate": 2.8722222222222222e-05,
      "loss": 0.0019,
      "step": 11490
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 0.08615094423294067,
      "learning_rate": 2.8703703703703706e-05,
      "loss": 0.0018,
      "step": 11500
    },
    {
      "epoch": 0.8525925925925926,
      "grad_norm": 0.09788603335618973,
      "learning_rate": 2.8685185185185187e-05,
      "loss": 0.0021,
      "step": 11510
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.11371154338121414,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0016,
      "step": 11520
    },
    {
      "epoch": 0.8540740740740741,
      "grad_norm": 0.1505810171365738,
      "learning_rate": 2.8648148148148152e-05,
      "loss": 0.0021,
      "step": 11530
    },
    {
      "epoch": 0.8548148148148148,
      "grad_norm": 0.1012871116399765,
      "learning_rate": 2.862962962962963e-05,
      "loss": 0.0012,
      "step": 11540
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 0.0,
      "learning_rate": 2.861111111111111e-05,
      "loss": 0.0012,
      "step": 11550
    },
    {
      "epoch": 0.8562962962962963,
      "grad_norm": 0.07995876669883728,
      "learning_rate": 2.8592592592592594e-05,
      "loss": 0.0024,
      "step": 11560
    },
    {
      "epoch": 0.857037037037037,
      "grad_norm": 0.0613509900867939,
      "learning_rate": 2.8574074074074075e-05,
      "loss": 0.003,
      "step": 11570
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 0.10053907334804535,
      "learning_rate": 2.855555555555556e-05,
      "loss": 0.0012,
      "step": 11580
    },
    {
      "epoch": 0.8585185185185186,
      "grad_norm": 0.09917020052671432,
      "learning_rate": 2.853703703703704e-05,
      "loss": 0.001,
      "step": 11590
    },
    {
      "epoch": 0.8592592592592593,
      "grad_norm": 0.2448330819606781,
      "learning_rate": 2.851851851851852e-05,
      "loss": 0.0011,
      "step": 11600
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.1257103979587555,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0015,
      "step": 11610
    },
    {
      "epoch": 0.8607407407407407,
      "grad_norm": 0.09778258949518204,
      "learning_rate": 2.8481481481481482e-05,
      "loss": 0.0013,
      "step": 11620
    },
    {
      "epoch": 0.8614814814814815,
      "grad_norm": 0.07997111976146698,
      "learning_rate": 2.8462962962962963e-05,
      "loss": 0.0017,
      "step": 11630
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 0.07611741125583649,
      "learning_rate": 2.8444444444444447e-05,
      "loss": 0.0012,
      "step": 11640
    },
    {
      "epoch": 0.8629629629629629,
      "grad_norm": 0.09909464418888092,
      "learning_rate": 2.8425925925925928e-05,
      "loss": 0.0022,
      "step": 11650
    },
    {
      "epoch": 0.8637037037037038,
      "grad_norm": 0.04050496220588684,
      "learning_rate": 2.840740740740741e-05,
      "loss": 0.0013,
      "step": 11660
    },
    {
      "epoch": 0.8644444444444445,
      "grad_norm": 0.07923067361116409,
      "learning_rate": 2.8388888888888893e-05,
      "loss": 0.0016,
      "step": 11670
    },
    {
      "epoch": 0.8651851851851852,
      "grad_norm": 0.06342179328203201,
      "learning_rate": 2.837037037037037e-05,
      "loss": 0.0007,
      "step": 11680
    },
    {
      "epoch": 0.8659259259259259,
      "grad_norm": 0.0,
      "learning_rate": 2.835185185185185e-05,
      "loss": 0.001,
      "step": 11690
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.16251379251480103,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0026,
      "step": 11700
    },
    {
      "epoch": 0.8674074074074074,
      "grad_norm": 0.1355944722890854,
      "learning_rate": 2.8314814814814816e-05,
      "loss": 0.002,
      "step": 11710
    },
    {
      "epoch": 0.8681481481481481,
      "grad_norm": 0.36221766471862793,
      "learning_rate": 2.8296296296296297e-05,
      "loss": 0.0012,
      "step": 11720
    },
    {
      "epoch": 0.8688888888888889,
      "grad_norm": 0.10030993819236755,
      "learning_rate": 2.827777777777778e-05,
      "loss": 0.0011,
      "step": 11730
    },
    {
      "epoch": 0.8696296296296296,
      "grad_norm": 0.08836371451616287,
      "learning_rate": 2.8259259259259262e-05,
      "loss": 0.0022,
      "step": 11740
    },
    {
      "epoch": 0.8703703703703703,
      "grad_norm": 0.12782253324985504,
      "learning_rate": 2.824074074074074e-05,
      "loss": 0.0021,
      "step": 11750
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.1489957720041275,
      "learning_rate": 2.8222222222222223e-05,
      "loss": 0.0016,
      "step": 11760
    },
    {
      "epoch": 0.8718518518518519,
      "grad_norm": 0.08563075959682465,
      "learning_rate": 2.8203703703703704e-05,
      "loss": 0.0014,
      "step": 11770
    },
    {
      "epoch": 0.8725925925925926,
      "grad_norm": 0.03899788856506348,
      "learning_rate": 2.8185185185185185e-05,
      "loss": 0.0019,
      "step": 11780
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.0801072046160698,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.0016,
      "step": 11790
    },
    {
      "epoch": 0.8740740740740741,
      "grad_norm": 0.08173754811286926,
      "learning_rate": 2.814814814814815e-05,
      "loss": 0.0016,
      "step": 11800
    },
    {
      "epoch": 0.8748148148148148,
      "grad_norm": 0.07802635431289673,
      "learning_rate": 2.8129629629629634e-05,
      "loss": 0.0021,
      "step": 11810
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 0.23985537886619568,
      "learning_rate": 2.811111111111111e-05,
      "loss": 0.0023,
      "step": 11820
    },
    {
      "epoch": 0.8762962962962964,
      "grad_norm": 0.07581239193677902,
      "learning_rate": 2.8092592592592592e-05,
      "loss": 0.0012,
      "step": 11830
    },
    {
      "epoch": 0.8770370370370371,
      "grad_norm": 0.1001845970749855,
      "learning_rate": 2.8074074074074076e-05,
      "loss": 0.0015,
      "step": 11840
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 0.04217607527971268,
      "learning_rate": 2.8055555555555557e-05,
      "loss": 0.0025,
      "step": 11850
    },
    {
      "epoch": 0.8785185185185185,
      "grad_norm": 0.03967275470495224,
      "learning_rate": 2.8037037037037038e-05,
      "loss": 0.0011,
      "step": 11860
    },
    {
      "epoch": 0.8792592592592593,
      "grad_norm": 0.0639662891626358,
      "learning_rate": 2.8018518518518522e-05,
      "loss": 0.0012,
      "step": 11870
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.0,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0014,
      "step": 11880
    },
    {
      "epoch": 0.8807407407407407,
      "grad_norm": 0.05621511489152908,
      "learning_rate": 2.798148148148148e-05,
      "loss": 0.0019,
      "step": 11890
    },
    {
      "epoch": 0.8814814814814815,
      "grad_norm": 0.07332856953144073,
      "learning_rate": 2.7962962962962965e-05,
      "loss": 0.0023,
      "step": 11900
    },
    {
      "epoch": 0.8822222222222222,
      "grad_norm": 0.07523781061172485,
      "learning_rate": 2.7944444444444445e-05,
      "loss": 0.0009,
      "step": 11910
    },
    {
      "epoch": 0.882962962962963,
      "grad_norm": 0.05488619580864906,
      "learning_rate": 2.7925925925925926e-05,
      "loss": 0.0031,
      "step": 11920
    },
    {
      "epoch": 0.8837037037037037,
      "grad_norm": 0.11605121940374374,
      "learning_rate": 2.790740740740741e-05,
      "loss": 0.0016,
      "step": 11930
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 0.07017351686954498,
      "learning_rate": 2.788888888888889e-05,
      "loss": 0.0012,
      "step": 11940
    },
    {
      "epoch": 0.8851851851851852,
      "grad_norm": 0.11608047038316727,
      "learning_rate": 2.7870370370370375e-05,
      "loss": 0.0019,
      "step": 11950
    },
    {
      "epoch": 0.8859259259259259,
      "grad_norm": 0.03890271857380867,
      "learning_rate": 2.7851851851851853e-05,
      "loss": 0.0009,
      "step": 11960
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.04611877351999283,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0021,
      "step": 11970
    },
    {
      "epoch": 0.8874074074074074,
      "grad_norm": 0.15137149393558502,
      "learning_rate": 2.7814814814814814e-05,
      "loss": 0.0019,
      "step": 11980
    },
    {
      "epoch": 0.8881481481481481,
      "grad_norm": 0.026366187259554863,
      "learning_rate": 2.77962962962963e-05,
      "loss": 0.0016,
      "step": 11990
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.1131555512547493,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.002,
      "step": 12000
    },
    {
      "epoch": 0.8896296296296297,
      "grad_norm": 0.0,
      "learning_rate": 2.7759259259259263e-05,
      "loss": 0.001,
      "step": 12010
    },
    {
      "epoch": 0.8903703703703704,
      "grad_norm": 0.09560566395521164,
      "learning_rate": 2.774074074074074e-05,
      "loss": 0.0026,
      "step": 12020
    },
    {
      "epoch": 0.8911111111111111,
      "grad_norm": 0.06941894441843033,
      "learning_rate": 2.772222222222222e-05,
      "loss": 0.0027,
      "step": 12030
    },
    {
      "epoch": 0.8918518518518519,
      "grad_norm": 0.040047984570264816,
      "learning_rate": 2.7703703703703706e-05,
      "loss": 0.0013,
      "step": 12040
    },
    {
      "epoch": 0.8925925925925926,
      "grad_norm": 0.04037757217884064,
      "learning_rate": 2.7685185185185186e-05,
      "loss": 0.0013,
      "step": 12050
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.22100530564785004,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0011,
      "step": 12060
    },
    {
      "epoch": 0.894074074074074,
      "grad_norm": 0.05994618684053421,
      "learning_rate": 2.764814814814815e-05,
      "loss": 0.0023,
      "step": 12070
    },
    {
      "epoch": 0.8948148148148148,
      "grad_norm": 0.070064976811409,
      "learning_rate": 2.7629629629629632e-05,
      "loss": 0.0028,
      "step": 12080
    },
    {
      "epoch": 0.8955555555555555,
      "grad_norm": 0.0,
      "learning_rate": 2.761111111111111e-05,
      "loss": 0.0014,
      "step": 12090
    },
    {
      "epoch": 0.8962962962962963,
      "grad_norm": 0.0,
      "learning_rate": 2.7592592592592594e-05,
      "loss": 0.0018,
      "step": 12100
    },
    {
      "epoch": 0.8970370370370371,
      "grad_norm": 0.07176439464092255,
      "learning_rate": 2.7574074074074074e-05,
      "loss": 0.0025,
      "step": 12110
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 0.11030958592891693,
      "learning_rate": 2.7555555555555555e-05,
      "loss": 0.0014,
      "step": 12120
    },
    {
      "epoch": 0.8985185185185185,
      "grad_norm": 0.15386249125003815,
      "learning_rate": 2.753703703703704e-05,
      "loss": 0.0014,
      "step": 12130
    },
    {
      "epoch": 0.8992592592592593,
      "grad_norm": 0.03919007256627083,
      "learning_rate": 2.751851851851852e-05,
      "loss": 0.0017,
      "step": 12140
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.0,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0011,
      "step": 12150
    },
    {
      "epoch": 0.9007407407407407,
      "grad_norm": 0.11669193208217621,
      "learning_rate": 2.7481481481481482e-05,
      "loss": 0.0026,
      "step": 12160
    },
    {
      "epoch": 0.9014814814814814,
      "grad_norm": 0.15430662035942078,
      "learning_rate": 2.7462962962962963e-05,
      "loss": 0.0016,
      "step": 12170
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 0.037521976977586746,
      "learning_rate": 2.7444444444444443e-05,
      "loss": 0.0018,
      "step": 12180
    },
    {
      "epoch": 0.902962962962963,
      "grad_norm": 0.10711267590522766,
      "learning_rate": 2.7425925925925927e-05,
      "loss": 0.0017,
      "step": 12190
    },
    {
      "epoch": 0.9037037037037037,
      "grad_norm": 0.09656842797994614,
      "learning_rate": 2.7407407407407408e-05,
      "loss": 0.0018,
      "step": 12200
    },
    {
      "epoch": 0.9044444444444445,
      "grad_norm": 0.1834283471107483,
      "learning_rate": 2.7388888888888892e-05,
      "loss": 0.0023,
      "step": 12210
    },
    {
      "epoch": 0.9051851851851852,
      "grad_norm": 0.08401484787464142,
      "learning_rate": 2.7370370370370373e-05,
      "loss": 0.0022,
      "step": 12220
    },
    {
      "epoch": 0.9059259259259259,
      "grad_norm": 0.18881261348724365,
      "learning_rate": 2.735185185185185e-05,
      "loss": 0.0024,
      "step": 12230
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.04958193004131317,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0015,
      "step": 12240
    },
    {
      "epoch": 0.9074074074074074,
      "grad_norm": 0.07286669313907623,
      "learning_rate": 2.7314814814814816e-05,
      "loss": 0.001,
      "step": 12250
    },
    {
      "epoch": 0.9081481481481481,
      "grad_norm": 0.03147321194410324,
      "learning_rate": 2.7296296296296296e-05,
      "loss": 0.0019,
      "step": 12260
    },
    {
      "epoch": 0.9088888888888889,
      "grad_norm": 0.14189580082893372,
      "learning_rate": 2.727777777777778e-05,
      "loss": 0.0012,
      "step": 12270
    },
    {
      "epoch": 0.9096296296296297,
      "grad_norm": 0.06113952025771141,
      "learning_rate": 2.725925925925926e-05,
      "loss": 0.0015,
      "step": 12280
    },
    {
      "epoch": 0.9103703703703704,
      "grad_norm": 0.15395799279212952,
      "learning_rate": 2.7240740740740745e-05,
      "loss": 0.0015,
      "step": 12290
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.06479156762361526,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 0.0012,
      "step": 12300
    },
    {
      "epoch": 0.9118518518518518,
      "grad_norm": 0.03979332000017166,
      "learning_rate": 2.7203703703703704e-05,
      "loss": 0.001,
      "step": 12310
    },
    {
      "epoch": 0.9125925925925926,
      "grad_norm": 0.05536976456642151,
      "learning_rate": 2.7185185185185184e-05,
      "loss": 0.0017,
      "step": 12320
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.01981976442039013,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0022,
      "step": 12330
    },
    {
      "epoch": 0.914074074074074,
      "grad_norm": 0.0,
      "learning_rate": 2.714814814814815e-05,
      "loss": 0.0015,
      "step": 12340
    },
    {
      "epoch": 0.9148148148148149,
      "grad_norm": 0.04431505501270294,
      "learning_rate": 2.7129629629629634e-05,
      "loss": 0.0007,
      "step": 12350
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 0.05936776101589203,
      "learning_rate": 2.7111111111111114e-05,
      "loss": 0.001,
      "step": 12360
    },
    {
      "epoch": 0.9162962962962963,
      "grad_norm": 0.039900798350572586,
      "learning_rate": 2.7092592592592592e-05,
      "loss": 0.0009,
      "step": 12370
    },
    {
      "epoch": 0.917037037037037,
      "grad_norm": 0.0,
      "learning_rate": 2.7074074074074072e-05,
      "loss": 0.0016,
      "step": 12380
    },
    {
      "epoch": 0.9177777777777778,
      "grad_norm": 0.1204918771982193,
      "learning_rate": 2.7055555555555557e-05,
      "loss": 0.0022,
      "step": 12390
    },
    {
      "epoch": 0.9185185185185185,
      "grad_norm": 0.029699482023715973,
      "learning_rate": 2.7037037037037037e-05,
      "loss": 0.0013,
      "step": 12400
    },
    {
      "epoch": 0.9192592592592592,
      "grad_norm": 0.04213796555995941,
      "learning_rate": 2.701851851851852e-05,
      "loss": 0.0012,
      "step": 12410
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.1032831072807312,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0016,
      "step": 12420
    },
    {
      "epoch": 0.9207407407407407,
      "grad_norm": 0.09047988802194595,
      "learning_rate": 2.6981481481481487e-05,
      "loss": 0.0018,
      "step": 12430
    },
    {
      "epoch": 0.9214814814814815,
      "grad_norm": 0.16215044260025024,
      "learning_rate": 2.696296296296296e-05,
      "loss": 0.0013,
      "step": 12440
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.06478110700845718,
      "learning_rate": 2.6944444444444445e-05,
      "loss": 0.0023,
      "step": 12450
    },
    {
      "epoch": 0.922962962962963,
      "grad_norm": 0.04873024299740791,
      "learning_rate": 2.6925925925925925e-05,
      "loss": 0.0019,
      "step": 12460
    },
    {
      "epoch": 0.9237037037037037,
      "grad_norm": 0.10368300974369049,
      "learning_rate": 2.690740740740741e-05,
      "loss": 0.0022,
      "step": 12470
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.04029550775885582,
      "learning_rate": 2.688888888888889e-05,
      "loss": 0.0021,
      "step": 12480
    },
    {
      "epoch": 0.9251851851851852,
      "grad_norm": 0.0,
      "learning_rate": 2.6870370370370375e-05,
      "loss": 0.0011,
      "step": 12490
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 0.041957154870033264,
      "learning_rate": 2.6851851851851855e-05,
      "loss": 0.0016,
      "step": 12500
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.08629501610994339,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0015,
      "step": 12510
    },
    {
      "epoch": 0.9274074074074075,
      "grad_norm": 0.048529550433158875,
      "learning_rate": 2.6814814814814814e-05,
      "loss": 0.0019,
      "step": 12520
    },
    {
      "epoch": 0.9281481481481482,
      "grad_norm": 0.059096936136484146,
      "learning_rate": 2.6796296296296298e-05,
      "loss": 0.0017,
      "step": 12530
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 0.08571358025074005,
      "learning_rate": 2.677777777777778e-05,
      "loss": 0.0011,
      "step": 12540
    },
    {
      "epoch": 0.9296296296296296,
      "grad_norm": 0.08550858497619629,
      "learning_rate": 2.6759259259259263e-05,
      "loss": 0.0022,
      "step": 12550
    },
    {
      "epoch": 0.9303703703703704,
      "grad_norm": 0.1330524981021881,
      "learning_rate": 2.6740740740740743e-05,
      "loss": 0.001,
      "step": 12560
    },
    {
      "epoch": 0.9311111111111111,
      "grad_norm": 0.10924020409584045,
      "learning_rate": 2.6722222222222228e-05,
      "loss": 0.0006,
      "step": 12570
    },
    {
      "epoch": 0.9318518518518518,
      "grad_norm": 0.030672196298837662,
      "learning_rate": 2.67037037037037e-05,
      "loss": 0.0015,
      "step": 12580
    },
    {
      "epoch": 0.9325925925925926,
      "grad_norm": 0.03025384061038494,
      "learning_rate": 2.6685185185185186e-05,
      "loss": 0.0018,
      "step": 12590
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.04334821179509163,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0016,
      "step": 12600
    },
    {
      "epoch": 0.9340740740740741,
      "grad_norm": 0.057889990508556366,
      "learning_rate": 2.664814814814815e-05,
      "loss": 0.0018,
      "step": 12610
    },
    {
      "epoch": 0.9348148148148148,
      "grad_norm": 0.08676651865243912,
      "learning_rate": 2.662962962962963e-05,
      "loss": 0.0025,
      "step": 12620
    },
    {
      "epoch": 0.9355555555555556,
      "grad_norm": 0.11664267629384995,
      "learning_rate": 2.6611111111111116e-05,
      "loss": 0.002,
      "step": 12630
    },
    {
      "epoch": 0.9362962962962963,
      "grad_norm": 0.12022732943296432,
      "learning_rate": 2.659259259259259e-05,
      "loss": 0.0015,
      "step": 12640
    },
    {
      "epoch": 0.937037037037037,
      "grad_norm": 0.12687963247299194,
      "learning_rate": 2.6574074074074074e-05,
      "loss": 0.0021,
      "step": 12650
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 0.07664112001657486,
      "learning_rate": 2.6555555555555555e-05,
      "loss": 0.0025,
      "step": 12660
    },
    {
      "epoch": 0.9385185185185185,
      "grad_norm": 0.0,
      "learning_rate": 2.653703703703704e-05,
      "loss": 0.0016,
      "step": 12670
    },
    {
      "epoch": 0.9392592592592592,
      "grad_norm": 0.10248038917779922,
      "learning_rate": 2.651851851851852e-05,
      "loss": 0.0011,
      "step": 12680
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.22934956848621368,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0015,
      "step": 12690
    },
    {
      "epoch": 0.9407407407407408,
      "grad_norm": 0.13075223565101624,
      "learning_rate": 2.6481481481481485e-05,
      "loss": 0.0019,
      "step": 12700
    },
    {
      "epoch": 0.9414814814814815,
      "grad_norm": 0.08381903171539307,
      "learning_rate": 2.6462962962962962e-05,
      "loss": 0.0015,
      "step": 12710
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.08393391221761703,
      "learning_rate": 2.6444444444444443e-05,
      "loss": 0.0015,
      "step": 12720
    },
    {
      "epoch": 0.942962962962963,
      "grad_norm": 0.026820236817002296,
      "learning_rate": 2.6425925925925927e-05,
      "loss": 0.0011,
      "step": 12730
    },
    {
      "epoch": 0.9437037037037037,
      "grad_norm": 0.0820387601852417,
      "learning_rate": 2.6407407407407408e-05,
      "loss": 0.0026,
      "step": 12740
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.022179014980793,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 0.0015,
      "step": 12750
    },
    {
      "epoch": 0.9451851851851852,
      "grad_norm": 0.08925522118806839,
      "learning_rate": 2.6370370370370373e-05,
      "loss": 0.0018,
      "step": 12760
    },
    {
      "epoch": 0.945925925925926,
      "grad_norm": 0.06624267250299454,
      "learning_rate": 2.6351851851851857e-05,
      "loss": 0.001,
      "step": 12770
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0014,
      "step": 12780
    },
    {
      "epoch": 0.9474074074074074,
      "grad_norm": 0.10536640137434006,
      "learning_rate": 2.6314814814814815e-05,
      "loss": 0.0018,
      "step": 12790
    },
    {
      "epoch": 0.9481481481481482,
      "grad_norm": 0.05716800317168236,
      "learning_rate": 2.6296296296296296e-05,
      "loss": 0.0016,
      "step": 12800
    },
    {
      "epoch": 0.9488888888888889,
      "grad_norm": 0.0,
      "learning_rate": 2.627777777777778e-05,
      "loss": 0.0015,
      "step": 12810
    },
    {
      "epoch": 0.9496296296296296,
      "grad_norm": 0.05573512241244316,
      "learning_rate": 2.625925925925926e-05,
      "loss": 0.0013,
      "step": 12820
    },
    {
      "epoch": 0.9503703703703704,
      "grad_norm": 0.07602780312299728,
      "learning_rate": 2.6240740740740745e-05,
      "loss": 0.0014,
      "step": 12830
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 0.05889710411429405,
      "learning_rate": 2.6222222222222226e-05,
      "loss": 0.0014,
      "step": 12840
    },
    {
      "epoch": 0.9518518518518518,
      "grad_norm": 0.09039834886789322,
      "learning_rate": 2.6203703703703703e-05,
      "loss": 0.0027,
      "step": 12850
    },
    {
      "epoch": 0.9525925925925925,
      "grad_norm": 0.04120128974318504,
      "learning_rate": 2.6185185185185184e-05,
      "loss": 0.0011,
      "step": 12860
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.01007101871073246,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0012,
      "step": 12870
    },
    {
      "epoch": 0.9540740740740741,
      "grad_norm": 0.06671378016471863,
      "learning_rate": 2.614814814814815e-05,
      "loss": 0.0018,
      "step": 12880
    },
    {
      "epoch": 0.9548148148148148,
      "grad_norm": 0.15123461186885834,
      "learning_rate": 2.6129629629629633e-05,
      "loss": 0.0018,
      "step": 12890
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.04132116585969925,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 0.0012,
      "step": 12900
    },
    {
      "epoch": 0.9562962962962963,
      "grad_norm": 0.13748984038829803,
      "learning_rate": 2.6092592592592598e-05,
      "loss": 0.0017,
      "step": 12910
    },
    {
      "epoch": 0.957037037037037,
      "grad_norm": 0.18892574310302734,
      "learning_rate": 2.6074074074074072e-05,
      "loss": 0.0028,
      "step": 12920
    },
    {
      "epoch": 0.9577777777777777,
      "grad_norm": 0.16620537638664246,
      "learning_rate": 2.6055555555555556e-05,
      "loss": 0.0029,
      "step": 12930
    },
    {
      "epoch": 0.9585185185185185,
      "grad_norm": 0.13163074851036072,
      "learning_rate": 2.6037037037037037e-05,
      "loss": 0.0015,
      "step": 12940
    },
    {
      "epoch": 0.9592592592592593,
      "grad_norm": 0.041566357016563416,
      "learning_rate": 2.601851851851852e-05,
      "loss": 0.0013,
      "step": 12950
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.04162215068936348,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0018,
      "step": 12960
    },
    {
      "epoch": 0.9607407407407408,
      "grad_norm": 0.03182844817638397,
      "learning_rate": 2.5981481481481486e-05,
      "loss": 0.0018,
      "step": 12970
    },
    {
      "epoch": 0.9614814814814815,
      "grad_norm": 0.11751588433980942,
      "learning_rate": 2.5962962962962967e-05,
      "loss": 0.0018,
      "step": 12980
    },
    {
      "epoch": 0.9622222222222222,
      "grad_norm": 0.16749943792819977,
      "learning_rate": 2.5944444444444444e-05,
      "loss": 0.0022,
      "step": 12990
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 0.04080658406019211,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 0.0013,
      "step": 13000
    },
    {
      "epoch": 0.9637037037037037,
      "grad_norm": 0.042309779673814774,
      "learning_rate": 2.590740740740741e-05,
      "loss": 0.0023,
      "step": 13010
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 0.0,
      "learning_rate": 2.588888888888889e-05,
      "loss": 0.0014,
      "step": 13020
    },
    {
      "epoch": 0.9651851851851851,
      "grad_norm": 0.0,
      "learning_rate": 2.5870370370370374e-05,
      "loss": 0.0012,
      "step": 13030
    },
    {
      "epoch": 0.965925925925926,
      "grad_norm": 0.1064583882689476,
      "learning_rate": 2.5851851851851855e-05,
      "loss": 0.0015,
      "step": 13040
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.110540471971035,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.0018,
      "step": 13050
    },
    {
      "epoch": 0.9674074074074074,
      "grad_norm": 0.0,
      "learning_rate": 2.5814814814814813e-05,
      "loss": 0.0013,
      "step": 13060
    },
    {
      "epoch": 0.9681481481481482,
      "grad_norm": 0.0,
      "learning_rate": 2.5796296296296297e-05,
      "loss": 0.0014,
      "step": 13070
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 0.039226233959198,
      "learning_rate": 2.5777777777777778e-05,
      "loss": 0.0019,
      "step": 13080
    },
    {
      "epoch": 0.9696296296296296,
      "grad_norm": 0.19680166244506836,
      "learning_rate": 2.5759259259259262e-05,
      "loss": 0.0017,
      "step": 13090
    },
    {
      "epoch": 0.9703703703703703,
      "grad_norm": 0.10712459683418274,
      "learning_rate": 2.5740740740740743e-05,
      "loss": 0.001,
      "step": 13100
    },
    {
      "epoch": 0.9711111111111111,
      "grad_norm": 0.10081275552511215,
      "learning_rate": 2.5722222222222224e-05,
      "loss": 0.0017,
      "step": 13110
    },
    {
      "epoch": 0.9718518518518519,
      "grad_norm": 0.07919570058584213,
      "learning_rate": 2.5703703703703708e-05,
      "loss": 0.0025,
      "step": 13120
    },
    {
      "epoch": 0.9725925925925926,
      "grad_norm": 0.10225281864404678,
      "learning_rate": 2.5685185185185185e-05,
      "loss": 0.0005,
      "step": 13130
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.04888613149523735,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0019,
      "step": 13140
    },
    {
      "epoch": 0.9740740740740741,
      "grad_norm": 0.13781803846359253,
      "learning_rate": 2.564814814814815e-05,
      "loss": 0.0018,
      "step": 13150
    },
    {
      "epoch": 0.9748148148148148,
      "grad_norm": 0.1231224313378334,
      "learning_rate": 2.562962962962963e-05,
      "loss": 0.0013,
      "step": 13160
    },
    {
      "epoch": 0.9755555555555555,
      "grad_norm": 0.16276298463344574,
      "learning_rate": 2.5611111111111115e-05,
      "loss": 0.0013,
      "step": 13170
    },
    {
      "epoch": 0.9762962962962963,
      "grad_norm": 0.041220732033252716,
      "learning_rate": 2.5592592592592596e-05,
      "loss": 0.0018,
      "step": 13180
    },
    {
      "epoch": 0.977037037037037,
      "grad_norm": 0.04515363648533821,
      "learning_rate": 2.5574074074074077e-05,
      "loss": 0.0016,
      "step": 13190
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.09578649699687958,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 0.0016,
      "step": 13200
    },
    {
      "epoch": 0.9785185185185186,
      "grad_norm": 0.0798720046877861,
      "learning_rate": 2.5537037037037038e-05,
      "loss": 0.0024,
      "step": 13210
    },
    {
      "epoch": 0.9792592592592593,
      "grad_norm": 0.06178173050284386,
      "learning_rate": 2.551851851851852e-05,
      "loss": 0.0015,
      "step": 13220
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.11199373751878738,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.003,
      "step": 13230
    },
    {
      "epoch": 0.9807407407407407,
      "grad_norm": 0.10244624316692352,
      "learning_rate": 2.5481481481481484e-05,
      "loss": 0.0019,
      "step": 13240
    },
    {
      "epoch": 0.9814814814814815,
      "grad_norm": 0.08729509264230728,
      "learning_rate": 2.5462962962962965e-05,
      "loss": 0.0014,
      "step": 13250
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 0.059003885835409164,
      "learning_rate": 2.5444444444444442e-05,
      "loss": 0.0012,
      "step": 13260
    },
    {
      "epoch": 0.9829629629629629,
      "grad_norm": 0.08663668483495712,
      "learning_rate": 2.5425925925925926e-05,
      "loss": 0.0021,
      "step": 13270
    },
    {
      "epoch": 0.9837037037037037,
      "grad_norm": 0.1947370022535324,
      "learning_rate": 2.5407407407407407e-05,
      "loss": 0.0025,
      "step": 13280
    },
    {
      "epoch": 0.9844444444444445,
      "grad_norm": 0.31324389576911926,
      "learning_rate": 2.538888888888889e-05,
      "loss": 0.002,
      "step": 13290
    },
    {
      "epoch": 0.9851851851851852,
      "grad_norm": 0.10923673957586288,
      "learning_rate": 2.5370370370370372e-05,
      "loss": 0.0016,
      "step": 13300
    },
    {
      "epoch": 0.9859259259259259,
      "grad_norm": 0.1170099601149559,
      "learning_rate": 2.5351851851851853e-05,
      "loss": 0.0035,
      "step": 13310
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.08762765675783157,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.001,
      "step": 13320
    },
    {
      "epoch": 0.9874074074074074,
      "grad_norm": 0.0,
      "learning_rate": 2.5314814814814814e-05,
      "loss": 0.0013,
      "step": 13330
    },
    {
      "epoch": 0.9881481481481481,
      "grad_norm": 0.06654705852270126,
      "learning_rate": 2.5296296296296295e-05,
      "loss": 0.0017,
      "step": 13340
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.04043973237276077,
      "learning_rate": 2.527777777777778e-05,
      "loss": 0.0021,
      "step": 13350
    },
    {
      "epoch": 0.9896296296296296,
      "grad_norm": 0.11039340496063232,
      "learning_rate": 2.525925925925926e-05,
      "loss": 0.0014,
      "step": 13360
    },
    {
      "epoch": 0.9903703703703703,
      "grad_norm": 0.03977927938103676,
      "learning_rate": 2.524074074074074e-05,
      "loss": 0.0011,
      "step": 13370
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 0.09381785243749619,
      "learning_rate": 2.5222222222222225e-05,
      "loss": 0.0017,
      "step": 13380
    },
    {
      "epoch": 0.9918518518518519,
      "grad_norm": 0.08108101040124893,
      "learning_rate": 2.5203703703703706e-05,
      "loss": 0.0018,
      "step": 13390
    },
    {
      "epoch": 0.9925925925925926,
      "grad_norm": 0.14662902057170868,
      "learning_rate": 2.5185185185185183e-05,
      "loss": 0.0022,
      "step": 13400
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0014,
      "step": 13410
    },
    {
      "epoch": 0.9940740740740741,
      "grad_norm": 0.07666885107755661,
      "learning_rate": 2.5148148148148148e-05,
      "loss": 0.0026,
      "step": 13420
    },
    {
      "epoch": 0.9948148148148148,
      "grad_norm": 0.08885566890239716,
      "learning_rate": 2.5129629629629632e-05,
      "loss": 0.0013,
      "step": 13430
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.04024234786629677,
      "learning_rate": 2.5111111111111113e-05,
      "loss": 0.0018,
      "step": 13440
    },
    {
      "epoch": 0.9962962962962963,
      "grad_norm": 0.04028927907347679,
      "learning_rate": 2.5092592592592594e-05,
      "loss": 0.0017,
      "step": 13450
    },
    {
      "epoch": 0.997037037037037,
      "grad_norm": 0.15240702033042908,
      "learning_rate": 2.5074074074074078e-05,
      "loss": 0.0017,
      "step": 13460
    },
    {
      "epoch": 0.9977777777777778,
      "grad_norm": 0.08976495265960693,
      "learning_rate": 2.5055555555555555e-05,
      "loss": 0.0019,
      "step": 13470
    },
    {
      "epoch": 0.9985185185185185,
      "grad_norm": 0.09442556649446487,
      "learning_rate": 2.5037037037037036e-05,
      "loss": 0.0024,
      "step": 13480
    },
    {
      "epoch": 0.9992592592592593,
      "grad_norm": 0.0,
      "learning_rate": 2.501851851851852e-05,
      "loss": 0.001,
      "step": 13490
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.0420820377767086,
      "learning_rate": 2.5e-05,
      "loss": 0.0013,
      "step": 13500
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0014654138358309865,
      "eval_runtime": 6.2055,
      "eval_samples_per_second": 966.887,
      "eval_steps_per_second": 24.172,
      "step": 13500
    },
    {
      "epoch": 1.0007407407407407,
      "grad_norm": 0.1438683569431305,
      "learning_rate": 2.4981481481481482e-05,
      "loss": 0.0015,
      "step": 13510
    },
    {
      "epoch": 1.0014814814814814,
      "grad_norm": 0.040134187787771225,
      "learning_rate": 2.4962962962962963e-05,
      "loss": 0.002,
      "step": 13520
    },
    {
      "epoch": 1.0022222222222221,
      "grad_norm": 0.0,
      "learning_rate": 2.4944444444444447e-05,
      "loss": 0.0018,
      "step": 13530
    },
    {
      "epoch": 1.002962962962963,
      "grad_norm": 0.057392511516809464,
      "learning_rate": 2.4925925925925928e-05,
      "loss": 0.0016,
      "step": 13540
    },
    {
      "epoch": 1.0037037037037038,
      "grad_norm": 0.08365558087825775,
      "learning_rate": 2.490740740740741e-05,
      "loss": 0.0017,
      "step": 13550
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 0.12309755384922028,
      "learning_rate": 2.488888888888889e-05,
      "loss": 0.0022,
      "step": 13560
    },
    {
      "epoch": 1.0051851851851852,
      "grad_norm": 0.0975182056427002,
      "learning_rate": 2.487037037037037e-05,
      "loss": 0.001,
      "step": 13570
    },
    {
      "epoch": 1.005925925925926,
      "grad_norm": 0.0743347629904747,
      "learning_rate": 2.4851851851851854e-05,
      "loss": 0.0016,
      "step": 13580
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.24002143740653992,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0014,
      "step": 13590
    },
    {
      "epoch": 1.0074074074074073,
      "grad_norm": 0.05579611659049988,
      "learning_rate": 2.4814814814814816e-05,
      "loss": 0.0019,
      "step": 13600
    },
    {
      "epoch": 1.0081481481481482,
      "grad_norm": 0.0766592025756836,
      "learning_rate": 2.4796296296296297e-05,
      "loss": 0.0008,
      "step": 13610
    },
    {
      "epoch": 1.008888888888889,
      "grad_norm": 0.13318492472171783,
      "learning_rate": 2.477777777777778e-05,
      "loss": 0.0011,
      "step": 13620
    },
    {
      "epoch": 1.0096296296296297,
      "grad_norm": 0.05383216589689255,
      "learning_rate": 2.475925925925926e-05,
      "loss": 0.0023,
      "step": 13630
    },
    {
      "epoch": 1.0103703703703704,
      "grad_norm": 0.039601583033800125,
      "learning_rate": 2.4740740740740742e-05,
      "loss": 0.0023,
      "step": 13640
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 0.1372123509645462,
      "learning_rate": 2.4722222222222223e-05,
      "loss": 0.0018,
      "step": 13650
    },
    {
      "epoch": 1.0118518518518518,
      "grad_norm": 0.045474641025066376,
      "learning_rate": 2.4703703703703704e-05,
      "loss": 0.0026,
      "step": 13660
    },
    {
      "epoch": 1.0125925925925925,
      "grad_norm": 0.08638901263475418,
      "learning_rate": 2.4685185185185185e-05,
      "loss": 0.0022,
      "step": 13670
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.08913727849721909,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0015,
      "step": 13680
    },
    {
      "epoch": 1.0140740740740741,
      "grad_norm": 0.055327728390693665,
      "learning_rate": 2.464814814814815e-05,
      "loss": 0.0009,
      "step": 13690
    },
    {
      "epoch": 1.0148148148148148,
      "grad_norm": 0.07641637325286865,
      "learning_rate": 2.462962962962963e-05,
      "loss": 0.0015,
      "step": 13700
    },
    {
      "epoch": 1.0155555555555555,
      "grad_norm": 0.06734858453273773,
      "learning_rate": 2.461111111111111e-05,
      "loss": 0.0012,
      "step": 13710
    },
    {
      "epoch": 1.0162962962962963,
      "grad_norm": 0.1479867398738861,
      "learning_rate": 2.4592592592592595e-05,
      "loss": 0.0015,
      "step": 13720
    },
    {
      "epoch": 1.017037037037037,
      "grad_norm": 0.12176774442195892,
      "learning_rate": 2.4574074074074073e-05,
      "loss": 0.0015,
      "step": 13730
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 0.0,
      "learning_rate": 2.4555555555555557e-05,
      "loss": 0.0011,
      "step": 13740
    },
    {
      "epoch": 1.0185185185185186,
      "grad_norm": 0.07083166390657425,
      "learning_rate": 2.4537037037037038e-05,
      "loss": 0.0013,
      "step": 13750
    },
    {
      "epoch": 1.0192592592592593,
      "grad_norm": 0.0,
      "learning_rate": 2.451851851851852e-05,
      "loss": 0.0014,
      "step": 13760
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.11017914116382599,
      "learning_rate": 2.45e-05,
      "loss": 0.0014,
      "step": 13770
    },
    {
      "epoch": 1.0207407407407407,
      "grad_norm": 0.1283058226108551,
      "learning_rate": 2.4481481481481483e-05,
      "loss": 0.0022,
      "step": 13780
    },
    {
      "epoch": 1.0214814814814814,
      "grad_norm": 0.10148075968027115,
      "learning_rate": 2.4462962962962964e-05,
      "loss": 0.0013,
      "step": 13790
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.022058358415961266,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.0019,
      "step": 13800
    },
    {
      "epoch": 1.0229629629629629,
      "grad_norm": 0.042384836822748184,
      "learning_rate": 2.4425925925925926e-05,
      "loss": 0.0015,
      "step": 13810
    },
    {
      "epoch": 1.0237037037037038,
      "grad_norm": 0.1220819354057312,
      "learning_rate": 2.440740740740741e-05,
      "loss": 0.0016,
      "step": 13820
    },
    {
      "epoch": 1.0244444444444445,
      "grad_norm": 0.08949501812458038,
      "learning_rate": 2.4388888888888887e-05,
      "loss": 0.0015,
      "step": 13830
    },
    {
      "epoch": 1.0251851851851852,
      "grad_norm": 0.1574644148349762,
      "learning_rate": 2.437037037037037e-05,
      "loss": 0.0028,
      "step": 13840
    },
    {
      "epoch": 1.025925925925926,
      "grad_norm": 0.09733320027589798,
      "learning_rate": 2.4351851851851852e-05,
      "loss": 0.0017,
      "step": 13850
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.08952321857213974,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0019,
      "step": 13860
    },
    {
      "epoch": 1.0274074074074073,
      "grad_norm": 0.04011767357587814,
      "learning_rate": 2.4314814814814814e-05,
      "loss": 0.0009,
      "step": 13870
    },
    {
      "epoch": 1.0281481481481483,
      "grad_norm": 0.23175524175167084,
      "learning_rate": 2.4296296296296298e-05,
      "loss": 0.0015,
      "step": 13880
    },
    {
      "epoch": 1.028888888888889,
      "grad_norm": 0.16577023267745972,
      "learning_rate": 2.427777777777778e-05,
      "loss": 0.0022,
      "step": 13890
    },
    {
      "epoch": 1.0296296296296297,
      "grad_norm": 0.04065687954425812,
      "learning_rate": 2.425925925925926e-05,
      "loss": 0.0009,
      "step": 13900
    },
    {
      "epoch": 1.0303703703703704,
      "grad_norm": 0.13206543028354645,
      "learning_rate": 2.424074074074074e-05,
      "loss": 0.0016,
      "step": 13910
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 0.089452363550663,
      "learning_rate": 2.4222222222222224e-05,
      "loss": 0.0017,
      "step": 13920
    },
    {
      "epoch": 1.0318518518518518,
      "grad_norm": 0.0,
      "learning_rate": 2.4203703703703705e-05,
      "loss": 0.002,
      "step": 13930
    },
    {
      "epoch": 1.0325925925925925,
      "grad_norm": 0.0,
      "learning_rate": 2.4185185185185186e-05,
      "loss": 0.0006,
      "step": 13940
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.04307671636343002,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0019,
      "step": 13950
    },
    {
      "epoch": 1.0340740740740741,
      "grad_norm": 0.047381091862916946,
      "learning_rate": 2.414814814814815e-05,
      "loss": 0.0008,
      "step": 13960
    },
    {
      "epoch": 1.0348148148148149,
      "grad_norm": 0.06557679176330566,
      "learning_rate": 2.412962962962963e-05,
      "loss": 0.0016,
      "step": 13970
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.04094986245036125,
      "learning_rate": 2.4111111111111113e-05,
      "loss": 0.0015,
      "step": 13980
    },
    {
      "epoch": 1.0362962962962963,
      "grad_norm": 0.0,
      "learning_rate": 2.4092592592592593e-05,
      "loss": 0.0013,
      "step": 13990
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 0.02651694230735302,
      "learning_rate": 2.4074074074074074e-05,
      "loss": 0.0015,
      "step": 14000
    },
    {
      "epoch": 1.0377777777777777,
      "grad_norm": 0.15487726032733917,
      "learning_rate": 2.4055555555555555e-05,
      "loss": 0.0012,
      "step": 14010
    },
    {
      "epoch": 1.0385185185185186,
      "grad_norm": 0.13036119937896729,
      "learning_rate": 2.403703703703704e-05,
      "loss": 0.0019,
      "step": 14020
    },
    {
      "epoch": 1.0392592592592593,
      "grad_norm": 0.11080998927354813,
      "learning_rate": 2.401851851851852e-05,
      "loss": 0.0015,
      "step": 14030
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.06705775856971741,
      "learning_rate": 2.4e-05,
      "loss": 0.0017,
      "step": 14040
    },
    {
      "epoch": 1.0407407407407407,
      "grad_norm": 0.08799701929092407,
      "learning_rate": 2.398148148148148e-05,
      "loss": 0.0012,
      "step": 14050
    },
    {
      "epoch": 1.0414814814814815,
      "grad_norm": 0.0214227382093668,
      "learning_rate": 2.3962962962962966e-05,
      "loss": 0.0021,
      "step": 14060
    },
    {
      "epoch": 1.0422222222222222,
      "grad_norm": 0.23799782991409302,
      "learning_rate": 2.3944444444444443e-05,
      "loss": 0.0017,
      "step": 14070
    },
    {
      "epoch": 1.0429629629629629,
      "grad_norm": 0.09759367257356644,
      "learning_rate": 2.3925925925925927e-05,
      "loss": 0.0004,
      "step": 14080
    },
    {
      "epoch": 1.0437037037037038,
      "grad_norm": 0.0,
      "learning_rate": 2.3907407407407408e-05,
      "loss": 0.0019,
      "step": 14090
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 0.0522817000746727,
      "learning_rate": 2.3888888888888892e-05,
      "loss": 0.0031,
      "step": 14100
    },
    {
      "epoch": 1.0451851851851852,
      "grad_norm": 0.10294771939516068,
      "learning_rate": 2.387037037037037e-05,
      "loss": 0.0017,
      "step": 14110
    },
    {
      "epoch": 1.045925925925926,
      "grad_norm": 0.1393500417470932,
      "learning_rate": 2.3851851851851854e-05,
      "loss": 0.0022,
      "step": 14120
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.12542904913425446,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.0017,
      "step": 14130
    },
    {
      "epoch": 1.0474074074074073,
      "grad_norm": 0.0,
      "learning_rate": 2.3814814814814815e-05,
      "loss": 0.0016,
      "step": 14140
    },
    {
      "epoch": 1.048148148148148,
      "grad_norm": 0.045710932463407516,
      "learning_rate": 2.3796296296296296e-05,
      "loss": 0.0008,
      "step": 14150
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 0.20198749005794525,
      "learning_rate": 2.377777777777778e-05,
      "loss": 0.0026,
      "step": 14160
    },
    {
      "epoch": 1.0496296296296297,
      "grad_norm": 0.16222572326660156,
      "learning_rate": 2.375925925925926e-05,
      "loss": 0.0023,
      "step": 14170
    },
    {
      "epoch": 1.0503703703703704,
      "grad_norm": 0.06297294795513153,
      "learning_rate": 2.3740740740740742e-05,
      "loss": 0.0018,
      "step": 14180
    },
    {
      "epoch": 1.051111111111111,
      "grad_norm": 0.0862463042140007,
      "learning_rate": 2.3722222222222222e-05,
      "loss": 0.0012,
      "step": 14190
    },
    {
      "epoch": 1.0518518518518518,
      "grad_norm": 0.1507999300956726,
      "learning_rate": 2.3703703703703707e-05,
      "loss": 0.0026,
      "step": 14200
    },
    {
      "epoch": 1.0525925925925925,
      "grad_norm": 0.0,
      "learning_rate": 2.3685185185185184e-05,
      "loss": 0.0024,
      "step": 14210
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.03974184766411781,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0024,
      "step": 14220
    },
    {
      "epoch": 1.0540740740740742,
      "grad_norm": 0.052290163934230804,
      "learning_rate": 2.364814814814815e-05,
      "loss": 0.0021,
      "step": 14230
    },
    {
      "epoch": 1.0548148148148149,
      "grad_norm": 0.08614528924226761,
      "learning_rate": 2.3629629629629633e-05,
      "loss": 0.0017,
      "step": 14240
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.0575149767100811,
      "learning_rate": 2.361111111111111e-05,
      "loss": 0.0014,
      "step": 14250
    },
    {
      "epoch": 1.0562962962962963,
      "grad_norm": 0.06651651859283447,
      "learning_rate": 2.3592592592592595e-05,
      "loss": 0.0012,
      "step": 14260
    },
    {
      "epoch": 1.057037037037037,
      "grad_norm": 0.08599350601434708,
      "learning_rate": 2.3574074074074075e-05,
      "loss": 0.0017,
      "step": 14270
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 0.1591554433107376,
      "learning_rate": 2.3555555555555556e-05,
      "loss": 0.002,
      "step": 14280
    },
    {
      "epoch": 1.0585185185185184,
      "grad_norm": 0.04135454073548317,
      "learning_rate": 2.3537037037037037e-05,
      "loss": 0.0025,
      "step": 14290
    },
    {
      "epoch": 1.0592592592592593,
      "grad_norm": 0.0,
      "learning_rate": 2.351851851851852e-05,
      "loss": 0.0013,
      "step": 14300
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.08311799168586731,
      "learning_rate": 2.35e-05,
      "loss": 0.0025,
      "step": 14310
    },
    {
      "epoch": 1.0607407407407408,
      "grad_norm": 0.04216701164841652,
      "learning_rate": 2.3481481481481483e-05,
      "loss": 0.0008,
      "step": 14320
    },
    {
      "epoch": 1.0614814814814815,
      "grad_norm": 0.14211814105510712,
      "learning_rate": 2.3462962962962964e-05,
      "loss": 0.0014,
      "step": 14330
    },
    {
      "epoch": 1.0622222222222222,
      "grad_norm": 0.08568111807107925,
      "learning_rate": 2.3444444444444448e-05,
      "loss": 0.0006,
      "step": 14340
    },
    {
      "epoch": 1.0629629629629629,
      "grad_norm": 0.0,
      "learning_rate": 2.3425925925925925e-05,
      "loss": 0.0007,
      "step": 14350
    },
    {
      "epoch": 1.0637037037037036,
      "grad_norm": 0.024298980832099915,
      "learning_rate": 2.340740740740741e-05,
      "loss": 0.0019,
      "step": 14360
    },
    {
      "epoch": 1.0644444444444445,
      "grad_norm": 0.09731298685073853,
      "learning_rate": 2.338888888888889e-05,
      "loss": 0.0018,
      "step": 14370
    },
    {
      "epoch": 1.0651851851851852,
      "grad_norm": 0.13898047804832458,
      "learning_rate": 2.337037037037037e-05,
      "loss": 0.002,
      "step": 14380
    },
    {
      "epoch": 1.065925925925926,
      "grad_norm": 0.19568149745464325,
      "learning_rate": 2.335185185185185e-05,
      "loss": 0.0018,
      "step": 14390
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.11433977633714676,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0023,
      "step": 14400
    },
    {
      "epoch": 1.0674074074074074,
      "grad_norm": 0.09365196526050568,
      "learning_rate": 2.3314814814814817e-05,
      "loss": 0.0015,
      "step": 14410
    },
    {
      "epoch": 1.068148148148148,
      "grad_norm": 0.18699564039707184,
      "learning_rate": 2.3296296296296297e-05,
      "loss": 0.0013,
      "step": 14420
    },
    {
      "epoch": 1.068888888888889,
      "grad_norm": 0.04091629758477211,
      "learning_rate": 2.3277777777777778e-05,
      "loss": 0.0008,
      "step": 14430
    },
    {
      "epoch": 1.0696296296296297,
      "grad_norm": 0.07948833703994751,
      "learning_rate": 2.3259259259259262e-05,
      "loss": 0.0016,
      "step": 14440
    },
    {
      "epoch": 1.0703703703703704,
      "grad_norm": 0.09236346930265427,
      "learning_rate": 2.324074074074074e-05,
      "loss": 0.0021,
      "step": 14450
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.09127358347177505,
      "learning_rate": 2.3222222222222224e-05,
      "loss": 0.0023,
      "step": 14460
    },
    {
      "epoch": 1.0718518518518518,
      "grad_norm": 0.1531602442264557,
      "learning_rate": 2.3203703703703705e-05,
      "loss": 0.0023,
      "step": 14470
    },
    {
      "epoch": 1.0725925925925925,
      "grad_norm": 0.17986634373664856,
      "learning_rate": 2.318518518518519e-05,
      "loss": 0.002,
      "step": 14480
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.045559901744127274,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0023,
      "step": 14490
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 0.048686154186725616,
      "learning_rate": 2.314814814814815e-05,
      "loss": 0.0014,
      "step": 14500
    },
    {
      "epoch": 1.074814814814815,
      "grad_norm": 0.18760886788368225,
      "learning_rate": 2.312962962962963e-05,
      "loss": 0.002,
      "step": 14510
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 0.08254723995923996,
      "learning_rate": 2.3111111111111112e-05,
      "loss": 0.0023,
      "step": 14520
    },
    {
      "epoch": 1.0762962962962963,
      "grad_norm": 0.0,
      "learning_rate": 2.3092592592592593e-05,
      "loss": 0.0006,
      "step": 14530
    },
    {
      "epoch": 1.077037037037037,
      "grad_norm": 0.23121143877506256,
      "learning_rate": 2.3074074074074077e-05,
      "loss": 0.001,
      "step": 14540
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 0.013881400227546692,
      "learning_rate": 2.3055555555555558e-05,
      "loss": 0.0014,
      "step": 14550
    },
    {
      "epoch": 1.0785185185185184,
      "grad_norm": 0.12717407941818237,
      "learning_rate": 2.303703703703704e-05,
      "loss": 0.002,
      "step": 14560
    },
    {
      "epoch": 1.0792592592592594,
      "grad_norm": 0.08288247138261795,
      "learning_rate": 2.301851851851852e-05,
      "loss": 0.0016,
      "step": 14570
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.09213637560606003,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0019,
      "step": 14580
    },
    {
      "epoch": 1.0807407407407408,
      "grad_norm": 0.05840671434998512,
      "learning_rate": 2.298148148148148e-05,
      "loss": 0.0012,
      "step": 14590
    },
    {
      "epoch": 1.0814814814814815,
      "grad_norm": 0.039060018956661224,
      "learning_rate": 2.2962962962962965e-05,
      "loss": 0.0011,
      "step": 14600
    },
    {
      "epoch": 1.0822222222222222,
      "grad_norm": 0.0990089401602745,
      "learning_rate": 2.2944444444444446e-05,
      "loss": 0.0024,
      "step": 14610
    },
    {
      "epoch": 1.082962962962963,
      "grad_norm": 0.0,
      "learning_rate": 2.2925925925925927e-05,
      "loss": 0.0012,
      "step": 14620
    },
    {
      "epoch": 1.0837037037037036,
      "grad_norm": 0.04425966367125511,
      "learning_rate": 2.2907407407407407e-05,
      "loss": 0.0007,
      "step": 14630
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 0.04595393314957619,
      "learning_rate": 2.288888888888889e-05,
      "loss": 0.0017,
      "step": 14640
    },
    {
      "epoch": 1.0851851851851853,
      "grad_norm": 0.06644042581319809,
      "learning_rate": 2.2870370370370372e-05,
      "loss": 0.0015,
      "step": 14650
    },
    {
      "epoch": 1.085925925925926,
      "grad_norm": 0.08774854987859726,
      "learning_rate": 2.2851851851851853e-05,
      "loss": 0.0015,
      "step": 14660
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.11749482899904251,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0009,
      "step": 14670
    },
    {
      "epoch": 1.0874074074074074,
      "grad_norm": 0.05931650102138519,
      "learning_rate": 2.2814814814814818e-05,
      "loss": 0.0018,
      "step": 14680
    },
    {
      "epoch": 1.088148148148148,
      "grad_norm": 0.0,
      "learning_rate": 2.2796296296296295e-05,
      "loss": 0.0015,
      "step": 14690
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.08879221230745316,
      "learning_rate": 2.277777777777778e-05,
      "loss": 0.001,
      "step": 14700
    },
    {
      "epoch": 1.0896296296296297,
      "grad_norm": 0.0,
      "learning_rate": 2.275925925925926e-05,
      "loss": 0.0015,
      "step": 14710
    },
    {
      "epoch": 1.0903703703703704,
      "grad_norm": 0.108177050948143,
      "learning_rate": 2.2740740740740744e-05,
      "loss": 0.0012,
      "step": 14720
    },
    {
      "epoch": 1.0911111111111111,
      "grad_norm": 0.0,
      "learning_rate": 2.2722222222222222e-05,
      "loss": 0.0007,
      "step": 14730
    },
    {
      "epoch": 1.0918518518518519,
      "grad_norm": 0.1167687401175499,
      "learning_rate": 2.2703703703703706e-05,
      "loss": 0.0022,
      "step": 14740
    },
    {
      "epoch": 1.0925925925925926,
      "grad_norm": 0.04884396120905876,
      "learning_rate": 2.2685185185185187e-05,
      "loss": 0.0014,
      "step": 14750
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.1285092532634735,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0005,
      "step": 14760
    },
    {
      "epoch": 1.094074074074074,
      "grad_norm": 0.07824722677469254,
      "learning_rate": 2.264814814814815e-05,
      "loss": 0.0007,
      "step": 14770
    },
    {
      "epoch": 1.094814814814815,
      "grad_norm": 0.04325365275144577,
      "learning_rate": 2.2629629629629633e-05,
      "loss": 0.0012,
      "step": 14780
    },
    {
      "epoch": 1.0955555555555556,
      "grad_norm": 0.0,
      "learning_rate": 2.2611111111111113e-05,
      "loss": 0.0017,
      "step": 14790
    },
    {
      "epoch": 1.0962962962962963,
      "grad_norm": 0.11982058733701706,
      "learning_rate": 2.2592592592592594e-05,
      "loss": 0.0016,
      "step": 14800
    },
    {
      "epoch": 1.097037037037037,
      "grad_norm": 0.05522959679365158,
      "learning_rate": 2.2574074074074075e-05,
      "loss": 0.0024,
      "step": 14810
    },
    {
      "epoch": 1.0977777777777777,
      "grad_norm": 0.15296928584575653,
      "learning_rate": 2.255555555555556e-05,
      "loss": 0.0014,
      "step": 14820
    },
    {
      "epoch": 1.0985185185185184,
      "grad_norm": 0.07018721848726273,
      "learning_rate": 2.2537037037037036e-05,
      "loss": 0.0024,
      "step": 14830
    },
    {
      "epoch": 1.0992592592592592,
      "grad_norm": 0.0,
      "learning_rate": 2.251851851851852e-05,
      "loss": 0.0025,
      "step": 14840
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.06693064421415329,
      "learning_rate": 2.25e-05,
      "loss": 0.0021,
      "step": 14850
    },
    {
      "epoch": 1.1007407407407408,
      "grad_norm": 0.040273621678352356,
      "learning_rate": 2.2481481481481486e-05,
      "loss": 0.0015,
      "step": 14860
    },
    {
      "epoch": 1.1014814814814815,
      "grad_norm": 0.04150338098406792,
      "learning_rate": 2.2462962962962963e-05,
      "loss": 0.0013,
      "step": 14870
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 0.08884543180465698,
      "learning_rate": 2.2444444444444447e-05,
      "loss": 0.0006,
      "step": 14880
    },
    {
      "epoch": 1.102962962962963,
      "grad_norm": 0.1008545309305191,
      "learning_rate": 2.2425925925925928e-05,
      "loss": 0.0029,
      "step": 14890
    },
    {
      "epoch": 1.1037037037037036,
      "grad_norm": 0.0,
      "learning_rate": 2.240740740740741e-05,
      "loss": 0.0015,
      "step": 14900
    },
    {
      "epoch": 1.1044444444444443,
      "grad_norm": 0.07305025309324265,
      "learning_rate": 2.238888888888889e-05,
      "loss": 0.0022,
      "step": 14910
    },
    {
      "epoch": 1.1051851851851853,
      "grad_norm": 0.08508343249559402,
      "learning_rate": 2.2370370370370374e-05,
      "loss": 0.0011,
      "step": 14920
    },
    {
      "epoch": 1.105925925925926,
      "grad_norm": 0.1467881053686142,
      "learning_rate": 2.235185185185185e-05,
      "loss": 0.0018,
      "step": 14930
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.05376216024160385,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0013,
      "step": 14940
    },
    {
      "epoch": 1.1074074074074074,
      "grad_norm": 0.04124339297413826,
      "learning_rate": 2.2314814814814816e-05,
      "loss": 0.0023,
      "step": 14950
    },
    {
      "epoch": 1.108148148148148,
      "grad_norm": 0.14805299043655396,
      "learning_rate": 2.2296296296296297e-05,
      "loss": 0.0014,
      "step": 14960
    },
    {
      "epoch": 1.1088888888888888,
      "grad_norm": 0.06005252152681351,
      "learning_rate": 2.2277777777777778e-05,
      "loss": 0.0019,
      "step": 14970
    },
    {
      "epoch": 1.1096296296296297,
      "grad_norm": 0.1442708671092987,
      "learning_rate": 2.2259259259259262e-05,
      "loss": 0.0008,
      "step": 14980
    },
    {
      "epoch": 1.1103703703703705,
      "grad_norm": 0.058273207396268845,
      "learning_rate": 2.2240740740740743e-05,
      "loss": 0.0015,
      "step": 14990
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.07041200995445251,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.0017,
      "step": 15000
    },
    {
      "epoch": 1.1118518518518519,
      "grad_norm": 0.07797017693519592,
      "learning_rate": 2.2203703703703704e-05,
      "loss": 0.0011,
      "step": 15010
    },
    {
      "epoch": 1.1125925925925926,
      "grad_norm": 0.0581786222755909,
      "learning_rate": 2.2185185185185188e-05,
      "loss": 0.0012,
      "step": 15020
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.04340311139822006,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0013,
      "step": 15030
    },
    {
      "epoch": 1.114074074074074,
      "grad_norm": 0.1253276914358139,
      "learning_rate": 2.214814814814815e-05,
      "loss": 0.0022,
      "step": 15040
    },
    {
      "epoch": 1.1148148148148147,
      "grad_norm": 0.25708532333374023,
      "learning_rate": 2.212962962962963e-05,
      "loss": 0.002,
      "step": 15050
    },
    {
      "epoch": 1.1155555555555556,
      "grad_norm": 0.031739793717861176,
      "learning_rate": 2.211111111111111e-05,
      "loss": 0.0015,
      "step": 15060
    },
    {
      "epoch": 1.1162962962962963,
      "grad_norm": 0.041174862533807755,
      "learning_rate": 2.2092592592592592e-05,
      "loss": 0.0007,
      "step": 15070
    },
    {
      "epoch": 1.117037037037037,
      "grad_norm": 0.055350903421640396,
      "learning_rate": 2.2074074074074076e-05,
      "loss": 0.0007,
      "step": 15080
    },
    {
      "epoch": 1.1177777777777778,
      "grad_norm": 0.09375957399606705,
      "learning_rate": 2.2055555555555557e-05,
      "loss": 0.002,
      "step": 15090
    },
    {
      "epoch": 1.1185185185185185,
      "grad_norm": 0.0,
      "learning_rate": 2.2037037037037038e-05,
      "loss": 0.0013,
      "step": 15100
    },
    {
      "epoch": 1.1192592592592592,
      "grad_norm": 0.12399578839540482,
      "learning_rate": 2.201851851851852e-05,
      "loss": 0.0011,
      "step": 15110
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.12739884853363037,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0015,
      "step": 15120
    },
    {
      "epoch": 1.1207407407407408,
      "grad_norm": 0.0,
      "learning_rate": 2.1981481481481484e-05,
      "loss": 0.0015,
      "step": 15130
    },
    {
      "epoch": 1.1214814814814815,
      "grad_norm": 0.03245636075735092,
      "learning_rate": 2.1962962962962964e-05,
      "loss": 0.0018,
      "step": 15140
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 0.11075592041015625,
      "learning_rate": 2.1944444444444445e-05,
      "loss": 0.0019,
      "step": 15150
    },
    {
      "epoch": 1.122962962962963,
      "grad_norm": 0.06616318970918655,
      "learning_rate": 2.1925925925925926e-05,
      "loss": 0.0026,
      "step": 15160
    },
    {
      "epoch": 1.1237037037037036,
      "grad_norm": 0.04385468736290932,
      "learning_rate": 2.190740740740741e-05,
      "loss": 0.0017,
      "step": 15170
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 0.10491927713155746,
      "learning_rate": 2.188888888888889e-05,
      "loss": 0.0008,
      "step": 15180
    },
    {
      "epoch": 1.125185185185185,
      "grad_norm": 0.047881096601486206,
      "learning_rate": 2.187037037037037e-05,
      "loss": 0.0009,
      "step": 15190
    },
    {
      "epoch": 1.125925925925926,
      "grad_norm": 0.06813594698905945,
      "learning_rate": 2.1851851851851852e-05,
      "loss": 0.0011,
      "step": 15200
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.08558633178472519,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0017,
      "step": 15210
    },
    {
      "epoch": 1.1274074074074074,
      "grad_norm": 0.08352880924940109,
      "learning_rate": 2.1814814814814817e-05,
      "loss": 0.0018,
      "step": 15220
    },
    {
      "epoch": 1.1281481481481481,
      "grad_norm": 0.26537853479385376,
      "learning_rate": 2.1796296296296298e-05,
      "loss": 0.0023,
      "step": 15230
    },
    {
      "epoch": 1.1288888888888888,
      "grad_norm": 0.07629652321338654,
      "learning_rate": 2.177777777777778e-05,
      "loss": 0.0019,
      "step": 15240
    },
    {
      "epoch": 1.1296296296296295,
      "grad_norm": 0.060326267033815384,
      "learning_rate": 2.175925925925926e-05,
      "loss": 0.0009,
      "step": 15250
    },
    {
      "epoch": 1.1303703703703705,
      "grad_norm": 0.040612950921058655,
      "learning_rate": 2.174074074074074e-05,
      "loss": 0.0015,
      "step": 15260
    },
    {
      "epoch": 1.1311111111111112,
      "grad_norm": 0.058599334210157394,
      "learning_rate": 2.1722222222222225e-05,
      "loss": 0.002,
      "step": 15270
    },
    {
      "epoch": 1.1318518518518519,
      "grad_norm": 0.08679835498332977,
      "learning_rate": 2.1703703703703705e-05,
      "loss": 0.001,
      "step": 15280
    },
    {
      "epoch": 1.1325925925925926,
      "grad_norm": 0.04350168630480766,
      "learning_rate": 2.1685185185185186e-05,
      "loss": 0.003,
      "step": 15290
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0014,
      "step": 15300
    },
    {
      "epoch": 1.134074074074074,
      "grad_norm": 0.04196703061461449,
      "learning_rate": 2.1648148148148148e-05,
      "loss": 0.0008,
      "step": 15310
    },
    {
      "epoch": 1.1348148148148147,
      "grad_norm": 0.10615405440330505,
      "learning_rate": 2.162962962962963e-05,
      "loss": 0.0012,
      "step": 15320
    },
    {
      "epoch": 1.1355555555555557,
      "grad_norm": 0.09740350395441055,
      "learning_rate": 2.1611111111111113e-05,
      "loss": 0.0012,
      "step": 15330
    },
    {
      "epoch": 1.1362962962962964,
      "grad_norm": 0.0,
      "learning_rate": 2.1592592592592594e-05,
      "loss": 0.0015,
      "step": 15340
    },
    {
      "epoch": 1.137037037037037,
      "grad_norm": 0.05032780021429062,
      "learning_rate": 2.1574074074074074e-05,
      "loss": 0.0026,
      "step": 15350
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 0.10684853047132492,
      "learning_rate": 2.1555555555555555e-05,
      "loss": 0.0013,
      "step": 15360
    },
    {
      "epoch": 1.1385185185185185,
      "grad_norm": 0.05718575045466423,
      "learning_rate": 2.153703703703704e-05,
      "loss": 0.0015,
      "step": 15370
    },
    {
      "epoch": 1.1392592592592592,
      "grad_norm": 0.08147170394659042,
      "learning_rate": 2.151851851851852e-05,
      "loss": 0.0012,
      "step": 15380
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.057723142206668854,
      "learning_rate": 2.15e-05,
      "loss": 0.0009,
      "step": 15390
    },
    {
      "epoch": 1.1407407407407408,
      "grad_norm": 0.0,
      "learning_rate": 2.148148148148148e-05,
      "loss": 0.0017,
      "step": 15400
    },
    {
      "epoch": 1.1414814814814815,
      "grad_norm": 0.12585102021694183,
      "learning_rate": 2.1462962962962966e-05,
      "loss": 0.0022,
      "step": 15410
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 0.09899941831827164,
      "learning_rate": 2.1444444444444443e-05,
      "loss": 0.0011,
      "step": 15420
    },
    {
      "epoch": 1.142962962962963,
      "grad_norm": 0.11068893224000931,
      "learning_rate": 2.1425925925925927e-05,
      "loss": 0.0021,
      "step": 15430
    },
    {
      "epoch": 1.1437037037037037,
      "grad_norm": 0.06841657310724258,
      "learning_rate": 2.1407407407407408e-05,
      "loss": 0.0012,
      "step": 15440
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 0.1197783425450325,
      "learning_rate": 2.138888888888889e-05,
      "loss": 0.0016,
      "step": 15450
    },
    {
      "epoch": 1.145185185185185,
      "grad_norm": 0.1330779492855072,
      "learning_rate": 2.137037037037037e-05,
      "loss": 0.0017,
      "step": 15460
    },
    {
      "epoch": 1.145925925925926,
      "grad_norm": 0.06940514594316483,
      "learning_rate": 2.1351851851851854e-05,
      "loss": 0.0011,
      "step": 15470
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.04580802097916603,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0024,
      "step": 15480
    },
    {
      "epoch": 1.1474074074074074,
      "grad_norm": 0.1419459879398346,
      "learning_rate": 2.1314814814814815e-05,
      "loss": 0.0015,
      "step": 15490
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 0.039862994104623795,
      "learning_rate": 2.1296296296296296e-05,
      "loss": 0.0013,
      "step": 15500
    },
    {
      "epoch": 1.1488888888888888,
      "grad_norm": 0.09195481985807419,
      "learning_rate": 2.127777777777778e-05,
      "loss": 0.0006,
      "step": 15510
    },
    {
      "epoch": 1.1496296296296296,
      "grad_norm": 0.058199066668748856,
      "learning_rate": 2.1259259259259258e-05,
      "loss": 0.0018,
      "step": 15520
    },
    {
      "epoch": 1.1503703703703705,
      "grad_norm": 0.22354385256767273,
      "learning_rate": 2.1240740740740742e-05,
      "loss": 0.0018,
      "step": 15530
    },
    {
      "epoch": 1.1511111111111112,
      "grad_norm": 0.03454253450036049,
      "learning_rate": 2.1222222222222223e-05,
      "loss": 0.0018,
      "step": 15540
    },
    {
      "epoch": 1.151851851851852,
      "grad_norm": 0.0,
      "learning_rate": 2.1203703703703703e-05,
      "loss": 0.0018,
      "step": 15550
    },
    {
      "epoch": 1.1525925925925926,
      "grad_norm": 0.17244915664196014,
      "learning_rate": 2.1185185185185184e-05,
      "loss": 0.002,
      "step": 15560
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.09998759627342224,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.0009,
      "step": 15570
    },
    {
      "epoch": 1.154074074074074,
      "grad_norm": 0.11441823840141296,
      "learning_rate": 2.114814814814815e-05,
      "loss": 0.0018,
      "step": 15580
    },
    {
      "epoch": 1.1548148148148147,
      "grad_norm": 0.0,
      "learning_rate": 2.112962962962963e-05,
      "loss": 0.0017,
      "step": 15590
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.07173282653093338,
      "learning_rate": 2.111111111111111e-05,
      "loss": 0.001,
      "step": 15600
    },
    {
      "epoch": 1.1562962962962964,
      "grad_norm": 0.05472385510802269,
      "learning_rate": 2.1092592592592595e-05,
      "loss": 0.0018,
      "step": 15610
    },
    {
      "epoch": 1.157037037037037,
      "grad_norm": 0.11352772265672684,
      "learning_rate": 2.1074074074074072e-05,
      "loss": 0.001,
      "step": 15620
    },
    {
      "epoch": 1.1577777777777778,
      "grad_norm": 0.07548096776008606,
      "learning_rate": 2.1055555555555556e-05,
      "loss": 0.0015,
      "step": 15630
    },
    {
      "epoch": 1.1585185185185185,
      "grad_norm": 0.0,
      "learning_rate": 2.1037037037037037e-05,
      "loss": 0.0012,
      "step": 15640
    },
    {
      "epoch": 1.1592592592592592,
      "grad_norm": 0.08780445158481598,
      "learning_rate": 2.101851851851852e-05,
      "loss": 0.0007,
      "step": 15650
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.10674089938402176,
      "learning_rate": 2.1e-05,
      "loss": 0.0011,
      "step": 15660
    },
    {
      "epoch": 1.1607407407407409,
      "grad_norm": 0.0,
      "learning_rate": 2.0981481481481483e-05,
      "loss": 0.0013,
      "step": 15670
    },
    {
      "epoch": 1.1614814814814816,
      "grad_norm": 0.0,
      "learning_rate": 2.0962962962962964e-05,
      "loss": 0.0009,
      "step": 15680
    },
    {
      "epoch": 1.1622222222222223,
      "grad_norm": 0.08545059710741043,
      "learning_rate": 2.0944444444444445e-05,
      "loss": 0.0011,
      "step": 15690
    },
    {
      "epoch": 1.162962962962963,
      "grad_norm": 0.10055206716060638,
      "learning_rate": 2.0925925925925925e-05,
      "loss": 0.0006,
      "step": 15700
    },
    {
      "epoch": 1.1637037037037037,
      "grad_norm": 0.08434655517339706,
      "learning_rate": 2.090740740740741e-05,
      "loss": 0.0015,
      "step": 15710
    },
    {
      "epoch": 1.1644444444444444,
      "grad_norm": 0.1286337524652481,
      "learning_rate": 2.088888888888889e-05,
      "loss": 0.002,
      "step": 15720
    },
    {
      "epoch": 1.165185185185185,
      "grad_norm": 0.12659768760204315,
      "learning_rate": 2.087037037037037e-05,
      "loss": 0.0017,
      "step": 15730
    },
    {
      "epoch": 1.1659259259259258,
      "grad_norm": 0.07056853175163269,
      "learning_rate": 2.0851851851851852e-05,
      "loss": 0.0017,
      "step": 15740
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0012,
      "step": 15750
    },
    {
      "epoch": 1.1674074074074074,
      "grad_norm": 0.1430222988128662,
      "learning_rate": 2.0814814814814813e-05,
      "loss": 0.0014,
      "step": 15760
    },
    {
      "epoch": 1.1681481481481482,
      "grad_norm": 0.11613960564136505,
      "learning_rate": 2.0796296296296298e-05,
      "loss": 0.0008,
      "step": 15770
    },
    {
      "epoch": 1.1688888888888889,
      "grad_norm": 0.04345686733722687,
      "learning_rate": 2.077777777777778e-05,
      "loss": 0.0006,
      "step": 15780
    },
    {
      "epoch": 1.1696296296296296,
      "grad_norm": 0.0,
      "learning_rate": 2.0759259259259263e-05,
      "loss": 0.0017,
      "step": 15790
    },
    {
      "epoch": 1.1703703703703703,
      "grad_norm": 0.1289234161376953,
      "learning_rate": 2.074074074074074e-05,
      "loss": 0.0012,
      "step": 15800
    },
    {
      "epoch": 1.1711111111111112,
      "grad_norm": 0.03159677982330322,
      "learning_rate": 2.0722222222222224e-05,
      "loss": 0.0032,
      "step": 15810
    },
    {
      "epoch": 1.171851851851852,
      "grad_norm": 0.05691058561205864,
      "learning_rate": 2.0703703703703705e-05,
      "loss": 0.0015,
      "step": 15820
    },
    {
      "epoch": 1.1725925925925926,
      "grad_norm": 0.05251483619213104,
      "learning_rate": 2.0685185185185186e-05,
      "loss": 0.0016,
      "step": 15830
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.04924163594841957,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0019,
      "step": 15840
    },
    {
      "epoch": 1.174074074074074,
      "grad_norm": 0.07358838617801666,
      "learning_rate": 2.064814814814815e-05,
      "loss": 0.0015,
      "step": 15850
    },
    {
      "epoch": 1.1748148148148148,
      "grad_norm": 0.07223009318113327,
      "learning_rate": 2.0629629629629628e-05,
      "loss": 0.0005,
      "step": 15860
    },
    {
      "epoch": 1.1755555555555555,
      "grad_norm": 0.09083931893110275,
      "learning_rate": 2.0611111111111112e-05,
      "loss": 0.0011,
      "step": 15870
    },
    {
      "epoch": 1.1762962962962962,
      "grad_norm": 0.15891940891742706,
      "learning_rate": 2.0592592592592593e-05,
      "loss": 0.0013,
      "step": 15880
    },
    {
      "epoch": 1.177037037037037,
      "grad_norm": 0.17275965213775635,
      "learning_rate": 2.0574074074074077e-05,
      "loss": 0.001,
      "step": 15890
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.09054262191057205,
      "learning_rate": 2.0555555555555555e-05,
      "loss": 0.0013,
      "step": 15900
    },
    {
      "epoch": 1.1785185185185185,
      "grad_norm": 0.04128333926200867,
      "learning_rate": 2.053703703703704e-05,
      "loss": 0.0012,
      "step": 15910
    },
    {
      "epoch": 1.1792592592592592,
      "grad_norm": 0.23156265914440155,
      "learning_rate": 2.051851851851852e-05,
      "loss": 0.0012,
      "step": 15920
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.08136362582445145,
      "learning_rate": 2.05e-05,
      "loss": 0.0008,
      "step": 15930
    },
    {
      "epoch": 1.1807407407407406,
      "grad_norm": 0.0,
      "learning_rate": 2.048148148148148e-05,
      "loss": 0.0012,
      "step": 15940
    },
    {
      "epoch": 1.1814814814814816,
      "grad_norm": 0.0,
      "learning_rate": 2.0462962962962965e-05,
      "loss": 0.0012,
      "step": 15950
    },
    {
      "epoch": 1.1822222222222223,
      "grad_norm": 0.10251227021217346,
      "learning_rate": 2.0444444444444446e-05,
      "loss": 0.0008,
      "step": 15960
    },
    {
      "epoch": 1.182962962962963,
      "grad_norm": 0.19061791896820068,
      "learning_rate": 2.0425925925925927e-05,
      "loss": 0.0019,
      "step": 15970
    },
    {
      "epoch": 1.1837037037037037,
      "grad_norm": 0.10158326476812363,
      "learning_rate": 2.0407407407407408e-05,
      "loss": 0.0013,
      "step": 15980
    },
    {
      "epoch": 1.1844444444444444,
      "grad_norm": 0.033728860318660736,
      "learning_rate": 2.0388888888888892e-05,
      "loss": 0.0014,
      "step": 15990
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 0.07982590794563293,
      "learning_rate": 2.037037037037037e-05,
      "loss": 0.0018,
      "step": 16000
    },
    {
      "epoch": 1.1859259259259258,
      "grad_norm": 0.1243315264582634,
      "learning_rate": 2.0351851851851853e-05,
      "loss": 0.0013,
      "step": 16010
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.13533005118370056,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0018,
      "step": 16020
    },
    {
      "epoch": 1.1874074074074075,
      "grad_norm": 0.0,
      "learning_rate": 2.0314814814814818e-05,
      "loss": 0.0015,
      "step": 16030
    },
    {
      "epoch": 1.1881481481481482,
      "grad_norm": 0.12550269067287445,
      "learning_rate": 2.0296296296296296e-05,
      "loss": 0.0011,
      "step": 16040
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 0.0787498876452446,
      "learning_rate": 2.027777777777778e-05,
      "loss": 0.0013,
      "step": 16050
    },
    {
      "epoch": 1.1896296296296296,
      "grad_norm": 0.0,
      "learning_rate": 2.025925925925926e-05,
      "loss": 0.0017,
      "step": 16060
    },
    {
      "epoch": 1.1903703703703703,
      "grad_norm": 0.16847160458564758,
      "learning_rate": 2.024074074074074e-05,
      "loss": 0.0016,
      "step": 16070
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 0.051888223737478256,
      "learning_rate": 2.0222222222222222e-05,
      "loss": 0.0016,
      "step": 16080
    },
    {
      "epoch": 1.191851851851852,
      "grad_norm": 0.08656851202249527,
      "learning_rate": 2.0203703703703706e-05,
      "loss": 0.0013,
      "step": 16090
    },
    {
      "epoch": 1.1925925925925926,
      "grad_norm": 0.0,
      "learning_rate": 2.0185185185185187e-05,
      "loss": 0.0016,
      "step": 16100
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.09670848399400711,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0022,
      "step": 16110
    },
    {
      "epoch": 1.194074074074074,
      "grad_norm": 0.0,
      "learning_rate": 2.014814814814815e-05,
      "loss": 0.0019,
      "step": 16120
    },
    {
      "epoch": 1.1948148148148148,
      "grad_norm": 0.043469637632369995,
      "learning_rate": 2.0129629629629633e-05,
      "loss": 0.0013,
      "step": 16130
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 0.11735979467630386,
      "learning_rate": 2.011111111111111e-05,
      "loss": 0.0016,
      "step": 16140
    },
    {
      "epoch": 1.1962962962962962,
      "grad_norm": 0.07389114797115326,
      "learning_rate": 2.0092592592592594e-05,
      "loss": 0.0016,
      "step": 16150
    },
    {
      "epoch": 1.1970370370370371,
      "grad_norm": 0.12034381926059723,
      "learning_rate": 2.0074074074074075e-05,
      "loss": 0.0019,
      "step": 16160
    },
    {
      "epoch": 1.1977777777777778,
      "grad_norm": 0.0,
      "learning_rate": 2.0055555555555556e-05,
      "loss": 0.0013,
      "step": 16170
    },
    {
      "epoch": 1.1985185185185185,
      "grad_norm": 0.0,
      "learning_rate": 2.0037037037037037e-05,
      "loss": 0.0012,
      "step": 16180
    },
    {
      "epoch": 1.1992592592592592,
      "grad_norm": 0.06568832695484161,
      "learning_rate": 2.001851851851852e-05,
      "loss": 0.0015,
      "step": 16190
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.12819644808769226,
      "learning_rate": 2e-05,
      "loss": 0.0025,
      "step": 16200
    },
    {
      "epoch": 1.2007407407407407,
      "grad_norm": 0.13499762117862701,
      "learning_rate": 1.9981481481481482e-05,
      "loss": 0.0006,
      "step": 16210
    },
    {
      "epoch": 1.2014814814814816,
      "grad_norm": 0.0,
      "learning_rate": 1.9962962962962963e-05,
      "loss": 0.0013,
      "step": 16220
    },
    {
      "epoch": 1.2022222222222223,
      "grad_norm": 0.13793914020061493,
      "learning_rate": 1.9944444444444447e-05,
      "loss": 0.0018,
      "step": 16230
    },
    {
      "epoch": 1.202962962962963,
      "grad_norm": 0.04202720522880554,
      "learning_rate": 1.9925925925925925e-05,
      "loss": 0.0009,
      "step": 16240
    },
    {
      "epoch": 1.2037037037037037,
      "grad_norm": 0.09723875671625137,
      "learning_rate": 1.990740740740741e-05,
      "loss": 0.0018,
      "step": 16250
    },
    {
      "epoch": 1.2044444444444444,
      "grad_norm": 0.11495719105005264,
      "learning_rate": 1.988888888888889e-05,
      "loss": 0.0008,
      "step": 16260
    },
    {
      "epoch": 1.2051851851851851,
      "grad_norm": 0.0,
      "learning_rate": 1.9870370370370374e-05,
      "loss": 0.0007,
      "step": 16270
    },
    {
      "epoch": 1.2059259259259258,
      "grad_norm": 0.08520451188087463,
      "learning_rate": 1.985185185185185e-05,
      "loss": 0.0014,
      "step": 16280
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0008,
      "step": 16290
    },
    {
      "epoch": 1.2074074074074075,
      "grad_norm": 0.02523074857890606,
      "learning_rate": 1.9814814814814816e-05,
      "loss": 0.0013,
      "step": 16300
    },
    {
      "epoch": 1.2081481481481482,
      "grad_norm": 0.08093101531267166,
      "learning_rate": 1.9796296296296297e-05,
      "loss": 0.0015,
      "step": 16310
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 0.0,
      "learning_rate": 1.9777777777777778e-05,
      "loss": 0.0014,
      "step": 16320
    },
    {
      "epoch": 1.2096296296296296,
      "grad_norm": 0.0,
      "learning_rate": 1.9759259259259262e-05,
      "loss": 0.0016,
      "step": 16330
    },
    {
      "epoch": 1.2103703703703703,
      "grad_norm": 0.0498497299849987,
      "learning_rate": 1.9740740740740743e-05,
      "loss": 0.002,
      "step": 16340
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 0.08703522384166718,
      "learning_rate": 1.9722222222222224e-05,
      "loss": 0.0015,
      "step": 16350
    },
    {
      "epoch": 1.211851851851852,
      "grad_norm": 0.04927204176783562,
      "learning_rate": 1.9703703703703704e-05,
      "loss": 0.0015,
      "step": 16360
    },
    {
      "epoch": 1.2125925925925927,
      "grad_norm": 0.08968318998813629,
      "learning_rate": 1.968518518518519e-05,
      "loss": 0.002,
      "step": 16370
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.1359545886516571,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.002,
      "step": 16380
    },
    {
      "epoch": 1.214074074074074,
      "grad_norm": 0.05139705538749695,
      "learning_rate": 1.964814814814815e-05,
      "loss": 0.0015,
      "step": 16390
    },
    {
      "epoch": 1.2148148148148148,
      "grad_norm": 0.13597741723060608,
      "learning_rate": 1.962962962962963e-05,
      "loss": 0.0017,
      "step": 16400
    },
    {
      "epoch": 1.2155555555555555,
      "grad_norm": 0.04241589084267616,
      "learning_rate": 1.9611111111111115e-05,
      "loss": 0.0016,
      "step": 16410
    },
    {
      "epoch": 1.2162962962962962,
      "grad_norm": 0.07941047102212906,
      "learning_rate": 1.9592592592592592e-05,
      "loss": 0.0017,
      "step": 16420
    },
    {
      "epoch": 1.217037037037037,
      "grad_norm": 0.04866088181734085,
      "learning_rate": 1.9574074074074077e-05,
      "loss": 0.0014,
      "step": 16430
    },
    {
      "epoch": 1.2177777777777778,
      "grad_norm": 0.0391412191092968,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 0.0012,
      "step": 16440
    },
    {
      "epoch": 1.2185185185185186,
      "grad_norm": 0.13163843750953674,
      "learning_rate": 1.9537037037037038e-05,
      "loss": 0.0017,
      "step": 16450
    },
    {
      "epoch": 1.2192592592592593,
      "grad_norm": 0.10909154266119003,
      "learning_rate": 1.951851851851852e-05,
      "loss": 0.0017,
      "step": 16460
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.1448858380317688,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.0015,
      "step": 16470
    },
    {
      "epoch": 1.2207407407407407,
      "grad_norm": 0.1591690480709076,
      "learning_rate": 1.948148148148148e-05,
      "loss": 0.0014,
      "step": 16480
    },
    {
      "epoch": 1.2214814814814814,
      "grad_norm": 0.04225108027458191,
      "learning_rate": 1.9462962962962965e-05,
      "loss": 0.0021,
      "step": 16490
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.04948411509394646,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 0.0013,
      "step": 16500
    },
    {
      "epoch": 1.222962962962963,
      "grad_norm": 0.04164225235581398,
      "learning_rate": 1.942592592592593e-05,
      "loss": 0.0015,
      "step": 16510
    },
    {
      "epoch": 1.2237037037037037,
      "grad_norm": 0.10611267387866974,
      "learning_rate": 1.9407407407407407e-05,
      "loss": 0.0026,
      "step": 16520
    },
    {
      "epoch": 1.2244444444444444,
      "grad_norm": 0.15604525804519653,
      "learning_rate": 1.938888888888889e-05,
      "loss": 0.0015,
      "step": 16530
    },
    {
      "epoch": 1.2251851851851852,
      "grad_norm": 0.049878522753715515,
      "learning_rate": 1.9370370370370372e-05,
      "loss": 0.0016,
      "step": 16540
    },
    {
      "epoch": 1.2259259259259259,
      "grad_norm": 0.055496394634246826,
      "learning_rate": 1.9351851851851853e-05,
      "loss": 0.0012,
      "step": 16550
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.04184994846582413,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0015,
      "step": 16560
    },
    {
      "epoch": 1.2274074074074075,
      "grad_norm": 0.08020450919866562,
      "learning_rate": 1.9314814814814818e-05,
      "loss": 0.0026,
      "step": 16570
    },
    {
      "epoch": 1.2281481481481482,
      "grad_norm": 0.1415310651063919,
      "learning_rate": 1.92962962962963e-05,
      "loss": 0.0013,
      "step": 16580
    },
    {
      "epoch": 1.228888888888889,
      "grad_norm": 0.1316664218902588,
      "learning_rate": 1.927777777777778e-05,
      "loss": 0.001,
      "step": 16590
    },
    {
      "epoch": 1.2296296296296296,
      "grad_norm": 0.021319426596164703,
      "learning_rate": 1.925925925925926e-05,
      "loss": 0.0012,
      "step": 16600
    },
    {
      "epoch": 1.2303703703703703,
      "grad_norm": 0.122410848736763,
      "learning_rate": 1.9240740740740744e-05,
      "loss": 0.0022,
      "step": 16610
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 0.04074987769126892,
      "learning_rate": 1.922222222222222e-05,
      "loss": 0.0015,
      "step": 16620
    },
    {
      "epoch": 1.231851851851852,
      "grad_norm": 0.11808089911937714,
      "learning_rate": 1.9203703703703706e-05,
      "loss": 0.0015,
      "step": 16630
    },
    {
      "epoch": 1.2325925925925927,
      "grad_norm": 0.0,
      "learning_rate": 1.9185185185185186e-05,
      "loss": 0.0011,
      "step": 16640
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.12956221401691437,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0016,
      "step": 16650
    },
    {
      "epoch": 1.234074074074074,
      "grad_norm": 0.056730855256319046,
      "learning_rate": 1.9148148148148148e-05,
      "loss": 0.0018,
      "step": 16660
    },
    {
      "epoch": 1.2348148148148148,
      "grad_norm": 0.04838084429502487,
      "learning_rate": 1.9129629629629632e-05,
      "loss": 0.0014,
      "step": 16670
    },
    {
      "epoch": 1.2355555555555555,
      "grad_norm": 0.08273594826459885,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.0012,
      "step": 16680
    },
    {
      "epoch": 1.2362962962962962,
      "grad_norm": 0.0,
      "learning_rate": 1.9092592592592594e-05,
      "loss": 0.0008,
      "step": 16690
    },
    {
      "epoch": 1.237037037037037,
      "grad_norm": 0.05432596802711487,
      "learning_rate": 1.9074074074074075e-05,
      "loss": 0.0013,
      "step": 16700
    },
    {
      "epoch": 1.2377777777777779,
      "grad_norm": 0.12381277233362198,
      "learning_rate": 1.905555555555556e-05,
      "loss": 0.0012,
      "step": 16710
    },
    {
      "epoch": 1.2385185185185186,
      "grad_norm": 0.0,
      "learning_rate": 1.903703703703704e-05,
      "loss": 0.0018,
      "step": 16720
    },
    {
      "epoch": 1.2392592592592593,
      "grad_norm": 0.04485514760017395,
      "learning_rate": 1.901851851851852e-05,
      "loss": 0.001,
      "step": 16730
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.04456401988863945,
      "learning_rate": 1.9e-05,
      "loss": 0.0011,
      "step": 16740
    },
    {
      "epoch": 1.2407407407407407,
      "grad_norm": 0.0453777089715004,
      "learning_rate": 1.8981481481481482e-05,
      "loss": 0.0018,
      "step": 16750
    },
    {
      "epoch": 1.2414814814814814,
      "grad_norm": 0.09272337704896927,
      "learning_rate": 1.8962962962962963e-05,
      "loss": 0.0011,
      "step": 16760
    },
    {
      "epoch": 1.2422222222222223,
      "grad_norm": 0.08000463247299194,
      "learning_rate": 1.8944444444444447e-05,
      "loss": 0.0015,
      "step": 16770
    },
    {
      "epoch": 1.242962962962963,
      "grad_norm": 0.0782441571354866,
      "learning_rate": 1.8925925925925928e-05,
      "loss": 0.001,
      "step": 16780
    },
    {
      "epoch": 1.2437037037037038,
      "grad_norm": 0.07270453870296478,
      "learning_rate": 1.890740740740741e-05,
      "loss": 0.002,
      "step": 16790
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.07299823313951492,
      "learning_rate": 1.888888888888889e-05,
      "loss": 0.0017,
      "step": 16800
    },
    {
      "epoch": 1.2451851851851852,
      "grad_norm": 0.04463770240545273,
      "learning_rate": 1.887037037037037e-05,
      "loss": 0.0014,
      "step": 16810
    },
    {
      "epoch": 1.2459259259259259,
      "grad_norm": 0.054635290056467056,
      "learning_rate": 1.8851851851851854e-05,
      "loss": 0.0015,
      "step": 16820
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.13430805504322052,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.001,
      "step": 16830
    },
    {
      "epoch": 1.2474074074074073,
      "grad_norm": 0.03384466469287872,
      "learning_rate": 1.8814814814814816e-05,
      "loss": 0.0017,
      "step": 16840
    },
    {
      "epoch": 1.2481481481481482,
      "grad_norm": 0.08498070389032364,
      "learning_rate": 1.8796296296296296e-05,
      "loss": 0.0016,
      "step": 16850
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 0.040932487696409225,
      "learning_rate": 1.8777777777777777e-05,
      "loss": 0.0015,
      "step": 16860
    },
    {
      "epoch": 1.2496296296296296,
      "grad_norm": 0.07448223978281021,
      "learning_rate": 1.875925925925926e-05,
      "loss": 0.0012,
      "step": 16870
    },
    {
      "epoch": 1.2503703703703704,
      "grad_norm": 0.17569895088672638,
      "learning_rate": 1.8740740740740742e-05,
      "loss": 0.0016,
      "step": 16880
    },
    {
      "epoch": 1.251111111111111,
      "grad_norm": 0.16563202440738678,
      "learning_rate": 1.8722222222222223e-05,
      "loss": 0.0033,
      "step": 16890
    },
    {
      "epoch": 1.2518518518518518,
      "grad_norm": 0.07463353127241135,
      "learning_rate": 1.8703703703703704e-05,
      "loss": 0.0013,
      "step": 16900
    },
    {
      "epoch": 1.2525925925925927,
      "grad_norm": 0.10112982988357544,
      "learning_rate": 1.8685185185185184e-05,
      "loss": 0.0014,
      "step": 16910
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.04477690905332565,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.001,
      "step": 16920
    },
    {
      "epoch": 1.2540740740740741,
      "grad_norm": 0.08117903769016266,
      "learning_rate": 1.864814814814815e-05,
      "loss": 0.0023,
      "step": 16930
    },
    {
      "epoch": 1.2548148148148148,
      "grad_norm": 0.04203437641263008,
      "learning_rate": 1.862962962962963e-05,
      "loss": 0.0014,
      "step": 16940
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 0.10480331629514694,
      "learning_rate": 1.861111111111111e-05,
      "loss": 0.0011,
      "step": 16950
    },
    {
      "epoch": 1.2562962962962962,
      "grad_norm": 0.0,
      "learning_rate": 1.8592592592592595e-05,
      "loss": 0.0014,
      "step": 16960
    },
    {
      "epoch": 1.257037037037037,
      "grad_norm": 0.0,
      "learning_rate": 1.8574074074074076e-05,
      "loss": 0.0021,
      "step": 16970
    },
    {
      "epoch": 1.2577777777777777,
      "grad_norm": 0.0883616954088211,
      "learning_rate": 1.8555555555555557e-05,
      "loss": 0.0012,
      "step": 16980
    },
    {
      "epoch": 1.2585185185185186,
      "grad_norm": 0.09077148884534836,
      "learning_rate": 1.8537037037037037e-05,
      "loss": 0.001,
      "step": 16990
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 0.0817972868680954,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 0.0015,
      "step": 17000
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.08039596676826477,
      "learning_rate": 1.85e-05,
      "loss": 0.0014,
      "step": 17010
    },
    {
      "epoch": 1.2607407407407407,
      "grad_norm": 0.12306898087263107,
      "learning_rate": 1.8481481481481483e-05,
      "loss": 0.0013,
      "step": 17020
    },
    {
      "epoch": 1.2614814814814814,
      "grad_norm": 0.07911080867052078,
      "learning_rate": 1.8462962962962964e-05,
      "loss": 0.001,
      "step": 17030
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 0.09911505877971649,
      "learning_rate": 1.8444444444444445e-05,
      "loss": 0.0013,
      "step": 17040
    },
    {
      "epoch": 1.262962962962963,
      "grad_norm": 0.09216979891061783,
      "learning_rate": 1.8425925925925926e-05,
      "loss": 0.0012,
      "step": 17050
    },
    {
      "epoch": 1.2637037037037038,
      "grad_norm": 0.08084840327501297,
      "learning_rate": 1.840740740740741e-05,
      "loss": 0.0013,
      "step": 17060
    },
    {
      "epoch": 1.2644444444444445,
      "grad_norm": 0.083204485476017,
      "learning_rate": 1.838888888888889e-05,
      "loss": 0.0009,
      "step": 17070
    },
    {
      "epoch": 1.2651851851851852,
      "grad_norm": 0.0749104917049408,
      "learning_rate": 1.837037037037037e-05,
      "loss": 0.0028,
      "step": 17080
    },
    {
      "epoch": 1.265925925925926,
      "grad_norm": 0.06600214540958405,
      "learning_rate": 1.8351851851851852e-05,
      "loss": 0.0014,
      "step": 17090
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.041205476969480515,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0014,
      "step": 17100
    },
    {
      "epoch": 1.2674074074074073,
      "grad_norm": 0.09755895286798477,
      "learning_rate": 1.8314814814814814e-05,
      "loss": 0.0012,
      "step": 17110
    },
    {
      "epoch": 1.268148148148148,
      "grad_norm": 0.08290132880210876,
      "learning_rate": 1.8296296296296298e-05,
      "loss": 0.002,
      "step": 17120
    },
    {
      "epoch": 1.268888888888889,
      "grad_norm": 0.05443256348371506,
      "learning_rate": 1.827777777777778e-05,
      "loss": 0.0014,
      "step": 17130
    },
    {
      "epoch": 1.2696296296296297,
      "grad_norm": 0.05060024559497833,
      "learning_rate": 1.825925925925926e-05,
      "loss": 0.0012,
      "step": 17140
    },
    {
      "epoch": 1.2703703703703704,
      "grad_norm": 0.1029338389635086,
      "learning_rate": 1.824074074074074e-05,
      "loss": 0.0017,
      "step": 17150
    },
    {
      "epoch": 1.271111111111111,
      "grad_norm": 0.06697551161050797,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 0.0008,
      "step": 17160
    },
    {
      "epoch": 1.2718518518518518,
      "grad_norm": 0.11761607974767685,
      "learning_rate": 1.8203703703703702e-05,
      "loss": 0.0017,
      "step": 17170
    },
    {
      "epoch": 1.2725925925925927,
      "grad_norm": 0.08131419122219086,
      "learning_rate": 1.8185185185185186e-05,
      "loss": 0.0016,
      "step": 17180
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.043434202671051025,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0015,
      "step": 17190
    },
    {
      "epoch": 1.2740740740740741,
      "grad_norm": 0.08122123032808304,
      "learning_rate": 1.814814814814815e-05,
      "loss": 0.001,
      "step": 17200
    },
    {
      "epoch": 1.2748148148148148,
      "grad_norm": 0.0,
      "learning_rate": 1.8129629629629628e-05,
      "loss": 0.0009,
      "step": 17210
    },
    {
      "epoch": 1.2755555555555556,
      "grad_norm": 0.04833044484257698,
      "learning_rate": 1.8111111111111112e-05,
      "loss": 0.0006,
      "step": 17220
    },
    {
      "epoch": 1.2762962962962963,
      "grad_norm": 0.044722069054841995,
      "learning_rate": 1.8092592592592593e-05,
      "loss": 0.0024,
      "step": 17230
    },
    {
      "epoch": 1.277037037037037,
      "grad_norm": 0.06081303209066391,
      "learning_rate": 1.8074074074074074e-05,
      "loss": 0.0022,
      "step": 17240
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.04098135977983475,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 0.001,
      "step": 17250
    },
    {
      "epoch": 1.2785185185185184,
      "grad_norm": 0.11477082222700119,
      "learning_rate": 1.803703703703704e-05,
      "loss": 0.0022,
      "step": 17260
    },
    {
      "epoch": 1.2792592592592593,
      "grad_norm": 0.0728485956788063,
      "learning_rate": 1.801851851851852e-05,
      "loss": 0.0016,
      "step": 17270
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.11819739639759064,
      "learning_rate": 1.8e-05,
      "loss": 0.0016,
      "step": 17280
    },
    {
      "epoch": 1.2807407407407407,
      "grad_norm": 0.1168956384062767,
      "learning_rate": 1.798148148148148e-05,
      "loss": 0.0023,
      "step": 17290
    },
    {
      "epoch": 1.2814814814814814,
      "grad_norm": 0.09401637315750122,
      "learning_rate": 1.7962962962962965e-05,
      "loss": 0.0012,
      "step": 17300
    },
    {
      "epoch": 1.2822222222222222,
      "grad_norm": 0.048056211322546005,
      "learning_rate": 1.7944444444444443e-05,
      "loss": 0.0017,
      "step": 17310
    },
    {
      "epoch": 1.282962962962963,
      "grad_norm": 0.14900489151477814,
      "learning_rate": 1.7925925925925927e-05,
      "loss": 0.002,
      "step": 17320
    },
    {
      "epoch": 1.2837037037037038,
      "grad_norm": 0.07813525199890137,
      "learning_rate": 1.7907407407407408e-05,
      "loss": 0.0019,
      "step": 17330
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 0.04875959828495979,
      "learning_rate": 1.788888888888889e-05,
      "loss": 0.0012,
      "step": 17340
    },
    {
      "epoch": 1.2851851851851852,
      "grad_norm": 0.0,
      "learning_rate": 1.787037037037037e-05,
      "loss": 0.0016,
      "step": 17350
    },
    {
      "epoch": 1.285925925925926,
      "grad_norm": 0.0,
      "learning_rate": 1.7851851851851853e-05,
      "loss": 0.0008,
      "step": 17360
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.099360391497612,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0024,
      "step": 17370
    },
    {
      "epoch": 1.2874074074074073,
      "grad_norm": 0.08909258991479874,
      "learning_rate": 1.7814814814814815e-05,
      "loss": 0.0012,
      "step": 17380
    },
    {
      "epoch": 1.288148148148148,
      "grad_norm": 0.0,
      "learning_rate": 1.7796296296296296e-05,
      "loss": 0.0009,
      "step": 17390
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.055966489017009735,
      "learning_rate": 1.777777777777778e-05,
      "loss": 0.0009,
      "step": 17400
    },
    {
      "epoch": 1.2896296296296297,
      "grad_norm": 0.11001469939947128,
      "learning_rate": 1.7759259259259257e-05,
      "loss": 0.002,
      "step": 17410
    },
    {
      "epoch": 1.2903703703703704,
      "grad_norm": 0.04216368868947029,
      "learning_rate": 1.774074074074074e-05,
      "loss": 0.0014,
      "step": 17420
    },
    {
      "epoch": 1.291111111111111,
      "grad_norm": 0.07633798569440842,
      "learning_rate": 1.7722222222222222e-05,
      "loss": 0.0016,
      "step": 17430
    },
    {
      "epoch": 1.2918518518518518,
      "grad_norm": 0.13094179332256317,
      "learning_rate": 1.7703703703703706e-05,
      "loss": 0.0018,
      "step": 17440
    },
    {
      "epoch": 1.2925925925925925,
      "grad_norm": 0.08723240345716476,
      "learning_rate": 1.7685185185185184e-05,
      "loss": 0.0009,
      "step": 17450
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.04694882035255432,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.001,
      "step": 17460
    },
    {
      "epoch": 1.2940740740740742,
      "grad_norm": 0.07773979753255844,
      "learning_rate": 1.764814814814815e-05,
      "loss": 0.0011,
      "step": 17470
    },
    {
      "epoch": 1.2948148148148149,
      "grad_norm": 0.08368220180273056,
      "learning_rate": 1.762962962962963e-05,
      "loss": 0.0014,
      "step": 17480
    },
    {
      "epoch": 1.2955555555555556,
      "grad_norm": 0.15464822947978973,
      "learning_rate": 1.761111111111111e-05,
      "loss": 0.0012,
      "step": 17490
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 0.12052714079618454,
      "learning_rate": 1.7592592592592595e-05,
      "loss": 0.0011,
      "step": 17500
    },
    {
      "epoch": 1.297037037037037,
      "grad_norm": 0.13948729634284973,
      "learning_rate": 1.7574074074074075e-05,
      "loss": 0.0012,
      "step": 17510
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 0.04956908896565437,
      "learning_rate": 1.7555555555555556e-05,
      "loss": 0.0024,
      "step": 17520
    },
    {
      "epoch": 1.2985185185185184,
      "grad_norm": 0.06789375096559525,
      "learning_rate": 1.7537037037037037e-05,
      "loss": 0.0016,
      "step": 17530
    },
    {
      "epoch": 1.2992592592592593,
      "grad_norm": 0.1351563036441803,
      "learning_rate": 1.751851851851852e-05,
      "loss": 0.0016,
      "step": 17540
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.0581439733505249,
      "learning_rate": 1.75e-05,
      "loss": 0.0017,
      "step": 17550
    },
    {
      "epoch": 1.3007407407407408,
      "grad_norm": 0.12396068125963211,
      "learning_rate": 1.7481481481481483e-05,
      "loss": 0.0015,
      "step": 17560
    },
    {
      "epoch": 1.3014814814814815,
      "grad_norm": 0.09492991119623184,
      "learning_rate": 1.7462962962962963e-05,
      "loss": 0.0013,
      "step": 17570
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 0.04345623031258583,
      "learning_rate": 1.7444444444444448e-05,
      "loss": 0.0018,
      "step": 17580
    },
    {
      "epoch": 1.302962962962963,
      "grad_norm": 0.0,
      "learning_rate": 1.7425925925925925e-05,
      "loss": 0.001,
      "step": 17590
    },
    {
      "epoch": 1.3037037037037038,
      "grad_norm": 0.0448579341173172,
      "learning_rate": 1.740740740740741e-05,
      "loss": 0.0018,
      "step": 17600
    },
    {
      "epoch": 1.3044444444444445,
      "grad_norm": 0.0874689519405365,
      "learning_rate": 1.738888888888889e-05,
      "loss": 0.002,
      "step": 17610
    },
    {
      "epoch": 1.3051851851851852,
      "grad_norm": 0.05597362294793129,
      "learning_rate": 1.737037037037037e-05,
      "loss": 0.0008,
      "step": 17620
    },
    {
      "epoch": 1.305925925925926,
      "grad_norm": 0.019689608365297318,
      "learning_rate": 1.735185185185185e-05,
      "loss": 0.0016,
      "step": 17630
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.08280438184738159,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.001,
      "step": 17640
    },
    {
      "epoch": 1.3074074074074074,
      "grad_norm": 0.21502043306827545,
      "learning_rate": 1.7314814814814813e-05,
      "loss": 0.0016,
      "step": 17650
    },
    {
      "epoch": 1.308148148148148,
      "grad_norm": 0.09485875070095062,
      "learning_rate": 1.7296296296296297e-05,
      "loss": 0.0015,
      "step": 17660
    },
    {
      "epoch": 1.3088888888888888,
      "grad_norm": 0.08182892948389053,
      "learning_rate": 1.7277777777777778e-05,
      "loss": 0.0008,
      "step": 17670
    },
    {
      "epoch": 1.3096296296296297,
      "grad_norm": 0.09451740980148315,
      "learning_rate": 1.7259259259259262e-05,
      "loss": 0.0017,
      "step": 17680
    },
    {
      "epoch": 1.3103703703703704,
      "grad_norm": 0.16273841261863708,
      "learning_rate": 1.724074074074074e-05,
      "loss": 0.0008,
      "step": 17690
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 0.0,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 0.0009,
      "step": 17700
    },
    {
      "epoch": 1.3118518518518518,
      "grad_norm": 0.047256581485271454,
      "learning_rate": 1.7203703703703705e-05,
      "loss": 0.0013,
      "step": 17710
    },
    {
      "epoch": 1.3125925925925925,
      "grad_norm": 0.10314825922250748,
      "learning_rate": 1.7185185185185185e-05,
      "loss": 0.002,
      "step": 17720
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.04633016884326935,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.002,
      "step": 17730
    },
    {
      "epoch": 1.3140740740740742,
      "grad_norm": 0.0921887755393982,
      "learning_rate": 1.714814814814815e-05,
      "loss": 0.0013,
      "step": 17740
    },
    {
      "epoch": 1.3148148148148149,
      "grad_norm": 0.04152586683630943,
      "learning_rate": 1.712962962962963e-05,
      "loss": 0.0016,
      "step": 17750
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 0.0926017090678215,
      "learning_rate": 1.7111111111111112e-05,
      "loss": 0.0018,
      "step": 17760
    },
    {
      "epoch": 1.3162962962962963,
      "grad_norm": 0.0,
      "learning_rate": 1.7092592592592593e-05,
      "loss": 0.0013,
      "step": 17770
    },
    {
      "epoch": 1.317037037037037,
      "grad_norm": 0.0,
      "learning_rate": 1.7074074074074077e-05,
      "loss": 0.0008,
      "step": 17780
    },
    {
      "epoch": 1.3177777777777777,
      "grad_norm": 0.09374640136957169,
      "learning_rate": 1.7055555555555554e-05,
      "loss": 0.0018,
      "step": 17790
    },
    {
      "epoch": 1.3185185185185184,
      "grad_norm": 0.06552194803953171,
      "learning_rate": 1.7037037037037038e-05,
      "loss": 0.0013,
      "step": 17800
    },
    {
      "epoch": 1.3192592592592591,
      "grad_norm": 0.07211795449256897,
      "learning_rate": 1.701851851851852e-05,
      "loss": 0.0012,
      "step": 17810
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.1302817016839981,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0015,
      "step": 17820
    },
    {
      "epoch": 1.3207407407407408,
      "grad_norm": 0.0,
      "learning_rate": 1.698148148148148e-05,
      "loss": 0.0013,
      "step": 17830
    },
    {
      "epoch": 1.3214814814814815,
      "grad_norm": 0.07494105398654938,
      "learning_rate": 1.6962962962962965e-05,
      "loss": 0.0022,
      "step": 17840
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 0.04489229992032051,
      "learning_rate": 1.6944444444444446e-05,
      "loss": 0.0016,
      "step": 17850
    },
    {
      "epoch": 1.322962962962963,
      "grad_norm": 0.10440856218338013,
      "learning_rate": 1.6925925925925926e-05,
      "loss": 0.001,
      "step": 17860
    },
    {
      "epoch": 1.3237037037037038,
      "grad_norm": 0.08763598650693893,
      "learning_rate": 1.6907407407407407e-05,
      "loss": 0.0013,
      "step": 17870
    },
    {
      "epoch": 1.3244444444444445,
      "grad_norm": 0.026522893458604813,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.0018,
      "step": 17880
    },
    {
      "epoch": 1.3251851851851852,
      "grad_norm": 0.17136235535144806,
      "learning_rate": 1.6870370370370372e-05,
      "loss": 0.0011,
      "step": 17890
    },
    {
      "epoch": 1.325925925925926,
      "grad_norm": 0.08137046545743942,
      "learning_rate": 1.6851851851851853e-05,
      "loss": 0.0017,
      "step": 17900
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.09500687569379807,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0009,
      "step": 17910
    },
    {
      "epoch": 1.3274074074074074,
      "grad_norm": 0.0,
      "learning_rate": 1.6814814814814818e-05,
      "loss": 0.0007,
      "step": 17920
    },
    {
      "epoch": 1.328148148148148,
      "grad_norm": 0.08586183190345764,
      "learning_rate": 1.6796296296296295e-05,
      "loss": 0.001,
      "step": 17930
    },
    {
      "epoch": 1.3288888888888888,
      "grad_norm": 0.029908545315265656,
      "learning_rate": 1.677777777777778e-05,
      "loss": 0.0018,
      "step": 17940
    },
    {
      "epoch": 1.3296296296296295,
      "grad_norm": 0.08963056653738022,
      "learning_rate": 1.675925925925926e-05,
      "loss": 0.0012,
      "step": 17950
    },
    {
      "epoch": 1.3303703703703704,
      "grad_norm": 0.0,
      "learning_rate": 1.674074074074074e-05,
      "loss": 0.0013,
      "step": 17960
    },
    {
      "epoch": 1.3311111111111111,
      "grad_norm": 0.0,
      "learning_rate": 1.6722222222222222e-05,
      "loss": 0.001,
      "step": 17970
    },
    {
      "epoch": 1.3318518518518518,
      "grad_norm": 0.0,
      "learning_rate": 1.6703703703703706e-05,
      "loss": 0.0007,
      "step": 17980
    },
    {
      "epoch": 1.3325925925925926,
      "grad_norm": 0.0647769570350647,
      "learning_rate": 1.6685185185185187e-05,
      "loss": 0.001,
      "step": 17990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.07020380347967148,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0005,
      "step": 18000
    },
    {
      "epoch": 1.3340740740740742,
      "grad_norm": 0.04338759928941727,
      "learning_rate": 1.6648148148148148e-05,
      "loss": 0.0013,
      "step": 18010
    },
    {
      "epoch": 1.334814814814815,
      "grad_norm": 0.10244103521108627,
      "learning_rate": 1.6629629629629632e-05,
      "loss": 0.0007,
      "step": 18020
    },
    {
      "epoch": 1.3355555555555556,
      "grad_norm": 0.12600158154964447,
      "learning_rate": 1.661111111111111e-05,
      "loss": 0.0005,
      "step": 18030
    },
    {
      "epoch": 1.3362962962962963,
      "grad_norm": 0.1107495129108429,
      "learning_rate": 1.6592592592592594e-05,
      "loss": 0.0021,
      "step": 18040
    },
    {
      "epoch": 1.337037037037037,
      "grad_norm": 0.05048386752605438,
      "learning_rate": 1.6574074074074075e-05,
      "loss": 0.0012,
      "step": 18050
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 0.0,
      "learning_rate": 1.655555555555556e-05,
      "loss": 0.001,
      "step": 18060
    },
    {
      "epoch": 1.3385185185185184,
      "grad_norm": 0.04044538736343384,
      "learning_rate": 1.6537037037037036e-05,
      "loss": 0.0012,
      "step": 18070
    },
    {
      "epoch": 1.3392592592592591,
      "grad_norm": 0.07187000662088394,
      "learning_rate": 1.651851851851852e-05,
      "loss": 0.0013,
      "step": 18080
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.0,
      "learning_rate": 1.65e-05,
      "loss": 0.0016,
      "step": 18090
    },
    {
      "epoch": 1.3407407407407408,
      "grad_norm": 0.09362665563821793,
      "learning_rate": 1.6481481481481482e-05,
      "loss": 0.0012,
      "step": 18100
    },
    {
      "epoch": 1.3414814814814815,
      "grad_norm": 0.08372976630926132,
      "learning_rate": 1.6462962962962963e-05,
      "loss": 0.0027,
      "step": 18110
    },
    {
      "epoch": 1.3422222222222222,
      "grad_norm": 0.041594646871089935,
      "learning_rate": 1.6444444444444447e-05,
      "loss": 0.0016,
      "step": 18120
    },
    {
      "epoch": 1.342962962962963,
      "grad_norm": 0.0,
      "learning_rate": 1.6425925925925928e-05,
      "loss": 0.0015,
      "step": 18130
    },
    {
      "epoch": 1.3437037037037036,
      "grad_norm": 0.041226308792829514,
      "learning_rate": 1.640740740740741e-05,
      "loss": 0.0013,
      "step": 18140
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 0.13097259402275085,
      "learning_rate": 1.638888888888889e-05,
      "loss": 0.001,
      "step": 18150
    },
    {
      "epoch": 1.3451851851851853,
      "grad_norm": 0.07722654193639755,
      "learning_rate": 1.6370370370370374e-05,
      "loss": 0.0014,
      "step": 18160
    },
    {
      "epoch": 1.345925925925926,
      "grad_norm": 0.12361010909080505,
      "learning_rate": 1.635185185185185e-05,
      "loss": 0.0011,
      "step": 18170
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.11583632230758667,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0018,
      "step": 18180
    },
    {
      "epoch": 1.3474074074074074,
      "grad_norm": 0.057594407349824905,
      "learning_rate": 1.6314814814814816e-05,
      "loss": 0.0007,
      "step": 18190
    },
    {
      "epoch": 1.348148148148148,
      "grad_norm": 0.15801182389259338,
      "learning_rate": 1.62962962962963e-05,
      "loss": 0.001,
      "step": 18200
    },
    {
      "epoch": 1.3488888888888888,
      "grad_norm": 0.04179757460951805,
      "learning_rate": 1.6277777777777777e-05,
      "loss": 0.0014,
      "step": 18210
    },
    {
      "epoch": 1.3496296296296295,
      "grad_norm": 0.14622315764427185,
      "learning_rate": 1.625925925925926e-05,
      "loss": 0.0025,
      "step": 18220
    },
    {
      "epoch": 1.3503703703703704,
      "grad_norm": 0.10157692432403564,
      "learning_rate": 1.6240740740740742e-05,
      "loss": 0.0007,
      "step": 18230
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 0.1422625184059143,
      "learning_rate": 1.6222222222222223e-05,
      "loss": 0.0013,
      "step": 18240
    },
    {
      "epoch": 1.3518518518518519,
      "grad_norm": 0.06269323825836182,
      "learning_rate": 1.6203703703703704e-05,
      "loss": 0.002,
      "step": 18250
    },
    {
      "epoch": 1.3525925925925926,
      "grad_norm": 0.0,
      "learning_rate": 1.6185185185185188e-05,
      "loss": 0.001,
      "step": 18260
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0011,
      "step": 18270
    },
    {
      "epoch": 1.3540740740740742,
      "grad_norm": 0.013737776316702366,
      "learning_rate": 1.614814814814815e-05,
      "loss": 0.0012,
      "step": 18280
    },
    {
      "epoch": 1.354814814814815,
      "grad_norm": 0.11358553171157837,
      "learning_rate": 1.612962962962963e-05,
      "loss": 0.0022,
      "step": 18290
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.06761497259140015,
      "learning_rate": 1.6111111111111115e-05,
      "loss": 0.001,
      "step": 18300
    },
    {
      "epoch": 1.3562962962962963,
      "grad_norm": 0.21465319395065308,
      "learning_rate": 1.6092592592592592e-05,
      "loss": 0.0021,
      "step": 18310
    },
    {
      "epoch": 1.357037037037037,
      "grad_norm": 0.17255693674087524,
      "learning_rate": 1.6074074074074076e-05,
      "loss": 0.001,
      "step": 18320
    },
    {
      "epoch": 1.3577777777777778,
      "grad_norm": 0.07742877304553986,
      "learning_rate": 1.6055555555555557e-05,
      "loss": 0.0018,
      "step": 18330
    },
    {
      "epoch": 1.3585185185185185,
      "grad_norm": 0.10520846396684647,
      "learning_rate": 1.6037037037037038e-05,
      "loss": 0.0022,
      "step": 18340
    },
    {
      "epoch": 1.3592592592592592,
      "grad_norm": 0.046733539551496506,
      "learning_rate": 1.601851851851852e-05,
      "loss": 0.0012,
      "step": 18350
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.08719810098409653,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0016,
      "step": 18360
    },
    {
      "epoch": 1.3607407407407408,
      "grad_norm": 0.09583297371864319,
      "learning_rate": 1.5981481481481483e-05,
      "loss": 0.0017,
      "step": 18370
    },
    {
      "epoch": 1.3614814814814815,
      "grad_norm": 0.04434102401137352,
      "learning_rate": 1.5962962962962964e-05,
      "loss": 0.0016,
      "step": 18380
    },
    {
      "epoch": 1.3622222222222222,
      "grad_norm": 0.10979913920164108,
      "learning_rate": 1.5944444444444445e-05,
      "loss": 0.0017,
      "step": 18390
    },
    {
      "epoch": 1.362962962962963,
      "grad_norm": 0.043828703463077545,
      "learning_rate": 1.5925925925925926e-05,
      "loss": 0.0006,
      "step": 18400
    },
    {
      "epoch": 1.3637037037037036,
      "grad_norm": 0.06265590339899063,
      "learning_rate": 1.5907407407407407e-05,
      "loss": 0.0016,
      "step": 18410
    },
    {
      "epoch": 1.3644444444444446,
      "grad_norm": 0.05699772760272026,
      "learning_rate": 1.588888888888889e-05,
      "loss": 0.0009,
      "step": 18420
    },
    {
      "epoch": 1.3651851851851853,
      "grad_norm": 0.09990667551755905,
      "learning_rate": 1.587037037037037e-05,
      "loss": 0.0016,
      "step": 18430
    },
    {
      "epoch": 1.365925925925926,
      "grad_norm": 0.0,
      "learning_rate": 1.5851851851851852e-05,
      "loss": 0.0008,
      "step": 18440
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.14628301560878754,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.0016,
      "step": 18450
    },
    {
      "epoch": 1.3674074074074074,
      "grad_norm": 0.05833273008465767,
      "learning_rate": 1.5814814814814817e-05,
      "loss": 0.0023,
      "step": 18460
    },
    {
      "epoch": 1.3681481481481481,
      "grad_norm": 0.2134069800376892,
      "learning_rate": 1.5796296296296298e-05,
      "loss": 0.0012,
      "step": 18470
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 0.045321423560380936,
      "learning_rate": 1.577777777777778e-05,
      "loss": 0.0015,
      "step": 18480
    },
    {
      "epoch": 1.3696296296296295,
      "grad_norm": 0.04780232906341553,
      "learning_rate": 1.575925925925926e-05,
      "loss": 0.0017,
      "step": 18490
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 0.0,
      "learning_rate": 1.574074074074074e-05,
      "loss": 0.0016,
      "step": 18500
    },
    {
      "epoch": 1.3711111111111112,
      "grad_norm": 0.15100936591625214,
      "learning_rate": 1.5722222222222225e-05,
      "loss": 0.0012,
      "step": 18510
    },
    {
      "epoch": 1.3718518518518519,
      "grad_norm": 0.041045986115932465,
      "learning_rate": 1.5703703703703705e-05,
      "loss": 0.0017,
      "step": 18520
    },
    {
      "epoch": 1.3725925925925926,
      "grad_norm": 0.04118075221776962,
      "learning_rate": 1.5685185185185186e-05,
      "loss": 0.002,
      "step": 18530
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.11931886523962021,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0011,
      "step": 18540
    },
    {
      "epoch": 1.374074074074074,
      "grad_norm": 0.0,
      "learning_rate": 1.5648148148148148e-05,
      "loss": 0.0011,
      "step": 18550
    },
    {
      "epoch": 1.374814814814815,
      "grad_norm": 0.1015741303563118,
      "learning_rate": 1.5629629629629632e-05,
      "loss": 0.0011,
      "step": 18560
    },
    {
      "epoch": 1.3755555555555556,
      "grad_norm": 0.0,
      "learning_rate": 1.5611111111111113e-05,
      "loss": 0.0013,
      "step": 18570
    },
    {
      "epoch": 1.3762962962962964,
      "grad_norm": 0.07416324317455292,
      "learning_rate": 1.5592592592592593e-05,
      "loss": 0.0011,
      "step": 18580
    },
    {
      "epoch": 1.377037037037037,
      "grad_norm": 0.08554709702730179,
      "learning_rate": 1.5574074074074074e-05,
      "loss": 0.0004,
      "step": 18590
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.11133953183889389,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 0.0016,
      "step": 18600
    },
    {
      "epoch": 1.3785185185185185,
      "grad_norm": 0.11388615518808365,
      "learning_rate": 1.553703703703704e-05,
      "loss": 0.0008,
      "step": 18610
    },
    {
      "epoch": 1.3792592592592592,
      "grad_norm": 0.09875086694955826,
      "learning_rate": 1.551851851851852e-05,
      "loss": 0.0017,
      "step": 18620
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.0,
      "learning_rate": 1.55e-05,
      "loss": 0.0028,
      "step": 18630
    },
    {
      "epoch": 1.3807407407407408,
      "grad_norm": 0.1328570395708084,
      "learning_rate": 1.548148148148148e-05,
      "loss": 0.0015,
      "step": 18640
    },
    {
      "epoch": 1.3814814814814815,
      "grad_norm": 0.0933060422539711,
      "learning_rate": 1.5462962962962962e-05,
      "loss": 0.0012,
      "step": 18650
    },
    {
      "epoch": 1.3822222222222222,
      "grad_norm": 0.0,
      "learning_rate": 1.5444444444444446e-05,
      "loss": 0.0008,
      "step": 18660
    },
    {
      "epoch": 1.382962962962963,
      "grad_norm": 0.08225410431623459,
      "learning_rate": 1.5425925925925927e-05,
      "loss": 0.0011,
      "step": 18670
    },
    {
      "epoch": 1.3837037037037037,
      "grad_norm": 0.08500991761684418,
      "learning_rate": 1.5407407407407408e-05,
      "loss": 0.0015,
      "step": 18680
    },
    {
      "epoch": 1.3844444444444444,
      "grad_norm": 0.10477819293737411,
      "learning_rate": 1.538888888888889e-05,
      "loss": 0.0012,
      "step": 18690
    },
    {
      "epoch": 1.3851851851851853,
      "grad_norm": 0.0,
      "learning_rate": 1.537037037037037e-05,
      "loss": 0.0011,
      "step": 18700
    },
    {
      "epoch": 1.385925925925926,
      "grad_norm": 0.0,
      "learning_rate": 1.5351851851851854e-05,
      "loss": 0.0012,
      "step": 18710
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0013,
      "step": 18720
    },
    {
      "epoch": 1.3874074074074074,
      "grad_norm": 0.1451408863067627,
      "learning_rate": 1.5314814814814815e-05,
      "loss": 0.0011,
      "step": 18730
    },
    {
      "epoch": 1.3881481481481481,
      "grad_norm": 0.11083029955625534,
      "learning_rate": 1.5296296296296296e-05,
      "loss": 0.0016,
      "step": 18740
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.08836977183818817,
      "learning_rate": 1.527777777777778e-05,
      "loss": 0.0014,
      "step": 18750
    },
    {
      "epoch": 1.3896296296296295,
      "grad_norm": 0.05987083539366722,
      "learning_rate": 1.5259259259259258e-05,
      "loss": 0.0012,
      "step": 18760
    },
    {
      "epoch": 1.3903703703703703,
      "grad_norm": 0.08509185910224915,
      "learning_rate": 1.5240740740740742e-05,
      "loss": 0.0024,
      "step": 18770
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 0.09997483342885971,
      "learning_rate": 1.5222222222222224e-05,
      "loss": 0.0015,
      "step": 18780
    },
    {
      "epoch": 1.391851851851852,
      "grad_norm": 0.05450453609228134,
      "learning_rate": 1.5203703703703703e-05,
      "loss": 0.001,
      "step": 18790
    },
    {
      "epoch": 1.3925925925925926,
      "grad_norm": 0.06981266289949417,
      "learning_rate": 1.5185185185185186e-05,
      "loss": 0.0005,
      "step": 18800
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.1271919459104538,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.002,
      "step": 18810
    },
    {
      "epoch": 1.394074074074074,
      "grad_norm": 0.10421805828809738,
      "learning_rate": 1.514814814814815e-05,
      "loss": 0.0017,
      "step": 18820
    },
    {
      "epoch": 1.394814814814815,
      "grad_norm": 0.09215520322322845,
      "learning_rate": 1.512962962962963e-05,
      "loss": 0.0024,
      "step": 18830
    },
    {
      "epoch": 1.3955555555555557,
      "grad_norm": 0.08461755514144897,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 0.0015,
      "step": 18840
    },
    {
      "epoch": 1.3962962962962964,
      "grad_norm": 0.06978166848421097,
      "learning_rate": 1.5092592592592595e-05,
      "loss": 0.0019,
      "step": 18850
    },
    {
      "epoch": 1.397037037037037,
      "grad_norm": 0.12558118999004364,
      "learning_rate": 1.5074074074074074e-05,
      "loss": 0.0007,
      "step": 18860
    },
    {
      "epoch": 1.3977777777777778,
      "grad_norm": 0.09551244229078293,
      "learning_rate": 1.5055555555555556e-05,
      "loss": 0.0012,
      "step": 18870
    },
    {
      "epoch": 1.3985185185185185,
      "grad_norm": 0.2049744725227356,
      "learning_rate": 1.5037037037037039e-05,
      "loss": 0.0013,
      "step": 18880
    },
    {
      "epoch": 1.3992592592592592,
      "grad_norm": 0.09652099013328552,
      "learning_rate": 1.5018518518518518e-05,
      "loss": 0.0005,
      "step": 18890
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.09557966142892838,
      "learning_rate": 1.5e-05,
      "loss": 0.0011,
      "step": 18900
    },
    {
      "epoch": 1.4007407407407406,
      "grad_norm": 0.0781610831618309,
      "learning_rate": 1.4981481481481483e-05,
      "loss": 0.0025,
      "step": 18910
    },
    {
      "epoch": 1.4014814814814816,
      "grad_norm": 0.06365899741649628,
      "learning_rate": 1.4962962962962965e-05,
      "loss": 0.0014,
      "step": 18920
    },
    {
      "epoch": 1.4022222222222223,
      "grad_norm": 0.12328556180000305,
      "learning_rate": 1.4944444444444444e-05,
      "loss": 0.0017,
      "step": 18930
    },
    {
      "epoch": 1.402962962962963,
      "grad_norm": 0.17598655819892883,
      "learning_rate": 1.4925925925925927e-05,
      "loss": 0.0016,
      "step": 18940
    },
    {
      "epoch": 1.4037037037037037,
      "grad_norm": 0.11621108651161194,
      "learning_rate": 1.490740740740741e-05,
      "loss": 0.0008,
      "step": 18950
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 0.07057630270719528,
      "learning_rate": 1.4888888888888888e-05,
      "loss": 0.0014,
      "step": 18960
    },
    {
      "epoch": 1.4051851851851853,
      "grad_norm": 0.048506785184144974,
      "learning_rate": 1.4870370370370371e-05,
      "loss": 0.0017,
      "step": 18970
    },
    {
      "epoch": 1.405925925925926,
      "grad_norm": 0.0,
      "learning_rate": 1.4851851851851853e-05,
      "loss": 0.0011,
      "step": 18980
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.1378346085548401,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0016,
      "step": 18990
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 0.08760543167591095,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 0.0015,
      "step": 19000
    },
    {
      "epoch": 1.4081481481481481,
      "grad_norm": 0.0839906707406044,
      "learning_rate": 1.4796296296296297e-05,
      "loss": 0.001,
      "step": 19010
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.08036913722753525,
      "learning_rate": 1.477777777777778e-05,
      "loss": 0.0013,
      "step": 19020
    },
    {
      "epoch": 1.4096296296296296,
      "grad_norm": 0.050971999764442444,
      "learning_rate": 1.4759259259259259e-05,
      "loss": 0.0017,
      "step": 19030
    },
    {
      "epoch": 1.4103703703703703,
      "grad_norm": 0.0,
      "learning_rate": 1.4740740740740741e-05,
      "loss": 0.0008,
      "step": 19040
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 0.2100362479686737,
      "learning_rate": 1.4722222222222224e-05,
      "loss": 0.0012,
      "step": 19050
    },
    {
      "epoch": 1.411851851851852,
      "grad_norm": 0.16985434293746948,
      "learning_rate": 1.4703703703703706e-05,
      "loss": 0.0017,
      "step": 19060
    },
    {
      "epoch": 1.4125925925925926,
      "grad_norm": 0.12670399248600006,
      "learning_rate": 1.4685185185185186e-05,
      "loss": 0.0013,
      "step": 19070
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.0792313739657402,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0017,
      "step": 19080
    },
    {
      "epoch": 1.414074074074074,
      "grad_norm": 0.05961642414331436,
      "learning_rate": 1.464814814814815e-05,
      "loss": 0.0027,
      "step": 19090
    },
    {
      "epoch": 1.4148148148148147,
      "grad_norm": 0.12499914318323135,
      "learning_rate": 1.462962962962963e-05,
      "loss": 0.0013,
      "step": 19100
    },
    {
      "epoch": 1.4155555555555557,
      "grad_norm": 0.12946705520153046,
      "learning_rate": 1.4611111111111112e-05,
      "loss": 0.0018,
      "step": 19110
    },
    {
      "epoch": 1.4162962962962964,
      "grad_norm": 0.15380451083183289,
      "learning_rate": 1.4592592592592594e-05,
      "loss": 0.0014,
      "step": 19120
    },
    {
      "epoch": 1.417037037037037,
      "grad_norm": 0.045307405292987823,
      "learning_rate": 1.4574074074074075e-05,
      "loss": 0.0017,
      "step": 19130
    },
    {
      "epoch": 1.4177777777777778,
      "grad_norm": 0.144704207777977,
      "learning_rate": 1.4555555555555556e-05,
      "loss": 0.0014,
      "step": 19140
    },
    {
      "epoch": 1.4185185185185185,
      "grad_norm": 0.1601913571357727,
      "learning_rate": 1.4537037037037039e-05,
      "loss": 0.001,
      "step": 19150
    },
    {
      "epoch": 1.4192592592592592,
      "grad_norm": 0.0,
      "learning_rate": 1.4518518518518521e-05,
      "loss": 0.0023,
      "step": 19160
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.05744429677724838,
      "learning_rate": 1.45e-05,
      "loss": 0.0021,
      "step": 19170
    },
    {
      "epoch": 1.4207407407407406,
      "grad_norm": 0.05332468822598457,
      "learning_rate": 1.4481481481481483e-05,
      "loss": 0.0015,
      "step": 19180
    },
    {
      "epoch": 1.4214814814814813,
      "grad_norm": 0.10424929857254028,
      "learning_rate": 1.4462962962962965e-05,
      "loss": 0.0019,
      "step": 19190
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.0,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 0.0011,
      "step": 19200
    },
    {
      "epoch": 1.422962962962963,
      "grad_norm": 0.10616008937358856,
      "learning_rate": 1.4425925925925927e-05,
      "loss": 0.0015,
      "step": 19210
    },
    {
      "epoch": 1.4237037037037037,
      "grad_norm": 0.0287790484726429,
      "learning_rate": 1.4407407407407409e-05,
      "loss": 0.0012,
      "step": 19220
    },
    {
      "epoch": 1.4244444444444444,
      "grad_norm": 0.09309825301170349,
      "learning_rate": 1.438888888888889e-05,
      "loss": 0.0013,
      "step": 19230
    },
    {
      "epoch": 1.425185185185185,
      "grad_norm": 0.0,
      "learning_rate": 1.437037037037037e-05,
      "loss": 0.0014,
      "step": 19240
    },
    {
      "epoch": 1.425925925925926,
      "grad_norm": 0.05787263065576553,
      "learning_rate": 1.4351851851851853e-05,
      "loss": 0.0009,
      "step": 19250
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.09607832878828049,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0004,
      "step": 19260
    },
    {
      "epoch": 1.4274074074074075,
      "grad_norm": 0.15393589437007904,
      "learning_rate": 1.4314814814814815e-05,
      "loss": 0.0012,
      "step": 19270
    },
    {
      "epoch": 1.4281481481481482,
      "grad_norm": 0.13255222141742706,
      "learning_rate": 1.4296296296296297e-05,
      "loss": 0.0024,
      "step": 19280
    },
    {
      "epoch": 1.4288888888888889,
      "grad_norm": 0.0,
      "learning_rate": 1.427777777777778e-05,
      "loss": 0.0006,
      "step": 19290
    },
    {
      "epoch": 1.4296296296296296,
      "grad_norm": 0.05643819645047188,
      "learning_rate": 1.425925925925926e-05,
      "loss": 0.0011,
      "step": 19300
    },
    {
      "epoch": 1.4303703703703703,
      "grad_norm": 0.0,
      "learning_rate": 1.4240740740740741e-05,
      "loss": 0.0012,
      "step": 19310
    },
    {
      "epoch": 1.431111111111111,
      "grad_norm": 0.04285145178437233,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 0.0014,
      "step": 19320
    },
    {
      "epoch": 1.431851851851852,
      "grad_norm": 0.10270066559314728,
      "learning_rate": 1.4203703703703704e-05,
      "loss": 0.0017,
      "step": 19330
    },
    {
      "epoch": 1.4325925925925926,
      "grad_norm": 0.09776964038610458,
      "learning_rate": 1.4185185185185185e-05,
      "loss": 0.001,
      "step": 19340
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.13349823653697968,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0018,
      "step": 19350
    },
    {
      "epoch": 1.434074074074074,
      "grad_norm": 0.12291951477527618,
      "learning_rate": 1.4148148148148148e-05,
      "loss": 0.0007,
      "step": 19360
    },
    {
      "epoch": 1.4348148148148148,
      "grad_norm": 0.12541227042675018,
      "learning_rate": 1.4129629629629631e-05,
      "loss": 0.0017,
      "step": 19370
    },
    {
      "epoch": 1.4355555555555555,
      "grad_norm": 0.1681729555130005,
      "learning_rate": 1.4111111111111112e-05,
      "loss": 0.0014,
      "step": 19380
    },
    {
      "epoch": 1.4362962962962964,
      "grad_norm": 0.0,
      "learning_rate": 1.4092592592592592e-05,
      "loss": 0.0006,
      "step": 19390
    },
    {
      "epoch": 1.4370370370370371,
      "grad_norm": 0.089934803545475,
      "learning_rate": 1.4074074074074075e-05,
      "loss": 0.002,
      "step": 19400
    },
    {
      "epoch": 1.4377777777777778,
      "grad_norm": 0.09127416461706161,
      "learning_rate": 1.4055555555555556e-05,
      "loss": 0.0015,
      "step": 19410
    },
    {
      "epoch": 1.4385185185185185,
      "grad_norm": 0.08856870234012604,
      "learning_rate": 1.4037037037037038e-05,
      "loss": 0.0011,
      "step": 19420
    },
    {
      "epoch": 1.4392592592592592,
      "grad_norm": 0.0,
      "learning_rate": 1.4018518518518519e-05,
      "loss": 0.0019,
      "step": 19430
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.15574008226394653,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0016,
      "step": 19440
    },
    {
      "epoch": 1.4407407407407407,
      "grad_norm": 0.06843029707670212,
      "learning_rate": 1.3981481481481482e-05,
      "loss": 0.002,
      "step": 19450
    },
    {
      "epoch": 1.4414814814814814,
      "grad_norm": 0.20017559826374054,
      "learning_rate": 1.3962962962962963e-05,
      "loss": 0.0015,
      "step": 19460
    },
    {
      "epoch": 1.4422222222222223,
      "grad_norm": 0.125624418258667,
      "learning_rate": 1.3944444444444446e-05,
      "loss": 0.0024,
      "step": 19470
    },
    {
      "epoch": 1.442962962962963,
      "grad_norm": 0.06992337107658386,
      "learning_rate": 1.3925925925925926e-05,
      "loss": 0.0021,
      "step": 19480
    },
    {
      "epoch": 1.4437037037037037,
      "grad_norm": 0.13732901215553284,
      "learning_rate": 1.3907407407407407e-05,
      "loss": 0.0019,
      "step": 19490
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.0,
      "learning_rate": 1.388888888888889e-05,
      "loss": 0.0015,
      "step": 19500
    },
    {
      "epoch": 1.4451851851851851,
      "grad_norm": 0.0834031030535698,
      "learning_rate": 1.387037037037037e-05,
      "loss": 0.0013,
      "step": 19510
    },
    {
      "epoch": 1.445925925925926,
      "grad_norm": 0.0,
      "learning_rate": 1.3851851851851853e-05,
      "loss": 0.0024,
      "step": 19520
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.15001583099365234,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.0018,
      "step": 19530
    },
    {
      "epoch": 1.4474074074074075,
      "grad_norm": 0.1115831509232521,
      "learning_rate": 1.3814814814814816e-05,
      "loss": 0.0027,
      "step": 19540
    },
    {
      "epoch": 1.4481481481481482,
      "grad_norm": 0.12925074994564056,
      "learning_rate": 1.3796296296296297e-05,
      "loss": 0.0015,
      "step": 19550
    },
    {
      "epoch": 1.448888888888889,
      "grad_norm": 0.0,
      "learning_rate": 1.3777777777777778e-05,
      "loss": 0.0016,
      "step": 19560
    },
    {
      "epoch": 1.4496296296296296,
      "grad_norm": 0.05099371820688248,
      "learning_rate": 1.375925925925926e-05,
      "loss": 0.0014,
      "step": 19570
    },
    {
      "epoch": 1.4503703703703703,
      "grad_norm": 0.0,
      "learning_rate": 1.3740740740740741e-05,
      "loss": 0.002,
      "step": 19580
    },
    {
      "epoch": 1.451111111111111,
      "grad_norm": 0.11778111010789871,
      "learning_rate": 1.3722222222222222e-05,
      "loss": 0.0009,
      "step": 19590
    },
    {
      "epoch": 1.4518518518518517,
      "grad_norm": 0.11324235796928406,
      "learning_rate": 1.3703703703703704e-05,
      "loss": 0.0012,
      "step": 19600
    },
    {
      "epoch": 1.4525925925925927,
      "grad_norm": 0.045961715281009674,
      "learning_rate": 1.3685185185185187e-05,
      "loss": 0.0021,
      "step": 19610
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.15531383454799652,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0022,
      "step": 19620
    },
    {
      "epoch": 1.454074074074074,
      "grad_norm": 0.09295913577079773,
      "learning_rate": 1.3648148148148148e-05,
      "loss": 0.0014,
      "step": 19630
    },
    {
      "epoch": 1.4548148148148148,
      "grad_norm": 0.06294389814138412,
      "learning_rate": 1.362962962962963e-05,
      "loss": 0.0018,
      "step": 19640
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 0.04623841121792793,
      "learning_rate": 1.3611111111111111e-05,
      "loss": 0.0017,
      "step": 19650
    },
    {
      "epoch": 1.4562962962962964,
      "grad_norm": 0.0,
      "learning_rate": 1.3592592592592592e-05,
      "loss": 0.0003,
      "step": 19660
    },
    {
      "epoch": 1.4570370370370371,
      "grad_norm": 0.1345067024230957,
      "learning_rate": 1.3574074074074075e-05,
      "loss": 0.0013,
      "step": 19670
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 0.10301416367292404,
      "learning_rate": 1.3555555555555557e-05,
      "loss": 0.0014,
      "step": 19680
    },
    {
      "epoch": 1.4585185185185185,
      "grad_norm": 0.0767710730433464,
      "learning_rate": 1.3537037037037036e-05,
      "loss": 0.0018,
      "step": 19690
    },
    {
      "epoch": 1.4592592592592593,
      "grad_norm": 0.0679977685213089,
      "learning_rate": 1.3518518518518519e-05,
      "loss": 0.0011,
      "step": 19700
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.16062092781066895,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0014,
      "step": 19710
    },
    {
      "epoch": 1.4607407407407407,
      "grad_norm": 0.045337773859500885,
      "learning_rate": 1.348148148148148e-05,
      "loss": 0.0016,
      "step": 19720
    },
    {
      "epoch": 1.4614814814814814,
      "grad_norm": 0.08224350959062576,
      "learning_rate": 1.3462962962962963e-05,
      "loss": 0.0008,
      "step": 19730
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 0.11259647458791733,
      "learning_rate": 1.3444444444444445e-05,
      "loss": 0.0014,
      "step": 19740
    },
    {
      "epoch": 1.462962962962963,
      "grad_norm": 0.06403503566980362,
      "learning_rate": 1.3425925925925928e-05,
      "loss": 0.0012,
      "step": 19750
    },
    {
      "epoch": 1.4637037037037037,
      "grad_norm": 0.0886450707912445,
      "learning_rate": 1.3407407407407407e-05,
      "loss": 0.0009,
      "step": 19760
    },
    {
      "epoch": 1.4644444444444444,
      "grad_norm": 0.04408144950866699,
      "learning_rate": 1.338888888888889e-05,
      "loss": 0.0017,
      "step": 19770
    },
    {
      "epoch": 1.4651851851851851,
      "grad_norm": 0.12195791304111481,
      "learning_rate": 1.3370370370370372e-05,
      "loss": 0.0019,
      "step": 19780
    },
    {
      "epoch": 1.4659259259259259,
      "grad_norm": 0.0,
      "learning_rate": 1.335185185185185e-05,
      "loss": 0.0006,
      "step": 19790
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.04043755307793617,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0022,
      "step": 19800
    },
    {
      "epoch": 1.4674074074074075,
      "grad_norm": 0.08897950500249863,
      "learning_rate": 1.3314814814814816e-05,
      "loss": 0.0006,
      "step": 19810
    },
    {
      "epoch": 1.4681481481481482,
      "grad_norm": 0.07837974280118942,
      "learning_rate": 1.3296296296296295e-05,
      "loss": 0.0011,
      "step": 19820
    },
    {
      "epoch": 1.468888888888889,
      "grad_norm": 0.124991774559021,
      "learning_rate": 1.3277777777777777e-05,
      "loss": 0.001,
      "step": 19830
    },
    {
      "epoch": 1.4696296296296296,
      "grad_norm": 0.10396493971347809,
      "learning_rate": 1.325925925925926e-05,
      "loss": 0.0022,
      "step": 19840
    },
    {
      "epoch": 1.4703703703703703,
      "grad_norm": 0.07789458334445953,
      "learning_rate": 1.3240740740740742e-05,
      "loss": 0.0018,
      "step": 19850
    },
    {
      "epoch": 1.471111111111111,
      "grad_norm": 0.07309436798095703,
      "learning_rate": 1.3222222222222221e-05,
      "loss": 0.0011,
      "step": 19860
    },
    {
      "epoch": 1.4718518518518517,
      "grad_norm": 0.04324348643422127,
      "learning_rate": 1.3203703703703704e-05,
      "loss": 0.0024,
      "step": 19870
    },
    {
      "epoch": 1.4725925925925927,
      "grad_norm": 0.09679950773715973,
      "learning_rate": 1.3185185185185186e-05,
      "loss": 0.0021,
      "step": 19880
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.09597700089216232,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.002,
      "step": 19890
    },
    {
      "epoch": 1.474074074074074,
      "grad_norm": 0.07456973195075989,
      "learning_rate": 1.3148148148148148e-05,
      "loss": 0.0013,
      "step": 19900
    },
    {
      "epoch": 1.4748148148148148,
      "grad_norm": 0.19473916292190552,
      "learning_rate": 1.312962962962963e-05,
      "loss": 0.0018,
      "step": 19910
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 0.05512366071343422,
      "learning_rate": 1.3111111111111113e-05,
      "loss": 0.0018,
      "step": 19920
    },
    {
      "epoch": 1.4762962962962962,
      "grad_norm": 0.11120373755693436,
      "learning_rate": 1.3092592592592592e-05,
      "loss": 0.0008,
      "step": 19930
    },
    {
      "epoch": 1.4770370370370371,
      "grad_norm": 0.0691838264465332,
      "learning_rate": 1.3074074074074074e-05,
      "loss": 0.0017,
      "step": 19940
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 0.0,
      "learning_rate": 1.3055555555555557e-05,
      "loss": 0.001,
      "step": 19950
    },
    {
      "epoch": 1.4785185185185186,
      "grad_norm": 0.10613549500703812,
      "learning_rate": 1.3037037037037036e-05,
      "loss": 0.0021,
      "step": 19960
    },
    {
      "epoch": 1.4792592592592593,
      "grad_norm": 0.1849115490913391,
      "learning_rate": 1.3018518518518518e-05,
      "loss": 0.0015,
      "step": 19970
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.04202382266521454,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0022,
      "step": 19980
    },
    {
      "epoch": 1.4807407407407407,
      "grad_norm": 0.0,
      "learning_rate": 1.2981481481481483e-05,
      "loss": 0.0007,
      "step": 19990
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 0.0,
      "learning_rate": 1.2962962962962962e-05,
      "loss": 0.0007,
      "step": 20000
    },
    {
      "epoch": 1.482222222222222,
      "grad_norm": 0.04859485104680061,
      "learning_rate": 1.2944444444444445e-05,
      "loss": 0.0019,
      "step": 20010
    },
    {
      "epoch": 1.482962962962963,
      "grad_norm": 0.1545298546552658,
      "learning_rate": 1.2925925925925927e-05,
      "loss": 0.0016,
      "step": 20020
    },
    {
      "epoch": 1.4837037037037037,
      "grad_norm": 0.11643269658088684,
      "learning_rate": 1.2907407407407406e-05,
      "loss": 0.0007,
      "step": 20030
    },
    {
      "epoch": 1.4844444444444445,
      "grad_norm": 0.02572478912770748,
      "learning_rate": 1.2888888888888889e-05,
      "loss": 0.0012,
      "step": 20040
    },
    {
      "epoch": 1.4851851851851852,
      "grad_norm": 0.042155612260103226,
      "learning_rate": 1.2870370370370371e-05,
      "loss": 0.0015,
      "step": 20050
    },
    {
      "epoch": 1.4859259259259259,
      "grad_norm": 0.0898001492023468,
      "learning_rate": 1.2851851851851854e-05,
      "loss": 0.0017,
      "step": 20060
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.0418325774371624,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0015,
      "step": 20070
    },
    {
      "epoch": 1.4874074074074075,
      "grad_norm": 0.10324583202600479,
      "learning_rate": 1.2814814814814815e-05,
      "loss": 0.0012,
      "step": 20080
    },
    {
      "epoch": 1.4881481481481482,
      "grad_norm": 0.08383920043706894,
      "learning_rate": 1.2796296296296298e-05,
      "loss": 0.0015,
      "step": 20090
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.1303834468126297,
      "learning_rate": 1.2777777777777777e-05,
      "loss": 0.0005,
      "step": 20100
    },
    {
      "epoch": 1.4896296296296296,
      "grad_norm": 0.05425102636218071,
      "learning_rate": 1.275925925925926e-05,
      "loss": 0.0029,
      "step": 20110
    },
    {
      "epoch": 1.4903703703703703,
      "grad_norm": 0.0,
      "learning_rate": 1.2740740740740742e-05,
      "loss": 0.0017,
      "step": 20120
    },
    {
      "epoch": 1.491111111111111,
      "grad_norm": 0.04257504269480705,
      "learning_rate": 1.2722222222222221e-05,
      "loss": 0.0013,
      "step": 20130
    },
    {
      "epoch": 1.4918518518518518,
      "grad_norm": 0.05306442454457283,
      "learning_rate": 1.2703703703703704e-05,
      "loss": 0.0011,
      "step": 20140
    },
    {
      "epoch": 1.4925925925925925,
      "grad_norm": 0.0821029543876648,
      "learning_rate": 1.2685185185185186e-05,
      "loss": 0.001,
      "step": 20150
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.07301480323076248,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0018,
      "step": 20160
    },
    {
      "epoch": 1.494074074074074,
      "grad_norm": 0.0,
      "learning_rate": 1.2648148148148148e-05,
      "loss": 0.0009,
      "step": 20170
    },
    {
      "epoch": 1.4948148148148148,
      "grad_norm": 0.11817802488803864,
      "learning_rate": 1.262962962962963e-05,
      "loss": 0.0028,
      "step": 20180
    },
    {
      "epoch": 1.4955555555555555,
      "grad_norm": 0.0,
      "learning_rate": 1.2611111111111113e-05,
      "loss": 0.0018,
      "step": 20190
    },
    {
      "epoch": 1.4962962962962962,
      "grad_norm": 0.08416759967803955,
      "learning_rate": 1.2592592592592592e-05,
      "loss": 0.0013,
      "step": 20200
    },
    {
      "epoch": 1.4970370370370372,
      "grad_norm": 0.06236456334590912,
      "learning_rate": 1.2574074074074074e-05,
      "loss": 0.0017,
      "step": 20210
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 0.0,
      "learning_rate": 1.2555555555555557e-05,
      "loss": 0.0012,
      "step": 20220
    },
    {
      "epoch": 1.4985185185185186,
      "grad_norm": 0.10764031857252121,
      "learning_rate": 1.2537037037037039e-05,
      "loss": 0.0004,
      "step": 20230
    },
    {
      "epoch": 1.4992592592592593,
      "grad_norm": 0.08394879847764969,
      "learning_rate": 1.2518518518518518e-05,
      "loss": 0.0021,
      "step": 20240
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.10506708174943924,
      "learning_rate": 1.25e-05,
      "loss": 0.0012,
      "step": 20250
    },
    {
      "epoch": 1.5007407407407407,
      "grad_norm": 0.12688440084457397,
      "learning_rate": 1.2481481481481481e-05,
      "loss": 0.0011,
      "step": 20260
    },
    {
      "epoch": 1.5014814814814814,
      "grad_norm": 0.0567767359316349,
      "learning_rate": 1.2462962962962964e-05,
      "loss": 0.0019,
      "step": 20270
    },
    {
      "epoch": 1.5022222222222221,
      "grad_norm": 0.04332859441637993,
      "learning_rate": 1.2444444444444445e-05,
      "loss": 0.0017,
      "step": 20280
    },
    {
      "epoch": 1.5029629629629628,
      "grad_norm": 0.08186151087284088,
      "learning_rate": 1.2425925925925927e-05,
      "loss": 0.001,
      "step": 20290
    },
    {
      "epoch": 1.5037037037037035,
      "grad_norm": 0.08728955686092377,
      "learning_rate": 1.2407407407407408e-05,
      "loss": 0.001,
      "step": 20300
    },
    {
      "epoch": 1.5044444444444445,
      "grad_norm": 0.09836477041244507,
      "learning_rate": 1.238888888888889e-05,
      "loss": 0.0011,
      "step": 20310
    },
    {
      "epoch": 1.5051851851851852,
      "grad_norm": 0.054139092564582825,
      "learning_rate": 1.2370370370370371e-05,
      "loss": 0.0008,
      "step": 20320
    },
    {
      "epoch": 1.505925925925926,
      "grad_norm": 0.15761993825435638,
      "learning_rate": 1.2351851851851852e-05,
      "loss": 0.0009,
      "step": 20330
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.051821380853652954,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0018,
      "step": 20340
    },
    {
      "epoch": 1.5074074074074075,
      "grad_norm": 0.06622008979320526,
      "learning_rate": 1.2314814814814815e-05,
      "loss": 0.0025,
      "step": 20350
    },
    {
      "epoch": 1.5081481481481482,
      "grad_norm": 0.07498287409543991,
      "learning_rate": 1.2296296296296298e-05,
      "loss": 0.0007,
      "step": 20360
    },
    {
      "epoch": 1.508888888888889,
      "grad_norm": 0.052760448306798935,
      "learning_rate": 1.2277777777777778e-05,
      "loss": 0.0012,
      "step": 20370
    },
    {
      "epoch": 1.5096296296296297,
      "grad_norm": 0.1457713544368744,
      "learning_rate": 1.225925925925926e-05,
      "loss": 0.0012,
      "step": 20380
    },
    {
      "epoch": 1.5103703703703704,
      "grad_norm": 0.06475807726383209,
      "learning_rate": 1.2240740740740742e-05,
      "loss": 0.0011,
      "step": 20390
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.050787366926670074,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 0.0019,
      "step": 20400
    },
    {
      "epoch": 1.5118518518518518,
      "grad_norm": 0.0,
      "learning_rate": 1.2203703703703705e-05,
      "loss": 0.0007,
      "step": 20410
    },
    {
      "epoch": 1.5125925925925925,
      "grad_norm": 0.08335957676172256,
      "learning_rate": 1.2185185185185186e-05,
      "loss": 0.0014,
      "step": 20420
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.14293520152568817,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0009,
      "step": 20430
    },
    {
      "epoch": 1.5140740740740741,
      "grad_norm": 0.04336727038025856,
      "learning_rate": 1.2148148148148149e-05,
      "loss": 0.0007,
      "step": 20440
    },
    {
      "epoch": 1.5148148148148148,
      "grad_norm": 0.052331626415252686,
      "learning_rate": 1.212962962962963e-05,
      "loss": 0.0011,
      "step": 20450
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.1668357402086258,
      "learning_rate": 1.2111111111111112e-05,
      "loss": 0.0019,
      "step": 20460
    },
    {
      "epoch": 1.5162962962962963,
      "grad_norm": 0.11804564297199249,
      "learning_rate": 1.2092592592592593e-05,
      "loss": 0.001,
      "step": 20470
    },
    {
      "epoch": 1.5170370370370372,
      "grad_norm": 0.11271078139543533,
      "learning_rate": 1.2074074074074075e-05,
      "loss": 0.0023,
      "step": 20480
    },
    {
      "epoch": 1.517777777777778,
      "grad_norm": 0.08358969539403915,
      "learning_rate": 1.2055555555555556e-05,
      "loss": 0.0007,
      "step": 20490
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 0.04997718706727028,
      "learning_rate": 1.2037037037037037e-05,
      "loss": 0.0009,
      "step": 20500
    },
    {
      "epoch": 1.5192592592592593,
      "grad_norm": 0.15652398765087128,
      "learning_rate": 1.201851851851852e-05,
      "loss": 0.0012,
      "step": 20510
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.08245506137609482,
      "learning_rate": 1.2e-05,
      "loss": 0.001,
      "step": 20520
    },
    {
      "epoch": 1.5207407407407407,
      "grad_norm": 0.23253343999385834,
      "learning_rate": 1.1981481481481483e-05,
      "loss": 0.001,
      "step": 20530
    },
    {
      "epoch": 1.5214814814814814,
      "grad_norm": 0.14810942113399506,
      "learning_rate": 1.1962962962962964e-05,
      "loss": 0.0009,
      "step": 20540
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 0.09147893637418747,
      "learning_rate": 1.1944444444444446e-05,
      "loss": 0.0014,
      "step": 20550
    },
    {
      "epoch": 1.5229629629629629,
      "grad_norm": 0.05288063362240791,
      "learning_rate": 1.1925925925925927e-05,
      "loss": 0.0015,
      "step": 20560
    },
    {
      "epoch": 1.5237037037037036,
      "grad_norm": 0.0,
      "learning_rate": 1.1907407407407408e-05,
      "loss": 0.0008,
      "step": 20570
    },
    {
      "epoch": 1.5244444444444445,
      "grad_norm": 0.12952345609664917,
      "learning_rate": 1.188888888888889e-05,
      "loss": 0.0009,
      "step": 20580
    },
    {
      "epoch": 1.5251851851851852,
      "grad_norm": 0.0,
      "learning_rate": 1.1870370370370371e-05,
      "loss": 0.0005,
      "step": 20590
    },
    {
      "epoch": 1.525925925925926,
      "grad_norm": 0.0,
      "learning_rate": 1.1851851851851853e-05,
      "loss": 0.0014,
      "step": 20600
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.0001,
      "step": 20610
    },
    {
      "epoch": 1.5274074074074075,
      "grad_norm": 0.0,
      "learning_rate": 1.1814814814814817e-05,
      "loss": 0.001,
      "step": 20620
    },
    {
      "epoch": 1.5281481481481483,
      "grad_norm": 0.09967982023954391,
      "learning_rate": 1.1796296296296297e-05,
      "loss": 0.0016,
      "step": 20630
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 0.125758558511734,
      "learning_rate": 1.1777777777777778e-05,
      "loss": 0.0009,
      "step": 20640
    },
    {
      "epoch": 1.5296296296296297,
      "grad_norm": 0.0,
      "learning_rate": 1.175925925925926e-05,
      "loss": 0.001,
      "step": 20650
    },
    {
      "epoch": 1.5303703703703704,
      "grad_norm": 0.0,
      "learning_rate": 1.1740740740740741e-05,
      "loss": 0.0011,
      "step": 20660
    },
    {
      "epoch": 1.531111111111111,
      "grad_norm": 0.04257616400718689,
      "learning_rate": 1.1722222222222224e-05,
      "loss": 0.0011,
      "step": 20670
    },
    {
      "epoch": 1.5318518518518518,
      "grad_norm": 0.048860207200050354,
      "learning_rate": 1.1703703703703705e-05,
      "loss": 0.0013,
      "step": 20680
    },
    {
      "epoch": 1.5325925925925925,
      "grad_norm": 0.05240004137158394,
      "learning_rate": 1.1685185185185185e-05,
      "loss": 0.001,
      "step": 20690
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0006,
      "step": 20700
    },
    {
      "epoch": 1.534074074074074,
      "grad_norm": 0.04375419020652771,
      "learning_rate": 1.1648148148148149e-05,
      "loss": 0.0021,
      "step": 20710
    },
    {
      "epoch": 1.5348148148148149,
      "grad_norm": 0.046038053929805756,
      "learning_rate": 1.1629629629629631e-05,
      "loss": 0.0007,
      "step": 20720
    },
    {
      "epoch": 1.5355555555555556,
      "grad_norm": 0.0456722117960453,
      "learning_rate": 1.1611111111111112e-05,
      "loss": 0.0016,
      "step": 20730
    },
    {
      "epoch": 1.5362962962962963,
      "grad_norm": 0.05856766179203987,
      "learning_rate": 1.1592592592592594e-05,
      "loss": 0.0011,
      "step": 20740
    },
    {
      "epoch": 1.5370370370370372,
      "grad_norm": 0.09139736741781235,
      "learning_rate": 1.1574074074074075e-05,
      "loss": 0.0008,
      "step": 20750
    },
    {
      "epoch": 1.537777777777778,
      "grad_norm": 0.0,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 0.0015,
      "step": 20760
    },
    {
      "epoch": 1.5385185185185186,
      "grad_norm": 0.06304126232862473,
      "learning_rate": 1.1537037037037038e-05,
      "loss": 0.0015,
      "step": 20770
    },
    {
      "epoch": 1.5392592592592593,
      "grad_norm": 0.0794980451464653,
      "learning_rate": 1.151851851851852e-05,
      "loss": 0.0018,
      "step": 20780
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.0427665039896965,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0022,
      "step": 20790
    },
    {
      "epoch": 1.5407407407407407,
      "grad_norm": 0.04500491917133331,
      "learning_rate": 1.1481481481481482e-05,
      "loss": 0.0007,
      "step": 20800
    },
    {
      "epoch": 1.5414814814814815,
      "grad_norm": 0.06547103077173233,
      "learning_rate": 1.1462962962962963e-05,
      "loss": 0.0008,
      "step": 20810
    },
    {
      "epoch": 1.5422222222222222,
      "grad_norm": 0.05738021805882454,
      "learning_rate": 1.1444444444444446e-05,
      "loss": 0.001,
      "step": 20820
    },
    {
      "epoch": 1.5429629629629629,
      "grad_norm": 0.08430977165699005,
      "learning_rate": 1.1425925925925927e-05,
      "loss": 0.0016,
      "step": 20830
    },
    {
      "epoch": 1.5437037037037036,
      "grad_norm": 0.21372319757938385,
      "learning_rate": 1.1407407407407409e-05,
      "loss": 0.0009,
      "step": 20840
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 0.06037209555506706,
      "learning_rate": 1.138888888888889e-05,
      "loss": 0.0008,
      "step": 20850
    },
    {
      "epoch": 1.5451851851851852,
      "grad_norm": 0.06253952533006668,
      "learning_rate": 1.1370370370370372e-05,
      "loss": 0.0017,
      "step": 20860
    },
    {
      "epoch": 1.545925925925926,
      "grad_norm": 0.06511754542589188,
      "learning_rate": 1.1351851851851853e-05,
      "loss": 0.0011,
      "step": 20870
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.1092892736196518,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0012,
      "step": 20880
    },
    {
      "epoch": 1.5474074074074076,
      "grad_norm": 0.08746648579835892,
      "learning_rate": 1.1314814814814816e-05,
      "loss": 0.0009,
      "step": 20890
    },
    {
      "epoch": 1.5481481481481483,
      "grad_norm": 0.0,
      "learning_rate": 1.1296296296296297e-05,
      "loss": 0.0009,
      "step": 20900
    },
    {
      "epoch": 1.548888888888889,
      "grad_norm": 0.04285288602113724,
      "learning_rate": 1.127777777777778e-05,
      "loss": 0.0009,
      "step": 20910
    },
    {
      "epoch": 1.5496296296296297,
      "grad_norm": 0.0524434857070446,
      "learning_rate": 1.125925925925926e-05,
      "loss": 0.0022,
      "step": 20920
    },
    {
      "epoch": 1.5503703703703704,
      "grad_norm": 0.08401945233345032,
      "learning_rate": 1.1240740740740743e-05,
      "loss": 0.001,
      "step": 20930
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 0.15141580998897552,
      "learning_rate": 1.1222222222222224e-05,
      "loss": 0.0017,
      "step": 20940
    },
    {
      "epoch": 1.5518518518518518,
      "grad_norm": 0.1386970430612564,
      "learning_rate": 1.1203703703703704e-05,
      "loss": 0.0013,
      "step": 20950
    },
    {
      "epoch": 1.5525925925925925,
      "grad_norm": 0.047970373183488846,
      "learning_rate": 1.1185185185185187e-05,
      "loss": 0.0006,
      "step": 20960
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.10699213296175003,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0029,
      "step": 20970
    },
    {
      "epoch": 1.554074074074074,
      "grad_norm": 0.05867871642112732,
      "learning_rate": 1.1148148148148148e-05,
      "loss": 0.0013,
      "step": 20980
    },
    {
      "epoch": 1.5548148148148146,
      "grad_norm": 0.041976943612098694,
      "learning_rate": 1.1129629629629631e-05,
      "loss": 0.0018,
      "step": 20990
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.10075416415929794,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.0024,
      "step": 21000
    },
    {
      "epoch": 1.5562962962962963,
      "grad_norm": 0.0,
      "learning_rate": 1.1092592592592594e-05,
      "loss": 0.0009,
      "step": 21010
    },
    {
      "epoch": 1.557037037037037,
      "grad_norm": 0.08288435637950897,
      "learning_rate": 1.1074074074074075e-05,
      "loss": 0.0008,
      "step": 21020
    },
    {
      "epoch": 1.557777777777778,
      "grad_norm": 0.16138440370559692,
      "learning_rate": 1.1055555555555556e-05,
      "loss": 0.0017,
      "step": 21030
    },
    {
      "epoch": 1.5585185185185186,
      "grad_norm": 0.0662701278924942,
      "learning_rate": 1.1037037037037038e-05,
      "loss": 0.0006,
      "step": 21040
    },
    {
      "epoch": 1.5592592592592593,
      "grad_norm": 0.07426410913467407,
      "learning_rate": 1.1018518518518519e-05,
      "loss": 0.0013,
      "step": 21050
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.12766879796981812,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0021,
      "step": 21060
    },
    {
      "epoch": 1.5607407407407408,
      "grad_norm": 0.13132581114768982,
      "learning_rate": 1.0981481481481482e-05,
      "loss": 0.0011,
      "step": 21070
    },
    {
      "epoch": 1.5614814814814815,
      "grad_norm": 0.0,
      "learning_rate": 1.0962962962962963e-05,
      "loss": 0.0009,
      "step": 21080
    },
    {
      "epoch": 1.5622222222222222,
      "grad_norm": 0.09418029338121414,
      "learning_rate": 1.0944444444444445e-05,
      "loss": 0.0008,
      "step": 21090
    },
    {
      "epoch": 1.5629629629629629,
      "grad_norm": 0.11205683648586273,
      "learning_rate": 1.0925925925925926e-05,
      "loss": 0.002,
      "step": 21100
    },
    {
      "epoch": 1.5637037037037036,
      "grad_norm": 0.07741338759660721,
      "learning_rate": 1.0907407407407409e-05,
      "loss": 0.0025,
      "step": 21110
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 0.05576631799340248,
      "learning_rate": 1.088888888888889e-05,
      "loss": 0.0019,
      "step": 21120
    },
    {
      "epoch": 1.5651851851851852,
      "grad_norm": 0.1216885969042778,
      "learning_rate": 1.087037037037037e-05,
      "loss": 0.0012,
      "step": 21130
    },
    {
      "epoch": 1.565925925925926,
      "grad_norm": 0.09910248219966888,
      "learning_rate": 1.0851851851851853e-05,
      "loss": 0.0015,
      "step": 21140
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.04404325783252716,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0013,
      "step": 21150
    },
    {
      "epoch": 1.5674074074074074,
      "grad_norm": 0.0,
      "learning_rate": 1.0814814814814814e-05,
      "loss": 0.001,
      "step": 21160
    },
    {
      "epoch": 1.5681481481481483,
      "grad_norm": 0.0,
      "learning_rate": 1.0796296296296297e-05,
      "loss": 0.0019,
      "step": 21170
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.12180941551923752,
      "learning_rate": 1.0777777777777778e-05,
      "loss": 0.0012,
      "step": 21180
    },
    {
      "epoch": 1.5696296296296297,
      "grad_norm": 0.12152427434921265,
      "learning_rate": 1.075925925925926e-05,
      "loss": 0.0011,
      "step": 21190
    },
    {
      "epoch": 1.5703703703703704,
      "grad_norm": 0.1163906529545784,
      "learning_rate": 1.074074074074074e-05,
      "loss": 0.0012,
      "step": 21200
    },
    {
      "epoch": 1.5711111111111111,
      "grad_norm": 0.12631414830684662,
      "learning_rate": 1.0722222222222222e-05,
      "loss": 0.0022,
      "step": 21210
    },
    {
      "epoch": 1.5718518518518518,
      "grad_norm": 0.0492749847471714,
      "learning_rate": 1.0703703703703704e-05,
      "loss": 0.0024,
      "step": 21220
    },
    {
      "epoch": 1.5725925925925925,
      "grad_norm": 0.09951777011156082,
      "learning_rate": 1.0685185185185185e-05,
      "loss": 0.0007,
      "step": 21230
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.09932664036750793,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0014,
      "step": 21240
    },
    {
      "epoch": 1.574074074074074,
      "grad_norm": 0.07893475890159607,
      "learning_rate": 1.0648148148148148e-05,
      "loss": 0.0014,
      "step": 21250
    },
    {
      "epoch": 1.5748148148148147,
      "grad_norm": 0.11614866554737091,
      "learning_rate": 1.0629629629629629e-05,
      "loss": 0.0011,
      "step": 21260
    },
    {
      "epoch": 1.5755555555555556,
      "grad_norm": 0.22039973735809326,
      "learning_rate": 1.0611111111111111e-05,
      "loss": 0.0014,
      "step": 21270
    },
    {
      "epoch": 1.5762962962962963,
      "grad_norm": 0.0449812225997448,
      "learning_rate": 1.0592592592592592e-05,
      "loss": 0.0012,
      "step": 21280
    },
    {
      "epoch": 1.577037037037037,
      "grad_norm": 0.0656571090221405,
      "learning_rate": 1.0574074074074075e-05,
      "loss": 0.0019,
      "step": 21290
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.057761482894420624,
      "learning_rate": 1.0555555555555555e-05,
      "loss": 0.0014,
      "step": 21300
    },
    {
      "epoch": 1.5785185185185187,
      "grad_norm": 0.1006932258605957,
      "learning_rate": 1.0537037037037036e-05,
      "loss": 0.0015,
      "step": 21310
    },
    {
      "epoch": 1.5792592592592594,
      "grad_norm": 0.0,
      "learning_rate": 1.0518518518518519e-05,
      "loss": 0.0013,
      "step": 21320
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.04026830568909645,
      "learning_rate": 1.05e-05,
      "loss": 0.0014,
      "step": 21330
    },
    {
      "epoch": 1.5807407407407408,
      "grad_norm": 0.0793258473277092,
      "learning_rate": 1.0481481481481482e-05,
      "loss": 0.0012,
      "step": 21340
    },
    {
      "epoch": 1.5814814814814815,
      "grad_norm": 0.07497771829366684,
      "learning_rate": 1.0462962962962963e-05,
      "loss": 0.0016,
      "step": 21350
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 0.045215509831905365,
      "learning_rate": 1.0444444444444445e-05,
      "loss": 0.0012,
      "step": 21360
    },
    {
      "epoch": 1.582962962962963,
      "grad_norm": 0.08829410374164581,
      "learning_rate": 1.0425925925925926e-05,
      "loss": 0.0014,
      "step": 21370
    },
    {
      "epoch": 1.5837037037037036,
      "grad_norm": 0.09254183620214462,
      "learning_rate": 1.0407407407407407e-05,
      "loss": 0.0014,
      "step": 21380
    },
    {
      "epoch": 1.5844444444444443,
      "grad_norm": 0.11929535120725632,
      "learning_rate": 1.038888888888889e-05,
      "loss": 0.0019,
      "step": 21390
    },
    {
      "epoch": 1.585185185185185,
      "grad_norm": 0.12607580423355103,
      "learning_rate": 1.037037037037037e-05,
      "loss": 0.0021,
      "step": 21400
    },
    {
      "epoch": 1.585925925925926,
      "grad_norm": 0.05933162197470665,
      "learning_rate": 1.0351851851851852e-05,
      "loss": 0.001,
      "step": 21410
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.04675733298063278,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0012,
      "step": 21420
    },
    {
      "epoch": 1.5874074074074074,
      "grad_norm": 0.09052833914756775,
      "learning_rate": 1.0314814814814814e-05,
      "loss": 0.0013,
      "step": 21430
    },
    {
      "epoch": 1.5881481481481483,
      "grad_norm": 0.0,
      "learning_rate": 1.0296296296296296e-05,
      "loss": 0.0011,
      "step": 21440
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 0.0,
      "learning_rate": 1.0277777777777777e-05,
      "loss": 0.0013,
      "step": 21450
    },
    {
      "epoch": 1.5896296296296297,
      "grad_norm": 0.04648933559656143,
      "learning_rate": 1.025925925925926e-05,
      "loss": 0.0009,
      "step": 21460
    },
    {
      "epoch": 1.5903703703703704,
      "grad_norm": 0.056207988411188126,
      "learning_rate": 1.024074074074074e-05,
      "loss": 0.0015,
      "step": 21470
    },
    {
      "epoch": 1.5911111111111111,
      "grad_norm": 0.18645146489143372,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 0.0011,
      "step": 21480
    },
    {
      "epoch": 1.5918518518518519,
      "grad_norm": 0.08686599135398865,
      "learning_rate": 1.0203703703703704e-05,
      "loss": 0.0016,
      "step": 21490
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 0.3134721517562866,
      "learning_rate": 1.0185185185185185e-05,
      "loss": 0.0017,
      "step": 21500
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.116566002368927,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0012,
      "step": 21510
    },
    {
      "epoch": 1.594074074074074,
      "grad_norm": 0.0,
      "learning_rate": 1.0148148148148148e-05,
      "loss": 0.0013,
      "step": 21520
    },
    {
      "epoch": 1.5948148148148147,
      "grad_norm": 0.10265738517045975,
      "learning_rate": 1.012962962962963e-05,
      "loss": 0.0016,
      "step": 21530
    },
    {
      "epoch": 1.5955555555555554,
      "grad_norm": 0.1634574830532074,
      "learning_rate": 1.0111111111111111e-05,
      "loss": 0.0006,
      "step": 21540
    },
    {
      "epoch": 1.5962962962962963,
      "grad_norm": 0.053272806107997894,
      "learning_rate": 1.0092592592592594e-05,
      "loss": 0.0006,
      "step": 21550
    },
    {
      "epoch": 1.597037037037037,
      "grad_norm": 0.1272139847278595,
      "learning_rate": 1.0074074074074074e-05,
      "loss": 0.0018,
      "step": 21560
    },
    {
      "epoch": 1.5977777777777777,
      "grad_norm": 0.1135648712515831,
      "learning_rate": 1.0055555555555555e-05,
      "loss": 0.0011,
      "step": 21570
    },
    {
      "epoch": 1.5985185185185187,
      "grad_norm": 0.069272980093956,
      "learning_rate": 1.0037037037037038e-05,
      "loss": 0.0014,
      "step": 21580
    },
    {
      "epoch": 1.5992592592592594,
      "grad_norm": 0.07494017481803894,
      "learning_rate": 1.0018518518518518e-05,
      "loss": 0.0013,
      "step": 21590
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.06911138445138931,
      "learning_rate": 1e-05,
      "loss": 0.001,
      "step": 21600
    },
    {
      "epoch": 1.6007407407407408,
      "grad_norm": 0.06952281296253204,
      "learning_rate": 9.981481481481482e-06,
      "loss": 0.0014,
      "step": 21610
    },
    {
      "epoch": 1.6014814814814815,
      "grad_norm": 0.05239543318748474,
      "learning_rate": 9.962962962962962e-06,
      "loss": 0.0017,
      "step": 21620
    },
    {
      "epoch": 1.6022222222222222,
      "grad_norm": 0.08956366032361984,
      "learning_rate": 9.944444444444445e-06,
      "loss": 0.0004,
      "step": 21630
    },
    {
      "epoch": 1.602962962962963,
      "grad_norm": 0.19584394991397858,
      "learning_rate": 9.925925925925926e-06,
      "loss": 0.0023,
      "step": 21640
    },
    {
      "epoch": 1.6037037037037036,
      "grad_norm": 0.07424544543027878,
      "learning_rate": 9.907407407407408e-06,
      "loss": 0.0019,
      "step": 21650
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.1316836029291153,
      "learning_rate": 9.888888888888889e-06,
      "loss": 0.0016,
      "step": 21660
    },
    {
      "epoch": 1.605185185185185,
      "grad_norm": 0.1626313477754593,
      "learning_rate": 9.870370370370371e-06,
      "loss": 0.0013,
      "step": 21670
    },
    {
      "epoch": 1.605925925925926,
      "grad_norm": 0.11512476205825806,
      "learning_rate": 9.851851851851852e-06,
      "loss": 0.0011,
      "step": 21680
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.1566489189863205,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0009,
      "step": 21690
    },
    {
      "epoch": 1.6074074074074074,
      "grad_norm": 0.13470353186130524,
      "learning_rate": 9.814814814814815e-06,
      "loss": 0.0008,
      "step": 21700
    },
    {
      "epoch": 1.608148148148148,
      "grad_norm": 0.0,
      "learning_rate": 9.796296296296296e-06,
      "loss": 0.0009,
      "step": 21710
    },
    {
      "epoch": 1.608888888888889,
      "grad_norm": 0.14683686196804047,
      "learning_rate": 9.777777777777779e-06,
      "loss": 0.001,
      "step": 21720
    },
    {
      "epoch": 1.6096296296296297,
      "grad_norm": 0.17743507027626038,
      "learning_rate": 9.75925925925926e-06,
      "loss": 0.0018,
      "step": 21730
    },
    {
      "epoch": 1.6103703703703705,
      "grad_norm": 0.07939939200878143,
      "learning_rate": 9.74074074074074e-06,
      "loss": 0.0008,
      "step": 21740
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.10783001035451889,
      "learning_rate": 9.722222222222223e-06,
      "loss": 0.0022,
      "step": 21750
    },
    {
      "epoch": 1.6118518518518519,
      "grad_norm": 0.04417958855628967,
      "learning_rate": 9.703703703703703e-06,
      "loss": 0.0007,
      "step": 21760
    },
    {
      "epoch": 1.6125925925925926,
      "grad_norm": 0.14581182599067688,
      "learning_rate": 9.685185185185186e-06,
      "loss": 0.0012,
      "step": 21770
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.05551578477025032,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0009,
      "step": 21780
    },
    {
      "epoch": 1.614074074074074,
      "grad_norm": 0.10129551589488983,
      "learning_rate": 9.64814814814815e-06,
      "loss": 0.0009,
      "step": 21790
    },
    {
      "epoch": 1.6148148148148147,
      "grad_norm": 0.08601221442222595,
      "learning_rate": 9.62962962962963e-06,
      "loss": 0.0008,
      "step": 21800
    },
    {
      "epoch": 1.6155555555555554,
      "grad_norm": 0.08970022946596146,
      "learning_rate": 9.61111111111111e-06,
      "loss": 0.0005,
      "step": 21810
    },
    {
      "epoch": 1.6162962962962963,
      "grad_norm": 0.04569103196263313,
      "learning_rate": 9.592592592592593e-06,
      "loss": 0.0009,
      "step": 21820
    },
    {
      "epoch": 1.617037037037037,
      "grad_norm": 0.13047310709953308,
      "learning_rate": 9.574074074074074e-06,
      "loss": 0.0008,
      "step": 21830
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 0.15537701547145844,
      "learning_rate": 9.555555555555556e-06,
      "loss": 0.0012,
      "step": 21840
    },
    {
      "epoch": 1.6185185185185185,
      "grad_norm": 0.042240116745233536,
      "learning_rate": 9.537037037037037e-06,
      "loss": 0.0009,
      "step": 21850
    },
    {
      "epoch": 1.6192592592592594,
      "grad_norm": 0.0842025950551033,
      "learning_rate": 9.51851851851852e-06,
      "loss": 0.0009,
      "step": 21860
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.07857895642518997,
      "learning_rate": 9.5e-06,
      "loss": 0.0007,
      "step": 21870
    },
    {
      "epoch": 1.6207407407407408,
      "grad_norm": 0.09730768203735352,
      "learning_rate": 9.481481481481481e-06,
      "loss": 0.0024,
      "step": 21880
    },
    {
      "epoch": 1.6214814814814815,
      "grad_norm": 0.07751720398664474,
      "learning_rate": 9.462962962962964e-06,
      "loss": 0.0015,
      "step": 21890
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.0,
      "learning_rate": 9.444444444444445e-06,
      "loss": 0.0013,
      "step": 21900
    },
    {
      "epoch": 1.622962962962963,
      "grad_norm": 0.16520607471466064,
      "learning_rate": 9.425925925925927e-06,
      "loss": 0.0015,
      "step": 21910
    },
    {
      "epoch": 1.6237037037037036,
      "grad_norm": 0.05020654946565628,
      "learning_rate": 9.407407407407408e-06,
      "loss": 0.0016,
      "step": 21920
    },
    {
      "epoch": 1.6244444444444444,
      "grad_norm": 0.0,
      "learning_rate": 9.388888888888889e-06,
      "loss": 0.0006,
      "step": 21930
    },
    {
      "epoch": 1.625185185185185,
      "grad_norm": 0.07397418469190598,
      "learning_rate": 9.370370370370371e-06,
      "loss": 0.0009,
      "step": 21940
    },
    {
      "epoch": 1.6259259259259258,
      "grad_norm": 0.046545907855033875,
      "learning_rate": 9.351851851851852e-06,
      "loss": 0.0025,
      "step": 21950
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.04672807455062866,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0009,
      "step": 21960
    },
    {
      "epoch": 1.6274074074074074,
      "grad_norm": 0.04879845678806305,
      "learning_rate": 9.314814814814815e-06,
      "loss": 0.0007,
      "step": 21970
    },
    {
      "epoch": 1.6281481481481481,
      "grad_norm": 0.0,
      "learning_rate": 9.296296296296298e-06,
      "loss": 0.0007,
      "step": 21980
    },
    {
      "epoch": 1.628888888888889,
      "grad_norm": 0.103753961622715,
      "learning_rate": 9.277777777777778e-06,
      "loss": 0.0015,
      "step": 21990
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 0.0,
      "learning_rate": 9.259259259259259e-06,
      "loss": 0.0017,
      "step": 22000
    },
    {
      "epoch": 1.6303703703703705,
      "grad_norm": 0.0,
      "learning_rate": 9.240740740740742e-06,
      "loss": 0.001,
      "step": 22010
    },
    {
      "epoch": 1.6311111111111112,
      "grad_norm": 0.0,
      "learning_rate": 9.222222222222222e-06,
      "loss": 0.0016,
      "step": 22020
    },
    {
      "epoch": 1.6318518518518519,
      "grad_norm": 0.10438837856054306,
      "learning_rate": 9.203703703703705e-06,
      "loss": 0.001,
      "step": 22030
    },
    {
      "epoch": 1.6325925925925926,
      "grad_norm": 0.04347098618745804,
      "learning_rate": 9.185185185185186e-06,
      "loss": 0.0019,
      "step": 22040
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.16351649165153503,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0008,
      "step": 22050
    },
    {
      "epoch": 1.634074074074074,
      "grad_norm": 0.07007140666246414,
      "learning_rate": 9.148148148148149e-06,
      "loss": 0.0016,
      "step": 22060
    },
    {
      "epoch": 1.6348148148148147,
      "grad_norm": 0.07094931602478027,
      "learning_rate": 9.12962962962963e-06,
      "loss": 0.0012,
      "step": 22070
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 0.0,
      "learning_rate": 9.111111111111112e-06,
      "loss": 0.0016,
      "step": 22080
    },
    {
      "epoch": 1.6362962962962961,
      "grad_norm": 0.07551190257072449,
      "learning_rate": 9.092592592592593e-06,
      "loss": 0.0015,
      "step": 22090
    },
    {
      "epoch": 1.637037037037037,
      "grad_norm": 0.1888941526412964,
      "learning_rate": 9.074074074074075e-06,
      "loss": 0.0016,
      "step": 22100
    },
    {
      "epoch": 1.6377777777777778,
      "grad_norm": 0.04707299917936325,
      "learning_rate": 9.055555555555556e-06,
      "loss": 0.0012,
      "step": 22110
    },
    {
      "epoch": 1.6385185185185185,
      "grad_norm": 0.12234795838594437,
      "learning_rate": 9.037037037037037e-06,
      "loss": 0.0012,
      "step": 22120
    },
    {
      "epoch": 1.6392592592592594,
      "grad_norm": 0.10997084528207779,
      "learning_rate": 9.01851851851852e-06,
      "loss": 0.0009,
      "step": 22130
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.0,
      "learning_rate": 9e-06,
      "loss": 0.0009,
      "step": 22140
    },
    {
      "epoch": 1.6407407407407408,
      "grad_norm": 0.08853926509618759,
      "learning_rate": 8.981481481481483e-06,
      "loss": 0.0018,
      "step": 22150
    },
    {
      "epoch": 1.6414814814814815,
      "grad_norm": 0.19292932748794556,
      "learning_rate": 8.962962962962963e-06,
      "loss": 0.0012,
      "step": 22160
    },
    {
      "epoch": 1.6422222222222222,
      "grad_norm": 0.07069841027259827,
      "learning_rate": 8.944444444444444e-06,
      "loss": 0.0018,
      "step": 22170
    },
    {
      "epoch": 1.642962962962963,
      "grad_norm": 0.0,
      "learning_rate": 8.925925925925927e-06,
      "loss": 0.0013,
      "step": 22180
    },
    {
      "epoch": 1.6437037037037037,
      "grad_norm": 0.10211560130119324,
      "learning_rate": 8.907407407407408e-06,
      "loss": 0.0009,
      "step": 22190
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 0.08696097880601883,
      "learning_rate": 8.88888888888889e-06,
      "loss": 0.0009,
      "step": 22200
    },
    {
      "epoch": 1.645185185185185,
      "grad_norm": 0.0,
      "learning_rate": 8.87037037037037e-06,
      "loss": 0.0018,
      "step": 22210
    },
    {
      "epoch": 1.6459259259259258,
      "grad_norm": 0.1595098376274109,
      "learning_rate": 8.851851851851853e-06,
      "loss": 0.0016,
      "step": 22220
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.10986772179603577,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0012,
      "step": 22230
    },
    {
      "epoch": 1.6474074074074074,
      "grad_norm": 0.09611362963914871,
      "learning_rate": 8.814814814814815e-06,
      "loss": 0.0011,
      "step": 22240
    },
    {
      "epoch": 1.6481481481481481,
      "grad_norm": 0.0,
      "learning_rate": 8.796296296296297e-06,
      "loss": 0.0005,
      "step": 22250
    },
    {
      "epoch": 1.6488888888888888,
      "grad_norm": 0.04815579578280449,
      "learning_rate": 8.777777777777778e-06,
      "loss": 0.0006,
      "step": 22260
    },
    {
      "epoch": 1.6496296296296298,
      "grad_norm": 0.14163433015346527,
      "learning_rate": 8.75925925925926e-06,
      "loss": 0.0016,
      "step": 22270
    },
    {
      "epoch": 1.6503703703703705,
      "grad_norm": 0.0,
      "learning_rate": 8.740740740740741e-06,
      "loss": 0.001,
      "step": 22280
    },
    {
      "epoch": 1.6511111111111112,
      "grad_norm": 0.0,
      "learning_rate": 8.722222222222224e-06,
      "loss": 0.001,
      "step": 22290
    },
    {
      "epoch": 1.651851851851852,
      "grad_norm": 0.08971866220235825,
      "learning_rate": 8.703703703703705e-06,
      "loss": 0.0009,
      "step": 22300
    },
    {
      "epoch": 1.6525925925925926,
      "grad_norm": 0.0,
      "learning_rate": 8.685185185185185e-06,
      "loss": 0.001,
      "step": 22310
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.09421171993017197,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0012,
      "step": 22320
    },
    {
      "epoch": 1.654074074074074,
      "grad_norm": 0.15450267493724823,
      "learning_rate": 8.648148148148149e-06,
      "loss": 0.001,
      "step": 22330
    },
    {
      "epoch": 1.6548148148148147,
      "grad_norm": 0.04438280686736107,
      "learning_rate": 8.629629629629631e-06,
      "loss": 0.001,
      "step": 22340
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 0.10606700927019119,
      "learning_rate": 8.611111111111112e-06,
      "loss": 0.0008,
      "step": 22350
    },
    {
      "epoch": 1.6562962962962962,
      "grad_norm": 0.09793069213628769,
      "learning_rate": 8.592592592592593e-06,
      "loss": 0.0021,
      "step": 22360
    },
    {
      "epoch": 1.657037037037037,
      "grad_norm": 0.04531789943575859,
      "learning_rate": 8.574074074074075e-06,
      "loss": 0.0018,
      "step": 22370
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 0.079364113509655,
      "learning_rate": 8.555555555555556e-06,
      "loss": 0.0015,
      "step": 22380
    },
    {
      "epoch": 1.6585185185185185,
      "grad_norm": 0.1615656018257141,
      "learning_rate": 8.537037037037038e-06,
      "loss": 0.0012,
      "step": 22390
    },
    {
      "epoch": 1.6592592592592592,
      "grad_norm": 0.07236022502183914,
      "learning_rate": 8.518518518518519e-06,
      "loss": 0.0006,
      "step": 22400
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.0,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0007,
      "step": 22410
    },
    {
      "epoch": 1.6607407407407409,
      "grad_norm": 0.18209022283554077,
      "learning_rate": 8.481481481481482e-06,
      "loss": 0.0003,
      "step": 22420
    },
    {
      "epoch": 1.6614814814814816,
      "grad_norm": 0.06795334815979004,
      "learning_rate": 8.462962962962963e-06,
      "loss": 0.0011,
      "step": 22430
    },
    {
      "epoch": 1.6622222222222223,
      "grad_norm": 0.04322965070605278,
      "learning_rate": 8.444444444444446e-06,
      "loss": 0.0011,
      "step": 22440
    },
    {
      "epoch": 1.662962962962963,
      "grad_norm": 0.0,
      "learning_rate": 8.425925925925926e-06,
      "loss": 0.0013,
      "step": 22450
    },
    {
      "epoch": 1.6637037037037037,
      "grad_norm": 0.042632315307855606,
      "learning_rate": 8.407407407407409e-06,
      "loss": 0.0017,
      "step": 22460
    },
    {
      "epoch": 1.6644444444444444,
      "grad_norm": 0.0464504212141037,
      "learning_rate": 8.38888888888889e-06,
      "loss": 0.0012,
      "step": 22470
    },
    {
      "epoch": 1.665185185185185,
      "grad_norm": 0.2394070327281952,
      "learning_rate": 8.37037037037037e-06,
      "loss": 0.0006,
      "step": 22480
    },
    {
      "epoch": 1.6659259259259258,
      "grad_norm": 0.05037783831357956,
      "learning_rate": 8.351851851851853e-06,
      "loss": 0.0018,
      "step": 22490
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.11396795511245728,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0018,
      "step": 22500
    },
    {
      "epoch": 1.6674074074074074,
      "grad_norm": 0.08893175423145294,
      "learning_rate": 8.314814814814816e-06,
      "loss": 0.0009,
      "step": 22510
    },
    {
      "epoch": 1.6681481481481482,
      "grad_norm": 0.04387214034795761,
      "learning_rate": 8.296296296296297e-06,
      "loss": 0.0006,
      "step": 22520
    },
    {
      "epoch": 1.6688888888888889,
      "grad_norm": 0.08587684482336044,
      "learning_rate": 8.27777777777778e-06,
      "loss": 0.0017,
      "step": 22530
    },
    {
      "epoch": 1.6696296296296296,
      "grad_norm": 0.132425919175148,
      "learning_rate": 8.25925925925926e-06,
      "loss": 0.0007,
      "step": 22540
    },
    {
      "epoch": 1.6703703703703705,
      "grad_norm": 0.1276978999376297,
      "learning_rate": 8.240740740740741e-06,
      "loss": 0.0011,
      "step": 22550
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 0.07528135180473328,
      "learning_rate": 8.222222222222223e-06,
      "loss": 0.0008,
      "step": 22560
    },
    {
      "epoch": 1.671851851851852,
      "grad_norm": 0.0,
      "learning_rate": 8.203703703703704e-06,
      "loss": 0.0019,
      "step": 22570
    },
    {
      "epoch": 1.6725925925925926,
      "grad_norm": 0.06642968952655792,
      "learning_rate": 8.185185185185187e-06,
      "loss": 0.002,
      "step": 22580
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.04483499750494957,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.0011,
      "step": 22590
    },
    {
      "epoch": 1.674074074074074,
      "grad_norm": 0.1563762128353119,
      "learning_rate": 8.14814814814815e-06,
      "loss": 0.0013,
      "step": 22600
    },
    {
      "epoch": 1.6748148148148148,
      "grad_norm": 0.07401609420776367,
      "learning_rate": 8.12962962962963e-06,
      "loss": 0.0008,
      "step": 22610
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 0.1546306610107422,
      "learning_rate": 8.111111111111112e-06,
      "loss": 0.0013,
      "step": 22620
    },
    {
      "epoch": 1.6762962962962962,
      "grad_norm": 0.013818986713886261,
      "learning_rate": 8.092592592592594e-06,
      "loss": 0.001,
      "step": 22630
    },
    {
      "epoch": 1.6770370370370369,
      "grad_norm": 0.0743672251701355,
      "learning_rate": 8.074074074074075e-06,
      "loss": 0.0019,
      "step": 22640
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 0.20780017971992493,
      "learning_rate": 8.055555555555557e-06,
      "loss": 0.0015,
      "step": 22650
    },
    {
      "epoch": 1.6785185185185185,
      "grad_norm": 0.12619051337242126,
      "learning_rate": 8.037037037037038e-06,
      "loss": 0.0016,
      "step": 22660
    },
    {
      "epoch": 1.6792592592592592,
      "grad_norm": 0.2306428849697113,
      "learning_rate": 8.018518518518519e-06,
      "loss": 0.0019,
      "step": 22670
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.11183439195156097,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0008,
      "step": 22680
    },
    {
      "epoch": 1.6807407407407409,
      "grad_norm": 0.12611024081707,
      "learning_rate": 7.981481481481482e-06,
      "loss": 0.0013,
      "step": 22690
    },
    {
      "epoch": 1.6814814814814816,
      "grad_norm": 0.14530639350414276,
      "learning_rate": 7.962962962962963e-06,
      "loss": 0.0011,
      "step": 22700
    },
    {
      "epoch": 1.6822222222222223,
      "grad_norm": 0.04406861215829849,
      "learning_rate": 7.944444444444445e-06,
      "loss": 0.0015,
      "step": 22710
    },
    {
      "epoch": 1.682962962962963,
      "grad_norm": 0.11177457869052887,
      "learning_rate": 7.925925925925926e-06,
      "loss": 0.0011,
      "step": 22720
    },
    {
      "epoch": 1.6837037037037037,
      "grad_norm": 0.1661148965358734,
      "learning_rate": 7.907407407407409e-06,
      "loss": 0.0017,
      "step": 22730
    },
    {
      "epoch": 1.6844444444444444,
      "grad_norm": 0.1309625655412674,
      "learning_rate": 7.88888888888889e-06,
      "loss": 0.0023,
      "step": 22740
    },
    {
      "epoch": 1.6851851851851851,
      "grad_norm": 0.10519416630268097,
      "learning_rate": 7.87037037037037e-06,
      "loss": 0.0011,
      "step": 22750
    },
    {
      "epoch": 1.6859259259259258,
      "grad_norm": 0.16258451342582703,
      "learning_rate": 7.851851851851853e-06,
      "loss": 0.0012,
      "step": 22760
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.043087027966976166,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.001,
      "step": 22770
    },
    {
      "epoch": 1.6874074074074072,
      "grad_norm": 0.05543047562241554,
      "learning_rate": 7.814814814814816e-06,
      "loss": 0.0012,
      "step": 22780
    },
    {
      "epoch": 1.6881481481481482,
      "grad_norm": 0.044900570064783096,
      "learning_rate": 7.796296296296297e-06,
      "loss": 0.0009,
      "step": 22790
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.0943032056093216,
      "learning_rate": 7.777777777777777e-06,
      "loss": 0.0012,
      "step": 22800
    },
    {
      "epoch": 1.6896296296296296,
      "grad_norm": 0.1925588846206665,
      "learning_rate": 7.75925925925926e-06,
      "loss": 0.0012,
      "step": 22810
    },
    {
      "epoch": 1.6903703703703705,
      "grad_norm": 0.047091640532016754,
      "learning_rate": 7.74074074074074e-06,
      "loss": 0.0014,
      "step": 22820
    },
    {
      "epoch": 1.6911111111111112,
      "grad_norm": 0.09039562940597534,
      "learning_rate": 7.722222222222223e-06,
      "loss": 0.002,
      "step": 22830
    },
    {
      "epoch": 1.691851851851852,
      "grad_norm": 0.0675349161028862,
      "learning_rate": 7.703703703703704e-06,
      "loss": 0.0015,
      "step": 22840
    },
    {
      "epoch": 1.6925925925925926,
      "grad_norm": 0.0441201850771904,
      "learning_rate": 7.685185185185185e-06,
      "loss": 0.001,
      "step": 22850
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.07138955593109131,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0016,
      "step": 22860
    },
    {
      "epoch": 1.694074074074074,
      "grad_norm": 0.07318991422653198,
      "learning_rate": 7.648148148148148e-06,
      "loss": 0.0018,
      "step": 22870
    },
    {
      "epoch": 1.6948148148148148,
      "grad_norm": 0.05511747673153877,
      "learning_rate": 7.629629629629629e-06,
      "loss": 0.0012,
      "step": 22880
    },
    {
      "epoch": 1.6955555555555555,
      "grad_norm": 0.0,
      "learning_rate": 7.611111111111112e-06,
      "loss": 0.0011,
      "step": 22890
    },
    {
      "epoch": 1.6962962962962962,
      "grad_norm": 0.06145443394780159,
      "learning_rate": 7.592592592592593e-06,
      "loss": 0.0013,
      "step": 22900
    },
    {
      "epoch": 1.697037037037037,
      "grad_norm": 0.09594661742448807,
      "learning_rate": 7.574074074074075e-06,
      "loss": 0.0012,
      "step": 22910
    },
    {
      "epoch": 1.6977777777777778,
      "grad_norm": 0.07694467157125473,
      "learning_rate": 7.555555555555556e-06,
      "loss": 0.0008,
      "step": 22920
    },
    {
      "epoch": 1.6985185185185185,
      "grad_norm": 0.02861272543668747,
      "learning_rate": 7.537037037037037e-06,
      "loss": 0.001,
      "step": 22930
    },
    {
      "epoch": 1.6992592592592592,
      "grad_norm": 0.09558099508285522,
      "learning_rate": 7.518518518518519e-06,
      "loss": 0.0009,
      "step": 22940
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.2733880281448364,
      "learning_rate": 7.5e-06,
      "loss": 0.0016,
      "step": 22950
    },
    {
      "epoch": 1.7007407407407409,
      "grad_norm": 0.10474971681833267,
      "learning_rate": 7.481481481481483e-06,
      "loss": 0.001,
      "step": 22960
    },
    {
      "epoch": 1.7014814814814816,
      "grad_norm": 0.10999245941638947,
      "learning_rate": 7.4629629629629634e-06,
      "loss": 0.0011,
      "step": 22970
    },
    {
      "epoch": 1.7022222222222223,
      "grad_norm": 0.16353431344032288,
      "learning_rate": 7.444444444444444e-06,
      "loss": 0.0012,
      "step": 22980
    },
    {
      "epoch": 1.702962962962963,
      "grad_norm": 0.09457355737686157,
      "learning_rate": 7.425925925925927e-06,
      "loss": 0.0008,
      "step": 22990
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 0.0,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.002,
      "step": 23000
    },
    {
      "epoch": 1.7044444444444444,
      "grad_norm": 0.10030962526798248,
      "learning_rate": 7.38888888888889e-06,
      "loss": 0.0011,
      "step": 23010
    },
    {
      "epoch": 1.7051851851851851,
      "grad_norm": 0.0,
      "learning_rate": 7.370370370370371e-06,
      "loss": 0.0013,
      "step": 23020
    },
    {
      "epoch": 1.7059259259259258,
      "grad_norm": 0.0,
      "learning_rate": 7.351851851851853e-06,
      "loss": 0.0007,
      "step": 23030
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.05871778726577759,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0018,
      "step": 23040
    },
    {
      "epoch": 1.7074074074074073,
      "grad_norm": 0.10382445901632309,
      "learning_rate": 7.314814814814815e-06,
      "loss": 0.001,
      "step": 23050
    },
    {
      "epoch": 1.7081481481481482,
      "grad_norm": 0.23636820912361145,
      "learning_rate": 7.296296296296297e-06,
      "loss": 0.0015,
      "step": 23060
    },
    {
      "epoch": 1.708888888888889,
      "grad_norm": 0.0,
      "learning_rate": 7.277777777777778e-06,
      "loss": 0.0009,
      "step": 23070
    },
    {
      "epoch": 1.7096296296296296,
      "grad_norm": 0.0,
      "learning_rate": 7.2592592592592605e-06,
      "loss": 0.0007,
      "step": 23080
    },
    {
      "epoch": 1.7103703703703703,
      "grad_norm": 0.0,
      "learning_rate": 7.240740740740741e-06,
      "loss": 0.0014,
      "step": 23090
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.09009073674678802,
      "learning_rate": 7.222222222222222e-06,
      "loss": 0.0008,
      "step": 23100
    },
    {
      "epoch": 1.711851851851852,
      "grad_norm": 0.04050937667489052,
      "learning_rate": 7.2037037037037045e-06,
      "loss": 0.0015,
      "step": 23110
    },
    {
      "epoch": 1.7125925925925927,
      "grad_norm": 0.06567263603210449,
      "learning_rate": 7.185185185185185e-06,
      "loss": 0.0008,
      "step": 23120
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.06652399152517319,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0018,
      "step": 23130
    },
    {
      "epoch": 1.714074074074074,
      "grad_norm": 0.06000391021370888,
      "learning_rate": 7.1481481481481486e-06,
      "loss": 0.0008,
      "step": 23140
    },
    {
      "epoch": 1.7148148148148148,
      "grad_norm": 0.11373435705900192,
      "learning_rate": 7.12962962962963e-06,
      "loss": 0.0004,
      "step": 23150
    },
    {
      "epoch": 1.7155555555555555,
      "grad_norm": 0.088097482919693,
      "learning_rate": 7.111111111111112e-06,
      "loss": 0.0014,
      "step": 23160
    },
    {
      "epoch": 1.7162962962962962,
      "grad_norm": 0.0,
      "learning_rate": 7.092592592592593e-06,
      "loss": 0.0007,
      "step": 23170
    },
    {
      "epoch": 1.717037037037037,
      "grad_norm": 0.05975670367479324,
      "learning_rate": 7.074074074074074e-06,
      "loss": 0.0012,
      "step": 23180
    },
    {
      "epoch": 1.7177777777777776,
      "grad_norm": 0.04303112253546715,
      "learning_rate": 7.055555555555556e-06,
      "loss": 0.0017,
      "step": 23190
    },
    {
      "epoch": 1.7185185185185186,
      "grad_norm": 0.026740392670035362,
      "learning_rate": 7.0370370370370375e-06,
      "loss": 0.0012,
      "step": 23200
    },
    {
      "epoch": 1.7192592592592593,
      "grad_norm": 0.0,
      "learning_rate": 7.018518518518519e-06,
      "loss": 0.0018,
      "step": 23210
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.0,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.001,
      "step": 23220
    },
    {
      "epoch": 1.720740740740741,
      "grad_norm": 0.0,
      "learning_rate": 6.9814814814814815e-06,
      "loss": 0.0014,
      "step": 23230
    },
    {
      "epoch": 1.7214814814814816,
      "grad_norm": 0.05076940730214119,
      "learning_rate": 6.962962962962963e-06,
      "loss": 0.0024,
      "step": 23240
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.05802050977945328,
      "learning_rate": 6.944444444444445e-06,
      "loss": 0.001,
      "step": 23250
    },
    {
      "epoch": 1.722962962962963,
      "grad_norm": 0.0410752147436142,
      "learning_rate": 6.925925925925926e-06,
      "loss": 0.0018,
      "step": 23260
    },
    {
      "epoch": 1.7237037037037037,
      "grad_norm": 0.09088151901960373,
      "learning_rate": 6.907407407407408e-06,
      "loss": 0.0013,
      "step": 23270
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 0.0,
      "learning_rate": 6.888888888888889e-06,
      "loss": 0.0017,
      "step": 23280
    },
    {
      "epoch": 1.7251851851851852,
      "grad_norm": 0.0,
      "learning_rate": 6.8703703703703704e-06,
      "loss": 0.0011,
      "step": 23290
    },
    {
      "epoch": 1.7259259259259259,
      "grad_norm": 0.0,
      "learning_rate": 6.851851851851852e-06,
      "loss": 0.0016,
      "step": 23300
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.045988794416189194,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0018,
      "step": 23310
    },
    {
      "epoch": 1.7274074074074073,
      "grad_norm": 0.05158603936433792,
      "learning_rate": 6.814814814814815e-06,
      "loss": 0.0012,
      "step": 23320
    },
    {
      "epoch": 1.728148148148148,
      "grad_norm": 0.03127949684858322,
      "learning_rate": 6.796296296296296e-06,
      "loss": 0.0008,
      "step": 23330
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 0.05222175270318985,
      "learning_rate": 6.777777777777779e-06,
      "loss": 0.0018,
      "step": 23340
    },
    {
      "epoch": 1.7296296296296296,
      "grad_norm": 0.11721564829349518,
      "learning_rate": 6.759259259259259e-06,
      "loss": 0.0013,
      "step": 23350
    },
    {
      "epoch": 1.7303703703703703,
      "grad_norm": 0.0,
      "learning_rate": 6.74074074074074e-06,
      "loss": 0.0011,
      "step": 23360
    },
    {
      "epoch": 1.7311111111111113,
      "grad_norm": 0.12070375680923462,
      "learning_rate": 6.722222222222223e-06,
      "loss": 0.0017,
      "step": 23370
    },
    {
      "epoch": 1.731851851851852,
      "grad_norm": 0.11160627752542496,
      "learning_rate": 6.703703703703703e-06,
      "loss": 0.0012,
      "step": 23380
    },
    {
      "epoch": 1.7325925925925927,
      "grad_norm": 0.0,
      "learning_rate": 6.685185185185186e-06,
      "loss": 0.0008,
      "step": 23390
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.11803386360406876,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0022,
      "step": 23400
    },
    {
      "epoch": 1.734074074074074,
      "grad_norm": 0.05086436867713928,
      "learning_rate": 6.6481481481481474e-06,
      "loss": 0.0017,
      "step": 23410
    },
    {
      "epoch": 1.7348148148148148,
      "grad_norm": 0.053716711699962616,
      "learning_rate": 6.62962962962963e-06,
      "loss": 0.0013,
      "step": 23420
    },
    {
      "epoch": 1.7355555555555555,
      "grad_norm": 0.08245925605297089,
      "learning_rate": 6.611111111111111e-06,
      "loss": 0.0014,
      "step": 23430
    },
    {
      "epoch": 1.7362962962962962,
      "grad_norm": 0.09012462198734283,
      "learning_rate": 6.592592592592593e-06,
      "loss": 0.0009,
      "step": 23440
    },
    {
      "epoch": 1.737037037037037,
      "grad_norm": 0.05664408206939697,
      "learning_rate": 6.574074074074074e-06,
      "loss": 0.0015,
      "step": 23450
    },
    {
      "epoch": 1.7377777777777776,
      "grad_norm": 0.0,
      "learning_rate": 6.555555555555556e-06,
      "loss": 0.0011,
      "step": 23460
    },
    {
      "epoch": 1.7385185185185184,
      "grad_norm": 0.05832920968532562,
      "learning_rate": 6.537037037037037e-06,
      "loss": 0.0014,
      "step": 23470
    },
    {
      "epoch": 1.7392592592592593,
      "grad_norm": 0.09637580811977386,
      "learning_rate": 6.518518518518518e-06,
      "loss": 0.0013,
      "step": 23480
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.05565454065799713,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0024,
      "step": 23490
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 0.08081880956888199,
      "learning_rate": 6.481481481481481e-06,
      "loss": 0.0014,
      "step": 23500
    },
    {
      "epoch": 1.7414814814814816,
      "grad_norm": 0.0,
      "learning_rate": 6.462962962962964e-06,
      "loss": 0.0012,
      "step": 23510
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 0.04459008574485779,
      "learning_rate": 6.4444444444444445e-06,
      "loss": 0.001,
      "step": 23520
    },
    {
      "epoch": 1.742962962962963,
      "grad_norm": 0.04990185424685478,
      "learning_rate": 6.425925925925927e-06,
      "loss": 0.0019,
      "step": 23530
    },
    {
      "epoch": 1.7437037037037038,
      "grad_norm": 0.056288257241249084,
      "learning_rate": 6.407407407407408e-06,
      "loss": 0.0011,
      "step": 23540
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 0.09657211601734161,
      "learning_rate": 6.3888888888888885e-06,
      "loss": 0.0023,
      "step": 23550
    },
    {
      "epoch": 1.7451851851851852,
      "grad_norm": 0.0,
      "learning_rate": 6.370370370370371e-06,
      "loss": 0.0007,
      "step": 23560
    },
    {
      "epoch": 1.7459259259259259,
      "grad_norm": 0.09668195247650146,
      "learning_rate": 6.351851851851852e-06,
      "loss": 0.0015,
      "step": 23570
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.1630226969718933,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0009,
      "step": 23580
    },
    {
      "epoch": 1.7474074074074073,
      "grad_norm": 0.05302473530173302,
      "learning_rate": 6.314814814814815e-06,
      "loss": 0.0006,
      "step": 23590
    },
    {
      "epoch": 1.748148148148148,
      "grad_norm": 0.04443863779306412,
      "learning_rate": 6.296296296296296e-06,
      "loss": 0.0014,
      "step": 23600
    },
    {
      "epoch": 1.748888888888889,
      "grad_norm": 0.14546102285385132,
      "learning_rate": 6.277777777777778e-06,
      "loss": 0.0018,
      "step": 23610
    },
    {
      "epoch": 1.7496296296296296,
      "grad_norm": 0.12685386836528778,
      "learning_rate": 6.259259259259259e-06,
      "loss": 0.001,
      "step": 23620
    },
    {
      "epoch": 1.7503703703703704,
      "grad_norm": 0.045859187841415405,
      "learning_rate": 6.240740740740741e-06,
      "loss": 0.0009,
      "step": 23630
    },
    {
      "epoch": 1.751111111111111,
      "grad_norm": 0.11234556883573532,
      "learning_rate": 6.222222222222222e-06,
      "loss": 0.0028,
      "step": 23640
    },
    {
      "epoch": 1.751851851851852,
      "grad_norm": 0.09316172450780869,
      "learning_rate": 6.203703703703704e-06,
      "loss": 0.0012,
      "step": 23650
    },
    {
      "epoch": 1.7525925925925927,
      "grad_norm": 0.08193470537662506,
      "learning_rate": 6.1851851851851856e-06,
      "loss": 0.0014,
      "step": 23660
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.08351773768663406,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0007,
      "step": 23670
    },
    {
      "epoch": 1.7540740740740741,
      "grad_norm": 0.09302151203155518,
      "learning_rate": 6.148148148148149e-06,
      "loss": 0.0006,
      "step": 23680
    },
    {
      "epoch": 1.7548148148148148,
      "grad_norm": 0.04757549986243248,
      "learning_rate": 6.12962962962963e-06,
      "loss": 0.0009,
      "step": 23690
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.049310579895973206,
      "learning_rate": 6.111111111111111e-06,
      "loss": 0.0012,
      "step": 23700
    },
    {
      "epoch": 1.7562962962962962,
      "grad_norm": 0.05137854069471359,
      "learning_rate": 6.092592592592593e-06,
      "loss": 0.001,
      "step": 23710
    },
    {
      "epoch": 1.757037037037037,
      "grad_norm": 0.0,
      "learning_rate": 6.0740740740740745e-06,
      "loss": 0.0013,
      "step": 23720
    },
    {
      "epoch": 1.7577777777777777,
      "grad_norm": 0.13054506480693817,
      "learning_rate": 6.055555555555556e-06,
      "loss": 0.0015,
      "step": 23730
    },
    {
      "epoch": 1.7585185185185184,
      "grad_norm": 0.17975372076034546,
      "learning_rate": 6.037037037037038e-06,
      "loss": 0.0013,
      "step": 23740
    },
    {
      "epoch": 1.7592592592592593,
      "grad_norm": 0.06778193265199661,
      "learning_rate": 6.0185185185185185e-06,
      "loss": 0.0009,
      "step": 23750
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.0,
      "learning_rate": 6e-06,
      "loss": 0.001,
      "step": 23760
    },
    {
      "epoch": 1.7607407407407407,
      "grad_norm": 0.05771380662918091,
      "learning_rate": 5.981481481481482e-06,
      "loss": 0.001,
      "step": 23770
    },
    {
      "epoch": 1.7614814814814816,
      "grad_norm": 0.08544088900089264,
      "learning_rate": 5.962962962962963e-06,
      "loss": 0.0012,
      "step": 23780
    },
    {
      "epoch": 1.7622222222222224,
      "grad_norm": 0.0,
      "learning_rate": 5.944444444444445e-06,
      "loss": 0.0011,
      "step": 23790
    },
    {
      "epoch": 1.762962962962963,
      "grad_norm": 0.09163422137498856,
      "learning_rate": 5.925925925925927e-06,
      "loss": 0.0006,
      "step": 23800
    },
    {
      "epoch": 1.7637037037037038,
      "grad_norm": 0.09199533611536026,
      "learning_rate": 5.907407407407408e-06,
      "loss": 0.0014,
      "step": 23810
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.11941810697317123,
      "learning_rate": 5.888888888888889e-06,
      "loss": 0.002,
      "step": 23820
    },
    {
      "epoch": 1.7651851851851852,
      "grad_norm": 0.13644248247146606,
      "learning_rate": 5.870370370370371e-06,
      "loss": 0.0016,
      "step": 23830
    },
    {
      "epoch": 1.765925925925926,
      "grad_norm": 0.08611376583576202,
      "learning_rate": 5.851851851851852e-06,
      "loss": 0.0007,
      "step": 23840
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.043881606310606,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0021,
      "step": 23850
    },
    {
      "epoch": 1.7674074074074073,
      "grad_norm": 0.08492474257946014,
      "learning_rate": 5.814814814814816e-06,
      "loss": 0.001,
      "step": 23860
    },
    {
      "epoch": 1.768148148148148,
      "grad_norm": 0.0,
      "learning_rate": 5.796296296296297e-06,
      "loss": 0.0016,
      "step": 23870
    },
    {
      "epoch": 1.7688888888888887,
      "grad_norm": 0.15950103104114532,
      "learning_rate": 5.777777777777778e-06,
      "loss": 0.0014,
      "step": 23880
    },
    {
      "epoch": 1.7696296296296297,
      "grad_norm": 0.10396113991737366,
      "learning_rate": 5.75925925925926e-06,
      "loss": 0.0011,
      "step": 23890
    },
    {
      "epoch": 1.7703703703703704,
      "grad_norm": 0.0663655698299408,
      "learning_rate": 5.740740740740741e-06,
      "loss": 0.001,
      "step": 23900
    },
    {
      "epoch": 1.771111111111111,
      "grad_norm": 0.06549625843763351,
      "learning_rate": 5.722222222222223e-06,
      "loss": 0.0011,
      "step": 23910
    },
    {
      "epoch": 1.771851851851852,
      "grad_norm": 0.11015718430280685,
      "learning_rate": 5.7037037037037045e-06,
      "loss": 0.0013,
      "step": 23920
    },
    {
      "epoch": 1.7725925925925927,
      "grad_norm": 0.09960128366947174,
      "learning_rate": 5.685185185185186e-06,
      "loss": 0.0009,
      "step": 23930
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.09899312257766724,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0015,
      "step": 23940
    },
    {
      "epoch": 1.7740740740740741,
      "grad_norm": 0.09558836370706558,
      "learning_rate": 5.6481481481481485e-06,
      "loss": 0.0021,
      "step": 23950
    },
    {
      "epoch": 1.7748148148148148,
      "grad_norm": 0.04573008790612221,
      "learning_rate": 5.62962962962963e-06,
      "loss": 0.0024,
      "step": 23960
    },
    {
      "epoch": 1.7755555555555556,
      "grad_norm": 0.14033165574073792,
      "learning_rate": 5.611111111111112e-06,
      "loss": 0.0008,
      "step": 23970
    },
    {
      "epoch": 1.7762962962962963,
      "grad_norm": 0.091102734208107,
      "learning_rate": 5.592592592592593e-06,
      "loss": 0.0018,
      "step": 23980
    },
    {
      "epoch": 1.777037037037037,
      "grad_norm": 0.06833464652299881,
      "learning_rate": 5.574074074074074e-06,
      "loss": 0.0016,
      "step": 23990
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.09315339475870132,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.001,
      "step": 24000
    },
    {
      "epoch": 1.7785185185185184,
      "grad_norm": 0.044504035264253616,
      "learning_rate": 5.5370370370370374e-06,
      "loss": 0.0019,
      "step": 24010
    },
    {
      "epoch": 1.779259259259259,
      "grad_norm": 0.045451436191797256,
      "learning_rate": 5.518518518518519e-06,
      "loss": 0.0014,
      "step": 24020
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.04919852316379547,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.001,
      "step": 24030
    },
    {
      "epoch": 1.7807407407407407,
      "grad_norm": 0.09962134063243866,
      "learning_rate": 5.4814814814814815e-06,
      "loss": 0.001,
      "step": 24040
    },
    {
      "epoch": 1.7814814814814814,
      "grad_norm": 0.04444309324026108,
      "learning_rate": 5.462962962962963e-06,
      "loss": 0.0014,
      "step": 24050
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 0.042532969266176224,
      "learning_rate": 5.444444444444445e-06,
      "loss": 0.0009,
      "step": 24060
    },
    {
      "epoch": 1.782962962962963,
      "grad_norm": 0.04444895312190056,
      "learning_rate": 5.425925925925926e-06,
      "loss": 0.0016,
      "step": 24070
    },
    {
      "epoch": 1.7837037037037038,
      "grad_norm": 0.048115964978933334,
      "learning_rate": 5.407407407407407e-06,
      "loss": 0.0013,
      "step": 24080
    },
    {
      "epoch": 1.7844444444444445,
      "grad_norm": 0.09336426109075546,
      "learning_rate": 5.388888888888889e-06,
      "loss": 0.001,
      "step": 24090
    },
    {
      "epoch": 1.7851851851851852,
      "grad_norm": 0.10385740548372269,
      "learning_rate": 5.37037037037037e-06,
      "loss": 0.001,
      "step": 24100
    },
    {
      "epoch": 1.785925925925926,
      "grad_norm": 0.0,
      "learning_rate": 5.351851851851852e-06,
      "loss": 0.0008,
      "step": 24110
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.04752098768949509,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0022,
      "step": 24120
    },
    {
      "epoch": 1.7874074074074073,
      "grad_norm": 0.03592439368367195,
      "learning_rate": 5.3148148148148144e-06,
      "loss": 0.0011,
      "step": 24130
    },
    {
      "epoch": 1.788148148148148,
      "grad_norm": 0.07931043952703476,
      "learning_rate": 5.296296296296296e-06,
      "loss": 0.0012,
      "step": 24140
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 0.13250410556793213,
      "learning_rate": 5.277777777777778e-06,
      "loss": 0.0019,
      "step": 24150
    },
    {
      "epoch": 1.7896296296296297,
      "grad_norm": 0.09379065781831741,
      "learning_rate": 5.259259259259259e-06,
      "loss": 0.0015,
      "step": 24160
    },
    {
      "epoch": 1.7903703703703704,
      "grad_norm": 0.049421023577451706,
      "learning_rate": 5.240740740740741e-06,
      "loss": 0.0016,
      "step": 24170
    },
    {
      "epoch": 1.791111111111111,
      "grad_norm": 0.057911377400159836,
      "learning_rate": 5.2222222222222226e-06,
      "loss": 0.0015,
      "step": 24180
    },
    {
      "epoch": 1.7918518518518518,
      "grad_norm": 0.0928577110171318,
      "learning_rate": 5.203703703703703e-06,
      "loss": 0.0017,
      "step": 24190
    },
    {
      "epoch": 1.7925925925925927,
      "grad_norm": 0.1132616251707077,
      "learning_rate": 5.185185185185185e-06,
      "loss": 0.0013,
      "step": 24200
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.10055799782276154,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.0014,
      "step": 24210
    },
    {
      "epoch": 1.7940740740740742,
      "grad_norm": 0.11768001317977905,
      "learning_rate": 5.148148148148148e-06,
      "loss": 0.0015,
      "step": 24220
    },
    {
      "epoch": 1.7948148148148149,
      "grad_norm": 0.10057282447814941,
      "learning_rate": 5.12962962962963e-06,
      "loss": 0.0015,
      "step": 24230
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 0.03996644541621208,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 0.0017,
      "step": 24240
    },
    {
      "epoch": 1.7962962962962963,
      "grad_norm": 0.05783320218324661,
      "learning_rate": 5.092592592592592e-06,
      "loss": 0.0016,
      "step": 24250
    },
    {
      "epoch": 1.797037037037037,
      "grad_norm": 0.0442483089864254,
      "learning_rate": 5.074074074074074e-06,
      "loss": 0.0007,
      "step": 24260
    },
    {
      "epoch": 1.7977777777777777,
      "grad_norm": 0.12834668159484863,
      "learning_rate": 5.0555555555555555e-06,
      "loss": 0.0014,
      "step": 24270
    },
    {
      "epoch": 1.7985185185185184,
      "grad_norm": 0.1348484605550766,
      "learning_rate": 5.037037037037037e-06,
      "loss": 0.0019,
      "step": 24280
    },
    {
      "epoch": 1.7992592592592591,
      "grad_norm": 0.0,
      "learning_rate": 5.018518518518519e-06,
      "loss": 0.0016,
      "step": 24290
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.060256145894527435,
      "learning_rate": 5e-06,
      "loss": 0.0008,
      "step": 24300
    },
    {
      "epoch": 1.8007407407407408,
      "grad_norm": 0.10071499645709991,
      "learning_rate": 4.981481481481481e-06,
      "loss": 0.0009,
      "step": 24310
    },
    {
      "epoch": 1.8014814814814815,
      "grad_norm": 0.13673418760299683,
      "learning_rate": 4.962962962962963e-06,
      "loss": 0.0017,
      "step": 24320
    },
    {
      "epoch": 1.8022222222222222,
      "grad_norm": 0.0997532308101654,
      "learning_rate": 4.9444444444444444e-06,
      "loss": 0.0009,
      "step": 24330
    },
    {
      "epoch": 1.802962962962963,
      "grad_norm": 0.10345932841300964,
      "learning_rate": 4.925925925925926e-06,
      "loss": 0.0014,
      "step": 24340
    },
    {
      "epoch": 1.8037037037037038,
      "grad_norm": 0.09993302077054977,
      "learning_rate": 4.907407407407408e-06,
      "loss": 0.001,
      "step": 24350
    },
    {
      "epoch": 1.8044444444444445,
      "grad_norm": 0.0,
      "learning_rate": 4.888888888888889e-06,
      "loss": 0.0007,
      "step": 24360
    },
    {
      "epoch": 1.8051851851851852,
      "grad_norm": 0.0,
      "learning_rate": 4.87037037037037e-06,
      "loss": 0.0015,
      "step": 24370
    },
    {
      "epoch": 1.805925925925926,
      "grad_norm": 0.15169382095336914,
      "learning_rate": 4.851851851851852e-06,
      "loss": 0.0017,
      "step": 24380
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.050264596939086914,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0011,
      "step": 24390
    },
    {
      "epoch": 1.8074074074074074,
      "grad_norm": 0.04871847853064537,
      "learning_rate": 4.814814814814815e-06,
      "loss": 0.0012,
      "step": 24400
    },
    {
      "epoch": 1.808148148148148,
      "grad_norm": 0.0,
      "learning_rate": 4.796296296296297e-06,
      "loss": 0.0016,
      "step": 24410
    },
    {
      "epoch": 1.8088888888888888,
      "grad_norm": 0.0,
      "learning_rate": 4.777777777777778e-06,
      "loss": 0.0009,
      "step": 24420
    },
    {
      "epoch": 1.8096296296296295,
      "grad_norm": 0.19118241965770721,
      "learning_rate": 4.75925925925926e-06,
      "loss": 0.0013,
      "step": 24430
    },
    {
      "epoch": 1.8103703703703704,
      "grad_norm": 0.10742593556642532,
      "learning_rate": 4.740740740740741e-06,
      "loss": 0.0015,
      "step": 24440
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 0.042932331562042236,
      "learning_rate": 4.722222222222222e-06,
      "loss": 0.0026,
      "step": 24450
    },
    {
      "epoch": 1.8118518518518518,
      "grad_norm": 0.030204201117157936,
      "learning_rate": 4.703703703703704e-06,
      "loss": 0.0006,
      "step": 24460
    },
    {
      "epoch": 1.8125925925925928,
      "grad_norm": 0.08006859570741653,
      "learning_rate": 4.6851851851851855e-06,
      "loss": 0.0012,
      "step": 24470
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.0,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0018,
      "step": 24480
    },
    {
      "epoch": 1.8140740740740742,
      "grad_norm": 0.06871351599693298,
      "learning_rate": 4.648148148148149e-06,
      "loss": 0.001,
      "step": 24490
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 0.13488838076591492,
      "learning_rate": 4.6296296296296296e-06,
      "loss": 0.0011,
      "step": 24500
    },
    {
      "epoch": 1.8155555555555556,
      "grad_norm": 0.11885395646095276,
      "learning_rate": 4.611111111111111e-06,
      "loss": 0.0024,
      "step": 24510
    },
    {
      "epoch": 1.8162962962962963,
      "grad_norm": 0.08922526985406876,
      "learning_rate": 4.592592592592593e-06,
      "loss": 0.0016,
      "step": 24520
    },
    {
      "epoch": 1.817037037037037,
      "grad_norm": 0.0,
      "learning_rate": 4.5740740740740745e-06,
      "loss": 0.0019,
      "step": 24530
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 0.06036768853664398,
      "learning_rate": 4.555555555555556e-06,
      "loss": 0.0013,
      "step": 24540
    },
    {
      "epoch": 1.8185185185185184,
      "grad_norm": 0.21743342280387878,
      "learning_rate": 4.537037037037038e-06,
      "loss": 0.0011,
      "step": 24550
    },
    {
      "epoch": 1.8192592592592591,
      "grad_norm": 0.0,
      "learning_rate": 4.5185185185185185e-06,
      "loss": 0.0004,
      "step": 24560
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.10672538727521896,
      "learning_rate": 4.5e-06,
      "loss": 0.0011,
      "step": 24570
    },
    {
      "epoch": 1.8207407407407408,
      "grad_norm": 0.0449095256626606,
      "learning_rate": 4.481481481481482e-06,
      "loss": 0.0007,
      "step": 24580
    },
    {
      "epoch": 1.8214814814814815,
      "grad_norm": 0.053604137152433395,
      "learning_rate": 4.462962962962963e-06,
      "loss": 0.0007,
      "step": 24590
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.111385278403759,
      "learning_rate": 4.444444444444445e-06,
      "loss": 0.0017,
      "step": 24600
    },
    {
      "epoch": 1.8229629629629631,
      "grad_norm": 0.06321728229522705,
      "learning_rate": 4.425925925925927e-06,
      "loss": 0.0007,
      "step": 24610
    },
    {
      "epoch": 1.8237037037037038,
      "grad_norm": 0.0,
      "learning_rate": 4.407407407407407e-06,
      "loss": 0.0015,
      "step": 24620
    },
    {
      "epoch": 1.8244444444444445,
      "grad_norm": 0.11106907576322556,
      "learning_rate": 4.388888888888889e-06,
      "loss": 0.0014,
      "step": 24630
    },
    {
      "epoch": 1.8251851851851852,
      "grad_norm": 0.08141926676034927,
      "learning_rate": 4.370370370370371e-06,
      "loss": 0.0014,
      "step": 24640
    },
    {
      "epoch": 1.825925925925926,
      "grad_norm": 0.09609030932188034,
      "learning_rate": 4.351851851851852e-06,
      "loss": 0.0012,
      "step": 24650
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.18494813144207,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0013,
      "step": 24660
    },
    {
      "epoch": 1.8274074074074074,
      "grad_norm": 0.0,
      "learning_rate": 4.3148148148148155e-06,
      "loss": 0.0022,
      "step": 24670
    },
    {
      "epoch": 1.828148148148148,
      "grad_norm": 0.044385578483343124,
      "learning_rate": 4.296296296296296e-06,
      "loss": 0.001,
      "step": 24680
    },
    {
      "epoch": 1.8288888888888888,
      "grad_norm": 0.08673499524593353,
      "learning_rate": 4.277777777777778e-06,
      "loss": 0.0009,
      "step": 24690
    },
    {
      "epoch": 1.8296296296296295,
      "grad_norm": 0.08303077518939972,
      "learning_rate": 4.2592592592592596e-06,
      "loss": 0.0015,
      "step": 24700
    },
    {
      "epoch": 1.8303703703703704,
      "grad_norm": 0.06245206668972969,
      "learning_rate": 4.240740740740741e-06,
      "loss": 0.0012,
      "step": 24710
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 0.13537776470184326,
      "learning_rate": 4.222222222222223e-06,
      "loss": 0.0008,
      "step": 24720
    },
    {
      "epoch": 1.8318518518518518,
      "grad_norm": 0.13028168678283691,
      "learning_rate": 4.2037037037037045e-06,
      "loss": 0.001,
      "step": 24730
    },
    {
      "epoch": 1.8325925925925926,
      "grad_norm": 0.15812134742736816,
      "learning_rate": 4.185185185185185e-06,
      "loss": 0.0006,
      "step": 24740
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.04782012850046158,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0009,
      "step": 24750
    },
    {
      "epoch": 1.8340740740740742,
      "grad_norm": 0.10338236391544342,
      "learning_rate": 4.1481481481481485e-06,
      "loss": 0.0009,
      "step": 24760
    },
    {
      "epoch": 1.834814814814815,
      "grad_norm": 0.0,
      "learning_rate": 4.12962962962963e-06,
      "loss": 0.0009,
      "step": 24770
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 0.11911694705486298,
      "learning_rate": 4.111111111111112e-06,
      "loss": 0.0019,
      "step": 24780
    },
    {
      "epoch": 1.8362962962962963,
      "grad_norm": 0.055238254368305206,
      "learning_rate": 4.092592592592593e-06,
      "loss": 0.0008,
      "step": 24790
    },
    {
      "epoch": 1.837037037037037,
      "grad_norm": 0.06129373610019684,
      "learning_rate": 4.074074074074075e-06,
      "loss": 0.0008,
      "step": 24800
    },
    {
      "epoch": 1.8377777777777777,
      "grad_norm": 0.0,
      "learning_rate": 4.055555555555556e-06,
      "loss": 0.0008,
      "step": 24810
    },
    {
      "epoch": 1.8385185185185184,
      "grad_norm": 0.11890687048435211,
      "learning_rate": 4.037037037037037e-06,
      "loss": 0.0017,
      "step": 24820
    },
    {
      "epoch": 1.8392592592592591,
      "grad_norm": 0.1533203125,
      "learning_rate": 4.018518518518519e-06,
      "loss": 0.0012,
      "step": 24830
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.07666850090026855,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0015,
      "step": 24840
    },
    {
      "epoch": 1.8407407407407408,
      "grad_norm": 0.13997246325016022,
      "learning_rate": 3.9814814814814814e-06,
      "loss": 0.0024,
      "step": 24850
    },
    {
      "epoch": 1.8414814814814815,
      "grad_norm": 0.06873085349798203,
      "learning_rate": 3.962962962962963e-06,
      "loss": 0.001,
      "step": 24860
    },
    {
      "epoch": 1.8422222222222222,
      "grad_norm": 0.07529088109731674,
      "learning_rate": 3.944444444444445e-06,
      "loss": 0.0015,
      "step": 24870
    },
    {
      "epoch": 1.842962962962963,
      "grad_norm": 0.07692291587591171,
      "learning_rate": 3.925925925925926e-06,
      "loss": 0.0019,
      "step": 24880
    },
    {
      "epoch": 1.8437037037037038,
      "grad_norm": 0.11713622510433197,
      "learning_rate": 3.907407407407408e-06,
      "loss": 0.001,
      "step": 24890
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 0.0,
      "learning_rate": 3.888888888888889e-06,
      "loss": 0.0012,
      "step": 24900
    },
    {
      "epoch": 1.8451851851851853,
      "grad_norm": 0.10025540739297867,
      "learning_rate": 3.87037037037037e-06,
      "loss": 0.0011,
      "step": 24910
    },
    {
      "epoch": 1.845925925925926,
      "grad_norm": 0.12716668844223022,
      "learning_rate": 3.851851851851852e-06,
      "loss": 0.0008,
      "step": 24920
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.09318185597658157,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.0015,
      "step": 24930
    },
    {
      "epoch": 1.8474074074074074,
      "grad_norm": 0.18381109833717346,
      "learning_rate": 3.814814814814814e-06,
      "loss": 0.0011,
      "step": 24940
    },
    {
      "epoch": 1.848148148148148,
      "grad_norm": 0.0,
      "learning_rate": 3.7962962962962964e-06,
      "loss": 0.0017,
      "step": 24950
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 0.15977732837200165,
      "learning_rate": 3.777777777777778e-06,
      "loss": 0.0023,
      "step": 24960
    },
    {
      "epoch": 1.8496296296296295,
      "grad_norm": 0.06464865058660507,
      "learning_rate": 3.7592592592592597e-06,
      "loss": 0.0012,
      "step": 24970
    },
    {
      "epoch": 1.8503703703703702,
      "grad_norm": 0.05923333391547203,
      "learning_rate": 3.7407407407407413e-06,
      "loss": 0.0009,
      "step": 24980
    },
    {
      "epoch": 1.8511111111111112,
      "grad_norm": 0.11422916501760483,
      "learning_rate": 3.722222222222222e-06,
      "loss": 0.0007,
      "step": 24990
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 0.10566706210374832,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 0.0014,
      "step": 25000
    },
    {
      "epoch": 1.8525925925925926,
      "grad_norm": 0.0,
      "learning_rate": 3.6851851851851854e-06,
      "loss": 0.0007,
      "step": 25010
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.08934876322746277,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0014,
      "step": 25020
    },
    {
      "epoch": 1.8540740740740742,
      "grad_norm": 0.0,
      "learning_rate": 3.6481481481481486e-06,
      "loss": 0.0011,
      "step": 25030
    },
    {
      "epoch": 1.854814814814815,
      "grad_norm": 0.09642074257135391,
      "learning_rate": 3.6296296296296302e-06,
      "loss": 0.0011,
      "step": 25040
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 0.0,
      "learning_rate": 3.611111111111111e-06,
      "loss": 0.0014,
      "step": 25050
    },
    {
      "epoch": 1.8562962962962963,
      "grad_norm": 0.15483537316322327,
      "learning_rate": 3.5925925925925927e-06,
      "loss": 0.0019,
      "step": 25060
    },
    {
      "epoch": 1.857037037037037,
      "grad_norm": 0.08957808464765549,
      "learning_rate": 3.5740740740740743e-06,
      "loss": 0.0014,
      "step": 25070
    },
    {
      "epoch": 1.8577777777777778,
      "grad_norm": 0.07066528499126434,
      "learning_rate": 3.555555555555556e-06,
      "loss": 0.0016,
      "step": 25080
    },
    {
      "epoch": 1.8585185185185185,
      "grad_norm": 0.0,
      "learning_rate": 3.537037037037037e-06,
      "loss": 0.0006,
      "step": 25090
    },
    {
      "epoch": 1.8592592592592592,
      "grad_norm": 0.05197639763355255,
      "learning_rate": 3.5185185185185187e-06,
      "loss": 0.0019,
      "step": 25100
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.0,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0017,
      "step": 25110
    },
    {
      "epoch": 1.8607407407407406,
      "grad_norm": 0.0,
      "learning_rate": 3.4814814814814816e-06,
      "loss": 0.001,
      "step": 25120
    },
    {
      "epoch": 1.8614814814814815,
      "grad_norm": 0.04006442055106163,
      "learning_rate": 3.462962962962963e-06,
      "loss": 0.0021,
      "step": 25130
    },
    {
      "epoch": 1.8622222222222222,
      "grad_norm": 0.08364265412092209,
      "learning_rate": 3.4444444444444444e-06,
      "loss": 0.0008,
      "step": 25140
    },
    {
      "epoch": 1.862962962962963,
      "grad_norm": 0.04476279020309448,
      "learning_rate": 3.425925925925926e-06,
      "loss": 0.0012,
      "step": 25150
    },
    {
      "epoch": 1.8637037037037039,
      "grad_norm": 0.0,
      "learning_rate": 3.4074074074074077e-06,
      "loss": 0.0024,
      "step": 25160
    },
    {
      "epoch": 1.8644444444444446,
      "grad_norm": 0.0,
      "learning_rate": 3.3888888888888893e-06,
      "loss": 0.0017,
      "step": 25170
    },
    {
      "epoch": 1.8651851851851853,
      "grad_norm": 0.0875547006726265,
      "learning_rate": 3.37037037037037e-06,
      "loss": 0.001,
      "step": 25180
    },
    {
      "epoch": 1.865925925925926,
      "grad_norm": 0.12613508105278015,
      "learning_rate": 3.3518518518518517e-06,
      "loss": 0.0006,
      "step": 25190
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.07092728465795517,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0009,
      "step": 25200
    },
    {
      "epoch": 1.8674074074074074,
      "grad_norm": 0.0794345811009407,
      "learning_rate": 3.314814814814815e-06,
      "loss": 0.0012,
      "step": 25210
    },
    {
      "epoch": 1.8681481481481481,
      "grad_norm": 0.08460430800914764,
      "learning_rate": 3.2962962962962966e-06,
      "loss": 0.0015,
      "step": 25220
    },
    {
      "epoch": 1.8688888888888888,
      "grad_norm": 0.05980079993605614,
      "learning_rate": 3.277777777777778e-06,
      "loss": 0.0011,
      "step": 25230
    },
    {
      "epoch": 1.8696296296296295,
      "grad_norm": 0.049901507794857025,
      "learning_rate": 3.259259259259259e-06,
      "loss": 0.001,
      "step": 25240
    },
    {
      "epoch": 1.8703703703703702,
      "grad_norm": 0.09764454513788223,
      "learning_rate": 3.2407407407407406e-06,
      "loss": 0.001,
      "step": 25250
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 0.13111533224582672,
      "learning_rate": 3.2222222222222222e-06,
      "loss": 0.0014,
      "step": 25260
    },
    {
      "epoch": 1.8718518518518519,
      "grad_norm": 0.08655989915132523,
      "learning_rate": 3.203703703703704e-06,
      "loss": 0.0013,
      "step": 25270
    },
    {
      "epoch": 1.8725925925925926,
      "grad_norm": 0.18707609176635742,
      "learning_rate": 3.1851851851851855e-06,
      "loss": 0.0015,
      "step": 25280
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0016,
      "step": 25290
    },
    {
      "epoch": 1.8740740740740742,
      "grad_norm": 0.10071280598640442,
      "learning_rate": 3.148148148148148e-06,
      "loss": 0.002,
      "step": 25300
    },
    {
      "epoch": 1.874814814814815,
      "grad_norm": 0.0,
      "learning_rate": 3.1296296296296295e-06,
      "loss": 0.0013,
      "step": 25310
    },
    {
      "epoch": 1.8755555555555556,
      "grad_norm": 0.13871647417545319,
      "learning_rate": 3.111111111111111e-06,
      "loss": 0.0015,
      "step": 25320
    },
    {
      "epoch": 1.8762962962962964,
      "grad_norm": 0.13590891659259796,
      "learning_rate": 3.0925925925925928e-06,
      "loss": 0.0014,
      "step": 25330
    },
    {
      "epoch": 1.877037037037037,
      "grad_norm": 0.11520523577928543,
      "learning_rate": 3.0740740740740744e-06,
      "loss": 0.0008,
      "step": 25340
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 0.10975012928247452,
      "learning_rate": 3.0555555555555556e-06,
      "loss": 0.0011,
      "step": 25350
    },
    {
      "epoch": 1.8785185185185185,
      "grad_norm": 0.0,
      "learning_rate": 3.0370370370370372e-06,
      "loss": 0.002,
      "step": 25360
    },
    {
      "epoch": 1.8792592592592592,
      "grad_norm": 0.14046242833137512,
      "learning_rate": 3.018518518518519e-06,
      "loss": 0.0011,
      "step": 25370
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.05174553766846657,
      "learning_rate": 3e-06,
      "loss": 0.001,
      "step": 25380
    },
    {
      "epoch": 1.8807407407407406,
      "grad_norm": 0.11007048189640045,
      "learning_rate": 2.9814814814814817e-06,
      "loss": 0.0014,
      "step": 25390
    },
    {
      "epoch": 1.8814814814814815,
      "grad_norm": 0.09270019829273224,
      "learning_rate": 2.9629629629629633e-06,
      "loss": 0.0012,
      "step": 25400
    },
    {
      "epoch": 1.8822222222222222,
      "grad_norm": 0.06202029809355736,
      "learning_rate": 2.9444444444444445e-06,
      "loss": 0.0015,
      "step": 25410
    },
    {
      "epoch": 1.882962962962963,
      "grad_norm": 0.08068758249282837,
      "learning_rate": 2.925925925925926e-06,
      "loss": 0.0007,
      "step": 25420
    },
    {
      "epoch": 1.8837037037037037,
      "grad_norm": 0.08322468400001526,
      "learning_rate": 2.907407407407408e-06,
      "loss": 0.0008,
      "step": 25430
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 0.13759368658065796,
      "learning_rate": 2.888888888888889e-06,
      "loss": 0.0013,
      "step": 25440
    },
    {
      "epoch": 1.8851851851851853,
      "grad_norm": 0.06744758784770966,
      "learning_rate": 2.8703703703703706e-06,
      "loss": 0.0019,
      "step": 25450
    },
    {
      "epoch": 1.885925925925926,
      "grad_norm": 0.0,
      "learning_rate": 2.8518518518518522e-06,
      "loss": 0.0012,
      "step": 25460
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.07780710607767105,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.0006,
      "step": 25470
    },
    {
      "epoch": 1.8874074074074074,
      "grad_norm": 0.0,
      "learning_rate": 2.814814814814815e-06,
      "loss": 0.0007,
      "step": 25480
    },
    {
      "epoch": 1.8881481481481481,
      "grad_norm": 0.1015612781047821,
      "learning_rate": 2.7962962962962967e-06,
      "loss": 0.0006,
      "step": 25490
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.06756559014320374,
      "learning_rate": 2.777777777777778e-06,
      "loss": 0.0013,
      "step": 25500
    },
    {
      "epoch": 1.8896296296296295,
      "grad_norm": 0.09650115668773651,
      "learning_rate": 2.7592592592592595e-06,
      "loss": 0.0018,
      "step": 25510
    },
    {
      "epoch": 1.8903703703703703,
      "grad_norm": 0.18875674903392792,
      "learning_rate": 2.7407407407407407e-06,
      "loss": 0.0012,
      "step": 25520
    },
    {
      "epoch": 1.891111111111111,
      "grad_norm": 0.07958441227674484,
      "learning_rate": 2.7222222222222224e-06,
      "loss": 0.001,
      "step": 25530
    },
    {
      "epoch": 1.891851851851852,
      "grad_norm": 0.07847677916288376,
      "learning_rate": 2.7037037037037036e-06,
      "loss": 0.0007,
      "step": 25540
    },
    {
      "epoch": 1.8925925925925926,
      "grad_norm": 0.05360089987516403,
      "learning_rate": 2.685185185185185e-06,
      "loss": 0.0009,
      "step": 25550
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.09075367450714111,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.001,
      "step": 25560
    },
    {
      "epoch": 1.894074074074074,
      "grad_norm": 0.058473680168390274,
      "learning_rate": 2.648148148148148e-06,
      "loss": 0.001,
      "step": 25570
    },
    {
      "epoch": 1.894814814814815,
      "grad_norm": 0.07086502760648727,
      "learning_rate": 2.6296296296296297e-06,
      "loss": 0.0011,
      "step": 25580
    },
    {
      "epoch": 1.8955555555555557,
      "grad_norm": 0.08313389867544174,
      "learning_rate": 2.6111111111111113e-06,
      "loss": 0.0012,
      "step": 25590
    },
    {
      "epoch": 1.8962962962962964,
      "grad_norm": 0.0,
      "learning_rate": 2.5925925925925925e-06,
      "loss": 0.0007,
      "step": 25600
    },
    {
      "epoch": 1.897037037037037,
      "grad_norm": 0.13892515003681183,
      "learning_rate": 2.574074074074074e-06,
      "loss": 0.0012,
      "step": 25610
    },
    {
      "epoch": 1.8977777777777778,
      "grad_norm": 0.045314520597457886,
      "learning_rate": 2.5555555555555557e-06,
      "loss": 0.0013,
      "step": 25620
    },
    {
      "epoch": 1.8985185185185185,
      "grad_norm": 0.05745790898799896,
      "learning_rate": 2.537037037037037e-06,
      "loss": 0.0014,
      "step": 25630
    },
    {
      "epoch": 1.8992592592592592,
      "grad_norm": 0.06857581436634064,
      "learning_rate": 2.5185185185185186e-06,
      "loss": 0.0015,
      "step": 25640
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.08040913939476013,
      "learning_rate": 2.5e-06,
      "loss": 0.0016,
      "step": 25650
    },
    {
      "epoch": 1.9007407407407406,
      "grad_norm": 0.08599697053432465,
      "learning_rate": 2.4814814814814814e-06,
      "loss": 0.001,
      "step": 25660
    },
    {
      "epoch": 1.9014814814814813,
      "grad_norm": 0.14747048914432526,
      "learning_rate": 2.462962962962963e-06,
      "loss": 0.001,
      "step": 25670
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 0.1474059373140335,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 0.0011,
      "step": 25680
    },
    {
      "epoch": 1.902962962962963,
      "grad_norm": 0.1022900938987732,
      "learning_rate": 2.425925925925926e-06,
      "loss": 0.0015,
      "step": 25690
    },
    {
      "epoch": 1.9037037037037037,
      "grad_norm": 0.05240470916032791,
      "learning_rate": 2.4074074074074075e-06,
      "loss": 0.0017,
      "step": 25700
    },
    {
      "epoch": 1.9044444444444446,
      "grad_norm": 0.0,
      "learning_rate": 2.388888888888889e-06,
      "loss": 0.0008,
      "step": 25710
    },
    {
      "epoch": 1.9051851851851853,
      "grad_norm": 0.0,
      "learning_rate": 2.3703703703703703e-06,
      "loss": 0.0012,
      "step": 25720
    },
    {
      "epoch": 1.905925925925926,
      "grad_norm": 0.04088680446147919,
      "learning_rate": 2.351851851851852e-06,
      "loss": 0.0014,
      "step": 25730
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0009,
      "step": 25740
    },
    {
      "epoch": 1.9074074074074074,
      "grad_norm": 0.05378876253962517,
      "learning_rate": 2.3148148148148148e-06,
      "loss": 0.0007,
      "step": 25750
    },
    {
      "epoch": 1.9081481481481481,
      "grad_norm": 0.11270787566900253,
      "learning_rate": 2.2962962962962964e-06,
      "loss": 0.0013,
      "step": 25760
    },
    {
      "epoch": 1.9088888888888889,
      "grad_norm": 0.1729128509759903,
      "learning_rate": 2.277777777777778e-06,
      "loss": 0.0014,
      "step": 25770
    },
    {
      "epoch": 1.9096296296296296,
      "grad_norm": 0.10266631096601486,
      "learning_rate": 2.2592592592592592e-06,
      "loss": 0.0017,
      "step": 25780
    },
    {
      "epoch": 1.9103703703703703,
      "grad_norm": 0.07600413262844086,
      "learning_rate": 2.240740740740741e-06,
      "loss": 0.0009,
      "step": 25790
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.0,
      "learning_rate": 2.2222222222222225e-06,
      "loss": 0.0018,
      "step": 25800
    },
    {
      "epoch": 1.9118518518518517,
      "grad_norm": 0.06532777100801468,
      "learning_rate": 2.2037037037037037e-06,
      "loss": 0.0013,
      "step": 25810
    },
    {
      "epoch": 1.9125925925925926,
      "grad_norm": 0.07162654399871826,
      "learning_rate": 2.1851851851851853e-06,
      "loss": 0.0007,
      "step": 25820
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.058452729135751724,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.0008,
      "step": 25830
    },
    {
      "epoch": 1.914074074074074,
      "grad_norm": 0.08381973206996918,
      "learning_rate": 2.148148148148148e-06,
      "loss": 0.0016,
      "step": 25840
    },
    {
      "epoch": 1.914814814814815,
      "grad_norm": 0.0454583466053009,
      "learning_rate": 2.1296296296296298e-06,
      "loss": 0.0016,
      "step": 25850
    },
    {
      "epoch": 1.9155555555555557,
      "grad_norm": 0.15330226719379425,
      "learning_rate": 2.1111111111111114e-06,
      "loss": 0.0025,
      "step": 25860
    },
    {
      "epoch": 1.9162962962962964,
      "grad_norm": 0.056288015097379684,
      "learning_rate": 2.0925925925925926e-06,
      "loss": 0.0022,
      "step": 25870
    },
    {
      "epoch": 1.917037037037037,
      "grad_norm": 0.16513538360595703,
      "learning_rate": 2.0740740740740742e-06,
      "loss": 0.0018,
      "step": 25880
    },
    {
      "epoch": 1.9177777777777778,
      "grad_norm": 0.04354975000023842,
      "learning_rate": 2.055555555555556e-06,
      "loss": 0.0006,
      "step": 25890
    },
    {
      "epoch": 1.9185185185185185,
      "grad_norm": 0.05115153640508652,
      "learning_rate": 2.0370370370370375e-06,
      "loss": 0.001,
      "step": 25900
    },
    {
      "epoch": 1.9192592592592592,
      "grad_norm": 0.11537963151931763,
      "learning_rate": 2.0185185185185187e-06,
      "loss": 0.001,
      "step": 25910
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.08658219873905182,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0011,
      "step": 25920
    },
    {
      "epoch": 1.9207407407407406,
      "grad_norm": 0.0708211213350296,
      "learning_rate": 1.9814814814814815e-06,
      "loss": 0.0019,
      "step": 25930
    },
    {
      "epoch": 1.9214814814814813,
      "grad_norm": 0.0589480996131897,
      "learning_rate": 1.962962962962963e-06,
      "loss": 0.0011,
      "step": 25940
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 0.0,
      "learning_rate": 1.9444444444444444e-06,
      "loss": 0.0016,
      "step": 25950
    },
    {
      "epoch": 1.922962962962963,
      "grad_norm": 0.09852397441864014,
      "learning_rate": 1.925925925925926e-06,
      "loss": 0.0014,
      "step": 25960
    },
    {
      "epoch": 1.9237037037037037,
      "grad_norm": 0.09868663549423218,
      "learning_rate": 1.907407407407407e-06,
      "loss": 0.0005,
      "step": 25970
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 0.04088270291686058,
      "learning_rate": 1.888888888888889e-06,
      "loss": 0.0011,
      "step": 25980
    },
    {
      "epoch": 1.9251851851851853,
      "grad_norm": 0.065767802298069,
      "learning_rate": 1.8703703703703707e-06,
      "loss": 0.0014,
      "step": 25990
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 0.12630116939544678,
      "learning_rate": 1.8518518518518519e-06,
      "loss": 0.0011,
      "step": 26000
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.11298937350511551,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0009,
      "step": 26010
    },
    {
      "epoch": 1.9274074074074075,
      "grad_norm": 0.0,
      "learning_rate": 1.8148148148148151e-06,
      "loss": 0.0009,
      "step": 26020
    },
    {
      "epoch": 1.9281481481481482,
      "grad_norm": 0.14866846799850464,
      "learning_rate": 1.7962962962962963e-06,
      "loss": 0.001,
      "step": 26030
    },
    {
      "epoch": 1.9288888888888889,
      "grad_norm": 0.15754954516887665,
      "learning_rate": 1.777777777777778e-06,
      "loss": 0.0011,
      "step": 26040
    },
    {
      "epoch": 1.9296296296296296,
      "grad_norm": 0.08478812128305435,
      "learning_rate": 1.7592592592592594e-06,
      "loss": 0.0008,
      "step": 26050
    },
    {
      "epoch": 1.9303703703703703,
      "grad_norm": 0.08779606223106384,
      "learning_rate": 1.7407407407407408e-06,
      "loss": 0.0022,
      "step": 26060
    },
    {
      "epoch": 1.931111111111111,
      "grad_norm": 0.05325990170240402,
      "learning_rate": 1.7222222222222222e-06,
      "loss": 0.0018,
      "step": 26070
    },
    {
      "epoch": 1.9318518518518517,
      "grad_norm": 0.07989678531885147,
      "learning_rate": 1.7037037037037038e-06,
      "loss": 0.001,
      "step": 26080
    },
    {
      "epoch": 1.9325925925925926,
      "grad_norm": 0.11143267899751663,
      "learning_rate": 1.685185185185185e-06,
      "loss": 0.0009,
      "step": 26090
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0012,
      "step": 26100
    },
    {
      "epoch": 1.934074074074074,
      "grad_norm": 0.0473167710006237,
      "learning_rate": 1.6481481481481483e-06,
      "loss": 0.0015,
      "step": 26110
    },
    {
      "epoch": 1.9348148148148148,
      "grad_norm": 0.1287820190191269,
      "learning_rate": 1.6296296296296295e-06,
      "loss": 0.0014,
      "step": 26120
    },
    {
      "epoch": 1.9355555555555557,
      "grad_norm": 0.04652433097362518,
      "learning_rate": 1.6111111111111111e-06,
      "loss": 0.0016,
      "step": 26130
    },
    {
      "epoch": 1.9362962962962964,
      "grad_norm": 0.06721111387014389,
      "learning_rate": 1.5925925925925927e-06,
      "loss": 0.0008,
      "step": 26140
    },
    {
      "epoch": 1.9370370370370371,
      "grad_norm": 0.10503149032592773,
      "learning_rate": 1.574074074074074e-06,
      "loss": 0.0023,
      "step": 26150
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 0.07797765731811523,
      "learning_rate": 1.5555555555555556e-06,
      "loss": 0.0011,
      "step": 26160
    },
    {
      "epoch": 1.9385185185185185,
      "grad_norm": 0.09664525091648102,
      "learning_rate": 1.5370370370370372e-06,
      "loss": 0.0009,
      "step": 26170
    },
    {
      "epoch": 1.9392592592592592,
      "grad_norm": 0.05457961931824684,
      "learning_rate": 1.5185185185185186e-06,
      "loss": 0.0016,
      "step": 26180
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.0,
      "learning_rate": 1.5e-06,
      "loss": 0.0011,
      "step": 26190
    },
    {
      "epoch": 1.9407407407407407,
      "grad_norm": 0.09434289485216141,
      "learning_rate": 1.4814814814814817e-06,
      "loss": 0.002,
      "step": 26200
    },
    {
      "epoch": 1.9414814814814814,
      "grad_norm": 0.05782218277454376,
      "learning_rate": 1.462962962962963e-06,
      "loss": 0.0026,
      "step": 26210
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 0.04595950245857239,
      "learning_rate": 1.4444444444444445e-06,
      "loss": 0.001,
      "step": 26220
    },
    {
      "epoch": 1.942962962962963,
      "grad_norm": 0.13086959719657898,
      "learning_rate": 1.4259259259259261e-06,
      "loss": 0.0008,
      "step": 26230
    },
    {
      "epoch": 1.9437037037037037,
      "grad_norm": 0.10301633179187775,
      "learning_rate": 1.4074074074074075e-06,
      "loss": 0.0007,
      "step": 26240
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.09634873270988464,
      "learning_rate": 1.388888888888889e-06,
      "loss": 0.0024,
      "step": 26250
    },
    {
      "epoch": 1.9451851851851854,
      "grad_norm": 0.1694219410419464,
      "learning_rate": 1.3703703703703704e-06,
      "loss": 0.0013,
      "step": 26260
    },
    {
      "epoch": 1.945925925925926,
      "grad_norm": 0.04633853957056999,
      "learning_rate": 1.3518518518518518e-06,
      "loss": 0.0015,
      "step": 26270
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.11662586033344269,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0009,
      "step": 26280
    },
    {
      "epoch": 1.9474074074074075,
      "grad_norm": 0.12060952186584473,
      "learning_rate": 1.3148148148148148e-06,
      "loss": 0.0019,
      "step": 26290
    },
    {
      "epoch": 1.9481481481481482,
      "grad_norm": 0.0529511384665966,
      "learning_rate": 1.2962962962962962e-06,
      "loss": 0.0008,
      "step": 26300
    },
    {
      "epoch": 1.948888888888889,
      "grad_norm": 0.0,
      "learning_rate": 1.2777777777777779e-06,
      "loss": 0.0008,
      "step": 26310
    },
    {
      "epoch": 1.9496296296296296,
      "grad_norm": 0.10666502267122269,
      "learning_rate": 1.2592592592592593e-06,
      "loss": 0.0012,
      "step": 26320
    },
    {
      "epoch": 1.9503703703703703,
      "grad_norm": 0.12647505104541779,
      "learning_rate": 1.2407407407407407e-06,
      "loss": 0.0006,
      "step": 26330
    },
    {
      "epoch": 1.951111111111111,
      "grad_norm": 0.05131525173783302,
      "learning_rate": 1.2222222222222223e-06,
      "loss": 0.0008,
      "step": 26340
    },
    {
      "epoch": 1.9518518518518517,
      "grad_norm": 0.13396385312080383,
      "learning_rate": 1.2037037037037037e-06,
      "loss": 0.0009,
      "step": 26350
    },
    {
      "epoch": 1.9525925925925924,
      "grad_norm": 0.14574237167835236,
      "learning_rate": 1.1851851851851852e-06,
      "loss": 0.0009,
      "step": 26360
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.05991775915026665,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0014,
      "step": 26370
    },
    {
      "epoch": 1.954074074074074,
      "grad_norm": 0.1346437931060791,
      "learning_rate": 1.1481481481481482e-06,
      "loss": 0.0023,
      "step": 26380
    },
    {
      "epoch": 1.9548148148148148,
      "grad_norm": 0.0,
      "learning_rate": 1.1296296296296296e-06,
      "loss": 0.0019,
      "step": 26390
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.07289355993270874,
      "learning_rate": 1.1111111111111112e-06,
      "loss": 0.0017,
      "step": 26400
    },
    {
      "epoch": 1.9562962962962964,
      "grad_norm": 0.04769999533891678,
      "learning_rate": 1.0925925925925927e-06,
      "loss": 0.0009,
      "step": 26410
    },
    {
      "epoch": 1.9570370370370371,
      "grad_norm": 0.06693340837955475,
      "learning_rate": 1.074074074074074e-06,
      "loss": 0.0016,
      "step": 26420
    },
    {
      "epoch": 1.9577777777777778,
      "grad_norm": 0.15848316252231598,
      "learning_rate": 1.0555555555555557e-06,
      "loss": 0.0006,
      "step": 26430
    },
    {
      "epoch": 1.9585185185185185,
      "grad_norm": 0.044679149985313416,
      "learning_rate": 1.0370370370370371e-06,
      "loss": 0.0012,
      "step": 26440
    },
    {
      "epoch": 1.9592592592592593,
      "grad_norm": 0.14352039992809296,
      "learning_rate": 1.0185185185185188e-06,
      "loss": 0.001,
      "step": 26450
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.06479121744632721,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0011,
      "step": 26460
    },
    {
      "epoch": 1.9607407407407407,
      "grad_norm": 0.0,
      "learning_rate": 9.814814814814816e-07,
      "loss": 0.0012,
      "step": 26470
    },
    {
      "epoch": 1.9614814814814814,
      "grad_norm": 0.05450861155986786,
      "learning_rate": 9.62962962962963e-07,
      "loss": 0.0015,
      "step": 26480
    },
    {
      "epoch": 1.962222222222222,
      "grad_norm": 0.11653708666563034,
      "learning_rate": 9.444444444444445e-07,
      "loss": 0.0018,
      "step": 26490
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 0.08701682835817337,
      "learning_rate": 9.259259259259259e-07,
      "loss": 0.0008,
      "step": 26500
    },
    {
      "epoch": 1.9637037037037037,
      "grad_norm": 0.0888364240527153,
      "learning_rate": 9.074074074074076e-07,
      "loss": 0.001,
      "step": 26510
    },
    {
      "epoch": 1.9644444444444444,
      "grad_norm": 0.06857048720121384,
      "learning_rate": 8.88888888888889e-07,
      "loss": 0.0008,
      "step": 26520
    },
    {
      "epoch": 1.9651851851851851,
      "grad_norm": 0.11091512441635132,
      "learning_rate": 8.703703703703704e-07,
      "loss": 0.0011,
      "step": 26530
    },
    {
      "epoch": 1.965925925925926,
      "grad_norm": 0.0456257238984108,
      "learning_rate": 8.518518518518519e-07,
      "loss": 0.001,
      "step": 26540
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.0,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0005,
      "step": 26550
    },
    {
      "epoch": 1.9674074074074075,
      "grad_norm": 0.06558498740196228,
      "learning_rate": 8.148148148148147e-07,
      "loss": 0.0005,
      "step": 26560
    },
    {
      "epoch": 1.9681481481481482,
      "grad_norm": 0.07515064626932144,
      "learning_rate": 7.962962962962964e-07,
      "loss": 0.0014,
      "step": 26570
    },
    {
      "epoch": 1.968888888888889,
      "grad_norm": 0.0,
      "learning_rate": 7.777777777777778e-07,
      "loss": 0.0012,
      "step": 26580
    },
    {
      "epoch": 1.9696296296296296,
      "grad_norm": 0.14208976924419403,
      "learning_rate": 7.592592592592593e-07,
      "loss": 0.0008,
      "step": 26590
    },
    {
      "epoch": 1.9703703703703703,
      "grad_norm": 0.13940881192684174,
      "learning_rate": 7.407407407407408e-07,
      "loss": 0.0011,
      "step": 26600
    },
    {
      "epoch": 1.971111111111111,
      "grad_norm": 0.049127910286188126,
      "learning_rate": 7.222222222222222e-07,
      "loss": 0.001,
      "step": 26610
    },
    {
      "epoch": 1.9718518518518517,
      "grad_norm": 0.09172967076301575,
      "learning_rate": 7.037037037037038e-07,
      "loss": 0.0012,
      "step": 26620
    },
    {
      "epoch": 1.9725925925925925,
      "grad_norm": 0.0,
      "learning_rate": 6.851851851851852e-07,
      "loss": 0.0015,
      "step": 26630
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.0,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0017,
      "step": 26640
    },
    {
      "epoch": 1.974074074074074,
      "grad_norm": 0.0,
      "learning_rate": 6.481481481481481e-07,
      "loss": 0.001,
      "step": 26650
    },
    {
      "epoch": 1.9748148148148148,
      "grad_norm": 0.08687266707420349,
      "learning_rate": 6.296296296296296e-07,
      "loss": 0.0014,
      "step": 26660
    },
    {
      "epoch": 1.9755555555555555,
      "grad_norm": 0.051341596990823746,
      "learning_rate": 6.111111111111112e-07,
      "loss": 0.0013,
      "step": 26670
    },
    {
      "epoch": 1.9762962962962964,
      "grad_norm": 0.06741552799940109,
      "learning_rate": 5.925925925925926e-07,
      "loss": 0.0021,
      "step": 26680
    },
    {
      "epoch": 1.9770370370370371,
      "grad_norm": 0.22426237165927887,
      "learning_rate": 5.740740740740741e-07,
      "loss": 0.0008,
      "step": 26690
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.10799519717693329,
      "learning_rate": 5.555555555555556e-07,
      "loss": 0.0014,
      "step": 26700
    },
    {
      "epoch": 1.9785185185185186,
      "grad_norm": 0.0,
      "learning_rate": 5.37037037037037e-07,
      "loss": 0.0008,
      "step": 26710
    },
    {
      "epoch": 1.9792592592592593,
      "grad_norm": 0.1139756515622139,
      "learning_rate": 5.185185185185186e-07,
      "loss": 0.0018,
      "step": 26720
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.09577105194330215,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0013,
      "step": 26730
    },
    {
      "epoch": 1.9807407407407407,
      "grad_norm": 0.09426591545343399,
      "learning_rate": 4.814814814814815e-07,
      "loss": 0.0012,
      "step": 26740
    },
    {
      "epoch": 1.9814814814814814,
      "grad_norm": 0.06104293465614319,
      "learning_rate": 4.6296296296296297e-07,
      "loss": 0.001,
      "step": 26750
    },
    {
      "epoch": 1.982222222222222,
      "grad_norm": 0.11195516586303711,
      "learning_rate": 4.444444444444445e-07,
      "loss": 0.0015,
      "step": 26760
    },
    {
      "epoch": 1.9829629629629628,
      "grad_norm": 0.17191216349601746,
      "learning_rate": 4.2592592592592596e-07,
      "loss": 0.0009,
      "step": 26770
    },
    {
      "epoch": 1.9837037037037037,
      "grad_norm": 0.097344309091568,
      "learning_rate": 4.0740740740740737e-07,
      "loss": 0.0014,
      "step": 26780
    },
    {
      "epoch": 1.9844444444444445,
      "grad_norm": 0.0810907632112503,
      "learning_rate": 3.888888888888889e-07,
      "loss": 0.0018,
      "step": 26790
    },
    {
      "epoch": 1.9851851851851852,
      "grad_norm": 0.06460510939359665,
      "learning_rate": 3.703703703703704e-07,
      "loss": 0.0007,
      "step": 26800
    },
    {
      "epoch": 1.9859259259259259,
      "grad_norm": 0.045215629041194916,
      "learning_rate": 3.518518518518519e-07,
      "loss": 0.0016,
      "step": 26810
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.11625798791646957,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.001,
      "step": 26820
    },
    {
      "epoch": 1.9874074074074075,
      "grad_norm": 0.0,
      "learning_rate": 3.148148148148148e-07,
      "loss": 0.0012,
      "step": 26830
    },
    {
      "epoch": 1.9881481481481482,
      "grad_norm": 0.095587819814682,
      "learning_rate": 2.962962962962963e-07,
      "loss": 0.0009,
      "step": 26840
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 0.0,
      "learning_rate": 2.777777777777778e-07,
      "loss": 0.0004,
      "step": 26850
    },
    {
      "epoch": 1.9896296296296296,
      "grad_norm": 0.06617692112922668,
      "learning_rate": 2.592592592592593e-07,
      "loss": 0.0015,
      "step": 26860
    },
    {
      "epoch": 1.9903703703703703,
      "grad_norm": 0.08433748781681061,
      "learning_rate": 2.4074074074074075e-07,
      "loss": 0.0015,
      "step": 26870
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 0.09791547060012817,
      "learning_rate": 2.2222222222222224e-07,
      "loss": 0.0023,
      "step": 26880
    },
    {
      "epoch": 1.9918518518518518,
      "grad_norm": 0.056773923337459564,
      "learning_rate": 2.0370370370370369e-07,
      "loss": 0.0013,
      "step": 26890
    },
    {
      "epoch": 1.9925925925925925,
      "grad_norm": 0.0,
      "learning_rate": 1.851851851851852e-07,
      "loss": 0.0019,
      "step": 26900
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.1061987355351448,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.001,
      "step": 26910
    },
    {
      "epoch": 1.994074074074074,
      "grad_norm": 0.0,
      "learning_rate": 1.4814814814814815e-07,
      "loss": 0.0009,
      "step": 26920
    },
    {
      "epoch": 1.9948148148148148,
      "grad_norm": 0.13800740242004395,
      "learning_rate": 1.2962962962962964e-07,
      "loss": 0.0014,
      "step": 26930
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 0.10154829919338226,
      "learning_rate": 1.1111111111111112e-07,
      "loss": 0.0015,
      "step": 26940
    },
    {
      "epoch": 1.9962962962962965,
      "grad_norm": 0.05934332683682442,
      "learning_rate": 9.25925925925926e-08,
      "loss": 0.0011,
      "step": 26950
    },
    {
      "epoch": 1.9970370370370372,
      "grad_norm": 0.11878238618373871,
      "learning_rate": 7.407407407407407e-08,
      "loss": 0.0012,
      "step": 26960
    },
    {
      "epoch": 1.9977777777777779,
      "grad_norm": 0.05232228711247444,
      "learning_rate": 5.555555555555556e-08,
      "loss": 0.0005,
      "step": 26970
    },
    {
      "epoch": 1.9985185185185186,
      "grad_norm": 0.16411295533180237,
      "learning_rate": 3.7037037037037036e-08,
      "loss": 0.0012,
      "step": 26980
    },
    {
      "epoch": 1.9992592592592593,
      "grad_norm": 0.030207917094230652,
      "learning_rate": 1.8518518518518518e-08,
      "loss": 0.0011,
      "step": 26990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.06936465948820114,
      "learning_rate": 0.0,
      "loss": 0.001,
      "step": 27000
    }
  ],
  "logging_steps": 10,
  "max_steps": 27000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

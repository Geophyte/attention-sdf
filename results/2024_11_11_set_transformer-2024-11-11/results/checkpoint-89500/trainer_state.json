{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.972222222222222,
  "eval_steps": 500,
  "global_step": 89500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005555555555555556,
      "grad_norm": 1.239774465560913,
      "learning_rate": 4.9994444444444446e-05,
      "loss": 0.0187,
      "step": 10
    },
    {
      "epoch": 0.0011111111111111111,
      "grad_norm": 0.7377306222915649,
      "learning_rate": 4.998888888888889e-05,
      "loss": 0.0087,
      "step": 20
    },
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.7301199436187744,
      "learning_rate": 4.998333333333334e-05,
      "loss": 0.0091,
      "step": 30
    },
    {
      "epoch": 0.0022222222222222222,
      "grad_norm": 0.8633569478988647,
      "learning_rate": 4.997777777777778e-05,
      "loss": 0.0071,
      "step": 40
    },
    {
      "epoch": 0.002777777777777778,
      "grad_norm": 0.6636713743209839,
      "learning_rate": 4.997222222222223e-05,
      "loss": 0.0057,
      "step": 50
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.5335493087768555,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.006,
      "step": 60
    },
    {
      "epoch": 0.0038888888888888888,
      "grad_norm": 0.9268941879272461,
      "learning_rate": 4.9961111111111114e-05,
      "loss": 0.0059,
      "step": 70
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 0.596102237701416,
      "learning_rate": 4.995555555555556e-05,
      "loss": 0.0056,
      "step": 80
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.07587701082229614,
      "learning_rate": 4.995e-05,
      "loss": 0.0069,
      "step": 90
    },
    {
      "epoch": 0.005555555555555556,
      "grad_norm": 0.264676958322525,
      "learning_rate": 4.994444444444445e-05,
      "loss": 0.0058,
      "step": 100
    },
    {
      "epoch": 0.006111111111111111,
      "grad_norm": 0.26749297976493835,
      "learning_rate": 4.993888888888889e-05,
      "loss": 0.0081,
      "step": 110
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.9286215901374817,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0069,
      "step": 120
    },
    {
      "epoch": 0.007222222222222222,
      "grad_norm": 0.03273189440369606,
      "learning_rate": 4.992777777777778e-05,
      "loss": 0.0041,
      "step": 130
    },
    {
      "epoch": 0.0077777777777777776,
      "grad_norm": 0.02112528868019581,
      "learning_rate": 4.9922222222222226e-05,
      "loss": 0.0055,
      "step": 140
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.5303000211715698,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.0064,
      "step": 150
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 0.0697380006313324,
      "learning_rate": 4.991111111111111e-05,
      "loss": 0.0059,
      "step": 160
    },
    {
      "epoch": 0.009444444444444445,
      "grad_norm": 0.030074166133999825,
      "learning_rate": 4.9905555555555556e-05,
      "loss": 0.0063,
      "step": 170
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.6614378094673157,
      "learning_rate": 4.99e-05,
      "loss": 0.0085,
      "step": 180
    },
    {
      "epoch": 0.010555555555555556,
      "grad_norm": 0.1342369168996811,
      "learning_rate": 4.989444444444445e-05,
      "loss": 0.0057,
      "step": 190
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 0.396845281124115,
      "learning_rate": 4.9888888888888894e-05,
      "loss": 0.0077,
      "step": 200
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 0.07365752756595612,
      "learning_rate": 4.988333333333334e-05,
      "loss": 0.0063,
      "step": 210
    },
    {
      "epoch": 0.012222222222222223,
      "grad_norm": 0.6602312922477722,
      "learning_rate": 4.987777777777778e-05,
      "loss": 0.0054,
      "step": 220
    },
    {
      "epoch": 0.012777777777777779,
      "grad_norm": 0.6616018414497375,
      "learning_rate": 4.9872222222222225e-05,
      "loss": 0.0072,
      "step": 230
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.33247366547584534,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0071,
      "step": 240
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 0.2671035826206207,
      "learning_rate": 4.986111111111111e-05,
      "loss": 0.0059,
      "step": 250
    },
    {
      "epoch": 0.014444444444444444,
      "grad_norm": 0.5283027291297913,
      "learning_rate": 4.9855555555555555e-05,
      "loss": 0.0044,
      "step": 260
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.06903146207332611,
      "learning_rate": 4.9850000000000006e-05,
      "loss": 0.0081,
      "step": 270
    },
    {
      "epoch": 0.015555555555555555,
      "grad_norm": 0.19879932701587677,
      "learning_rate": 4.984444444444445e-05,
      "loss": 0.0087,
      "step": 280
    },
    {
      "epoch": 0.01611111111111111,
      "grad_norm": 0.1360476315021515,
      "learning_rate": 4.983888888888889e-05,
      "loss": 0.0053,
      "step": 290
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.7262363433837891,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0044,
      "step": 300
    },
    {
      "epoch": 0.017222222222222222,
      "grad_norm": 0.3301849961280823,
      "learning_rate": 4.982777777777778e-05,
      "loss": 0.0061,
      "step": 310
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 0.5983487367630005,
      "learning_rate": 4.982222222222222e-05,
      "loss": 0.0067,
      "step": 320
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.725166916847229,
      "learning_rate": 4.981666666666667e-05,
      "loss": 0.0059,
      "step": 330
    },
    {
      "epoch": 0.01888888888888889,
      "grad_norm": 0.464235782623291,
      "learning_rate": 4.981111111111112e-05,
      "loss": 0.007,
      "step": 340
    },
    {
      "epoch": 0.019444444444444445,
      "grad_norm": 0.19905565679073334,
      "learning_rate": 4.9805555555555554e-05,
      "loss": 0.0079,
      "step": 350
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.07355430722236633,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0077,
      "step": 360
    },
    {
      "epoch": 0.020555555555555556,
      "grad_norm": 0.39671915769577026,
      "learning_rate": 4.979444444444445e-05,
      "loss": 0.0044,
      "step": 370
    },
    {
      "epoch": 0.021111111111111112,
      "grad_norm": 0.264938622713089,
      "learning_rate": 4.978888888888889e-05,
      "loss": 0.0079,
      "step": 380
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.2621304988861084,
      "learning_rate": 4.9783333333333335e-05,
      "loss": 0.0066,
      "step": 390
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 0.6544634699821472,
      "learning_rate": 4.977777777777778e-05,
      "loss": 0.0065,
      "step": 400
    },
    {
      "epoch": 0.02277777777777778,
      "grad_norm": 0.852764368057251,
      "learning_rate": 4.977222222222223e-05,
      "loss": 0.0077,
      "step": 410
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.2640240490436554,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0059,
      "step": 420
    },
    {
      "epoch": 0.02388888888888889,
      "grad_norm": 0.8486216068267822,
      "learning_rate": 4.9761111111111116e-05,
      "loss": 0.0055,
      "step": 430
    },
    {
      "epoch": 0.024444444444444446,
      "grad_norm": 0.13388220965862274,
      "learning_rate": 4.975555555555555e-05,
      "loss": 0.0079,
      "step": 440
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.32756590843200684,
      "learning_rate": 4.975e-05,
      "loss": 0.005,
      "step": 450
    },
    {
      "epoch": 0.025555555555555557,
      "grad_norm": 0.7165035605430603,
      "learning_rate": 4.974444444444445e-05,
      "loss": 0.0087,
      "step": 460
    },
    {
      "epoch": 0.026111111111111113,
      "grad_norm": 0.6524074077606201,
      "learning_rate": 4.973888888888889e-05,
      "loss": 0.0071,
      "step": 470
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.33064088225364685,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0064,
      "step": 480
    },
    {
      "epoch": 0.02722222222222222,
      "grad_norm": 0.5889397859573364,
      "learning_rate": 4.972777777777778e-05,
      "loss": 0.0061,
      "step": 490
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 0.13269254565238953,
      "learning_rate": 4.972222222222223e-05,
      "loss": 0.0083,
      "step": 500
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.19712673127651215,
      "learning_rate": 4.9716666666666664e-05,
      "loss": 0.0072,
      "step": 510
    },
    {
      "epoch": 0.028888888888888888,
      "grad_norm": 0.06769655644893646,
      "learning_rate": 4.9711111111111115e-05,
      "loss": 0.0072,
      "step": 520
    },
    {
      "epoch": 0.029444444444444443,
      "grad_norm": 0.3293716013431549,
      "learning_rate": 4.970555555555556e-05,
      "loss": 0.0062,
      "step": 530
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.9816104173660278,
      "learning_rate": 4.97e-05,
      "loss": 0.0067,
      "step": 540
    },
    {
      "epoch": 0.030555555555555555,
      "grad_norm": 0.1315741091966629,
      "learning_rate": 4.969444444444445e-05,
      "loss": 0.0074,
      "step": 550
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 0.13099992275238037,
      "learning_rate": 4.968888888888889e-05,
      "loss": 0.0073,
      "step": 560
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.5895217061042786,
      "learning_rate": 4.968333333333334e-05,
      "loss": 0.0064,
      "step": 570
    },
    {
      "epoch": 0.03222222222222222,
      "grad_norm": 0.1311577707529068,
      "learning_rate": 4.9677777777777776e-05,
      "loss": 0.0076,
      "step": 580
    },
    {
      "epoch": 0.03277777777777778,
      "grad_norm": 0.01581086404621601,
      "learning_rate": 4.9672222222222226e-05,
      "loss": 0.008,
      "step": 590
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.39255380630493164,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0069,
      "step": 600
    },
    {
      "epoch": 0.03388888888888889,
      "grad_norm": 0.45706793665885925,
      "learning_rate": 4.9661111111111114e-05,
      "loss": 0.0078,
      "step": 610
    },
    {
      "epoch": 0.034444444444444444,
      "grad_norm": 0.8502568006515503,
      "learning_rate": 4.965555555555556e-05,
      "loss": 0.0057,
      "step": 620
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.4568655490875244,
      "learning_rate": 4.965e-05,
      "loss": 0.0054,
      "step": 630
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 0.1358109563589096,
      "learning_rate": 4.964444444444445e-05,
      "loss": 0.0064,
      "step": 640
    },
    {
      "epoch": 0.03611111111111111,
      "grad_norm": 0.19567659497261047,
      "learning_rate": 4.963888888888889e-05,
      "loss": 0.0075,
      "step": 650
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.1342146247625351,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0078,
      "step": 660
    },
    {
      "epoch": 0.03722222222222222,
      "grad_norm": 0.784631609916687,
      "learning_rate": 4.962777777777778e-05,
      "loss": 0.0071,
      "step": 670
    },
    {
      "epoch": 0.03777777777777778,
      "grad_norm": 0.19895318150520325,
      "learning_rate": 4.9622222222222225e-05,
      "loss": 0.0077,
      "step": 680
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.07242608815431595,
      "learning_rate": 4.961666666666667e-05,
      "loss": 0.0057,
      "step": 690
    },
    {
      "epoch": 0.03888888888888889,
      "grad_norm": 0.06894352287054062,
      "learning_rate": 4.961111111111111e-05,
      "loss": 0.0061,
      "step": 700
    },
    {
      "epoch": 0.03944444444444444,
      "grad_norm": 0.26188287138938904,
      "learning_rate": 4.960555555555556e-05,
      "loss": 0.0058,
      "step": 710
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01656178943812847,
      "learning_rate": 4.96e-05,
      "loss": 0.0069,
      "step": 720
    },
    {
      "epoch": 0.04055555555555555,
      "grad_norm": 0.04439287632703781,
      "learning_rate": 4.959444444444445e-05,
      "loss": 0.0064,
      "step": 730
    },
    {
      "epoch": 0.04111111111111111,
      "grad_norm": 0.1964261382818222,
      "learning_rate": 4.958888888888889e-05,
      "loss": 0.0062,
      "step": 740
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.8479849696159363,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.007,
      "step": 750
    },
    {
      "epoch": 0.042222222222222223,
      "grad_norm": 0.01921221613883972,
      "learning_rate": 4.957777777777778e-05,
      "loss": 0.0058,
      "step": 760
    },
    {
      "epoch": 0.042777777777777776,
      "grad_norm": 0.3941815495491028,
      "learning_rate": 4.9572222222222224e-05,
      "loss": 0.0065,
      "step": 770
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.5863618850708008,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.0049,
      "step": 780
    },
    {
      "epoch": 0.04388888888888889,
      "grad_norm": 0.9749467968940735,
      "learning_rate": 4.956111111111111e-05,
      "loss": 0.0053,
      "step": 790
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.587817907333374,
      "learning_rate": 4.955555555555556e-05,
      "loss": 0.006,
      "step": 800
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.8477064967155457,
      "learning_rate": 4.9550000000000005e-05,
      "loss": 0.0044,
      "step": 810
    },
    {
      "epoch": 0.04555555555555556,
      "grad_norm": 0.6544151306152344,
      "learning_rate": 4.954444444444445e-05,
      "loss": 0.0043,
      "step": 820
    },
    {
      "epoch": 0.04611111111111111,
      "grad_norm": 0.3264915943145752,
      "learning_rate": 4.953888888888889e-05,
      "loss": 0.0059,
      "step": 830
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.13094544410705566,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0049,
      "step": 840
    },
    {
      "epoch": 0.04722222222222222,
      "grad_norm": 0.13521113991737366,
      "learning_rate": 4.952777777777778e-05,
      "loss": 0.0065,
      "step": 850
    },
    {
      "epoch": 0.04777777777777778,
      "grad_norm": 0.3271152973175049,
      "learning_rate": 4.952222222222222e-05,
      "loss": 0.0063,
      "step": 860
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.5218414664268494,
      "learning_rate": 4.9516666666666666e-05,
      "loss": 0.0074,
      "step": 870
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 0.45470917224884033,
      "learning_rate": 4.951111111111112e-05,
      "loss": 0.009,
      "step": 880
    },
    {
      "epoch": 0.049444444444444444,
      "grad_norm": 0.7147126793861389,
      "learning_rate": 4.950555555555556e-05,
      "loss": 0.0064,
      "step": 890
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.13183611631393433,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.0079,
      "step": 900
    },
    {
      "epoch": 0.050555555555555555,
      "grad_norm": 0.5857452154159546,
      "learning_rate": 4.949444444444445e-05,
      "loss": 0.0046,
      "step": 910
    },
    {
      "epoch": 0.051111111111111114,
      "grad_norm": 0.19537705183029175,
      "learning_rate": 4.948888888888889e-05,
      "loss": 0.0051,
      "step": 920
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.06991462409496307,
      "learning_rate": 4.9483333333333334e-05,
      "loss": 0.008,
      "step": 930
    },
    {
      "epoch": 0.052222222222222225,
      "grad_norm": 0.06637503951787949,
      "learning_rate": 4.947777777777778e-05,
      "loss": 0.0056,
      "step": 940
    },
    {
      "epoch": 0.05277777777777778,
      "grad_norm": 0.6497951745986938,
      "learning_rate": 4.947222222222223e-05,
      "loss": 0.006,
      "step": 950
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.5853528380393982,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0062,
      "step": 960
    },
    {
      "epoch": 0.05388888888888889,
      "grad_norm": 0.13214488327503204,
      "learning_rate": 4.9461111111111115e-05,
      "loss": 0.007,
      "step": 970
    },
    {
      "epoch": 0.05444444444444444,
      "grad_norm": 0.32475554943084717,
      "learning_rate": 4.945555555555556e-05,
      "loss": 0.0053,
      "step": 980
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.326628714799881,
      "learning_rate": 4.945e-05,
      "loss": 0.0086,
      "step": 990
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.650295615196228,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 0.0041,
      "step": 1000
    },
    {
      "epoch": 0.05611111111111111,
      "grad_norm": 0.45321255922317505,
      "learning_rate": 4.943888888888889e-05,
      "loss": 0.0051,
      "step": 1010
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.646271288394928,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.007,
      "step": 1020
    },
    {
      "epoch": 0.05722222222222222,
      "grad_norm": 0.06869365274906158,
      "learning_rate": 4.942777777777778e-05,
      "loss": 0.0061,
      "step": 1030
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 0.32338497042655945,
      "learning_rate": 4.942222222222223e-05,
      "loss": 0.0056,
      "step": 1040
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.7749736309051514,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.0055,
      "step": 1050
    },
    {
      "epoch": 0.058888888888888886,
      "grad_norm": 1.0328476428985596,
      "learning_rate": 4.9411111111111114e-05,
      "loss": 0.0069,
      "step": 1060
    },
    {
      "epoch": 0.059444444444444446,
      "grad_norm": 0.5175073742866516,
      "learning_rate": 4.940555555555556e-05,
      "loss": 0.0053,
      "step": 1070
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.32247498631477356,
      "learning_rate": 4.94e-05,
      "loss": 0.0067,
      "step": 1080
    },
    {
      "epoch": 0.06055555555555556,
      "grad_norm": 0.3881823718547821,
      "learning_rate": 4.939444444444445e-05,
      "loss": 0.0053,
      "step": 1090
    },
    {
      "epoch": 0.06111111111111111,
      "grad_norm": 0.451150119304657,
      "learning_rate": 4.938888888888889e-05,
      "loss": 0.0061,
      "step": 1100
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.3871738016605377,
      "learning_rate": 4.938333333333334e-05,
      "loss": 0.0052,
      "step": 1110
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 0.13137634098529816,
      "learning_rate": 4.9377777777777776e-05,
      "loss": 0.0053,
      "step": 1120
    },
    {
      "epoch": 0.06277777777777778,
      "grad_norm": 0.3221827447414398,
      "learning_rate": 4.9372222222222226e-05,
      "loss": 0.0067,
      "step": 1130
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.1292157769203186,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.0059,
      "step": 1140
    },
    {
      "epoch": 0.06388888888888888,
      "grad_norm": 0.12900863587856293,
      "learning_rate": 4.936111111111111e-05,
      "loss": 0.005,
      "step": 1150
    },
    {
      "epoch": 0.06444444444444444,
      "grad_norm": 0.38881754875183105,
      "learning_rate": 4.935555555555556e-05,
      "loss": 0.0062,
      "step": 1160
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.908265233039856,
      "learning_rate": 4.935e-05,
      "loss": 0.0082,
      "step": 1170
    },
    {
      "epoch": 0.06555555555555556,
      "grad_norm": 0.3288837671279907,
      "learning_rate": 4.934444444444445e-05,
      "loss": 0.0082,
      "step": 1180
    },
    {
      "epoch": 0.0661111111111111,
      "grad_norm": 0.19422273337841034,
      "learning_rate": 4.933888888888889e-05,
      "loss": 0.0082,
      "step": 1190
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.06921140104532242,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0058,
      "step": 1200
    },
    {
      "epoch": 0.06722222222222222,
      "grad_norm": 0.2584514021873474,
      "learning_rate": 4.932777777777778e-05,
      "loss": 0.007,
      "step": 1210
    },
    {
      "epoch": 0.06777777777777778,
      "grad_norm": 0.3234233856201172,
      "learning_rate": 4.9322222222222225e-05,
      "loss": 0.0065,
      "step": 1220
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.022891314700245857,
      "learning_rate": 4.931666666666667e-05,
      "loss": 0.0069,
      "step": 1230
    },
    {
      "epoch": 0.06888888888888889,
      "grad_norm": 0.32345160841941833,
      "learning_rate": 4.931111111111111e-05,
      "loss": 0.0071,
      "step": 1240
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 0.5159492492675781,
      "learning_rate": 4.930555555555556e-05,
      "loss": 0.0069,
      "step": 1250
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.2575080096721649,
      "learning_rate": 4.93e-05,
      "loss": 0.0058,
      "step": 1260
    },
    {
      "epoch": 0.07055555555555555,
      "grad_norm": 0.19448170065879822,
      "learning_rate": 4.929444444444445e-05,
      "loss": 0.0065,
      "step": 1270
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.38820135593414307,
      "learning_rate": 4.928888888888889e-05,
      "loss": 0.0051,
      "step": 1280
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.021448994055390358,
      "learning_rate": 4.9283333333333336e-05,
      "loss": 0.0066,
      "step": 1290
    },
    {
      "epoch": 0.07222222222222222,
      "grad_norm": 0.7739097476005554,
      "learning_rate": 4.927777777777778e-05,
      "loss": 0.0052,
      "step": 1300
    },
    {
      "epoch": 0.07277777777777777,
      "grad_norm": 0.07096996158361435,
      "learning_rate": 4.9272222222222223e-05,
      "loss": 0.0062,
      "step": 1310
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.0696435198187828,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0057,
      "step": 1320
    },
    {
      "epoch": 0.07388888888888889,
      "grad_norm": 0.19641122221946716,
      "learning_rate": 4.926111111111111e-05,
      "loss": 0.0045,
      "step": 1330
    },
    {
      "epoch": 0.07444444444444444,
      "grad_norm": 0.2012431025505066,
      "learning_rate": 4.925555555555556e-05,
      "loss": 0.0072,
      "step": 1340
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.12958663702011108,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.0052,
      "step": 1350
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 0.38887298107147217,
      "learning_rate": 4.924444444444445e-05,
      "loss": 0.0053,
      "step": 1360
    },
    {
      "epoch": 0.07611111111111112,
      "grad_norm": 0.8395040035247803,
      "learning_rate": 4.923888888888889e-05,
      "loss": 0.0069,
      "step": 1370
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.07217284291982651,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.0065,
      "step": 1380
    },
    {
      "epoch": 0.07722222222222222,
      "grad_norm": 0.5191468000411987,
      "learning_rate": 4.922777777777778e-05,
      "loss": 0.0065,
      "step": 1390
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 0.012683815322816372,
      "learning_rate": 4.922222222222222e-05,
      "loss": 0.0051,
      "step": 1400
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.3305205702781677,
      "learning_rate": 4.9216666666666666e-05,
      "loss": 0.0077,
      "step": 1410
    },
    {
      "epoch": 0.07888888888888888,
      "grad_norm": 0.1947549432516098,
      "learning_rate": 4.9211111111111116e-05,
      "loss": 0.0075,
      "step": 1420
    },
    {
      "epoch": 0.07944444444444444,
      "grad_norm": 0.5808249711990356,
      "learning_rate": 4.920555555555556e-05,
      "loss": 0.0069,
      "step": 1430
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.2588074803352356,
      "learning_rate": 4.92e-05,
      "loss": 0.0064,
      "step": 1440
    },
    {
      "epoch": 0.08055555555555556,
      "grad_norm": 0.517139732837677,
      "learning_rate": 4.919444444444445e-05,
      "loss": 0.0069,
      "step": 1450
    },
    {
      "epoch": 0.0811111111111111,
      "grad_norm": 0.02455855719745159,
      "learning_rate": 4.918888888888889e-05,
      "loss": 0.0063,
      "step": 1460
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.3211531639099121,
      "learning_rate": 4.9183333333333334e-05,
      "loss": 0.0071,
      "step": 1470
    },
    {
      "epoch": 0.08222222222222222,
      "grad_norm": 0.32311710715293884,
      "learning_rate": 4.917777777777778e-05,
      "loss": 0.0049,
      "step": 1480
    },
    {
      "epoch": 0.08277777777777778,
      "grad_norm": 0.644072413444519,
      "learning_rate": 4.917222222222223e-05,
      "loss": 0.0048,
      "step": 1490
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.5800027847290039,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0059,
      "step": 1500
    },
    {
      "epoch": 0.08388888888888889,
      "grad_norm": 0.13334211707115173,
      "learning_rate": 4.9161111111111115e-05,
      "loss": 0.0052,
      "step": 1510
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 0.5154018998146057,
      "learning_rate": 4.915555555555556e-05,
      "loss": 0.0074,
      "step": 1520
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.4527391493320465,
      "learning_rate": 4.915e-05,
      "loss": 0.0081,
      "step": 1530
    },
    {
      "epoch": 0.08555555555555555,
      "grad_norm": 0.5160685777664185,
      "learning_rate": 4.9144444444444446e-05,
      "loss": 0.0058,
      "step": 1540
    },
    {
      "epoch": 0.08611111111111111,
      "grad_norm": 0.3895626366138458,
      "learning_rate": 4.913888888888889e-05,
      "loss": 0.006,
      "step": 1550
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.8403877019882202,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0083,
      "step": 1560
    },
    {
      "epoch": 0.08722222222222223,
      "grad_norm": 0.6439455151557922,
      "learning_rate": 4.9127777777777776e-05,
      "loss": 0.0055,
      "step": 1570
    },
    {
      "epoch": 0.08777777777777777,
      "grad_norm": 0.06608588993549347,
      "learning_rate": 4.912222222222223e-05,
      "loss": 0.007,
      "step": 1580
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.38752061128616333,
      "learning_rate": 4.9116666666666663e-05,
      "loss": 0.0059,
      "step": 1590
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.32229548692703247,
      "learning_rate": 4.9111111111111114e-05,
      "loss": 0.007,
      "step": 1600
    },
    {
      "epoch": 0.08944444444444444,
      "grad_norm": 0.7754039168357849,
      "learning_rate": 4.910555555555556e-05,
      "loss": 0.0063,
      "step": 1610
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.26086607575416565,
      "learning_rate": 4.91e-05,
      "loss": 0.0086,
      "step": 1620
    },
    {
      "epoch": 0.09055555555555556,
      "grad_norm": 0.19506914913654327,
      "learning_rate": 4.909444444444445e-05,
      "loss": 0.007,
      "step": 1630
    },
    {
      "epoch": 0.09111111111111111,
      "grad_norm": 0.19459253549575806,
      "learning_rate": 4.908888888888889e-05,
      "loss": 0.0077,
      "step": 1640
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.32230690121650696,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.0062,
      "step": 1650
    },
    {
      "epoch": 0.09222222222222222,
      "grad_norm": 0.00869563315063715,
      "learning_rate": 4.9077777777777775e-05,
      "loss": 0.0047,
      "step": 1660
    },
    {
      "epoch": 0.09277777777777778,
      "grad_norm": 0.3864993155002594,
      "learning_rate": 4.9072222222222225e-05,
      "loss": 0.0069,
      "step": 1670
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.4509717524051666,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0071,
      "step": 1680
    },
    {
      "epoch": 0.09388888888888888,
      "grad_norm": 0.6419957876205444,
      "learning_rate": 4.906111111111111e-05,
      "loss": 0.0061,
      "step": 1690
    },
    {
      "epoch": 0.09444444444444444,
      "grad_norm": 0.2575456500053406,
      "learning_rate": 4.905555555555556e-05,
      "loss": 0.0072,
      "step": 1700
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.13030093908309937,
      "learning_rate": 4.905e-05,
      "loss": 0.0057,
      "step": 1710
    },
    {
      "epoch": 0.09555555555555556,
      "grad_norm": 0.25650131702423096,
      "learning_rate": 4.904444444444445e-05,
      "loss": 0.0057,
      "step": 1720
    },
    {
      "epoch": 0.0961111111111111,
      "grad_norm": 0.0658002719283104,
      "learning_rate": 4.903888888888889e-05,
      "loss": 0.0068,
      "step": 1730
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.32144054770469666,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.0059,
      "step": 1740
    },
    {
      "epoch": 0.09722222222222222,
      "grad_norm": 0.2565799653530121,
      "learning_rate": 4.902777777777778e-05,
      "loss": 0.0085,
      "step": 1750
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 0.4482725262641907,
      "learning_rate": 4.9022222222222224e-05,
      "loss": 0.0059,
      "step": 1760
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.7685508728027344,
      "learning_rate": 4.901666666666667e-05,
      "loss": 0.0077,
      "step": 1770
    },
    {
      "epoch": 0.09888888888888889,
      "grad_norm": 0.12871287763118744,
      "learning_rate": 4.901111111111111e-05,
      "loss": 0.0075,
      "step": 1780
    },
    {
      "epoch": 0.09944444444444445,
      "grad_norm": 0.44829192757606506,
      "learning_rate": 4.900555555555556e-05,
      "loss": 0.0064,
      "step": 1790
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0648304894566536,
      "learning_rate": 4.9e-05,
      "loss": 0.0065,
      "step": 1800
    },
    {
      "epoch": 0.10055555555555555,
      "grad_norm": 0.5760021805763245,
      "learning_rate": 4.899444444444445e-05,
      "loss": 0.0065,
      "step": 1810
    },
    {
      "epoch": 0.10111111111111111,
      "grad_norm": 0.2565584182739258,
      "learning_rate": 4.898888888888889e-05,
      "loss": 0.0065,
      "step": 1820
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.016652431339025497,
      "learning_rate": 4.8983333333333336e-05,
      "loss": 0.0044,
      "step": 1830
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 0.06528041511774063,
      "learning_rate": 4.897777777777778e-05,
      "loss": 0.0056,
      "step": 1840
    },
    {
      "epoch": 0.10277777777777777,
      "grad_norm": 0.38336044549942017,
      "learning_rate": 4.897222222222222e-05,
      "loss": 0.0069,
      "step": 1850
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.01976422592997551,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.0066,
      "step": 1860
    },
    {
      "epoch": 0.10388888888888889,
      "grad_norm": 0.06557063013315201,
      "learning_rate": 4.896111111111111e-05,
      "loss": 0.0063,
      "step": 1870
    },
    {
      "epoch": 0.10444444444444445,
      "grad_norm": 0.32056254148483276,
      "learning_rate": 4.895555555555556e-05,
      "loss": 0.0089,
      "step": 1880
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.3838077485561371,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 0.0057,
      "step": 1890
    },
    {
      "epoch": 0.10555555555555556,
      "grad_norm": 0.06821893155574799,
      "learning_rate": 4.894444444444445e-05,
      "loss": 0.0079,
      "step": 1900
    },
    {
      "epoch": 0.10611111111111111,
      "grad_norm": 0.3197305202484131,
      "learning_rate": 4.893888888888889e-05,
      "loss": 0.0068,
      "step": 1910
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.25591138005256653,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.008,
      "step": 1920
    },
    {
      "epoch": 0.10722222222222222,
      "grad_norm": 0.0647430494427681,
      "learning_rate": 4.892777777777778e-05,
      "loss": 0.006,
      "step": 1930
    },
    {
      "epoch": 0.10777777777777778,
      "grad_norm": 0.31953299045562744,
      "learning_rate": 4.892222222222222e-05,
      "loss": 0.0059,
      "step": 1940
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.25663483142852783,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.0056,
      "step": 1950
    },
    {
      "epoch": 0.10888888888888888,
      "grad_norm": 0.3879888951778412,
      "learning_rate": 4.8911111111111116e-05,
      "loss": 0.0073,
      "step": 1960
    },
    {
      "epoch": 0.10944444444444444,
      "grad_norm": 0.38610705733299255,
      "learning_rate": 4.890555555555556e-05,
      "loss": 0.0063,
      "step": 1970
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.5130094885826111,
      "learning_rate": 4.89e-05,
      "loss": 0.0075,
      "step": 1980
    },
    {
      "epoch": 0.11055555555555556,
      "grad_norm": 0.5772138833999634,
      "learning_rate": 4.8894444444444446e-05,
      "loss": 0.0069,
      "step": 1990
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.19437327980995178,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.0075,
      "step": 2000
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.06626097857952118,
      "learning_rate": 4.8883333333333333e-05,
      "loss": 0.0051,
      "step": 2010
    },
    {
      "epoch": 0.11222222222222222,
      "grad_norm": 0.44806230068206787,
      "learning_rate": 4.887777777777778e-05,
      "loss": 0.0059,
      "step": 2020
    },
    {
      "epoch": 0.11277777777777778,
      "grad_norm": 0.1322193294763565,
      "learning_rate": 4.887222222222223e-05,
      "loss": 0.0074,
      "step": 2030
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.09136252105236053,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0062,
      "step": 2040
    },
    {
      "epoch": 0.11388888888888889,
      "grad_norm": 0.700826108455658,
      "learning_rate": 4.8861111111111114e-05,
      "loss": 0.0053,
      "step": 2050
    },
    {
      "epoch": 0.11444444444444445,
      "grad_norm": 0.6063262224197388,
      "learning_rate": 4.885555555555556e-05,
      "loss": 0.0053,
      "step": 2060
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.034932903945446014,
      "learning_rate": 4.885e-05,
      "loss": 0.0094,
      "step": 2070
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 0.25951331853866577,
      "learning_rate": 4.8844444444444445e-05,
      "loss": 0.0058,
      "step": 2080
    },
    {
      "epoch": 0.11611111111111111,
      "grad_norm": 0.10305071622133255,
      "learning_rate": 4.883888888888889e-05,
      "loss": 0.0081,
      "step": 2090
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.2557149827480316,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0058,
      "step": 2100
    },
    {
      "epoch": 0.11722222222222223,
      "grad_norm": 0.12877830862998962,
      "learning_rate": 4.8827777777777776e-05,
      "loss": 0.0065,
      "step": 2110
    },
    {
      "epoch": 0.11777777777777777,
      "grad_norm": 0.953582227230072,
      "learning_rate": 4.8822222222222226e-05,
      "loss": 0.0048,
      "step": 2120
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.02971402369439602,
      "learning_rate": 4.881666666666667e-05,
      "loss": 0.0056,
      "step": 2130
    },
    {
      "epoch": 0.11888888888888889,
      "grad_norm": 0.31962141394615173,
      "learning_rate": 4.881111111111111e-05,
      "loss": 0.0069,
      "step": 2140
    },
    {
      "epoch": 0.11944444444444445,
      "grad_norm": 0.3920481204986572,
      "learning_rate": 4.880555555555556e-05,
      "loss": 0.007,
      "step": 2150
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.3180220425128937,
      "learning_rate": 4.88e-05,
      "loss": 0.0076,
      "step": 2160
    },
    {
      "epoch": 0.12055555555555555,
      "grad_norm": 0.2607378363609314,
      "learning_rate": 4.879444444444445e-05,
      "loss": 0.0063,
      "step": 2170
    },
    {
      "epoch": 0.12111111111111111,
      "grad_norm": 0.12826982140541077,
      "learning_rate": 4.878888888888889e-05,
      "loss": 0.0058,
      "step": 2180
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.20327767729759216,
      "learning_rate": 4.878333333333334e-05,
      "loss": 0.0062,
      "step": 2190
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 0.06885045766830444,
      "learning_rate": 4.8777777777777775e-05,
      "loss": 0.0059,
      "step": 2200
    },
    {
      "epoch": 0.12277777777777778,
      "grad_norm": 0.5123737454414368,
      "learning_rate": 4.8772222222222225e-05,
      "loss": 0.006,
      "step": 2210
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.13157963752746582,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0064,
      "step": 2220
    },
    {
      "epoch": 0.1238888888888889,
      "grad_norm": 0.44499626755714417,
      "learning_rate": 4.876111111111111e-05,
      "loss": 0.0046,
      "step": 2230
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 0.31918007135391235,
      "learning_rate": 4.875555555555556e-05,
      "loss": 0.0063,
      "step": 2240
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.5098077654838562,
      "learning_rate": 4.875e-05,
      "loss": 0.0058,
      "step": 2250
    },
    {
      "epoch": 0.12555555555555556,
      "grad_norm": 0.19304102659225464,
      "learning_rate": 4.874444444444445e-05,
      "loss": 0.005,
      "step": 2260
    },
    {
      "epoch": 0.12611111111111112,
      "grad_norm": 0.123112253844738,
      "learning_rate": 4.8738888888888886e-05,
      "loss": 0.0061,
      "step": 2270
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.06910721212625504,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0078,
      "step": 2280
    },
    {
      "epoch": 0.1272222222222222,
      "grad_norm": 0.45275601744651794,
      "learning_rate": 4.872777777777778e-05,
      "loss": 0.0065,
      "step": 2290
    },
    {
      "epoch": 0.12777777777777777,
      "grad_norm": 0.15926077961921692,
      "learning_rate": 4.8722222222222224e-05,
      "loss": 0.0057,
      "step": 2300
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.3206714391708374,
      "learning_rate": 4.8716666666666674e-05,
      "loss": 0.0077,
      "step": 2310
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 0.14743036031723022,
      "learning_rate": 4.871111111111111e-05,
      "loss": 0.0049,
      "step": 2320
    },
    {
      "epoch": 0.12944444444444445,
      "grad_norm": 0.1284574419260025,
      "learning_rate": 4.870555555555556e-05,
      "loss": 0.005,
      "step": 2330
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.02710704319179058,
      "learning_rate": 4.87e-05,
      "loss": 0.0047,
      "step": 2340
    },
    {
      "epoch": 0.13055555555555556,
      "grad_norm": 0.20198574662208557,
      "learning_rate": 4.869444444444445e-05,
      "loss": 0.0062,
      "step": 2350
    },
    {
      "epoch": 0.13111111111111112,
      "grad_norm": 0.12746961414813995,
      "learning_rate": 4.868888888888889e-05,
      "loss": 0.0064,
      "step": 2360
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.06508155912160873,
      "learning_rate": 4.8683333333333335e-05,
      "loss": 0.0061,
      "step": 2370
    },
    {
      "epoch": 0.1322222222222222,
      "grad_norm": 0.06703647971153259,
      "learning_rate": 4.867777777777778e-05,
      "loss": 0.0062,
      "step": 2380
    },
    {
      "epoch": 0.13277777777777777,
      "grad_norm": 0.3833693861961365,
      "learning_rate": 4.867222222222222e-05,
      "loss": 0.0063,
      "step": 2390
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.32168734073638916,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0073,
      "step": 2400
    },
    {
      "epoch": 0.1338888888888889,
      "grad_norm": 0.38204023241996765,
      "learning_rate": 4.866111111111111e-05,
      "loss": 0.0037,
      "step": 2410
    },
    {
      "epoch": 0.13444444444444445,
      "grad_norm": 0.06492182612419128,
      "learning_rate": 4.865555555555556e-05,
      "loss": 0.0081,
      "step": 2420
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.8274421095848083,
      "learning_rate": 4.8650000000000003e-05,
      "loss": 0.0062,
      "step": 2430
    },
    {
      "epoch": 0.13555555555555557,
      "grad_norm": 0.5717412233352661,
      "learning_rate": 4.864444444444445e-05,
      "loss": 0.0062,
      "step": 2440
    },
    {
      "epoch": 0.1361111111111111,
      "grad_norm": 0.7002622485160828,
      "learning_rate": 4.863888888888889e-05,
      "loss": 0.0054,
      "step": 2450
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.3490535616874695,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0058,
      "step": 2460
    },
    {
      "epoch": 0.13722222222222222,
      "grad_norm": 0.6392121315002441,
      "learning_rate": 4.862777777777778e-05,
      "loss": 0.0046,
      "step": 2470
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 0.17148450016975403,
      "learning_rate": 4.862222222222222e-05,
      "loss": 0.0069,
      "step": 2480
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.44295454025268555,
      "learning_rate": 4.861666666666667e-05,
      "loss": 0.0052,
      "step": 2490
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 0.31751418113708496,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 0.0055,
      "step": 2500
    },
    {
      "epoch": 0.13944444444444445,
      "grad_norm": 0.31878361105918884,
      "learning_rate": 4.860555555555556e-05,
      "loss": 0.0067,
      "step": 2510
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.28606361150741577,
      "learning_rate": 4.86e-05,
      "loss": 0.006,
      "step": 2520
    },
    {
      "epoch": 0.14055555555555554,
      "grad_norm": 0.44388046860694885,
      "learning_rate": 4.8594444444444446e-05,
      "loss": 0.0052,
      "step": 2530
    },
    {
      "epoch": 0.1411111111111111,
      "grad_norm": 0.15811070799827576,
      "learning_rate": 4.858888888888889e-05,
      "loss": 0.0058,
      "step": 2540
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.160718634724617,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.0062,
      "step": 2550
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 0.1902252435684204,
      "learning_rate": 4.8577777777777776e-05,
      "loss": 0.0054,
      "step": 2560
    },
    {
      "epoch": 0.14277777777777778,
      "grad_norm": 0.6346954107284546,
      "learning_rate": 4.857222222222223e-05,
      "loss": 0.0054,
      "step": 2570
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.2553170323371887,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0059,
      "step": 2580
    },
    {
      "epoch": 0.1438888888888889,
      "grad_norm": 0.26671817898750305,
      "learning_rate": 4.8561111111111114e-05,
      "loss": 0.0076,
      "step": 2590
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 0.3283838927745819,
      "learning_rate": 4.855555555555556e-05,
      "loss": 0.0071,
      "step": 2600
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.7625128030776978,
      "learning_rate": 4.855e-05,
      "loss": 0.0072,
      "step": 2610
    },
    {
      "epoch": 0.14555555555555555,
      "grad_norm": 0.06465131044387817,
      "learning_rate": 4.8544444444444445e-05,
      "loss": 0.0056,
      "step": 2620
    },
    {
      "epoch": 0.1461111111111111,
      "grad_norm": 0.5391251444816589,
      "learning_rate": 4.853888888888889e-05,
      "loss": 0.0075,
      "step": 2630
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.3803156614303589,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0067,
      "step": 2640
    },
    {
      "epoch": 0.14722222222222223,
      "grad_norm": 0.4815380871295929,
      "learning_rate": 4.8527777777777775e-05,
      "loss": 0.0061,
      "step": 2650
    },
    {
      "epoch": 0.14777777777777779,
      "grad_norm": 0.8326540589332581,
      "learning_rate": 4.8522222222222226e-05,
      "loss": 0.0063,
      "step": 2660
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.027006154879927635,
      "learning_rate": 4.851666666666667e-05,
      "loss": 0.0064,
      "step": 2670
    },
    {
      "epoch": 0.14888888888888888,
      "grad_norm": 0.028303928673267365,
      "learning_rate": 4.851111111111111e-05,
      "loss": 0.0066,
      "step": 2680
    },
    {
      "epoch": 0.14944444444444444,
      "grad_norm": 0.22301523387432098,
      "learning_rate": 4.8505555555555556e-05,
      "loss": 0.0056,
      "step": 2690
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.20989890396595,
      "learning_rate": 4.85e-05,
      "loss": 0.0056,
      "step": 2700
    },
    {
      "epoch": 0.15055555555555555,
      "grad_norm": 0.253607839345932,
      "learning_rate": 4.849444444444445e-05,
      "loss": 0.0057,
      "step": 2710
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 0.1282443255186081,
      "learning_rate": 4.848888888888889e-05,
      "loss": 0.0064,
      "step": 2720
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.31589868664741516,
      "learning_rate": 4.848333333333334e-05,
      "loss": 0.0052,
      "step": 2730
    },
    {
      "epoch": 0.15222222222222223,
      "grad_norm": 0.16974911093711853,
      "learning_rate": 4.847777777777778e-05,
      "loss": 0.0056,
      "step": 2740
    },
    {
      "epoch": 0.1527777777777778,
      "grad_norm": 0.06456462293863297,
      "learning_rate": 4.8472222222222224e-05,
      "loss": 0.0063,
      "step": 2750
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.8255702257156372,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0059,
      "step": 2760
    },
    {
      "epoch": 0.15388888888888888,
      "grad_norm": 0.06521835923194885,
      "learning_rate": 4.846111111111111e-05,
      "loss": 0.0063,
      "step": 2770
    },
    {
      "epoch": 0.15444444444444444,
      "grad_norm": 0.3289656639099121,
      "learning_rate": 4.845555555555556e-05,
      "loss": 0.0062,
      "step": 2780
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.34821027517318726,
      "learning_rate": 4.845e-05,
      "loss": 0.0055,
      "step": 2790
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.22154906392097473,
      "learning_rate": 4.844444444444445e-05,
      "loss": 0.0046,
      "step": 2800
    },
    {
      "epoch": 0.15611111111111112,
      "grad_norm": 0.5080471634864807,
      "learning_rate": 4.843888888888889e-05,
      "loss": 0.0079,
      "step": 2810
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.570067286491394,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.0064,
      "step": 2820
    },
    {
      "epoch": 0.15722222222222224,
      "grad_norm": 0.379867821931839,
      "learning_rate": 4.842777777777778e-05,
      "loss": 0.0044,
      "step": 2830
    },
    {
      "epoch": 0.15777777777777777,
      "grad_norm": 0.06614601612091064,
      "learning_rate": 4.842222222222222e-05,
      "loss": 0.0052,
      "step": 2840
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.4194607138633728,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.0097,
      "step": 2850
    },
    {
      "epoch": 0.15888888888888889,
      "grad_norm": 0.4436803460121155,
      "learning_rate": 4.841111111111111e-05,
      "loss": 0.005,
      "step": 2860
    },
    {
      "epoch": 0.15944444444444444,
      "grad_norm": 0.5064931511878967,
      "learning_rate": 4.840555555555556e-05,
      "loss": 0.0065,
      "step": 2870
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.11018706113100052,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.006,
      "step": 2880
    },
    {
      "epoch": 0.16055555555555556,
      "grad_norm": 0.31558549404144287,
      "learning_rate": 4.839444444444445e-05,
      "loss": 0.0057,
      "step": 2890
    },
    {
      "epoch": 0.16111111111111112,
      "grad_norm": 0.09521260112524033,
      "learning_rate": 4.838888888888889e-05,
      "loss": 0.0085,
      "step": 2900
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.06645020842552185,
      "learning_rate": 4.8383333333333335e-05,
      "loss": 0.0049,
      "step": 2910
    },
    {
      "epoch": 0.1622222222222222,
      "grad_norm": 0.3183869421482086,
      "learning_rate": 4.837777777777778e-05,
      "loss": 0.0066,
      "step": 2920
    },
    {
      "epoch": 0.16277777777777777,
      "grad_norm": 0.8210739493370056,
      "learning_rate": 4.837222222222222e-05,
      "loss": 0.0063,
      "step": 2930
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.4421550929546356,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.0064,
      "step": 2940
    },
    {
      "epoch": 0.1638888888888889,
      "grad_norm": 0.1905328631401062,
      "learning_rate": 4.8361111111111116e-05,
      "loss": 0.0061,
      "step": 2950
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 0.2262762039899826,
      "learning_rate": 4.835555555555556e-05,
      "loss": 0.0053,
      "step": 2960
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.6958878040313721,
      "learning_rate": 4.835e-05,
      "loss": 0.0056,
      "step": 2970
    },
    {
      "epoch": 0.16555555555555557,
      "grad_norm": 0.5694372057914734,
      "learning_rate": 4.8344444444444447e-05,
      "loss": 0.0055,
      "step": 2980
    },
    {
      "epoch": 0.1661111111111111,
      "grad_norm": 0.3797112703323364,
      "learning_rate": 4.833888888888889e-05,
      "loss": 0.007,
      "step": 2990
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.6327360272407532,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0046,
      "step": 3000
    },
    {
      "epoch": 0.16722222222222222,
      "grad_norm": 0.3151898682117462,
      "learning_rate": 4.832777777777778e-05,
      "loss": 0.0067,
      "step": 3010
    },
    {
      "epoch": 0.16777777777777778,
      "grad_norm": 0.2549159526824951,
      "learning_rate": 4.832222222222223e-05,
      "loss": 0.0076,
      "step": 3020
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.0253846924751997,
      "learning_rate": 4.831666666666667e-05,
      "loss": 0.0065,
      "step": 3030
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 0.09515080600976944,
      "learning_rate": 4.8311111111111115e-05,
      "loss": 0.0071,
      "step": 3040
    },
    {
      "epoch": 0.16944444444444445,
      "grad_norm": 0.41003477573394775,
      "learning_rate": 4.830555555555556e-05,
      "loss": 0.0048,
      "step": 3050
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.31644406914711,
      "learning_rate": 4.83e-05,
      "loss": 0.0064,
      "step": 3060
    },
    {
      "epoch": 0.17055555555555554,
      "grad_norm": 0.4408847689628601,
      "learning_rate": 4.8294444444444445e-05,
      "loss": 0.0053,
      "step": 3070
    },
    {
      "epoch": 0.1711111111111111,
      "grad_norm": 0.1897139996290207,
      "learning_rate": 4.828888888888889e-05,
      "loss": 0.0093,
      "step": 3080
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 0.6912052631378174,
      "learning_rate": 4.828333333333334e-05,
      "loss": 0.0069,
      "step": 3090
    },
    {
      "epoch": 0.17222222222222222,
      "grad_norm": 0.3148197829723358,
      "learning_rate": 4.8277777777777776e-05,
      "loss": 0.0057,
      "step": 3100
    },
    {
      "epoch": 0.17277777777777778,
      "grad_norm": 0.21073634922504425,
      "learning_rate": 4.8272222222222226e-05,
      "loss": 0.0073,
      "step": 3110
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.21432086825370789,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0076,
      "step": 3120
    },
    {
      "epoch": 0.1738888888888889,
      "grad_norm": 0.25646334886550903,
      "learning_rate": 4.8261111111111113e-05,
      "loss": 0.0091,
      "step": 3130
    },
    {
      "epoch": 0.17444444444444446,
      "grad_norm": 0.18909335136413574,
      "learning_rate": 4.825555555555556e-05,
      "loss": 0.0054,
      "step": 3140
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.28493043780326843,
      "learning_rate": 4.825e-05,
      "loss": 0.004,
      "step": 3150
    },
    {
      "epoch": 0.17555555555555555,
      "grad_norm": 0.4390545189380646,
      "learning_rate": 4.824444444444445e-05,
      "loss": 0.0062,
      "step": 3160
    },
    {
      "epoch": 0.1761111111111111,
      "grad_norm": 0.0649518147110939,
      "learning_rate": 4.823888888888889e-05,
      "loss": 0.005,
      "step": 3170
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.25152379274368286,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.006,
      "step": 3180
    },
    {
      "epoch": 0.17722222222222223,
      "grad_norm": 0.1260003298521042,
      "learning_rate": 4.822777777777778e-05,
      "loss": 0.0046,
      "step": 3190
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.41046270728111267,
      "learning_rate": 4.8222222222222225e-05,
      "loss": 0.0068,
      "step": 3200
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.12668661773204803,
      "learning_rate": 4.821666666666667e-05,
      "loss": 0.0064,
      "step": 3210
    },
    {
      "epoch": 0.17888888888888888,
      "grad_norm": 0.06549422442913055,
      "learning_rate": 4.821111111111111e-05,
      "loss": 0.0057,
      "step": 3220
    },
    {
      "epoch": 0.17944444444444443,
      "grad_norm": 0.06413498520851135,
      "learning_rate": 4.820555555555556e-05,
      "loss": 0.0034,
      "step": 3230
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.18969973921775818,
      "learning_rate": 4.82e-05,
      "loss": 0.0052,
      "step": 3240
    },
    {
      "epoch": 0.18055555555555555,
      "grad_norm": 0.19022123515605927,
      "learning_rate": 4.819444444444445e-05,
      "loss": 0.0047,
      "step": 3250
    },
    {
      "epoch": 0.1811111111111111,
      "grad_norm": 0.1896505206823349,
      "learning_rate": 4.8188888888888886e-05,
      "loss": 0.0051,
      "step": 3260
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.13308311998844147,
      "learning_rate": 4.818333333333334e-05,
      "loss": 0.0056,
      "step": 3270
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 0.7532418966293335,
      "learning_rate": 4.817777777777778e-05,
      "loss": 0.0069,
      "step": 3280
    },
    {
      "epoch": 0.1827777777777778,
      "grad_norm": 0.1911095529794693,
      "learning_rate": 4.8172222222222224e-05,
      "loss": 0.0073,
      "step": 3290
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.6935168504714966,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.0049,
      "step": 3300
    },
    {
      "epoch": 0.18388888888888888,
      "grad_norm": 0.7557570338249207,
      "learning_rate": 4.816111111111111e-05,
      "loss": 0.0063,
      "step": 3310
    },
    {
      "epoch": 0.18444444444444444,
      "grad_norm": 0.12717916071414948,
      "learning_rate": 4.815555555555556e-05,
      "loss": 0.0071,
      "step": 3320
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.12557846307754517,
      "learning_rate": 4.815e-05,
      "loss": 0.0051,
      "step": 3330
    },
    {
      "epoch": 0.18555555555555556,
      "grad_norm": 0.38129013776779175,
      "learning_rate": 4.814444444444445e-05,
      "loss": 0.0061,
      "step": 3340
    },
    {
      "epoch": 0.18611111111111112,
      "grad_norm": 0.06358321011066437,
      "learning_rate": 4.813888888888889e-05,
      "loss": 0.006,
      "step": 3350
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.5026416182518005,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.007,
      "step": 3360
    },
    {
      "epoch": 0.18722222222222223,
      "grad_norm": 0.37821492552757263,
      "learning_rate": 4.8127777777777786e-05,
      "loss": 0.0054,
      "step": 3370
    },
    {
      "epoch": 0.18777777777777777,
      "grad_norm": 0.3164421021938324,
      "learning_rate": 4.812222222222222e-05,
      "loss": 0.0046,
      "step": 3380
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.5654362440109253,
      "learning_rate": 4.811666666666667e-05,
      "loss": 0.0057,
      "step": 3390
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.7541200518608093,
      "learning_rate": 4.811111111111111e-05,
      "loss": 0.0075,
      "step": 3400
    },
    {
      "epoch": 0.18944444444444444,
      "grad_norm": 0.5021746158599854,
      "learning_rate": 4.810555555555556e-05,
      "loss": 0.0077,
      "step": 3410
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.06473823636770248,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0053,
      "step": 3420
    },
    {
      "epoch": 0.19055555555555556,
      "grad_norm": 0.18863195180892944,
      "learning_rate": 4.809444444444445e-05,
      "loss": 0.004,
      "step": 3430
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 0.043644748628139496,
      "learning_rate": 4.808888888888889e-05,
      "loss": 0.0062,
      "step": 3440
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.3775474429130554,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.0034,
      "step": 3450
    },
    {
      "epoch": 0.1922222222222222,
      "grad_norm": 0.2515508830547333,
      "learning_rate": 4.8077777777777785e-05,
      "loss": 0.006,
      "step": 3460
    },
    {
      "epoch": 0.19277777777777777,
      "grad_norm": 0.8225107192993164,
      "learning_rate": 4.807222222222222e-05,
      "loss": 0.0058,
      "step": 3470
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.3148740231990814,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0059,
      "step": 3480
    },
    {
      "epoch": 0.1938888888888889,
      "grad_norm": 0.06363672018051147,
      "learning_rate": 4.8061111111111115e-05,
      "loss": 0.0075,
      "step": 3490
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 0.06359367817640305,
      "learning_rate": 4.805555555555556e-05,
      "loss": 0.0055,
      "step": 3500
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.5351234078407288,
      "learning_rate": 4.805e-05,
      "loss": 0.0059,
      "step": 3510
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 0.3145602345466614,
      "learning_rate": 4.8044444444444446e-05,
      "loss": 0.0052,
      "step": 3520
    },
    {
      "epoch": 0.19611111111111112,
      "grad_norm": 0.01617802307009697,
      "learning_rate": 4.803888888888889e-05,
      "loss": 0.0066,
      "step": 3530
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.1899237185716629,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.0066,
      "step": 3540
    },
    {
      "epoch": 0.19722222222222222,
      "grad_norm": 0.18874713778495789,
      "learning_rate": 4.8027777777777783e-05,
      "loss": 0.0071,
      "step": 3550
    },
    {
      "epoch": 0.19777777777777777,
      "grad_norm": 0.15374819934368134,
      "learning_rate": 4.802222222222223e-05,
      "loss": 0.0052,
      "step": 3560
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.44170260429382324,
      "learning_rate": 4.801666666666667e-05,
      "loss": 0.0071,
      "step": 3570
    },
    {
      "epoch": 0.1988888888888889,
      "grad_norm": 0.1276448369026184,
      "learning_rate": 4.8011111111111114e-05,
      "loss": 0.0046,
      "step": 3580
    },
    {
      "epoch": 0.19944444444444445,
      "grad_norm": 0.25207850337028503,
      "learning_rate": 4.800555555555556e-05,
      "loss": 0.0063,
      "step": 3590
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.25143855810165405,
      "learning_rate": 4.8e-05,
      "loss": 0.0075,
      "step": 3600
    },
    {
      "epoch": 0.20055555555555554,
      "grad_norm": 0.06471923738718033,
      "learning_rate": 4.7994444444444445e-05,
      "loss": 0.0054,
      "step": 3610
    },
    {
      "epoch": 0.2011111111111111,
      "grad_norm": 0.4079982340335846,
      "learning_rate": 4.798888888888889e-05,
      "loss": 0.0055,
      "step": 3620
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.12571696937084198,
      "learning_rate": 4.798333333333334e-05,
      "loss": 0.0049,
      "step": 3630
    },
    {
      "epoch": 0.20222222222222222,
      "grad_norm": 0.504867434501648,
      "learning_rate": 4.797777777777778e-05,
      "loss": 0.007,
      "step": 3640
    },
    {
      "epoch": 0.20277777777777778,
      "grad_norm": 0.4409339725971222,
      "learning_rate": 4.7972222222222226e-05,
      "loss": 0.0056,
      "step": 3650
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.3142406940460205,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0059,
      "step": 3660
    },
    {
      "epoch": 0.2038888888888889,
      "grad_norm": 0.18927638232707977,
      "learning_rate": 4.796111111111111e-05,
      "loss": 0.0074,
      "step": 3670
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 0.1888946294784546,
      "learning_rate": 4.7955555555555556e-05,
      "loss": 0.0053,
      "step": 3680
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.5978046655654907,
      "learning_rate": 4.795e-05,
      "loss": 0.0057,
      "step": 3690
    },
    {
      "epoch": 0.20555555555555555,
      "grad_norm": 0.7552091479301453,
      "learning_rate": 4.794444444444445e-05,
      "loss": 0.007,
      "step": 3700
    },
    {
      "epoch": 0.2061111111111111,
      "grad_norm": 0.44098538160324097,
      "learning_rate": 4.793888888888889e-05,
      "loss": 0.0064,
      "step": 3710
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.8549054265022278,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0057,
      "step": 3720
    },
    {
      "epoch": 0.20722222222222222,
      "grad_norm": 0.8166152238845825,
      "learning_rate": 4.792777777777778e-05,
      "loss": 0.0076,
      "step": 3730
    },
    {
      "epoch": 0.20777777777777778,
      "grad_norm": 0.15677118301391602,
      "learning_rate": 4.7922222222222225e-05,
      "loss": 0.0065,
      "step": 3740
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.6250749826431274,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.0056,
      "step": 3750
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 0.01619916409254074,
      "learning_rate": 4.791111111111111e-05,
      "loss": 0.0049,
      "step": 3760
    },
    {
      "epoch": 0.20944444444444443,
      "grad_norm": 0.2531057894229889,
      "learning_rate": 4.790555555555556e-05,
      "loss": 0.0034,
      "step": 3770
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.3770310878753662,
      "learning_rate": 4.79e-05,
      "loss": 0.0067,
      "step": 3780
    },
    {
      "epoch": 0.21055555555555555,
      "grad_norm": 0.12727415561676025,
      "learning_rate": 4.789444444444445e-05,
      "loss": 0.0056,
      "step": 3790
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 0.12576764822006226,
      "learning_rate": 4.7888888888888886e-05,
      "loss": 0.0075,
      "step": 3800
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.12730447947978973,
      "learning_rate": 4.7883333333333336e-05,
      "loss": 0.0052,
      "step": 3810
    },
    {
      "epoch": 0.21222222222222223,
      "grad_norm": 0.06898917257785797,
      "learning_rate": 4.787777777777778e-05,
      "loss": 0.0066,
      "step": 3820
    },
    {
      "epoch": 0.2127777777777778,
      "grad_norm": 0.3134089708328247,
      "learning_rate": 4.787222222222222e-05,
      "loss": 0.0081,
      "step": 3830
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.18948791921138763,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0055,
      "step": 3840
    },
    {
      "epoch": 0.21388888888888888,
      "grad_norm": 0.3182371258735657,
      "learning_rate": 4.786111111111111e-05,
      "loss": 0.0059,
      "step": 3850
    },
    {
      "epoch": 0.21444444444444444,
      "grad_norm": 0.8161631226539612,
      "learning_rate": 4.785555555555556e-05,
      "loss": 0.0066,
      "step": 3860
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.12571978569030762,
      "learning_rate": 4.785e-05,
      "loss": 0.0066,
      "step": 3870
    },
    {
      "epoch": 0.21555555555555556,
      "grad_norm": 0.31369808316230774,
      "learning_rate": 4.784444444444445e-05,
      "loss": 0.0057,
      "step": 3880
    },
    {
      "epoch": 0.21611111111111111,
      "grad_norm": 0.12691201269626617,
      "learning_rate": 4.783888888888889e-05,
      "loss": 0.0051,
      "step": 3890
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.1270771026611328,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0054,
      "step": 3900
    },
    {
      "epoch": 0.21722222222222223,
      "grad_norm": 0.019934257492423058,
      "learning_rate": 4.7827777777777785e-05,
      "loss": 0.0067,
      "step": 3910
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 0.5981634855270386,
      "learning_rate": 4.782222222222222e-05,
      "loss": 0.0047,
      "step": 3920
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.6899541020393372,
      "learning_rate": 4.781666666666667e-05,
      "loss": 0.0081,
      "step": 3930
    },
    {
      "epoch": 0.21888888888888888,
      "grad_norm": 0.06340891122817993,
      "learning_rate": 4.781111111111111e-05,
      "loss": 0.0039,
      "step": 3940
    },
    {
      "epoch": 0.21944444444444444,
      "grad_norm": 0.015334399417042732,
      "learning_rate": 4.780555555555556e-05,
      "loss": 0.0053,
      "step": 3950
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.6869633197784424,
      "learning_rate": 4.78e-05,
      "loss": 0.0069,
      "step": 3960
    },
    {
      "epoch": 0.22055555555555556,
      "grad_norm": 0.31214043498039246,
      "learning_rate": 4.779444444444445e-05,
      "loss": 0.006,
      "step": 3970
    },
    {
      "epoch": 0.22111111111111112,
      "grad_norm": 0.12613089382648468,
      "learning_rate": 4.778888888888889e-05,
      "loss": 0.0038,
      "step": 3980
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.1256100833415985,
      "learning_rate": 4.7783333333333334e-05,
      "loss": 0.0053,
      "step": 3990
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.06654912233352661,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 0.0059,
      "step": 4000
    },
    {
      "epoch": 0.22277777777777777,
      "grad_norm": 0.37511342763900757,
      "learning_rate": 4.777222222222222e-05,
      "loss": 0.0066,
      "step": 4010
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.33651164174079895,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0052,
      "step": 4020
    },
    {
      "epoch": 0.2238888888888889,
      "grad_norm": 0.6252207159996033,
      "learning_rate": 4.7761111111111115e-05,
      "loss": 0.0054,
      "step": 4030
    },
    {
      "epoch": 0.22444444444444445,
      "grad_norm": 0.059218309819698334,
      "learning_rate": 4.775555555555556e-05,
      "loss": 0.0065,
      "step": 4040
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.21986991167068481,
      "learning_rate": 4.775e-05,
      "loss": 0.0057,
      "step": 4050
    },
    {
      "epoch": 0.22555555555555556,
      "grad_norm": 0.09541100263595581,
      "learning_rate": 4.7744444444444445e-05,
      "loss": 0.0061,
      "step": 4060
    },
    {
      "epoch": 0.22611111111111112,
      "grad_norm": 0.025105096399784088,
      "learning_rate": 4.773888888888889e-05,
      "loss": 0.0057,
      "step": 4070
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.3761584460735321,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0057,
      "step": 4080
    },
    {
      "epoch": 0.22722222222222221,
      "grad_norm": 0.06296069175004959,
      "learning_rate": 4.772777777777778e-05,
      "loss": 0.0057,
      "step": 4090
    },
    {
      "epoch": 0.22777777777777777,
      "grad_norm": 0.12680082023143768,
      "learning_rate": 4.7722222222222226e-05,
      "loss": 0.0083,
      "step": 4100
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.06318123638629913,
      "learning_rate": 4.771666666666667e-05,
      "loss": 0.0067,
      "step": 4110
    },
    {
      "epoch": 0.2288888888888889,
      "grad_norm": 0.1566685289144516,
      "learning_rate": 4.7711111111111114e-05,
      "loss": 0.0048,
      "step": 4120
    },
    {
      "epoch": 0.22944444444444445,
      "grad_norm": 0.0633118525147438,
      "learning_rate": 4.770555555555556e-05,
      "loss": 0.0063,
      "step": 4130
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.5625069737434387,
      "learning_rate": 4.77e-05,
      "loss": 0.0057,
      "step": 4140
    },
    {
      "epoch": 0.23055555555555557,
      "grad_norm": 0.3139288127422333,
      "learning_rate": 4.7694444444444444e-05,
      "loss": 0.0075,
      "step": 4150
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.9053276777267456,
      "learning_rate": 4.768888888888889e-05,
      "loss": 0.0044,
      "step": 4160
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 0.5004347562789917,
      "learning_rate": 4.768333333333334e-05,
      "loss": 0.0057,
      "step": 4170
    },
    {
      "epoch": 0.23222222222222222,
      "grad_norm": 0.1872919499874115,
      "learning_rate": 4.767777777777778e-05,
      "loss": 0.0073,
      "step": 4180
    },
    {
      "epoch": 0.23277777777777778,
      "grad_norm": 0.1875125914812088,
      "learning_rate": 4.7672222222222225e-05,
      "loss": 0.0051,
      "step": 4190
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.06297551840543747,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0072,
      "step": 4200
    },
    {
      "epoch": 0.2338888888888889,
      "grad_norm": 0.187148317694664,
      "learning_rate": 4.766111111111111e-05,
      "loss": 0.0077,
      "step": 4210
    },
    {
      "epoch": 0.23444444444444446,
      "grad_norm": 0.43622127175331116,
      "learning_rate": 4.7655555555555556e-05,
      "loss": 0.007,
      "step": 4220
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.25004956126213074,
      "learning_rate": 4.765e-05,
      "loss": 0.0058,
      "step": 4230
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 0.09505750238895416,
      "learning_rate": 4.764444444444445e-05,
      "loss": 0.0062,
      "step": 4240
    },
    {
      "epoch": 0.2361111111111111,
      "grad_norm": 0.7190635800361633,
      "learning_rate": 4.7638888888888887e-05,
      "loss": 0.0054,
      "step": 4250
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.25058504939079285,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.0073,
      "step": 4260
    },
    {
      "epoch": 0.23722222222222222,
      "grad_norm": 0.686744213104248,
      "learning_rate": 4.762777777777778e-05,
      "loss": 0.0062,
      "step": 4270
    },
    {
      "epoch": 0.23777777777777778,
      "grad_norm": 0.6480388045310974,
      "learning_rate": 4.7622222222222224e-05,
      "loss": 0.0079,
      "step": 4280
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.08531030267477036,
      "learning_rate": 4.761666666666667e-05,
      "loss": 0.0041,
      "step": 4290
    },
    {
      "epoch": 0.2388888888888889,
      "grad_norm": 0.6592313051223755,
      "learning_rate": 4.761111111111111e-05,
      "loss": 0.0067,
      "step": 4300
    },
    {
      "epoch": 0.23944444444444443,
      "grad_norm": 0.12733130156993866,
      "learning_rate": 4.760555555555556e-05,
      "loss": 0.006,
      "step": 4310
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.12635387480258942,
      "learning_rate": 4.76e-05,
      "loss": 0.0084,
      "step": 4320
    },
    {
      "epoch": 0.24055555555555555,
      "grad_norm": 0.09456972032785416,
      "learning_rate": 4.759444444444445e-05,
      "loss": 0.0061,
      "step": 4330
    },
    {
      "epoch": 0.2411111111111111,
      "grad_norm": 0.06357362121343613,
      "learning_rate": 4.7588888888888885e-05,
      "loss": 0.0062,
      "step": 4340
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.2183692306280136,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.0056,
      "step": 4350
    },
    {
      "epoch": 0.24222222222222223,
      "grad_norm": 0.15076351165771484,
      "learning_rate": 4.757777777777778e-05,
      "loss": 0.0064,
      "step": 4360
    },
    {
      "epoch": 0.2427777777777778,
      "grad_norm": 0.31214815378189087,
      "learning_rate": 4.757222222222222e-05,
      "loss": 0.0071,
      "step": 4370
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.15661832690238953,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.0058,
      "step": 4380
    },
    {
      "epoch": 0.24388888888888888,
      "grad_norm": 0.3754657506942749,
      "learning_rate": 4.756111111111111e-05,
      "loss": 0.0092,
      "step": 4390
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.13433121144771576,
      "learning_rate": 4.755555555555556e-05,
      "loss": 0.0053,
      "step": 4400
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.5011753439903259,
      "learning_rate": 4.755e-05,
      "loss": 0.0091,
      "step": 4410
    },
    {
      "epoch": 0.24555555555555555,
      "grad_norm": 0.688014030456543,
      "learning_rate": 4.754444444444445e-05,
      "loss": 0.0052,
      "step": 4420
    },
    {
      "epoch": 0.2461111111111111,
      "grad_norm": 0.4391607344150543,
      "learning_rate": 4.753888888888889e-05,
      "loss": 0.0075,
      "step": 4430
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.3754462003707886,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0044,
      "step": 4440
    },
    {
      "epoch": 0.24722222222222223,
      "grad_norm": 0.25185874104499817,
      "learning_rate": 4.7527777777777785e-05,
      "loss": 0.0049,
      "step": 4450
    },
    {
      "epoch": 0.2477777777777778,
      "grad_norm": 0.46876829862594604,
      "learning_rate": 4.752222222222222e-05,
      "loss": 0.0059,
      "step": 4460
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.50067538022995,
      "learning_rate": 4.751666666666667e-05,
      "loss": 0.0072,
      "step": 4470
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.26790550351142883,
      "learning_rate": 4.751111111111111e-05,
      "loss": 0.0074,
      "step": 4480
    },
    {
      "epoch": 0.24944444444444444,
      "grad_norm": 0.06334614753723145,
      "learning_rate": 4.750555555555556e-05,
      "loss": 0.0066,
      "step": 4490
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.37591132521629333,
      "learning_rate": 4.75e-05,
      "loss": 0.0051,
      "step": 4500
    },
    {
      "epoch": 0.25055555555555553,
      "grad_norm": 0.3164188861846924,
      "learning_rate": 4.7494444444444446e-05,
      "loss": 0.0061,
      "step": 4510
    },
    {
      "epoch": 0.2511111111111111,
      "grad_norm": 0.3123011887073517,
      "learning_rate": 4.7488888888888897e-05,
      "loss": 0.0061,
      "step": 4520
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.30803388357162476,
      "learning_rate": 4.748333333333333e-05,
      "loss": 0.0058,
      "step": 4530
    },
    {
      "epoch": 0.25222222222222224,
      "grad_norm": 0.12527704238891602,
      "learning_rate": 4.7477777777777784e-05,
      "loss": 0.0059,
      "step": 4540
    },
    {
      "epoch": 0.25277777777777777,
      "grad_norm": 0.25082698464393616,
      "learning_rate": 4.747222222222222e-05,
      "loss": 0.0062,
      "step": 4550
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.25025296211242676,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0066,
      "step": 4560
    },
    {
      "epoch": 0.2538888888888889,
      "grad_norm": 0.3749985992908478,
      "learning_rate": 4.7461111111111114e-05,
      "loss": 0.0061,
      "step": 4570
    },
    {
      "epoch": 0.2544444444444444,
      "grad_norm": 0.313102662563324,
      "learning_rate": 4.745555555555556e-05,
      "loss": 0.0065,
      "step": 4580
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.17824149131774902,
      "learning_rate": 4.745e-05,
      "loss": 0.0061,
      "step": 4590
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 0.2856138348579407,
      "learning_rate": 4.7444444444444445e-05,
      "loss": 0.0069,
      "step": 4600
    },
    {
      "epoch": 0.2561111111111111,
      "grad_norm": 0.3741605281829834,
      "learning_rate": 4.7438888888888895e-05,
      "loss": 0.0051,
      "step": 4610
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.12676161527633667,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0054,
      "step": 4620
    },
    {
      "epoch": 0.25722222222222224,
      "grad_norm": 0.6208437085151672,
      "learning_rate": 4.742777777777778e-05,
      "loss": 0.0063,
      "step": 4630
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 0.4997737407684326,
      "learning_rate": 4.7422222222222226e-05,
      "loss": 0.0073,
      "step": 4640
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.49959930777549744,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.0066,
      "step": 4650
    },
    {
      "epoch": 0.2588888888888889,
      "grad_norm": 0.5635547637939453,
      "learning_rate": 4.741111111111111e-05,
      "loss": 0.005,
      "step": 4660
    },
    {
      "epoch": 0.2594444444444444,
      "grad_norm": 0.562104344367981,
      "learning_rate": 4.740555555555556e-05,
      "loss": 0.0078,
      "step": 4670
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.18713146448135376,
      "learning_rate": 4.74e-05,
      "loss": 0.0062,
      "step": 4680
    },
    {
      "epoch": 0.26055555555555554,
      "grad_norm": 0.37437519431114197,
      "learning_rate": 4.7394444444444444e-05,
      "loss": 0.0074,
      "step": 4690
    },
    {
      "epoch": 0.2611111111111111,
      "grad_norm": 0.18734440207481384,
      "learning_rate": 4.7388888888888894e-05,
      "loss": 0.0061,
      "step": 4700
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.258192777633667,
      "learning_rate": 4.738333333333334e-05,
      "loss": 0.007,
      "step": 4710
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 0.09456519782543182,
      "learning_rate": 4.737777777777778e-05,
      "loss": 0.0047,
      "step": 4720
    },
    {
      "epoch": 0.2627777777777778,
      "grad_norm": 0.21892599761486053,
      "learning_rate": 4.7372222222222225e-05,
      "loss": 0.0062,
      "step": 4730
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.06861545890569687,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.0078,
      "step": 4740
    },
    {
      "epoch": 0.2638888888888889,
      "grad_norm": 0.24913661181926727,
      "learning_rate": 4.736111111111111e-05,
      "loss": 0.0048,
      "step": 4750
    },
    {
      "epoch": 0.2644444444444444,
      "grad_norm": 0.6857064962387085,
      "learning_rate": 4.7355555555555555e-05,
      "loss": 0.0087,
      "step": 4760
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.3740755319595337,
      "learning_rate": 4.735e-05,
      "loss": 0.0061,
      "step": 4770
    },
    {
      "epoch": 0.26555555555555554,
      "grad_norm": 0.4976278841495514,
      "learning_rate": 4.734444444444445e-05,
      "loss": 0.0072,
      "step": 4780
    },
    {
      "epoch": 0.26611111111111113,
      "grad_norm": 0.3107330799102783,
      "learning_rate": 4.733888888888889e-05,
      "loss": 0.0064,
      "step": 4790
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.37517234683036804,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0061,
      "step": 4800
    },
    {
      "epoch": 0.26722222222222225,
      "grad_norm": 0.4366281032562256,
      "learning_rate": 4.732777777777778e-05,
      "loss": 0.0036,
      "step": 4810
    },
    {
      "epoch": 0.2677777777777778,
      "grad_norm": 0.1259540617465973,
      "learning_rate": 4.7322222222222224e-05,
      "loss": 0.0063,
      "step": 4820
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 0.5611881017684937,
      "learning_rate": 4.731666666666667e-05,
      "loss": 0.006,
      "step": 4830
    },
    {
      "epoch": 0.2688888888888889,
      "grad_norm": 0.1262056827545166,
      "learning_rate": 4.731111111111111e-05,
      "loss": 0.0033,
      "step": 4840
    },
    {
      "epoch": 0.26944444444444443,
      "grad_norm": 0.01230598147958517,
      "learning_rate": 4.730555555555556e-05,
      "loss": 0.0067,
      "step": 4850
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.02219073846936226,
      "learning_rate": 4.73e-05,
      "loss": 0.006,
      "step": 4860
    },
    {
      "epoch": 0.27055555555555555,
      "grad_norm": 0.26849761605262756,
      "learning_rate": 4.729444444444445e-05,
      "loss": 0.0063,
      "step": 4870
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 0.06386765837669373,
      "learning_rate": 4.728888888888889e-05,
      "loss": 0.0055,
      "step": 4880
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.2182367593050003,
      "learning_rate": 4.7283333333333335e-05,
      "loss": 0.0054,
      "step": 4890
    },
    {
      "epoch": 0.2722222222222222,
      "grad_norm": 0.09352173656225204,
      "learning_rate": 4.727777777777778e-05,
      "loss": 0.0078,
      "step": 4900
    },
    {
      "epoch": 0.2727777777777778,
      "grad_norm": 0.3437351882457733,
      "learning_rate": 4.727222222222222e-05,
      "loss": 0.0059,
      "step": 4910
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.022597629576921463,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0052,
      "step": 4920
    },
    {
      "epoch": 0.2738888888888889,
      "grad_norm": 0.342672199010849,
      "learning_rate": 4.726111111111111e-05,
      "loss": 0.0059,
      "step": 4930
    },
    {
      "epoch": 0.27444444444444444,
      "grad_norm": 0.15089090168476105,
      "learning_rate": 4.725555555555556e-05,
      "loss": 0.0064,
      "step": 4940
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.12454362213611603,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.0072,
      "step": 4950
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 0.42658549547195435,
      "learning_rate": 4.724444444444445e-05,
      "loss": 0.0065,
      "step": 4960
    },
    {
      "epoch": 0.2761111111111111,
      "grad_norm": 0.24902988970279694,
      "learning_rate": 4.723888888888889e-05,
      "loss": 0.0066,
      "step": 4970
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.18712058663368225,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0052,
      "step": 4980
    },
    {
      "epoch": 0.2772222222222222,
      "grad_norm": 0.18697203695774078,
      "learning_rate": 4.7227777777777784e-05,
      "loss": 0.0073,
      "step": 4990
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.5144912004470825,
      "learning_rate": 4.722222222222222e-05,
      "loss": 0.0042,
      "step": 5000
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.37373387813568115,
      "learning_rate": 4.721666666666667e-05,
      "loss": 0.0052,
      "step": 5010
    },
    {
      "epoch": 0.2788888888888889,
      "grad_norm": 0.09851811081171036,
      "learning_rate": 4.721111111111111e-05,
      "loss": 0.0056,
      "step": 5020
    },
    {
      "epoch": 0.27944444444444444,
      "grad_norm": 0.06471375375986099,
      "learning_rate": 4.720555555555556e-05,
      "loss": 0.0064,
      "step": 5030
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3112511932849884,
      "learning_rate": 4.72e-05,
      "loss": 0.0047,
      "step": 5040
    },
    {
      "epoch": 0.28055555555555556,
      "grad_norm": 0.06362274289131165,
      "learning_rate": 4.7194444444444446e-05,
      "loss": 0.007,
      "step": 5050
    },
    {
      "epoch": 0.2811111111111111,
      "grad_norm": 0.2496560662984848,
      "learning_rate": 4.7188888888888896e-05,
      "loss": 0.0062,
      "step": 5060
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.12395621091127396,
      "learning_rate": 4.718333333333333e-05,
      "loss": 0.0065,
      "step": 5070
    },
    {
      "epoch": 0.2822222222222222,
      "grad_norm": 0.49725133180618286,
      "learning_rate": 4.717777777777778e-05,
      "loss": 0.0058,
      "step": 5080
    },
    {
      "epoch": 0.2827777777777778,
      "grad_norm": 0.06326404958963394,
      "learning_rate": 4.717222222222222e-05,
      "loss": 0.0055,
      "step": 5090
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.15551190078258514,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0052,
      "step": 5100
    },
    {
      "epoch": 0.2838888888888889,
      "grad_norm": 0.1738201379776001,
      "learning_rate": 4.7161111111111114e-05,
      "loss": 0.0045,
      "step": 5110
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.01836942695081234,
      "learning_rate": 4.715555555555556e-05,
      "loss": 0.0055,
      "step": 5120
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.12596993148326874,
      "learning_rate": 4.715e-05,
      "loss": 0.0078,
      "step": 5130
    },
    {
      "epoch": 0.28555555555555556,
      "grad_norm": 0.1310831606388092,
      "learning_rate": 4.7144444444444444e-05,
      "loss": 0.0053,
      "step": 5140
    },
    {
      "epoch": 0.2861111111111111,
      "grad_norm": 0.37205618619918823,
      "learning_rate": 4.7138888888888895e-05,
      "loss": 0.005,
      "step": 5150
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.18609236180782318,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0059,
      "step": 5160
    },
    {
      "epoch": 0.2872222222222222,
      "grad_norm": 0.24790963530540466,
      "learning_rate": 4.712777777777778e-05,
      "loss": 0.0039,
      "step": 5170
    },
    {
      "epoch": 0.2877777777777778,
      "grad_norm": 0.3726688027381897,
      "learning_rate": 4.7122222222222225e-05,
      "loss": 0.0048,
      "step": 5180
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.40320447087287903,
      "learning_rate": 4.711666666666667e-05,
      "loss": 0.0064,
      "step": 5190
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.6833801865577698,
      "learning_rate": 4.711111111111111e-05,
      "loss": 0.0051,
      "step": 5200
    },
    {
      "epoch": 0.28944444444444445,
      "grad_norm": 0.31059977412223816,
      "learning_rate": 4.7105555555555556e-05,
      "loss": 0.0069,
      "step": 5210
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.12588965892791748,
      "learning_rate": 4.71e-05,
      "loss": 0.0077,
      "step": 5220
    },
    {
      "epoch": 0.29055555555555557,
      "grad_norm": 0.21898560225963593,
      "learning_rate": 4.709444444444444e-05,
      "loss": 0.0074,
      "step": 5230
    },
    {
      "epoch": 0.2911111111111111,
      "grad_norm": 0.21985585987567902,
      "learning_rate": 4.7088888888888894e-05,
      "loss": 0.0053,
      "step": 5240
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.3418247699737549,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.0065,
      "step": 5250
    },
    {
      "epoch": 0.2922222222222222,
      "grad_norm": 0.12617778778076172,
      "learning_rate": 4.707777777777778e-05,
      "loss": 0.0056,
      "step": 5260
    },
    {
      "epoch": 0.2927777777777778,
      "grad_norm": 0.027471911162137985,
      "learning_rate": 4.7072222222222224e-05,
      "loss": 0.0061,
      "step": 5270
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.18800592422485352,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0054,
      "step": 5280
    },
    {
      "epoch": 0.29388888888888887,
      "grad_norm": 0.6836704611778259,
      "learning_rate": 4.706111111111111e-05,
      "loss": 0.0074,
      "step": 5290
    },
    {
      "epoch": 0.29444444444444445,
      "grad_norm": 0.3120605945587158,
      "learning_rate": 4.7055555555555555e-05,
      "loss": 0.0047,
      "step": 5300
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.3733460605144501,
      "learning_rate": 4.705e-05,
      "loss": 0.0066,
      "step": 5310
    },
    {
      "epoch": 0.29555555555555557,
      "grad_norm": 0.3551696538925171,
      "learning_rate": 4.704444444444445e-05,
      "loss": 0.0078,
      "step": 5320
    },
    {
      "epoch": 0.2961111111111111,
      "grad_norm": 0.4353522062301636,
      "learning_rate": 4.703888888888889e-05,
      "loss": 0.0057,
      "step": 5330
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.19453738629817963,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.0079,
      "step": 5340
    },
    {
      "epoch": 0.2972222222222222,
      "grad_norm": 0.560080885887146,
      "learning_rate": 4.702777777777778e-05,
      "loss": 0.0057,
      "step": 5350
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 0.744800865650177,
      "learning_rate": 4.702222222222222e-05,
      "loss": 0.0053,
      "step": 5360
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.5586296916007996,
      "learning_rate": 4.701666666666667e-05,
      "loss": 0.0069,
      "step": 5370
    },
    {
      "epoch": 0.29888888888888887,
      "grad_norm": 0.12426745891571045,
      "learning_rate": 4.701111111111111e-05,
      "loss": 0.0059,
      "step": 5380
    },
    {
      "epoch": 0.29944444444444446,
      "grad_norm": 0.6517220735549927,
      "learning_rate": 4.700555555555556e-05,
      "loss": 0.006,
      "step": 5390
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.5581066012382507,
      "learning_rate": 4.7e-05,
      "loss": 0.0055,
      "step": 5400
    },
    {
      "epoch": 0.3005555555555556,
      "grad_norm": 0.06250225007534027,
      "learning_rate": 4.699444444444445e-05,
      "loss": 0.0058,
      "step": 5410
    },
    {
      "epoch": 0.3011111111111111,
      "grad_norm": 0.0239628367125988,
      "learning_rate": 4.698888888888889e-05,
      "loss": 0.0049,
      "step": 5420
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.3091171085834503,
      "learning_rate": 4.6983333333333335e-05,
      "loss": 0.006,
      "step": 5430
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 0.1862783133983612,
      "learning_rate": 4.6977777777777785e-05,
      "loss": 0.0052,
      "step": 5440
    },
    {
      "epoch": 0.30277777777777776,
      "grad_norm": 0.01904268004000187,
      "learning_rate": 4.697222222222222e-05,
      "loss": 0.006,
      "step": 5450
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 0.24867376685142517,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0046,
      "step": 5460
    },
    {
      "epoch": 0.3038888888888889,
      "grad_norm": 0.01393700111657381,
      "learning_rate": 4.696111111111111e-05,
      "loss": 0.0071,
      "step": 5470
    },
    {
      "epoch": 0.30444444444444446,
      "grad_norm": 0.3317139744758606,
      "learning_rate": 4.695555555555556e-05,
      "loss": 0.0069,
      "step": 5480
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.1253773719072342,
      "learning_rate": 4.695e-05,
      "loss": 0.0054,
      "step": 5490
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 0.49630141258239746,
      "learning_rate": 4.6944444444444446e-05,
      "loss": 0.0079,
      "step": 5500
    },
    {
      "epoch": 0.3061111111111111,
      "grad_norm": 0.4954838454723358,
      "learning_rate": 4.69388888888889e-05,
      "loss": 0.0072,
      "step": 5510
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.7419805526733398,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.007,
      "step": 5520
    },
    {
      "epoch": 0.30722222222222223,
      "grad_norm": 0.3101823329925537,
      "learning_rate": 4.6927777777777784e-05,
      "loss": 0.0065,
      "step": 5530
    },
    {
      "epoch": 0.30777777777777776,
      "grad_norm": 0.12528516352176666,
      "learning_rate": 4.692222222222222e-05,
      "loss": 0.006,
      "step": 5540
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.3099961578845978,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.0044,
      "step": 5550
    },
    {
      "epoch": 0.3088888888888889,
      "grad_norm": 0.5575579404830933,
      "learning_rate": 4.6911111111111114e-05,
      "loss": 0.0061,
      "step": 5560
    },
    {
      "epoch": 0.30944444444444447,
      "grad_norm": 0.5577303767204285,
      "learning_rate": 4.690555555555556e-05,
      "loss": 0.0048,
      "step": 5570
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.033976294100284576,
      "learning_rate": 4.69e-05,
      "loss": 0.0069,
      "step": 5580
    },
    {
      "epoch": 0.31055555555555553,
      "grad_norm": 0.1247967854142189,
      "learning_rate": 4.6894444444444445e-05,
      "loss": 0.0052,
      "step": 5590
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.5573112964630127,
      "learning_rate": 4.6888888888888895e-05,
      "loss": 0.0049,
      "step": 5600
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 0.2473655343055725,
      "learning_rate": 4.688333333333333e-05,
      "loss": 0.0055,
      "step": 5610
    },
    {
      "epoch": 0.31222222222222223,
      "grad_norm": 0.4022037088871002,
      "learning_rate": 4.687777777777778e-05,
      "loss": 0.0062,
      "step": 5620
    },
    {
      "epoch": 0.31277777777777777,
      "grad_norm": 0.6801818013191223,
      "learning_rate": 4.6872222222222226e-05,
      "loss": 0.0041,
      "step": 5630
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.37122848629951477,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0073,
      "step": 5640
    },
    {
      "epoch": 0.3138888888888889,
      "grad_norm": 0.49562978744506836,
      "learning_rate": 4.686111111111111e-05,
      "loss": 0.0051,
      "step": 5650
    },
    {
      "epoch": 0.31444444444444447,
      "grad_norm": 0.12360340356826782,
      "learning_rate": 4.685555555555556e-05,
      "loss": 0.0048,
      "step": 5660
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.37057197093963623,
      "learning_rate": 4.685000000000001e-05,
      "loss": 0.0055,
      "step": 5670
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 0.37099066376686096,
      "learning_rate": 4.6844444444444444e-05,
      "loss": 0.005,
      "step": 5680
    },
    {
      "epoch": 0.3161111111111111,
      "grad_norm": 0.46296554803848267,
      "learning_rate": 4.6838888888888894e-05,
      "loss": 0.007,
      "step": 5690
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.4433174133300781,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0062,
      "step": 5700
    },
    {
      "epoch": 0.31722222222222224,
      "grad_norm": 0.746311604976654,
      "learning_rate": 4.682777777777778e-05,
      "loss": 0.0059,
      "step": 5710
    },
    {
      "epoch": 0.31777777777777777,
      "grad_norm": 0.630500078201294,
      "learning_rate": 4.6822222222222225e-05,
      "loss": 0.0054,
      "step": 5720
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.2478318065404892,
      "learning_rate": 4.681666666666667e-05,
      "loss": 0.0057,
      "step": 5730
    },
    {
      "epoch": 0.3188888888888889,
      "grad_norm": 0.4017007052898407,
      "learning_rate": 4.681111111111111e-05,
      "loss": 0.0047,
      "step": 5740
    },
    {
      "epoch": 0.3194444444444444,
      "grad_norm": 0.3705118000507355,
      "learning_rate": 4.6805555555555556e-05,
      "loss": 0.0059,
      "step": 5750
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.1266452819108963,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0062,
      "step": 5760
    },
    {
      "epoch": 0.32055555555555554,
      "grad_norm": 0.24851581454277039,
      "learning_rate": 4.679444444444445e-05,
      "loss": 0.0053,
      "step": 5770
    },
    {
      "epoch": 0.3211111111111111,
      "grad_norm": 0.18514925241470337,
      "learning_rate": 4.678888888888889e-05,
      "loss": 0.0054,
      "step": 5780
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 0.15465888381004333,
      "learning_rate": 4.6783333333333337e-05,
      "loss": 0.0064,
      "step": 5790
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 0.0669981837272644,
      "learning_rate": 4.677777777777778e-05,
      "loss": 0.0061,
      "step": 5800
    },
    {
      "epoch": 0.3227777777777778,
      "grad_norm": 0.5875834226608276,
      "learning_rate": 4.6772222222222224e-05,
      "loss": 0.0059,
      "step": 5810
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.619610071182251,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.0067,
      "step": 5820
    },
    {
      "epoch": 0.3238888888888889,
      "grad_norm": 0.008964928798377514,
      "learning_rate": 4.676111111111111e-05,
      "loss": 0.0072,
      "step": 5830
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 0.2473960667848587,
      "learning_rate": 4.675555555555556e-05,
      "loss": 0.0054,
      "step": 5840
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.9284021258354187,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.0046,
      "step": 5850
    },
    {
      "epoch": 0.32555555555555554,
      "grad_norm": 0.13844069838523865,
      "learning_rate": 4.674444444444445e-05,
      "loss": 0.0068,
      "step": 5860
    },
    {
      "epoch": 0.32611111111111113,
      "grad_norm": 0.12404606491327286,
      "learning_rate": 4.673888888888889e-05,
      "loss": 0.0055,
      "step": 5870
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.49470940232276917,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0078,
      "step": 5880
    },
    {
      "epoch": 0.32722222222222225,
      "grad_norm": 0.27896490693092346,
      "learning_rate": 4.672777777777778e-05,
      "loss": 0.0063,
      "step": 5890
    },
    {
      "epoch": 0.3277777777777778,
      "grad_norm": 0.1246124655008316,
      "learning_rate": 4.672222222222222e-05,
      "loss": 0.0059,
      "step": 5900
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.010311304591596127,
      "learning_rate": 4.671666666666667e-05,
      "loss": 0.0068,
      "step": 5910
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 0.09319918602705002,
      "learning_rate": 4.671111111111111e-05,
      "loss": 0.0068,
      "step": 5920
    },
    {
      "epoch": 0.32944444444444443,
      "grad_norm": 0.0632990226149559,
      "learning_rate": 4.670555555555556e-05,
      "loss": 0.0054,
      "step": 5930
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.43334537744522095,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.006,
      "step": 5940
    },
    {
      "epoch": 0.33055555555555555,
      "grad_norm": 0.12423998862504959,
      "learning_rate": 4.669444444444445e-05,
      "loss": 0.0084,
      "step": 5950
    },
    {
      "epoch": 0.33111111111111113,
      "grad_norm": 0.06756830215454102,
      "learning_rate": 4.668888888888889e-05,
      "loss": 0.0044,
      "step": 5960
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.0626508891582489,
      "learning_rate": 4.6683333333333334e-05,
      "loss": 0.0064,
      "step": 5970
    },
    {
      "epoch": 0.3322222222222222,
      "grad_norm": 0.43365707993507385,
      "learning_rate": 4.6677777777777785e-05,
      "loss": 0.0062,
      "step": 5980
    },
    {
      "epoch": 0.3327777777777778,
      "grad_norm": 0.5567811727523804,
      "learning_rate": 4.667222222222222e-05,
      "loss": 0.0077,
      "step": 5990
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.1548433005809784,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.006,
      "step": 6000
    },
    {
      "epoch": 0.3338888888888889,
      "grad_norm": 0.09449084103107452,
      "learning_rate": 4.666111111111111e-05,
      "loss": 0.0059,
      "step": 6010
    },
    {
      "epoch": 0.33444444444444443,
      "grad_norm": 0.24691598117351532,
      "learning_rate": 4.665555555555556e-05,
      "loss": 0.0064,
      "step": 6020
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.3717647194862366,
      "learning_rate": 4.665e-05,
      "loss": 0.0069,
      "step": 6030
    },
    {
      "epoch": 0.33555555555555555,
      "grad_norm": 0.8647180795669556,
      "learning_rate": 4.6644444444444446e-05,
      "loss": 0.0073,
      "step": 6040
    },
    {
      "epoch": 0.33611111111111114,
      "grad_norm": 0.8000019788742065,
      "learning_rate": 4.6638888888888896e-05,
      "loss": 0.0061,
      "step": 6050
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.369839072227478,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.006,
      "step": 6060
    },
    {
      "epoch": 0.3372222222222222,
      "grad_norm": 0.06286604702472687,
      "learning_rate": 4.662777777777778e-05,
      "loss": 0.0057,
      "step": 6070
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.5550788640975952,
      "learning_rate": 4.662222222222222e-05,
      "loss": 0.0059,
      "step": 6080
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.8627796173095703,
      "learning_rate": 4.661666666666667e-05,
      "loss": 0.0069,
      "step": 6090
    },
    {
      "epoch": 0.3388888888888889,
      "grad_norm": 0.7435598969459534,
      "learning_rate": 4.6611111111111114e-05,
      "loss": 0.0071,
      "step": 6100
    },
    {
      "epoch": 0.33944444444444444,
      "grad_norm": 0.18473559617996216,
      "learning_rate": 4.660555555555556e-05,
      "loss": 0.0062,
      "step": 6110
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.18616661429405212,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0054,
      "step": 6120
    },
    {
      "epoch": 0.34055555555555556,
      "grad_norm": 0.24727973341941833,
      "learning_rate": 4.6594444444444445e-05,
      "loss": 0.0044,
      "step": 6130
    },
    {
      "epoch": 0.3411111111111111,
      "grad_norm": 0.1248079165816307,
      "learning_rate": 4.6588888888888895e-05,
      "loss": 0.0062,
      "step": 6140
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.30926376581192017,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.006,
      "step": 6150
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 0.06390444189310074,
      "learning_rate": 4.657777777777778e-05,
      "loss": 0.0052,
      "step": 6160
    },
    {
      "epoch": 0.3427777777777778,
      "grad_norm": 0.44492366909980774,
      "learning_rate": 4.6572222222222226e-05,
      "loss": 0.0067,
      "step": 6170
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.4320490062236786,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0065,
      "step": 6180
    },
    {
      "epoch": 0.3438888888888889,
      "grad_norm": 0.063460573554039,
      "learning_rate": 4.656111111111111e-05,
      "loss": 0.0053,
      "step": 6190
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 0.2486346811056137,
      "learning_rate": 4.6555555555555556e-05,
      "loss": 0.0068,
      "step": 6200
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.06274599581956863,
      "learning_rate": 4.655000000000001e-05,
      "loss": 0.0059,
      "step": 6210
    },
    {
      "epoch": 0.34555555555555556,
      "grad_norm": 0.12400247156620026,
      "learning_rate": 4.6544444444444443e-05,
      "loss": 0.0072,
      "step": 6220
    },
    {
      "epoch": 0.3461111111111111,
      "grad_norm": 0.06333266198635101,
      "learning_rate": 4.6538888888888894e-05,
      "loss": 0.0043,
      "step": 6230
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.154920294880867,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0062,
      "step": 6240
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 0.12364961951971054,
      "learning_rate": 4.652777777777778e-05,
      "loss": 0.006,
      "step": 6250
    },
    {
      "epoch": 0.3477777777777778,
      "grad_norm": 0.3698360025882721,
      "learning_rate": 4.6522222222222224e-05,
      "loss": 0.0045,
      "step": 6260
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.24635440111160278,
      "learning_rate": 4.651666666666667e-05,
      "loss": 0.0073,
      "step": 6270
    },
    {
      "epoch": 0.3488888888888889,
      "grad_norm": 0.2472173273563385,
      "learning_rate": 4.651111111111111e-05,
      "loss": 0.008,
      "step": 6280
    },
    {
      "epoch": 0.34944444444444445,
      "grad_norm": 0.011736828833818436,
      "learning_rate": 4.6505555555555555e-05,
      "loss": 0.0069,
      "step": 6290
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.2463178038597107,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0044,
      "step": 6300
    },
    {
      "epoch": 0.35055555555555556,
      "grad_norm": 0.23915062844753265,
      "learning_rate": 4.649444444444445e-05,
      "loss": 0.0065,
      "step": 6310
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 0.2472362369298935,
      "learning_rate": 4.648888888888889e-05,
      "loss": 0.0068,
      "step": 6320
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.1621599942445755,
      "learning_rate": 4.6483333333333336e-05,
      "loss": 0.0057,
      "step": 6330
    },
    {
      "epoch": 0.3522222222222222,
      "grad_norm": 0.27762266993522644,
      "learning_rate": 4.647777777777778e-05,
      "loss": 0.006,
      "step": 6340
    },
    {
      "epoch": 0.3527777777777778,
      "grad_norm": 0.4322247803211212,
      "learning_rate": 4.647222222222222e-05,
      "loss": 0.0063,
      "step": 6350
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.3690683841705322,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0066,
      "step": 6360
    },
    {
      "epoch": 0.35388888888888886,
      "grad_norm": 0.24359366297721863,
      "learning_rate": 4.646111111111111e-05,
      "loss": 0.0058,
      "step": 6370
    },
    {
      "epoch": 0.35444444444444445,
      "grad_norm": 0.3093207776546478,
      "learning_rate": 4.645555555555556e-05,
      "loss": 0.007,
      "step": 6380
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.21649958193302155,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.0042,
      "step": 6390
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.2871510088443756,
      "learning_rate": 4.644444444444445e-05,
      "loss": 0.0066,
      "step": 6400
    },
    {
      "epoch": 0.3561111111111111,
      "grad_norm": 0.24674032628536224,
      "learning_rate": 4.643888888888889e-05,
      "loss": 0.0047,
      "step": 6410
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.06431924551725388,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.006,
      "step": 6420
    },
    {
      "epoch": 0.3572222222222222,
      "grad_norm": 0.1544702798128128,
      "learning_rate": 4.642777777777778e-05,
      "loss": 0.0057,
      "step": 6430
    },
    {
      "epoch": 0.35777777777777775,
      "grad_norm": 0.40098732709884644,
      "learning_rate": 4.642222222222222e-05,
      "loss": 0.0054,
      "step": 6440
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.2774989902973175,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.0055,
      "step": 6450
    },
    {
      "epoch": 0.35888888888888887,
      "grad_norm": 0.3389618992805481,
      "learning_rate": 4.641111111111111e-05,
      "loss": 0.0053,
      "step": 6460
    },
    {
      "epoch": 0.35944444444444446,
      "grad_norm": 0.5540891885757446,
      "learning_rate": 4.640555555555556e-05,
      "loss": 0.0058,
      "step": 6470
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.6470895409584045,
      "learning_rate": 4.64e-05,
      "loss": 0.0068,
      "step": 6480
    },
    {
      "epoch": 0.3605555555555556,
      "grad_norm": 0.36958369612693787,
      "learning_rate": 4.6394444444444447e-05,
      "loss": 0.0075,
      "step": 6490
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 0.2432541400194168,
      "learning_rate": 4.638888888888889e-05,
      "loss": 0.0085,
      "step": 6500
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.18465425074100494,
      "learning_rate": 4.6383333333333334e-05,
      "loss": 0.0065,
      "step": 6510
    },
    {
      "epoch": 0.3622222222222222,
      "grad_norm": 0.2820744514465332,
      "learning_rate": 4.6377777777777784e-05,
      "loss": 0.0074,
      "step": 6520
    },
    {
      "epoch": 0.36277777777777775,
      "grad_norm": 0.3078054189682007,
      "learning_rate": 4.637222222222222e-05,
      "loss": 0.0059,
      "step": 6530
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.6156145930290222,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0063,
      "step": 6540
    },
    {
      "epoch": 0.3638888888888889,
      "grad_norm": 0.1553381383419037,
      "learning_rate": 4.636111111111111e-05,
      "loss": 0.0065,
      "step": 6550
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 0.06206308305263519,
      "learning_rate": 4.635555555555556e-05,
      "loss": 0.0056,
      "step": 6560
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.18604542315006256,
      "learning_rate": 4.635e-05,
      "loss": 0.0053,
      "step": 6570
    },
    {
      "epoch": 0.3655555555555556,
      "grad_norm": 0.041186098009347916,
      "learning_rate": 4.6344444444444445e-05,
      "loss": 0.0073,
      "step": 6580
    },
    {
      "epoch": 0.3661111111111111,
      "grad_norm": 0.3695180118083954,
      "learning_rate": 4.6338888888888896e-05,
      "loss": 0.0073,
      "step": 6590
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.4935370981693268,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0057,
      "step": 6600
    },
    {
      "epoch": 0.3672222222222222,
      "grad_norm": 0.06276670098304749,
      "learning_rate": 4.632777777777778e-05,
      "loss": 0.0039,
      "step": 6610
    },
    {
      "epoch": 0.36777777777777776,
      "grad_norm": 0.33856937289237976,
      "learning_rate": 4.632222222222222e-05,
      "loss": 0.0056,
      "step": 6620
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.12592250108718872,
      "learning_rate": 4.631666666666667e-05,
      "loss": 0.0051,
      "step": 6630
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 0.7402161359786987,
      "learning_rate": 4.6311111111111113e-05,
      "loss": 0.0069,
      "step": 6640
    },
    {
      "epoch": 0.36944444444444446,
      "grad_norm": 0.04333608224987984,
      "learning_rate": 4.630555555555556e-05,
      "loss": 0.0066,
      "step": 6650
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.23448078334331512,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0073,
      "step": 6660
    },
    {
      "epoch": 0.3705555555555556,
      "grad_norm": 0.06507692486047745,
      "learning_rate": 4.6294444444444444e-05,
      "loss": 0.0058,
      "step": 6670
    },
    {
      "epoch": 0.3711111111111111,
      "grad_norm": 0.1240355521440506,
      "learning_rate": 4.6288888888888894e-05,
      "loss": 0.0062,
      "step": 6680
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.2474926859140396,
      "learning_rate": 4.628333333333333e-05,
      "loss": 0.0049,
      "step": 6690
    },
    {
      "epoch": 0.37222222222222223,
      "grad_norm": 0.30773571133613586,
      "learning_rate": 4.627777777777778e-05,
      "loss": 0.0073,
      "step": 6700
    },
    {
      "epoch": 0.37277777777777776,
      "grad_norm": 0.5539528727531433,
      "learning_rate": 4.6272222222222225e-05,
      "loss": 0.007,
      "step": 6710
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.06285250931978226,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0069,
      "step": 6720
    },
    {
      "epoch": 0.3738888888888889,
      "grad_norm": 0.18449649214744568,
      "learning_rate": 4.626111111111111e-05,
      "loss": 0.0059,
      "step": 6730
    },
    {
      "epoch": 0.37444444444444447,
      "grad_norm": 0.01862991787493229,
      "learning_rate": 4.6255555555555556e-05,
      "loss": 0.0063,
      "step": 6740
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.12460595369338989,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.0054,
      "step": 6750
    },
    {
      "epoch": 0.37555555555555553,
      "grad_norm": 0.16447804868221283,
      "learning_rate": 4.624444444444444e-05,
      "loss": 0.0073,
      "step": 6760
    },
    {
      "epoch": 0.3761111111111111,
      "grad_norm": 0.706885039806366,
      "learning_rate": 4.623888888888889e-05,
      "loss": 0.0062,
      "step": 6770
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.4604247212409973,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.0052,
      "step": 6780
    },
    {
      "epoch": 0.37722222222222224,
      "grad_norm": 0.16329622268676758,
      "learning_rate": 4.622777777777778e-05,
      "loss": 0.0071,
      "step": 6790
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 0.24728934466838837,
      "learning_rate": 4.6222222222222224e-05,
      "loss": 0.0052,
      "step": 6800
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 0.01931769587099552,
      "learning_rate": 4.621666666666667e-05,
      "loss": 0.0066,
      "step": 6810
    },
    {
      "epoch": 0.3788888888888889,
      "grad_norm": 0.12363921105861664,
      "learning_rate": 4.621111111111111e-05,
      "loss": 0.0046,
      "step": 6820
    },
    {
      "epoch": 0.3794444444444444,
      "grad_norm": 0.033792659640312195,
      "learning_rate": 4.6205555555555555e-05,
      "loss": 0.0072,
      "step": 6830
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.12346293032169342,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0052,
      "step": 6840
    },
    {
      "epoch": 0.38055555555555554,
      "grad_norm": 0.06457721441984177,
      "learning_rate": 4.619444444444445e-05,
      "loss": 0.0051,
      "step": 6850
    },
    {
      "epoch": 0.3811111111111111,
      "grad_norm": 0.3067643940448761,
      "learning_rate": 4.618888888888889e-05,
      "loss": 0.0082,
      "step": 6860
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.12298153340816498,
      "learning_rate": 4.6183333333333336e-05,
      "loss": 0.0059,
      "step": 6870
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 0.615332841873169,
      "learning_rate": 4.617777777777778e-05,
      "loss": 0.0063,
      "step": 6880
    },
    {
      "epoch": 0.3827777777777778,
      "grad_norm": 0.4808521568775177,
      "learning_rate": 4.617222222222222e-05,
      "loss": 0.0063,
      "step": 6890
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.07173241674900055,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0041,
      "step": 6900
    },
    {
      "epoch": 0.3838888888888889,
      "grad_norm": 0.245871901512146,
      "learning_rate": 4.6161111111111117e-05,
      "loss": 0.0045,
      "step": 6910
    },
    {
      "epoch": 0.3844444444444444,
      "grad_norm": 0.19405102729797363,
      "learning_rate": 4.615555555555556e-05,
      "loss": 0.0066,
      "step": 6920
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.06258673965930939,
      "learning_rate": 4.6150000000000004e-05,
      "loss": 0.0067,
      "step": 6930
    },
    {
      "epoch": 0.38555555555555554,
      "grad_norm": 0.1865260899066925,
      "learning_rate": 4.614444444444445e-05,
      "loss": 0.0073,
      "step": 6940
    },
    {
      "epoch": 0.3861111111111111,
      "grad_norm": 0.1245792806148529,
      "learning_rate": 4.613888888888889e-05,
      "loss": 0.0054,
      "step": 6950
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.03319476544857025,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0059,
      "step": 6960
    },
    {
      "epoch": 0.38722222222222225,
      "grad_norm": 0.09413944184780121,
      "learning_rate": 4.612777777777778e-05,
      "loss": 0.0047,
      "step": 6970
    },
    {
      "epoch": 0.3877777777777778,
      "grad_norm": 0.3075026571750641,
      "learning_rate": 4.612222222222222e-05,
      "loss": 0.0051,
      "step": 6980
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 0.1533391922712326,
      "learning_rate": 4.611666666666667e-05,
      "loss": 0.0041,
      "step": 6990
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.17248551547527313,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 0.0074,
      "step": 7000
    },
    {
      "epoch": 0.3894444444444444,
      "grad_norm": 0.28047943115234375,
      "learning_rate": 4.610555555555556e-05,
      "loss": 0.0054,
      "step": 7010
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.3070535361766815,
      "learning_rate": 4.61e-05,
      "loss": 0.0056,
      "step": 7020
    },
    {
      "epoch": 0.39055555555555554,
      "grad_norm": 0.6146832704544067,
      "learning_rate": 4.6094444444444446e-05,
      "loss": 0.006,
      "step": 7030
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 0.1269541233778,
      "learning_rate": 4.608888888888889e-05,
      "loss": 0.0055,
      "step": 7040
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.02186420001089573,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.0051,
      "step": 7050
    },
    {
      "epoch": 0.39222222222222225,
      "grad_norm": 0.12222833931446075,
      "learning_rate": 4.6077777777777783e-05,
      "loss": 0.0052,
      "step": 7060
    },
    {
      "epoch": 0.3927777777777778,
      "grad_norm": 0.24596668779850006,
      "learning_rate": 4.607222222222222e-05,
      "loss": 0.0064,
      "step": 7070
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.3136557340621948,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0056,
      "step": 7080
    },
    {
      "epoch": 0.3938888888888889,
      "grad_norm": 0.21576690673828125,
      "learning_rate": 4.6061111111111114e-05,
      "loss": 0.0043,
      "step": 7090
    },
    {
      "epoch": 0.39444444444444443,
      "grad_norm": 0.014688864350318909,
      "learning_rate": 4.605555555555556e-05,
      "loss": 0.0065,
      "step": 7100
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.18314211070537567,
      "learning_rate": 4.605e-05,
      "loss": 0.0067,
      "step": 7110
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 0.06348568946123123,
      "learning_rate": 4.6044444444444445e-05,
      "loss": 0.0067,
      "step": 7120
    },
    {
      "epoch": 0.39611111111111114,
      "grad_norm": 0.0713857114315033,
      "learning_rate": 4.6038888888888895e-05,
      "loss": 0.0052,
      "step": 7130
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.30658602714538574,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.0051,
      "step": 7140
    },
    {
      "epoch": 0.3972222222222222,
      "grad_norm": 0.6133872866630554,
      "learning_rate": 4.602777777777778e-05,
      "loss": 0.0079,
      "step": 7150
    },
    {
      "epoch": 0.3977777777777778,
      "grad_norm": 0.12597796320915222,
      "learning_rate": 4.602222222222222e-05,
      "loss": 0.0061,
      "step": 7160
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.12244179844856262,
      "learning_rate": 4.601666666666667e-05,
      "loss": 0.0064,
      "step": 7170
    },
    {
      "epoch": 0.3988888888888889,
      "grad_norm": 0.12427569925785065,
      "learning_rate": 4.601111111111111e-05,
      "loss": 0.0059,
      "step": 7180
    },
    {
      "epoch": 0.39944444444444444,
      "grad_norm": 0.3065340220928192,
      "learning_rate": 4.6005555555555556e-05,
      "loss": 0.004,
      "step": 7190
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.7041242718696594,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0042,
      "step": 7200
    },
    {
      "epoch": 0.40055555555555555,
      "grad_norm": 0.5513155460357666,
      "learning_rate": 4.5994444444444444e-05,
      "loss": 0.0053,
      "step": 7210
    },
    {
      "epoch": 0.4011111111111111,
      "grad_norm": 0.013304765336215496,
      "learning_rate": 4.5988888888888894e-05,
      "loss": 0.0065,
      "step": 7220
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 0.0108123067766428,
      "learning_rate": 4.598333333333333e-05,
      "loss": 0.0074,
      "step": 7230
    },
    {
      "epoch": 0.4022222222222222,
      "grad_norm": 0.24503377079963684,
      "learning_rate": 4.597777777777778e-05,
      "loss": 0.0062,
      "step": 7240
    },
    {
      "epoch": 0.4027777777777778,
      "grad_norm": 0.061188310384750366,
      "learning_rate": 4.5972222222222225e-05,
      "loss": 0.0065,
      "step": 7250
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.48994889855384827,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.0052,
      "step": 7260
    },
    {
      "epoch": 0.4038888888888889,
      "grad_norm": 0.06344309449195862,
      "learning_rate": 4.596111111111112e-05,
      "loss": 0.0057,
      "step": 7270
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 0.06434207409620285,
      "learning_rate": 4.5955555555555555e-05,
      "loss": 0.0052,
      "step": 7280
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.5823231935501099,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 0.0064,
      "step": 7290
    },
    {
      "epoch": 0.40555555555555556,
      "grad_norm": 0.062215957790613174,
      "learning_rate": 4.594444444444444e-05,
      "loss": 0.0055,
      "step": 7300
    },
    {
      "epoch": 0.4061111111111111,
      "grad_norm": 0.24615854024887085,
      "learning_rate": 4.593888888888889e-05,
      "loss": 0.0049,
      "step": 7310
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.39952975511550903,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.006,
      "step": 7320
    },
    {
      "epoch": 0.4072222222222222,
      "grad_norm": 0.12300843745470047,
      "learning_rate": 4.592777777777778e-05,
      "loss": 0.0074,
      "step": 7330
    },
    {
      "epoch": 0.4077777777777778,
      "grad_norm": 0.18460236489772797,
      "learning_rate": 4.592222222222222e-05,
      "loss": 0.0048,
      "step": 7340
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.30635854601860046,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.0055,
      "step": 7350
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.035731241106987,
      "learning_rate": 4.591111111111112e-05,
      "loss": 0.0057,
      "step": 7360
    },
    {
      "epoch": 0.40944444444444444,
      "grad_norm": 0.0619913786649704,
      "learning_rate": 4.5905555555555554e-05,
      "loss": 0.0066,
      "step": 7370
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.5501177310943604,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0057,
      "step": 7380
    },
    {
      "epoch": 0.41055555555555556,
      "grad_norm": 0.09213004261255264,
      "learning_rate": 4.589444444444445e-05,
      "loss": 0.0068,
      "step": 7390
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 0.15338349342346191,
      "learning_rate": 4.588888888888889e-05,
      "loss": 0.0058,
      "step": 7400
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 0.1846805065870285,
      "learning_rate": 4.5883333333333335e-05,
      "loss": 0.0053,
      "step": 7410
    },
    {
      "epoch": 0.4122222222222222,
      "grad_norm": 0.30686643719673157,
      "learning_rate": 4.587777777777778e-05,
      "loss": 0.0055,
      "step": 7420
    },
    {
      "epoch": 0.4127777777777778,
      "grad_norm": 0.30631083250045776,
      "learning_rate": 4.587222222222222e-05,
      "loss": 0.0068,
      "step": 7430
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.07617180049419403,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0074,
      "step": 7440
    },
    {
      "epoch": 0.41388888888888886,
      "grad_norm": 0.012182372622191906,
      "learning_rate": 4.5861111111111116e-05,
      "loss": 0.0046,
      "step": 7450
    },
    {
      "epoch": 0.41444444444444445,
      "grad_norm": 0.3058353364467621,
      "learning_rate": 4.585555555555556e-05,
      "loss": 0.0068,
      "step": 7460
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.48914313316345215,
      "learning_rate": 4.585e-05,
      "loss": 0.0069,
      "step": 7470
    },
    {
      "epoch": 0.41555555555555557,
      "grad_norm": 0.062180470675230026,
      "learning_rate": 4.584444444444445e-05,
      "loss": 0.0057,
      "step": 7480
    },
    {
      "epoch": 0.4161111111111111,
      "grad_norm": 0.24604468047618866,
      "learning_rate": 4.583888888888889e-05,
      "loss": 0.006,
      "step": 7490
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.1232534721493721,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0046,
      "step": 7500
    },
    {
      "epoch": 0.4172222222222222,
      "grad_norm": 0.30563366413116455,
      "learning_rate": 4.582777777777778e-05,
      "loss": 0.0044,
      "step": 7510
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 0.18571624159812927,
      "learning_rate": 4.582222222222222e-05,
      "loss": 0.0063,
      "step": 7520
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.3977963328361511,
      "learning_rate": 4.581666666666667e-05,
      "loss": 0.0061,
      "step": 7530
    },
    {
      "epoch": 0.41888888888888887,
      "grad_norm": 0.3685287833213806,
      "learning_rate": 4.5811111111111115e-05,
      "loss": 0.007,
      "step": 7540
    },
    {
      "epoch": 0.41944444444444445,
      "grad_norm": 0.3386511206626892,
      "learning_rate": 4.580555555555556e-05,
      "loss": 0.0062,
      "step": 7550
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.12301655113697052,
      "learning_rate": 4.58e-05,
      "loss": 0.0075,
      "step": 7560
    },
    {
      "epoch": 0.42055555555555557,
      "grad_norm": 0.36754193902015686,
      "learning_rate": 4.5794444444444446e-05,
      "loss": 0.0063,
      "step": 7570
    },
    {
      "epoch": 0.4211111111111111,
      "grad_norm": 0.1276887208223343,
      "learning_rate": 4.578888888888889e-05,
      "loss": 0.0062,
      "step": 7580
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.24540148675441742,
      "learning_rate": 4.578333333333333e-05,
      "loss": 0.0083,
      "step": 7590
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 0.12617959082126617,
      "learning_rate": 4.577777777777778e-05,
      "loss": 0.0074,
      "step": 7600
    },
    {
      "epoch": 0.42277777777777775,
      "grad_norm": 0.1697445809841156,
      "learning_rate": 4.577222222222222e-05,
      "loss": 0.0054,
      "step": 7610
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.15646953880786896,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.0059,
      "step": 7620
    },
    {
      "epoch": 0.42388888888888887,
      "grad_norm": 0.06622963398694992,
      "learning_rate": 4.5761111111111114e-05,
      "loss": 0.0054,
      "step": 7630
    },
    {
      "epoch": 0.42444444444444446,
      "grad_norm": 0.1234961748123169,
      "learning_rate": 4.575555555555556e-05,
      "loss": 0.0073,
      "step": 7640
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.24589332938194275,
      "learning_rate": 4.575e-05,
      "loss": 0.0043,
      "step": 7650
    },
    {
      "epoch": 0.4255555555555556,
      "grad_norm": 0.034864384680986404,
      "learning_rate": 4.5744444444444444e-05,
      "loss": 0.0055,
      "step": 7660
    },
    {
      "epoch": 0.4261111111111111,
      "grad_norm": 0.1838684231042862,
      "learning_rate": 4.5738888888888895e-05,
      "loss": 0.005,
      "step": 7670
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.27616021037101746,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0069,
      "step": 7680
    },
    {
      "epoch": 0.4272222222222222,
      "grad_norm": 0.12408094108104706,
      "learning_rate": 4.572777777777778e-05,
      "loss": 0.0054,
      "step": 7690
    },
    {
      "epoch": 0.42777777777777776,
      "grad_norm": 0.1230718120932579,
      "learning_rate": 4.572222222222222e-05,
      "loss": 0.005,
      "step": 7700
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.15474091470241547,
      "learning_rate": 4.571666666666667e-05,
      "loss": 0.0049,
      "step": 7710
    },
    {
      "epoch": 0.4288888888888889,
      "grad_norm": 0.30726972222328186,
      "learning_rate": 4.571111111111111e-05,
      "loss": 0.0058,
      "step": 7720
    },
    {
      "epoch": 0.42944444444444446,
      "grad_norm": 0.1533343344926834,
      "learning_rate": 4.5705555555555556e-05,
      "loss": 0.006,
      "step": 7730
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.21500264108181,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.0042,
      "step": 7740
    },
    {
      "epoch": 0.4305555555555556,
      "grad_norm": 0.3079177737236023,
      "learning_rate": 4.569444444444444e-05,
      "loss": 0.0051,
      "step": 7750
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 0.06366468220949173,
      "learning_rate": 4.5688888888888893e-05,
      "loss": 0.0056,
      "step": 7760
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.017149176448583603,
      "learning_rate": 4.568333333333333e-05,
      "loss": 0.0061,
      "step": 7770
    },
    {
      "epoch": 0.43222222222222223,
      "grad_norm": 0.029856175184249878,
      "learning_rate": 4.567777777777778e-05,
      "loss": 0.0059,
      "step": 7780
    },
    {
      "epoch": 0.43277777777777776,
      "grad_norm": 0.30650845170021057,
      "learning_rate": 4.5672222222222224e-05,
      "loss": 0.0056,
      "step": 7790
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.2780698835849762,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0049,
      "step": 7800
    },
    {
      "epoch": 0.4338888888888889,
      "grad_norm": 0.4905772805213928,
      "learning_rate": 4.566111111111112e-05,
      "loss": 0.0068,
      "step": 7810
    },
    {
      "epoch": 0.43444444444444447,
      "grad_norm": 0.3067534267902374,
      "learning_rate": 4.5655555555555555e-05,
      "loss": 0.0044,
      "step": 7820
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.5514689683914185,
      "learning_rate": 4.5650000000000005e-05,
      "loss": 0.006,
      "step": 7830
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 0.27877646684646606,
      "learning_rate": 4.564444444444444e-05,
      "loss": 0.0055,
      "step": 7840
    },
    {
      "epoch": 0.4361111111111111,
      "grad_norm": 0.1852159947156906,
      "learning_rate": 4.563888888888889e-05,
      "loss": 0.008,
      "step": 7850
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.24608562886714935,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.0065,
      "step": 7860
    },
    {
      "epoch": 0.43722222222222223,
      "grad_norm": 0.06694492697715759,
      "learning_rate": 4.562777777777778e-05,
      "loss": 0.0071,
      "step": 7870
    },
    {
      "epoch": 0.43777777777777777,
      "grad_norm": 0.0638764351606369,
      "learning_rate": 4.562222222222222e-05,
      "loss": 0.0052,
      "step": 7880
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.12310914695262909,
      "learning_rate": 4.5616666666666666e-05,
      "loss": 0.0071,
      "step": 7890
    },
    {
      "epoch": 0.4388888888888889,
      "grad_norm": 0.2468239665031433,
      "learning_rate": 4.561111111111112e-05,
      "loss": 0.006,
      "step": 7900
    },
    {
      "epoch": 0.43944444444444447,
      "grad_norm": 0.45230990648269653,
      "learning_rate": 4.560555555555556e-05,
      "loss": 0.0048,
      "step": 7910
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.30754777789115906,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.007,
      "step": 7920
    },
    {
      "epoch": 0.44055555555555553,
      "grad_norm": 0.061463821679353714,
      "learning_rate": 4.559444444444445e-05,
      "loss": 0.0039,
      "step": 7930
    },
    {
      "epoch": 0.4411111111111111,
      "grad_norm": 0.3684043884277344,
      "learning_rate": 4.558888888888889e-05,
      "loss": 0.0061,
      "step": 7940
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.18176572024822235,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.0061,
      "step": 7950
    },
    {
      "epoch": 0.44222222222222224,
      "grad_norm": 0.15452754497528076,
      "learning_rate": 4.557777777777778e-05,
      "loss": 0.0039,
      "step": 7960
    },
    {
      "epoch": 0.44277777777777777,
      "grad_norm": 0.29255223274230957,
      "learning_rate": 4.557222222222222e-05,
      "loss": 0.0065,
      "step": 7970
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.18444296717643738,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0046,
      "step": 7980
    },
    {
      "epoch": 0.4438888888888889,
      "grad_norm": 0.46320396661758423,
      "learning_rate": 4.5561111111111116e-05,
      "loss": 0.0059,
      "step": 7990
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.1836802065372467,
      "learning_rate": 4.555555555555556e-05,
      "loss": 0.0042,
      "step": 8000
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.2760641574859619,
      "learning_rate": 4.555e-05,
      "loss": 0.0064,
      "step": 8010
    },
    {
      "epoch": 0.44555555555555554,
      "grad_norm": 0.18496303260326385,
      "learning_rate": 4.5544444444444446e-05,
      "loss": 0.0066,
      "step": 8020
    },
    {
      "epoch": 0.4461111111111111,
      "grad_norm": 0.2654770612716675,
      "learning_rate": 4.553888888888889e-05,
      "loss": 0.0055,
      "step": 8030
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.7982169985771179,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0068,
      "step": 8040
    },
    {
      "epoch": 0.44722222222222224,
      "grad_norm": 0.12371908873319626,
      "learning_rate": 4.5527777777777784e-05,
      "loss": 0.0042,
      "step": 8050
    },
    {
      "epoch": 0.4477777777777778,
      "grad_norm": 0.36811715364456177,
      "learning_rate": 4.552222222222222e-05,
      "loss": 0.0071,
      "step": 8060
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.12708161771297455,
      "learning_rate": 4.551666666666667e-05,
      "loss": 0.006,
      "step": 8070
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 0.18823185563087463,
      "learning_rate": 4.5511111111111114e-05,
      "loss": 0.0065,
      "step": 8080
    },
    {
      "epoch": 0.4494444444444444,
      "grad_norm": 0.7358725666999817,
      "learning_rate": 4.550555555555556e-05,
      "loss": 0.0062,
      "step": 8090
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.06357470154762268,
      "learning_rate": 4.55e-05,
      "loss": 0.0068,
      "step": 8100
    },
    {
      "epoch": 0.45055555555555554,
      "grad_norm": 0.06222186237573624,
      "learning_rate": 4.5494444444444445e-05,
      "loss": 0.0063,
      "step": 8110
    },
    {
      "epoch": 0.45111111111111113,
      "grad_norm": 0.2757304906845093,
      "learning_rate": 4.5488888888888895e-05,
      "loss": 0.0047,
      "step": 8120
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.5521736145019531,
      "learning_rate": 4.548333333333333e-05,
      "loss": 0.0053,
      "step": 8130
    },
    {
      "epoch": 0.45222222222222225,
      "grad_norm": 0.30698633193969727,
      "learning_rate": 4.547777777777778e-05,
      "loss": 0.0058,
      "step": 8140
    },
    {
      "epoch": 0.4527777777777778,
      "grad_norm": 0.36781609058380127,
      "learning_rate": 4.5472222222222226e-05,
      "loss": 0.0048,
      "step": 8150
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.24641768634319305,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0067,
      "step": 8160
    },
    {
      "epoch": 0.4538888888888889,
      "grad_norm": 0.0941196084022522,
      "learning_rate": 4.546111111111111e-05,
      "loss": 0.0053,
      "step": 8170
    },
    {
      "epoch": 0.45444444444444443,
      "grad_norm": 0.06562068313360214,
      "learning_rate": 4.545555555555556e-05,
      "loss": 0.0062,
      "step": 8180
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.42917096614837646,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.0052,
      "step": 8190
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 0.06356655806303024,
      "learning_rate": 4.5444444444444444e-05,
      "loss": 0.0058,
      "step": 8200
    },
    {
      "epoch": 0.45611111111111113,
      "grad_norm": 0.3701237440109253,
      "learning_rate": 4.5438888888888894e-05,
      "loss": 0.0059,
      "step": 8210
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 0.521912693977356,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.0054,
      "step": 8220
    },
    {
      "epoch": 0.4572222222222222,
      "grad_norm": 0.49090173840522766,
      "learning_rate": 4.542777777777778e-05,
      "loss": 0.006,
      "step": 8230
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 0.368281751871109,
      "learning_rate": 4.5422222222222225e-05,
      "loss": 0.0049,
      "step": 8240
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.15001438558101654,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.005,
      "step": 8250
    },
    {
      "epoch": 0.4588888888888889,
      "grad_norm": 0.42884206771850586,
      "learning_rate": 4.541111111111112e-05,
      "loss": 0.0062,
      "step": 8260
    },
    {
      "epoch": 0.45944444444444443,
      "grad_norm": 0.1225833147764206,
      "learning_rate": 4.5405555555555555e-05,
      "loss": 0.0036,
      "step": 8270
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.1836898922920227,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0067,
      "step": 8280
    },
    {
      "epoch": 0.46055555555555555,
      "grad_norm": 0.4901469349861145,
      "learning_rate": 4.539444444444444e-05,
      "loss": 0.0061,
      "step": 8290
    },
    {
      "epoch": 0.46111111111111114,
      "grad_norm": 0.5417004823684692,
      "learning_rate": 4.538888888888889e-05,
      "loss": 0.0053,
      "step": 8300
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.24523471295833588,
      "learning_rate": 4.5383333333333336e-05,
      "loss": 0.0069,
      "step": 8310
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.154032900929451,
      "learning_rate": 4.537777777777778e-05,
      "loss": 0.0053,
      "step": 8320
    },
    {
      "epoch": 0.4627777777777778,
      "grad_norm": 0.13315708935260773,
      "learning_rate": 4.537222222222223e-05,
      "loss": 0.0048,
      "step": 8330
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.1226857602596283,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.0062,
      "step": 8340
    },
    {
      "epoch": 0.4638888888888889,
      "grad_norm": 0.07618729025125504,
      "learning_rate": 4.536111111111112e-05,
      "loss": 0.0048,
      "step": 8350
    },
    {
      "epoch": 0.46444444444444444,
      "grad_norm": 0.3063540756702423,
      "learning_rate": 4.5355555555555554e-05,
      "loss": 0.0066,
      "step": 8360
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.49808672070503235,
      "learning_rate": 4.5350000000000005e-05,
      "loss": 0.0072,
      "step": 8370
    },
    {
      "epoch": 0.46555555555555556,
      "grad_norm": 0.05135951191186905,
      "learning_rate": 4.534444444444445e-05,
      "loss": 0.0053,
      "step": 8380
    },
    {
      "epoch": 0.4661111111111111,
      "grad_norm": 0.24932131171226501,
      "learning_rate": 4.533888888888889e-05,
      "loss": 0.0058,
      "step": 8390
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.09247742593288422,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0049,
      "step": 8400
    },
    {
      "epoch": 0.4672222222222222,
      "grad_norm": 0.24563811719417572,
      "learning_rate": 4.532777777777778e-05,
      "loss": 0.0065,
      "step": 8410
    },
    {
      "epoch": 0.4677777777777778,
      "grad_norm": 0.24416348338127136,
      "learning_rate": 4.532222222222223e-05,
      "loss": 0.0067,
      "step": 8420
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.5140975713729858,
      "learning_rate": 4.5316666666666666e-05,
      "loss": 0.0061,
      "step": 8430
    },
    {
      "epoch": 0.4688888888888889,
      "grad_norm": 0.6104419827461243,
      "learning_rate": 4.5311111111111116e-05,
      "loss": 0.0051,
      "step": 8440
    },
    {
      "epoch": 0.46944444444444444,
      "grad_norm": 0.1998327523469925,
      "learning_rate": 4.530555555555556e-05,
      "loss": 0.0064,
      "step": 8450
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.6122665405273438,
      "learning_rate": 4.53e-05,
      "loss": 0.0056,
      "step": 8460
    },
    {
      "epoch": 0.47055555555555556,
      "grad_norm": 0.12297933548688889,
      "learning_rate": 4.529444444444445e-05,
      "loss": 0.0052,
      "step": 8470
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 0.12321172654628754,
      "learning_rate": 4.528888888888889e-05,
      "loss": 0.0052,
      "step": 8480
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.4287208914756775,
      "learning_rate": 4.5283333333333334e-05,
      "loss": 0.0061,
      "step": 8490
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.4520827531814575,
      "learning_rate": 4.527777777777778e-05,
      "loss": 0.0068,
      "step": 8500
    },
    {
      "epoch": 0.4727777777777778,
      "grad_norm": 0.24460825324058533,
      "learning_rate": 4.527222222222223e-05,
      "loss": 0.0055,
      "step": 8510
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.3677498698234558,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.004,
      "step": 8520
    },
    {
      "epoch": 0.4738888888888889,
      "grad_norm": 0.09270700812339783,
      "learning_rate": 4.5261111111111115e-05,
      "loss": 0.005,
      "step": 8530
    },
    {
      "epoch": 0.47444444444444445,
      "grad_norm": 0.244899719953537,
      "learning_rate": 4.525555555555556e-05,
      "loss": 0.0053,
      "step": 8540
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.12344688922166824,
      "learning_rate": 4.525e-05,
      "loss": 0.0054,
      "step": 8550
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 0.3683323264122009,
      "learning_rate": 4.5244444444444446e-05,
      "loss": 0.0073,
      "step": 8560
    },
    {
      "epoch": 0.4761111111111111,
      "grad_norm": 0.15368664264678955,
      "learning_rate": 4.523888888888889e-05,
      "loss": 0.005,
      "step": 8570
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 0.1839286983013153,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.0044,
      "step": 8580
    },
    {
      "epoch": 0.4772222222222222,
      "grad_norm": 0.12445003539323807,
      "learning_rate": 4.522777777777778e-05,
      "loss": 0.0063,
      "step": 8590
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 0.03297140449285507,
      "learning_rate": 4.522222222222223e-05,
      "loss": 0.0053,
      "step": 8600
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.03168568015098572,
      "learning_rate": 4.521666666666667e-05,
      "loss": 0.0075,
      "step": 8610
    },
    {
      "epoch": 0.47888888888888886,
      "grad_norm": 0.20922091603279114,
      "learning_rate": 4.5211111111111114e-05,
      "loss": 0.0063,
      "step": 8620
    },
    {
      "epoch": 0.47944444444444445,
      "grad_norm": 0.1569322794675827,
      "learning_rate": 4.520555555555556e-05,
      "loss": 0.0053,
      "step": 8630
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.4896150231361389,
      "learning_rate": 4.52e-05,
      "loss": 0.0062,
      "step": 8640
    },
    {
      "epoch": 0.48055555555555557,
      "grad_norm": 0.3068988025188446,
      "learning_rate": 4.5194444444444444e-05,
      "loss": 0.0063,
      "step": 8650
    },
    {
      "epoch": 0.4811111111111111,
      "grad_norm": 0.18724600970745087,
      "learning_rate": 4.5188888888888895e-05,
      "loss": 0.0058,
      "step": 8660
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.09271926432847977,
      "learning_rate": 4.518333333333333e-05,
      "loss": 0.0057,
      "step": 8670
    },
    {
      "epoch": 0.4822222222222222,
      "grad_norm": 0.41358789801597595,
      "learning_rate": 4.517777777777778e-05,
      "loss": 0.006,
      "step": 8680
    },
    {
      "epoch": 0.48277777777777775,
      "grad_norm": 0.06533358991146088,
      "learning_rate": 4.5172222222222225e-05,
      "loss": 0.006,
      "step": 8690
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.12754662334918976,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0052,
      "step": 8700
    },
    {
      "epoch": 0.48388888888888887,
      "grad_norm": 0.3372635245323181,
      "learning_rate": 4.516111111111111e-05,
      "loss": 0.0052,
      "step": 8710
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 0.03332250937819481,
      "learning_rate": 4.5155555555555556e-05,
      "loss": 0.0052,
      "step": 8720
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.06160597503185272,
      "learning_rate": 4.5150000000000006e-05,
      "loss": 0.0057,
      "step": 8730
    },
    {
      "epoch": 0.4855555555555556,
      "grad_norm": 0.43114590644836426,
      "learning_rate": 4.514444444444444e-05,
      "loss": 0.0054,
      "step": 8740
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 0.3273240923881531,
      "learning_rate": 4.5138888888888894e-05,
      "loss": 0.0052,
      "step": 8750
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.3675297200679779,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0065,
      "step": 8760
    },
    {
      "epoch": 0.4872222222222222,
      "grad_norm": 0.4287814795970917,
      "learning_rate": 4.512777777777778e-05,
      "loss": 0.0072,
      "step": 8770
    },
    {
      "epoch": 0.48777777777777775,
      "grad_norm": 0.3060569167137146,
      "learning_rate": 4.5122222222222224e-05,
      "loss": 0.0063,
      "step": 8780
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.5501517057418823,
      "learning_rate": 4.511666666666667e-05,
      "loss": 0.0052,
      "step": 8790
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.06454599648714066,
      "learning_rate": 4.511111111111112e-05,
      "loss": 0.0066,
      "step": 8800
    },
    {
      "epoch": 0.48944444444444446,
      "grad_norm": 0.09421224892139435,
      "learning_rate": 4.5105555555555555e-05,
      "loss": 0.0052,
      "step": 8810
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.06209389120340347,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0056,
      "step": 8820
    },
    {
      "epoch": 0.4905555555555556,
      "grad_norm": 0.3671923577785492,
      "learning_rate": 4.509444444444444e-05,
      "loss": 0.0049,
      "step": 8830
    },
    {
      "epoch": 0.4911111111111111,
      "grad_norm": 0.18338549137115479,
      "learning_rate": 4.508888888888889e-05,
      "loss": 0.0054,
      "step": 8840
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.5807515978813171,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.0055,
      "step": 8850
    },
    {
      "epoch": 0.4922222222222222,
      "grad_norm": 0.30543550848960876,
      "learning_rate": 4.507777777777778e-05,
      "loss": 0.0042,
      "step": 8860
    },
    {
      "epoch": 0.49277777777777776,
      "grad_norm": 0.15331120789051056,
      "learning_rate": 4.507222222222223e-05,
      "loss": 0.0065,
      "step": 8870
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.12267328798770905,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0053,
      "step": 8880
    },
    {
      "epoch": 0.4938888888888889,
      "grad_norm": 0.27533191442489624,
      "learning_rate": 4.506111111111112e-05,
      "loss": 0.0047,
      "step": 8890
    },
    {
      "epoch": 0.49444444444444446,
      "grad_norm": 0.12249024212360382,
      "learning_rate": 4.5055555555555554e-05,
      "loss": 0.0048,
      "step": 8900
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.24481484293937683,
      "learning_rate": 4.5050000000000004e-05,
      "loss": 0.0041,
      "step": 8910
    },
    {
      "epoch": 0.4955555555555556,
      "grad_norm": 0.30597156286239624,
      "learning_rate": 4.504444444444445e-05,
      "loss": 0.0059,
      "step": 8920
    },
    {
      "epoch": 0.4961111111111111,
      "grad_norm": 0.12235850840806961,
      "learning_rate": 4.503888888888889e-05,
      "loss": 0.0053,
      "step": 8930
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 0.06226207688450813,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0074,
      "step": 8940
    },
    {
      "epoch": 0.49722222222222223,
      "grad_norm": 0.034024376422166824,
      "learning_rate": 4.502777777777778e-05,
      "loss": 0.0061,
      "step": 8950
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.1226213350892067,
      "learning_rate": 4.502222222222223e-05,
      "loss": 0.0065,
      "step": 8960
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.1426173895597458,
      "learning_rate": 4.5016666666666665e-05,
      "loss": 0.0063,
      "step": 8970
    },
    {
      "epoch": 0.4988888888888889,
      "grad_norm": 0.5186924338340759,
      "learning_rate": 4.5011111111111116e-05,
      "loss": 0.0062,
      "step": 8980
    },
    {
      "epoch": 0.49944444444444447,
      "grad_norm": 0.18349714577198029,
      "learning_rate": 4.500555555555556e-05,
      "loss": 0.0039,
      "step": 8990
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.45935407280921936,
      "learning_rate": 4.5e-05,
      "loss": 0.0055,
      "step": 9000
    },
    {
      "epoch": 0.5005555555555555,
      "grad_norm": 0.4358980059623718,
      "learning_rate": 4.4994444444444446e-05,
      "loss": 0.0066,
      "step": 9010
    },
    {
      "epoch": 0.5011111111111111,
      "grad_norm": 0.24878935515880585,
      "learning_rate": 4.498888888888889e-05,
      "loss": 0.0072,
      "step": 9020
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.2747511863708496,
      "learning_rate": 4.4983333333333334e-05,
      "loss": 0.0055,
      "step": 9030
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 0.12291677296161652,
      "learning_rate": 4.497777777777778e-05,
      "loss": 0.0068,
      "step": 9040
    },
    {
      "epoch": 0.5027777777777778,
      "grad_norm": 0.6098487377166748,
      "learning_rate": 4.497222222222223e-05,
      "loss": 0.0072,
      "step": 9050
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.1837756484746933,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.005,
      "step": 9060
    },
    {
      "epoch": 0.5038888888888889,
      "grad_norm": 0.3055105209350586,
      "learning_rate": 4.4961111111111115e-05,
      "loss": 0.0057,
      "step": 9070
    },
    {
      "epoch": 0.5044444444444445,
      "grad_norm": 0.3668340742588043,
      "learning_rate": 4.495555555555556e-05,
      "loss": 0.0059,
      "step": 9080
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.7325976490974426,
      "learning_rate": 4.495e-05,
      "loss": 0.0047,
      "step": 9090
    },
    {
      "epoch": 0.5055555555555555,
      "grad_norm": 0.45746707916259766,
      "learning_rate": 4.4944444444444445e-05,
      "loss": 0.0048,
      "step": 9100
    },
    {
      "epoch": 0.5061111111111111,
      "grad_norm": 0.1832328587770462,
      "learning_rate": 4.493888888888889e-05,
      "loss": 0.0054,
      "step": 9110
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.12264334410429001,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0059,
      "step": 9120
    },
    {
      "epoch": 0.5072222222222222,
      "grad_norm": 0.12248674780130386,
      "learning_rate": 4.492777777777778e-05,
      "loss": 0.004,
      "step": 9130
    },
    {
      "epoch": 0.5077777777777778,
      "grad_norm": 0.12382469326257706,
      "learning_rate": 4.4922222222222226e-05,
      "loss": 0.0065,
      "step": 9140
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.21363556385040283,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.0052,
      "step": 9150
    },
    {
      "epoch": 0.5088888888888888,
      "grad_norm": 0.4266262352466583,
      "learning_rate": 4.491111111111111e-05,
      "loss": 0.0057,
      "step": 9160
    },
    {
      "epoch": 0.5094444444444445,
      "grad_norm": 0.2438865303993225,
      "learning_rate": 4.490555555555556e-05,
      "loss": 0.0067,
      "step": 9170
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.061682187020778656,
      "learning_rate": 4.49e-05,
      "loss": 0.0069,
      "step": 9180
    },
    {
      "epoch": 0.5105555555555555,
      "grad_norm": 0.18343065679073334,
      "learning_rate": 4.4894444444444444e-05,
      "loss": 0.0062,
      "step": 9190
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 0.182833731174469,
      "learning_rate": 4.4888888888888894e-05,
      "loss": 0.0075,
      "step": 9200
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.19118162989616394,
      "learning_rate": 4.488333333333333e-05,
      "loss": 0.008,
      "step": 9210
    },
    {
      "epoch": 0.5122222222222222,
      "grad_norm": 0.24453617632389069,
      "learning_rate": 4.487777777777778e-05,
      "loss": 0.0048,
      "step": 9220
    },
    {
      "epoch": 0.5127777777777778,
      "grad_norm": 0.2436203509569168,
      "learning_rate": 4.4872222222222225e-05,
      "loss": 0.0057,
      "step": 9230
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.3052506744861603,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0053,
      "step": 9240
    },
    {
      "epoch": 0.5138888888888888,
      "grad_norm": 0.2198144644498825,
      "learning_rate": 4.486111111111111e-05,
      "loss": 0.0053,
      "step": 9250
    },
    {
      "epoch": 0.5144444444444445,
      "grad_norm": 0.0643545538187027,
      "learning_rate": 4.4855555555555556e-05,
      "loss": 0.0046,
      "step": 9260
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.36568528413772583,
      "learning_rate": 4.4850000000000006e-05,
      "loss": 0.0052,
      "step": 9270
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.6091595888137817,
      "learning_rate": 4.484444444444444e-05,
      "loss": 0.0048,
      "step": 9280
    },
    {
      "epoch": 0.5161111111111111,
      "grad_norm": 0.42682400345802307,
      "learning_rate": 4.483888888888889e-05,
      "loss": 0.0059,
      "step": 9290
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.1843576580286026,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0049,
      "step": 9300
    },
    {
      "epoch": 0.5172222222222222,
      "grad_norm": 0.061311833560466766,
      "learning_rate": 4.482777777777778e-05,
      "loss": 0.006,
      "step": 9310
    },
    {
      "epoch": 0.5177777777777778,
      "grad_norm": 0.18278439342975616,
      "learning_rate": 4.4822222222222224e-05,
      "loss": 0.0063,
      "step": 9320
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.01474766992032528,
      "learning_rate": 4.481666666666667e-05,
      "loss": 0.0066,
      "step": 9330
    },
    {
      "epoch": 0.5188888888888888,
      "grad_norm": 0.016750125214457512,
      "learning_rate": 4.481111111111112e-05,
      "loss": 0.0054,
      "step": 9340
    },
    {
      "epoch": 0.5194444444444445,
      "grad_norm": 0.30496615171432495,
      "learning_rate": 4.4805555555555554e-05,
      "loss": 0.0066,
      "step": 9350
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.12309436500072479,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0064,
      "step": 9360
    },
    {
      "epoch": 0.5205555555555555,
      "grad_norm": 0.1803845316171646,
      "learning_rate": 4.479444444444444e-05,
      "loss": 0.0039,
      "step": 9370
    },
    {
      "epoch": 0.5211111111111111,
      "grad_norm": 0.06301826983690262,
      "learning_rate": 4.478888888888889e-05,
      "loss": 0.0059,
      "step": 9380
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.35831141471862793,
      "learning_rate": 4.4783333333333335e-05,
      "loss": 0.005,
      "step": 9390
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 0.06856873631477356,
      "learning_rate": 4.477777777777778e-05,
      "loss": 0.0073,
      "step": 9400
    },
    {
      "epoch": 0.5227777777777778,
      "grad_norm": 0.18255554139614105,
      "learning_rate": 4.477222222222223e-05,
      "loss": 0.0056,
      "step": 9410
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.30450618267059326,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0048,
      "step": 9420
    },
    {
      "epoch": 0.5238888888888888,
      "grad_norm": 0.3218075931072235,
      "learning_rate": 4.4761111111111116e-05,
      "loss": 0.0044,
      "step": 9430
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 0.12282992154359818,
      "learning_rate": 4.475555555555555e-05,
      "loss": 0.0051,
      "step": 9440
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.31518223881721497,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.0065,
      "step": 9450
    },
    {
      "epoch": 0.5255555555555556,
      "grad_norm": 0.37960535287857056,
      "learning_rate": 4.474444444444445e-05,
      "loss": 0.0031,
      "step": 9460
    },
    {
      "epoch": 0.5261111111111111,
      "grad_norm": 0.5040804743766785,
      "learning_rate": 4.473888888888889e-05,
      "loss": 0.0069,
      "step": 9470
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.7324831485748291,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0056,
      "step": 9480
    },
    {
      "epoch": 0.5272222222222223,
      "grad_norm": 0.6122503280639648,
      "learning_rate": 4.472777777777778e-05,
      "loss": 0.0077,
      "step": 9490
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 0.7939530611038208,
      "learning_rate": 4.472222222222223e-05,
      "loss": 0.0058,
      "step": 9500
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 0.6402150988578796,
      "learning_rate": 4.4716666666666665e-05,
      "loss": 0.0047,
      "step": 9510
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 0.1833006739616394,
      "learning_rate": 4.4711111111111115e-05,
      "loss": 0.0073,
      "step": 9520
    },
    {
      "epoch": 0.5294444444444445,
      "grad_norm": 0.12203707545995712,
      "learning_rate": 4.470555555555556e-05,
      "loss": 0.005,
      "step": 9530
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.3654622733592987,
      "learning_rate": 4.47e-05,
      "loss": 0.0064,
      "step": 9540
    },
    {
      "epoch": 0.5305555555555556,
      "grad_norm": 0.46052590012550354,
      "learning_rate": 4.4694444444444446e-05,
      "loss": 0.0051,
      "step": 9550
    },
    {
      "epoch": 0.5311111111111111,
      "grad_norm": 0.5782521963119507,
      "learning_rate": 4.468888888888889e-05,
      "loss": 0.0048,
      "step": 9560
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.1522863656282425,
      "learning_rate": 4.468333333333334e-05,
      "loss": 0.0049,
      "step": 9570
    },
    {
      "epoch": 0.5322222222222223,
      "grad_norm": 0.5471071600914001,
      "learning_rate": 4.4677777777777777e-05,
      "loss": 0.0064,
      "step": 9580
    },
    {
      "epoch": 0.5327777777777778,
      "grad_norm": 0.12536662817001343,
      "learning_rate": 4.467222222222223e-05,
      "loss": 0.0048,
      "step": 9590
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.06390558928251266,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0063,
      "step": 9600
    },
    {
      "epoch": 0.5338888888888889,
      "grad_norm": 0.4882946014404297,
      "learning_rate": 4.4661111111111114e-05,
      "loss": 0.0061,
      "step": 9610
    },
    {
      "epoch": 0.5344444444444445,
      "grad_norm": 0.12303663790225983,
      "learning_rate": 4.465555555555556e-05,
      "loss": 0.0055,
      "step": 9620
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.305530309677124,
      "learning_rate": 4.465e-05,
      "loss": 0.0067,
      "step": 9630
    },
    {
      "epoch": 0.5355555555555556,
      "grad_norm": 0.12270410358905792,
      "learning_rate": 4.4644444444444445e-05,
      "loss": 0.0063,
      "step": 9640
    },
    {
      "epoch": 0.5361111111111111,
      "grad_norm": 0.30597445368766785,
      "learning_rate": 4.463888888888889e-05,
      "loss": 0.0075,
      "step": 9650
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.3049986660480499,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0037,
      "step": 9660
    },
    {
      "epoch": 0.5372222222222223,
      "grad_norm": 0.42796939611434937,
      "learning_rate": 4.462777777777778e-05,
      "loss": 0.0077,
      "step": 9670
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 0.007285755593329668,
      "learning_rate": 4.4622222222222226e-05,
      "loss": 0.0046,
      "step": 9680
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.25004035234451294,
      "learning_rate": 4.461666666666667e-05,
      "loss": 0.0059,
      "step": 9690
    },
    {
      "epoch": 0.5388888888888889,
      "grad_norm": 0.25928041338920593,
      "learning_rate": 4.461111111111111e-05,
      "loss": 0.0056,
      "step": 9700
    },
    {
      "epoch": 0.5394444444444444,
      "grad_norm": 0.06245873123407364,
      "learning_rate": 4.4605555555555556e-05,
      "loss": 0.0066,
      "step": 9710
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.30514007806777954,
      "learning_rate": 4.46e-05,
      "loss": 0.0063,
      "step": 9720
    },
    {
      "epoch": 0.5405555555555556,
      "grad_norm": 0.21384915709495544,
      "learning_rate": 4.4594444444444443e-05,
      "loss": 0.0053,
      "step": 9730
    },
    {
      "epoch": 0.5411111111111111,
      "grad_norm": 0.12202110886573792,
      "learning_rate": 4.4588888888888894e-05,
      "loss": 0.0048,
      "step": 9740
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.03721792995929718,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.0058,
      "step": 9750
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 0.3274230360984802,
      "learning_rate": 4.457777777777778e-05,
      "loss": 0.0066,
      "step": 9760
    },
    {
      "epoch": 0.5427777777777778,
      "grad_norm": 0.30446869134902954,
      "learning_rate": 4.4572222222222224e-05,
      "loss": 0.0052,
      "step": 9770
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.4875074028968811,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0046,
      "step": 9780
    },
    {
      "epoch": 0.5438888888888889,
      "grad_norm": 0.2750290334224701,
      "learning_rate": 4.456111111111111e-05,
      "loss": 0.0047,
      "step": 9790
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.6095555424690247,
      "learning_rate": 4.4555555555555555e-05,
      "loss": 0.0038,
      "step": 9800
    },
    {
      "epoch": 0.545,
      "grad_norm": 0.2966424226760864,
      "learning_rate": 4.4550000000000005e-05,
      "loss": 0.0042,
      "step": 9810
    },
    {
      "epoch": 0.5455555555555556,
      "grad_norm": 0.24355803430080414,
      "learning_rate": 4.454444444444444e-05,
      "loss": 0.006,
      "step": 9820
    },
    {
      "epoch": 0.5461111111111111,
      "grad_norm": 0.012844137847423553,
      "learning_rate": 4.453888888888889e-05,
      "loss": 0.0061,
      "step": 9830
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.4564920663833618,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0061,
      "step": 9840
    },
    {
      "epoch": 0.5472222222222223,
      "grad_norm": 0.02481169067323208,
      "learning_rate": 4.452777777777778e-05,
      "loss": 0.0055,
      "step": 9850
    },
    {
      "epoch": 0.5477777777777778,
      "grad_norm": 0.21668852865695953,
      "learning_rate": 4.452222222222222e-05,
      "loss": 0.0058,
      "step": 9860
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.24390779435634613,
      "learning_rate": 4.451666666666667e-05,
      "loss": 0.005,
      "step": 9870
    },
    {
      "epoch": 0.5488888888888889,
      "grad_norm": 0.24325205385684967,
      "learning_rate": 4.451111111111112e-05,
      "loss": 0.0054,
      "step": 9880
    },
    {
      "epoch": 0.5494444444444444,
      "grad_norm": 0.30512452125549316,
      "learning_rate": 4.4505555555555554e-05,
      "loss": 0.0048,
      "step": 9890
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.3959452509880066,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.0068,
      "step": 9900
    },
    {
      "epoch": 0.5505555555555556,
      "grad_norm": 0.18341811001300812,
      "learning_rate": 4.449444444444444e-05,
      "loss": 0.0053,
      "step": 9910
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.5469512939453125,
      "learning_rate": 4.448888888888889e-05,
      "loss": 0.0059,
      "step": 9920
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.18287917971611023,
      "learning_rate": 4.4483333333333335e-05,
      "loss": 0.006,
      "step": 9930
    },
    {
      "epoch": 0.5522222222222222,
      "grad_norm": 0.42683714628219604,
      "learning_rate": 4.447777777777778e-05,
      "loss": 0.0061,
      "step": 9940
    },
    {
      "epoch": 0.5527777777777778,
      "grad_norm": 0.3041364550590515,
      "learning_rate": 4.447222222222223e-05,
      "loss": 0.0045,
      "step": 9950
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.3864634037017822,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0055,
      "step": 9960
    },
    {
      "epoch": 0.5538888888888889,
      "grad_norm": 0.3037338852882385,
      "learning_rate": 4.4461111111111116e-05,
      "loss": 0.0041,
      "step": 9970
    },
    {
      "epoch": 0.5544444444444444,
      "grad_norm": 0.12259913235902786,
      "learning_rate": 4.445555555555555e-05,
      "loss": 0.0047,
      "step": 9980
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.18729443848133087,
      "learning_rate": 4.445e-05,
      "loss": 0.0049,
      "step": 9990
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.3037923574447632,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.0054,
      "step": 10000
    },
    {
      "epoch": 0.5561111111111111,
      "grad_norm": 0.24837568402290344,
      "learning_rate": 4.443888888888889e-05,
      "loss": 0.0055,
      "step": 10010
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.1458044797182083,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.0058,
      "step": 10020
    },
    {
      "epoch": 0.5572222222222222,
      "grad_norm": 0.09309959411621094,
      "learning_rate": 4.442777777777778e-05,
      "loss": 0.0043,
      "step": 10030
    },
    {
      "epoch": 0.5577777777777778,
      "grad_norm": 0.09074755012989044,
      "learning_rate": 4.442222222222223e-05,
      "loss": 0.0066,
      "step": 10040
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.18314330279827118,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.0045,
      "step": 10050
    },
    {
      "epoch": 0.5588888888888889,
      "grad_norm": 0.3649800717830658,
      "learning_rate": 4.4411111111111115e-05,
      "loss": 0.0053,
      "step": 10060
    },
    {
      "epoch": 0.5594444444444444,
      "grad_norm": 0.07949381321668625,
      "learning_rate": 4.440555555555556e-05,
      "loss": 0.0063,
      "step": 10070
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.4861551821231842,
      "learning_rate": 4.44e-05,
      "loss": 0.0065,
      "step": 10080
    },
    {
      "epoch": 0.5605555555555556,
      "grad_norm": 0.583866536617279,
      "learning_rate": 4.4394444444444445e-05,
      "loss": 0.0039,
      "step": 10090
    },
    {
      "epoch": 0.5611111111111111,
      "grad_norm": 0.12184605002403259,
      "learning_rate": 4.438888888888889e-05,
      "loss": 0.0062,
      "step": 10100
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.243386909365654,
      "learning_rate": 4.438333333333334e-05,
      "loss": 0.0046,
      "step": 10110
    },
    {
      "epoch": 0.5622222222222222,
      "grad_norm": 0.48728716373443604,
      "learning_rate": 4.4377777777777776e-05,
      "loss": 0.0045,
      "step": 10120
    },
    {
      "epoch": 0.5627777777777778,
      "grad_norm": 0.07548663765192032,
      "learning_rate": 4.4372222222222226e-05,
      "loss": 0.0063,
      "step": 10130
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.24423429369926453,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.0073,
      "step": 10140
    },
    {
      "epoch": 0.5638888888888889,
      "grad_norm": 0.15680968761444092,
      "learning_rate": 4.4361111111111113e-05,
      "loss": 0.0064,
      "step": 10150
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 0.3588404357433319,
      "learning_rate": 4.435555555555556e-05,
      "loss": 0.0053,
      "step": 10160
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.22539034485816956,
      "learning_rate": 4.435e-05,
      "loss": 0.0052,
      "step": 10170
    },
    {
      "epoch": 0.5655555555555556,
      "grad_norm": 0.3400474190711975,
      "learning_rate": 4.4344444444444444e-05,
      "loss": 0.0045,
      "step": 10180
    },
    {
      "epoch": 0.5661111111111111,
      "grad_norm": 0.6703149080276489,
      "learning_rate": 4.433888888888889e-05,
      "loss": 0.0043,
      "step": 10190
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.2438412755727768,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0051,
      "step": 10200
    },
    {
      "epoch": 0.5672222222222222,
      "grad_norm": 0.0617683082818985,
      "learning_rate": 4.432777777777778e-05,
      "loss": 0.0055,
      "step": 10210
    },
    {
      "epoch": 0.5677777777777778,
      "grad_norm": 0.48699137568473816,
      "learning_rate": 4.4322222222222225e-05,
      "loss": 0.0073,
      "step": 10220
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.19149307906627655,
      "learning_rate": 4.431666666666667e-05,
      "loss": 0.0062,
      "step": 10230
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.307416170835495,
      "learning_rate": 4.431111111111111e-05,
      "loss": 0.007,
      "step": 10240
    },
    {
      "epoch": 0.5694444444444444,
      "grad_norm": 0.09205113351345062,
      "learning_rate": 4.4305555555555556e-05,
      "loss": 0.0057,
      "step": 10250
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.03790636733174324,
      "learning_rate": 4.43e-05,
      "loss": 0.0074,
      "step": 10260
    },
    {
      "epoch": 0.5705555555555556,
      "grad_norm": 0.22441354393959045,
      "learning_rate": 4.429444444444444e-05,
      "loss": 0.0062,
      "step": 10270
    },
    {
      "epoch": 0.5711111111111111,
      "grad_norm": 0.4256991446018219,
      "learning_rate": 4.428888888888889e-05,
      "loss": 0.0063,
      "step": 10280
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.15307772159576416,
      "learning_rate": 4.428333333333334e-05,
      "loss": 0.0068,
      "step": 10290
    },
    {
      "epoch": 0.5722222222222222,
      "grad_norm": 0.5495141744613647,
      "learning_rate": 4.427777777777778e-05,
      "loss": 0.0058,
      "step": 10300
    },
    {
      "epoch": 0.5727777777777778,
      "grad_norm": 0.2434522956609726,
      "learning_rate": 4.4272222222222224e-05,
      "loss": 0.004,
      "step": 10310
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.24389424920082092,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0057,
      "step": 10320
    },
    {
      "epoch": 0.5738888888888889,
      "grad_norm": 0.1221020370721817,
      "learning_rate": 4.426111111111111e-05,
      "loss": 0.0062,
      "step": 10330
    },
    {
      "epoch": 0.5744444444444444,
      "grad_norm": 0.3040318191051483,
      "learning_rate": 4.4255555555555555e-05,
      "loss": 0.0055,
      "step": 10340
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.15220579504966736,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.0071,
      "step": 10350
    },
    {
      "epoch": 0.5755555555555556,
      "grad_norm": 0.3642650246620178,
      "learning_rate": 4.424444444444444e-05,
      "loss": 0.0063,
      "step": 10360
    },
    {
      "epoch": 0.5761111111111111,
      "grad_norm": 0.18292616307735443,
      "learning_rate": 4.423888888888889e-05,
      "loss": 0.0058,
      "step": 10370
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.42572543025016785,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0054,
      "step": 10380
    },
    {
      "epoch": 0.5772222222222222,
      "grad_norm": 0.6916205286979675,
      "learning_rate": 4.422777777777778e-05,
      "loss": 0.005,
      "step": 10390
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.24805721640586853,
      "learning_rate": 4.422222222222222e-05,
      "loss": 0.0068,
      "step": 10400
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.15240347385406494,
      "learning_rate": 4.4216666666666666e-05,
      "loss": 0.0058,
      "step": 10410
    },
    {
      "epoch": 0.5788888888888889,
      "grad_norm": 0.36608850955963135,
      "learning_rate": 4.4211111111111117e-05,
      "loss": 0.0047,
      "step": 10420
    },
    {
      "epoch": 0.5794444444444444,
      "grad_norm": 0.15287809073925018,
      "learning_rate": 4.420555555555555e-05,
      "loss": 0.0059,
      "step": 10430
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.153609961271286,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0067,
      "step": 10440
    },
    {
      "epoch": 0.5805555555555556,
      "grad_norm": 0.18301978707313538,
      "learning_rate": 4.419444444444444e-05,
      "loss": 0.0051,
      "step": 10450
    },
    {
      "epoch": 0.5811111111111111,
      "grad_norm": 0.21252867579460144,
      "learning_rate": 4.418888888888889e-05,
      "loss": 0.006,
      "step": 10460
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.3041200637817383,
      "learning_rate": 4.4183333333333334e-05,
      "loss": 0.0046,
      "step": 10470
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 0.29208651185035706,
      "learning_rate": 4.417777777777778e-05,
      "loss": 0.0058,
      "step": 10480
    },
    {
      "epoch": 0.5827777777777777,
      "grad_norm": 0.01681559719145298,
      "learning_rate": 4.417222222222223e-05,
      "loss": 0.0062,
      "step": 10490
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.06539814174175262,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0059,
      "step": 10500
    },
    {
      "epoch": 0.5838888888888889,
      "grad_norm": 0.36479344964027405,
      "learning_rate": 4.4161111111111115e-05,
      "loss": 0.0046,
      "step": 10510
    },
    {
      "epoch": 0.5844444444444444,
      "grad_norm": 0.0619284063577652,
      "learning_rate": 4.415555555555556e-05,
      "loss": 0.0052,
      "step": 10520
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.21941624581813812,
      "learning_rate": 4.415e-05,
      "loss": 0.0069,
      "step": 10530
    },
    {
      "epoch": 0.5855555555555556,
      "grad_norm": 0.48633021116256714,
      "learning_rate": 4.4144444444444446e-05,
      "loss": 0.0072,
      "step": 10540
    },
    {
      "epoch": 0.5861111111111111,
      "grad_norm": 0.06174401193857193,
      "learning_rate": 4.413888888888889e-05,
      "loss": 0.0069,
      "step": 10550
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.19702835381031036,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.005,
      "step": 10560
    },
    {
      "epoch": 0.5872222222222222,
      "grad_norm": 0.4871658682823181,
      "learning_rate": 4.412777777777778e-05,
      "loss": 0.0078,
      "step": 10570
    },
    {
      "epoch": 0.5877777777777777,
      "grad_norm": 0.18229685723781586,
      "learning_rate": 4.412222222222223e-05,
      "loss": 0.005,
      "step": 10580
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.3051631450653076,
      "learning_rate": 4.411666666666667e-05,
      "loss": 0.0061,
      "step": 10590
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.18288496136665344,
      "learning_rate": 4.4111111111111114e-05,
      "loss": 0.0061,
      "step": 10600
    },
    {
      "epoch": 0.5894444444444444,
      "grad_norm": 0.42767730355262756,
      "learning_rate": 4.410555555555556e-05,
      "loss": 0.006,
      "step": 10610
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.25542548298835754,
      "learning_rate": 4.41e-05,
      "loss": 0.0065,
      "step": 10620
    },
    {
      "epoch": 0.5905555555555555,
      "grad_norm": 0.012229274958372116,
      "learning_rate": 4.409444444444445e-05,
      "loss": 0.0078,
      "step": 10630
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 0.062176965177059174,
      "learning_rate": 4.408888888888889e-05,
      "loss": 0.0057,
      "step": 10640
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.36590859293937683,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.0049,
      "step": 10650
    },
    {
      "epoch": 0.5922222222222222,
      "grad_norm": 0.33587446808815,
      "learning_rate": 4.407777777777778e-05,
      "loss": 0.005,
      "step": 10660
    },
    {
      "epoch": 0.5927777777777777,
      "grad_norm": 0.06344594061374664,
      "learning_rate": 4.4072222222222226e-05,
      "loss": 0.0059,
      "step": 10670
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.3139674961566925,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0068,
      "step": 10680
    },
    {
      "epoch": 0.5938888888888889,
      "grad_norm": 0.06308358907699585,
      "learning_rate": 4.406111111111111e-05,
      "loss": 0.0056,
      "step": 10690
    },
    {
      "epoch": 0.5944444444444444,
      "grad_norm": 0.13476784527301788,
      "learning_rate": 4.4055555555555557e-05,
      "loss": 0.0057,
      "step": 10700
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.3347259759902954,
      "learning_rate": 4.405e-05,
      "loss": 0.0054,
      "step": 10710
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 0.12172271311283112,
      "learning_rate": 4.404444444444445e-05,
      "loss": 0.0081,
      "step": 10720
    },
    {
      "epoch": 0.5961111111111111,
      "grad_norm": 0.18207499384880066,
      "learning_rate": 4.4038888888888894e-05,
      "loss": 0.0071,
      "step": 10730
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.3030504882335663,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.0051,
      "step": 10740
    },
    {
      "epoch": 0.5972222222222222,
      "grad_norm": 0.43525487184524536,
      "learning_rate": 4.402777777777778e-05,
      "loss": 0.0071,
      "step": 10750
    },
    {
      "epoch": 0.5977777777777777,
      "grad_norm": 0.43490925431251526,
      "learning_rate": 4.4022222222222225e-05,
      "loss": 0.0068,
      "step": 10760
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.5479924082756042,
      "learning_rate": 4.401666666666667e-05,
      "loss": 0.0048,
      "step": 10770
    },
    {
      "epoch": 0.5988888888888889,
      "grad_norm": 0.06484203040599823,
      "learning_rate": 4.401111111111111e-05,
      "loss": 0.0061,
      "step": 10780
    },
    {
      "epoch": 0.5994444444444444,
      "grad_norm": 0.062380194664001465,
      "learning_rate": 4.4005555555555555e-05,
      "loss": 0.0045,
      "step": 10790
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.062173232436180115,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0057,
      "step": 10800
    },
    {
      "epoch": 0.6005555555555555,
      "grad_norm": 0.3332742750644684,
      "learning_rate": 4.399444444444445e-05,
      "loss": 0.0054,
      "step": 10810
    },
    {
      "epoch": 0.6011111111111112,
      "grad_norm": 0.3638410270214081,
      "learning_rate": 4.398888888888889e-05,
      "loss": 0.0057,
      "step": 10820
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.06169137731194496,
      "learning_rate": 4.3983333333333336e-05,
      "loss": 0.0076,
      "step": 10830
    },
    {
      "epoch": 0.6022222222222222,
      "grad_norm": 0.032244715839624405,
      "learning_rate": 4.397777777777778e-05,
      "loss": 0.0053,
      "step": 10840
    },
    {
      "epoch": 0.6027777777777777,
      "grad_norm": 0.12247167527675629,
      "learning_rate": 4.3972222222222223e-05,
      "loss": 0.0047,
      "step": 10850
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.12321990728378296,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0063,
      "step": 10860
    },
    {
      "epoch": 0.6038888888888889,
      "grad_norm": 0.27372509241104126,
      "learning_rate": 4.396111111111112e-05,
      "loss": 0.0058,
      "step": 10870
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.27437806129455566,
      "learning_rate": 4.3955555555555554e-05,
      "loss": 0.0079,
      "step": 10880
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.09153064340353012,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 0.0044,
      "step": 10890
    },
    {
      "epoch": 0.6055555555555555,
      "grad_norm": 0.15230394899845123,
      "learning_rate": 4.394444444444445e-05,
      "loss": 0.0043,
      "step": 10900
    },
    {
      "epoch": 0.6061111111111112,
      "grad_norm": 0.2815845012664795,
      "learning_rate": 4.393888888888889e-05,
      "loss": 0.0047,
      "step": 10910
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.11208905279636383,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0069,
      "step": 10920
    },
    {
      "epoch": 0.6072222222222222,
      "grad_norm": 0.5161484479904175,
      "learning_rate": 4.392777777777778e-05,
      "loss": 0.006,
      "step": 10930
    },
    {
      "epoch": 0.6077777777777778,
      "grad_norm": 0.45582783222198486,
      "learning_rate": 4.392222222222223e-05,
      "loss": 0.0055,
      "step": 10940
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.7279840707778931,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.0057,
      "step": 10950
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 0.24883291125297546,
      "learning_rate": 4.3911111111111116e-05,
      "loss": 0.0068,
      "step": 10960
    },
    {
      "epoch": 0.6094444444444445,
      "grad_norm": 0.3708714544773102,
      "learning_rate": 4.390555555555555e-05,
      "loss": 0.0039,
      "step": 10970
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.42754167318344116,
      "learning_rate": 4.39e-05,
      "loss": 0.0056,
      "step": 10980
    },
    {
      "epoch": 0.6105555555555555,
      "grad_norm": 0.320027232170105,
      "learning_rate": 4.389444444444445e-05,
      "loss": 0.0043,
      "step": 10990
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.4996100962162018,
      "learning_rate": 4.388888888888889e-05,
      "loss": 0.005,
      "step": 11000
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.24392957985401154,
      "learning_rate": 4.388333333333334e-05,
      "loss": 0.0061,
      "step": 11010
    },
    {
      "epoch": 0.6122222222222222,
      "grad_norm": 0.12372582405805588,
      "learning_rate": 4.387777777777778e-05,
      "loss": 0.0068,
      "step": 11020
    },
    {
      "epoch": 0.6127777777777778,
      "grad_norm": 0.48918887972831726,
      "learning_rate": 4.387222222222223e-05,
      "loss": 0.0061,
      "step": 11030
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.410826176404953,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0044,
      "step": 11040
    },
    {
      "epoch": 0.6138888888888889,
      "grad_norm": 0.030957169830799103,
      "learning_rate": 4.3861111111111115e-05,
      "loss": 0.0057,
      "step": 11050
    },
    {
      "epoch": 0.6144444444444445,
      "grad_norm": 0.291591614484787,
      "learning_rate": 4.385555555555556e-05,
      "loss": 0.0049,
      "step": 11060
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.12256351858377457,
      "learning_rate": 4.385e-05,
      "loss": 0.0054,
      "step": 11070
    },
    {
      "epoch": 0.6155555555555555,
      "grad_norm": 0.12234704196453094,
      "learning_rate": 4.384444444444445e-05,
      "loss": 0.005,
      "step": 11080
    },
    {
      "epoch": 0.6161111111111112,
      "grad_norm": 0.03272430598735809,
      "learning_rate": 4.383888888888889e-05,
      "loss": 0.0055,
      "step": 11090
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.2741861641407013,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.0071,
      "step": 11100
    },
    {
      "epoch": 0.6172222222222222,
      "grad_norm": 0.10452278703451157,
      "learning_rate": 4.3827777777777776e-05,
      "loss": 0.0063,
      "step": 11110
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 0.1218913272023201,
      "learning_rate": 4.3822222222222227e-05,
      "loss": 0.0048,
      "step": 11120
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.0197356715798378,
      "learning_rate": 4.381666666666667e-05,
      "loss": 0.0068,
      "step": 11130
    },
    {
      "epoch": 0.6188888888888889,
      "grad_norm": 0.12201133370399475,
      "learning_rate": 4.3811111111111114e-05,
      "loss": 0.0043,
      "step": 11140
    },
    {
      "epoch": 0.6194444444444445,
      "grad_norm": 0.06224583834409714,
      "learning_rate": 4.380555555555556e-05,
      "loss": 0.005,
      "step": 11150
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.18231739103794098,
      "learning_rate": 4.38e-05,
      "loss": 0.0032,
      "step": 11160
    },
    {
      "epoch": 0.6205555555555555,
      "grad_norm": 0.12513293325901031,
      "learning_rate": 4.379444444444445e-05,
      "loss": 0.0046,
      "step": 11170
    },
    {
      "epoch": 0.6211111111111111,
      "grad_norm": 0.18299438059329987,
      "learning_rate": 4.378888888888889e-05,
      "loss": 0.0038,
      "step": 11180
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.3658043146133423,
      "learning_rate": 4.378333333333334e-05,
      "loss": 0.0052,
      "step": 11190
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.017813770100474358,
      "learning_rate": 4.377777777777778e-05,
      "loss": 0.0039,
      "step": 11200
    },
    {
      "epoch": 0.6227777777777778,
      "grad_norm": 0.15494799613952637,
      "learning_rate": 4.3772222222222225e-05,
      "loss": 0.0067,
      "step": 11210
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.12240374088287354,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0053,
      "step": 11220
    },
    {
      "epoch": 0.6238888888888889,
      "grad_norm": 0.27450746297836304,
      "learning_rate": 4.376111111111111e-05,
      "loss": 0.0047,
      "step": 11230
    },
    {
      "epoch": 0.6244444444444445,
      "grad_norm": 0.10853453725576401,
      "learning_rate": 4.3755555555555556e-05,
      "loss": 0.0036,
      "step": 11240
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.02436099201440811,
      "learning_rate": 4.375e-05,
      "loss": 0.0042,
      "step": 11250
    },
    {
      "epoch": 0.6255555555555555,
      "grad_norm": 0.2427736073732376,
      "learning_rate": 4.374444444444445e-05,
      "loss": 0.0054,
      "step": 11260
    },
    {
      "epoch": 0.6261111111111111,
      "grad_norm": 0.1834656000137329,
      "learning_rate": 4.3738888888888893e-05,
      "loss": 0.0046,
      "step": 11270
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.30328139662742615,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0068,
      "step": 11280
    },
    {
      "epoch": 0.6272222222222222,
      "grad_norm": 0.5047001242637634,
      "learning_rate": 4.372777777777778e-05,
      "loss": 0.0071,
      "step": 11290
    },
    {
      "epoch": 0.6277777777777778,
      "grad_norm": 0.18422619998455048,
      "learning_rate": 4.3722222222222224e-05,
      "loss": 0.0056,
      "step": 11300
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.12092659622430801,
      "learning_rate": 4.371666666666667e-05,
      "loss": 0.0038,
      "step": 11310
    },
    {
      "epoch": 0.6288888888888889,
      "grad_norm": 0.3337448835372925,
      "learning_rate": 4.371111111111111e-05,
      "loss": 0.006,
      "step": 11320
    },
    {
      "epoch": 0.6294444444444445,
      "grad_norm": 0.15173915028572083,
      "learning_rate": 4.3705555555555555e-05,
      "loss": 0.0061,
      "step": 11330
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.1856965273618698,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.0055,
      "step": 11340
    },
    {
      "epoch": 0.6305555555555555,
      "grad_norm": 0.3635244369506836,
      "learning_rate": 4.369444444444445e-05,
      "loss": 0.0049,
      "step": 11350
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 0.12292414158582687,
      "learning_rate": 4.368888888888889e-05,
      "loss": 0.0071,
      "step": 11360
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.12312333285808563,
      "learning_rate": 4.3683333333333336e-05,
      "loss": 0.0043,
      "step": 11370
    },
    {
      "epoch": 0.6322222222222222,
      "grad_norm": 0.032495878636837006,
      "learning_rate": 4.367777777777778e-05,
      "loss": 0.0051,
      "step": 11380
    },
    {
      "epoch": 0.6327777777777778,
      "grad_norm": 0.06292308121919632,
      "learning_rate": 4.367222222222222e-05,
      "loss": 0.006,
      "step": 11390
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.3129310607910156,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0073,
      "step": 11400
    },
    {
      "epoch": 0.6338888888888888,
      "grad_norm": 0.6756174564361572,
      "learning_rate": 4.366111111111112e-05,
      "loss": 0.0043,
      "step": 11410
    },
    {
      "epoch": 0.6344444444444445,
      "grad_norm": 0.09193392097949982,
      "learning_rate": 4.3655555555555554e-05,
      "loss": 0.0049,
      "step": 11420
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.3053055703639984,
      "learning_rate": 4.3650000000000004e-05,
      "loss": 0.0046,
      "step": 11430
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 0.4263792932033539,
      "learning_rate": 4.364444444444445e-05,
      "loss": 0.0081,
      "step": 11440
    },
    {
      "epoch": 0.6361111111111111,
      "grad_norm": 0.14336909353733063,
      "learning_rate": 4.363888888888889e-05,
      "loss": 0.0063,
      "step": 11450
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.24458430707454681,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0045,
      "step": 11460
    },
    {
      "epoch": 0.6372222222222222,
      "grad_norm": 0.3658013343811035,
      "learning_rate": 4.362777777777778e-05,
      "loss": 0.0077,
      "step": 11470
    },
    {
      "epoch": 0.6377777777777778,
      "grad_norm": 0.4882510304450989,
      "learning_rate": 4.362222222222223e-05,
      "loss": 0.006,
      "step": 11480
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.1231110543012619,
      "learning_rate": 4.3616666666666665e-05,
      "loss": 0.0056,
      "step": 11490
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 0.1221589669585228,
      "learning_rate": 4.3611111111111116e-05,
      "loss": 0.0068,
      "step": 11500
    },
    {
      "epoch": 0.6394444444444445,
      "grad_norm": 0.00998570118099451,
      "learning_rate": 4.360555555555555e-05,
      "loss": 0.0049,
      "step": 11510
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.013783522881567478,
      "learning_rate": 4.36e-05,
      "loss": 0.0057,
      "step": 11520
    },
    {
      "epoch": 0.6405555555555555,
      "grad_norm": 0.05885590240359306,
      "learning_rate": 4.3594444444444446e-05,
      "loss": 0.0052,
      "step": 11530
    },
    {
      "epoch": 0.6411111111111111,
      "grad_norm": 0.09143659472465515,
      "learning_rate": 4.358888888888889e-05,
      "loss": 0.0055,
      "step": 11540
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.1216074749827385,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.0051,
      "step": 11550
    },
    {
      "epoch": 0.6422222222222222,
      "grad_norm": 0.5454078316688538,
      "learning_rate": 4.357777777777778e-05,
      "loss": 0.0055,
      "step": 11560
    },
    {
      "epoch": 0.6427777777777778,
      "grad_norm": 0.4259806275367737,
      "learning_rate": 4.357222222222223e-05,
      "loss": 0.0062,
      "step": 11570
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.15239952504634857,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.0059,
      "step": 11580
    },
    {
      "epoch": 0.6438888888888888,
      "grad_norm": 0.6066471934318542,
      "learning_rate": 4.3561111111111114e-05,
      "loss": 0.0043,
      "step": 11590
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.0771309956908226,
      "learning_rate": 4.355555555555556e-05,
      "loss": 0.0066,
      "step": 11600
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.35834604501724243,
      "learning_rate": 4.355e-05,
      "loss": 0.0066,
      "step": 11610
    },
    {
      "epoch": 0.6455555555555555,
      "grad_norm": 0.1218920648097992,
      "learning_rate": 4.354444444444445e-05,
      "loss": 0.0057,
      "step": 11620
    },
    {
      "epoch": 0.6461111111111111,
      "grad_norm": 0.01481377612799406,
      "learning_rate": 4.353888888888889e-05,
      "loss": 0.0057,
      "step": 11630
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.3638647794723511,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.004,
      "step": 11640
    },
    {
      "epoch": 0.6472222222222223,
      "grad_norm": 0.01129087619483471,
      "learning_rate": 4.3527777777777776e-05,
      "loss": 0.0055,
      "step": 11650
    },
    {
      "epoch": 0.6477777777777778,
      "grad_norm": 0.24293921887874603,
      "learning_rate": 4.3522222222222226e-05,
      "loss": 0.0066,
      "step": 11660
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.31596073508262634,
      "learning_rate": 4.351666666666667e-05,
      "loss": 0.0054,
      "step": 11670
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 0.0606568269431591,
      "learning_rate": 4.351111111111111e-05,
      "loss": 0.0044,
      "step": 11680
    },
    {
      "epoch": 0.6494444444444445,
      "grad_norm": 0.09479454159736633,
      "learning_rate": 4.350555555555556e-05,
      "loss": 0.0069,
      "step": 11690
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.5444892048835754,
      "learning_rate": 4.35e-05,
      "loss": 0.0057,
      "step": 11700
    },
    {
      "epoch": 0.6505555555555556,
      "grad_norm": 0.4242520034313202,
      "learning_rate": 4.349444444444445e-05,
      "loss": 0.0056,
      "step": 11710
    },
    {
      "epoch": 0.6511111111111111,
      "grad_norm": 0.06202200800180435,
      "learning_rate": 4.348888888888889e-05,
      "loss": 0.0037,
      "step": 11720
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.4954378008842468,
      "learning_rate": 4.348333333333334e-05,
      "loss": 0.0063,
      "step": 11730
    },
    {
      "epoch": 0.6522222222222223,
      "grad_norm": 0.061576005071401596,
      "learning_rate": 4.347777777777778e-05,
      "loss": 0.0054,
      "step": 11740
    },
    {
      "epoch": 0.6527777777777778,
      "grad_norm": 0.15193121135234833,
      "learning_rate": 4.3472222222222225e-05,
      "loss": 0.004,
      "step": 11750
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.42524030804634094,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0051,
      "step": 11760
    },
    {
      "epoch": 0.6538888888888889,
      "grad_norm": 0.4243360459804535,
      "learning_rate": 4.346111111111111e-05,
      "loss": 0.0064,
      "step": 11770
    },
    {
      "epoch": 0.6544444444444445,
      "grad_norm": 0.4882386326789856,
      "learning_rate": 4.3455555555555555e-05,
      "loss": 0.0032,
      "step": 11780
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.304162859916687,
      "learning_rate": 4.345e-05,
      "loss": 0.008,
      "step": 11790
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 0.17054283618927002,
      "learning_rate": 4.344444444444445e-05,
      "loss": 0.0063,
      "step": 11800
    },
    {
      "epoch": 0.6561111111111111,
      "grad_norm": 0.1010885164141655,
      "learning_rate": 4.343888888888889e-05,
      "loss": 0.0055,
      "step": 11810
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.49023252725601196,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0045,
      "step": 11820
    },
    {
      "epoch": 0.6572222222222223,
      "grad_norm": 0.20050908625125885,
      "learning_rate": 4.342777777777778e-05,
      "loss": 0.0055,
      "step": 11830
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 0.4275846779346466,
      "learning_rate": 4.3422222222222224e-05,
      "loss": 0.006,
      "step": 11840
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.0617363415658474,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.0059,
      "step": 11850
    },
    {
      "epoch": 0.6588888888888889,
      "grad_norm": 0.18300403654575348,
      "learning_rate": 4.341111111111111e-05,
      "loss": 0.005,
      "step": 11860
    },
    {
      "epoch": 0.6594444444444445,
      "grad_norm": 0.48413175344467163,
      "learning_rate": 4.3405555555555554e-05,
      "loss": 0.0069,
      "step": 11870
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.15170718729496002,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0063,
      "step": 11880
    },
    {
      "epoch": 0.6605555555555556,
      "grad_norm": 0.42411333322525024,
      "learning_rate": 4.339444444444445e-05,
      "loss": 0.0054,
      "step": 11890
    },
    {
      "epoch": 0.6611111111111111,
      "grad_norm": 0.27234554290771484,
      "learning_rate": 4.338888888888889e-05,
      "loss": 0.004,
      "step": 11900
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.2735700309276581,
      "learning_rate": 4.3383333333333335e-05,
      "loss": 0.0054,
      "step": 11910
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 0.36381277441978455,
      "learning_rate": 4.337777777777778e-05,
      "loss": 0.0058,
      "step": 11920
    },
    {
      "epoch": 0.6627777777777778,
      "grad_norm": 0.03770091012120247,
      "learning_rate": 4.337222222222222e-05,
      "loss": 0.0055,
      "step": 11930
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.24264320731163025,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0045,
      "step": 11940
    },
    {
      "epoch": 0.6638888888888889,
      "grad_norm": 0.09270289540290833,
      "learning_rate": 4.3361111111111116e-05,
      "loss": 0.0067,
      "step": 11950
    },
    {
      "epoch": 0.6644444444444444,
      "grad_norm": 0.009400702081620693,
      "learning_rate": 4.335555555555556e-05,
      "loss": 0.0038,
      "step": 11960
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.011587041430175304,
      "learning_rate": 4.335e-05,
      "loss": 0.0045,
      "step": 11970
    },
    {
      "epoch": 0.6655555555555556,
      "grad_norm": 0.6667875647544861,
      "learning_rate": 4.334444444444445e-05,
      "loss": 0.0046,
      "step": 11980
    },
    {
      "epoch": 0.6661111111111111,
      "grad_norm": 0.23474347591400146,
      "learning_rate": 4.333888888888889e-05,
      "loss": 0.0056,
      "step": 11990
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.009799963794648647,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.005,
      "step": 12000
    },
    {
      "epoch": 0.6672222222222223,
      "grad_norm": 0.29778581857681274,
      "learning_rate": 4.332777777777778e-05,
      "loss": 0.0058,
      "step": 12010
    },
    {
      "epoch": 0.6677777777777778,
      "grad_norm": 0.24429446458816528,
      "learning_rate": 4.332222222222223e-05,
      "loss": 0.006,
      "step": 12020
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.6364574432373047,
      "learning_rate": 4.3316666666666665e-05,
      "loss": 0.0047,
      "step": 12030
    },
    {
      "epoch": 0.6688888888888889,
      "grad_norm": 0.42429119348526,
      "learning_rate": 4.3311111111111115e-05,
      "loss": 0.0047,
      "step": 12040
    },
    {
      "epoch": 0.6694444444444444,
      "grad_norm": 0.12177446484565735,
      "learning_rate": 4.330555555555556e-05,
      "loss": 0.0044,
      "step": 12050
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.06128021329641342,
      "learning_rate": 4.33e-05,
      "loss": 0.0063,
      "step": 12060
    },
    {
      "epoch": 0.6705555555555556,
      "grad_norm": 0.03582412004470825,
      "learning_rate": 4.3294444444444446e-05,
      "loss": 0.0049,
      "step": 12070
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 0.06312617659568787,
      "learning_rate": 4.328888888888889e-05,
      "loss": 0.0045,
      "step": 12080
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.6076996922492981,
      "learning_rate": 4.328333333333334e-05,
      "loss": 0.0067,
      "step": 12090
    },
    {
      "epoch": 0.6722222222222223,
      "grad_norm": 0.273658812046051,
      "learning_rate": 4.3277777777777776e-05,
      "loss": 0.0047,
      "step": 12100
    },
    {
      "epoch": 0.6727777777777778,
      "grad_norm": 0.011310012079775333,
      "learning_rate": 4.327222222222223e-05,
      "loss": 0.0044,
      "step": 12110
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.2425416260957718,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0058,
      "step": 12120
    },
    {
      "epoch": 0.6738888888888889,
      "grad_norm": 0.21264031529426575,
      "learning_rate": 4.3261111111111114e-05,
      "loss": 0.0042,
      "step": 12130
    },
    {
      "epoch": 0.6744444444444444,
      "grad_norm": 0.2725673019886017,
      "learning_rate": 4.325555555555556e-05,
      "loss": 0.0058,
      "step": 12140
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.3376222848892212,
      "learning_rate": 4.325e-05,
      "loss": 0.0063,
      "step": 12150
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.12521876394748688,
      "learning_rate": 4.324444444444445e-05,
      "loss": 0.0045,
      "step": 12160
    },
    {
      "epoch": 0.6761111111111111,
      "grad_norm": 0.21759650111198425,
      "learning_rate": 4.323888888888889e-05,
      "loss": 0.0049,
      "step": 12170
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.45420214533805847,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0056,
      "step": 12180
    },
    {
      "epoch": 0.6772222222222222,
      "grad_norm": 0.2689987123012543,
      "learning_rate": 4.3227777777777775e-05,
      "loss": 0.0063,
      "step": 12190
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 0.09142284840345383,
      "learning_rate": 4.3222222222222226e-05,
      "loss": 0.0041,
      "step": 12200
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.4849083125591278,
      "learning_rate": 4.321666666666667e-05,
      "loss": 0.007,
      "step": 12210
    },
    {
      "epoch": 0.6788888888888889,
      "grad_norm": 0.24106533825397491,
      "learning_rate": 4.321111111111111e-05,
      "loss": 0.0053,
      "step": 12220
    },
    {
      "epoch": 0.6794444444444444,
      "grad_norm": 0.1522849202156067,
      "learning_rate": 4.320555555555556e-05,
      "loss": 0.0061,
      "step": 12230
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.16120685636997223,
      "learning_rate": 4.32e-05,
      "loss": 0.0046,
      "step": 12240
    },
    {
      "epoch": 0.6805555555555556,
      "grad_norm": 0.40014782547950745,
      "learning_rate": 4.319444444444445e-05,
      "loss": 0.006,
      "step": 12250
    },
    {
      "epoch": 0.6811111111111111,
      "grad_norm": 0.24255378544330597,
      "learning_rate": 4.318888888888889e-05,
      "loss": 0.0083,
      "step": 12260
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.265715092420578,
      "learning_rate": 4.318333333333334e-05,
      "loss": 0.0051,
      "step": 12270
    },
    {
      "epoch": 0.6822222222222222,
      "grad_norm": 0.48752638697624207,
      "learning_rate": 4.317777777777778e-05,
      "loss": 0.0044,
      "step": 12280
    },
    {
      "epoch": 0.6827777777777778,
      "grad_norm": 0.7316644191741943,
      "learning_rate": 4.3172222222222224e-05,
      "loss": 0.0065,
      "step": 12290
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.30412745475769043,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0052,
      "step": 12300
    },
    {
      "epoch": 0.6838888888888889,
      "grad_norm": 0.032575562596321106,
      "learning_rate": 4.316111111111111e-05,
      "loss": 0.0069,
      "step": 12310
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 0.1833759993314743,
      "learning_rate": 4.315555555555556e-05,
      "loss": 0.0064,
      "step": 12320
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.6070005297660828,
      "learning_rate": 4.315e-05,
      "loss": 0.0066,
      "step": 12330
    },
    {
      "epoch": 0.6855555555555556,
      "grad_norm": 0.03301504626870155,
      "learning_rate": 4.314444444444445e-05,
      "loss": 0.0063,
      "step": 12340
    },
    {
      "epoch": 0.6861111111111111,
      "grad_norm": 0.45241549611091614,
      "learning_rate": 4.313888888888889e-05,
      "loss": 0.004,
      "step": 12350
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.022874243557453156,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0065,
      "step": 12360
    },
    {
      "epoch": 0.6872222222222222,
      "grad_norm": 0.24231548607349396,
      "learning_rate": 4.312777777777778e-05,
      "loss": 0.0055,
      "step": 12370
    },
    {
      "epoch": 0.6877777777777778,
      "grad_norm": 0.7880127429962158,
      "learning_rate": 4.312222222222222e-05,
      "loss": 0.0049,
      "step": 12380
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.4535175859928131,
      "learning_rate": 4.311666666666667e-05,
      "loss": 0.0058,
      "step": 12390
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.28879398107528687,
      "learning_rate": 4.311111111111111e-05,
      "loss": 0.0069,
      "step": 12400
    },
    {
      "epoch": 0.6894444444444444,
      "grad_norm": 0.1219215989112854,
      "learning_rate": 4.310555555555556e-05,
      "loss": 0.0047,
      "step": 12410
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.3961617350578308,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.0044,
      "step": 12420
    },
    {
      "epoch": 0.6905555555555556,
      "grad_norm": 0.36349302530288696,
      "learning_rate": 4.309444444444445e-05,
      "loss": 0.0064,
      "step": 12430
    },
    {
      "epoch": 0.6911111111111111,
      "grad_norm": 0.013984437100589275,
      "learning_rate": 4.308888888888889e-05,
      "loss": 0.0066,
      "step": 12440
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.30408695340156555,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.0056,
      "step": 12450
    },
    {
      "epoch": 0.6922222222222222,
      "grad_norm": 0.18187899887561798,
      "learning_rate": 4.307777777777778e-05,
      "loss": 0.0052,
      "step": 12460
    },
    {
      "epoch": 0.6927777777777778,
      "grad_norm": 0.152126744389534,
      "learning_rate": 4.307222222222222e-05,
      "loss": 0.0037,
      "step": 12470
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.014361679553985596,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0066,
      "step": 12480
    },
    {
      "epoch": 0.6938888888888889,
      "grad_norm": 0.09093058109283447,
      "learning_rate": 4.3061111111111116e-05,
      "loss": 0.0043,
      "step": 12490
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.4231857359409332,
      "learning_rate": 4.305555555555556e-05,
      "loss": 0.0052,
      "step": 12500
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.12222252786159515,
      "learning_rate": 4.305e-05,
      "loss": 0.0058,
      "step": 12510
    },
    {
      "epoch": 0.6955555555555556,
      "grad_norm": 0.24310842156410217,
      "learning_rate": 4.3044444444444446e-05,
      "loss": 0.0058,
      "step": 12520
    },
    {
      "epoch": 0.6961111111111111,
      "grad_norm": 0.2478027492761612,
      "learning_rate": 4.303888888888889e-05,
      "loss": 0.0061,
      "step": 12530
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.21239542961120605,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.006,
      "step": 12540
    },
    {
      "epoch": 0.6972222222222222,
      "grad_norm": 0.1817125529050827,
      "learning_rate": 4.302777777777778e-05,
      "loss": 0.0051,
      "step": 12550
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 0.1814085692167282,
      "learning_rate": 4.302222222222223e-05,
      "loss": 0.0071,
      "step": 12560
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.45357745885849,
      "learning_rate": 4.3016666666666664e-05,
      "loss": 0.0051,
      "step": 12570
    },
    {
      "epoch": 0.6988888888888889,
      "grad_norm": 0.21189182996749878,
      "learning_rate": 4.3011111111111115e-05,
      "loss": 0.0059,
      "step": 12580
    },
    {
      "epoch": 0.6994444444444444,
      "grad_norm": 0.18252986669540405,
      "learning_rate": 4.300555555555556e-05,
      "loss": 0.0057,
      "step": 12590
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.24133916199207306,
      "learning_rate": 4.3e-05,
      "loss": 0.0047,
      "step": 12600
    },
    {
      "epoch": 0.7005555555555556,
      "grad_norm": 0.12172357738018036,
      "learning_rate": 4.2994444444444445e-05,
      "loss": 0.0066,
      "step": 12610
    },
    {
      "epoch": 0.7011111111111111,
      "grad_norm": 0.3788880705833435,
      "learning_rate": 4.298888888888889e-05,
      "loss": 0.0064,
      "step": 12620
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.24228675663471222,
      "learning_rate": 4.298333333333334e-05,
      "loss": 0.0063,
      "step": 12630
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 0.011033338494598866,
      "learning_rate": 4.2977777777777776e-05,
      "loss": 0.0039,
      "step": 12640
    },
    {
      "epoch": 0.7027777777777777,
      "grad_norm": 0.009532470256090164,
      "learning_rate": 4.2972222222222226e-05,
      "loss": 0.0051,
      "step": 12650
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.1833171546459198,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0044,
      "step": 12660
    },
    {
      "epoch": 0.7038888888888889,
      "grad_norm": 0.012123188935220242,
      "learning_rate": 4.296111111111111e-05,
      "loss": 0.0047,
      "step": 12670
    },
    {
      "epoch": 0.7044444444444444,
      "grad_norm": 0.33268892765045166,
      "learning_rate": 4.295555555555556e-05,
      "loss": 0.005,
      "step": 12680
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.5571480989456177,
      "learning_rate": 4.295e-05,
      "loss": 0.0037,
      "step": 12690
    },
    {
      "epoch": 0.7055555555555556,
      "grad_norm": 0.6198995113372803,
      "learning_rate": 4.294444444444445e-05,
      "loss": 0.0067,
      "step": 12700
    },
    {
      "epoch": 0.7061111111111111,
      "grad_norm": 0.060938090085983276,
      "learning_rate": 4.293888888888889e-05,
      "loss": 0.0077,
      "step": 12710
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.36364302039146423,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0071,
      "step": 12720
    },
    {
      "epoch": 0.7072222222222222,
      "grad_norm": 0.09118369221687317,
      "learning_rate": 4.2927777777777775e-05,
      "loss": 0.0048,
      "step": 12730
    },
    {
      "epoch": 0.7077777777777777,
      "grad_norm": 0.061130087822675705,
      "learning_rate": 4.2922222222222225e-05,
      "loss": 0.0059,
      "step": 12740
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.23845748603343964,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.005,
      "step": 12750
    },
    {
      "epoch": 0.7088888888888889,
      "grad_norm": 0.03229241445660591,
      "learning_rate": 4.291111111111111e-05,
      "loss": 0.0066,
      "step": 12760
    },
    {
      "epoch": 0.7094444444444444,
      "grad_norm": 0.4228459894657135,
      "learning_rate": 4.290555555555556e-05,
      "loss": 0.0048,
      "step": 12770
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.45181357860565186,
      "learning_rate": 4.29e-05,
      "loss": 0.0068,
      "step": 12780
    },
    {
      "epoch": 0.7105555555555556,
      "grad_norm": 0.12306373566389084,
      "learning_rate": 4.289444444444445e-05,
      "loss": 0.0055,
      "step": 12790
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.06140564754605293,
      "learning_rate": 4.2888888888888886e-05,
      "loss": 0.0042,
      "step": 12800
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.2526596784591675,
      "learning_rate": 4.288333333333334e-05,
      "loss": 0.007,
      "step": 12810
    },
    {
      "epoch": 0.7122222222222222,
      "grad_norm": 0.13066160678863525,
      "learning_rate": 4.287777777777778e-05,
      "loss": 0.0064,
      "step": 12820
    },
    {
      "epoch": 0.7127777777777777,
      "grad_norm": 0.03281887248158455,
      "learning_rate": 4.2872222222222224e-05,
      "loss": 0.0079,
      "step": 12830
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.3370412588119507,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0059,
      "step": 12840
    },
    {
      "epoch": 0.7138888888888889,
      "grad_norm": 0.06090014427900314,
      "learning_rate": 4.286111111111111e-05,
      "loss": 0.0062,
      "step": 12850
    },
    {
      "epoch": 0.7144444444444444,
      "grad_norm": 0.1797209233045578,
      "learning_rate": 4.285555555555556e-05,
      "loss": 0.0076,
      "step": 12860
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.010137801989912987,
      "learning_rate": 4.285e-05,
      "loss": 0.005,
      "step": 12870
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 0.5440949201583862,
      "learning_rate": 4.284444444444445e-05,
      "loss": 0.0046,
      "step": 12880
    },
    {
      "epoch": 0.7161111111111111,
      "grad_norm": 0.27281996607780457,
      "learning_rate": 4.283888888888889e-05,
      "loss": 0.0055,
      "step": 12890
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.5297479629516602,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0063,
      "step": 12900
    },
    {
      "epoch": 0.7172222222222222,
      "grad_norm": 0.1815326064825058,
      "learning_rate": 4.282777777777778e-05,
      "loss": 0.0053,
      "step": 12910
    },
    {
      "epoch": 0.7177777777777777,
      "grad_norm": 0.12220804393291473,
      "learning_rate": 4.282222222222222e-05,
      "loss": 0.007,
      "step": 12920
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.24203439056873322,
      "learning_rate": 4.2816666666666666e-05,
      "loss": 0.0051,
      "step": 12930
    },
    {
      "epoch": 0.7188888888888889,
      "grad_norm": 0.06168731674551964,
      "learning_rate": 4.281111111111111e-05,
      "loss": 0.0045,
      "step": 12940
    },
    {
      "epoch": 0.7194444444444444,
      "grad_norm": 0.009668725542724133,
      "learning_rate": 4.280555555555556e-05,
      "loss": 0.0049,
      "step": 12950
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.42576107382774353,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0055,
      "step": 12960
    },
    {
      "epoch": 0.7205555555555555,
      "grad_norm": 0.1943434327840805,
      "learning_rate": 4.279444444444445e-05,
      "loss": 0.0046,
      "step": 12970
    },
    {
      "epoch": 0.7211111111111111,
      "grad_norm": 0.2410976141691208,
      "learning_rate": 4.278888888888889e-05,
      "loss": 0.0063,
      "step": 12980
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 0.5130662322044373,
      "learning_rate": 4.2783333333333334e-05,
      "loss": 0.0051,
      "step": 12990
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.17534351348876953,
      "learning_rate": 4.277777777777778e-05,
      "loss": 0.005,
      "step": 13000
    },
    {
      "epoch": 0.7227777777777777,
      "grad_norm": 0.062182728201150894,
      "learning_rate": 4.277222222222222e-05,
      "loss": 0.0052,
      "step": 13010
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.12083195894956589,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0045,
      "step": 13020
    },
    {
      "epoch": 0.7238888888888889,
      "grad_norm": 0.21274277567863464,
      "learning_rate": 4.2761111111111115e-05,
      "loss": 0.0046,
      "step": 13030
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 0.06282233446836472,
      "learning_rate": 4.275555555555556e-05,
      "loss": 0.0046,
      "step": 13040
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.2420225441455841,
      "learning_rate": 4.275e-05,
      "loss": 0.0047,
      "step": 13050
    },
    {
      "epoch": 0.7255555555555555,
      "grad_norm": 0.06150800734758377,
      "learning_rate": 4.2744444444444446e-05,
      "loss": 0.004,
      "step": 13060
    },
    {
      "epoch": 0.7261111111111112,
      "grad_norm": 0.18120735883712769,
      "learning_rate": 4.273888888888889e-05,
      "loss": 0.0051,
      "step": 13070
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.06186116486787796,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0043,
      "step": 13080
    },
    {
      "epoch": 0.7272222222222222,
      "grad_norm": 0.12143043428659439,
      "learning_rate": 4.2727777777777777e-05,
      "loss": 0.0055,
      "step": 13090
    },
    {
      "epoch": 0.7277777777777777,
      "grad_norm": 0.3021380305290222,
      "learning_rate": 4.272222222222223e-05,
      "loss": 0.0074,
      "step": 13100
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.060548774898052216,
      "learning_rate": 4.2716666666666664e-05,
      "loss": 0.0048,
      "step": 13110
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.07329781353473663,
      "learning_rate": 4.2711111111111114e-05,
      "loss": 0.0067,
      "step": 13120
    },
    {
      "epoch": 0.7294444444444445,
      "grad_norm": 0.48339521884918213,
      "learning_rate": 4.270555555555556e-05,
      "loss": 0.0053,
      "step": 13130
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.4842569828033447,
      "learning_rate": 4.27e-05,
      "loss": 0.0049,
      "step": 13140
    },
    {
      "epoch": 0.7305555555555555,
      "grad_norm": 0.3027462065219879,
      "learning_rate": 4.2694444444444445e-05,
      "loss": 0.008,
      "step": 13150
    },
    {
      "epoch": 0.7311111111111112,
      "grad_norm": 0.27241313457489014,
      "learning_rate": 4.268888888888889e-05,
      "loss": 0.0057,
      "step": 13160
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.1229662224650383,
      "learning_rate": 4.268333333333334e-05,
      "loss": 0.0052,
      "step": 13170
    },
    {
      "epoch": 0.7322222222222222,
      "grad_norm": 0.06176121160387993,
      "learning_rate": 4.2677777777777775e-05,
      "loss": 0.0052,
      "step": 13180
    },
    {
      "epoch": 0.7327777777777778,
      "grad_norm": 0.5143656134605408,
      "learning_rate": 4.2672222222222226e-05,
      "loss": 0.0064,
      "step": 13190
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.6048604846000671,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0055,
      "step": 13200
    },
    {
      "epoch": 0.7338888888888889,
      "grad_norm": 0.5441111326217651,
      "learning_rate": 4.266111111111111e-05,
      "loss": 0.0061,
      "step": 13210
    },
    {
      "epoch": 0.7344444444444445,
      "grad_norm": 0.2414533495903015,
      "learning_rate": 4.2655555555555556e-05,
      "loss": 0.0062,
      "step": 13220
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.033315714448690414,
      "learning_rate": 4.265e-05,
      "loss": 0.0058,
      "step": 13230
    },
    {
      "epoch": 0.7355555555555555,
      "grad_norm": 0.3022495210170746,
      "learning_rate": 4.264444444444445e-05,
      "loss": 0.0045,
      "step": 13240
    },
    {
      "epoch": 0.7361111111111112,
      "grad_norm": 0.06309491395950317,
      "learning_rate": 4.263888888888889e-05,
      "loss": 0.0053,
      "step": 13250
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.33139005303382874,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0083,
      "step": 13260
    },
    {
      "epoch": 0.7372222222222222,
      "grad_norm": 0.14854277670383453,
      "learning_rate": 4.262777777777778e-05,
      "loss": 0.0051,
      "step": 13270
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 0.1529640257358551,
      "learning_rate": 4.2622222222222224e-05,
      "loss": 0.0053,
      "step": 13280
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 0.06104177609086037,
      "learning_rate": 4.261666666666667e-05,
      "loss": 0.0053,
      "step": 13290
    },
    {
      "epoch": 0.7388888888888889,
      "grad_norm": 0.18078665435314178,
      "learning_rate": 4.261111111111111e-05,
      "loss": 0.0062,
      "step": 13300
    },
    {
      "epoch": 0.7394444444444445,
      "grad_norm": 0.39272311329841614,
      "learning_rate": 4.260555555555556e-05,
      "loss": 0.0052,
      "step": 13310
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.12235338985919952,
      "learning_rate": 4.26e-05,
      "loss": 0.0036,
      "step": 13320
    },
    {
      "epoch": 0.7405555555555555,
      "grad_norm": 0.03243907168507576,
      "learning_rate": 4.259444444444445e-05,
      "loss": 0.0044,
      "step": 13330
    },
    {
      "epoch": 0.7411111111111112,
      "grad_norm": 0.24149703979492188,
      "learning_rate": 4.258888888888889e-05,
      "loss": 0.0051,
      "step": 13340
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.635228157043457,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.0058,
      "step": 13350
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 0.031427353620529175,
      "learning_rate": 4.257777777777778e-05,
      "loss": 0.0064,
      "step": 13360
    },
    {
      "epoch": 0.7427777777777778,
      "grad_norm": 0.06281623989343643,
      "learning_rate": 4.257222222222222e-05,
      "loss": 0.005,
      "step": 13370
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.29942426085472107,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0036,
      "step": 13380
    },
    {
      "epoch": 0.7438888888888889,
      "grad_norm": 0.1204824224114418,
      "learning_rate": 4.256111111111111e-05,
      "loss": 0.0053,
      "step": 13390
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.2414775788784027,
      "learning_rate": 4.255555555555556e-05,
      "loss": 0.0065,
      "step": 13400
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.4242730438709259,
      "learning_rate": 4.2550000000000004e-05,
      "loss": 0.0051,
      "step": 13410
    },
    {
      "epoch": 0.7455555555555555,
      "grad_norm": 0.30598554015159607,
      "learning_rate": 4.254444444444445e-05,
      "loss": 0.0074,
      "step": 13420
    },
    {
      "epoch": 0.7461111111111111,
      "grad_norm": 0.5131832361221313,
      "learning_rate": 4.253888888888889e-05,
      "loss": 0.0054,
      "step": 13430
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.12220660597085953,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0057,
      "step": 13440
    },
    {
      "epoch": 0.7472222222222222,
      "grad_norm": 0.062309712171554565,
      "learning_rate": 4.252777777777778e-05,
      "loss": 0.0045,
      "step": 13450
    },
    {
      "epoch": 0.7477777777777778,
      "grad_norm": 0.33351463079452515,
      "learning_rate": 4.252222222222222e-05,
      "loss": 0.0059,
      "step": 13460
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.18277862668037415,
      "learning_rate": 4.251666666666667e-05,
      "loss": 0.0059,
      "step": 13470
    },
    {
      "epoch": 0.7488888888888889,
      "grad_norm": 0.30277159810066223,
      "learning_rate": 4.2511111111111116e-05,
      "loss": 0.004,
      "step": 13480
    },
    {
      "epoch": 0.7494444444444445,
      "grad_norm": 0.06283146142959595,
      "learning_rate": 4.250555555555556e-05,
      "loss": 0.0061,
      "step": 13490
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.3933202028274536,
      "learning_rate": 4.25e-05,
      "loss": 0.0047,
      "step": 13500
    },
    {
      "epoch": 0.7505555555555555,
      "grad_norm": 0.24270746111869812,
      "learning_rate": 4.2494444444444447e-05,
      "loss": 0.006,
      "step": 13510
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 0.06245757266879082,
      "learning_rate": 4.248888888888889e-05,
      "loss": 0.007,
      "step": 13520
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.3949589729309082,
      "learning_rate": 4.2483333333333334e-05,
      "loss": 0.006,
      "step": 13530
    },
    {
      "epoch": 0.7522222222222222,
      "grad_norm": 0.061145760118961334,
      "learning_rate": 4.247777777777778e-05,
      "loss": 0.0051,
      "step": 13540
    },
    {
      "epoch": 0.7527777777777778,
      "grad_norm": 0.45477309823036194,
      "learning_rate": 4.247222222222223e-05,
      "loss": 0.0045,
      "step": 13550
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.06118287518620491,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0054,
      "step": 13560
    },
    {
      "epoch": 0.7538888888888889,
      "grad_norm": 0.06411965191364288,
      "learning_rate": 4.2461111111111115e-05,
      "loss": 0.006,
      "step": 13570
    },
    {
      "epoch": 0.7544444444444445,
      "grad_norm": 0.06241941452026367,
      "learning_rate": 4.245555555555556e-05,
      "loss": 0.0036,
      "step": 13580
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.45400694012641907,
      "learning_rate": 4.245e-05,
      "loss": 0.0053,
      "step": 13590
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.36192944645881653,
      "learning_rate": 4.2444444444444445e-05,
      "loss": 0.0043,
      "step": 13600
    },
    {
      "epoch": 0.7561111111111111,
      "grad_norm": 0.18209972977638245,
      "learning_rate": 4.243888888888889e-05,
      "loss": 0.0056,
      "step": 13610
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.15513953566551208,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.0044,
      "step": 13620
    },
    {
      "epoch": 0.7572222222222222,
      "grad_norm": 0.15165551006793976,
      "learning_rate": 4.2427777777777776e-05,
      "loss": 0.0066,
      "step": 13630
    },
    {
      "epoch": 0.7577777777777778,
      "grad_norm": 0.03188565373420715,
      "learning_rate": 4.2422222222222226e-05,
      "loss": 0.0045,
      "step": 13640
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.1216735690832138,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.0056,
      "step": 13650
    },
    {
      "epoch": 0.7588888888888888,
      "grad_norm": 0.12131691724061966,
      "learning_rate": 4.2411111111111114e-05,
      "loss": 0.0056,
      "step": 13660
    },
    {
      "epoch": 0.7594444444444445,
      "grad_norm": 0.06279598921537399,
      "learning_rate": 4.240555555555556e-05,
      "loss": 0.0062,
      "step": 13670
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.18262693285942078,
      "learning_rate": 4.24e-05,
      "loss": 0.0043,
      "step": 13680
    },
    {
      "epoch": 0.7605555555555555,
      "grad_norm": 0.1813405603170395,
      "learning_rate": 4.239444444444445e-05,
      "loss": 0.0069,
      "step": 13690
    },
    {
      "epoch": 0.7611111111111111,
      "grad_norm": 0.4825021028518677,
      "learning_rate": 4.238888888888889e-05,
      "loss": 0.0048,
      "step": 13700
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.31776049733161926,
      "learning_rate": 4.238333333333334e-05,
      "loss": 0.0056,
      "step": 13710
    },
    {
      "epoch": 0.7622222222222222,
      "grad_norm": 0.01634901762008667,
      "learning_rate": 4.2377777777777775e-05,
      "loss": 0.0047,
      "step": 13720
    },
    {
      "epoch": 0.7627777777777778,
      "grad_norm": 0.30222076177597046,
      "learning_rate": 4.2372222222222225e-05,
      "loss": 0.0052,
      "step": 13730
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 0.27150705456733704,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0043,
      "step": 13740
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 0.06341676414012909,
      "learning_rate": 4.236111111111111e-05,
      "loss": 0.0054,
      "step": 13750
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.062446147203445435,
      "learning_rate": 4.235555555555556e-05,
      "loss": 0.0051,
      "step": 13760
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.636106014251709,
      "learning_rate": 4.235e-05,
      "loss": 0.006,
      "step": 13770
    },
    {
      "epoch": 0.7655555555555555,
      "grad_norm": 0.36234989762306213,
      "learning_rate": 4.234444444444445e-05,
      "loss": 0.006,
      "step": 13780
    },
    {
      "epoch": 0.7661111111111111,
      "grad_norm": 0.01055547408759594,
      "learning_rate": 4.2338888888888887e-05,
      "loss": 0.004,
      "step": 13790
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.18212421238422394,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0047,
      "step": 13800
    },
    {
      "epoch": 0.7672222222222222,
      "grad_norm": 0.25611022114753723,
      "learning_rate": 4.232777777777778e-05,
      "loss": 0.0052,
      "step": 13810
    },
    {
      "epoch": 0.7677777777777778,
      "grad_norm": 0.12140168994665146,
      "learning_rate": 4.2322222222222224e-05,
      "loss": 0.0044,
      "step": 13820
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.33270034193992615,
      "learning_rate": 4.2316666666666674e-05,
      "loss": 0.0047,
      "step": 13830
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 0.6351221203804016,
      "learning_rate": 4.231111111111111e-05,
      "loss": 0.0057,
      "step": 13840
    },
    {
      "epoch": 0.7694444444444445,
      "grad_norm": 0.18166349828243256,
      "learning_rate": 4.230555555555556e-05,
      "loss": 0.0046,
      "step": 13850
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.3028338849544525,
      "learning_rate": 4.23e-05,
      "loss": 0.0069,
      "step": 13860
    },
    {
      "epoch": 0.7705555555555555,
      "grad_norm": 0.5437092781066895,
      "learning_rate": 4.229444444444445e-05,
      "loss": 0.0047,
      "step": 13870
    },
    {
      "epoch": 0.7711111111111111,
      "grad_norm": 0.013375068083405495,
      "learning_rate": 4.228888888888889e-05,
      "loss": 0.005,
      "step": 13880
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.11936355382204056,
      "learning_rate": 4.2283333333333336e-05,
      "loss": 0.0057,
      "step": 13890
    },
    {
      "epoch": 0.7722222222222223,
      "grad_norm": 0.4845033586025238,
      "learning_rate": 4.227777777777778e-05,
      "loss": 0.0051,
      "step": 13900
    },
    {
      "epoch": 0.7727777777777778,
      "grad_norm": 0.484134703874588,
      "learning_rate": 4.227222222222222e-05,
      "loss": 0.005,
      "step": 13910
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.3032337725162506,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0059,
      "step": 13920
    },
    {
      "epoch": 0.7738888888888888,
      "grad_norm": 0.18257646262645721,
      "learning_rate": 4.226111111111111e-05,
      "loss": 0.0038,
      "step": 13930
    },
    {
      "epoch": 0.7744444444444445,
      "grad_norm": 0.2413187026977539,
      "learning_rate": 4.225555555555556e-05,
      "loss": 0.0036,
      "step": 13940
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.061218973249197006,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.0054,
      "step": 13950
    },
    {
      "epoch": 0.7755555555555556,
      "grad_norm": 0.21308885514736176,
      "learning_rate": 4.224444444444445e-05,
      "loss": 0.0063,
      "step": 13960
    },
    {
      "epoch": 0.7761111111111111,
      "grad_norm": 0.39320108294487,
      "learning_rate": 4.223888888888889e-05,
      "loss": 0.0071,
      "step": 13970
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.30215713381767273,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.0067,
      "step": 13980
    },
    {
      "epoch": 0.7772222222222223,
      "grad_norm": 0.12803176045417786,
      "learning_rate": 4.222777777777778e-05,
      "loss": 0.0058,
      "step": 13990
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.2030504196882248,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.0049,
      "step": 14000
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 0.12190072238445282,
      "learning_rate": 4.221666666666667e-05,
      "loss": 0.0039,
      "step": 14010
    },
    {
      "epoch": 0.7788888888888889,
      "grad_norm": 0.154154434800148,
      "learning_rate": 4.2211111111111115e-05,
      "loss": 0.0042,
      "step": 14020
    },
    {
      "epoch": 0.7794444444444445,
      "grad_norm": 0.20626668632030487,
      "learning_rate": 4.220555555555556e-05,
      "loss": 0.0061,
      "step": 14030
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.3619133532047272,
      "learning_rate": 4.22e-05,
      "loss": 0.0062,
      "step": 14040
    },
    {
      "epoch": 0.7805555555555556,
      "grad_norm": 0.787042498588562,
      "learning_rate": 4.2194444444444446e-05,
      "loss": 0.0061,
      "step": 14050
    },
    {
      "epoch": 0.7811111111111111,
      "grad_norm": 0.6083489060401917,
      "learning_rate": 4.218888888888889e-05,
      "loss": 0.0067,
      "step": 14060
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.18174630403518677,
      "learning_rate": 4.218333333333333e-05,
      "loss": 0.0053,
      "step": 14070
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 0.12149664014577866,
      "learning_rate": 4.217777777777778e-05,
      "loss": 0.0068,
      "step": 14080
    },
    {
      "epoch": 0.7827777777777778,
      "grad_norm": 0.2418801635503769,
      "learning_rate": 4.217222222222223e-05,
      "loss": 0.0055,
      "step": 14090
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.243490532040596,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0046,
      "step": 14100
    },
    {
      "epoch": 0.7838888888888889,
      "grad_norm": 0.18134662508964539,
      "learning_rate": 4.2161111111111114e-05,
      "loss": 0.006,
      "step": 14110
    },
    {
      "epoch": 0.7844444444444445,
      "grad_norm": 0.2618655264377594,
      "learning_rate": 4.215555555555556e-05,
      "loss": 0.005,
      "step": 14120
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.3023194968700409,
      "learning_rate": 4.215e-05,
      "loss": 0.0053,
      "step": 14130
    },
    {
      "epoch": 0.7855555555555556,
      "grad_norm": 0.3031250536441803,
      "learning_rate": 4.2144444444444445e-05,
      "loss": 0.0039,
      "step": 14140
    },
    {
      "epoch": 0.7861111111111111,
      "grad_norm": 0.24312730133533478,
      "learning_rate": 4.213888888888889e-05,
      "loss": 0.0075,
      "step": 14150
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.2115577906370163,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0061,
      "step": 14160
    },
    {
      "epoch": 0.7872222222222223,
      "grad_norm": 0.42253702878952026,
      "learning_rate": 4.2127777777777776e-05,
      "loss": 0.0042,
      "step": 14170
    },
    {
      "epoch": 0.7877777777777778,
      "grad_norm": 0.3050335645675659,
      "learning_rate": 4.2122222222222226e-05,
      "loss": 0.0067,
      "step": 14180
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.6043070554733276,
      "learning_rate": 4.211666666666667e-05,
      "loss": 0.0045,
      "step": 14190
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 0.3642539083957672,
      "learning_rate": 4.211111111111111e-05,
      "loss": 0.0052,
      "step": 14200
    },
    {
      "epoch": 0.7894444444444444,
      "grad_norm": 0.06194291263818741,
      "learning_rate": 4.2105555555555557e-05,
      "loss": 0.004,
      "step": 14210
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.24278859794139862,
      "learning_rate": 4.21e-05,
      "loss": 0.0054,
      "step": 14220
    },
    {
      "epoch": 0.7905555555555556,
      "grad_norm": 0.12130461633205414,
      "learning_rate": 4.209444444444445e-05,
      "loss": 0.0043,
      "step": 14230
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 0.033000826835632324,
      "learning_rate": 4.208888888888889e-05,
      "loss": 0.0051,
      "step": 14240
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.031966451555490494,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.0055,
      "step": 14250
    },
    {
      "epoch": 0.7922222222222223,
      "grad_norm": 0.3523743152618408,
      "learning_rate": 4.2077777777777774e-05,
      "loss": 0.0042,
      "step": 14260
    },
    {
      "epoch": 0.7927777777777778,
      "grad_norm": 0.11443515866994858,
      "learning_rate": 4.2072222222222225e-05,
      "loss": 0.0054,
      "step": 14270
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.3692517578601837,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0052,
      "step": 14280
    },
    {
      "epoch": 0.7938888888888889,
      "grad_norm": 0.03842812776565552,
      "learning_rate": 4.206111111111111e-05,
      "loss": 0.0038,
      "step": 14290
    },
    {
      "epoch": 0.7944444444444444,
      "grad_norm": 0.21175573766231537,
      "learning_rate": 4.205555555555556e-05,
      "loss": 0.0073,
      "step": 14300
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.12110108882188797,
      "learning_rate": 4.205e-05,
      "loss": 0.0043,
      "step": 14310
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 0.7868032455444336,
      "learning_rate": 4.204444444444445e-05,
      "loss": 0.0028,
      "step": 14320
    },
    {
      "epoch": 0.7961111111111111,
      "grad_norm": 0.09104042500257492,
      "learning_rate": 4.2038888888888886e-05,
      "loss": 0.0061,
      "step": 14330
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.4144158959388733,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0049,
      "step": 14340
    },
    {
      "epoch": 0.7972222222222223,
      "grad_norm": 0.0237713810056448,
      "learning_rate": 4.202777777777778e-05,
      "loss": 0.0043,
      "step": 14350
    },
    {
      "epoch": 0.7977777777777778,
      "grad_norm": 0.15210160613059998,
      "learning_rate": 4.2022222222222223e-05,
      "loss": 0.0067,
      "step": 14360
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.19205698370933533,
      "learning_rate": 4.2016666666666674e-05,
      "loss": 0.0051,
      "step": 14370
    },
    {
      "epoch": 0.7988888888888889,
      "grad_norm": 0.36337554454803467,
      "learning_rate": 4.201111111111111e-05,
      "loss": 0.0047,
      "step": 14380
    },
    {
      "epoch": 0.7994444444444444,
      "grad_norm": 0.21354375779628754,
      "learning_rate": 4.200555555555556e-05,
      "loss": 0.0058,
      "step": 14390
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.24299819767475128,
      "learning_rate": 4.2e-05,
      "loss": 0.0064,
      "step": 14400
    },
    {
      "epoch": 0.8005555555555556,
      "grad_norm": 0.36214593052864075,
      "learning_rate": 4.199444444444445e-05,
      "loss": 0.0033,
      "step": 14410
    },
    {
      "epoch": 0.8011111111111111,
      "grad_norm": 0.15134574472904205,
      "learning_rate": 4.198888888888889e-05,
      "loss": 0.0052,
      "step": 14420
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.033079925924539566,
      "learning_rate": 4.1983333333333335e-05,
      "loss": 0.0047,
      "step": 14430
    },
    {
      "epoch": 0.8022222222222222,
      "grad_norm": 0.23556098341941833,
      "learning_rate": 4.1977777777777785e-05,
      "loss": 0.0052,
      "step": 14440
    },
    {
      "epoch": 0.8027777777777778,
      "grad_norm": 0.48318254947662354,
      "learning_rate": 4.197222222222222e-05,
      "loss": 0.0039,
      "step": 14450
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.4827679395675659,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0058,
      "step": 14460
    },
    {
      "epoch": 0.8038888888888889,
      "grad_norm": 0.4227118194103241,
      "learning_rate": 4.196111111111111e-05,
      "loss": 0.0065,
      "step": 14470
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 0.2119171917438507,
      "learning_rate": 4.195555555555556e-05,
      "loss": 0.0066,
      "step": 14480
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.22302547097206116,
      "learning_rate": 4.195e-05,
      "loss": 0.0063,
      "step": 14490
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 0.3621995151042938,
      "learning_rate": 4.194444444444445e-05,
      "loss": 0.0056,
      "step": 14500
    },
    {
      "epoch": 0.8061111111111111,
      "grad_norm": 0.06623521447181702,
      "learning_rate": 4.193888888888889e-05,
      "loss": 0.0062,
      "step": 14510
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.5126758813858032,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0068,
      "step": 14520
    },
    {
      "epoch": 0.8072222222222222,
      "grad_norm": 0.24153104424476624,
      "learning_rate": 4.1927777777777784e-05,
      "loss": 0.0056,
      "step": 14530
    },
    {
      "epoch": 0.8077777777777778,
      "grad_norm": 0.20706228911876678,
      "learning_rate": 4.192222222222222e-05,
      "loss": 0.0047,
      "step": 14540
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.48188018798828125,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.0075,
      "step": 14550
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 0.1809169203042984,
      "learning_rate": 4.1911111111111115e-05,
      "loss": 0.0054,
      "step": 14560
    },
    {
      "epoch": 0.8094444444444444,
      "grad_norm": 0.2719198167324066,
      "learning_rate": 4.190555555555556e-05,
      "loss": 0.005,
      "step": 14570
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.0908152237534523,
      "learning_rate": 4.19e-05,
      "loss": 0.0056,
      "step": 14580
    },
    {
      "epoch": 0.8105555555555556,
      "grad_norm": 0.06237822398543358,
      "learning_rate": 4.1894444444444446e-05,
      "loss": 0.0049,
      "step": 14590
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.010731653310358524,
      "learning_rate": 4.188888888888889e-05,
      "loss": 0.0055,
      "step": 14600
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.06131845340132713,
      "learning_rate": 4.188333333333333e-05,
      "loss": 0.0044,
      "step": 14610
    },
    {
      "epoch": 0.8122222222222222,
      "grad_norm": 0.2719590961933136,
      "learning_rate": 4.187777777777778e-05,
      "loss": 0.0045,
      "step": 14620
    },
    {
      "epoch": 0.8127777777777778,
      "grad_norm": 0.014906266704201698,
      "learning_rate": 4.1872222222222227e-05,
      "loss": 0.005,
      "step": 14630
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.03366394340991974,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0053,
      "step": 14640
    },
    {
      "epoch": 0.8138888888888889,
      "grad_norm": 0.23966054618358612,
      "learning_rate": 4.1861111111111114e-05,
      "loss": 0.0035,
      "step": 14650
    },
    {
      "epoch": 0.8144444444444444,
      "grad_norm": 0.4817148447036743,
      "learning_rate": 4.185555555555556e-05,
      "loss": 0.0036,
      "step": 14660
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.2414807826280594,
      "learning_rate": 4.185e-05,
      "loss": 0.0052,
      "step": 14670
    },
    {
      "epoch": 0.8155555555555556,
      "grad_norm": 0.3016396462917328,
      "learning_rate": 4.1844444444444444e-05,
      "loss": 0.0042,
      "step": 14680
    },
    {
      "epoch": 0.8161111111111111,
      "grad_norm": 0.1811363846063614,
      "learning_rate": 4.183888888888889e-05,
      "loss": 0.0037,
      "step": 14690
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.24938048422336578,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0057,
      "step": 14700
    },
    {
      "epoch": 0.8172222222222222,
      "grad_norm": 0.9049599170684814,
      "learning_rate": 4.182777777777778e-05,
      "loss": 0.0051,
      "step": 14710
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.15317261219024658,
      "learning_rate": 4.1822222222222225e-05,
      "loss": 0.0067,
      "step": 14720
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.18149131536483765,
      "learning_rate": 4.181666666666667e-05,
      "loss": 0.0041,
      "step": 14730
    },
    {
      "epoch": 0.8188888888888889,
      "grad_norm": 0.2115914672613144,
      "learning_rate": 4.181111111111111e-05,
      "loss": 0.004,
      "step": 14740
    },
    {
      "epoch": 0.8194444444444444,
      "grad_norm": 0.2712450921535492,
      "learning_rate": 4.1805555555555556e-05,
      "loss": 0.0042,
      "step": 14750
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.25405362248420715,
      "learning_rate": 4.18e-05,
      "loss": 0.006,
      "step": 14760
    },
    {
      "epoch": 0.8205555555555556,
      "grad_norm": 0.06451336294412613,
      "learning_rate": 4.179444444444445e-05,
      "loss": 0.0064,
      "step": 14770
    },
    {
      "epoch": 0.8211111111111111,
      "grad_norm": 0.30235204100608826,
      "learning_rate": 4.178888888888889e-05,
      "loss": 0.0032,
      "step": 14780
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.3317994177341461,
      "learning_rate": 4.178333333333334e-05,
      "loss": 0.0054,
      "step": 14790
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.36159074306488037,
      "learning_rate": 4.177777777777778e-05,
      "loss": 0.0055,
      "step": 14800
    },
    {
      "epoch": 0.8227777777777778,
      "grad_norm": 0.03203456103801727,
      "learning_rate": 4.1772222222222224e-05,
      "loss": 0.0048,
      "step": 14810
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.18145669996738434,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0046,
      "step": 14820
    },
    {
      "epoch": 0.8238888888888889,
      "grad_norm": 0.03205493092536926,
      "learning_rate": 4.176111111111111e-05,
      "loss": 0.0042,
      "step": 14830
    },
    {
      "epoch": 0.8244444444444444,
      "grad_norm": 0.09083353728055954,
      "learning_rate": 4.175555555555556e-05,
      "loss": 0.0063,
      "step": 14840
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.36194610595703125,
      "learning_rate": 4.175e-05,
      "loss": 0.0039,
      "step": 14850
    },
    {
      "epoch": 0.8255555555555556,
      "grad_norm": 0.12135067582130432,
      "learning_rate": 4.174444444444445e-05,
      "loss": 0.0055,
      "step": 14860
    },
    {
      "epoch": 0.8261111111111111,
      "grad_norm": 0.12166345864534378,
      "learning_rate": 4.1738888888888885e-05,
      "loss": 0.0051,
      "step": 14870
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.301790326833725,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0063,
      "step": 14880
    },
    {
      "epoch": 0.8272222222222222,
      "grad_norm": 0.009055539965629578,
      "learning_rate": 4.172777777777778e-05,
      "loss": 0.004,
      "step": 14890
    },
    {
      "epoch": 0.8277777777777777,
      "grad_norm": 0.3919126093387604,
      "learning_rate": 4.172222222222222e-05,
      "loss": 0.0054,
      "step": 14900
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.10618887841701508,
      "learning_rate": 4.171666666666667e-05,
      "loss": 0.0061,
      "step": 14910
    },
    {
      "epoch": 0.8288888888888889,
      "grad_norm": 0.21150046586990356,
      "learning_rate": 4.171111111111111e-05,
      "loss": 0.0047,
      "step": 14920
    },
    {
      "epoch": 0.8294444444444444,
      "grad_norm": 0.06137380003929138,
      "learning_rate": 4.170555555555556e-05,
      "loss": 0.0049,
      "step": 14930
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.2405613511800766,
      "learning_rate": 4.17e-05,
      "loss": 0.0055,
      "step": 14940
    },
    {
      "epoch": 0.8305555555555556,
      "grad_norm": 0.18153275549411774,
      "learning_rate": 4.169444444444445e-05,
      "loss": 0.0055,
      "step": 14950
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 0.20453466475009918,
      "learning_rate": 4.168888888888889e-05,
      "loss": 0.0055,
      "step": 14960
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.13005942106246948,
      "learning_rate": 4.1683333333333335e-05,
      "loss": 0.0069,
      "step": 14970
    },
    {
      "epoch": 0.8322222222222222,
      "grad_norm": 0.29597702622413635,
      "learning_rate": 4.1677777777777785e-05,
      "loss": 0.004,
      "step": 14980
    },
    {
      "epoch": 0.8327777777777777,
      "grad_norm": 0.03459372743964195,
      "learning_rate": 4.167222222222222e-05,
      "loss": 0.0043,
      "step": 14990
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.061181917786598206,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0043,
      "step": 15000
    },
    {
      "epoch": 0.8338888888888889,
      "grad_norm": 0.7523227334022522,
      "learning_rate": 4.166111111111111e-05,
      "loss": 0.0066,
      "step": 15010
    },
    {
      "epoch": 0.8344444444444444,
      "grad_norm": 0.18015696108341217,
      "learning_rate": 4.165555555555556e-05,
      "loss": 0.0057,
      "step": 15020
    },
    {
      "epoch": 0.835,
      "grad_norm": 0.23510503768920898,
      "learning_rate": 4.165e-05,
      "loss": 0.0053,
      "step": 15030
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.33175918459892273,
      "learning_rate": 4.1644444444444446e-05,
      "loss": 0.0061,
      "step": 15040
    },
    {
      "epoch": 0.8361111111111111,
      "grad_norm": 0.039571214467287064,
      "learning_rate": 4.163888888888889e-05,
      "loss": 0.0073,
      "step": 15050
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.09157814830541611,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0046,
      "step": 15060
    },
    {
      "epoch": 0.8372222222222222,
      "grad_norm": 0.3011198043823242,
      "learning_rate": 4.1627777777777784e-05,
      "loss": 0.0054,
      "step": 15070
    },
    {
      "epoch": 0.8377777777777777,
      "grad_norm": 0.3905602693557739,
      "learning_rate": 4.162222222222222e-05,
      "loss": 0.0039,
      "step": 15080
    },
    {
      "epoch": 0.8383333333333334,
      "grad_norm": 0.5128753185272217,
      "learning_rate": 4.161666666666667e-05,
      "loss": 0.0027,
      "step": 15090
    },
    {
      "epoch": 0.8388888888888889,
      "grad_norm": 0.3650034964084625,
      "learning_rate": 4.1611111111111114e-05,
      "loss": 0.0042,
      "step": 15100
    },
    {
      "epoch": 0.8394444444444444,
      "grad_norm": 0.3609589636325836,
      "learning_rate": 4.160555555555556e-05,
      "loss": 0.0049,
      "step": 15110
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.09104834496974945,
      "learning_rate": 4.16e-05,
      "loss": 0.0053,
      "step": 15120
    },
    {
      "epoch": 0.8405555555555555,
      "grad_norm": 0.362363338470459,
      "learning_rate": 4.1594444444444445e-05,
      "loss": 0.0047,
      "step": 15130
    },
    {
      "epoch": 0.8411111111111111,
      "grad_norm": 0.01659701205790043,
      "learning_rate": 4.158888888888889e-05,
      "loss": 0.0078,
      "step": 15140
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.12128346413373947,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.0041,
      "step": 15150
    },
    {
      "epoch": 0.8422222222222222,
      "grad_norm": 0.12135354429483414,
      "learning_rate": 4.157777777777778e-05,
      "loss": 0.0067,
      "step": 15160
    },
    {
      "epoch": 0.8427777777777777,
      "grad_norm": 0.18160879611968994,
      "learning_rate": 4.1572222222222226e-05,
      "loss": 0.0058,
      "step": 15170
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.061604052782058716,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.0081,
      "step": 15180
    },
    {
      "epoch": 0.8438888888888889,
      "grad_norm": 0.33475619554519653,
      "learning_rate": 4.156111111111111e-05,
      "loss": 0.0055,
      "step": 15190
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.09090977162122726,
      "learning_rate": 4.155555555555556e-05,
      "loss": 0.005,
      "step": 15200
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.3014412224292755,
      "learning_rate": 4.155e-05,
      "loss": 0.0046,
      "step": 15210
    },
    {
      "epoch": 0.8455555555555555,
      "grad_norm": 0.24355468153953552,
      "learning_rate": 4.1544444444444444e-05,
      "loss": 0.0051,
      "step": 15220
    },
    {
      "epoch": 0.8461111111111111,
      "grad_norm": 0.6615692973136902,
      "learning_rate": 4.153888888888889e-05,
      "loss": 0.0064,
      "step": 15230
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.4816211760044098,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0051,
      "step": 15240
    },
    {
      "epoch": 0.8472222222222222,
      "grad_norm": 0.5117565989494324,
      "learning_rate": 4.152777777777778e-05,
      "loss": 0.0051,
      "step": 15250
    },
    {
      "epoch": 0.8477777777777777,
      "grad_norm": 0.2422507256269455,
      "learning_rate": 4.1522222222222225e-05,
      "loss": 0.0045,
      "step": 15260
    },
    {
      "epoch": 0.8483333333333334,
      "grad_norm": 0.2914331555366516,
      "learning_rate": 4.151666666666667e-05,
      "loss": 0.0056,
      "step": 15270
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 0.3311523497104645,
      "learning_rate": 4.151111111111111e-05,
      "loss": 0.0048,
      "step": 15280
    },
    {
      "epoch": 0.8494444444444444,
      "grad_norm": 0.391457200050354,
      "learning_rate": 4.1505555555555556e-05,
      "loss": 0.0047,
      "step": 15290
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.21056018769741058,
      "learning_rate": 4.15e-05,
      "loss": 0.005,
      "step": 15300
    },
    {
      "epoch": 0.8505555555555555,
      "grad_norm": 0.5262276530265808,
      "learning_rate": 4.149444444444445e-05,
      "loss": 0.0058,
      "step": 15310
    },
    {
      "epoch": 0.8511111111111112,
      "grad_norm": 0.06582746654748917,
      "learning_rate": 4.1488888888888886e-05,
      "loss": 0.0046,
      "step": 15320
    },
    {
      "epoch": 0.8516666666666667,
      "grad_norm": 0.6681051850318909,
      "learning_rate": 4.1483333333333337e-05,
      "loss": 0.0063,
      "step": 15330
    },
    {
      "epoch": 0.8522222222222222,
      "grad_norm": 0.7238093018531799,
      "learning_rate": 4.147777777777778e-05,
      "loss": 0.008,
      "step": 15340
    },
    {
      "epoch": 0.8527777777777777,
      "grad_norm": 0.4530162811279297,
      "learning_rate": 4.1472222222222224e-05,
      "loss": 0.0057,
      "step": 15350
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.22962285578250885,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0046,
      "step": 15360
    },
    {
      "epoch": 0.8538888888888889,
      "grad_norm": 0.09101097285747528,
      "learning_rate": 4.146111111111111e-05,
      "loss": 0.0051,
      "step": 15370
    },
    {
      "epoch": 0.8544444444444445,
      "grad_norm": 0.1339525431394577,
      "learning_rate": 4.145555555555556e-05,
      "loss": 0.0055,
      "step": 15380
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.4524093568325043,
      "learning_rate": 4.145e-05,
      "loss": 0.0056,
      "step": 15390
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 0.18123085796833038,
      "learning_rate": 4.144444444444445e-05,
      "loss": 0.006,
      "step": 15400
    },
    {
      "epoch": 0.8561111111111112,
      "grad_norm": 0.01070753950625658,
      "learning_rate": 4.1438888888888885e-05,
      "loss": 0.0046,
      "step": 15410
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 0.15196000039577484,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0052,
      "step": 15420
    },
    {
      "epoch": 0.8572222222222222,
      "grad_norm": 0.42244529724121094,
      "learning_rate": 4.142777777777778e-05,
      "loss": 0.0068,
      "step": 15430
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 0.4241790473461151,
      "learning_rate": 4.142222222222222e-05,
      "loss": 0.0048,
      "step": 15440
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.06091107428073883,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.0058,
      "step": 15450
    },
    {
      "epoch": 0.8588888888888889,
      "grad_norm": 0.2415260225534439,
      "learning_rate": 4.141111111111111e-05,
      "loss": 0.0068,
      "step": 15460
    },
    {
      "epoch": 0.8594444444444445,
      "grad_norm": 0.272087961435318,
      "learning_rate": 4.140555555555556e-05,
      "loss": 0.0035,
      "step": 15470
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.1513896882534027,
      "learning_rate": 4.14e-05,
      "loss": 0.0072,
      "step": 15480
    },
    {
      "epoch": 0.8605555555555555,
      "grad_norm": 0.18078671395778656,
      "learning_rate": 4.139444444444445e-05,
      "loss": 0.0069,
      "step": 15490
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 0.2849239110946655,
      "learning_rate": 4.138888888888889e-05,
      "loss": 0.0043,
      "step": 15500
    },
    {
      "epoch": 0.8616666666666667,
      "grad_norm": 0.06312479823827744,
      "learning_rate": 4.1383333333333334e-05,
      "loss": 0.0044,
      "step": 15510
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 0.2747749090194702,
      "learning_rate": 4.1377777777777784e-05,
      "loss": 0.0044,
      "step": 15520
    },
    {
      "epoch": 0.8627777777777778,
      "grad_norm": 0.03231032192707062,
      "learning_rate": 4.137222222222222e-05,
      "loss": 0.0045,
      "step": 15530
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 0.12121299654245377,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0048,
      "step": 15540
    },
    {
      "epoch": 0.8638888888888889,
      "grad_norm": 0.2230948954820633,
      "learning_rate": 4.136111111111111e-05,
      "loss": 0.006,
      "step": 15550
    },
    {
      "epoch": 0.8644444444444445,
      "grad_norm": 0.5087082982063293,
      "learning_rate": 4.135555555555556e-05,
      "loss": 0.005,
      "step": 15560
    },
    {
      "epoch": 0.865,
      "grad_norm": 0.48273032903671265,
      "learning_rate": 4.135e-05,
      "loss": 0.0037,
      "step": 15570
    },
    {
      "epoch": 0.8655555555555555,
      "grad_norm": 0.12113607674837112,
      "learning_rate": 4.1344444444444446e-05,
      "loss": 0.0053,
      "step": 15580
    },
    {
      "epoch": 0.8661111111111112,
      "grad_norm": 0.18109391629695892,
      "learning_rate": 4.133888888888889e-05,
      "loss": 0.0044,
      "step": 15590
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.19185101985931396,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0056,
      "step": 15600
    },
    {
      "epoch": 0.8672222222222222,
      "grad_norm": 0.42246440052986145,
      "learning_rate": 4.132777777777778e-05,
      "loss": 0.0039,
      "step": 15610
    },
    {
      "epoch": 0.8677777777777778,
      "grad_norm": 0.18233740329742432,
      "learning_rate": 4.132222222222222e-05,
      "loss": 0.0053,
      "step": 15620
    },
    {
      "epoch": 0.8683333333333333,
      "grad_norm": 0.3063993453979492,
      "learning_rate": 4.131666666666667e-05,
      "loss": 0.0059,
      "step": 15630
    },
    {
      "epoch": 0.8688888888888889,
      "grad_norm": 0.0607246570289135,
      "learning_rate": 4.1311111111111114e-05,
      "loss": 0.0054,
      "step": 15640
    },
    {
      "epoch": 0.8694444444444445,
      "grad_norm": 0.013810476288199425,
      "learning_rate": 4.130555555555556e-05,
      "loss": 0.004,
      "step": 15650
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.24152398109436035,
      "learning_rate": 4.13e-05,
      "loss": 0.0047,
      "step": 15660
    },
    {
      "epoch": 0.8705555555555555,
      "grad_norm": 0.578982949256897,
      "learning_rate": 4.1294444444444445e-05,
      "loss": 0.0056,
      "step": 15670
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.6028556227684021,
      "learning_rate": 4.1288888888888895e-05,
      "loss": 0.0056,
      "step": 15680
    },
    {
      "epoch": 0.8716666666666667,
      "grad_norm": 0.1326030194759369,
      "learning_rate": 4.128333333333333e-05,
      "loss": 0.0052,
      "step": 15690
    },
    {
      "epoch": 0.8722222222222222,
      "grad_norm": 0.013248921371996403,
      "learning_rate": 4.127777777777778e-05,
      "loss": 0.0053,
      "step": 15700
    },
    {
      "epoch": 0.8727777777777778,
      "grad_norm": 0.4239807426929474,
      "learning_rate": 4.1272222222222226e-05,
      "loss": 0.0044,
      "step": 15710
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.6943721771240234,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0041,
      "step": 15720
    },
    {
      "epoch": 0.8738888888888889,
      "grad_norm": 0.42298153042793274,
      "learning_rate": 4.126111111111111e-05,
      "loss": 0.0049,
      "step": 15730
    },
    {
      "epoch": 0.8744444444444445,
      "grad_norm": 0.1516575664281845,
      "learning_rate": 4.1255555555555556e-05,
      "loss": 0.0063,
      "step": 15740
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.39307352900505066,
      "learning_rate": 4.125e-05,
      "loss": 0.0057,
      "step": 15750
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 0.06116639822721481,
      "learning_rate": 4.124444444444444e-05,
      "loss": 0.0054,
      "step": 15760
    },
    {
      "epoch": 0.8761111111111111,
      "grad_norm": 0.2415476143360138,
      "learning_rate": 4.1238888888888894e-05,
      "loss": 0.0035,
      "step": 15770
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 0.6030755043029785,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.0056,
      "step": 15780
    },
    {
      "epoch": 0.8772222222222222,
      "grad_norm": 0.39238813519477844,
      "learning_rate": 4.122777777777778e-05,
      "loss": 0.0057,
      "step": 15790
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 0.30152955651283264,
      "learning_rate": 4.1222222222222224e-05,
      "loss": 0.0047,
      "step": 15800
    },
    {
      "epoch": 0.8783333333333333,
      "grad_norm": 0.031463611871004105,
      "learning_rate": 4.121666666666667e-05,
      "loss": 0.004,
      "step": 15810
    },
    {
      "epoch": 0.8788888888888889,
      "grad_norm": 0.24184414744377136,
      "learning_rate": 4.121111111111111e-05,
      "loss": 0.0044,
      "step": 15820
    },
    {
      "epoch": 0.8794444444444445,
      "grad_norm": 0.539207935333252,
      "learning_rate": 4.1205555555555555e-05,
      "loss": 0.0042,
      "step": 15830
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.6318272352218628,
      "learning_rate": 4.12e-05,
      "loss": 0.0053,
      "step": 15840
    },
    {
      "epoch": 0.8805555555555555,
      "grad_norm": 0.24115297198295593,
      "learning_rate": 4.119444444444445e-05,
      "loss": 0.0062,
      "step": 15850
    },
    {
      "epoch": 0.8811111111111111,
      "grad_norm": 0.18123649060726166,
      "learning_rate": 4.118888888888889e-05,
      "loss": 0.0057,
      "step": 15860
    },
    {
      "epoch": 0.8816666666666667,
      "grad_norm": 0.45147499442100525,
      "learning_rate": 4.1183333333333336e-05,
      "loss": 0.005,
      "step": 15870
    },
    {
      "epoch": 0.8822222222222222,
      "grad_norm": 0.18164747953414917,
      "learning_rate": 4.117777777777778e-05,
      "loss": 0.0052,
      "step": 15880
    },
    {
      "epoch": 0.8827777777777778,
      "grad_norm": 0.1211877316236496,
      "learning_rate": 4.117222222222222e-05,
      "loss": 0.0045,
      "step": 15890
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.24084199965000153,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0055,
      "step": 15900
    },
    {
      "epoch": 0.8838888888888888,
      "grad_norm": 0.09094814211130142,
      "learning_rate": 4.116111111111111e-05,
      "loss": 0.0057,
      "step": 15910
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 0.3906390368938446,
      "learning_rate": 4.115555555555556e-05,
      "loss": 0.0035,
      "step": 15920
    },
    {
      "epoch": 0.885,
      "grad_norm": 0.2965632379055023,
      "learning_rate": 4.115e-05,
      "loss": 0.0046,
      "step": 15930
    },
    {
      "epoch": 0.8855555555555555,
      "grad_norm": 0.09542905539274216,
      "learning_rate": 4.114444444444445e-05,
      "loss": 0.0059,
      "step": 15940
    },
    {
      "epoch": 0.8861111111111111,
      "grad_norm": 0.2107315957546234,
      "learning_rate": 4.113888888888889e-05,
      "loss": 0.0077,
      "step": 15950
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.29532840847969055,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0061,
      "step": 15960
    },
    {
      "epoch": 0.8872222222222222,
      "grad_norm": 0.4513947665691376,
      "learning_rate": 4.1127777777777785e-05,
      "loss": 0.0051,
      "step": 15970
    },
    {
      "epoch": 0.8877777777777778,
      "grad_norm": 0.0346028134226799,
      "learning_rate": 4.112222222222222e-05,
      "loss": 0.0057,
      "step": 15980
    },
    {
      "epoch": 0.8883333333333333,
      "grad_norm": 0.2750096321105957,
      "learning_rate": 4.111666666666667e-05,
      "loss": 0.007,
      "step": 15990
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.06104928255081177,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.0073,
      "step": 16000
    },
    {
      "epoch": 0.8894444444444445,
      "grad_norm": 0.39223194122314453,
      "learning_rate": 4.110555555555556e-05,
      "loss": 0.0074,
      "step": 16010
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.459513783454895,
      "learning_rate": 4.11e-05,
      "loss": 0.0049,
      "step": 16020
    },
    {
      "epoch": 0.8905555555555555,
      "grad_norm": 0.12097658962011337,
      "learning_rate": 4.1094444444444446e-05,
      "loss": 0.0049,
      "step": 16030
    },
    {
      "epoch": 0.8911111111111111,
      "grad_norm": 0.2997741997241974,
      "learning_rate": 4.10888888888889e-05,
      "loss": 0.0045,
      "step": 16040
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 0.033007312566041946,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.0054,
      "step": 16050
    },
    {
      "epoch": 0.8922222222222222,
      "grad_norm": 0.06152300164103508,
      "learning_rate": 4.1077777777777784e-05,
      "loss": 0.0059,
      "step": 16060
    },
    {
      "epoch": 0.8927777777777778,
      "grad_norm": 0.09153635054826736,
      "learning_rate": 4.107222222222222e-05,
      "loss": 0.0046,
      "step": 16070
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.4250328838825226,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.004,
      "step": 16080
    },
    {
      "epoch": 0.8938888888888888,
      "grad_norm": 0.06234533712267876,
      "learning_rate": 4.1061111111111115e-05,
      "loss": 0.0046,
      "step": 16090
    },
    {
      "epoch": 0.8944444444444445,
      "grad_norm": 0.2426876276731491,
      "learning_rate": 4.105555555555556e-05,
      "loss": 0.0048,
      "step": 16100
    },
    {
      "epoch": 0.895,
      "grad_norm": 0.03682543337345123,
      "learning_rate": 4.105e-05,
      "loss": 0.0041,
      "step": 16110
    },
    {
      "epoch": 0.8955555555555555,
      "grad_norm": 0.24139553308486938,
      "learning_rate": 4.1044444444444445e-05,
      "loss": 0.0044,
      "step": 16120
    },
    {
      "epoch": 0.8961111111111111,
      "grad_norm": 0.2712712287902832,
      "learning_rate": 4.1038888888888896e-05,
      "loss": 0.0058,
      "step": 16130
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.35826966166496277,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0047,
      "step": 16140
    },
    {
      "epoch": 0.8972222222222223,
      "grad_norm": 0.4512566328048706,
      "learning_rate": 4.102777777777778e-05,
      "loss": 0.0083,
      "step": 16150
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 0.5721966028213501,
      "learning_rate": 4.1022222222222226e-05,
      "loss": 0.0047,
      "step": 16160
    },
    {
      "epoch": 0.8983333333333333,
      "grad_norm": 0.33085617423057556,
      "learning_rate": 4.101666666666667e-05,
      "loss": 0.0042,
      "step": 16170
    },
    {
      "epoch": 0.8988888888888888,
      "grad_norm": 0.03433610126376152,
      "learning_rate": 4.101111111111111e-05,
      "loss": 0.0054,
      "step": 16180
    },
    {
      "epoch": 0.8994444444444445,
      "grad_norm": 0.15264402329921722,
      "learning_rate": 4.100555555555556e-05,
      "loss": 0.0044,
      "step": 16190
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.2410264015197754,
      "learning_rate": 4.1e-05,
      "loss": 0.005,
      "step": 16200
    },
    {
      "epoch": 0.9005555555555556,
      "grad_norm": 0.2704278528690338,
      "learning_rate": 4.0994444444444444e-05,
      "loss": 0.0038,
      "step": 16210
    },
    {
      "epoch": 0.9011111111111111,
      "grad_norm": 0.1523863673210144,
      "learning_rate": 4.0988888888888894e-05,
      "loss": 0.0061,
      "step": 16220
    },
    {
      "epoch": 0.9016666666666666,
      "grad_norm": 0.09253320097923279,
      "learning_rate": 4.098333333333334e-05,
      "loss": 0.0054,
      "step": 16230
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 0.12020868062973022,
      "learning_rate": 4.097777777777778e-05,
      "loss": 0.0034,
      "step": 16240
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 0.3305601477622986,
      "learning_rate": 4.0972222222222225e-05,
      "loss": 0.0057,
      "step": 16250
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.36097684502601624,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0051,
      "step": 16260
    },
    {
      "epoch": 0.9038888888888889,
      "grad_norm": 0.5717095136642456,
      "learning_rate": 4.096111111111111e-05,
      "loss": 0.0063,
      "step": 16270
    },
    {
      "epoch": 0.9044444444444445,
      "grad_norm": 0.38571617007255554,
      "learning_rate": 4.0955555555555556e-05,
      "loss": 0.0052,
      "step": 16280
    },
    {
      "epoch": 0.905,
      "grad_norm": 0.1219978854060173,
      "learning_rate": 4.095e-05,
      "loss": 0.003,
      "step": 16290
    },
    {
      "epoch": 0.9055555555555556,
      "grad_norm": 0.21168255805969238,
      "learning_rate": 4.094444444444445e-05,
      "loss": 0.0042,
      "step": 16300
    },
    {
      "epoch": 0.9061111111111111,
      "grad_norm": 0.451928973197937,
      "learning_rate": 4.093888888888889e-05,
      "loss": 0.0058,
      "step": 16310
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.12180392444133759,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0051,
      "step": 16320
    },
    {
      "epoch": 0.9072222222222223,
      "grad_norm": 0.15028586983680725,
      "learning_rate": 4.092777777777778e-05,
      "loss": 0.0053,
      "step": 16330
    },
    {
      "epoch": 0.9077777777777778,
      "grad_norm": 0.09055773168802261,
      "learning_rate": 4.0922222222222224e-05,
      "loss": 0.0054,
      "step": 16340
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.48053812980651855,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.0047,
      "step": 16350
    },
    {
      "epoch": 0.9088888888888889,
      "grad_norm": 0.18201643228530884,
      "learning_rate": 4.091111111111111e-05,
      "loss": 0.0064,
      "step": 16360
    },
    {
      "epoch": 0.9094444444444445,
      "grad_norm": 0.4214361906051636,
      "learning_rate": 4.090555555555556e-05,
      "loss": 0.006,
      "step": 16370
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.012998759746551514,
      "learning_rate": 4.09e-05,
      "loss": 0.0059,
      "step": 16380
    },
    {
      "epoch": 0.9105555555555556,
      "grad_norm": 0.31941863894462585,
      "learning_rate": 4.089444444444445e-05,
      "loss": 0.0046,
      "step": 16390
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.15048643946647644,
      "learning_rate": 4.088888888888889e-05,
      "loss": 0.0055,
      "step": 16400
    },
    {
      "epoch": 0.9116666666666666,
      "grad_norm": 0.27065908908843994,
      "learning_rate": 4.0883333333333335e-05,
      "loss": 0.007,
      "step": 16410
    },
    {
      "epoch": 0.9122222222222223,
      "grad_norm": 0.36123132705688477,
      "learning_rate": 4.087777777777778e-05,
      "loss": 0.0045,
      "step": 16420
    },
    {
      "epoch": 0.9127777777777778,
      "grad_norm": 0.3011275827884674,
      "learning_rate": 4.087222222222222e-05,
      "loss": 0.0049,
      "step": 16430
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.25866857171058655,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0051,
      "step": 16440
    },
    {
      "epoch": 0.9138888888888889,
      "grad_norm": 0.1810319870710373,
      "learning_rate": 4.086111111111111e-05,
      "loss": 0.004,
      "step": 16450
    },
    {
      "epoch": 0.9144444444444444,
      "grad_norm": 0.06097938120365143,
      "learning_rate": 4.085555555555556e-05,
      "loss": 0.0046,
      "step": 16460
    },
    {
      "epoch": 0.915,
      "grad_norm": 0.22494444251060486,
      "learning_rate": 4.085e-05,
      "loss": 0.0055,
      "step": 16470
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 0.3621685802936554,
      "learning_rate": 4.084444444444445e-05,
      "loss": 0.0036,
      "step": 16480
    },
    {
      "epoch": 0.9161111111111111,
      "grad_norm": 0.06289441138505936,
      "learning_rate": 4.083888888888889e-05,
      "loss": 0.0053,
      "step": 16490
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.6619595885276794,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0052,
      "step": 16500
    },
    {
      "epoch": 0.9172222222222223,
      "grad_norm": 0.21096928417682648,
      "learning_rate": 4.0827777777777785e-05,
      "loss": 0.0043,
      "step": 16510
    },
    {
      "epoch": 0.9177777777777778,
      "grad_norm": 0.36688706278800964,
      "learning_rate": 4.082222222222222e-05,
      "loss": 0.005,
      "step": 16520
    },
    {
      "epoch": 0.9183333333333333,
      "grad_norm": 0.18123100697994232,
      "learning_rate": 4.081666666666667e-05,
      "loss": 0.0068,
      "step": 16530
    },
    {
      "epoch": 0.9188888888888889,
      "grad_norm": 0.06248994171619415,
      "learning_rate": 4.081111111111111e-05,
      "loss": 0.0045,
      "step": 16540
    },
    {
      "epoch": 0.9194444444444444,
      "grad_norm": 0.025314684957265854,
      "learning_rate": 4.080555555555556e-05,
      "loss": 0.0086,
      "step": 16550
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.24157319962978363,
      "learning_rate": 4.08e-05,
      "loss": 0.0033,
      "step": 16560
    },
    {
      "epoch": 0.9205555555555556,
      "grad_norm": 0.3913029432296753,
      "learning_rate": 4.0794444444444446e-05,
      "loss": 0.0046,
      "step": 16570
    },
    {
      "epoch": 0.9211111111111111,
      "grad_norm": 0.15155723690986633,
      "learning_rate": 4.0788888888888896e-05,
      "loss": 0.0045,
      "step": 16580
    },
    {
      "epoch": 0.9216666666666666,
      "grad_norm": 0.33300620317459106,
      "learning_rate": 4.078333333333333e-05,
      "loss": 0.006,
      "step": 16590
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.3011108338832855,
      "learning_rate": 4.0777777777777783e-05,
      "loss": 0.0051,
      "step": 16600
    },
    {
      "epoch": 0.9227777777777778,
      "grad_norm": 0.33090144395828247,
      "learning_rate": 4.077222222222222e-05,
      "loss": 0.0047,
      "step": 16610
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 0.19510634243488312,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.0052,
      "step": 16620
    },
    {
      "epoch": 0.9238888888888889,
      "grad_norm": 0.4206569194793701,
      "learning_rate": 4.0761111111111114e-05,
      "loss": 0.0047,
      "step": 16630
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.6311421394348145,
      "learning_rate": 4.075555555555556e-05,
      "loss": 0.005,
      "step": 16640
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.1707485318183899,
      "learning_rate": 4.075e-05,
      "loss": 0.0046,
      "step": 16650
    },
    {
      "epoch": 0.9255555555555556,
      "grad_norm": 0.3666287958621979,
      "learning_rate": 4.0744444444444445e-05,
      "loss": 0.0045,
      "step": 16660
    },
    {
      "epoch": 0.9261111111111111,
      "grad_norm": 0.24080632627010345,
      "learning_rate": 4.0738888888888895e-05,
      "loss": 0.005,
      "step": 16670
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.28172338008880615,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0034,
      "step": 16680
    },
    {
      "epoch": 0.9272222222222222,
      "grad_norm": 0.4214285910129547,
      "learning_rate": 4.072777777777778e-05,
      "loss": 0.0043,
      "step": 16690
    },
    {
      "epoch": 0.9277777777777778,
      "grad_norm": 0.010407046414911747,
      "learning_rate": 4.0722222222222226e-05,
      "loss": 0.0065,
      "step": 16700
    },
    {
      "epoch": 0.9283333333333333,
      "grad_norm": 0.2517719864845276,
      "learning_rate": 4.071666666666667e-05,
      "loss": 0.0065,
      "step": 16710
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 0.27075350284576416,
      "learning_rate": 4.071111111111111e-05,
      "loss": 0.0061,
      "step": 16720
    },
    {
      "epoch": 0.9294444444444444,
      "grad_norm": 0.30053552985191345,
      "learning_rate": 4.0705555555555556e-05,
      "loss": 0.0061,
      "step": 16730
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.06173686683177948,
      "learning_rate": 4.07e-05,
      "loss": 0.0067,
      "step": 16740
    },
    {
      "epoch": 0.9305555555555556,
      "grad_norm": 0.3614874482154846,
      "learning_rate": 4.0694444444444444e-05,
      "loss": 0.006,
      "step": 16750
    },
    {
      "epoch": 0.9311111111111111,
      "grad_norm": 0.03173613175749779,
      "learning_rate": 4.0688888888888894e-05,
      "loss": 0.0046,
      "step": 16760
    },
    {
      "epoch": 0.9316666666666666,
      "grad_norm": 0.12173942476511002,
      "learning_rate": 4.068333333333334e-05,
      "loss": 0.0052,
      "step": 16770
    },
    {
      "epoch": 0.9322222222222222,
      "grad_norm": 0.3051389753818512,
      "learning_rate": 4.067777777777778e-05,
      "loss": 0.0047,
      "step": 16780
    },
    {
      "epoch": 0.9327777777777778,
      "grad_norm": 0.25424736738204956,
      "learning_rate": 4.0672222222222225e-05,
      "loss": 0.0059,
      "step": 16790
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.2406332939863205,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0045,
      "step": 16800
    },
    {
      "epoch": 0.9338888888888889,
      "grad_norm": 0.18531958758831024,
      "learning_rate": 4.066111111111111e-05,
      "loss": 0.0033,
      "step": 16810
    },
    {
      "epoch": 0.9344444444444444,
      "grad_norm": 0.09379233419895172,
      "learning_rate": 4.0655555555555555e-05,
      "loss": 0.0039,
      "step": 16820
    },
    {
      "epoch": 0.935,
      "grad_norm": 0.24054406583309174,
      "learning_rate": 4.065e-05,
      "loss": 0.0041,
      "step": 16830
    },
    {
      "epoch": 0.9355555555555556,
      "grad_norm": 0.37033408880233765,
      "learning_rate": 4.064444444444445e-05,
      "loss": 0.0066,
      "step": 16840
    },
    {
      "epoch": 0.9361111111111111,
      "grad_norm": 0.06911145150661469,
      "learning_rate": 4.063888888888889e-05,
      "loss": 0.0046,
      "step": 16850
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.061133503913879395,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.0037,
      "step": 16860
    },
    {
      "epoch": 0.9372222222222222,
      "grad_norm": 0.09668180346488953,
      "learning_rate": 4.062777777777778e-05,
      "loss": 0.0056,
      "step": 16870
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 0.5037065744400024,
      "learning_rate": 4.062222222222222e-05,
      "loss": 0.0044,
      "step": 16880
    },
    {
      "epoch": 0.9383333333333334,
      "grad_norm": 0.573093056678772,
      "learning_rate": 4.061666666666667e-05,
      "loss": 0.0063,
      "step": 16890
    },
    {
      "epoch": 0.9388888888888889,
      "grad_norm": 0.06260202825069427,
      "learning_rate": 4.061111111111111e-05,
      "loss": 0.0077,
      "step": 16900
    },
    {
      "epoch": 0.9394444444444444,
      "grad_norm": 0.4219790995121002,
      "learning_rate": 4.060555555555556e-05,
      "loss": 0.0063,
      "step": 16910
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.1806076616048813,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.006,
      "step": 16920
    },
    {
      "epoch": 0.9405555555555556,
      "grad_norm": 0.24182726442813873,
      "learning_rate": 4.059444444444445e-05,
      "loss": 0.0047,
      "step": 16930
    },
    {
      "epoch": 0.9411111111111111,
      "grad_norm": 0.1210036650300026,
      "learning_rate": 4.058888888888889e-05,
      "loss": 0.0057,
      "step": 16940
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.36162763833999634,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.0044,
      "step": 16950
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.5777896046638489,
      "learning_rate": 4.057777777777778e-05,
      "loss": 0.0041,
      "step": 16960
    },
    {
      "epoch": 0.9427777777777778,
      "grad_norm": 0.3431657552719116,
      "learning_rate": 4.057222222222222e-05,
      "loss": 0.0072,
      "step": 16970
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.364795446395874,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0047,
      "step": 16980
    },
    {
      "epoch": 0.9438888888888889,
      "grad_norm": 0.21086597442626953,
      "learning_rate": 4.056111111111111e-05,
      "loss": 0.0045,
      "step": 16990
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.1803721934556961,
      "learning_rate": 4.055555555555556e-05,
      "loss": 0.0053,
      "step": 17000
    },
    {
      "epoch": 0.945,
      "grad_norm": 0.48992833495140076,
      "learning_rate": 4.055e-05,
      "loss": 0.0058,
      "step": 17010
    },
    {
      "epoch": 0.9455555555555556,
      "grad_norm": 0.30485638976097107,
      "learning_rate": 4.054444444444445e-05,
      "loss": 0.0052,
      "step": 17020
    },
    {
      "epoch": 0.9461111111111111,
      "grad_norm": 0.21091410517692566,
      "learning_rate": 4.053888888888889e-05,
      "loss": 0.0057,
      "step": 17030
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.1807769536972046,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.005,
      "step": 17040
    },
    {
      "epoch": 0.9472222222222222,
      "grad_norm": 0.18131756782531738,
      "learning_rate": 4.0527777777777784e-05,
      "loss": 0.0075,
      "step": 17050
    },
    {
      "epoch": 0.9477777777777778,
      "grad_norm": 0.21113112568855286,
      "learning_rate": 4.052222222222222e-05,
      "loss": 0.0079,
      "step": 17060
    },
    {
      "epoch": 0.9483333333333334,
      "grad_norm": 0.042256761342287064,
      "learning_rate": 4.051666666666667e-05,
      "loss": 0.0061,
      "step": 17070
    },
    {
      "epoch": 0.9488888888888889,
      "grad_norm": 0.06105254590511322,
      "learning_rate": 4.051111111111111e-05,
      "loss": 0.0059,
      "step": 17080
    },
    {
      "epoch": 0.9494444444444444,
      "grad_norm": 0.27097809314727783,
      "learning_rate": 4.050555555555556e-05,
      "loss": 0.0057,
      "step": 17090
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.42207545042037964,
      "learning_rate": 4.05e-05,
      "loss": 0.0043,
      "step": 17100
    },
    {
      "epoch": 0.9505555555555556,
      "grad_norm": 0.03222279250621796,
      "learning_rate": 4.0494444444444445e-05,
      "loss": 0.0053,
      "step": 17110
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 0.42194509506225586,
      "learning_rate": 4.0488888888888896e-05,
      "loss": 0.0055,
      "step": 17120
    },
    {
      "epoch": 0.9516666666666667,
      "grad_norm": 0.6323665976524353,
      "learning_rate": 4.048333333333333e-05,
      "loss": 0.0047,
      "step": 17130
    },
    {
      "epoch": 0.9522222222222222,
      "grad_norm": 0.4061858654022217,
      "learning_rate": 4.047777777777778e-05,
      "loss": 0.0061,
      "step": 17140
    },
    {
      "epoch": 0.9527777777777777,
      "grad_norm": 0.3014060854911804,
      "learning_rate": 4.047222222222222e-05,
      "loss": 0.0055,
      "step": 17150
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.1144929751753807,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0043,
      "step": 17160
    },
    {
      "epoch": 0.9538888888888889,
      "grad_norm": 0.056054335087537766,
      "learning_rate": 4.0461111111111114e-05,
      "loss": 0.005,
      "step": 17170
    },
    {
      "epoch": 0.9544444444444444,
      "grad_norm": 0.24515332281589508,
      "learning_rate": 4.045555555555556e-05,
      "loss": 0.0051,
      "step": 17180
    },
    {
      "epoch": 0.955,
      "grad_norm": 0.09025125950574875,
      "learning_rate": 4.045000000000001e-05,
      "loss": 0.005,
      "step": 17190
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.3603897988796234,
      "learning_rate": 4.0444444444444444e-05,
      "loss": 0.0036,
      "step": 17200
    },
    {
      "epoch": 0.9561111111111111,
      "grad_norm": 0.39810997247695923,
      "learning_rate": 4.0438888888888895e-05,
      "loss": 0.0037,
      "step": 17210
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.15052539110183716,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0035,
      "step": 17220
    },
    {
      "epoch": 0.9572222222222222,
      "grad_norm": 0.18094569444656372,
      "learning_rate": 4.042777777777778e-05,
      "loss": 0.0045,
      "step": 17230
    },
    {
      "epoch": 0.9577777777777777,
      "grad_norm": 0.24053151905536652,
      "learning_rate": 4.0422222222222225e-05,
      "loss": 0.0048,
      "step": 17240
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.4805032014846802,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.005,
      "step": 17250
    },
    {
      "epoch": 0.9588888888888889,
      "grad_norm": 0.4217408001422882,
      "learning_rate": 4.041111111111111e-05,
      "loss": 0.0054,
      "step": 17260
    },
    {
      "epoch": 0.9594444444444444,
      "grad_norm": 0.20166367292404175,
      "learning_rate": 4.0405555555555556e-05,
      "loss": 0.0049,
      "step": 17270
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.06084277108311653,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0042,
      "step": 17280
    },
    {
      "epoch": 0.9605555555555556,
      "grad_norm": 0.2426697313785553,
      "learning_rate": 4.039444444444444e-05,
      "loss": 0.0044,
      "step": 17290
    },
    {
      "epoch": 0.9611111111111111,
      "grad_norm": 0.06135471165180206,
      "learning_rate": 4.038888888888889e-05,
      "loss": 0.0049,
      "step": 17300
    },
    {
      "epoch": 0.9616666666666667,
      "grad_norm": 0.15691809356212616,
      "learning_rate": 4.038333333333334e-05,
      "loss": 0.0046,
      "step": 17310
    },
    {
      "epoch": 0.9622222222222222,
      "grad_norm": 0.09387759864330292,
      "learning_rate": 4.037777777777778e-05,
      "loss": 0.0041,
      "step": 17320
    },
    {
      "epoch": 0.9627777777777777,
      "grad_norm": 0.30071505904197693,
      "learning_rate": 4.0372222222222224e-05,
      "loss": 0.0049,
      "step": 17330
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 0.3319648504257202,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.004,
      "step": 17340
    },
    {
      "epoch": 0.9638888888888889,
      "grad_norm": 0.09031429886817932,
      "learning_rate": 4.036111111111111e-05,
      "loss": 0.0057,
      "step": 17350
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 0.24558821320533752,
      "learning_rate": 4.0355555555555555e-05,
      "loss": 0.0048,
      "step": 17360
    },
    {
      "epoch": 0.965,
      "grad_norm": 0.5730140209197998,
      "learning_rate": 4.0350000000000005e-05,
      "loss": 0.0053,
      "step": 17370
    },
    {
      "epoch": 0.9655555555555555,
      "grad_norm": 0.6622968912124634,
      "learning_rate": 4.034444444444445e-05,
      "loss": 0.0057,
      "step": 17380
    },
    {
      "epoch": 0.9661111111111111,
      "grad_norm": 0.26288026571273804,
      "learning_rate": 4.033888888888889e-05,
      "loss": 0.0053,
      "step": 17390
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.6438779234886169,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0051,
      "step": 17400
    },
    {
      "epoch": 0.9672222222222222,
      "grad_norm": 0.06076109781861305,
      "learning_rate": 4.032777777777778e-05,
      "loss": 0.004,
      "step": 17410
    },
    {
      "epoch": 0.9677777777777777,
      "grad_norm": 0.15311315655708313,
      "learning_rate": 4.032222222222222e-05,
      "loss": 0.0063,
      "step": 17420
    },
    {
      "epoch": 0.9683333333333334,
      "grad_norm": 0.12164849042892456,
      "learning_rate": 4.0316666666666666e-05,
      "loss": 0.0053,
      "step": 17430
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 0.1507079005241394,
      "learning_rate": 4.031111111111111e-05,
      "loss": 0.0061,
      "step": 17440
    },
    {
      "epoch": 0.9694444444444444,
      "grad_norm": 0.1506897211074829,
      "learning_rate": 4.030555555555556e-05,
      "loss": 0.0055,
      "step": 17450
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.3023511469364166,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0046,
      "step": 17460
    },
    {
      "epoch": 0.9705555555555555,
      "grad_norm": 0.09361549466848373,
      "learning_rate": 4.029444444444445e-05,
      "loss": 0.0054,
      "step": 17470
    },
    {
      "epoch": 0.9711111111111111,
      "grad_norm": 0.14339609444141388,
      "learning_rate": 4.028888888888889e-05,
      "loss": 0.0051,
      "step": 17480
    },
    {
      "epoch": 0.9716666666666667,
      "grad_norm": 0.541016697883606,
      "learning_rate": 4.0283333333333334e-05,
      "loss": 0.0057,
      "step": 17490
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 0.09198218584060669,
      "learning_rate": 4.027777777777778e-05,
      "loss": 0.007,
      "step": 17500
    },
    {
      "epoch": 0.9727777777777777,
      "grad_norm": 0.694780170917511,
      "learning_rate": 4.027222222222222e-05,
      "loss": 0.0067,
      "step": 17510
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.24138835072517395,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.005,
      "step": 17520
    },
    {
      "epoch": 0.9738888888888889,
      "grad_norm": 0.30060437321662903,
      "learning_rate": 4.026111111111111e-05,
      "loss": 0.0047,
      "step": 17530
    },
    {
      "epoch": 0.9744444444444444,
      "grad_norm": 0.5126093626022339,
      "learning_rate": 4.025555555555556e-05,
      "loss": 0.0065,
      "step": 17540
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.2403445690870285,
      "learning_rate": 4.025e-05,
      "loss": 0.0068,
      "step": 17550
    },
    {
      "epoch": 0.9755555555555555,
      "grad_norm": 0.5102855563163757,
      "learning_rate": 4.0244444444444446e-05,
      "loss": 0.006,
      "step": 17560
    },
    {
      "epoch": 0.9761111111111112,
      "grad_norm": 0.42114683985710144,
      "learning_rate": 4.023888888888889e-05,
      "loss": 0.0052,
      "step": 17570
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 0.30146923661231995,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.0059,
      "step": 17580
    },
    {
      "epoch": 0.9772222222222222,
      "grad_norm": 0.4510260820388794,
      "learning_rate": 4.0227777777777784e-05,
      "loss": 0.0048,
      "step": 17590
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.35633307695388794,
      "learning_rate": 4.022222222222222e-05,
      "loss": 0.0056,
      "step": 17600
    },
    {
      "epoch": 0.9783333333333334,
      "grad_norm": 0.013432955369353294,
      "learning_rate": 4.021666666666667e-05,
      "loss": 0.0042,
      "step": 17610
    },
    {
      "epoch": 0.9788888888888889,
      "grad_norm": 0.03275957703590393,
      "learning_rate": 4.021111111111111e-05,
      "loss": 0.0056,
      "step": 17620
    },
    {
      "epoch": 0.9794444444444445,
      "grad_norm": 0.48892346024513245,
      "learning_rate": 4.020555555555556e-05,
      "loss": 0.0066,
      "step": 17630
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.4210038185119629,
      "learning_rate": 4.02e-05,
      "loss": 0.0052,
      "step": 17640
    },
    {
      "epoch": 0.9805555555555555,
      "grad_norm": 0.1532905548810959,
      "learning_rate": 4.0194444444444445e-05,
      "loss": 0.0062,
      "step": 17650
    },
    {
      "epoch": 0.9811111111111112,
      "grad_norm": 0.7207045555114746,
      "learning_rate": 4.0188888888888895e-05,
      "loss": 0.0051,
      "step": 17660
    },
    {
      "epoch": 0.9816666666666667,
      "grad_norm": 0.5368221998214722,
      "learning_rate": 4.018333333333333e-05,
      "loss": 0.0046,
      "step": 17670
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 0.24069343507289886,
      "learning_rate": 4.017777777777778e-05,
      "loss": 0.0057,
      "step": 17680
    },
    {
      "epoch": 0.9827777777777778,
      "grad_norm": 0.42104798555374146,
      "learning_rate": 4.017222222222222e-05,
      "loss": 0.0052,
      "step": 17690
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.212566077709198,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0038,
      "step": 17700
    },
    {
      "epoch": 0.9838888888888889,
      "grad_norm": 0.12117395550012589,
      "learning_rate": 4.016111111111111e-05,
      "loss": 0.0047,
      "step": 17710
    },
    {
      "epoch": 0.9844444444444445,
      "grad_norm": 0.06145989149808884,
      "learning_rate": 4.0155555555555557e-05,
      "loss": 0.0042,
      "step": 17720
    },
    {
      "epoch": 0.985,
      "grad_norm": 0.12119605392217636,
      "learning_rate": 4.015000000000001e-05,
      "loss": 0.0059,
      "step": 17730
    },
    {
      "epoch": 0.9855555555555555,
      "grad_norm": 0.24136683344841003,
      "learning_rate": 4.0144444444444444e-05,
      "loss": 0.0051,
      "step": 17740
    },
    {
      "epoch": 0.9861111111111112,
      "grad_norm": 0.2120129019021988,
      "learning_rate": 4.0138888888888894e-05,
      "loss": 0.0071,
      "step": 17750
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.3608027696609497,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0033,
      "step": 17760
    },
    {
      "epoch": 0.9872222222222222,
      "grad_norm": 0.24082127213478088,
      "learning_rate": 4.012777777777778e-05,
      "loss": 0.0056,
      "step": 17770
    },
    {
      "epoch": 0.9877777777777778,
      "grad_norm": 0.29990166425704956,
      "learning_rate": 4.0122222222222225e-05,
      "loss": 0.0052,
      "step": 17780
    },
    {
      "epoch": 0.9883333333333333,
      "grad_norm": 0.18496055901050568,
      "learning_rate": 4.011666666666667e-05,
      "loss": 0.0037,
      "step": 17790
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.3010675013065338,
      "learning_rate": 4.011111111111111e-05,
      "loss": 0.0038,
      "step": 17800
    },
    {
      "epoch": 0.9894444444444445,
      "grad_norm": 0.1806725710630417,
      "learning_rate": 4.0105555555555555e-05,
      "loss": 0.0044,
      "step": 17810
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.21717457473278046,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.0042,
      "step": 17820
    },
    {
      "epoch": 0.9905555555555555,
      "grad_norm": 0.27157577872276306,
      "learning_rate": 4.009444444444444e-05,
      "loss": 0.0066,
      "step": 17830
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 0.3004831075668335,
      "learning_rate": 4.008888888888889e-05,
      "loss": 0.0045,
      "step": 17840
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.010853493586182594,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.0051,
      "step": 17850
    },
    {
      "epoch": 0.9922222222222222,
      "grad_norm": 0.27129366993904114,
      "learning_rate": 4.007777777777778e-05,
      "loss": 0.0067,
      "step": 17860
    },
    {
      "epoch": 0.9927777777777778,
      "grad_norm": 0.3311459720134735,
      "learning_rate": 4.0072222222222223e-05,
      "loss": 0.0042,
      "step": 17870
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.12080603092908859,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0047,
      "step": 17880
    },
    {
      "epoch": 0.9938888888888889,
      "grad_norm": 0.3010168671607971,
      "learning_rate": 4.006111111111111e-05,
      "loss": 0.0069,
      "step": 17890
    },
    {
      "epoch": 0.9944444444444445,
      "grad_norm": 0.062983438372612,
      "learning_rate": 4.0055555555555554e-05,
      "loss": 0.0049,
      "step": 17900
    },
    {
      "epoch": 0.995,
      "grad_norm": 0.3911285698413849,
      "learning_rate": 4.0050000000000004e-05,
      "loss": 0.0045,
      "step": 17910
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.4933459460735321,
      "learning_rate": 4.004444444444445e-05,
      "loss": 0.0033,
      "step": 17920
    },
    {
      "epoch": 0.9961111111111111,
      "grad_norm": 0.060347456485033035,
      "learning_rate": 4.003888888888889e-05,
      "loss": 0.0051,
      "step": 17930
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.3610133230686188,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0041,
      "step": 17940
    },
    {
      "epoch": 0.9972222222222222,
      "grad_norm": 0.2415841668844223,
      "learning_rate": 4.002777777777778e-05,
      "loss": 0.0059,
      "step": 17950
    },
    {
      "epoch": 0.9977777777777778,
      "grad_norm": 0.1504739671945572,
      "learning_rate": 4.002222222222222e-05,
      "loss": 0.0067,
      "step": 17960
    },
    {
      "epoch": 0.9983333333333333,
      "grad_norm": 0.3013092577457428,
      "learning_rate": 4.0016666666666666e-05,
      "loss": 0.0053,
      "step": 17970
    },
    {
      "epoch": 0.9988888888888889,
      "grad_norm": 0.30119651556015015,
      "learning_rate": 4.001111111111111e-05,
      "loss": 0.0065,
      "step": 17980
    },
    {
      "epoch": 0.9994444444444445,
      "grad_norm": 0.2409135103225708,
      "learning_rate": 4.000555555555556e-05,
      "loss": 0.0045,
      "step": 17990
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.19175225496292114,
      "learning_rate": 4e-05,
      "loss": 0.0048,
      "step": 18000
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.004777882713824511,
      "eval_runtime": 117.5172,
      "eval_samples_per_second": 1532.507,
      "eval_steps_per_second": 38.318,
      "step": 18000
    },
    {
      "epoch": 1.0005555555555556,
      "grad_norm": 0.48081842064857483,
      "learning_rate": 3.999444444444445e-05,
      "loss": 0.0058,
      "step": 18010
    },
    {
      "epoch": 1.001111111111111,
      "grad_norm": 0.660835862159729,
      "learning_rate": 3.998888888888889e-05,
      "loss": 0.0055,
      "step": 18020
    },
    {
      "epoch": 1.0016666666666667,
      "grad_norm": 0.14352993667125702,
      "learning_rate": 3.9983333333333334e-05,
      "loss": 0.0052,
      "step": 18030
    },
    {
      "epoch": 1.0022222222222221,
      "grad_norm": 0.32098639011383057,
      "learning_rate": 3.997777777777778e-05,
      "loss": 0.0051,
      "step": 18040
    },
    {
      "epoch": 1.0027777777777778,
      "grad_norm": 0.061732593923807144,
      "learning_rate": 3.997222222222222e-05,
      "loss": 0.005,
      "step": 18050
    },
    {
      "epoch": 1.0033333333333334,
      "grad_norm": 0.24115508794784546,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.0053,
      "step": 18060
    },
    {
      "epoch": 1.0038888888888888,
      "grad_norm": 0.11292140930891037,
      "learning_rate": 3.996111111111111e-05,
      "loss": 0.0044,
      "step": 18070
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 0.6018632650375366,
      "learning_rate": 3.995555555555556e-05,
      "loss": 0.0055,
      "step": 18080
    },
    {
      "epoch": 1.005,
      "grad_norm": 0.42641347646713257,
      "learning_rate": 3.995e-05,
      "loss": 0.0046,
      "step": 18090
    },
    {
      "epoch": 1.0055555555555555,
      "grad_norm": 0.4808305501937866,
      "learning_rate": 3.9944444444444446e-05,
      "loss": 0.0031,
      "step": 18100
    },
    {
      "epoch": 1.0061111111111112,
      "grad_norm": 0.09084502607584,
      "learning_rate": 3.993888888888889e-05,
      "loss": 0.0042,
      "step": 18110
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.331133633852005,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0047,
      "step": 18120
    },
    {
      "epoch": 1.0072222222222222,
      "grad_norm": 0.18128760159015656,
      "learning_rate": 3.992777777777778e-05,
      "loss": 0.0042,
      "step": 18130
    },
    {
      "epoch": 1.0077777777777779,
      "grad_norm": 0.15176455676555634,
      "learning_rate": 3.992222222222222e-05,
      "loss": 0.0043,
      "step": 18140
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 0.3250541388988495,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.0077,
      "step": 18150
    },
    {
      "epoch": 1.008888888888889,
      "grad_norm": 0.7578873038291931,
      "learning_rate": 3.9911111111111114e-05,
      "loss": 0.0053,
      "step": 18160
    },
    {
      "epoch": 1.0094444444444444,
      "grad_norm": 0.2771041989326477,
      "learning_rate": 3.990555555555556e-05,
      "loss": 0.0056,
      "step": 18170
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.2425825148820877,
      "learning_rate": 3.99e-05,
      "loss": 0.0044,
      "step": 18180
    },
    {
      "epoch": 1.0105555555555557,
      "grad_norm": 0.3314111828804016,
      "learning_rate": 3.9894444444444444e-05,
      "loss": 0.0052,
      "step": 18190
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 0.26476410031318665,
      "learning_rate": 3.9888888888888895e-05,
      "loss": 0.0057,
      "step": 18200
    },
    {
      "epoch": 1.0116666666666667,
      "grad_norm": 0.15104073286056519,
      "learning_rate": 3.988333333333333e-05,
      "loss": 0.0056,
      "step": 18210
    },
    {
      "epoch": 1.0122222222222221,
      "grad_norm": 0.30064210295677185,
      "learning_rate": 3.987777777777778e-05,
      "loss": 0.0052,
      "step": 18220
    },
    {
      "epoch": 1.0127777777777778,
      "grad_norm": 0.0710889995098114,
      "learning_rate": 3.987222222222222e-05,
      "loss": 0.0047,
      "step": 18230
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.26282551884651184,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0056,
      "step": 18240
    },
    {
      "epoch": 1.0138888888888888,
      "grad_norm": 0.3614778220653534,
      "learning_rate": 3.986111111111111e-05,
      "loss": 0.0054,
      "step": 18250
    },
    {
      "epoch": 1.0144444444444445,
      "grad_norm": 0.0612841360270977,
      "learning_rate": 3.9855555555555556e-05,
      "loss": 0.0055,
      "step": 18260
    },
    {
      "epoch": 1.015,
      "grad_norm": 0.30131611227989197,
      "learning_rate": 3.9850000000000006e-05,
      "loss": 0.0043,
      "step": 18270
    },
    {
      "epoch": 1.0155555555555555,
      "grad_norm": 0.06051347777247429,
      "learning_rate": 3.984444444444444e-05,
      "loss": 0.0056,
      "step": 18280
    },
    {
      "epoch": 1.0161111111111112,
      "grad_norm": 0.6610403656959534,
      "learning_rate": 3.9838888888888894e-05,
      "loss": 0.0065,
      "step": 18290
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.5834961533546448,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0042,
      "step": 18300
    },
    {
      "epoch": 1.0172222222222222,
      "grad_norm": 0.14633698761463165,
      "learning_rate": 3.982777777777778e-05,
      "loss": 0.0072,
      "step": 18310
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 0.42127281427383423,
      "learning_rate": 3.9822222222222224e-05,
      "loss": 0.004,
      "step": 18320
    },
    {
      "epoch": 1.0183333333333333,
      "grad_norm": 0.013503658585250378,
      "learning_rate": 3.981666666666667e-05,
      "loss": 0.0041,
      "step": 18330
    },
    {
      "epoch": 1.018888888888889,
      "grad_norm": 0.28232434391975403,
      "learning_rate": 3.981111111111112e-05,
      "loss": 0.0068,
      "step": 18340
    },
    {
      "epoch": 1.0194444444444444,
      "grad_norm": 0.5417637228965759,
      "learning_rate": 3.9805555555555555e-05,
      "loss": 0.0063,
      "step": 18350
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.6016088724136353,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0056,
      "step": 18360
    },
    {
      "epoch": 1.0205555555555557,
      "grad_norm": 0.061093468219041824,
      "learning_rate": 3.979444444444444e-05,
      "loss": 0.0039,
      "step": 18370
    },
    {
      "epoch": 1.021111111111111,
      "grad_norm": 0.270546555519104,
      "learning_rate": 3.978888888888889e-05,
      "loss": 0.0051,
      "step": 18380
    },
    {
      "epoch": 1.0216666666666667,
      "grad_norm": 0.24016334116458893,
      "learning_rate": 3.9783333333333336e-05,
      "loss": 0.006,
      "step": 18390
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.10896965116262436,
      "learning_rate": 3.977777777777778e-05,
      "loss": 0.0038,
      "step": 18400
    },
    {
      "epoch": 1.0227777777777778,
      "grad_norm": 0.11898807436227798,
      "learning_rate": 3.977222222222222e-05,
      "loss": 0.0045,
      "step": 18410
    },
    {
      "epoch": 1.0233333333333334,
      "grad_norm": 0.4808427393436432,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0063,
      "step": 18420
    },
    {
      "epoch": 1.0238888888888888,
      "grad_norm": 0.00899367593228817,
      "learning_rate": 3.976111111111112e-05,
      "loss": 0.0046,
      "step": 18430
    },
    {
      "epoch": 1.0244444444444445,
      "grad_norm": 0.30134931206703186,
      "learning_rate": 3.9755555555555554e-05,
      "loss": 0.003,
      "step": 18440
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.414591521024704,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.0033,
      "step": 18450
    },
    {
      "epoch": 1.0255555555555556,
      "grad_norm": 0.18174993991851807,
      "learning_rate": 3.974444444444445e-05,
      "loss": 0.0038,
      "step": 18460
    },
    {
      "epoch": 1.0261111111111112,
      "grad_norm": 0.5763047337532043,
      "learning_rate": 3.973888888888889e-05,
      "loss": 0.0051,
      "step": 18470
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.18056009709835052,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0052,
      "step": 18480
    },
    {
      "epoch": 1.0272222222222223,
      "grad_norm": 0.29958173632621765,
      "learning_rate": 3.972777777777778e-05,
      "loss": 0.004,
      "step": 18490
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 0.1850530505180359,
      "learning_rate": 3.972222222222222e-05,
      "loss": 0.0055,
      "step": 18500
    },
    {
      "epoch": 1.0283333333333333,
      "grad_norm": 0.42897653579711914,
      "learning_rate": 3.9716666666666665e-05,
      "loss": 0.0054,
      "step": 18510
    },
    {
      "epoch": 1.028888888888889,
      "grad_norm": 0.061187852174043655,
      "learning_rate": 3.9711111111111116e-05,
      "loss": 0.0056,
      "step": 18520
    },
    {
      "epoch": 1.0294444444444444,
      "grad_norm": 0.06034853309392929,
      "learning_rate": 3.970555555555556e-05,
      "loss": 0.0055,
      "step": 18530
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.06196799874305725,
      "learning_rate": 3.97e-05,
      "loss": 0.0043,
      "step": 18540
    },
    {
      "epoch": 1.0305555555555554,
      "grad_norm": 0.18037240207195282,
      "learning_rate": 3.9694444444444446e-05,
      "loss": 0.0046,
      "step": 18550
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 0.26141053438186646,
      "learning_rate": 3.968888888888889e-05,
      "loss": 0.007,
      "step": 18560
    },
    {
      "epoch": 1.0316666666666667,
      "grad_norm": 0.21104192733764648,
      "learning_rate": 3.9683333333333333e-05,
      "loss": 0.0058,
      "step": 18570
    },
    {
      "epoch": 1.0322222222222222,
      "grad_norm": 0.09095769375562668,
      "learning_rate": 3.9677777777777784e-05,
      "loss": 0.0048,
      "step": 18580
    },
    {
      "epoch": 1.0327777777777778,
      "grad_norm": 0.3898884952068329,
      "learning_rate": 3.967222222222222e-05,
      "loss": 0.0055,
      "step": 18590
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.24155133962631226,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0051,
      "step": 18600
    },
    {
      "epoch": 1.0338888888888889,
      "grad_norm": 0.3604463040828705,
      "learning_rate": 3.9661111111111114e-05,
      "loss": 0.0051,
      "step": 18610
    },
    {
      "epoch": 1.0344444444444445,
      "grad_norm": 0.031863607466220856,
      "learning_rate": 3.965555555555556e-05,
      "loss": 0.0048,
      "step": 18620
    },
    {
      "epoch": 1.035,
      "grad_norm": 0.33017855882644653,
      "learning_rate": 3.965e-05,
      "loss": 0.0051,
      "step": 18630
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.44841647148132324,
      "learning_rate": 3.9644444444444445e-05,
      "loss": 0.0059,
      "step": 18640
    },
    {
      "epoch": 1.0361111111111112,
      "grad_norm": 0.060272883623838425,
      "learning_rate": 3.9638888888888895e-05,
      "loss": 0.0054,
      "step": 18650
    },
    {
      "epoch": 1.0366666666666666,
      "grad_norm": 0.36066362261772156,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0049,
      "step": 18660
    },
    {
      "epoch": 1.0372222222222223,
      "grad_norm": 0.45078909397125244,
      "learning_rate": 3.962777777777778e-05,
      "loss": 0.0051,
      "step": 18670
    },
    {
      "epoch": 1.0377777777777777,
      "grad_norm": 0.24040846526622772,
      "learning_rate": 3.962222222222222e-05,
      "loss": 0.0052,
      "step": 18680
    },
    {
      "epoch": 1.0383333333333333,
      "grad_norm": 0.24000650644302368,
      "learning_rate": 3.961666666666667e-05,
      "loss": 0.0032,
      "step": 18690
    },
    {
      "epoch": 1.038888888888889,
      "grad_norm": 0.06554988771677017,
      "learning_rate": 3.961111111111111e-05,
      "loss": 0.0041,
      "step": 18700
    },
    {
      "epoch": 1.0394444444444444,
      "grad_norm": 0.3009728193283081,
      "learning_rate": 3.960555555555556e-05,
      "loss": 0.0041,
      "step": 18710
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.12326065450906754,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0038,
      "step": 18720
    },
    {
      "epoch": 1.0405555555555555,
      "grad_norm": 0.2109214961528778,
      "learning_rate": 3.9594444444444444e-05,
      "loss": 0.0037,
      "step": 18730
    },
    {
      "epoch": 1.041111111111111,
      "grad_norm": 0.15094375610351562,
      "learning_rate": 3.9588888888888894e-05,
      "loss": 0.0038,
      "step": 18740
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.3611508905887604,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.0032,
      "step": 18750
    },
    {
      "epoch": 1.0422222222222222,
      "grad_norm": 0.0626743957400322,
      "learning_rate": 3.957777777777778e-05,
      "loss": 0.004,
      "step": 18760
    },
    {
      "epoch": 1.0427777777777778,
      "grad_norm": 0.15127909183502197,
      "learning_rate": 3.9572222222222225e-05,
      "loss": 0.0047,
      "step": 18770
    },
    {
      "epoch": 1.0433333333333334,
      "grad_norm": 0.2998211681842804,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.006,
      "step": 18780
    },
    {
      "epoch": 1.0438888888888889,
      "grad_norm": 0.1061386987566948,
      "learning_rate": 3.956111111111112e-05,
      "loss": 0.0055,
      "step": 18790
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 0.6017307043075562,
      "learning_rate": 3.9555555555555556e-05,
      "loss": 0.0064,
      "step": 18800
    },
    {
      "epoch": 1.045,
      "grad_norm": 0.26936060190200806,
      "learning_rate": 3.9550000000000006e-05,
      "loss": 0.0052,
      "step": 18810
    },
    {
      "epoch": 1.0455555555555556,
      "grad_norm": 0.2968766987323761,
      "learning_rate": 3.954444444444444e-05,
      "loss": 0.0047,
      "step": 18820
    },
    {
      "epoch": 1.0461111111111112,
      "grad_norm": 0.13296304643154144,
      "learning_rate": 3.953888888888889e-05,
      "loss": 0.0053,
      "step": 18830
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.12157583236694336,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0075,
      "step": 18840
    },
    {
      "epoch": 1.0472222222222223,
      "grad_norm": 0.06062764674425125,
      "learning_rate": 3.952777777777778e-05,
      "loss": 0.0053,
      "step": 18850
    },
    {
      "epoch": 1.0477777777777777,
      "grad_norm": 0.010073957964777946,
      "learning_rate": 3.9522222222222224e-05,
      "loss": 0.0044,
      "step": 18860
    },
    {
      "epoch": 1.0483333333333333,
      "grad_norm": 0.36200737953186035,
      "learning_rate": 3.951666666666667e-05,
      "loss": 0.0047,
      "step": 18870
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 0.33610448241233826,
      "learning_rate": 3.951111111111112e-05,
      "loss": 0.006,
      "step": 18880
    },
    {
      "epoch": 1.0494444444444444,
      "grad_norm": 0.5109349489212036,
      "learning_rate": 3.9505555555555554e-05,
      "loss": 0.0068,
      "step": 18890
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.27055954933166504,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.004,
      "step": 18900
    },
    {
      "epoch": 1.0505555555555555,
      "grad_norm": 0.16022330522537231,
      "learning_rate": 3.949444444444445e-05,
      "loss": 0.0058,
      "step": 18910
    },
    {
      "epoch": 1.051111111111111,
      "grad_norm": 0.12274443358182907,
      "learning_rate": 3.948888888888889e-05,
      "loss": 0.0043,
      "step": 18920
    },
    {
      "epoch": 1.0516666666666667,
      "grad_norm": 0.35650500655174255,
      "learning_rate": 3.9483333333333335e-05,
      "loss": 0.0046,
      "step": 18930
    },
    {
      "epoch": 1.0522222222222222,
      "grad_norm": 0.060464680194854736,
      "learning_rate": 3.947777777777778e-05,
      "loss": 0.0061,
      "step": 18940
    },
    {
      "epoch": 1.0527777777777778,
      "grad_norm": 0.2504206597805023,
      "learning_rate": 3.947222222222222e-05,
      "loss": 0.0051,
      "step": 18950
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.30135560035705566,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0026,
      "step": 18960
    },
    {
      "epoch": 1.0538888888888889,
      "grad_norm": 0.3011104464530945,
      "learning_rate": 3.9461111111111116e-05,
      "loss": 0.0056,
      "step": 18970
    },
    {
      "epoch": 1.0544444444444445,
      "grad_norm": 0.23151902854442596,
      "learning_rate": 3.945555555555556e-05,
      "loss": 0.0044,
      "step": 18980
    },
    {
      "epoch": 1.055,
      "grad_norm": 0.36035504937171936,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 0.0067,
      "step": 18990
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.1514512151479721,
      "learning_rate": 3.944444444444445e-05,
      "loss": 0.0034,
      "step": 19000
    },
    {
      "epoch": 1.056111111111111,
      "grad_norm": 0.3618152439594269,
      "learning_rate": 3.943888888888889e-05,
      "loss": 0.0042,
      "step": 19010
    },
    {
      "epoch": 1.0566666666666666,
      "grad_norm": 0.12056053429841995,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.0058,
      "step": 19020
    },
    {
      "epoch": 1.0572222222222223,
      "grad_norm": 0.037526924163103104,
      "learning_rate": 3.942777777777778e-05,
      "loss": 0.0039,
      "step": 19030
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 0.1818205714225769,
      "learning_rate": 3.942222222222222e-05,
      "loss": 0.0045,
      "step": 19040
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 0.33028557896614075,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.006,
      "step": 19050
    },
    {
      "epoch": 1.058888888888889,
      "grad_norm": 0.4819580614566803,
      "learning_rate": 3.9411111111111115e-05,
      "loss": 0.0039,
      "step": 19060
    },
    {
      "epoch": 1.0594444444444444,
      "grad_norm": 0.1217522919178009,
      "learning_rate": 3.940555555555556e-05,
      "loss": 0.0059,
      "step": 19070
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.12130983918905258,
      "learning_rate": 3.94e-05,
      "loss": 0.0035,
      "step": 19080
    },
    {
      "epoch": 1.0605555555555555,
      "grad_norm": 0.30030500888824463,
      "learning_rate": 3.9394444444444446e-05,
      "loss": 0.004,
      "step": 19090
    },
    {
      "epoch": 1.0611111111111111,
      "grad_norm": 0.09038210660219193,
      "learning_rate": 3.938888888888889e-05,
      "loss": 0.004,
      "step": 19100
    },
    {
      "epoch": 1.0616666666666668,
      "grad_norm": 0.07933478057384491,
      "learning_rate": 3.938333333333333e-05,
      "loss": 0.0048,
      "step": 19110
    },
    {
      "epoch": 1.0622222222222222,
      "grad_norm": 0.032297175377607346,
      "learning_rate": 3.937777777777778e-05,
      "loss": 0.0034,
      "step": 19120
    },
    {
      "epoch": 1.0627777777777778,
      "grad_norm": 0.06132727861404419,
      "learning_rate": 3.937222222222222e-05,
      "loss": 0.0057,
      "step": 19130
    },
    {
      "epoch": 1.0633333333333332,
      "grad_norm": 0.5093933343887329,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.0055,
      "step": 19140
    },
    {
      "epoch": 1.0638888888888889,
      "grad_norm": 0.42804327607154846,
      "learning_rate": 3.9361111111111114e-05,
      "loss": 0.0048,
      "step": 19150
    },
    {
      "epoch": 1.0644444444444445,
      "grad_norm": 0.4403827488422394,
      "learning_rate": 3.935555555555556e-05,
      "loss": 0.0045,
      "step": 19160
    },
    {
      "epoch": 1.065,
      "grad_norm": 0.24080418050289154,
      "learning_rate": 3.935e-05,
      "loss": 0.0059,
      "step": 19170
    },
    {
      "epoch": 1.0655555555555556,
      "grad_norm": 0.09110888093709946,
      "learning_rate": 3.9344444444444445e-05,
      "loss": 0.0056,
      "step": 19180
    },
    {
      "epoch": 1.066111111111111,
      "grad_norm": 0.5703209638595581,
      "learning_rate": 3.9338888888888895e-05,
      "loss": 0.0054,
      "step": 19190
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.2729495167732239,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0048,
      "step": 19200
    },
    {
      "epoch": 1.0672222222222223,
      "grad_norm": 0.48048609495162964,
      "learning_rate": 3.932777777777778e-05,
      "loss": 0.0048,
      "step": 19210
    },
    {
      "epoch": 1.0677777777777777,
      "grad_norm": 0.6607657670974731,
      "learning_rate": 3.932222222222222e-05,
      "loss": 0.0056,
      "step": 19220
    },
    {
      "epoch": 1.0683333333333334,
      "grad_norm": 0.15446633100509644,
      "learning_rate": 3.931666666666667e-05,
      "loss": 0.0059,
      "step": 19230
    },
    {
      "epoch": 1.068888888888889,
      "grad_norm": 0.322243869304657,
      "learning_rate": 3.931111111111111e-05,
      "loss": 0.0045,
      "step": 19240
    },
    {
      "epoch": 1.0694444444444444,
      "grad_norm": 0.0278228260576725,
      "learning_rate": 3.9305555555555556e-05,
      "loss": 0.0047,
      "step": 19250
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.5710513591766357,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.0041,
      "step": 19260
    },
    {
      "epoch": 1.0705555555555555,
      "grad_norm": 0.3607073128223419,
      "learning_rate": 3.929444444444444e-05,
      "loss": 0.0046,
      "step": 19270
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.2723856270313263,
      "learning_rate": 3.9288888888888894e-05,
      "loss": 0.0054,
      "step": 19280
    },
    {
      "epoch": 1.0716666666666668,
      "grad_norm": 0.09078159183263779,
      "learning_rate": 3.928333333333333e-05,
      "loss": 0.0034,
      "step": 19290
    },
    {
      "epoch": 1.0722222222222222,
      "grad_norm": 0.060860756784677505,
      "learning_rate": 3.927777777777778e-05,
      "loss": 0.0055,
      "step": 19300
    },
    {
      "epoch": 1.0727777777777778,
      "grad_norm": 0.14435112476348877,
      "learning_rate": 3.9272222222222224e-05,
      "loss": 0.0042,
      "step": 19310
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.33482450246810913,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.007,
      "step": 19320
    },
    {
      "epoch": 1.073888888888889,
      "grad_norm": 0.5413923859596252,
      "learning_rate": 3.926111111111112e-05,
      "loss": 0.0037,
      "step": 19330
    },
    {
      "epoch": 1.0744444444444445,
      "grad_norm": 0.12186550348997116,
      "learning_rate": 3.9255555555555555e-05,
      "loss": 0.0046,
      "step": 19340
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.03203657269477844,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.005,
      "step": 19350
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 0.6009247899055481,
      "learning_rate": 3.924444444444444e-05,
      "loss": 0.004,
      "step": 19360
    },
    {
      "epoch": 1.076111111111111,
      "grad_norm": 0.03291264548897743,
      "learning_rate": 3.923888888888889e-05,
      "loss": 0.005,
      "step": 19370
    },
    {
      "epoch": 1.0766666666666667,
      "grad_norm": 0.5411102771759033,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0053,
      "step": 19380
    },
    {
      "epoch": 1.0772222222222223,
      "grad_norm": 0.2694181501865387,
      "learning_rate": 3.922777777777778e-05,
      "loss": 0.0043,
      "step": 19390
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 0.10491561144590378,
      "learning_rate": 3.922222222222223e-05,
      "loss": 0.005,
      "step": 19400
    },
    {
      "epoch": 1.0783333333333334,
      "grad_norm": 0.0600152425467968,
      "learning_rate": 3.921666666666667e-05,
      "loss": 0.0053,
      "step": 19410
    },
    {
      "epoch": 1.0788888888888888,
      "grad_norm": 0.18017002940177917,
      "learning_rate": 3.921111111111112e-05,
      "loss": 0.0041,
      "step": 19420
    },
    {
      "epoch": 1.0794444444444444,
      "grad_norm": 0.1810840517282486,
      "learning_rate": 3.9205555555555554e-05,
      "loss": 0.0042,
      "step": 19430
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.3008110225200653,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0041,
      "step": 19440
    },
    {
      "epoch": 1.0805555555555555,
      "grad_norm": 0.061067596077919006,
      "learning_rate": 3.919444444444445e-05,
      "loss": 0.0051,
      "step": 19450
    },
    {
      "epoch": 1.0811111111111111,
      "grad_norm": 0.10283610224723816,
      "learning_rate": 3.918888888888889e-05,
      "loss": 0.007,
      "step": 19460
    },
    {
      "epoch": 1.0816666666666666,
      "grad_norm": 0.3257002830505371,
      "learning_rate": 3.9183333333333335e-05,
      "loss": 0.0047,
      "step": 19470
    },
    {
      "epoch": 1.0822222222222222,
      "grad_norm": 0.20757615566253662,
      "learning_rate": 3.917777777777778e-05,
      "loss": 0.0067,
      "step": 19480
    },
    {
      "epoch": 1.0827777777777778,
      "grad_norm": 0.4216853976249695,
      "learning_rate": 3.917222222222223e-05,
      "loss": 0.0055,
      "step": 19490
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.34378573298454285,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0047,
      "step": 19500
    },
    {
      "epoch": 1.083888888888889,
      "grad_norm": 0.30222347378730774,
      "learning_rate": 3.9161111111111116e-05,
      "loss": 0.0048,
      "step": 19510
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 0.3308354914188385,
      "learning_rate": 3.915555555555556e-05,
      "loss": 0.0051,
      "step": 19520
    },
    {
      "epoch": 1.085,
      "grad_norm": 0.06416256725788116,
      "learning_rate": 3.915e-05,
      "loss": 0.0044,
      "step": 19530
    },
    {
      "epoch": 1.0855555555555556,
      "grad_norm": 0.20386135578155518,
      "learning_rate": 3.9144444444444446e-05,
      "loss": 0.0052,
      "step": 19540
    },
    {
      "epoch": 1.086111111111111,
      "grad_norm": 0.3036452531814575,
      "learning_rate": 3.913888888888889e-05,
      "loss": 0.0053,
      "step": 19550
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.06125912442803383,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0051,
      "step": 19560
    },
    {
      "epoch": 1.0872222222222223,
      "grad_norm": 0.2998027205467224,
      "learning_rate": 3.912777777777778e-05,
      "loss": 0.0056,
      "step": 19570
    },
    {
      "epoch": 1.0877777777777777,
      "grad_norm": 0.39040088653564453,
      "learning_rate": 3.912222222222223e-05,
      "loss": 0.0042,
      "step": 19580
    },
    {
      "epoch": 1.0883333333333334,
      "grad_norm": 0.48192012310028076,
      "learning_rate": 3.911666666666667e-05,
      "loss": 0.005,
      "step": 19590
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.36061903834342957,
      "learning_rate": 3.9111111111111115e-05,
      "loss": 0.0066,
      "step": 19600
    },
    {
      "epoch": 1.0894444444444444,
      "grad_norm": 0.4552778899669647,
      "learning_rate": 3.910555555555556e-05,
      "loss": 0.0061,
      "step": 19610
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.40209242701530457,
      "learning_rate": 3.91e-05,
      "loss": 0.0041,
      "step": 19620
    },
    {
      "epoch": 1.0905555555555555,
      "grad_norm": 0.4209226667881012,
      "learning_rate": 3.9094444444444445e-05,
      "loss": 0.0041,
      "step": 19630
    },
    {
      "epoch": 1.0911111111111111,
      "grad_norm": 0.3915750980377197,
      "learning_rate": 3.908888888888889e-05,
      "loss": 0.0055,
      "step": 19640
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 0.15017366409301758,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.004,
      "step": 19650
    },
    {
      "epoch": 1.0922222222222222,
      "grad_norm": 0.29297181963920593,
      "learning_rate": 3.907777777777778e-05,
      "loss": 0.0042,
      "step": 19660
    },
    {
      "epoch": 1.0927777777777778,
      "grad_norm": 0.09024260193109512,
      "learning_rate": 3.9072222222222226e-05,
      "loss": 0.0042,
      "step": 19670
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.753445029258728,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0046,
      "step": 19680
    },
    {
      "epoch": 1.093888888888889,
      "grad_norm": 0.23215645551681519,
      "learning_rate": 3.9061111111111113e-05,
      "loss": 0.0047,
      "step": 19690
    },
    {
      "epoch": 1.0944444444444446,
      "grad_norm": 0.18108707666397095,
      "learning_rate": 3.905555555555556e-05,
      "loss": 0.0042,
      "step": 19700
    },
    {
      "epoch": 1.095,
      "grad_norm": 0.03221028670668602,
      "learning_rate": 3.905e-05,
      "loss": 0.0046,
      "step": 19710
    },
    {
      "epoch": 1.0955555555555556,
      "grad_norm": 0.4506021738052368,
      "learning_rate": 3.9044444444444444e-05,
      "loss": 0.004,
      "step": 19720
    },
    {
      "epoch": 1.096111111111111,
      "grad_norm": 0.29912135004997253,
      "learning_rate": 3.9038888888888894e-05,
      "loss": 0.0061,
      "step": 19730
    },
    {
      "epoch": 1.0966666666666667,
      "grad_norm": 0.2211369276046753,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0045,
      "step": 19740
    },
    {
      "epoch": 1.0972222222222223,
      "grad_norm": 0.3618597686290741,
      "learning_rate": 3.902777777777778e-05,
      "loss": 0.0057,
      "step": 19750
    },
    {
      "epoch": 1.0977777777777777,
      "grad_norm": 0.28295060992240906,
      "learning_rate": 3.9022222222222225e-05,
      "loss": 0.0066,
      "step": 19760
    },
    {
      "epoch": 1.0983333333333334,
      "grad_norm": 0.0909208208322525,
      "learning_rate": 3.901666666666667e-05,
      "loss": 0.0051,
      "step": 19770
    },
    {
      "epoch": 1.0988888888888888,
      "grad_norm": 0.12787657976150513,
      "learning_rate": 3.901111111111111e-05,
      "loss": 0.0043,
      "step": 19780
    },
    {
      "epoch": 1.0994444444444444,
      "grad_norm": 0.16151900589466095,
      "learning_rate": 3.9005555555555556e-05,
      "loss": 0.004,
      "step": 19790
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.11088186502456665,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0055,
      "step": 19800
    },
    {
      "epoch": 1.1005555555555555,
      "grad_norm": 0.5478246212005615,
      "learning_rate": 3.899444444444444e-05,
      "loss": 0.0056,
      "step": 19810
    },
    {
      "epoch": 1.1011111111111112,
      "grad_norm": 0.7224142551422119,
      "learning_rate": 3.898888888888889e-05,
      "loss": 0.0041,
      "step": 19820
    },
    {
      "epoch": 1.1016666666666666,
      "grad_norm": 0.12084544450044632,
      "learning_rate": 3.898333333333333e-05,
      "loss": 0.0049,
      "step": 19830
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 0.010224553756415844,
      "learning_rate": 3.897777777777778e-05,
      "loss": 0.0031,
      "step": 19840
    },
    {
      "epoch": 1.1027777777777779,
      "grad_norm": 0.011610954999923706,
      "learning_rate": 3.8972222222222224e-05,
      "loss": 0.0044,
      "step": 19850
    },
    {
      "epoch": 1.1033333333333333,
      "grad_norm": 0.308162122964859,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.0054,
      "step": 19860
    },
    {
      "epoch": 1.103888888888889,
      "grad_norm": 0.2116633653640747,
      "learning_rate": 3.896111111111112e-05,
      "loss": 0.0039,
      "step": 19870
    },
    {
      "epoch": 1.1044444444444443,
      "grad_norm": 0.09090577065944672,
      "learning_rate": 3.8955555555555555e-05,
      "loss": 0.0047,
      "step": 19880
    },
    {
      "epoch": 1.105,
      "grad_norm": 0.3009064495563507,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 0.005,
      "step": 19890
    },
    {
      "epoch": 1.1055555555555556,
      "grad_norm": 0.06187892705202103,
      "learning_rate": 3.894444444444444e-05,
      "loss": 0.0059,
      "step": 19900
    },
    {
      "epoch": 1.106111111111111,
      "grad_norm": 0.3387916684150696,
      "learning_rate": 3.893888888888889e-05,
      "loss": 0.0054,
      "step": 19910
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.18100033700466156,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0051,
      "step": 19920
    },
    {
      "epoch": 1.107222222222222,
      "grad_norm": 0.31944161653518677,
      "learning_rate": 3.892777777777778e-05,
      "loss": 0.0061,
      "step": 19930
    },
    {
      "epoch": 1.1077777777777778,
      "grad_norm": 0.1230137050151825,
      "learning_rate": 3.892222222222223e-05,
      "loss": 0.0047,
      "step": 19940
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.5144640803337097,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.005,
      "step": 19950
    },
    {
      "epoch": 1.1088888888888888,
      "grad_norm": 0.37027743458747864,
      "learning_rate": 3.8911111111111117e-05,
      "loss": 0.0061,
      "step": 19960
    },
    {
      "epoch": 1.1094444444444445,
      "grad_norm": 0.25076594948768616,
      "learning_rate": 3.890555555555555e-05,
      "loss": 0.0058,
      "step": 19970
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.015476204454898834,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.0048,
      "step": 19980
    },
    {
      "epoch": 1.1105555555555555,
      "grad_norm": 0.3615224063396454,
      "learning_rate": 3.889444444444445e-05,
      "loss": 0.0054,
      "step": 19990
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.09128483384847641,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.0045,
      "step": 20000
    },
    {
      "epoch": 1.1116666666666666,
      "grad_norm": 0.42206257581710815,
      "learning_rate": 3.8883333333333334e-05,
      "loss": 0.0061,
      "step": 20010
    },
    {
      "epoch": 1.1122222222222222,
      "grad_norm": 0.5414348244667053,
      "learning_rate": 3.887777777777778e-05,
      "loss": 0.0039,
      "step": 20020
    },
    {
      "epoch": 1.1127777777777779,
      "grad_norm": 0.36132147908210754,
      "learning_rate": 3.887222222222223e-05,
      "loss": 0.0045,
      "step": 20030
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.15104827284812927,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0051,
      "step": 20040
    },
    {
      "epoch": 1.113888888888889,
      "grad_norm": 0.42415645718574524,
      "learning_rate": 3.8861111111111115e-05,
      "loss": 0.0039,
      "step": 20050
    },
    {
      "epoch": 1.1144444444444443,
      "grad_norm": 0.5729494690895081,
      "learning_rate": 3.885555555555556e-05,
      "loss": 0.0066,
      "step": 20060
    },
    {
      "epoch": 1.115,
      "grad_norm": 0.1843356341123581,
      "learning_rate": 3.885e-05,
      "loss": 0.0047,
      "step": 20070
    },
    {
      "epoch": 1.1155555555555556,
      "grad_norm": 0.18355776369571686,
      "learning_rate": 3.8844444444444446e-05,
      "loss": 0.0047,
      "step": 20080
    },
    {
      "epoch": 1.116111111111111,
      "grad_norm": 0.09095725417137146,
      "learning_rate": 3.883888888888889e-05,
      "loss": 0.0047,
      "step": 20090
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.18087805807590485,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0053,
      "step": 20100
    },
    {
      "epoch": 1.1172222222222223,
      "grad_norm": 0.33018213510513306,
      "learning_rate": 3.882777777777778e-05,
      "loss": 0.0068,
      "step": 20110
    },
    {
      "epoch": 1.1177777777777778,
      "grad_norm": 0.12094078212976456,
      "learning_rate": 3.882222222222223e-05,
      "loss": 0.0061,
      "step": 20120
    },
    {
      "epoch": 1.1183333333333334,
      "grad_norm": 0.541318953037262,
      "learning_rate": 3.881666666666667e-05,
      "loss": 0.0056,
      "step": 20130
    },
    {
      "epoch": 1.1188888888888888,
      "grad_norm": 0.06278622895479202,
      "learning_rate": 3.8811111111111114e-05,
      "loss": 0.0051,
      "step": 20140
    },
    {
      "epoch": 1.1194444444444445,
      "grad_norm": 0.4261177182197571,
      "learning_rate": 3.880555555555556e-05,
      "loss": 0.0035,
      "step": 20150
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.06112385913729668,
      "learning_rate": 3.88e-05,
      "loss": 0.004,
      "step": 20160
    },
    {
      "epoch": 1.1205555555555555,
      "grad_norm": 0.3014173209667206,
      "learning_rate": 3.8794444444444445e-05,
      "loss": 0.0082,
      "step": 20170
    },
    {
      "epoch": 1.1211111111111112,
      "grad_norm": 0.39199700951576233,
      "learning_rate": 3.878888888888889e-05,
      "loss": 0.0059,
      "step": 20180
    },
    {
      "epoch": 1.1216666666666666,
      "grad_norm": 0.08908723294734955,
      "learning_rate": 3.878333333333333e-05,
      "loss": 0.0052,
      "step": 20190
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 0.4071006178855896,
      "learning_rate": 3.877777777777778e-05,
      "loss": 0.0048,
      "step": 20200
    },
    {
      "epoch": 1.1227777777777779,
      "grad_norm": 0.3975831866264343,
      "learning_rate": 3.8772222222222226e-05,
      "loss": 0.0053,
      "step": 20210
    },
    {
      "epoch": 1.1233333333333333,
      "grad_norm": 0.012409033253788948,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.003,
      "step": 20220
    },
    {
      "epoch": 1.123888888888889,
      "grad_norm": 0.39125770330429077,
      "learning_rate": 3.876111111111111e-05,
      "loss": 0.0041,
      "step": 20230
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 0.17995329201221466,
      "learning_rate": 3.8755555555555556e-05,
      "loss": 0.0045,
      "step": 20240
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.4519440829753876,
      "learning_rate": 3.875e-05,
      "loss": 0.004,
      "step": 20250
    },
    {
      "epoch": 1.1255555555555556,
      "grad_norm": 0.3615517020225525,
      "learning_rate": 3.8744444444444444e-05,
      "loss": 0.0036,
      "step": 20260
    },
    {
      "epoch": 1.126111111111111,
      "grad_norm": 0.06285469979047775,
      "learning_rate": 3.8738888888888894e-05,
      "loss": 0.0045,
      "step": 20270
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.4512444734573364,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.006,
      "step": 20280
    },
    {
      "epoch": 1.1272222222222221,
      "grad_norm": 0.010260450653731823,
      "learning_rate": 3.872777777777778e-05,
      "loss": 0.0054,
      "step": 20290
    },
    {
      "epoch": 1.1277777777777778,
      "grad_norm": 0.06146400421857834,
      "learning_rate": 3.8722222222222225e-05,
      "loss": 0.0035,
      "step": 20300
    },
    {
      "epoch": 1.1283333333333334,
      "grad_norm": 0.0908459946513176,
      "learning_rate": 3.871666666666667e-05,
      "loss": 0.0065,
      "step": 20310
    },
    {
      "epoch": 1.1288888888888888,
      "grad_norm": 0.062053218483924866,
      "learning_rate": 3.871111111111111e-05,
      "loss": 0.0054,
      "step": 20320
    },
    {
      "epoch": 1.1294444444444445,
      "grad_norm": 0.212167426943779,
      "learning_rate": 3.8705555555555555e-05,
      "loss": 0.0049,
      "step": 20330
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.09134001284837723,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0041,
      "step": 20340
    },
    {
      "epoch": 1.1305555555555555,
      "grad_norm": 0.09902326762676239,
      "learning_rate": 3.869444444444444e-05,
      "loss": 0.0042,
      "step": 20350
    },
    {
      "epoch": 1.1311111111111112,
      "grad_norm": 0.2994365692138672,
      "learning_rate": 3.868888888888889e-05,
      "loss": 0.0044,
      "step": 20360
    },
    {
      "epoch": 1.1316666666666666,
      "grad_norm": 0.481013685464859,
      "learning_rate": 3.868333333333333e-05,
      "loss": 0.0052,
      "step": 20370
    },
    {
      "epoch": 1.1322222222222222,
      "grad_norm": 0.22480495274066925,
      "learning_rate": 3.867777777777778e-05,
      "loss": 0.0048,
      "step": 20380
    },
    {
      "epoch": 1.1327777777777777,
      "grad_norm": 0.32078415155410767,
      "learning_rate": 3.867222222222222e-05,
      "loss": 0.0036,
      "step": 20390
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.24310851097106934,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0051,
      "step": 20400
    },
    {
      "epoch": 1.133888888888889,
      "grad_norm": 0.1802310049533844,
      "learning_rate": 3.866111111111112e-05,
      "loss": 0.0035,
      "step": 20410
    },
    {
      "epoch": 1.1344444444444444,
      "grad_norm": 0.30094465613365173,
      "learning_rate": 3.8655555555555554e-05,
      "loss": 0.005,
      "step": 20420
    },
    {
      "epoch": 1.135,
      "grad_norm": 0.6318730115890503,
      "learning_rate": 3.8650000000000004e-05,
      "loss": 0.0046,
      "step": 20430
    },
    {
      "epoch": 1.1355555555555557,
      "grad_norm": 0.18085207045078278,
      "learning_rate": 3.864444444444444e-05,
      "loss": 0.0056,
      "step": 20440
    },
    {
      "epoch": 1.136111111111111,
      "grad_norm": 0.0742347314953804,
      "learning_rate": 3.863888888888889e-05,
      "loss": 0.004,
      "step": 20450
    },
    {
      "epoch": 1.1366666666666667,
      "grad_norm": 0.4507260322570801,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.0059,
      "step": 20460
    },
    {
      "epoch": 1.1372222222222221,
      "grad_norm": 0.22622017562389374,
      "learning_rate": 3.862777777777778e-05,
      "loss": 0.0059,
      "step": 20470
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 0.5139308571815491,
      "learning_rate": 3.862222222222223e-05,
      "loss": 0.0046,
      "step": 20480
    },
    {
      "epoch": 1.1383333333333334,
      "grad_norm": 0.008374585770070553,
      "learning_rate": 3.8616666666666666e-05,
      "loss": 0.0047,
      "step": 20490
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 0.3317672908306122,
      "learning_rate": 3.8611111111111116e-05,
      "loss": 0.0038,
      "step": 20500
    },
    {
      "epoch": 1.1394444444444445,
      "grad_norm": 0.21053874492645264,
      "learning_rate": 3.860555555555555e-05,
      "loss": 0.0045,
      "step": 20510
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.061767831444740295,
      "learning_rate": 3.86e-05,
      "loss": 0.0061,
      "step": 20520
    },
    {
      "epoch": 1.1405555555555555,
      "grad_norm": 0.240622416138649,
      "learning_rate": 3.859444444444445e-05,
      "loss": 0.0049,
      "step": 20530
    },
    {
      "epoch": 1.1411111111111112,
      "grad_norm": 0.18433745205402374,
      "learning_rate": 3.858888888888889e-05,
      "loss": 0.0062,
      "step": 20540
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 0.3716357350349426,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.0027,
      "step": 20550
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 0.27089422941207886,
      "learning_rate": 3.857777777777778e-05,
      "loss": 0.0071,
      "step": 20560
    },
    {
      "epoch": 1.142777777777778,
      "grad_norm": 0.6422460675239563,
      "learning_rate": 3.857222222222223e-05,
      "loss": 0.0029,
      "step": 20570
    },
    {
      "epoch": 1.1433333333333333,
      "grad_norm": 0.18494124710559845,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.0046,
      "step": 20580
    },
    {
      "epoch": 1.143888888888889,
      "grad_norm": 0.21058349311351776,
      "learning_rate": 3.8561111111111115e-05,
      "loss": 0.0046,
      "step": 20590
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 0.009089874103665352,
      "learning_rate": 3.855555555555556e-05,
      "loss": 0.0055,
      "step": 20600
    },
    {
      "epoch": 1.145,
      "grad_norm": 0.18254373967647552,
      "learning_rate": 3.855e-05,
      "loss": 0.0055,
      "step": 20610
    },
    {
      "epoch": 1.1455555555555557,
      "grad_norm": 0.032169781625270844,
      "learning_rate": 3.8544444444444445e-05,
      "loss": 0.0052,
      "step": 20620
    },
    {
      "epoch": 1.146111111111111,
      "grad_norm": 0.18084532022476196,
      "learning_rate": 3.853888888888889e-05,
      "loss": 0.0047,
      "step": 20630
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.42132601141929626,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0032,
      "step": 20640
    },
    {
      "epoch": 1.1472222222222221,
      "grad_norm": 0.1805092841386795,
      "learning_rate": 3.8527777777777776e-05,
      "loss": 0.0045,
      "step": 20650
    },
    {
      "epoch": 1.1477777777777778,
      "grad_norm": 0.33227473497390747,
      "learning_rate": 3.8522222222222226e-05,
      "loss": 0.0059,
      "step": 20660
    },
    {
      "epoch": 1.1483333333333334,
      "grad_norm": 0.06185842677950859,
      "learning_rate": 3.851666666666667e-05,
      "loss": 0.0046,
      "step": 20670
    },
    {
      "epoch": 1.1488888888888888,
      "grad_norm": 0.18077170848846436,
      "learning_rate": 3.8511111111111114e-05,
      "loss": 0.0036,
      "step": 20680
    },
    {
      "epoch": 1.1494444444444445,
      "grad_norm": 0.09131986647844315,
      "learning_rate": 3.850555555555556e-05,
      "loss": 0.0042,
      "step": 20690
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.3603671193122864,
      "learning_rate": 3.85e-05,
      "loss": 0.0033,
      "step": 20700
    },
    {
      "epoch": 1.1505555555555556,
      "grad_norm": 0.4221547842025757,
      "learning_rate": 3.8494444444444444e-05,
      "loss": 0.0043,
      "step": 20710
    },
    {
      "epoch": 1.1511111111111112,
      "grad_norm": 0.33288174867630005,
      "learning_rate": 3.848888888888889e-05,
      "loss": 0.0052,
      "step": 20720
    },
    {
      "epoch": 1.1516666666666666,
      "grad_norm": 0.3010246455669403,
      "learning_rate": 3.848333333333334e-05,
      "loss": 0.0048,
      "step": 20730
    },
    {
      "epoch": 1.1522222222222223,
      "grad_norm": 0.08303485810756683,
      "learning_rate": 3.847777777777778e-05,
      "loss": 0.0053,
      "step": 20740
    },
    {
      "epoch": 1.1527777777777777,
      "grad_norm": 0.5421258807182312,
      "learning_rate": 3.8472222222222225e-05,
      "loss": 0.0066,
      "step": 20750
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.18136504292488098,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0055,
      "step": 20760
    },
    {
      "epoch": 1.153888888888889,
      "grad_norm": 0.33113640546798706,
      "learning_rate": 3.846111111111111e-05,
      "loss": 0.0043,
      "step": 20770
    },
    {
      "epoch": 1.1544444444444444,
      "grad_norm": 0.1805943101644516,
      "learning_rate": 3.8455555555555556e-05,
      "loss": 0.0045,
      "step": 20780
    },
    {
      "epoch": 1.155,
      "grad_norm": 0.39189612865448,
      "learning_rate": 3.845e-05,
      "loss": 0.0054,
      "step": 20790
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.3014070689678192,
      "learning_rate": 3.844444444444444e-05,
      "loss": 0.0043,
      "step": 20800
    },
    {
      "epoch": 1.156111111111111,
      "grad_norm": 0.29921188950538635,
      "learning_rate": 3.843888888888889e-05,
      "loss": 0.0052,
      "step": 20810
    },
    {
      "epoch": 1.1566666666666667,
      "grad_norm": 0.12063391506671906,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0042,
      "step": 20820
    },
    {
      "epoch": 1.1572222222222222,
      "grad_norm": 0.03227732703089714,
      "learning_rate": 3.842777777777778e-05,
      "loss": 0.0054,
      "step": 20830
    },
    {
      "epoch": 1.1577777777777778,
      "grad_norm": 0.21400462090969086,
      "learning_rate": 3.8422222222222224e-05,
      "loss": 0.0046,
      "step": 20840
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.12075874954462051,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.0036,
      "step": 20850
    },
    {
      "epoch": 1.1588888888888889,
      "grad_norm": 0.481147438287735,
      "learning_rate": 3.841111111111111e-05,
      "loss": 0.0038,
      "step": 20860
    },
    {
      "epoch": 1.1594444444444445,
      "grad_norm": 0.1495511382818222,
      "learning_rate": 3.8405555555555555e-05,
      "loss": 0.0059,
      "step": 20870
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.5577904582023621,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0048,
      "step": 20880
    },
    {
      "epoch": 1.1605555555555556,
      "grad_norm": 0.70486980676651,
      "learning_rate": 3.839444444444444e-05,
      "loss": 0.0041,
      "step": 20890
    },
    {
      "epoch": 1.1611111111111112,
      "grad_norm": 0.5418033003807068,
      "learning_rate": 3.838888888888889e-05,
      "loss": 0.004,
      "step": 20900
    },
    {
      "epoch": 1.1616666666666666,
      "grad_norm": 0.2707100510597229,
      "learning_rate": 3.8383333333333336e-05,
      "loss": 0.004,
      "step": 20910
    },
    {
      "epoch": 1.1622222222222223,
      "grad_norm": 0.4268655776977539,
      "learning_rate": 3.837777777777778e-05,
      "loss": 0.0047,
      "step": 20920
    },
    {
      "epoch": 1.1627777777777777,
      "grad_norm": 0.42529547214508057,
      "learning_rate": 3.837222222222222e-05,
      "loss": 0.0051,
      "step": 20930
    },
    {
      "epoch": 1.1633333333333333,
      "grad_norm": 0.33118030428886414,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.0044,
      "step": 20940
    },
    {
      "epoch": 1.163888888888889,
      "grad_norm": 0.03307431563735008,
      "learning_rate": 3.836111111111112e-05,
      "loss": 0.0039,
      "step": 20950
    },
    {
      "epoch": 1.1644444444444444,
      "grad_norm": 0.09400990605354309,
      "learning_rate": 3.8355555555555553e-05,
      "loss": 0.0045,
      "step": 20960
    },
    {
      "epoch": 1.165,
      "grad_norm": 0.21125365793704987,
      "learning_rate": 3.8350000000000004e-05,
      "loss": 0.0051,
      "step": 20970
    },
    {
      "epoch": 1.1655555555555557,
      "grad_norm": 0.06115325912833214,
      "learning_rate": 3.834444444444444e-05,
      "loss": 0.0048,
      "step": 20980
    },
    {
      "epoch": 1.166111111111111,
      "grad_norm": 0.0914311408996582,
      "learning_rate": 3.833888888888889e-05,
      "loss": 0.0043,
      "step": 20990
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.06232502683997154,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0039,
      "step": 21000
    },
    {
      "epoch": 1.1672222222222222,
      "grad_norm": 0.3592633605003357,
      "learning_rate": 3.832777777777778e-05,
      "loss": 0.0042,
      "step": 21010
    },
    {
      "epoch": 1.1677777777777778,
      "grad_norm": 0.11855261027812958,
      "learning_rate": 3.832222222222223e-05,
      "loss": 0.0047,
      "step": 21020
    },
    {
      "epoch": 1.1683333333333334,
      "grad_norm": 0.42612937092781067,
      "learning_rate": 3.8316666666666665e-05,
      "loss": 0.0047,
      "step": 21030
    },
    {
      "epoch": 1.1688888888888889,
      "grad_norm": 0.3914397358894348,
      "learning_rate": 3.8311111111111115e-05,
      "loss": 0.0063,
      "step": 21040
    },
    {
      "epoch": 1.1694444444444445,
      "grad_norm": 0.033996254205703735,
      "learning_rate": 3.830555555555555e-05,
      "loss": 0.006,
      "step": 21050
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.2106005996465683,
      "learning_rate": 3.83e-05,
      "loss": 0.0051,
      "step": 21060
    },
    {
      "epoch": 1.1705555555555556,
      "grad_norm": 0.4958650767803192,
      "learning_rate": 3.8294444444444446e-05,
      "loss": 0.0034,
      "step": 21070
    },
    {
      "epoch": 1.1711111111111112,
      "grad_norm": 0.12101653218269348,
      "learning_rate": 3.828888888888889e-05,
      "loss": 0.0041,
      "step": 21080
    },
    {
      "epoch": 1.1716666666666666,
      "grad_norm": 0.30163463950157166,
      "learning_rate": 3.828333333333334e-05,
      "loss": 0.0045,
      "step": 21090
    },
    {
      "epoch": 1.1722222222222223,
      "grad_norm": 0.3026100695133209,
      "learning_rate": 3.827777777777778e-05,
      "loss": 0.0054,
      "step": 21100
    },
    {
      "epoch": 1.1727777777777777,
      "grad_norm": 0.08447916060686111,
      "learning_rate": 3.827222222222223e-05,
      "loss": 0.0051,
      "step": 21110
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.0311442818492651,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0055,
      "step": 21120
    },
    {
      "epoch": 1.173888888888889,
      "grad_norm": 0.5942245125770569,
      "learning_rate": 3.8261111111111114e-05,
      "loss": 0.0057,
      "step": 21130
    },
    {
      "epoch": 1.1744444444444444,
      "grad_norm": 0.39100080728530884,
      "learning_rate": 3.825555555555556e-05,
      "loss": 0.0057,
      "step": 21140
    },
    {
      "epoch": 1.175,
      "grad_norm": 0.372494101524353,
      "learning_rate": 3.825e-05,
      "loss": 0.0048,
      "step": 21150
    },
    {
      "epoch": 1.1755555555555555,
      "grad_norm": 0.2494436800479889,
      "learning_rate": 3.8244444444444445e-05,
      "loss": 0.0037,
      "step": 21160
    },
    {
      "epoch": 1.176111111111111,
      "grad_norm": 0.15057092905044556,
      "learning_rate": 3.823888888888889e-05,
      "loss": 0.0051,
      "step": 21170
    },
    {
      "epoch": 1.1766666666666667,
      "grad_norm": 0.09036388248205185,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.0042,
      "step": 21180
    },
    {
      "epoch": 1.1772222222222222,
      "grad_norm": 0.23135419189929962,
      "learning_rate": 3.822777777777778e-05,
      "loss": 0.0058,
      "step": 21190
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.06042420491576195,
      "learning_rate": 3.8222222222222226e-05,
      "loss": 0.0054,
      "step": 21200
    },
    {
      "epoch": 1.1783333333333332,
      "grad_norm": 0.2308124452829361,
      "learning_rate": 3.821666666666667e-05,
      "loss": 0.0051,
      "step": 21210
    },
    {
      "epoch": 1.1788888888888889,
      "grad_norm": 0.08168673515319824,
      "learning_rate": 3.821111111111111e-05,
      "loss": 0.0038,
      "step": 21220
    },
    {
      "epoch": 1.1794444444444445,
      "grad_norm": 0.33054521679878235,
      "learning_rate": 3.820555555555556e-05,
      "loss": 0.0035,
      "step": 21230
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.1519693285226822,
      "learning_rate": 3.82e-05,
      "loss": 0.0037,
      "step": 21240
    },
    {
      "epoch": 1.1805555555555556,
      "grad_norm": 0.005840789992362261,
      "learning_rate": 3.8194444444444444e-05,
      "loss": 0.0049,
      "step": 21250
    },
    {
      "epoch": 1.181111111111111,
      "grad_norm": 0.1567564606666565,
      "learning_rate": 3.8188888888888894e-05,
      "loss": 0.0051,
      "step": 21260
    },
    {
      "epoch": 1.1816666666666666,
      "grad_norm": 0.17086070775985718,
      "learning_rate": 3.818333333333334e-05,
      "loss": 0.0044,
      "step": 21270
    },
    {
      "epoch": 1.1822222222222223,
      "grad_norm": 0.3002796769142151,
      "learning_rate": 3.817777777777778e-05,
      "loss": 0.005,
      "step": 21280
    },
    {
      "epoch": 1.1827777777777777,
      "grad_norm": 0.24031120538711548,
      "learning_rate": 3.8172222222222225e-05,
      "loss": 0.0027,
      "step": 21290
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.035085275769233704,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0047,
      "step": 21300
    },
    {
      "epoch": 1.1838888888888888,
      "grad_norm": 0.3303934335708618,
      "learning_rate": 3.816111111111111e-05,
      "loss": 0.0043,
      "step": 21310
    },
    {
      "epoch": 1.1844444444444444,
      "grad_norm": 0.09029112011194229,
      "learning_rate": 3.8155555555555555e-05,
      "loss": 0.005,
      "step": 21320
    },
    {
      "epoch": 1.185,
      "grad_norm": 0.2715686559677124,
      "learning_rate": 3.8150000000000006e-05,
      "loss": 0.005,
      "step": 21330
    },
    {
      "epoch": 1.1855555555555555,
      "grad_norm": 0.18083469569683075,
      "learning_rate": 3.814444444444444e-05,
      "loss": 0.0042,
      "step": 21340
    },
    {
      "epoch": 1.1861111111111111,
      "grad_norm": 0.15041707456111908,
      "learning_rate": 3.813888888888889e-05,
      "loss": 0.005,
      "step": 21350
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.24013417959213257,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0046,
      "step": 21360
    },
    {
      "epoch": 1.1872222222222222,
      "grad_norm": 0.2784600555896759,
      "learning_rate": 3.812777777777778e-05,
      "loss": 0.0047,
      "step": 21370
    },
    {
      "epoch": 1.1877777777777778,
      "grad_norm": 0.2300802618265152,
      "learning_rate": 3.8122222222222224e-05,
      "loss": 0.0047,
      "step": 21380
    },
    {
      "epoch": 1.1883333333333332,
      "grad_norm": 0.0613003745675087,
      "learning_rate": 3.811666666666667e-05,
      "loss": 0.0037,
      "step": 21390
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 0.1036514863371849,
      "learning_rate": 3.811111111111112e-05,
      "loss": 0.0045,
      "step": 21400
    },
    {
      "epoch": 1.1894444444444445,
      "grad_norm": 0.3732103109359741,
      "learning_rate": 3.8105555555555554e-05,
      "loss": 0.0039,
      "step": 21410
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.42938703298568726,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.0029,
      "step": 21420
    },
    {
      "epoch": 1.1905555555555556,
      "grad_norm": 0.7513735890388489,
      "learning_rate": 3.809444444444444e-05,
      "loss": 0.0045,
      "step": 21430
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 0.48050111532211304,
      "learning_rate": 3.808888888888889e-05,
      "loss": 0.0049,
      "step": 21440
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 0.061205677688121796,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.0043,
      "step": 21450
    },
    {
      "epoch": 1.1922222222222223,
      "grad_norm": 0.09167148917913437,
      "learning_rate": 3.807777777777778e-05,
      "loss": 0.0061,
      "step": 21460
    },
    {
      "epoch": 1.1927777777777777,
      "grad_norm": 0.4206310510635376,
      "learning_rate": 3.807222222222223e-05,
      "loss": 0.0063,
      "step": 21470
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.3611592948436737,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0038,
      "step": 21480
    },
    {
      "epoch": 1.193888888888889,
      "grad_norm": 0.2104393094778061,
      "learning_rate": 3.8061111111111116e-05,
      "loss": 0.0053,
      "step": 21490
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 0.35981082916259766,
      "learning_rate": 3.805555555555555e-05,
      "loss": 0.0046,
      "step": 21500
    },
    {
      "epoch": 1.195,
      "grad_norm": 0.21083949506282806,
      "learning_rate": 3.805e-05,
      "loss": 0.0043,
      "step": 21510
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 0.032423023134469986,
      "learning_rate": 3.804444444444445e-05,
      "loss": 0.003,
      "step": 21520
    },
    {
      "epoch": 1.1961111111111111,
      "grad_norm": 0.11691010743379593,
      "learning_rate": 3.803888888888889e-05,
      "loss": 0.0046,
      "step": 21530
    },
    {
      "epoch": 1.1966666666666668,
      "grad_norm": 0.04942990094423294,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.004,
      "step": 21540
    },
    {
      "epoch": 1.1972222222222222,
      "grad_norm": 0.24421285092830658,
      "learning_rate": 3.802777777777778e-05,
      "loss": 0.0038,
      "step": 21550
    },
    {
      "epoch": 1.1977777777777778,
      "grad_norm": 0.60707688331604,
      "learning_rate": 3.802222222222223e-05,
      "loss": 0.0027,
      "step": 21560
    },
    {
      "epoch": 1.1983333333333333,
      "grad_norm": 0.410842627286911,
      "learning_rate": 3.8016666666666665e-05,
      "loss": 0.0039,
      "step": 21570
    },
    {
      "epoch": 1.198888888888889,
      "grad_norm": 0.15286678075790405,
      "learning_rate": 3.8011111111111115e-05,
      "loss": 0.0061,
      "step": 21580
    },
    {
      "epoch": 1.1994444444444445,
      "grad_norm": 0.08379597961902618,
      "learning_rate": 3.800555555555556e-05,
      "loss": 0.0039,
      "step": 21590
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.3107932507991791,
      "learning_rate": 3.8e-05,
      "loss": 0.0049,
      "step": 21600
    },
    {
      "epoch": 1.2005555555555556,
      "grad_norm": 0.06147206574678421,
      "learning_rate": 3.7994444444444446e-05,
      "loss": 0.0047,
      "step": 21610
    },
    {
      "epoch": 1.201111111111111,
      "grad_norm": 0.10685224086046219,
      "learning_rate": 3.798888888888889e-05,
      "loss": 0.0045,
      "step": 21620
    },
    {
      "epoch": 1.2016666666666667,
      "grad_norm": 0.481687992811203,
      "learning_rate": 3.798333333333334e-05,
      "loss": 0.0044,
      "step": 21630
    },
    {
      "epoch": 1.2022222222222223,
      "grad_norm": 0.4627144932746887,
      "learning_rate": 3.7977777777777776e-05,
      "loss": 0.0047,
      "step": 21640
    },
    {
      "epoch": 1.2027777777777777,
      "grad_norm": 0.11735806614160538,
      "learning_rate": 3.797222222222223e-05,
      "loss": 0.0059,
      "step": 21650
    },
    {
      "epoch": 1.2033333333333334,
      "grad_norm": 0.30266740918159485,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.0034,
      "step": 21660
    },
    {
      "epoch": 1.2038888888888888,
      "grad_norm": 0.30109578371047974,
      "learning_rate": 3.7961111111111114e-05,
      "loss": 0.0032,
      "step": 21670
    },
    {
      "epoch": 1.2044444444444444,
      "grad_norm": 0.09764759242534637,
      "learning_rate": 3.795555555555556e-05,
      "loss": 0.004,
      "step": 21680
    },
    {
      "epoch": 1.205,
      "grad_norm": 0.21238389611244202,
      "learning_rate": 3.795e-05,
      "loss": 0.0034,
      "step": 21690
    },
    {
      "epoch": 1.2055555555555555,
      "grad_norm": 0.5104522705078125,
      "learning_rate": 3.7944444444444444e-05,
      "loss": 0.0048,
      "step": 21700
    },
    {
      "epoch": 1.2061111111111111,
      "grad_norm": 0.27635088562965393,
      "learning_rate": 3.793888888888889e-05,
      "loss": 0.0054,
      "step": 21710
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.03329330310225487,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0034,
      "step": 21720
    },
    {
      "epoch": 1.2072222222222222,
      "grad_norm": 0.31565961241722107,
      "learning_rate": 3.792777777777778e-05,
      "loss": 0.0031,
      "step": 21730
    },
    {
      "epoch": 1.2077777777777778,
      "grad_norm": 0.3980073034763336,
      "learning_rate": 3.7922222222222225e-05,
      "loss": 0.0041,
      "step": 21740
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.12180580198764801,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.0044,
      "step": 21750
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 0.3007511794567108,
      "learning_rate": 3.791111111111111e-05,
      "loss": 0.005,
      "step": 21760
    },
    {
      "epoch": 1.2094444444444443,
      "grad_norm": 0.24067816138267517,
      "learning_rate": 3.7905555555555556e-05,
      "loss": 0.0024,
      "step": 21770
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.3653750717639923,
      "learning_rate": 3.79e-05,
      "loss": 0.0044,
      "step": 21780
    },
    {
      "epoch": 1.2105555555555556,
      "grad_norm": 0.12020367383956909,
      "learning_rate": 3.789444444444444e-05,
      "loss": 0.0041,
      "step": 21790
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 0.12066729366779327,
      "learning_rate": 3.7888888888888894e-05,
      "loss": 0.0041,
      "step": 21800
    },
    {
      "epoch": 1.2116666666666667,
      "grad_norm": 0.2704797089099884,
      "learning_rate": 3.788333333333334e-05,
      "loss": 0.0047,
      "step": 21810
    },
    {
      "epoch": 1.2122222222222223,
      "grad_norm": 0.6002342104911804,
      "learning_rate": 3.787777777777778e-05,
      "loss": 0.0051,
      "step": 21820
    },
    {
      "epoch": 1.2127777777777777,
      "grad_norm": 0.2121698409318924,
      "learning_rate": 3.7872222222222224e-05,
      "loss": 0.0052,
      "step": 21830
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.15058781206607819,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0035,
      "step": 21840
    },
    {
      "epoch": 1.2138888888888888,
      "grad_norm": 0.1162959560751915,
      "learning_rate": 3.786111111111111e-05,
      "loss": 0.0042,
      "step": 21850
    },
    {
      "epoch": 1.2144444444444444,
      "grad_norm": 0.12057199329137802,
      "learning_rate": 3.7855555555555555e-05,
      "loss": 0.0051,
      "step": 21860
    },
    {
      "epoch": 1.215,
      "grad_norm": 0.3600095808506012,
      "learning_rate": 3.7850000000000005e-05,
      "loss": 0.0059,
      "step": 21870
    },
    {
      "epoch": 1.2155555555555555,
      "grad_norm": 0.1818135529756546,
      "learning_rate": 3.784444444444445e-05,
      "loss": 0.0054,
      "step": 21880
    },
    {
      "epoch": 1.2161111111111111,
      "grad_norm": 0.07682719826698303,
      "learning_rate": 3.783888888888889e-05,
      "loss": 0.0043,
      "step": 21890
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.22903376817703247,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0049,
      "step": 21900
    },
    {
      "epoch": 1.2172222222222222,
      "grad_norm": 0.5111006498336792,
      "learning_rate": 3.782777777777778e-05,
      "loss": 0.005,
      "step": 21910
    },
    {
      "epoch": 1.2177777777777778,
      "grad_norm": 0.30067163705825806,
      "learning_rate": 3.782222222222222e-05,
      "loss": 0.0041,
      "step": 21920
    },
    {
      "epoch": 1.2183333333333333,
      "grad_norm": 0.27607524394989014,
      "learning_rate": 3.7816666666666667e-05,
      "loss": 0.0045,
      "step": 21930
    },
    {
      "epoch": 1.218888888888889,
      "grad_norm": 0.3294185698032379,
      "learning_rate": 3.781111111111112e-05,
      "loss": 0.0052,
      "step": 21940
    },
    {
      "epoch": 1.2194444444444446,
      "grad_norm": 0.12009025365114212,
      "learning_rate": 3.7805555555555554e-05,
      "loss": 0.0053,
      "step": 21950
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.15091562271118164,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0045,
      "step": 21960
    },
    {
      "epoch": 1.2205555555555556,
      "grad_norm": 0.5688257813453674,
      "learning_rate": 3.779444444444445e-05,
      "loss": 0.004,
      "step": 21970
    },
    {
      "epoch": 1.221111111111111,
      "grad_norm": 0.4038274884223938,
      "learning_rate": 3.778888888888889e-05,
      "loss": 0.0056,
      "step": 21980
    },
    {
      "epoch": 1.2216666666666667,
      "grad_norm": 0.3012433648109436,
      "learning_rate": 3.7783333333333335e-05,
      "loss": 0.0059,
      "step": 21990
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.15050822496414185,
      "learning_rate": 3.777777777777778e-05,
      "loss": 0.0033,
      "step": 22000
    },
    {
      "epoch": 1.2227777777777777,
      "grad_norm": 0.5710938572883606,
      "learning_rate": 3.777222222222223e-05,
      "loss": 0.0036,
      "step": 22010
    },
    {
      "epoch": 1.2233333333333334,
      "grad_norm": 0.30120888352394104,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0039,
      "step": 22020
    },
    {
      "epoch": 1.2238888888888888,
      "grad_norm": 0.00865685474127531,
      "learning_rate": 3.7761111111111116e-05,
      "loss": 0.004,
      "step": 22030
    },
    {
      "epoch": 1.2244444444444444,
      "grad_norm": 0.36038362979888916,
      "learning_rate": 3.775555555555555e-05,
      "loss": 0.0042,
      "step": 22040
    },
    {
      "epoch": 1.225,
      "grad_norm": 0.03361113741993904,
      "learning_rate": 3.775e-05,
      "loss": 0.0041,
      "step": 22050
    },
    {
      "epoch": 1.2255555555555555,
      "grad_norm": 0.12009647488594055,
      "learning_rate": 3.7744444444444446e-05,
      "loss": 0.0039,
      "step": 22060
    },
    {
      "epoch": 1.2261111111111112,
      "grad_norm": 0.09005976468324661,
      "learning_rate": 3.773888888888889e-05,
      "loss": 0.0043,
      "step": 22070
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.009943188168108463,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0039,
      "step": 22080
    },
    {
      "epoch": 1.2272222222222222,
      "grad_norm": 0.12118875980377197,
      "learning_rate": 3.772777777777778e-05,
      "loss": 0.0032,
      "step": 22090
    },
    {
      "epoch": 1.2277777777777779,
      "grad_norm": 0.13137471675872803,
      "learning_rate": 3.772222222222223e-05,
      "loss": 0.0043,
      "step": 22100
    },
    {
      "epoch": 1.2283333333333333,
      "grad_norm": 0.21060308814048767,
      "learning_rate": 3.7716666666666664e-05,
      "loss": 0.0029,
      "step": 22110
    },
    {
      "epoch": 1.228888888888889,
      "grad_norm": 0.18005825579166412,
      "learning_rate": 3.7711111111111114e-05,
      "loss": 0.0036,
      "step": 22120
    },
    {
      "epoch": 1.2294444444444443,
      "grad_norm": 0.12018484622240067,
      "learning_rate": 3.770555555555556e-05,
      "loss": 0.0047,
      "step": 22130
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.15021342039108276,
      "learning_rate": 3.77e-05,
      "loss": 0.0046,
      "step": 22140
    },
    {
      "epoch": 1.2305555555555556,
      "grad_norm": 0.18179693818092346,
      "learning_rate": 3.769444444444445e-05,
      "loss": 0.0055,
      "step": 22150
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 0.09104554355144501,
      "learning_rate": 3.768888888888889e-05,
      "loss": 0.0045,
      "step": 22160
    },
    {
      "epoch": 1.2316666666666667,
      "grad_norm": 0.1801316887140274,
      "learning_rate": 3.768333333333334e-05,
      "loss": 0.0046,
      "step": 22170
    },
    {
      "epoch": 1.232222222222222,
      "grad_norm": 0.18324685096740723,
      "learning_rate": 3.7677777777777776e-05,
      "loss": 0.0056,
      "step": 22180
    },
    {
      "epoch": 1.2327777777777778,
      "grad_norm": 0.17983028292655945,
      "learning_rate": 3.7672222222222226e-05,
      "loss": 0.004,
      "step": 22190
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.48054513335227966,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0045,
      "step": 22200
    },
    {
      "epoch": 1.2338888888888888,
      "grad_norm": 0.5401654243469238,
      "learning_rate": 3.766111111111111e-05,
      "loss": 0.0035,
      "step": 22210
    },
    {
      "epoch": 1.2344444444444445,
      "grad_norm": 0.09705650061368942,
      "learning_rate": 3.765555555555556e-05,
      "loss": 0.0034,
      "step": 22220
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 0.15035156905651093,
      "learning_rate": 3.765e-05,
      "loss": 0.004,
      "step": 22230
    },
    {
      "epoch": 1.2355555555555555,
      "grad_norm": 0.12018983066082001,
      "learning_rate": 3.764444444444445e-05,
      "loss": 0.0033,
      "step": 22240
    },
    {
      "epoch": 1.2361111111111112,
      "grad_norm": 0.12065518647432327,
      "learning_rate": 3.763888888888889e-05,
      "loss": 0.0048,
      "step": 22250
    },
    {
      "epoch": 1.2366666666666666,
      "grad_norm": 0.12193451076745987,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0048,
      "step": 22260
    },
    {
      "epoch": 1.2372222222222222,
      "grad_norm": 0.1584872603416443,
      "learning_rate": 3.762777777777778e-05,
      "loss": 0.0061,
      "step": 22270
    },
    {
      "epoch": 1.2377777777777779,
      "grad_norm": 0.019756488502025604,
      "learning_rate": 3.7622222222222225e-05,
      "loss": 0.0039,
      "step": 22280
    },
    {
      "epoch": 1.2383333333333333,
      "grad_norm": 0.30098339915275574,
      "learning_rate": 3.761666666666667e-05,
      "loss": 0.0031,
      "step": 22290
    },
    {
      "epoch": 1.238888888888889,
      "grad_norm": 0.1799541711807251,
      "learning_rate": 3.761111111111111e-05,
      "loss": 0.0065,
      "step": 22300
    },
    {
      "epoch": 1.2394444444444443,
      "grad_norm": 0.42116570472717285,
      "learning_rate": 3.7605555555555556e-05,
      "loss": 0.0062,
      "step": 22310
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.1059304028749466,
      "learning_rate": 3.76e-05,
      "loss": 0.0059,
      "step": 22320
    },
    {
      "epoch": 1.2405555555555556,
      "grad_norm": 0.03069399856030941,
      "learning_rate": 3.759444444444445e-05,
      "loss": 0.0058,
      "step": 22330
    },
    {
      "epoch": 1.241111111111111,
      "grad_norm": 0.3312552869319916,
      "learning_rate": 3.758888888888889e-05,
      "loss": 0.0039,
      "step": 22340
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 0.180402934551239,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.0037,
      "step": 22350
    },
    {
      "epoch": 1.2422222222222223,
      "grad_norm": 0.5834537744522095,
      "learning_rate": 3.757777777777778e-05,
      "loss": 0.0041,
      "step": 22360
    },
    {
      "epoch": 1.2427777777777778,
      "grad_norm": 0.2900139391422272,
      "learning_rate": 3.7572222222222224e-05,
      "loss": 0.0055,
      "step": 22370
    },
    {
      "epoch": 1.2433333333333334,
      "grad_norm": 0.48064929246902466,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.0052,
      "step": 22380
    },
    {
      "epoch": 1.2438888888888888,
      "grad_norm": 0.18059417605400085,
      "learning_rate": 3.756111111111111e-05,
      "loss": 0.0053,
      "step": 22390
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.4610401690006256,
      "learning_rate": 3.7555555555555554e-05,
      "loss": 0.0039,
      "step": 22400
    },
    {
      "epoch": 1.245,
      "grad_norm": 0.2702396810054779,
      "learning_rate": 3.7550000000000005e-05,
      "loss": 0.0055,
      "step": 22410
    },
    {
      "epoch": 1.2455555555555555,
      "grad_norm": 0.01402097288519144,
      "learning_rate": 3.754444444444445e-05,
      "loss": 0.0048,
      "step": 22420
    },
    {
      "epoch": 1.2461111111111112,
      "grad_norm": 0.15129628777503967,
      "learning_rate": 3.753888888888889e-05,
      "loss": 0.0053,
      "step": 22430
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.3919699490070343,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0041,
      "step": 22440
    },
    {
      "epoch": 1.2472222222222222,
      "grad_norm": 0.24036431312561035,
      "learning_rate": 3.752777777777778e-05,
      "loss": 0.0049,
      "step": 22450
    },
    {
      "epoch": 1.2477777777777779,
      "grad_norm": 0.14464540779590607,
      "learning_rate": 3.752222222222222e-05,
      "loss": 0.006,
      "step": 22460
    },
    {
      "epoch": 1.2483333333333333,
      "grad_norm": 0.18113967776298523,
      "learning_rate": 3.7516666666666666e-05,
      "loss": 0.0062,
      "step": 22470
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 0.5317954421043396,
      "learning_rate": 3.7511111111111116e-05,
      "loss": 0.0053,
      "step": 22480
    },
    {
      "epoch": 1.2494444444444444,
      "grad_norm": 0.3003278374671936,
      "learning_rate": 3.750555555555555e-05,
      "loss": 0.0039,
      "step": 22490
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.3305988013744354,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0049,
      "step": 22500
    },
    {
      "epoch": 1.2505555555555556,
      "grad_norm": 0.35761740803718567,
      "learning_rate": 3.749444444444445e-05,
      "loss": 0.0034,
      "step": 22510
    },
    {
      "epoch": 1.251111111111111,
      "grad_norm": 0.12013383209705353,
      "learning_rate": 3.748888888888889e-05,
      "loss": 0.0029,
      "step": 22520
    },
    {
      "epoch": 1.2516666666666667,
      "grad_norm": 0.23983953893184662,
      "learning_rate": 3.7483333333333334e-05,
      "loss": 0.0043,
      "step": 22530
    },
    {
      "epoch": 1.2522222222222221,
      "grad_norm": 0.15880578756332397,
      "learning_rate": 3.747777777777778e-05,
      "loss": 0.0039,
      "step": 22540
    },
    {
      "epoch": 1.2527777777777778,
      "grad_norm": 0.17226733267307281,
      "learning_rate": 3.747222222222223e-05,
      "loss": 0.0052,
      "step": 22550
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.18152707815170288,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0053,
      "step": 22560
    },
    {
      "epoch": 1.2538888888888888,
      "grad_norm": 0.22651001811027527,
      "learning_rate": 3.7461111111111115e-05,
      "loss": 0.0039,
      "step": 22570
    },
    {
      "epoch": 1.2544444444444445,
      "grad_norm": 0.15088756382465363,
      "learning_rate": 3.745555555555555e-05,
      "loss": 0.0051,
      "step": 22580
    },
    {
      "epoch": 1.255,
      "grad_norm": 0.1509348750114441,
      "learning_rate": 3.745e-05,
      "loss": 0.0049,
      "step": 22590
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 0.5408514738082886,
      "learning_rate": 3.7444444444444446e-05,
      "loss": 0.0049,
      "step": 22600
    },
    {
      "epoch": 1.2561111111111112,
      "grad_norm": 0.09354887902736664,
      "learning_rate": 3.743888888888889e-05,
      "loss": 0.0046,
      "step": 22610
    },
    {
      "epoch": 1.2566666666666666,
      "grad_norm": 0.3010832965373993,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.0051,
      "step": 22620
    },
    {
      "epoch": 1.2572222222222222,
      "grad_norm": 0.6012399196624756,
      "learning_rate": 3.7427777777777777e-05,
      "loss": 0.0051,
      "step": 22630
    },
    {
      "epoch": 1.2577777777777777,
      "grad_norm": 0.30741268396377563,
      "learning_rate": 3.742222222222223e-05,
      "loss": 0.0074,
      "step": 22640
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.15190985798835754,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.0051,
      "step": 22650
    },
    {
      "epoch": 1.258888888888889,
      "grad_norm": 0.510924220085144,
      "learning_rate": 3.7411111111111114e-05,
      "loss": 0.0063,
      "step": 22660
    },
    {
      "epoch": 1.2594444444444444,
      "grad_norm": 0.010972038842737675,
      "learning_rate": 3.740555555555556e-05,
      "loss": 0.0033,
      "step": 22670
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.07008566707372665,
      "learning_rate": 3.74e-05,
      "loss": 0.0034,
      "step": 22680
    },
    {
      "epoch": 1.2605555555555554,
      "grad_norm": 0.06295925378799438,
      "learning_rate": 3.739444444444445e-05,
      "loss": 0.0043,
      "step": 22690
    },
    {
      "epoch": 1.261111111111111,
      "grad_norm": 0.3002821207046509,
      "learning_rate": 3.738888888888889e-05,
      "loss": 0.0058,
      "step": 22700
    },
    {
      "epoch": 1.2616666666666667,
      "grad_norm": 0.01005504745990038,
      "learning_rate": 3.738333333333334e-05,
      "loss": 0.0057,
      "step": 22710
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 0.34957268834114075,
      "learning_rate": 3.7377777777777775e-05,
      "loss": 0.0038,
      "step": 22720
    },
    {
      "epoch": 1.2627777777777778,
      "grad_norm": 0.3006457984447479,
      "learning_rate": 3.7372222222222226e-05,
      "loss": 0.0049,
      "step": 22730
    },
    {
      "epoch": 1.2633333333333332,
      "grad_norm": 0.03277169540524483,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.0064,
      "step": 22740
    },
    {
      "epoch": 1.2638888888888888,
      "grad_norm": 0.38939720392227173,
      "learning_rate": 3.736111111111111e-05,
      "loss": 0.0058,
      "step": 22750
    },
    {
      "epoch": 1.2644444444444445,
      "grad_norm": 0.15298625826835632,
      "learning_rate": 3.7355555555555556e-05,
      "loss": 0.0058,
      "step": 22760
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 0.5402517914772034,
      "learning_rate": 3.735e-05,
      "loss": 0.0033,
      "step": 22770
    },
    {
      "epoch": 1.2655555555555555,
      "grad_norm": 0.0912335067987442,
      "learning_rate": 3.734444444444445e-05,
      "loss": 0.0053,
      "step": 22780
    },
    {
      "epoch": 1.2661111111111112,
      "grad_norm": 0.09179657697677612,
      "learning_rate": 3.733888888888889e-05,
      "loss": 0.0056,
      "step": 22790
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.12297062575817108,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0057,
      "step": 22800
    },
    {
      "epoch": 1.2672222222222222,
      "grad_norm": 0.4477272033691406,
      "learning_rate": 3.732777777777778e-05,
      "loss": 0.004,
      "step": 22810
    },
    {
      "epoch": 1.267777777777778,
      "grad_norm": 0.15811797976493835,
      "learning_rate": 3.7322222222222224e-05,
      "loss": 0.0057,
      "step": 22820
    },
    {
      "epoch": 1.2683333333333333,
      "grad_norm": 0.2412555068731308,
      "learning_rate": 3.731666666666667e-05,
      "loss": 0.0048,
      "step": 22830
    },
    {
      "epoch": 1.268888888888889,
      "grad_norm": 0.18195931613445282,
      "learning_rate": 3.731111111111111e-05,
      "loss": 0.0033,
      "step": 22840
    },
    {
      "epoch": 1.2694444444444444,
      "grad_norm": 0.300260454416275,
      "learning_rate": 3.7305555555555555e-05,
      "loss": 0.0048,
      "step": 22850
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.6013974547386169,
      "learning_rate": 3.73e-05,
      "loss": 0.0059,
      "step": 22860
    },
    {
      "epoch": 1.2705555555555557,
      "grad_norm": 0.5104449987411499,
      "learning_rate": 3.729444444444445e-05,
      "loss": 0.0051,
      "step": 22870
    },
    {
      "epoch": 1.271111111111111,
      "grad_norm": 0.06090097874403,
      "learning_rate": 3.728888888888889e-05,
      "loss": 0.0049,
      "step": 22880
    },
    {
      "epoch": 1.2716666666666667,
      "grad_norm": 0.5115702152252197,
      "learning_rate": 3.7283333333333336e-05,
      "loss": 0.0041,
      "step": 22890
    },
    {
      "epoch": 1.2722222222222221,
      "grad_norm": 0.1803804337978363,
      "learning_rate": 3.727777777777778e-05,
      "loss": 0.0035,
      "step": 22900
    },
    {
      "epoch": 1.2727777777777778,
      "grad_norm": 0.12150223553180695,
      "learning_rate": 3.727222222222222e-05,
      "loss": 0.0044,
      "step": 22910
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.36186373233795166,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0042,
      "step": 22920
    },
    {
      "epoch": 1.2738888888888888,
      "grad_norm": 0.3147355020046234,
      "learning_rate": 3.726111111111111e-05,
      "loss": 0.0045,
      "step": 22930
    },
    {
      "epoch": 1.2744444444444445,
      "grad_norm": 0.1806599348783493,
      "learning_rate": 3.7255555555555554e-05,
      "loss": 0.0051,
      "step": 22940
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.22618678212165833,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.003,
      "step": 22950
    },
    {
      "epoch": 1.2755555555555556,
      "grad_norm": 0.21404427289962769,
      "learning_rate": 3.724444444444445e-05,
      "loss": 0.0054,
      "step": 22960
    },
    {
      "epoch": 1.2761111111111112,
      "grad_norm": 0.24531322717666626,
      "learning_rate": 3.723888888888889e-05,
      "loss": 0.0066,
      "step": 22970
    },
    {
      "epoch": 1.2766666666666666,
      "grad_norm": 0.06144426763057709,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0056,
      "step": 22980
    },
    {
      "epoch": 1.2772222222222223,
      "grad_norm": 0.3307427167892456,
      "learning_rate": 3.722777777777778e-05,
      "loss": 0.0028,
      "step": 22990
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.18427084386348724,
      "learning_rate": 3.722222222222222e-05,
      "loss": 0.0047,
      "step": 23000
    },
    {
      "epoch": 1.2783333333333333,
      "grad_norm": 0.02147609554231167,
      "learning_rate": 3.7216666666666666e-05,
      "loss": 0.0063,
      "step": 23010
    },
    {
      "epoch": 1.278888888888889,
      "grad_norm": 0.09139272570610046,
      "learning_rate": 3.7211111111111116e-05,
      "loss": 0.0045,
      "step": 23020
    },
    {
      "epoch": 1.2794444444444444,
      "grad_norm": 0.5409489274024963,
      "learning_rate": 3.720555555555555e-05,
      "loss": 0.0047,
      "step": 23030
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.3235726058483124,
      "learning_rate": 3.72e-05,
      "loss": 0.0051,
      "step": 23040
    },
    {
      "epoch": 1.2805555555555554,
      "grad_norm": 0.3924846649169922,
      "learning_rate": 3.7194444444444447e-05,
      "loss": 0.0051,
      "step": 23050
    },
    {
      "epoch": 1.281111111111111,
      "grad_norm": 0.1803034096956253,
      "learning_rate": 3.718888888888889e-05,
      "loss": 0.0045,
      "step": 23060
    },
    {
      "epoch": 1.2816666666666667,
      "grad_norm": 0.0904049500823021,
      "learning_rate": 3.7183333333333334e-05,
      "loss": 0.0044,
      "step": 23070
    },
    {
      "epoch": 1.2822222222222222,
      "grad_norm": 0.2701908051967621,
      "learning_rate": 3.717777777777778e-05,
      "loss": 0.0038,
      "step": 23080
    },
    {
      "epoch": 1.2827777777777778,
      "grad_norm": 0.12013246864080429,
      "learning_rate": 3.717222222222223e-05,
      "loss": 0.0037,
      "step": 23090
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.31683605909347534,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.0046,
      "step": 23100
    },
    {
      "epoch": 1.2838888888888889,
      "grad_norm": 0.4209463894367218,
      "learning_rate": 3.7161111111111115e-05,
      "loss": 0.0054,
      "step": 23110
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 0.013799410313367844,
      "learning_rate": 3.715555555555555e-05,
      "loss": 0.0041,
      "step": 23120
    },
    {
      "epoch": 1.285,
      "grad_norm": 0.39015159010887146,
      "learning_rate": 3.715e-05,
      "loss": 0.0048,
      "step": 23130
    },
    {
      "epoch": 1.2855555555555556,
      "grad_norm": 0.20470736920833588,
      "learning_rate": 3.7144444444444445e-05,
      "loss": 0.0043,
      "step": 23140
    },
    {
      "epoch": 1.286111111111111,
      "grad_norm": 0.06145559996366501,
      "learning_rate": 3.713888888888889e-05,
      "loss": 0.003,
      "step": 23150
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.44995978474617004,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0055,
      "step": 23160
    },
    {
      "epoch": 1.2872222222222223,
      "grad_norm": 0.18119673430919647,
      "learning_rate": 3.7127777777777776e-05,
      "loss": 0.0046,
      "step": 23170
    },
    {
      "epoch": 1.287777777777778,
      "grad_norm": 0.12185486406087875,
      "learning_rate": 3.7122222222222226e-05,
      "loss": 0.0031,
      "step": 23180
    },
    {
      "epoch": 1.2883333333333333,
      "grad_norm": 0.6646204590797424,
      "learning_rate": 3.711666666666666e-05,
      "loss": 0.0041,
      "step": 23190
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.2993668019771576,
      "learning_rate": 3.7111111111111113e-05,
      "loss": 0.003,
      "step": 23200
    },
    {
      "epoch": 1.2894444444444444,
      "grad_norm": 0.29941344261169434,
      "learning_rate": 3.710555555555556e-05,
      "loss": 0.0046,
      "step": 23210
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.061826545745134354,
      "learning_rate": 3.71e-05,
      "loss": 0.006,
      "step": 23220
    },
    {
      "epoch": 1.2905555555555557,
      "grad_norm": 0.15060973167419434,
      "learning_rate": 3.709444444444445e-05,
      "loss": 0.0037,
      "step": 23230
    },
    {
      "epoch": 1.291111111111111,
      "grad_norm": 0.2593993544578552,
      "learning_rate": 3.708888888888889e-05,
      "loss": 0.0037,
      "step": 23240
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.30761033296585083,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.0035,
      "step": 23250
    },
    {
      "epoch": 1.2922222222222222,
      "grad_norm": 0.1857413500547409,
      "learning_rate": 3.7077777777777775e-05,
      "loss": 0.0046,
      "step": 23260
    },
    {
      "epoch": 1.2927777777777778,
      "grad_norm": 0.30137792229652405,
      "learning_rate": 3.7072222222222225e-05,
      "loss": 0.0044,
      "step": 23270
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.6506932973861694,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0066,
      "step": 23280
    },
    {
      "epoch": 1.2938888888888889,
      "grad_norm": 0.030564969405531883,
      "learning_rate": 3.706111111111111e-05,
      "loss": 0.0052,
      "step": 23290
    },
    {
      "epoch": 1.2944444444444445,
      "grad_norm": 0.24214641749858856,
      "learning_rate": 3.705555555555556e-05,
      "loss": 0.0041,
      "step": 23300
    },
    {
      "epoch": 1.295,
      "grad_norm": 0.008576987311244011,
      "learning_rate": 3.705e-05,
      "loss": 0.0041,
      "step": 23310
    },
    {
      "epoch": 1.2955555555555556,
      "grad_norm": 0.3296707570552826,
      "learning_rate": 3.704444444444445e-05,
      "loss": 0.0041,
      "step": 23320
    },
    {
      "epoch": 1.2961111111111112,
      "grad_norm": 0.3700398802757263,
      "learning_rate": 3.7038888888888886e-05,
      "loss": 0.005,
      "step": 23330
    },
    {
      "epoch": 1.2966666666666666,
      "grad_norm": 0.15021739900112152,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0038,
      "step": 23340
    },
    {
      "epoch": 1.2972222222222223,
      "grad_norm": 0.18121002614498138,
      "learning_rate": 3.702777777777778e-05,
      "loss": 0.0049,
      "step": 23350
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 0.21201002597808838,
      "learning_rate": 3.7022222222222224e-05,
      "loss": 0.0064,
      "step": 23360
    },
    {
      "epoch": 1.2983333333333333,
      "grad_norm": 0.4499111473560333,
      "learning_rate": 3.701666666666667e-05,
      "loss": 0.0035,
      "step": 23370
    },
    {
      "epoch": 1.298888888888889,
      "grad_norm": 0.41969653964042664,
      "learning_rate": 3.701111111111111e-05,
      "loss": 0.0051,
      "step": 23380
    },
    {
      "epoch": 1.2994444444444444,
      "grad_norm": 0.4205928444862366,
      "learning_rate": 3.700555555555556e-05,
      "loss": 0.0037,
      "step": 23390
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.12062530219554901,
      "learning_rate": 3.7e-05,
      "loss": 0.0053,
      "step": 23400
    },
    {
      "epoch": 1.3005555555555555,
      "grad_norm": 0.3311510682106018,
      "learning_rate": 3.699444444444445e-05,
      "loss": 0.0045,
      "step": 23410
    },
    {
      "epoch": 1.301111111111111,
      "grad_norm": 0.39068669080734253,
      "learning_rate": 3.698888888888889e-05,
      "loss": 0.0049,
      "step": 23420
    },
    {
      "epoch": 1.3016666666666667,
      "grad_norm": 0.547360897064209,
      "learning_rate": 3.6983333333333336e-05,
      "loss": 0.0048,
      "step": 23430
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 0.06110452115535736,
      "learning_rate": 3.697777777777778e-05,
      "loss": 0.0047,
      "step": 23440
    },
    {
      "epoch": 1.3027777777777778,
      "grad_norm": 0.27657875418663025,
      "learning_rate": 3.697222222222222e-05,
      "loss": 0.004,
      "step": 23450
    },
    {
      "epoch": 1.3033333333333332,
      "grad_norm": 0.1503763198852539,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0056,
      "step": 23460
    },
    {
      "epoch": 1.3038888888888889,
      "grad_norm": 0.03413816913962364,
      "learning_rate": 3.696111111111111e-05,
      "loss": 0.0049,
      "step": 23470
    },
    {
      "epoch": 1.3044444444444445,
      "grad_norm": 0.06264492124319077,
      "learning_rate": 3.695555555555556e-05,
      "loss": 0.0072,
      "step": 23480
    },
    {
      "epoch": 1.305,
      "grad_norm": 0.018521077930927277,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 0.0038,
      "step": 23490
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 0.21065707504749298,
      "learning_rate": 3.694444444444445e-05,
      "loss": 0.0037,
      "step": 23500
    },
    {
      "epoch": 1.306111111111111,
      "grad_norm": 0.4863949716091156,
      "learning_rate": 3.693888888888889e-05,
      "loss": 0.0039,
      "step": 23510
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.16102348268032074,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0043,
      "step": 23520
    },
    {
      "epoch": 1.3072222222222223,
      "grad_norm": 0.21018075942993164,
      "learning_rate": 3.692777777777778e-05,
      "loss": 0.0037,
      "step": 23530
    },
    {
      "epoch": 1.3077777777777777,
      "grad_norm": 0.035575021058321,
      "learning_rate": 3.692222222222222e-05,
      "loss": 0.0044,
      "step": 23540
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.03571552038192749,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.0039,
      "step": 23550
    },
    {
      "epoch": 1.3088888888888888,
      "grad_norm": 0.3300967812538147,
      "learning_rate": 3.6911111111111115e-05,
      "loss": 0.0041,
      "step": 23560
    },
    {
      "epoch": 1.3094444444444444,
      "grad_norm": 0.1483743041753769,
      "learning_rate": 3.690555555555556e-05,
      "loss": 0.0037,
      "step": 23570
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.060824621468782425,
      "learning_rate": 3.69e-05,
      "loss": 0.0029,
      "step": 23580
    },
    {
      "epoch": 1.3105555555555555,
      "grad_norm": 0.49586740136146545,
      "learning_rate": 3.6894444444444446e-05,
      "loss": 0.0045,
      "step": 23590
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 0.3021983802318573,
      "learning_rate": 3.688888888888889e-05,
      "loss": 0.0046,
      "step": 23600
    },
    {
      "epoch": 1.3116666666666665,
      "grad_norm": 0.38758569955825806,
      "learning_rate": 3.688333333333333e-05,
      "loss": 0.0043,
      "step": 23610
    },
    {
      "epoch": 1.3122222222222222,
      "grad_norm": 0.47975772619247437,
      "learning_rate": 3.687777777777778e-05,
      "loss": 0.0051,
      "step": 23620
    },
    {
      "epoch": 1.3127777777777778,
      "grad_norm": 0.21002161502838135,
      "learning_rate": 3.687222222222223e-05,
      "loss": 0.0055,
      "step": 23630
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.21240632236003876,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0057,
      "step": 23640
    },
    {
      "epoch": 1.3138888888888889,
      "grad_norm": 0.09028783440589905,
      "learning_rate": 3.6861111111111114e-05,
      "loss": 0.0042,
      "step": 23650
    },
    {
      "epoch": 1.3144444444444445,
      "grad_norm": 0.031283359974622726,
      "learning_rate": 3.685555555555556e-05,
      "loss": 0.0035,
      "step": 23660
    },
    {
      "epoch": 1.315,
      "grad_norm": 0.1808439940214157,
      "learning_rate": 3.685e-05,
      "loss": 0.0049,
      "step": 23670
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 0.06086120009422302,
      "learning_rate": 3.6844444444444445e-05,
      "loss": 0.0059,
      "step": 23680
    },
    {
      "epoch": 1.3161111111111112,
      "grad_norm": 0.0901753157377243,
      "learning_rate": 3.683888888888889e-05,
      "loss": 0.0051,
      "step": 23690
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.5711396336555481,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0042,
      "step": 23700
    },
    {
      "epoch": 1.3172222222222223,
      "grad_norm": 0.21680869162082672,
      "learning_rate": 3.6827777777777775e-05,
      "loss": 0.005,
      "step": 23710
    },
    {
      "epoch": 1.3177777777777777,
      "grad_norm": 0.12095708400011063,
      "learning_rate": 3.6822222222222226e-05,
      "loss": 0.0051,
      "step": 23720
    },
    {
      "epoch": 1.3183333333333334,
      "grad_norm": 0.09056810289621353,
      "learning_rate": 3.681666666666667e-05,
      "loss": 0.0055,
      "step": 23730
    },
    {
      "epoch": 1.318888888888889,
      "grad_norm": 0.21440790593624115,
      "learning_rate": 3.681111111111111e-05,
      "loss": 0.0044,
      "step": 23740
    },
    {
      "epoch": 1.3194444444444444,
      "grad_norm": 0.16136661171913147,
      "learning_rate": 3.6805555555555556e-05,
      "loss": 0.0049,
      "step": 23750
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.18162097036838531,
      "learning_rate": 3.68e-05,
      "loss": 0.0035,
      "step": 23760
    },
    {
      "epoch": 1.3205555555555555,
      "grad_norm": 0.08299273997545242,
      "learning_rate": 3.679444444444445e-05,
      "loss": 0.0049,
      "step": 23770
    },
    {
      "epoch": 1.3211111111111111,
      "grad_norm": 0.10645316541194916,
      "learning_rate": 3.678888888888889e-05,
      "loss": 0.0045,
      "step": 23780
    },
    {
      "epoch": 1.3216666666666668,
      "grad_norm": 0.15174360573291779,
      "learning_rate": 3.678333333333334e-05,
      "loss": 0.0048,
      "step": 23790
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 0.2610383629798889,
      "learning_rate": 3.677777777777778e-05,
      "loss": 0.0059,
      "step": 23800
    },
    {
      "epoch": 1.3227777777777778,
      "grad_norm": 0.02503998577594757,
      "learning_rate": 3.6772222222222225e-05,
      "loss": 0.0034,
      "step": 23810
    },
    {
      "epoch": 1.3233333333333333,
      "grad_norm": 0.33825600147247314,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.0066,
      "step": 23820
    },
    {
      "epoch": 1.323888888888889,
      "grad_norm": 0.16514088213443756,
      "learning_rate": 3.676111111111111e-05,
      "loss": 0.005,
      "step": 23830
    },
    {
      "epoch": 1.3244444444444445,
      "grad_norm": 0.014340287074446678,
      "learning_rate": 3.675555555555556e-05,
      "loss": 0.0043,
      "step": 23840
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.36086562275886536,
      "learning_rate": 3.675e-05,
      "loss": 0.0043,
      "step": 23850
    },
    {
      "epoch": 1.3255555555555556,
      "grad_norm": 0.09062854200601578,
      "learning_rate": 3.674444444444445e-05,
      "loss": 0.0029,
      "step": 23860
    },
    {
      "epoch": 1.326111111111111,
      "grad_norm": 0.3447493016719818,
      "learning_rate": 3.673888888888889e-05,
      "loss": 0.0038,
      "step": 23870
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.1621929109096527,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0036,
      "step": 23880
    },
    {
      "epoch": 1.3272222222222223,
      "grad_norm": 0.2403934895992279,
      "learning_rate": 3.672777777777778e-05,
      "loss": 0.0036,
      "step": 23890
    },
    {
      "epoch": 1.3277777777777777,
      "grad_norm": 0.12089088559150696,
      "learning_rate": 3.672222222222222e-05,
      "loss": 0.0049,
      "step": 23900
    },
    {
      "epoch": 1.3283333333333334,
      "grad_norm": 0.2190254181623459,
      "learning_rate": 3.671666666666667e-05,
      "loss": 0.0048,
      "step": 23910
    },
    {
      "epoch": 1.3288888888888888,
      "grad_norm": 0.15146511793136597,
      "learning_rate": 3.671111111111111e-05,
      "loss": 0.0036,
      "step": 23920
    },
    {
      "epoch": 1.3294444444444444,
      "grad_norm": 0.06120070070028305,
      "learning_rate": 3.670555555555556e-05,
      "loss": 0.0045,
      "step": 23930
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.012072276324033737,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0042,
      "step": 23940
    },
    {
      "epoch": 1.3305555555555555,
      "grad_norm": 0.3043018877506256,
      "learning_rate": 3.669444444444445e-05,
      "loss": 0.0052,
      "step": 23950
    },
    {
      "epoch": 1.3311111111111111,
      "grad_norm": 0.03225046768784523,
      "learning_rate": 3.668888888888889e-05,
      "loss": 0.0041,
      "step": 23960
    },
    {
      "epoch": 1.3316666666666666,
      "grad_norm": 0.1211220994591713,
      "learning_rate": 3.6683333333333335e-05,
      "loss": 0.0051,
      "step": 23970
    },
    {
      "epoch": 1.3322222222222222,
      "grad_norm": 0.6895435452461243,
      "learning_rate": 3.667777777777778e-05,
      "loss": 0.0041,
      "step": 23980
    },
    {
      "epoch": 1.3327777777777778,
      "grad_norm": 0.1498558223247528,
      "learning_rate": 3.667222222222222e-05,
      "loss": 0.0038,
      "step": 23990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.09124133735895157,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0039,
      "step": 24000
    },
    {
      "epoch": 1.333888888888889,
      "grad_norm": 0.07613496482372284,
      "learning_rate": 3.6661111111111116e-05,
      "loss": 0.0049,
      "step": 24010
    },
    {
      "epoch": 1.3344444444444443,
      "grad_norm": 0.11012692004442215,
      "learning_rate": 3.665555555555556e-05,
      "loss": 0.0049,
      "step": 24020
    },
    {
      "epoch": 1.335,
      "grad_norm": 0.18559212982654572,
      "learning_rate": 3.665e-05,
      "loss": 0.0056,
      "step": 24030
    },
    {
      "epoch": 1.3355555555555556,
      "grad_norm": 0.15018732845783234,
      "learning_rate": 3.664444444444445e-05,
      "loss": 0.0035,
      "step": 24040
    },
    {
      "epoch": 1.3361111111111112,
      "grad_norm": 0.032825302332639694,
      "learning_rate": 3.663888888888889e-05,
      "loss": 0.0046,
      "step": 24050
    },
    {
      "epoch": 1.3366666666666667,
      "grad_norm": 0.12262396514415741,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.0049,
      "step": 24060
    },
    {
      "epoch": 1.337222222222222,
      "grad_norm": 0.03279798850417137,
      "learning_rate": 3.662777777777778e-05,
      "loss": 0.0045,
      "step": 24070
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 0.21362599730491638,
      "learning_rate": 3.662222222222223e-05,
      "loss": 0.0053,
      "step": 24080
    },
    {
      "epoch": 1.3383333333333334,
      "grad_norm": 0.12078751623630524,
      "learning_rate": 3.6616666666666664e-05,
      "loss": 0.0046,
      "step": 24090
    },
    {
      "epoch": 1.338888888888889,
      "grad_norm": 0.4048788249492645,
      "learning_rate": 3.6611111111111115e-05,
      "loss": 0.0062,
      "step": 24100
    },
    {
      "epoch": 1.3394444444444444,
      "grad_norm": 0.06242160499095917,
      "learning_rate": 3.660555555555556e-05,
      "loss": 0.0043,
      "step": 24110
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.09201256930828094,
      "learning_rate": 3.66e-05,
      "loss": 0.0048,
      "step": 24120
    },
    {
      "epoch": 1.3405555555555555,
      "grad_norm": 0.13679374754428864,
      "learning_rate": 3.6594444444444446e-05,
      "loss": 0.0062,
      "step": 24130
    },
    {
      "epoch": 1.3411111111111111,
      "grad_norm": 0.23398099839687347,
      "learning_rate": 3.658888888888889e-05,
      "loss": 0.0049,
      "step": 24140
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 0.45099836587905884,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.005,
      "step": 24150
    },
    {
      "epoch": 1.3422222222222222,
      "grad_norm": 0.5762039422988892,
      "learning_rate": 3.6577777777777776e-05,
      "loss": 0.0048,
      "step": 24160
    },
    {
      "epoch": 1.3427777777777778,
      "grad_norm": 0.2729668915271759,
      "learning_rate": 3.6572222222222227e-05,
      "loss": 0.0041,
      "step": 24170
    },
    {
      "epoch": 1.3433333333333333,
      "grad_norm": 0.2109767347574234,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.0035,
      "step": 24180
    },
    {
      "epoch": 1.343888888888889,
      "grad_norm": 0.49060648679733276,
      "learning_rate": 3.6561111111111114e-05,
      "loss": 0.0047,
      "step": 24190
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 0.2642473578453064,
      "learning_rate": 3.655555555555556e-05,
      "loss": 0.0035,
      "step": 24200
    },
    {
      "epoch": 1.345,
      "grad_norm": 0.331400066614151,
      "learning_rate": 3.655e-05,
      "loss": 0.0037,
      "step": 24210
    },
    {
      "epoch": 1.3455555555555556,
      "grad_norm": 0.12017911672592163,
      "learning_rate": 3.654444444444445e-05,
      "loss": 0.0042,
      "step": 24220
    },
    {
      "epoch": 1.346111111111111,
      "grad_norm": 0.15025664865970612,
      "learning_rate": 3.653888888888889e-05,
      "loss": 0.0035,
      "step": 24230
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.12032915651798248,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.004,
      "step": 24240
    },
    {
      "epoch": 1.3472222222222223,
      "grad_norm": 0.7211688756942749,
      "learning_rate": 3.6527777777777775e-05,
      "loss": 0.0033,
      "step": 24250
    },
    {
      "epoch": 1.3477777777777777,
      "grad_norm": 0.2408474236726761,
      "learning_rate": 3.6522222222222225e-05,
      "loss": 0.0045,
      "step": 24260
    },
    {
      "epoch": 1.3483333333333334,
      "grad_norm": 0.0844096913933754,
      "learning_rate": 3.651666666666667e-05,
      "loss": 0.0048,
      "step": 24270
    },
    {
      "epoch": 1.3488888888888888,
      "grad_norm": 0.1512269377708435,
      "learning_rate": 3.651111111111111e-05,
      "loss": 0.005,
      "step": 24280
    },
    {
      "epoch": 1.3494444444444444,
      "grad_norm": 0.12144837528467178,
      "learning_rate": 3.650555555555556e-05,
      "loss": 0.0029,
      "step": 24290
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.20963579416275024,
      "learning_rate": 3.65e-05,
      "loss": 0.0048,
      "step": 24300
    },
    {
      "epoch": 1.3505555555555555,
      "grad_norm": 0.03294859826564789,
      "learning_rate": 3.649444444444445e-05,
      "loss": 0.0039,
      "step": 24310
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 0.39462536573410034,
      "learning_rate": 3.648888888888889e-05,
      "loss": 0.0046,
      "step": 24320
    },
    {
      "epoch": 1.3516666666666666,
      "grad_norm": 0.062286458909511566,
      "learning_rate": 3.648333333333334e-05,
      "loss": 0.0039,
      "step": 24330
    },
    {
      "epoch": 1.3522222222222222,
      "grad_norm": 0.23199458420276642,
      "learning_rate": 3.647777777777778e-05,
      "loss": 0.007,
      "step": 24340
    },
    {
      "epoch": 1.3527777777777779,
      "grad_norm": 0.5415925979614258,
      "learning_rate": 3.6472222222222224e-05,
      "loss": 0.0039,
      "step": 24350
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.18249687552452087,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0043,
      "step": 24360
    },
    {
      "epoch": 1.353888888888889,
      "grad_norm": 0.30013248324394226,
      "learning_rate": 3.646111111111111e-05,
      "loss": 0.0045,
      "step": 24370
    },
    {
      "epoch": 1.3544444444444443,
      "grad_norm": 0.2699536383152008,
      "learning_rate": 3.645555555555556e-05,
      "loss": 0.004,
      "step": 24380
    },
    {
      "epoch": 1.355,
      "grad_norm": 0.0188852921128273,
      "learning_rate": 3.645e-05,
      "loss": 0.0037,
      "step": 24390
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.03277987241744995,
      "learning_rate": 3.644444444444445e-05,
      "loss": 0.0034,
      "step": 24400
    },
    {
      "epoch": 1.356111111111111,
      "grad_norm": 0.18027852475643158,
      "learning_rate": 3.643888888888889e-05,
      "loss": 0.0037,
      "step": 24410
    },
    {
      "epoch": 1.3566666666666667,
      "grad_norm": 0.12068644911050797,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0065,
      "step": 24420
    },
    {
      "epoch": 1.357222222222222,
      "grad_norm": 0.20987270772457123,
      "learning_rate": 3.642777777777778e-05,
      "loss": 0.0039,
      "step": 24430
    },
    {
      "epoch": 1.3577777777777778,
      "grad_norm": 0.27679887413978577,
      "learning_rate": 3.642222222222222e-05,
      "loss": 0.005,
      "step": 24440
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.3041653633117676,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.0054,
      "step": 24450
    },
    {
      "epoch": 1.3588888888888888,
      "grad_norm": 0.09096209704875946,
      "learning_rate": 3.641111111111111e-05,
      "loss": 0.0049,
      "step": 24460
    },
    {
      "epoch": 1.3594444444444445,
      "grad_norm": 0.27219158411026,
      "learning_rate": 3.640555555555556e-05,
      "loss": 0.0051,
      "step": 24470
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.4885103106498718,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0057,
      "step": 24480
    },
    {
      "epoch": 1.3605555555555555,
      "grad_norm": 0.4501161575317383,
      "learning_rate": 3.639444444444445e-05,
      "loss": 0.0038,
      "step": 24490
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 0.5159616470336914,
      "learning_rate": 3.638888888888889e-05,
      "loss": 0.0057,
      "step": 24500
    },
    {
      "epoch": 1.3616666666666668,
      "grad_norm": 0.11231417208909988,
      "learning_rate": 3.6383333333333335e-05,
      "loss": 0.0042,
      "step": 24510
    },
    {
      "epoch": 1.3622222222222222,
      "grad_norm": 0.51114422082901,
      "learning_rate": 3.637777777777778e-05,
      "loss": 0.0033,
      "step": 24520
    },
    {
      "epoch": 1.3627777777777776,
      "grad_norm": 0.15050169825553894,
      "learning_rate": 3.637222222222222e-05,
      "loss": 0.004,
      "step": 24530
    },
    {
      "epoch": 1.3633333333333333,
      "grad_norm": 0.1815192997455597,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.0055,
      "step": 24540
    },
    {
      "epoch": 1.363888888888889,
      "grad_norm": 0.474685400724411,
      "learning_rate": 3.6361111111111116e-05,
      "loss": 0.0041,
      "step": 24550
    },
    {
      "epoch": 1.3644444444444446,
      "grad_norm": 0.9455094337463379,
      "learning_rate": 3.635555555555556e-05,
      "loss": 0.0061,
      "step": 24560
    },
    {
      "epoch": 1.365,
      "grad_norm": 0.3905738890171051,
      "learning_rate": 3.635e-05,
      "loss": 0.0052,
      "step": 24570
    },
    {
      "epoch": 1.3655555555555556,
      "grad_norm": 0.3000853657722473,
      "learning_rate": 3.6344444444444446e-05,
      "loss": 0.0038,
      "step": 24580
    },
    {
      "epoch": 1.366111111111111,
      "grad_norm": 0.09070182591676712,
      "learning_rate": 3.633888888888889e-05,
      "loss": 0.0044,
      "step": 24590
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.3002057671546936,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0062,
      "step": 24600
    },
    {
      "epoch": 1.3672222222222223,
      "grad_norm": 0.2101268321275711,
      "learning_rate": 3.632777777777778e-05,
      "loss": 0.0046,
      "step": 24610
    },
    {
      "epoch": 1.3677777777777778,
      "grad_norm": 0.20983392000198364,
      "learning_rate": 3.632222222222223e-05,
      "loss": 0.0062,
      "step": 24620
    },
    {
      "epoch": 1.3683333333333334,
      "grad_norm": 0.21308283507823944,
      "learning_rate": 3.631666666666667e-05,
      "loss": 0.0046,
      "step": 24630
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 0.28366008400917053,
      "learning_rate": 3.6311111111111114e-05,
      "loss": 0.0057,
      "step": 24640
    },
    {
      "epoch": 1.3694444444444445,
      "grad_norm": 0.30202528834342957,
      "learning_rate": 3.630555555555556e-05,
      "loss": 0.0053,
      "step": 24650
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.4526844918727875,
      "learning_rate": 3.63e-05,
      "loss": 0.0049,
      "step": 24660
    },
    {
      "epoch": 1.3705555555555555,
      "grad_norm": 0.3901521563529968,
      "learning_rate": 3.6294444444444445e-05,
      "loss": 0.0058,
      "step": 24670
    },
    {
      "epoch": 1.3711111111111112,
      "grad_norm": 0.420193612575531,
      "learning_rate": 3.628888888888889e-05,
      "loss": 0.0038,
      "step": 24680
    },
    {
      "epoch": 1.3716666666666666,
      "grad_norm": 0.12086823582649231,
      "learning_rate": 3.628333333333334e-05,
      "loss": 0.0047,
      "step": 24690
    },
    {
      "epoch": 1.3722222222222222,
      "grad_norm": 0.21076355874538422,
      "learning_rate": 3.6277777777777776e-05,
      "loss": 0.0056,
      "step": 24700
    },
    {
      "epoch": 1.3727777777777779,
      "grad_norm": 0.03135858103632927,
      "learning_rate": 3.6272222222222226e-05,
      "loss": 0.0054,
      "step": 24710
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.18045161664485931,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0039,
      "step": 24720
    },
    {
      "epoch": 1.373888888888889,
      "grad_norm": 0.42169490456581116,
      "learning_rate": 3.626111111111111e-05,
      "loss": 0.0038,
      "step": 24730
    },
    {
      "epoch": 1.3744444444444444,
      "grad_norm": 0.12042971700429916,
      "learning_rate": 3.625555555555556e-05,
      "loss": 0.0036,
      "step": 24740
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.3616105020046234,
      "learning_rate": 3.625e-05,
      "loss": 0.0041,
      "step": 24750
    },
    {
      "epoch": 1.3755555555555556,
      "grad_norm": 0.30681025981903076,
      "learning_rate": 3.624444444444445e-05,
      "loss": 0.0043,
      "step": 24760
    },
    {
      "epoch": 1.376111111111111,
      "grad_norm": 0.2699255347251892,
      "learning_rate": 3.623888888888889e-05,
      "loss": 0.0036,
      "step": 24770
    },
    {
      "epoch": 1.3766666666666667,
      "grad_norm": 0.2406819760799408,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.0043,
      "step": 24780
    },
    {
      "epoch": 1.3772222222222221,
      "grad_norm": 0.0671515241265297,
      "learning_rate": 3.6227777777777774e-05,
      "loss": 0.0055,
      "step": 24790
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.09002597630023956,
      "learning_rate": 3.6222222222222225e-05,
      "loss": 0.0038,
      "step": 24800
    },
    {
      "epoch": 1.3783333333333334,
      "grad_norm": 0.18767815828323364,
      "learning_rate": 3.621666666666667e-05,
      "loss": 0.0052,
      "step": 24810
    },
    {
      "epoch": 1.3788888888888888,
      "grad_norm": 0.239988312125206,
      "learning_rate": 3.621111111111111e-05,
      "loss": 0.0039,
      "step": 24820
    },
    {
      "epoch": 1.3794444444444445,
      "grad_norm": 0.2176927924156189,
      "learning_rate": 3.620555555555556e-05,
      "loss": 0.0041,
      "step": 24830
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.3598329424858093,
      "learning_rate": 3.62e-05,
      "loss": 0.0037,
      "step": 24840
    },
    {
      "epoch": 1.3805555555555555,
      "grad_norm": 0.062943235039711,
      "learning_rate": 3.619444444444445e-05,
      "loss": 0.0045,
      "step": 24850
    },
    {
      "epoch": 1.3811111111111112,
      "grad_norm": 0.025656808167696,
      "learning_rate": 3.6188888888888886e-05,
      "loss": 0.0041,
      "step": 24860
    },
    {
      "epoch": 1.3816666666666666,
      "grad_norm": 0.12131001055240631,
      "learning_rate": 3.6183333333333336e-05,
      "loss": 0.0056,
      "step": 24870
    },
    {
      "epoch": 1.3822222222222222,
      "grad_norm": 0.12323339283466339,
      "learning_rate": 3.617777777777778e-05,
      "loss": 0.0049,
      "step": 24880
    },
    {
      "epoch": 1.3827777777777777,
      "grad_norm": 0.03179931640625,
      "learning_rate": 3.6172222222222224e-05,
      "loss": 0.0042,
      "step": 24890
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.18017174303531647,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0056,
      "step": 24900
    },
    {
      "epoch": 1.383888888888889,
      "grad_norm": 0.21000871062278748,
      "learning_rate": 3.616111111111111e-05,
      "loss": 0.0046,
      "step": 24910
    },
    {
      "epoch": 1.3844444444444444,
      "grad_norm": 0.5230236649513245,
      "learning_rate": 3.615555555555556e-05,
      "loss": 0.004,
      "step": 24920
    },
    {
      "epoch": 1.385,
      "grad_norm": 0.5101648569107056,
      "learning_rate": 3.615e-05,
      "loss": 0.005,
      "step": 24930
    },
    {
      "epoch": 1.3855555555555554,
      "grad_norm": 0.0904589593410492,
      "learning_rate": 3.614444444444445e-05,
      "loss": 0.004,
      "step": 24940
    },
    {
      "epoch": 1.386111111111111,
      "grad_norm": 0.24785704910755157,
      "learning_rate": 3.613888888888889e-05,
      "loss": 0.0042,
      "step": 24950
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.6003648042678833,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0054,
      "step": 24960
    },
    {
      "epoch": 1.3872222222222224,
      "grad_norm": 0.1832621842622757,
      "learning_rate": 3.612777777777778e-05,
      "loss": 0.0027,
      "step": 24970
    },
    {
      "epoch": 1.3877777777777778,
      "grad_norm": 0.009947185404598713,
      "learning_rate": 3.612222222222222e-05,
      "loss": 0.0051,
      "step": 24980
    },
    {
      "epoch": 1.3883333333333332,
      "grad_norm": 0.16317181289196014,
      "learning_rate": 3.611666666666667e-05,
      "loss": 0.0048,
      "step": 24990
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.30474042892456055,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.0038,
      "step": 25000
    },
    {
      "epoch": 1.3894444444444445,
      "grad_norm": 0.6357131004333496,
      "learning_rate": 3.610555555555556e-05,
      "loss": 0.0053,
      "step": 25010
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.03245263919234276,
      "learning_rate": 3.61e-05,
      "loss": 0.0047,
      "step": 25020
    },
    {
      "epoch": 1.3905555555555555,
      "grad_norm": 0.24033918976783752,
      "learning_rate": 3.609444444444445e-05,
      "loss": 0.0041,
      "step": 25030
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 0.06126031652092934,
      "learning_rate": 3.608888888888889e-05,
      "loss": 0.0031,
      "step": 25040
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 0.2999951243400574,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.0061,
      "step": 25050
    },
    {
      "epoch": 1.3922222222222222,
      "grad_norm": 0.2101423293352127,
      "learning_rate": 3.607777777777778e-05,
      "loss": 0.0044,
      "step": 25060
    },
    {
      "epoch": 1.392777777777778,
      "grad_norm": 0.1500900536775589,
      "learning_rate": 3.607222222222222e-05,
      "loss": 0.0039,
      "step": 25070
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.10925175249576569,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0034,
      "step": 25080
    },
    {
      "epoch": 1.393888888888889,
      "grad_norm": 0.35952627658843994,
      "learning_rate": 3.6061111111111115e-05,
      "loss": 0.0046,
      "step": 25090
    },
    {
      "epoch": 1.3944444444444444,
      "grad_norm": 0.09297166764736176,
      "learning_rate": 3.605555555555556e-05,
      "loss": 0.0041,
      "step": 25100
    },
    {
      "epoch": 1.395,
      "grad_norm": 0.11977779865264893,
      "learning_rate": 3.605e-05,
      "loss": 0.0043,
      "step": 25110
    },
    {
      "epoch": 1.3955555555555557,
      "grad_norm": 0.4188424050807953,
      "learning_rate": 3.6044444444444446e-05,
      "loss": 0.0062,
      "step": 25120
    },
    {
      "epoch": 1.396111111111111,
      "grad_norm": 0.18005354702472687,
      "learning_rate": 3.603888888888889e-05,
      "loss": 0.004,
      "step": 25130
    },
    {
      "epoch": 1.3966666666666667,
      "grad_norm": 0.36130115389823914,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0055,
      "step": 25140
    },
    {
      "epoch": 1.3972222222222221,
      "grad_norm": 0.4812757968902588,
      "learning_rate": 3.6027777777777776e-05,
      "loss": 0.0059,
      "step": 25150
    },
    {
      "epoch": 1.3977777777777778,
      "grad_norm": 0.15141187608242035,
      "learning_rate": 3.602222222222223e-05,
      "loss": 0.0045,
      "step": 25160
    },
    {
      "epoch": 1.3983333333333334,
      "grad_norm": 0.20984192192554474,
      "learning_rate": 3.601666666666667e-05,
      "loss": 0.0032,
      "step": 25170
    },
    {
      "epoch": 1.3988888888888888,
      "grad_norm": 0.24324113130569458,
      "learning_rate": 3.6011111111111114e-05,
      "loss": 0.0047,
      "step": 25180
    },
    {
      "epoch": 1.3994444444444445,
      "grad_norm": 0.20496642589569092,
      "learning_rate": 3.600555555555556e-05,
      "loss": 0.0047,
      "step": 25190
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.24010077118873596,
      "learning_rate": 3.6e-05,
      "loss": 0.004,
      "step": 25200
    },
    {
      "epoch": 1.4005555555555556,
      "grad_norm": 0.299993634223938,
      "learning_rate": 3.5994444444444444e-05,
      "loss": 0.0043,
      "step": 25210
    },
    {
      "epoch": 1.4011111111111112,
      "grad_norm": 0.6311163902282715,
      "learning_rate": 3.598888888888889e-05,
      "loss": 0.0055,
      "step": 25220
    },
    {
      "epoch": 1.4016666666666666,
      "grad_norm": 0.44185900688171387,
      "learning_rate": 3.598333333333334e-05,
      "loss": 0.0049,
      "step": 25230
    },
    {
      "epoch": 1.4022222222222223,
      "grad_norm": 0.5436250567436218,
      "learning_rate": 3.5977777777777775e-05,
      "loss": 0.0035,
      "step": 25240
    },
    {
      "epoch": 1.4027777777777777,
      "grad_norm": 0.2407235950231552,
      "learning_rate": 3.5972222222222225e-05,
      "loss": 0.0027,
      "step": 25250
    },
    {
      "epoch": 1.4033333333333333,
      "grad_norm": 0.04191754758358002,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.0047,
      "step": 25260
    },
    {
      "epoch": 1.403888888888889,
      "grad_norm": 0.21170563995838165,
      "learning_rate": 3.596111111111111e-05,
      "loss": 0.0041,
      "step": 25270
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 0.2572230100631714,
      "learning_rate": 3.5955555555555556e-05,
      "loss": 0.0046,
      "step": 25280
    },
    {
      "epoch": 1.405,
      "grad_norm": 0.32914164662361145,
      "learning_rate": 3.595e-05,
      "loss": 0.0048,
      "step": 25290
    },
    {
      "epoch": 1.4055555555555554,
      "grad_norm": 0.3960896134376526,
      "learning_rate": 3.594444444444445e-05,
      "loss": 0.0046,
      "step": 25300
    },
    {
      "epoch": 1.406111111111111,
      "grad_norm": 0.12245835363864899,
      "learning_rate": 3.593888888888889e-05,
      "loss": 0.0029,
      "step": 25310
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.5459415912628174,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0049,
      "step": 25320
    },
    {
      "epoch": 1.4072222222222222,
      "grad_norm": 0.18022044003009796,
      "learning_rate": 3.5927777777777774e-05,
      "loss": 0.0048,
      "step": 25330
    },
    {
      "epoch": 1.4077777777777778,
      "grad_norm": 0.11986193060874939,
      "learning_rate": 3.5922222222222224e-05,
      "loss": 0.0045,
      "step": 25340
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 0.1901361644268036,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.0031,
      "step": 25350
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.21009017527103424,
      "learning_rate": 3.591111111111111e-05,
      "loss": 0.0028,
      "step": 25360
    },
    {
      "epoch": 1.4094444444444445,
      "grad_norm": 0.48409733176231384,
      "learning_rate": 3.590555555555556e-05,
      "loss": 0.0037,
      "step": 25370
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.39240628480911255,
      "learning_rate": 3.59e-05,
      "loss": 0.0036,
      "step": 25380
    },
    {
      "epoch": 1.4105555555555556,
      "grad_norm": 0.0322253592312336,
      "learning_rate": 3.589444444444445e-05,
      "loss": 0.0047,
      "step": 25390
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 0.4281114637851715,
      "learning_rate": 3.5888888888888886e-05,
      "loss": 0.0038,
      "step": 25400
    },
    {
      "epoch": 1.4116666666666666,
      "grad_norm": 0.1499517858028412,
      "learning_rate": 3.5883333333333336e-05,
      "loss": 0.0046,
      "step": 25410
    },
    {
      "epoch": 1.4122222222222223,
      "grad_norm": 0.12059670686721802,
      "learning_rate": 3.587777777777778e-05,
      "loss": 0.0033,
      "step": 25420
    },
    {
      "epoch": 1.412777777777778,
      "grad_norm": 0.601340115070343,
      "learning_rate": 3.587222222222222e-05,
      "loss": 0.004,
      "step": 25430
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.35986196994781494,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0039,
      "step": 25440
    },
    {
      "epoch": 1.4138888888888888,
      "grad_norm": 0.39051440358161926,
      "learning_rate": 3.586111111111111e-05,
      "loss": 0.0039,
      "step": 25450
    },
    {
      "epoch": 1.4144444444444444,
      "grad_norm": 0.2415795773267746,
      "learning_rate": 3.585555555555556e-05,
      "loss": 0.0047,
      "step": 25460
    },
    {
      "epoch": 1.415,
      "grad_norm": 0.061168372631073,
      "learning_rate": 3.585e-05,
      "loss": 0.004,
      "step": 25470
    },
    {
      "epoch": 1.4155555555555557,
      "grad_norm": 0.33076944947242737,
      "learning_rate": 3.584444444444445e-05,
      "loss": 0.0031,
      "step": 25480
    },
    {
      "epoch": 1.416111111111111,
      "grad_norm": 0.20114921033382416,
      "learning_rate": 3.583888888888889e-05,
      "loss": 0.0043,
      "step": 25490
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.30125415325164795,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0059,
      "step": 25500
    },
    {
      "epoch": 1.4172222222222222,
      "grad_norm": 0.8123350739479065,
      "learning_rate": 3.582777777777778e-05,
      "loss": 0.0037,
      "step": 25510
    },
    {
      "epoch": 1.4177777777777778,
      "grad_norm": 0.03240542858839035,
      "learning_rate": 3.582222222222222e-05,
      "loss": 0.0047,
      "step": 25520
    },
    {
      "epoch": 1.4183333333333334,
      "grad_norm": 0.272103875875473,
      "learning_rate": 3.581666666666667e-05,
      "loss": 0.0029,
      "step": 25530
    },
    {
      "epoch": 1.4188888888888889,
      "grad_norm": 0.009691841900348663,
      "learning_rate": 3.581111111111111e-05,
      "loss": 0.0041,
      "step": 25540
    },
    {
      "epoch": 1.4194444444444445,
      "grad_norm": 0.22285838425159454,
      "learning_rate": 3.580555555555556e-05,
      "loss": 0.0044,
      "step": 25550
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.03204922750592232,
      "learning_rate": 3.58e-05,
      "loss": 0.0055,
      "step": 25560
    },
    {
      "epoch": 1.4205555555555556,
      "grad_norm": 0.1806582510471344,
      "learning_rate": 3.5794444444444446e-05,
      "loss": 0.004,
      "step": 25570
    },
    {
      "epoch": 1.4211111111111112,
      "grad_norm": 0.3897303342819214,
      "learning_rate": 3.578888888888889e-05,
      "loss": 0.005,
      "step": 25580
    },
    {
      "epoch": 1.4216666666666666,
      "grad_norm": 0.23073208332061768,
      "learning_rate": 3.5783333333333333e-05,
      "loss": 0.0047,
      "step": 25590
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.359828382730484,
      "learning_rate": 3.577777777777778e-05,
      "loss": 0.0044,
      "step": 25600
    },
    {
      "epoch": 1.4227777777777777,
      "grad_norm": 0.15036296844482422,
      "learning_rate": 3.577222222222222e-05,
      "loss": 0.0037,
      "step": 25610
    },
    {
      "epoch": 1.4233333333333333,
      "grad_norm": 0.09104164689779282,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.0047,
      "step": 25620
    },
    {
      "epoch": 1.423888888888889,
      "grad_norm": 0.18043307960033417,
      "learning_rate": 3.5761111111111114e-05,
      "loss": 0.0054,
      "step": 25630
    },
    {
      "epoch": 1.4244444444444444,
      "grad_norm": 0.42059674859046936,
      "learning_rate": 3.575555555555556e-05,
      "loss": 0.0038,
      "step": 25640
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.17044000327587128,
      "learning_rate": 3.575e-05,
      "loss": 0.0045,
      "step": 25650
    },
    {
      "epoch": 1.4255555555555555,
      "grad_norm": 0.27005138993263245,
      "learning_rate": 3.5744444444444445e-05,
      "loss": 0.0047,
      "step": 25660
    },
    {
      "epoch": 1.426111111111111,
      "grad_norm": 0.2403116077184677,
      "learning_rate": 3.573888888888889e-05,
      "loss": 0.0042,
      "step": 25670
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.03124343603849411,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0046,
      "step": 25680
    },
    {
      "epoch": 1.4272222222222222,
      "grad_norm": 0.35999423265457153,
      "learning_rate": 3.572777777777778e-05,
      "loss": 0.0041,
      "step": 25690
    },
    {
      "epoch": 1.4277777777777778,
      "grad_norm": 0.4229969382286072,
      "learning_rate": 3.5722222222222226e-05,
      "loss": 0.0044,
      "step": 25700
    },
    {
      "epoch": 1.4283333333333332,
      "grad_norm": 0.15227322280406952,
      "learning_rate": 3.571666666666667e-05,
      "loss": 0.0048,
      "step": 25710
    },
    {
      "epoch": 1.4288888888888889,
      "grad_norm": 0.2746260166168213,
      "learning_rate": 3.571111111111111e-05,
      "loss": 0.004,
      "step": 25720
    },
    {
      "epoch": 1.4294444444444445,
      "grad_norm": 0.17962642014026642,
      "learning_rate": 3.570555555555556e-05,
      "loss": 0.005,
      "step": 25730
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.32900893688201904,
      "learning_rate": 3.57e-05,
      "loss": 0.0056,
      "step": 25740
    },
    {
      "epoch": 1.4305555555555556,
      "grad_norm": 0.21036337316036224,
      "learning_rate": 3.5694444444444444e-05,
      "loss": 0.0061,
      "step": 25750
    },
    {
      "epoch": 1.431111111111111,
      "grad_norm": 0.47948017716407776,
      "learning_rate": 3.568888888888889e-05,
      "loss": 0.0041,
      "step": 25760
    },
    {
      "epoch": 1.4316666666666666,
      "grad_norm": 0.5103442668914795,
      "learning_rate": 3.568333333333334e-05,
      "loss": 0.004,
      "step": 25770
    },
    {
      "epoch": 1.4322222222222223,
      "grad_norm": 0.12135054171085358,
      "learning_rate": 3.567777777777778e-05,
      "loss": 0.0036,
      "step": 25780
    },
    {
      "epoch": 1.4327777777777777,
      "grad_norm": 0.12049391865730286,
      "learning_rate": 3.5672222222222225e-05,
      "loss": 0.004,
      "step": 25790
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.27003180980682373,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0028,
      "step": 25800
    },
    {
      "epoch": 1.4338888888888888,
      "grad_norm": 0.869223952293396,
      "learning_rate": 3.566111111111111e-05,
      "loss": 0.0045,
      "step": 25810
    },
    {
      "epoch": 1.4344444444444444,
      "grad_norm": 0.2434777319431305,
      "learning_rate": 3.5655555555555556e-05,
      "loss": 0.0038,
      "step": 25820
    },
    {
      "epoch": 1.435,
      "grad_norm": 0.5402721166610718,
      "learning_rate": 3.565e-05,
      "loss": 0.0047,
      "step": 25830
    },
    {
      "epoch": 1.4355555555555555,
      "grad_norm": 0.2102363258600235,
      "learning_rate": 3.564444444444445e-05,
      "loss": 0.0042,
      "step": 25840
    },
    {
      "epoch": 1.4361111111111111,
      "grad_norm": 0.13716864585876465,
      "learning_rate": 3.5638888888888886e-05,
      "loss": 0.004,
      "step": 25850
    },
    {
      "epoch": 1.4366666666666665,
      "grad_norm": 0.09049081057310104,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0041,
      "step": 25860
    },
    {
      "epoch": 1.4372222222222222,
      "grad_norm": 0.03354595601558685,
      "learning_rate": 3.562777777777778e-05,
      "loss": 0.0052,
      "step": 25870
    },
    {
      "epoch": 1.4377777777777778,
      "grad_norm": 0.24610693752765656,
      "learning_rate": 3.5622222222222224e-05,
      "loss": 0.0035,
      "step": 25880
    },
    {
      "epoch": 1.4383333333333335,
      "grad_norm": 0.0665186420083046,
      "learning_rate": 3.561666666666667e-05,
      "loss": 0.0044,
      "step": 25890
    },
    {
      "epoch": 1.4388888888888889,
      "grad_norm": 0.3167916536331177,
      "learning_rate": 3.561111111111111e-05,
      "loss": 0.0055,
      "step": 25900
    },
    {
      "epoch": 1.4394444444444445,
      "grad_norm": 0.37643352150917053,
      "learning_rate": 3.560555555555556e-05,
      "loss": 0.0034,
      "step": 25910
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.010513332672417164,
      "learning_rate": 3.56e-05,
      "loss": 0.0036,
      "step": 25920
    },
    {
      "epoch": 1.4405555555555556,
      "grad_norm": 0.24674971401691437,
      "learning_rate": 3.559444444444445e-05,
      "loss": 0.0047,
      "step": 25930
    },
    {
      "epoch": 1.4411111111111112,
      "grad_norm": 0.18067291378974915,
      "learning_rate": 3.5588888888888885e-05,
      "loss": 0.0042,
      "step": 25940
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.09174907952547073,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.0053,
      "step": 25950
    },
    {
      "epoch": 1.4422222222222223,
      "grad_norm": 0.18671029806137085,
      "learning_rate": 3.557777777777778e-05,
      "loss": 0.0049,
      "step": 25960
    },
    {
      "epoch": 1.4427777777777777,
      "grad_norm": 0.2826477587223053,
      "learning_rate": 3.557222222222222e-05,
      "loss": 0.0061,
      "step": 25970
    },
    {
      "epoch": 1.4433333333333334,
      "grad_norm": 0.6660374402999878,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.0045,
      "step": 25980
    },
    {
      "epoch": 1.443888888888889,
      "grad_norm": 0.1800595074892044,
      "learning_rate": 3.556111111111111e-05,
      "loss": 0.0059,
      "step": 25990
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.15248191356658936,
      "learning_rate": 3.555555555555556e-05,
      "loss": 0.0042,
      "step": 26000
    },
    {
      "epoch": 1.445,
      "grad_norm": 0.329341322183609,
      "learning_rate": 3.555e-05,
      "loss": 0.004,
      "step": 26010
    },
    {
      "epoch": 1.4455555555555555,
      "grad_norm": 0.15038305521011353,
      "learning_rate": 3.554444444444445e-05,
      "loss": 0.0052,
      "step": 26020
    },
    {
      "epoch": 1.4461111111111111,
      "grad_norm": 0.030971527099609375,
      "learning_rate": 3.553888888888889e-05,
      "loss": 0.006,
      "step": 26030
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.15206830203533173,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0048,
      "step": 26040
    },
    {
      "epoch": 1.4472222222222222,
      "grad_norm": 0.0325060598552227,
      "learning_rate": 3.5527777777777785e-05,
      "loss": 0.0044,
      "step": 26050
    },
    {
      "epoch": 1.4477777777777778,
      "grad_norm": 0.2516760230064392,
      "learning_rate": 3.552222222222222e-05,
      "loss": 0.0048,
      "step": 26060
    },
    {
      "epoch": 1.4483333333333333,
      "grad_norm": 0.27119019627571106,
      "learning_rate": 3.551666666666667e-05,
      "loss": 0.0044,
      "step": 26070
    },
    {
      "epoch": 1.448888888888889,
      "grad_norm": 0.055344242602586746,
      "learning_rate": 3.551111111111111e-05,
      "loss": 0.0041,
      "step": 26080
    },
    {
      "epoch": 1.4494444444444445,
      "grad_norm": 0.2712351083755493,
      "learning_rate": 3.550555555555556e-05,
      "loss": 0.0046,
      "step": 26090
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.4202428162097931,
      "learning_rate": 3.55e-05,
      "loss": 0.0038,
      "step": 26100
    },
    {
      "epoch": 1.4505555555555556,
      "grad_norm": 0.480817586183548,
      "learning_rate": 3.5494444444444446e-05,
      "loss": 0.0028,
      "step": 26110
    },
    {
      "epoch": 1.451111111111111,
      "grad_norm": 0.04707828536629677,
      "learning_rate": 3.548888888888889e-05,
      "loss": 0.0064,
      "step": 26120
    },
    {
      "epoch": 1.4516666666666667,
      "grad_norm": 0.15268997848033905,
      "learning_rate": 3.548333333333333e-05,
      "loss": 0.005,
      "step": 26130
    },
    {
      "epoch": 1.4522222222222223,
      "grad_norm": 0.060593705624341965,
      "learning_rate": 3.547777777777778e-05,
      "loss": 0.0042,
      "step": 26140
    },
    {
      "epoch": 1.4527777777777777,
      "grad_norm": 0.060339752584695816,
      "learning_rate": 3.547222222222222e-05,
      "loss": 0.004,
      "step": 26150
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.15209107100963593,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0037,
      "step": 26160
    },
    {
      "epoch": 1.4538888888888888,
      "grad_norm": 0.21006903052330017,
      "learning_rate": 3.5461111111111114e-05,
      "loss": 0.0045,
      "step": 26170
    },
    {
      "epoch": 1.4544444444444444,
      "grad_norm": 0.3002832233905792,
      "learning_rate": 3.545555555555556e-05,
      "loss": 0.0035,
      "step": 26180
    },
    {
      "epoch": 1.455,
      "grad_norm": 0.12673144042491913,
      "learning_rate": 3.545e-05,
      "loss": 0.003,
      "step": 26190
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 0.5400736331939697,
      "learning_rate": 3.5444444444444445e-05,
      "loss": 0.0032,
      "step": 26200
    },
    {
      "epoch": 1.4561111111111111,
      "grad_norm": 0.2270229160785675,
      "learning_rate": 3.543888888888889e-05,
      "loss": 0.0049,
      "step": 26210
    },
    {
      "epoch": 1.4566666666666666,
      "grad_norm": 0.16946502029895782,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.0032,
      "step": 26220
    },
    {
      "epoch": 1.4572222222222222,
      "grad_norm": 0.06173224747180939,
      "learning_rate": 3.542777777777778e-05,
      "loss": 0.0043,
      "step": 26230
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 0.24023331701755524,
      "learning_rate": 3.5422222222222226e-05,
      "loss": 0.0032,
      "step": 26240
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.06033406779170036,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.0036,
      "step": 26250
    },
    {
      "epoch": 1.458888888888889,
      "grad_norm": 0.1503380388021469,
      "learning_rate": 3.541111111111111e-05,
      "loss": 0.0029,
      "step": 26260
    },
    {
      "epoch": 1.4594444444444443,
      "grad_norm": 0.20986375212669373,
      "learning_rate": 3.5405555555555556e-05,
      "loss": 0.0054,
      "step": 26270
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.5106261968612671,
      "learning_rate": 3.54e-05,
      "loss": 0.0041,
      "step": 26280
    },
    {
      "epoch": 1.4605555555555556,
      "grad_norm": 0.30084577202796936,
      "learning_rate": 3.5394444444444443e-05,
      "loss": 0.0038,
      "step": 26290
    },
    {
      "epoch": 1.4611111111111112,
      "grad_norm": 0.03298003971576691,
      "learning_rate": 3.538888888888889e-05,
      "loss": 0.005,
      "step": 26300
    },
    {
      "epoch": 1.4616666666666667,
      "grad_norm": 0.12148044258356094,
      "learning_rate": 3.538333333333334e-05,
      "loss": 0.0039,
      "step": 26310
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 0.33049526810646057,
      "learning_rate": 3.537777777777778e-05,
      "loss": 0.004,
      "step": 26320
    },
    {
      "epoch": 1.4627777777777777,
      "grad_norm": 0.3600306212902069,
      "learning_rate": 3.5372222222222224e-05,
      "loss": 0.0033,
      "step": 26330
    },
    {
      "epoch": 1.4633333333333334,
      "grad_norm": 0.41516366600990295,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0045,
      "step": 26340
    },
    {
      "epoch": 1.463888888888889,
      "grad_norm": 0.12136664241552353,
      "learning_rate": 3.536111111111111e-05,
      "loss": 0.0051,
      "step": 26350
    },
    {
      "epoch": 1.4644444444444444,
      "grad_norm": 0.12022405117750168,
      "learning_rate": 3.5355555555555555e-05,
      "loss": 0.0039,
      "step": 26360
    },
    {
      "epoch": 1.465,
      "grad_norm": 0.33153867721557617,
      "learning_rate": 3.535e-05,
      "loss": 0.0054,
      "step": 26370
    },
    {
      "epoch": 1.4655555555555555,
      "grad_norm": 0.11990807950496674,
      "learning_rate": 3.534444444444445e-05,
      "loss": 0.0045,
      "step": 26380
    },
    {
      "epoch": 1.4661111111111111,
      "grad_norm": 0.5642874240875244,
      "learning_rate": 3.5338888888888886e-05,
      "loss": 0.0046,
      "step": 26390
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.13482634723186493,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0038,
      "step": 26400
    },
    {
      "epoch": 1.4672222222222222,
      "grad_norm": 0.17987209558486938,
      "learning_rate": 3.532777777777778e-05,
      "loss": 0.0039,
      "step": 26410
    },
    {
      "epoch": 1.4677777777777778,
      "grad_norm": 0.011977952904999256,
      "learning_rate": 3.532222222222222e-05,
      "loss": 0.0033,
      "step": 26420
    },
    {
      "epoch": 1.4683333333333333,
      "grad_norm": 0.6042604446411133,
      "learning_rate": 3.531666666666667e-05,
      "loss": 0.0035,
      "step": 26430
    },
    {
      "epoch": 1.468888888888889,
      "grad_norm": 0.3901567757129669,
      "learning_rate": 3.531111111111111e-05,
      "loss": 0.0048,
      "step": 26440
    },
    {
      "epoch": 1.4694444444444446,
      "grad_norm": 0.46496447920799255,
      "learning_rate": 3.530555555555556e-05,
      "loss": 0.0033,
      "step": 26450
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.47919416427612305,
      "learning_rate": 3.53e-05,
      "loss": 0.0052,
      "step": 26460
    },
    {
      "epoch": 1.4705555555555556,
      "grad_norm": 0.09057509899139404,
      "learning_rate": 3.529444444444445e-05,
      "loss": 0.005,
      "step": 26470
    },
    {
      "epoch": 1.471111111111111,
      "grad_norm": 0.1204821914434433,
      "learning_rate": 3.528888888888889e-05,
      "loss": 0.0039,
      "step": 26480
    },
    {
      "epoch": 1.4716666666666667,
      "grad_norm": 0.21902450919151306,
      "learning_rate": 3.5283333333333335e-05,
      "loss": 0.0056,
      "step": 26490
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.09017330408096313,
      "learning_rate": 3.527777777777778e-05,
      "loss": 0.0043,
      "step": 26500
    },
    {
      "epoch": 1.4727777777777777,
      "grad_norm": 0.41993847489356995,
      "learning_rate": 3.527222222222222e-05,
      "loss": 0.0051,
      "step": 26510
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.23580004274845123,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0052,
      "step": 26520
    },
    {
      "epoch": 1.4738888888888888,
      "grad_norm": 0.06121883541345596,
      "learning_rate": 3.526111111111111e-05,
      "loss": 0.0046,
      "step": 26530
    },
    {
      "epoch": 1.4744444444444444,
      "grad_norm": 0.013696039095520973,
      "learning_rate": 3.525555555555556e-05,
      "loss": 0.0035,
      "step": 26540
    },
    {
      "epoch": 1.475,
      "grad_norm": 0.36276692152023315,
      "learning_rate": 3.525e-05,
      "loss": 0.0032,
      "step": 26550
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 0.19750267267227173,
      "learning_rate": 3.5244444444444447e-05,
      "loss": 0.0041,
      "step": 26560
    },
    {
      "epoch": 1.4761111111111112,
      "grad_norm": 0.2469668835401535,
      "learning_rate": 3.523888888888889e-05,
      "loss": 0.0062,
      "step": 26570
    },
    {
      "epoch": 1.4766666666666666,
      "grad_norm": 0.19661645591259003,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.0045,
      "step": 26580
    },
    {
      "epoch": 1.4772222222222222,
      "grad_norm": 0.2100229263305664,
      "learning_rate": 3.5227777777777784e-05,
      "loss": 0.0065,
      "step": 26590
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 0.06075211986899376,
      "learning_rate": 3.522222222222222e-05,
      "loss": 0.0035,
      "step": 26600
    },
    {
      "epoch": 1.4783333333333333,
      "grad_norm": 0.20091724395751953,
      "learning_rate": 3.521666666666667e-05,
      "loss": 0.004,
      "step": 26610
    },
    {
      "epoch": 1.478888888888889,
      "grad_norm": 0.1547505408525467,
      "learning_rate": 3.5211111111111115e-05,
      "loss": 0.0023,
      "step": 26620
    },
    {
      "epoch": 1.4794444444444443,
      "grad_norm": 0.09295697510242462,
      "learning_rate": 3.520555555555556e-05,
      "loss": 0.0041,
      "step": 26630
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.12022636085748672,
      "learning_rate": 3.52e-05,
      "loss": 0.0043,
      "step": 26640
    },
    {
      "epoch": 1.4805555555555556,
      "grad_norm": 0.5506249070167542,
      "learning_rate": 3.5194444444444445e-05,
      "loss": 0.0036,
      "step": 26650
    },
    {
      "epoch": 1.481111111111111,
      "grad_norm": 0.13369308412075043,
      "learning_rate": 3.518888888888889e-05,
      "loss": 0.004,
      "step": 26660
    },
    {
      "epoch": 1.4816666666666667,
      "grad_norm": 0.14998090267181396,
      "learning_rate": 3.518333333333333e-05,
      "loss": 0.0037,
      "step": 26670
    },
    {
      "epoch": 1.482222222222222,
      "grad_norm": 0.033520568162202835,
      "learning_rate": 3.517777777777778e-05,
      "loss": 0.0046,
      "step": 26680
    },
    {
      "epoch": 1.4827777777777778,
      "grad_norm": 0.0337255522608757,
      "learning_rate": 3.5172222222222226e-05,
      "loss": 0.0036,
      "step": 26690
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.5392868518829346,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0043,
      "step": 26700
    },
    {
      "epoch": 1.4838888888888888,
      "grad_norm": 0.06149722635746002,
      "learning_rate": 3.5161111111111113e-05,
      "loss": 0.0041,
      "step": 26710
    },
    {
      "epoch": 1.4844444444444445,
      "grad_norm": 0.4259171783924103,
      "learning_rate": 3.515555555555556e-05,
      "loss": 0.0042,
      "step": 26720
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 0.24017299711704254,
      "learning_rate": 3.515e-05,
      "loss": 0.0055,
      "step": 26730
    },
    {
      "epoch": 1.4855555555555555,
      "grad_norm": 0.21002180874347687,
      "learning_rate": 3.5144444444444444e-05,
      "loss": 0.0054,
      "step": 26740
    },
    {
      "epoch": 1.4861111111111112,
      "grad_norm": 0.12002212554216385,
      "learning_rate": 3.513888888888889e-05,
      "loss": 0.0038,
      "step": 26750
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.2700730264186859,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0032,
      "step": 26760
    },
    {
      "epoch": 1.4872222222222222,
      "grad_norm": 0.2698787450790405,
      "learning_rate": 3.512777777777778e-05,
      "loss": 0.0035,
      "step": 26770
    },
    {
      "epoch": 1.4877777777777776,
      "grad_norm": 0.33016204833984375,
      "learning_rate": 3.5122222222222225e-05,
      "loss": 0.0041,
      "step": 26780
    },
    {
      "epoch": 1.4883333333333333,
      "grad_norm": 0.13727207481861115,
      "learning_rate": 3.511666666666667e-05,
      "loss": 0.0044,
      "step": 26790
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.21662801504135132,
      "learning_rate": 3.511111111111111e-05,
      "loss": 0.0041,
      "step": 26800
    },
    {
      "epoch": 1.4894444444444446,
      "grad_norm": 0.27064356207847595,
      "learning_rate": 3.5105555555555556e-05,
      "loss": 0.0034,
      "step": 26810
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.09120820462703705,
      "learning_rate": 3.51e-05,
      "loss": 0.0053,
      "step": 26820
    },
    {
      "epoch": 1.4905555555555556,
      "grad_norm": 0.5361618995666504,
      "learning_rate": 3.509444444444445e-05,
      "loss": 0.0048,
      "step": 26830
    },
    {
      "epoch": 1.491111111111111,
      "grad_norm": 0.16206884384155273,
      "learning_rate": 3.5088888888888886e-05,
      "loss": 0.0027,
      "step": 26840
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.15132775902748108,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.0033,
      "step": 26850
    },
    {
      "epoch": 1.4922222222222223,
      "grad_norm": 0.35981929302215576,
      "learning_rate": 3.507777777777778e-05,
      "loss": 0.0034,
      "step": 26860
    },
    {
      "epoch": 1.4927777777777778,
      "grad_norm": 0.12130703777074814,
      "learning_rate": 3.5072222222222224e-05,
      "loss": 0.0044,
      "step": 26870
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.6545619368553162,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0051,
      "step": 26880
    },
    {
      "epoch": 1.4938888888888888,
      "grad_norm": 0.10641730576753616,
      "learning_rate": 3.506111111111111e-05,
      "loss": 0.0052,
      "step": 26890
    },
    {
      "epoch": 1.4944444444444445,
      "grad_norm": 0.18031956255435944,
      "learning_rate": 3.505555555555556e-05,
      "loss": 0.0037,
      "step": 26900
    },
    {
      "epoch": 1.495,
      "grad_norm": 0.3594001531600952,
      "learning_rate": 3.505e-05,
      "loss": 0.0051,
      "step": 26910
    },
    {
      "epoch": 1.4955555555555555,
      "grad_norm": 0.32921844720840454,
      "learning_rate": 3.504444444444445e-05,
      "loss": 0.0035,
      "step": 26920
    },
    {
      "epoch": 1.4961111111111112,
      "grad_norm": 0.062399160116910934,
      "learning_rate": 3.503888888888889e-05,
      "loss": 0.0039,
      "step": 26930
    },
    {
      "epoch": 1.4966666666666666,
      "grad_norm": 0.21135644614696503,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.0047,
      "step": 26940
    },
    {
      "epoch": 1.4972222222222222,
      "grad_norm": 0.06052582710981369,
      "learning_rate": 3.502777777777778e-05,
      "loss": 0.0042,
      "step": 26950
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 0.3245818614959717,
      "learning_rate": 3.502222222222222e-05,
      "loss": 0.0047,
      "step": 26960
    },
    {
      "epoch": 1.4983333333333333,
      "grad_norm": 0.01511509157717228,
      "learning_rate": 3.501666666666667e-05,
      "loss": 0.0042,
      "step": 26970
    },
    {
      "epoch": 1.498888888888889,
      "grad_norm": 0.6286068558692932,
      "learning_rate": 3.501111111111111e-05,
      "loss": 0.0035,
      "step": 26980
    },
    {
      "epoch": 1.4994444444444444,
      "grad_norm": 0.24221806228160858,
      "learning_rate": 3.500555555555556e-05,
      "loss": 0.0039,
      "step": 26990
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.061624471098184586,
      "learning_rate": 3.5e-05,
      "loss": 0.0046,
      "step": 27000
    },
    {
      "epoch": 1.5005555555555556,
      "grad_norm": 0.2866378128528595,
      "learning_rate": 3.499444444444445e-05,
      "loss": 0.0035,
      "step": 27010
    },
    {
      "epoch": 1.501111111111111,
      "grad_norm": 0.17953218519687653,
      "learning_rate": 3.498888888888889e-05,
      "loss": 0.0043,
      "step": 27020
    },
    {
      "epoch": 1.5016666666666667,
      "grad_norm": 0.09071514010429382,
      "learning_rate": 3.4983333333333334e-05,
      "loss": 0.0048,
      "step": 27030
    },
    {
      "epoch": 1.5022222222222221,
      "grad_norm": 0.060901764780282974,
      "learning_rate": 3.4977777777777785e-05,
      "loss": 0.005,
      "step": 27040
    },
    {
      "epoch": 1.5027777777777778,
      "grad_norm": 0.2099105715751648,
      "learning_rate": 3.497222222222222e-05,
      "loss": 0.0036,
      "step": 27050
    },
    {
      "epoch": 1.5033333333333334,
      "grad_norm": 0.3532468378543854,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0038,
      "step": 27060
    },
    {
      "epoch": 1.503888888888889,
      "grad_norm": 0.23899924755096436,
      "learning_rate": 3.496111111111111e-05,
      "loss": 0.0048,
      "step": 27070
    },
    {
      "epoch": 1.5044444444444445,
      "grad_norm": 0.2629682123661041,
      "learning_rate": 3.495555555555556e-05,
      "loss": 0.0035,
      "step": 27080
    },
    {
      "epoch": 1.505,
      "grad_norm": 0.16569910943508148,
      "learning_rate": 3.495e-05,
      "loss": 0.0035,
      "step": 27090
    },
    {
      "epoch": 1.5055555555555555,
      "grad_norm": 0.18030570447444916,
      "learning_rate": 3.4944444444444446e-05,
      "loss": 0.0044,
      "step": 27100
    },
    {
      "epoch": 1.5061111111111112,
      "grad_norm": 0.36099937558174133,
      "learning_rate": 3.4938888888888896e-05,
      "loss": 0.0027,
      "step": 27110
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.047248996794223785,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0044,
      "step": 27120
    },
    {
      "epoch": 1.5072222222222222,
      "grad_norm": 0.36195385456085205,
      "learning_rate": 3.4927777777777783e-05,
      "loss": 0.0028,
      "step": 27130
    },
    {
      "epoch": 1.5077777777777777,
      "grad_norm": 0.185667023062706,
      "learning_rate": 3.492222222222222e-05,
      "loss": 0.0042,
      "step": 27140
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 0.47994011640548706,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.0037,
      "step": 27150
    },
    {
      "epoch": 1.508888888888889,
      "grad_norm": 0.20918604731559753,
      "learning_rate": 3.4911111111111114e-05,
      "loss": 0.0041,
      "step": 27160
    },
    {
      "epoch": 1.5094444444444446,
      "grad_norm": 0.4196106195449829,
      "learning_rate": 3.490555555555556e-05,
      "loss": 0.0041,
      "step": 27170
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.03480652719736099,
      "learning_rate": 3.49e-05,
      "loss": 0.0029,
      "step": 27180
    },
    {
      "epoch": 1.5105555555555554,
      "grad_norm": 0.12879173457622528,
      "learning_rate": 3.4894444444444445e-05,
      "loss": 0.0051,
      "step": 27190
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.31797146797180176,
      "learning_rate": 3.4888888888888895e-05,
      "loss": 0.005,
      "step": 27200
    },
    {
      "epoch": 1.5116666666666667,
      "grad_norm": 0.3592004179954529,
      "learning_rate": 3.488333333333333e-05,
      "loss": 0.0061,
      "step": 27210
    },
    {
      "epoch": 1.5122222222222224,
      "grad_norm": 0.17969879508018494,
      "learning_rate": 3.487777777777778e-05,
      "loss": 0.0038,
      "step": 27220
    },
    {
      "epoch": 1.5127777777777778,
      "grad_norm": 0.18162666261196136,
      "learning_rate": 3.4872222222222226e-05,
      "loss": 0.004,
      "step": 27230
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.15020574629306793,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.004,
      "step": 27240
    },
    {
      "epoch": 1.5138888888888888,
      "grad_norm": 0.18002542853355408,
      "learning_rate": 3.486111111111111e-05,
      "loss": 0.0052,
      "step": 27250
    },
    {
      "epoch": 1.5144444444444445,
      "grad_norm": 0.3017522096633911,
      "learning_rate": 3.4855555555555557e-05,
      "loss": 0.0049,
      "step": 27260
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 0.06136656180024147,
      "learning_rate": 3.485e-05,
      "loss": 0.0022,
      "step": 27270
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.6299499869346619,
      "learning_rate": 3.4844444444444444e-05,
      "loss": 0.0057,
      "step": 27280
    },
    {
      "epoch": 1.516111111111111,
      "grad_norm": 0.1817210614681244,
      "learning_rate": 3.4838888888888894e-05,
      "loss": 0.0053,
      "step": 27290
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.12089301645755768,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.006,
      "step": 27300
    },
    {
      "epoch": 1.5172222222222222,
      "grad_norm": 0.28320619463920593,
      "learning_rate": 3.482777777777778e-05,
      "loss": 0.005,
      "step": 27310
    },
    {
      "epoch": 1.517777777777778,
      "grad_norm": 0.014137191697955132,
      "learning_rate": 3.4822222222222225e-05,
      "loss": 0.0054,
      "step": 27320
    },
    {
      "epoch": 1.5183333333333333,
      "grad_norm": 0.014610910788178444,
      "learning_rate": 3.481666666666667e-05,
      "loss": 0.0041,
      "step": 27330
    },
    {
      "epoch": 1.5188888888888887,
      "grad_norm": 0.14965741336345673,
      "learning_rate": 3.481111111111111e-05,
      "loss": 0.0039,
      "step": 27340
    },
    {
      "epoch": 1.5194444444444444,
      "grad_norm": 0.4773184061050415,
      "learning_rate": 3.4805555555555555e-05,
      "loss": 0.0034,
      "step": 27350
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.0408589243888855,
      "learning_rate": 3.48e-05,
      "loss": 0.0052,
      "step": 27360
    },
    {
      "epoch": 1.5205555555555557,
      "grad_norm": 0.4627936780452728,
      "learning_rate": 3.479444444444445e-05,
      "loss": 0.005,
      "step": 27370
    },
    {
      "epoch": 1.521111111111111,
      "grad_norm": 0.44563931226730347,
      "learning_rate": 3.478888888888889e-05,
      "loss": 0.0064,
      "step": 27380
    },
    {
      "epoch": 1.5216666666666665,
      "grad_norm": 0.3591277599334717,
      "learning_rate": 3.4783333333333336e-05,
      "loss": 0.0038,
      "step": 27390
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 0.06242137402296066,
      "learning_rate": 3.477777777777778e-05,
      "loss": 0.0051,
      "step": 27400
    },
    {
      "epoch": 1.5227777777777778,
      "grad_norm": 0.2584361433982849,
      "learning_rate": 3.4772222222222223e-05,
      "loss": 0.0047,
      "step": 27410
    },
    {
      "epoch": 1.5233333333333334,
      "grad_norm": 0.2097959667444229,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0024,
      "step": 27420
    },
    {
      "epoch": 1.5238888888888888,
      "grad_norm": 0.21000735461711884,
      "learning_rate": 3.476111111111111e-05,
      "loss": 0.0029,
      "step": 27430
    },
    {
      "epoch": 1.5244444444444445,
      "grad_norm": 0.20738033950328827,
      "learning_rate": 3.475555555555556e-05,
      "loss": 0.003,
      "step": 27440
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.21140436828136444,
      "learning_rate": 3.475e-05,
      "loss": 0.0044,
      "step": 27450
    },
    {
      "epoch": 1.5255555555555556,
      "grad_norm": 0.24058009684085846,
      "learning_rate": 3.474444444444445e-05,
      "loss": 0.0028,
      "step": 27460
    },
    {
      "epoch": 1.5261111111111112,
      "grad_norm": 0.20263414084911346,
      "learning_rate": 3.473888888888889e-05,
      "loss": 0.0034,
      "step": 27470
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.29993942379951477,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0047,
      "step": 27480
    },
    {
      "epoch": 1.5272222222222223,
      "grad_norm": 0.35839617252349854,
      "learning_rate": 3.472777777777778e-05,
      "loss": 0.0044,
      "step": 27490
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.034396782517433167,
      "learning_rate": 3.472222222222222e-05,
      "loss": 0.0045,
      "step": 27500
    },
    {
      "epoch": 1.5283333333333333,
      "grad_norm": 0.3301898241043091,
      "learning_rate": 3.471666666666667e-05,
      "loss": 0.0043,
      "step": 27510
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 0.20949788391590118,
      "learning_rate": 3.471111111111111e-05,
      "loss": 0.0038,
      "step": 27520
    },
    {
      "epoch": 1.5294444444444446,
      "grad_norm": 0.391179621219635,
      "learning_rate": 3.470555555555556e-05,
      "loss": 0.0043,
      "step": 27530
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.07429540157318115,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0056,
      "step": 27540
    },
    {
      "epoch": 1.5305555555555554,
      "grad_norm": 0.21045635640621185,
      "learning_rate": 3.469444444444445e-05,
      "loss": 0.0044,
      "step": 27550
    },
    {
      "epoch": 1.531111111111111,
      "grad_norm": 0.27015525102615356,
      "learning_rate": 3.468888888888889e-05,
      "loss": 0.0028,
      "step": 27560
    },
    {
      "epoch": 1.5316666666666667,
      "grad_norm": 0.2697644829750061,
      "learning_rate": 3.4683333333333334e-05,
      "loss": 0.005,
      "step": 27570
    },
    {
      "epoch": 1.5322222222222224,
      "grad_norm": 0.2972254157066345,
      "learning_rate": 3.4677777777777784e-05,
      "loss": 0.0041,
      "step": 27580
    },
    {
      "epoch": 1.5327777777777778,
      "grad_norm": 0.1267891228199005,
      "learning_rate": 3.467222222222222e-05,
      "loss": 0.0056,
      "step": 27590
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.12198171019554138,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0041,
      "step": 27600
    },
    {
      "epoch": 1.5338888888888889,
      "grad_norm": 0.239716038107872,
      "learning_rate": 3.466111111111111e-05,
      "loss": 0.0057,
      "step": 27610
    },
    {
      "epoch": 1.5344444444444445,
      "grad_norm": 0.15127350389957428,
      "learning_rate": 3.465555555555556e-05,
      "loss": 0.0039,
      "step": 27620
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 0.3679884076118469,
      "learning_rate": 3.465e-05,
      "loss": 0.0045,
      "step": 27630
    },
    {
      "epoch": 1.5355555555555556,
      "grad_norm": 0.44859758019447327,
      "learning_rate": 3.4644444444444446e-05,
      "loss": 0.0046,
      "step": 27640
    },
    {
      "epoch": 1.536111111111111,
      "grad_norm": 0.5101779103279114,
      "learning_rate": 3.4638888888888896e-05,
      "loss": 0.003,
      "step": 27650
    },
    {
      "epoch": 1.5366666666666666,
      "grad_norm": 0.4201697111129761,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0036,
      "step": 27660
    },
    {
      "epoch": 1.5372222222222223,
      "grad_norm": 0.24887369573116302,
      "learning_rate": 3.462777777777778e-05,
      "loss": 0.0041,
      "step": 27670
    },
    {
      "epoch": 1.537777777777778,
      "grad_norm": 0.1199464499950409,
      "learning_rate": 3.462222222222222e-05,
      "loss": 0.0037,
      "step": 27680
    },
    {
      "epoch": 1.5383333333333333,
      "grad_norm": 0.3448956310749054,
      "learning_rate": 3.461666666666667e-05,
      "loss": 0.0045,
      "step": 27690
    },
    {
      "epoch": 1.5388888888888888,
      "grad_norm": 0.3313080370426178,
      "learning_rate": 3.4611111111111114e-05,
      "loss": 0.005,
      "step": 27700
    },
    {
      "epoch": 1.5394444444444444,
      "grad_norm": 0.20960474014282227,
      "learning_rate": 3.460555555555556e-05,
      "loss": 0.0028,
      "step": 27710
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.509034276008606,
      "learning_rate": 3.46e-05,
      "loss": 0.0049,
      "step": 27720
    },
    {
      "epoch": 1.5405555555555557,
      "grad_norm": 0.34723368287086487,
      "learning_rate": 3.4594444444444444e-05,
      "loss": 0.0036,
      "step": 27730
    },
    {
      "epoch": 1.541111111111111,
      "grad_norm": 0.583177387714386,
      "learning_rate": 3.4588888888888895e-05,
      "loss": 0.0041,
      "step": 27740
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.2438124418258667,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.0052,
      "step": 27750
    },
    {
      "epoch": 1.5422222222222222,
      "grad_norm": 0.5076531171798706,
      "learning_rate": 3.457777777777778e-05,
      "loss": 0.0051,
      "step": 27760
    },
    {
      "epoch": 1.5427777777777778,
      "grad_norm": 0.2637230455875397,
      "learning_rate": 3.4572222222222225e-05,
      "loss": 0.0054,
      "step": 27770
    },
    {
      "epoch": 1.5433333333333334,
      "grad_norm": 0.360351026058197,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0034,
      "step": 27780
    },
    {
      "epoch": 1.5438888888888889,
      "grad_norm": 0.1516493260860443,
      "learning_rate": 3.456111111111111e-05,
      "loss": 0.005,
      "step": 27790
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 0.6171427369117737,
      "learning_rate": 3.4555555555555556e-05,
      "loss": 0.0036,
      "step": 27800
    },
    {
      "epoch": 1.545,
      "grad_norm": 0.21084868907928467,
      "learning_rate": 3.455e-05,
      "loss": 0.0043,
      "step": 27810
    },
    {
      "epoch": 1.5455555555555556,
      "grad_norm": 0.09779105335474014,
      "learning_rate": 3.454444444444444e-05,
      "loss": 0.0058,
      "step": 27820
    },
    {
      "epoch": 1.5461111111111112,
      "grad_norm": 0.38962236046791077,
      "learning_rate": 3.4538888888888893e-05,
      "loss": 0.0048,
      "step": 27830
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.03133800998330116,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0048,
      "step": 27840
    },
    {
      "epoch": 1.5472222222222223,
      "grad_norm": 0.36028432846069336,
      "learning_rate": 3.452777777777778e-05,
      "loss": 0.0044,
      "step": 27850
    },
    {
      "epoch": 1.5477777777777777,
      "grad_norm": 0.06147151440382004,
      "learning_rate": 3.4522222222222224e-05,
      "loss": 0.0038,
      "step": 27860
    },
    {
      "epoch": 1.5483333333333333,
      "grad_norm": 0.24021750688552856,
      "learning_rate": 3.451666666666667e-05,
      "loss": 0.0053,
      "step": 27870
    },
    {
      "epoch": 1.548888888888889,
      "grad_norm": 0.014140164479613304,
      "learning_rate": 3.451111111111111e-05,
      "loss": 0.0033,
      "step": 27880
    },
    {
      "epoch": 1.5494444444444444,
      "grad_norm": 0.2099263072013855,
      "learning_rate": 3.4505555555555555e-05,
      "loss": 0.0041,
      "step": 27890
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.09026055037975311,
      "learning_rate": 3.45e-05,
      "loss": 0.0058,
      "step": 27900
    },
    {
      "epoch": 1.5505555555555555,
      "grad_norm": 0.12066473066806793,
      "learning_rate": 3.449444444444445e-05,
      "loss": 0.0049,
      "step": 27910
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 0.03284156322479248,
      "learning_rate": 3.448888888888889e-05,
      "loss": 0.0049,
      "step": 27920
    },
    {
      "epoch": 1.5516666666666667,
      "grad_norm": 0.4500739574432373,
      "learning_rate": 3.4483333333333336e-05,
      "loss": 0.0042,
      "step": 27930
    },
    {
      "epoch": 1.5522222222222222,
      "grad_norm": 0.12018651515245438,
      "learning_rate": 3.447777777777778e-05,
      "loss": 0.0036,
      "step": 27940
    },
    {
      "epoch": 1.5527777777777778,
      "grad_norm": 0.38894811272621155,
      "learning_rate": 3.447222222222222e-05,
      "loss": 0.0038,
      "step": 27950
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.7186673283576965,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0034,
      "step": 27960
    },
    {
      "epoch": 1.5538888888888889,
      "grad_norm": 0.28077730536460876,
      "learning_rate": 3.446111111111111e-05,
      "loss": 0.004,
      "step": 27970
    },
    {
      "epoch": 1.5544444444444445,
      "grad_norm": 0.5389947295188904,
      "learning_rate": 3.445555555555556e-05,
      "loss": 0.0053,
      "step": 27980
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 0.3291841447353363,
      "learning_rate": 3.445e-05,
      "loss": 0.0035,
      "step": 27990
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.1526215523481369,
      "learning_rate": 3.444444444444445e-05,
      "loss": 0.0036,
      "step": 28000
    },
    {
      "epoch": 1.556111111111111,
      "grad_norm": 0.12016632407903671,
      "learning_rate": 3.443888888888889e-05,
      "loss": 0.0037,
      "step": 28010
    },
    {
      "epoch": 1.5566666666666666,
      "grad_norm": 0.2544994652271271,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.004,
      "step": 28020
    },
    {
      "epoch": 1.5572222222222223,
      "grad_norm": 0.2342403680086136,
      "learning_rate": 3.442777777777778e-05,
      "loss": 0.0041,
      "step": 28030
    },
    {
      "epoch": 1.557777777777778,
      "grad_norm": 0.39848583936691284,
      "learning_rate": 3.442222222222222e-05,
      "loss": 0.0038,
      "step": 28040
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 0.10240612924098969,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.0042,
      "step": 28050
    },
    {
      "epoch": 1.5588888888888888,
      "grad_norm": 0.12034961581230164,
      "learning_rate": 3.441111111111111e-05,
      "loss": 0.0035,
      "step": 28060
    },
    {
      "epoch": 1.5594444444444444,
      "grad_norm": 0.3318576514720917,
      "learning_rate": 3.440555555555556e-05,
      "loss": 0.0032,
      "step": 28070
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.18151947855949402,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0055,
      "step": 28080
    },
    {
      "epoch": 1.5605555555555557,
      "grad_norm": 0.8109132051467896,
      "learning_rate": 3.4394444444444446e-05,
      "loss": 0.0042,
      "step": 28090
    },
    {
      "epoch": 1.5611111111111111,
      "grad_norm": 0.32977494597435,
      "learning_rate": 3.438888888888889e-05,
      "loss": 0.0033,
      "step": 28100
    },
    {
      "epoch": 1.5616666666666665,
      "grad_norm": 0.330048143863678,
      "learning_rate": 3.438333333333333e-05,
      "loss": 0.0045,
      "step": 28110
    },
    {
      "epoch": 1.5622222222222222,
      "grad_norm": 0.46769124269485474,
      "learning_rate": 3.4377777777777784e-05,
      "loss": 0.0058,
      "step": 28120
    },
    {
      "epoch": 1.5627777777777778,
      "grad_norm": 0.8565210103988647,
      "learning_rate": 3.437222222222222e-05,
      "loss": 0.0046,
      "step": 28130
    },
    {
      "epoch": 1.5633333333333335,
      "grad_norm": 0.48005005717277527,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.004,
      "step": 28140
    },
    {
      "epoch": 1.5638888888888889,
      "grad_norm": 0.3596607744693756,
      "learning_rate": 3.436111111111111e-05,
      "loss": 0.0047,
      "step": 28150
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 0.3598364591598511,
      "learning_rate": 3.435555555555556e-05,
      "loss": 0.0051,
      "step": 28160
    },
    {
      "epoch": 1.565,
      "grad_norm": 0.3381326496601105,
      "learning_rate": 3.435e-05,
      "loss": 0.0045,
      "step": 28170
    },
    {
      "epoch": 1.5655555555555556,
      "grad_norm": 0.15570861101150513,
      "learning_rate": 3.4344444444444445e-05,
      "loss": 0.0052,
      "step": 28180
    },
    {
      "epoch": 1.5661111111111112,
      "grad_norm": 0.12107127904891968,
      "learning_rate": 3.4338888888888895e-05,
      "loss": 0.005,
      "step": 28190
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.18094225227832794,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0034,
      "step": 28200
    },
    {
      "epoch": 1.567222222222222,
      "grad_norm": 0.06864345073699951,
      "learning_rate": 3.432777777777778e-05,
      "loss": 0.0038,
      "step": 28210
    },
    {
      "epoch": 1.5677777777777777,
      "grad_norm": 0.17941229045391083,
      "learning_rate": 3.432222222222222e-05,
      "loss": 0.0042,
      "step": 28220
    },
    {
      "epoch": 1.5683333333333334,
      "grad_norm": 0.23951251804828644,
      "learning_rate": 3.431666666666667e-05,
      "loss": 0.0034,
      "step": 28230
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.03389539569616318,
      "learning_rate": 3.431111111111111e-05,
      "loss": 0.003,
      "step": 28240
    },
    {
      "epoch": 1.5694444444444444,
      "grad_norm": 0.0907510295510292,
      "learning_rate": 3.430555555555556e-05,
      "loss": 0.0044,
      "step": 28250
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.5944096446037292,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0056,
      "step": 28260
    },
    {
      "epoch": 1.5705555555555555,
      "grad_norm": 0.3596998155117035,
      "learning_rate": 3.4294444444444444e-05,
      "loss": 0.0038,
      "step": 28270
    },
    {
      "epoch": 1.5711111111111111,
      "grad_norm": 0.35982441902160645,
      "learning_rate": 3.4288888888888894e-05,
      "loss": 0.0047,
      "step": 28280
    },
    {
      "epoch": 1.5716666666666668,
      "grad_norm": 0.040631312876939774,
      "learning_rate": 3.428333333333333e-05,
      "loss": 0.0037,
      "step": 28290
    },
    {
      "epoch": 1.5722222222222222,
      "grad_norm": 0.27041083574295044,
      "learning_rate": 3.427777777777778e-05,
      "loss": 0.0048,
      "step": 28300
    },
    {
      "epoch": 1.5727777777777778,
      "grad_norm": 0.061288800090551376,
      "learning_rate": 3.4272222222222225e-05,
      "loss": 0.0042,
      "step": 28310
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.2169066071510315,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0044,
      "step": 28320
    },
    {
      "epoch": 1.573888888888889,
      "grad_norm": 0.12542085349559784,
      "learning_rate": 3.426111111111111e-05,
      "loss": 0.0033,
      "step": 28330
    },
    {
      "epoch": 1.5744444444444445,
      "grad_norm": 0.2979287803173065,
      "learning_rate": 3.4255555555555555e-05,
      "loss": 0.0056,
      "step": 28340
    },
    {
      "epoch": 1.575,
      "grad_norm": 0.18048956990242004,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.0043,
      "step": 28350
    },
    {
      "epoch": 1.5755555555555556,
      "grad_norm": 0.38769295811653137,
      "learning_rate": 3.424444444444444e-05,
      "loss": 0.005,
      "step": 28360
    },
    {
      "epoch": 1.576111111111111,
      "grad_norm": 0.17926771938800812,
      "learning_rate": 3.423888888888889e-05,
      "loss": 0.0029,
      "step": 28370
    },
    {
      "epoch": 1.5766666666666667,
      "grad_norm": 0.49471479654312134,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.0035,
      "step": 28380
    },
    {
      "epoch": 1.5772222222222223,
      "grad_norm": 0.3912501037120819,
      "learning_rate": 3.422777777777778e-05,
      "loss": 0.0041,
      "step": 28390
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.04669125750660896,
      "learning_rate": 3.4222222222222224e-05,
      "loss": 0.004,
      "step": 28400
    },
    {
      "epoch": 1.5783333333333334,
      "grad_norm": 0.46773025393486023,
      "learning_rate": 3.421666666666667e-05,
      "loss": 0.0049,
      "step": 28410
    },
    {
      "epoch": 1.5788888888888888,
      "grad_norm": 0.24015742540359497,
      "learning_rate": 3.421111111111111e-05,
      "loss": 0.0037,
      "step": 28420
    },
    {
      "epoch": 1.5794444444444444,
      "grad_norm": 0.03388751298189163,
      "learning_rate": 3.4205555555555554e-05,
      "loss": 0.0033,
      "step": 28430
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.07296618074178696,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0049,
      "step": 28440
    },
    {
      "epoch": 1.5805555555555557,
      "grad_norm": 0.016366850584745407,
      "learning_rate": 3.419444444444445e-05,
      "loss": 0.0033,
      "step": 28450
    },
    {
      "epoch": 1.5811111111111111,
      "grad_norm": 0.2988933324813843,
      "learning_rate": 3.418888888888889e-05,
      "loss": 0.0043,
      "step": 28460
    },
    {
      "epoch": 1.5816666666666666,
      "grad_norm": 0.27800682187080383,
      "learning_rate": 3.4183333333333335e-05,
      "loss": 0.0028,
      "step": 28470
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 0.4186987280845642,
      "learning_rate": 3.417777777777778e-05,
      "loss": 0.004,
      "step": 28480
    },
    {
      "epoch": 1.5827777777777778,
      "grad_norm": 0.10347598791122437,
      "learning_rate": 3.417222222222222e-05,
      "loss": 0.0045,
      "step": 28490
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.30010437965393066,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0047,
      "step": 28500
    },
    {
      "epoch": 1.583888888888889,
      "grad_norm": 0.12034492939710617,
      "learning_rate": 3.416111111111111e-05,
      "loss": 0.004,
      "step": 28510
    },
    {
      "epoch": 1.5844444444444443,
      "grad_norm": 0.379686176776886,
      "learning_rate": 3.415555555555556e-05,
      "loss": 0.0036,
      "step": 28520
    },
    {
      "epoch": 1.585,
      "grad_norm": 0.35865047574043274,
      "learning_rate": 3.415e-05,
      "loss": 0.0035,
      "step": 28530
    },
    {
      "epoch": 1.5855555555555556,
      "grad_norm": 0.0118925916031003,
      "learning_rate": 3.414444444444445e-05,
      "loss": 0.0051,
      "step": 28540
    },
    {
      "epoch": 1.5861111111111112,
      "grad_norm": 0.23891910910606384,
      "learning_rate": 3.413888888888889e-05,
      "loss": 0.0043,
      "step": 28550
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.18060879409313202,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0039,
      "step": 28560
    },
    {
      "epoch": 1.587222222222222,
      "grad_norm": 0.3296046555042267,
      "learning_rate": 3.412777777777778e-05,
      "loss": 0.0042,
      "step": 28570
    },
    {
      "epoch": 1.5877777777777777,
      "grad_norm": 0.21003910899162292,
      "learning_rate": 3.412222222222222e-05,
      "loss": 0.0048,
      "step": 28580
    },
    {
      "epoch": 1.5883333333333334,
      "grad_norm": 0.1825992614030838,
      "learning_rate": 3.411666666666667e-05,
      "loss": 0.0038,
      "step": 28590
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 0.3900693655014038,
      "learning_rate": 3.411111111111111e-05,
      "loss": 0.0039,
      "step": 28600
    },
    {
      "epoch": 1.5894444444444444,
      "grad_norm": 0.19783355295658112,
      "learning_rate": 3.410555555555556e-05,
      "loss": 0.004,
      "step": 28610
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.12092623859643936,
      "learning_rate": 3.41e-05,
      "loss": 0.0047,
      "step": 28620
    },
    {
      "epoch": 1.5905555555555555,
      "grad_norm": 0.24050620198249817,
      "learning_rate": 3.4094444444444446e-05,
      "loss": 0.0044,
      "step": 28630
    },
    {
      "epoch": 1.5911111111111111,
      "grad_norm": 0.3707658648490906,
      "learning_rate": 3.408888888888889e-05,
      "loss": 0.0041,
      "step": 28640
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.009909671731293201,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.0035,
      "step": 28650
    },
    {
      "epoch": 1.5922222222222222,
      "grad_norm": 0.034036990255117416,
      "learning_rate": 3.407777777777778e-05,
      "loss": 0.0033,
      "step": 28660
    },
    {
      "epoch": 1.5927777777777776,
      "grad_norm": 0.3587550222873688,
      "learning_rate": 3.407222222222222e-05,
      "loss": 0.0039,
      "step": 28670
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.18284328281879425,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0026,
      "step": 28680
    },
    {
      "epoch": 1.593888888888889,
      "grad_norm": 0.2294878363609314,
      "learning_rate": 3.406111111111111e-05,
      "loss": 0.0053,
      "step": 28690
    },
    {
      "epoch": 1.5944444444444446,
      "grad_norm": 0.06108749285340309,
      "learning_rate": 3.405555555555556e-05,
      "loss": 0.0037,
      "step": 28700
    },
    {
      "epoch": 1.595,
      "grad_norm": 0.29909569025039673,
      "learning_rate": 3.405e-05,
      "loss": 0.0046,
      "step": 28710
    },
    {
      "epoch": 1.5955555555555554,
      "grad_norm": 0.017551785334944725,
      "learning_rate": 3.4044444444444445e-05,
      "loss": 0.002,
      "step": 28720
    },
    {
      "epoch": 1.596111111111111,
      "grad_norm": 0.21031054854393005,
      "learning_rate": 3.4038888888888895e-05,
      "loss": 0.0031,
      "step": 28730
    },
    {
      "epoch": 1.5966666666666667,
      "grad_norm": 0.12053493410348892,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0029,
      "step": 28740
    },
    {
      "epoch": 1.5972222222222223,
      "grad_norm": 0.30175623297691345,
      "learning_rate": 3.402777777777778e-05,
      "loss": 0.0039,
      "step": 28750
    },
    {
      "epoch": 1.5977777777777777,
      "grad_norm": 0.12298345565795898,
      "learning_rate": 3.402222222222222e-05,
      "loss": 0.0035,
      "step": 28760
    },
    {
      "epoch": 1.5983333333333334,
      "grad_norm": 0.09131676703691483,
      "learning_rate": 3.401666666666667e-05,
      "loss": 0.0039,
      "step": 28770
    },
    {
      "epoch": 1.5988888888888888,
      "grad_norm": 0.3134740889072418,
      "learning_rate": 3.401111111111111e-05,
      "loss": 0.0029,
      "step": 28780
    },
    {
      "epoch": 1.5994444444444444,
      "grad_norm": 0.26963287591934204,
      "learning_rate": 3.4005555555555556e-05,
      "loss": 0.0041,
      "step": 28790
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.0612008236348629,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.003,
      "step": 28800
    },
    {
      "epoch": 1.6005555555555555,
      "grad_norm": 0.2642320394515991,
      "learning_rate": 3.399444444444444e-05,
      "loss": 0.0039,
      "step": 28810
    },
    {
      "epoch": 1.6011111111111112,
      "grad_norm": 0.12249390035867691,
      "learning_rate": 3.3988888888888894e-05,
      "loss": 0.0032,
      "step": 28820
    },
    {
      "epoch": 1.6016666666666666,
      "grad_norm": 0.04403883218765259,
      "learning_rate": 3.398333333333333e-05,
      "loss": 0.0044,
      "step": 28830
    },
    {
      "epoch": 1.6022222222222222,
      "grad_norm": 0.018727999180555344,
      "learning_rate": 3.397777777777778e-05,
      "loss": 0.0031,
      "step": 28840
    },
    {
      "epoch": 1.6027777777777779,
      "grad_norm": 0.026003703474998474,
      "learning_rate": 3.3972222222222224e-05,
      "loss": 0.0033,
      "step": 28850
    },
    {
      "epoch": 1.6033333333333335,
      "grad_norm": 0.061994872987270355,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.0036,
      "step": 28860
    },
    {
      "epoch": 1.603888888888889,
      "grad_norm": 0.19777272641658783,
      "learning_rate": 3.396111111111111e-05,
      "loss": 0.0033,
      "step": 28870
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.21502481400966644,
      "learning_rate": 3.3955555555555555e-05,
      "loss": 0.0038,
      "step": 28880
    },
    {
      "epoch": 1.605,
      "grad_norm": 0.24370448291301727,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 0.003,
      "step": 28890
    },
    {
      "epoch": 1.6055555555555556,
      "grad_norm": 0.15029296278953552,
      "learning_rate": 3.394444444444444e-05,
      "loss": 0.0029,
      "step": 28900
    },
    {
      "epoch": 1.6061111111111113,
      "grad_norm": 0.6877511739730835,
      "learning_rate": 3.393888888888889e-05,
      "loss": 0.0043,
      "step": 28910
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.5432273149490356,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.003,
      "step": 28920
    },
    {
      "epoch": 1.607222222222222,
      "grad_norm": 0.5107163786888123,
      "learning_rate": 3.392777777777778e-05,
      "loss": 0.0029,
      "step": 28930
    },
    {
      "epoch": 1.6077777777777778,
      "grad_norm": 0.062041353434324265,
      "learning_rate": 3.392222222222222e-05,
      "loss": 0.0027,
      "step": 28940
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.36047878861427307,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.0032,
      "step": 28950
    },
    {
      "epoch": 1.608888888888889,
      "grad_norm": 0.30460280179977417,
      "learning_rate": 3.391111111111111e-05,
      "loss": 0.0034,
      "step": 28960
    },
    {
      "epoch": 1.6094444444444445,
      "grad_norm": 0.27027758955955505,
      "learning_rate": 3.3905555555555554e-05,
      "loss": 0.005,
      "step": 28970
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.3303411304950714,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.0049,
      "step": 28980
    },
    {
      "epoch": 1.6105555555555555,
      "grad_norm": 0.36200153827667236,
      "learning_rate": 3.389444444444445e-05,
      "loss": 0.0053,
      "step": 28990
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.09114507585763931,
      "learning_rate": 3.388888888888889e-05,
      "loss": 0.0032,
      "step": 29000
    },
    {
      "epoch": 1.6116666666666668,
      "grad_norm": 0.0615793876349926,
      "learning_rate": 3.3883333333333335e-05,
      "loss": 0.0035,
      "step": 29010
    },
    {
      "epoch": 1.6122222222222222,
      "grad_norm": 0.2102946639060974,
      "learning_rate": 3.387777777777778e-05,
      "loss": 0.0062,
      "step": 29020
    },
    {
      "epoch": 1.6127777777777776,
      "grad_norm": 0.031032433733344078,
      "learning_rate": 3.387222222222222e-05,
      "loss": 0.0036,
      "step": 29030
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.44951745867729187,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0023,
      "step": 29040
    },
    {
      "epoch": 1.613888888888889,
      "grad_norm": 0.18460595607757568,
      "learning_rate": 3.386111111111111e-05,
      "loss": 0.0047,
      "step": 29050
    },
    {
      "epoch": 1.6144444444444446,
      "grad_norm": 0.02175098843872547,
      "learning_rate": 3.385555555555556e-05,
      "loss": 0.0052,
      "step": 29060
    },
    {
      "epoch": 1.615,
      "grad_norm": 0.060465991497039795,
      "learning_rate": 3.385e-05,
      "loss": 0.0042,
      "step": 29070
    },
    {
      "epoch": 1.6155555555555554,
      "grad_norm": 0.09156659990549088,
      "learning_rate": 3.3844444444444446e-05,
      "loss": 0.0043,
      "step": 29080
    },
    {
      "epoch": 1.616111111111111,
      "grad_norm": 0.27051523327827454,
      "learning_rate": 3.383888888888889e-05,
      "loss": 0.0055,
      "step": 29090
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.18099501729011536,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0041,
      "step": 29100
    },
    {
      "epoch": 1.6172222222222223,
      "grad_norm": 0.3003118634223938,
      "learning_rate": 3.382777777777778e-05,
      "loss": 0.0042,
      "step": 29110
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 0.3596295118331909,
      "learning_rate": 3.382222222222222e-05,
      "loss": 0.005,
      "step": 29120
    },
    {
      "epoch": 1.6183333333333332,
      "grad_norm": 0.15275269746780396,
      "learning_rate": 3.381666666666667e-05,
      "loss": 0.0044,
      "step": 29130
    },
    {
      "epoch": 1.6188888888888888,
      "grad_norm": 0.23979569971561432,
      "learning_rate": 3.381111111111111e-05,
      "loss": 0.0027,
      "step": 29140
    },
    {
      "epoch": 1.6194444444444445,
      "grad_norm": 0.15013688802719116,
      "learning_rate": 3.380555555555556e-05,
      "loss": 0.0042,
      "step": 29150
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.09314363449811935,
      "learning_rate": 3.38e-05,
      "loss": 0.0042,
      "step": 29160
    },
    {
      "epoch": 1.6205555555555555,
      "grad_norm": 0.4823618531227112,
      "learning_rate": 3.3794444444444445e-05,
      "loss": 0.0042,
      "step": 29170
    },
    {
      "epoch": 1.621111111111111,
      "grad_norm": 0.42032891511917114,
      "learning_rate": 3.378888888888889e-05,
      "loss": 0.0053,
      "step": 29180
    },
    {
      "epoch": 1.6216666666666666,
      "grad_norm": 0.715266227722168,
      "learning_rate": 3.378333333333333e-05,
      "loss": 0.0047,
      "step": 29190
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.2696048319339752,
      "learning_rate": 3.377777777777778e-05,
      "loss": 0.0047,
      "step": 29200
    },
    {
      "epoch": 1.6227777777777779,
      "grad_norm": 0.18078120052814484,
      "learning_rate": 3.377222222222222e-05,
      "loss": 0.0042,
      "step": 29210
    },
    {
      "epoch": 1.6233333333333333,
      "grad_norm": 0.06247714161872864,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0048,
      "step": 29220
    },
    {
      "epoch": 1.623888888888889,
      "grad_norm": 0.01521067414432764,
      "learning_rate": 3.376111111111111e-05,
      "loss": 0.0045,
      "step": 29230
    },
    {
      "epoch": 1.6244444444444444,
      "grad_norm": 0.3610771596431732,
      "learning_rate": 3.375555555555556e-05,
      "loss": 0.0047,
      "step": 29240
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.2226007878780365,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.003,
      "step": 29250
    },
    {
      "epoch": 1.6255555555555556,
      "grad_norm": 0.629779577255249,
      "learning_rate": 3.3744444444444444e-05,
      "loss": 0.0025,
      "step": 29260
    },
    {
      "epoch": 1.626111111111111,
      "grad_norm": 0.34805870056152344,
      "learning_rate": 3.3738888888888894e-05,
      "loss": 0.0052,
      "step": 29270
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.12046501040458679,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0049,
      "step": 29280
    },
    {
      "epoch": 1.6272222222222221,
      "grad_norm": 0.26922446489334106,
      "learning_rate": 3.372777777777778e-05,
      "loss": 0.0035,
      "step": 29290
    },
    {
      "epoch": 1.6277777777777778,
      "grad_norm": 0.12228379398584366,
      "learning_rate": 3.3722222222222225e-05,
      "loss": 0.0046,
      "step": 29300
    },
    {
      "epoch": 1.6283333333333334,
      "grad_norm": 0.2825367748737335,
      "learning_rate": 3.371666666666667e-05,
      "loss": 0.0047,
      "step": 29310
    },
    {
      "epoch": 1.628888888888889,
      "grad_norm": 0.5996574759483337,
      "learning_rate": 3.371111111111111e-05,
      "loss": 0.0043,
      "step": 29320
    },
    {
      "epoch": 1.6294444444444445,
      "grad_norm": 0.5388227105140686,
      "learning_rate": 3.3705555555555556e-05,
      "loss": 0.0043,
      "step": 29330
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.11601583659648895,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0046,
      "step": 29340
    },
    {
      "epoch": 1.6305555555555555,
      "grad_norm": 0.27694153785705566,
      "learning_rate": 3.369444444444444e-05,
      "loss": 0.0044,
      "step": 29350
    },
    {
      "epoch": 1.6311111111111112,
      "grad_norm": 0.4033721089363098,
      "learning_rate": 3.368888888888889e-05,
      "loss": 0.0041,
      "step": 29360
    },
    {
      "epoch": 1.6316666666666668,
      "grad_norm": 0.4828745722770691,
      "learning_rate": 3.368333333333334e-05,
      "loss": 0.0042,
      "step": 29370
    },
    {
      "epoch": 1.6322222222222222,
      "grad_norm": 0.035830412060022354,
      "learning_rate": 3.367777777777778e-05,
      "loss": 0.0048,
      "step": 29380
    },
    {
      "epoch": 1.6327777777777777,
      "grad_norm": 0.09245330095291138,
      "learning_rate": 3.3672222222222224e-05,
      "loss": 0.0043,
      "step": 29390
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.244839146733284,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0036,
      "step": 29400
    },
    {
      "epoch": 1.633888888888889,
      "grad_norm": 0.35933610796928406,
      "learning_rate": 3.366111111111112e-05,
      "loss": 0.0038,
      "step": 29410
    },
    {
      "epoch": 1.6344444444444446,
      "grad_norm": 0.1799817532300949,
      "learning_rate": 3.3655555555555554e-05,
      "loss": 0.0043,
      "step": 29420
    },
    {
      "epoch": 1.635,
      "grad_norm": 0.09111057966947556,
      "learning_rate": 3.3650000000000005e-05,
      "loss": 0.0036,
      "step": 29430
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 0.18049247562885284,
      "learning_rate": 3.364444444444445e-05,
      "loss": 0.0029,
      "step": 29440
    },
    {
      "epoch": 1.636111111111111,
      "grad_norm": 0.29961463809013367,
      "learning_rate": 3.363888888888889e-05,
      "loss": 0.0034,
      "step": 29450
    },
    {
      "epoch": 1.6366666666666667,
      "grad_norm": 0.08177440613508224,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0041,
      "step": 29460
    },
    {
      "epoch": 1.6372222222222224,
      "grad_norm": 0.1796473115682602,
      "learning_rate": 3.362777777777778e-05,
      "loss": 0.0036,
      "step": 29470
    },
    {
      "epoch": 1.6377777777777778,
      "grad_norm": 0.5313279032707214,
      "learning_rate": 3.362222222222222e-05,
      "loss": 0.0041,
      "step": 29480
    },
    {
      "epoch": 1.6383333333333332,
      "grad_norm": 0.3755614161491394,
      "learning_rate": 3.3616666666666666e-05,
      "loss": 0.0037,
      "step": 29490
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 0.325524240732193,
      "learning_rate": 3.3611111111111116e-05,
      "loss": 0.0032,
      "step": 29500
    },
    {
      "epoch": 1.6394444444444445,
      "grad_norm": 0.48590078949928284,
      "learning_rate": 3.360555555555556e-05,
      "loss": 0.0046,
      "step": 29510
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.012772529385983944,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0047,
      "step": 29520
    },
    {
      "epoch": 1.6405555555555555,
      "grad_norm": 0.14998221397399902,
      "learning_rate": 3.359444444444445e-05,
      "loss": 0.0044,
      "step": 29530
    },
    {
      "epoch": 1.641111111111111,
      "grad_norm": 0.21793514490127563,
      "learning_rate": 3.358888888888889e-05,
      "loss": 0.0043,
      "step": 29540
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 0.2984735071659088,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.0051,
      "step": 29550
    },
    {
      "epoch": 1.6422222222222222,
      "grad_norm": 0.3003363013267517,
      "learning_rate": 3.357777777777778e-05,
      "loss": 0.0044,
      "step": 29560
    },
    {
      "epoch": 1.642777777777778,
      "grad_norm": 0.3306891620159149,
      "learning_rate": 3.357222222222222e-05,
      "loss": 0.0036,
      "step": 29570
    },
    {
      "epoch": 1.6433333333333333,
      "grad_norm": 0.21002250909805298,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0046,
      "step": 29580
    },
    {
      "epoch": 1.6438888888888887,
      "grad_norm": 0.3588695824146271,
      "learning_rate": 3.3561111111111115e-05,
      "loss": 0.0035,
      "step": 29590
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 0.30029088258743286,
      "learning_rate": 3.355555555555556e-05,
      "loss": 0.005,
      "step": 29600
    },
    {
      "epoch": 1.645,
      "grad_norm": 0.17975719273090363,
      "learning_rate": 3.355e-05,
      "loss": 0.0034,
      "step": 29610
    },
    {
      "epoch": 1.6455555555555557,
      "grad_norm": 0.28671711683273315,
      "learning_rate": 3.3544444444444446e-05,
      "loss": 0.0049,
      "step": 29620
    },
    {
      "epoch": 1.646111111111111,
      "grad_norm": 0.20178592205047607,
      "learning_rate": 3.353888888888889e-05,
      "loss": 0.0034,
      "step": 29630
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.4011467695236206,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.004,
      "step": 29640
    },
    {
      "epoch": 1.6472222222222221,
      "grad_norm": 0.2743243873119354,
      "learning_rate": 3.352777777777778e-05,
      "loss": 0.004,
      "step": 29650
    },
    {
      "epoch": 1.6477777777777778,
      "grad_norm": 0.41833925247192383,
      "learning_rate": 3.352222222222222e-05,
      "loss": 0.0033,
      "step": 29660
    },
    {
      "epoch": 1.6483333333333334,
      "grad_norm": 0.4496748149394989,
      "learning_rate": 3.351666666666667e-05,
      "loss": 0.0038,
      "step": 29670
    },
    {
      "epoch": 1.6488888888888888,
      "grad_norm": 0.06306957453489304,
      "learning_rate": 3.3511111111111114e-05,
      "loss": 0.0049,
      "step": 29680
    },
    {
      "epoch": 1.6494444444444445,
      "grad_norm": 0.3224630355834961,
      "learning_rate": 3.350555555555556e-05,
      "loss": 0.0056,
      "step": 29690
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.12194190919399261,
      "learning_rate": 3.35e-05,
      "loss": 0.0045,
      "step": 29700
    },
    {
      "epoch": 1.6505555555555556,
      "grad_norm": 0.06055406108498573,
      "learning_rate": 3.3494444444444445e-05,
      "loss": 0.0037,
      "step": 29710
    },
    {
      "epoch": 1.6511111111111112,
      "grad_norm": 0.4705221652984619,
      "learning_rate": 3.3488888888888895e-05,
      "loss": 0.0049,
      "step": 29720
    },
    {
      "epoch": 1.6516666666666666,
      "grad_norm": 0.37376168370246887,
      "learning_rate": 3.348333333333333e-05,
      "loss": 0.0058,
      "step": 29730
    },
    {
      "epoch": 1.6522222222222223,
      "grad_norm": 0.4196723997592926,
      "learning_rate": 3.347777777777778e-05,
      "loss": 0.004,
      "step": 29740
    },
    {
      "epoch": 1.6527777777777777,
      "grad_norm": 0.48049893975257874,
      "learning_rate": 3.347222222222222e-05,
      "loss": 0.0046,
      "step": 29750
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.06056451052427292,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0037,
      "step": 29760
    },
    {
      "epoch": 1.653888888888889,
      "grad_norm": 0.5496031641960144,
      "learning_rate": 3.346111111111111e-05,
      "loss": 0.0042,
      "step": 29770
    },
    {
      "epoch": 1.6544444444444446,
      "grad_norm": 0.4879227876663208,
      "learning_rate": 3.3455555555555556e-05,
      "loss": 0.0039,
      "step": 29780
    },
    {
      "epoch": 1.655,
      "grad_norm": 0.32928597927093506,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.0053,
      "step": 29790
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 0.4317563474178314,
      "learning_rate": 3.3444444444444443e-05,
      "loss": 0.0033,
      "step": 29800
    },
    {
      "epoch": 1.656111111111111,
      "grad_norm": 0.41920801997184753,
      "learning_rate": 3.3438888888888894e-05,
      "loss": 0.0031,
      "step": 29810
    },
    {
      "epoch": 1.6566666666666667,
      "grad_norm": 0.1816297173500061,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0039,
      "step": 29820
    },
    {
      "epoch": 1.6572222222222224,
      "grad_norm": 0.031081071123480797,
      "learning_rate": 3.342777777777778e-05,
      "loss": 0.0047,
      "step": 29830
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 0.41897332668304443,
      "learning_rate": 3.3422222222222224e-05,
      "loss": 0.004,
      "step": 29840
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 0.25258010625839233,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.0043,
      "step": 29850
    },
    {
      "epoch": 1.6588888888888889,
      "grad_norm": 0.18527042865753174,
      "learning_rate": 3.341111111111112e-05,
      "loss": 0.0042,
      "step": 29860
    },
    {
      "epoch": 1.6594444444444445,
      "grad_norm": 0.5994625091552734,
      "learning_rate": 3.3405555555555555e-05,
      "loss": 0.0054,
      "step": 29870
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.1619027853012085,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0045,
      "step": 29880
    },
    {
      "epoch": 1.6605555555555556,
      "grad_norm": 0.12017311900854111,
      "learning_rate": 3.339444444444444e-05,
      "loss": 0.0039,
      "step": 29890
    },
    {
      "epoch": 1.661111111111111,
      "grad_norm": 0.5690462589263916,
      "learning_rate": 3.338888888888889e-05,
      "loss": 0.0033,
      "step": 29900
    },
    {
      "epoch": 1.6616666666666666,
      "grad_norm": 0.1678897738456726,
      "learning_rate": 3.3383333333333336e-05,
      "loss": 0.0039,
      "step": 29910
    },
    {
      "epoch": 1.6622222222222223,
      "grad_norm": 0.12407538294792175,
      "learning_rate": 3.337777777777778e-05,
      "loss": 0.0044,
      "step": 29920
    },
    {
      "epoch": 1.662777777777778,
      "grad_norm": 0.09055761247873306,
      "learning_rate": 3.337222222222222e-05,
      "loss": 0.0042,
      "step": 29930
    },
    {
      "epoch": 1.6633333333333333,
      "grad_norm": 0.06144717335700989,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.004,
      "step": 29940
    },
    {
      "epoch": 1.6638888888888888,
      "grad_norm": 0.15018899738788605,
      "learning_rate": 3.336111111111112e-05,
      "loss": 0.004,
      "step": 29950
    },
    {
      "epoch": 1.6644444444444444,
      "grad_norm": 0.27718812227249146,
      "learning_rate": 3.3355555555555554e-05,
      "loss": 0.0046,
      "step": 29960
    },
    {
      "epoch": 1.665,
      "grad_norm": 0.4078748822212219,
      "learning_rate": 3.3350000000000004e-05,
      "loss": 0.006,
      "step": 29970
    },
    {
      "epoch": 1.6655555555555557,
      "grad_norm": 0.45324039459228516,
      "learning_rate": 3.334444444444445e-05,
      "loss": 0.0028,
      "step": 29980
    },
    {
      "epoch": 1.666111111111111,
      "grad_norm": 0.2533224821090698,
      "learning_rate": 3.333888888888889e-05,
      "loss": 0.0043,
      "step": 29990
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.47875386476516724,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0047,
      "step": 30000
    },
    {
      "epoch": 1.6672222222222222,
      "grad_norm": 0.23889724910259247,
      "learning_rate": 3.332777777777778e-05,
      "loss": 0.005,
      "step": 30010
    },
    {
      "epoch": 1.6677777777777778,
      "grad_norm": 0.39097973704338074,
      "learning_rate": 3.332222222222222e-05,
      "loss": 0.005,
      "step": 30020
    },
    {
      "epoch": 1.6683333333333334,
      "grad_norm": 0.15140670537948608,
      "learning_rate": 3.3316666666666666e-05,
      "loss": 0.0038,
      "step": 30030
    },
    {
      "epoch": 1.6688888888888889,
      "grad_norm": 0.2999129593372345,
      "learning_rate": 3.3311111111111116e-05,
      "loss": 0.0053,
      "step": 30040
    },
    {
      "epoch": 1.6694444444444443,
      "grad_norm": 0.0907987579703331,
      "learning_rate": 3.330555555555556e-05,
      "loss": 0.0053,
      "step": 30050
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.06395682692527771,
      "learning_rate": 3.33e-05,
      "loss": 0.0051,
      "step": 30060
    },
    {
      "epoch": 1.6705555555555556,
      "grad_norm": 0.26888328790664673,
      "learning_rate": 3.3294444444444447e-05,
      "loss": 0.0054,
      "step": 30070
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 0.47948625683784485,
      "learning_rate": 3.328888888888889e-05,
      "loss": 0.0042,
      "step": 30080
    },
    {
      "epoch": 1.6716666666666666,
      "grad_norm": 0.268705278635025,
      "learning_rate": 3.3283333333333334e-05,
      "loss": 0.0044,
      "step": 30090
    },
    {
      "epoch": 1.6722222222222223,
      "grad_norm": 0.1206405982375145,
      "learning_rate": 3.327777777777778e-05,
      "loss": 0.0049,
      "step": 30100
    },
    {
      "epoch": 1.6727777777777777,
      "grad_norm": 0.26873788237571716,
      "learning_rate": 3.327222222222222e-05,
      "loss": 0.0057,
      "step": 30110
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.17972394824028015,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0052,
      "step": 30120
    },
    {
      "epoch": 1.673888888888889,
      "grad_norm": 0.011325343511998653,
      "learning_rate": 3.3261111111111115e-05,
      "loss": 0.005,
      "step": 30130
    },
    {
      "epoch": 1.6744444444444444,
      "grad_norm": 0.4791337549686432,
      "learning_rate": 3.325555555555556e-05,
      "loss": 0.0045,
      "step": 30140
    },
    {
      "epoch": 1.675,
      "grad_norm": 0.036176059395074844,
      "learning_rate": 3.325e-05,
      "loss": 0.0037,
      "step": 30150
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 0.4707651436328888,
      "learning_rate": 3.3244444444444445e-05,
      "loss": 0.0032,
      "step": 30160
    },
    {
      "epoch": 1.676111111111111,
      "grad_norm": 0.11994673311710358,
      "learning_rate": 3.323888888888889e-05,
      "loss": 0.0044,
      "step": 30170
    },
    {
      "epoch": 1.6766666666666667,
      "grad_norm": 0.060192033648490906,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0036,
      "step": 30180
    },
    {
      "epoch": 1.6772222222222222,
      "grad_norm": 0.38938722014427185,
      "learning_rate": 3.322777777777778e-05,
      "loss": 0.004,
      "step": 30190
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 0.16393588483333588,
      "learning_rate": 3.322222222222222e-05,
      "loss": 0.0041,
      "step": 30200
    },
    {
      "epoch": 1.6783333333333332,
      "grad_norm": 0.06352242827415466,
      "learning_rate": 3.321666666666667e-05,
      "loss": 0.0042,
      "step": 30210
    },
    {
      "epoch": 1.6788888888888889,
      "grad_norm": 0.315973162651062,
      "learning_rate": 3.3211111111111114e-05,
      "loss": 0.0027,
      "step": 30220
    },
    {
      "epoch": 1.6794444444444445,
      "grad_norm": 0.09209342300891876,
      "learning_rate": 3.320555555555556e-05,
      "loss": 0.0033,
      "step": 30230
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.18437571823596954,
      "learning_rate": 3.32e-05,
      "loss": 0.0039,
      "step": 30240
    },
    {
      "epoch": 1.6805555555555556,
      "grad_norm": 0.3438800573348999,
      "learning_rate": 3.3194444444444444e-05,
      "loss": 0.0051,
      "step": 30250
    },
    {
      "epoch": 1.681111111111111,
      "grad_norm": 0.2698879837989807,
      "learning_rate": 3.3188888888888895e-05,
      "loss": 0.0038,
      "step": 30260
    },
    {
      "epoch": 1.6816666666666666,
      "grad_norm": 0.32280442118644714,
      "learning_rate": 3.318333333333333e-05,
      "loss": 0.0034,
      "step": 30270
    },
    {
      "epoch": 1.6822222222222223,
      "grad_norm": 0.0624171607196331,
      "learning_rate": 3.317777777777778e-05,
      "loss": 0.0042,
      "step": 30280
    },
    {
      "epoch": 1.682777777777778,
      "grad_norm": 0.15215319395065308,
      "learning_rate": 3.317222222222222e-05,
      "loss": 0.0023,
      "step": 30290
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.24137432873249054,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0028,
      "step": 30300
    },
    {
      "epoch": 1.6838888888888888,
      "grad_norm": 0.0371696837246418,
      "learning_rate": 3.316111111111111e-05,
      "loss": 0.0051,
      "step": 30310
    },
    {
      "epoch": 1.6844444444444444,
      "grad_norm": 0.09084469079971313,
      "learning_rate": 3.3155555555555556e-05,
      "loss": 0.0033,
      "step": 30320
    },
    {
      "epoch": 1.685,
      "grad_norm": 0.3594277501106262,
      "learning_rate": 3.3150000000000006e-05,
      "loss": 0.004,
      "step": 30330
    },
    {
      "epoch": 1.6855555555555557,
      "grad_norm": 0.3594326078891754,
      "learning_rate": 3.314444444444444e-05,
      "loss": 0.0049,
      "step": 30340
    },
    {
      "epoch": 1.6861111111111111,
      "grad_norm": 0.1499900370836258,
      "learning_rate": 3.313888888888889e-05,
      "loss": 0.0037,
      "step": 30350
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.28802305459976196,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0043,
      "step": 30360
    },
    {
      "epoch": 1.6872222222222222,
      "grad_norm": 0.22920985519886017,
      "learning_rate": 3.312777777777778e-05,
      "loss": 0.0038,
      "step": 30370
    },
    {
      "epoch": 1.6877777777777778,
      "grad_norm": 0.29939526319503784,
      "learning_rate": 3.3122222222222224e-05,
      "loss": 0.0037,
      "step": 30380
    },
    {
      "epoch": 1.6883333333333335,
      "grad_norm": 0.39056938886642456,
      "learning_rate": 3.311666666666667e-05,
      "loss": 0.0041,
      "step": 30390
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.15129896998405457,
      "learning_rate": 3.311111111111112e-05,
      "loss": 0.0037,
      "step": 30400
    },
    {
      "epoch": 1.6894444444444443,
      "grad_norm": 0.12786974012851715,
      "learning_rate": 3.3105555555555555e-05,
      "loss": 0.0039,
      "step": 30410
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.16461752355098724,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.0037,
      "step": 30420
    },
    {
      "epoch": 1.6905555555555556,
      "grad_norm": 0.011614510789513588,
      "learning_rate": 3.309444444444444e-05,
      "loss": 0.0036,
      "step": 30430
    },
    {
      "epoch": 1.6911111111111112,
      "grad_norm": 0.5980919003486633,
      "learning_rate": 3.308888888888889e-05,
      "loss": 0.0025,
      "step": 30440
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 0.12020490318536758,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.006,
      "step": 30450
    },
    {
      "epoch": 1.692222222222222,
      "grad_norm": 0.23921868205070496,
      "learning_rate": 3.307777777777778e-05,
      "loss": 0.0055,
      "step": 30460
    },
    {
      "epoch": 1.6927777777777777,
      "grad_norm": 0.060434579849243164,
      "learning_rate": 3.307222222222222e-05,
      "loss": 0.0033,
      "step": 30470
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.20905764400959015,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0047,
      "step": 30480
    },
    {
      "epoch": 1.693888888888889,
      "grad_norm": 0.3592757284641266,
      "learning_rate": 3.306111111111112e-05,
      "loss": 0.0042,
      "step": 30490
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 0.15091083943843842,
      "learning_rate": 3.3055555555555553e-05,
      "loss": 0.0042,
      "step": 30500
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 0.5911520719528198,
      "learning_rate": 3.3050000000000004e-05,
      "loss": 0.0051,
      "step": 30510
    },
    {
      "epoch": 1.6955555555555555,
      "grad_norm": 0.06248406320810318,
      "learning_rate": 3.304444444444445e-05,
      "loss": 0.0037,
      "step": 30520
    },
    {
      "epoch": 1.6961111111111111,
      "grad_norm": 0.21006931364536285,
      "learning_rate": 3.303888888888889e-05,
      "loss": 0.0026,
      "step": 30530
    },
    {
      "epoch": 1.6966666666666668,
      "grad_norm": 0.4193740785121918,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.0046,
      "step": 30540
    },
    {
      "epoch": 1.6972222222222222,
      "grad_norm": 0.23985238373279572,
      "learning_rate": 3.302777777777778e-05,
      "loss": 0.0043,
      "step": 30550
    },
    {
      "epoch": 1.6977777777777778,
      "grad_norm": 0.09093011170625687,
      "learning_rate": 3.302222222222222e-05,
      "loss": 0.0034,
      "step": 30560
    },
    {
      "epoch": 1.6983333333333333,
      "grad_norm": 0.33351626992225647,
      "learning_rate": 3.3016666666666665e-05,
      "loss": 0.0035,
      "step": 30570
    },
    {
      "epoch": 1.698888888888889,
      "grad_norm": 0.17880944907665253,
      "learning_rate": 3.3011111111111115e-05,
      "loss": 0.0052,
      "step": 30580
    },
    {
      "epoch": 1.6994444444444445,
      "grad_norm": 0.33630210161209106,
      "learning_rate": 3.300555555555556e-05,
      "loss": 0.0035,
      "step": 30590
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.022030482068657875,
      "learning_rate": 3.3e-05,
      "loss": 0.0052,
      "step": 30600
    },
    {
      "epoch": 1.7005555555555556,
      "grad_norm": 0.23930393159389496,
      "learning_rate": 3.2994444444444446e-05,
      "loss": 0.0036,
      "step": 30610
    },
    {
      "epoch": 1.701111111111111,
      "grad_norm": 0.241193488240242,
      "learning_rate": 3.298888888888889e-05,
      "loss": 0.0038,
      "step": 30620
    },
    {
      "epoch": 1.7016666666666667,
      "grad_norm": 0.009504398331046104,
      "learning_rate": 3.298333333333333e-05,
      "loss": 0.004,
      "step": 30630
    },
    {
      "epoch": 1.7022222222222223,
      "grad_norm": 0.09383470565080643,
      "learning_rate": 3.297777777777778e-05,
      "loss": 0.0051,
      "step": 30640
    },
    {
      "epoch": 1.7027777777777777,
      "grad_norm": 0.574385404586792,
      "learning_rate": 3.297222222222223e-05,
      "loss": 0.0041,
      "step": 30650
    },
    {
      "epoch": 1.7033333333333334,
      "grad_norm": 0.6506603956222534,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0036,
      "step": 30660
    },
    {
      "epoch": 1.7038888888888888,
      "grad_norm": 0.14905749261379242,
      "learning_rate": 3.2961111111111114e-05,
      "loss": 0.0062,
      "step": 30670
    },
    {
      "epoch": 1.7044444444444444,
      "grad_norm": 0.5091214776039124,
      "learning_rate": 3.295555555555556e-05,
      "loss": 0.0035,
      "step": 30680
    },
    {
      "epoch": 1.705,
      "grad_norm": 0.2114572823047638,
      "learning_rate": 3.295e-05,
      "loss": 0.0035,
      "step": 30690
    },
    {
      "epoch": 1.7055555555555557,
      "grad_norm": 0.3822703957557678,
      "learning_rate": 3.2944444444444445e-05,
      "loss": 0.0039,
      "step": 30700
    },
    {
      "epoch": 1.7061111111111111,
      "grad_norm": 0.5380957126617432,
      "learning_rate": 3.293888888888889e-05,
      "loss": 0.0033,
      "step": 30710
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.43031126260757446,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0034,
      "step": 30720
    },
    {
      "epoch": 1.7072222222222222,
      "grad_norm": 0.6273078322410583,
      "learning_rate": 3.292777777777778e-05,
      "loss": 0.0053,
      "step": 30730
    },
    {
      "epoch": 1.7077777777777778,
      "grad_norm": 0.16451726853847504,
      "learning_rate": 3.2922222222222226e-05,
      "loss": 0.0041,
      "step": 30740
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.06389140337705612,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.005,
      "step": 30750
    },
    {
      "epoch": 1.708888888888889,
      "grad_norm": 0.15092206001281738,
      "learning_rate": 3.291111111111111e-05,
      "loss": 0.004,
      "step": 30760
    },
    {
      "epoch": 1.7094444444444443,
      "grad_norm": 0.24496911466121674,
      "learning_rate": 3.2905555555555557e-05,
      "loss": 0.0051,
      "step": 30770
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.13638827204704285,
      "learning_rate": 3.29e-05,
      "loss": 0.0037,
      "step": 30780
    },
    {
      "epoch": 1.7105555555555556,
      "grad_norm": 0.09925007820129395,
      "learning_rate": 3.2894444444444444e-05,
      "loss": 0.0043,
      "step": 30790
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.17969588935375214,
      "learning_rate": 3.2888888888888894e-05,
      "loss": 0.0033,
      "step": 30800
    },
    {
      "epoch": 1.7116666666666667,
      "grad_norm": 0.20990312099456787,
      "learning_rate": 3.288333333333333e-05,
      "loss": 0.0039,
      "step": 30810
    },
    {
      "epoch": 1.712222222222222,
      "grad_norm": 0.20980721712112427,
      "learning_rate": 3.287777777777778e-05,
      "loss": 0.0044,
      "step": 30820
    },
    {
      "epoch": 1.7127777777777777,
      "grad_norm": 0.2433738112449646,
      "learning_rate": 3.2872222222222225e-05,
      "loss": 0.0033,
      "step": 30830
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.39682817459106445,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0051,
      "step": 30840
    },
    {
      "epoch": 1.713888888888889,
      "grad_norm": 0.01175777055323124,
      "learning_rate": 3.286111111111111e-05,
      "loss": 0.0033,
      "step": 30850
    },
    {
      "epoch": 1.7144444444444444,
      "grad_norm": 0.1806567758321762,
      "learning_rate": 3.2855555555555555e-05,
      "loss": 0.0033,
      "step": 30860
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 0.06305112689733505,
      "learning_rate": 3.2850000000000006e-05,
      "loss": 0.0043,
      "step": 30870
    },
    {
      "epoch": 1.7155555555555555,
      "grad_norm": 0.4195019006729126,
      "learning_rate": 3.284444444444444e-05,
      "loss": 0.0049,
      "step": 30880
    },
    {
      "epoch": 1.7161111111111111,
      "grad_norm": 0.03357372432947159,
      "learning_rate": 3.283888888888889e-05,
      "loss": 0.0043,
      "step": 30890
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.32813340425491333,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0037,
      "step": 30900
    },
    {
      "epoch": 1.7172222222222222,
      "grad_norm": 0.3587508797645569,
      "learning_rate": 3.282777777777778e-05,
      "loss": 0.0029,
      "step": 30910
    },
    {
      "epoch": 1.7177777777777776,
      "grad_norm": 0.3001125454902649,
      "learning_rate": 3.2822222222222223e-05,
      "loss": 0.003,
      "step": 30920
    },
    {
      "epoch": 1.7183333333333333,
      "grad_norm": 0.4491058588027954,
      "learning_rate": 3.281666666666667e-05,
      "loss": 0.0025,
      "step": 30930
    },
    {
      "epoch": 1.718888888888889,
      "grad_norm": 0.4250083267688751,
      "learning_rate": 3.281111111111112e-05,
      "loss": 0.0028,
      "step": 30940
    },
    {
      "epoch": 1.7194444444444446,
      "grad_norm": 0.5083855986595154,
      "learning_rate": 3.2805555555555554e-05,
      "loss": 0.0044,
      "step": 30950
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.1806914210319519,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0041,
      "step": 30960
    },
    {
      "epoch": 1.7205555555555554,
      "grad_norm": 0.037388548254966736,
      "learning_rate": 3.279444444444444e-05,
      "loss": 0.0038,
      "step": 30970
    },
    {
      "epoch": 1.721111111111111,
      "grad_norm": 0.16283945739269257,
      "learning_rate": 3.278888888888889e-05,
      "loss": 0.0039,
      "step": 30980
    },
    {
      "epoch": 1.7216666666666667,
      "grad_norm": 0.424196720123291,
      "learning_rate": 3.2783333333333335e-05,
      "loss": 0.0038,
      "step": 30990
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.15101854503154755,
      "learning_rate": 3.277777777777778e-05,
      "loss": 0.0048,
      "step": 31000
    },
    {
      "epoch": 1.7227777777777777,
      "grad_norm": 0.06038448587059975,
      "learning_rate": 3.277222222222223e-05,
      "loss": 0.0043,
      "step": 31010
    },
    {
      "epoch": 1.7233333333333334,
      "grad_norm": 0.17962343990802765,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.0037,
      "step": 31020
    },
    {
      "epoch": 1.7238888888888888,
      "grad_norm": 0.09112910181283951,
      "learning_rate": 3.2761111111111116e-05,
      "loss": 0.0051,
      "step": 31030
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 0.2938292324542999,
      "learning_rate": 3.275555555555555e-05,
      "loss": 0.0048,
      "step": 31040
    },
    {
      "epoch": 1.725,
      "grad_norm": 0.15032152831554413,
      "learning_rate": 3.275e-05,
      "loss": 0.0031,
      "step": 31050
    },
    {
      "epoch": 1.7255555555555555,
      "grad_norm": 0.03170270845293999,
      "learning_rate": 3.274444444444445e-05,
      "loss": 0.0036,
      "step": 31060
    },
    {
      "epoch": 1.7261111111111112,
      "grad_norm": 0.11940843611955643,
      "learning_rate": 3.273888888888889e-05,
      "loss": 0.0037,
      "step": 31070
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.17951111495494843,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0032,
      "step": 31080
    },
    {
      "epoch": 1.7272222222222222,
      "grad_norm": 0.16591301560401917,
      "learning_rate": 3.272777777777778e-05,
      "loss": 0.0036,
      "step": 31090
    },
    {
      "epoch": 1.7277777777777779,
      "grad_norm": 0.21074335277080536,
      "learning_rate": 3.272222222222223e-05,
      "loss": 0.0052,
      "step": 31100
    },
    {
      "epoch": 1.7283333333333335,
      "grad_norm": 0.12182186543941498,
      "learning_rate": 3.2716666666666665e-05,
      "loss": 0.0031,
      "step": 31110
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 0.06260409206151962,
      "learning_rate": 3.2711111111111115e-05,
      "loss": 0.0047,
      "step": 31120
    },
    {
      "epoch": 1.7294444444444443,
      "grad_norm": 0.24268116056919098,
      "learning_rate": 3.270555555555556e-05,
      "loss": 0.0038,
      "step": 31130
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.23599176108837128,
      "learning_rate": 3.27e-05,
      "loss": 0.004,
      "step": 31140
    },
    {
      "epoch": 1.7305555555555556,
      "grad_norm": 0.09063968062400818,
      "learning_rate": 3.2694444444444446e-05,
      "loss": 0.0038,
      "step": 31150
    },
    {
      "epoch": 1.7311111111111113,
      "grad_norm": 0.03131556138396263,
      "learning_rate": 3.268888888888889e-05,
      "loss": 0.004,
      "step": 31160
    },
    {
      "epoch": 1.7316666666666667,
      "grad_norm": 0.12084037810564041,
      "learning_rate": 3.268333333333333e-05,
      "loss": 0.0034,
      "step": 31170
    },
    {
      "epoch": 1.732222222222222,
      "grad_norm": 0.24000751972198486,
      "learning_rate": 3.2677777777777776e-05,
      "loss": 0.0036,
      "step": 31180
    },
    {
      "epoch": 1.7327777777777778,
      "grad_norm": 0.010177764110267162,
      "learning_rate": 3.2672222222222227e-05,
      "loss": 0.0033,
      "step": 31190
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.09105435013771057,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0059,
      "step": 31200
    },
    {
      "epoch": 1.733888888888889,
      "grad_norm": 0.2994111478328705,
      "learning_rate": 3.2661111111111114e-05,
      "loss": 0.0043,
      "step": 31210
    },
    {
      "epoch": 1.7344444444444445,
      "grad_norm": 0.18115077912807465,
      "learning_rate": 3.265555555555556e-05,
      "loss": 0.0022,
      "step": 31220
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 0.13851377367973328,
      "learning_rate": 3.265e-05,
      "loss": 0.0034,
      "step": 31230
    },
    {
      "epoch": 1.7355555555555555,
      "grad_norm": 0.11988767236471176,
      "learning_rate": 3.2644444444444444e-05,
      "loss": 0.0044,
      "step": 31240
    },
    {
      "epoch": 1.7361111111111112,
      "grad_norm": 0.21050769090652466,
      "learning_rate": 3.263888888888889e-05,
      "loss": 0.0036,
      "step": 31250
    },
    {
      "epoch": 1.7366666666666668,
      "grad_norm": 0.5586289763450623,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.0047,
      "step": 31260
    },
    {
      "epoch": 1.7372222222222222,
      "grad_norm": 0.47850221395492554,
      "learning_rate": 3.262777777777778e-05,
      "loss": 0.0031,
      "step": 31270
    },
    {
      "epoch": 1.7377777777777776,
      "grad_norm": 0.013724368996918201,
      "learning_rate": 3.2622222222222225e-05,
      "loss": 0.0027,
      "step": 31280
    },
    {
      "epoch": 1.7383333333333333,
      "grad_norm": 0.3179699778556824,
      "learning_rate": 3.261666666666667e-05,
      "loss": 0.0045,
      "step": 31290
    },
    {
      "epoch": 1.738888888888889,
      "grad_norm": 0.20886263251304626,
      "learning_rate": 3.261111111111111e-05,
      "loss": 0.003,
      "step": 31300
    },
    {
      "epoch": 1.7394444444444446,
      "grad_norm": 0.27004483342170715,
      "learning_rate": 3.2605555555555556e-05,
      "loss": 0.0041,
      "step": 31310
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.05996229127049446,
      "learning_rate": 3.26e-05,
      "loss": 0.0029,
      "step": 31320
    },
    {
      "epoch": 1.7405555555555554,
      "grad_norm": 0.298761785030365,
      "learning_rate": 3.259444444444444e-05,
      "loss": 0.0041,
      "step": 31330
    },
    {
      "epoch": 1.741111111111111,
      "grad_norm": 0.838455855846405,
      "learning_rate": 3.2588888888888893e-05,
      "loss": 0.003,
      "step": 31340
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 0.27003586292266846,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.0028,
      "step": 31350
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 0.4332923889160156,
      "learning_rate": 3.257777777777778e-05,
      "loss": 0.0035,
      "step": 31360
    },
    {
      "epoch": 1.7427777777777778,
      "grad_norm": 0.32859471440315247,
      "learning_rate": 3.2572222222222224e-05,
      "loss": 0.0026,
      "step": 31370
    },
    {
      "epoch": 1.7433333333333332,
      "grad_norm": 0.031213417649269104,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0047,
      "step": 31380
    },
    {
      "epoch": 1.7438888888888888,
      "grad_norm": 0.36004552245140076,
      "learning_rate": 3.256111111111111e-05,
      "loss": 0.0034,
      "step": 31390
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 0.14978338778018951,
      "learning_rate": 3.2555555555555555e-05,
      "loss": 0.003,
      "step": 31400
    },
    {
      "epoch": 1.745,
      "grad_norm": 0.09068097174167633,
      "learning_rate": 3.2550000000000005e-05,
      "loss": 0.0035,
      "step": 31410
    },
    {
      "epoch": 1.7455555555555555,
      "grad_norm": 0.23999622464179993,
      "learning_rate": 3.254444444444444e-05,
      "loss": 0.0034,
      "step": 31420
    },
    {
      "epoch": 1.746111111111111,
      "grad_norm": 0.4817173480987549,
      "learning_rate": 3.253888888888889e-05,
      "loss": 0.0031,
      "step": 31430
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.1499294638633728,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0031,
      "step": 31440
    },
    {
      "epoch": 1.7472222222222222,
      "grad_norm": 0.2095351219177246,
      "learning_rate": 3.252777777777778e-05,
      "loss": 0.0034,
      "step": 31450
    },
    {
      "epoch": 1.7477777777777779,
      "grad_norm": 0.1556486040353775,
      "learning_rate": 3.252222222222222e-05,
      "loss": 0.0052,
      "step": 31460
    },
    {
      "epoch": 1.7483333333333333,
      "grad_norm": 0.2997315227985382,
      "learning_rate": 3.2516666666666666e-05,
      "loss": 0.0032,
      "step": 31470
    },
    {
      "epoch": 1.748888888888889,
      "grad_norm": 0.27031973004341125,
      "learning_rate": 3.251111111111112e-05,
      "loss": 0.0031,
      "step": 31480
    },
    {
      "epoch": 1.7494444444444444,
      "grad_norm": 0.11977854371070862,
      "learning_rate": 3.2505555555555554e-05,
      "loss": 0.0043,
      "step": 31490
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.09193910658359528,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0039,
      "step": 31500
    },
    {
      "epoch": 1.7505555555555556,
      "grad_norm": 0.2693459689617157,
      "learning_rate": 3.249444444444444e-05,
      "loss": 0.0035,
      "step": 31510
    },
    {
      "epoch": 1.751111111111111,
      "grad_norm": 0.12040276825428009,
      "learning_rate": 3.248888888888889e-05,
      "loss": 0.0035,
      "step": 31520
    },
    {
      "epoch": 1.7516666666666667,
      "grad_norm": 0.5030773878097534,
      "learning_rate": 3.2483333333333335e-05,
      "loss": 0.0032,
      "step": 31530
    },
    {
      "epoch": 1.7522222222222221,
      "grad_norm": 0.26904401183128357,
      "learning_rate": 3.247777777777778e-05,
      "loss": 0.0041,
      "step": 31540
    },
    {
      "epoch": 1.7527777777777778,
      "grad_norm": 0.01046407874673605,
      "learning_rate": 3.247222222222223e-05,
      "loss": 0.0057,
      "step": 31550
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.34147340059280396,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0042,
      "step": 31560
    },
    {
      "epoch": 1.753888888888889,
      "grad_norm": 0.27756616473197937,
      "learning_rate": 3.2461111111111116e-05,
      "loss": 0.0066,
      "step": 31570
    },
    {
      "epoch": 1.7544444444444445,
      "grad_norm": 0.09000134468078613,
      "learning_rate": 3.245555555555555e-05,
      "loss": 0.0025,
      "step": 31580
    },
    {
      "epoch": 1.755,
      "grad_norm": 0.007937777787446976,
      "learning_rate": 3.245e-05,
      "loss": 0.0028,
      "step": 31590
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.33020690083503723,
      "learning_rate": 3.2444444444444446e-05,
      "loss": 0.0051,
      "step": 31600
    },
    {
      "epoch": 1.7561111111111112,
      "grad_norm": 0.36853891611099243,
      "learning_rate": 3.243888888888889e-05,
      "loss": 0.0022,
      "step": 31610
    },
    {
      "epoch": 1.7566666666666668,
      "grad_norm": 0.062269534915685654,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.004,
      "step": 31620
    },
    {
      "epoch": 1.7572222222222222,
      "grad_norm": 0.3585848808288574,
      "learning_rate": 3.242777777777778e-05,
      "loss": 0.0033,
      "step": 31630
    },
    {
      "epoch": 1.7577777777777777,
      "grad_norm": 0.090188167989254,
      "learning_rate": 3.242222222222223e-05,
      "loss": 0.0042,
      "step": 31640
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 0.15013602375984192,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.0053,
      "step": 31650
    },
    {
      "epoch": 1.758888888888889,
      "grad_norm": 0.2990821301937103,
      "learning_rate": 3.2411111111111114e-05,
      "loss": 0.0049,
      "step": 31660
    },
    {
      "epoch": 1.7594444444444446,
      "grad_norm": 0.4809827506542206,
      "learning_rate": 3.240555555555556e-05,
      "loss": 0.0038,
      "step": 31670
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.09043614566326141,
      "learning_rate": 3.24e-05,
      "loss": 0.0036,
      "step": 31680
    },
    {
      "epoch": 1.7605555555555554,
      "grad_norm": 0.09034974128007889,
      "learning_rate": 3.2394444444444445e-05,
      "loss": 0.0034,
      "step": 31690
    },
    {
      "epoch": 1.761111111111111,
      "grad_norm": 0.06091757118701935,
      "learning_rate": 3.238888888888889e-05,
      "loss": 0.0036,
      "step": 31700
    },
    {
      "epoch": 1.7616666666666667,
      "grad_norm": 0.11935095489025116,
      "learning_rate": 3.238333333333333e-05,
      "loss": 0.0042,
      "step": 31710
    },
    {
      "epoch": 1.7622222222222224,
      "grad_norm": 0.30070143938064575,
      "learning_rate": 3.2377777777777776e-05,
      "loss": 0.004,
      "step": 31720
    },
    {
      "epoch": 1.7627777777777778,
      "grad_norm": 0.4789465069770813,
      "learning_rate": 3.2372222222222226e-05,
      "loss": 0.0045,
      "step": 31730
    },
    {
      "epoch": 1.7633333333333332,
      "grad_norm": 0.24516084790229797,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0048,
      "step": 31740
    },
    {
      "epoch": 1.7638888888888888,
      "grad_norm": 0.19076724350452423,
      "learning_rate": 3.236111111111111e-05,
      "loss": 0.0039,
      "step": 31750
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.033629514276981354,
      "learning_rate": 3.235555555555556e-05,
      "loss": 0.0024,
      "step": 31760
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 0.17992477118968964,
      "learning_rate": 3.235e-05,
      "loss": 0.0041,
      "step": 31770
    },
    {
      "epoch": 1.7655555555555555,
      "grad_norm": 0.17946569621562958,
      "learning_rate": 3.2344444444444444e-05,
      "loss": 0.0038,
      "step": 31780
    },
    {
      "epoch": 1.766111111111111,
      "grad_norm": 0.030448226258158684,
      "learning_rate": 3.2338888888888894e-05,
      "loss": 0.0032,
      "step": 31790
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.23862220346927643,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0038,
      "step": 31800
    },
    {
      "epoch": 1.7672222222222222,
      "grad_norm": 0.14987753331661224,
      "learning_rate": 3.232777777777778e-05,
      "loss": 0.0035,
      "step": 31810
    },
    {
      "epoch": 1.767777777777778,
      "grad_norm": 0.24383942782878876,
      "learning_rate": 3.2322222222222225e-05,
      "loss": 0.0039,
      "step": 31820
    },
    {
      "epoch": 1.7683333333333333,
      "grad_norm": 0.09025831520557404,
      "learning_rate": 3.231666666666667e-05,
      "loss": 0.0035,
      "step": 31830
    },
    {
      "epoch": 1.7688888888888887,
      "grad_norm": 0.38839760422706604,
      "learning_rate": 3.231111111111111e-05,
      "loss": 0.0043,
      "step": 31840
    },
    {
      "epoch": 1.7694444444444444,
      "grad_norm": 0.09636856615543365,
      "learning_rate": 3.2305555555555556e-05,
      "loss": 0.0045,
      "step": 31850
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.09066923707723618,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0046,
      "step": 31860
    },
    {
      "epoch": 1.7705555555555557,
      "grad_norm": 0.1871815323829651,
      "learning_rate": 3.229444444444444e-05,
      "loss": 0.0039,
      "step": 31870
    },
    {
      "epoch": 1.771111111111111,
      "grad_norm": 0.031711217015981674,
      "learning_rate": 3.228888888888889e-05,
      "loss": 0.0042,
      "step": 31880
    },
    {
      "epoch": 1.7716666666666665,
      "grad_norm": 0.149805948138237,
      "learning_rate": 3.2283333333333337e-05,
      "loss": 0.0032,
      "step": 31890
    },
    {
      "epoch": 1.7722222222222221,
      "grad_norm": 0.4787159264087677,
      "learning_rate": 3.227777777777778e-05,
      "loss": 0.0039,
      "step": 31900
    },
    {
      "epoch": 1.7727777777777778,
      "grad_norm": 0.1794039011001587,
      "learning_rate": 3.2272222222222224e-05,
      "loss": 0.0035,
      "step": 31910
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.03164922446012497,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0042,
      "step": 31920
    },
    {
      "epoch": 1.7738888888888888,
      "grad_norm": 0.01102354098111391,
      "learning_rate": 3.226111111111112e-05,
      "loss": 0.0036,
      "step": 31930
    },
    {
      "epoch": 1.7744444444444445,
      "grad_norm": 0.2320980578660965,
      "learning_rate": 3.2255555555555554e-05,
      "loss": 0.003,
      "step": 31940
    },
    {
      "epoch": 1.775,
      "grad_norm": 0.32840436697006226,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.0036,
      "step": 31950
    },
    {
      "epoch": 1.7755555555555556,
      "grad_norm": 0.1854896992444992,
      "learning_rate": 3.224444444444444e-05,
      "loss": 0.0037,
      "step": 31960
    },
    {
      "epoch": 1.7761111111111112,
      "grad_norm": 0.3901686370372772,
      "learning_rate": 3.223888888888889e-05,
      "loss": 0.0045,
      "step": 31970
    },
    {
      "epoch": 1.7766666666666666,
      "grad_norm": 0.24086451530456543,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.0056,
      "step": 31980
    },
    {
      "epoch": 1.7772222222222223,
      "grad_norm": 0.03209102526307106,
      "learning_rate": 3.222777777777778e-05,
      "loss": 0.0035,
      "step": 31990
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.35955533385276794,
      "learning_rate": 3.222222222222223e-05,
      "loss": 0.0047,
      "step": 32000
    },
    {
      "epoch": 1.7783333333333333,
      "grad_norm": 0.6593232154846191,
      "learning_rate": 3.2216666666666666e-05,
      "loss": 0.0057,
      "step": 32010
    },
    {
      "epoch": 1.778888888888889,
      "grad_norm": 0.1892181634902954,
      "learning_rate": 3.2211111111111116e-05,
      "loss": 0.0052,
      "step": 32020
    },
    {
      "epoch": 1.7794444444444446,
      "grad_norm": 0.12127173691987991,
      "learning_rate": 3.220555555555555e-05,
      "loss": 0.0044,
      "step": 32030
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.2101462483406067,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.005,
      "step": 32040
    },
    {
      "epoch": 1.7805555555555554,
      "grad_norm": 0.3595191538333893,
      "learning_rate": 3.219444444444445e-05,
      "loss": 0.0045,
      "step": 32050
    },
    {
      "epoch": 1.781111111111111,
      "grad_norm": 0.15001463890075684,
      "learning_rate": 3.218888888888889e-05,
      "loss": 0.0045,
      "step": 32060
    },
    {
      "epoch": 1.7816666666666667,
      "grad_norm": 0.032676104456186295,
      "learning_rate": 3.218333333333334e-05,
      "loss": 0.0038,
      "step": 32070
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 0.1468060463666916,
      "learning_rate": 3.217777777777778e-05,
      "loss": 0.0039,
      "step": 32080
    },
    {
      "epoch": 1.7827777777777778,
      "grad_norm": 0.12859882414340973,
      "learning_rate": 3.217222222222223e-05,
      "loss": 0.0043,
      "step": 32090
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.42011818289756775,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0037,
      "step": 32100
    },
    {
      "epoch": 1.7838888888888889,
      "grad_norm": 0.5104674696922302,
      "learning_rate": 3.2161111111111115e-05,
      "loss": 0.0032,
      "step": 32110
    },
    {
      "epoch": 1.7844444444444445,
      "grad_norm": 0.26947644352912903,
      "learning_rate": 3.215555555555556e-05,
      "loss": 0.0036,
      "step": 32120
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 0.2703443169593811,
      "learning_rate": 3.215e-05,
      "loss": 0.0052,
      "step": 32130
    },
    {
      "epoch": 1.7855555555555556,
      "grad_norm": 0.2768705189228058,
      "learning_rate": 3.2144444444444446e-05,
      "loss": 0.0033,
      "step": 32140
    },
    {
      "epoch": 1.786111111111111,
      "grad_norm": 0.06128277629613876,
      "learning_rate": 3.213888888888889e-05,
      "loss": 0.0028,
      "step": 32150
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.18041729927062988,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0041,
      "step": 32160
    },
    {
      "epoch": 1.7872222222222223,
      "grad_norm": 0.29925063252449036,
      "learning_rate": 3.2127777777777776e-05,
      "loss": 0.0046,
      "step": 32170
    },
    {
      "epoch": 1.787777777777778,
      "grad_norm": 0.29400986433029175,
      "learning_rate": 3.212222222222223e-05,
      "loss": 0.004,
      "step": 32180
    },
    {
      "epoch": 1.7883333333333333,
      "grad_norm": 0.558329701423645,
      "learning_rate": 3.211666666666667e-05,
      "loss": 0.0038,
      "step": 32190
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 0.033527176827192307,
      "learning_rate": 3.2111111111111114e-05,
      "loss": 0.004,
      "step": 32200
    },
    {
      "epoch": 1.7894444444444444,
      "grad_norm": 0.32804742455482483,
      "learning_rate": 3.210555555555556e-05,
      "loss": 0.0033,
      "step": 32210
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.036105699837207794,
      "learning_rate": 3.21e-05,
      "loss": 0.0046,
      "step": 32220
    },
    {
      "epoch": 1.7905555555555557,
      "grad_norm": 0.08960623294115067,
      "learning_rate": 3.2094444444444445e-05,
      "loss": 0.004,
      "step": 32230
    },
    {
      "epoch": 1.791111111111111,
      "grad_norm": 0.12127452343702316,
      "learning_rate": 3.208888888888889e-05,
      "loss": 0.0035,
      "step": 32240
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.6296176910400391,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.004,
      "step": 32250
    },
    {
      "epoch": 1.7922222222222222,
      "grad_norm": 0.032936159521341324,
      "learning_rate": 3.207777777777778e-05,
      "loss": 0.0038,
      "step": 32260
    },
    {
      "epoch": 1.7927777777777778,
      "grad_norm": 0.08977141976356506,
      "learning_rate": 3.2072222222222226e-05,
      "loss": 0.0048,
      "step": 32270
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.06135572865605354,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.004,
      "step": 32280
    },
    {
      "epoch": 1.7938888888888889,
      "grad_norm": 0.3323858678340912,
      "learning_rate": 3.206111111111111e-05,
      "loss": 0.0052,
      "step": 32290
    },
    {
      "epoch": 1.7944444444444443,
      "grad_norm": 0.5696913003921509,
      "learning_rate": 3.2055555555555556e-05,
      "loss": 0.0056,
      "step": 32300
    },
    {
      "epoch": 1.795,
      "grad_norm": 0.2985398769378662,
      "learning_rate": 3.205e-05,
      "loss": 0.0032,
      "step": 32310
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 0.11975573003292084,
      "learning_rate": 3.204444444444444e-05,
      "loss": 0.004,
      "step": 32320
    },
    {
      "epoch": 1.7961111111111112,
      "grad_norm": 0.03390328213572502,
      "learning_rate": 3.2038888888888894e-05,
      "loss": 0.0026,
      "step": 32330
    },
    {
      "epoch": 1.7966666666666666,
      "grad_norm": 0.20895586907863617,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0048,
      "step": 32340
    },
    {
      "epoch": 1.7972222222222223,
      "grad_norm": 0.06113061308860779,
      "learning_rate": 3.202777777777778e-05,
      "loss": 0.0032,
      "step": 32350
    },
    {
      "epoch": 1.7977777777777777,
      "grad_norm": 0.239935040473938,
      "learning_rate": 3.2022222222222224e-05,
      "loss": 0.0044,
      "step": 32360
    },
    {
      "epoch": 1.7983333333333333,
      "grad_norm": 0.598259687423706,
      "learning_rate": 3.201666666666667e-05,
      "loss": 0.0053,
      "step": 32370
    },
    {
      "epoch": 1.798888888888889,
      "grad_norm": 0.060683347284793854,
      "learning_rate": 3.201111111111111e-05,
      "loss": 0.0044,
      "step": 32380
    },
    {
      "epoch": 1.7994444444444444,
      "grad_norm": 0.20995719730854034,
      "learning_rate": 3.2005555555555555e-05,
      "loss": 0.0056,
      "step": 32390
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.3292315900325775,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0039,
      "step": 32400
    },
    {
      "epoch": 1.8005555555555555,
      "grad_norm": 0.30067571997642517,
      "learning_rate": 3.199444444444444e-05,
      "loss": 0.0053,
      "step": 32410
    },
    {
      "epoch": 1.801111111111111,
      "grad_norm": 0.3002931773662567,
      "learning_rate": 3.198888888888889e-05,
      "loss": 0.0044,
      "step": 32420
    },
    {
      "epoch": 1.8016666666666667,
      "grad_norm": 0.24165192246437073,
      "learning_rate": 3.1983333333333336e-05,
      "loss": 0.0034,
      "step": 32430
    },
    {
      "epoch": 1.8022222222222222,
      "grad_norm": 0.09004843980073929,
      "learning_rate": 3.197777777777778e-05,
      "loss": 0.0035,
      "step": 32440
    },
    {
      "epoch": 1.8027777777777778,
      "grad_norm": 0.30061525106430054,
      "learning_rate": 3.197222222222222e-05,
      "loss": 0.0046,
      "step": 32450
    },
    {
      "epoch": 1.8033333333333332,
      "grad_norm": 0.3881796598434448,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.004,
      "step": 32460
    },
    {
      "epoch": 1.8038888888888889,
      "grad_norm": 0.11970409750938416,
      "learning_rate": 3.196111111111112e-05,
      "loss": 0.0039,
      "step": 32470
    },
    {
      "epoch": 1.8044444444444445,
      "grad_norm": 0.18596050143241882,
      "learning_rate": 3.1955555555555554e-05,
      "loss": 0.0036,
      "step": 32480
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 0.06025773659348488,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.0035,
      "step": 32490
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 0.388924241065979,
      "learning_rate": 3.194444444444444e-05,
      "loss": 0.0034,
      "step": 32500
    },
    {
      "epoch": 1.806111111111111,
      "grad_norm": 0.4196016788482666,
      "learning_rate": 3.193888888888889e-05,
      "loss": 0.0035,
      "step": 32510
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.29822680354118347,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0027,
      "step": 32520
    },
    {
      "epoch": 1.8072222222222223,
      "grad_norm": 0.1821758896112442,
      "learning_rate": 3.192777777777778e-05,
      "loss": 0.0037,
      "step": 32530
    },
    {
      "epoch": 1.807777777777778,
      "grad_norm": 0.09020676463842392,
      "learning_rate": 3.192222222222223e-05,
      "loss": 0.0035,
      "step": 32540
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 0.0117338290438056,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.0043,
      "step": 32550
    },
    {
      "epoch": 1.8088888888888888,
      "grad_norm": 0.0317210778594017,
      "learning_rate": 3.1911111111111116e-05,
      "loss": 0.0022,
      "step": 32560
    },
    {
      "epoch": 1.8094444444444444,
      "grad_norm": 0.09080210328102112,
      "learning_rate": 3.190555555555555e-05,
      "loss": 0.004,
      "step": 32570
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.09042903035879135,
      "learning_rate": 3.19e-05,
      "loss": 0.0032,
      "step": 32580
    },
    {
      "epoch": 1.8105555555555557,
      "grad_norm": 0.6485760807991028,
      "learning_rate": 3.1894444444444446e-05,
      "loss": 0.0041,
      "step": 32590
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 0.15156550705432892,
      "learning_rate": 3.188888888888889e-05,
      "loss": 0.0034,
      "step": 32600
    },
    {
      "epoch": 1.8116666666666665,
      "grad_norm": 0.535789430141449,
      "learning_rate": 3.188333333333334e-05,
      "loss": 0.0036,
      "step": 32610
    },
    {
      "epoch": 1.8122222222222222,
      "grad_norm": 0.3008572459220886,
      "learning_rate": 3.187777777777778e-05,
      "loss": 0.0033,
      "step": 32620
    },
    {
      "epoch": 1.8127777777777778,
      "grad_norm": 0.6583232283592224,
      "learning_rate": 3.187222222222223e-05,
      "loss": 0.0049,
      "step": 32630
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.013356802985072136,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0032,
      "step": 32640
    },
    {
      "epoch": 1.8138888888888889,
      "grad_norm": 0.44876790046691895,
      "learning_rate": 3.1861111111111115e-05,
      "loss": 0.0037,
      "step": 32650
    },
    {
      "epoch": 1.8144444444444443,
      "grad_norm": 0.16462789475917816,
      "learning_rate": 3.185555555555556e-05,
      "loss": 0.0029,
      "step": 32660
    },
    {
      "epoch": 1.815,
      "grad_norm": 0.23909226059913635,
      "learning_rate": 3.185e-05,
      "loss": 0.0036,
      "step": 32670
    },
    {
      "epoch": 1.8155555555555556,
      "grad_norm": 0.4846631586551666,
      "learning_rate": 3.1844444444444445e-05,
      "loss": 0.0048,
      "step": 32680
    },
    {
      "epoch": 1.8161111111111112,
      "grad_norm": 0.10172343254089355,
      "learning_rate": 3.183888888888889e-05,
      "loss": 0.0043,
      "step": 32690
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.18183977901935577,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0052,
      "step": 32700
    },
    {
      "epoch": 1.817222222222222,
      "grad_norm": 0.23907265067100525,
      "learning_rate": 3.1827777777777776e-05,
      "loss": 0.0029,
      "step": 32710
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 0.015340516343712807,
      "learning_rate": 3.1822222222222226e-05,
      "loss": 0.0041,
      "step": 32720
    },
    {
      "epoch": 1.8183333333333334,
      "grad_norm": 0.1587379425764084,
      "learning_rate": 3.181666666666667e-05,
      "loss": 0.0035,
      "step": 32730
    },
    {
      "epoch": 1.818888888888889,
      "grad_norm": 0.09202069789171219,
      "learning_rate": 3.181111111111111e-05,
      "loss": 0.0043,
      "step": 32740
    },
    {
      "epoch": 1.8194444444444444,
      "grad_norm": 0.12029422074556351,
      "learning_rate": 3.180555555555556e-05,
      "loss": 0.0034,
      "step": 32750
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.06013144180178642,
      "learning_rate": 3.18e-05,
      "loss": 0.0045,
      "step": 32760
    },
    {
      "epoch": 1.8205555555555555,
      "grad_norm": 0.12023956328630447,
      "learning_rate": 3.1794444444444444e-05,
      "loss": 0.0049,
      "step": 32770
    },
    {
      "epoch": 1.8211111111111111,
      "grad_norm": 0.44940125942230225,
      "learning_rate": 3.178888888888889e-05,
      "loss": 0.0049,
      "step": 32780
    },
    {
      "epoch": 1.8216666666666668,
      "grad_norm": 0.26961448788642883,
      "learning_rate": 3.178333333333334e-05,
      "loss": 0.0043,
      "step": 32790
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.15064392983913422,
      "learning_rate": 3.177777777777778e-05,
      "loss": 0.0041,
      "step": 32800
    },
    {
      "epoch": 1.8227777777777778,
      "grad_norm": 0.19252774119377136,
      "learning_rate": 3.1772222222222225e-05,
      "loss": 0.004,
      "step": 32810
    },
    {
      "epoch": 1.8233333333333333,
      "grad_norm": 0.019659899175167084,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.004,
      "step": 32820
    },
    {
      "epoch": 1.823888888888889,
      "grad_norm": 0.27241793274879456,
      "learning_rate": 3.176111111111111e-05,
      "loss": 0.0034,
      "step": 32830
    },
    {
      "epoch": 1.8244444444444445,
      "grad_norm": 0.08212783187627792,
      "learning_rate": 3.1755555555555556e-05,
      "loss": 0.0028,
      "step": 32840
    },
    {
      "epoch": 1.825,
      "grad_norm": 0.41650769114494324,
      "learning_rate": 3.175e-05,
      "loss": 0.0043,
      "step": 32850
    },
    {
      "epoch": 1.8255555555555556,
      "grad_norm": 0.20969076454639435,
      "learning_rate": 3.174444444444444e-05,
      "loss": 0.0043,
      "step": 32860
    },
    {
      "epoch": 1.826111111111111,
      "grad_norm": 0.21138006448745728,
      "learning_rate": 3.173888888888889e-05,
      "loss": 0.0046,
      "step": 32870
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.037790920585393906,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.003,
      "step": 32880
    },
    {
      "epoch": 1.8272222222222223,
      "grad_norm": 0.3522326946258545,
      "learning_rate": 3.172777777777778e-05,
      "loss": 0.0045,
      "step": 32890
    },
    {
      "epoch": 1.8277777777777777,
      "grad_norm": 0.6587356328964233,
      "learning_rate": 3.1722222222222224e-05,
      "loss": 0.0037,
      "step": 32900
    },
    {
      "epoch": 1.8283333333333334,
      "grad_norm": 0.05645614117383957,
      "learning_rate": 3.171666666666667e-05,
      "loss": 0.0047,
      "step": 32910
    },
    {
      "epoch": 1.8288888888888888,
      "grad_norm": 0.09041372686624527,
      "learning_rate": 3.171111111111111e-05,
      "loss": 0.0038,
      "step": 32920
    },
    {
      "epoch": 1.8294444444444444,
      "grad_norm": 0.2399909943342209,
      "learning_rate": 3.1705555555555554e-05,
      "loss": 0.0059,
      "step": 32930
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.6889902949333191,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0048,
      "step": 32940
    },
    {
      "epoch": 1.8305555555555557,
      "grad_norm": 0.24080078303813934,
      "learning_rate": 3.169444444444444e-05,
      "loss": 0.0045,
      "step": 32950
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 0.7183211445808411,
      "learning_rate": 3.168888888888889e-05,
      "loss": 0.0039,
      "step": 32960
    },
    {
      "epoch": 1.8316666666666666,
      "grad_norm": 0.20903052389621735,
      "learning_rate": 3.1683333333333335e-05,
      "loss": 0.0036,
      "step": 32970
    },
    {
      "epoch": 1.8322222222222222,
      "grad_norm": 0.2693432569503784,
      "learning_rate": 3.167777777777778e-05,
      "loss": 0.0056,
      "step": 32980
    },
    {
      "epoch": 1.8327777777777778,
      "grad_norm": 0.14963006973266602,
      "learning_rate": 3.167222222222222e-05,
      "loss": 0.0033,
      "step": 32990
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.20963037014007568,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0046,
      "step": 33000
    },
    {
      "epoch": 1.833888888888889,
      "grad_norm": 0.022613203153014183,
      "learning_rate": 3.1661111111111116e-05,
      "loss": 0.0052,
      "step": 33010
    },
    {
      "epoch": 1.8344444444444443,
      "grad_norm": 0.21149535477161407,
      "learning_rate": 3.165555555555555e-05,
      "loss": 0.0056,
      "step": 33020
    },
    {
      "epoch": 1.835,
      "grad_norm": 0.24074387550354004,
      "learning_rate": 3.1650000000000004e-05,
      "loss": 0.0037,
      "step": 33030
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 0.35887011885643005,
      "learning_rate": 3.164444444444444e-05,
      "loss": 0.0047,
      "step": 33040
    },
    {
      "epoch": 1.8361111111111112,
      "grad_norm": 0.3880872428417206,
      "learning_rate": 3.163888888888889e-05,
      "loss": 0.0042,
      "step": 33050
    },
    {
      "epoch": 1.8366666666666667,
      "grad_norm": 0.11974674463272095,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.0048,
      "step": 33060
    },
    {
      "epoch": 1.837222222222222,
      "grad_norm": 0.27004382014274597,
      "learning_rate": 3.162777777777778e-05,
      "loss": 0.0039,
      "step": 33070
    },
    {
      "epoch": 1.8377777777777777,
      "grad_norm": 0.3291513919830322,
      "learning_rate": 3.162222222222223e-05,
      "loss": 0.0029,
      "step": 33080
    },
    {
      "epoch": 1.8383333333333334,
      "grad_norm": 0.2099277526140213,
      "learning_rate": 3.1616666666666665e-05,
      "loss": 0.0055,
      "step": 33090
    },
    {
      "epoch": 1.838888888888889,
      "grad_norm": 0.27053457498550415,
      "learning_rate": 3.1611111111111115e-05,
      "loss": 0.0041,
      "step": 33100
    },
    {
      "epoch": 1.8394444444444444,
      "grad_norm": 0.07088412344455719,
      "learning_rate": 3.160555555555555e-05,
      "loss": 0.0046,
      "step": 33110
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.1531977653503418,
      "learning_rate": 3.16e-05,
      "loss": 0.0045,
      "step": 33120
    },
    {
      "epoch": 1.8405555555555555,
      "grad_norm": 0.03148990496993065,
      "learning_rate": 3.1594444444444446e-05,
      "loss": 0.0039,
      "step": 33130
    },
    {
      "epoch": 1.8411111111111111,
      "grad_norm": 0.2920188903808594,
      "learning_rate": 3.158888888888889e-05,
      "loss": 0.0031,
      "step": 33140
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 0.031115679070353508,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.0031,
      "step": 33150
    },
    {
      "epoch": 1.8422222222222222,
      "grad_norm": 0.08983884751796722,
      "learning_rate": 3.1577777777777777e-05,
      "loss": 0.0042,
      "step": 33160
    },
    {
      "epoch": 1.8427777777777776,
      "grad_norm": 0.030967071652412415,
      "learning_rate": 3.157222222222223e-05,
      "loss": 0.0054,
      "step": 33170
    },
    {
      "epoch": 1.8433333333333333,
      "grad_norm": 0.20796054601669312,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0034,
      "step": 33180
    },
    {
      "epoch": 1.843888888888889,
      "grad_norm": 0.25198328495025635,
      "learning_rate": 3.1561111111111114e-05,
      "loss": 0.0046,
      "step": 33190
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 0.24797579646110535,
      "learning_rate": 3.155555555555556e-05,
      "loss": 0.0026,
      "step": 33200
    },
    {
      "epoch": 1.845,
      "grad_norm": 0.18096528947353363,
      "learning_rate": 3.155e-05,
      "loss": 0.0023,
      "step": 33210
    },
    {
      "epoch": 1.8455555555555554,
      "grad_norm": 0.3829255998134613,
      "learning_rate": 3.154444444444445e-05,
      "loss": 0.0031,
      "step": 33220
    },
    {
      "epoch": 1.846111111111111,
      "grad_norm": 0.29942819476127625,
      "learning_rate": 3.153888888888889e-05,
      "loss": 0.0041,
      "step": 33230
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.5978536605834961,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0033,
      "step": 33240
    },
    {
      "epoch": 1.8472222222222223,
      "grad_norm": 0.3591114282608032,
      "learning_rate": 3.1527777777777775e-05,
      "loss": 0.0041,
      "step": 33250
    },
    {
      "epoch": 1.8477777777777777,
      "grad_norm": 0.2199929803609848,
      "learning_rate": 3.1522222222222226e-05,
      "loss": 0.0032,
      "step": 33260
    },
    {
      "epoch": 1.8483333333333334,
      "grad_norm": 0.3887006938457489,
      "learning_rate": 3.151666666666667e-05,
      "loss": 0.0034,
      "step": 33270
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 0.09438848495483398,
      "learning_rate": 3.151111111111111e-05,
      "loss": 0.0048,
      "step": 33280
    },
    {
      "epoch": 1.8494444444444444,
      "grad_norm": 0.414567232131958,
      "learning_rate": 3.1505555555555556e-05,
      "loss": 0.003,
      "step": 33290
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.031179575249552727,
      "learning_rate": 3.15e-05,
      "loss": 0.0031,
      "step": 33300
    },
    {
      "epoch": 1.8505555555555555,
      "grad_norm": 0.15162880718708038,
      "learning_rate": 3.149444444444445e-05,
      "loss": 0.0037,
      "step": 33310
    },
    {
      "epoch": 1.8511111111111112,
      "grad_norm": 0.17267601191997528,
      "learning_rate": 3.148888888888889e-05,
      "loss": 0.0042,
      "step": 33320
    },
    {
      "epoch": 1.8516666666666666,
      "grad_norm": 0.4492756128311157,
      "learning_rate": 3.148333333333334e-05,
      "loss": 0.0036,
      "step": 33330
    },
    {
      "epoch": 1.8522222222222222,
      "grad_norm": 0.3878425061702728,
      "learning_rate": 3.147777777777778e-05,
      "loss": 0.0033,
      "step": 33340
    },
    {
      "epoch": 1.8527777777777779,
      "grad_norm": 0.4521731734275818,
      "learning_rate": 3.1472222222222225e-05,
      "loss": 0.0056,
      "step": 33350
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.09030518680810928,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0036,
      "step": 33360
    },
    {
      "epoch": 1.853888888888889,
      "grad_norm": 0.09032049030065536,
      "learning_rate": 3.146111111111111e-05,
      "loss": 0.0037,
      "step": 33370
    },
    {
      "epoch": 1.8544444444444443,
      "grad_norm": 0.008894965052604675,
      "learning_rate": 3.1455555555555555e-05,
      "loss": 0.0035,
      "step": 33380
    },
    {
      "epoch": 1.855,
      "grad_norm": 0.1986304223537445,
      "learning_rate": 3.145e-05,
      "loss": 0.0037,
      "step": 33390
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 0.5388286709785461,
      "learning_rate": 3.144444444444445e-05,
      "loss": 0.0058,
      "step": 33400
    },
    {
      "epoch": 1.8561111111111113,
      "grad_norm": 0.2993622124195099,
      "learning_rate": 3.143888888888889e-05,
      "loss": 0.0041,
      "step": 33410
    },
    {
      "epoch": 1.8566666666666667,
      "grad_norm": 0.5448992848396301,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.0036,
      "step": 33420
    },
    {
      "epoch": 1.857222222222222,
      "grad_norm": 0.20055809617042542,
      "learning_rate": 3.142777777777778e-05,
      "loss": 0.0027,
      "step": 33430
    },
    {
      "epoch": 1.8577777777777778,
      "grad_norm": 0.2098357379436493,
      "learning_rate": 3.142222222222222e-05,
      "loss": 0.0045,
      "step": 33440
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 0.06113913282752037,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.005,
      "step": 33450
    },
    {
      "epoch": 1.858888888888889,
      "grad_norm": 0.011626139283180237,
      "learning_rate": 3.141111111111111e-05,
      "loss": 0.0045,
      "step": 33460
    },
    {
      "epoch": 1.8594444444444445,
      "grad_norm": 0.478762149810791,
      "learning_rate": 3.1405555555555554e-05,
      "loss": 0.0034,
      "step": 33470
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.2990587651729584,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0033,
      "step": 33480
    },
    {
      "epoch": 1.8605555555555555,
      "grad_norm": 0.14955930411815643,
      "learning_rate": 3.139444444444445e-05,
      "loss": 0.005,
      "step": 33490
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 0.29363834857940674,
      "learning_rate": 3.138888888888889e-05,
      "loss": 0.0035,
      "step": 33500
    },
    {
      "epoch": 1.8616666666666668,
      "grad_norm": 0.12014938145875931,
      "learning_rate": 3.1383333333333335e-05,
      "loss": 0.0047,
      "step": 33510
    },
    {
      "epoch": 1.8622222222222222,
      "grad_norm": 0.06042376533150673,
      "learning_rate": 3.137777777777778e-05,
      "loss": 0.0052,
      "step": 33520
    },
    {
      "epoch": 1.8627777777777776,
      "grad_norm": 0.5683584809303284,
      "learning_rate": 3.137222222222222e-05,
      "loss": 0.0022,
      "step": 33530
    },
    {
      "epoch": 1.8633333333333333,
      "grad_norm": 0.2689193785190582,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0035,
      "step": 33540
    },
    {
      "epoch": 1.863888888888889,
      "grad_norm": 0.15119680762290955,
      "learning_rate": 3.1361111111111116e-05,
      "loss": 0.0036,
      "step": 33550
    },
    {
      "epoch": 1.8644444444444446,
      "grad_norm": 0.4219970703125,
      "learning_rate": 3.135555555555555e-05,
      "loss": 0.0049,
      "step": 33560
    },
    {
      "epoch": 1.865,
      "grad_norm": 0.12131908535957336,
      "learning_rate": 3.135e-05,
      "loss": 0.0054,
      "step": 33570
    },
    {
      "epoch": 1.8655555555555554,
      "grad_norm": 0.29895350337028503,
      "learning_rate": 3.134444444444445e-05,
      "loss": 0.0035,
      "step": 33580
    },
    {
      "epoch": 1.866111111111111,
      "grad_norm": 0.26936766505241394,
      "learning_rate": 3.133888888888889e-05,
      "loss": 0.003,
      "step": 33590
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.34425678849220276,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0027,
      "step": 33600
    },
    {
      "epoch": 1.8672222222222223,
      "grad_norm": 0.2391064316034317,
      "learning_rate": 3.132777777777778e-05,
      "loss": 0.0035,
      "step": 33610
    },
    {
      "epoch": 1.8677777777777778,
      "grad_norm": 0.017232926562428474,
      "learning_rate": 3.132222222222223e-05,
      "loss": 0.0042,
      "step": 33620
    },
    {
      "epoch": 1.8683333333333332,
      "grad_norm": 0.46604007482528687,
      "learning_rate": 3.1316666666666664e-05,
      "loss": 0.0034,
      "step": 33630
    },
    {
      "epoch": 1.8688888888888888,
      "grad_norm": 0.1794649362564087,
      "learning_rate": 3.1311111111111115e-05,
      "loss": 0.0037,
      "step": 33640
    },
    {
      "epoch": 1.8694444444444445,
      "grad_norm": 0.151935875415802,
      "learning_rate": 3.130555555555555e-05,
      "loss": 0.0031,
      "step": 33650
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.5977420806884766,
      "learning_rate": 3.13e-05,
      "loss": 0.0036,
      "step": 33660
    },
    {
      "epoch": 1.8705555555555555,
      "grad_norm": 0.03293656185269356,
      "learning_rate": 3.1294444444444445e-05,
      "loss": 0.0042,
      "step": 33670
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 0.26940613985061646,
      "learning_rate": 3.128888888888889e-05,
      "loss": 0.006,
      "step": 33680
    },
    {
      "epoch": 1.8716666666666666,
      "grad_norm": 0.060194943100214005,
      "learning_rate": 3.128333333333334e-05,
      "loss": 0.0041,
      "step": 33690
    },
    {
      "epoch": 1.8722222222222222,
      "grad_norm": 0.29875433444976807,
      "learning_rate": 3.1277777777777776e-05,
      "loss": 0.0035,
      "step": 33700
    },
    {
      "epoch": 1.8727777777777779,
      "grad_norm": 0.1791885793209076,
      "learning_rate": 3.1272222222222226e-05,
      "loss": 0.0036,
      "step": 33710
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.42651161551475525,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0039,
      "step": 33720
    },
    {
      "epoch": 1.873888888888889,
      "grad_norm": 0.10138435661792755,
      "learning_rate": 3.1261111111111114e-05,
      "loss": 0.0039,
      "step": 33730
    },
    {
      "epoch": 1.8744444444444444,
      "grad_norm": 0.715951681137085,
      "learning_rate": 3.125555555555556e-05,
      "loss": 0.0032,
      "step": 33740
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.3548000156879425,
      "learning_rate": 3.125e-05,
      "loss": 0.0039,
      "step": 33750
    },
    {
      "epoch": 1.8755555555555556,
      "grad_norm": 0.2501906454563141,
      "learning_rate": 3.124444444444445e-05,
      "loss": 0.0031,
      "step": 33760
    },
    {
      "epoch": 1.876111111111111,
      "grad_norm": 0.359165221452713,
      "learning_rate": 3.123888888888889e-05,
      "loss": 0.0029,
      "step": 33770
    },
    {
      "epoch": 1.8766666666666667,
      "grad_norm": 0.17905819416046143,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0042,
      "step": 33780
    },
    {
      "epoch": 1.8772222222222221,
      "grad_norm": 0.17901097238063812,
      "learning_rate": 3.1227777777777775e-05,
      "loss": 0.0043,
      "step": 33790
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 0.03201813995838165,
      "learning_rate": 3.1222222222222225e-05,
      "loss": 0.0049,
      "step": 33800
    },
    {
      "epoch": 1.8783333333333334,
      "grad_norm": 0.14992651343345642,
      "learning_rate": 3.121666666666667e-05,
      "loss": 0.0045,
      "step": 33810
    },
    {
      "epoch": 1.878888888888889,
      "grad_norm": 0.4037017822265625,
      "learning_rate": 3.121111111111111e-05,
      "loss": 0.0033,
      "step": 33820
    },
    {
      "epoch": 1.8794444444444445,
      "grad_norm": 0.14969488978385925,
      "learning_rate": 3.1205555555555556e-05,
      "loss": 0.0024,
      "step": 33830
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.5816800594329834,
      "learning_rate": 3.12e-05,
      "loss": 0.0032,
      "step": 33840
    },
    {
      "epoch": 1.8805555555555555,
      "grad_norm": 0.6271365284919739,
      "learning_rate": 3.119444444444445e-05,
      "loss": 0.0039,
      "step": 33850
    },
    {
      "epoch": 1.8811111111111112,
      "grad_norm": 0.2001076489686966,
      "learning_rate": 3.1188888888888887e-05,
      "loss": 0.0035,
      "step": 33860
    },
    {
      "epoch": 1.8816666666666668,
      "grad_norm": 0.10226034373044968,
      "learning_rate": 3.118333333333334e-05,
      "loss": 0.0035,
      "step": 33870
    },
    {
      "epoch": 1.8822222222222222,
      "grad_norm": 0.037065181881189346,
      "learning_rate": 3.117777777777778e-05,
      "loss": 0.0049,
      "step": 33880
    },
    {
      "epoch": 1.8827777777777777,
      "grad_norm": 0.2925625145435333,
      "learning_rate": 3.1172222222222224e-05,
      "loss": 0.0045,
      "step": 33890
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.41793403029441833,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.0019,
      "step": 33900
    },
    {
      "epoch": 1.883888888888889,
      "grad_norm": 0.010550539940595627,
      "learning_rate": 3.116111111111111e-05,
      "loss": 0.0037,
      "step": 33910
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 0.2375757247209549,
      "learning_rate": 3.1155555555555555e-05,
      "loss": 0.004,
      "step": 33920
    },
    {
      "epoch": 1.885,
      "grad_norm": 0.35911640524864197,
      "learning_rate": 3.115e-05,
      "loss": 0.0035,
      "step": 33930
    },
    {
      "epoch": 1.8855555555555554,
      "grad_norm": 0.418620228767395,
      "learning_rate": 3.114444444444445e-05,
      "loss": 0.0029,
      "step": 33940
    },
    {
      "epoch": 1.886111111111111,
      "grad_norm": 0.41839441657066345,
      "learning_rate": 3.113888888888889e-05,
      "loss": 0.0048,
      "step": 33950
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.33849942684173584,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.005,
      "step": 33960
    },
    {
      "epoch": 1.8872222222222224,
      "grad_norm": 0.38875770568847656,
      "learning_rate": 3.112777777777778e-05,
      "loss": 0.0046,
      "step": 33970
    },
    {
      "epoch": 1.8877777777777778,
      "grad_norm": 0.14478886127471924,
      "learning_rate": 3.112222222222222e-05,
      "loss": 0.0045,
      "step": 33980
    },
    {
      "epoch": 1.8883333333333332,
      "grad_norm": 0.17948611080646515,
      "learning_rate": 3.1116666666666666e-05,
      "loss": 0.0045,
      "step": 33990
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.12757399678230286,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.0045,
      "step": 34000
    },
    {
      "epoch": 1.8894444444444445,
      "grad_norm": 0.14953097701072693,
      "learning_rate": 3.1105555555555553e-05,
      "loss": 0.0035,
      "step": 34010
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.27675312757492065,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.0028,
      "step": 34020
    },
    {
      "epoch": 1.8905555555555555,
      "grad_norm": 0.13930068910121918,
      "learning_rate": 3.109444444444445e-05,
      "loss": 0.0035,
      "step": 34030
    },
    {
      "epoch": 1.891111111111111,
      "grad_norm": 0.08239966630935669,
      "learning_rate": 3.108888888888889e-05,
      "loss": 0.0034,
      "step": 34040
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 0.6579404473304749,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.004,
      "step": 34050
    },
    {
      "epoch": 1.8922222222222222,
      "grad_norm": 0.18731153011322021,
      "learning_rate": 3.107777777777778e-05,
      "loss": 0.0044,
      "step": 34060
    },
    {
      "epoch": 1.892777777777778,
      "grad_norm": 0.033305324614048004,
      "learning_rate": 3.107222222222222e-05,
      "loss": 0.0039,
      "step": 34070
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.1800844818353653,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0032,
      "step": 34080
    },
    {
      "epoch": 1.8938888888888887,
      "grad_norm": 0.3594321012496948,
      "learning_rate": 3.1061111111111115e-05,
      "loss": 0.0043,
      "step": 34090
    },
    {
      "epoch": 1.8944444444444444,
      "grad_norm": 0.03210834413766861,
      "learning_rate": 3.105555555555555e-05,
      "loss": 0.0048,
      "step": 34100
    },
    {
      "epoch": 1.895,
      "grad_norm": 0.5399067401885986,
      "learning_rate": 3.105e-05,
      "loss": 0.0039,
      "step": 34110
    },
    {
      "epoch": 1.8955555555555557,
      "grad_norm": 0.0325949527323246,
      "learning_rate": 3.1044444444444446e-05,
      "loss": 0.0049,
      "step": 34120
    },
    {
      "epoch": 1.896111111111111,
      "grad_norm": 0.12077867239713669,
      "learning_rate": 3.103888888888889e-05,
      "loss": 0.004,
      "step": 34130
    },
    {
      "epoch": 1.8966666666666665,
      "grad_norm": 0.2996630072593689,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0021,
      "step": 34140
    },
    {
      "epoch": 1.8972222222222221,
      "grad_norm": 0.29895392060279846,
      "learning_rate": 3.102777777777778e-05,
      "loss": 0.0022,
      "step": 34150
    },
    {
      "epoch": 1.8977777777777778,
      "grad_norm": 0.16514486074447632,
      "learning_rate": 3.102222222222223e-05,
      "loss": 0.0043,
      "step": 34160
    },
    {
      "epoch": 1.8983333333333334,
      "grad_norm": 0.4186408221721649,
      "learning_rate": 3.1016666666666664e-05,
      "loss": 0.0028,
      "step": 34170
    },
    {
      "epoch": 1.8988888888888888,
      "grad_norm": 0.18204179406166077,
      "learning_rate": 3.1011111111111114e-05,
      "loss": 0.0035,
      "step": 34180
    },
    {
      "epoch": 1.8994444444444445,
      "grad_norm": 0.18017174303531647,
      "learning_rate": 3.100555555555555e-05,
      "loss": 0.0029,
      "step": 34190
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.6002060174942017,
      "learning_rate": 3.1e-05,
      "loss": 0.0033,
      "step": 34200
    },
    {
      "epoch": 1.9005555555555556,
      "grad_norm": 0.24131518602371216,
      "learning_rate": 3.0994444444444445e-05,
      "loss": 0.0028,
      "step": 34210
    },
    {
      "epoch": 1.9011111111111112,
      "grad_norm": 0.2809258699417114,
      "learning_rate": 3.098888888888889e-05,
      "loss": 0.0021,
      "step": 34220
    },
    {
      "epoch": 1.9016666666666666,
      "grad_norm": 0.2099333256483078,
      "learning_rate": 3.098333333333334e-05,
      "loss": 0.0042,
      "step": 34230
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 0.3298255503177643,
      "learning_rate": 3.0977777777777776e-05,
      "loss": 0.0034,
      "step": 34240
    },
    {
      "epoch": 1.9027777777777777,
      "grad_norm": 0.06028297170996666,
      "learning_rate": 3.0972222222222226e-05,
      "loss": 0.0037,
      "step": 34250
    },
    {
      "epoch": 1.9033333333333333,
      "grad_norm": 0.3284546434879303,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.004,
      "step": 34260
    },
    {
      "epoch": 1.903888888888889,
      "grad_norm": 0.15112783014774323,
      "learning_rate": 3.096111111111111e-05,
      "loss": 0.0033,
      "step": 34270
    },
    {
      "epoch": 1.9044444444444446,
      "grad_norm": 0.08967455476522446,
      "learning_rate": 3.0955555555555557e-05,
      "loss": 0.0026,
      "step": 34280
    },
    {
      "epoch": 1.905,
      "grad_norm": 0.3214663863182068,
      "learning_rate": 3.095e-05,
      "loss": 0.0031,
      "step": 34290
    },
    {
      "epoch": 1.9055555555555554,
      "grad_norm": 0.47984832525253296,
      "learning_rate": 3.094444444444445e-05,
      "loss": 0.0047,
      "step": 34300
    },
    {
      "epoch": 1.906111111111111,
      "grad_norm": 0.12246942520141602,
      "learning_rate": 3.093888888888889e-05,
      "loss": 0.0034,
      "step": 34310
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.22197787463665009,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0046,
      "step": 34320
    },
    {
      "epoch": 1.9072222222222224,
      "grad_norm": 0.6614513397216797,
      "learning_rate": 3.0927777777777774e-05,
      "loss": 0.0067,
      "step": 34330
    },
    {
      "epoch": 1.9077777777777778,
      "grad_norm": 0.11982889473438263,
      "learning_rate": 3.0922222222222225e-05,
      "loss": 0.005,
      "step": 34340
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 0.1797919124364853,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.0043,
      "step": 34350
    },
    {
      "epoch": 1.9088888888888889,
      "grad_norm": 0.5683038830757141,
      "learning_rate": 3.091111111111111e-05,
      "loss": 0.0038,
      "step": 34360
    },
    {
      "epoch": 1.9094444444444445,
      "grad_norm": 0.24330469965934753,
      "learning_rate": 3.0905555555555555e-05,
      "loss": 0.0044,
      "step": 34370
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.18448051810264587,
      "learning_rate": 3.09e-05,
      "loss": 0.0042,
      "step": 34380
    },
    {
      "epoch": 1.9105555555555556,
      "grad_norm": 0.3893868923187256,
      "learning_rate": 3.089444444444445e-05,
      "loss": 0.003,
      "step": 34390
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.03262908011674881,
      "learning_rate": 3.088888888888889e-05,
      "loss": 0.0046,
      "step": 34400
    },
    {
      "epoch": 1.9116666666666666,
      "grad_norm": 0.1847817301750183,
      "learning_rate": 3.0883333333333336e-05,
      "loss": 0.0055,
      "step": 34410
    },
    {
      "epoch": 1.9122222222222223,
      "grad_norm": 0.3589828312397003,
      "learning_rate": 3.087777777777778e-05,
      "loss": 0.0062,
      "step": 34420
    },
    {
      "epoch": 1.912777777777778,
      "grad_norm": 0.3425006568431854,
      "learning_rate": 3.0872222222222223e-05,
      "loss": 0.0028,
      "step": 34430
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.14983497560024261,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0033,
      "step": 34440
    },
    {
      "epoch": 1.9138888888888888,
      "grad_norm": 0.4196743369102478,
      "learning_rate": 3.086111111111111e-05,
      "loss": 0.0042,
      "step": 34450
    },
    {
      "epoch": 1.9144444444444444,
      "grad_norm": 0.08996888250112534,
      "learning_rate": 3.085555555555556e-05,
      "loss": 0.0032,
      "step": 34460
    },
    {
      "epoch": 1.915,
      "grad_norm": 0.181542307138443,
      "learning_rate": 3.0850000000000004e-05,
      "loss": 0.0041,
      "step": 34470
    },
    {
      "epoch": 1.9155555555555557,
      "grad_norm": 0.38092392683029175,
      "learning_rate": 3.084444444444445e-05,
      "loss": 0.0034,
      "step": 34480
    },
    {
      "epoch": 1.916111111111111,
      "grad_norm": 0.22092102468013763,
      "learning_rate": 3.083888888888889e-05,
      "loss": 0.004,
      "step": 34490
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.1764535754919052,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0038,
      "step": 34500
    },
    {
      "epoch": 1.9172222222222222,
      "grad_norm": 0.3889918327331543,
      "learning_rate": 3.082777777777778e-05,
      "loss": 0.0046,
      "step": 34510
    },
    {
      "epoch": 1.9177777777777778,
      "grad_norm": 0.06028785556554794,
      "learning_rate": 3.082222222222222e-05,
      "loss": 0.0051,
      "step": 34520
    },
    {
      "epoch": 1.9183333333333334,
      "grad_norm": 0.2698642313480377,
      "learning_rate": 3.0816666666666666e-05,
      "loss": 0.0031,
      "step": 34530
    },
    {
      "epoch": 1.9188888888888889,
      "grad_norm": 0.29931825399398804,
      "learning_rate": 3.0811111111111116e-05,
      "loss": 0.0037,
      "step": 34540
    },
    {
      "epoch": 1.9194444444444443,
      "grad_norm": 0.5390440821647644,
      "learning_rate": 3.080555555555556e-05,
      "loss": 0.0037,
      "step": 34550
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.09177791327238083,
      "learning_rate": 3.08e-05,
      "loss": 0.0051,
      "step": 34560
    },
    {
      "epoch": 1.9205555555555556,
      "grad_norm": 0.12011724710464478,
      "learning_rate": 3.079444444444445e-05,
      "loss": 0.0036,
      "step": 34570
    },
    {
      "epoch": 1.9211111111111112,
      "grad_norm": 0.18011099100112915,
      "learning_rate": 3.078888888888889e-05,
      "loss": 0.0029,
      "step": 34580
    },
    {
      "epoch": 1.9216666666666666,
      "grad_norm": 0.06195305287837982,
      "learning_rate": 3.0783333333333334e-05,
      "loss": 0.003,
      "step": 34590
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 0.09129610657691956,
      "learning_rate": 3.077777777777778e-05,
      "loss": 0.0039,
      "step": 34600
    },
    {
      "epoch": 1.9227777777777777,
      "grad_norm": 0.23887048661708832,
      "learning_rate": 3.077222222222223e-05,
      "loss": 0.004,
      "step": 34610
    },
    {
      "epoch": 1.9233333333333333,
      "grad_norm": 0.060461074113845825,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.0029,
      "step": 34620
    },
    {
      "epoch": 1.923888888888889,
      "grad_norm": 0.49257782101631165,
      "learning_rate": 3.0761111111111115e-05,
      "loss": 0.0039,
      "step": 34630
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 0.3156166672706604,
      "learning_rate": 3.075555555555556e-05,
      "loss": 0.0022,
      "step": 34640
    },
    {
      "epoch": 1.925,
      "grad_norm": 0.251030296087265,
      "learning_rate": 3.075e-05,
      "loss": 0.0041,
      "step": 34650
    },
    {
      "epoch": 1.9255555555555555,
      "grad_norm": 0.32934024930000305,
      "learning_rate": 3.0744444444444446e-05,
      "loss": 0.0048,
      "step": 34660
    },
    {
      "epoch": 1.926111111111111,
      "grad_norm": 0.061676058918237686,
      "learning_rate": 3.073888888888889e-05,
      "loss": 0.0036,
      "step": 34670
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.06062961742281914,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.004,
      "step": 34680
    },
    {
      "epoch": 1.9272222222222222,
      "grad_norm": 0.2398490458726883,
      "learning_rate": 3.0727777777777776e-05,
      "loss": 0.0032,
      "step": 34690
    },
    {
      "epoch": 1.9277777777777778,
      "grad_norm": 0.11994044482707977,
      "learning_rate": 3.0722222222222227e-05,
      "loss": 0.0041,
      "step": 34700
    },
    {
      "epoch": 1.9283333333333332,
      "grad_norm": 0.26918143033981323,
      "learning_rate": 3.0716666666666663e-05,
      "loss": 0.0045,
      "step": 34710
    },
    {
      "epoch": 1.9288888888888889,
      "grad_norm": 0.2613891363143921,
      "learning_rate": 3.0711111111111114e-05,
      "loss": 0.0036,
      "step": 34720
    },
    {
      "epoch": 1.9294444444444445,
      "grad_norm": 0.32873257994651794,
      "learning_rate": 3.070555555555556e-05,
      "loss": 0.0022,
      "step": 34730
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.32945379614830017,
      "learning_rate": 3.07e-05,
      "loss": 0.0041,
      "step": 34740
    },
    {
      "epoch": 1.9305555555555556,
      "grad_norm": 0.32855379581451416,
      "learning_rate": 3.069444444444445e-05,
      "loss": 0.0033,
      "step": 34750
    },
    {
      "epoch": 1.931111111111111,
      "grad_norm": 0.12146169692277908,
      "learning_rate": 3.068888888888889e-05,
      "loss": 0.0053,
      "step": 34760
    },
    {
      "epoch": 1.9316666666666666,
      "grad_norm": 0.3591216206550598,
      "learning_rate": 3.068333333333334e-05,
      "loss": 0.0034,
      "step": 34770
    },
    {
      "epoch": 1.9322222222222223,
      "grad_norm": 0.036433130502700806,
      "learning_rate": 3.0677777777777775e-05,
      "loss": 0.0037,
      "step": 34780
    },
    {
      "epoch": 1.932777777777778,
      "grad_norm": 0.1336519420146942,
      "learning_rate": 3.0672222222222225e-05,
      "loss": 0.0043,
      "step": 34790
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.19448521733283997,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0042,
      "step": 34800
    },
    {
      "epoch": 1.9338888888888888,
      "grad_norm": 0.5078281760215759,
      "learning_rate": 3.066111111111111e-05,
      "loss": 0.0031,
      "step": 34810
    },
    {
      "epoch": 1.9344444444444444,
      "grad_norm": 0.2388972043991089,
      "learning_rate": 3.065555555555556e-05,
      "loss": 0.0035,
      "step": 34820
    },
    {
      "epoch": 1.935,
      "grad_norm": 0.030780747532844543,
      "learning_rate": 3.065e-05,
      "loss": 0.0027,
      "step": 34830
    },
    {
      "epoch": 1.9355555555555557,
      "grad_norm": 0.07445958256721497,
      "learning_rate": 3.064444444444445e-05,
      "loss": 0.0038,
      "step": 34840
    },
    {
      "epoch": 1.9361111111111111,
      "grad_norm": 0.2732708156108856,
      "learning_rate": 3.063888888888889e-05,
      "loss": 0.0039,
      "step": 34850
    },
    {
      "epoch": 1.9366666666666665,
      "grad_norm": 0.28331220149993896,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0033,
      "step": 34860
    },
    {
      "epoch": 1.9372222222222222,
      "grad_norm": 0.030648652464151382,
      "learning_rate": 3.062777777777778e-05,
      "loss": 0.0027,
      "step": 34870
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 0.388527512550354,
      "learning_rate": 3.0622222222222224e-05,
      "loss": 0.0035,
      "step": 34880
    },
    {
      "epoch": 1.9383333333333335,
      "grad_norm": 0.17985409498214722,
      "learning_rate": 3.061666666666667e-05,
      "loss": 0.0036,
      "step": 34890
    },
    {
      "epoch": 1.9388888888888889,
      "grad_norm": 0.3845665454864502,
      "learning_rate": 3.061111111111111e-05,
      "loss": 0.0025,
      "step": 34900
    },
    {
      "epoch": 1.9394444444444443,
      "grad_norm": 0.5399911999702454,
      "learning_rate": 3.060555555555556e-05,
      "loss": 0.0035,
      "step": 34910
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.0905277281999588,
      "learning_rate": 3.06e-05,
      "loss": 0.0039,
      "step": 34920
    },
    {
      "epoch": 1.9405555555555556,
      "grad_norm": 0.2422376126050949,
      "learning_rate": 3.059444444444445e-05,
      "loss": 0.0028,
      "step": 34930
    },
    {
      "epoch": 1.9411111111111112,
      "grad_norm": 0.11958092451095581,
      "learning_rate": 3.058888888888889e-05,
      "loss": 0.0042,
      "step": 34940
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 0.12096031755208969,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.0042,
      "step": 34950
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 0.0324106328189373,
      "learning_rate": 3.057777777777778e-05,
      "loss": 0.0034,
      "step": 34960
    },
    {
      "epoch": 1.9427777777777777,
      "grad_norm": 0.04238525405526161,
      "learning_rate": 3.057222222222222e-05,
      "loss": 0.0044,
      "step": 34970
    },
    {
      "epoch": 1.9433333333333334,
      "grad_norm": 0.20953036844730377,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0032,
      "step": 34980
    },
    {
      "epoch": 1.943888888888889,
      "grad_norm": 0.47869017720222473,
      "learning_rate": 3.056111111111111e-05,
      "loss": 0.0029,
      "step": 34990
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.6227093935012817,
      "learning_rate": 3.055555555555556e-05,
      "loss": 0.0043,
      "step": 35000
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 0.3869471549987793,
      "learning_rate": 3.0550000000000004e-05,
      "loss": 0.0041,
      "step": 35010
    },
    {
      "epoch": 1.9455555555555555,
      "grad_norm": 0.2694005072116852,
      "learning_rate": 3.054444444444445e-05,
      "loss": 0.0029,
      "step": 35020
    },
    {
      "epoch": 1.9461111111111111,
      "grad_norm": 0.35907846689224243,
      "learning_rate": 3.053888888888889e-05,
      "loss": 0.0038,
      "step": 35030
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.1509726196527481,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0037,
      "step": 35040
    },
    {
      "epoch": 1.9472222222222222,
      "grad_norm": 0.38802582025527954,
      "learning_rate": 3.052777777777778e-05,
      "loss": 0.0038,
      "step": 35050
    },
    {
      "epoch": 1.9477777777777778,
      "grad_norm": 0.061681944876909256,
      "learning_rate": 3.052222222222222e-05,
      "loss": 0.0044,
      "step": 35060
    },
    {
      "epoch": 1.9483333333333333,
      "grad_norm": 0.269060879945755,
      "learning_rate": 3.0516666666666665e-05,
      "loss": 0.0027,
      "step": 35070
    },
    {
      "epoch": 1.948888888888889,
      "grad_norm": 0.03642963618040085,
      "learning_rate": 3.0511111111111112e-05,
      "loss": 0.0036,
      "step": 35080
    },
    {
      "epoch": 1.9494444444444445,
      "grad_norm": 0.12091151624917984,
      "learning_rate": 3.050555555555556e-05,
      "loss": 0.0038,
      "step": 35090
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.059781428426504135,
      "learning_rate": 3.05e-05,
      "loss": 0.0037,
      "step": 35100
    },
    {
      "epoch": 1.9505555555555556,
      "grad_norm": 0.12063596397638321,
      "learning_rate": 3.0494444444444446e-05,
      "loss": 0.0038,
      "step": 35110
    },
    {
      "epoch": 1.951111111111111,
      "grad_norm": 0.09019457548856735,
      "learning_rate": 3.048888888888889e-05,
      "loss": 0.0026,
      "step": 35120
    },
    {
      "epoch": 1.9516666666666667,
      "grad_norm": 0.3587051331996918,
      "learning_rate": 3.0483333333333337e-05,
      "loss": 0.0043,
      "step": 35130
    },
    {
      "epoch": 1.9522222222222223,
      "grad_norm": 0.360856831073761,
      "learning_rate": 3.0477777777777777e-05,
      "loss": 0.0028,
      "step": 35140
    },
    {
      "epoch": 1.9527777777777777,
      "grad_norm": 0.5475490093231201,
      "learning_rate": 3.0472222222222224e-05,
      "loss": 0.0037,
      "step": 35150
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.6872944831848145,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.003,
      "step": 35160
    },
    {
      "epoch": 1.9538888888888888,
      "grad_norm": 0.26211851835250854,
      "learning_rate": 3.046111111111111e-05,
      "loss": 0.0035,
      "step": 35170
    },
    {
      "epoch": 1.9544444444444444,
      "grad_norm": 0.33005261421203613,
      "learning_rate": 3.0455555555555558e-05,
      "loss": 0.0039,
      "step": 35180
    },
    {
      "epoch": 1.955,
      "grad_norm": 0.12065163254737854,
      "learning_rate": 3.045e-05,
      "loss": 0.0026,
      "step": 35190
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.5376771092414856,
      "learning_rate": 3.044444444444445e-05,
      "loss": 0.0039,
      "step": 35200
    },
    {
      "epoch": 1.9561111111111111,
      "grad_norm": 0.015031923539936543,
      "learning_rate": 3.043888888888889e-05,
      "loss": 0.0037,
      "step": 35210
    },
    {
      "epoch": 1.9566666666666666,
      "grad_norm": 0.45681995153427124,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.004,
      "step": 35220
    },
    {
      "epoch": 1.9572222222222222,
      "grad_norm": 0.06133703887462616,
      "learning_rate": 3.0427777777777776e-05,
      "loss": 0.0037,
      "step": 35230
    },
    {
      "epoch": 1.9577777777777778,
      "grad_norm": 0.1196325272321701,
      "learning_rate": 3.0422222222222223e-05,
      "loss": 0.0025,
      "step": 35240
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.20902569591999054,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.0031,
      "step": 35250
    },
    {
      "epoch": 1.958888888888889,
      "grad_norm": 0.11975932866334915,
      "learning_rate": 3.0411111111111113e-05,
      "loss": 0.0045,
      "step": 35260
    },
    {
      "epoch": 1.9594444444444443,
      "grad_norm": 0.03313007205724716,
      "learning_rate": 3.040555555555556e-05,
      "loss": 0.0026,
      "step": 35270
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.41837623715400696,
      "learning_rate": 3.04e-05,
      "loss": 0.0027,
      "step": 35280
    },
    {
      "epoch": 1.9605555555555556,
      "grad_norm": 0.20946387946605682,
      "learning_rate": 3.0394444444444447e-05,
      "loss": 0.0031,
      "step": 35290
    },
    {
      "epoch": 1.9611111111111112,
      "grad_norm": 0.3580085337162018,
      "learning_rate": 3.0388888888888887e-05,
      "loss": 0.0029,
      "step": 35300
    },
    {
      "epoch": 1.9616666666666667,
      "grad_norm": 0.2236931025981903,
      "learning_rate": 3.0383333333333334e-05,
      "loss": 0.0041,
      "step": 35310
    },
    {
      "epoch": 1.962222222222222,
      "grad_norm": 0.5379554033279419,
      "learning_rate": 3.0377777777777778e-05,
      "loss": 0.0024,
      "step": 35320
    },
    {
      "epoch": 1.9627777777777777,
      "grad_norm": 0.2987375259399414,
      "learning_rate": 3.0372222222222225e-05,
      "loss": 0.0036,
      "step": 35330
    },
    {
      "epoch": 1.9633333333333334,
      "grad_norm": 0.06118054315447807,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0041,
      "step": 35340
    },
    {
      "epoch": 1.963888888888889,
      "grad_norm": 0.06172269582748413,
      "learning_rate": 3.0361111111111112e-05,
      "loss": 0.0054,
      "step": 35350
    },
    {
      "epoch": 1.9644444444444444,
      "grad_norm": 0.41798534989356995,
      "learning_rate": 3.035555555555556e-05,
      "loss": 0.0035,
      "step": 35360
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 0.15939298272132874,
      "learning_rate": 3.035e-05,
      "loss": 0.0042,
      "step": 35370
    },
    {
      "epoch": 1.9655555555555555,
      "grad_norm": 0.12029314041137695,
      "learning_rate": 3.0344444444444446e-05,
      "loss": 0.0032,
      "step": 35380
    },
    {
      "epoch": 1.9661111111111111,
      "grad_norm": 0.17919926345348358,
      "learning_rate": 3.033888888888889e-05,
      "loss": 0.0036,
      "step": 35390
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.4783145487308502,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0037,
      "step": 35400
    },
    {
      "epoch": 1.9672222222222222,
      "grad_norm": 0.29741913080215454,
      "learning_rate": 3.0327777777777777e-05,
      "loss": 0.0038,
      "step": 35410
    },
    {
      "epoch": 1.9677777777777776,
      "grad_norm": 0.15046168863773346,
      "learning_rate": 3.0322222222222224e-05,
      "loss": 0.0032,
      "step": 35420
    },
    {
      "epoch": 1.9683333333333333,
      "grad_norm": 0.47800472378730774,
      "learning_rate": 3.0316666666666664e-05,
      "loss": 0.0036,
      "step": 35430
    },
    {
      "epoch": 1.968888888888889,
      "grad_norm": 0.0936422273516655,
      "learning_rate": 3.031111111111111e-05,
      "loss": 0.0038,
      "step": 35440
    },
    {
      "epoch": 1.9694444444444446,
      "grad_norm": 0.21306876838207245,
      "learning_rate": 3.0305555555555558e-05,
      "loss": 0.003,
      "step": 35450
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.2980498969554901,
      "learning_rate": 3.03e-05,
      "loss": 0.0041,
      "step": 35460
    },
    {
      "epoch": 1.9705555555555554,
      "grad_norm": 0.2994060218334198,
      "learning_rate": 3.0294444444444448e-05,
      "loss": 0.0035,
      "step": 35470
    },
    {
      "epoch": 1.971111111111111,
      "grad_norm": 0.2090355008840561,
      "learning_rate": 3.028888888888889e-05,
      "loss": 0.0029,
      "step": 35480
    },
    {
      "epoch": 1.9716666666666667,
      "grad_norm": 0.3592846393585205,
      "learning_rate": 3.0283333333333335e-05,
      "loss": 0.0025,
      "step": 35490
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 0.2094641923904419,
      "learning_rate": 3.0277777777777776e-05,
      "loss": 0.0038,
      "step": 35500
    },
    {
      "epoch": 1.9727777777777777,
      "grad_norm": 0.3285214900970459,
      "learning_rate": 3.0272222222222222e-05,
      "loss": 0.0043,
      "step": 35510
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.03604384511709213,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0034,
      "step": 35520
    },
    {
      "epoch": 1.9738888888888888,
      "grad_norm": 0.3597946763038635,
      "learning_rate": 3.0261111111111113e-05,
      "loss": 0.0032,
      "step": 35530
    },
    {
      "epoch": 1.9744444444444444,
      "grad_norm": 0.17899690568447113,
      "learning_rate": 3.025555555555556e-05,
      "loss": 0.0029,
      "step": 35540
    },
    {
      "epoch": 1.975,
      "grad_norm": 0.14928805828094482,
      "learning_rate": 3.025e-05,
      "loss": 0.0045,
      "step": 35550
    },
    {
      "epoch": 1.9755555555555555,
      "grad_norm": 0.03283300623297691,
      "learning_rate": 3.0244444444444447e-05,
      "loss": 0.0043,
      "step": 35560
    },
    {
      "epoch": 1.9761111111111112,
      "grad_norm": 0.09006901830434799,
      "learning_rate": 3.0238888888888887e-05,
      "loss": 0.0036,
      "step": 35570
    },
    {
      "epoch": 1.9766666666666666,
      "grad_norm": 0.007272564806044102,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.0046,
      "step": 35580
    },
    {
      "epoch": 1.9772222222222222,
      "grad_norm": 0.1216568648815155,
      "learning_rate": 3.0227777777777778e-05,
      "loss": 0.0043,
      "step": 35590
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.031174201518297195,
      "learning_rate": 3.0222222222222225e-05,
      "loss": 0.005,
      "step": 35600
    },
    {
      "epoch": 1.9783333333333335,
      "grad_norm": 0.48788774013519287,
      "learning_rate": 3.0216666666666665e-05,
      "loss": 0.0035,
      "step": 35610
    },
    {
      "epoch": 1.978888888888889,
      "grad_norm": 0.6872596144676208,
      "learning_rate": 3.0211111111111112e-05,
      "loss": 0.0045,
      "step": 35620
    },
    {
      "epoch": 1.9794444444444443,
      "grad_norm": 0.11813090741634369,
      "learning_rate": 3.020555555555556e-05,
      "loss": 0.0023,
      "step": 35630
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.2397887110710144,
      "learning_rate": 3.02e-05,
      "loss": 0.0028,
      "step": 35640
    },
    {
      "epoch": 1.9805555555555556,
      "grad_norm": 0.1853821724653244,
      "learning_rate": 3.0194444444444446e-05,
      "loss": 0.0035,
      "step": 35650
    },
    {
      "epoch": 1.9811111111111113,
      "grad_norm": 0.3101018965244293,
      "learning_rate": 3.018888888888889e-05,
      "loss": 0.0048,
      "step": 35660
    },
    {
      "epoch": 1.9816666666666667,
      "grad_norm": 0.10211740434169769,
      "learning_rate": 3.0183333333333336e-05,
      "loss": 0.0039,
      "step": 35670
    },
    {
      "epoch": 1.982222222222222,
      "grad_norm": 0.015274888835847378,
      "learning_rate": 3.0177777777777776e-05,
      "loss": 0.0028,
      "step": 35680
    },
    {
      "epoch": 1.9827777777777778,
      "grad_norm": 0.26701226830482483,
      "learning_rate": 3.0172222222222223e-05,
      "loss": 0.0035,
      "step": 35690
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.3297654092311859,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0037,
      "step": 35700
    },
    {
      "epoch": 1.983888888888889,
      "grad_norm": 0.11930578947067261,
      "learning_rate": 3.016111111111111e-05,
      "loss": 0.0037,
      "step": 35710
    },
    {
      "epoch": 1.9844444444444445,
      "grad_norm": 0.09764134883880615,
      "learning_rate": 3.0155555555555557e-05,
      "loss": 0.0036,
      "step": 35720
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 0.1269793063402176,
      "learning_rate": 3.015e-05,
      "loss": 0.002,
      "step": 35730
    },
    {
      "epoch": 1.9855555555555555,
      "grad_norm": 0.1205391138792038,
      "learning_rate": 3.0144444444444448e-05,
      "loss": 0.0051,
      "step": 35740
    },
    {
      "epoch": 1.9861111111111112,
      "grad_norm": 0.4175531268119812,
      "learning_rate": 3.0138888888888888e-05,
      "loss": 0.0036,
      "step": 35750
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.12014364451169968,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0036,
      "step": 35760
    },
    {
      "epoch": 1.9872222222222222,
      "grad_norm": 0.508358359336853,
      "learning_rate": 3.012777777777778e-05,
      "loss": 0.0029,
      "step": 35770
    },
    {
      "epoch": 1.9877777777777776,
      "grad_norm": 0.06076807901263237,
      "learning_rate": 3.0122222222222226e-05,
      "loss": 0.0041,
      "step": 35780
    },
    {
      "epoch": 1.9883333333333333,
      "grad_norm": 0.179540753364563,
      "learning_rate": 3.011666666666667e-05,
      "loss": 0.0036,
      "step": 35790
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 0.12028925865888596,
      "learning_rate": 3.0111111111111113e-05,
      "loss": 0.0035,
      "step": 35800
    },
    {
      "epoch": 1.9894444444444446,
      "grad_norm": 0.21335269510746002,
      "learning_rate": 3.010555555555556e-05,
      "loss": 0.0038,
      "step": 35810
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.4390711188316345,
      "learning_rate": 3.01e-05,
      "loss": 0.0029,
      "step": 35820
    },
    {
      "epoch": 1.9905555555555554,
      "grad_norm": 0.1668819934129715,
      "learning_rate": 3.0094444444444447e-05,
      "loss": 0.0047,
      "step": 35830
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 0.12012927979230881,
      "learning_rate": 3.008888888888889e-05,
      "loss": 0.004,
      "step": 35840
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 0.42261967062950134,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.003,
      "step": 35850
    },
    {
      "epoch": 1.9922222222222223,
      "grad_norm": 0.030622687190771103,
      "learning_rate": 3.0077777777777777e-05,
      "loss": 0.0032,
      "step": 35860
    },
    {
      "epoch": 1.9927777777777778,
      "grad_norm": 0.061067577451467514,
      "learning_rate": 3.0072222222222224e-05,
      "loss": 0.0036,
      "step": 35870
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.30473029613494873,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0034,
      "step": 35880
    },
    {
      "epoch": 1.9938888888888888,
      "grad_norm": 0.2003999799489975,
      "learning_rate": 3.006111111111111e-05,
      "loss": 0.0032,
      "step": 35890
    },
    {
      "epoch": 1.9944444444444445,
      "grad_norm": 0.12021427601575851,
      "learning_rate": 3.005555555555556e-05,
      "loss": 0.0041,
      "step": 35900
    },
    {
      "epoch": 1.995,
      "grad_norm": 0.3737814724445343,
      "learning_rate": 3.0050000000000002e-05,
      "loss": 0.0058,
      "step": 35910
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 0.0446670800447464,
      "learning_rate": 3.004444444444445e-05,
      "loss": 0.0025,
      "step": 35920
    },
    {
      "epoch": 1.996111111111111,
      "grad_norm": 0.06853295862674713,
      "learning_rate": 3.003888888888889e-05,
      "loss": 0.0031,
      "step": 35930
    },
    {
      "epoch": 1.9966666666666666,
      "grad_norm": 0.40628141164779663,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0031,
      "step": 35940
    },
    {
      "epoch": 1.9972222222222222,
      "grad_norm": 0.20887835323810577,
      "learning_rate": 3.0027777777777776e-05,
      "loss": 0.0031,
      "step": 35950
    },
    {
      "epoch": 1.9977777777777779,
      "grad_norm": 0.03098290227353573,
      "learning_rate": 3.0022222222222223e-05,
      "loss": 0.0025,
      "step": 35960
    },
    {
      "epoch": 1.9983333333333333,
      "grad_norm": 0.08993321657180786,
      "learning_rate": 3.001666666666667e-05,
      "loss": 0.0033,
      "step": 35970
    },
    {
      "epoch": 1.998888888888889,
      "grad_norm": 0.35933488607406616,
      "learning_rate": 3.0011111111111114e-05,
      "loss": 0.0032,
      "step": 35980
    },
    {
      "epoch": 1.9994444444444444,
      "grad_norm": 0.12050367146730423,
      "learning_rate": 3.000555555555556e-05,
      "loss": 0.0042,
      "step": 35990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.09124481678009033,
      "learning_rate": 3e-05,
      "loss": 0.0032,
      "step": 36000
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.003455488011240959,
      "eval_runtime": 116.5293,
      "eval_samples_per_second": 1545.5,
      "eval_steps_per_second": 38.643,
      "step": 36000
    },
    {
      "epoch": 2.0005555555555556,
      "grad_norm": 0.03370257094502449,
      "learning_rate": 2.9994444444444448e-05,
      "loss": 0.0051,
      "step": 36010
    },
    {
      "epoch": 2.0011111111111113,
      "grad_norm": 0.06067823991179466,
      "learning_rate": 2.9988888888888888e-05,
      "loss": 0.0028,
      "step": 36020
    },
    {
      "epoch": 2.0016666666666665,
      "grad_norm": 0.17996826767921448,
      "learning_rate": 2.9983333333333335e-05,
      "loss": 0.0033,
      "step": 36030
    },
    {
      "epoch": 2.002222222222222,
      "grad_norm": 0.35691550374031067,
      "learning_rate": 2.997777777777778e-05,
      "loss": 0.0037,
      "step": 36040
    },
    {
      "epoch": 2.0027777777777778,
      "grad_norm": 0.3199816346168518,
      "learning_rate": 2.9972222222222225e-05,
      "loss": 0.0041,
      "step": 36050
    },
    {
      "epoch": 2.0033333333333334,
      "grad_norm": 0.3896274268627167,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.003,
      "step": 36060
    },
    {
      "epoch": 2.003888888888889,
      "grad_norm": 0.21552802622318268,
      "learning_rate": 2.9961111111111112e-05,
      "loss": 0.0039,
      "step": 36070
    },
    {
      "epoch": 2.0044444444444443,
      "grad_norm": 0.5770612955093384,
      "learning_rate": 2.995555555555556e-05,
      "loss": 0.0036,
      "step": 36080
    },
    {
      "epoch": 2.005,
      "grad_norm": 0.03348254784941673,
      "learning_rate": 2.995e-05,
      "loss": 0.0035,
      "step": 36090
    },
    {
      "epoch": 2.0055555555555555,
      "grad_norm": 0.2393004298210144,
      "learning_rate": 2.9944444444444446e-05,
      "loss": 0.0041,
      "step": 36100
    },
    {
      "epoch": 2.006111111111111,
      "grad_norm": 0.1792188286781311,
      "learning_rate": 2.993888888888889e-05,
      "loss": 0.0029,
      "step": 36110
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.42624637484550476,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0039,
      "step": 36120
    },
    {
      "epoch": 2.007222222222222,
      "grad_norm": 0.12002184242010117,
      "learning_rate": 2.9927777777777777e-05,
      "loss": 0.0029,
      "step": 36130
    },
    {
      "epoch": 2.0077777777777777,
      "grad_norm": 0.09450014680624008,
      "learning_rate": 2.9922222222222224e-05,
      "loss": 0.004,
      "step": 36140
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 0.27086496353149414,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.0042,
      "step": 36150
    },
    {
      "epoch": 2.008888888888889,
      "grad_norm": 0.35842353105545044,
      "learning_rate": 2.991111111111111e-05,
      "loss": 0.0041,
      "step": 36160
    },
    {
      "epoch": 2.0094444444444446,
      "grad_norm": 0.5676445960998535,
      "learning_rate": 2.9905555555555558e-05,
      "loss": 0.0036,
      "step": 36170
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.3956613540649414,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0036,
      "step": 36180
    },
    {
      "epoch": 2.0105555555555554,
      "grad_norm": 0.0603695847094059,
      "learning_rate": 2.989444444444445e-05,
      "loss": 0.0036,
      "step": 36190
    },
    {
      "epoch": 2.011111111111111,
      "grad_norm": 0.12262904644012451,
      "learning_rate": 2.988888888888889e-05,
      "loss": 0.0024,
      "step": 36200
    },
    {
      "epoch": 2.0116666666666667,
      "grad_norm": 0.32867181301116943,
      "learning_rate": 2.9883333333333336e-05,
      "loss": 0.0036,
      "step": 36210
    },
    {
      "epoch": 2.0122222222222224,
      "grad_norm": 0.5812570452690125,
      "learning_rate": 2.9877777777777776e-05,
      "loss": 0.0035,
      "step": 36220
    },
    {
      "epoch": 2.012777777777778,
      "grad_norm": 0.024133000522851944,
      "learning_rate": 2.9872222222222223e-05,
      "loss": 0.0042,
      "step": 36230
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.03321940824389458,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0031,
      "step": 36240
    },
    {
      "epoch": 2.013888888888889,
      "grad_norm": 0.3578900992870331,
      "learning_rate": 2.9861111111111113e-05,
      "loss": 0.0041,
      "step": 36250
    },
    {
      "epoch": 2.0144444444444445,
      "grad_norm": 0.218672513961792,
      "learning_rate": 2.985555555555556e-05,
      "loss": 0.0042,
      "step": 36260
    },
    {
      "epoch": 2.015,
      "grad_norm": 0.18326957523822784,
      "learning_rate": 2.985e-05,
      "loss": 0.0034,
      "step": 36270
    },
    {
      "epoch": 2.0155555555555558,
      "grad_norm": 0.06069768965244293,
      "learning_rate": 2.9844444444444447e-05,
      "loss": 0.005,
      "step": 36280
    },
    {
      "epoch": 2.016111111111111,
      "grad_norm": 0.13857102394104004,
      "learning_rate": 2.9838888888888888e-05,
      "loss": 0.0051,
      "step": 36290
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.14986732602119446,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0027,
      "step": 36300
    },
    {
      "epoch": 2.0172222222222222,
      "grad_norm": 0.09980414807796478,
      "learning_rate": 2.9827777777777778e-05,
      "loss": 0.0043,
      "step": 36310
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 0.08953767269849777,
      "learning_rate": 2.9822222222222225e-05,
      "loss": 0.0022,
      "step": 36320
    },
    {
      "epoch": 2.0183333333333335,
      "grad_norm": 0.2986365258693695,
      "learning_rate": 2.9816666666666672e-05,
      "loss": 0.0041,
      "step": 36330
    },
    {
      "epoch": 2.0188888888888887,
      "grad_norm": 0.6430321931838989,
      "learning_rate": 2.9811111111111112e-05,
      "loss": 0.0048,
      "step": 36340
    },
    {
      "epoch": 2.0194444444444444,
      "grad_norm": 0.09002348780632019,
      "learning_rate": 2.980555555555556e-05,
      "loss": 0.0042,
      "step": 36350
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.1798802614212036,
      "learning_rate": 2.98e-05,
      "loss": 0.0031,
      "step": 36360
    },
    {
      "epoch": 2.0205555555555557,
      "grad_norm": 0.4319298267364502,
      "learning_rate": 2.9794444444444446e-05,
      "loss": 0.0045,
      "step": 36370
    },
    {
      "epoch": 2.0211111111111113,
      "grad_norm": 0.06148644536733627,
      "learning_rate": 2.978888888888889e-05,
      "loss": 0.0039,
      "step": 36380
    },
    {
      "epoch": 2.0216666666666665,
      "grad_norm": 0.11993285268545151,
      "learning_rate": 2.9783333333333337e-05,
      "loss": 0.0031,
      "step": 36390
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 0.0898308977484703,
      "learning_rate": 2.9777777777777777e-05,
      "loss": 0.0045,
      "step": 36400
    },
    {
      "epoch": 2.022777777777778,
      "grad_norm": 0.011490056291222572,
      "learning_rate": 2.9772222222222224e-05,
      "loss": 0.0041,
      "step": 36410
    },
    {
      "epoch": 2.0233333333333334,
      "grad_norm": 0.60076904296875,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0046,
      "step": 36420
    },
    {
      "epoch": 2.023888888888889,
      "grad_norm": 0.30286705493927,
      "learning_rate": 2.976111111111111e-05,
      "loss": 0.003,
      "step": 36430
    },
    {
      "epoch": 2.0244444444444443,
      "grad_norm": 0.12086226046085358,
      "learning_rate": 2.9755555555555558e-05,
      "loss": 0.0045,
      "step": 36440
    },
    {
      "epoch": 2.025,
      "grad_norm": 0.26940932869911194,
      "learning_rate": 2.975e-05,
      "loss": 0.0042,
      "step": 36450
    },
    {
      "epoch": 2.0255555555555556,
      "grad_norm": 0.35861799120903015,
      "learning_rate": 2.974444444444445e-05,
      "loss": 0.0041,
      "step": 36460
    },
    {
      "epoch": 2.026111111111111,
      "grad_norm": 0.2401021122932434,
      "learning_rate": 2.973888888888889e-05,
      "loss": 0.0035,
      "step": 36470
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.4782407283782959,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0031,
      "step": 36480
    },
    {
      "epoch": 2.027222222222222,
      "grad_norm": 0.23886622488498688,
      "learning_rate": 2.9727777777777776e-05,
      "loss": 0.0047,
      "step": 36490
    },
    {
      "epoch": 2.0277777777777777,
      "grad_norm": 0.030695805326104164,
      "learning_rate": 2.9722222222222223e-05,
      "loss": 0.0028,
      "step": 36500
    },
    {
      "epoch": 2.0283333333333333,
      "grad_norm": 0.35842254757881165,
      "learning_rate": 2.971666666666667e-05,
      "loss": 0.0034,
      "step": 36510
    },
    {
      "epoch": 2.028888888888889,
      "grad_norm": 0.1797640323638916,
      "learning_rate": 2.9711111111111113e-05,
      "loss": 0.0038,
      "step": 36520
    },
    {
      "epoch": 2.0294444444444446,
      "grad_norm": 0.024450508877635002,
      "learning_rate": 2.970555555555556e-05,
      "loss": 0.0032,
      "step": 36530
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.03250090405344963,
      "learning_rate": 2.97e-05,
      "loss": 0.0035,
      "step": 36540
    },
    {
      "epoch": 2.0305555555555554,
      "grad_norm": 0.09039781242609024,
      "learning_rate": 2.9694444444444447e-05,
      "loss": 0.0028,
      "step": 36550
    },
    {
      "epoch": 2.031111111111111,
      "grad_norm": 0.38078024983406067,
      "learning_rate": 2.9688888888888887e-05,
      "loss": 0.0033,
      "step": 36560
    },
    {
      "epoch": 2.0316666666666667,
      "grad_norm": 0.034367721527814865,
      "learning_rate": 2.9683333333333334e-05,
      "loss": 0.0036,
      "step": 36570
    },
    {
      "epoch": 2.0322222222222224,
      "grad_norm": 0.33174261450767517,
      "learning_rate": 2.9677777777777778e-05,
      "loss": 0.0036,
      "step": 36580
    },
    {
      "epoch": 2.0327777777777776,
      "grad_norm": 0.3288881182670593,
      "learning_rate": 2.9672222222222225e-05,
      "loss": 0.0034,
      "step": 36590
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.18023188412189484,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0041,
      "step": 36600
    },
    {
      "epoch": 2.033888888888889,
      "grad_norm": 0.20712175965309143,
      "learning_rate": 2.9661111111111112e-05,
      "loss": 0.0028,
      "step": 36610
    },
    {
      "epoch": 2.0344444444444445,
      "grad_norm": 0.2087273746728897,
      "learning_rate": 2.965555555555556e-05,
      "loss": 0.004,
      "step": 36620
    },
    {
      "epoch": 2.035,
      "grad_norm": 0.06288538128137589,
      "learning_rate": 2.965e-05,
      "loss": 0.0034,
      "step": 36630
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 0.13490021228790283,
      "learning_rate": 2.9644444444444446e-05,
      "loss": 0.0039,
      "step": 36640
    },
    {
      "epoch": 2.036111111111111,
      "grad_norm": 0.2686232626438141,
      "learning_rate": 2.963888888888889e-05,
      "loss": 0.0039,
      "step": 36650
    },
    {
      "epoch": 2.0366666666666666,
      "grad_norm": 0.06412236392498016,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0029,
      "step": 36660
    },
    {
      "epoch": 2.0372222222222223,
      "grad_norm": 0.09058482199907303,
      "learning_rate": 2.9627777777777777e-05,
      "loss": 0.0038,
      "step": 36670
    },
    {
      "epoch": 2.037777777777778,
      "grad_norm": 0.41855981945991516,
      "learning_rate": 2.9622222222222224e-05,
      "loss": 0.004,
      "step": 36680
    },
    {
      "epoch": 2.038333333333333,
      "grad_norm": 0.20908169448375702,
      "learning_rate": 2.961666666666667e-05,
      "loss": 0.0044,
      "step": 36690
    },
    {
      "epoch": 2.0388888888888888,
      "grad_norm": 0.35793331265449524,
      "learning_rate": 2.961111111111111e-05,
      "loss": 0.0034,
      "step": 36700
    },
    {
      "epoch": 2.0394444444444444,
      "grad_norm": 0.23917034268379211,
      "learning_rate": 2.9605555555555558e-05,
      "loss": 0.0032,
      "step": 36710
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.03072613663971424,
      "learning_rate": 2.96e-05,
      "loss": 0.0035,
      "step": 36720
    },
    {
      "epoch": 2.0405555555555557,
      "grad_norm": 0.14946505427360535,
      "learning_rate": 2.9594444444444448e-05,
      "loss": 0.0038,
      "step": 36730
    },
    {
      "epoch": 2.0411111111111113,
      "grad_norm": 0.29859402775764465,
      "learning_rate": 2.958888888888889e-05,
      "loss": 0.0029,
      "step": 36740
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 0.32665175199508667,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.0051,
      "step": 36750
    },
    {
      "epoch": 2.042222222222222,
      "grad_norm": 0.03696054592728615,
      "learning_rate": 2.9577777777777775e-05,
      "loss": 0.0049,
      "step": 36760
    },
    {
      "epoch": 2.042777777777778,
      "grad_norm": 0.29827961325645447,
      "learning_rate": 2.9572222222222222e-05,
      "loss": 0.0031,
      "step": 36770
    },
    {
      "epoch": 2.0433333333333334,
      "grad_norm": 0.18620005249977112,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0034,
      "step": 36780
    },
    {
      "epoch": 2.043888888888889,
      "grad_norm": 0.2380216121673584,
      "learning_rate": 2.9561111111111113e-05,
      "loss": 0.0032,
      "step": 36790
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 0.06016884371638298,
      "learning_rate": 2.955555555555556e-05,
      "loss": 0.0023,
      "step": 36800
    },
    {
      "epoch": 2.045,
      "grad_norm": 0.19550906121730804,
      "learning_rate": 2.955e-05,
      "loss": 0.003,
      "step": 36810
    },
    {
      "epoch": 2.0455555555555556,
      "grad_norm": 0.4470042288303375,
      "learning_rate": 2.9544444444444447e-05,
      "loss": 0.0026,
      "step": 36820
    },
    {
      "epoch": 2.046111111111111,
      "grad_norm": 0.18851333856582642,
      "learning_rate": 2.9538888888888887e-05,
      "loss": 0.0028,
      "step": 36830
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.14927197992801666,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0047,
      "step": 36840
    },
    {
      "epoch": 2.047222222222222,
      "grad_norm": 0.3287491202354431,
      "learning_rate": 2.9527777777777778e-05,
      "loss": 0.0038,
      "step": 36850
    },
    {
      "epoch": 2.0477777777777777,
      "grad_norm": 0.0317748486995697,
      "learning_rate": 2.9522222222222225e-05,
      "loss": 0.0037,
      "step": 36860
    },
    {
      "epoch": 2.0483333333333333,
      "grad_norm": 0.3582126498222351,
      "learning_rate": 2.951666666666667e-05,
      "loss": 0.0031,
      "step": 36870
    },
    {
      "epoch": 2.048888888888889,
      "grad_norm": 0.03734011948108673,
      "learning_rate": 2.951111111111111e-05,
      "loss": 0.0039,
      "step": 36880
    },
    {
      "epoch": 2.0494444444444446,
      "grad_norm": 0.4831443130970001,
      "learning_rate": 2.950555555555556e-05,
      "loss": 0.0029,
      "step": 36890
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.014686807990074158,
      "learning_rate": 2.95e-05,
      "loss": 0.0028,
      "step": 36900
    },
    {
      "epoch": 2.0505555555555555,
      "grad_norm": 0.2087683528661728,
      "learning_rate": 2.9494444444444446e-05,
      "loss": 0.004,
      "step": 36910
    },
    {
      "epoch": 2.051111111111111,
      "grad_norm": 0.32210013270378113,
      "learning_rate": 2.948888888888889e-05,
      "loss": 0.004,
      "step": 36920
    },
    {
      "epoch": 2.0516666666666667,
      "grad_norm": 0.12094437330961227,
      "learning_rate": 2.9483333333333336e-05,
      "loss": 0.0022,
      "step": 36930
    },
    {
      "epoch": 2.0522222222222224,
      "grad_norm": 0.45166152715682983,
      "learning_rate": 2.9477777777777783e-05,
      "loss": 0.0034,
      "step": 36940
    },
    {
      "epoch": 2.0527777777777776,
      "grad_norm": 0.35757729411125183,
      "learning_rate": 2.9472222222222223e-05,
      "loss": 0.0057,
      "step": 36950
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.11998940259218216,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0037,
      "step": 36960
    },
    {
      "epoch": 2.053888888888889,
      "grad_norm": 0.3284691274166107,
      "learning_rate": 2.946111111111111e-05,
      "loss": 0.0039,
      "step": 36970
    },
    {
      "epoch": 2.0544444444444445,
      "grad_norm": 0.09244050085544586,
      "learning_rate": 2.9455555555555557e-05,
      "loss": 0.006,
      "step": 36980
    },
    {
      "epoch": 2.055,
      "grad_norm": 0.14958080649375916,
      "learning_rate": 2.945e-05,
      "loss": 0.0042,
      "step": 36990
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 0.04477006196975708,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 0.0032,
      "step": 37000
    },
    {
      "epoch": 2.056111111111111,
      "grad_norm": 0.15117184817790985,
      "learning_rate": 2.9438888888888888e-05,
      "loss": 0.0034,
      "step": 37010
    },
    {
      "epoch": 2.0566666666666666,
      "grad_norm": 0.09002093225717545,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0044,
      "step": 37020
    },
    {
      "epoch": 2.0572222222222223,
      "grad_norm": 0.11990921199321747,
      "learning_rate": 2.9427777777777782e-05,
      "loss": 0.0039,
      "step": 37030
    },
    {
      "epoch": 2.057777777777778,
      "grad_norm": 0.12035864591598511,
      "learning_rate": 2.9422222222222222e-05,
      "loss": 0.0052,
      "step": 37040
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 0.06028440222144127,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.0043,
      "step": 37050
    },
    {
      "epoch": 2.0588888888888888,
      "grad_norm": 0.06136832758784294,
      "learning_rate": 2.9411111111111113e-05,
      "loss": 0.0044,
      "step": 37060
    },
    {
      "epoch": 2.0594444444444444,
      "grad_norm": 0.1804054230451584,
      "learning_rate": 2.940555555555556e-05,
      "loss": 0.0026,
      "step": 37070
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.0606795959174633,
      "learning_rate": 2.94e-05,
      "loss": 0.0035,
      "step": 37080
    },
    {
      "epoch": 2.0605555555555557,
      "grad_norm": 0.2690839469432831,
      "learning_rate": 2.9394444444444447e-05,
      "loss": 0.0026,
      "step": 37090
    },
    {
      "epoch": 2.061111111111111,
      "grad_norm": 0.5188118815422058,
      "learning_rate": 2.9388888888888887e-05,
      "loss": 0.003,
      "step": 37100
    },
    {
      "epoch": 2.0616666666666665,
      "grad_norm": 0.005983538459986448,
      "learning_rate": 2.9383333333333334e-05,
      "loss": 0.0026,
      "step": 37110
    },
    {
      "epoch": 2.062222222222222,
      "grad_norm": 0.29685044288635254,
      "learning_rate": 2.937777777777778e-05,
      "loss": 0.0035,
      "step": 37120
    },
    {
      "epoch": 2.062777777777778,
      "grad_norm": 0.4824569523334503,
      "learning_rate": 2.9372222222222224e-05,
      "loss": 0.0038,
      "step": 37130
    },
    {
      "epoch": 2.0633333333333335,
      "grad_norm": 0.4599115550518036,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.0034,
      "step": 37140
    },
    {
      "epoch": 2.063888888888889,
      "grad_norm": 0.7813712954521179,
      "learning_rate": 2.936111111111111e-05,
      "loss": 0.0037,
      "step": 37150
    },
    {
      "epoch": 2.0644444444444443,
      "grad_norm": 0.3459676504135132,
      "learning_rate": 2.935555555555556e-05,
      "loss": 0.0025,
      "step": 37160
    },
    {
      "epoch": 2.065,
      "grad_norm": 0.3303638994693756,
      "learning_rate": 2.935e-05,
      "loss": 0.0038,
      "step": 37170
    },
    {
      "epoch": 2.0655555555555556,
      "grad_norm": 0.35801947116851807,
      "learning_rate": 2.9344444444444445e-05,
      "loss": 0.0038,
      "step": 37180
    },
    {
      "epoch": 2.0661111111111112,
      "grad_norm": 0.23843269050121307,
      "learning_rate": 2.933888888888889e-05,
      "loss": 0.0028,
      "step": 37190
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.21450334787368774,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0066,
      "step": 37200
    },
    {
      "epoch": 2.067222222222222,
      "grad_norm": 0.15011762082576752,
      "learning_rate": 2.9327777777777783e-05,
      "loss": 0.005,
      "step": 37210
    },
    {
      "epoch": 2.0677777777777777,
      "grad_norm": 0.14905543625354767,
      "learning_rate": 2.9322222222222223e-05,
      "loss": 0.0045,
      "step": 37220
    },
    {
      "epoch": 2.0683333333333334,
      "grad_norm": 0.34425660967826843,
      "learning_rate": 2.931666666666667e-05,
      "loss": 0.0046,
      "step": 37230
    },
    {
      "epoch": 2.068888888888889,
      "grad_norm": 0.11960713565349579,
      "learning_rate": 2.931111111111111e-05,
      "loss": 0.0025,
      "step": 37240
    },
    {
      "epoch": 2.0694444444444446,
      "grad_norm": 0.2690868079662323,
      "learning_rate": 2.9305555555555557e-05,
      "loss": 0.0021,
      "step": 37250
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.4177762269973755,
      "learning_rate": 2.93e-05,
      "loss": 0.0039,
      "step": 37260
    },
    {
      "epoch": 2.0705555555555555,
      "grad_norm": 0.778642475605011,
      "learning_rate": 2.9294444444444448e-05,
      "loss": 0.0058,
      "step": 37270
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 0.23939171433448792,
      "learning_rate": 2.9288888888888888e-05,
      "loss": 0.0036,
      "step": 37280
    },
    {
      "epoch": 2.0716666666666668,
      "grad_norm": 0.14992313086986542,
      "learning_rate": 2.9283333333333335e-05,
      "loss": 0.0032,
      "step": 37290
    },
    {
      "epoch": 2.0722222222222224,
      "grad_norm": 0.476485937833786,
      "learning_rate": 2.927777777777778e-05,
      "loss": 0.004,
      "step": 37300
    },
    {
      "epoch": 2.0727777777777776,
      "grad_norm": 0.031912662088871,
      "learning_rate": 2.9272222222222222e-05,
      "loss": 0.0031,
      "step": 37310
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.06304105371236801,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.003,
      "step": 37320
    },
    {
      "epoch": 2.073888888888889,
      "grad_norm": 0.1305055469274521,
      "learning_rate": 2.9261111111111112e-05,
      "loss": 0.0037,
      "step": 37330
    },
    {
      "epoch": 2.0744444444444445,
      "grad_norm": 0.47690877318382263,
      "learning_rate": 2.925555555555556e-05,
      "loss": 0.0042,
      "step": 37340
    },
    {
      "epoch": 2.075,
      "grad_norm": 0.18099375069141388,
      "learning_rate": 2.925e-05,
      "loss": 0.0039,
      "step": 37350
    },
    {
      "epoch": 2.0755555555555554,
      "grad_norm": 0.06013565883040428,
      "learning_rate": 2.9244444444444446e-05,
      "loss": 0.0043,
      "step": 37360
    },
    {
      "epoch": 2.076111111111111,
      "grad_norm": 0.47858452796936035,
      "learning_rate": 2.9238888888888887e-05,
      "loss": 0.0041,
      "step": 37370
    },
    {
      "epoch": 2.0766666666666667,
      "grad_norm": 0.4772215783596039,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.0039,
      "step": 37380
    },
    {
      "epoch": 2.0772222222222223,
      "grad_norm": 0.7456658482551575,
      "learning_rate": 2.922777777777778e-05,
      "loss": 0.0042,
      "step": 37390
    },
    {
      "epoch": 2.077777777777778,
      "grad_norm": 0.08984953165054321,
      "learning_rate": 2.9222222222222224e-05,
      "loss": 0.0036,
      "step": 37400
    },
    {
      "epoch": 2.078333333333333,
      "grad_norm": 0.20916850864887238,
      "learning_rate": 2.921666666666667e-05,
      "loss": 0.0029,
      "step": 37410
    },
    {
      "epoch": 2.078888888888889,
      "grad_norm": 0.03159866854548454,
      "learning_rate": 2.921111111111111e-05,
      "loss": 0.0039,
      "step": 37420
    },
    {
      "epoch": 2.0794444444444444,
      "grad_norm": 0.11972363293170929,
      "learning_rate": 2.9205555555555558e-05,
      "loss": 0.0027,
      "step": 37430
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.07428409159183502,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0039,
      "step": 37440
    },
    {
      "epoch": 2.0805555555555557,
      "grad_norm": 0.050832606852054596,
      "learning_rate": 2.9194444444444445e-05,
      "loss": 0.0061,
      "step": 37450
    },
    {
      "epoch": 2.081111111111111,
      "grad_norm": 0.03120484948158264,
      "learning_rate": 2.918888888888889e-05,
      "loss": 0.0058,
      "step": 37460
    },
    {
      "epoch": 2.0816666666666666,
      "grad_norm": 0.5641836524009705,
      "learning_rate": 2.9183333333333336e-05,
      "loss": 0.0022,
      "step": 37470
    },
    {
      "epoch": 2.082222222222222,
      "grad_norm": 0.35792768001556396,
      "learning_rate": 2.9177777777777783e-05,
      "loss": 0.0034,
      "step": 37480
    },
    {
      "epoch": 2.082777777777778,
      "grad_norm": 0.31349706649780273,
      "learning_rate": 2.9172222222222223e-05,
      "loss": 0.0029,
      "step": 37490
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.17949341237545013,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0034,
      "step": 37500
    },
    {
      "epoch": 2.0838888888888887,
      "grad_norm": 0.3428139090538025,
      "learning_rate": 2.916111111111111e-05,
      "loss": 0.0052,
      "step": 37510
    },
    {
      "epoch": 2.0844444444444443,
      "grad_norm": 0.0603189542889595,
      "learning_rate": 2.9155555555555557e-05,
      "loss": 0.0037,
      "step": 37520
    },
    {
      "epoch": 2.085,
      "grad_norm": 0.031206278130412102,
      "learning_rate": 2.915e-05,
      "loss": 0.0027,
      "step": 37530
    },
    {
      "epoch": 2.0855555555555556,
      "grad_norm": 0.14937806129455566,
      "learning_rate": 2.9144444444444447e-05,
      "loss": 0.0046,
      "step": 37540
    },
    {
      "epoch": 2.0861111111111112,
      "grad_norm": 0.35535678267478943,
      "learning_rate": 2.9138888888888888e-05,
      "loss": 0.0028,
      "step": 37550
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.022334817796945572,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0027,
      "step": 37560
    },
    {
      "epoch": 2.087222222222222,
      "grad_norm": 0.14953257143497467,
      "learning_rate": 2.912777777777778e-05,
      "loss": 0.0033,
      "step": 37570
    },
    {
      "epoch": 2.0877777777777777,
      "grad_norm": 0.12111588567495346,
      "learning_rate": 2.912222222222222e-05,
      "loss": 0.0051,
      "step": 37580
    },
    {
      "epoch": 2.0883333333333334,
      "grad_norm": 0.031452711671590805,
      "learning_rate": 2.911666666666667e-05,
      "loss": 0.002,
      "step": 37590
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.23798690736293793,
      "learning_rate": 2.9111111111111112e-05,
      "loss": 0.0051,
      "step": 37600
    },
    {
      "epoch": 2.089444444444444,
      "grad_norm": 0.23853963613510132,
      "learning_rate": 2.910555555555556e-05,
      "loss": 0.0028,
      "step": 37610
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.6136423945426941,
      "learning_rate": 2.91e-05,
      "loss": 0.0027,
      "step": 37620
    },
    {
      "epoch": 2.0905555555555555,
      "grad_norm": 0.23945359885692596,
      "learning_rate": 2.9094444444444446e-05,
      "loss": 0.004,
      "step": 37630
    },
    {
      "epoch": 2.091111111111111,
      "grad_norm": 0.17920354008674622,
      "learning_rate": 2.9088888888888886e-05,
      "loss": 0.0031,
      "step": 37640
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 0.17985470592975616,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.0038,
      "step": 37650
    },
    {
      "epoch": 2.0922222222222224,
      "grad_norm": 0.34546467661857605,
      "learning_rate": 2.907777777777778e-05,
      "loss": 0.0034,
      "step": 37660
    },
    {
      "epoch": 2.0927777777777776,
      "grad_norm": 0.4981520473957062,
      "learning_rate": 2.9072222222222224e-05,
      "loss": 0.0031,
      "step": 37670
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.20867851376533508,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0029,
      "step": 37680
    },
    {
      "epoch": 2.093888888888889,
      "grad_norm": 0.4937373995780945,
      "learning_rate": 2.906111111111111e-05,
      "loss": 0.0035,
      "step": 37690
    },
    {
      "epoch": 2.0944444444444446,
      "grad_norm": 0.03211117535829544,
      "learning_rate": 2.9055555555555558e-05,
      "loss": 0.0041,
      "step": 37700
    },
    {
      "epoch": 2.095,
      "grad_norm": 0.6281882524490356,
      "learning_rate": 2.9049999999999998e-05,
      "loss": 0.0041,
      "step": 37710
    },
    {
      "epoch": 2.0955555555555554,
      "grad_norm": 0.12133977562189102,
      "learning_rate": 2.9044444444444445e-05,
      "loss": 0.0042,
      "step": 37720
    },
    {
      "epoch": 2.096111111111111,
      "grad_norm": 0.27179285883903503,
      "learning_rate": 2.903888888888889e-05,
      "loss": 0.0041,
      "step": 37730
    },
    {
      "epoch": 2.0966666666666667,
      "grad_norm": 0.06300771981477737,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0034,
      "step": 37740
    },
    {
      "epoch": 2.0972222222222223,
      "grad_norm": 0.13097277283668518,
      "learning_rate": 2.9027777777777782e-05,
      "loss": 0.0036,
      "step": 37750
    },
    {
      "epoch": 2.097777777777778,
      "grad_norm": 0.1208336353302002,
      "learning_rate": 2.9022222222222223e-05,
      "loss": 0.0031,
      "step": 37760
    },
    {
      "epoch": 2.098333333333333,
      "grad_norm": 0.21030472218990326,
      "learning_rate": 2.901666666666667e-05,
      "loss": 0.0044,
      "step": 37770
    },
    {
      "epoch": 2.098888888888889,
      "grad_norm": 0.4197229743003845,
      "learning_rate": 2.901111111111111e-05,
      "loss": 0.0031,
      "step": 37780
    },
    {
      "epoch": 2.0994444444444444,
      "grad_norm": 0.03105076774954796,
      "learning_rate": 2.9005555555555557e-05,
      "loss": 0.0044,
      "step": 37790
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.09219439327716827,
      "learning_rate": 2.9e-05,
      "loss": 0.0052,
      "step": 37800
    },
    {
      "epoch": 2.1005555555555557,
      "grad_norm": 0.1791851669549942,
      "learning_rate": 2.8994444444444447e-05,
      "loss": 0.0029,
      "step": 37810
    },
    {
      "epoch": 2.101111111111111,
      "grad_norm": 0.09203463792800903,
      "learning_rate": 2.8988888888888887e-05,
      "loss": 0.0029,
      "step": 37820
    },
    {
      "epoch": 2.1016666666666666,
      "grad_norm": 0.1497182548046112,
      "learning_rate": 2.8983333333333334e-05,
      "loss": 0.0032,
      "step": 37830
    },
    {
      "epoch": 2.102222222222222,
      "grad_norm": 0.10669500380754471,
      "learning_rate": 2.897777777777778e-05,
      "loss": 0.0052,
      "step": 37840
    },
    {
      "epoch": 2.102777777777778,
      "grad_norm": 0.1817515641450882,
      "learning_rate": 2.897222222222222e-05,
      "loss": 0.0031,
      "step": 37850
    },
    {
      "epoch": 2.1033333333333335,
      "grad_norm": 0.11991839855909348,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0026,
      "step": 37860
    },
    {
      "epoch": 2.1038888888888887,
      "grad_norm": 0.18001636862754822,
      "learning_rate": 2.8961111111111112e-05,
      "loss": 0.004,
      "step": 37870
    },
    {
      "epoch": 2.1044444444444443,
      "grad_norm": 0.15007169544696808,
      "learning_rate": 2.895555555555556e-05,
      "loss": 0.0032,
      "step": 37880
    },
    {
      "epoch": 2.105,
      "grad_norm": 0.06201571971178055,
      "learning_rate": 2.895e-05,
      "loss": 0.0029,
      "step": 37890
    },
    {
      "epoch": 2.1055555555555556,
      "grad_norm": 0.11943899095058441,
      "learning_rate": 2.8944444444444446e-05,
      "loss": 0.003,
      "step": 37900
    },
    {
      "epoch": 2.1061111111111113,
      "grad_norm": 0.17909759283065796,
      "learning_rate": 2.8938888888888886e-05,
      "loss": 0.0028,
      "step": 37910
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.43401899933815,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0027,
      "step": 37920
    },
    {
      "epoch": 2.107222222222222,
      "grad_norm": 0.3988569676876068,
      "learning_rate": 2.892777777777778e-05,
      "loss": 0.0038,
      "step": 37930
    },
    {
      "epoch": 2.1077777777777778,
      "grad_norm": 0.12220185995101929,
      "learning_rate": 2.8922222222222224e-05,
      "loss": 0.0031,
      "step": 37940
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 0.17945370078086853,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.004,
      "step": 37950
    },
    {
      "epoch": 2.108888888888889,
      "grad_norm": 0.14985120296478271,
      "learning_rate": 2.891111111111111e-05,
      "loss": 0.0039,
      "step": 37960
    },
    {
      "epoch": 2.1094444444444442,
      "grad_norm": 0.29815664887428284,
      "learning_rate": 2.8905555555555558e-05,
      "loss": 0.0043,
      "step": 37970
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.09048739820718765,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0042,
      "step": 37980
    },
    {
      "epoch": 2.1105555555555555,
      "grad_norm": 0.06235484033823013,
      "learning_rate": 2.8894444444444445e-05,
      "loss": 0.0035,
      "step": 37990
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.06071679666638374,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 0.0036,
      "step": 38000
    },
    {
      "epoch": 2.111666666666667,
      "grad_norm": 0.329044371843338,
      "learning_rate": 2.8883333333333335e-05,
      "loss": 0.0038,
      "step": 38010
    },
    {
      "epoch": 2.112222222222222,
      "grad_norm": 0.09052330255508423,
      "learning_rate": 2.8877777777777782e-05,
      "loss": 0.0046,
      "step": 38020
    },
    {
      "epoch": 2.1127777777777776,
      "grad_norm": 0.11973343789577484,
      "learning_rate": 2.8872222222222222e-05,
      "loss": 0.0041,
      "step": 38030
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.34435775876045227,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0032,
      "step": 38040
    },
    {
      "epoch": 2.113888888888889,
      "grad_norm": 0.1195225641131401,
      "learning_rate": 2.886111111111111e-05,
      "loss": 0.003,
      "step": 38050
    },
    {
      "epoch": 2.1144444444444446,
      "grad_norm": 0.507412314414978,
      "learning_rate": 2.8855555555555556e-05,
      "loss": 0.0044,
      "step": 38060
    },
    {
      "epoch": 2.115,
      "grad_norm": 0.38772913813591003,
      "learning_rate": 2.885e-05,
      "loss": 0.0023,
      "step": 38070
    },
    {
      "epoch": 2.1155555555555554,
      "grad_norm": 0.20933175086975098,
      "learning_rate": 2.8844444444444447e-05,
      "loss": 0.0035,
      "step": 38080
    },
    {
      "epoch": 2.116111111111111,
      "grad_norm": 0.2982455790042877,
      "learning_rate": 2.8838888888888887e-05,
      "loss": 0.0029,
      "step": 38090
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.2692349851131439,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0044,
      "step": 38100
    },
    {
      "epoch": 2.1172222222222223,
      "grad_norm": 0.03203001245856285,
      "learning_rate": 2.882777777777778e-05,
      "loss": 0.0026,
      "step": 38110
    },
    {
      "epoch": 2.117777777777778,
      "grad_norm": 0.727043867111206,
      "learning_rate": 2.882222222222222e-05,
      "loss": 0.0039,
      "step": 38120
    },
    {
      "epoch": 2.118333333333333,
      "grad_norm": 0.35850146412849426,
      "learning_rate": 2.8816666666666668e-05,
      "loss": 0.0027,
      "step": 38130
    },
    {
      "epoch": 2.118888888888889,
      "grad_norm": 0.32861554622650146,
      "learning_rate": 2.881111111111111e-05,
      "loss": 0.0055,
      "step": 38140
    },
    {
      "epoch": 2.1194444444444445,
      "grad_norm": 0.47702646255493164,
      "learning_rate": 2.880555555555556e-05,
      "loss": 0.0032,
      "step": 38150
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.1793193221092224,
      "learning_rate": 2.88e-05,
      "loss": 0.0034,
      "step": 38160
    },
    {
      "epoch": 2.1205555555555557,
      "grad_norm": 0.4178330898284912,
      "learning_rate": 2.8794444444444446e-05,
      "loss": 0.0039,
      "step": 38170
    },
    {
      "epoch": 2.121111111111111,
      "grad_norm": 0.11945547163486481,
      "learning_rate": 2.8788888888888893e-05,
      "loss": 0.0043,
      "step": 38180
    },
    {
      "epoch": 2.1216666666666666,
      "grad_norm": 0.11985912919044495,
      "learning_rate": 2.8783333333333333e-05,
      "loss": 0.0029,
      "step": 38190
    },
    {
      "epoch": 2.1222222222222222,
      "grad_norm": 0.15035408735275269,
      "learning_rate": 2.877777777777778e-05,
      "loss": 0.0035,
      "step": 38200
    },
    {
      "epoch": 2.122777777777778,
      "grad_norm": 0.032606471329927444,
      "learning_rate": 2.8772222222222223e-05,
      "loss": 0.0038,
      "step": 38210
    },
    {
      "epoch": 2.1233333333333335,
      "grad_norm": 0.15025556087493896,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.0046,
      "step": 38220
    },
    {
      "epoch": 2.1238888888888887,
      "grad_norm": 0.16182106733322144,
      "learning_rate": 2.876111111111111e-05,
      "loss": 0.0038,
      "step": 38230
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 0.23901396989822388,
      "learning_rate": 2.8755555555555557e-05,
      "loss": 0.0039,
      "step": 38240
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.2983250319957733,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.0045,
      "step": 38250
    },
    {
      "epoch": 2.1255555555555556,
      "grad_norm": 0.15028680860996246,
      "learning_rate": 2.8744444444444444e-05,
      "loss": 0.0038,
      "step": 38260
    },
    {
      "epoch": 2.1261111111111113,
      "grad_norm": 0.06220293417572975,
      "learning_rate": 2.873888888888889e-05,
      "loss": 0.0036,
      "step": 38270
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.23866604268550873,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0055,
      "step": 38280
    },
    {
      "epoch": 2.127222222222222,
      "grad_norm": 0.03186018019914627,
      "learning_rate": 2.8727777777777782e-05,
      "loss": 0.0047,
      "step": 38290
    },
    {
      "epoch": 2.1277777777777778,
      "grad_norm": 0.00959811545908451,
      "learning_rate": 2.8722222222222222e-05,
      "loss": 0.0031,
      "step": 38300
    },
    {
      "epoch": 2.1283333333333334,
      "grad_norm": 0.4472421407699585,
      "learning_rate": 2.871666666666667e-05,
      "loss": 0.003,
      "step": 38310
    },
    {
      "epoch": 2.128888888888889,
      "grad_norm": 0.1096581444144249,
      "learning_rate": 2.8711111111111113e-05,
      "loss": 0.0036,
      "step": 38320
    },
    {
      "epoch": 2.1294444444444443,
      "grad_norm": 0.15007156133651733,
      "learning_rate": 2.8705555555555556e-05,
      "loss": 0.0028,
      "step": 38330
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.2740873098373413,
      "learning_rate": 2.87e-05,
      "loss": 0.003,
      "step": 38340
    },
    {
      "epoch": 2.1305555555555555,
      "grad_norm": 0.06163376569747925,
      "learning_rate": 2.8694444444444447e-05,
      "loss": 0.0058,
      "step": 38350
    },
    {
      "epoch": 2.131111111111111,
      "grad_norm": 0.2386062741279602,
      "learning_rate": 2.8688888888888894e-05,
      "loss": 0.0034,
      "step": 38360
    },
    {
      "epoch": 2.131666666666667,
      "grad_norm": 0.47936880588531494,
      "learning_rate": 2.8683333333333334e-05,
      "loss": 0.0036,
      "step": 38370
    },
    {
      "epoch": 2.132222222222222,
      "grad_norm": 0.20915040373802185,
      "learning_rate": 2.867777777777778e-05,
      "loss": 0.0045,
      "step": 38380
    },
    {
      "epoch": 2.1327777777777777,
      "grad_norm": 0.320171058177948,
      "learning_rate": 2.8672222222222224e-05,
      "loss": 0.0034,
      "step": 38390
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.09553766995668411,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0041,
      "step": 38400
    },
    {
      "epoch": 2.133888888888889,
      "grad_norm": 0.5075400471687317,
      "learning_rate": 2.866111111111111e-05,
      "loss": 0.0038,
      "step": 38410
    },
    {
      "epoch": 2.1344444444444446,
      "grad_norm": 0.4806101322174072,
      "learning_rate": 2.8655555555555558e-05,
      "loss": 0.0038,
      "step": 38420
    },
    {
      "epoch": 2.135,
      "grad_norm": 0.4172332286834717,
      "learning_rate": 2.865e-05,
      "loss": 0.004,
      "step": 38430
    },
    {
      "epoch": 2.1355555555555554,
      "grad_norm": 0.2682529389858246,
      "learning_rate": 2.8644444444444445e-05,
      "loss": 0.0033,
      "step": 38440
    },
    {
      "epoch": 2.136111111111111,
      "grad_norm": 0.29975196719169617,
      "learning_rate": 2.8638888888888892e-05,
      "loss": 0.0038,
      "step": 38450
    },
    {
      "epoch": 2.1366666666666667,
      "grad_norm": 0.039217956364154816,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.0031,
      "step": 38460
    },
    {
      "epoch": 2.1372222222222224,
      "grad_norm": 0.06925773620605469,
      "learning_rate": 2.8627777777777783e-05,
      "loss": 0.0043,
      "step": 38470
    },
    {
      "epoch": 2.137777777777778,
      "grad_norm": 0.35047584772109985,
      "learning_rate": 2.8622222222222223e-05,
      "loss": 0.0049,
      "step": 38480
    },
    {
      "epoch": 2.138333333333333,
      "grad_norm": 0.14930440485477448,
      "learning_rate": 2.861666666666667e-05,
      "loss": 0.0029,
      "step": 38490
    },
    {
      "epoch": 2.138888888888889,
      "grad_norm": 0.1743999719619751,
      "learning_rate": 2.861111111111111e-05,
      "loss": 0.0041,
      "step": 38500
    },
    {
      "epoch": 2.1394444444444445,
      "grad_norm": 0.29064804315567017,
      "learning_rate": 2.8605555555555557e-05,
      "loss": 0.0035,
      "step": 38510
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.1977648138999939,
      "learning_rate": 2.86e-05,
      "loss": 0.0027,
      "step": 38520
    },
    {
      "epoch": 2.1405555555555553,
      "grad_norm": 0.23828265070915222,
      "learning_rate": 2.8594444444444448e-05,
      "loss": 0.0045,
      "step": 38530
    },
    {
      "epoch": 2.141111111111111,
      "grad_norm": 0.20997203886508942,
      "learning_rate": 2.8588888888888895e-05,
      "loss": 0.0046,
      "step": 38540
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 0.687324583530426,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.0041,
      "step": 38550
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 0.1500849574804306,
      "learning_rate": 2.857777777777778e-05,
      "loss": 0.0034,
      "step": 38560
    },
    {
      "epoch": 2.142777777777778,
      "grad_norm": 0.2806810140609741,
      "learning_rate": 2.8572222222222222e-05,
      "loss": 0.0036,
      "step": 38570
    },
    {
      "epoch": 2.1433333333333335,
      "grad_norm": 0.2656562328338623,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0041,
      "step": 38580
    },
    {
      "epoch": 2.1438888888888887,
      "grad_norm": 0.032041046768426895,
      "learning_rate": 2.8561111111111112e-05,
      "loss": 0.0029,
      "step": 38590
    },
    {
      "epoch": 2.1444444444444444,
      "grad_norm": 0.3631123900413513,
      "learning_rate": 2.855555555555556e-05,
      "loss": 0.0035,
      "step": 38600
    },
    {
      "epoch": 2.145,
      "grad_norm": 0.06075986474752426,
      "learning_rate": 2.855e-05,
      "loss": 0.0028,
      "step": 38610
    },
    {
      "epoch": 2.1455555555555557,
      "grad_norm": 0.032174743711948395,
      "learning_rate": 2.8544444444444446e-05,
      "loss": 0.0037,
      "step": 38620
    },
    {
      "epoch": 2.1461111111111113,
      "grad_norm": 0.0305954422801733,
      "learning_rate": 2.8538888888888893e-05,
      "loss": 0.0027,
      "step": 38630
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.3623582720756531,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0039,
      "step": 38640
    },
    {
      "epoch": 2.147222222222222,
      "grad_norm": 0.23858274519443512,
      "learning_rate": 2.852777777777778e-05,
      "loss": 0.0032,
      "step": 38650
    },
    {
      "epoch": 2.147777777777778,
      "grad_norm": 0.2383672446012497,
      "learning_rate": 2.8522222222222224e-05,
      "loss": 0.0037,
      "step": 38660
    },
    {
      "epoch": 2.1483333333333334,
      "grad_norm": 0.18022935092449188,
      "learning_rate": 2.851666666666667e-05,
      "loss": 0.004,
      "step": 38670
    },
    {
      "epoch": 2.148888888888889,
      "grad_norm": 0.11263635009527206,
      "learning_rate": 2.851111111111111e-05,
      "loss": 0.0041,
      "step": 38680
    },
    {
      "epoch": 2.1494444444444443,
      "grad_norm": 0.03362071514129639,
      "learning_rate": 2.8505555555555558e-05,
      "loss": 0.004,
      "step": 38690
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.17914505302906036,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0023,
      "step": 38700
    },
    {
      "epoch": 2.1505555555555556,
      "grad_norm": 0.26949962973594666,
      "learning_rate": 2.8494444444444445e-05,
      "loss": 0.0024,
      "step": 38710
    },
    {
      "epoch": 2.151111111111111,
      "grad_norm": 0.26221340894699097,
      "learning_rate": 2.8488888888888892e-05,
      "loss": 0.0041,
      "step": 38720
    },
    {
      "epoch": 2.151666666666667,
      "grad_norm": 0.20940576493740082,
      "learning_rate": 2.8483333333333336e-05,
      "loss": 0.0031,
      "step": 38730
    },
    {
      "epoch": 2.152222222222222,
      "grad_norm": 0.08988214284181595,
      "learning_rate": 2.8477777777777783e-05,
      "loss": 0.0025,
      "step": 38740
    },
    {
      "epoch": 2.1527777777777777,
      "grad_norm": 0.0602252334356308,
      "learning_rate": 2.8472222222222223e-05,
      "loss": 0.0044,
      "step": 38750
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.27195021510124207,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0037,
      "step": 38760
    },
    {
      "epoch": 2.153888888888889,
      "grad_norm": 0.26883915066719055,
      "learning_rate": 2.846111111111111e-05,
      "loss": 0.0036,
      "step": 38770
    },
    {
      "epoch": 2.1544444444444446,
      "grad_norm": 0.03194381296634674,
      "learning_rate": 2.8455555555555557e-05,
      "loss": 0.0042,
      "step": 38780
    },
    {
      "epoch": 2.155,
      "grad_norm": 0.7037097811698914,
      "learning_rate": 2.845e-05,
      "loss": 0.0032,
      "step": 38790
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 0.23829315602779388,
      "learning_rate": 2.8444444444444447e-05,
      "loss": 0.0035,
      "step": 38800
    },
    {
      "epoch": 2.156111111111111,
      "grad_norm": 0.02076709270477295,
      "learning_rate": 2.8438888888888894e-05,
      "loss": 0.004,
      "step": 38810
    },
    {
      "epoch": 2.1566666666666667,
      "grad_norm": 0.18097460269927979,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.004,
      "step": 38820
    },
    {
      "epoch": 2.1572222222222224,
      "grad_norm": 0.2503379285335541,
      "learning_rate": 2.842777777777778e-05,
      "loss": 0.004,
      "step": 38830
    },
    {
      "epoch": 2.1577777777777776,
      "grad_norm": 0.5361414551734924,
      "learning_rate": 2.842222222222222e-05,
      "loss": 0.003,
      "step": 38840
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 0.44760861992836,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.0044,
      "step": 38850
    },
    {
      "epoch": 2.158888888888889,
      "grad_norm": 0.6276411414146423,
      "learning_rate": 2.8411111111111112e-05,
      "loss": 0.0033,
      "step": 38860
    },
    {
      "epoch": 2.1594444444444445,
      "grad_norm": 0.2988111078739166,
      "learning_rate": 2.840555555555556e-05,
      "loss": 0.0029,
      "step": 38870
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.3309899866580963,
      "learning_rate": 2.84e-05,
      "loss": 0.0034,
      "step": 38880
    },
    {
      "epoch": 2.160555555555556,
      "grad_norm": 0.4706578552722931,
      "learning_rate": 2.8394444444444446e-05,
      "loss": 0.003,
      "step": 38890
    },
    {
      "epoch": 2.161111111111111,
      "grad_norm": 0.4574870765209198,
      "learning_rate": 2.8388888888888893e-05,
      "loss": 0.0036,
      "step": 38900
    },
    {
      "epoch": 2.1616666666666666,
      "grad_norm": 0.06229246035218239,
      "learning_rate": 2.8383333333333333e-05,
      "loss": 0.0044,
      "step": 38910
    },
    {
      "epoch": 2.1622222222222223,
      "grad_norm": 0.37375596165657043,
      "learning_rate": 2.837777777777778e-05,
      "loss": 0.0034,
      "step": 38920
    },
    {
      "epoch": 2.162777777777778,
      "grad_norm": 0.18004506826400757,
      "learning_rate": 2.8372222222222224e-05,
      "loss": 0.0038,
      "step": 38930
    },
    {
      "epoch": 2.163333333333333,
      "grad_norm": 0.44662997126579285,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.0044,
      "step": 38940
    },
    {
      "epoch": 2.1638888888888888,
      "grad_norm": 0.35906603932380676,
      "learning_rate": 2.836111111111111e-05,
      "loss": 0.0043,
      "step": 38950
    },
    {
      "epoch": 2.1644444444444444,
      "grad_norm": 0.2679993510246277,
      "learning_rate": 2.8355555555555558e-05,
      "loss": 0.0041,
      "step": 38960
    },
    {
      "epoch": 2.165,
      "grad_norm": 0.06089169904589653,
      "learning_rate": 2.8349999999999998e-05,
      "loss": 0.0026,
      "step": 38970
    },
    {
      "epoch": 2.1655555555555557,
      "grad_norm": 0.3883282244205475,
      "learning_rate": 2.8344444444444445e-05,
      "loss": 0.0048,
      "step": 38980
    },
    {
      "epoch": 2.1661111111111113,
      "grad_norm": 0.27585211396217346,
      "learning_rate": 2.8338888888888892e-05,
      "loss": 0.0029,
      "step": 38990
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.09124547243118286,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0027,
      "step": 39000
    },
    {
      "epoch": 2.167222222222222,
      "grad_norm": 0.060683682560920715,
      "learning_rate": 2.8327777777777782e-05,
      "loss": 0.0033,
      "step": 39010
    },
    {
      "epoch": 2.167777777777778,
      "grad_norm": 0.11963348835706711,
      "learning_rate": 2.8322222222222222e-05,
      "loss": 0.0027,
      "step": 39020
    },
    {
      "epoch": 2.1683333333333334,
      "grad_norm": 0.5966293215751648,
      "learning_rate": 2.831666666666667e-05,
      "loss": 0.0029,
      "step": 39030
    },
    {
      "epoch": 2.168888888888889,
      "grad_norm": 0.11989249289035797,
      "learning_rate": 2.831111111111111e-05,
      "loss": 0.0032,
      "step": 39040
    },
    {
      "epoch": 2.1694444444444443,
      "grad_norm": 0.09012310951948166,
      "learning_rate": 2.8305555555555557e-05,
      "loss": 0.0032,
      "step": 39050
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.20827065408229828,
      "learning_rate": 2.83e-05,
      "loss": 0.0043,
      "step": 39060
    },
    {
      "epoch": 2.1705555555555556,
      "grad_norm": 0.20897482335567474,
      "learning_rate": 2.8294444444444447e-05,
      "loss": 0.0036,
      "step": 39070
    },
    {
      "epoch": 2.171111111111111,
      "grad_norm": 0.6859800815582275,
      "learning_rate": 2.8288888888888894e-05,
      "loss": 0.0036,
      "step": 39080
    },
    {
      "epoch": 2.171666666666667,
      "grad_norm": 0.5444441437721252,
      "learning_rate": 2.8283333333333334e-05,
      "loss": 0.005,
      "step": 39090
    },
    {
      "epoch": 2.172222222222222,
      "grad_norm": 0.03220909833908081,
      "learning_rate": 2.827777777777778e-05,
      "loss": 0.0034,
      "step": 39100
    },
    {
      "epoch": 2.1727777777777777,
      "grad_norm": 0.10982921719551086,
      "learning_rate": 2.827222222222222e-05,
      "loss": 0.0037,
      "step": 39110
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.38859525322914124,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0044,
      "step": 39120
    },
    {
      "epoch": 2.173888888888889,
      "grad_norm": 0.07913007587194443,
      "learning_rate": 2.8261111111111112e-05,
      "loss": 0.0023,
      "step": 39130
    },
    {
      "epoch": 2.1744444444444446,
      "grad_norm": 0.24194680154323578,
      "learning_rate": 2.825555555555556e-05,
      "loss": 0.0033,
      "step": 39140
    },
    {
      "epoch": 2.175,
      "grad_norm": 0.29909130930900574,
      "learning_rate": 2.825e-05,
      "loss": 0.0037,
      "step": 39150
    },
    {
      "epoch": 2.1755555555555555,
      "grad_norm": 0.5363534688949585,
      "learning_rate": 2.8244444444444446e-05,
      "loss": 0.004,
      "step": 39160
    },
    {
      "epoch": 2.176111111111111,
      "grad_norm": 0.40146076679229736,
      "learning_rate": 2.8238888888888893e-05,
      "loss": 0.0037,
      "step": 39170
    },
    {
      "epoch": 2.1766666666666667,
      "grad_norm": 0.11985156685113907,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0053,
      "step": 39180
    },
    {
      "epoch": 2.1772222222222224,
      "grad_norm": 0.20405542850494385,
      "learning_rate": 2.822777777777778e-05,
      "loss": 0.0045,
      "step": 39190
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 0.32616186141967773,
      "learning_rate": 2.8222222222222223e-05,
      "loss": 0.0037,
      "step": 39200
    },
    {
      "epoch": 2.1783333333333332,
      "grad_norm": 0.3884440064430237,
      "learning_rate": 2.821666666666667e-05,
      "loss": 0.0036,
      "step": 39210
    },
    {
      "epoch": 2.178888888888889,
      "grad_norm": 0.23818767070770264,
      "learning_rate": 2.821111111111111e-05,
      "loss": 0.0037,
      "step": 39220
    },
    {
      "epoch": 2.1794444444444445,
      "grad_norm": 0.19701458513736725,
      "learning_rate": 2.8205555555555557e-05,
      "loss": 0.0036,
      "step": 39230
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.442649245262146,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.004,
      "step": 39240
    },
    {
      "epoch": 2.1805555555555554,
      "grad_norm": 0.2428388148546219,
      "learning_rate": 2.8194444444444445e-05,
      "loss": 0.0025,
      "step": 39250
    },
    {
      "epoch": 2.181111111111111,
      "grad_norm": 0.12077520787715912,
      "learning_rate": 2.818888888888889e-05,
      "loss": 0.0037,
      "step": 39260
    },
    {
      "epoch": 2.1816666666666666,
      "grad_norm": 0.12013738602399826,
      "learning_rate": 2.8183333333333335e-05,
      "loss": 0.0036,
      "step": 39270
    },
    {
      "epoch": 2.1822222222222223,
      "grad_norm": 0.14998209476470947,
      "learning_rate": 2.8177777777777782e-05,
      "loss": 0.0039,
      "step": 39280
    },
    {
      "epoch": 2.182777777777778,
      "grad_norm": 0.14920438826084137,
      "learning_rate": 2.8172222222222222e-05,
      "loss": 0.0038,
      "step": 39290
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.1591266542673111,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.003,
      "step": 39300
    },
    {
      "epoch": 2.1838888888888888,
      "grad_norm": 0.11919225752353668,
      "learning_rate": 2.816111111111111e-05,
      "loss": 0.0046,
      "step": 39310
    },
    {
      "epoch": 2.1844444444444444,
      "grad_norm": 0.18570716679096222,
      "learning_rate": 2.8155555555555556e-05,
      "loss": 0.0037,
      "step": 39320
    },
    {
      "epoch": 2.185,
      "grad_norm": 0.38836389780044556,
      "learning_rate": 2.815e-05,
      "loss": 0.0034,
      "step": 39330
    },
    {
      "epoch": 2.1855555555555557,
      "grad_norm": 0.2989667057991028,
      "learning_rate": 2.8144444444444447e-05,
      "loss": 0.0037,
      "step": 39340
    },
    {
      "epoch": 2.186111111111111,
      "grad_norm": 0.03849893435835838,
      "learning_rate": 2.8138888888888894e-05,
      "loss": 0.005,
      "step": 39350
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.23892976343631744,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0025,
      "step": 39360
    },
    {
      "epoch": 2.187222222222222,
      "grad_norm": 0.42584148049354553,
      "learning_rate": 2.812777777777778e-05,
      "loss": 0.0034,
      "step": 39370
    },
    {
      "epoch": 2.187777777777778,
      "grad_norm": 0.03350178524851799,
      "learning_rate": 2.812222222222222e-05,
      "loss": 0.0037,
      "step": 39380
    },
    {
      "epoch": 2.1883333333333335,
      "grad_norm": 0.09264159202575684,
      "learning_rate": 2.8116666666666668e-05,
      "loss": 0.004,
      "step": 39390
    },
    {
      "epoch": 2.188888888888889,
      "grad_norm": 0.2086838036775589,
      "learning_rate": 2.811111111111111e-05,
      "loss": 0.0032,
      "step": 39400
    },
    {
      "epoch": 2.1894444444444443,
      "grad_norm": 0.5575869679450989,
      "learning_rate": 2.810555555555556e-05,
      "loss": 0.0029,
      "step": 39410
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.01034068875014782,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.0024,
      "step": 39420
    },
    {
      "epoch": 2.1905555555555556,
      "grad_norm": 0.2727782130241394,
      "learning_rate": 2.8094444444444446e-05,
      "loss": 0.0048,
      "step": 39430
    },
    {
      "epoch": 2.1911111111111112,
      "grad_norm": 0.23866012692451477,
      "learning_rate": 2.8088888888888893e-05,
      "loss": 0.003,
      "step": 39440
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 0.08987857401371002,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.0042,
      "step": 39450
    },
    {
      "epoch": 2.192222222222222,
      "grad_norm": 0.17897352576255798,
      "learning_rate": 2.807777777777778e-05,
      "loss": 0.0034,
      "step": 39460
    },
    {
      "epoch": 2.1927777777777777,
      "grad_norm": 0.1796991378068924,
      "learning_rate": 2.8072222222222223e-05,
      "loss": 0.0035,
      "step": 39470
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.2934512794017792,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0026,
      "step": 39480
    },
    {
      "epoch": 2.193888888888889,
      "grad_norm": 0.1251591444015503,
      "learning_rate": 2.806111111111111e-05,
      "loss": 0.0031,
      "step": 39490
    },
    {
      "epoch": 2.1944444444444446,
      "grad_norm": 0.034388985484838486,
      "learning_rate": 2.8055555555555557e-05,
      "loss": 0.0045,
      "step": 39500
    },
    {
      "epoch": 2.195,
      "grad_norm": 0.03134772181510925,
      "learning_rate": 2.8050000000000004e-05,
      "loss": 0.0043,
      "step": 39510
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 0.23892347514629364,
      "learning_rate": 2.8044444444444444e-05,
      "loss": 0.0039,
      "step": 39520
    },
    {
      "epoch": 2.196111111111111,
      "grad_norm": 0.47845181822776794,
      "learning_rate": 2.803888888888889e-05,
      "loss": 0.0032,
      "step": 39530
    },
    {
      "epoch": 2.1966666666666668,
      "grad_norm": 0.018903518095612526,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0042,
      "step": 39540
    },
    {
      "epoch": 2.1972222222222224,
      "grad_norm": 0.7165036797523499,
      "learning_rate": 2.8027777777777782e-05,
      "loss": 0.0033,
      "step": 39550
    },
    {
      "epoch": 2.1977777777777776,
      "grad_norm": 0.5263089537620544,
      "learning_rate": 2.8022222222222222e-05,
      "loss": 0.004,
      "step": 39560
    },
    {
      "epoch": 2.1983333333333333,
      "grad_norm": 0.29840415716171265,
      "learning_rate": 2.801666666666667e-05,
      "loss": 0.0027,
      "step": 39570
    },
    {
      "epoch": 2.198888888888889,
      "grad_norm": 0.41861793398857117,
      "learning_rate": 2.801111111111111e-05,
      "loss": 0.0028,
      "step": 39580
    },
    {
      "epoch": 2.1994444444444445,
      "grad_norm": 0.4167877435684204,
      "learning_rate": 2.8005555555555556e-05,
      "loss": 0.0038,
      "step": 39590
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.11921317130327225,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.003,
      "step": 39600
    },
    {
      "epoch": 2.2005555555555554,
      "grad_norm": 0.23919792473316193,
      "learning_rate": 2.7994444444444447e-05,
      "loss": 0.0034,
      "step": 39610
    },
    {
      "epoch": 2.201111111111111,
      "grad_norm": 0.12784463167190552,
      "learning_rate": 2.7988888888888893e-05,
      "loss": 0.0028,
      "step": 39620
    },
    {
      "epoch": 2.2016666666666667,
      "grad_norm": 0.6816268563270569,
      "learning_rate": 2.7983333333333334e-05,
      "loss": 0.0033,
      "step": 39630
    },
    {
      "epoch": 2.2022222222222223,
      "grad_norm": 0.4176883399486542,
      "learning_rate": 2.797777777777778e-05,
      "loss": 0.0046,
      "step": 39640
    },
    {
      "epoch": 2.202777777777778,
      "grad_norm": 0.3282238245010376,
      "learning_rate": 2.797222222222222e-05,
      "loss": 0.0063,
      "step": 39650
    },
    {
      "epoch": 2.203333333333333,
      "grad_norm": 0.30786561965942383,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.006,
      "step": 39660
    },
    {
      "epoch": 2.203888888888889,
      "grad_norm": 0.17912736535072327,
      "learning_rate": 2.796111111111111e-05,
      "loss": 0.0032,
      "step": 39670
    },
    {
      "epoch": 2.2044444444444444,
      "grad_norm": 0.15345308184623718,
      "learning_rate": 2.7955555555555558e-05,
      "loss": 0.0039,
      "step": 39680
    },
    {
      "epoch": 2.205,
      "grad_norm": 0.1804015338420868,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 0.0041,
      "step": 39690
    },
    {
      "epoch": 2.2055555555555557,
      "grad_norm": 0.009422802366316319,
      "learning_rate": 2.7944444444444445e-05,
      "loss": 0.0045,
      "step": 39700
    },
    {
      "epoch": 2.206111111111111,
      "grad_norm": 0.14984378218650818,
      "learning_rate": 2.7938888888888892e-05,
      "loss": 0.0028,
      "step": 39710
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.44643041491508484,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0032,
      "step": 39720
    },
    {
      "epoch": 2.207222222222222,
      "grad_norm": 0.2092347890138626,
      "learning_rate": 2.792777777777778e-05,
      "loss": 0.0035,
      "step": 39730
    },
    {
      "epoch": 2.207777777777778,
      "grad_norm": 0.3581293821334839,
      "learning_rate": 2.7922222222222223e-05,
      "loss": 0.0039,
      "step": 39740
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 0.5064273476600647,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.0029,
      "step": 39750
    },
    {
      "epoch": 2.2088888888888887,
      "grad_norm": 0.20926730334758759,
      "learning_rate": 2.791111111111111e-05,
      "loss": 0.0041,
      "step": 39760
    },
    {
      "epoch": 2.2094444444444443,
      "grad_norm": 0.32782241702079773,
      "learning_rate": 2.7905555555555557e-05,
      "loss": 0.0027,
      "step": 39770
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.24275487661361694,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0038,
      "step": 39780
    },
    {
      "epoch": 2.2105555555555556,
      "grad_norm": 0.0632893368601799,
      "learning_rate": 2.7894444444444444e-05,
      "loss": 0.004,
      "step": 39790
    },
    {
      "epoch": 2.2111111111111112,
      "grad_norm": 0.49210062623023987,
      "learning_rate": 2.788888888888889e-05,
      "loss": 0.0034,
      "step": 39800
    },
    {
      "epoch": 2.211666666666667,
      "grad_norm": 0.6149166822433472,
      "learning_rate": 2.7883333333333335e-05,
      "loss": 0.0029,
      "step": 39810
    },
    {
      "epoch": 2.212222222222222,
      "grad_norm": 0.09049047529697418,
      "learning_rate": 2.787777777777778e-05,
      "loss": 0.0042,
      "step": 39820
    },
    {
      "epoch": 2.2127777777777777,
      "grad_norm": 0.12024707347154617,
      "learning_rate": 2.7872222222222222e-05,
      "loss": 0.0031,
      "step": 39830
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.11954598128795624,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.004,
      "step": 39840
    },
    {
      "epoch": 2.213888888888889,
      "grad_norm": 0.014734729193150997,
      "learning_rate": 2.786111111111111e-05,
      "loss": 0.0036,
      "step": 39850
    },
    {
      "epoch": 2.214444444444444,
      "grad_norm": 0.25879406929016113,
      "learning_rate": 2.7855555555555556e-05,
      "loss": 0.0038,
      "step": 39860
    },
    {
      "epoch": 2.215,
      "grad_norm": 0.09055829048156738,
      "learning_rate": 2.7850000000000003e-05,
      "loss": 0.0028,
      "step": 39870
    },
    {
      "epoch": 2.2155555555555555,
      "grad_norm": 0.18113799393177032,
      "learning_rate": 2.7844444444444446e-05,
      "loss": 0.0053,
      "step": 39880
    },
    {
      "epoch": 2.216111111111111,
      "grad_norm": 0.4594018757343292,
      "learning_rate": 2.7838888888888893e-05,
      "loss": 0.0045,
      "step": 39890
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.009574935771524906,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0039,
      "step": 39900
    },
    {
      "epoch": 2.2172222222222224,
      "grad_norm": 0.25590890645980835,
      "learning_rate": 2.782777777777778e-05,
      "loss": 0.0036,
      "step": 39910
    },
    {
      "epoch": 2.2177777777777776,
      "grad_norm": 0.12223349511623383,
      "learning_rate": 2.782222222222222e-05,
      "loss": 0.0057,
      "step": 39920
    },
    {
      "epoch": 2.2183333333333333,
      "grad_norm": 0.23971249163150787,
      "learning_rate": 2.7816666666666667e-05,
      "loss": 0.0034,
      "step": 39930
    },
    {
      "epoch": 2.218888888888889,
      "grad_norm": 0.32807573676109314,
      "learning_rate": 2.781111111111111e-05,
      "loss": 0.0034,
      "step": 39940
    },
    {
      "epoch": 2.2194444444444446,
      "grad_norm": 0.2740723788738251,
      "learning_rate": 2.7805555555555558e-05,
      "loss": 0.0035,
      "step": 39950
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.14959125220775604,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0026,
      "step": 39960
    },
    {
      "epoch": 2.2205555555555554,
      "grad_norm": 0.060398682951927185,
      "learning_rate": 2.7794444444444445e-05,
      "loss": 0.0028,
      "step": 39970
    },
    {
      "epoch": 2.221111111111111,
      "grad_norm": 0.17988911271095276,
      "learning_rate": 2.7788888888888892e-05,
      "loss": 0.0035,
      "step": 39980
    },
    {
      "epoch": 2.2216666666666667,
      "grad_norm": 0.06150068715214729,
      "learning_rate": 2.7783333333333332e-05,
      "loss": 0.0035,
      "step": 39990
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.1605912446975708,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.0029,
      "step": 40000
    },
    {
      "epoch": 2.222777777777778,
      "grad_norm": 0.26973965764045715,
      "learning_rate": 2.7772222222222223e-05,
      "loss": 0.0039,
      "step": 40010
    },
    {
      "epoch": 2.223333333333333,
      "grad_norm": 0.1493270844221115,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.0021,
      "step": 40020
    },
    {
      "epoch": 2.223888888888889,
      "grad_norm": 0.06190285086631775,
      "learning_rate": 2.776111111111111e-05,
      "loss": 0.0024,
      "step": 40030
    },
    {
      "epoch": 2.2244444444444444,
      "grad_norm": 0.4769069254398346,
      "learning_rate": 2.7755555555555557e-05,
      "loss": 0.0042,
      "step": 40040
    },
    {
      "epoch": 2.225,
      "grad_norm": 0.09068598598241806,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.0041,
      "step": 40050
    },
    {
      "epoch": 2.2255555555555557,
      "grad_norm": 0.27114200592041016,
      "learning_rate": 2.7744444444444444e-05,
      "loss": 0.0044,
      "step": 40060
    },
    {
      "epoch": 2.226111111111111,
      "grad_norm": 0.40648871660232544,
      "learning_rate": 2.773888888888889e-05,
      "loss": 0.0037,
      "step": 40070
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.2688441872596741,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0032,
      "step": 40080
    },
    {
      "epoch": 2.227222222222222,
      "grad_norm": 0.34135591983795166,
      "learning_rate": 2.772777777777778e-05,
      "loss": 0.0031,
      "step": 40090
    },
    {
      "epoch": 2.227777777777778,
      "grad_norm": 0.034100230783224106,
      "learning_rate": 2.772222222222222e-05,
      "loss": 0.0024,
      "step": 40100
    },
    {
      "epoch": 2.2283333333333335,
      "grad_norm": 0.20929060876369476,
      "learning_rate": 2.771666666666667e-05,
      "loss": 0.0038,
      "step": 40110
    },
    {
      "epoch": 2.2288888888888887,
      "grad_norm": 0.4340552091598511,
      "learning_rate": 2.771111111111111e-05,
      "loss": 0.0026,
      "step": 40120
    },
    {
      "epoch": 2.2294444444444443,
      "grad_norm": 0.2382589429616928,
      "learning_rate": 2.7705555555555556e-05,
      "loss": 0.0042,
      "step": 40130
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.4765995442867279,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.0042,
      "step": 40140
    },
    {
      "epoch": 2.2305555555555556,
      "grad_norm": 0.38143184781074524,
      "learning_rate": 2.7694444444444446e-05,
      "loss": 0.004,
      "step": 40150
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 0.1788398027420044,
      "learning_rate": 2.7688888888888893e-05,
      "loss": 0.0043,
      "step": 40160
    },
    {
      "epoch": 2.2316666666666665,
      "grad_norm": 0.42727115750312805,
      "learning_rate": 2.7683333333333333e-05,
      "loss": 0.0041,
      "step": 40170
    },
    {
      "epoch": 2.232222222222222,
      "grad_norm": 0.08953692018985748,
      "learning_rate": 2.767777777777778e-05,
      "loss": 0.0039,
      "step": 40180
    },
    {
      "epoch": 2.2327777777777778,
      "grad_norm": 0.23932158946990967,
      "learning_rate": 2.767222222222222e-05,
      "loss": 0.0037,
      "step": 40190
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.2688406705856323,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0039,
      "step": 40200
    },
    {
      "epoch": 2.233888888888889,
      "grad_norm": 0.20864826440811157,
      "learning_rate": 2.766111111111111e-05,
      "loss": 0.0029,
      "step": 40210
    },
    {
      "epoch": 2.2344444444444447,
      "grad_norm": 0.11937001347541809,
      "learning_rate": 2.7655555555555558e-05,
      "loss": 0.0038,
      "step": 40220
    },
    {
      "epoch": 2.235,
      "grad_norm": 0.09238332509994507,
      "learning_rate": 2.7650000000000005e-05,
      "loss": 0.0045,
      "step": 40230
    },
    {
      "epoch": 2.2355555555555555,
      "grad_norm": 0.27132752537727356,
      "learning_rate": 2.7644444444444445e-05,
      "loss": 0.0028,
      "step": 40240
    },
    {
      "epoch": 2.236111111111111,
      "grad_norm": 0.5665899515151978,
      "learning_rate": 2.7638888888888892e-05,
      "loss": 0.0031,
      "step": 40250
    },
    {
      "epoch": 2.236666666666667,
      "grad_norm": 0.4171959459781647,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0045,
      "step": 40260
    },
    {
      "epoch": 2.237222222222222,
      "grad_norm": 0.12289150059223175,
      "learning_rate": 2.762777777777778e-05,
      "loss": 0.0042,
      "step": 40270
    },
    {
      "epoch": 2.2377777777777776,
      "grad_norm": 0.1193518340587616,
      "learning_rate": 2.7622222222222222e-05,
      "loss": 0.0035,
      "step": 40280
    },
    {
      "epoch": 2.2383333333333333,
      "grad_norm": 0.1542692631483078,
      "learning_rate": 2.761666666666667e-05,
      "loss": 0.0038,
      "step": 40290
    },
    {
      "epoch": 2.238888888888889,
      "grad_norm": 0.1791868954896927,
      "learning_rate": 2.761111111111111e-05,
      "loss": 0.0043,
      "step": 40300
    },
    {
      "epoch": 2.2394444444444446,
      "grad_norm": 0.1195690706372261,
      "learning_rate": 2.7605555555555556e-05,
      "loss": 0.0036,
      "step": 40310
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.06020579859614372,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.004,
      "step": 40320
    },
    {
      "epoch": 2.2405555555555554,
      "grad_norm": 0.12016085535287857,
      "learning_rate": 2.7594444444444444e-05,
      "loss": 0.0039,
      "step": 40330
    },
    {
      "epoch": 2.241111111111111,
      "grad_norm": 0.20818382501602173,
      "learning_rate": 2.758888888888889e-05,
      "loss": 0.0029,
      "step": 40340
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 0.20855095982551575,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.0036,
      "step": 40350
    },
    {
      "epoch": 2.2422222222222223,
      "grad_norm": 0.09082100540399551,
      "learning_rate": 2.757777777777778e-05,
      "loss": 0.0034,
      "step": 40360
    },
    {
      "epoch": 2.242777777777778,
      "grad_norm": 0.28787702322006226,
      "learning_rate": 2.757222222222222e-05,
      "loss": 0.0038,
      "step": 40370
    },
    {
      "epoch": 2.243333333333333,
      "grad_norm": 0.5369738340377808,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0033,
      "step": 40380
    },
    {
      "epoch": 2.243888888888889,
      "grad_norm": 0.476757675409317,
      "learning_rate": 2.7561111111111108e-05,
      "loss": 0.0039,
      "step": 40390
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 0.06231691688299179,
      "learning_rate": 2.7555555555555555e-05,
      "loss": 0.0031,
      "step": 40400
    },
    {
      "epoch": 2.245,
      "grad_norm": 0.12011060118675232,
      "learning_rate": 2.7550000000000002e-05,
      "loss": 0.0038,
      "step": 40410
    },
    {
      "epoch": 2.2455555555555557,
      "grad_norm": 0.2397783100605011,
      "learning_rate": 2.7544444444444446e-05,
      "loss": 0.0028,
      "step": 40420
    },
    {
      "epoch": 2.246111111111111,
      "grad_norm": 0.5215973854064941,
      "learning_rate": 2.7538888888888893e-05,
      "loss": 0.0036,
      "step": 40430
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.23887403309345245,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0042,
      "step": 40440
    },
    {
      "epoch": 2.2472222222222222,
      "grad_norm": 0.19476057589054108,
      "learning_rate": 2.752777777777778e-05,
      "loss": 0.0026,
      "step": 40450
    },
    {
      "epoch": 2.247777777777778,
      "grad_norm": 0.5670856833457947,
      "learning_rate": 2.752222222222222e-05,
      "loss": 0.0032,
      "step": 40460
    },
    {
      "epoch": 2.2483333333333335,
      "grad_norm": 0.3587140738964081,
      "learning_rate": 2.7516666666666667e-05,
      "loss": 0.0027,
      "step": 40470
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 0.29829394817352295,
      "learning_rate": 2.751111111111111e-05,
      "loss": 0.0024,
      "step": 40480
    },
    {
      "epoch": 2.2494444444444444,
      "grad_norm": 0.062184516340494156,
      "learning_rate": 2.7505555555555557e-05,
      "loss": 0.0031,
      "step": 40490
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.06270095705986023,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0036,
      "step": 40500
    },
    {
      "epoch": 2.2505555555555556,
      "grad_norm": 0.17972539365291595,
      "learning_rate": 2.7494444444444445e-05,
      "loss": 0.0043,
      "step": 40510
    },
    {
      "epoch": 2.2511111111111113,
      "grad_norm": 0.21239323914051056,
      "learning_rate": 2.748888888888889e-05,
      "loss": 0.0046,
      "step": 40520
    },
    {
      "epoch": 2.2516666666666665,
      "grad_norm": 0.269921213388443,
      "learning_rate": 2.748333333333333e-05,
      "loss": 0.0034,
      "step": 40530
    },
    {
      "epoch": 2.252222222222222,
      "grad_norm": 0.03264404460787773,
      "learning_rate": 2.747777777777778e-05,
      "loss": 0.005,
      "step": 40540
    },
    {
      "epoch": 2.2527777777777778,
      "grad_norm": 0.41703274846076965,
      "learning_rate": 2.7472222222222222e-05,
      "loss": 0.0039,
      "step": 40550
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.009876159951090813,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0026,
      "step": 40560
    },
    {
      "epoch": 2.253888888888889,
      "grad_norm": 0.4175350069999695,
      "learning_rate": 2.746111111111111e-05,
      "loss": 0.004,
      "step": 40570
    },
    {
      "epoch": 2.2544444444444443,
      "grad_norm": 0.17977184057235718,
      "learning_rate": 2.7455555555555556e-05,
      "loss": 0.0035,
      "step": 40580
    },
    {
      "epoch": 2.255,
      "grad_norm": 0.3579231798648834,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 0.0049,
      "step": 40590
    },
    {
      "epoch": 2.2555555555555555,
      "grad_norm": 0.29413774609565735,
      "learning_rate": 2.7444444444444443e-05,
      "loss": 0.0061,
      "step": 40600
    },
    {
      "epoch": 2.256111111111111,
      "grad_norm": 0.08988859504461288,
      "learning_rate": 2.743888888888889e-05,
      "loss": 0.003,
      "step": 40610
    },
    {
      "epoch": 2.256666666666667,
      "grad_norm": 0.18159064650535583,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0035,
      "step": 40620
    },
    {
      "epoch": 2.2572222222222225,
      "grad_norm": 0.2983582019805908,
      "learning_rate": 2.742777777777778e-05,
      "loss": 0.0033,
      "step": 40630
    },
    {
      "epoch": 2.2577777777777777,
      "grad_norm": 0.17980092763900757,
      "learning_rate": 2.742222222222222e-05,
      "loss": 0.0028,
      "step": 40640
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 0.21800340712070465,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.0044,
      "step": 40650
    },
    {
      "epoch": 2.258888888888889,
      "grad_norm": 0.12000680714845657,
      "learning_rate": 2.7411111111111115e-05,
      "loss": 0.0034,
      "step": 40660
    },
    {
      "epoch": 2.2594444444444446,
      "grad_norm": 0.1804279386997223,
      "learning_rate": 2.7405555555555555e-05,
      "loss": 0.0029,
      "step": 40670
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.09103532135486603,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0042,
      "step": 40680
    },
    {
      "epoch": 2.2605555555555554,
      "grad_norm": 0.14903944730758667,
      "learning_rate": 2.7394444444444445e-05,
      "loss": 0.0029,
      "step": 40690
    },
    {
      "epoch": 2.261111111111111,
      "grad_norm": 0.29881519079208374,
      "learning_rate": 2.7388888888888892e-05,
      "loss": 0.0049,
      "step": 40700
    },
    {
      "epoch": 2.2616666666666667,
      "grad_norm": 0.0190031286329031,
      "learning_rate": 2.7383333333333333e-05,
      "loss": 0.0036,
      "step": 40710
    },
    {
      "epoch": 2.2622222222222224,
      "grad_norm": 0.11906205862760544,
      "learning_rate": 2.737777777777778e-05,
      "loss": 0.0026,
      "step": 40720
    },
    {
      "epoch": 2.262777777777778,
      "grad_norm": 0.38720211386680603,
      "learning_rate": 2.737222222222222e-05,
      "loss": 0.0031,
      "step": 40730
    },
    {
      "epoch": 2.263333333333333,
      "grad_norm": 0.17959842085838318,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0037,
      "step": 40740
    },
    {
      "epoch": 2.263888888888889,
      "grad_norm": 0.2686063051223755,
      "learning_rate": 2.7361111111111114e-05,
      "loss": 0.0039,
      "step": 40750
    },
    {
      "epoch": 2.2644444444444445,
      "grad_norm": 0.281445175409317,
      "learning_rate": 2.7355555555555557e-05,
      "loss": 0.0039,
      "step": 40760
    },
    {
      "epoch": 2.265,
      "grad_norm": 0.14948973059654236,
      "learning_rate": 2.7350000000000004e-05,
      "loss": 0.0033,
      "step": 40770
    },
    {
      "epoch": 2.2655555555555553,
      "grad_norm": 0.11974047869443893,
      "learning_rate": 2.7344444444444444e-05,
      "loss": 0.004,
      "step": 40780
    },
    {
      "epoch": 2.266111111111111,
      "grad_norm": 0.4287790358066559,
      "learning_rate": 2.733888888888889e-05,
      "loss": 0.004,
      "step": 40790
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.03126515820622444,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0025,
      "step": 40800
    },
    {
      "epoch": 2.2672222222222222,
      "grad_norm": 0.40156927704811096,
      "learning_rate": 2.732777777777778e-05,
      "loss": 0.0036,
      "step": 40810
    },
    {
      "epoch": 2.267777777777778,
      "grad_norm": 0.15050838887691498,
      "learning_rate": 2.7322222222222222e-05,
      "loss": 0.003,
      "step": 40820
    },
    {
      "epoch": 2.2683333333333335,
      "grad_norm": 0.0923641175031662,
      "learning_rate": 2.731666666666667e-05,
      "loss": 0.0045,
      "step": 40830
    },
    {
      "epoch": 2.2688888888888887,
      "grad_norm": 0.36663421988487244,
      "learning_rate": 2.7311111111111116e-05,
      "loss": 0.0043,
      "step": 40840
    },
    {
      "epoch": 2.2694444444444444,
      "grad_norm": 0.031589701771736145,
      "learning_rate": 2.7305555555555556e-05,
      "loss": 0.0041,
      "step": 40850
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.11963177472352982,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0044,
      "step": 40860
    },
    {
      "epoch": 2.2705555555555557,
      "grad_norm": 0.03268682211637497,
      "learning_rate": 2.7294444444444443e-05,
      "loss": 0.0032,
      "step": 40870
    },
    {
      "epoch": 2.2711111111111113,
      "grad_norm": 0.09086520969867706,
      "learning_rate": 2.728888888888889e-05,
      "loss": 0.0042,
      "step": 40880
    },
    {
      "epoch": 2.2716666666666665,
      "grad_norm": 0.2837659418582916,
      "learning_rate": 2.7283333333333334e-05,
      "loss": 0.0034,
      "step": 40890
    },
    {
      "epoch": 2.272222222222222,
      "grad_norm": 0.506658673286438,
      "learning_rate": 2.727777777777778e-05,
      "loss": 0.0038,
      "step": 40900
    },
    {
      "epoch": 2.272777777777778,
      "grad_norm": 0.238472118973732,
      "learning_rate": 2.727222222222222e-05,
      "loss": 0.0036,
      "step": 40910
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.3285970687866211,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0033,
      "step": 40920
    },
    {
      "epoch": 2.273888888888889,
      "grad_norm": 0.05593622475862503,
      "learning_rate": 2.7261111111111115e-05,
      "loss": 0.0053,
      "step": 40930
    },
    {
      "epoch": 2.2744444444444443,
      "grad_norm": 0.02518259547650814,
      "learning_rate": 2.7255555555555555e-05,
      "loss": 0.0035,
      "step": 40940
    },
    {
      "epoch": 2.275,
      "grad_norm": 0.14883163571357727,
      "learning_rate": 2.725e-05,
      "loss": 0.004,
      "step": 40950
    },
    {
      "epoch": 2.2755555555555556,
      "grad_norm": 0.060629043728113174,
      "learning_rate": 2.7244444444444445e-05,
      "loss": 0.0046,
      "step": 40960
    },
    {
      "epoch": 2.276111111111111,
      "grad_norm": 0.38840314745903015,
      "learning_rate": 2.7238888888888892e-05,
      "loss": 0.0052,
      "step": 40970
    },
    {
      "epoch": 2.276666666666667,
      "grad_norm": 0.10204574465751648,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0036,
      "step": 40980
    },
    {
      "epoch": 2.277222222222222,
      "grad_norm": 0.17991535365581512,
      "learning_rate": 2.722777777777778e-05,
      "loss": 0.0025,
      "step": 40990
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 0.21333248913288116,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 0.0029,
      "step": 41000
    },
    {
      "epoch": 2.2783333333333333,
      "grad_norm": 0.06426887214183807,
      "learning_rate": 2.7216666666666666e-05,
      "loss": 0.0024,
      "step": 41010
    },
    {
      "epoch": 2.278888888888889,
      "grad_norm": 0.12180938571691513,
      "learning_rate": 2.7211111111111113e-05,
      "loss": 0.002,
      "step": 41020
    },
    {
      "epoch": 2.2794444444444446,
      "grad_norm": 0.29786592721939087,
      "learning_rate": 2.7205555555555557e-05,
      "loss": 0.0024,
      "step": 41030
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.3273983299732208,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0047,
      "step": 41040
    },
    {
      "epoch": 2.2805555555555554,
      "grad_norm": 0.2977246940135956,
      "learning_rate": 2.7194444444444444e-05,
      "loss": 0.0021,
      "step": 41050
    },
    {
      "epoch": 2.281111111111111,
      "grad_norm": 0.14938503503799438,
      "learning_rate": 2.718888888888889e-05,
      "loss": 0.0035,
      "step": 41060
    },
    {
      "epoch": 2.2816666666666667,
      "grad_norm": 0.29755550622940063,
      "learning_rate": 2.7183333333333335e-05,
      "loss": 0.0031,
      "step": 41070
    },
    {
      "epoch": 2.2822222222222224,
      "grad_norm": 0.08986853808164597,
      "learning_rate": 2.717777777777778e-05,
      "loss": 0.0038,
      "step": 41080
    },
    {
      "epoch": 2.2827777777777776,
      "grad_norm": 0.26850101351737976,
      "learning_rate": 2.717222222222222e-05,
      "loss": 0.0034,
      "step": 41090
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.1495954841375351,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0048,
      "step": 41100
    },
    {
      "epoch": 2.283888888888889,
      "grad_norm": 0.05650635436177254,
      "learning_rate": 2.7161111111111116e-05,
      "loss": 0.0033,
      "step": 41110
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 0.11891235411167145,
      "learning_rate": 2.7155555555555556e-05,
      "loss": 0.003,
      "step": 41120
    },
    {
      "epoch": 2.285,
      "grad_norm": 0.00942589994519949,
      "learning_rate": 2.7150000000000003e-05,
      "loss": 0.0039,
      "step": 41130
    },
    {
      "epoch": 2.285555555555556,
      "grad_norm": 0.1493614763021469,
      "learning_rate": 2.7144444444444446e-05,
      "loss": 0.0031,
      "step": 41140
    },
    {
      "epoch": 2.286111111111111,
      "grad_norm": 0.03047284670174122,
      "learning_rate": 2.7138888888888893e-05,
      "loss": 0.0025,
      "step": 41150
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.23832467198371887,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0025,
      "step": 41160
    },
    {
      "epoch": 2.2872222222222223,
      "grad_norm": 0.2383139282464981,
      "learning_rate": 2.712777777777778e-05,
      "loss": 0.0044,
      "step": 41170
    },
    {
      "epoch": 2.287777777777778,
      "grad_norm": 0.44680970907211304,
      "learning_rate": 2.712222222222222e-05,
      "loss": 0.0042,
      "step": 41180
    },
    {
      "epoch": 2.288333333333333,
      "grad_norm": 0.6000242233276367,
      "learning_rate": 2.7116666666666667e-05,
      "loss": 0.0025,
      "step": 41190
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 0.43343135714530945,
      "learning_rate": 2.7111111111111114e-05,
      "loss": 0.0039,
      "step": 41200
    },
    {
      "epoch": 2.2894444444444444,
      "grad_norm": 0.09240782260894775,
      "learning_rate": 2.7105555555555558e-05,
      "loss": 0.003,
      "step": 41210
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.2830926775932312,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0031,
      "step": 41220
    },
    {
      "epoch": 2.2905555555555557,
      "grad_norm": 0.2085663378238678,
      "learning_rate": 2.7094444444444445e-05,
      "loss": 0.004,
      "step": 41230
    },
    {
      "epoch": 2.2911111111111113,
      "grad_norm": 0.17086383700370789,
      "learning_rate": 2.7088888888888892e-05,
      "loss": 0.0039,
      "step": 41240
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 0.35024961829185486,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.0039,
      "step": 41250
    },
    {
      "epoch": 2.292222222222222,
      "grad_norm": 0.4493178427219391,
      "learning_rate": 2.707777777777778e-05,
      "loss": 0.0036,
      "step": 41260
    },
    {
      "epoch": 2.292777777777778,
      "grad_norm": 0.09005358070135117,
      "learning_rate": 2.7072222222222223e-05,
      "loss": 0.0025,
      "step": 41270
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.1791190505027771,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0051,
      "step": 41280
    },
    {
      "epoch": 2.2938888888888886,
      "grad_norm": 0.12020090222358704,
      "learning_rate": 2.7061111111111116e-05,
      "loss": 0.0043,
      "step": 41290
    },
    {
      "epoch": 2.2944444444444443,
      "grad_norm": 0.03547971323132515,
      "learning_rate": 2.7055555555555557e-05,
      "loss": 0.0031,
      "step": 41300
    },
    {
      "epoch": 2.295,
      "grad_norm": 0.06180579960346222,
      "learning_rate": 2.7050000000000004e-05,
      "loss": 0.0029,
      "step": 41310
    },
    {
      "epoch": 2.2955555555555556,
      "grad_norm": 0.3520437777042389,
      "learning_rate": 2.7044444444444444e-05,
      "loss": 0.0036,
      "step": 41320
    },
    {
      "epoch": 2.296111111111111,
      "grad_norm": 0.12026382982730865,
      "learning_rate": 2.703888888888889e-05,
      "loss": 0.0038,
      "step": 41330
    },
    {
      "epoch": 2.296666666666667,
      "grad_norm": 0.11931537836790085,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.0043,
      "step": 41340
    },
    {
      "epoch": 2.297222222222222,
      "grad_norm": 0.031408995389938354,
      "learning_rate": 2.702777777777778e-05,
      "loss": 0.0034,
      "step": 41350
    },
    {
      "epoch": 2.2977777777777777,
      "grad_norm": 0.32888922095298767,
      "learning_rate": 2.702222222222222e-05,
      "loss": 0.0038,
      "step": 41360
    },
    {
      "epoch": 2.2983333333333333,
      "grad_norm": 0.03275550156831741,
      "learning_rate": 2.701666666666667e-05,
      "loss": 0.0036,
      "step": 41370
    },
    {
      "epoch": 2.298888888888889,
      "grad_norm": 0.11894619464874268,
      "learning_rate": 2.7011111111111115e-05,
      "loss": 0.004,
      "step": 41380
    },
    {
      "epoch": 2.2994444444444446,
      "grad_norm": 0.4178811013698578,
      "learning_rate": 2.7005555555555555e-05,
      "loss": 0.0035,
      "step": 41390
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.2608174979686737,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0038,
      "step": 41400
    },
    {
      "epoch": 2.3005555555555555,
      "grad_norm": 0.2992768883705139,
      "learning_rate": 2.6994444444444446e-05,
      "loss": 0.0039,
      "step": 41410
    },
    {
      "epoch": 2.301111111111111,
      "grad_norm": 0.04287751391530037,
      "learning_rate": 2.6988888888888893e-05,
      "loss": 0.0027,
      "step": 41420
    },
    {
      "epoch": 2.3016666666666667,
      "grad_norm": 0.030260473489761353,
      "learning_rate": 2.6983333333333333e-05,
      "loss": 0.0047,
      "step": 41430
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 0.20935316383838654,
      "learning_rate": 2.697777777777778e-05,
      "loss": 0.0034,
      "step": 41440
    },
    {
      "epoch": 2.3027777777777776,
      "grad_norm": 0.12052424252033234,
      "learning_rate": 2.697222222222222e-05,
      "loss": 0.0039,
      "step": 41450
    },
    {
      "epoch": 2.3033333333333332,
      "grad_norm": 0.2984161078929901,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.0042,
      "step": 41460
    },
    {
      "epoch": 2.303888888888889,
      "grad_norm": 0.04847632348537445,
      "learning_rate": 2.6961111111111114e-05,
      "loss": 0.0034,
      "step": 41470
    },
    {
      "epoch": 2.3044444444444445,
      "grad_norm": 0.09306548535823822,
      "learning_rate": 2.6955555555555558e-05,
      "loss": 0.0029,
      "step": 41480
    },
    {
      "epoch": 2.305,
      "grad_norm": 0.2991694211959839,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 0.0037,
      "step": 41490
    },
    {
      "epoch": 2.3055555555555554,
      "grad_norm": 0.12491761893033981,
      "learning_rate": 2.6944444444444445e-05,
      "loss": 0.0027,
      "step": 41500
    },
    {
      "epoch": 2.306111111111111,
      "grad_norm": 0.60578852891922,
      "learning_rate": 2.693888888888889e-05,
      "loss": 0.0026,
      "step": 41510
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.5066147446632385,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0028,
      "step": 41520
    },
    {
      "epoch": 2.3072222222222223,
      "grad_norm": 0.2086864858865738,
      "learning_rate": 2.692777777777778e-05,
      "loss": 0.0027,
      "step": 41530
    },
    {
      "epoch": 2.307777777777778,
      "grad_norm": 0.09099538624286652,
      "learning_rate": 2.6922222222222222e-05,
      "loss": 0.0035,
      "step": 41540
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 0.08976898342370987,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.003,
      "step": 41550
    },
    {
      "epoch": 2.3088888888888888,
      "grad_norm": 0.4034997820854187,
      "learning_rate": 2.6911111111111116e-05,
      "loss": 0.0055,
      "step": 41560
    },
    {
      "epoch": 2.3094444444444444,
      "grad_norm": 0.6039623022079468,
      "learning_rate": 2.6905555555555556e-05,
      "loss": 0.0037,
      "step": 41570
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.08990585058927536,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.0031,
      "step": 41580
    },
    {
      "epoch": 2.3105555555555557,
      "grad_norm": 0.48864415287971497,
      "learning_rate": 2.6894444444444444e-05,
      "loss": 0.0026,
      "step": 41590
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.3568826913833618,
      "learning_rate": 2.688888888888889e-05,
      "loss": 0.0033,
      "step": 41600
    },
    {
      "epoch": 2.3116666666666665,
      "grad_norm": 0.012256325222551823,
      "learning_rate": 2.6883333333333334e-05,
      "loss": 0.005,
      "step": 41610
    },
    {
      "epoch": 2.312222222222222,
      "grad_norm": 0.29864776134490967,
      "learning_rate": 2.687777777777778e-05,
      "loss": 0.0028,
      "step": 41620
    },
    {
      "epoch": 2.312777777777778,
      "grad_norm": 0.26871758699417114,
      "learning_rate": 2.687222222222222e-05,
      "loss": 0.004,
      "step": 41630
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.8344847559928894,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0024,
      "step": 41640
    },
    {
      "epoch": 2.313888888888889,
      "grad_norm": 0.5281897783279419,
      "learning_rate": 2.6861111111111115e-05,
      "loss": 0.0056,
      "step": 41650
    },
    {
      "epoch": 2.3144444444444443,
      "grad_norm": 0.11980251967906952,
      "learning_rate": 2.6855555555555555e-05,
      "loss": 0.0029,
      "step": 41660
    },
    {
      "epoch": 2.315,
      "grad_norm": 0.19962185621261597,
      "learning_rate": 2.6850000000000002e-05,
      "loss": 0.0038,
      "step": 41670
    },
    {
      "epoch": 2.3155555555555556,
      "grad_norm": 0.18005509674549103,
      "learning_rate": 2.6844444444444446e-05,
      "loss": 0.0028,
      "step": 41680
    },
    {
      "epoch": 2.3161111111111112,
      "grad_norm": 0.2092968374490738,
      "learning_rate": 2.6838888888888893e-05,
      "loss": 0.0032,
      "step": 41690
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.20862938463687897,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0037,
      "step": 41700
    },
    {
      "epoch": 2.317222222222222,
      "grad_norm": 0.06072980538010597,
      "learning_rate": 2.682777777777778e-05,
      "loss": 0.0038,
      "step": 41710
    },
    {
      "epoch": 2.3177777777777777,
      "grad_norm": 0.29967209696769714,
      "learning_rate": 2.682222222222222e-05,
      "loss": 0.0027,
      "step": 41720
    },
    {
      "epoch": 2.3183333333333334,
      "grad_norm": 0.298415869474411,
      "learning_rate": 2.6816666666666667e-05,
      "loss": 0.004,
      "step": 41730
    },
    {
      "epoch": 2.318888888888889,
      "grad_norm": 0.3280758857727051,
      "learning_rate": 2.6811111111111114e-05,
      "loss": 0.0032,
      "step": 41740
    },
    {
      "epoch": 2.3194444444444446,
      "grad_norm": 0.23850585520267487,
      "learning_rate": 2.6805555555555557e-05,
      "loss": 0.0038,
      "step": 41750
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.06025076285004616,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0046,
      "step": 41760
    },
    {
      "epoch": 2.3205555555555555,
      "grad_norm": 0.11912278085947037,
      "learning_rate": 2.6794444444444444e-05,
      "loss": 0.0041,
      "step": 41770
    },
    {
      "epoch": 2.321111111111111,
      "grad_norm": 0.3282420337200165,
      "learning_rate": 2.678888888888889e-05,
      "loss": 0.0047,
      "step": 41780
    },
    {
      "epoch": 2.3216666666666668,
      "grad_norm": 0.05966924503445625,
      "learning_rate": 2.678333333333333e-05,
      "loss": 0.0021,
      "step": 41790
    },
    {
      "epoch": 2.3222222222222224,
      "grad_norm": 0.031787481158971786,
      "learning_rate": 2.677777777777778e-05,
      "loss": 0.0044,
      "step": 41800
    },
    {
      "epoch": 2.3227777777777776,
      "grad_norm": 0.14949023723602295,
      "learning_rate": 2.6772222222222222e-05,
      "loss": 0.0054,
      "step": 41810
    },
    {
      "epoch": 2.3233333333333333,
      "grad_norm": 0.268757700920105,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.0033,
      "step": 41820
    },
    {
      "epoch": 2.323888888888889,
      "grad_norm": 0.10686194151639938,
      "learning_rate": 2.6761111111111116e-05,
      "loss": 0.0033,
      "step": 41830
    },
    {
      "epoch": 2.3244444444444445,
      "grad_norm": 0.03167197108268738,
      "learning_rate": 2.6755555555555556e-05,
      "loss": 0.0038,
      "step": 41840
    },
    {
      "epoch": 2.325,
      "grad_norm": 0.11987839639186859,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.0043,
      "step": 41850
    },
    {
      "epoch": 2.3255555555555554,
      "grad_norm": 0.2827625274658203,
      "learning_rate": 2.6744444444444443e-05,
      "loss": 0.0046,
      "step": 41860
    },
    {
      "epoch": 2.326111111111111,
      "grad_norm": 0.1198289692401886,
      "learning_rate": 2.673888888888889e-05,
      "loss": 0.0044,
      "step": 41870
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.2381765991449356,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0042,
      "step": 41880
    },
    {
      "epoch": 2.3272222222222223,
      "grad_norm": 0.447670578956604,
      "learning_rate": 2.672777777777778e-05,
      "loss": 0.0037,
      "step": 41890
    },
    {
      "epoch": 2.327777777777778,
      "grad_norm": 0.12392386049032211,
      "learning_rate": 2.6722222222222228e-05,
      "loss": 0.0031,
      "step": 41900
    },
    {
      "epoch": 2.328333333333333,
      "grad_norm": 0.28236255049705505,
      "learning_rate": 2.6716666666666668e-05,
      "loss": 0.004,
      "step": 41910
    },
    {
      "epoch": 2.328888888888889,
      "grad_norm": 0.2507377564907074,
      "learning_rate": 2.6711111111111115e-05,
      "loss": 0.0033,
      "step": 41920
    },
    {
      "epoch": 2.3294444444444444,
      "grad_norm": 0.01721298135817051,
      "learning_rate": 2.6705555555555555e-05,
      "loss": 0.0045,
      "step": 41930
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.2984788119792938,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0034,
      "step": 41940
    },
    {
      "epoch": 2.3305555555555557,
      "grad_norm": 0.2689756751060486,
      "learning_rate": 2.6694444444444445e-05,
      "loss": 0.0036,
      "step": 41950
    },
    {
      "epoch": 2.3311111111111114,
      "grad_norm": 0.11958305537700653,
      "learning_rate": 2.6688888888888892e-05,
      "loss": 0.0041,
      "step": 41960
    },
    {
      "epoch": 2.3316666666666666,
      "grad_norm": 0.5954580903053284,
      "learning_rate": 2.6683333333333333e-05,
      "loss": 0.0035,
      "step": 41970
    },
    {
      "epoch": 2.332222222222222,
      "grad_norm": 0.41718024015426636,
      "learning_rate": 2.667777777777778e-05,
      "loss": 0.0046,
      "step": 41980
    },
    {
      "epoch": 2.332777777777778,
      "grad_norm": 0.1191936656832695,
      "learning_rate": 2.6672222222222226e-05,
      "loss": 0.0044,
      "step": 41990
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.22145633399486542,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0041,
      "step": 42000
    },
    {
      "epoch": 2.3338888888888887,
      "grad_norm": 0.01442709844559431,
      "learning_rate": 2.6661111111111114e-05,
      "loss": 0.0041,
      "step": 42010
    },
    {
      "epoch": 2.3344444444444443,
      "grad_norm": 0.6263675689697266,
      "learning_rate": 2.6655555555555557e-05,
      "loss": 0.004,
      "step": 42020
    },
    {
      "epoch": 2.335,
      "grad_norm": 0.3589814603328705,
      "learning_rate": 2.6650000000000004e-05,
      "loss": 0.0024,
      "step": 42030
    },
    {
      "epoch": 2.3355555555555556,
      "grad_norm": 0.3951537609100342,
      "learning_rate": 2.6644444444444444e-05,
      "loss": 0.0029,
      "step": 42040
    },
    {
      "epoch": 2.3361111111111112,
      "grad_norm": 0.3283836245536804,
      "learning_rate": 2.663888888888889e-05,
      "loss": 0.0027,
      "step": 42050
    },
    {
      "epoch": 2.336666666666667,
      "grad_norm": 0.17847608029842377,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.0037,
      "step": 42060
    },
    {
      "epoch": 2.337222222222222,
      "grad_norm": 0.20955786108970642,
      "learning_rate": 2.6627777777777778e-05,
      "loss": 0.0031,
      "step": 42070
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 0.4534708261489868,
      "learning_rate": 2.6622222222222225e-05,
      "loss": 0.0035,
      "step": 42080
    },
    {
      "epoch": 2.3383333333333334,
      "grad_norm": 0.5956928730010986,
      "learning_rate": 2.661666666666667e-05,
      "loss": 0.004,
      "step": 42090
    },
    {
      "epoch": 2.338888888888889,
      "grad_norm": 0.01934976316988468,
      "learning_rate": 2.6611111111111116e-05,
      "loss": 0.0041,
      "step": 42100
    },
    {
      "epoch": 2.339444444444444,
      "grad_norm": 0.5067647099494934,
      "learning_rate": 2.6605555555555556e-05,
      "loss": 0.003,
      "step": 42110
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.446075975894928,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0045,
      "step": 42120
    },
    {
      "epoch": 2.3405555555555555,
      "grad_norm": 0.09070894122123718,
      "learning_rate": 2.6594444444444443e-05,
      "loss": 0.0048,
      "step": 42130
    },
    {
      "epoch": 2.341111111111111,
      "grad_norm": 0.4201313555240631,
      "learning_rate": 2.658888888888889e-05,
      "loss": 0.0038,
      "step": 42140
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 0.2980583906173706,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.0038,
      "step": 42150
    },
    {
      "epoch": 2.3422222222222224,
      "grad_norm": 0.17876403033733368,
      "learning_rate": 2.657777777777778e-05,
      "loss": 0.0032,
      "step": 42160
    },
    {
      "epoch": 2.3427777777777776,
      "grad_norm": 0.46459683775901794,
      "learning_rate": 2.6572222222222227e-05,
      "loss": 0.0046,
      "step": 42170
    },
    {
      "epoch": 2.3433333333333333,
      "grad_norm": 0.09519437700510025,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0045,
      "step": 42180
    },
    {
      "epoch": 2.343888888888889,
      "grad_norm": 0.20884396135807037,
      "learning_rate": 2.6561111111111114e-05,
      "loss": 0.0042,
      "step": 42190
    },
    {
      "epoch": 2.3444444444444446,
      "grad_norm": 0.2092713862657547,
      "learning_rate": 2.6555555555555555e-05,
      "loss": 0.0021,
      "step": 42200
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 0.31581932306289673,
      "learning_rate": 2.655e-05,
      "loss": 0.0032,
      "step": 42210
    },
    {
      "epoch": 2.3455555555555554,
      "grad_norm": 0.23837266862392426,
      "learning_rate": 2.6544444444444445e-05,
      "loss": 0.0032,
      "step": 42220
    },
    {
      "epoch": 2.346111111111111,
      "grad_norm": 0.11996782571077347,
      "learning_rate": 2.6538888888888892e-05,
      "loss": 0.0033,
      "step": 42230
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.2686504125595093,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.003,
      "step": 42240
    },
    {
      "epoch": 2.3472222222222223,
      "grad_norm": 0.08920897543430328,
      "learning_rate": 2.652777777777778e-05,
      "loss": 0.003,
      "step": 42250
    },
    {
      "epoch": 2.347777777777778,
      "grad_norm": 0.6431847810745239,
      "learning_rate": 2.6522222222222226e-05,
      "loss": 0.0024,
      "step": 42260
    },
    {
      "epoch": 2.348333333333333,
      "grad_norm": 0.29828229546546936,
      "learning_rate": 2.6516666666666666e-05,
      "loss": 0.0038,
      "step": 42270
    },
    {
      "epoch": 2.348888888888889,
      "grad_norm": 0.07810664921998978,
      "learning_rate": 2.6511111111111113e-05,
      "loss": 0.0026,
      "step": 42280
    },
    {
      "epoch": 2.3494444444444444,
      "grad_norm": 0.17917169630527496,
      "learning_rate": 2.6505555555555557e-05,
      "loss": 0.003,
      "step": 42290
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.37185442447662354,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0039,
      "step": 42300
    },
    {
      "epoch": 2.3505555555555557,
      "grad_norm": 0.44639790058135986,
      "learning_rate": 2.6494444444444444e-05,
      "loss": 0.0023,
      "step": 42310
    },
    {
      "epoch": 2.351111111111111,
      "grad_norm": 0.14920084178447723,
      "learning_rate": 2.648888888888889e-05,
      "loss": 0.003,
      "step": 42320
    },
    {
      "epoch": 2.3516666666666666,
      "grad_norm": 0.29698067903518677,
      "learning_rate": 2.648333333333333e-05,
      "loss": 0.0047,
      "step": 42330
    },
    {
      "epoch": 2.352222222222222,
      "grad_norm": 0.5362589955329895,
      "learning_rate": 2.6477777777777778e-05,
      "loss": 0.0027,
      "step": 42340
    },
    {
      "epoch": 2.352777777777778,
      "grad_norm": 0.34580865502357483,
      "learning_rate": 2.6472222222222225e-05,
      "loss": 0.0042,
      "step": 42350
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.47944244742393494,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0019,
      "step": 42360
    },
    {
      "epoch": 2.3538888888888887,
      "grad_norm": 0.3276377320289612,
      "learning_rate": 2.6461111111111115e-05,
      "loss": 0.0041,
      "step": 42370
    },
    {
      "epoch": 2.3544444444444443,
      "grad_norm": 0.38822874426841736,
      "learning_rate": 2.6455555555555556e-05,
      "loss": 0.0041,
      "step": 42380
    },
    {
      "epoch": 2.355,
      "grad_norm": 0.5664092898368835,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 0.0024,
      "step": 42390
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.5364828705787659,
      "learning_rate": 2.6444444444444443e-05,
      "loss": 0.0044,
      "step": 42400
    },
    {
      "epoch": 2.3561111111111113,
      "grad_norm": 0.5103554725646973,
      "learning_rate": 2.643888888888889e-05,
      "loss": 0.0029,
      "step": 42410
    },
    {
      "epoch": 2.3566666666666665,
      "grad_norm": 0.30088794231414795,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.0049,
      "step": 42420
    },
    {
      "epoch": 2.357222222222222,
      "grad_norm": 0.14946582913398743,
      "learning_rate": 2.642777777777778e-05,
      "loss": 0.0023,
      "step": 42430
    },
    {
      "epoch": 2.3577777777777778,
      "grad_norm": 0.17862597107887268,
      "learning_rate": 2.6422222222222227e-05,
      "loss": 0.0042,
      "step": 42440
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 0.11971966922283173,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.0034,
      "step": 42450
    },
    {
      "epoch": 2.358888888888889,
      "grad_norm": 0.09166158735752106,
      "learning_rate": 2.6411111111111114e-05,
      "loss": 0.0022,
      "step": 42460
    },
    {
      "epoch": 2.3594444444444447,
      "grad_norm": 0.21219691634178162,
      "learning_rate": 2.6405555555555554e-05,
      "loss": 0.0034,
      "step": 42470
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.06531847268342972,
      "learning_rate": 2.64e-05,
      "loss": 0.0027,
      "step": 42480
    },
    {
      "epoch": 2.3605555555555555,
      "grad_norm": 0.008225969970226288,
      "learning_rate": 2.6394444444444445e-05,
      "loss": 0.0037,
      "step": 42490
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 0.1135951429605484,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 0.0043,
      "step": 42500
    },
    {
      "epoch": 2.361666666666667,
      "grad_norm": 0.20902922749519348,
      "learning_rate": 2.6383333333333332e-05,
      "loss": 0.003,
      "step": 42510
    },
    {
      "epoch": 2.362222222222222,
      "grad_norm": 0.23798881471157074,
      "learning_rate": 2.637777777777778e-05,
      "loss": 0.0023,
      "step": 42520
    },
    {
      "epoch": 2.3627777777777776,
      "grad_norm": 0.2678833305835724,
      "learning_rate": 2.6372222222222226e-05,
      "loss": 0.004,
      "step": 42530
    },
    {
      "epoch": 2.3633333333333333,
      "grad_norm": 0.12140225619077682,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.0027,
      "step": 42540
    },
    {
      "epoch": 2.363888888888889,
      "grad_norm": 0.448167085647583,
      "learning_rate": 2.6361111111111113e-05,
      "loss": 0.003,
      "step": 42550
    },
    {
      "epoch": 2.3644444444444446,
      "grad_norm": 0.3589501678943634,
      "learning_rate": 2.6355555555555557e-05,
      "loss": 0.0045,
      "step": 42560
    },
    {
      "epoch": 2.365,
      "grad_norm": 0.060616765171289444,
      "learning_rate": 2.6350000000000004e-05,
      "loss": 0.0023,
      "step": 42570
    },
    {
      "epoch": 2.3655555555555554,
      "grad_norm": 0.20849189162254333,
      "learning_rate": 2.6344444444444444e-05,
      "loss": 0.002,
      "step": 42580
    },
    {
      "epoch": 2.366111111111111,
      "grad_norm": 0.17881131172180176,
      "learning_rate": 2.633888888888889e-05,
      "loss": 0.0031,
      "step": 42590
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.12337257713079453,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0036,
      "step": 42600
    },
    {
      "epoch": 2.3672222222222223,
      "grad_norm": 0.1372978836297989,
      "learning_rate": 2.6327777777777778e-05,
      "loss": 0.0037,
      "step": 42610
    },
    {
      "epoch": 2.3677777777777775,
      "grad_norm": 0.1653422862291336,
      "learning_rate": 2.6322222222222225e-05,
      "loss": 0.0031,
      "step": 42620
    },
    {
      "epoch": 2.368333333333333,
      "grad_norm": 0.5478312969207764,
      "learning_rate": 2.6316666666666668e-05,
      "loss": 0.0033,
      "step": 42630
    },
    {
      "epoch": 2.368888888888889,
      "grad_norm": 0.09048646688461304,
      "learning_rate": 2.6311111111111115e-05,
      "loss": 0.0035,
      "step": 42640
    },
    {
      "epoch": 2.3694444444444445,
      "grad_norm": 0.3281736373901367,
      "learning_rate": 2.6305555555555555e-05,
      "loss": 0.0028,
      "step": 42650
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.17934109270572662,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0028,
      "step": 42660
    },
    {
      "epoch": 2.3705555555555557,
      "grad_norm": 0.06146564707159996,
      "learning_rate": 2.6294444444444442e-05,
      "loss": 0.0028,
      "step": 42670
    },
    {
      "epoch": 2.371111111111111,
      "grad_norm": 0.3575977683067322,
      "learning_rate": 2.628888888888889e-05,
      "loss": 0.0021,
      "step": 42680
    },
    {
      "epoch": 2.3716666666666666,
      "grad_norm": 0.09002690762281418,
      "learning_rate": 2.6283333333333333e-05,
      "loss": 0.0028,
      "step": 42690
    },
    {
      "epoch": 2.3722222222222222,
      "grad_norm": 0.1505732238292694,
      "learning_rate": 2.627777777777778e-05,
      "loss": 0.0036,
      "step": 42700
    },
    {
      "epoch": 2.372777777777778,
      "grad_norm": 0.6499742865562439,
      "learning_rate": 2.6272222222222227e-05,
      "loss": 0.0045,
      "step": 42710
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.21251659095287323,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0022,
      "step": 42720
    },
    {
      "epoch": 2.3738888888888887,
      "grad_norm": 0.30879467725753784,
      "learning_rate": 2.6261111111111114e-05,
      "loss": 0.0029,
      "step": 42730
    },
    {
      "epoch": 2.3744444444444444,
      "grad_norm": 0.24217277765274048,
      "learning_rate": 2.6255555555555554e-05,
      "loss": 0.0037,
      "step": 42740
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.2520033121109009,
      "learning_rate": 2.625e-05,
      "loss": 0.0029,
      "step": 42750
    },
    {
      "epoch": 2.3755555555555556,
      "grad_norm": 0.33562979102134705,
      "learning_rate": 2.6244444444444445e-05,
      "loss": 0.0036,
      "step": 42760
    },
    {
      "epoch": 2.3761111111111113,
      "grad_norm": 0.06002042442560196,
      "learning_rate": 2.623888888888889e-05,
      "loss": 0.0039,
      "step": 42770
    },
    {
      "epoch": 2.3766666666666665,
      "grad_norm": 0.1803458333015442,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.0044,
      "step": 42780
    },
    {
      "epoch": 2.377222222222222,
      "grad_norm": 0.031379420310258865,
      "learning_rate": 2.622777777777778e-05,
      "loss": 0.0029,
      "step": 42790
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 0.09251440316438675,
      "learning_rate": 2.6222222222222226e-05,
      "loss": 0.0023,
      "step": 42800
    },
    {
      "epoch": 2.3783333333333334,
      "grad_norm": 0.008161968551576138,
      "learning_rate": 2.6216666666666666e-05,
      "loss": 0.0035,
      "step": 42810
    },
    {
      "epoch": 2.378888888888889,
      "grad_norm": 0.29848748445510864,
      "learning_rate": 2.6211111111111113e-05,
      "loss": 0.0027,
      "step": 42820
    },
    {
      "epoch": 2.3794444444444443,
      "grad_norm": 0.4769074022769928,
      "learning_rate": 2.6205555555555556e-05,
      "loss": 0.0045,
      "step": 42830
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.008641612716019154,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0026,
      "step": 42840
    },
    {
      "epoch": 2.3805555555555555,
      "grad_norm": 0.20833434164524078,
      "learning_rate": 2.6194444444444443e-05,
      "loss": 0.003,
      "step": 42850
    },
    {
      "epoch": 2.381111111111111,
      "grad_norm": 0.03410051763057709,
      "learning_rate": 2.618888888888889e-05,
      "loss": 0.0029,
      "step": 42860
    },
    {
      "epoch": 2.381666666666667,
      "grad_norm": 0.009474417194724083,
      "learning_rate": 2.618333333333333e-05,
      "loss": 0.0041,
      "step": 42870
    },
    {
      "epoch": 2.3822222222222225,
      "grad_norm": 0.09264836460351944,
      "learning_rate": 2.6177777777777777e-05,
      "loss": 0.0025,
      "step": 42880
    },
    {
      "epoch": 2.3827777777777777,
      "grad_norm": 0.11934245377779007,
      "learning_rate": 2.6172222222222224e-05,
      "loss": 0.0039,
      "step": 42890
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.09200026094913483,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0035,
      "step": 42900
    },
    {
      "epoch": 2.383888888888889,
      "grad_norm": 0.08970962464809418,
      "learning_rate": 2.6161111111111115e-05,
      "loss": 0.003,
      "step": 42910
    },
    {
      "epoch": 2.3844444444444446,
      "grad_norm": 0.03632306680083275,
      "learning_rate": 2.6155555555555555e-05,
      "loss": 0.0043,
      "step": 42920
    },
    {
      "epoch": 2.385,
      "grad_norm": 0.12237308919429779,
      "learning_rate": 2.6150000000000002e-05,
      "loss": 0.0026,
      "step": 42930
    },
    {
      "epoch": 2.3855555555555554,
      "grad_norm": 0.3868298828601837,
      "learning_rate": 2.6144444444444442e-05,
      "loss": 0.0026,
      "step": 42940
    },
    {
      "epoch": 2.386111111111111,
      "grad_norm": 0.3128558397293091,
      "learning_rate": 2.613888888888889e-05,
      "loss": 0.0035,
      "step": 42950
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.16294801235198975,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0029,
      "step": 42960
    },
    {
      "epoch": 2.3872222222222224,
      "grad_norm": 0.46515050530433655,
      "learning_rate": 2.612777777777778e-05,
      "loss": 0.0029,
      "step": 42970
    },
    {
      "epoch": 2.387777777777778,
      "grad_norm": 0.358189195394516,
      "learning_rate": 2.6122222222222227e-05,
      "loss": 0.0047,
      "step": 42980
    },
    {
      "epoch": 2.388333333333333,
      "grad_norm": 0.1491536945104599,
      "learning_rate": 2.6116666666666667e-05,
      "loss": 0.0026,
      "step": 42990
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 0.6358550786972046,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 0.0045,
      "step": 43000
    },
    {
      "epoch": 2.3894444444444445,
      "grad_norm": 0.1506325751543045,
      "learning_rate": 2.6105555555555554e-05,
      "loss": 0.0039,
      "step": 43010
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.11937323957681656,
      "learning_rate": 2.61e-05,
      "loss": 0.004,
      "step": 43020
    },
    {
      "epoch": 2.3905555555555553,
      "grad_norm": 0.08959492295980453,
      "learning_rate": 2.6094444444444444e-05,
      "loss": 0.0022,
      "step": 43030
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 0.18819670379161835,
      "learning_rate": 2.608888888888889e-05,
      "loss": 0.0034,
      "step": 43040
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 0.06122715398669243,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.0034,
      "step": 43050
    },
    {
      "epoch": 2.3922222222222222,
      "grad_norm": 0.2527174949645996,
      "learning_rate": 2.607777777777778e-05,
      "loss": 0.0036,
      "step": 43060
    },
    {
      "epoch": 2.392777777777778,
      "grad_norm": 0.2404138445854187,
      "learning_rate": 2.6072222222222225e-05,
      "loss": 0.0022,
      "step": 43070
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.1027655377984047,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0042,
      "step": 43080
    },
    {
      "epoch": 2.3938888888888887,
      "grad_norm": 0.3193625807762146,
      "learning_rate": 2.6061111111111113e-05,
      "loss": 0.003,
      "step": 43090
    },
    {
      "epoch": 2.3944444444444444,
      "grad_norm": 0.24225713312625885,
      "learning_rate": 2.6055555555555556e-05,
      "loss": 0.0035,
      "step": 43100
    },
    {
      "epoch": 2.395,
      "grad_norm": 0.2979663610458374,
      "learning_rate": 2.6050000000000003e-05,
      "loss": 0.0044,
      "step": 43110
    },
    {
      "epoch": 2.3955555555555557,
      "grad_norm": 0.08961524814367294,
      "learning_rate": 2.6044444444444443e-05,
      "loss": 0.0041,
      "step": 43120
    },
    {
      "epoch": 2.3961111111111113,
      "grad_norm": 0.35888370871543884,
      "learning_rate": 2.603888888888889e-05,
      "loss": 0.0036,
      "step": 43130
    },
    {
      "epoch": 2.3966666666666665,
      "grad_norm": 0.07106781005859375,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.0037,
      "step": 43140
    },
    {
      "epoch": 2.397222222222222,
      "grad_norm": 0.09039733558893204,
      "learning_rate": 2.6027777777777777e-05,
      "loss": 0.0038,
      "step": 43150
    },
    {
      "epoch": 2.397777777777778,
      "grad_norm": 0.5243317484855652,
      "learning_rate": 2.6022222222222224e-05,
      "loss": 0.0052,
      "step": 43160
    },
    {
      "epoch": 2.3983333333333334,
      "grad_norm": 0.47657135128974915,
      "learning_rate": 2.6016666666666668e-05,
      "loss": 0.0036,
      "step": 43170
    },
    {
      "epoch": 2.398888888888889,
      "grad_norm": 0.23941034078598022,
      "learning_rate": 2.6011111111111115e-05,
      "loss": 0.0038,
      "step": 43180
    },
    {
      "epoch": 2.3994444444444443,
      "grad_norm": 0.5359547734260559,
      "learning_rate": 2.6005555555555555e-05,
      "loss": 0.0039,
      "step": 43190
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.5444196462631226,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0032,
      "step": 43200
    },
    {
      "epoch": 2.4005555555555556,
      "grad_norm": 0.25286921858787537,
      "learning_rate": 2.5994444444444442e-05,
      "loss": 0.0033,
      "step": 43210
    },
    {
      "epoch": 2.401111111111111,
      "grad_norm": 0.298188179731369,
      "learning_rate": 2.598888888888889e-05,
      "loss": 0.0043,
      "step": 43220
    },
    {
      "epoch": 2.401666666666667,
      "grad_norm": 0.016271762549877167,
      "learning_rate": 2.5983333333333336e-05,
      "loss": 0.0025,
      "step": 43230
    },
    {
      "epoch": 2.402222222222222,
      "grad_norm": 0.060522183775901794,
      "learning_rate": 2.597777777777778e-05,
      "loss": 0.0028,
      "step": 43240
    },
    {
      "epoch": 2.4027777777777777,
      "grad_norm": 0.2199748009443283,
      "learning_rate": 2.5972222222222226e-05,
      "loss": 0.0019,
      "step": 43250
    },
    {
      "epoch": 2.4033333333333333,
      "grad_norm": 0.03138506039977074,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.0036,
      "step": 43260
    },
    {
      "epoch": 2.403888888888889,
      "grad_norm": 0.35567232966423035,
      "learning_rate": 2.5961111111111113e-05,
      "loss": 0.0042,
      "step": 43270
    },
    {
      "epoch": 2.4044444444444446,
      "grad_norm": 0.42027774453163147,
      "learning_rate": 2.5955555555555554e-05,
      "loss": 0.0031,
      "step": 43280
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 0.031681839376688004,
      "learning_rate": 2.595e-05,
      "loss": 0.0036,
      "step": 43290
    },
    {
      "epoch": 2.4055555555555554,
      "grad_norm": 0.03148611634969711,
      "learning_rate": 2.5944444444444444e-05,
      "loss": 0.0035,
      "step": 43300
    },
    {
      "epoch": 2.406111111111111,
      "grad_norm": 0.1756475567817688,
      "learning_rate": 2.593888888888889e-05,
      "loss": 0.004,
      "step": 43310
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.3015601336956024,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0035,
      "step": 43320
    },
    {
      "epoch": 2.4072222222222224,
      "grad_norm": 0.03138790279626846,
      "learning_rate": 2.5927777777777778e-05,
      "loss": 0.0039,
      "step": 43330
    },
    {
      "epoch": 2.4077777777777776,
      "grad_norm": 0.13964810967445374,
      "learning_rate": 2.5922222222222225e-05,
      "loss": 0.0027,
      "step": 43340
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 0.1487680971622467,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.0037,
      "step": 43350
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 0.46807125210762024,
      "learning_rate": 2.5911111111111112e-05,
      "loss": 0.0039,
      "step": 43360
    },
    {
      "epoch": 2.4094444444444445,
      "grad_norm": 0.1194365844130516,
      "learning_rate": 2.5905555555555556e-05,
      "loss": 0.0029,
      "step": 43370
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.3295726776123047,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0034,
      "step": 43380
    },
    {
      "epoch": 2.410555555555556,
      "grad_norm": 0.23851101100444794,
      "learning_rate": 2.5894444444444443e-05,
      "loss": 0.0038,
      "step": 43390
    },
    {
      "epoch": 2.411111111111111,
      "grad_norm": 0.6853981614112854,
      "learning_rate": 2.588888888888889e-05,
      "loss": 0.0031,
      "step": 43400
    },
    {
      "epoch": 2.4116666666666666,
      "grad_norm": 0.35055387020111084,
      "learning_rate": 2.5883333333333337e-05,
      "loss": 0.0027,
      "step": 43410
    },
    {
      "epoch": 2.4122222222222223,
      "grad_norm": 0.14983294904232025,
      "learning_rate": 2.5877777777777777e-05,
      "loss": 0.0024,
      "step": 43420
    },
    {
      "epoch": 2.412777777777778,
      "grad_norm": 0.3577176034450531,
      "learning_rate": 2.5872222222222224e-05,
      "loss": 0.0026,
      "step": 43430
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.16927145421504974,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0035,
      "step": 43440
    },
    {
      "epoch": 2.4138888888888888,
      "grad_norm": 0.3405851423740387,
      "learning_rate": 2.5861111111111114e-05,
      "loss": 0.0031,
      "step": 43450
    },
    {
      "epoch": 2.4144444444444444,
      "grad_norm": 0.3271740674972534,
      "learning_rate": 2.5855555555555555e-05,
      "loss": 0.0031,
      "step": 43460
    },
    {
      "epoch": 2.415,
      "grad_norm": 0.14912022650241852,
      "learning_rate": 2.585e-05,
      "loss": 0.0032,
      "step": 43470
    },
    {
      "epoch": 2.4155555555555557,
      "grad_norm": 0.2925988435745239,
      "learning_rate": 2.5844444444444442e-05,
      "loss": 0.0046,
      "step": 43480
    },
    {
      "epoch": 2.4161111111111113,
      "grad_norm": 0.2676232159137726,
      "learning_rate": 2.583888888888889e-05,
      "loss": 0.0033,
      "step": 43490
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.20862677693367004,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.0032,
      "step": 43500
    },
    {
      "epoch": 2.417222222222222,
      "grad_norm": 0.20833425223827362,
      "learning_rate": 2.582777777777778e-05,
      "loss": 0.003,
      "step": 43510
    },
    {
      "epoch": 2.417777777777778,
      "grad_norm": 0.06939051300287247,
      "learning_rate": 2.5822222222222226e-05,
      "loss": 0.004,
      "step": 43520
    },
    {
      "epoch": 2.4183333333333334,
      "grad_norm": 0.2403198480606079,
      "learning_rate": 2.5816666666666666e-05,
      "loss": 0.0038,
      "step": 43530
    },
    {
      "epoch": 2.4188888888888886,
      "grad_norm": 0.17898131906986237,
      "learning_rate": 2.5811111111111113e-05,
      "loss": 0.0043,
      "step": 43540
    },
    {
      "epoch": 2.4194444444444443,
      "grad_norm": 0.17156238853931427,
      "learning_rate": 2.5805555555555553e-05,
      "loss": 0.0032,
      "step": 43550
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.007190852891653776,
      "learning_rate": 2.58e-05,
      "loss": 0.004,
      "step": 43560
    },
    {
      "epoch": 2.4205555555555556,
      "grad_norm": 0.1495535969734192,
      "learning_rate": 2.5794444444444444e-05,
      "loss": 0.0028,
      "step": 43570
    },
    {
      "epoch": 2.421111111111111,
      "grad_norm": 0.6782942414283752,
      "learning_rate": 2.578888888888889e-05,
      "loss": 0.0039,
      "step": 43580
    },
    {
      "epoch": 2.421666666666667,
      "grad_norm": 0.5815380811691284,
      "learning_rate": 2.5783333333333338e-05,
      "loss": 0.0039,
      "step": 43590
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 0.387248694896698,
      "learning_rate": 2.5777777777777778e-05,
      "loss": 0.0034,
      "step": 43600
    },
    {
      "epoch": 2.4227777777777777,
      "grad_norm": 0.20840515196323395,
      "learning_rate": 2.5772222222222225e-05,
      "loss": 0.0027,
      "step": 43610
    },
    {
      "epoch": 2.4233333333333333,
      "grad_norm": 0.060479097068309784,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.0027,
      "step": 43620
    },
    {
      "epoch": 2.423888888888889,
      "grad_norm": 0.1189722940325737,
      "learning_rate": 2.5761111111111112e-05,
      "loss": 0.0042,
      "step": 43630
    },
    {
      "epoch": 2.4244444444444446,
      "grad_norm": 0.06155188009142876,
      "learning_rate": 2.5755555555555556e-05,
      "loss": 0.0034,
      "step": 43640
    },
    {
      "epoch": 2.425,
      "grad_norm": 0.0538354329764843,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.0038,
      "step": 43650
    },
    {
      "epoch": 2.4255555555555555,
      "grad_norm": 0.14961467683315277,
      "learning_rate": 2.5744444444444443e-05,
      "loss": 0.0042,
      "step": 43660
    },
    {
      "epoch": 2.426111111111111,
      "grad_norm": 0.20840205252170563,
      "learning_rate": 2.573888888888889e-05,
      "loss": 0.0035,
      "step": 43670
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.6763105392456055,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0051,
      "step": 43680
    },
    {
      "epoch": 2.4272222222222224,
      "grad_norm": 0.0608731284737587,
      "learning_rate": 2.572777777777778e-05,
      "loss": 0.0036,
      "step": 43690
    },
    {
      "epoch": 2.4277777777777776,
      "grad_norm": 0.31203654408454895,
      "learning_rate": 2.5722222222222224e-05,
      "loss": 0.0027,
      "step": 43700
    },
    {
      "epoch": 2.4283333333333332,
      "grad_norm": 0.031682487577199936,
      "learning_rate": 2.5716666666666667e-05,
      "loss": 0.0031,
      "step": 43710
    },
    {
      "epoch": 2.428888888888889,
      "grad_norm": 0.21765801310539246,
      "learning_rate": 2.5711111111111114e-05,
      "loss": 0.0037,
      "step": 43720
    },
    {
      "epoch": 2.4294444444444445,
      "grad_norm": 0.13522715866565704,
      "learning_rate": 2.5705555555555554e-05,
      "loss": 0.0031,
      "step": 43730
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.32740893959999084,
      "learning_rate": 2.57e-05,
      "loss": 0.0026,
      "step": 43740
    },
    {
      "epoch": 2.4305555555555554,
      "grad_norm": 0.0901808813214302,
      "learning_rate": 2.5694444444444445e-05,
      "loss": 0.0037,
      "step": 43750
    },
    {
      "epoch": 2.431111111111111,
      "grad_norm": 0.0939616933465004,
      "learning_rate": 2.5688888888888892e-05,
      "loss": 0.0025,
      "step": 43760
    },
    {
      "epoch": 2.4316666666666666,
      "grad_norm": 0.030691297724843025,
      "learning_rate": 2.5683333333333335e-05,
      "loss": 0.0027,
      "step": 43770
    },
    {
      "epoch": 2.4322222222222223,
      "grad_norm": 0.20829182863235474,
      "learning_rate": 2.567777777777778e-05,
      "loss": 0.0025,
      "step": 43780
    },
    {
      "epoch": 2.432777777777778,
      "grad_norm": 0.37974634766578674,
      "learning_rate": 2.5672222222222226e-05,
      "loss": 0.0031,
      "step": 43790
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.03529532998800278,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0017,
      "step": 43800
    },
    {
      "epoch": 2.4338888888888888,
      "grad_norm": 0.20042818784713745,
      "learning_rate": 2.5661111111111113e-05,
      "loss": 0.0027,
      "step": 43810
    },
    {
      "epoch": 2.4344444444444444,
      "grad_norm": 0.2982998788356781,
      "learning_rate": 2.5655555555555557e-05,
      "loss": 0.0031,
      "step": 43820
    },
    {
      "epoch": 2.435,
      "grad_norm": 0.5629850029945374,
      "learning_rate": 2.5650000000000003e-05,
      "loss": 0.0037,
      "step": 43830
    },
    {
      "epoch": 2.4355555555555557,
      "grad_norm": 0.15005846321582794,
      "learning_rate": 2.5644444444444444e-05,
      "loss": 0.0028,
      "step": 43840
    },
    {
      "epoch": 2.436111111111111,
      "grad_norm": 0.046408381313085556,
      "learning_rate": 2.563888888888889e-05,
      "loss": 0.0041,
      "step": 43850
    },
    {
      "epoch": 2.4366666666666665,
      "grad_norm": 0.11971638351678848,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0033,
      "step": 43860
    },
    {
      "epoch": 2.437222222222222,
      "grad_norm": 0.26698312163352966,
      "learning_rate": 2.5627777777777778e-05,
      "loss": 0.0035,
      "step": 43870
    },
    {
      "epoch": 2.437777777777778,
      "grad_norm": 0.5362551808357239,
      "learning_rate": 2.5622222222222225e-05,
      "loss": 0.0039,
      "step": 43880
    },
    {
      "epoch": 2.4383333333333335,
      "grad_norm": 0.09367898851633072,
      "learning_rate": 2.5616666666666668e-05,
      "loss": 0.0043,
      "step": 43890
    },
    {
      "epoch": 2.438888888888889,
      "grad_norm": 0.15773801505565643,
      "learning_rate": 2.5611111111111115e-05,
      "loss": 0.0037,
      "step": 43900
    },
    {
      "epoch": 2.4394444444444443,
      "grad_norm": 0.0907961055636406,
      "learning_rate": 2.5605555555555555e-05,
      "loss": 0.0042,
      "step": 43910
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.2982896566390991,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0029,
      "step": 43920
    },
    {
      "epoch": 2.4405555555555556,
      "grad_norm": 0.2223668247461319,
      "learning_rate": 2.5594444444444442e-05,
      "loss": 0.0036,
      "step": 43930
    },
    {
      "epoch": 2.4411111111111112,
      "grad_norm": 0.3985030949115753,
      "learning_rate": 2.558888888888889e-05,
      "loss": 0.0029,
      "step": 43940
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 0.14945003390312195,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.0043,
      "step": 43950
    },
    {
      "epoch": 2.442222222222222,
      "grad_norm": 0.41377463936805725,
      "learning_rate": 2.557777777777778e-05,
      "loss": 0.0038,
      "step": 43960
    },
    {
      "epoch": 2.4427777777777777,
      "grad_norm": 0.060225099325180054,
      "learning_rate": 2.5572222222222227e-05,
      "loss": 0.0038,
      "step": 43970
    },
    {
      "epoch": 2.4433333333333334,
      "grad_norm": 0.2980325520038605,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0029,
      "step": 43980
    },
    {
      "epoch": 2.443888888888889,
      "grad_norm": 0.4354168772697449,
      "learning_rate": 2.5561111111111114e-05,
      "loss": 0.004,
      "step": 43990
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.4167327284812927,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 0.0029,
      "step": 44000
    },
    {
      "epoch": 2.445,
      "grad_norm": 0.08948163688182831,
      "learning_rate": 2.555e-05,
      "loss": 0.0038,
      "step": 44010
    },
    {
      "epoch": 2.4455555555555555,
      "grad_norm": 0.5754360556602478,
      "learning_rate": 2.5544444444444445e-05,
      "loss": 0.0039,
      "step": 44020
    },
    {
      "epoch": 2.446111111111111,
      "grad_norm": 0.2516371011734009,
      "learning_rate": 2.553888888888889e-05,
      "loss": 0.0033,
      "step": 44030
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.03699313476681709,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0036,
      "step": 44040
    },
    {
      "epoch": 2.4472222222222224,
      "grad_norm": 0.14845728874206543,
      "learning_rate": 2.552777777777778e-05,
      "loss": 0.0034,
      "step": 44050
    },
    {
      "epoch": 2.4477777777777776,
      "grad_norm": 0.35719987750053406,
      "learning_rate": 2.5522222222222226e-05,
      "loss": 0.0038,
      "step": 44060
    },
    {
      "epoch": 2.4483333333333333,
      "grad_norm": 0.17978960275650024,
      "learning_rate": 2.5516666666666666e-05,
      "loss": 0.0033,
      "step": 44070
    },
    {
      "epoch": 2.448888888888889,
      "grad_norm": 0.32895559072494507,
      "learning_rate": 2.5511111111111113e-05,
      "loss": 0.0044,
      "step": 44080
    },
    {
      "epoch": 2.4494444444444445,
      "grad_norm": 0.3395768702030182,
      "learning_rate": 2.5505555555555556e-05,
      "loss": 0.0037,
      "step": 44090
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.010566270910203457,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0039,
      "step": 44100
    },
    {
      "epoch": 2.4505555555555554,
      "grad_norm": 0.18758536875247955,
      "learning_rate": 2.5494444444444443e-05,
      "loss": 0.0029,
      "step": 44110
    },
    {
      "epoch": 2.451111111111111,
      "grad_norm": 0.12424209713935852,
      "learning_rate": 2.548888888888889e-05,
      "loss": 0.0033,
      "step": 44120
    },
    {
      "epoch": 2.4516666666666667,
      "grad_norm": 0.060728881508111954,
      "learning_rate": 2.5483333333333337e-05,
      "loss": 0.0021,
      "step": 44130
    },
    {
      "epoch": 2.4522222222222223,
      "grad_norm": 0.08451518416404724,
      "learning_rate": 2.5477777777777777e-05,
      "loss": 0.0035,
      "step": 44140
    },
    {
      "epoch": 2.452777777777778,
      "grad_norm": 0.44291841983795166,
      "learning_rate": 2.5472222222222224e-05,
      "loss": 0.0029,
      "step": 44150
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.010709199123084545,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0037,
      "step": 44160
    },
    {
      "epoch": 2.453888888888889,
      "grad_norm": 0.06121297553181648,
      "learning_rate": 2.5461111111111115e-05,
      "loss": 0.0042,
      "step": 44170
    },
    {
      "epoch": 2.4544444444444444,
      "grad_norm": 0.17907847464084625,
      "learning_rate": 2.5455555555555555e-05,
      "loss": 0.005,
      "step": 44180
    },
    {
      "epoch": 2.455,
      "grad_norm": 0.23841683566570282,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 0.0036,
      "step": 44190
    },
    {
      "epoch": 2.4555555555555557,
      "grad_norm": 0.17903579771518707,
      "learning_rate": 2.5444444444444442e-05,
      "loss": 0.0029,
      "step": 44200
    },
    {
      "epoch": 2.4561111111111114,
      "grad_norm": 0.1940363496541977,
      "learning_rate": 2.543888888888889e-05,
      "loss": 0.0043,
      "step": 44210
    },
    {
      "epoch": 2.4566666666666666,
      "grad_norm": 0.6558946967124939,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0026,
      "step": 44220
    },
    {
      "epoch": 2.457222222222222,
      "grad_norm": 0.14898420870304108,
      "learning_rate": 2.542777777777778e-05,
      "loss": 0.0039,
      "step": 44230
    },
    {
      "epoch": 2.457777777777778,
      "grad_norm": 0.19060397148132324,
      "learning_rate": 2.5422222222222227e-05,
      "loss": 0.0041,
      "step": 44240
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 0.042306721210479736,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.0042,
      "step": 44250
    },
    {
      "epoch": 2.4588888888888887,
      "grad_norm": 0.0609654076397419,
      "learning_rate": 2.5411111111111114e-05,
      "loss": 0.0028,
      "step": 44260
    },
    {
      "epoch": 2.4594444444444443,
      "grad_norm": 0.3619360625743866,
      "learning_rate": 2.5405555555555554e-05,
      "loss": 0.0022,
      "step": 44270
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.43090763688087463,
      "learning_rate": 2.54e-05,
      "loss": 0.0038,
      "step": 44280
    },
    {
      "epoch": 2.4605555555555556,
      "grad_norm": 0.2979913353919983,
      "learning_rate": 2.5394444444444444e-05,
      "loss": 0.0037,
      "step": 44290
    },
    {
      "epoch": 2.4611111111111112,
      "grad_norm": 0.14923763275146484,
      "learning_rate": 2.538888888888889e-05,
      "loss": 0.0038,
      "step": 44300
    },
    {
      "epoch": 2.461666666666667,
      "grad_norm": 0.060837894678115845,
      "learning_rate": 2.5383333333333338e-05,
      "loss": 0.0021,
      "step": 44310
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 0.15034008026123047,
      "learning_rate": 2.537777777777778e-05,
      "loss": 0.0032,
      "step": 44320
    },
    {
      "epoch": 2.4627777777777777,
      "grad_norm": 0.23861517012119293,
      "learning_rate": 2.5372222222222225e-05,
      "loss": 0.0031,
      "step": 44330
    },
    {
      "epoch": 2.4633333333333334,
      "grad_norm": 0.3273352086544037,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0047,
      "step": 44340
    },
    {
      "epoch": 2.463888888888889,
      "grad_norm": 0.011125479824841022,
      "learning_rate": 2.5361111111111112e-05,
      "loss": 0.0037,
      "step": 44350
    },
    {
      "epoch": 2.464444444444444,
      "grad_norm": 0.12131628394126892,
      "learning_rate": 2.5355555555555556e-05,
      "loss": 0.0042,
      "step": 44360
    },
    {
      "epoch": 2.465,
      "grad_norm": 0.7082337141036987,
      "learning_rate": 2.5350000000000003e-05,
      "loss": 0.0032,
      "step": 44370
    },
    {
      "epoch": 2.4655555555555555,
      "grad_norm": 0.40975165367126465,
      "learning_rate": 2.534444444444445e-05,
      "loss": 0.0037,
      "step": 44380
    },
    {
      "epoch": 2.466111111111111,
      "grad_norm": 0.11935720592737198,
      "learning_rate": 2.533888888888889e-05,
      "loss": 0.0048,
      "step": 44390
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.03742227703332901,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0038,
      "step": 44400
    },
    {
      "epoch": 2.4672222222222224,
      "grad_norm": 0.11988691985607147,
      "learning_rate": 2.5327777777777777e-05,
      "loss": 0.0046,
      "step": 44410
    },
    {
      "epoch": 2.4677777777777776,
      "grad_norm": 0.2685238718986511,
      "learning_rate": 2.5322222222222224e-05,
      "loss": 0.0043,
      "step": 44420
    },
    {
      "epoch": 2.4683333333333333,
      "grad_norm": 0.03232590854167938,
      "learning_rate": 2.5316666666666668e-05,
      "loss": 0.0028,
      "step": 44430
    },
    {
      "epoch": 2.468888888888889,
      "grad_norm": 0.03466111049056053,
      "learning_rate": 2.5311111111111115e-05,
      "loss": 0.0038,
      "step": 44440
    },
    {
      "epoch": 2.4694444444444446,
      "grad_norm": 0.0329672247171402,
      "learning_rate": 2.5305555555555555e-05,
      "loss": 0.0041,
      "step": 44450
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.1493448168039322,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.0031,
      "step": 44460
    },
    {
      "epoch": 2.4705555555555554,
      "grad_norm": 0.14920613169670105,
      "learning_rate": 2.529444444444445e-05,
      "loss": 0.0023,
      "step": 44470
    },
    {
      "epoch": 2.471111111111111,
      "grad_norm": 0.2383219301700592,
      "learning_rate": 2.528888888888889e-05,
      "loss": 0.0021,
      "step": 44480
    },
    {
      "epoch": 2.4716666666666667,
      "grad_norm": 0.3141189515590668,
      "learning_rate": 2.5283333333333336e-05,
      "loss": 0.0029,
      "step": 44490
    },
    {
      "epoch": 2.4722222222222223,
      "grad_norm": 0.5859717130661011,
      "learning_rate": 2.527777777777778e-05,
      "loss": 0.0027,
      "step": 44500
    },
    {
      "epoch": 2.472777777777778,
      "grad_norm": 0.14979788661003113,
      "learning_rate": 2.5272222222222226e-05,
      "loss": 0.0028,
      "step": 44510
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.3283125162124634,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0025,
      "step": 44520
    },
    {
      "epoch": 2.473888888888889,
      "grad_norm": 0.623767614364624,
      "learning_rate": 2.5261111111111113e-05,
      "loss": 0.0032,
      "step": 44530
    },
    {
      "epoch": 2.4744444444444444,
      "grad_norm": 0.09066756814718246,
      "learning_rate": 2.5255555555555554e-05,
      "loss": 0.0037,
      "step": 44540
    },
    {
      "epoch": 2.475,
      "grad_norm": 0.24277594685554504,
      "learning_rate": 2.525e-05,
      "loss": 0.0036,
      "step": 44550
    },
    {
      "epoch": 2.4755555555555557,
      "grad_norm": 0.08951576799154282,
      "learning_rate": 2.5244444444444447e-05,
      "loss": 0.0031,
      "step": 44560
    },
    {
      "epoch": 2.476111111111111,
      "grad_norm": 0.23845873773097992,
      "learning_rate": 2.523888888888889e-05,
      "loss": 0.0037,
      "step": 44570
    },
    {
      "epoch": 2.4766666666666666,
      "grad_norm": 0.11911777406930923,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0029,
      "step": 44580
    },
    {
      "epoch": 2.477222222222222,
      "grad_norm": 0.12257292121648788,
      "learning_rate": 2.5227777777777778e-05,
      "loss": 0.0048,
      "step": 44590
    },
    {
      "epoch": 2.477777777777778,
      "grad_norm": 0.06097997725009918,
      "learning_rate": 2.5222222222222225e-05,
      "loss": 0.0023,
      "step": 44600
    },
    {
      "epoch": 2.4783333333333335,
      "grad_norm": 0.09238983690738678,
      "learning_rate": 2.5216666666666665e-05,
      "loss": 0.003,
      "step": 44610
    },
    {
      "epoch": 2.4788888888888887,
      "grad_norm": 0.009703806601464748,
      "learning_rate": 2.5211111111111112e-05,
      "loss": 0.0037,
      "step": 44620
    },
    {
      "epoch": 2.4794444444444443,
      "grad_norm": 0.4205685257911682,
      "learning_rate": 2.5205555555555556e-05,
      "loss": 0.004,
      "step": 44630
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.031156940385699272,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0035,
      "step": 44640
    },
    {
      "epoch": 2.4805555555555556,
      "grad_norm": 0.06496836245059967,
      "learning_rate": 2.519444444444445e-05,
      "loss": 0.0036,
      "step": 44650
    },
    {
      "epoch": 2.4811111111111113,
      "grad_norm": 0.3387991189956665,
      "learning_rate": 2.518888888888889e-05,
      "loss": 0.0041,
      "step": 44660
    },
    {
      "epoch": 2.4816666666666665,
      "grad_norm": 0.031571660190820694,
      "learning_rate": 2.5183333333333337e-05,
      "loss": 0.0036,
      "step": 44670
    },
    {
      "epoch": 2.482222222222222,
      "grad_norm": 0.26837900280952454,
      "learning_rate": 2.5177777777777777e-05,
      "loss": 0.0034,
      "step": 44680
    },
    {
      "epoch": 2.4827777777777778,
      "grad_norm": 0.09032151848077774,
      "learning_rate": 2.5172222222222224e-05,
      "loss": 0.0031,
      "step": 44690
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.061354860663414,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0029,
      "step": 44700
    },
    {
      "epoch": 2.483888888888889,
      "grad_norm": 0.08997540920972824,
      "learning_rate": 2.5161111111111114e-05,
      "loss": 0.003,
      "step": 44710
    },
    {
      "epoch": 2.4844444444444447,
      "grad_norm": 0.09004998207092285,
      "learning_rate": 2.5155555555555555e-05,
      "loss": 0.0026,
      "step": 44720
    },
    {
      "epoch": 2.485,
      "grad_norm": 0.20840680599212646,
      "learning_rate": 2.515e-05,
      "loss": 0.0036,
      "step": 44730
    },
    {
      "epoch": 2.4855555555555555,
      "grad_norm": 0.2383723109960556,
      "learning_rate": 2.514444444444445e-05,
      "loss": 0.0038,
      "step": 44740
    },
    {
      "epoch": 2.486111111111111,
      "grad_norm": 0.47483745217323303,
      "learning_rate": 2.513888888888889e-05,
      "loss": 0.0044,
      "step": 44750
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.1164662167429924,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0027,
      "step": 44760
    },
    {
      "epoch": 2.487222222222222,
      "grad_norm": 0.11982396245002747,
      "learning_rate": 2.512777777777778e-05,
      "loss": 0.0032,
      "step": 44770
    },
    {
      "epoch": 2.4877777777777776,
      "grad_norm": 0.10599804669618607,
      "learning_rate": 2.5122222222222226e-05,
      "loss": 0.0033,
      "step": 44780
    },
    {
      "epoch": 2.4883333333333333,
      "grad_norm": 0.23824311792850494,
      "learning_rate": 2.5116666666666666e-05,
      "loss": 0.0039,
      "step": 44790
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 0.013444473966956139,
      "learning_rate": 2.5111111111111113e-05,
      "loss": 0.0026,
      "step": 44800
    },
    {
      "epoch": 2.4894444444444446,
      "grad_norm": 0.033595010638237,
      "learning_rate": 2.5105555555555553e-05,
      "loss": 0.0038,
      "step": 44810
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.1196911558508873,
      "learning_rate": 2.51e-05,
      "loss": 0.0043,
      "step": 44820
    },
    {
      "epoch": 2.4905555555555554,
      "grad_norm": 0.27908802032470703,
      "learning_rate": 2.5094444444444447e-05,
      "loss": 0.0031,
      "step": 44830
    },
    {
      "epoch": 2.491111111111111,
      "grad_norm": 0.03162893280386925,
      "learning_rate": 2.508888888888889e-05,
      "loss": 0.0025,
      "step": 44840
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 0.06892949342727661,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.0038,
      "step": 44850
    },
    {
      "epoch": 2.4922222222222223,
      "grad_norm": 0.14961746335029602,
      "learning_rate": 2.5077777777777778e-05,
      "loss": 0.0021,
      "step": 44860
    },
    {
      "epoch": 2.4927777777777775,
      "grad_norm": 0.06052514165639877,
      "learning_rate": 2.5072222222222225e-05,
      "loss": 0.0025,
      "step": 44870
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.12087780982255936,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0028,
      "step": 44880
    },
    {
      "epoch": 2.493888888888889,
      "grad_norm": 0.5957023501396179,
      "learning_rate": 2.5061111111111112e-05,
      "loss": 0.0028,
      "step": 44890
    },
    {
      "epoch": 2.4944444444444445,
      "grad_norm": 0.2684645354747772,
      "learning_rate": 2.5055555555555555e-05,
      "loss": 0.0039,
      "step": 44900
    },
    {
      "epoch": 2.495,
      "grad_norm": 0.3627293109893799,
      "learning_rate": 2.5050000000000002e-05,
      "loss": 0.0036,
      "step": 44910
    },
    {
      "epoch": 2.4955555555555557,
      "grad_norm": 0.061653658747673035,
      "learning_rate": 2.504444444444445e-05,
      "loss": 0.0028,
      "step": 44920
    },
    {
      "epoch": 2.496111111111111,
      "grad_norm": 0.03269495815038681,
      "learning_rate": 2.503888888888889e-05,
      "loss": 0.0048,
      "step": 44930
    },
    {
      "epoch": 2.4966666666666666,
      "grad_norm": 0.32432693243026733,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.0028,
      "step": 44940
    },
    {
      "epoch": 2.4972222222222222,
      "grad_norm": 0.1505851298570633,
      "learning_rate": 2.5027777777777777e-05,
      "loss": 0.0045,
      "step": 44950
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 0.3963147699832916,
      "learning_rate": 2.5022222222222224e-05,
      "loss": 0.0043,
      "step": 44960
    },
    {
      "epoch": 2.4983333333333335,
      "grad_norm": 0.08974936604499817,
      "learning_rate": 2.5016666666666667e-05,
      "loss": 0.0032,
      "step": 44970
    },
    {
      "epoch": 2.4988888888888887,
      "grad_norm": 0.3897336423397064,
      "learning_rate": 2.5011111111111114e-05,
      "loss": 0.0038,
      "step": 44980
    },
    {
      "epoch": 2.4994444444444444,
      "grad_norm": 0.06203864887356758,
      "learning_rate": 2.5005555555555554e-05,
      "loss": 0.0039,
      "step": 44990
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.06095235049724579,
      "learning_rate": 2.5e-05,
      "loss": 0.0031,
      "step": 45000
    },
    {
      "epoch": 2.5005555555555556,
      "grad_norm": 0.08964302390813828,
      "learning_rate": 2.4994444444444445e-05,
      "loss": 0.0033,
      "step": 45010
    },
    {
      "epoch": 2.5011111111111113,
      "grad_norm": 0.2679334580898285,
      "learning_rate": 2.498888888888889e-05,
      "loss": 0.0026,
      "step": 45020
    },
    {
      "epoch": 2.501666666666667,
      "grad_norm": 0.36276352405548096,
      "learning_rate": 2.4983333333333335e-05,
      "loss": 0.0032,
      "step": 45030
    },
    {
      "epoch": 2.502222222222222,
      "grad_norm": 0.20859485864639282,
      "learning_rate": 2.497777777777778e-05,
      "loss": 0.003,
      "step": 45040
    },
    {
      "epoch": 2.5027777777777778,
      "grad_norm": 0.3271496295928955,
      "learning_rate": 2.4972222222222226e-05,
      "loss": 0.004,
      "step": 45050
    },
    {
      "epoch": 2.5033333333333334,
      "grad_norm": 0.18191348016262054,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0027,
      "step": 45060
    },
    {
      "epoch": 2.503888888888889,
      "grad_norm": 0.06086055934429169,
      "learning_rate": 2.4961111111111113e-05,
      "loss": 0.0032,
      "step": 45070
    },
    {
      "epoch": 2.5044444444444443,
      "grad_norm": 0.3882558345794678,
      "learning_rate": 2.4955555555555556e-05,
      "loss": 0.0033,
      "step": 45080
    },
    {
      "epoch": 2.505,
      "grad_norm": 0.15166358649730682,
      "learning_rate": 2.495e-05,
      "loss": 0.0047,
      "step": 45090
    },
    {
      "epoch": 2.5055555555555555,
      "grad_norm": 0.11922552436590195,
      "learning_rate": 2.4944444444444447e-05,
      "loss": 0.0028,
      "step": 45100
    },
    {
      "epoch": 2.506111111111111,
      "grad_norm": 0.2683643698692322,
      "learning_rate": 2.493888888888889e-05,
      "loss": 0.0032,
      "step": 45110
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.011581423692405224,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0035,
      "step": 45120
    },
    {
      "epoch": 2.5072222222222225,
      "grad_norm": 0.1501888930797577,
      "learning_rate": 2.4927777777777778e-05,
      "loss": 0.0034,
      "step": 45130
    },
    {
      "epoch": 2.5077777777777777,
      "grad_norm": 0.11968140304088593,
      "learning_rate": 2.4922222222222225e-05,
      "loss": 0.0037,
      "step": 45140
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 0.20267733931541443,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.0046,
      "step": 45150
    },
    {
      "epoch": 2.508888888888889,
      "grad_norm": 0.1240241676568985,
      "learning_rate": 2.491111111111111e-05,
      "loss": 0.0019,
      "step": 45160
    },
    {
      "epoch": 2.5094444444444446,
      "grad_norm": 0.5407828092575073,
      "learning_rate": 2.490555555555556e-05,
      "loss": 0.0048,
      "step": 45170
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.1793137490749359,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0025,
      "step": 45180
    },
    {
      "epoch": 2.5105555555555554,
      "grad_norm": 0.32535141706466675,
      "learning_rate": 2.4894444444444446e-05,
      "loss": 0.0024,
      "step": 45190
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 0.29786986112594604,
      "learning_rate": 2.488888888888889e-05,
      "loss": 0.0028,
      "step": 45200
    },
    {
      "epoch": 2.5116666666666667,
      "grad_norm": 0.1789952665567398,
      "learning_rate": 2.4883333333333333e-05,
      "loss": 0.0049,
      "step": 45210
    },
    {
      "epoch": 2.5122222222222224,
      "grad_norm": 0.381752610206604,
      "learning_rate": 2.4877777777777776e-05,
      "loss": 0.0027,
      "step": 45220
    },
    {
      "epoch": 2.512777777777778,
      "grad_norm": 0.3025625944137573,
      "learning_rate": 2.4872222222222223e-05,
      "loss": 0.0037,
      "step": 45230
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.006688973866403103,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0036,
      "step": 45240
    },
    {
      "epoch": 2.513888888888889,
      "grad_norm": 0.362588107585907,
      "learning_rate": 2.4861111111111114e-05,
      "loss": 0.003,
      "step": 45250
    },
    {
      "epoch": 2.5144444444444445,
      "grad_norm": 0.17027699947357178,
      "learning_rate": 2.4855555555555557e-05,
      "loss": 0.0035,
      "step": 45260
    },
    {
      "epoch": 2.515,
      "grad_norm": 0.14936377108097076,
      "learning_rate": 2.485e-05,
      "loss": 0.0038,
      "step": 45270
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 0.20842306315898895,
      "learning_rate": 2.4844444444444444e-05,
      "loss": 0.0027,
      "step": 45280
    },
    {
      "epoch": 2.516111111111111,
      "grad_norm": 0.4706525206565857,
      "learning_rate": 2.4838888888888888e-05,
      "loss": 0.0028,
      "step": 45290
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 0.019655408337712288,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0048,
      "step": 45300
    },
    {
      "epoch": 2.5172222222222222,
      "grad_norm": 0.12070484459400177,
      "learning_rate": 2.482777777777778e-05,
      "loss": 0.0028,
      "step": 45310
    },
    {
      "epoch": 2.517777777777778,
      "grad_norm": 0.2093617022037506,
      "learning_rate": 2.4822222222222225e-05,
      "loss": 0.0026,
      "step": 45320
    },
    {
      "epoch": 2.5183333333333335,
      "grad_norm": 0.14889247715473175,
      "learning_rate": 2.481666666666667e-05,
      "loss": 0.0028,
      "step": 45330
    },
    {
      "epoch": 2.5188888888888887,
      "grad_norm": 0.03334306925535202,
      "learning_rate": 2.4811111111111113e-05,
      "loss": 0.002,
      "step": 45340
    },
    {
      "epoch": 2.5194444444444444,
      "grad_norm": 0.034730810672044754,
      "learning_rate": 2.4805555555555556e-05,
      "loss": 0.0038,
      "step": 45350
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.19844095408916473,
      "learning_rate": 2.48e-05,
      "loss": 0.0023,
      "step": 45360
    },
    {
      "epoch": 2.5205555555555557,
      "grad_norm": 0.09844392538070679,
      "learning_rate": 2.4794444444444447e-05,
      "loss": 0.0043,
      "step": 45370
    },
    {
      "epoch": 2.521111111111111,
      "grad_norm": 0.2501225769519806,
      "learning_rate": 2.478888888888889e-05,
      "loss": 0.0032,
      "step": 45380
    },
    {
      "epoch": 2.5216666666666665,
      "grad_norm": 0.3119891881942749,
      "learning_rate": 2.4783333333333334e-05,
      "loss": 0.0036,
      "step": 45390
    },
    {
      "epoch": 2.522222222222222,
      "grad_norm": 0.2082696259021759,
      "learning_rate": 2.477777777777778e-05,
      "loss": 0.0044,
      "step": 45400
    },
    {
      "epoch": 2.522777777777778,
      "grad_norm": 0.14973394572734833,
      "learning_rate": 2.4772222222222224e-05,
      "loss": 0.0035,
      "step": 45410
    },
    {
      "epoch": 2.5233333333333334,
      "grad_norm": 0.17218941450119019,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0032,
      "step": 45420
    },
    {
      "epoch": 2.523888888888889,
      "grad_norm": 0.41661861538887024,
      "learning_rate": 2.476111111111111e-05,
      "loss": 0.003,
      "step": 45430
    },
    {
      "epoch": 2.5244444444444447,
      "grad_norm": 0.20914141833782196,
      "learning_rate": 2.475555555555556e-05,
      "loss": 0.0026,
      "step": 45440
    },
    {
      "epoch": 2.525,
      "grad_norm": 0.3261747360229492,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.004,
      "step": 45450
    },
    {
      "epoch": 2.5255555555555556,
      "grad_norm": 0.032779525965452194,
      "learning_rate": 2.4744444444444445e-05,
      "loss": 0.004,
      "step": 45460
    },
    {
      "epoch": 2.526111111111111,
      "grad_norm": 0.010727490298449993,
      "learning_rate": 2.473888888888889e-05,
      "loss": 0.0027,
      "step": 45470
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.14944973587989807,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0036,
      "step": 45480
    },
    {
      "epoch": 2.527222222222222,
      "grad_norm": 0.0327981673181057,
      "learning_rate": 2.472777777777778e-05,
      "loss": 0.004,
      "step": 45490
    },
    {
      "epoch": 2.5277777777777777,
      "grad_norm": 0.2386522889137268,
      "learning_rate": 2.4722222222222223e-05,
      "loss": 0.0026,
      "step": 45500
    },
    {
      "epoch": 2.5283333333333333,
      "grad_norm": 0.06014581769704819,
      "learning_rate": 2.471666666666667e-05,
      "loss": 0.0035,
      "step": 45510
    },
    {
      "epoch": 2.528888888888889,
      "grad_norm": 0.11992181092500687,
      "learning_rate": 2.4711111111111114e-05,
      "loss": 0.0026,
      "step": 45520
    },
    {
      "epoch": 2.5294444444444446,
      "grad_norm": 0.29779306054115295,
      "learning_rate": 2.4705555555555557e-05,
      "loss": 0.0037,
      "step": 45530
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 0.38696107268333435,
      "learning_rate": 2.47e-05,
      "loss": 0.0032,
      "step": 45540
    },
    {
      "epoch": 2.5305555555555554,
      "grad_norm": 0.6846429705619812,
      "learning_rate": 2.4694444444444444e-05,
      "loss": 0.0024,
      "step": 45550
    },
    {
      "epoch": 2.531111111111111,
      "grad_norm": 0.15170635282993317,
      "learning_rate": 2.4688888888888888e-05,
      "loss": 0.0019,
      "step": 45560
    },
    {
      "epoch": 2.5316666666666667,
      "grad_norm": 0.20866994559764862,
      "learning_rate": 2.4683333333333335e-05,
      "loss": 0.0036,
      "step": 45570
    },
    {
      "epoch": 2.5322222222222224,
      "grad_norm": 0.41729894280433655,
      "learning_rate": 2.467777777777778e-05,
      "loss": 0.0034,
      "step": 45580
    },
    {
      "epoch": 2.5327777777777776,
      "grad_norm": 0.565617561340332,
      "learning_rate": 2.4672222222222225e-05,
      "loss": 0.0041,
      "step": 45590
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.08970333635807037,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0027,
      "step": 45600
    },
    {
      "epoch": 2.533888888888889,
      "grad_norm": 0.1799030303955078,
      "learning_rate": 2.4661111111111112e-05,
      "loss": 0.0024,
      "step": 45610
    },
    {
      "epoch": 2.5344444444444445,
      "grad_norm": 0.09080307185649872,
      "learning_rate": 2.4655555555555556e-05,
      "loss": 0.0035,
      "step": 45620
    },
    {
      "epoch": 2.535,
      "grad_norm": 0.5373029708862305,
      "learning_rate": 2.465e-05,
      "loss": 0.0028,
      "step": 45630
    },
    {
      "epoch": 2.535555555555556,
      "grad_norm": 0.011916928924620152,
      "learning_rate": 2.4644444444444446e-05,
      "loss": 0.0038,
      "step": 45640
    },
    {
      "epoch": 2.536111111111111,
      "grad_norm": 0.30882757902145386,
      "learning_rate": 2.463888888888889e-05,
      "loss": 0.0032,
      "step": 45650
    },
    {
      "epoch": 2.5366666666666666,
      "grad_norm": 0.2681353986263275,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.0051,
      "step": 45660
    },
    {
      "epoch": 2.5372222222222223,
      "grad_norm": 0.0905986949801445,
      "learning_rate": 2.462777777777778e-05,
      "loss": 0.0036,
      "step": 45670
    },
    {
      "epoch": 2.537777777777778,
      "grad_norm": 0.2685297727584839,
      "learning_rate": 2.4622222222222224e-05,
      "loss": 0.0027,
      "step": 45680
    },
    {
      "epoch": 2.538333333333333,
      "grad_norm": 0.17918485403060913,
      "learning_rate": 2.4616666666666668e-05,
      "loss": 0.0031,
      "step": 45690
    },
    {
      "epoch": 2.5388888888888888,
      "grad_norm": 0.1805400550365448,
      "learning_rate": 2.461111111111111e-05,
      "loss": 0.0027,
      "step": 45700
    },
    {
      "epoch": 2.5394444444444444,
      "grad_norm": 0.3528071939945221,
      "learning_rate": 2.4605555555555558e-05,
      "loss": 0.003,
      "step": 45710
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.2993473708629608,
      "learning_rate": 2.46e-05,
      "loss": 0.0032,
      "step": 45720
    },
    {
      "epoch": 2.5405555555555557,
      "grad_norm": 0.33891016244888306,
      "learning_rate": 2.4594444444444445e-05,
      "loss": 0.003,
      "step": 45730
    },
    {
      "epoch": 2.5411111111111113,
      "grad_norm": 0.5956568717956543,
      "learning_rate": 2.458888888888889e-05,
      "loss": 0.0029,
      "step": 45740
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 0.3045039474964142,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.0046,
      "step": 45750
    },
    {
      "epoch": 2.542222222222222,
      "grad_norm": 0.1490277647972107,
      "learning_rate": 2.457777777777778e-05,
      "loss": 0.0041,
      "step": 45760
    },
    {
      "epoch": 2.542777777777778,
      "grad_norm": 0.060537148267030716,
      "learning_rate": 2.4572222222222223e-05,
      "loss": 0.0038,
      "step": 45770
    },
    {
      "epoch": 2.5433333333333334,
      "grad_norm": 0.14883306622505188,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0043,
      "step": 45780
    },
    {
      "epoch": 2.5438888888888886,
      "grad_norm": 0.0955386757850647,
      "learning_rate": 2.4561111111111113e-05,
      "loss": 0.0036,
      "step": 45790
    },
    {
      "epoch": 2.5444444444444443,
      "grad_norm": 0.15072564780712128,
      "learning_rate": 2.4555555555555557e-05,
      "loss": 0.0035,
      "step": 45800
    },
    {
      "epoch": 2.545,
      "grad_norm": 0.3582112193107605,
      "learning_rate": 2.455e-05,
      "loss": 0.0053,
      "step": 45810
    },
    {
      "epoch": 2.5455555555555556,
      "grad_norm": 0.5261030197143555,
      "learning_rate": 2.4544444444444444e-05,
      "loss": 0.0049,
      "step": 45820
    },
    {
      "epoch": 2.546111111111111,
      "grad_norm": 0.40764281153678894,
      "learning_rate": 2.4538888888888888e-05,
      "loss": 0.0035,
      "step": 45830
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.17071816325187683,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0034,
      "step": 45840
    },
    {
      "epoch": 2.5472222222222225,
      "grad_norm": 0.2596434950828552,
      "learning_rate": 2.452777777777778e-05,
      "loss": 0.0031,
      "step": 45850
    },
    {
      "epoch": 2.5477777777777777,
      "grad_norm": 0.35735195875167847,
      "learning_rate": 2.4522222222222225e-05,
      "loss": 0.0035,
      "step": 45860
    },
    {
      "epoch": 2.5483333333333333,
      "grad_norm": 0.08984168618917465,
      "learning_rate": 2.451666666666667e-05,
      "loss": 0.0036,
      "step": 45870
    },
    {
      "epoch": 2.548888888888889,
      "grad_norm": 0.20100463926792145,
      "learning_rate": 2.4511111111111112e-05,
      "loss": 0.003,
      "step": 45880
    },
    {
      "epoch": 2.549444444444444,
      "grad_norm": 0.14936819672584534,
      "learning_rate": 2.4505555555555556e-05,
      "loss": 0.0043,
      "step": 45890
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.17837777733802795,
      "learning_rate": 2.45e-05,
      "loss": 0.003,
      "step": 45900
    },
    {
      "epoch": 2.5505555555555555,
      "grad_norm": 0.08974103629589081,
      "learning_rate": 2.4494444444444446e-05,
      "loss": 0.0021,
      "step": 45910
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 0.09212294220924377,
      "learning_rate": 2.448888888888889e-05,
      "loss": 0.0031,
      "step": 45920
    },
    {
      "epoch": 2.5516666666666667,
      "grad_norm": 0.21671685576438904,
      "learning_rate": 2.4483333333333333e-05,
      "loss": 0.003,
      "step": 45930
    },
    {
      "epoch": 2.5522222222222224,
      "grad_norm": 0.6255081295967102,
      "learning_rate": 2.447777777777778e-05,
      "loss": 0.003,
      "step": 45940
    },
    {
      "epoch": 2.552777777777778,
      "grad_norm": 0.47897782921791077,
      "learning_rate": 2.4472222222222224e-05,
      "loss": 0.0037,
      "step": 45950
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.3577938675880432,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0025,
      "step": 45960
    },
    {
      "epoch": 2.553888888888889,
      "grad_norm": 0.32980039715766907,
      "learning_rate": 2.446111111111111e-05,
      "loss": 0.0023,
      "step": 45970
    },
    {
      "epoch": 2.5544444444444445,
      "grad_norm": 0.3572540581226349,
      "learning_rate": 2.4455555555555558e-05,
      "loss": 0.003,
      "step": 45980
    },
    {
      "epoch": 2.555,
      "grad_norm": 0.08926375955343246,
      "learning_rate": 2.445e-05,
      "loss": 0.003,
      "step": 45990
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.14851386845111847,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.0034,
      "step": 46000
    },
    {
      "epoch": 2.556111111111111,
      "grad_norm": 0.14901478588581085,
      "learning_rate": 2.443888888888889e-05,
      "loss": 0.002,
      "step": 46010
    },
    {
      "epoch": 2.5566666666666666,
      "grad_norm": 0.48737937211990356,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0028,
      "step": 46020
    },
    {
      "epoch": 2.5572222222222223,
      "grad_norm": 0.5748875141143799,
      "learning_rate": 2.442777777777778e-05,
      "loss": 0.0029,
      "step": 46030
    },
    {
      "epoch": 2.557777777777778,
      "grad_norm": 0.46972641348838806,
      "learning_rate": 2.4422222222222223e-05,
      "loss": 0.0056,
      "step": 46040
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 0.32776764035224915,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.0032,
      "step": 46050
    },
    {
      "epoch": 2.5588888888888888,
      "grad_norm": 0.3987407982349396,
      "learning_rate": 2.4411111111111113e-05,
      "loss": 0.0023,
      "step": 46060
    },
    {
      "epoch": 2.5594444444444444,
      "grad_norm": 0.19922782480716705,
      "learning_rate": 2.4405555555555557e-05,
      "loss": 0.0042,
      "step": 46070
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.06067383289337158,
      "learning_rate": 2.44e-05,
      "loss": 0.0046,
      "step": 46080
    },
    {
      "epoch": 2.5605555555555557,
      "grad_norm": 0.30693167448043823,
      "learning_rate": 2.4394444444444444e-05,
      "loss": 0.0026,
      "step": 46090
    },
    {
      "epoch": 2.561111111111111,
      "grad_norm": 0.06014847010374069,
      "learning_rate": 2.4388888888888887e-05,
      "loss": 0.0033,
      "step": 46100
    },
    {
      "epoch": 2.5616666666666665,
      "grad_norm": 0.11936254799365997,
      "learning_rate": 2.4383333333333334e-05,
      "loss": 0.0027,
      "step": 46110
    },
    {
      "epoch": 2.562222222222222,
      "grad_norm": 0.031044533476233482,
      "learning_rate": 2.437777777777778e-05,
      "loss": 0.0033,
      "step": 46120
    },
    {
      "epoch": 2.562777777777778,
      "grad_norm": 0.11932998895645142,
      "learning_rate": 2.4372222222222225e-05,
      "loss": 0.0034,
      "step": 46130
    },
    {
      "epoch": 2.5633333333333335,
      "grad_norm": 0.3871026635169983,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.0021,
      "step": 46140
    },
    {
      "epoch": 2.563888888888889,
      "grad_norm": 0.6443890333175659,
      "learning_rate": 2.4361111111111112e-05,
      "loss": 0.0032,
      "step": 46150
    },
    {
      "epoch": 2.5644444444444443,
      "grad_norm": 0.04636313021183014,
      "learning_rate": 2.4355555555555555e-05,
      "loss": 0.0037,
      "step": 46160
    },
    {
      "epoch": 2.565,
      "grad_norm": 0.2105921357870102,
      "learning_rate": 2.435e-05,
      "loss": 0.0051,
      "step": 46170
    },
    {
      "epoch": 2.5655555555555556,
      "grad_norm": 0.03140673041343689,
      "learning_rate": 2.4344444444444446e-05,
      "loss": 0.0045,
      "step": 46180
    },
    {
      "epoch": 2.5661111111111112,
      "grad_norm": 0.51630038022995,
      "learning_rate": 2.433888888888889e-05,
      "loss": 0.0042,
      "step": 46190
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.2682054042816162,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0038,
      "step": 46200
    },
    {
      "epoch": 2.567222222222222,
      "grad_norm": 0.1197420284152031,
      "learning_rate": 2.432777777777778e-05,
      "loss": 0.0041,
      "step": 46210
    },
    {
      "epoch": 2.5677777777777777,
      "grad_norm": 0.5056450366973877,
      "learning_rate": 2.4322222222222224e-05,
      "loss": 0.0023,
      "step": 46220
    },
    {
      "epoch": 2.5683333333333334,
      "grad_norm": 0.1486101597547531,
      "learning_rate": 2.4316666666666667e-05,
      "loss": 0.0028,
      "step": 46230
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 0.11893390864133835,
      "learning_rate": 2.431111111111111e-05,
      "loss": 0.0044,
      "step": 46240
    },
    {
      "epoch": 2.5694444444444446,
      "grad_norm": 0.008288406766951084,
      "learning_rate": 2.4305555555555558e-05,
      "loss": 0.0032,
      "step": 46250
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.09519661217927933,
      "learning_rate": 2.43e-05,
      "loss": 0.0044,
      "step": 46260
    },
    {
      "epoch": 2.5705555555555555,
      "grad_norm": 0.15000014007091522,
      "learning_rate": 2.4294444444444445e-05,
      "loss": 0.0024,
      "step": 46270
    },
    {
      "epoch": 2.571111111111111,
      "grad_norm": 0.11886876821517944,
      "learning_rate": 2.4288888888888888e-05,
      "loss": 0.0037,
      "step": 46280
    },
    {
      "epoch": 2.5716666666666668,
      "grad_norm": 0.4125213325023651,
      "learning_rate": 2.4283333333333335e-05,
      "loss": 0.0036,
      "step": 46290
    },
    {
      "epoch": 2.572222222222222,
      "grad_norm": 0.14889098703861237,
      "learning_rate": 2.427777777777778e-05,
      "loss": 0.0034,
      "step": 46300
    },
    {
      "epoch": 2.5727777777777776,
      "grad_norm": 0.3280782699584961,
      "learning_rate": 2.4272222222222222e-05,
      "loss": 0.0028,
      "step": 46310
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.09060875326395035,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0027,
      "step": 46320
    },
    {
      "epoch": 2.573888888888889,
      "grad_norm": 0.09017477929592133,
      "learning_rate": 2.4261111111111113e-05,
      "loss": 0.003,
      "step": 46330
    },
    {
      "epoch": 2.5744444444444445,
      "grad_norm": 0.17891176044940948,
      "learning_rate": 2.4255555555555556e-05,
      "loss": 0.0041,
      "step": 46340
    },
    {
      "epoch": 2.575,
      "grad_norm": 0.416311115026474,
      "learning_rate": 2.425e-05,
      "loss": 0.0033,
      "step": 46350
    },
    {
      "epoch": 2.575555555555556,
      "grad_norm": 0.44619354605674744,
      "learning_rate": 2.4244444444444443e-05,
      "loss": 0.0033,
      "step": 46360
    },
    {
      "epoch": 2.576111111111111,
      "grad_norm": 0.08945611864328384,
      "learning_rate": 2.423888888888889e-05,
      "loss": 0.0024,
      "step": 46370
    },
    {
      "epoch": 2.5766666666666667,
      "grad_norm": 0.20817501842975616,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0031,
      "step": 46380
    },
    {
      "epoch": 2.5772222222222223,
      "grad_norm": 0.1026104986667633,
      "learning_rate": 2.422777777777778e-05,
      "loss": 0.0029,
      "step": 46390
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 0.09301601350307465,
      "learning_rate": 2.4222222222222224e-05,
      "loss": 0.004,
      "step": 46400
    },
    {
      "epoch": 2.578333333333333,
      "grad_norm": 0.2697145938873291,
      "learning_rate": 2.4216666666666668e-05,
      "loss": 0.0027,
      "step": 46410
    },
    {
      "epoch": 2.578888888888889,
      "grad_norm": 0.6549515128135681,
      "learning_rate": 2.421111111111111e-05,
      "loss": 0.0037,
      "step": 46420
    },
    {
      "epoch": 2.5794444444444444,
      "grad_norm": 0.5350726842880249,
      "learning_rate": 2.4205555555555555e-05,
      "loss": 0.0028,
      "step": 46430
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.5557683706283569,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0048,
      "step": 46440
    },
    {
      "epoch": 2.5805555555555557,
      "grad_norm": 0.6120405793190002,
      "learning_rate": 2.4194444444444446e-05,
      "loss": 0.0042,
      "step": 46450
    },
    {
      "epoch": 2.5811111111111114,
      "grad_norm": 0.1788652241230011,
      "learning_rate": 2.418888888888889e-05,
      "loss": 0.0041,
      "step": 46460
    },
    {
      "epoch": 2.5816666666666666,
      "grad_norm": 0.43900027871131897,
      "learning_rate": 2.4183333333333336e-05,
      "loss": 0.004,
      "step": 46470
    },
    {
      "epoch": 2.582222222222222,
      "grad_norm": 0.12257315963506699,
      "learning_rate": 2.417777777777778e-05,
      "loss": 0.0026,
      "step": 46480
    },
    {
      "epoch": 2.582777777777778,
      "grad_norm": 0.238443985581398,
      "learning_rate": 2.4172222222222223e-05,
      "loss": 0.0025,
      "step": 46490
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.35682082176208496,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0039,
      "step": 46500
    },
    {
      "epoch": 2.5838888888888887,
      "grad_norm": 0.4959849715232849,
      "learning_rate": 2.4161111111111114e-05,
      "loss": 0.0023,
      "step": 46510
    },
    {
      "epoch": 2.5844444444444443,
      "grad_norm": 0.12067951261997223,
      "learning_rate": 2.4155555555555557e-05,
      "loss": 0.003,
      "step": 46520
    },
    {
      "epoch": 2.585,
      "grad_norm": 0.14959178864955902,
      "learning_rate": 2.415e-05,
      "loss": 0.0036,
      "step": 46530
    },
    {
      "epoch": 2.5855555555555556,
      "grad_norm": 0.30184465646743774,
      "learning_rate": 2.4144444444444444e-05,
      "loss": 0.005,
      "step": 46540
    },
    {
      "epoch": 2.5861111111111112,
      "grad_norm": 0.6537359952926636,
      "learning_rate": 2.4138888888888888e-05,
      "loss": 0.0029,
      "step": 46550
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.20799130201339722,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0041,
      "step": 46560
    },
    {
      "epoch": 2.587222222222222,
      "grad_norm": 0.2689651846885681,
      "learning_rate": 2.412777777777778e-05,
      "loss": 0.0028,
      "step": 46570
    },
    {
      "epoch": 2.5877777777777777,
      "grad_norm": 0.03497091680765152,
      "learning_rate": 2.4122222222222225e-05,
      "loss": 0.0032,
      "step": 46580
    },
    {
      "epoch": 2.5883333333333334,
      "grad_norm": 0.2975737452507019,
      "learning_rate": 2.411666666666667e-05,
      "loss": 0.0025,
      "step": 46590
    },
    {
      "epoch": 2.588888888888889,
      "grad_norm": 0.5053378939628601,
      "learning_rate": 2.4111111111111113e-05,
      "loss": 0.0031,
      "step": 46600
    },
    {
      "epoch": 2.589444444444444,
      "grad_norm": 0.2715398967266083,
      "learning_rate": 2.4105555555555556e-05,
      "loss": 0.0033,
      "step": 46610
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.2678350806236267,
      "learning_rate": 2.41e-05,
      "loss": 0.0035,
      "step": 46620
    },
    {
      "epoch": 2.5905555555555555,
      "grad_norm": 0.4780391752719879,
      "learning_rate": 2.4094444444444443e-05,
      "loss": 0.0039,
      "step": 46630
    },
    {
      "epoch": 2.591111111111111,
      "grad_norm": 0.4758135974407196,
      "learning_rate": 2.408888888888889e-05,
      "loss": 0.0027,
      "step": 46640
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 0.23799338936805725,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.0034,
      "step": 46650
    },
    {
      "epoch": 2.5922222222222224,
      "grad_norm": 0.06024417653679848,
      "learning_rate": 2.407777777777778e-05,
      "loss": 0.003,
      "step": 46660
    },
    {
      "epoch": 2.5927777777777776,
      "grad_norm": 0.011756115593016148,
      "learning_rate": 2.4072222222222224e-05,
      "loss": 0.0028,
      "step": 46670
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.011198347434401512,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0021,
      "step": 46680
    },
    {
      "epoch": 2.593888888888889,
      "grad_norm": 0.5949801802635193,
      "learning_rate": 2.406111111111111e-05,
      "loss": 0.0033,
      "step": 46690
    },
    {
      "epoch": 2.5944444444444446,
      "grad_norm": 0.01974627934396267,
      "learning_rate": 2.4055555555555555e-05,
      "loss": 0.0038,
      "step": 46700
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 0.03341227397322655,
      "learning_rate": 2.4050000000000002e-05,
      "loss": 0.003,
      "step": 46710
    },
    {
      "epoch": 2.5955555555555554,
      "grad_norm": 0.3570159673690796,
      "learning_rate": 2.4044444444444445e-05,
      "loss": 0.003,
      "step": 46720
    },
    {
      "epoch": 2.596111111111111,
      "grad_norm": 0.12030167877674103,
      "learning_rate": 2.4038888888888892e-05,
      "loss": 0.0029,
      "step": 46730
    },
    {
      "epoch": 2.5966666666666667,
      "grad_norm": 0.08970117568969727,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.003,
      "step": 46740
    },
    {
      "epoch": 2.5972222222222223,
      "grad_norm": 0.5362802147865295,
      "learning_rate": 2.402777777777778e-05,
      "loss": 0.0038,
      "step": 46750
    },
    {
      "epoch": 2.597777777777778,
      "grad_norm": 0.17821753025054932,
      "learning_rate": 2.4022222222222223e-05,
      "loss": 0.0034,
      "step": 46760
    },
    {
      "epoch": 2.5983333333333336,
      "grad_norm": 0.08356568962335587,
      "learning_rate": 2.4016666666666667e-05,
      "loss": 0.0039,
      "step": 46770
    },
    {
      "epoch": 2.598888888888889,
      "grad_norm": 0.04638214409351349,
      "learning_rate": 2.4011111111111113e-05,
      "loss": 0.0036,
      "step": 46780
    },
    {
      "epoch": 2.5994444444444444,
      "grad_norm": 0.2089807242155075,
      "learning_rate": 2.4005555555555557e-05,
      "loss": 0.0044,
      "step": 46790
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.12035316228866577,
      "learning_rate": 2.4e-05,
      "loss": 0.0029,
      "step": 46800
    },
    {
      "epoch": 2.6005555555555553,
      "grad_norm": 0.2677929997444153,
      "learning_rate": 2.3994444444444444e-05,
      "loss": 0.003,
      "step": 46810
    },
    {
      "epoch": 2.601111111111111,
      "grad_norm": 0.29723766446113586,
      "learning_rate": 2.398888888888889e-05,
      "loss": 0.0038,
      "step": 46820
    },
    {
      "epoch": 2.6016666666666666,
      "grad_norm": 0.37715068459510803,
      "learning_rate": 2.3983333333333335e-05,
      "loss": 0.0023,
      "step": 46830
    },
    {
      "epoch": 2.602222222222222,
      "grad_norm": 0.2679346203804016,
      "learning_rate": 2.3977777777777778e-05,
      "loss": 0.0023,
      "step": 46840
    },
    {
      "epoch": 2.602777777777778,
      "grad_norm": 0.3797134757041931,
      "learning_rate": 2.3972222222222225e-05,
      "loss": 0.0039,
      "step": 46850
    },
    {
      "epoch": 2.6033333333333335,
      "grad_norm": 0.2093755155801773,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0027,
      "step": 46860
    },
    {
      "epoch": 2.603888888888889,
      "grad_norm": 0.12387407571077347,
      "learning_rate": 2.3961111111111112e-05,
      "loss": 0.0036,
      "step": 46870
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 0.41631951928138733,
      "learning_rate": 2.3955555555555556e-05,
      "loss": 0.0043,
      "step": 46880
    },
    {
      "epoch": 2.605,
      "grad_norm": 0.29784780740737915,
      "learning_rate": 2.395e-05,
      "loss": 0.0029,
      "step": 46890
    },
    {
      "epoch": 2.6055555555555556,
      "grad_norm": 0.03130523860454559,
      "learning_rate": 2.3944444444444443e-05,
      "loss": 0.0033,
      "step": 46900
    },
    {
      "epoch": 2.6061111111111113,
      "grad_norm": 0.5962971448898315,
      "learning_rate": 2.393888888888889e-05,
      "loss": 0.003,
      "step": 46910
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.27114951610565186,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0031,
      "step": 46920
    },
    {
      "epoch": 2.607222222222222,
      "grad_norm": 0.1790941059589386,
      "learning_rate": 2.392777777777778e-05,
      "loss": 0.0028,
      "step": 46930
    },
    {
      "epoch": 2.6077777777777778,
      "grad_norm": 0.060506172478199005,
      "learning_rate": 2.3922222222222224e-05,
      "loss": 0.0031,
      "step": 46940
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 0.23553457856178284,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.0043,
      "step": 46950
    },
    {
      "epoch": 2.608888888888889,
      "grad_norm": 0.6942065954208374,
      "learning_rate": 2.391111111111111e-05,
      "loss": 0.003,
      "step": 46960
    },
    {
      "epoch": 2.6094444444444447,
      "grad_norm": 0.15567515790462494,
      "learning_rate": 2.3905555555555555e-05,
      "loss": 0.002,
      "step": 46970
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.39297446608543396,
      "learning_rate": 2.39e-05,
      "loss": 0.0039,
      "step": 46980
    },
    {
      "epoch": 2.6105555555555555,
      "grad_norm": 0.12004425376653671,
      "learning_rate": 2.3894444444444445e-05,
      "loss": 0.0038,
      "step": 46990
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 0.2988477349281311,
      "learning_rate": 2.3888888888888892e-05,
      "loss": 0.0043,
      "step": 47000
    },
    {
      "epoch": 2.611666666666667,
      "grad_norm": 0.2385258823633194,
      "learning_rate": 2.3883333333333336e-05,
      "loss": 0.0027,
      "step": 47010
    },
    {
      "epoch": 2.612222222222222,
      "grad_norm": 0.4462401568889618,
      "learning_rate": 2.387777777777778e-05,
      "loss": 0.0041,
      "step": 47020
    },
    {
      "epoch": 2.6127777777777776,
      "grad_norm": 0.32688939571380615,
      "learning_rate": 2.3872222222222223e-05,
      "loss": 0.0027,
      "step": 47030
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.4420364797115326,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0029,
      "step": 47040
    },
    {
      "epoch": 2.613888888888889,
      "grad_norm": 0.2378571629524231,
      "learning_rate": 2.3861111111111113e-05,
      "loss": 0.0023,
      "step": 47050
    },
    {
      "epoch": 2.6144444444444446,
      "grad_norm": 0.41575661301612854,
      "learning_rate": 2.3855555555555557e-05,
      "loss": 0.0036,
      "step": 47060
    },
    {
      "epoch": 2.615,
      "grad_norm": 0.35695189237594604,
      "learning_rate": 2.385e-05,
      "loss": 0.0028,
      "step": 47070
    },
    {
      "epoch": 2.6155555555555554,
      "grad_norm": 0.23875990509986877,
      "learning_rate": 2.3844444444444444e-05,
      "loss": 0.0031,
      "step": 47080
    },
    {
      "epoch": 2.616111111111111,
      "grad_norm": 0.477279394865036,
      "learning_rate": 2.383888888888889e-05,
      "loss": 0.0026,
      "step": 47090
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.29762235283851624,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.0036,
      "step": 47100
    },
    {
      "epoch": 2.6172222222222223,
      "grad_norm": 0.043404582887887955,
      "learning_rate": 2.3827777777777778e-05,
      "loss": 0.0039,
      "step": 47110
    },
    {
      "epoch": 2.6177777777777775,
      "grad_norm": 0.06450199335813522,
      "learning_rate": 2.3822222222222225e-05,
      "loss": 0.0028,
      "step": 47120
    },
    {
      "epoch": 2.618333333333333,
      "grad_norm": 0.6134323477745056,
      "learning_rate": 2.381666666666667e-05,
      "loss": 0.0032,
      "step": 47130
    },
    {
      "epoch": 2.618888888888889,
      "grad_norm": 0.1195446327328682,
      "learning_rate": 2.3811111111111112e-05,
      "loss": 0.0028,
      "step": 47140
    },
    {
      "epoch": 2.6194444444444445,
      "grad_norm": 0.5293562412261963,
      "learning_rate": 2.3805555555555556e-05,
      "loss": 0.0023,
      "step": 47150
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.4183546304702759,
      "learning_rate": 2.38e-05,
      "loss": 0.0026,
      "step": 47160
    },
    {
      "epoch": 2.6205555555555557,
      "grad_norm": 0.23827916383743286,
      "learning_rate": 2.3794444444444443e-05,
      "loss": 0.0038,
      "step": 47170
    },
    {
      "epoch": 2.621111111111111,
      "grad_norm": 0.4457184970378876,
      "learning_rate": 2.378888888888889e-05,
      "loss": 0.0028,
      "step": 47180
    },
    {
      "epoch": 2.6216666666666666,
      "grad_norm": 0.03452082723379135,
      "learning_rate": 2.3783333333333337e-05,
      "loss": 0.0032,
      "step": 47190
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.04936320707201958,
      "learning_rate": 2.377777777777778e-05,
      "loss": 0.0028,
      "step": 47200
    },
    {
      "epoch": 2.622777777777778,
      "grad_norm": 0.14868737757205963,
      "learning_rate": 2.3772222222222224e-05,
      "loss": 0.0027,
      "step": 47210
    },
    {
      "epoch": 2.623333333333333,
      "grad_norm": 0.42008936405181885,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0038,
      "step": 47220
    },
    {
      "epoch": 2.6238888888888887,
      "grad_norm": 0.09145995229482651,
      "learning_rate": 2.376111111111111e-05,
      "loss": 0.0032,
      "step": 47230
    },
    {
      "epoch": 2.6244444444444444,
      "grad_norm": 0.47511303424835205,
      "learning_rate": 2.3755555555555554e-05,
      "loss": 0.003,
      "step": 47240
    },
    {
      "epoch": 2.625,
      "grad_norm": 0.2708040177822113,
      "learning_rate": 2.375e-05,
      "loss": 0.0031,
      "step": 47250
    },
    {
      "epoch": 2.6255555555555556,
      "grad_norm": 0.012069225311279297,
      "learning_rate": 2.3744444444444448e-05,
      "loss": 0.003,
      "step": 47260
    },
    {
      "epoch": 2.6261111111111113,
      "grad_norm": 0.03209076449275017,
      "learning_rate": 2.3738888888888892e-05,
      "loss": 0.0036,
      "step": 47270
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.26748600602149963,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0034,
      "step": 47280
    },
    {
      "epoch": 2.627222222222222,
      "grad_norm": 0.14991484582424164,
      "learning_rate": 2.372777777777778e-05,
      "loss": 0.003,
      "step": 47290
    },
    {
      "epoch": 2.6277777777777778,
      "grad_norm": 0.08957727253437042,
      "learning_rate": 2.3722222222222222e-05,
      "loss": 0.0027,
      "step": 47300
    },
    {
      "epoch": 2.6283333333333334,
      "grad_norm": 0.48064112663269043,
      "learning_rate": 2.3716666666666666e-05,
      "loss": 0.0039,
      "step": 47310
    },
    {
      "epoch": 2.628888888888889,
      "grad_norm": 0.3281812369823456,
      "learning_rate": 2.3711111111111113e-05,
      "loss": 0.0027,
      "step": 47320
    },
    {
      "epoch": 2.6294444444444443,
      "grad_norm": 0.23807701468467712,
      "learning_rate": 2.3705555555555557e-05,
      "loss": 0.0041,
      "step": 47330
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.35904935002326965,
      "learning_rate": 2.37e-05,
      "loss": 0.0027,
      "step": 47340
    },
    {
      "epoch": 2.6305555555555555,
      "grad_norm": 0.06111041083931923,
      "learning_rate": 2.3694444444444447e-05,
      "loss": 0.0035,
      "step": 47350
    },
    {
      "epoch": 2.631111111111111,
      "grad_norm": 0.14920714497566223,
      "learning_rate": 2.368888888888889e-05,
      "loss": 0.0037,
      "step": 47360
    },
    {
      "epoch": 2.631666666666667,
      "grad_norm": 0.09033489972352982,
      "learning_rate": 2.3683333333333334e-05,
      "loss": 0.0045,
      "step": 47370
    },
    {
      "epoch": 2.6322222222222225,
      "grad_norm": 0.40781110525131226,
      "learning_rate": 2.3677777777777778e-05,
      "loss": 0.0043,
      "step": 47380
    },
    {
      "epoch": 2.6327777777777777,
      "grad_norm": 0.08985652029514313,
      "learning_rate": 2.3672222222222225e-05,
      "loss": 0.0029,
      "step": 47390
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.26991692185401917,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0032,
      "step": 47400
    },
    {
      "epoch": 2.633888888888889,
      "grad_norm": 0.06153746694326401,
      "learning_rate": 2.3661111111111112e-05,
      "loss": 0.0041,
      "step": 47410
    },
    {
      "epoch": 2.6344444444444446,
      "grad_norm": 0.17874158918857574,
      "learning_rate": 2.3655555555555555e-05,
      "loss": 0.0036,
      "step": 47420
    },
    {
      "epoch": 2.635,
      "grad_norm": 0.5053027272224426,
      "learning_rate": 2.365e-05,
      "loss": 0.0023,
      "step": 47430
    },
    {
      "epoch": 2.6355555555555554,
      "grad_norm": 0.2085796743631363,
      "learning_rate": 2.3644444444444446e-05,
      "loss": 0.0022,
      "step": 47440
    },
    {
      "epoch": 2.636111111111111,
      "grad_norm": 0.06131281703710556,
      "learning_rate": 2.363888888888889e-05,
      "loss": 0.0029,
      "step": 47450
    },
    {
      "epoch": 2.6366666666666667,
      "grad_norm": 0.4664963185787201,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0024,
      "step": 47460
    },
    {
      "epoch": 2.6372222222222224,
      "grad_norm": 0.17928123474121094,
      "learning_rate": 2.362777777777778e-05,
      "loss": 0.0028,
      "step": 47470
    },
    {
      "epoch": 2.637777777777778,
      "grad_norm": 0.03498224914073944,
      "learning_rate": 2.3622222222222223e-05,
      "loss": 0.0041,
      "step": 47480
    },
    {
      "epoch": 2.638333333333333,
      "grad_norm": 0.20847059786319733,
      "learning_rate": 2.3616666666666667e-05,
      "loss": 0.0033,
      "step": 47490
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 0.08979056775569916,
      "learning_rate": 2.361111111111111e-05,
      "loss": 0.0029,
      "step": 47500
    },
    {
      "epoch": 2.6394444444444445,
      "grad_norm": 0.2712119519710541,
      "learning_rate": 2.3605555555555554e-05,
      "loss": 0.0039,
      "step": 47510
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.21844901144504547,
      "learning_rate": 2.36e-05,
      "loss": 0.0034,
      "step": 47520
    },
    {
      "epoch": 2.6405555555555553,
      "grad_norm": 0.18288597464561462,
      "learning_rate": 2.3594444444444448e-05,
      "loss": 0.0031,
      "step": 47530
    },
    {
      "epoch": 2.641111111111111,
      "grad_norm": 0.38839855790138245,
      "learning_rate": 2.358888888888889e-05,
      "loss": 0.0051,
      "step": 47540
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 0.1957891583442688,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.0027,
      "step": 47550
    },
    {
      "epoch": 2.6422222222222222,
      "grad_norm": 0.11990372836589813,
      "learning_rate": 2.357777777777778e-05,
      "loss": 0.0019,
      "step": 47560
    },
    {
      "epoch": 2.642777777777778,
      "grad_norm": 0.2091226726770401,
      "learning_rate": 2.3572222222222222e-05,
      "loss": 0.0049,
      "step": 47570
    },
    {
      "epoch": 2.6433333333333335,
      "grad_norm": 0.21246051788330078,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0026,
      "step": 47580
    },
    {
      "epoch": 2.6438888888888887,
      "grad_norm": 0.2976212501525879,
      "learning_rate": 2.3561111111111113e-05,
      "loss": 0.0036,
      "step": 47590
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 0.38727423548698425,
      "learning_rate": 2.3555555555555556e-05,
      "loss": 0.003,
      "step": 47600
    },
    {
      "epoch": 2.645,
      "grad_norm": 0.14877894520759583,
      "learning_rate": 2.355e-05,
      "loss": 0.0032,
      "step": 47610
    },
    {
      "epoch": 2.6455555555555557,
      "grad_norm": 0.2681289315223694,
      "learning_rate": 2.3544444444444447e-05,
      "loss": 0.0032,
      "step": 47620
    },
    {
      "epoch": 2.646111111111111,
      "grad_norm": 0.010424395091831684,
      "learning_rate": 2.353888888888889e-05,
      "loss": 0.0029,
      "step": 47630
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.06659423559904099,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0042,
      "step": 47640
    },
    {
      "epoch": 2.647222222222222,
      "grad_norm": 0.17974048852920532,
      "learning_rate": 2.3527777777777777e-05,
      "loss": 0.0024,
      "step": 47650
    },
    {
      "epoch": 2.647777777777778,
      "grad_norm": 0.21088624000549316,
      "learning_rate": 2.3522222222222224e-05,
      "loss": 0.0025,
      "step": 47660
    },
    {
      "epoch": 2.6483333333333334,
      "grad_norm": 0.3868776261806488,
      "learning_rate": 2.3516666666666668e-05,
      "loss": 0.0033,
      "step": 47670
    },
    {
      "epoch": 2.648888888888889,
      "grad_norm": 0.06156120449304581,
      "learning_rate": 2.351111111111111e-05,
      "loss": 0.0044,
      "step": 47680
    },
    {
      "epoch": 2.6494444444444447,
      "grad_norm": 0.5366359353065491,
      "learning_rate": 2.3505555555555555e-05,
      "loss": 0.003,
      "step": 47690
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.535602867603302,
      "learning_rate": 2.35e-05,
      "loss": 0.0036,
      "step": 47700
    },
    {
      "epoch": 2.6505555555555556,
      "grad_norm": 0.2678343951702118,
      "learning_rate": 2.3494444444444446e-05,
      "loss": 0.0017,
      "step": 47710
    },
    {
      "epoch": 2.651111111111111,
      "grad_norm": 0.2810916304588318,
      "learning_rate": 2.3488888888888893e-05,
      "loss": 0.0024,
      "step": 47720
    },
    {
      "epoch": 2.6516666666666664,
      "grad_norm": 0.32774999737739563,
      "learning_rate": 2.3483333333333336e-05,
      "loss": 0.0024,
      "step": 47730
    },
    {
      "epoch": 2.652222222222222,
      "grad_norm": 0.2974645495414734,
      "learning_rate": 2.347777777777778e-05,
      "loss": 0.0031,
      "step": 47740
    },
    {
      "epoch": 2.6527777777777777,
      "grad_norm": 0.03254105523228645,
      "learning_rate": 2.3472222222222223e-05,
      "loss": 0.0038,
      "step": 47750
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.2713513970375061,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0021,
      "step": 47760
    },
    {
      "epoch": 2.653888888888889,
      "grad_norm": 0.733091413974762,
      "learning_rate": 2.346111111111111e-05,
      "loss": 0.0029,
      "step": 47770
    },
    {
      "epoch": 2.6544444444444446,
      "grad_norm": 0.14934441447257996,
      "learning_rate": 2.3455555555555557e-05,
      "loss": 0.0034,
      "step": 47780
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 0.3868636190891266,
      "learning_rate": 2.345e-05,
      "loss": 0.0028,
      "step": 47790
    },
    {
      "epoch": 2.6555555555555554,
      "grad_norm": 0.011341609060764313,
      "learning_rate": 2.3444444444444448e-05,
      "loss": 0.0039,
      "step": 47800
    },
    {
      "epoch": 2.656111111111111,
      "grad_norm": 0.32693296670913696,
      "learning_rate": 2.343888888888889e-05,
      "loss": 0.0026,
      "step": 47810
    },
    {
      "epoch": 2.6566666666666667,
      "grad_norm": 0.6621853113174438,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.003,
      "step": 47820
    },
    {
      "epoch": 2.6572222222222224,
      "grad_norm": 0.14938703179359436,
      "learning_rate": 2.342777777777778e-05,
      "loss": 0.0024,
      "step": 47830
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 0.20836114883422852,
      "learning_rate": 2.3422222222222222e-05,
      "loss": 0.003,
      "step": 47840
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 0.03186338022351265,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.003,
      "step": 47850
    },
    {
      "epoch": 2.658888888888889,
      "grad_norm": 0.31365805864334106,
      "learning_rate": 2.3411111111111112e-05,
      "loss": 0.0025,
      "step": 47860
    },
    {
      "epoch": 2.6594444444444445,
      "grad_norm": 0.21301783621311188,
      "learning_rate": 2.3405555555555556e-05,
      "loss": 0.003,
      "step": 47870
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.0254086684435606,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0022,
      "step": 47880
    },
    {
      "epoch": 2.660555555555556,
      "grad_norm": 0.4157194495201111,
      "learning_rate": 2.3394444444444447e-05,
      "loss": 0.0045,
      "step": 47890
    },
    {
      "epoch": 2.661111111111111,
      "grad_norm": 0.5062252879142761,
      "learning_rate": 2.338888888888889e-05,
      "loss": 0.0025,
      "step": 47900
    },
    {
      "epoch": 2.6616666666666666,
      "grad_norm": 0.11935663223266602,
      "learning_rate": 2.3383333333333334e-05,
      "loss": 0.0039,
      "step": 47910
    },
    {
      "epoch": 2.6622222222222223,
      "grad_norm": 0.011415584944188595,
      "learning_rate": 2.337777777777778e-05,
      "loss": 0.0028,
      "step": 47920
    },
    {
      "epoch": 2.662777777777778,
      "grad_norm": 0.08973090350627899,
      "learning_rate": 2.3372222222222224e-05,
      "loss": 0.003,
      "step": 47930
    },
    {
      "epoch": 2.663333333333333,
      "grad_norm": 0.20893001556396484,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0036,
      "step": 47940
    },
    {
      "epoch": 2.6638888888888888,
      "grad_norm": 0.0316942073404789,
      "learning_rate": 2.336111111111111e-05,
      "loss": 0.0043,
      "step": 47950
    },
    {
      "epoch": 2.6644444444444444,
      "grad_norm": 0.32735833525657654,
      "learning_rate": 2.3355555555555555e-05,
      "loss": 0.0026,
      "step": 47960
    },
    {
      "epoch": 2.665,
      "grad_norm": 0.13058869540691376,
      "learning_rate": 2.3350000000000002e-05,
      "loss": 0.0032,
      "step": 47970
    },
    {
      "epoch": 2.6655555555555557,
      "grad_norm": 0.03923019394278526,
      "learning_rate": 2.3344444444444445e-05,
      "loss": 0.0036,
      "step": 47980
    },
    {
      "epoch": 2.6661111111111113,
      "grad_norm": 0.07898885756731033,
      "learning_rate": 2.3338888888888892e-05,
      "loss": 0.0024,
      "step": 47990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.32469290494918823,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0028,
      "step": 48000
    },
    {
      "epoch": 2.667222222222222,
      "grad_norm": 0.1193213015794754,
      "learning_rate": 2.332777777777778e-05,
      "loss": 0.0027,
      "step": 48010
    },
    {
      "epoch": 2.667777777777778,
      "grad_norm": 0.4160284698009491,
      "learning_rate": 2.3322222222222223e-05,
      "loss": 0.0029,
      "step": 48020
    },
    {
      "epoch": 2.6683333333333334,
      "grad_norm": 0.44593343138694763,
      "learning_rate": 2.3316666666666666e-05,
      "loss": 0.0032,
      "step": 48030
    },
    {
      "epoch": 2.6688888888888886,
      "grad_norm": 0.3861457109451294,
      "learning_rate": 2.331111111111111e-05,
      "loss": 0.0046,
      "step": 48040
    },
    {
      "epoch": 2.6694444444444443,
      "grad_norm": 0.41617077589035034,
      "learning_rate": 2.3305555555555557e-05,
      "loss": 0.0031,
      "step": 48050
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.2684118151664734,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0026,
      "step": 48060
    },
    {
      "epoch": 2.6705555555555556,
      "grad_norm": 0.2375802993774414,
      "learning_rate": 2.3294444444444447e-05,
      "loss": 0.0028,
      "step": 48070
    },
    {
      "epoch": 2.671111111111111,
      "grad_norm": 0.010608364827930927,
      "learning_rate": 2.328888888888889e-05,
      "loss": 0.0029,
      "step": 48080
    },
    {
      "epoch": 2.671666666666667,
      "grad_norm": 0.31040531396865845,
      "learning_rate": 2.3283333333333335e-05,
      "loss": 0.003,
      "step": 48090
    },
    {
      "epoch": 2.6722222222222225,
      "grad_norm": 0.03391042724251747,
      "learning_rate": 2.3277777777777778e-05,
      "loss": 0.0031,
      "step": 48100
    },
    {
      "epoch": 2.6727777777777777,
      "grad_norm": 0.2367745041847229,
      "learning_rate": 2.3272222222222222e-05,
      "loss": 0.0045,
      "step": 48110
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.17986539006233215,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0034,
      "step": 48120
    },
    {
      "epoch": 2.673888888888889,
      "grad_norm": 0.4460064768791199,
      "learning_rate": 2.3261111111111112e-05,
      "loss": 0.0037,
      "step": 48130
    },
    {
      "epoch": 2.674444444444444,
      "grad_norm": 0.3866734802722931,
      "learning_rate": 2.3255555555555556e-05,
      "loss": 0.0029,
      "step": 48140
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.11969173699617386,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.0023,
      "step": 48150
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 0.1488889455795288,
      "learning_rate": 2.3244444444444446e-05,
      "loss": 0.0028,
      "step": 48160
    },
    {
      "epoch": 2.676111111111111,
      "grad_norm": 0.2979544699192047,
      "learning_rate": 2.323888888888889e-05,
      "loss": 0.0029,
      "step": 48170
    },
    {
      "epoch": 2.6766666666666667,
      "grad_norm": 0.3276611864566803,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.0041,
      "step": 48180
    },
    {
      "epoch": 2.6772222222222224,
      "grad_norm": 0.1189587190747261,
      "learning_rate": 2.322777777777778e-05,
      "loss": 0.0025,
      "step": 48190
    },
    {
      "epoch": 2.677777777777778,
      "grad_norm": 0.1191183403134346,
      "learning_rate": 2.3222222222222224e-05,
      "loss": 0.0031,
      "step": 48200
    },
    {
      "epoch": 2.6783333333333332,
      "grad_norm": 0.29772016406059265,
      "learning_rate": 2.3216666666666667e-05,
      "loss": 0.0028,
      "step": 48210
    },
    {
      "epoch": 2.678888888888889,
      "grad_norm": 0.06084313243627548,
      "learning_rate": 2.321111111111111e-05,
      "loss": 0.004,
      "step": 48220
    },
    {
      "epoch": 2.6794444444444445,
      "grad_norm": 0.1510763317346573,
      "learning_rate": 2.3205555555555555e-05,
      "loss": 0.0037,
      "step": 48230
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.35723158717155457,
      "learning_rate": 2.32e-05,
      "loss": 0.0025,
      "step": 48240
    },
    {
      "epoch": 2.6805555555555554,
      "grad_norm": 0.03709261119365692,
      "learning_rate": 2.3194444444444445e-05,
      "loss": 0.0041,
      "step": 48250
    },
    {
      "epoch": 2.681111111111111,
      "grad_norm": 0.35706809163093567,
      "learning_rate": 2.3188888888888892e-05,
      "loss": 0.0029,
      "step": 48260
    },
    {
      "epoch": 2.6816666666666666,
      "grad_norm": 0.06279226392507553,
      "learning_rate": 2.3183333333333336e-05,
      "loss": 0.0039,
      "step": 48270
    },
    {
      "epoch": 2.6822222222222223,
      "grad_norm": 0.09093601256608963,
      "learning_rate": 2.317777777777778e-05,
      "loss": 0.0028,
      "step": 48280
    },
    {
      "epoch": 2.682777777777778,
      "grad_norm": 0.27759411931037903,
      "learning_rate": 2.3172222222222223e-05,
      "loss": 0.0035,
      "step": 48290
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.15370799601078033,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0034,
      "step": 48300
    },
    {
      "epoch": 2.6838888888888888,
      "grad_norm": 0.26744434237480164,
      "learning_rate": 2.316111111111111e-05,
      "loss": 0.0023,
      "step": 48310
    },
    {
      "epoch": 2.6844444444444444,
      "grad_norm": 0.06081007421016693,
      "learning_rate": 2.3155555555555557e-05,
      "loss": 0.0029,
      "step": 48320
    },
    {
      "epoch": 2.685,
      "grad_norm": 0.3035253584384918,
      "learning_rate": 2.3150000000000004e-05,
      "loss": 0.003,
      "step": 48330
    },
    {
      "epoch": 2.6855555555555557,
      "grad_norm": 0.29743343591690063,
      "learning_rate": 2.3144444444444447e-05,
      "loss": 0.0044,
      "step": 48340
    },
    {
      "epoch": 2.686111111111111,
      "grad_norm": 0.6473338603973389,
      "learning_rate": 2.313888888888889e-05,
      "loss": 0.0039,
      "step": 48350
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.016448605805635452,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0035,
      "step": 48360
    },
    {
      "epoch": 2.687222222222222,
      "grad_norm": 0.032970137894153595,
      "learning_rate": 2.3127777777777778e-05,
      "loss": 0.0026,
      "step": 48370
    },
    {
      "epoch": 2.687777777777778,
      "grad_norm": 0.016259152442216873,
      "learning_rate": 2.312222222222222e-05,
      "loss": 0.0025,
      "step": 48380
    },
    {
      "epoch": 2.6883333333333335,
      "grad_norm": 0.06042651832103729,
      "learning_rate": 2.311666666666667e-05,
      "loss": 0.0055,
      "step": 48390
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 0.17871583998203278,
      "learning_rate": 2.3111111111111112e-05,
      "loss": 0.0031,
      "step": 48400
    },
    {
      "epoch": 2.6894444444444443,
      "grad_norm": 0.18381622433662415,
      "learning_rate": 2.3105555555555556e-05,
      "loss": 0.0037,
      "step": 48410
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.0311010479927063,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.002,
      "step": 48420
    },
    {
      "epoch": 2.6905555555555556,
      "grad_norm": 0.14940260350704193,
      "learning_rate": 2.3094444444444446e-05,
      "loss": 0.0017,
      "step": 48430
    },
    {
      "epoch": 2.6911111111111112,
      "grad_norm": 0.03238567337393761,
      "learning_rate": 2.308888888888889e-05,
      "loss": 0.0041,
      "step": 48440
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 0.27879172563552856,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.0033,
      "step": 48450
    },
    {
      "epoch": 2.692222222222222,
      "grad_norm": 0.6687832474708557,
      "learning_rate": 2.307777777777778e-05,
      "loss": 0.0027,
      "step": 48460
    },
    {
      "epoch": 2.6927777777777777,
      "grad_norm": 0.2701137363910675,
      "learning_rate": 2.3072222222222224e-05,
      "loss": 0.0041,
      "step": 48470
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.35698768496513367,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0032,
      "step": 48480
    },
    {
      "epoch": 2.693888888888889,
      "grad_norm": 0.754463791847229,
      "learning_rate": 2.306111111111111e-05,
      "loss": 0.0035,
      "step": 48490
    },
    {
      "epoch": 2.6944444444444446,
      "grad_norm": 0.26779791712760925,
      "learning_rate": 2.3055555555555558e-05,
      "loss": 0.0036,
      "step": 48500
    },
    {
      "epoch": 2.695,
      "grad_norm": 0.5234976410865784,
      "learning_rate": 2.305e-05,
      "loss": 0.0028,
      "step": 48510
    },
    {
      "epoch": 2.6955555555555555,
      "grad_norm": 0.24897311627864838,
      "learning_rate": 2.3044444444444445e-05,
      "loss": 0.004,
      "step": 48520
    },
    {
      "epoch": 2.696111111111111,
      "grad_norm": 0.23801368474960327,
      "learning_rate": 2.3038888888888892e-05,
      "loss": 0.0039,
      "step": 48530
    },
    {
      "epoch": 2.6966666666666668,
      "grad_norm": 0.19912776350975037,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0047,
      "step": 48540
    },
    {
      "epoch": 2.697222222222222,
      "grad_norm": 0.03472808375954628,
      "learning_rate": 2.302777777777778e-05,
      "loss": 0.0033,
      "step": 48550
    },
    {
      "epoch": 2.6977777777777776,
      "grad_norm": 0.360642671585083,
      "learning_rate": 2.3022222222222222e-05,
      "loss": 0.003,
      "step": 48560
    },
    {
      "epoch": 2.6983333333333333,
      "grad_norm": 0.14937691390514374,
      "learning_rate": 2.3016666666666666e-05,
      "loss": 0.0029,
      "step": 48570
    },
    {
      "epoch": 2.698888888888889,
      "grad_norm": 0.03149911016225815,
      "learning_rate": 2.301111111111111e-05,
      "loss": 0.0029,
      "step": 48580
    },
    {
      "epoch": 2.6994444444444445,
      "grad_norm": 0.20829804241657257,
      "learning_rate": 2.3005555555555556e-05,
      "loss": 0.0042,
      "step": 48590
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.11942823231220245,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0051,
      "step": 48600
    },
    {
      "epoch": 2.700555555555556,
      "grad_norm": 0.43817177414894104,
      "learning_rate": 2.2994444444444447e-05,
      "loss": 0.0023,
      "step": 48610
    },
    {
      "epoch": 2.701111111111111,
      "grad_norm": 0.3570316433906555,
      "learning_rate": 2.298888888888889e-05,
      "loss": 0.0019,
      "step": 48620
    },
    {
      "epoch": 2.7016666666666667,
      "grad_norm": 0.24280402064323425,
      "learning_rate": 2.2983333333333334e-05,
      "loss": 0.0025,
      "step": 48630
    },
    {
      "epoch": 2.7022222222222223,
      "grad_norm": 0.15126222372055054,
      "learning_rate": 2.2977777777777778e-05,
      "loss": 0.0037,
      "step": 48640
    },
    {
      "epoch": 2.7027777777777775,
      "grad_norm": 0.2704019248485565,
      "learning_rate": 2.297222222222222e-05,
      "loss": 0.0044,
      "step": 48650
    },
    {
      "epoch": 2.703333333333333,
      "grad_norm": 0.6841462254524231,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.0025,
      "step": 48660
    },
    {
      "epoch": 2.703888888888889,
      "grad_norm": 0.2767394483089447,
      "learning_rate": 2.296111111111111e-05,
      "loss": 0.0036,
      "step": 48670
    },
    {
      "epoch": 2.7044444444444444,
      "grad_norm": 0.3869877755641937,
      "learning_rate": 2.295555555555556e-05,
      "loss": 0.002,
      "step": 48680
    },
    {
      "epoch": 2.705,
      "grad_norm": 0.7141008377075195,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 0.003,
      "step": 48690
    },
    {
      "epoch": 2.7055555555555557,
      "grad_norm": 0.27704155445098877,
      "learning_rate": 2.2944444444444446e-05,
      "loss": 0.0025,
      "step": 48700
    },
    {
      "epoch": 2.7061111111111114,
      "grad_norm": 0.010854304768145084,
      "learning_rate": 2.293888888888889e-05,
      "loss": 0.0028,
      "step": 48710
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.14926978945732117,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0024,
      "step": 48720
    },
    {
      "epoch": 2.707222222222222,
      "grad_norm": 0.23790223896503448,
      "learning_rate": 2.292777777777778e-05,
      "loss": 0.0019,
      "step": 48730
    },
    {
      "epoch": 2.707777777777778,
      "grad_norm": 0.02029484137892723,
      "learning_rate": 2.2922222222222223e-05,
      "loss": 0.0041,
      "step": 48740
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 0.12509392201900482,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.0034,
      "step": 48750
    },
    {
      "epoch": 2.7088888888888887,
      "grad_norm": 0.20852288603782654,
      "learning_rate": 2.291111111111111e-05,
      "loss": 0.0035,
      "step": 48760
    },
    {
      "epoch": 2.7094444444444443,
      "grad_norm": 0.0072857169434428215,
      "learning_rate": 2.2905555555555557e-05,
      "loss": 0.0029,
      "step": 48770
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.11939282715320587,
      "learning_rate": 2.29e-05,
      "loss": 0.0032,
      "step": 48780
    },
    {
      "epoch": 2.7105555555555556,
      "grad_norm": 0.248482346534729,
      "learning_rate": 2.2894444444444445e-05,
      "loss": 0.0034,
      "step": 48790
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.1492927223443985,
      "learning_rate": 2.288888888888889e-05,
      "loss": 0.0022,
      "step": 48800
    },
    {
      "epoch": 2.711666666666667,
      "grad_norm": 0.06236952170729637,
      "learning_rate": 2.2883333333333335e-05,
      "loss": 0.0041,
      "step": 48810
    },
    {
      "epoch": 2.712222222222222,
      "grad_norm": 0.2948501706123352,
      "learning_rate": 2.287777777777778e-05,
      "loss": 0.0041,
      "step": 48820
    },
    {
      "epoch": 2.7127777777777777,
      "grad_norm": 0.3869069218635559,
      "learning_rate": 2.2872222222222222e-05,
      "loss": 0.0032,
      "step": 48830
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.06095660850405693,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0039,
      "step": 48840
    },
    {
      "epoch": 2.713888888888889,
      "grad_norm": 0.2674979865550995,
      "learning_rate": 2.286111111111111e-05,
      "loss": 0.0033,
      "step": 48850
    },
    {
      "epoch": 2.714444444444444,
      "grad_norm": 0.29704275727272034,
      "learning_rate": 2.2855555555555556e-05,
      "loss": 0.0025,
      "step": 48860
    },
    {
      "epoch": 2.715,
      "grad_norm": 0.09083744883537292,
      "learning_rate": 2.2850000000000003e-05,
      "loss": 0.0027,
      "step": 48870
    },
    {
      "epoch": 2.7155555555555555,
      "grad_norm": 0.34203433990478516,
      "learning_rate": 2.2844444444444447e-05,
      "loss": 0.0027,
      "step": 48880
    },
    {
      "epoch": 2.716111111111111,
      "grad_norm": 0.1508662849664688,
      "learning_rate": 2.283888888888889e-05,
      "loss": 0.0024,
      "step": 48890
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 0.1668197214603424,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0025,
      "step": 48900
    },
    {
      "epoch": 2.7172222222222224,
      "grad_norm": 0.09026642888784409,
      "learning_rate": 2.2827777777777777e-05,
      "loss": 0.0029,
      "step": 48910
    },
    {
      "epoch": 2.7177777777777776,
      "grad_norm": 0.23822595179080963,
      "learning_rate": 2.282222222222222e-05,
      "loss": 0.0029,
      "step": 48920
    },
    {
      "epoch": 2.7183333333333333,
      "grad_norm": 0.11968430876731873,
      "learning_rate": 2.2816666666666668e-05,
      "loss": 0.0022,
      "step": 48930
    },
    {
      "epoch": 2.718888888888889,
      "grad_norm": 0.47336170077323914,
      "learning_rate": 2.281111111111111e-05,
      "loss": 0.0039,
      "step": 48940
    },
    {
      "epoch": 2.7194444444444446,
      "grad_norm": 0.012859384529292583,
      "learning_rate": 2.280555555555556e-05,
      "loss": 0.004,
      "step": 48950
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.5786871314048767,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0043,
      "step": 48960
    },
    {
      "epoch": 2.7205555555555554,
      "grad_norm": 0.20806588232517242,
      "learning_rate": 2.2794444444444445e-05,
      "loss": 0.0024,
      "step": 48970
    },
    {
      "epoch": 2.721111111111111,
      "grad_norm": 0.014774693176150322,
      "learning_rate": 2.278888888888889e-05,
      "loss": 0.0038,
      "step": 48980
    },
    {
      "epoch": 2.7216666666666667,
      "grad_norm": 0.17945200204849243,
      "learning_rate": 2.2783333333333336e-05,
      "loss": 0.002,
      "step": 48990
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 0.02320076711475849,
      "learning_rate": 2.277777777777778e-05,
      "loss": 0.0025,
      "step": 49000
    },
    {
      "epoch": 2.722777777777778,
      "grad_norm": 0.26047566533088684,
      "learning_rate": 2.2772222222222223e-05,
      "loss": 0.0038,
      "step": 49010
    },
    {
      "epoch": 2.7233333333333336,
      "grad_norm": 0.06283555924892426,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.0046,
      "step": 49020
    },
    {
      "epoch": 2.723888888888889,
      "grad_norm": 0.27777373790740967,
      "learning_rate": 2.276111111111111e-05,
      "loss": 0.0034,
      "step": 49030
    },
    {
      "epoch": 2.7244444444444444,
      "grad_norm": 0.2676546275615692,
      "learning_rate": 2.2755555555555557e-05,
      "loss": 0.003,
      "step": 49040
    },
    {
      "epoch": 2.725,
      "grad_norm": 0.2977270483970642,
      "learning_rate": 2.275e-05,
      "loss": 0.0031,
      "step": 49050
    },
    {
      "epoch": 2.7255555555555553,
      "grad_norm": 0.2680397629737854,
      "learning_rate": 2.2744444444444448e-05,
      "loss": 0.0037,
      "step": 49060
    },
    {
      "epoch": 2.726111111111111,
      "grad_norm": 0.208979532122612,
      "learning_rate": 2.273888888888889e-05,
      "loss": 0.0025,
      "step": 49070
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.18776819109916687,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0045,
      "step": 49080
    },
    {
      "epoch": 2.727222222222222,
      "grad_norm": 0.20848974585533142,
      "learning_rate": 2.272777777777778e-05,
      "loss": 0.004,
      "step": 49090
    },
    {
      "epoch": 2.727777777777778,
      "grad_norm": 0.01402320247143507,
      "learning_rate": 2.2722222222222222e-05,
      "loss": 0.0034,
      "step": 49100
    },
    {
      "epoch": 2.7283333333333335,
      "grad_norm": 0.44563212990760803,
      "learning_rate": 2.2716666666666665e-05,
      "loss": 0.0032,
      "step": 49110
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 0.06094403937458992,
      "learning_rate": 2.2711111111111112e-05,
      "loss": 0.0037,
      "step": 49120
    },
    {
      "epoch": 2.7294444444444443,
      "grad_norm": 0.20006044209003448,
      "learning_rate": 2.270555555555556e-05,
      "loss": 0.0045,
      "step": 49130
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.11764422804117203,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0032,
      "step": 49140
    },
    {
      "epoch": 2.7305555555555556,
      "grad_norm": 0.032647401094436646,
      "learning_rate": 2.2694444444444446e-05,
      "loss": 0.0036,
      "step": 49150
    },
    {
      "epoch": 2.7311111111111113,
      "grad_norm": 0.2681335210800171,
      "learning_rate": 2.268888888888889e-05,
      "loss": 0.0034,
      "step": 49160
    },
    {
      "epoch": 2.7316666666666665,
      "grad_norm": 0.2676597535610199,
      "learning_rate": 2.2683333333333334e-05,
      "loss": 0.0031,
      "step": 49170
    },
    {
      "epoch": 2.732222222222222,
      "grad_norm": 0.11925362050533295,
      "learning_rate": 2.2677777777777777e-05,
      "loss": 0.0048,
      "step": 49180
    },
    {
      "epoch": 2.7327777777777778,
      "grad_norm": 0.09050004184246063,
      "learning_rate": 2.2672222222222224e-05,
      "loss": 0.0033,
      "step": 49190
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.2681533098220825,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0033,
      "step": 49200
    },
    {
      "epoch": 2.733888888888889,
      "grad_norm": 0.42375168204307556,
      "learning_rate": 2.2661111111111115e-05,
      "loss": 0.0036,
      "step": 49210
    },
    {
      "epoch": 2.7344444444444447,
      "grad_norm": 0.04637061804533005,
      "learning_rate": 2.2655555555555558e-05,
      "loss": 0.0031,
      "step": 49220
    },
    {
      "epoch": 2.735,
      "grad_norm": 0.06213545426726341,
      "learning_rate": 2.265e-05,
      "loss": 0.0037,
      "step": 49230
    },
    {
      "epoch": 2.7355555555555555,
      "grad_norm": 0.5175487399101257,
      "learning_rate": 2.2644444444444445e-05,
      "loss": 0.0024,
      "step": 49240
    },
    {
      "epoch": 2.736111111111111,
      "grad_norm": 0.12090688198804855,
      "learning_rate": 2.263888888888889e-05,
      "loss": 0.0029,
      "step": 49250
    },
    {
      "epoch": 2.736666666666667,
      "grad_norm": 0.20909199118614197,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.0035,
      "step": 49260
    },
    {
      "epoch": 2.737222222222222,
      "grad_norm": 0.23906309902668,
      "learning_rate": 2.262777777777778e-05,
      "loss": 0.0034,
      "step": 49270
    },
    {
      "epoch": 2.7377777777777776,
      "grad_norm": 0.35620516538619995,
      "learning_rate": 2.2622222222222223e-05,
      "loss": 0.0018,
      "step": 49280
    },
    {
      "epoch": 2.7383333333333333,
      "grad_norm": 0.2082466036081314,
      "learning_rate": 2.2616666666666666e-05,
      "loss": 0.0027,
      "step": 49290
    },
    {
      "epoch": 2.738888888888889,
      "grad_norm": 0.3638977110385895,
      "learning_rate": 2.2611111111111113e-05,
      "loss": 0.003,
      "step": 49300
    },
    {
      "epoch": 2.7394444444444446,
      "grad_norm": 0.17427244782447815,
      "learning_rate": 2.2605555555555557e-05,
      "loss": 0.0027,
      "step": 49310
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.28811171650886536,
      "learning_rate": 2.26e-05,
      "loss": 0.0048,
      "step": 49320
    },
    {
      "epoch": 2.7405555555555554,
      "grad_norm": 0.06080291420221329,
      "learning_rate": 2.2594444444444447e-05,
      "loss": 0.0028,
      "step": 49330
    },
    {
      "epoch": 2.741111111111111,
      "grad_norm": 0.20859967172145844,
      "learning_rate": 2.258888888888889e-05,
      "loss": 0.0037,
      "step": 49340
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 0.2873421013355255,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.0031,
      "step": 49350
    },
    {
      "epoch": 2.7422222222222223,
      "grad_norm": 0.06356898695230484,
      "learning_rate": 2.2577777777777778e-05,
      "loss": 0.0032,
      "step": 49360
    },
    {
      "epoch": 2.7427777777777775,
      "grad_norm": 0.1781838834285736,
      "learning_rate": 2.257222222222222e-05,
      "loss": 0.0027,
      "step": 49370
    },
    {
      "epoch": 2.743333333333333,
      "grad_norm": 0.30287784337997437,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0037,
      "step": 49380
    },
    {
      "epoch": 2.743888888888889,
      "grad_norm": 0.23708173632621765,
      "learning_rate": 2.2561111111111112e-05,
      "loss": 0.0031,
      "step": 49390
    },
    {
      "epoch": 2.7444444444444445,
      "grad_norm": 0.26757562160491943,
      "learning_rate": 2.255555555555556e-05,
      "loss": 0.0034,
      "step": 49400
    },
    {
      "epoch": 2.745,
      "grad_norm": 0.06179239973425865,
      "learning_rate": 2.2550000000000003e-05,
      "loss": 0.002,
      "step": 49410
    },
    {
      "epoch": 2.7455555555555557,
      "grad_norm": 0.179865300655365,
      "learning_rate": 2.2544444444444446e-05,
      "loss": 0.003,
      "step": 49420
    },
    {
      "epoch": 2.746111111111111,
      "grad_norm": 0.2784444987773895,
      "learning_rate": 2.253888888888889e-05,
      "loss": 0.0036,
      "step": 49430
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.2682642638683319,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0027,
      "step": 49440
    },
    {
      "epoch": 2.7472222222222222,
      "grad_norm": 0.1486647129058838,
      "learning_rate": 2.2527777777777777e-05,
      "loss": 0.0036,
      "step": 49450
    },
    {
      "epoch": 2.747777777777778,
      "grad_norm": 0.3101741373538971,
      "learning_rate": 2.2522222222222224e-05,
      "loss": 0.0046,
      "step": 49460
    },
    {
      "epoch": 2.748333333333333,
      "grad_norm": 0.11973897367715836,
      "learning_rate": 2.2516666666666667e-05,
      "loss": 0.0032,
      "step": 49470
    },
    {
      "epoch": 2.7488888888888887,
      "grad_norm": 0.40016910433769226,
      "learning_rate": 2.2511111111111114e-05,
      "loss": 0.0028,
      "step": 49480
    },
    {
      "epoch": 2.7494444444444444,
      "grad_norm": 0.06119060888886452,
      "learning_rate": 2.2505555555555558e-05,
      "loss": 0.0031,
      "step": 49490
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.06048090383410454,
      "learning_rate": 2.25e-05,
      "loss": 0.0026,
      "step": 49500
    },
    {
      "epoch": 2.7505555555555556,
      "grad_norm": 0.09113194048404694,
      "learning_rate": 2.2494444444444445e-05,
      "loss": 0.0029,
      "step": 49510
    },
    {
      "epoch": 2.7511111111111113,
      "grad_norm": 0.06417541205883026,
      "learning_rate": 2.248888888888889e-05,
      "loss": 0.0037,
      "step": 49520
    },
    {
      "epoch": 2.751666666666667,
      "grad_norm": 0.16930168867111206,
      "learning_rate": 2.2483333333333335e-05,
      "loss": 0.0017,
      "step": 49530
    },
    {
      "epoch": 2.752222222222222,
      "grad_norm": 0.36222222447395325,
      "learning_rate": 2.247777777777778e-05,
      "loss": 0.0038,
      "step": 49540
    },
    {
      "epoch": 2.7527777777777778,
      "grad_norm": 0.4064640402793884,
      "learning_rate": 2.2472222222222223e-05,
      "loss": 0.0027,
      "step": 49550
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.12059998512268066,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0032,
      "step": 49560
    },
    {
      "epoch": 2.753888888888889,
      "grad_norm": 0.17210319638252258,
      "learning_rate": 2.2461111111111113e-05,
      "loss": 0.003,
      "step": 49570
    },
    {
      "epoch": 2.7544444444444443,
      "grad_norm": 0.12080191820859909,
      "learning_rate": 2.2455555555555557e-05,
      "loss": 0.0031,
      "step": 49580
    },
    {
      "epoch": 2.755,
      "grad_norm": 0.3271336853504181,
      "learning_rate": 2.245e-05,
      "loss": 0.0025,
      "step": 49590
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 0.11961010843515396,
      "learning_rate": 2.2444444444444447e-05,
      "loss": 0.0027,
      "step": 49600
    },
    {
      "epoch": 2.756111111111111,
      "grad_norm": 0.2090626358985901,
      "learning_rate": 2.243888888888889e-05,
      "loss": 0.0028,
      "step": 49610
    },
    {
      "epoch": 2.756666666666667,
      "grad_norm": 0.29749172925949097,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.0031,
      "step": 49620
    },
    {
      "epoch": 2.7572222222222225,
      "grad_norm": 0.14912907779216766,
      "learning_rate": 2.2427777777777778e-05,
      "loss": 0.0027,
      "step": 49630
    },
    {
      "epoch": 2.7577777777777777,
      "grad_norm": 0.2204556167125702,
      "learning_rate": 2.242222222222222e-05,
      "loss": 0.0036,
      "step": 49640
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 0.6845143437385559,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.0041,
      "step": 49650
    },
    {
      "epoch": 2.758888888888889,
      "grad_norm": 0.3627087473869324,
      "learning_rate": 2.2411111111111112e-05,
      "loss": 0.0033,
      "step": 49660
    },
    {
      "epoch": 2.7594444444444446,
      "grad_norm": 0.24184618890285492,
      "learning_rate": 2.240555555555556e-05,
      "loss": 0.0026,
      "step": 49670
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.17842116951942444,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0032,
      "step": 49680
    },
    {
      "epoch": 2.7605555555555554,
      "grad_norm": 0.41644421219825745,
      "learning_rate": 2.2394444444444446e-05,
      "loss": 0.0026,
      "step": 49690
    },
    {
      "epoch": 2.761111111111111,
      "grad_norm": 0.1204518973827362,
      "learning_rate": 2.238888888888889e-05,
      "loss": 0.0033,
      "step": 49700
    },
    {
      "epoch": 2.7616666666666667,
      "grad_norm": 0.008126404136419296,
      "learning_rate": 2.2383333333333333e-05,
      "loss": 0.004,
      "step": 49710
    },
    {
      "epoch": 2.7622222222222224,
      "grad_norm": 0.17827194929122925,
      "learning_rate": 2.2377777777777777e-05,
      "loss": 0.0029,
      "step": 49720
    },
    {
      "epoch": 2.762777777777778,
      "grad_norm": 0.0895073190331459,
      "learning_rate": 2.2372222222222224e-05,
      "loss": 0.0026,
      "step": 49730
    },
    {
      "epoch": 2.763333333333333,
      "grad_norm": 0.20821651816368103,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.0032,
      "step": 49740
    },
    {
      "epoch": 2.763888888888889,
      "grad_norm": 0.4914768636226654,
      "learning_rate": 2.2361111111111114e-05,
      "loss": 0.0028,
      "step": 49750
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 0.1792469322681427,
      "learning_rate": 2.2355555555555558e-05,
      "loss": 0.0033,
      "step": 49760
    },
    {
      "epoch": 2.765,
      "grad_norm": 0.1486242264509201,
      "learning_rate": 2.235e-05,
      "loss": 0.0034,
      "step": 49770
    },
    {
      "epoch": 2.7655555555555553,
      "grad_norm": 0.09207489341497421,
      "learning_rate": 2.2344444444444445e-05,
      "loss": 0.0027,
      "step": 49780
    },
    {
      "epoch": 2.766111111111111,
      "grad_norm": 0.38632819056510925,
      "learning_rate": 2.2338888888888888e-05,
      "loss": 0.0027,
      "step": 49790
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.03876837342977524,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0036,
      "step": 49800
    },
    {
      "epoch": 2.7672222222222222,
      "grad_norm": 0.1486281454563141,
      "learning_rate": 2.232777777777778e-05,
      "loss": 0.0042,
      "step": 49810
    },
    {
      "epoch": 2.767777777777778,
      "grad_norm": 0.23885008692741394,
      "learning_rate": 2.2322222222222222e-05,
      "loss": 0.0027,
      "step": 49820
    },
    {
      "epoch": 2.7683333333333335,
      "grad_norm": 0.09091438353061676,
      "learning_rate": 2.231666666666667e-05,
      "loss": 0.0038,
      "step": 49830
    },
    {
      "epoch": 2.7688888888888887,
      "grad_norm": 0.2680301368236542,
      "learning_rate": 2.2311111111111113e-05,
      "loss": 0.0032,
      "step": 49840
    },
    {
      "epoch": 2.7694444444444444,
      "grad_norm": 0.38703489303588867,
      "learning_rate": 2.2305555555555556e-05,
      "loss": 0.0024,
      "step": 49850
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.22900176048278809,
      "learning_rate": 2.23e-05,
      "loss": 0.0029,
      "step": 49860
    },
    {
      "epoch": 2.7705555555555557,
      "grad_norm": 0.034117769449949265,
      "learning_rate": 2.2294444444444447e-05,
      "loss": 0.0026,
      "step": 49870
    },
    {
      "epoch": 2.771111111111111,
      "grad_norm": 0.14887942373752594,
      "learning_rate": 2.228888888888889e-05,
      "loss": 0.0026,
      "step": 49880
    },
    {
      "epoch": 2.7716666666666665,
      "grad_norm": 0.301940381526947,
      "learning_rate": 2.2283333333333334e-05,
      "loss": 0.0028,
      "step": 49890
    },
    {
      "epoch": 2.772222222222222,
      "grad_norm": 0.09019774943590164,
      "learning_rate": 2.2277777777777778e-05,
      "loss": 0.0032,
      "step": 49900
    },
    {
      "epoch": 2.772777777777778,
      "grad_norm": 0.06287302821874619,
      "learning_rate": 2.227222222222222e-05,
      "loss": 0.0035,
      "step": 49910
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.26915279030799866,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0026,
      "step": 49920
    },
    {
      "epoch": 2.773888888888889,
      "grad_norm": 0.3880680501461029,
      "learning_rate": 2.226111111111111e-05,
      "loss": 0.0039,
      "step": 49930
    },
    {
      "epoch": 2.7744444444444447,
      "grad_norm": 0.32775014638900757,
      "learning_rate": 2.225555555555556e-05,
      "loss": 0.002,
      "step": 49940
    },
    {
      "epoch": 2.775,
      "grad_norm": 0.03134286031126976,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.004,
      "step": 49950
    },
    {
      "epoch": 2.7755555555555556,
      "grad_norm": 0.17871147394180298,
      "learning_rate": 2.2244444444444446e-05,
      "loss": 0.0025,
      "step": 49960
    },
    {
      "epoch": 2.776111111111111,
      "grad_norm": 0.2773675322532654,
      "learning_rate": 2.223888888888889e-05,
      "loss": 0.0022,
      "step": 49970
    },
    {
      "epoch": 2.7766666666666664,
      "grad_norm": 0.7807472348213196,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0036,
      "step": 49980
    },
    {
      "epoch": 2.777222222222222,
      "grad_norm": 0.47595202922821045,
      "learning_rate": 2.2227777777777776e-05,
      "loss": 0.0022,
      "step": 49990
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.16317613422870636,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.0029,
      "step": 50000
    },
    {
      "epoch": 2.7783333333333333,
      "grad_norm": 0.14946940541267395,
      "learning_rate": 2.221666666666667e-05,
      "loss": 0.0058,
      "step": 50010
    },
    {
      "epoch": 2.778888888888889,
      "grad_norm": 0.05975988134741783,
      "learning_rate": 2.2211111111111114e-05,
      "loss": 0.0028,
      "step": 50020
    },
    {
      "epoch": 2.7794444444444446,
      "grad_norm": 0.1412811130285263,
      "learning_rate": 2.2205555555555557e-05,
      "loss": 0.0028,
      "step": 50030
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.4856247305870056,
      "learning_rate": 2.22e-05,
      "loss": 0.0036,
      "step": 50040
    },
    {
      "epoch": 2.7805555555555554,
      "grad_norm": 0.26885005831718445,
      "learning_rate": 2.2194444444444444e-05,
      "loss": 0.0035,
      "step": 50050
    },
    {
      "epoch": 2.781111111111111,
      "grad_norm": 0.20833253860473633,
      "learning_rate": 2.2188888888888888e-05,
      "loss": 0.0028,
      "step": 50060
    },
    {
      "epoch": 2.7816666666666667,
      "grad_norm": 0.4757455885410309,
      "learning_rate": 2.2183333333333335e-05,
      "loss": 0.0027,
      "step": 50070
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 0.20313328504562378,
      "learning_rate": 2.217777777777778e-05,
      "loss": 0.0036,
      "step": 50080
    },
    {
      "epoch": 2.7827777777777776,
      "grad_norm": 0.5667770504951477,
      "learning_rate": 2.2172222222222222e-05,
      "loss": 0.0042,
      "step": 50090
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 0.3872078061103821,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0031,
      "step": 50100
    },
    {
      "epoch": 2.783888888888889,
      "grad_norm": 0.23896536231040955,
      "learning_rate": 2.2161111111111113e-05,
      "loss": 0.0024,
      "step": 50110
    },
    {
      "epoch": 2.7844444444444445,
      "grad_norm": 0.1202138289809227,
      "learning_rate": 2.2155555555555556e-05,
      "loss": 0.0026,
      "step": 50120
    },
    {
      "epoch": 2.785,
      "grad_norm": 0.08966559171676636,
      "learning_rate": 2.215e-05,
      "loss": 0.0043,
      "step": 50130
    },
    {
      "epoch": 2.785555555555556,
      "grad_norm": 0.03154481574892998,
      "learning_rate": 2.2144444444444447e-05,
      "loss": 0.0031,
      "step": 50140
    },
    {
      "epoch": 2.786111111111111,
      "grad_norm": 0.20810478925704956,
      "learning_rate": 2.213888888888889e-05,
      "loss": 0.0024,
      "step": 50150
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.07173856347799301,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0046,
      "step": 50160
    },
    {
      "epoch": 2.7872222222222223,
      "grad_norm": 0.19177165627479553,
      "learning_rate": 2.2127777777777777e-05,
      "loss": 0.0043,
      "step": 50170
    },
    {
      "epoch": 2.787777777777778,
      "grad_norm": 0.26721957325935364,
      "learning_rate": 2.212222222222222e-05,
      "loss": 0.0031,
      "step": 50180
    },
    {
      "epoch": 2.788333333333333,
      "grad_norm": 0.429375022649765,
      "learning_rate": 2.2116666666666668e-05,
      "loss": 0.0037,
      "step": 50190
    },
    {
      "epoch": 2.7888888888888888,
      "grad_norm": 0.3865506052970886,
      "learning_rate": 2.211111111111111e-05,
      "loss": 0.0013,
      "step": 50200
    },
    {
      "epoch": 2.7894444444444444,
      "grad_norm": 0.12751205265522003,
      "learning_rate": 2.2105555555555558e-05,
      "loss": 0.0026,
      "step": 50210
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.08959401398897171,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0022,
      "step": 50220
    },
    {
      "epoch": 2.7905555555555557,
      "grad_norm": 0.4169723391532898,
      "learning_rate": 2.2094444444444445e-05,
      "loss": 0.0027,
      "step": 50230
    },
    {
      "epoch": 2.7911111111111113,
      "grad_norm": 0.06136556342244148,
      "learning_rate": 2.208888888888889e-05,
      "loss": 0.0023,
      "step": 50240
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 0.11933720856904984,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.0043,
      "step": 50250
    },
    {
      "epoch": 2.792222222222222,
      "grad_norm": 0.14936459064483643,
      "learning_rate": 2.207777777777778e-05,
      "loss": 0.0032,
      "step": 50260
    },
    {
      "epoch": 2.792777777777778,
      "grad_norm": 0.3281943202018738,
      "learning_rate": 2.2072222222222223e-05,
      "loss": 0.0037,
      "step": 50270
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.32753413915634155,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0039,
      "step": 50280
    },
    {
      "epoch": 2.7938888888888886,
      "grad_norm": 0.11923126131296158,
      "learning_rate": 2.2061111111111114e-05,
      "loss": 0.0031,
      "step": 50290
    },
    {
      "epoch": 2.7944444444444443,
      "grad_norm": 0.29722100496292114,
      "learning_rate": 2.2055555555555557e-05,
      "loss": 0.0029,
      "step": 50300
    },
    {
      "epoch": 2.795,
      "grad_norm": 0.39693981409072876,
      "learning_rate": 2.205e-05,
      "loss": 0.0031,
      "step": 50310
    },
    {
      "epoch": 2.7955555555555556,
      "grad_norm": 0.08913865685462952,
      "learning_rate": 2.2044444444444444e-05,
      "loss": 0.003,
      "step": 50320
    },
    {
      "epoch": 2.796111111111111,
      "grad_norm": 0.4947662353515625,
      "learning_rate": 2.203888888888889e-05,
      "loss": 0.0043,
      "step": 50330
    },
    {
      "epoch": 2.796666666666667,
      "grad_norm": 0.344260036945343,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0032,
      "step": 50340
    },
    {
      "epoch": 2.7972222222222225,
      "grad_norm": 0.23517371714115143,
      "learning_rate": 2.2027777777777778e-05,
      "loss": 0.0035,
      "step": 50350
    },
    {
      "epoch": 2.7977777777777777,
      "grad_norm": 0.23751161992549896,
      "learning_rate": 2.2022222222222225e-05,
      "loss": 0.0029,
      "step": 50360
    },
    {
      "epoch": 2.7983333333333333,
      "grad_norm": 0.38549742102622986,
      "learning_rate": 2.201666666666667e-05,
      "loss": 0.0025,
      "step": 50370
    },
    {
      "epoch": 2.798888888888889,
      "grad_norm": 0.30197373032569885,
      "learning_rate": 2.2011111111111112e-05,
      "loss": 0.003,
      "step": 50380
    },
    {
      "epoch": 2.799444444444444,
      "grad_norm": 0.03265735134482384,
      "learning_rate": 2.2005555555555556e-05,
      "loss": 0.0036,
      "step": 50390
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.0405670627951622,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.003,
      "step": 50400
    },
    {
      "epoch": 2.8005555555555555,
      "grad_norm": 0.2714381814002991,
      "learning_rate": 2.1994444444444446e-05,
      "loss": 0.004,
      "step": 50410
    },
    {
      "epoch": 2.801111111111111,
      "grad_norm": 0.17861244082450867,
      "learning_rate": 2.198888888888889e-05,
      "loss": 0.0033,
      "step": 50420
    },
    {
      "epoch": 2.8016666666666667,
      "grad_norm": 0.5339661240577698,
      "learning_rate": 2.1983333333333333e-05,
      "loss": 0.0047,
      "step": 50430
    },
    {
      "epoch": 2.8022222222222224,
      "grad_norm": 0.03603363782167435,
      "learning_rate": 2.1977777777777777e-05,
      "loss": 0.0019,
      "step": 50440
    },
    {
      "epoch": 2.802777777777778,
      "grad_norm": 0.0314972847700119,
      "learning_rate": 2.1972222222222224e-05,
      "loss": 0.0029,
      "step": 50450
    },
    {
      "epoch": 2.8033333333333332,
      "grad_norm": 0.08134422451257706,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.0035,
      "step": 50460
    },
    {
      "epoch": 2.803888888888889,
      "grad_norm": 0.275234192609787,
      "learning_rate": 2.1961111111111114e-05,
      "loss": 0.0035,
      "step": 50470
    },
    {
      "epoch": 2.8044444444444445,
      "grad_norm": 0.13975051045417786,
      "learning_rate": 2.1955555555555558e-05,
      "loss": 0.0025,
      "step": 50480
    },
    {
      "epoch": 2.805,
      "grad_norm": 0.29668503999710083,
      "learning_rate": 2.195e-05,
      "loss": 0.0027,
      "step": 50490
    },
    {
      "epoch": 2.8055555555555554,
      "grad_norm": 0.23804041743278503,
      "learning_rate": 2.1944444444444445e-05,
      "loss": 0.003,
      "step": 50500
    },
    {
      "epoch": 2.806111111111111,
      "grad_norm": 0.1783370077610016,
      "learning_rate": 2.193888888888889e-05,
      "loss": 0.0044,
      "step": 50510
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.01097902376204729,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0039,
      "step": 50520
    },
    {
      "epoch": 2.8072222222222223,
      "grad_norm": 0.3011058568954468,
      "learning_rate": 2.192777777777778e-05,
      "loss": 0.0025,
      "step": 50530
    },
    {
      "epoch": 2.807777777777778,
      "grad_norm": 0.4464510977268219,
      "learning_rate": 2.1922222222222226e-05,
      "loss": 0.0028,
      "step": 50540
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 0.13699138164520264,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.0036,
      "step": 50550
    },
    {
      "epoch": 2.8088888888888888,
      "grad_norm": 0.29706671833992004,
      "learning_rate": 2.1911111111111113e-05,
      "loss": 0.0033,
      "step": 50560
    },
    {
      "epoch": 2.8094444444444444,
      "grad_norm": 0.06047060713171959,
      "learning_rate": 2.1905555555555557e-05,
      "loss": 0.0029,
      "step": 50570
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.17871682345867157,
      "learning_rate": 2.19e-05,
      "loss": 0.0018,
      "step": 50580
    },
    {
      "epoch": 2.8105555555555557,
      "grad_norm": 0.03284576162695885,
      "learning_rate": 2.1894444444444444e-05,
      "loss": 0.0029,
      "step": 50590
    },
    {
      "epoch": 2.811111111111111,
      "grad_norm": 0.20816467702388763,
      "learning_rate": 2.188888888888889e-05,
      "loss": 0.0024,
      "step": 50600
    },
    {
      "epoch": 2.8116666666666665,
      "grad_norm": 0.2974262833595276,
      "learning_rate": 2.1883333333333334e-05,
      "loss": 0.0031,
      "step": 50610
    },
    {
      "epoch": 2.812222222222222,
      "grad_norm": 0.29713690280914307,
      "learning_rate": 2.1877777777777778e-05,
      "loss": 0.0036,
      "step": 50620
    },
    {
      "epoch": 2.812777777777778,
      "grad_norm": 0.20841872692108154,
      "learning_rate": 2.1872222222222225e-05,
      "loss": 0.0031,
      "step": 50630
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.09006234258413315,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0033,
      "step": 50640
    },
    {
      "epoch": 2.813888888888889,
      "grad_norm": 0.20805345475673676,
      "learning_rate": 2.1861111111111112e-05,
      "loss": 0.0032,
      "step": 50650
    },
    {
      "epoch": 2.8144444444444443,
      "grad_norm": 0.24038207530975342,
      "learning_rate": 2.1855555555555556e-05,
      "loss": 0.0046,
      "step": 50660
    },
    {
      "epoch": 2.815,
      "grad_norm": 0.26697832345962524,
      "learning_rate": 2.1850000000000003e-05,
      "loss": 0.0026,
      "step": 50670
    },
    {
      "epoch": 2.8155555555555556,
      "grad_norm": 0.7427821755409241,
      "learning_rate": 2.1844444444444446e-05,
      "loss": 0.0039,
      "step": 50680
    },
    {
      "epoch": 2.8161111111111112,
      "grad_norm": 0.09057661145925522,
      "learning_rate": 2.183888888888889e-05,
      "loss": 0.0035,
      "step": 50690
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 0.28552448749542236,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0035,
      "step": 50700
    },
    {
      "epoch": 2.817222222222222,
      "grad_norm": 0.4456246495246887,
      "learning_rate": 2.1827777777777777e-05,
      "loss": 0.0017,
      "step": 50710
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 0.09811129420995712,
      "learning_rate": 2.1822222222222224e-05,
      "loss": 0.0023,
      "step": 50720
    },
    {
      "epoch": 2.8183333333333334,
      "grad_norm": 0.12084829062223434,
      "learning_rate": 2.1816666666666667e-05,
      "loss": 0.0032,
      "step": 50730
    },
    {
      "epoch": 2.818888888888889,
      "grad_norm": 0.474090039730072,
      "learning_rate": 2.1811111111111114e-05,
      "loss": 0.0032,
      "step": 50740
    },
    {
      "epoch": 2.8194444444444446,
      "grad_norm": 0.3124101161956787,
      "learning_rate": 2.1805555555555558e-05,
      "loss": 0.003,
      "step": 50750
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.3048427104949951,
      "learning_rate": 2.18e-05,
      "loss": 0.0046,
      "step": 50760
    },
    {
      "epoch": 2.8205555555555555,
      "grad_norm": 0.012195346876978874,
      "learning_rate": 2.1794444444444445e-05,
      "loss": 0.0042,
      "step": 50770
    },
    {
      "epoch": 2.821111111111111,
      "grad_norm": 0.03330962359905243,
      "learning_rate": 2.178888888888889e-05,
      "loss": 0.003,
      "step": 50780
    },
    {
      "epoch": 2.8216666666666668,
      "grad_norm": 0.14911194145679474,
      "learning_rate": 2.1783333333333332e-05,
      "loss": 0.0033,
      "step": 50790
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 0.38628503680229187,
      "learning_rate": 2.177777777777778e-05,
      "loss": 0.0018,
      "step": 50800
    },
    {
      "epoch": 2.8227777777777776,
      "grad_norm": 0.3558501899242401,
      "learning_rate": 2.1772222222222226e-05,
      "loss": 0.0024,
      "step": 50810
    },
    {
      "epoch": 2.8233333333333333,
      "grad_norm": 0.06099110096693039,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0043,
      "step": 50820
    },
    {
      "epoch": 2.823888888888889,
      "grad_norm": 0.1788462996482849,
      "learning_rate": 2.1761111111111113e-05,
      "loss": 0.0036,
      "step": 50830
    },
    {
      "epoch": 2.8244444444444445,
      "grad_norm": 0.4981713593006134,
      "learning_rate": 2.1755555555555557e-05,
      "loss": 0.0042,
      "step": 50840
    },
    {
      "epoch": 2.825,
      "grad_norm": 0.11979669332504272,
      "learning_rate": 2.175e-05,
      "loss": 0.0029,
      "step": 50850
    },
    {
      "epoch": 2.825555555555556,
      "grad_norm": 0.06006626784801483,
      "learning_rate": 2.1744444444444444e-05,
      "loss": 0.0028,
      "step": 50860
    },
    {
      "epoch": 2.826111111111111,
      "grad_norm": 0.09015213698148727,
      "learning_rate": 2.173888888888889e-05,
      "loss": 0.0033,
      "step": 50870
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.11936378479003906,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0043,
      "step": 50880
    },
    {
      "epoch": 2.8272222222222223,
      "grad_norm": 0.03142939507961273,
      "learning_rate": 2.1727777777777778e-05,
      "loss": 0.0035,
      "step": 50890
    },
    {
      "epoch": 2.8277777777777775,
      "grad_norm": 0.0684347078204155,
      "learning_rate": 2.1722222222222225e-05,
      "loss": 0.0031,
      "step": 50900
    },
    {
      "epoch": 2.828333333333333,
      "grad_norm": 0.0650075152516365,
      "learning_rate": 2.1716666666666668e-05,
      "loss": 0.0032,
      "step": 50910
    },
    {
      "epoch": 2.828888888888889,
      "grad_norm": 0.35601934790611267,
      "learning_rate": 2.1711111111111112e-05,
      "loss": 0.0032,
      "step": 50920
    },
    {
      "epoch": 2.8294444444444444,
      "grad_norm": 0.49304160475730896,
      "learning_rate": 2.1705555555555555e-05,
      "loss": 0.0028,
      "step": 50930
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.43335604667663574,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.0032,
      "step": 50940
    },
    {
      "epoch": 2.8305555555555557,
      "grad_norm": 0.20880211889743805,
      "learning_rate": 2.1694444444444446e-05,
      "loss": 0.0023,
      "step": 50950
    },
    {
      "epoch": 2.8311111111111114,
      "grad_norm": 0.41372209787368774,
      "learning_rate": 2.168888888888889e-05,
      "loss": 0.0025,
      "step": 50960
    },
    {
      "epoch": 2.8316666666666666,
      "grad_norm": 0.11886542290449142,
      "learning_rate": 2.1683333333333333e-05,
      "loss": 0.0022,
      "step": 50970
    },
    {
      "epoch": 2.832222222222222,
      "grad_norm": 0.5133892893791199,
      "learning_rate": 2.167777777777778e-05,
      "loss": 0.0019,
      "step": 50980
    },
    {
      "epoch": 2.832777777777778,
      "grad_norm": 0.35645702481269836,
      "learning_rate": 2.1672222222222223e-05,
      "loss": 0.0032,
      "step": 50990
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.35637202858924866,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0028,
      "step": 51000
    },
    {
      "epoch": 2.8338888888888887,
      "grad_norm": 0.26745331287384033,
      "learning_rate": 2.1661111111111114e-05,
      "loss": 0.0035,
      "step": 51010
    },
    {
      "epoch": 2.8344444444444443,
      "grad_norm": 0.3270532488822937,
      "learning_rate": 2.1655555555555558e-05,
      "loss": 0.0028,
      "step": 51020
    },
    {
      "epoch": 2.835,
      "grad_norm": 0.10778407752513885,
      "learning_rate": 2.165e-05,
      "loss": 0.0032,
      "step": 51030
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 0.03300313651561737,
      "learning_rate": 2.1644444444444445e-05,
      "loss": 0.0038,
      "step": 51040
    },
    {
      "epoch": 2.8361111111111112,
      "grad_norm": 0.5645750761032104,
      "learning_rate": 2.1638888888888888e-05,
      "loss": 0.0039,
      "step": 51050
    },
    {
      "epoch": 2.836666666666667,
      "grad_norm": 0.17885519564151764,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0031,
      "step": 51060
    },
    {
      "epoch": 2.837222222222222,
      "grad_norm": 0.12331269681453705,
      "learning_rate": 2.162777777777778e-05,
      "loss": 0.0034,
      "step": 51070
    },
    {
      "epoch": 2.8377777777777777,
      "grad_norm": 0.09046461433172226,
      "learning_rate": 2.1622222222222226e-05,
      "loss": 0.0034,
      "step": 51080
    },
    {
      "epoch": 2.8383333333333334,
      "grad_norm": 0.22045545279979706,
      "learning_rate": 2.161666666666667e-05,
      "loss": 0.0026,
      "step": 51090
    },
    {
      "epoch": 2.838888888888889,
      "grad_norm": 0.5972334742546082,
      "learning_rate": 2.1611111111111113e-05,
      "loss": 0.0033,
      "step": 51100
    },
    {
      "epoch": 2.839444444444444,
      "grad_norm": 0.0898515060544014,
      "learning_rate": 2.1605555555555556e-05,
      "loss": 0.0027,
      "step": 51110
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.42222079634666443,
      "learning_rate": 2.16e-05,
      "loss": 0.0042,
      "step": 51120
    },
    {
      "epoch": 2.8405555555555555,
      "grad_norm": 0.28542622923851013,
      "learning_rate": 2.1594444444444443e-05,
      "loss": 0.0045,
      "step": 51130
    },
    {
      "epoch": 2.841111111111111,
      "grad_norm": 0.12170779705047607,
      "learning_rate": 2.158888888888889e-05,
      "loss": 0.0046,
      "step": 51140
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 0.01223523449152708,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.0038,
      "step": 51150
    },
    {
      "epoch": 2.8422222222222224,
      "grad_norm": 0.03400038927793503,
      "learning_rate": 2.157777777777778e-05,
      "loss": 0.005,
      "step": 51160
    },
    {
      "epoch": 2.8427777777777776,
      "grad_norm": 0.17858585715293884,
      "learning_rate": 2.1572222222222224e-05,
      "loss": 0.0024,
      "step": 51170
    },
    {
      "epoch": 2.8433333333333333,
      "grad_norm": 0.24109502136707306,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.0027,
      "step": 51180
    },
    {
      "epoch": 2.843888888888889,
      "grad_norm": 0.6290437579154968,
      "learning_rate": 2.156111111111111e-05,
      "loss": 0.0025,
      "step": 51190
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 0.05989387631416321,
      "learning_rate": 2.1555555555555555e-05,
      "loss": 0.0029,
      "step": 51200
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 0.23514868319034576,
      "learning_rate": 2.1550000000000002e-05,
      "loss": 0.0035,
      "step": 51210
    },
    {
      "epoch": 2.8455555555555554,
      "grad_norm": 0.061375200748443604,
      "learning_rate": 2.1544444444444446e-05,
      "loss": 0.0036,
      "step": 51220
    },
    {
      "epoch": 2.846111111111111,
      "grad_norm": 0.23779094219207764,
      "learning_rate": 2.153888888888889e-05,
      "loss": 0.0039,
      "step": 51230
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.4868011772632599,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0026,
      "step": 51240
    },
    {
      "epoch": 2.8472222222222223,
      "grad_norm": 0.2482222467660904,
      "learning_rate": 2.152777777777778e-05,
      "loss": 0.0035,
      "step": 51250
    },
    {
      "epoch": 2.847777777777778,
      "grad_norm": 0.38223370909690857,
      "learning_rate": 2.1522222222222223e-05,
      "loss": 0.0031,
      "step": 51260
    },
    {
      "epoch": 2.8483333333333336,
      "grad_norm": 0.09827634692192078,
      "learning_rate": 2.1516666666666667e-05,
      "loss": 0.0047,
      "step": 51270
    },
    {
      "epoch": 2.848888888888889,
      "grad_norm": 0.17841464281082153,
      "learning_rate": 2.1511111111111114e-05,
      "loss": 0.0027,
      "step": 51280
    },
    {
      "epoch": 2.8494444444444444,
      "grad_norm": 0.11036083102226257,
      "learning_rate": 2.1505555555555557e-05,
      "loss": 0.0036,
      "step": 51290
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.06103377789258957,
      "learning_rate": 2.15e-05,
      "loss": 0.0032,
      "step": 51300
    },
    {
      "epoch": 2.8505555555555553,
      "grad_norm": 0.23881907761096954,
      "learning_rate": 2.1494444444444444e-05,
      "loss": 0.0027,
      "step": 51310
    },
    {
      "epoch": 2.851111111111111,
      "grad_norm": 0.6630778908729553,
      "learning_rate": 2.1488888888888888e-05,
      "loss": 0.0037,
      "step": 51320
    },
    {
      "epoch": 2.8516666666666666,
      "grad_norm": 0.0924769714474678,
      "learning_rate": 2.148333333333333e-05,
      "loss": 0.0023,
      "step": 51330
    },
    {
      "epoch": 2.852222222222222,
      "grad_norm": 0.23716570436954498,
      "learning_rate": 2.147777777777778e-05,
      "loss": 0.003,
      "step": 51340
    },
    {
      "epoch": 2.852777777777778,
      "grad_norm": 0.15043537318706512,
      "learning_rate": 2.1472222222222225e-05,
      "loss": 0.0042,
      "step": 51350
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.38538020849227905,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.004,
      "step": 51360
    },
    {
      "epoch": 2.853888888888889,
      "grad_norm": 0.1913173496723175,
      "learning_rate": 2.1461111111111112e-05,
      "loss": 0.0036,
      "step": 51370
    },
    {
      "epoch": 2.8544444444444443,
      "grad_norm": 0.3244522511959076,
      "learning_rate": 2.1455555555555556e-05,
      "loss": 0.003,
      "step": 51380
    },
    {
      "epoch": 2.855,
      "grad_norm": 0.6275107264518738,
      "learning_rate": 2.145e-05,
      "loss": 0.0041,
      "step": 51390
    },
    {
      "epoch": 2.8555555555555556,
      "grad_norm": 0.09088665246963501,
      "learning_rate": 2.1444444444444443e-05,
      "loss": 0.0023,
      "step": 51400
    },
    {
      "epoch": 2.8561111111111113,
      "grad_norm": 0.06216450780630112,
      "learning_rate": 2.143888888888889e-05,
      "loss": 0.0028,
      "step": 51410
    },
    {
      "epoch": 2.8566666666666665,
      "grad_norm": 0.23750251531600952,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.003,
      "step": 51420
    },
    {
      "epoch": 2.857222222222222,
      "grad_norm": 0.17978879809379578,
      "learning_rate": 2.142777777777778e-05,
      "loss": 0.004,
      "step": 51430
    },
    {
      "epoch": 2.8577777777777778,
      "grad_norm": 0.14863206446170807,
      "learning_rate": 2.1422222222222224e-05,
      "loss": 0.004,
      "step": 51440
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 0.15022112429141998,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.0024,
      "step": 51450
    },
    {
      "epoch": 2.858888888888889,
      "grad_norm": 0.09361732751131058,
      "learning_rate": 2.141111111111111e-05,
      "loss": 0.0027,
      "step": 51460
    },
    {
      "epoch": 2.8594444444444447,
      "grad_norm": 0.1282084882259369,
      "learning_rate": 2.1405555555555555e-05,
      "loss": 0.0049,
      "step": 51470
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.03615407645702362,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0019,
      "step": 51480
    },
    {
      "epoch": 2.8605555555555555,
      "grad_norm": 0.12273299694061279,
      "learning_rate": 2.1394444444444445e-05,
      "loss": 0.0041,
      "step": 51490
    },
    {
      "epoch": 2.861111111111111,
      "grad_norm": 0.5964936017990112,
      "learning_rate": 2.138888888888889e-05,
      "loss": 0.004,
      "step": 51500
    },
    {
      "epoch": 2.861666666666667,
      "grad_norm": 0.8881592750549316,
      "learning_rate": 2.1383333333333332e-05,
      "loss": 0.0042,
      "step": 51510
    },
    {
      "epoch": 2.862222222222222,
      "grad_norm": 0.17891930043697357,
      "learning_rate": 2.137777777777778e-05,
      "loss": 0.0037,
      "step": 51520
    },
    {
      "epoch": 2.8627777777777776,
      "grad_norm": 0.2390354871749878,
      "learning_rate": 2.1372222222222223e-05,
      "loss": 0.0027,
      "step": 51530
    },
    {
      "epoch": 2.8633333333333333,
      "grad_norm": 0.2980571687221527,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.0025,
      "step": 51540
    },
    {
      "epoch": 2.863888888888889,
      "grad_norm": 0.0893033966422081,
      "learning_rate": 2.1361111111111113e-05,
      "loss": 0.0031,
      "step": 51550
    },
    {
      "epoch": 2.8644444444444446,
      "grad_norm": 0.18333403766155243,
      "learning_rate": 2.1355555555555557e-05,
      "loss": 0.0032,
      "step": 51560
    },
    {
      "epoch": 2.865,
      "grad_norm": 0.36841681599617004,
      "learning_rate": 2.135e-05,
      "loss": 0.0039,
      "step": 51570
    },
    {
      "epoch": 2.8655555555555554,
      "grad_norm": 0.32318195700645447,
      "learning_rate": 2.1344444444444444e-05,
      "loss": 0.004,
      "step": 51580
    },
    {
      "epoch": 2.866111111111111,
      "grad_norm": 0.1192263513803482,
      "learning_rate": 2.1338888888888888e-05,
      "loss": 0.0037,
      "step": 51590
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.32897627353668213,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0034,
      "step": 51600
    },
    {
      "epoch": 2.8672222222222223,
      "grad_norm": 0.06284413486719131,
      "learning_rate": 2.1327777777777778e-05,
      "loss": 0.0022,
      "step": 51610
    },
    {
      "epoch": 2.8677777777777775,
      "grad_norm": 0.23784612119197845,
      "learning_rate": 2.1322222222222225e-05,
      "loss": 0.0019,
      "step": 51620
    },
    {
      "epoch": 2.868333333333333,
      "grad_norm": 0.03772684559226036,
      "learning_rate": 2.131666666666667e-05,
      "loss": 0.0036,
      "step": 51630
    },
    {
      "epoch": 2.868888888888889,
      "grad_norm": 0.09002023190259933,
      "learning_rate": 2.1311111111111112e-05,
      "loss": 0.0033,
      "step": 51640
    },
    {
      "epoch": 2.8694444444444445,
      "grad_norm": 0.2679300904273987,
      "learning_rate": 2.1305555555555556e-05,
      "loss": 0.0033,
      "step": 51650
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.09827358275651932,
      "learning_rate": 2.13e-05,
      "loss": 0.0033,
      "step": 51660
    },
    {
      "epoch": 2.8705555555555557,
      "grad_norm": 0.26816385984420776,
      "learning_rate": 2.1294444444444446e-05,
      "loss": 0.0037,
      "step": 51670
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 0.03190598264336586,
      "learning_rate": 2.128888888888889e-05,
      "loss": 0.0029,
      "step": 51680
    },
    {
      "epoch": 2.8716666666666666,
      "grad_norm": 0.11932530254125595,
      "learning_rate": 2.1283333333333337e-05,
      "loss": 0.003,
      "step": 51690
    },
    {
      "epoch": 2.8722222222222222,
      "grad_norm": 0.2688068449497223,
      "learning_rate": 2.127777777777778e-05,
      "loss": 0.0027,
      "step": 51700
    },
    {
      "epoch": 2.872777777777778,
      "grad_norm": 0.07857580482959747,
      "learning_rate": 2.1272222222222224e-05,
      "loss": 0.0048,
      "step": 51710
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.45508021116256714,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0031,
      "step": 51720
    },
    {
      "epoch": 2.8738888888888887,
      "grad_norm": 0.01614232547581196,
      "learning_rate": 2.126111111111111e-05,
      "loss": 0.0034,
      "step": 51730
    },
    {
      "epoch": 2.8744444444444444,
      "grad_norm": 0.2292664796113968,
      "learning_rate": 2.1255555555555558e-05,
      "loss": 0.0034,
      "step": 51740
    },
    {
      "epoch": 2.875,
      "grad_norm": 0.2676260471343994,
      "learning_rate": 2.125e-05,
      "loss": 0.0032,
      "step": 51750
    },
    {
      "epoch": 2.8755555555555556,
      "grad_norm": 0.03118661604821682,
      "learning_rate": 2.1244444444444445e-05,
      "loss": 0.0048,
      "step": 51760
    },
    {
      "epoch": 2.8761111111111113,
      "grad_norm": 0.21224567294120789,
      "learning_rate": 2.123888888888889e-05,
      "loss": 0.0027,
      "step": 51770
    },
    {
      "epoch": 2.876666666666667,
      "grad_norm": 0.32733893394470215,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0031,
      "step": 51780
    },
    {
      "epoch": 2.877222222222222,
      "grad_norm": 0.5943426489830017,
      "learning_rate": 2.122777777777778e-05,
      "loss": 0.0022,
      "step": 51790
    },
    {
      "epoch": 2.8777777777777778,
      "grad_norm": 0.23436173796653748,
      "learning_rate": 2.1222222222222223e-05,
      "loss": 0.0055,
      "step": 51800
    },
    {
      "epoch": 2.8783333333333334,
      "grad_norm": 0.35659506916999817,
      "learning_rate": 2.121666666666667e-05,
      "loss": 0.0028,
      "step": 51810
    },
    {
      "epoch": 2.878888888888889,
      "grad_norm": 0.05988002195954323,
      "learning_rate": 2.1211111111111113e-05,
      "loss": 0.0041,
      "step": 51820
    },
    {
      "epoch": 2.8794444444444443,
      "grad_norm": 0.011553043499588966,
      "learning_rate": 2.1205555555555557e-05,
      "loss": 0.0026,
      "step": 51830
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.06072308495640755,
      "learning_rate": 2.12e-05,
      "loss": 0.0031,
      "step": 51840
    },
    {
      "epoch": 2.8805555555555555,
      "grad_norm": 0.06097707152366638,
      "learning_rate": 2.1194444444444444e-05,
      "loss": 0.0031,
      "step": 51850
    },
    {
      "epoch": 2.881111111111111,
      "grad_norm": 0.12142816185951233,
      "learning_rate": 2.1188888888888887e-05,
      "loss": 0.0026,
      "step": 51860
    },
    {
      "epoch": 2.881666666666667,
      "grad_norm": 0.06005415320396423,
      "learning_rate": 2.1183333333333334e-05,
      "loss": 0.0023,
      "step": 51870
    },
    {
      "epoch": 2.8822222222222225,
      "grad_norm": 0.09930629283189774,
      "learning_rate": 2.117777777777778e-05,
      "loss": 0.003,
      "step": 51880
    },
    {
      "epoch": 2.8827777777777777,
      "grad_norm": 0.7721135020256042,
      "learning_rate": 2.1172222222222225e-05,
      "loss": 0.0019,
      "step": 51890
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 0.38653555512428284,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.0034,
      "step": 51900
    },
    {
      "epoch": 2.883888888888889,
      "grad_norm": 0.2960801422595978,
      "learning_rate": 2.1161111111111112e-05,
      "loss": 0.0026,
      "step": 51910
    },
    {
      "epoch": 2.8844444444444446,
      "grad_norm": 0.06120360270142555,
      "learning_rate": 2.1155555555555556e-05,
      "loss": 0.0032,
      "step": 51920
    },
    {
      "epoch": 2.885,
      "grad_norm": 0.12146149575710297,
      "learning_rate": 2.115e-05,
      "loss": 0.0029,
      "step": 51930
    },
    {
      "epoch": 2.8855555555555554,
      "grad_norm": 0.24221369624137878,
      "learning_rate": 2.1144444444444446e-05,
      "loss": 0.0039,
      "step": 51940
    },
    {
      "epoch": 2.886111111111111,
      "grad_norm": 0.08077597618103027,
      "learning_rate": 2.113888888888889e-05,
      "loss": 0.0037,
      "step": 51950
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.4582923948764801,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0033,
      "step": 51960
    },
    {
      "epoch": 2.8872222222222224,
      "grad_norm": 0.17982400953769684,
      "learning_rate": 2.112777777777778e-05,
      "loss": 0.0033,
      "step": 51970
    },
    {
      "epoch": 2.887777777777778,
      "grad_norm": 0.11914598941802979,
      "learning_rate": 2.1122222222222224e-05,
      "loss": 0.0042,
      "step": 51980
    },
    {
      "epoch": 2.888333333333333,
      "grad_norm": 0.1682812124490738,
      "learning_rate": 2.1116666666666667e-05,
      "loss": 0.0022,
      "step": 51990
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.1794203668832779,
      "learning_rate": 2.111111111111111e-05,
      "loss": 0.0031,
      "step": 52000
    },
    {
      "epoch": 2.8894444444444445,
      "grad_norm": 0.11930406838655472,
      "learning_rate": 2.1105555555555558e-05,
      "loss": 0.0023,
      "step": 52010
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.14884689450263977,
      "learning_rate": 2.11e-05,
      "loss": 0.003,
      "step": 52020
    },
    {
      "epoch": 2.8905555555555553,
      "grad_norm": 0.3477296829223633,
      "learning_rate": 2.1094444444444445e-05,
      "loss": 0.0026,
      "step": 52030
    },
    {
      "epoch": 2.891111111111111,
      "grad_norm": 0.3203754723072052,
      "learning_rate": 2.108888888888889e-05,
      "loss": 0.0036,
      "step": 52040
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 0.40900978446006775,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.0034,
      "step": 52050
    },
    {
      "epoch": 2.8922222222222222,
      "grad_norm": 0.08934235572814941,
      "learning_rate": 2.107777777777778e-05,
      "loss": 0.0025,
      "step": 52060
    },
    {
      "epoch": 2.892777777777778,
      "grad_norm": 0.2080848217010498,
      "learning_rate": 2.1072222222222222e-05,
      "loss": 0.0021,
      "step": 52070
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.20953916013240814,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0032,
      "step": 52080
    },
    {
      "epoch": 2.8938888888888887,
      "grad_norm": 0.11902987957000732,
      "learning_rate": 2.1061111111111113e-05,
      "loss": 0.0016,
      "step": 52090
    },
    {
      "epoch": 2.8944444444444444,
      "grad_norm": 0.06066002696752548,
      "learning_rate": 2.1055555555555556e-05,
      "loss": 0.0024,
      "step": 52100
    },
    {
      "epoch": 2.895,
      "grad_norm": 0.20824353396892548,
      "learning_rate": 2.105e-05,
      "loss": 0.0025,
      "step": 52110
    },
    {
      "epoch": 2.8955555555555557,
      "grad_norm": 0.24295037984848022,
      "learning_rate": 2.1044444444444444e-05,
      "loss": 0.0022,
      "step": 52120
    },
    {
      "epoch": 2.896111111111111,
      "grad_norm": 0.06423941999673843,
      "learning_rate": 2.1038888888888887e-05,
      "loss": 0.003,
      "step": 52130
    },
    {
      "epoch": 2.8966666666666665,
      "grad_norm": 0.18745988607406616,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0026,
      "step": 52140
    },
    {
      "epoch": 2.897222222222222,
      "grad_norm": 0.18560341000556946,
      "learning_rate": 2.102777777777778e-05,
      "loss": 0.0036,
      "step": 52150
    },
    {
      "epoch": 2.897777777777778,
      "grad_norm": 0.23740307986736298,
      "learning_rate": 2.1022222222222225e-05,
      "loss": 0.0024,
      "step": 52160
    },
    {
      "epoch": 2.8983333333333334,
      "grad_norm": 0.17859327793121338,
      "learning_rate": 2.1016666666666668e-05,
      "loss": 0.0034,
      "step": 52170
    },
    {
      "epoch": 2.898888888888889,
      "grad_norm": 0.23750805854797363,
      "learning_rate": 2.1011111111111112e-05,
      "loss": 0.0023,
      "step": 52180
    },
    {
      "epoch": 2.8994444444444447,
      "grad_norm": 0.4456186294555664,
      "learning_rate": 2.1005555555555555e-05,
      "loss": 0.0029,
      "step": 52190
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.20976580679416656,
      "learning_rate": 2.1e-05,
      "loss": 0.0029,
      "step": 52200
    },
    {
      "epoch": 2.9005555555555556,
      "grad_norm": 0.20719774067401886,
      "learning_rate": 2.0994444444444446e-05,
      "loss": 0.0028,
      "step": 52210
    },
    {
      "epoch": 2.901111111111111,
      "grad_norm": 0.4749860167503357,
      "learning_rate": 2.0988888888888893e-05,
      "loss": 0.003,
      "step": 52220
    },
    {
      "epoch": 2.9016666666666664,
      "grad_norm": 0.3319125771522522,
      "learning_rate": 2.0983333333333336e-05,
      "loss": 0.005,
      "step": 52230
    },
    {
      "epoch": 2.902222222222222,
      "grad_norm": 0.16171109676361084,
      "learning_rate": 2.097777777777778e-05,
      "loss": 0.0036,
      "step": 52240
    },
    {
      "epoch": 2.9027777777777777,
      "grad_norm": 0.2466842234134674,
      "learning_rate": 2.0972222222222223e-05,
      "loss": 0.0043,
      "step": 52250
    },
    {
      "epoch": 2.9033333333333333,
      "grad_norm": 0.44552671909332275,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0028,
      "step": 52260
    },
    {
      "epoch": 2.903888888888889,
      "grad_norm": 0.4985460042953491,
      "learning_rate": 2.096111111111111e-05,
      "loss": 0.0023,
      "step": 52270
    },
    {
      "epoch": 2.9044444444444446,
      "grad_norm": 0.03002851828932762,
      "learning_rate": 2.0955555555555557e-05,
      "loss": 0.003,
      "step": 52280
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 0.06016962602734566,
      "learning_rate": 2.095e-05,
      "loss": 0.0038,
      "step": 52290
    },
    {
      "epoch": 2.9055555555555554,
      "grad_norm": 0.42209309339523315,
      "learning_rate": 2.0944444444444445e-05,
      "loss": 0.0039,
      "step": 52300
    },
    {
      "epoch": 2.906111111111111,
      "grad_norm": 0.03167906031012535,
      "learning_rate": 2.093888888888889e-05,
      "loss": 0.0037,
      "step": 52310
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.3322334587574005,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0024,
      "step": 52320
    },
    {
      "epoch": 2.9072222222222224,
      "grad_norm": 0.34806859493255615,
      "learning_rate": 2.092777777777778e-05,
      "loss": 0.0036,
      "step": 52330
    },
    {
      "epoch": 2.9077777777777776,
      "grad_norm": 0.8141623139381409,
      "learning_rate": 2.0922222222222222e-05,
      "loss": 0.0032,
      "step": 52340
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 0.11927976459264755,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.0039,
      "step": 52350
    },
    {
      "epoch": 2.908888888888889,
      "grad_norm": 0.059648044407367706,
      "learning_rate": 2.0911111111111113e-05,
      "loss": 0.0032,
      "step": 52360
    },
    {
      "epoch": 2.9094444444444445,
      "grad_norm": 0.2674592435359955,
      "learning_rate": 2.0905555555555556e-05,
      "loss": 0.0026,
      "step": 52370
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.10854624956846237,
      "learning_rate": 2.09e-05,
      "loss": 0.0028,
      "step": 52380
    },
    {
      "epoch": 2.910555555555556,
      "grad_norm": 0.29717156291007996,
      "learning_rate": 2.0894444444444443e-05,
      "loss": 0.0044,
      "step": 52390
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 0.1492163985967636,
      "learning_rate": 2.088888888888889e-05,
      "loss": 0.0041,
      "step": 52400
    },
    {
      "epoch": 2.9116666666666666,
      "grad_norm": 0.2677145302295685,
      "learning_rate": 2.0883333333333334e-05,
      "loss": 0.0029,
      "step": 52410
    },
    {
      "epoch": 2.9122222222222223,
      "grad_norm": 0.45652514696121216,
      "learning_rate": 2.087777777777778e-05,
      "loss": 0.0029,
      "step": 52420
    },
    {
      "epoch": 2.912777777777778,
      "grad_norm": 0.2816582918167114,
      "learning_rate": 2.0872222222222224e-05,
      "loss": 0.0036,
      "step": 52430
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.2223178893327713,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0027,
      "step": 52440
    },
    {
      "epoch": 2.9138888888888888,
      "grad_norm": 0.4163391590118408,
      "learning_rate": 2.086111111111111e-05,
      "loss": 0.0033,
      "step": 52450
    },
    {
      "epoch": 2.9144444444444444,
      "grad_norm": 0.27376335859298706,
      "learning_rate": 2.0855555555555555e-05,
      "loss": 0.0029,
      "step": 52460
    },
    {
      "epoch": 2.915,
      "grad_norm": 0.11941485106945038,
      "learning_rate": 2.085e-05,
      "loss": 0.0035,
      "step": 52470
    },
    {
      "epoch": 2.9155555555555557,
      "grad_norm": 0.22871176898479462,
      "learning_rate": 2.0844444444444446e-05,
      "loss": 0.0038,
      "step": 52480
    },
    {
      "epoch": 2.9161111111111113,
      "grad_norm": 0.38628584146499634,
      "learning_rate": 2.0838888888888892e-05,
      "loss": 0.0033,
      "step": 52490
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.03190883994102478,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0037,
      "step": 52500
    },
    {
      "epoch": 2.917222222222222,
      "grad_norm": 0.38686200976371765,
      "learning_rate": 2.082777777777778e-05,
      "loss": 0.0033,
      "step": 52510
    },
    {
      "epoch": 2.917777777777778,
      "grad_norm": 0.17837420105934143,
      "learning_rate": 2.0822222222222223e-05,
      "loss": 0.003,
      "step": 52520
    },
    {
      "epoch": 2.9183333333333334,
      "grad_norm": 0.2382756471633911,
      "learning_rate": 2.0816666666666667e-05,
      "loss": 0.0022,
      "step": 52530
    },
    {
      "epoch": 2.9188888888888886,
      "grad_norm": 0.14957590401172638,
      "learning_rate": 2.081111111111111e-05,
      "loss": 0.0032,
      "step": 52540
    },
    {
      "epoch": 2.9194444444444443,
      "grad_norm": 0.5217456817626953,
      "learning_rate": 2.0805555555555557e-05,
      "loss": 0.0036,
      "step": 52550
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.5744139552116394,
      "learning_rate": 2.08e-05,
      "loss": 0.0035,
      "step": 52560
    },
    {
      "epoch": 2.9205555555555556,
      "grad_norm": 0.43578171730041504,
      "learning_rate": 2.0794444444444444e-05,
      "loss": 0.0038,
      "step": 52570
    },
    {
      "epoch": 2.921111111111111,
      "grad_norm": 0.08975159376859665,
      "learning_rate": 2.078888888888889e-05,
      "loss": 0.0034,
      "step": 52580
    },
    {
      "epoch": 2.921666666666667,
      "grad_norm": 0.09054180979728699,
      "learning_rate": 2.0783333333333335e-05,
      "loss": 0.0046,
      "step": 52590
    },
    {
      "epoch": 2.9222222222222225,
      "grad_norm": 0.7134901285171509,
      "learning_rate": 2.077777777777778e-05,
      "loss": 0.0026,
      "step": 52600
    },
    {
      "epoch": 2.9227777777777777,
      "grad_norm": 0.32650119066238403,
      "learning_rate": 2.0772222222222222e-05,
      "loss": 0.0034,
      "step": 52610
    },
    {
      "epoch": 2.9233333333333333,
      "grad_norm": 0.5347721576690674,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.0028,
      "step": 52620
    },
    {
      "epoch": 2.923888888888889,
      "grad_norm": 0.5672723054885864,
      "learning_rate": 2.0761111111111112e-05,
      "loss": 0.0046,
      "step": 52630
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 0.12011309713125229,
      "learning_rate": 2.0755555555555556e-05,
      "loss": 0.003,
      "step": 52640
    },
    {
      "epoch": 2.925,
      "grad_norm": 0.14907364547252655,
      "learning_rate": 2.075e-05,
      "loss": 0.0037,
      "step": 52650
    },
    {
      "epoch": 2.9255555555555555,
      "grad_norm": 0.06368665397167206,
      "learning_rate": 2.0744444444444443e-05,
      "loss": 0.0032,
      "step": 52660
    },
    {
      "epoch": 2.926111111111111,
      "grad_norm": 0.5641099214553833,
      "learning_rate": 2.073888888888889e-05,
      "loss": 0.0034,
      "step": 52670
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.3001401424407959,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0033,
      "step": 52680
    },
    {
      "epoch": 2.9272222222222224,
      "grad_norm": 0.7065926790237427,
      "learning_rate": 2.072777777777778e-05,
      "loss": 0.0021,
      "step": 52690
    },
    {
      "epoch": 2.927777777777778,
      "grad_norm": 0.37494465708732605,
      "learning_rate": 2.0722222222222224e-05,
      "loss": 0.0032,
      "step": 52700
    },
    {
      "epoch": 2.9283333333333332,
      "grad_norm": 0.4451557993888855,
      "learning_rate": 2.0716666666666668e-05,
      "loss": 0.0036,
      "step": 52710
    },
    {
      "epoch": 2.928888888888889,
      "grad_norm": 0.09084076434373856,
      "learning_rate": 2.071111111111111e-05,
      "loss": 0.003,
      "step": 52720
    },
    {
      "epoch": 2.9294444444444445,
      "grad_norm": 0.1789117008447647,
      "learning_rate": 2.0705555555555555e-05,
      "loss": 0.0034,
      "step": 52730
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.20822922885417938,
      "learning_rate": 2.07e-05,
      "loss": 0.0025,
      "step": 52740
    },
    {
      "epoch": 2.9305555555555554,
      "grad_norm": 0.03165358304977417,
      "learning_rate": 2.0694444444444445e-05,
      "loss": 0.0037,
      "step": 52750
    },
    {
      "epoch": 2.931111111111111,
      "grad_norm": 0.15028786659240723,
      "learning_rate": 2.0688888888888892e-05,
      "loss": 0.0026,
      "step": 52760
    },
    {
      "epoch": 2.9316666666666666,
      "grad_norm": 0.2382461130619049,
      "learning_rate": 2.0683333333333336e-05,
      "loss": 0.0032,
      "step": 52770
    },
    {
      "epoch": 2.9322222222222223,
      "grad_norm": 0.1487390697002411,
      "learning_rate": 2.067777777777778e-05,
      "loss": 0.0026,
      "step": 52780
    },
    {
      "epoch": 2.932777777777778,
      "grad_norm": 0.2617350220680237,
      "learning_rate": 2.0672222222222223e-05,
      "loss": 0.0042,
      "step": 52790
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.02168966457247734,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0043,
      "step": 52800
    },
    {
      "epoch": 2.9338888888888888,
      "grad_norm": 0.2825283110141754,
      "learning_rate": 2.066111111111111e-05,
      "loss": 0.0034,
      "step": 52810
    },
    {
      "epoch": 2.9344444444444444,
      "grad_norm": 0.11548827588558197,
      "learning_rate": 2.0655555555555557e-05,
      "loss": 0.0029,
      "step": 52820
    },
    {
      "epoch": 2.935,
      "grad_norm": 0.15616734325885773,
      "learning_rate": 2.065e-05,
      "loss": 0.0029,
      "step": 52830
    },
    {
      "epoch": 2.9355555555555557,
      "grad_norm": 0.5647359490394592,
      "learning_rate": 2.0644444444444447e-05,
      "loss": 0.0024,
      "step": 52840
    },
    {
      "epoch": 2.936111111111111,
      "grad_norm": 0.11978179216384888,
      "learning_rate": 2.063888888888889e-05,
      "loss": 0.0035,
      "step": 52850
    },
    {
      "epoch": 2.9366666666666665,
      "grad_norm": 0.4164312779903412,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.0032,
      "step": 52860
    },
    {
      "epoch": 2.937222222222222,
      "grad_norm": 0.2679908573627472,
      "learning_rate": 2.0627777777777778e-05,
      "loss": 0.0026,
      "step": 52870
    },
    {
      "epoch": 2.937777777777778,
      "grad_norm": 0.10003847628831863,
      "learning_rate": 2.062222222222222e-05,
      "loss": 0.0034,
      "step": 52880
    },
    {
      "epoch": 2.9383333333333335,
      "grad_norm": 0.237617626786232,
      "learning_rate": 2.061666666666667e-05,
      "loss": 0.0024,
      "step": 52890
    },
    {
      "epoch": 2.938888888888889,
      "grad_norm": 0.2980476915836334,
      "learning_rate": 2.0611111111111112e-05,
      "loss": 0.002,
      "step": 52900
    },
    {
      "epoch": 2.9394444444444443,
      "grad_norm": 0.09013335406780243,
      "learning_rate": 2.0605555555555556e-05,
      "loss": 0.0031,
      "step": 52910
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.3536168932914734,
      "learning_rate": 2.06e-05,
      "loss": 0.0021,
      "step": 52920
    },
    {
      "epoch": 2.9405555555555556,
      "grad_norm": 0.2857445180416107,
      "learning_rate": 2.0594444444444446e-05,
      "loss": 0.0051,
      "step": 52930
    },
    {
      "epoch": 2.9411111111111112,
      "grad_norm": 0.47521787881851196,
      "learning_rate": 2.058888888888889e-05,
      "loss": 0.0024,
      "step": 52940
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 0.17846985161304474,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.0035,
      "step": 52950
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 0.34066060185432434,
      "learning_rate": 2.057777777777778e-05,
      "loss": 0.0026,
      "step": 52960
    },
    {
      "epoch": 2.9427777777777777,
      "grad_norm": 0.5643647313117981,
      "learning_rate": 2.0572222222222224e-05,
      "loss": 0.0028,
      "step": 52970
    },
    {
      "epoch": 2.9433333333333334,
      "grad_norm": 0.29823827743530273,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0043,
      "step": 52980
    },
    {
      "epoch": 2.943888888888889,
      "grad_norm": 0.3785098195075989,
      "learning_rate": 2.056111111111111e-05,
      "loss": 0.0033,
      "step": 52990
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 0.03144453465938568,
      "learning_rate": 2.0555555555555555e-05,
      "loss": 0.0038,
      "step": 53000
    },
    {
      "epoch": 2.945,
      "grad_norm": 0.47500163316726685,
      "learning_rate": 2.055e-05,
      "loss": 0.003,
      "step": 53010
    },
    {
      "epoch": 2.9455555555555555,
      "grad_norm": 0.41380658745765686,
      "learning_rate": 2.054444444444445e-05,
      "loss": 0.0037,
      "step": 53020
    },
    {
      "epoch": 2.946111111111111,
      "grad_norm": 0.17819049954414368,
      "learning_rate": 2.0538888888888892e-05,
      "loss": 0.0033,
      "step": 53030
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.25261878967285156,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0039,
      "step": 53040
    },
    {
      "epoch": 2.947222222222222,
      "grad_norm": 0.25256219506263733,
      "learning_rate": 2.052777777777778e-05,
      "loss": 0.004,
      "step": 53050
    },
    {
      "epoch": 2.9477777777777776,
      "grad_norm": 0.35576093196868896,
      "learning_rate": 2.0522222222222223e-05,
      "loss": 0.0038,
      "step": 53060
    },
    {
      "epoch": 2.9483333333333333,
      "grad_norm": 0.17879262566566467,
      "learning_rate": 2.0516666666666666e-05,
      "loss": 0.0022,
      "step": 53070
    },
    {
      "epoch": 2.948888888888889,
      "grad_norm": 0.32241547107696533,
      "learning_rate": 2.0511111111111113e-05,
      "loss": 0.0028,
      "step": 53080
    },
    {
      "epoch": 2.9494444444444445,
      "grad_norm": 0.29788151383399963,
      "learning_rate": 2.0505555555555557e-05,
      "loss": 0.0035,
      "step": 53090
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.26372843980789185,
      "learning_rate": 2.05e-05,
      "loss": 0.003,
      "step": 53100
    },
    {
      "epoch": 2.950555555555556,
      "grad_norm": 0.3766533434391022,
      "learning_rate": 2.0494444444444447e-05,
      "loss": 0.0029,
      "step": 53110
    },
    {
      "epoch": 2.951111111111111,
      "grad_norm": 0.208624467253685,
      "learning_rate": 2.048888888888889e-05,
      "loss": 0.0033,
      "step": 53120
    },
    {
      "epoch": 2.9516666666666667,
      "grad_norm": 0.2289971262216568,
      "learning_rate": 2.0483333333333334e-05,
      "loss": 0.0032,
      "step": 53130
    },
    {
      "epoch": 2.9522222222222223,
      "grad_norm": 0.03249845653772354,
      "learning_rate": 2.0477777777777778e-05,
      "loss": 0.003,
      "step": 53140
    },
    {
      "epoch": 2.9527777777777775,
      "grad_norm": 0.23820218443870544,
      "learning_rate": 2.0472222222222225e-05,
      "loss": 0.003,
      "step": 53150
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.14990556240081787,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0023,
      "step": 53160
    },
    {
      "epoch": 2.953888888888889,
      "grad_norm": 0.09165757149457932,
      "learning_rate": 2.0461111111111112e-05,
      "loss": 0.0033,
      "step": 53170
    },
    {
      "epoch": 2.9544444444444444,
      "grad_norm": 0.2218988984823227,
      "learning_rate": 2.0455555555555555e-05,
      "loss": 0.0024,
      "step": 53180
    },
    {
      "epoch": 2.955,
      "grad_norm": 0.41684749722480774,
      "learning_rate": 2.045e-05,
      "loss": 0.004,
      "step": 53190
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 0.3188426196575165,
      "learning_rate": 2.0444444444444446e-05,
      "loss": 0.0037,
      "step": 53200
    },
    {
      "epoch": 2.9561111111111114,
      "grad_norm": 0.1484314650297165,
      "learning_rate": 2.043888888888889e-05,
      "loss": 0.0026,
      "step": 53210
    },
    {
      "epoch": 2.9566666666666666,
      "grad_norm": 0.20835839211940765,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.0025,
      "step": 53220
    },
    {
      "epoch": 2.957222222222222,
      "grad_norm": 0.0909774973988533,
      "learning_rate": 2.042777777777778e-05,
      "loss": 0.0037,
      "step": 53230
    },
    {
      "epoch": 2.957777777777778,
      "grad_norm": 0.27004727721214294,
      "learning_rate": 2.0422222222222224e-05,
      "loss": 0.0026,
      "step": 53240
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 0.14924705028533936,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.003,
      "step": 53250
    },
    {
      "epoch": 2.9588888888888887,
      "grad_norm": 0.3868120014667511,
      "learning_rate": 2.041111111111111e-05,
      "loss": 0.0022,
      "step": 53260
    },
    {
      "epoch": 2.9594444444444443,
      "grad_norm": 0.16665738821029663,
      "learning_rate": 2.0405555555555554e-05,
      "loss": 0.0039,
      "step": 53270
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.1785716414451599,
      "learning_rate": 2.04e-05,
      "loss": 0.0028,
      "step": 53280
    },
    {
      "epoch": 2.9605555555555556,
      "grad_norm": 0.2711493968963623,
      "learning_rate": 2.0394444444444448e-05,
      "loss": 0.0028,
      "step": 53290
    },
    {
      "epoch": 2.9611111111111112,
      "grad_norm": 0.12599299848079681,
      "learning_rate": 2.0388888888888892e-05,
      "loss": 0.0035,
      "step": 53300
    },
    {
      "epoch": 2.961666666666667,
      "grad_norm": 0.35649585723876953,
      "learning_rate": 2.0383333333333335e-05,
      "loss": 0.004,
      "step": 53310
    },
    {
      "epoch": 2.962222222222222,
      "grad_norm": 0.11943097412586212,
      "learning_rate": 2.037777777777778e-05,
      "loss": 0.0022,
      "step": 53320
    },
    {
      "epoch": 2.9627777777777777,
      "grad_norm": 0.0604914091527462,
      "learning_rate": 2.0372222222222222e-05,
      "loss": 0.0034,
      "step": 53330
    },
    {
      "epoch": 2.9633333333333334,
      "grad_norm": 0.03025311976671219,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0025,
      "step": 53340
    },
    {
      "epoch": 2.963888888888889,
      "grad_norm": 0.26741519570350647,
      "learning_rate": 2.0361111111111113e-05,
      "loss": 0.0038,
      "step": 53350
    },
    {
      "epoch": 2.964444444444444,
      "grad_norm": 0.38609299063682556,
      "learning_rate": 2.0355555555555556e-05,
      "loss": 0.0033,
      "step": 53360
    },
    {
      "epoch": 2.965,
      "grad_norm": 0.1787559539079666,
      "learning_rate": 2.035e-05,
      "loss": 0.0027,
      "step": 53370
    },
    {
      "epoch": 2.9655555555555555,
      "grad_norm": 0.016037659719586372,
      "learning_rate": 2.0344444444444447e-05,
      "loss": 0.0039,
      "step": 53380
    },
    {
      "epoch": 2.966111111111111,
      "grad_norm": 0.061237115412950516,
      "learning_rate": 2.033888888888889e-05,
      "loss": 0.0032,
      "step": 53390
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.0907042995095253,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0021,
      "step": 53400
    },
    {
      "epoch": 2.9672222222222224,
      "grad_norm": 0.20967920124530792,
      "learning_rate": 2.0327777777777778e-05,
      "loss": 0.0027,
      "step": 53410
    },
    {
      "epoch": 2.9677777777777776,
      "grad_norm": 0.1803448647260666,
      "learning_rate": 2.0322222222222225e-05,
      "loss": 0.0027,
      "step": 53420
    },
    {
      "epoch": 2.9683333333333333,
      "grad_norm": 0.05998788774013519,
      "learning_rate": 2.0316666666666668e-05,
      "loss": 0.0043,
      "step": 53430
    },
    {
      "epoch": 2.968888888888889,
      "grad_norm": 0.19902478158473969,
      "learning_rate": 2.031111111111111e-05,
      "loss": 0.0026,
      "step": 53440
    },
    {
      "epoch": 2.9694444444444446,
      "grad_norm": 0.3770666718482971,
      "learning_rate": 2.0305555555555555e-05,
      "loss": 0.0029,
      "step": 53450
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.20763641595840454,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0028,
      "step": 53460
    },
    {
      "epoch": 2.9705555555555554,
      "grad_norm": 0.18267783522605896,
      "learning_rate": 2.0294444444444446e-05,
      "loss": 0.0017,
      "step": 53470
    },
    {
      "epoch": 2.971111111111111,
      "grad_norm": 0.01177412923425436,
      "learning_rate": 2.028888888888889e-05,
      "loss": 0.0041,
      "step": 53480
    },
    {
      "epoch": 2.9716666666666667,
      "grad_norm": 0.019706133753061295,
      "learning_rate": 2.0283333333333336e-05,
      "loss": 0.0018,
      "step": 53490
    },
    {
      "epoch": 2.9722222222222223,
      "grad_norm": 0.09498301148414612,
      "learning_rate": 2.027777777777778e-05,
      "loss": 0.0028,
      "step": 53500
    },
    {
      "epoch": 2.972777777777778,
      "grad_norm": 0.06149812415242195,
      "learning_rate": 2.0272222222222223e-05,
      "loss": 0.0039,
      "step": 53510
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.03085322491824627,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0026,
      "step": 53520
    },
    {
      "epoch": 2.973888888888889,
      "grad_norm": 0.07432810962200165,
      "learning_rate": 2.026111111111111e-05,
      "loss": 0.0027,
      "step": 53530
    },
    {
      "epoch": 2.9744444444444444,
      "grad_norm": 0.11962365359067917,
      "learning_rate": 2.0255555555555554e-05,
      "loss": 0.0033,
      "step": 53540
    },
    {
      "epoch": 2.975,
      "grad_norm": 0.43877625465393066,
      "learning_rate": 2.025e-05,
      "loss": 0.0025,
      "step": 53550
    },
    {
      "epoch": 2.9755555555555553,
      "grad_norm": 0.3224732279777527,
      "learning_rate": 2.0244444444444448e-05,
      "loss": 0.0045,
      "step": 53560
    },
    {
      "epoch": 2.976111111111111,
      "grad_norm": 0.3649454414844513,
      "learning_rate": 2.023888888888889e-05,
      "loss": 0.0043,
      "step": 53570
    },
    {
      "epoch": 2.9766666666666666,
      "grad_norm": 0.3278893530368805,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.0029,
      "step": 53580
    },
    {
      "epoch": 2.977222222222222,
      "grad_norm": 0.23795029520988464,
      "learning_rate": 2.022777777777778e-05,
      "loss": 0.0035,
      "step": 53590
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.8615815043449402,
      "learning_rate": 2.0222222222222222e-05,
      "loss": 0.0032,
      "step": 53600
    },
    {
      "epoch": 2.9783333333333335,
      "grad_norm": 0.1486668586730957,
      "learning_rate": 2.0216666666666666e-05,
      "loss": 0.002,
      "step": 53610
    },
    {
      "epoch": 2.978888888888889,
      "grad_norm": 0.23774346709251404,
      "learning_rate": 2.0211111111111113e-05,
      "loss": 0.0026,
      "step": 53620
    },
    {
      "epoch": 2.9794444444444443,
      "grad_norm": 0.258670836687088,
      "learning_rate": 2.0205555555555556e-05,
      "loss": 0.0023,
      "step": 53630
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.7300685048103333,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0039,
      "step": 53640
    },
    {
      "epoch": 2.9805555555555556,
      "grad_norm": 0.1785738170146942,
      "learning_rate": 2.0194444444444447e-05,
      "loss": 0.0023,
      "step": 53650
    },
    {
      "epoch": 2.9811111111111113,
      "grad_norm": 0.2684333026409149,
      "learning_rate": 2.018888888888889e-05,
      "loss": 0.0028,
      "step": 53660
    },
    {
      "epoch": 2.9816666666666665,
      "grad_norm": 0.2394661009311676,
      "learning_rate": 2.0183333333333334e-05,
      "loss": 0.0042,
      "step": 53670
    },
    {
      "epoch": 2.982222222222222,
      "grad_norm": 0.6530866622924805,
      "learning_rate": 2.0177777777777777e-05,
      "loss": 0.0028,
      "step": 53680
    },
    {
      "epoch": 2.9827777777777778,
      "grad_norm": 0.019591275602579117,
      "learning_rate": 2.0172222222222224e-05,
      "loss": 0.0022,
      "step": 53690
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 0.2718126177787781,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0032,
      "step": 53700
    },
    {
      "epoch": 2.983888888888889,
      "grad_norm": 0.20855996012687683,
      "learning_rate": 2.016111111111111e-05,
      "loss": 0.0024,
      "step": 53710
    },
    {
      "epoch": 2.9844444444444447,
      "grad_norm": 0.3863089978694916,
      "learning_rate": 2.0155555555555555e-05,
      "loss": 0.0028,
      "step": 53720
    },
    {
      "epoch": 2.985,
      "grad_norm": 0.19054760038852692,
      "learning_rate": 2.0150000000000002e-05,
      "loss": 0.003,
      "step": 53730
    },
    {
      "epoch": 2.9855555555555555,
      "grad_norm": 0.17849597334861755,
      "learning_rate": 2.0144444444444445e-05,
      "loss": 0.0032,
      "step": 53740
    },
    {
      "epoch": 2.986111111111111,
      "grad_norm": 0.2284751683473587,
      "learning_rate": 2.013888888888889e-05,
      "loss": 0.0026,
      "step": 53750
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.38565877079963684,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0037,
      "step": 53760
    },
    {
      "epoch": 2.987222222222222,
      "grad_norm": 0.5655400156974792,
      "learning_rate": 2.012777777777778e-05,
      "loss": 0.0036,
      "step": 53770
    },
    {
      "epoch": 2.9877777777777776,
      "grad_norm": 0.5115352869033813,
      "learning_rate": 2.0122222222222223e-05,
      "loss": 0.0027,
      "step": 53780
    },
    {
      "epoch": 2.9883333333333333,
      "grad_norm": 0.11935586482286453,
      "learning_rate": 2.0116666666666667e-05,
      "loss": 0.004,
      "step": 53790
    },
    {
      "epoch": 2.988888888888889,
      "grad_norm": 0.14882305264472961,
      "learning_rate": 2.011111111111111e-05,
      "loss": 0.0034,
      "step": 53800
    },
    {
      "epoch": 2.9894444444444446,
      "grad_norm": 0.3863871693611145,
      "learning_rate": 2.0105555555555554e-05,
      "loss": 0.0038,
      "step": 53810
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.3485909700393677,
      "learning_rate": 2.01e-05,
      "loss": 0.0033,
      "step": 53820
    },
    {
      "epoch": 2.9905555555555554,
      "grad_norm": 0.06048373878002167,
      "learning_rate": 2.0094444444444448e-05,
      "loss": 0.0025,
      "step": 53830
    },
    {
      "epoch": 2.991111111111111,
      "grad_norm": 0.0626513883471489,
      "learning_rate": 2.008888888888889e-05,
      "loss": 0.0036,
      "step": 53840
    },
    {
      "epoch": 2.9916666666666667,
      "grad_norm": 0.06152115389704704,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.0027,
      "step": 53850
    },
    {
      "epoch": 2.9922222222222223,
      "grad_norm": 0.09047198295593262,
      "learning_rate": 2.0077777777777778e-05,
      "loss": 0.0031,
      "step": 53860
    },
    {
      "epoch": 2.9927777777777775,
      "grad_norm": 0.35593125224113464,
      "learning_rate": 2.0072222222222222e-05,
      "loss": 0.0033,
      "step": 53870
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.17843693494796753,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.003,
      "step": 53880
    },
    {
      "epoch": 2.993888888888889,
      "grad_norm": 0.23149842023849487,
      "learning_rate": 2.0061111111111112e-05,
      "loss": 0.0017,
      "step": 53890
    },
    {
      "epoch": 2.9944444444444445,
      "grad_norm": 0.08251215517520905,
      "learning_rate": 2.0055555555555556e-05,
      "loss": 0.0036,
      "step": 53900
    },
    {
      "epoch": 2.995,
      "grad_norm": 0.26287728548049927,
      "learning_rate": 2.0050000000000003e-05,
      "loss": 0.0035,
      "step": 53910
    },
    {
      "epoch": 2.9955555555555557,
      "grad_norm": 0.33657434582710266,
      "learning_rate": 2.0044444444444446e-05,
      "loss": 0.0027,
      "step": 53920
    },
    {
      "epoch": 2.996111111111111,
      "grad_norm": 0.20583583414554596,
      "learning_rate": 2.003888888888889e-05,
      "loss": 0.0034,
      "step": 53930
    },
    {
      "epoch": 2.9966666666666666,
      "grad_norm": 0.17878161370754242,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0035,
      "step": 53940
    },
    {
      "epoch": 2.9972222222222222,
      "grad_norm": 0.09085533767938614,
      "learning_rate": 2.0027777777777777e-05,
      "loss": 0.0029,
      "step": 53950
    },
    {
      "epoch": 2.997777777777778,
      "grad_norm": 0.06240055710077286,
      "learning_rate": 2.0022222222222224e-05,
      "loss": 0.0039,
      "step": 53960
    },
    {
      "epoch": 2.998333333333333,
      "grad_norm": 0.5210496783256531,
      "learning_rate": 2.0016666666666668e-05,
      "loss": 0.0042,
      "step": 53970
    },
    {
      "epoch": 2.9988888888888887,
      "grad_norm": 0.03177974373102188,
      "learning_rate": 2.001111111111111e-05,
      "loss": 0.0044,
      "step": 53980
    },
    {
      "epoch": 2.9994444444444444,
      "grad_norm": 0.1782114952802658,
      "learning_rate": 2.0005555555555555e-05,
      "loss": 0.0034,
      "step": 53990
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.36445119976997375,
      "learning_rate": 2e-05,
      "loss": 0.0046,
      "step": 54000
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.003135876962915063,
      "eval_runtime": 114.315,
      "eval_samples_per_second": 1575.436,
      "eval_steps_per_second": 39.391,
      "step": 54000
    },
    {
      "epoch": 3.0005555555555556,
      "grad_norm": 0.11939902603626251,
      "learning_rate": 1.9994444444444445e-05,
      "loss": 0.0032,
      "step": 54010
    },
    {
      "epoch": 3.0011111111111113,
      "grad_norm": 0.29717957973480225,
      "learning_rate": 1.998888888888889e-05,
      "loss": 0.0028,
      "step": 54020
    },
    {
      "epoch": 3.0016666666666665,
      "grad_norm": 0.009218208491802216,
      "learning_rate": 1.9983333333333336e-05,
      "loss": 0.0016,
      "step": 54030
    },
    {
      "epoch": 3.002222222222222,
      "grad_norm": 0.29685819149017334,
      "learning_rate": 1.997777777777778e-05,
      "loss": 0.0038,
      "step": 54040
    },
    {
      "epoch": 3.0027777777777778,
      "grad_norm": 0.14982913434505463,
      "learning_rate": 1.9972222222222223e-05,
      "loss": 0.0035,
      "step": 54050
    },
    {
      "epoch": 3.0033333333333334,
      "grad_norm": 0.12164395302534103,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0021,
      "step": 54060
    },
    {
      "epoch": 3.003888888888889,
      "grad_norm": 0.11885754019021988,
      "learning_rate": 1.996111111111111e-05,
      "loss": 0.003,
      "step": 54070
    },
    {
      "epoch": 3.0044444444444443,
      "grad_norm": 0.3633973002433777,
      "learning_rate": 1.9955555555555557e-05,
      "loss": 0.0036,
      "step": 54080
    },
    {
      "epoch": 3.005,
      "grad_norm": 0.29695388674736023,
      "learning_rate": 1.995e-05,
      "loss": 0.0033,
      "step": 54090
    },
    {
      "epoch": 3.0055555555555555,
      "grad_norm": 0.20848184823989868,
      "learning_rate": 1.9944444444444447e-05,
      "loss": 0.0028,
      "step": 54100
    },
    {
      "epoch": 3.006111111111111,
      "grad_norm": 0.20062917470932007,
      "learning_rate": 1.993888888888889e-05,
      "loss": 0.0032,
      "step": 54110
    },
    {
      "epoch": 3.006666666666667,
      "grad_norm": 0.4020111858844757,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0037,
      "step": 54120
    },
    {
      "epoch": 3.007222222222222,
      "grad_norm": 0.15894953906536102,
      "learning_rate": 1.9927777777777778e-05,
      "loss": 0.003,
      "step": 54130
    },
    {
      "epoch": 3.0077777777777777,
      "grad_norm": 0.4752974808216095,
      "learning_rate": 1.992222222222222e-05,
      "loss": 0.003,
      "step": 54140
    },
    {
      "epoch": 3.0083333333333333,
      "grad_norm": 0.47800883650779724,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.0045,
      "step": 54150
    },
    {
      "epoch": 3.008888888888889,
      "grad_norm": 0.2078937143087387,
      "learning_rate": 1.9911111111111112e-05,
      "loss": 0.0041,
      "step": 54160
    },
    {
      "epoch": 3.0094444444444446,
      "grad_norm": 0.17871998250484467,
      "learning_rate": 1.990555555555556e-05,
      "loss": 0.003,
      "step": 54170
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.23747596144676208,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0028,
      "step": 54180
    },
    {
      "epoch": 3.0105555555555554,
      "grad_norm": 0.061999063938856125,
      "learning_rate": 1.9894444444444446e-05,
      "loss": 0.0025,
      "step": 54190
    },
    {
      "epoch": 3.011111111111111,
      "grad_norm": 0.3939247131347656,
      "learning_rate": 1.988888888888889e-05,
      "loss": 0.0029,
      "step": 54200
    },
    {
      "epoch": 3.0116666666666667,
      "grad_norm": 0.11958877742290497,
      "learning_rate": 1.9883333333333333e-05,
      "loss": 0.0022,
      "step": 54210
    },
    {
      "epoch": 3.0122222222222224,
      "grad_norm": 0.17793923616409302,
      "learning_rate": 1.9877777777777777e-05,
      "loss": 0.0038,
      "step": 54220
    },
    {
      "epoch": 3.012777777777778,
      "grad_norm": 0.33635449409484863,
      "learning_rate": 1.9872222222222224e-05,
      "loss": 0.0044,
      "step": 54230
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.03221353888511658,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0029,
      "step": 54240
    },
    {
      "epoch": 3.013888888888889,
      "grad_norm": 0.14953918755054474,
      "learning_rate": 1.986111111111111e-05,
      "loss": 0.0032,
      "step": 54250
    },
    {
      "epoch": 3.0144444444444445,
      "grad_norm": 0.08931422978639603,
      "learning_rate": 1.9855555555555558e-05,
      "loss": 0.003,
      "step": 54260
    },
    {
      "epoch": 3.015,
      "grad_norm": 0.0915331318974495,
      "learning_rate": 1.985e-05,
      "loss": 0.0027,
      "step": 54270
    },
    {
      "epoch": 3.0155555555555558,
      "grad_norm": 0.17960570752620697,
      "learning_rate": 1.9844444444444445e-05,
      "loss": 0.0034,
      "step": 54280
    },
    {
      "epoch": 3.016111111111111,
      "grad_norm": 0.260795921087265,
      "learning_rate": 1.9838888888888892e-05,
      "loss": 0.0026,
      "step": 54290
    },
    {
      "epoch": 3.0166666666666666,
      "grad_norm": 0.32823169231414795,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0034,
      "step": 54300
    },
    {
      "epoch": 3.0172222222222222,
      "grad_norm": 0.06260131299495697,
      "learning_rate": 1.982777777777778e-05,
      "loss": 0.0026,
      "step": 54310
    },
    {
      "epoch": 3.017777777777778,
      "grad_norm": 0.14928682148456573,
      "learning_rate": 1.9822222222222223e-05,
      "loss": 0.0029,
      "step": 54320
    },
    {
      "epoch": 3.0183333333333335,
      "grad_norm": 0.06046013906598091,
      "learning_rate": 1.9816666666666666e-05,
      "loss": 0.005,
      "step": 54330
    },
    {
      "epoch": 3.0188888888888887,
      "grad_norm": 0.29745835065841675,
      "learning_rate": 1.981111111111111e-05,
      "loss": 0.0019,
      "step": 54340
    },
    {
      "epoch": 3.0194444444444444,
      "grad_norm": 0.09025806188583374,
      "learning_rate": 1.9805555555555557e-05,
      "loss": 0.0021,
      "step": 54350
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.2383853644132614,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0022,
      "step": 54360
    },
    {
      "epoch": 3.0205555555555557,
      "grad_norm": 0.059690773487091064,
      "learning_rate": 1.9794444444444447e-05,
      "loss": 0.0035,
      "step": 54370
    },
    {
      "epoch": 3.0211111111111113,
      "grad_norm": 0.2087201327085495,
      "learning_rate": 1.978888888888889e-05,
      "loss": 0.002,
      "step": 54380
    },
    {
      "epoch": 3.0216666666666665,
      "grad_norm": 0.23819248378276825,
      "learning_rate": 1.9783333333333334e-05,
      "loss": 0.0034,
      "step": 54390
    },
    {
      "epoch": 3.022222222222222,
      "grad_norm": 0.27940717339515686,
      "learning_rate": 1.9777777777777778e-05,
      "loss": 0.0024,
      "step": 54400
    },
    {
      "epoch": 3.022777777777778,
      "grad_norm": 0.06298007816076279,
      "learning_rate": 1.977222222222222e-05,
      "loss": 0.0035,
      "step": 54410
    },
    {
      "epoch": 3.0233333333333334,
      "grad_norm": 0.23817594349384308,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0032,
      "step": 54420
    },
    {
      "epoch": 3.023888888888889,
      "grad_norm": 0.14986583590507507,
      "learning_rate": 1.9761111111111112e-05,
      "loss": 0.0015,
      "step": 54430
    },
    {
      "epoch": 3.0244444444444443,
      "grad_norm": 0.09174371510744095,
      "learning_rate": 1.975555555555556e-05,
      "loss": 0.0025,
      "step": 54440
    },
    {
      "epoch": 3.025,
      "grad_norm": 0.11967268586158752,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.0029,
      "step": 54450
    },
    {
      "epoch": 3.0255555555555556,
      "grad_norm": 0.187212273478508,
      "learning_rate": 1.9744444444444446e-05,
      "loss": 0.0036,
      "step": 54460
    },
    {
      "epoch": 3.026111111111111,
      "grad_norm": 0.618104100227356,
      "learning_rate": 1.973888888888889e-05,
      "loss": 0.0031,
      "step": 54470
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.40015310049057007,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.003,
      "step": 54480
    },
    {
      "epoch": 3.027222222222222,
      "grad_norm": 0.26779550313949585,
      "learning_rate": 1.972777777777778e-05,
      "loss": 0.0036,
      "step": 54490
    },
    {
      "epoch": 3.0277777777777777,
      "grad_norm": 0.09286843985319138,
      "learning_rate": 1.9722222222222224e-05,
      "loss": 0.0025,
      "step": 54500
    },
    {
      "epoch": 3.0283333333333333,
      "grad_norm": 0.5091152191162109,
      "learning_rate": 1.9716666666666667e-05,
      "loss": 0.0036,
      "step": 54510
    },
    {
      "epoch": 3.028888888888889,
      "grad_norm": 0.6255959868431091,
      "learning_rate": 1.971111111111111e-05,
      "loss": 0.004,
      "step": 54520
    },
    {
      "epoch": 3.0294444444444446,
      "grad_norm": 0.06470723450183868,
      "learning_rate": 1.9705555555555558e-05,
      "loss": 0.0028,
      "step": 54530
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.23885534703731537,
      "learning_rate": 1.97e-05,
      "loss": 0.0029,
      "step": 54540
    },
    {
      "epoch": 3.0305555555555554,
      "grad_norm": 0.03087739087641239,
      "learning_rate": 1.9694444444444445e-05,
      "loss": 0.0031,
      "step": 54550
    },
    {
      "epoch": 3.031111111111111,
      "grad_norm": 0.05991951376199722,
      "learning_rate": 1.968888888888889e-05,
      "loss": 0.0043,
      "step": 54560
    },
    {
      "epoch": 3.0316666666666667,
      "grad_norm": 0.11923125386238098,
      "learning_rate": 1.9683333333333335e-05,
      "loss": 0.003,
      "step": 54570
    },
    {
      "epoch": 3.0322222222222224,
      "grad_norm": 0.3759174644947052,
      "learning_rate": 1.967777777777778e-05,
      "loss": 0.0038,
      "step": 54580
    },
    {
      "epoch": 3.0327777777777776,
      "grad_norm": 0.015305614098906517,
      "learning_rate": 1.9672222222222222e-05,
      "loss": 0.0027,
      "step": 54590
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.548412561416626,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0028,
      "step": 54600
    },
    {
      "epoch": 3.033888888888889,
      "grad_norm": 0.07067888975143433,
      "learning_rate": 1.966111111111111e-05,
      "loss": 0.0022,
      "step": 54610
    },
    {
      "epoch": 3.0344444444444445,
      "grad_norm": 0.00824122503399849,
      "learning_rate": 1.9655555555555556e-05,
      "loss": 0.0028,
      "step": 54620
    },
    {
      "epoch": 3.035,
      "grad_norm": 0.010506540536880493,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 0.0034,
      "step": 54630
    },
    {
      "epoch": 3.0355555555555553,
      "grad_norm": 0.4243941903114319,
      "learning_rate": 1.9644444444444447e-05,
      "loss": 0.003,
      "step": 54640
    },
    {
      "epoch": 3.036111111111111,
      "grad_norm": 0.38337427377700806,
      "learning_rate": 1.963888888888889e-05,
      "loss": 0.0028,
      "step": 54650
    },
    {
      "epoch": 3.0366666666666666,
      "grad_norm": 0.20849968492984772,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.0047,
      "step": 54660
    },
    {
      "epoch": 3.0372222222222223,
      "grad_norm": 0.11952191591262817,
      "learning_rate": 1.9627777777777778e-05,
      "loss": 0.0027,
      "step": 54670
    },
    {
      "epoch": 3.037777777777778,
      "grad_norm": 0.09034044295549393,
      "learning_rate": 1.962222222222222e-05,
      "loss": 0.0034,
      "step": 54680
    },
    {
      "epoch": 3.038333333333333,
      "grad_norm": 0.3863287568092346,
      "learning_rate": 1.9616666666666668e-05,
      "loss": 0.0023,
      "step": 54690
    },
    {
      "epoch": 3.0388888888888888,
      "grad_norm": 0.011987976729869843,
      "learning_rate": 1.9611111111111115e-05,
      "loss": 0.0035,
      "step": 54700
    },
    {
      "epoch": 3.0394444444444444,
      "grad_norm": 0.564750075340271,
      "learning_rate": 1.960555555555556e-05,
      "loss": 0.0028,
      "step": 54710
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.259274423122406,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0025,
      "step": 54720
    },
    {
      "epoch": 3.0405555555555557,
      "grad_norm": 0.27670496702194214,
      "learning_rate": 1.9594444444444446e-05,
      "loss": 0.002,
      "step": 54730
    },
    {
      "epoch": 3.0411111111111113,
      "grad_norm": 0.39317455887794495,
      "learning_rate": 1.958888888888889e-05,
      "loss": 0.0034,
      "step": 54740
    },
    {
      "epoch": 3.0416666666666665,
      "grad_norm": 0.3120823800563812,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0031,
      "step": 54750
    },
    {
      "epoch": 3.042222222222222,
      "grad_norm": 0.2669623792171478,
      "learning_rate": 1.957777777777778e-05,
      "loss": 0.0034,
      "step": 54760
    },
    {
      "epoch": 3.042777777777778,
      "grad_norm": 0.3268970251083374,
      "learning_rate": 1.9572222222222223e-05,
      "loss": 0.0025,
      "step": 54770
    },
    {
      "epoch": 3.0433333333333334,
      "grad_norm": 0.26760220527648926,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.0035,
      "step": 54780
    },
    {
      "epoch": 3.043888888888889,
      "grad_norm": 0.11197502166032791,
      "learning_rate": 1.9561111111111114e-05,
      "loss": 0.0028,
      "step": 54790
    },
    {
      "epoch": 3.0444444444444443,
      "grad_norm": 0.18012350797653198,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 0.0032,
      "step": 54800
    },
    {
      "epoch": 3.045,
      "grad_norm": 0.3561815023422241,
      "learning_rate": 1.955e-05,
      "loss": 0.0033,
      "step": 54810
    },
    {
      "epoch": 3.0455555555555556,
      "grad_norm": 0.060284070670604706,
      "learning_rate": 1.9544444444444444e-05,
      "loss": 0.0034,
      "step": 54820
    },
    {
      "epoch": 3.046111111111111,
      "grad_norm": 0.14869163930416107,
      "learning_rate": 1.953888888888889e-05,
      "loss": 0.0039,
      "step": 54830
    },
    {
      "epoch": 3.046666666666667,
      "grad_norm": 0.583607017993927,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0037,
      "step": 54840
    },
    {
      "epoch": 3.047222222222222,
      "grad_norm": 0.3565208315849304,
      "learning_rate": 1.952777777777778e-05,
      "loss": 0.0038,
      "step": 54850
    },
    {
      "epoch": 3.0477777777777777,
      "grad_norm": 0.37736472487449646,
      "learning_rate": 1.9522222222222222e-05,
      "loss": 0.0026,
      "step": 54860
    },
    {
      "epoch": 3.0483333333333333,
      "grad_norm": 0.06196083500981331,
      "learning_rate": 1.9516666666666666e-05,
      "loss": 0.0021,
      "step": 54870
    },
    {
      "epoch": 3.048888888888889,
      "grad_norm": 0.20862650871276855,
      "learning_rate": 1.9511111111111113e-05,
      "loss": 0.0037,
      "step": 54880
    },
    {
      "epoch": 3.0494444444444446,
      "grad_norm": 0.4606602191925049,
      "learning_rate": 1.9505555555555556e-05,
      "loss": 0.0028,
      "step": 54890
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.2975209951400757,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.003,
      "step": 54900
    },
    {
      "epoch": 3.0505555555555555,
      "grad_norm": 0.23891443014144897,
      "learning_rate": 1.9494444444444447e-05,
      "loss": 0.0023,
      "step": 54910
    },
    {
      "epoch": 3.051111111111111,
      "grad_norm": 0.38612788915634155,
      "learning_rate": 1.948888888888889e-05,
      "loss": 0.0031,
      "step": 54920
    },
    {
      "epoch": 3.0516666666666667,
      "grad_norm": 0.20826122164726257,
      "learning_rate": 1.9483333333333334e-05,
      "loss": 0.0039,
      "step": 54930
    },
    {
      "epoch": 3.0522222222222224,
      "grad_norm": 0.013900146819651127,
      "learning_rate": 1.9477777777777777e-05,
      "loss": 0.0031,
      "step": 54940
    },
    {
      "epoch": 3.0527777777777776,
      "grad_norm": 0.32720428705215454,
      "learning_rate": 1.947222222222222e-05,
      "loss": 0.0026,
      "step": 54950
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.1409517228603363,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0033,
      "step": 54960
    },
    {
      "epoch": 3.053888888888889,
      "grad_norm": 0.37789368629455566,
      "learning_rate": 1.9461111111111115e-05,
      "loss": 0.0035,
      "step": 54970
    },
    {
      "epoch": 3.0544444444444445,
      "grad_norm": 0.2465437650680542,
      "learning_rate": 1.9455555555555558e-05,
      "loss": 0.0027,
      "step": 54980
    },
    {
      "epoch": 3.055,
      "grad_norm": 0.2150508463382721,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 0.0035,
      "step": 54990
    },
    {
      "epoch": 3.0555555555555554,
      "grad_norm": 0.6557739973068237,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 0.0037,
      "step": 55000
    },
    {
      "epoch": 3.056111111111111,
      "grad_norm": 0.17828507721424103,
      "learning_rate": 1.943888888888889e-05,
      "loss": 0.0017,
      "step": 55010
    },
    {
      "epoch": 3.0566666666666666,
      "grad_norm": 0.7053372859954834,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0028,
      "step": 55020
    },
    {
      "epoch": 3.0572222222222223,
      "grad_norm": 0.17852868139743805,
      "learning_rate": 1.942777777777778e-05,
      "loss": 0.0027,
      "step": 55030
    },
    {
      "epoch": 3.057777777777778,
      "grad_norm": 0.41593924164772034,
      "learning_rate": 1.9422222222222223e-05,
      "loss": 0.0026,
      "step": 55040
    },
    {
      "epoch": 3.058333333333333,
      "grad_norm": 0.098478302359581,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.0026,
      "step": 55050
    },
    {
      "epoch": 3.0588888888888888,
      "grad_norm": 0.12441103160381317,
      "learning_rate": 1.9411111111111113e-05,
      "loss": 0.0025,
      "step": 55060
    },
    {
      "epoch": 3.0594444444444444,
      "grad_norm": 0.04144697263836861,
      "learning_rate": 1.9405555555555557e-05,
      "loss": 0.0028,
      "step": 55070
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.1653655767440796,
      "learning_rate": 1.94e-05,
      "loss": 0.0041,
      "step": 55080
    },
    {
      "epoch": 3.0605555555555557,
      "grad_norm": 0.19008047878742218,
      "learning_rate": 1.9394444444444444e-05,
      "loss": 0.0029,
      "step": 55090
    },
    {
      "epoch": 3.061111111111111,
      "grad_norm": 0.03180893883109093,
      "learning_rate": 1.938888888888889e-05,
      "loss": 0.0031,
      "step": 55100
    },
    {
      "epoch": 3.0616666666666665,
      "grad_norm": 0.47468289732933044,
      "learning_rate": 1.9383333333333335e-05,
      "loss": 0.0031,
      "step": 55110
    },
    {
      "epoch": 3.062222222222222,
      "grad_norm": 0.039852119982242584,
      "learning_rate": 1.9377777777777778e-05,
      "loss": 0.0025,
      "step": 55120
    },
    {
      "epoch": 3.062777777777778,
      "grad_norm": 0.34871336817741394,
      "learning_rate": 1.9372222222222222e-05,
      "loss": 0.0042,
      "step": 55130
    },
    {
      "epoch": 3.0633333333333335,
      "grad_norm": 0.3273634612560272,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.0041,
      "step": 55140
    },
    {
      "epoch": 3.063888888888889,
      "grad_norm": 0.08991974592208862,
      "learning_rate": 1.9361111111111112e-05,
      "loss": 0.0026,
      "step": 55150
    },
    {
      "epoch": 3.0644444444444443,
      "grad_norm": 0.03468985855579376,
      "learning_rate": 1.9355555555555556e-05,
      "loss": 0.0056,
      "step": 55160
    },
    {
      "epoch": 3.065,
      "grad_norm": 0.03147386759519577,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 0.0035,
      "step": 55170
    },
    {
      "epoch": 3.0655555555555556,
      "grad_norm": 0.21927091479301453,
      "learning_rate": 1.9344444444444446e-05,
      "loss": 0.0034,
      "step": 55180
    },
    {
      "epoch": 3.0661111111111112,
      "grad_norm": 0.33059436082839966,
      "learning_rate": 1.933888888888889e-05,
      "loss": 0.0027,
      "step": 55190
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.06094343587756157,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0041,
      "step": 55200
    },
    {
      "epoch": 3.067222222222222,
      "grad_norm": 0.1789679378271103,
      "learning_rate": 1.9327777777777777e-05,
      "loss": 0.0027,
      "step": 55210
    },
    {
      "epoch": 3.0677777777777777,
      "grad_norm": 0.20789535343647003,
      "learning_rate": 1.932222222222222e-05,
      "loss": 0.0035,
      "step": 55220
    },
    {
      "epoch": 3.0683333333333334,
      "grad_norm": 0.17857633531093597,
      "learning_rate": 1.9316666666666668e-05,
      "loss": 0.0038,
      "step": 55230
    },
    {
      "epoch": 3.068888888888889,
      "grad_norm": 0.2973245680332184,
      "learning_rate": 1.9311111111111114e-05,
      "loss": 0.0032,
      "step": 55240
    },
    {
      "epoch": 3.0694444444444446,
      "grad_norm": 0.41597500443458557,
      "learning_rate": 1.9305555555555558e-05,
      "loss": 0.0019,
      "step": 55250
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.1085730791091919,
      "learning_rate": 1.93e-05,
      "loss": 0.0025,
      "step": 55260
    },
    {
      "epoch": 3.0705555555555555,
      "grad_norm": 0.20897462964057922,
      "learning_rate": 1.9294444444444445e-05,
      "loss": 0.0027,
      "step": 55270
    },
    {
      "epoch": 3.071111111111111,
      "grad_norm": 0.20833782851696014,
      "learning_rate": 1.928888888888889e-05,
      "loss": 0.0027,
      "step": 55280
    },
    {
      "epoch": 3.0716666666666668,
      "grad_norm": 0.1789560168981552,
      "learning_rate": 1.9283333333333332e-05,
      "loss": 0.0033,
      "step": 55290
    },
    {
      "epoch": 3.0722222222222224,
      "grad_norm": 0.2087288349866867,
      "learning_rate": 1.927777777777778e-05,
      "loss": 0.0041,
      "step": 55300
    },
    {
      "epoch": 3.0727777777777776,
      "grad_norm": 0.4542352259159088,
      "learning_rate": 1.9272222222222223e-05,
      "loss": 0.0025,
      "step": 55310
    },
    {
      "epoch": 3.0733333333333333,
      "grad_norm": 0.29718223214149475,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0037,
      "step": 55320
    },
    {
      "epoch": 3.073888888888889,
      "grad_norm": 0.0749979093670845,
      "learning_rate": 1.9261111111111113e-05,
      "loss": 0.0039,
      "step": 55330
    },
    {
      "epoch": 3.0744444444444445,
      "grad_norm": 0.0911875069141388,
      "learning_rate": 1.9255555555555557e-05,
      "loss": 0.003,
      "step": 55340
    },
    {
      "epoch": 3.075,
      "grad_norm": 0.46039441227912903,
      "learning_rate": 1.925e-05,
      "loss": 0.0017,
      "step": 55350
    },
    {
      "epoch": 3.0755555555555554,
      "grad_norm": 0.570669412612915,
      "learning_rate": 1.9244444444444444e-05,
      "loss": 0.0026,
      "step": 55360
    },
    {
      "epoch": 3.076111111111111,
      "grad_norm": 0.5329403281211853,
      "learning_rate": 1.923888888888889e-05,
      "loss": 0.0032,
      "step": 55370
    },
    {
      "epoch": 3.0766666666666667,
      "grad_norm": 0.059693459421396255,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0027,
      "step": 55380
    },
    {
      "epoch": 3.0772222222222223,
      "grad_norm": 0.14933030307292938,
      "learning_rate": 1.9227777777777778e-05,
      "loss": 0.0038,
      "step": 55390
    },
    {
      "epoch": 3.077777777777778,
      "grad_norm": 0.20857644081115723,
      "learning_rate": 1.922222222222222e-05,
      "loss": 0.0033,
      "step": 55400
    },
    {
      "epoch": 3.078333333333333,
      "grad_norm": 0.013749458827078342,
      "learning_rate": 1.921666666666667e-05,
      "loss": 0.0035,
      "step": 55410
    },
    {
      "epoch": 3.078888888888889,
      "grad_norm": 0.26757192611694336,
      "learning_rate": 1.9211111111111112e-05,
      "loss": 0.0035,
      "step": 55420
    },
    {
      "epoch": 3.0794444444444444,
      "grad_norm": 0.2972344160079956,
      "learning_rate": 1.9205555555555556e-05,
      "loss": 0.0024,
      "step": 55430
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.32675713300704956,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0031,
      "step": 55440
    },
    {
      "epoch": 3.0805555555555557,
      "grad_norm": 0.297911673784256,
      "learning_rate": 1.9194444444444446e-05,
      "loss": 0.0035,
      "step": 55450
    },
    {
      "epoch": 3.081111111111111,
      "grad_norm": 0.27232110500335693,
      "learning_rate": 1.918888888888889e-05,
      "loss": 0.0044,
      "step": 55460
    },
    {
      "epoch": 3.0816666666666666,
      "grad_norm": 0.26723530888557434,
      "learning_rate": 1.9183333333333333e-05,
      "loss": 0.0034,
      "step": 55470
    },
    {
      "epoch": 3.082222222222222,
      "grad_norm": 0.32691553235054016,
      "learning_rate": 1.9177777777777777e-05,
      "loss": 0.004,
      "step": 55480
    },
    {
      "epoch": 3.082777777777778,
      "grad_norm": 0.030776094645261765,
      "learning_rate": 1.917222222222222e-05,
      "loss": 0.0037,
      "step": 55490
    },
    {
      "epoch": 3.0833333333333335,
      "grad_norm": 0.01457013189792633,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0035,
      "step": 55500
    },
    {
      "epoch": 3.0838888888888887,
      "grad_norm": 0.14866669476032257,
      "learning_rate": 1.9161111111111114e-05,
      "loss": 0.0033,
      "step": 55510
    },
    {
      "epoch": 3.0844444444444443,
      "grad_norm": 0.341268926858902,
      "learning_rate": 1.9155555555555558e-05,
      "loss": 0.0033,
      "step": 55520
    },
    {
      "epoch": 3.085,
      "grad_norm": 0.14840365946292877,
      "learning_rate": 1.915e-05,
      "loss": 0.0037,
      "step": 55530
    },
    {
      "epoch": 3.0855555555555556,
      "grad_norm": 0.12030600011348724,
      "learning_rate": 1.9144444444444445e-05,
      "loss": 0.0026,
      "step": 55540
    },
    {
      "epoch": 3.0861111111111112,
      "grad_norm": 0.12510761618614197,
      "learning_rate": 1.913888888888889e-05,
      "loss": 0.0034,
      "step": 55550
    },
    {
      "epoch": 3.086666666666667,
      "grad_norm": 0.39506322145462036,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0041,
      "step": 55560
    },
    {
      "epoch": 3.087222222222222,
      "grad_norm": 0.22494952380657196,
      "learning_rate": 1.912777777777778e-05,
      "loss": 0.0025,
      "step": 55570
    },
    {
      "epoch": 3.0877777777777777,
      "grad_norm": 0.2671925723552704,
      "learning_rate": 1.9122222222222222e-05,
      "loss": 0.0022,
      "step": 55580
    },
    {
      "epoch": 3.0883333333333334,
      "grad_norm": 0.23862145841121674,
      "learning_rate": 1.911666666666667e-05,
      "loss": 0.0024,
      "step": 55590
    },
    {
      "epoch": 3.088888888888889,
      "grad_norm": 0.267200767993927,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.0018,
      "step": 55600
    },
    {
      "epoch": 3.089444444444444,
      "grad_norm": 0.33942821621894836,
      "learning_rate": 1.9105555555555557e-05,
      "loss": 0.0032,
      "step": 55610
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.11195828765630722,
      "learning_rate": 1.91e-05,
      "loss": 0.0033,
      "step": 55620
    },
    {
      "epoch": 3.0905555555555555,
      "grad_norm": 0.11936821043491364,
      "learning_rate": 1.9094444444444447e-05,
      "loss": 0.0027,
      "step": 55630
    },
    {
      "epoch": 3.091111111111111,
      "grad_norm": 0.1251637041568756,
      "learning_rate": 1.908888888888889e-05,
      "loss": 0.0018,
      "step": 55640
    },
    {
      "epoch": 3.091666666666667,
      "grad_norm": 0.17106352746486664,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.0029,
      "step": 55650
    },
    {
      "epoch": 3.0922222222222224,
      "grad_norm": 0.035094935446977615,
      "learning_rate": 1.9077777777777778e-05,
      "loss": 0.0028,
      "step": 55660
    },
    {
      "epoch": 3.0927777777777776,
      "grad_norm": 0.4162101447582245,
      "learning_rate": 1.907222222222222e-05,
      "loss": 0.0037,
      "step": 55670
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.241062194108963,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0025,
      "step": 55680
    },
    {
      "epoch": 3.093888888888889,
      "grad_norm": 0.3363575339317322,
      "learning_rate": 1.9061111111111112e-05,
      "loss": 0.0028,
      "step": 55690
    },
    {
      "epoch": 3.0944444444444446,
      "grad_norm": 0.012205990962684155,
      "learning_rate": 1.905555555555556e-05,
      "loss": 0.003,
      "step": 55700
    },
    {
      "epoch": 3.095,
      "grad_norm": 0.29687532782554626,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 0.0028,
      "step": 55710
    },
    {
      "epoch": 3.0955555555555554,
      "grad_norm": 0.0157475583255291,
      "learning_rate": 1.9044444444444446e-05,
      "loss": 0.0035,
      "step": 55720
    },
    {
      "epoch": 3.096111111111111,
      "grad_norm": 0.3652922511100769,
      "learning_rate": 1.903888888888889e-05,
      "loss": 0.0033,
      "step": 55730
    },
    {
      "epoch": 3.0966666666666667,
      "grad_norm": 0.0895138829946518,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0039,
      "step": 55740
    },
    {
      "epoch": 3.0972222222222223,
      "grad_norm": 0.21619689464569092,
      "learning_rate": 1.9027777777777776e-05,
      "loss": 0.003,
      "step": 55750
    },
    {
      "epoch": 3.097777777777778,
      "grad_norm": 0.1194763258099556,
      "learning_rate": 1.9022222222222223e-05,
      "loss": 0.0039,
      "step": 55760
    },
    {
      "epoch": 3.098333333333333,
      "grad_norm": 0.06055428460240364,
      "learning_rate": 1.901666666666667e-05,
      "loss": 0.0037,
      "step": 55770
    },
    {
      "epoch": 3.098888888888889,
      "grad_norm": 0.4903009831905365,
      "learning_rate": 1.9011111111111114e-05,
      "loss": 0.0037,
      "step": 55780
    },
    {
      "epoch": 3.0994444444444444,
      "grad_norm": 0.5307074189186096,
      "learning_rate": 1.9005555555555557e-05,
      "loss": 0.0027,
      "step": 55790
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.37033993005752563,
      "learning_rate": 1.9e-05,
      "loss": 0.0036,
      "step": 55800
    },
    {
      "epoch": 3.1005555555555557,
      "grad_norm": 0.29998311400413513,
      "learning_rate": 1.8994444444444445e-05,
      "loss": 0.0032,
      "step": 55810
    },
    {
      "epoch": 3.101111111111111,
      "grad_norm": 0.2401457577943802,
      "learning_rate": 1.8988888888888888e-05,
      "loss": 0.0031,
      "step": 55820
    },
    {
      "epoch": 3.1016666666666666,
      "grad_norm": 0.5307314395904541,
      "learning_rate": 1.8983333333333335e-05,
      "loss": 0.0026,
      "step": 55830
    },
    {
      "epoch": 3.102222222222222,
      "grad_norm": 0.11924613267183304,
      "learning_rate": 1.897777777777778e-05,
      "loss": 0.0041,
      "step": 55840
    },
    {
      "epoch": 3.102777777777778,
      "grad_norm": 0.09057898074388504,
      "learning_rate": 1.8972222222222222e-05,
      "loss": 0.0026,
      "step": 55850
    },
    {
      "epoch": 3.1033333333333335,
      "grad_norm": 0.06047331169247627,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0031,
      "step": 55860
    },
    {
      "epoch": 3.1038888888888887,
      "grad_norm": 0.3565879762172699,
      "learning_rate": 1.8961111111111113e-05,
      "loss": 0.0032,
      "step": 55870
    },
    {
      "epoch": 3.1044444444444443,
      "grad_norm": 0.6445498466491699,
      "learning_rate": 1.8955555555555556e-05,
      "loss": 0.0034,
      "step": 55880
    },
    {
      "epoch": 3.105,
      "grad_norm": 0.03078474849462509,
      "learning_rate": 1.895e-05,
      "loss": 0.0032,
      "step": 55890
    },
    {
      "epoch": 3.1055555555555556,
      "grad_norm": 0.40134546160697937,
      "learning_rate": 1.8944444444444447e-05,
      "loss": 0.004,
      "step": 55900
    },
    {
      "epoch": 3.1061111111111113,
      "grad_norm": 0.20102006196975708,
      "learning_rate": 1.893888888888889e-05,
      "loss": 0.0038,
      "step": 55910
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.3847125172615051,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0025,
      "step": 55920
    },
    {
      "epoch": 3.107222222222222,
      "grad_norm": 0.5174071788787842,
      "learning_rate": 1.8927777777777777e-05,
      "loss": 0.0029,
      "step": 55930
    },
    {
      "epoch": 3.1077777777777778,
      "grad_norm": 0.06175672262907028,
      "learning_rate": 1.8922222222222224e-05,
      "loss": 0.0026,
      "step": 55940
    },
    {
      "epoch": 3.1083333333333334,
      "grad_norm": 0.18162085115909576,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.0025,
      "step": 55950
    },
    {
      "epoch": 3.108888888888889,
      "grad_norm": 0.17791803181171417,
      "learning_rate": 1.891111111111111e-05,
      "loss": 0.0027,
      "step": 55960
    },
    {
      "epoch": 3.1094444444444442,
      "grad_norm": 0.060930561274290085,
      "learning_rate": 1.890555555555556e-05,
      "loss": 0.0034,
      "step": 55970
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.26719704270362854,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.0027,
      "step": 55980
    },
    {
      "epoch": 3.1105555555555555,
      "grad_norm": 0.06121372431516647,
      "learning_rate": 1.8894444444444446e-05,
      "loss": 0.003,
      "step": 55990
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.06663037091493607,
      "learning_rate": 1.888888888888889e-05,
      "loss": 0.0033,
      "step": 56000
    },
    {
      "epoch": 3.111666666666667,
      "grad_norm": 0.0331282764673233,
      "learning_rate": 1.8883333333333333e-05,
      "loss": 0.0029,
      "step": 56010
    },
    {
      "epoch": 3.112222222222222,
      "grad_norm": 0.6696751713752747,
      "learning_rate": 1.8877777777777776e-05,
      "loss": 0.0044,
      "step": 56020
    },
    {
      "epoch": 3.1127777777777776,
      "grad_norm": 0.23775717616081238,
      "learning_rate": 1.8872222222222223e-05,
      "loss": 0.0023,
      "step": 56030
    },
    {
      "epoch": 3.1133333333333333,
      "grad_norm": 0.2671297490596771,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0023,
      "step": 56040
    },
    {
      "epoch": 3.113888888888889,
      "grad_norm": 0.031661782413721085,
      "learning_rate": 1.8861111111111114e-05,
      "loss": 0.0032,
      "step": 56050
    },
    {
      "epoch": 3.1144444444444446,
      "grad_norm": 0.31429851055145264,
      "learning_rate": 1.8855555555555557e-05,
      "loss": 0.003,
      "step": 56060
    },
    {
      "epoch": 3.115,
      "grad_norm": 0.03128660470247269,
      "learning_rate": 1.885e-05,
      "loss": 0.0033,
      "step": 56070
    },
    {
      "epoch": 3.1155555555555554,
      "grad_norm": 0.08984924852848053,
      "learning_rate": 1.8844444444444444e-05,
      "loss": 0.0039,
      "step": 56080
    },
    {
      "epoch": 3.116111111111111,
      "grad_norm": 0.11902233958244324,
      "learning_rate": 1.8838888888888888e-05,
      "loss": 0.0027,
      "step": 56090
    },
    {
      "epoch": 3.1166666666666667,
      "grad_norm": 0.06266417354345322,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0034,
      "step": 56100
    },
    {
      "epoch": 3.1172222222222223,
      "grad_norm": 0.06014364957809448,
      "learning_rate": 1.882777777777778e-05,
      "loss": 0.0033,
      "step": 56110
    },
    {
      "epoch": 3.117777777777778,
      "grad_norm": 0.060700517147779465,
      "learning_rate": 1.8822222222222225e-05,
      "loss": 0.0029,
      "step": 56120
    },
    {
      "epoch": 3.118333333333333,
      "grad_norm": 0.06092233583331108,
      "learning_rate": 1.881666666666667e-05,
      "loss": 0.003,
      "step": 56130
    },
    {
      "epoch": 3.118888888888889,
      "grad_norm": 0.06828102469444275,
      "learning_rate": 1.8811111111111112e-05,
      "loss": 0.0029,
      "step": 56140
    },
    {
      "epoch": 3.1194444444444445,
      "grad_norm": 0.11072739958763123,
      "learning_rate": 1.8805555555555556e-05,
      "loss": 0.0039,
      "step": 56150
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.06447194516658783,
      "learning_rate": 1.88e-05,
      "loss": 0.0036,
      "step": 56160
    },
    {
      "epoch": 3.1205555555555557,
      "grad_norm": 0.1787538379430771,
      "learning_rate": 1.8794444444444447e-05,
      "loss": 0.0032,
      "step": 56170
    },
    {
      "epoch": 3.121111111111111,
      "grad_norm": 0.2711835503578186,
      "learning_rate": 1.878888888888889e-05,
      "loss": 0.0031,
      "step": 56180
    },
    {
      "epoch": 3.1216666666666666,
      "grad_norm": 0.12071889638900757,
      "learning_rate": 1.8783333333333334e-05,
      "loss": 0.0035,
      "step": 56190
    },
    {
      "epoch": 3.1222222222222222,
      "grad_norm": 0.06038619950413704,
      "learning_rate": 1.8777777777777777e-05,
      "loss": 0.0032,
      "step": 56200
    },
    {
      "epoch": 3.122777777777778,
      "grad_norm": 0.38996848464012146,
      "learning_rate": 1.8772222222222224e-05,
      "loss": 0.0035,
      "step": 56210
    },
    {
      "epoch": 3.1233333333333335,
      "grad_norm": 0.061569131910800934,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.0037,
      "step": 56220
    },
    {
      "epoch": 3.1238888888888887,
      "grad_norm": 0.23754636943340302,
      "learning_rate": 1.876111111111111e-05,
      "loss": 0.0028,
      "step": 56230
    },
    {
      "epoch": 3.1244444444444444,
      "grad_norm": 0.17959962785243988,
      "learning_rate": 1.8755555555555558e-05,
      "loss": 0.003,
      "step": 56240
    },
    {
      "epoch": 3.125,
      "grad_norm": 0.10619888454675674,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.0035,
      "step": 56250
    },
    {
      "epoch": 3.1255555555555556,
      "grad_norm": 0.35855481028556824,
      "learning_rate": 1.8744444444444445e-05,
      "loss": 0.0037,
      "step": 56260
    },
    {
      "epoch": 3.1261111111111113,
      "grad_norm": 0.006714658811688423,
      "learning_rate": 1.873888888888889e-05,
      "loss": 0.0024,
      "step": 56270
    },
    {
      "epoch": 3.1266666666666665,
      "grad_norm": 0.19743847846984863,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.003,
      "step": 56280
    },
    {
      "epoch": 3.127222222222222,
      "grad_norm": 0.17904074490070343,
      "learning_rate": 1.8727777777777776e-05,
      "loss": 0.0028,
      "step": 56290
    },
    {
      "epoch": 3.1277777777777778,
      "grad_norm": 0.34979236125946045,
      "learning_rate": 1.8722222222222223e-05,
      "loss": 0.0024,
      "step": 56300
    },
    {
      "epoch": 3.1283333333333334,
      "grad_norm": 0.6437317728996277,
      "learning_rate": 1.871666666666667e-05,
      "loss": 0.0036,
      "step": 56310
    },
    {
      "epoch": 3.128888888888889,
      "grad_norm": 0.10919196158647537,
      "learning_rate": 1.8711111111111113e-05,
      "loss": 0.0018,
      "step": 56320
    },
    {
      "epoch": 3.1294444444444443,
      "grad_norm": 0.11979018896818161,
      "learning_rate": 1.8705555555555557e-05,
      "loss": 0.0027,
      "step": 56330
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.060016270726919174,
      "learning_rate": 1.87e-05,
      "loss": 0.0037,
      "step": 56340
    },
    {
      "epoch": 3.1305555555555555,
      "grad_norm": 0.14044834673404694,
      "learning_rate": 1.8694444444444444e-05,
      "loss": 0.0031,
      "step": 56350
    },
    {
      "epoch": 3.131111111111111,
      "grad_norm": 0.4164176881313324,
      "learning_rate": 1.8688888888888888e-05,
      "loss": 0.0022,
      "step": 56360
    },
    {
      "epoch": 3.131666666666667,
      "grad_norm": 0.2803545892238617,
      "learning_rate": 1.8683333333333335e-05,
      "loss": 0.0029,
      "step": 56370
    },
    {
      "epoch": 3.132222222222222,
      "grad_norm": 0.07409512996673584,
      "learning_rate": 1.8677777777777778e-05,
      "loss": 0.0022,
      "step": 56380
    },
    {
      "epoch": 3.1327777777777777,
      "grad_norm": 0.6818438172340393,
      "learning_rate": 1.8672222222222225e-05,
      "loss": 0.0039,
      "step": 56390
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.09109903126955032,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0026,
      "step": 56400
    },
    {
      "epoch": 3.133888888888889,
      "grad_norm": 0.06021234020590782,
      "learning_rate": 1.8661111111111112e-05,
      "loss": 0.003,
      "step": 56410
    },
    {
      "epoch": 3.1344444444444446,
      "grad_norm": 0.06067650392651558,
      "learning_rate": 1.8655555555555556e-05,
      "loss": 0.0032,
      "step": 56420
    },
    {
      "epoch": 3.135,
      "grad_norm": 0.17864786088466644,
      "learning_rate": 1.865e-05,
      "loss": 0.0032,
      "step": 56430
    },
    {
      "epoch": 3.1355555555555554,
      "grad_norm": 0.2669621706008911,
      "learning_rate": 1.8644444444444446e-05,
      "loss": 0.0026,
      "step": 56440
    },
    {
      "epoch": 3.136111111111111,
      "grad_norm": 0.010481014847755432,
      "learning_rate": 1.863888888888889e-05,
      "loss": 0.0029,
      "step": 56450
    },
    {
      "epoch": 3.1366666666666667,
      "grad_norm": 0.09072081744670868,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.0022,
      "step": 56460
    },
    {
      "epoch": 3.1372222222222224,
      "grad_norm": 0.11953248083591461,
      "learning_rate": 1.8627777777777777e-05,
      "loss": 0.0033,
      "step": 56470
    },
    {
      "epoch": 3.137777777777778,
      "grad_norm": 0.23737381398677826,
      "learning_rate": 1.8622222222222224e-05,
      "loss": 0.0031,
      "step": 56480
    },
    {
      "epoch": 3.138333333333333,
      "grad_norm": 0.09005092829465866,
      "learning_rate": 1.8616666666666667e-05,
      "loss": 0.0025,
      "step": 56490
    },
    {
      "epoch": 3.138888888888889,
      "grad_norm": 0.06886512786149979,
      "learning_rate": 1.861111111111111e-05,
      "loss": 0.0034,
      "step": 56500
    },
    {
      "epoch": 3.1394444444444445,
      "grad_norm": 0.29949402809143066,
      "learning_rate": 1.8605555555555558e-05,
      "loss": 0.0027,
      "step": 56510
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.14831678569316864,
      "learning_rate": 1.86e-05,
      "loss": 0.0018,
      "step": 56520
    },
    {
      "epoch": 3.1405555555555553,
      "grad_norm": 0.06889103353023529,
      "learning_rate": 1.8594444444444445e-05,
      "loss": 0.0022,
      "step": 56530
    },
    {
      "epoch": 3.141111111111111,
      "grad_norm": 0.05983031913638115,
      "learning_rate": 1.858888888888889e-05,
      "loss": 0.0019,
      "step": 56540
    },
    {
      "epoch": 3.1416666666666666,
      "grad_norm": 0.18875129520893097,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.0026,
      "step": 56550
    },
    {
      "epoch": 3.1422222222222222,
      "grad_norm": 0.08989816904067993,
      "learning_rate": 1.8577777777777776e-05,
      "loss": 0.0031,
      "step": 56560
    },
    {
      "epoch": 3.142777777777778,
      "grad_norm": 0.14886799454689026,
      "learning_rate": 1.8572222222222223e-05,
      "loss": 0.0039,
      "step": 56570
    },
    {
      "epoch": 3.1433333333333335,
      "grad_norm": 0.1505359262228012,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0044,
      "step": 56580
    },
    {
      "epoch": 3.1438888888888887,
      "grad_norm": 0.033806126564741135,
      "learning_rate": 1.8561111111111113e-05,
      "loss": 0.0026,
      "step": 56590
    },
    {
      "epoch": 3.1444444444444444,
      "grad_norm": 0.11887189000844955,
      "learning_rate": 1.8555555555555557e-05,
      "loss": 0.0043,
      "step": 56600
    },
    {
      "epoch": 3.145,
      "grad_norm": 0.09000176191329956,
      "learning_rate": 1.855e-05,
      "loss": 0.0031,
      "step": 56610
    },
    {
      "epoch": 3.1455555555555557,
      "grad_norm": 0.5831958055496216,
      "learning_rate": 1.8544444444444444e-05,
      "loss": 0.0027,
      "step": 56620
    },
    {
      "epoch": 3.1461111111111113,
      "grad_norm": 0.3960646688938141,
      "learning_rate": 1.8538888888888887e-05,
      "loss": 0.0025,
      "step": 56630
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.30200275778770447,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0028,
      "step": 56640
    },
    {
      "epoch": 3.147222222222222,
      "grad_norm": 0.17877426743507385,
      "learning_rate": 1.852777777777778e-05,
      "loss": 0.0029,
      "step": 56650
    },
    {
      "epoch": 3.147777777777778,
      "grad_norm": 0.3084712624549866,
      "learning_rate": 1.8522222222222225e-05,
      "loss": 0.0021,
      "step": 56660
    },
    {
      "epoch": 3.1483333333333334,
      "grad_norm": 0.14908121526241302,
      "learning_rate": 1.851666666666667e-05,
      "loss": 0.0039,
      "step": 56670
    },
    {
      "epoch": 3.148888888888889,
      "grad_norm": 0.23220327496528625,
      "learning_rate": 1.8511111111111112e-05,
      "loss": 0.0032,
      "step": 56680
    },
    {
      "epoch": 3.1494444444444443,
      "grad_norm": 0.14002051949501038,
      "learning_rate": 1.8505555555555556e-05,
      "loss": 0.004,
      "step": 56690
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.2993170917034149,
      "learning_rate": 1.85e-05,
      "loss": 0.0027,
      "step": 56700
    },
    {
      "epoch": 3.1505555555555556,
      "grad_norm": 0.03184754401445389,
      "learning_rate": 1.8494444444444446e-05,
      "loss": 0.0045,
      "step": 56710
    },
    {
      "epoch": 3.151111111111111,
      "grad_norm": 0.04942351207137108,
      "learning_rate": 1.848888888888889e-05,
      "loss": 0.0033,
      "step": 56720
    },
    {
      "epoch": 3.151666666666667,
      "grad_norm": 0.42563268542289734,
      "learning_rate": 1.8483333333333333e-05,
      "loss": 0.0038,
      "step": 56730
    },
    {
      "epoch": 3.152222222222222,
      "grad_norm": 0.624705970287323,
      "learning_rate": 1.847777777777778e-05,
      "loss": 0.002,
      "step": 56740
    },
    {
      "epoch": 3.1527777777777777,
      "grad_norm": 0.32729125022888184,
      "learning_rate": 1.8472222222222224e-05,
      "loss": 0.0032,
      "step": 56750
    },
    {
      "epoch": 3.1533333333333333,
      "grad_norm": 0.262480765581131,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.004,
      "step": 56760
    },
    {
      "epoch": 3.153888888888889,
      "grad_norm": 0.09040210396051407,
      "learning_rate": 1.846111111111111e-05,
      "loss": 0.0029,
      "step": 56770
    },
    {
      "epoch": 3.1544444444444446,
      "grad_norm": 0.38616615533828735,
      "learning_rate": 1.8455555555555558e-05,
      "loss": 0.0017,
      "step": 56780
    },
    {
      "epoch": 3.155,
      "grad_norm": 0.20769332349300385,
      "learning_rate": 1.845e-05,
      "loss": 0.0018,
      "step": 56790
    },
    {
      "epoch": 3.1555555555555554,
      "grad_norm": 0.032295431941747665,
      "learning_rate": 1.8444444444444445e-05,
      "loss": 0.003,
      "step": 56800
    },
    {
      "epoch": 3.156111111111111,
      "grad_norm": 0.012399474158883095,
      "learning_rate": 1.843888888888889e-05,
      "loss": 0.0035,
      "step": 56810
    },
    {
      "epoch": 3.1566666666666667,
      "grad_norm": 0.013124858029186726,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.003,
      "step": 56820
    },
    {
      "epoch": 3.1572222222222224,
      "grad_norm": 0.2674752473831177,
      "learning_rate": 1.842777777777778e-05,
      "loss": 0.0034,
      "step": 56830
    },
    {
      "epoch": 3.1577777777777776,
      "grad_norm": 0.21033862233161926,
      "learning_rate": 1.8422222222222222e-05,
      "loss": 0.0041,
      "step": 56840
    },
    {
      "epoch": 3.158333333333333,
      "grad_norm": 0.5288419723510742,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.0026,
      "step": 56850
    },
    {
      "epoch": 3.158888888888889,
      "grad_norm": 0.06002397462725639,
      "learning_rate": 1.8411111111111113e-05,
      "loss": 0.0033,
      "step": 56860
    },
    {
      "epoch": 3.1594444444444445,
      "grad_norm": 0.12008422613143921,
      "learning_rate": 1.8405555555555556e-05,
      "loss": 0.0019,
      "step": 56870
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.030658641830086708,
      "learning_rate": 1.84e-05,
      "loss": 0.0016,
      "step": 56880
    },
    {
      "epoch": 3.160555555555556,
      "grad_norm": 0.32692602276802063,
      "learning_rate": 1.8394444444444444e-05,
      "loss": 0.0028,
      "step": 56890
    },
    {
      "epoch": 3.161111111111111,
      "grad_norm": 0.11904580891132355,
      "learning_rate": 1.838888888888889e-05,
      "loss": 0.002,
      "step": 56900
    },
    {
      "epoch": 3.1616666666666666,
      "grad_norm": 0.0600164458155632,
      "learning_rate": 1.8383333333333334e-05,
      "loss": 0.002,
      "step": 56910
    },
    {
      "epoch": 3.1622222222222223,
      "grad_norm": 0.06083250045776367,
      "learning_rate": 1.837777777777778e-05,
      "loss": 0.0027,
      "step": 56920
    },
    {
      "epoch": 3.162777777777778,
      "grad_norm": 0.149371936917305,
      "learning_rate": 1.8372222222222225e-05,
      "loss": 0.003,
      "step": 56930
    },
    {
      "epoch": 3.163333333333333,
      "grad_norm": 0.38564375042915344,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.002,
      "step": 56940
    },
    {
      "epoch": 3.1638888888888888,
      "grad_norm": 0.06269794702529907,
      "learning_rate": 1.836111111111111e-05,
      "loss": 0.0023,
      "step": 56950
    },
    {
      "epoch": 3.1644444444444444,
      "grad_norm": 0.5692745447158813,
      "learning_rate": 1.8355555555555555e-05,
      "loss": 0.0039,
      "step": 56960
    },
    {
      "epoch": 3.165,
      "grad_norm": 0.21232397854328156,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 0.0027,
      "step": 56970
    },
    {
      "epoch": 3.1655555555555557,
      "grad_norm": 0.036203835159540176,
      "learning_rate": 1.8344444444444446e-05,
      "loss": 0.002,
      "step": 56980
    },
    {
      "epoch": 3.1661111111111113,
      "grad_norm": 0.11963988840579987,
      "learning_rate": 1.833888888888889e-05,
      "loss": 0.0039,
      "step": 56990
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.5283221006393433,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0028,
      "step": 57000
    },
    {
      "epoch": 3.167222222222222,
      "grad_norm": 0.03174225240945816,
      "learning_rate": 1.832777777777778e-05,
      "loss": 0.0039,
      "step": 57010
    },
    {
      "epoch": 3.167777777777778,
      "grad_norm": 0.416187584400177,
      "learning_rate": 1.8322222222222223e-05,
      "loss": 0.0023,
      "step": 57020
    },
    {
      "epoch": 3.1683333333333334,
      "grad_norm": 0.014003385789692402,
      "learning_rate": 1.8316666666666667e-05,
      "loss": 0.0048,
      "step": 57030
    },
    {
      "epoch": 3.168888888888889,
      "grad_norm": 0.03175368532538414,
      "learning_rate": 1.8311111111111114e-05,
      "loss": 0.0022,
      "step": 57040
    },
    {
      "epoch": 3.1694444444444443,
      "grad_norm": 0.47483232617378235,
      "learning_rate": 1.8305555555555557e-05,
      "loss": 0.0022,
      "step": 57050
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.149667426943779,
      "learning_rate": 1.83e-05,
      "loss": 0.0043,
      "step": 57060
    },
    {
      "epoch": 3.1705555555555556,
      "grad_norm": 0.2685878276824951,
      "learning_rate": 1.8294444444444445e-05,
      "loss": 0.0026,
      "step": 57070
    },
    {
      "epoch": 3.171111111111111,
      "grad_norm": 0.031198153272271156,
      "learning_rate": 1.8288888888888888e-05,
      "loss": 0.0026,
      "step": 57080
    },
    {
      "epoch": 3.171666666666667,
      "grad_norm": 0.06106628105044365,
      "learning_rate": 1.828333333333333e-05,
      "loss": 0.0024,
      "step": 57090
    },
    {
      "epoch": 3.172222222222222,
      "grad_norm": 0.74174565076828,
      "learning_rate": 1.827777777777778e-05,
      "loss": 0.0026,
      "step": 57100
    },
    {
      "epoch": 3.1727777777777777,
      "grad_norm": 0.08926843851804733,
      "learning_rate": 1.8272222222222226e-05,
      "loss": 0.0017,
      "step": 57110
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.6331762671470642,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0032,
      "step": 57120
    },
    {
      "epoch": 3.173888888888889,
      "grad_norm": 0.3001706302165985,
      "learning_rate": 1.8261111111111113e-05,
      "loss": 0.003,
      "step": 57130
    },
    {
      "epoch": 3.1744444444444446,
      "grad_norm": 0.14877240359783173,
      "learning_rate": 1.8255555555555556e-05,
      "loss": 0.003,
      "step": 57140
    },
    {
      "epoch": 3.175,
      "grad_norm": 0.0608181469142437,
      "learning_rate": 1.825e-05,
      "loss": 0.003,
      "step": 57150
    },
    {
      "epoch": 3.1755555555555555,
      "grad_norm": 0.08071798831224442,
      "learning_rate": 1.8244444444444443e-05,
      "loss": 0.0033,
      "step": 57160
    },
    {
      "epoch": 3.176111111111111,
      "grad_norm": 0.17828534543514252,
      "learning_rate": 1.823888888888889e-05,
      "loss": 0.002,
      "step": 57170
    },
    {
      "epoch": 3.1766666666666667,
      "grad_norm": 0.4336259961128235,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.0025,
      "step": 57180
    },
    {
      "epoch": 3.1772222222222224,
      "grad_norm": 0.45676878094673157,
      "learning_rate": 1.822777777777778e-05,
      "loss": 0.0025,
      "step": 57190
    },
    {
      "epoch": 3.1777777777777776,
      "grad_norm": 0.14893361926078796,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 0.0025,
      "step": 57200
    },
    {
      "epoch": 3.1783333333333332,
      "grad_norm": 0.20840243995189667,
      "learning_rate": 1.8216666666666668e-05,
      "loss": 0.0029,
      "step": 57210
    },
    {
      "epoch": 3.178888888888889,
      "grad_norm": 0.37080875039100647,
      "learning_rate": 1.821111111111111e-05,
      "loss": 0.0026,
      "step": 57220
    },
    {
      "epoch": 3.1794444444444445,
      "grad_norm": 0.31929636001586914,
      "learning_rate": 1.8205555555555555e-05,
      "loss": 0.0042,
      "step": 57230
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.17864990234375,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0037,
      "step": 57240
    },
    {
      "epoch": 3.1805555555555554,
      "grad_norm": 0.35630378127098083,
      "learning_rate": 1.8194444444444445e-05,
      "loss": 0.004,
      "step": 57250
    },
    {
      "epoch": 3.181111111111111,
      "grad_norm": 0.14971381425857544,
      "learning_rate": 1.818888888888889e-05,
      "loss": 0.0018,
      "step": 57260
    },
    {
      "epoch": 3.1816666666666666,
      "grad_norm": 0.47539854049682617,
      "learning_rate": 1.8183333333333336e-05,
      "loss": 0.0038,
      "step": 57270
    },
    {
      "epoch": 3.1822222222222223,
      "grad_norm": 0.060235798358917236,
      "learning_rate": 1.817777777777778e-05,
      "loss": 0.0023,
      "step": 57280
    },
    {
      "epoch": 3.182777777777778,
      "grad_norm": 0.2976248562335968,
      "learning_rate": 1.8172222222222223e-05,
      "loss": 0.0023,
      "step": 57290
    },
    {
      "epoch": 3.183333333333333,
      "grad_norm": 0.5949270725250244,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0023,
      "step": 57300
    },
    {
      "epoch": 3.1838888888888888,
      "grad_norm": 0.23867473006248474,
      "learning_rate": 1.8161111111111114e-05,
      "loss": 0.0041,
      "step": 57310
    },
    {
      "epoch": 3.1844444444444444,
      "grad_norm": 1.1918413639068604,
      "learning_rate": 1.8155555555555557e-05,
      "loss": 0.0035,
      "step": 57320
    },
    {
      "epoch": 3.185,
      "grad_norm": 0.2680535614490509,
      "learning_rate": 1.815e-05,
      "loss": 0.0039,
      "step": 57330
    },
    {
      "epoch": 3.1855555555555557,
      "grad_norm": 0.4356313943862915,
      "learning_rate": 1.8144444444444444e-05,
      "loss": 0.0034,
      "step": 57340
    },
    {
      "epoch": 3.186111111111111,
      "grad_norm": 0.09094630181789398,
      "learning_rate": 1.8138888888888888e-05,
      "loss": 0.0027,
      "step": 57350
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.20785512030124664,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0036,
      "step": 57360
    },
    {
      "epoch": 3.187222222222222,
      "grad_norm": 0.20759429037570953,
      "learning_rate": 1.812777777777778e-05,
      "loss": 0.0033,
      "step": 57370
    },
    {
      "epoch": 3.187777777777778,
      "grad_norm": 0.2966811954975128,
      "learning_rate": 1.8122222222222225e-05,
      "loss": 0.0025,
      "step": 57380
    },
    {
      "epoch": 3.1883333333333335,
      "grad_norm": 0.17762663960456848,
      "learning_rate": 1.811666666666667e-05,
      "loss": 0.0036,
      "step": 57390
    },
    {
      "epoch": 3.188888888888889,
      "grad_norm": 0.06051817536354065,
      "learning_rate": 1.8111111111111112e-05,
      "loss": 0.0033,
      "step": 57400
    },
    {
      "epoch": 3.1894444444444443,
      "grad_norm": 0.09236115962266922,
      "learning_rate": 1.8105555555555556e-05,
      "loss": 0.0025,
      "step": 57410
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.35656148195266724,
      "learning_rate": 1.81e-05,
      "loss": 0.0031,
      "step": 57420
    },
    {
      "epoch": 3.1905555555555556,
      "grad_norm": 0.208608478307724,
      "learning_rate": 1.8094444444444443e-05,
      "loss": 0.0028,
      "step": 57430
    },
    {
      "epoch": 3.1911111111111112,
      "grad_norm": 0.5351163148880005,
      "learning_rate": 1.808888888888889e-05,
      "loss": 0.0026,
      "step": 57440
    },
    {
      "epoch": 3.191666666666667,
      "grad_norm": 0.4513624906539917,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.0037,
      "step": 57450
    },
    {
      "epoch": 3.192222222222222,
      "grad_norm": 0.09280020743608475,
      "learning_rate": 1.807777777777778e-05,
      "loss": 0.003,
      "step": 57460
    },
    {
      "epoch": 3.1927777777777777,
      "grad_norm": 0.18101130425930023,
      "learning_rate": 1.8072222222222224e-05,
      "loss": 0.0033,
      "step": 57470
    },
    {
      "epoch": 3.1933333333333334,
      "grad_norm": 0.26629143953323364,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0044,
      "step": 57480
    },
    {
      "epoch": 3.193888888888889,
      "grad_norm": 0.20758484303951263,
      "learning_rate": 1.806111111111111e-05,
      "loss": 0.0032,
      "step": 57490
    },
    {
      "epoch": 3.1944444444444446,
      "grad_norm": 0.14854450523853302,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 0.002,
      "step": 57500
    },
    {
      "epoch": 3.195,
      "grad_norm": 0.18568043410778046,
      "learning_rate": 1.805e-05,
      "loss": 0.0045,
      "step": 57510
    },
    {
      "epoch": 3.1955555555555555,
      "grad_norm": 0.09014182537794113,
      "learning_rate": 1.8044444444444445e-05,
      "loss": 0.003,
      "step": 57520
    },
    {
      "epoch": 3.196111111111111,
      "grad_norm": 0.060388173907995224,
      "learning_rate": 1.803888888888889e-05,
      "loss": 0.0022,
      "step": 57530
    },
    {
      "epoch": 3.1966666666666668,
      "grad_norm": 0.06090797483921051,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0035,
      "step": 57540
    },
    {
      "epoch": 3.1972222222222224,
      "grad_norm": 0.18279382586479187,
      "learning_rate": 1.802777777777778e-05,
      "loss": 0.0046,
      "step": 57550
    },
    {
      "epoch": 3.1977777777777776,
      "grad_norm": 0.11985518038272858,
      "learning_rate": 1.8022222222222223e-05,
      "loss": 0.0031,
      "step": 57560
    },
    {
      "epoch": 3.1983333333333333,
      "grad_norm": 0.1784650683403015,
      "learning_rate": 1.8016666666666666e-05,
      "loss": 0.0035,
      "step": 57570
    },
    {
      "epoch": 3.198888888888889,
      "grad_norm": 0.2080121785402298,
      "learning_rate": 1.8011111111111113e-05,
      "loss": 0.003,
      "step": 57580
    },
    {
      "epoch": 3.1994444444444445,
      "grad_norm": 0.29980942606925964,
      "learning_rate": 1.8005555555555557e-05,
      "loss": 0.0026,
      "step": 57590
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.4156317114830017,
      "learning_rate": 1.8e-05,
      "loss": 0.0027,
      "step": 57600
    },
    {
      "epoch": 3.2005555555555554,
      "grad_norm": 0.03517646715044975,
      "learning_rate": 1.7994444444444444e-05,
      "loss": 0.0027,
      "step": 57610
    },
    {
      "epoch": 3.201111111111111,
      "grad_norm": 0.15040312707424164,
      "learning_rate": 1.7988888888888888e-05,
      "loss": 0.0025,
      "step": 57620
    },
    {
      "epoch": 3.2016666666666667,
      "grad_norm": 0.12024681270122528,
      "learning_rate": 1.7983333333333335e-05,
      "loss": 0.0031,
      "step": 57630
    },
    {
      "epoch": 3.2022222222222223,
      "grad_norm": 0.23768077790737152,
      "learning_rate": 1.7977777777777778e-05,
      "loss": 0.0023,
      "step": 57640
    },
    {
      "epoch": 3.202777777777778,
      "grad_norm": 0.6508508920669556,
      "learning_rate": 1.7972222222222225e-05,
      "loss": 0.0032,
      "step": 57650
    },
    {
      "epoch": 3.203333333333333,
      "grad_norm": 0.4007449448108673,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.0029,
      "step": 57660
    },
    {
      "epoch": 3.203888888888889,
      "grad_norm": 0.4750950038433075,
      "learning_rate": 1.7961111111111112e-05,
      "loss": 0.0027,
      "step": 57670
    },
    {
      "epoch": 3.2044444444444444,
      "grad_norm": 0.1483321636915207,
      "learning_rate": 1.7955555555555556e-05,
      "loss": 0.0034,
      "step": 57680
    },
    {
      "epoch": 3.205,
      "grad_norm": 0.26779302954673767,
      "learning_rate": 1.795e-05,
      "loss": 0.0035,
      "step": 57690
    },
    {
      "epoch": 3.2055555555555557,
      "grad_norm": 0.23754951357841492,
      "learning_rate": 1.7944444444444443e-05,
      "loss": 0.0031,
      "step": 57700
    },
    {
      "epoch": 3.206111111111111,
      "grad_norm": 0.14988195896148682,
      "learning_rate": 1.793888888888889e-05,
      "loss": 0.0038,
      "step": 57710
    },
    {
      "epoch": 3.2066666666666666,
      "grad_norm": 0.1779373735189438,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0034,
      "step": 57720
    },
    {
      "epoch": 3.207222222222222,
      "grad_norm": 0.177962988615036,
      "learning_rate": 1.792777777777778e-05,
      "loss": 0.0026,
      "step": 57730
    },
    {
      "epoch": 3.207777777777778,
      "grad_norm": 0.44325244426727295,
      "learning_rate": 1.7922222222222224e-05,
      "loss": 0.0029,
      "step": 57740
    },
    {
      "epoch": 3.2083333333333335,
      "grad_norm": 0.03258475661277771,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.003,
      "step": 57750
    },
    {
      "epoch": 3.2088888888888887,
      "grad_norm": 0.1701093167066574,
      "learning_rate": 1.791111111111111e-05,
      "loss": 0.0032,
      "step": 57760
    },
    {
      "epoch": 3.2094444444444443,
      "grad_norm": 0.008422493934631348,
      "learning_rate": 1.7905555555555554e-05,
      "loss": 0.003,
      "step": 57770
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.12309007346630096,
      "learning_rate": 1.79e-05,
      "loss": 0.0026,
      "step": 57780
    },
    {
      "epoch": 3.2105555555555556,
      "grad_norm": 0.12013185024261475,
      "learning_rate": 1.7894444444444445e-05,
      "loss": 0.0031,
      "step": 57790
    },
    {
      "epoch": 3.2111111111111112,
      "grad_norm": 0.25052836537361145,
      "learning_rate": 1.788888888888889e-05,
      "loss": 0.0034,
      "step": 57800
    },
    {
      "epoch": 3.211666666666667,
      "grad_norm": 0.0622587651014328,
      "learning_rate": 1.7883333333333335e-05,
      "loss": 0.004,
      "step": 57810
    },
    {
      "epoch": 3.212222222222222,
      "grad_norm": 0.41534942388534546,
      "learning_rate": 1.787777777777778e-05,
      "loss": 0.003,
      "step": 57820
    },
    {
      "epoch": 3.2127777777777777,
      "grad_norm": 0.43986040353775024,
      "learning_rate": 1.7872222222222223e-05,
      "loss": 0.0038,
      "step": 57830
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.10296127200126648,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0047,
      "step": 57840
    },
    {
      "epoch": 3.213888888888889,
      "grad_norm": 0.03242548555135727,
      "learning_rate": 1.7861111111111113e-05,
      "loss": 0.0044,
      "step": 57850
    },
    {
      "epoch": 3.214444444444444,
      "grad_norm": 0.3334864675998688,
      "learning_rate": 1.7855555555555557e-05,
      "loss": 0.0024,
      "step": 57860
    },
    {
      "epoch": 3.215,
      "grad_norm": 0.09363050758838654,
      "learning_rate": 1.785e-05,
      "loss": 0.0029,
      "step": 57870
    },
    {
      "epoch": 3.2155555555555555,
      "grad_norm": 0.32657554745674133,
      "learning_rate": 1.7844444444444444e-05,
      "loss": 0.0026,
      "step": 57880
    },
    {
      "epoch": 3.216111111111111,
      "grad_norm": 0.14897075295448303,
      "learning_rate": 1.783888888888889e-05,
      "loss": 0.004,
      "step": 57890
    },
    {
      "epoch": 3.216666666666667,
      "grad_norm": 0.07917223870754242,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0031,
      "step": 57900
    },
    {
      "epoch": 3.2172222222222224,
      "grad_norm": 0.22461046278476715,
      "learning_rate": 1.7827777777777778e-05,
      "loss": 0.0026,
      "step": 57910
    },
    {
      "epoch": 3.2177777777777776,
      "grad_norm": 0.09021714329719543,
      "learning_rate": 1.7822222222222225e-05,
      "loss": 0.0037,
      "step": 57920
    },
    {
      "epoch": 3.2183333333333333,
      "grad_norm": 0.5366382002830505,
      "learning_rate": 1.781666666666667e-05,
      "loss": 0.0041,
      "step": 57930
    },
    {
      "epoch": 3.218888888888889,
      "grad_norm": 0.2689390480518341,
      "learning_rate": 1.7811111111111112e-05,
      "loss": 0.0027,
      "step": 57940
    },
    {
      "epoch": 3.2194444444444446,
      "grad_norm": 0.35642218589782715,
      "learning_rate": 1.7805555555555555e-05,
      "loss": 0.0034,
      "step": 57950
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.17970457673072815,
      "learning_rate": 1.78e-05,
      "loss": 0.0035,
      "step": 57960
    },
    {
      "epoch": 3.2205555555555554,
      "grad_norm": 0.12020134925842285,
      "learning_rate": 1.7794444444444443e-05,
      "loss": 0.0034,
      "step": 57970
    },
    {
      "epoch": 3.221111111111111,
      "grad_norm": 0.07602684199810028,
      "learning_rate": 1.778888888888889e-05,
      "loss": 0.0042,
      "step": 57980
    },
    {
      "epoch": 3.2216666666666667,
      "grad_norm": 0.1194906160235405,
      "learning_rate": 1.7783333333333336e-05,
      "loss": 0.003,
      "step": 57990
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.03301422297954559,
      "learning_rate": 1.777777777777778e-05,
      "loss": 0.0032,
      "step": 58000
    },
    {
      "epoch": 3.222777777777778,
      "grad_norm": 0.11937347799539566,
      "learning_rate": 1.7772222222222224e-05,
      "loss": 0.0031,
      "step": 58010
    },
    {
      "epoch": 3.223333333333333,
      "grad_norm": 0.060055267065763474,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0034,
      "step": 58020
    },
    {
      "epoch": 3.223888888888889,
      "grad_norm": 0.014160580933094025,
      "learning_rate": 1.776111111111111e-05,
      "loss": 0.0027,
      "step": 58030
    },
    {
      "epoch": 3.2244444444444444,
      "grad_norm": 0.2376021146774292,
      "learning_rate": 1.7755555555555554e-05,
      "loss": 0.0024,
      "step": 58040
    },
    {
      "epoch": 3.225,
      "grad_norm": 0.03494905307888985,
      "learning_rate": 1.775e-05,
      "loss": 0.0029,
      "step": 58050
    },
    {
      "epoch": 3.2255555555555557,
      "grad_norm": 0.3859536945819855,
      "learning_rate": 1.7744444444444445e-05,
      "loss": 0.0033,
      "step": 58060
    },
    {
      "epoch": 3.226111111111111,
      "grad_norm": 0.1486573964357376,
      "learning_rate": 1.773888888888889e-05,
      "loss": 0.0027,
      "step": 58070
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.17843292653560638,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0028,
      "step": 58080
    },
    {
      "epoch": 3.227222222222222,
      "grad_norm": 0.17856013774871826,
      "learning_rate": 1.772777777777778e-05,
      "loss": 0.0024,
      "step": 58090
    },
    {
      "epoch": 3.227777777777778,
      "grad_norm": 0.12061335146427155,
      "learning_rate": 1.7722222222222222e-05,
      "loss": 0.0034,
      "step": 58100
    },
    {
      "epoch": 3.2283333333333335,
      "grad_norm": 0.15073871612548828,
      "learning_rate": 1.7716666666666666e-05,
      "loss": 0.0034,
      "step": 58110
    },
    {
      "epoch": 3.2288888888888887,
      "grad_norm": 0.15048277378082275,
      "learning_rate": 1.7711111111111113e-05,
      "loss": 0.0037,
      "step": 58120
    },
    {
      "epoch": 3.2294444444444443,
      "grad_norm": 0.41567283868789673,
      "learning_rate": 1.7705555555555556e-05,
      "loss": 0.0038,
      "step": 58130
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.05988055467605591,
      "learning_rate": 1.77e-05,
      "loss": 0.0037,
      "step": 58140
    },
    {
      "epoch": 3.2305555555555556,
      "grad_norm": 0.4614221453666687,
      "learning_rate": 1.7694444444444443e-05,
      "loss": 0.0046,
      "step": 58150
    },
    {
      "epoch": 3.2311111111111113,
      "grad_norm": 0.2381647825241089,
      "learning_rate": 1.768888888888889e-05,
      "loss": 0.003,
      "step": 58160
    },
    {
      "epoch": 3.2316666666666665,
      "grad_norm": 0.018353022634983063,
      "learning_rate": 1.7683333333333334e-05,
      "loss": 0.002,
      "step": 58170
    },
    {
      "epoch": 3.232222222222222,
      "grad_norm": 0.4383804500102997,
      "learning_rate": 1.7677777777777778e-05,
      "loss": 0.0024,
      "step": 58180
    },
    {
      "epoch": 3.2327777777777778,
      "grad_norm": 0.23741377890110016,
      "learning_rate": 1.7672222222222224e-05,
      "loss": 0.003,
      "step": 58190
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.19100143015384674,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0034,
      "step": 58200
    },
    {
      "epoch": 3.233888888888889,
      "grad_norm": 0.17945101857185364,
      "learning_rate": 1.766111111111111e-05,
      "loss": 0.0042,
      "step": 58210
    },
    {
      "epoch": 3.2344444444444447,
      "grad_norm": 0.120796337723732,
      "learning_rate": 1.7655555555555555e-05,
      "loss": 0.0037,
      "step": 58220
    },
    {
      "epoch": 3.235,
      "grad_norm": 0.11542702466249466,
      "learning_rate": 1.765e-05,
      "loss": 0.0036,
      "step": 58230
    },
    {
      "epoch": 3.2355555555555555,
      "grad_norm": 0.26801902055740356,
      "learning_rate": 1.7644444444444446e-05,
      "loss": 0.0025,
      "step": 58240
    },
    {
      "epoch": 3.236111111111111,
      "grad_norm": 0.15006999671459198,
      "learning_rate": 1.763888888888889e-05,
      "loss": 0.0016,
      "step": 58250
    },
    {
      "epoch": 3.236666666666667,
      "grad_norm": 0.3262840211391449,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0026,
      "step": 58260
    },
    {
      "epoch": 3.237222222222222,
      "grad_norm": 0.10737579315900803,
      "learning_rate": 1.762777777777778e-05,
      "loss": 0.0026,
      "step": 58270
    },
    {
      "epoch": 3.2377777777777776,
      "grad_norm": 0.11960916221141815,
      "learning_rate": 1.7622222222222223e-05,
      "loss": 0.0032,
      "step": 58280
    },
    {
      "epoch": 3.2383333333333333,
      "grad_norm": 0.26646026968955994,
      "learning_rate": 1.7616666666666667e-05,
      "loss": 0.0047,
      "step": 58290
    },
    {
      "epoch": 3.238888888888889,
      "grad_norm": 0.28510427474975586,
      "learning_rate": 1.761111111111111e-05,
      "loss": 0.0033,
      "step": 58300
    },
    {
      "epoch": 3.2394444444444446,
      "grad_norm": 0.4379703402519226,
      "learning_rate": 1.7605555555555557e-05,
      "loss": 0.0027,
      "step": 58310
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.29367542266845703,
      "learning_rate": 1.76e-05,
      "loss": 0.0033,
      "step": 58320
    },
    {
      "epoch": 3.2405555555555554,
      "grad_norm": 0.5047566890716553,
      "learning_rate": 1.7594444444444444e-05,
      "loss": 0.003,
      "step": 58330
    },
    {
      "epoch": 3.241111111111111,
      "grad_norm": 0.14920026063919067,
      "learning_rate": 1.758888888888889e-05,
      "loss": 0.0029,
      "step": 58340
    },
    {
      "epoch": 3.2416666666666667,
      "grad_norm": 0.5045391917228699,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.0031,
      "step": 58350
    },
    {
      "epoch": 3.2422222222222223,
      "grad_norm": 0.17892439663410187,
      "learning_rate": 1.757777777777778e-05,
      "loss": 0.0024,
      "step": 58360
    },
    {
      "epoch": 3.242777777777778,
      "grad_norm": 0.03260352835059166,
      "learning_rate": 1.7572222222222222e-05,
      "loss": 0.0033,
      "step": 58370
    },
    {
      "epoch": 3.243333333333333,
      "grad_norm": 0.2668609619140625,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.0031,
      "step": 58380
    },
    {
      "epoch": 3.243888888888889,
      "grad_norm": 0.0899457335472107,
      "learning_rate": 1.7561111111111113e-05,
      "loss": 0.0036,
      "step": 58390
    },
    {
      "epoch": 3.2444444444444445,
      "grad_norm": 0.509819507598877,
      "learning_rate": 1.7555555555555556e-05,
      "loss": 0.003,
      "step": 58400
    },
    {
      "epoch": 3.245,
      "grad_norm": 0.08940209448337555,
      "learning_rate": 1.755e-05,
      "loss": 0.0024,
      "step": 58410
    },
    {
      "epoch": 3.2455555555555557,
      "grad_norm": 0.20790798962116241,
      "learning_rate": 1.7544444444444443e-05,
      "loss": 0.004,
      "step": 58420
    },
    {
      "epoch": 3.246111111111111,
      "grad_norm": 0.2969999313354492,
      "learning_rate": 1.753888888888889e-05,
      "loss": 0.0025,
      "step": 58430
    },
    {
      "epoch": 3.2466666666666666,
      "grad_norm": 0.0600254125893116,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0031,
      "step": 58440
    },
    {
      "epoch": 3.2472222222222222,
      "grad_norm": 0.3574438989162445,
      "learning_rate": 1.752777777777778e-05,
      "loss": 0.0025,
      "step": 58450
    },
    {
      "epoch": 3.247777777777778,
      "grad_norm": 0.39345815777778625,
      "learning_rate": 1.7522222222222224e-05,
      "loss": 0.0028,
      "step": 58460
    },
    {
      "epoch": 3.2483333333333335,
      "grad_norm": 0.014023435302078724,
      "learning_rate": 1.7516666666666668e-05,
      "loss": 0.0029,
      "step": 58470
    },
    {
      "epoch": 3.2488888888888887,
      "grad_norm": 0.3884490728378296,
      "learning_rate": 1.751111111111111e-05,
      "loss": 0.004,
      "step": 58480
    },
    {
      "epoch": 3.2494444444444444,
      "grad_norm": 0.12461744993925095,
      "learning_rate": 1.7505555555555555e-05,
      "loss": 0.004,
      "step": 58490
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.016140151768922806,
      "learning_rate": 1.75e-05,
      "loss": 0.0028,
      "step": 58500
    },
    {
      "epoch": 3.2505555555555556,
      "grad_norm": 0.6535019874572754,
      "learning_rate": 1.7494444444444445e-05,
      "loss": 0.0028,
      "step": 58510
    },
    {
      "epoch": 3.2511111111111113,
      "grad_norm": 0.23455293476581573,
      "learning_rate": 1.7488888888888892e-05,
      "loss": 0.0035,
      "step": 58520
    },
    {
      "epoch": 3.2516666666666665,
      "grad_norm": 0.38639363646507263,
      "learning_rate": 1.7483333333333336e-05,
      "loss": 0.0027,
      "step": 58530
    },
    {
      "epoch": 3.252222222222222,
      "grad_norm": 0.08933131396770477,
      "learning_rate": 1.747777777777778e-05,
      "loss": 0.0023,
      "step": 58540
    },
    {
      "epoch": 3.2527777777777778,
      "grad_norm": 0.20838339626789093,
      "learning_rate": 1.7472222222222223e-05,
      "loss": 0.0039,
      "step": 58550
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.3046230673789978,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0026,
      "step": 58560
    },
    {
      "epoch": 3.253888888888889,
      "grad_norm": 0.4673750698566437,
      "learning_rate": 1.746111111111111e-05,
      "loss": 0.0024,
      "step": 58570
    },
    {
      "epoch": 3.2544444444444443,
      "grad_norm": 0.08953051269054413,
      "learning_rate": 1.7455555555555557e-05,
      "loss": 0.0021,
      "step": 58580
    },
    {
      "epoch": 3.255,
      "grad_norm": 0.14878778159618378,
      "learning_rate": 1.745e-05,
      "loss": 0.0022,
      "step": 58590
    },
    {
      "epoch": 3.2555555555555555,
      "grad_norm": 0.3457156717777252,
      "learning_rate": 1.7444444444444448e-05,
      "loss": 0.002,
      "step": 58600
    },
    {
      "epoch": 3.256111111111111,
      "grad_norm": 0.3994976878166199,
      "learning_rate": 1.743888888888889e-05,
      "loss": 0.0032,
      "step": 58610
    },
    {
      "epoch": 3.256666666666667,
      "grad_norm": 0.4989995062351227,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0031,
      "step": 58620
    },
    {
      "epoch": 3.2572222222222225,
      "grad_norm": 0.44593027234077454,
      "learning_rate": 1.7427777777777778e-05,
      "loss": 0.0031,
      "step": 58630
    },
    {
      "epoch": 3.2577777777777777,
      "grad_norm": 0.3249252140522003,
      "learning_rate": 1.7422222222222222e-05,
      "loss": 0.0038,
      "step": 58640
    },
    {
      "epoch": 3.2583333333333333,
      "grad_norm": 0.5351390242576599,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.0028,
      "step": 58650
    },
    {
      "epoch": 3.258888888888889,
      "grad_norm": 0.4364891052246094,
      "learning_rate": 1.7411111111111112e-05,
      "loss": 0.0038,
      "step": 58660
    },
    {
      "epoch": 3.2594444444444446,
      "grad_norm": 0.014807323925197124,
      "learning_rate": 1.7405555555555556e-05,
      "loss": 0.0021,
      "step": 58670
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.06202230229973793,
      "learning_rate": 1.74e-05,
      "loss": 0.0026,
      "step": 58680
    },
    {
      "epoch": 3.2605555555555554,
      "grad_norm": 0.1484111100435257,
      "learning_rate": 1.7394444444444446e-05,
      "loss": 0.0023,
      "step": 58690
    },
    {
      "epoch": 3.261111111111111,
      "grad_norm": 0.061938777565956116,
      "learning_rate": 1.738888888888889e-05,
      "loss": 0.0027,
      "step": 58700
    },
    {
      "epoch": 3.2616666666666667,
      "grad_norm": 0.36333680152893066,
      "learning_rate": 1.7383333333333333e-05,
      "loss": 0.0031,
      "step": 58710
    },
    {
      "epoch": 3.2622222222222224,
      "grad_norm": 0.016619648784399033,
      "learning_rate": 1.737777777777778e-05,
      "loss": 0.0036,
      "step": 58720
    },
    {
      "epoch": 3.262777777777778,
      "grad_norm": 0.0607215017080307,
      "learning_rate": 1.7372222222222224e-05,
      "loss": 0.0023,
      "step": 58730
    },
    {
      "epoch": 3.263333333333333,
      "grad_norm": 0.3267092704772949,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0031,
      "step": 58740
    },
    {
      "epoch": 3.263888888888889,
      "grad_norm": 0.1780853122472763,
      "learning_rate": 1.736111111111111e-05,
      "loss": 0.0028,
      "step": 58750
    },
    {
      "epoch": 3.2644444444444445,
      "grad_norm": 0.06101233884692192,
      "learning_rate": 1.7355555555555555e-05,
      "loss": 0.0028,
      "step": 58760
    },
    {
      "epoch": 3.265,
      "grad_norm": 0.23760810494422913,
      "learning_rate": 1.7349999999999998e-05,
      "loss": 0.0028,
      "step": 58770
    },
    {
      "epoch": 3.2655555555555553,
      "grad_norm": 0.060224466025829315,
      "learning_rate": 1.7344444444444445e-05,
      "loss": 0.0025,
      "step": 58780
    },
    {
      "epoch": 3.266111111111111,
      "grad_norm": 0.16160021722316742,
      "learning_rate": 1.7338888888888892e-05,
      "loss": 0.0033,
      "step": 58790
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.061961036175489426,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0043,
      "step": 58800
    },
    {
      "epoch": 3.2672222222222222,
      "grad_norm": 0.31790488958358765,
      "learning_rate": 1.732777777777778e-05,
      "loss": 0.0026,
      "step": 58810
    },
    {
      "epoch": 3.267777777777778,
      "grad_norm": 0.061319779604673386,
      "learning_rate": 1.7322222222222223e-05,
      "loss": 0.0038,
      "step": 58820
    },
    {
      "epoch": 3.2683333333333335,
      "grad_norm": 0.11949419230222702,
      "learning_rate": 1.7316666666666666e-05,
      "loss": 0.003,
      "step": 58830
    },
    {
      "epoch": 3.2688888888888887,
      "grad_norm": 0.4413875937461853,
      "learning_rate": 1.731111111111111e-05,
      "loss": 0.0028,
      "step": 58840
    },
    {
      "epoch": 3.2694444444444444,
      "grad_norm": 0.18603838980197906,
      "learning_rate": 1.7305555555555557e-05,
      "loss": 0.0037,
      "step": 58850
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.2681920528411865,
      "learning_rate": 1.73e-05,
      "loss": 0.0027,
      "step": 58860
    },
    {
      "epoch": 3.2705555555555557,
      "grad_norm": 0.1500827819108963,
      "learning_rate": 1.7294444444444447e-05,
      "loss": 0.0035,
      "step": 58870
    },
    {
      "epoch": 3.2711111111111113,
      "grad_norm": 0.12075551599264145,
      "learning_rate": 1.728888888888889e-05,
      "loss": 0.0031,
      "step": 58880
    },
    {
      "epoch": 3.2716666666666665,
      "grad_norm": 0.20815035700798035,
      "learning_rate": 1.7283333333333334e-05,
      "loss": 0.0019,
      "step": 58890
    },
    {
      "epoch": 3.272222222222222,
      "grad_norm": 0.2665650546550751,
      "learning_rate": 1.7277777777777778e-05,
      "loss": 0.0043,
      "step": 58900
    },
    {
      "epoch": 3.272777777777778,
      "grad_norm": 0.5358403325080872,
      "learning_rate": 1.727222222222222e-05,
      "loss": 0.0034,
      "step": 58910
    },
    {
      "epoch": 3.2733333333333334,
      "grad_norm": 0.17858611047267914,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0013,
      "step": 58920
    },
    {
      "epoch": 3.273888888888889,
      "grad_norm": 0.16534210741519928,
      "learning_rate": 1.7261111111111112e-05,
      "loss": 0.0021,
      "step": 58930
    },
    {
      "epoch": 3.2744444444444443,
      "grad_norm": 0.2673584520816803,
      "learning_rate": 1.7255555555555556e-05,
      "loss": 0.0035,
      "step": 58940
    },
    {
      "epoch": 3.275,
      "grad_norm": 0.09030713140964508,
      "learning_rate": 1.725e-05,
      "loss": 0.0016,
      "step": 58950
    },
    {
      "epoch": 3.2755555555555556,
      "grad_norm": 0.1790100634098053,
      "learning_rate": 1.7244444444444446e-05,
      "loss": 0.0025,
      "step": 58960
    },
    {
      "epoch": 3.276111111111111,
      "grad_norm": 0.30492642521858215,
      "learning_rate": 1.723888888888889e-05,
      "loss": 0.0027,
      "step": 58970
    },
    {
      "epoch": 3.276666666666667,
      "grad_norm": 0.11943965405225754,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.002,
      "step": 58980
    },
    {
      "epoch": 3.277222222222222,
      "grad_norm": 0.34261569380760193,
      "learning_rate": 1.722777777777778e-05,
      "loss": 0.0028,
      "step": 58990
    },
    {
      "epoch": 3.2777777777777777,
      "grad_norm": 0.29727378487586975,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 0.0027,
      "step": 59000
    },
    {
      "epoch": 3.2783333333333333,
      "grad_norm": 0.5344051718711853,
      "learning_rate": 1.7216666666666667e-05,
      "loss": 0.003,
      "step": 59010
    },
    {
      "epoch": 3.278888888888889,
      "grad_norm": 0.26100847125053406,
      "learning_rate": 1.721111111111111e-05,
      "loss": 0.003,
      "step": 59020
    },
    {
      "epoch": 3.2794444444444446,
      "grad_norm": 0.14982758462429047,
      "learning_rate": 1.7205555555555554e-05,
      "loss": 0.0024,
      "step": 59030
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.41595330834388733,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0027,
      "step": 59040
    },
    {
      "epoch": 3.2805555555555554,
      "grad_norm": 0.1198345199227333,
      "learning_rate": 1.7194444444444445e-05,
      "loss": 0.0031,
      "step": 59050
    },
    {
      "epoch": 3.281111111111111,
      "grad_norm": 0.24058571457862854,
      "learning_rate": 1.7188888888888892e-05,
      "loss": 0.0028,
      "step": 59060
    },
    {
      "epoch": 3.2816666666666667,
      "grad_norm": 0.24579161405563354,
      "learning_rate": 1.7183333333333335e-05,
      "loss": 0.0026,
      "step": 59070
    },
    {
      "epoch": 3.2822222222222224,
      "grad_norm": 0.17926079034805298,
      "learning_rate": 1.717777777777778e-05,
      "loss": 0.0031,
      "step": 59080
    },
    {
      "epoch": 3.2827777777777776,
      "grad_norm": 0.0898253321647644,
      "learning_rate": 1.7172222222222223e-05,
      "loss": 0.0023,
      "step": 59090
    },
    {
      "epoch": 3.283333333333333,
      "grad_norm": 0.31026914715766907,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.0027,
      "step": 59100
    },
    {
      "epoch": 3.283888888888889,
      "grad_norm": 0.29683205485343933,
      "learning_rate": 1.716111111111111e-05,
      "loss": 0.0034,
      "step": 59110
    },
    {
      "epoch": 3.2844444444444445,
      "grad_norm": 0.17861084640026093,
      "learning_rate": 1.7155555555555557e-05,
      "loss": 0.0035,
      "step": 59120
    },
    {
      "epoch": 3.285,
      "grad_norm": 0.4285830855369568,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.0027,
      "step": 59130
    },
    {
      "epoch": 3.285555555555556,
      "grad_norm": 0.2385440468788147,
      "learning_rate": 1.7144444444444447e-05,
      "loss": 0.0019,
      "step": 59140
    },
    {
      "epoch": 3.286111111111111,
      "grad_norm": 0.013576936908066273,
      "learning_rate": 1.713888888888889e-05,
      "loss": 0.0033,
      "step": 59150
    },
    {
      "epoch": 3.2866666666666666,
      "grad_norm": 0.2783532440662384,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0029,
      "step": 59160
    },
    {
      "epoch": 3.2872222222222223,
      "grad_norm": 0.08995336294174194,
      "learning_rate": 1.7127777777777778e-05,
      "loss": 0.003,
      "step": 59170
    },
    {
      "epoch": 3.287777777777778,
      "grad_norm": 0.18425752222537994,
      "learning_rate": 1.712222222222222e-05,
      "loss": 0.003,
      "step": 59180
    },
    {
      "epoch": 3.288333333333333,
      "grad_norm": 0.23718442022800446,
      "learning_rate": 1.7116666666666668e-05,
      "loss": 0.0024,
      "step": 59190
    },
    {
      "epoch": 3.2888888888888888,
      "grad_norm": 0.1199086382985115,
      "learning_rate": 1.7111111111111112e-05,
      "loss": 0.0023,
      "step": 59200
    },
    {
      "epoch": 3.2894444444444444,
      "grad_norm": 0.6133040189743042,
      "learning_rate": 1.7105555555555555e-05,
      "loss": 0.0035,
      "step": 59210
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.2711391746997833,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.0032,
      "step": 59220
    },
    {
      "epoch": 3.2905555555555557,
      "grad_norm": 0.3908620774745941,
      "learning_rate": 1.7094444444444446e-05,
      "loss": 0.0033,
      "step": 59230
    },
    {
      "epoch": 3.2911111111111113,
      "grad_norm": 0.23857754468917847,
      "learning_rate": 1.708888888888889e-05,
      "loss": 0.0023,
      "step": 59240
    },
    {
      "epoch": 3.2916666666666665,
      "grad_norm": 0.011423886753618717,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.0027,
      "step": 59250
    },
    {
      "epoch": 3.292222222222222,
      "grad_norm": 0.5072026252746582,
      "learning_rate": 1.707777777777778e-05,
      "loss": 0.003,
      "step": 59260
    },
    {
      "epoch": 3.292777777777778,
      "grad_norm": 0.14987435936927795,
      "learning_rate": 1.7072222222222223e-05,
      "loss": 0.0029,
      "step": 59270
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.2072633057832718,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.004,
      "step": 59280
    },
    {
      "epoch": 3.2938888888888886,
      "grad_norm": 0.23851554095745087,
      "learning_rate": 1.706111111111111e-05,
      "loss": 0.002,
      "step": 59290
    },
    {
      "epoch": 3.2944444444444443,
      "grad_norm": 0.31924664974212646,
      "learning_rate": 1.7055555555555554e-05,
      "loss": 0.0023,
      "step": 59300
    },
    {
      "epoch": 3.295,
      "grad_norm": 0.29770493507385254,
      "learning_rate": 1.705e-05,
      "loss": 0.002,
      "step": 59310
    },
    {
      "epoch": 3.2955555555555556,
      "grad_norm": 0.2509075999259949,
      "learning_rate": 1.7044444444444445e-05,
      "loss": 0.0038,
      "step": 59320
    },
    {
      "epoch": 3.296111111111111,
      "grad_norm": 0.01104664895683527,
      "learning_rate": 1.703888888888889e-05,
      "loss": 0.0023,
      "step": 59330
    },
    {
      "epoch": 3.296666666666667,
      "grad_norm": 0.03082815743982792,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0039,
      "step": 59340
    },
    {
      "epoch": 3.297222222222222,
      "grad_norm": 0.06808607280254364,
      "learning_rate": 1.702777777777778e-05,
      "loss": 0.0028,
      "step": 59350
    },
    {
      "epoch": 3.2977777777777777,
      "grad_norm": 0.1195765808224678,
      "learning_rate": 1.7022222222222222e-05,
      "loss": 0.0021,
      "step": 59360
    },
    {
      "epoch": 3.2983333333333333,
      "grad_norm": 0.3249594569206238,
      "learning_rate": 1.7016666666666666e-05,
      "loss": 0.0035,
      "step": 59370
    },
    {
      "epoch": 3.298888888888889,
      "grad_norm": 0.09969355911016464,
      "learning_rate": 1.701111111111111e-05,
      "loss": 0.003,
      "step": 59380
    },
    {
      "epoch": 3.2994444444444446,
      "grad_norm": 0.2670586109161377,
      "learning_rate": 1.7005555555555556e-05,
      "loss": 0.0029,
      "step": 59390
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.06113794818520546,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.003,
      "step": 59400
    },
    {
      "epoch": 3.3005555555555555,
      "grad_norm": 0.20808041095733643,
      "learning_rate": 1.6994444444444447e-05,
      "loss": 0.0028,
      "step": 59410
    },
    {
      "epoch": 3.301111111111111,
      "grad_norm": 0.07209472358226776,
      "learning_rate": 1.698888888888889e-05,
      "loss": 0.0038,
      "step": 59420
    },
    {
      "epoch": 3.3016666666666667,
      "grad_norm": 0.35679179430007935,
      "learning_rate": 1.6983333333333334e-05,
      "loss": 0.003,
      "step": 59430
    },
    {
      "epoch": 3.3022222222222224,
      "grad_norm": 0.2968948781490326,
      "learning_rate": 1.6977777777777777e-05,
      "loss": 0.0023,
      "step": 59440
    },
    {
      "epoch": 3.3027777777777776,
      "grad_norm": 0.03293349966406822,
      "learning_rate": 1.697222222222222e-05,
      "loss": 0.0025,
      "step": 59450
    },
    {
      "epoch": 3.3033333333333332,
      "grad_norm": 0.35795968770980835,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0022,
      "step": 59460
    },
    {
      "epoch": 3.303888888888889,
      "grad_norm": 0.4160170555114746,
      "learning_rate": 1.696111111111111e-05,
      "loss": 0.0017,
      "step": 59470
    },
    {
      "epoch": 3.3044444444444445,
      "grad_norm": 0.06217477470636368,
      "learning_rate": 1.6955555555555555e-05,
      "loss": 0.0025,
      "step": 59480
    },
    {
      "epoch": 3.305,
      "grad_norm": 0.0899188444018364,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.0045,
      "step": 59490
    },
    {
      "epoch": 3.3055555555555554,
      "grad_norm": 0.08979716151952744,
      "learning_rate": 1.6944444444444446e-05,
      "loss": 0.0033,
      "step": 59500
    },
    {
      "epoch": 3.306111111111111,
      "grad_norm": 0.11984140425920486,
      "learning_rate": 1.693888888888889e-05,
      "loss": 0.0026,
      "step": 59510
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.060745719820261,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0026,
      "step": 59520
    },
    {
      "epoch": 3.3072222222222223,
      "grad_norm": 0.20807108283042908,
      "learning_rate": 1.692777777777778e-05,
      "loss": 0.0039,
      "step": 59530
    },
    {
      "epoch": 3.307777777777778,
      "grad_norm": 0.6311125159263611,
      "learning_rate": 1.6922222222222223e-05,
      "loss": 0.0032,
      "step": 59540
    },
    {
      "epoch": 3.3083333333333336,
      "grad_norm": 0.031086809933185577,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.0027,
      "step": 59550
    },
    {
      "epoch": 3.3088888888888888,
      "grad_norm": 0.5854723453521729,
      "learning_rate": 1.691111111111111e-05,
      "loss": 0.0031,
      "step": 59560
    },
    {
      "epoch": 3.3094444444444444,
      "grad_norm": 0.06012298911809921,
      "learning_rate": 1.6905555555555554e-05,
      "loss": 0.003,
      "step": 59570
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.47477206587791443,
      "learning_rate": 1.69e-05,
      "loss": 0.003,
      "step": 59580
    },
    {
      "epoch": 3.3105555555555557,
      "grad_norm": 0.17846743762493134,
      "learning_rate": 1.6894444444444444e-05,
      "loss": 0.0036,
      "step": 59590
    },
    {
      "epoch": 3.311111111111111,
      "grad_norm": 0.2106843888759613,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.0045,
      "step": 59600
    },
    {
      "epoch": 3.3116666666666665,
      "grad_norm": 0.11983025074005127,
      "learning_rate": 1.6883333333333335e-05,
      "loss": 0.0023,
      "step": 59610
    },
    {
      "epoch": 3.312222222222222,
      "grad_norm": 0.32724425196647644,
      "learning_rate": 1.687777777777778e-05,
      "loss": 0.0029,
      "step": 59620
    },
    {
      "epoch": 3.312777777777778,
      "grad_norm": 0.26566338539123535,
      "learning_rate": 1.6872222222222222e-05,
      "loss": 0.0023,
      "step": 59630
    },
    {
      "epoch": 3.3133333333333335,
      "grad_norm": 0.20806129276752472,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0025,
      "step": 59640
    },
    {
      "epoch": 3.313888888888889,
      "grad_norm": 0.09995949268341064,
      "learning_rate": 1.6861111111111112e-05,
      "loss": 0.0039,
      "step": 59650
    },
    {
      "epoch": 3.3144444444444443,
      "grad_norm": 0.12701992690563202,
      "learning_rate": 1.6855555555555556e-05,
      "loss": 0.003,
      "step": 59660
    },
    {
      "epoch": 3.315,
      "grad_norm": 0.2669430077075958,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 0.0039,
      "step": 59670
    },
    {
      "epoch": 3.3155555555555556,
      "grad_norm": 0.26800602674484253,
      "learning_rate": 1.6844444444444447e-05,
      "loss": 0.0031,
      "step": 59680
    },
    {
      "epoch": 3.3161111111111112,
      "grad_norm": 0.35604846477508545,
      "learning_rate": 1.683888888888889e-05,
      "loss": 0.0032,
      "step": 59690
    },
    {
      "epoch": 3.3166666666666664,
      "grad_norm": 0.0623549148440361,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0036,
      "step": 59700
    },
    {
      "epoch": 3.317222222222222,
      "grad_norm": 0.03560836985707283,
      "learning_rate": 1.6827777777777777e-05,
      "loss": 0.0029,
      "step": 59710
    },
    {
      "epoch": 3.3177777777777777,
      "grad_norm": 0.8751167058944702,
      "learning_rate": 1.6822222222222224e-05,
      "loss": 0.0031,
      "step": 59720
    },
    {
      "epoch": 3.3183333333333334,
      "grad_norm": 0.5089206695556641,
      "learning_rate": 1.6816666666666668e-05,
      "loss": 0.0026,
      "step": 59730
    },
    {
      "epoch": 3.318888888888889,
      "grad_norm": 0.29751116037368774,
      "learning_rate": 1.681111111111111e-05,
      "loss": 0.0037,
      "step": 59740
    },
    {
      "epoch": 3.3194444444444446,
      "grad_norm": 0.09009761363267899,
      "learning_rate": 1.6805555555555558e-05,
      "loss": 0.0023,
      "step": 59750
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.16728153824806213,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0045,
      "step": 59760
    },
    {
      "epoch": 3.3205555555555555,
      "grad_norm": 0.2673264443874359,
      "learning_rate": 1.6794444444444445e-05,
      "loss": 0.0026,
      "step": 59770
    },
    {
      "epoch": 3.321111111111111,
      "grad_norm": 0.23831778764724731,
      "learning_rate": 1.678888888888889e-05,
      "loss": 0.0021,
      "step": 59780
    },
    {
      "epoch": 3.3216666666666668,
      "grad_norm": 0.26815611124038696,
      "learning_rate": 1.6783333333333336e-05,
      "loss": 0.0024,
      "step": 59790
    },
    {
      "epoch": 3.3222222222222224,
      "grad_norm": 0.06469418853521347,
      "learning_rate": 1.677777777777778e-05,
      "loss": 0.0028,
      "step": 59800
    },
    {
      "epoch": 3.3227777777777776,
      "grad_norm": 0.2083853930234909,
      "learning_rate": 1.6772222222222223e-05,
      "loss": 0.0038,
      "step": 59810
    },
    {
      "epoch": 3.3233333333333333,
      "grad_norm": 0.08999509364366531,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0031,
      "step": 59820
    },
    {
      "epoch": 3.323888888888889,
      "grad_norm": 0.08929985016584396,
      "learning_rate": 1.676111111111111e-05,
      "loss": 0.0021,
      "step": 59830
    },
    {
      "epoch": 3.3244444444444445,
      "grad_norm": 0.11452987045049667,
      "learning_rate": 1.6755555555555557e-05,
      "loss": 0.0026,
      "step": 59840
    },
    {
      "epoch": 3.325,
      "grad_norm": 0.2380988597869873,
      "learning_rate": 1.675e-05,
      "loss": 0.0025,
      "step": 59850
    },
    {
      "epoch": 3.3255555555555554,
      "grad_norm": 0.17800819873809814,
      "learning_rate": 1.6744444444444448e-05,
      "loss": 0.0038,
      "step": 59860
    },
    {
      "epoch": 3.326111111111111,
      "grad_norm": 0.14909806847572327,
      "learning_rate": 1.673888888888889e-05,
      "loss": 0.0036,
      "step": 59870
    },
    {
      "epoch": 3.3266666666666667,
      "grad_norm": 0.08938691765069962,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0017,
      "step": 59880
    },
    {
      "epoch": 3.3272222222222223,
      "grad_norm": 0.06086564436554909,
      "learning_rate": 1.6727777777777778e-05,
      "loss": 0.0023,
      "step": 59890
    },
    {
      "epoch": 3.327777777777778,
      "grad_norm": 0.11946673691272736,
      "learning_rate": 1.6722222222222222e-05,
      "loss": 0.0023,
      "step": 59900
    },
    {
      "epoch": 3.328333333333333,
      "grad_norm": 0.21503376960754395,
      "learning_rate": 1.6716666666666665e-05,
      "loss": 0.0035,
      "step": 59910
    },
    {
      "epoch": 3.328888888888889,
      "grad_norm": 0.24972225725650787,
      "learning_rate": 1.6711111111111112e-05,
      "loss": 0.0026,
      "step": 59920
    },
    {
      "epoch": 3.3294444444444444,
      "grad_norm": 0.38601669669151306,
      "learning_rate": 1.670555555555556e-05,
      "loss": 0.0038,
      "step": 59930
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.09015681594610214,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0026,
      "step": 59940
    },
    {
      "epoch": 3.3305555555555557,
      "grad_norm": 0.03517553582787514,
      "learning_rate": 1.6694444444444446e-05,
      "loss": 0.0036,
      "step": 59950
    },
    {
      "epoch": 3.3311111111111114,
      "grad_norm": 0.015810059383511543,
      "learning_rate": 1.668888888888889e-05,
      "loss": 0.0034,
      "step": 59960
    },
    {
      "epoch": 3.3316666666666666,
      "grad_norm": 0.6872400045394897,
      "learning_rate": 1.6683333333333333e-05,
      "loss": 0.0033,
      "step": 59970
    },
    {
      "epoch": 3.332222222222222,
      "grad_norm": 0.5438409447669983,
      "learning_rate": 1.6677777777777777e-05,
      "loss": 0.004,
      "step": 59980
    },
    {
      "epoch": 3.332777777777778,
      "grad_norm": 0.40510445833206177,
      "learning_rate": 1.6672222222222224e-05,
      "loss": 0.0024,
      "step": 59990
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.44455668330192566,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0024,
      "step": 60000
    },
    {
      "epoch": 3.3338888888888887,
      "grad_norm": 0.011961308307945728,
      "learning_rate": 1.666111111111111e-05,
      "loss": 0.002,
      "step": 60010
    },
    {
      "epoch": 3.3344444444444443,
      "grad_norm": 0.17781773209571838,
      "learning_rate": 1.6655555555555558e-05,
      "loss": 0.0025,
      "step": 60020
    },
    {
      "epoch": 3.335,
      "grad_norm": 0.2676425278186798,
      "learning_rate": 1.665e-05,
      "loss": 0.0018,
      "step": 60030
    },
    {
      "epoch": 3.3355555555555556,
      "grad_norm": 0.26699844002723694,
      "learning_rate": 1.6644444444444445e-05,
      "loss": 0.0019,
      "step": 60040
    },
    {
      "epoch": 3.3361111111111112,
      "grad_norm": 0.03789980709552765,
      "learning_rate": 1.663888888888889e-05,
      "loss": 0.0031,
      "step": 60050
    },
    {
      "epoch": 3.336666666666667,
      "grad_norm": 0.1204095333814621,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0036,
      "step": 60060
    },
    {
      "epoch": 3.337222222222222,
      "grad_norm": 0.09044381231069565,
      "learning_rate": 1.662777777777778e-05,
      "loss": 0.0031,
      "step": 60070
    },
    {
      "epoch": 3.3377777777777777,
      "grad_norm": 0.061022210866212845,
      "learning_rate": 1.6622222222222223e-05,
      "loss": 0.0036,
      "step": 60080
    },
    {
      "epoch": 3.3383333333333334,
      "grad_norm": 0.5465314388275146,
      "learning_rate": 1.6616666666666666e-05,
      "loss": 0.0025,
      "step": 60090
    },
    {
      "epoch": 3.338888888888889,
      "grad_norm": 0.09233329445123672,
      "learning_rate": 1.661111111111111e-05,
      "loss": 0.0052,
      "step": 60100
    },
    {
      "epoch": 3.339444444444444,
      "grad_norm": 0.1495029777288437,
      "learning_rate": 1.6605555555555557e-05,
      "loss": 0.003,
      "step": 60110
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.048424601554870605,
      "learning_rate": 1.66e-05,
      "loss": 0.0028,
      "step": 60120
    },
    {
      "epoch": 3.3405555555555555,
      "grad_norm": 0.24147853255271912,
      "learning_rate": 1.6594444444444447e-05,
      "loss": 0.0028,
      "step": 60130
    },
    {
      "epoch": 3.341111111111111,
      "grad_norm": 0.17818400263786316,
      "learning_rate": 1.658888888888889e-05,
      "loss": 0.0031,
      "step": 60140
    },
    {
      "epoch": 3.341666666666667,
      "grad_norm": 0.23790490627288818,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.002,
      "step": 60150
    },
    {
      "epoch": 3.3422222222222224,
      "grad_norm": 0.238535076379776,
      "learning_rate": 1.6577777777777778e-05,
      "loss": 0.003,
      "step": 60160
    },
    {
      "epoch": 3.3427777777777776,
      "grad_norm": 0.17795027792453766,
      "learning_rate": 1.657222222222222e-05,
      "loss": 0.0019,
      "step": 60170
    },
    {
      "epoch": 3.3433333333333333,
      "grad_norm": 0.062129195779561996,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0024,
      "step": 60180
    },
    {
      "epoch": 3.343888888888889,
      "grad_norm": 0.09002608805894852,
      "learning_rate": 1.6561111111111112e-05,
      "loss": 0.0049,
      "step": 60190
    },
    {
      "epoch": 3.3444444444444446,
      "grad_norm": 0.14889028668403625,
      "learning_rate": 1.655555555555556e-05,
      "loss": 0.0023,
      "step": 60200
    },
    {
      "epoch": 3.3449999999999998,
      "grad_norm": 0.20810329914093018,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 0.0029,
      "step": 60210
    },
    {
      "epoch": 3.3455555555555554,
      "grad_norm": 0.1488039493560791,
      "learning_rate": 1.6544444444444446e-05,
      "loss": 0.0022,
      "step": 60220
    },
    {
      "epoch": 3.346111111111111,
      "grad_norm": 0.17893151938915253,
      "learning_rate": 1.653888888888889e-05,
      "loss": 0.0025,
      "step": 60230
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.1780087649822235,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0025,
      "step": 60240
    },
    {
      "epoch": 3.3472222222222223,
      "grad_norm": 0.4747977554798126,
      "learning_rate": 1.6527777777777777e-05,
      "loss": 0.0023,
      "step": 60250
    },
    {
      "epoch": 3.347777777777778,
      "grad_norm": 0.0603899247944355,
      "learning_rate": 1.6522222222222224e-05,
      "loss": 0.0022,
      "step": 60260
    },
    {
      "epoch": 3.348333333333333,
      "grad_norm": 0.28187015652656555,
      "learning_rate": 1.6516666666666667e-05,
      "loss": 0.0026,
      "step": 60270
    },
    {
      "epoch": 3.348888888888889,
      "grad_norm": 0.04506663605570793,
      "learning_rate": 1.651111111111111e-05,
      "loss": 0.0043,
      "step": 60280
    },
    {
      "epoch": 3.3494444444444444,
      "grad_norm": 0.011741826310753822,
      "learning_rate": 1.6505555555555558e-05,
      "loss": 0.0034,
      "step": 60290
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.1485067456960678,
      "learning_rate": 1.65e-05,
      "loss": 0.0027,
      "step": 60300
    },
    {
      "epoch": 3.3505555555555557,
      "grad_norm": 0.089290551841259,
      "learning_rate": 1.6494444444444445e-05,
      "loss": 0.0038,
      "step": 60310
    },
    {
      "epoch": 3.351111111111111,
      "grad_norm": 0.23783957958221436,
      "learning_rate": 1.648888888888889e-05,
      "loss": 0.0038,
      "step": 60320
    },
    {
      "epoch": 3.3516666666666666,
      "grad_norm": 0.35508882999420166,
      "learning_rate": 1.6483333333333335e-05,
      "loss": 0.0035,
      "step": 60330
    },
    {
      "epoch": 3.352222222222222,
      "grad_norm": 0.03302096575498581,
      "learning_rate": 1.647777777777778e-05,
      "loss": 0.0024,
      "step": 60340
    },
    {
      "epoch": 3.352777777777778,
      "grad_norm": 0.06169222295284271,
      "learning_rate": 1.6472222222222222e-05,
      "loss": 0.002,
      "step": 60350
    },
    {
      "epoch": 3.3533333333333335,
      "grad_norm": 0.3269481360912323,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0023,
      "step": 60360
    },
    {
      "epoch": 3.3538888888888887,
      "grad_norm": 0.06018351390957832,
      "learning_rate": 1.6461111111111113e-05,
      "loss": 0.0032,
      "step": 60370
    },
    {
      "epoch": 3.3544444444444443,
      "grad_norm": 0.23959124088287354,
      "learning_rate": 1.6455555555555556e-05,
      "loss": 0.0028,
      "step": 60380
    },
    {
      "epoch": 3.355,
      "grad_norm": 0.03292234241962433,
      "learning_rate": 1.645e-05,
      "loss": 0.0026,
      "step": 60390
    },
    {
      "epoch": 3.3555555555555556,
      "grad_norm": 0.326635479927063,
      "learning_rate": 1.6444444444444447e-05,
      "loss": 0.0042,
      "step": 60400
    },
    {
      "epoch": 3.3561111111111113,
      "grad_norm": 0.23735465109348297,
      "learning_rate": 1.643888888888889e-05,
      "loss": 0.0029,
      "step": 60410
    },
    {
      "epoch": 3.3566666666666665,
      "grad_norm": 0.31795141100883484,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.0026,
      "step": 60420
    },
    {
      "epoch": 3.357222222222222,
      "grad_norm": 0.2972937822341919,
      "learning_rate": 1.6427777777777778e-05,
      "loss": 0.0032,
      "step": 60430
    },
    {
      "epoch": 3.3577777777777778,
      "grad_norm": 0.06714294850826263,
      "learning_rate": 1.642222222222222e-05,
      "loss": 0.0043,
      "step": 60440
    },
    {
      "epoch": 3.3583333333333334,
      "grad_norm": 0.11891987919807434,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.0036,
      "step": 60450
    },
    {
      "epoch": 3.358888888888889,
      "grad_norm": 0.5113728642463684,
      "learning_rate": 1.6411111111111112e-05,
      "loss": 0.0029,
      "step": 60460
    },
    {
      "epoch": 3.3594444444444447,
      "grad_norm": 0.09511197358369827,
      "learning_rate": 1.640555555555556e-05,
      "loss": 0.0031,
      "step": 60470
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.060591526329517365,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0029,
      "step": 60480
    },
    {
      "epoch": 3.3605555555555555,
      "grad_norm": 0.27076926827430725,
      "learning_rate": 1.6394444444444446e-05,
      "loss": 0.0024,
      "step": 60490
    },
    {
      "epoch": 3.361111111111111,
      "grad_norm": 0.10043443739414215,
      "learning_rate": 1.638888888888889e-05,
      "loss": 0.0021,
      "step": 60500
    },
    {
      "epoch": 3.361666666666667,
      "grad_norm": 0.1490393877029419,
      "learning_rate": 1.6383333333333333e-05,
      "loss": 0.003,
      "step": 60510
    },
    {
      "epoch": 3.362222222222222,
      "grad_norm": 0.2376532107591629,
      "learning_rate": 1.6377777777777776e-05,
      "loss": 0.0019,
      "step": 60520
    },
    {
      "epoch": 3.3627777777777776,
      "grad_norm": 0.03080478496849537,
      "learning_rate": 1.6372222222222223e-05,
      "loss": 0.004,
      "step": 60530
    },
    {
      "epoch": 3.3633333333333333,
      "grad_norm": 0.2670719027519226,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0035,
      "step": 60540
    },
    {
      "epoch": 3.363888888888889,
      "grad_norm": 0.011881153099238873,
      "learning_rate": 1.6361111111111114e-05,
      "loss": 0.0039,
      "step": 60550
    },
    {
      "epoch": 3.3644444444444446,
      "grad_norm": 0.008709481917321682,
      "learning_rate": 1.6355555555555557e-05,
      "loss": 0.0032,
      "step": 60560
    },
    {
      "epoch": 3.365,
      "grad_norm": 0.14856284856796265,
      "learning_rate": 1.635e-05,
      "loss": 0.0026,
      "step": 60570
    },
    {
      "epoch": 3.3655555555555554,
      "grad_norm": 0.06209167093038559,
      "learning_rate": 1.6344444444444445e-05,
      "loss": 0.0031,
      "step": 60580
    },
    {
      "epoch": 3.366111111111111,
      "grad_norm": 0.32721689343452454,
      "learning_rate": 1.6338888888888888e-05,
      "loss": 0.0029,
      "step": 60590
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.41714075207710266,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.002,
      "step": 60600
    },
    {
      "epoch": 3.3672222222222223,
      "grad_norm": 0.06801242381334305,
      "learning_rate": 1.632777777777778e-05,
      "loss": 0.0032,
      "step": 60610
    },
    {
      "epoch": 3.3677777777777775,
      "grad_norm": 0.27146339416503906,
      "learning_rate": 1.6322222222222222e-05,
      "loss": 0.0023,
      "step": 60620
    },
    {
      "epoch": 3.368333333333333,
      "grad_norm": 0.07211267948150635,
      "learning_rate": 1.6316666666666666e-05,
      "loss": 0.0032,
      "step": 60630
    },
    {
      "epoch": 3.368888888888889,
      "grad_norm": 0.11886132508516312,
      "learning_rate": 1.6311111111111113e-05,
      "loss": 0.0019,
      "step": 60640
    },
    {
      "epoch": 3.3694444444444445,
      "grad_norm": 0.13350164890289307,
      "learning_rate": 1.6305555555555556e-05,
      "loss": 0.0025,
      "step": 60650
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.11952576786279678,
      "learning_rate": 1.63e-05,
      "loss": 0.0031,
      "step": 60660
    },
    {
      "epoch": 3.3705555555555557,
      "grad_norm": 0.5307933688163757,
      "learning_rate": 1.6294444444444447e-05,
      "loss": 0.003,
      "step": 60670
    },
    {
      "epoch": 3.371111111111111,
      "grad_norm": 0.37123993039131165,
      "learning_rate": 1.628888888888889e-05,
      "loss": 0.0029,
      "step": 60680
    },
    {
      "epoch": 3.3716666666666666,
      "grad_norm": 0.3126455843448639,
      "learning_rate": 1.6283333333333334e-05,
      "loss": 0.0029,
      "step": 60690
    },
    {
      "epoch": 3.3722222222222222,
      "grad_norm": 0.03229239210486412,
      "learning_rate": 1.6277777777777777e-05,
      "loss": 0.0039,
      "step": 60700
    },
    {
      "epoch": 3.372777777777778,
      "grad_norm": 0.09029587358236313,
      "learning_rate": 1.627222222222222e-05,
      "loss": 0.0026,
      "step": 60710
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.39246731996536255,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0037,
      "step": 60720
    },
    {
      "epoch": 3.3738888888888887,
      "grad_norm": 0.17893457412719727,
      "learning_rate": 1.626111111111111e-05,
      "loss": 0.0036,
      "step": 60730
    },
    {
      "epoch": 3.3744444444444444,
      "grad_norm": 0.20760609209537506,
      "learning_rate": 1.625555555555556e-05,
      "loss": 0.0036,
      "step": 60740
    },
    {
      "epoch": 3.375,
      "grad_norm": 0.297086238861084,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.0021,
      "step": 60750
    },
    {
      "epoch": 3.3755555555555556,
      "grad_norm": 0.3736363649368286,
      "learning_rate": 1.6244444444444446e-05,
      "loss": 0.0028,
      "step": 60760
    },
    {
      "epoch": 3.3761111111111113,
      "grad_norm": 0.1385834813117981,
      "learning_rate": 1.623888888888889e-05,
      "loss": 0.003,
      "step": 60770
    },
    {
      "epoch": 3.3766666666666665,
      "grad_norm": 0.14866986870765686,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.0025,
      "step": 60780
    },
    {
      "epoch": 3.377222222222222,
      "grad_norm": 0.8694798350334167,
      "learning_rate": 1.6227777777777776e-05,
      "loss": 0.0028,
      "step": 60790
    },
    {
      "epoch": 3.3777777777777778,
      "grad_norm": 0.14904020726680756,
      "learning_rate": 1.6222222222222223e-05,
      "loss": 0.0024,
      "step": 60800
    },
    {
      "epoch": 3.3783333333333334,
      "grad_norm": 0.011907636187970638,
      "learning_rate": 1.6216666666666667e-05,
      "loss": 0.0039,
      "step": 60810
    },
    {
      "epoch": 3.378888888888889,
      "grad_norm": 0.20839343965053558,
      "learning_rate": 1.6211111111111114e-05,
      "loss": 0.0028,
      "step": 60820
    },
    {
      "epoch": 3.3794444444444443,
      "grad_norm": 0.03204984962940216,
      "learning_rate": 1.6205555555555557e-05,
      "loss": 0.0033,
      "step": 60830
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.35687410831451416,
      "learning_rate": 1.62e-05,
      "loss": 0.0042,
      "step": 60840
    },
    {
      "epoch": 3.3805555555555555,
      "grad_norm": 0.29780882596969604,
      "learning_rate": 1.6194444444444444e-05,
      "loss": 0.0033,
      "step": 60850
    },
    {
      "epoch": 3.381111111111111,
      "grad_norm": 0.47245657444000244,
      "learning_rate": 1.6188888888888888e-05,
      "loss": 0.0024,
      "step": 60860
    },
    {
      "epoch": 3.381666666666667,
      "grad_norm": 0.08929738402366638,
      "learning_rate": 1.6183333333333335e-05,
      "loss": 0.002,
      "step": 60870
    },
    {
      "epoch": 3.3822222222222225,
      "grad_norm": 0.0613241046667099,
      "learning_rate": 1.617777777777778e-05,
      "loss": 0.003,
      "step": 60880
    },
    {
      "epoch": 3.3827777777777777,
      "grad_norm": 0.26730218529701233,
      "learning_rate": 1.6172222222222222e-05,
      "loss": 0.0022,
      "step": 60890
    },
    {
      "epoch": 3.3833333333333333,
      "grad_norm": 0.46649032831192017,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0025,
      "step": 60900
    },
    {
      "epoch": 3.383888888888889,
      "grad_norm": 0.08946091681718826,
      "learning_rate": 1.6161111111111112e-05,
      "loss": 0.0032,
      "step": 60910
    },
    {
      "epoch": 3.3844444444444446,
      "grad_norm": 0.32183754444122314,
      "learning_rate": 1.6155555555555556e-05,
      "loss": 0.0032,
      "step": 60920
    },
    {
      "epoch": 3.385,
      "grad_norm": 0.41529178619384766,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.0028,
      "step": 60930
    },
    {
      "epoch": 3.3855555555555554,
      "grad_norm": 0.03169234097003937,
      "learning_rate": 1.6144444444444446e-05,
      "loss": 0.0029,
      "step": 60940
    },
    {
      "epoch": 3.386111111111111,
      "grad_norm": 0.23750808835029602,
      "learning_rate": 1.613888888888889e-05,
      "loss": 0.0021,
      "step": 60950
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.41547825932502747,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0018,
      "step": 60960
    },
    {
      "epoch": 3.3872222222222224,
      "grad_norm": 0.09210168570280075,
      "learning_rate": 1.6127777777777777e-05,
      "loss": 0.0033,
      "step": 60970
    },
    {
      "epoch": 3.387777777777778,
      "grad_norm": 0.20931150019168854,
      "learning_rate": 1.612222222222222e-05,
      "loss": 0.0026,
      "step": 60980
    },
    {
      "epoch": 3.388333333333333,
      "grad_norm": 0.17838114500045776,
      "learning_rate": 1.6116666666666668e-05,
      "loss": 0.0022,
      "step": 60990
    },
    {
      "epoch": 3.388888888888889,
      "grad_norm": 0.20912297070026398,
      "learning_rate": 1.6111111111111115e-05,
      "loss": 0.0019,
      "step": 61000
    },
    {
      "epoch": 3.3894444444444445,
      "grad_norm": 0.6232335567474365,
      "learning_rate": 1.6105555555555558e-05,
      "loss": 0.0037,
      "step": 61010
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.47581368684768677,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0036,
      "step": 61020
    },
    {
      "epoch": 3.3905555555555553,
      "grad_norm": 0.3578934669494629,
      "learning_rate": 1.6094444444444445e-05,
      "loss": 0.0042,
      "step": 61030
    },
    {
      "epoch": 3.391111111111111,
      "grad_norm": 0.6854342818260193,
      "learning_rate": 1.608888888888889e-05,
      "loss": 0.004,
      "step": 61040
    },
    {
      "epoch": 3.3916666666666666,
      "grad_norm": 0.1491607129573822,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.0027,
      "step": 61050
    },
    {
      "epoch": 3.3922222222222222,
      "grad_norm": 0.20848171412944794,
      "learning_rate": 1.607777777777778e-05,
      "loss": 0.0029,
      "step": 61060
    },
    {
      "epoch": 3.392777777777778,
      "grad_norm": 0.20879380404949188,
      "learning_rate": 1.6072222222222223e-05,
      "loss": 0.0027,
      "step": 61070
    },
    {
      "epoch": 3.3933333333333335,
      "grad_norm": 0.0915277823805809,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0034,
      "step": 61080
    },
    {
      "epoch": 3.3938888888888887,
      "grad_norm": 0.11969276517629623,
      "learning_rate": 1.6061111111111113e-05,
      "loss": 0.0018,
      "step": 61090
    },
    {
      "epoch": 3.3944444444444444,
      "grad_norm": 0.3571993410587311,
      "learning_rate": 1.6055555555555557e-05,
      "loss": 0.0025,
      "step": 61100
    },
    {
      "epoch": 3.395,
      "grad_norm": 0.38614946603775024,
      "learning_rate": 1.605e-05,
      "loss": 0.0035,
      "step": 61110
    },
    {
      "epoch": 3.3955555555555557,
      "grad_norm": 0.1194595992565155,
      "learning_rate": 1.6044444444444444e-05,
      "loss": 0.0038,
      "step": 61120
    },
    {
      "epoch": 3.3961111111111113,
      "grad_norm": 0.26775819063186646,
      "learning_rate": 1.603888888888889e-05,
      "loss": 0.0026,
      "step": 61130
    },
    {
      "epoch": 3.3966666666666665,
      "grad_norm": 0.5574650168418884,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.0029,
      "step": 61140
    },
    {
      "epoch": 3.397222222222222,
      "grad_norm": 0.20812852680683136,
      "learning_rate": 1.6027777777777778e-05,
      "loss": 0.003,
      "step": 61150
    },
    {
      "epoch": 3.397777777777778,
      "grad_norm": 0.1097271591424942,
      "learning_rate": 1.602222222222222e-05,
      "loss": 0.0019,
      "step": 61160
    },
    {
      "epoch": 3.3983333333333334,
      "grad_norm": 0.8010269999504089,
      "learning_rate": 1.601666666666667e-05,
      "loss": 0.0031,
      "step": 61170
    },
    {
      "epoch": 3.398888888888889,
      "grad_norm": 0.20827998220920563,
      "learning_rate": 1.6011111111111112e-05,
      "loss": 0.0023,
      "step": 61180
    },
    {
      "epoch": 3.3994444444444443,
      "grad_norm": 0.17846018075942993,
      "learning_rate": 1.6005555555555556e-05,
      "loss": 0.0029,
      "step": 61190
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.1504824012517929,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.003,
      "step": 61200
    },
    {
      "epoch": 3.4005555555555556,
      "grad_norm": 0.23751422762870789,
      "learning_rate": 1.5994444444444446e-05,
      "loss": 0.0042,
      "step": 61210
    },
    {
      "epoch": 3.401111111111111,
      "grad_norm": 0.11928106099367142,
      "learning_rate": 1.598888888888889e-05,
      "loss": 0.0028,
      "step": 61220
    },
    {
      "epoch": 3.401666666666667,
      "grad_norm": 0.14948484301567078,
      "learning_rate": 1.5983333333333333e-05,
      "loss": 0.0031,
      "step": 61230
    },
    {
      "epoch": 3.402222222222222,
      "grad_norm": 0.40817761421203613,
      "learning_rate": 1.5977777777777777e-05,
      "loss": 0.0021,
      "step": 61240
    },
    {
      "epoch": 3.4027777777777777,
      "grad_norm": 0.08929956704378128,
      "learning_rate": 1.597222222222222e-05,
      "loss": 0.0028,
      "step": 61250
    },
    {
      "epoch": 3.4033333333333333,
      "grad_norm": 0.2388889044523239,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0023,
      "step": 61260
    },
    {
      "epoch": 3.403888888888889,
      "grad_norm": 0.24190835654735565,
      "learning_rate": 1.5961111111111114e-05,
      "loss": 0.0028,
      "step": 61270
    },
    {
      "epoch": 3.4044444444444446,
      "grad_norm": 0.3393458425998688,
      "learning_rate": 1.5955555555555558e-05,
      "loss": 0.003,
      "step": 61280
    },
    {
      "epoch": 3.4050000000000002,
      "grad_norm": 0.16753549873828888,
      "learning_rate": 1.595e-05,
      "loss": 0.004,
      "step": 61290
    },
    {
      "epoch": 3.4055555555555554,
      "grad_norm": 0.4321998655796051,
      "learning_rate": 1.5944444444444445e-05,
      "loss": 0.0034,
      "step": 61300
    },
    {
      "epoch": 3.406111111111111,
      "grad_norm": 0.6614153385162354,
      "learning_rate": 1.593888888888889e-05,
      "loss": 0.0028,
      "step": 61310
    },
    {
      "epoch": 3.4066666666666667,
      "grad_norm": 0.05954941734671593,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0018,
      "step": 61320
    },
    {
      "epoch": 3.4072222222222224,
      "grad_norm": 0.12145645171403885,
      "learning_rate": 1.592777777777778e-05,
      "loss": 0.0037,
      "step": 61330
    },
    {
      "epoch": 3.4077777777777776,
      "grad_norm": 0.27374792098999023,
      "learning_rate": 1.5922222222222223e-05,
      "loss": 0.0026,
      "step": 61340
    },
    {
      "epoch": 3.408333333333333,
      "grad_norm": 0.1489964872598648,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.0023,
      "step": 61350
    },
    {
      "epoch": 3.408888888888889,
      "grad_norm": 0.5051519274711609,
      "learning_rate": 1.5911111111111113e-05,
      "loss": 0.0029,
      "step": 61360
    },
    {
      "epoch": 3.4094444444444445,
      "grad_norm": 0.06210630387067795,
      "learning_rate": 1.5905555555555557e-05,
      "loss": 0.0028,
      "step": 61370
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.4353068768978119,
      "learning_rate": 1.59e-05,
      "loss": 0.0039,
      "step": 61380
    },
    {
      "epoch": 3.410555555555556,
      "grad_norm": 0.17920643091201782,
      "learning_rate": 1.5894444444444444e-05,
      "loss": 0.004,
      "step": 61390
    },
    {
      "epoch": 3.411111111111111,
      "grad_norm": 0.17830076813697815,
      "learning_rate": 1.588888888888889e-05,
      "loss": 0.0034,
      "step": 61400
    },
    {
      "epoch": 3.4116666666666666,
      "grad_norm": 0.2378869652748108,
      "learning_rate": 1.5883333333333334e-05,
      "loss": 0.002,
      "step": 61410
    },
    {
      "epoch": 3.4122222222222223,
      "grad_norm": 0.14859457314014435,
      "learning_rate": 1.5877777777777778e-05,
      "loss": 0.0029,
      "step": 61420
    },
    {
      "epoch": 3.412777777777778,
      "grad_norm": 0.2672451138496399,
      "learning_rate": 1.587222222222222e-05,
      "loss": 0.0034,
      "step": 61430
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.17926785349845886,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0034,
      "step": 61440
    },
    {
      "epoch": 3.4138888888888888,
      "grad_norm": 0.18996737897396088,
      "learning_rate": 1.5861111111111112e-05,
      "loss": 0.003,
      "step": 61450
    },
    {
      "epoch": 3.4144444444444444,
      "grad_norm": 0.014517359435558319,
      "learning_rate": 1.5855555555555555e-05,
      "loss": 0.0029,
      "step": 61460
    },
    {
      "epoch": 3.415,
      "grad_norm": 0.26719027757644653,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.0036,
      "step": 61470
    },
    {
      "epoch": 3.4155555555555557,
      "grad_norm": 0.1783469021320343,
      "learning_rate": 1.5844444444444446e-05,
      "loss": 0.0023,
      "step": 61480
    },
    {
      "epoch": 3.4161111111111113,
      "grad_norm": 0.06120607256889343,
      "learning_rate": 1.583888888888889e-05,
      "loss": 0.0044,
      "step": 61490
    },
    {
      "epoch": 3.4166666666666665,
      "grad_norm": 0.2087506353855133,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.0028,
      "step": 61500
    },
    {
      "epoch": 3.417222222222222,
      "grad_norm": 0.17878387868404388,
      "learning_rate": 1.5827777777777777e-05,
      "loss": 0.0024,
      "step": 61510
    },
    {
      "epoch": 3.417777777777778,
      "grad_norm": 0.12080277502536774,
      "learning_rate": 1.582222222222222e-05,
      "loss": 0.0015,
      "step": 61520
    },
    {
      "epoch": 3.4183333333333334,
      "grad_norm": 0.034985270351171494,
      "learning_rate": 1.5816666666666667e-05,
      "loss": 0.0034,
      "step": 61530
    },
    {
      "epoch": 3.4188888888888886,
      "grad_norm": 0.0330442450940609,
      "learning_rate": 1.5811111111111114e-05,
      "loss": 0.0035,
      "step": 61540
    },
    {
      "epoch": 3.4194444444444443,
      "grad_norm": 0.1787993162870407,
      "learning_rate": 1.5805555555555558e-05,
      "loss": 0.0032,
      "step": 61550
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.23444993793964386,
      "learning_rate": 1.58e-05,
      "loss": 0.0029,
      "step": 61560
    },
    {
      "epoch": 3.4205555555555556,
      "grad_norm": 0.04533582553267479,
      "learning_rate": 1.5794444444444445e-05,
      "loss": 0.0022,
      "step": 61570
    },
    {
      "epoch": 3.421111111111111,
      "grad_norm": 0.2669626772403717,
      "learning_rate": 1.5788888888888888e-05,
      "loss": 0.004,
      "step": 61580
    },
    {
      "epoch": 3.421666666666667,
      "grad_norm": 0.32755064964294434,
      "learning_rate": 1.5783333333333332e-05,
      "loss": 0.0036,
      "step": 61590
    },
    {
      "epoch": 3.422222222222222,
      "grad_norm": 0.12703213095664978,
      "learning_rate": 1.577777777777778e-05,
      "loss": 0.0017,
      "step": 61600
    },
    {
      "epoch": 3.4227777777777777,
      "grad_norm": 0.26685288548469543,
      "learning_rate": 1.5772222222222226e-05,
      "loss": 0.0033,
      "step": 61610
    },
    {
      "epoch": 3.4233333333333333,
      "grad_norm": 0.31623509526252747,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0024,
      "step": 61620
    },
    {
      "epoch": 3.423888888888889,
      "grad_norm": 0.21412114799022675,
      "learning_rate": 1.5761111111111113e-05,
      "loss": 0.0025,
      "step": 61630
    },
    {
      "epoch": 3.4244444444444446,
      "grad_norm": 0.060145214200019836,
      "learning_rate": 1.5755555555555556e-05,
      "loss": 0.0027,
      "step": 61640
    },
    {
      "epoch": 3.425,
      "grad_norm": 0.17919228971004486,
      "learning_rate": 1.575e-05,
      "loss": 0.0028,
      "step": 61650
    },
    {
      "epoch": 3.4255555555555555,
      "grad_norm": 0.01712503656744957,
      "learning_rate": 1.5744444444444444e-05,
      "loss": 0.0023,
      "step": 61660
    },
    {
      "epoch": 3.426111111111111,
      "grad_norm": 0.3721444606781006,
      "learning_rate": 1.573888888888889e-05,
      "loss": 0.0032,
      "step": 61670
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.3163326382637024,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.003,
      "step": 61680
    },
    {
      "epoch": 3.4272222222222224,
      "grad_norm": 0.6236144304275513,
      "learning_rate": 1.5727777777777778e-05,
      "loss": 0.0034,
      "step": 61690
    },
    {
      "epoch": 3.4277777777777776,
      "grad_norm": 0.034590527415275574,
      "learning_rate": 1.5722222222222225e-05,
      "loss": 0.0038,
      "step": 61700
    },
    {
      "epoch": 3.4283333333333332,
      "grad_norm": 0.09618739783763885,
      "learning_rate": 1.5716666666666668e-05,
      "loss": 0.0042,
      "step": 61710
    },
    {
      "epoch": 3.428888888888889,
      "grad_norm": 0.2980972230434418,
      "learning_rate": 1.571111111111111e-05,
      "loss": 0.0021,
      "step": 61720
    },
    {
      "epoch": 3.4294444444444445,
      "grad_norm": 0.1190578043460846,
      "learning_rate": 1.5705555555555555e-05,
      "loss": 0.0031,
      "step": 61730
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.26915988326072693,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.0034,
      "step": 61740
    },
    {
      "epoch": 3.4305555555555554,
      "grad_norm": 0.031112460419535637,
      "learning_rate": 1.5694444444444446e-05,
      "loss": 0.003,
      "step": 61750
    },
    {
      "epoch": 3.431111111111111,
      "grad_norm": 0.8103359341621399,
      "learning_rate": 1.568888888888889e-05,
      "loss": 0.0028,
      "step": 61760
    },
    {
      "epoch": 3.4316666666666666,
      "grad_norm": 0.059825848788022995,
      "learning_rate": 1.5683333333333333e-05,
      "loss": 0.0032,
      "step": 61770
    },
    {
      "epoch": 3.4322222222222223,
      "grad_norm": 0.0365484245121479,
      "learning_rate": 1.5677777777777776e-05,
      "loss": 0.0031,
      "step": 61780
    },
    {
      "epoch": 3.432777777777778,
      "grad_norm": 0.7722874283790588,
      "learning_rate": 1.5672222222222223e-05,
      "loss": 0.0039,
      "step": 61790
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.2373207062482834,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0026,
      "step": 61800
    },
    {
      "epoch": 3.4338888888888888,
      "grad_norm": 0.03692983090877533,
      "learning_rate": 1.5661111111111114e-05,
      "loss": 0.0021,
      "step": 61810
    },
    {
      "epoch": 3.4344444444444444,
      "grad_norm": 0.06004662439227104,
      "learning_rate": 1.5655555555555557e-05,
      "loss": 0.0031,
      "step": 61820
    },
    {
      "epoch": 3.435,
      "grad_norm": 0.17820055782794952,
      "learning_rate": 1.565e-05,
      "loss": 0.0029,
      "step": 61830
    },
    {
      "epoch": 3.4355555555555557,
      "grad_norm": 0.03301247954368591,
      "learning_rate": 1.5644444444444444e-05,
      "loss": 0.0039,
      "step": 61840
    },
    {
      "epoch": 3.436111111111111,
      "grad_norm": 0.2760452628135681,
      "learning_rate": 1.5638888888888888e-05,
      "loss": 0.0027,
      "step": 61850
    },
    {
      "epoch": 3.4366666666666665,
      "grad_norm": 0.1809369921684265,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.002,
      "step": 61860
    },
    {
      "epoch": 3.437222222222222,
      "grad_norm": 0.03684787452220917,
      "learning_rate": 1.562777777777778e-05,
      "loss": 0.0031,
      "step": 61870
    },
    {
      "epoch": 3.437777777777778,
      "grad_norm": 0.13405095040798187,
      "learning_rate": 1.5622222222222225e-05,
      "loss": 0.0031,
      "step": 61880
    },
    {
      "epoch": 3.4383333333333335,
      "grad_norm": 0.20874010026454926,
      "learning_rate": 1.561666666666667e-05,
      "loss": 0.0031,
      "step": 61890
    },
    {
      "epoch": 3.438888888888889,
      "grad_norm": 0.41904982924461365,
      "learning_rate": 1.5611111111111113e-05,
      "loss": 0.003,
      "step": 61900
    },
    {
      "epoch": 3.4394444444444443,
      "grad_norm": 0.4745008051395416,
      "learning_rate": 1.5605555555555556e-05,
      "loss": 0.0046,
      "step": 61910
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.5640227198600769,
      "learning_rate": 1.56e-05,
      "loss": 0.0033,
      "step": 61920
    },
    {
      "epoch": 3.4405555555555556,
      "grad_norm": 0.23820535838603973,
      "learning_rate": 1.5594444444444443e-05,
      "loss": 0.0018,
      "step": 61930
    },
    {
      "epoch": 3.4411111111111112,
      "grad_norm": 0.2696308493614197,
      "learning_rate": 1.558888888888889e-05,
      "loss": 0.0024,
      "step": 61940
    },
    {
      "epoch": 3.4416666666666664,
      "grad_norm": 0.11940652877092361,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.0035,
      "step": 61950
    },
    {
      "epoch": 3.442222222222222,
      "grad_norm": 0.23958350718021393,
      "learning_rate": 1.5577777777777777e-05,
      "loss": 0.0024,
      "step": 61960
    },
    {
      "epoch": 3.4427777777777777,
      "grad_norm": 0.20890061557292938,
      "learning_rate": 1.5572222222222224e-05,
      "loss": 0.0035,
      "step": 61970
    },
    {
      "epoch": 3.4433333333333334,
      "grad_norm": 0.11866488307714462,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0028,
      "step": 61980
    },
    {
      "epoch": 3.443888888888889,
      "grad_norm": 0.6059895157814026,
      "learning_rate": 1.556111111111111e-05,
      "loss": 0.0035,
      "step": 61990
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.6353579163551331,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 0.0033,
      "step": 62000
    },
    {
      "epoch": 3.445,
      "grad_norm": 0.09053032100200653,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 0.0017,
      "step": 62010
    },
    {
      "epoch": 3.4455555555555555,
      "grad_norm": 0.10461152344942093,
      "learning_rate": 1.5544444444444445e-05,
      "loss": 0.0022,
      "step": 62020
    },
    {
      "epoch": 3.446111111111111,
      "grad_norm": 0.03329300507903099,
      "learning_rate": 1.553888888888889e-05,
      "loss": 0.0031,
      "step": 62030
    },
    {
      "epoch": 3.4466666666666668,
      "grad_norm": 0.2670838236808777,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.002,
      "step": 62040
    },
    {
      "epoch": 3.4472222222222224,
      "grad_norm": 0.1777862161397934,
      "learning_rate": 1.5527777777777776e-05,
      "loss": 0.0035,
      "step": 62050
    },
    {
      "epoch": 3.4477777777777776,
      "grad_norm": 0.23722772300243378,
      "learning_rate": 1.5522222222222223e-05,
      "loss": 0.003,
      "step": 62060
    },
    {
      "epoch": 3.4483333333333333,
      "grad_norm": 0.11924665421247482,
      "learning_rate": 1.5516666666666667e-05,
      "loss": 0.0025,
      "step": 62070
    },
    {
      "epoch": 3.448888888888889,
      "grad_norm": 0.4455547034740448,
      "learning_rate": 1.5511111111111114e-05,
      "loss": 0.0021,
      "step": 62080
    },
    {
      "epoch": 3.4494444444444445,
      "grad_norm": 0.20813243091106415,
      "learning_rate": 1.5505555555555557e-05,
      "loss": 0.004,
      "step": 62090
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.09006073325872421,
      "learning_rate": 1.55e-05,
      "loss": 0.0026,
      "step": 62100
    },
    {
      "epoch": 3.4505555555555554,
      "grad_norm": 0.03137359768152237,
      "learning_rate": 1.5494444444444444e-05,
      "loss": 0.0021,
      "step": 62110
    },
    {
      "epoch": 3.451111111111111,
      "grad_norm": 0.297607958316803,
      "learning_rate": 1.5488888888888888e-05,
      "loss": 0.0035,
      "step": 62120
    },
    {
      "epoch": 3.4516666666666667,
      "grad_norm": 0.09096338599920273,
      "learning_rate": 1.548333333333333e-05,
      "loss": 0.0033,
      "step": 62130
    },
    {
      "epoch": 3.4522222222222223,
      "grad_norm": 0.09910683333873749,
      "learning_rate": 1.5477777777777778e-05,
      "loss": 0.003,
      "step": 62140
    },
    {
      "epoch": 3.452777777777778,
      "grad_norm": 0.0909438356757164,
      "learning_rate": 1.5472222222222225e-05,
      "loss": 0.0027,
      "step": 62150
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.14967767894268036,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0049,
      "step": 62160
    },
    {
      "epoch": 3.453888888888889,
      "grad_norm": 0.016919195652008057,
      "learning_rate": 1.5461111111111112e-05,
      "loss": 0.0029,
      "step": 62170
    },
    {
      "epoch": 3.4544444444444444,
      "grad_norm": 0.09289247542619705,
      "learning_rate": 1.5455555555555556e-05,
      "loss": 0.0016,
      "step": 62180
    },
    {
      "epoch": 3.455,
      "grad_norm": 0.0697726160287857,
      "learning_rate": 1.545e-05,
      "loss": 0.002,
      "step": 62190
    },
    {
      "epoch": 3.4555555555555557,
      "grad_norm": 0.01297678705304861,
      "learning_rate": 1.5444444444444446e-05,
      "loss": 0.0028,
      "step": 62200
    },
    {
      "epoch": 3.4561111111111114,
      "grad_norm": 0.06177620589733124,
      "learning_rate": 1.543888888888889e-05,
      "loss": 0.0033,
      "step": 62210
    },
    {
      "epoch": 3.4566666666666666,
      "grad_norm": 0.06079582870006561,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.0029,
      "step": 62220
    },
    {
      "epoch": 3.457222222222222,
      "grad_norm": 0.1191418394446373,
      "learning_rate": 1.542777777777778e-05,
      "loss": 0.0028,
      "step": 62230
    },
    {
      "epoch": 3.457777777777778,
      "grad_norm": 0.06118776276707649,
      "learning_rate": 1.5422222222222224e-05,
      "loss": 0.0021,
      "step": 62240
    },
    {
      "epoch": 3.4583333333333335,
      "grad_norm": 0.17878884077072144,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.0025,
      "step": 62250
    },
    {
      "epoch": 3.4588888888888887,
      "grad_norm": 0.18148553371429443,
      "learning_rate": 1.541111111111111e-05,
      "loss": 0.0034,
      "step": 62260
    },
    {
      "epoch": 3.4594444444444443,
      "grad_norm": 0.012306842021644115,
      "learning_rate": 1.5405555555555558e-05,
      "loss": 0.0019,
      "step": 62270
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.356484055519104,
      "learning_rate": 1.54e-05,
      "loss": 0.0025,
      "step": 62280
    },
    {
      "epoch": 3.4605555555555556,
      "grad_norm": 0.11876309663057327,
      "learning_rate": 1.5394444444444445e-05,
      "loss": 0.0029,
      "step": 62290
    },
    {
      "epoch": 3.4611111111111112,
      "grad_norm": 0.2092353105545044,
      "learning_rate": 1.538888888888889e-05,
      "loss": 0.0029,
      "step": 62300
    },
    {
      "epoch": 3.461666666666667,
      "grad_norm": 0.06406362354755402,
      "learning_rate": 1.5383333333333332e-05,
      "loss": 0.0039,
      "step": 62310
    },
    {
      "epoch": 3.462222222222222,
      "grad_norm": 0.14799055457115173,
      "learning_rate": 1.537777777777778e-05,
      "loss": 0.0045,
      "step": 62320
    },
    {
      "epoch": 3.4627777777777777,
      "grad_norm": 0.20845019817352295,
      "learning_rate": 1.5372222222222223e-05,
      "loss": 0.0034,
      "step": 62330
    },
    {
      "epoch": 3.4633333333333334,
      "grad_norm": 0.46209558844566345,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0023,
      "step": 62340
    },
    {
      "epoch": 3.463888888888889,
      "grad_norm": 0.1498079150915146,
      "learning_rate": 1.5361111111111113e-05,
      "loss": 0.0034,
      "step": 62350
    },
    {
      "epoch": 3.464444444444444,
      "grad_norm": 0.20820124447345734,
      "learning_rate": 1.5355555555555557e-05,
      "loss": 0.0029,
      "step": 62360
    },
    {
      "epoch": 3.465,
      "grad_norm": 0.0895899161696434,
      "learning_rate": 1.535e-05,
      "loss": 0.0037,
      "step": 62370
    },
    {
      "epoch": 3.4655555555555555,
      "grad_norm": 0.7284454107284546,
      "learning_rate": 1.5344444444444444e-05,
      "loss": 0.0024,
      "step": 62380
    },
    {
      "epoch": 3.466111111111111,
      "grad_norm": 0.4099094867706299,
      "learning_rate": 1.5338888888888888e-05,
      "loss": 0.0034,
      "step": 62390
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.06143632531166077,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0031,
      "step": 62400
    },
    {
      "epoch": 3.4672222222222224,
      "grad_norm": 0.2675016224384308,
      "learning_rate": 1.532777777777778e-05,
      "loss": 0.0025,
      "step": 62410
    },
    {
      "epoch": 3.4677777777777776,
      "grad_norm": 0.2969134449958801,
      "learning_rate": 1.5322222222222225e-05,
      "loss": 0.0026,
      "step": 62420
    },
    {
      "epoch": 3.4683333333333333,
      "grad_norm": 0.2755080759525299,
      "learning_rate": 1.531666666666667e-05,
      "loss": 0.003,
      "step": 62430
    },
    {
      "epoch": 3.468888888888889,
      "grad_norm": 0.9061357975006104,
      "learning_rate": 1.5311111111111112e-05,
      "loss": 0.0023,
      "step": 62440
    },
    {
      "epoch": 3.4694444444444446,
      "grad_norm": 0.14834992587566376,
      "learning_rate": 1.5305555555555556e-05,
      "loss": 0.0036,
      "step": 62450
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 0.13242682814598083,
      "learning_rate": 1.53e-05,
      "loss": 0.0015,
      "step": 62460
    },
    {
      "epoch": 3.4705555555555554,
      "grad_norm": 0.4015473425388336,
      "learning_rate": 1.5294444444444446e-05,
      "loss": 0.003,
      "step": 62470
    },
    {
      "epoch": 3.471111111111111,
      "grad_norm": 0.15078429877758026,
      "learning_rate": 1.528888888888889e-05,
      "loss": 0.0029,
      "step": 62480
    },
    {
      "epoch": 3.4716666666666667,
      "grad_norm": 0.031643420457839966,
      "learning_rate": 1.5283333333333333e-05,
      "loss": 0.0024,
      "step": 62490
    },
    {
      "epoch": 3.4722222222222223,
      "grad_norm": 0.17724300920963287,
      "learning_rate": 1.527777777777778e-05,
      "loss": 0.0026,
      "step": 62500
    },
    {
      "epoch": 3.472777777777778,
      "grad_norm": 0.6263562440872192,
      "learning_rate": 1.5272222222222224e-05,
      "loss": 0.002,
      "step": 62510
    },
    {
      "epoch": 3.473333333333333,
      "grad_norm": 0.21167783439159393,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0026,
      "step": 62520
    },
    {
      "epoch": 3.473888888888889,
      "grad_norm": 0.20896877348423004,
      "learning_rate": 1.526111111111111e-05,
      "loss": 0.0028,
      "step": 62530
    },
    {
      "epoch": 3.4744444444444444,
      "grad_norm": 0.032264310866594315,
      "learning_rate": 1.5255555555555556e-05,
      "loss": 0.003,
      "step": 62540
    },
    {
      "epoch": 3.475,
      "grad_norm": 0.14907465875148773,
      "learning_rate": 1.525e-05,
      "loss": 0.0032,
      "step": 62550
    },
    {
      "epoch": 3.4755555555555557,
      "grad_norm": 0.29657718539237976,
      "learning_rate": 1.5244444444444445e-05,
      "loss": 0.0027,
      "step": 62560
    },
    {
      "epoch": 3.476111111111111,
      "grad_norm": 0.12524205446243286,
      "learning_rate": 1.5238888888888888e-05,
      "loss": 0.0039,
      "step": 62570
    },
    {
      "epoch": 3.4766666666666666,
      "grad_norm": 0.03138263523578644,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0029,
      "step": 62580
    },
    {
      "epoch": 3.477222222222222,
      "grad_norm": 0.2671821117401123,
      "learning_rate": 1.5227777777777779e-05,
      "loss": 0.0037,
      "step": 62590
    },
    {
      "epoch": 3.477777777777778,
      "grad_norm": 0.12016429752111435,
      "learning_rate": 1.5222222222222224e-05,
      "loss": 0.0034,
      "step": 62600
    },
    {
      "epoch": 3.4783333333333335,
      "grad_norm": 0.4238490164279938,
      "learning_rate": 1.5216666666666668e-05,
      "loss": 0.0034,
      "step": 62610
    },
    {
      "epoch": 3.4788888888888887,
      "grad_norm": 0.01840301789343357,
      "learning_rate": 1.5211111111111111e-05,
      "loss": 0.0039,
      "step": 62620
    },
    {
      "epoch": 3.4794444444444443,
      "grad_norm": 0.23919162154197693,
      "learning_rate": 1.5205555555555557e-05,
      "loss": 0.0025,
      "step": 62630
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.032836150377988815,
      "learning_rate": 1.52e-05,
      "loss": 0.0031,
      "step": 62640
    },
    {
      "epoch": 3.4805555555555556,
      "grad_norm": 0.03127560392022133,
      "learning_rate": 1.5194444444444444e-05,
      "loss": 0.003,
      "step": 62650
    },
    {
      "epoch": 3.4811111111111113,
      "grad_norm": 0.1791769564151764,
      "learning_rate": 1.5188888888888889e-05,
      "loss": 0.0016,
      "step": 62660
    },
    {
      "epoch": 3.4816666666666665,
      "grad_norm": 0.10673732310533524,
      "learning_rate": 1.5183333333333333e-05,
      "loss": 0.0024,
      "step": 62670
    },
    {
      "epoch": 3.482222222222222,
      "grad_norm": 0.3271976411342621,
      "learning_rate": 1.517777777777778e-05,
      "loss": 0.0024,
      "step": 62680
    },
    {
      "epoch": 3.4827777777777778,
      "grad_norm": 0.08981043100357056,
      "learning_rate": 1.5172222222222223e-05,
      "loss": 0.0029,
      "step": 62690
    },
    {
      "epoch": 3.4833333333333334,
      "grad_norm": 0.04244232177734375,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0032,
      "step": 62700
    },
    {
      "epoch": 3.483888888888889,
      "grad_norm": 0.20798221230506897,
      "learning_rate": 1.5161111111111112e-05,
      "loss": 0.0021,
      "step": 62710
    },
    {
      "epoch": 3.4844444444444447,
      "grad_norm": 0.15054205060005188,
      "learning_rate": 1.5155555555555555e-05,
      "loss": 0.0031,
      "step": 62720
    },
    {
      "epoch": 3.485,
      "grad_norm": 0.012365670874714851,
      "learning_rate": 1.515e-05,
      "loss": 0.0026,
      "step": 62730
    },
    {
      "epoch": 3.4855555555555555,
      "grad_norm": 0.42711877822875977,
      "learning_rate": 1.5144444444444444e-05,
      "loss": 0.0034,
      "step": 62740
    },
    {
      "epoch": 3.486111111111111,
      "grad_norm": 0.6670167446136475,
      "learning_rate": 1.5138888888888888e-05,
      "loss": 0.0031,
      "step": 62750
    },
    {
      "epoch": 3.486666666666667,
      "grad_norm": 0.12916144728660583,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0022,
      "step": 62760
    },
    {
      "epoch": 3.487222222222222,
      "grad_norm": 0.034458357840776443,
      "learning_rate": 1.512777777777778e-05,
      "loss": 0.0035,
      "step": 62770
    },
    {
      "epoch": 3.4877777777777776,
      "grad_norm": 0.08933590352535248,
      "learning_rate": 1.5122222222222224e-05,
      "loss": 0.0024,
      "step": 62780
    },
    {
      "epoch": 3.4883333333333333,
      "grad_norm": 0.134307861328125,
      "learning_rate": 1.5116666666666667e-05,
      "loss": 0.003,
      "step": 62790
    },
    {
      "epoch": 3.488888888888889,
      "grad_norm": 0.03053402341902256,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 0.0032,
      "step": 62800
    },
    {
      "epoch": 3.4894444444444446,
      "grad_norm": 0.17859084904193878,
      "learning_rate": 1.5105555555555556e-05,
      "loss": 0.003,
      "step": 62810
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.6535649299621582,
      "learning_rate": 1.51e-05,
      "loss": 0.0029,
      "step": 62820
    },
    {
      "epoch": 3.4905555555555554,
      "grad_norm": 0.3558752238750458,
      "learning_rate": 1.5094444444444445e-05,
      "loss": 0.0026,
      "step": 62830
    },
    {
      "epoch": 3.491111111111111,
      "grad_norm": 0.3953450918197632,
      "learning_rate": 1.5088888888888888e-05,
      "loss": 0.0028,
      "step": 62840
    },
    {
      "epoch": 3.4916666666666667,
      "grad_norm": 0.025549374520778656,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.0026,
      "step": 62850
    },
    {
      "epoch": 3.4922222222222223,
      "grad_norm": 0.238979771733284,
      "learning_rate": 1.5077777777777779e-05,
      "loss": 0.002,
      "step": 62860
    },
    {
      "epoch": 3.4927777777777775,
      "grad_norm": 0.20716148614883423,
      "learning_rate": 1.5072222222222224e-05,
      "loss": 0.0018,
      "step": 62870
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.6995777487754822,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0022,
      "step": 62880
    },
    {
      "epoch": 3.493888888888889,
      "grad_norm": 0.1409987360239029,
      "learning_rate": 1.5061111111111113e-05,
      "loss": 0.0045,
      "step": 62890
    },
    {
      "epoch": 3.4944444444444445,
      "grad_norm": 0.43439915776252747,
      "learning_rate": 1.5055555555555556e-05,
      "loss": 0.0038,
      "step": 62900
    },
    {
      "epoch": 3.495,
      "grad_norm": 0.3213593363761902,
      "learning_rate": 1.505e-05,
      "loss": 0.0028,
      "step": 62910
    },
    {
      "epoch": 3.4955555555555557,
      "grad_norm": 0.23903591930866241,
      "learning_rate": 1.5044444444444445e-05,
      "loss": 0.0038,
      "step": 62920
    },
    {
      "epoch": 3.496111111111111,
      "grad_norm": 0.20741882920265198,
      "learning_rate": 1.5038888888888889e-05,
      "loss": 0.0037,
      "step": 62930
    },
    {
      "epoch": 3.4966666666666666,
      "grad_norm": 0.09006793051958084,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0025,
      "step": 62940
    },
    {
      "epoch": 3.4972222222222222,
      "grad_norm": 0.415518581867218,
      "learning_rate": 1.502777777777778e-05,
      "loss": 0.0032,
      "step": 62950
    },
    {
      "epoch": 3.497777777777778,
      "grad_norm": 0.1784912645816803,
      "learning_rate": 1.5022222222222224e-05,
      "loss": 0.0019,
      "step": 62960
    },
    {
      "epoch": 3.4983333333333335,
      "grad_norm": 0.23443546891212463,
      "learning_rate": 1.5016666666666668e-05,
      "loss": 0.0029,
      "step": 62970
    },
    {
      "epoch": 3.4988888888888887,
      "grad_norm": 0.1800815761089325,
      "learning_rate": 1.5011111111111112e-05,
      "loss": 0.0023,
      "step": 62980
    },
    {
      "epoch": 3.4994444444444444,
      "grad_norm": 0.32700860500335693,
      "learning_rate": 1.5005555555555557e-05,
      "loss": 0.0027,
      "step": 62990
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.48200610280036926,
      "learning_rate": 1.5e-05,
      "loss": 0.0028,
      "step": 63000
    },
    {
      "epoch": 3.5005555555555556,
      "grad_norm": 0.013022433035075665,
      "learning_rate": 1.4994444444444444e-05,
      "loss": 0.0013,
      "step": 63010
    },
    {
      "epoch": 3.5011111111111113,
      "grad_norm": 0.32608968019485474,
      "learning_rate": 1.498888888888889e-05,
      "loss": 0.0029,
      "step": 63020
    },
    {
      "epoch": 3.501666666666667,
      "grad_norm": 0.4162205457687378,
      "learning_rate": 1.4983333333333336e-05,
      "loss": 0.0028,
      "step": 63030
    },
    {
      "epoch": 3.502222222222222,
      "grad_norm": 0.0907018706202507,
      "learning_rate": 1.497777777777778e-05,
      "loss": 0.0027,
      "step": 63040
    },
    {
      "epoch": 3.5027777777777778,
      "grad_norm": 0.32723236083984375,
      "learning_rate": 1.4972222222222223e-05,
      "loss": 0.0026,
      "step": 63050
    },
    {
      "epoch": 3.5033333333333334,
      "grad_norm": 0.6684538722038269,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0023,
      "step": 63060
    },
    {
      "epoch": 3.503888888888889,
      "grad_norm": 0.5677409172058105,
      "learning_rate": 1.4961111111111112e-05,
      "loss": 0.0034,
      "step": 63070
    },
    {
      "epoch": 3.5044444444444443,
      "grad_norm": 0.42138445377349854,
      "learning_rate": 1.4955555555555556e-05,
      "loss": 0.0028,
      "step": 63080
    },
    {
      "epoch": 3.505,
      "grad_norm": 0.20793741941452026,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 0.0028,
      "step": 63090
    },
    {
      "epoch": 3.5055555555555555,
      "grad_norm": 0.06071651726961136,
      "learning_rate": 1.4944444444444444e-05,
      "loss": 0.0032,
      "step": 63100
    },
    {
      "epoch": 3.506111111111111,
      "grad_norm": 0.00999760627746582,
      "learning_rate": 1.4938888888888888e-05,
      "loss": 0.0032,
      "step": 63110
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.20858314633369446,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0021,
      "step": 63120
    },
    {
      "epoch": 3.5072222222222225,
      "grad_norm": 0.03504013642668724,
      "learning_rate": 1.492777777777778e-05,
      "loss": 0.0026,
      "step": 63130
    },
    {
      "epoch": 3.5077777777777777,
      "grad_norm": 0.05981734022498131,
      "learning_rate": 1.4922222222222224e-05,
      "loss": 0.003,
      "step": 63140
    },
    {
      "epoch": 3.5083333333333333,
      "grad_norm": 0.14926360547542572,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.0021,
      "step": 63150
    },
    {
      "epoch": 3.508888888888889,
      "grad_norm": 0.1957104206085205,
      "learning_rate": 1.4911111111111113e-05,
      "loss": 0.0037,
      "step": 63160
    },
    {
      "epoch": 3.5094444444444446,
      "grad_norm": 0.19959445297718048,
      "learning_rate": 1.4905555555555556e-05,
      "loss": 0.0033,
      "step": 63170
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.4321877658367157,
      "learning_rate": 1.49e-05,
      "loss": 0.0036,
      "step": 63180
    },
    {
      "epoch": 3.5105555555555554,
      "grad_norm": 0.20905616879463196,
      "learning_rate": 1.4894444444444445e-05,
      "loss": 0.0022,
      "step": 63190
    },
    {
      "epoch": 3.511111111111111,
      "grad_norm": 0.5872277617454529,
      "learning_rate": 1.4888888888888888e-05,
      "loss": 0.0029,
      "step": 63200
    },
    {
      "epoch": 3.5116666666666667,
      "grad_norm": 0.03984164819121361,
      "learning_rate": 1.4883333333333335e-05,
      "loss": 0.0028,
      "step": 63210
    },
    {
      "epoch": 3.5122222222222224,
      "grad_norm": 0.05958506464958191,
      "learning_rate": 1.4877777777777779e-05,
      "loss": 0.0026,
      "step": 63220
    },
    {
      "epoch": 3.512777777777778,
      "grad_norm": 0.4968353807926178,
      "learning_rate": 1.4872222222222224e-05,
      "loss": 0.0035,
      "step": 63230
    },
    {
      "epoch": 3.513333333333333,
      "grad_norm": 0.14847132563591003,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0023,
      "step": 63240
    },
    {
      "epoch": 3.513888888888889,
      "grad_norm": 0.6127475500106812,
      "learning_rate": 1.4861111111111111e-05,
      "loss": 0.0045,
      "step": 63250
    },
    {
      "epoch": 3.5144444444444445,
      "grad_norm": 0.1483679562807083,
      "learning_rate": 1.4855555555555557e-05,
      "loss": 0.0021,
      "step": 63260
    },
    {
      "epoch": 3.515,
      "grad_norm": 0.32649463415145874,
      "learning_rate": 1.485e-05,
      "loss": 0.0026,
      "step": 63270
    },
    {
      "epoch": 3.5155555555555553,
      "grad_norm": 0.34094542264938354,
      "learning_rate": 1.4844444444444444e-05,
      "loss": 0.0033,
      "step": 63280
    },
    {
      "epoch": 3.516111111111111,
      "grad_norm": 0.1953965574502945,
      "learning_rate": 1.4838888888888889e-05,
      "loss": 0.0029,
      "step": 63290
    },
    {
      "epoch": 3.5166666666666666,
      "grad_norm": 0.09067943692207336,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0025,
      "step": 63300
    },
    {
      "epoch": 3.5172222222222222,
      "grad_norm": 0.4450116753578186,
      "learning_rate": 1.482777777777778e-05,
      "loss": 0.0024,
      "step": 63310
    },
    {
      "epoch": 3.517777777777778,
      "grad_norm": 0.4797482490539551,
      "learning_rate": 1.4822222222222223e-05,
      "loss": 0.0033,
      "step": 63320
    },
    {
      "epoch": 3.5183333333333335,
      "grad_norm": 0.28068050742149353,
      "learning_rate": 1.4816666666666668e-05,
      "loss": 0.0028,
      "step": 63330
    },
    {
      "epoch": 3.5188888888888887,
      "grad_norm": 0.09193423390388489,
      "learning_rate": 1.4811111111111112e-05,
      "loss": 0.0024,
      "step": 63340
    },
    {
      "epoch": 3.5194444444444444,
      "grad_norm": 0.2566596567630768,
      "learning_rate": 1.4805555555555555e-05,
      "loss": 0.0034,
      "step": 63350
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.17784184217453003,
      "learning_rate": 1.48e-05,
      "loss": 0.0029,
      "step": 63360
    },
    {
      "epoch": 3.5205555555555557,
      "grad_norm": 0.11976416409015656,
      "learning_rate": 1.4794444444444444e-05,
      "loss": 0.002,
      "step": 63370
    },
    {
      "epoch": 3.521111111111111,
      "grad_norm": 0.1505080759525299,
      "learning_rate": 1.4788888888888888e-05,
      "loss": 0.0022,
      "step": 63380
    },
    {
      "epoch": 3.5216666666666665,
      "grad_norm": 0.09404567629098892,
      "learning_rate": 1.4783333333333335e-05,
      "loss": 0.0021,
      "step": 63390
    },
    {
      "epoch": 3.522222222222222,
      "grad_norm": 0.09022191166877747,
      "learning_rate": 1.477777777777778e-05,
      "loss": 0.0034,
      "step": 63400
    },
    {
      "epoch": 3.522777777777778,
      "grad_norm": 0.20756970345973969,
      "learning_rate": 1.4772222222222223e-05,
      "loss": 0.003,
      "step": 63410
    },
    {
      "epoch": 3.5233333333333334,
      "grad_norm": 0.3850499391555786,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.0029,
      "step": 63420
    },
    {
      "epoch": 3.523888888888889,
      "grad_norm": 0.060491226613521576,
      "learning_rate": 1.4761111111111112e-05,
      "loss": 0.0017,
      "step": 63430
    },
    {
      "epoch": 3.5244444444444447,
      "grad_norm": 0.1776830106973648,
      "learning_rate": 1.4755555555555556e-05,
      "loss": 0.0022,
      "step": 63440
    },
    {
      "epoch": 3.525,
      "grad_norm": 0.4243171513080597,
      "learning_rate": 1.475e-05,
      "loss": 0.0031,
      "step": 63450
    },
    {
      "epoch": 3.5255555555555556,
      "grad_norm": 0.46228551864624023,
      "learning_rate": 1.4744444444444445e-05,
      "loss": 0.0028,
      "step": 63460
    },
    {
      "epoch": 3.526111111111111,
      "grad_norm": 0.14892448484897614,
      "learning_rate": 1.4738888888888892e-05,
      "loss": 0.0032,
      "step": 63470
    },
    {
      "epoch": 3.5266666666666664,
      "grad_norm": 0.5053012371063232,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0023,
      "step": 63480
    },
    {
      "epoch": 3.527222222222222,
      "grad_norm": 0.23759321868419647,
      "learning_rate": 1.4727777777777779e-05,
      "loss": 0.0036,
      "step": 63490
    },
    {
      "epoch": 3.5277777777777777,
      "grad_norm": 0.03368557244539261,
      "learning_rate": 1.4722222222222224e-05,
      "loss": 0.0023,
      "step": 63500
    },
    {
      "epoch": 3.5283333333333333,
      "grad_norm": 0.6250539422035217,
      "learning_rate": 1.4716666666666668e-05,
      "loss": 0.0034,
      "step": 63510
    },
    {
      "epoch": 3.528888888888889,
      "grad_norm": 0.5147383809089661,
      "learning_rate": 1.4711111111111111e-05,
      "loss": 0.0036,
      "step": 63520
    },
    {
      "epoch": 3.5294444444444446,
      "grad_norm": 0.5480532646179199,
      "learning_rate": 1.4705555555555556e-05,
      "loss": 0.0033,
      "step": 63530
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 0.2074344903230667,
      "learning_rate": 1.47e-05,
      "loss": 0.0034,
      "step": 63540
    },
    {
      "epoch": 3.5305555555555554,
      "grad_norm": 0.37667346000671387,
      "learning_rate": 1.4694444444444443e-05,
      "loss": 0.0038,
      "step": 63550
    },
    {
      "epoch": 3.531111111111111,
      "grad_norm": 0.26744964718818665,
      "learning_rate": 1.468888888888889e-05,
      "loss": 0.003,
      "step": 63560
    },
    {
      "epoch": 3.5316666666666667,
      "grad_norm": 0.18110571801662445,
      "learning_rate": 1.4683333333333336e-05,
      "loss": 0.0024,
      "step": 63570
    },
    {
      "epoch": 3.5322222222222224,
      "grad_norm": 0.8901863098144531,
      "learning_rate": 1.467777777777778e-05,
      "loss": 0.0041,
      "step": 63580
    },
    {
      "epoch": 3.5327777777777776,
      "grad_norm": 0.013437081128358841,
      "learning_rate": 1.4672222222222223e-05,
      "loss": 0.0025,
      "step": 63590
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.3846866488456726,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0024,
      "step": 63600
    },
    {
      "epoch": 3.533888888888889,
      "grad_norm": 0.07439006119966507,
      "learning_rate": 1.4661111111111112e-05,
      "loss": 0.0029,
      "step": 63610
    },
    {
      "epoch": 3.5344444444444445,
      "grad_norm": 0.1715107262134552,
      "learning_rate": 1.4655555555555555e-05,
      "loss": 0.0032,
      "step": 63620
    },
    {
      "epoch": 3.535,
      "grad_norm": 0.3015977144241333,
      "learning_rate": 1.465e-05,
      "loss": 0.0019,
      "step": 63630
    },
    {
      "epoch": 3.535555555555556,
      "grad_norm": 0.03516194596886635,
      "learning_rate": 1.4644444444444444e-05,
      "loss": 0.0029,
      "step": 63640
    },
    {
      "epoch": 3.536111111111111,
      "grad_norm": 0.14879965782165527,
      "learning_rate": 1.463888888888889e-05,
      "loss": 0.0017,
      "step": 63650
    },
    {
      "epoch": 3.5366666666666666,
      "grad_norm": 0.1489248275756836,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0032,
      "step": 63660
    },
    {
      "epoch": 3.5372222222222223,
      "grad_norm": 0.09096281230449677,
      "learning_rate": 1.462777777777778e-05,
      "loss": 0.0031,
      "step": 63670
    },
    {
      "epoch": 3.537777777777778,
      "grad_norm": 0.11862849444150925,
      "learning_rate": 1.4622222222222223e-05,
      "loss": 0.0028,
      "step": 63680
    },
    {
      "epoch": 3.538333333333333,
      "grad_norm": 0.13140590488910675,
      "learning_rate": 1.4616666666666667e-05,
      "loss": 0.0033,
      "step": 63690
    },
    {
      "epoch": 3.5388888888888888,
      "grad_norm": 0.03135497868061066,
      "learning_rate": 1.4611111111111112e-05,
      "loss": 0.0033,
      "step": 63700
    },
    {
      "epoch": 3.5394444444444444,
      "grad_norm": 0.06273096054792404,
      "learning_rate": 1.4605555555555556e-05,
      "loss": 0.0033,
      "step": 63710
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.32748720049858093,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0023,
      "step": 63720
    },
    {
      "epoch": 3.5405555555555557,
      "grad_norm": 0.034633468836545944,
      "learning_rate": 1.4594444444444444e-05,
      "loss": 0.0027,
      "step": 63730
    },
    {
      "epoch": 3.5411111111111113,
      "grad_norm": 0.11892329156398773,
      "learning_rate": 1.4588888888888891e-05,
      "loss": 0.0028,
      "step": 63740
    },
    {
      "epoch": 3.5416666666666665,
      "grad_norm": 0.20696291327476501,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.0042,
      "step": 63750
    },
    {
      "epoch": 3.542222222222222,
      "grad_norm": 0.23710650205612183,
      "learning_rate": 1.4577777777777778e-05,
      "loss": 0.0039,
      "step": 63760
    },
    {
      "epoch": 3.542777777777778,
      "grad_norm": 0.15585534274578094,
      "learning_rate": 1.4572222222222224e-05,
      "loss": 0.0027,
      "step": 63770
    },
    {
      "epoch": 3.5433333333333334,
      "grad_norm": 0.12377650290727615,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0042,
      "step": 63780
    },
    {
      "epoch": 3.5438888888888886,
      "grad_norm": 0.15141785144805908,
      "learning_rate": 1.456111111111111e-05,
      "loss": 0.0036,
      "step": 63790
    },
    {
      "epoch": 3.5444444444444443,
      "grad_norm": 0.23768074810504913,
      "learning_rate": 1.4555555555555556e-05,
      "loss": 0.0032,
      "step": 63800
    },
    {
      "epoch": 3.545,
      "grad_norm": 0.20850326120853424,
      "learning_rate": 1.455e-05,
      "loss": 0.0035,
      "step": 63810
    },
    {
      "epoch": 3.5455555555555556,
      "grad_norm": 0.2121955156326294,
      "learning_rate": 1.4544444444444443e-05,
      "loss": 0.0025,
      "step": 63820
    },
    {
      "epoch": 3.546111111111111,
      "grad_norm": 0.03969713672995567,
      "learning_rate": 1.453888888888889e-05,
      "loss": 0.0043,
      "step": 63830
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.12048047035932541,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0031,
      "step": 63840
    },
    {
      "epoch": 3.5472222222222225,
      "grad_norm": 0.26720261573791504,
      "learning_rate": 1.4527777777777779e-05,
      "loss": 0.0036,
      "step": 63850
    },
    {
      "epoch": 3.5477777777777777,
      "grad_norm": 0.23812200129032135,
      "learning_rate": 1.4522222222222222e-05,
      "loss": 0.0031,
      "step": 63860
    },
    {
      "epoch": 3.5483333333333333,
      "grad_norm": 0.2370648831129074,
      "learning_rate": 1.4516666666666668e-05,
      "loss": 0.0031,
      "step": 63870
    },
    {
      "epoch": 3.548888888888889,
      "grad_norm": 0.2404840737581253,
      "learning_rate": 1.4511111111111111e-05,
      "loss": 0.0027,
      "step": 63880
    },
    {
      "epoch": 3.549444444444444,
      "grad_norm": 0.32843518257141113,
      "learning_rate": 1.4505555555555555e-05,
      "loss": 0.0033,
      "step": 63890
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.03136751800775528,
      "learning_rate": 1.45e-05,
      "loss": 0.0028,
      "step": 63900
    },
    {
      "epoch": 3.5505555555555555,
      "grad_norm": 0.0895821750164032,
      "learning_rate": 1.4494444444444444e-05,
      "loss": 0.0023,
      "step": 63910
    },
    {
      "epoch": 3.551111111111111,
      "grad_norm": 0.1824234426021576,
      "learning_rate": 1.448888888888889e-05,
      "loss": 0.0032,
      "step": 63920
    },
    {
      "epoch": 3.5516666666666667,
      "grad_norm": 0.08923462778329849,
      "learning_rate": 1.4483333333333334e-05,
      "loss": 0.0032,
      "step": 63930
    },
    {
      "epoch": 3.5522222222222224,
      "grad_norm": 0.32679659128189087,
      "learning_rate": 1.447777777777778e-05,
      "loss": 0.0023,
      "step": 63940
    },
    {
      "epoch": 3.552777777777778,
      "grad_norm": 0.03135588392615318,
      "learning_rate": 1.4472222222222223e-05,
      "loss": 0.0024,
      "step": 63950
    },
    {
      "epoch": 3.5533333333333332,
      "grad_norm": 0.06069949269294739,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.003,
      "step": 63960
    },
    {
      "epoch": 3.553888888888889,
      "grad_norm": 0.2373885214328766,
      "learning_rate": 1.4461111111111112e-05,
      "loss": 0.0034,
      "step": 63970
    },
    {
      "epoch": 3.5544444444444445,
      "grad_norm": 0.05899791046977043,
      "learning_rate": 1.4455555555555555e-05,
      "loss": 0.0026,
      "step": 63980
    },
    {
      "epoch": 3.555,
      "grad_norm": 0.059816040098667145,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 0.0024,
      "step": 63990
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.0316329225897789,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 0.0031,
      "step": 64000
    },
    {
      "epoch": 3.556111111111111,
      "grad_norm": 0.07203807681798935,
      "learning_rate": 1.4438888888888891e-05,
      "loss": 0.0027,
      "step": 64010
    },
    {
      "epoch": 3.5566666666666666,
      "grad_norm": 0.2667682468891144,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0028,
      "step": 64020
    },
    {
      "epoch": 3.5572222222222223,
      "grad_norm": 0.8947023153305054,
      "learning_rate": 1.4427777777777778e-05,
      "loss": 0.003,
      "step": 64030
    },
    {
      "epoch": 3.557777777777778,
      "grad_norm": 0.23804841935634613,
      "learning_rate": 1.4422222222222223e-05,
      "loss": 0.0033,
      "step": 64040
    },
    {
      "epoch": 3.5583333333333336,
      "grad_norm": 0.26702383160591125,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.0034,
      "step": 64050
    },
    {
      "epoch": 3.5588888888888888,
      "grad_norm": 0.1484181135892868,
      "learning_rate": 1.441111111111111e-05,
      "loss": 0.0024,
      "step": 64060
    },
    {
      "epoch": 3.5594444444444444,
      "grad_norm": 0.2921614348888397,
      "learning_rate": 1.4405555555555556e-05,
      "loss": 0.003,
      "step": 64070
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.20770668983459473,
      "learning_rate": 1.44e-05,
      "loss": 0.0022,
      "step": 64080
    },
    {
      "epoch": 3.5605555555555557,
      "grad_norm": 0.08913569152355194,
      "learning_rate": 1.4394444444444446e-05,
      "loss": 0.0029,
      "step": 64090
    },
    {
      "epoch": 3.561111111111111,
      "grad_norm": 0.26673561334609985,
      "learning_rate": 1.438888888888889e-05,
      "loss": 0.0017,
      "step": 64100
    },
    {
      "epoch": 3.5616666666666665,
      "grad_norm": 0.08984804898500443,
      "learning_rate": 1.4383333333333335e-05,
      "loss": 0.0017,
      "step": 64110
    },
    {
      "epoch": 3.562222222222222,
      "grad_norm": 0.3858433961868286,
      "learning_rate": 1.4377777777777779e-05,
      "loss": 0.0032,
      "step": 64120
    },
    {
      "epoch": 3.562777777777778,
      "grad_norm": 0.4166783094406128,
      "learning_rate": 1.4372222222222222e-05,
      "loss": 0.0025,
      "step": 64130
    },
    {
      "epoch": 3.5633333333333335,
      "grad_norm": 0.5343419909477234,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0029,
      "step": 64140
    },
    {
      "epoch": 3.563888888888889,
      "grad_norm": 0.14880158007144928,
      "learning_rate": 1.4361111111111111e-05,
      "loss": 0.0029,
      "step": 64150
    },
    {
      "epoch": 3.5644444444444443,
      "grad_norm": 0.35554948449134827,
      "learning_rate": 1.4355555555555556e-05,
      "loss": 0.0031,
      "step": 64160
    },
    {
      "epoch": 3.565,
      "grad_norm": 0.6890431046485901,
      "learning_rate": 1.435e-05,
      "loss": 0.0035,
      "step": 64170
    },
    {
      "epoch": 3.5655555555555556,
      "grad_norm": 0.11898305267095566,
      "learning_rate": 1.4344444444444447e-05,
      "loss": 0.0032,
      "step": 64180
    },
    {
      "epoch": 3.5661111111111112,
      "grad_norm": 0.23742902278900146,
      "learning_rate": 1.433888888888889e-05,
      "loss": 0.0025,
      "step": 64190
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.031238967552781105,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0027,
      "step": 64200
    },
    {
      "epoch": 3.567222222222222,
      "grad_norm": 0.1816708892583847,
      "learning_rate": 1.4327777777777779e-05,
      "loss": 0.003,
      "step": 64210
    },
    {
      "epoch": 3.5677777777777777,
      "grad_norm": 0.32672563195228577,
      "learning_rate": 1.4322222222222223e-05,
      "loss": 0.0028,
      "step": 64220
    },
    {
      "epoch": 3.5683333333333334,
      "grad_norm": 0.1623990684747696,
      "learning_rate": 1.4316666666666668e-05,
      "loss": 0.0041,
      "step": 64230
    },
    {
      "epoch": 3.568888888888889,
      "grad_norm": 0.08967309445142746,
      "learning_rate": 1.4311111111111111e-05,
      "loss": 0.0023,
      "step": 64240
    },
    {
      "epoch": 3.5694444444444446,
      "grad_norm": 0.09028054773807526,
      "learning_rate": 1.4305555555555555e-05,
      "loss": 0.0025,
      "step": 64250
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.05958832427859306,
      "learning_rate": 1.43e-05,
      "loss": 0.002,
      "step": 64260
    },
    {
      "epoch": 3.5705555555555555,
      "grad_norm": 0.29658856987953186,
      "learning_rate": 1.4294444444444447e-05,
      "loss": 0.0029,
      "step": 64270
    },
    {
      "epoch": 3.571111111111111,
      "grad_norm": 0.29654064774513245,
      "learning_rate": 1.428888888888889e-05,
      "loss": 0.002,
      "step": 64280
    },
    {
      "epoch": 3.5716666666666668,
      "grad_norm": 0.33340802788734436,
      "learning_rate": 1.4283333333333334e-05,
      "loss": 0.0038,
      "step": 64290
    },
    {
      "epoch": 3.572222222222222,
      "grad_norm": 0.26680463552474976,
      "learning_rate": 1.427777777777778e-05,
      "loss": 0.0019,
      "step": 64300
    },
    {
      "epoch": 3.5727777777777776,
      "grad_norm": 0.29669472575187683,
      "learning_rate": 1.4272222222222223e-05,
      "loss": 0.0037,
      "step": 64310
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.3151717483997345,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0039,
      "step": 64320
    },
    {
      "epoch": 3.573888888888889,
      "grad_norm": 0.2083364874124527,
      "learning_rate": 1.4261111111111112e-05,
      "loss": 0.0022,
      "step": 64330
    },
    {
      "epoch": 3.5744444444444445,
      "grad_norm": 0.12072117626667023,
      "learning_rate": 1.4255555555555556e-05,
      "loss": 0.0029,
      "step": 64340
    },
    {
      "epoch": 3.575,
      "grad_norm": 0.1780122071504593,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.0023,
      "step": 64350
    },
    {
      "epoch": 3.575555555555556,
      "grad_norm": 0.17923904955387115,
      "learning_rate": 1.4244444444444446e-05,
      "loss": 0.0029,
      "step": 64360
    },
    {
      "epoch": 3.576111111111111,
      "grad_norm": 0.06298831105232239,
      "learning_rate": 1.4238888888888891e-05,
      "loss": 0.0018,
      "step": 64370
    },
    {
      "epoch": 3.5766666666666667,
      "grad_norm": 0.06111131235957146,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.0034,
      "step": 64380
    },
    {
      "epoch": 3.5772222222222223,
      "grad_norm": 0.060115862637758255,
      "learning_rate": 1.4227777777777778e-05,
      "loss": 0.0027,
      "step": 64390
    },
    {
      "epoch": 3.5777777777777775,
      "grad_norm": 0.3610486686229706,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 0.003,
      "step": 64400
    },
    {
      "epoch": 3.578333333333333,
      "grad_norm": 0.28959235548973083,
      "learning_rate": 1.4216666666666667e-05,
      "loss": 0.0022,
      "step": 64410
    },
    {
      "epoch": 3.578888888888889,
      "grad_norm": 0.1501898318529129,
      "learning_rate": 1.421111111111111e-05,
      "loss": 0.0029,
      "step": 64420
    },
    {
      "epoch": 3.5794444444444444,
      "grad_norm": 0.11930464208126068,
      "learning_rate": 1.4205555555555556e-05,
      "loss": 0.0016,
      "step": 64430
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.03182060271501541,
      "learning_rate": 1.42e-05,
      "loss": 0.0028,
      "step": 64440
    },
    {
      "epoch": 3.5805555555555557,
      "grad_norm": 0.5931219458580017,
      "learning_rate": 1.4194444444444447e-05,
      "loss": 0.0023,
      "step": 64450
    },
    {
      "epoch": 3.5811111111111114,
      "grad_norm": 0.09612865000963211,
      "learning_rate": 1.418888888888889e-05,
      "loss": 0.0028,
      "step": 64460
    },
    {
      "epoch": 3.5816666666666666,
      "grad_norm": 0.06007816642522812,
      "learning_rate": 1.4183333333333335e-05,
      "loss": 0.0039,
      "step": 64470
    },
    {
      "epoch": 3.582222222222222,
      "grad_norm": 0.4219547212123871,
      "learning_rate": 1.4177777777777779e-05,
      "loss": 0.0031,
      "step": 64480
    },
    {
      "epoch": 3.582777777777778,
      "grad_norm": 0.41466209292411804,
      "learning_rate": 1.4172222222222222e-05,
      "loss": 0.0033,
      "step": 64490
    },
    {
      "epoch": 3.5833333333333335,
      "grad_norm": 0.03712736815214157,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0035,
      "step": 64500
    },
    {
      "epoch": 3.5838888888888887,
      "grad_norm": 0.01410636492073536,
      "learning_rate": 1.4161111111111111e-05,
      "loss": 0.0026,
      "step": 64510
    },
    {
      "epoch": 3.5844444444444443,
      "grad_norm": 0.11950092762708664,
      "learning_rate": 1.4155555555555555e-05,
      "loss": 0.0031,
      "step": 64520
    },
    {
      "epoch": 3.585,
      "grad_norm": 0.23724070191383362,
      "learning_rate": 1.415e-05,
      "loss": 0.003,
      "step": 64530
    },
    {
      "epoch": 3.5855555555555556,
      "grad_norm": 0.20792165398597717,
      "learning_rate": 1.4144444444444447e-05,
      "loss": 0.0024,
      "step": 64540
    },
    {
      "epoch": 3.5861111111111112,
      "grad_norm": 0.3563792109489441,
      "learning_rate": 1.413888888888889e-05,
      "loss": 0.0023,
      "step": 64550
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.2963106334209442,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0035,
      "step": 64560
    },
    {
      "epoch": 3.587222222222222,
      "grad_norm": 0.11847969144582748,
      "learning_rate": 1.412777777777778e-05,
      "loss": 0.0037,
      "step": 64570
    },
    {
      "epoch": 3.5877777777777777,
      "grad_norm": 0.014577064663171768,
      "learning_rate": 1.4122222222222223e-05,
      "loss": 0.0032,
      "step": 64580
    },
    {
      "epoch": 3.5883333333333334,
      "grad_norm": 0.12917941808700562,
      "learning_rate": 1.4116666666666666e-05,
      "loss": 0.0028,
      "step": 64590
    },
    {
      "epoch": 3.588888888888889,
      "grad_norm": 0.2967187464237213,
      "learning_rate": 1.4111111111111112e-05,
      "loss": 0.0034,
      "step": 64600
    },
    {
      "epoch": 3.589444444444444,
      "grad_norm": 0.38617733120918274,
      "learning_rate": 1.4105555555555555e-05,
      "loss": 0.0029,
      "step": 64610
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.030910639092326164,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0029,
      "step": 64620
    },
    {
      "epoch": 3.5905555555555555,
      "grad_norm": 0.15473224222660065,
      "learning_rate": 1.4094444444444446e-05,
      "loss": 0.0039,
      "step": 64630
    },
    {
      "epoch": 3.591111111111111,
      "grad_norm": 0.18907931447029114,
      "learning_rate": 1.4088888888888891e-05,
      "loss": 0.0031,
      "step": 64640
    },
    {
      "epoch": 3.591666666666667,
      "grad_norm": 0.37533098459243774,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.0024,
      "step": 64650
    },
    {
      "epoch": 3.5922222222222224,
      "grad_norm": 0.14969982206821442,
      "learning_rate": 1.4077777777777778e-05,
      "loss": 0.0035,
      "step": 64660
    },
    {
      "epoch": 3.5927777777777776,
      "grad_norm": 0.3013278841972351,
      "learning_rate": 1.4072222222222223e-05,
      "loss": 0.0036,
      "step": 64670
    },
    {
      "epoch": 3.5933333333333333,
      "grad_norm": 0.47443556785583496,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0033,
      "step": 64680
    },
    {
      "epoch": 3.593888888888889,
      "grad_norm": 0.17820653319358826,
      "learning_rate": 1.406111111111111e-05,
      "loss": 0.0033,
      "step": 64690
    },
    {
      "epoch": 3.5944444444444446,
      "grad_norm": 0.47538021206855774,
      "learning_rate": 1.4055555555555556e-05,
      "loss": 0.0028,
      "step": 64700
    },
    {
      "epoch": 3.5949999999999998,
      "grad_norm": 0.06028023734688759,
      "learning_rate": 1.4050000000000003e-05,
      "loss": 0.0024,
      "step": 64710
    },
    {
      "epoch": 3.5955555555555554,
      "grad_norm": 0.01644250378012657,
      "learning_rate": 1.4044444444444446e-05,
      "loss": 0.0027,
      "step": 64720
    },
    {
      "epoch": 3.596111111111111,
      "grad_norm": 0.2376110851764679,
      "learning_rate": 1.403888888888889e-05,
      "loss": 0.0043,
      "step": 64730
    },
    {
      "epoch": 3.5966666666666667,
      "grad_norm": 0.24362145364284515,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0036,
      "step": 64740
    },
    {
      "epoch": 3.5972222222222223,
      "grad_norm": 0.5040582418441772,
      "learning_rate": 1.4027777777777779e-05,
      "loss": 0.0032,
      "step": 64750
    },
    {
      "epoch": 3.597777777777778,
      "grad_norm": 0.12231452763080597,
      "learning_rate": 1.4022222222222222e-05,
      "loss": 0.0032,
      "step": 64760
    },
    {
      "epoch": 3.5983333333333336,
      "grad_norm": 0.3462027609348297,
      "learning_rate": 1.4016666666666667e-05,
      "loss": 0.0043,
      "step": 64770
    },
    {
      "epoch": 3.598888888888889,
      "grad_norm": 0.3270050585269928,
      "learning_rate": 1.4011111111111111e-05,
      "loss": 0.0028,
      "step": 64780
    },
    {
      "epoch": 3.5994444444444444,
      "grad_norm": 0.2746224105358124,
      "learning_rate": 1.4005555555555555e-05,
      "loss": 0.0035,
      "step": 64790
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.2664070427417755,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0027,
      "step": 64800
    },
    {
      "epoch": 3.6005555555555553,
      "grad_norm": 0.1887114942073822,
      "learning_rate": 1.3994444444444447e-05,
      "loss": 0.0031,
      "step": 64810
    },
    {
      "epoch": 3.601111111111111,
      "grad_norm": 0.4154292047023773,
      "learning_rate": 1.398888888888889e-05,
      "loss": 0.002,
      "step": 64820
    },
    {
      "epoch": 3.6016666666666666,
      "grad_norm": 0.06054120510816574,
      "learning_rate": 1.3983333333333334e-05,
      "loss": 0.0029,
      "step": 64830
    },
    {
      "epoch": 3.602222222222222,
      "grad_norm": 0.32607632875442505,
      "learning_rate": 1.3977777777777779e-05,
      "loss": 0.0028,
      "step": 64840
    },
    {
      "epoch": 3.602777777777778,
      "grad_norm": 0.20914028584957123,
      "learning_rate": 1.3972222222222223e-05,
      "loss": 0.0029,
      "step": 64850
    },
    {
      "epoch": 3.6033333333333335,
      "grad_norm": 0.04027383774518967,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.0036,
      "step": 64860
    },
    {
      "epoch": 3.603888888888889,
      "grad_norm": 0.17858926951885223,
      "learning_rate": 1.3961111111111111e-05,
      "loss": 0.0031,
      "step": 64870
    },
    {
      "epoch": 3.6044444444444443,
      "grad_norm": 0.23819279670715332,
      "learning_rate": 1.3955555555555555e-05,
      "loss": 0.003,
      "step": 64880
    },
    {
      "epoch": 3.605,
      "grad_norm": 0.06059698387980461,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 0.0027,
      "step": 64890
    },
    {
      "epoch": 3.6055555555555556,
      "grad_norm": 0.6084876656532288,
      "learning_rate": 1.3944444444444446e-05,
      "loss": 0.0034,
      "step": 64900
    },
    {
      "epoch": 3.6061111111111113,
      "grad_norm": 0.03365505859255791,
      "learning_rate": 1.393888888888889e-05,
      "loss": 0.0028,
      "step": 64910
    },
    {
      "epoch": 3.6066666666666665,
      "grad_norm": 0.4594539701938629,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0039,
      "step": 64920
    },
    {
      "epoch": 3.607222222222222,
      "grad_norm": 0.31105536222457886,
      "learning_rate": 1.3927777777777778e-05,
      "loss": 0.0023,
      "step": 64930
    },
    {
      "epoch": 3.6077777777777778,
      "grad_norm": 0.3274960517883301,
      "learning_rate": 1.3922222222222223e-05,
      "loss": 0.0033,
      "step": 64940
    },
    {
      "epoch": 3.6083333333333334,
      "grad_norm": 0.032934293150901794,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.004,
      "step": 64950
    },
    {
      "epoch": 3.608888888888889,
      "grad_norm": 0.5020179152488708,
      "learning_rate": 1.391111111111111e-05,
      "loss": 0.0022,
      "step": 64960
    },
    {
      "epoch": 3.6094444444444447,
      "grad_norm": 0.32619431614875793,
      "learning_rate": 1.3905555555555555e-05,
      "loss": 0.0024,
      "step": 64970
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.17972439527511597,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0027,
      "step": 64980
    },
    {
      "epoch": 3.6105555555555555,
      "grad_norm": 0.17834703624248505,
      "learning_rate": 1.3894444444444446e-05,
      "loss": 0.0029,
      "step": 64990
    },
    {
      "epoch": 3.611111111111111,
      "grad_norm": 0.32622772455215454,
      "learning_rate": 1.388888888888889e-05,
      "loss": 0.0021,
      "step": 65000
    },
    {
      "epoch": 3.611666666666667,
      "grad_norm": 0.20830631256103516,
      "learning_rate": 1.3883333333333335e-05,
      "loss": 0.0033,
      "step": 65010
    },
    {
      "epoch": 3.612222222222222,
      "grad_norm": 0.24738600850105286,
      "learning_rate": 1.3877777777777778e-05,
      "loss": 0.0027,
      "step": 65020
    },
    {
      "epoch": 3.6127777777777776,
      "grad_norm": 0.14994360506534576,
      "learning_rate": 1.3872222222222222e-05,
      "loss": 0.003,
      "step": 65030
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.09076591581106186,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0027,
      "step": 65040
    },
    {
      "epoch": 3.613888888888889,
      "grad_norm": 0.1491534262895584,
      "learning_rate": 1.386111111111111e-05,
      "loss": 0.0022,
      "step": 65050
    },
    {
      "epoch": 3.6144444444444446,
      "grad_norm": 0.03180687874555588,
      "learning_rate": 1.3855555555555554e-05,
      "loss": 0.0019,
      "step": 65060
    },
    {
      "epoch": 3.615,
      "grad_norm": 0.23735179007053375,
      "learning_rate": 1.3850000000000001e-05,
      "loss": 0.0027,
      "step": 65070
    },
    {
      "epoch": 3.6155555555555554,
      "grad_norm": 0.18058398365974426,
      "learning_rate": 1.3844444444444446e-05,
      "loss": 0.0035,
      "step": 65080
    },
    {
      "epoch": 3.616111111111111,
      "grad_norm": 0.19970844686031342,
      "learning_rate": 1.383888888888889e-05,
      "loss": 0.0022,
      "step": 65090
    },
    {
      "epoch": 3.6166666666666667,
      "grad_norm": 0.3719927966594696,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.0034,
      "step": 65100
    },
    {
      "epoch": 3.6172222222222223,
      "grad_norm": 0.4837452173233032,
      "learning_rate": 1.3827777777777779e-05,
      "loss": 0.0028,
      "step": 65110
    },
    {
      "epoch": 3.6177777777777775,
      "grad_norm": 0.1838233470916748,
      "learning_rate": 1.3822222222222222e-05,
      "loss": 0.0025,
      "step": 65120
    },
    {
      "epoch": 3.618333333333333,
      "grad_norm": 0.2103978395462036,
      "learning_rate": 1.3816666666666666e-05,
      "loss": 0.004,
      "step": 65130
    },
    {
      "epoch": 3.618888888888889,
      "grad_norm": 0.2679402828216553,
      "learning_rate": 1.3811111111111111e-05,
      "loss": 0.0035,
      "step": 65140
    },
    {
      "epoch": 3.6194444444444445,
      "grad_norm": 0.017360473051667213,
      "learning_rate": 1.3805555555555555e-05,
      "loss": 0.002,
      "step": 65150
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.19528116285800934,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0022,
      "step": 65160
    },
    {
      "epoch": 3.6205555555555557,
      "grad_norm": 0.08998292684555054,
      "learning_rate": 1.3794444444444445e-05,
      "loss": 0.0023,
      "step": 65170
    },
    {
      "epoch": 3.621111111111111,
      "grad_norm": 0.0905938372015953,
      "learning_rate": 1.378888888888889e-05,
      "loss": 0.0029,
      "step": 65180
    },
    {
      "epoch": 3.6216666666666666,
      "grad_norm": 0.26735004782676697,
      "learning_rate": 1.3783333333333334e-05,
      "loss": 0.0034,
      "step": 65190
    },
    {
      "epoch": 3.6222222222222222,
      "grad_norm": 0.119268037378788,
      "learning_rate": 1.3777777777777778e-05,
      "loss": 0.0032,
      "step": 65200
    },
    {
      "epoch": 3.622777777777778,
      "grad_norm": 0.13868114352226257,
      "learning_rate": 1.3772222222222223e-05,
      "loss": 0.0027,
      "step": 65210
    },
    {
      "epoch": 3.623333333333333,
      "grad_norm": 0.03162168338894844,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.003,
      "step": 65220
    },
    {
      "epoch": 3.6238888888888887,
      "grad_norm": 0.1193954274058342,
      "learning_rate": 1.376111111111111e-05,
      "loss": 0.0029,
      "step": 65230
    },
    {
      "epoch": 3.6244444444444444,
      "grad_norm": 0.12024775892496109,
      "learning_rate": 1.3755555555555555e-05,
      "loss": 0.0036,
      "step": 65240
    },
    {
      "epoch": 3.625,
      "grad_norm": 0.28184881806373596,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.0033,
      "step": 65250
    },
    {
      "epoch": 3.6255555555555556,
      "grad_norm": 0.013790635392069817,
      "learning_rate": 1.3744444444444446e-05,
      "loss": 0.0026,
      "step": 65260
    },
    {
      "epoch": 3.6261111111111113,
      "grad_norm": 0.013164803385734558,
      "learning_rate": 1.373888888888889e-05,
      "loss": 0.0028,
      "step": 65270
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.5038226246833801,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0029,
      "step": 65280
    },
    {
      "epoch": 3.627222222222222,
      "grad_norm": 0.14937561750411987,
      "learning_rate": 1.3727777777777778e-05,
      "loss": 0.0051,
      "step": 65290
    },
    {
      "epoch": 3.6277777777777778,
      "grad_norm": 0.23719719052314758,
      "learning_rate": 1.3722222222222222e-05,
      "loss": 0.0036,
      "step": 65300
    },
    {
      "epoch": 3.6283333333333334,
      "grad_norm": 0.09025207906961441,
      "learning_rate": 1.3716666666666667e-05,
      "loss": 0.0026,
      "step": 65310
    },
    {
      "epoch": 3.628888888888889,
      "grad_norm": 0.2405070811510086,
      "learning_rate": 1.371111111111111e-05,
      "loss": 0.0031,
      "step": 65320
    },
    {
      "epoch": 3.6294444444444443,
      "grad_norm": 0.3394518196582794,
      "learning_rate": 1.3705555555555557e-05,
      "loss": 0.0023,
      "step": 65330
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.1841665506362915,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.0032,
      "step": 65340
    },
    {
      "epoch": 3.6305555555555555,
      "grad_norm": 0.41511422395706177,
      "learning_rate": 1.3694444444444446e-05,
      "loss": 0.003,
      "step": 65350
    },
    {
      "epoch": 3.631111111111111,
      "grad_norm": 0.14907214045524597,
      "learning_rate": 1.368888888888889e-05,
      "loss": 0.0021,
      "step": 65360
    },
    {
      "epoch": 3.631666666666667,
      "grad_norm": 0.2966787815093994,
      "learning_rate": 1.3683333333333333e-05,
      "loss": 0.003,
      "step": 65370
    },
    {
      "epoch": 3.6322222222222225,
      "grad_norm": 0.4461718797683716,
      "learning_rate": 1.3677777777777779e-05,
      "loss": 0.0024,
      "step": 65380
    },
    {
      "epoch": 3.6327777777777777,
      "grad_norm": 0.20765231549739838,
      "learning_rate": 1.3672222222222222e-05,
      "loss": 0.0022,
      "step": 65390
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.23694849014282227,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0028,
      "step": 65400
    },
    {
      "epoch": 3.633888888888889,
      "grad_norm": 0.11983423680067062,
      "learning_rate": 1.3661111111111111e-05,
      "loss": 0.0022,
      "step": 65410
    },
    {
      "epoch": 3.6344444444444446,
      "grad_norm": 0.2969784438610077,
      "learning_rate": 1.3655555555555558e-05,
      "loss": 0.0026,
      "step": 65420
    },
    {
      "epoch": 3.635,
      "grad_norm": 0.1194041520357132,
      "learning_rate": 1.3650000000000001e-05,
      "loss": 0.0034,
      "step": 65430
    },
    {
      "epoch": 3.6355555555555554,
      "grad_norm": 0.03139612823724747,
      "learning_rate": 1.3644444444444445e-05,
      "loss": 0.0028,
      "step": 65440
    },
    {
      "epoch": 3.636111111111111,
      "grad_norm": 0.1485653817653656,
      "learning_rate": 1.363888888888889e-05,
      "loss": 0.0031,
      "step": 65450
    },
    {
      "epoch": 3.6366666666666667,
      "grad_norm": 0.580642580986023,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0035,
      "step": 65460
    },
    {
      "epoch": 3.6372222222222224,
      "grad_norm": 0.06779740005731583,
      "learning_rate": 1.3627777777777777e-05,
      "loss": 0.0018,
      "step": 65470
    },
    {
      "epoch": 3.637777777777778,
      "grad_norm": 0.1856553703546524,
      "learning_rate": 1.3622222222222223e-05,
      "loss": 0.0031,
      "step": 65480
    },
    {
      "epoch": 3.638333333333333,
      "grad_norm": 0.12227320671081543,
      "learning_rate": 1.3616666666666666e-05,
      "loss": 0.0023,
      "step": 65490
    },
    {
      "epoch": 3.638888888888889,
      "grad_norm": 0.17819669842720032,
      "learning_rate": 1.3611111111111111e-05,
      "loss": 0.0022,
      "step": 65500
    },
    {
      "epoch": 3.6394444444444445,
      "grad_norm": 0.4512902796268463,
      "learning_rate": 1.3605555555555557e-05,
      "loss": 0.0021,
      "step": 65510
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.4159058630466461,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0025,
      "step": 65520
    },
    {
      "epoch": 3.6405555555555553,
      "grad_norm": 0.0610608235001564,
      "learning_rate": 1.3594444444444445e-05,
      "loss": 0.0034,
      "step": 65530
    },
    {
      "epoch": 3.641111111111111,
      "grad_norm": 0.14837656915187836,
      "learning_rate": 1.358888888888889e-05,
      "loss": 0.0021,
      "step": 65540
    },
    {
      "epoch": 3.6416666666666666,
      "grad_norm": 0.6800314784049988,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.002,
      "step": 65550
    },
    {
      "epoch": 3.6422222222222222,
      "grad_norm": 0.14867904782295227,
      "learning_rate": 1.3577777777777778e-05,
      "loss": 0.0033,
      "step": 65560
    },
    {
      "epoch": 3.642777777777778,
      "grad_norm": 0.1490849405527115,
      "learning_rate": 1.3572222222222223e-05,
      "loss": 0.0028,
      "step": 65570
    },
    {
      "epoch": 3.6433333333333335,
      "grad_norm": 0.14991584420204163,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0029,
      "step": 65580
    },
    {
      "epoch": 3.6438888888888887,
      "grad_norm": 0.030049769207835197,
      "learning_rate": 1.356111111111111e-05,
      "loss": 0.0031,
      "step": 65590
    },
    {
      "epoch": 3.6444444444444444,
      "grad_norm": 0.35733893513679504,
      "learning_rate": 1.3555555555555557e-05,
      "loss": 0.0038,
      "step": 65600
    },
    {
      "epoch": 3.645,
      "grad_norm": 0.08941103518009186,
      "learning_rate": 1.3550000000000002e-05,
      "loss": 0.0029,
      "step": 65610
    },
    {
      "epoch": 3.6455555555555557,
      "grad_norm": 0.11955396831035614,
      "learning_rate": 1.3544444444444446e-05,
      "loss": 0.0031,
      "step": 65620
    },
    {
      "epoch": 3.646111111111111,
      "grad_norm": 0.12083403766155243,
      "learning_rate": 1.353888888888889e-05,
      "loss": 0.0037,
      "step": 65630
    },
    {
      "epoch": 3.6466666666666665,
      "grad_norm": 0.20743370056152344,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0022,
      "step": 65640
    },
    {
      "epoch": 3.647222222222222,
      "grad_norm": 0.17846232652664185,
      "learning_rate": 1.3527777777777778e-05,
      "loss": 0.0033,
      "step": 65650
    },
    {
      "epoch": 3.647777777777778,
      "grad_norm": 0.03807362914085388,
      "learning_rate": 1.3522222222222222e-05,
      "loss": 0.0034,
      "step": 65660
    },
    {
      "epoch": 3.6483333333333334,
      "grad_norm": 0.3079417645931244,
      "learning_rate": 1.3516666666666667e-05,
      "loss": 0.0029,
      "step": 65670
    },
    {
      "epoch": 3.648888888888889,
      "grad_norm": 0.28244754672050476,
      "learning_rate": 1.351111111111111e-05,
      "loss": 0.0025,
      "step": 65680
    },
    {
      "epoch": 3.6494444444444447,
      "grad_norm": 0.17924244701862335,
      "learning_rate": 1.3505555555555558e-05,
      "loss": 0.0023,
      "step": 65690
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.1490124613046646,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0027,
      "step": 65700
    },
    {
      "epoch": 3.6505555555555556,
      "grad_norm": 0.23294281959533691,
      "learning_rate": 1.3494444444444446e-05,
      "loss": 0.0023,
      "step": 65710
    },
    {
      "epoch": 3.651111111111111,
      "grad_norm": 0.3263833224773407,
      "learning_rate": 1.348888888888889e-05,
      "loss": 0.0028,
      "step": 65720
    },
    {
      "epoch": 3.6516666666666664,
      "grad_norm": 0.8647390007972717,
      "learning_rate": 1.3483333333333334e-05,
      "loss": 0.0028,
      "step": 65730
    },
    {
      "epoch": 3.652222222222222,
      "grad_norm": 0.23773226141929626,
      "learning_rate": 1.3477777777777779e-05,
      "loss": 0.003,
      "step": 65740
    },
    {
      "epoch": 3.6527777777777777,
      "grad_norm": 0.23727132380008698,
      "learning_rate": 1.3472222222222222e-05,
      "loss": 0.002,
      "step": 65750
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.10971223562955856,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0024,
      "step": 65760
    },
    {
      "epoch": 3.653888888888889,
      "grad_norm": 0.1272968202829361,
      "learning_rate": 1.3461111111111111e-05,
      "loss": 0.0029,
      "step": 65770
    },
    {
      "epoch": 3.6544444444444446,
      "grad_norm": 0.0906318947672844,
      "learning_rate": 1.3455555555555558e-05,
      "loss": 0.0032,
      "step": 65780
    },
    {
      "epoch": 3.6550000000000002,
      "grad_norm": 0.06272877752780914,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 0.0026,
      "step": 65790
    },
    {
      "epoch": 3.6555555555555554,
      "grad_norm": 0.18359902501106262,
      "learning_rate": 1.3444444444444445e-05,
      "loss": 0.0036,
      "step": 65800
    },
    {
      "epoch": 3.656111111111111,
      "grad_norm": 0.17848509550094604,
      "learning_rate": 1.343888888888889e-05,
      "loss": 0.0046,
      "step": 65810
    },
    {
      "epoch": 3.6566666666666667,
      "grad_norm": 0.03482265770435333,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.0031,
      "step": 65820
    },
    {
      "epoch": 3.6572222222222224,
      "grad_norm": 0.09095577895641327,
      "learning_rate": 1.3427777777777778e-05,
      "loss": 0.0025,
      "step": 65830
    },
    {
      "epoch": 3.6577777777777776,
      "grad_norm": 0.18035078048706055,
      "learning_rate": 1.3422222222222223e-05,
      "loss": 0.0031,
      "step": 65840
    },
    {
      "epoch": 3.658333333333333,
      "grad_norm": 0.23800970613956451,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.0027,
      "step": 65850
    },
    {
      "epoch": 3.658888888888889,
      "grad_norm": 0.07237687706947327,
      "learning_rate": 1.341111111111111e-05,
      "loss": 0.0046,
      "step": 65860
    },
    {
      "epoch": 3.6594444444444445,
      "grad_norm": 0.3007848858833313,
      "learning_rate": 1.3405555555555557e-05,
      "loss": 0.0034,
      "step": 65870
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.17923004925251007,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0039,
      "step": 65880
    },
    {
      "epoch": 3.660555555555556,
      "grad_norm": 0.17849579453468323,
      "learning_rate": 1.3394444444444446e-05,
      "loss": 0.0035,
      "step": 65890
    },
    {
      "epoch": 3.661111111111111,
      "grad_norm": 0.1019757091999054,
      "learning_rate": 1.338888888888889e-05,
      "loss": 0.0018,
      "step": 65900
    },
    {
      "epoch": 3.6616666666666666,
      "grad_norm": 0.03206350654363632,
      "learning_rate": 1.3383333333333335e-05,
      "loss": 0.0038,
      "step": 65910
    },
    {
      "epoch": 3.6622222222222223,
      "grad_norm": 0.1481841802597046,
      "learning_rate": 1.3377777777777778e-05,
      "loss": 0.0028,
      "step": 65920
    },
    {
      "epoch": 3.662777777777778,
      "grad_norm": 0.327179491519928,
      "learning_rate": 1.3372222222222222e-05,
      "loss": 0.0035,
      "step": 65930
    },
    {
      "epoch": 3.663333333333333,
      "grad_norm": 0.35627925395965576,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.0021,
      "step": 65940
    },
    {
      "epoch": 3.6638888888888888,
      "grad_norm": 0.41531452536582947,
      "learning_rate": 1.3361111111111114e-05,
      "loss": 0.0022,
      "step": 65950
    },
    {
      "epoch": 3.6644444444444444,
      "grad_norm": 0.11945217102766037,
      "learning_rate": 1.3355555555555557e-05,
      "loss": 0.0033,
      "step": 65960
    },
    {
      "epoch": 3.665,
      "grad_norm": 0.2381180077791214,
      "learning_rate": 1.3350000000000001e-05,
      "loss": 0.0029,
      "step": 65970
    },
    {
      "epoch": 3.6655555555555557,
      "grad_norm": 0.2379521131515503,
      "learning_rate": 1.3344444444444446e-05,
      "loss": 0.0039,
      "step": 65980
    },
    {
      "epoch": 3.6661111111111113,
      "grad_norm": 0.41704055666923523,
      "learning_rate": 1.333888888888889e-05,
      "loss": 0.003,
      "step": 65990
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.23723970353603363,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0029,
      "step": 66000
    },
    {
      "epoch": 3.667222222222222,
      "grad_norm": 0.2965545058250427,
      "learning_rate": 1.3327777777777779e-05,
      "loss": 0.0035,
      "step": 66010
    },
    {
      "epoch": 3.667777777777778,
      "grad_norm": 0.5951101183891296,
      "learning_rate": 1.3322222222222222e-05,
      "loss": 0.0033,
      "step": 66020
    },
    {
      "epoch": 3.6683333333333334,
      "grad_norm": 0.32659614086151123,
      "learning_rate": 1.3316666666666666e-05,
      "loss": 0.0032,
      "step": 66030
    },
    {
      "epoch": 3.6688888888888886,
      "grad_norm": 0.12091603130102158,
      "learning_rate": 1.3311111111111113e-05,
      "loss": 0.003,
      "step": 66040
    },
    {
      "epoch": 3.6694444444444443,
      "grad_norm": 0.23767101764678955,
      "learning_rate": 1.3305555555555558e-05,
      "loss": 0.002,
      "step": 66050
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.5177258253097534,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.003,
      "step": 66060
    },
    {
      "epoch": 3.6705555555555556,
      "grad_norm": 0.14848552644252777,
      "learning_rate": 1.3294444444444445e-05,
      "loss": 0.0039,
      "step": 66070
    },
    {
      "epoch": 3.671111111111111,
      "grad_norm": 0.06206713244318962,
      "learning_rate": 1.328888888888889e-05,
      "loss": 0.0027,
      "step": 66080
    },
    {
      "epoch": 3.671666666666667,
      "grad_norm": 0.35630807280540466,
      "learning_rate": 1.3283333333333334e-05,
      "loss": 0.0029,
      "step": 66090
    },
    {
      "epoch": 3.6722222222222225,
      "grad_norm": 0.6526464819908142,
      "learning_rate": 1.3277777777777777e-05,
      "loss": 0.0025,
      "step": 66100
    },
    {
      "epoch": 3.6727777777777777,
      "grad_norm": 0.11945611238479614,
      "learning_rate": 1.3272222222222223e-05,
      "loss": 0.0029,
      "step": 66110
    },
    {
      "epoch": 3.6733333333333333,
      "grad_norm": 0.2374267876148224,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0024,
      "step": 66120
    },
    {
      "epoch": 3.673888888888889,
      "grad_norm": 0.26765984296798706,
      "learning_rate": 1.3261111111111113e-05,
      "loss": 0.0024,
      "step": 66130
    },
    {
      "epoch": 3.674444444444444,
      "grad_norm": 0.08949650079011917,
      "learning_rate": 1.3255555555555557e-05,
      "loss": 0.0031,
      "step": 66140
    },
    {
      "epoch": 3.675,
      "grad_norm": 0.0637364313006401,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.0025,
      "step": 66150
    },
    {
      "epoch": 3.6755555555555555,
      "grad_norm": 0.2677512764930725,
      "learning_rate": 1.3244444444444445e-05,
      "loss": 0.0026,
      "step": 66160
    },
    {
      "epoch": 3.676111111111111,
      "grad_norm": 0.20841571688652039,
      "learning_rate": 1.3238888888888889e-05,
      "loss": 0.0027,
      "step": 66170
    },
    {
      "epoch": 3.6766666666666667,
      "grad_norm": 0.031752895563840866,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0025,
      "step": 66180
    },
    {
      "epoch": 3.6772222222222224,
      "grad_norm": 0.29750990867614746,
      "learning_rate": 1.3227777777777778e-05,
      "loss": 0.0028,
      "step": 66190
    },
    {
      "epoch": 3.677777777777778,
      "grad_norm": 0.12033576518297195,
      "learning_rate": 1.3222222222222221e-05,
      "loss": 0.0022,
      "step": 66200
    },
    {
      "epoch": 3.6783333333333332,
      "grad_norm": 0.14833059906959534,
      "learning_rate": 1.3216666666666667e-05,
      "loss": 0.0025,
      "step": 66210
    },
    {
      "epoch": 3.678888888888889,
      "grad_norm": 0.3966783881187439,
      "learning_rate": 1.3211111111111114e-05,
      "loss": 0.0038,
      "step": 66220
    },
    {
      "epoch": 3.6794444444444445,
      "grad_norm": 0.2378484457731247,
      "learning_rate": 1.3205555555555557e-05,
      "loss": 0.0033,
      "step": 66230
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.6363055109977722,
      "learning_rate": 1.32e-05,
      "loss": 0.0029,
      "step": 66240
    },
    {
      "epoch": 3.6805555555555554,
      "grad_norm": 0.09008638560771942,
      "learning_rate": 1.3194444444444446e-05,
      "loss": 0.0031,
      "step": 66250
    },
    {
      "epoch": 3.681111111111111,
      "grad_norm": 0.031263042241334915,
      "learning_rate": 1.318888888888889e-05,
      "loss": 0.0031,
      "step": 66260
    },
    {
      "epoch": 3.6816666666666666,
      "grad_norm": 0.060959406197071075,
      "learning_rate": 1.3183333333333333e-05,
      "loss": 0.0027,
      "step": 66270
    },
    {
      "epoch": 3.6822222222222223,
      "grad_norm": 0.03205758333206177,
      "learning_rate": 1.3177777777777778e-05,
      "loss": 0.0044,
      "step": 66280
    },
    {
      "epoch": 3.682777777777778,
      "grad_norm": 0.2383468896150589,
      "learning_rate": 1.3172222222222222e-05,
      "loss": 0.004,
      "step": 66290
    },
    {
      "epoch": 3.6833333333333336,
      "grad_norm": 0.26927515864372253,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.0035,
      "step": 66300
    },
    {
      "epoch": 3.6838888888888888,
      "grad_norm": 0.29385247826576233,
      "learning_rate": 1.3161111111111112e-05,
      "loss": 0.0021,
      "step": 66310
    },
    {
      "epoch": 3.6844444444444444,
      "grad_norm": 0.038077108561992645,
      "learning_rate": 1.3155555555555558e-05,
      "loss": 0.003,
      "step": 66320
    },
    {
      "epoch": 3.685,
      "grad_norm": 0.17804774641990662,
      "learning_rate": 1.3150000000000001e-05,
      "loss": 0.0029,
      "step": 66330
    },
    {
      "epoch": 3.6855555555555557,
      "grad_norm": 0.0898795798420906,
      "learning_rate": 1.3144444444444445e-05,
      "loss": 0.0021,
      "step": 66340
    },
    {
      "epoch": 3.686111111111111,
      "grad_norm": 0.14873097836971283,
      "learning_rate": 1.313888888888889e-05,
      "loss": 0.0029,
      "step": 66350
    },
    {
      "epoch": 3.6866666666666665,
      "grad_norm": 0.03162647411227226,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0025,
      "step": 66360
    },
    {
      "epoch": 3.687222222222222,
      "grad_norm": 0.7027285695075989,
      "learning_rate": 1.3127777777777777e-05,
      "loss": 0.0025,
      "step": 66370
    },
    {
      "epoch": 3.687777777777778,
      "grad_norm": 0.2284311056137085,
      "learning_rate": 1.3122222222222222e-05,
      "loss": 0.0033,
      "step": 66380
    },
    {
      "epoch": 3.6883333333333335,
      "grad_norm": 0.031430043280124664,
      "learning_rate": 1.3116666666666666e-05,
      "loss": 0.0037,
      "step": 66390
    },
    {
      "epoch": 3.688888888888889,
      "grad_norm": 0.5123789310455322,
      "learning_rate": 1.3111111111111113e-05,
      "loss": 0.0029,
      "step": 66400
    },
    {
      "epoch": 3.6894444444444443,
      "grad_norm": 0.4150620996952057,
      "learning_rate": 1.3105555555555556e-05,
      "loss": 0.0032,
      "step": 66410
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.1778288185596466,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0025,
      "step": 66420
    },
    {
      "epoch": 3.6905555555555556,
      "grad_norm": 0.26943832635879517,
      "learning_rate": 1.3094444444444445e-05,
      "loss": 0.0028,
      "step": 66430
    },
    {
      "epoch": 3.6911111111111112,
      "grad_norm": 0.09296492487192154,
      "learning_rate": 1.3088888888888889e-05,
      "loss": 0.004,
      "step": 66440
    },
    {
      "epoch": 3.6916666666666664,
      "grad_norm": 0.14305192232131958,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.0021,
      "step": 66450
    },
    {
      "epoch": 3.692222222222222,
      "grad_norm": 0.23723776638507843,
      "learning_rate": 1.3077777777777778e-05,
      "loss": 0.0038,
      "step": 66460
    },
    {
      "epoch": 3.6927777777777777,
      "grad_norm": 0.03111397661268711,
      "learning_rate": 1.3072222222222221e-05,
      "loss": 0.0022,
      "step": 66470
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.2372213900089264,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0025,
      "step": 66480
    },
    {
      "epoch": 3.693888888888889,
      "grad_norm": 0.060446836054325104,
      "learning_rate": 1.3061111111111113e-05,
      "loss": 0.0024,
      "step": 66490
    },
    {
      "epoch": 3.6944444444444446,
      "grad_norm": 0.11953141540288925,
      "learning_rate": 1.3055555555555557e-05,
      "loss": 0.0021,
      "step": 66500
    },
    {
      "epoch": 3.695,
      "grad_norm": 0.06691421568393707,
      "learning_rate": 1.305e-05,
      "loss": 0.0028,
      "step": 66510
    },
    {
      "epoch": 3.6955555555555555,
      "grad_norm": 0.08930493891239166,
      "learning_rate": 1.3044444444444446e-05,
      "loss": 0.0024,
      "step": 66520
    },
    {
      "epoch": 3.696111111111111,
      "grad_norm": 0.23750746250152588,
      "learning_rate": 1.303888888888889e-05,
      "loss": 0.0019,
      "step": 66530
    },
    {
      "epoch": 3.6966666666666668,
      "grad_norm": 0.25230082869529724,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.0047,
      "step": 66540
    },
    {
      "epoch": 3.697222222222222,
      "grad_norm": 0.2669261693954468,
      "learning_rate": 1.3027777777777778e-05,
      "loss": 0.0023,
      "step": 66550
    },
    {
      "epoch": 3.6977777777777776,
      "grad_norm": 0.08974646031856537,
      "learning_rate": 1.3022222222222222e-05,
      "loss": 0.0021,
      "step": 66560
    },
    {
      "epoch": 3.6983333333333333,
      "grad_norm": 0.11935148388147354,
      "learning_rate": 1.3016666666666669e-05,
      "loss": 0.0031,
      "step": 66570
    },
    {
      "epoch": 3.698888888888889,
      "grad_norm": 0.03315706551074982,
      "learning_rate": 1.3011111111111112e-05,
      "loss": 0.0027,
      "step": 66580
    },
    {
      "epoch": 3.6994444444444445,
      "grad_norm": 0.6442789435386658,
      "learning_rate": 1.3005555555555557e-05,
      "loss": 0.0032,
      "step": 66590
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.030902309343218803,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.002,
      "step": 66600
    },
    {
      "epoch": 3.700555555555556,
      "grad_norm": 0.4745442867279053,
      "learning_rate": 1.2994444444444444e-05,
      "loss": 0.0027,
      "step": 66610
    },
    {
      "epoch": 3.701111111111111,
      "grad_norm": 0.14844921231269836,
      "learning_rate": 1.298888888888889e-05,
      "loss": 0.003,
      "step": 66620
    },
    {
      "epoch": 3.7016666666666667,
      "grad_norm": 0.20753948390483856,
      "learning_rate": 1.2983333333333333e-05,
      "loss": 0.0028,
      "step": 66630
    },
    {
      "epoch": 3.7022222222222223,
      "grad_norm": 0.11956380307674408,
      "learning_rate": 1.2977777777777777e-05,
      "loss": 0.0035,
      "step": 66640
    },
    {
      "epoch": 3.7027777777777775,
      "grad_norm": 0.03195132687687874,
      "learning_rate": 1.2972222222222222e-05,
      "loss": 0.0023,
      "step": 66650
    },
    {
      "epoch": 3.703333333333333,
      "grad_norm": 0.34807339310646057,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0029,
      "step": 66660
    },
    {
      "epoch": 3.703888888888889,
      "grad_norm": 0.364544153213501,
      "learning_rate": 1.2961111111111113e-05,
      "loss": 0.0024,
      "step": 66670
    },
    {
      "epoch": 3.7044444444444444,
      "grad_norm": 0.1183730959892273,
      "learning_rate": 1.2955555555555556e-05,
      "loss": 0.002,
      "step": 66680
    },
    {
      "epoch": 3.705,
      "grad_norm": 0.3559820055961609,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 0.0019,
      "step": 66690
    },
    {
      "epoch": 3.7055555555555557,
      "grad_norm": 0.2666034996509552,
      "learning_rate": 1.2944444444444445e-05,
      "loss": 0.0021,
      "step": 66700
    },
    {
      "epoch": 3.7061111111111114,
      "grad_norm": 0.23805443942546844,
      "learning_rate": 1.2938888888888888e-05,
      "loss": 0.0028,
      "step": 66710
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.8744681477546692,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0032,
      "step": 66720
    },
    {
      "epoch": 3.707222222222222,
      "grad_norm": 0.1616957038640976,
      "learning_rate": 1.2927777777777777e-05,
      "loss": 0.003,
      "step": 66730
    },
    {
      "epoch": 3.707777777777778,
      "grad_norm": 0.6023826003074646,
      "learning_rate": 1.2922222222222221e-05,
      "loss": 0.0029,
      "step": 66740
    },
    {
      "epoch": 3.7083333333333335,
      "grad_norm": 0.11903107911348343,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0023,
      "step": 66750
    },
    {
      "epoch": 3.7088888888888887,
      "grad_norm": 0.149093359708786,
      "learning_rate": 1.2911111111111113e-05,
      "loss": 0.0024,
      "step": 66760
    },
    {
      "epoch": 3.7094444444444443,
      "grad_norm": 0.031359825283288956,
      "learning_rate": 1.2905555555555557e-05,
      "loss": 0.0024,
      "step": 66770
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.03227429464459419,
      "learning_rate": 1.29e-05,
      "loss": 0.0024,
      "step": 66780
    },
    {
      "epoch": 3.7105555555555556,
      "grad_norm": 0.03254261985421181,
      "learning_rate": 1.2894444444444445e-05,
      "loss": 0.0038,
      "step": 66790
    },
    {
      "epoch": 3.7111111111111112,
      "grad_norm": 0.14830704033374786,
      "learning_rate": 1.2888888888888889e-05,
      "loss": 0.003,
      "step": 66800
    },
    {
      "epoch": 3.711666666666667,
      "grad_norm": 0.2078489363193512,
      "learning_rate": 1.2883333333333333e-05,
      "loss": 0.0021,
      "step": 66810
    },
    {
      "epoch": 3.712222222222222,
      "grad_norm": 0.2250431776046753,
      "learning_rate": 1.2877777777777778e-05,
      "loss": 0.003,
      "step": 66820
    },
    {
      "epoch": 3.7127777777777777,
      "grad_norm": 0.4388049244880676,
      "learning_rate": 1.2872222222222221e-05,
      "loss": 0.0028,
      "step": 66830
    },
    {
      "epoch": 3.7133333333333334,
      "grad_norm": 0.0913003608584404,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0022,
      "step": 66840
    },
    {
      "epoch": 3.713888888888889,
      "grad_norm": 0.27438032627105713,
      "learning_rate": 1.2861111111111112e-05,
      "loss": 0.0027,
      "step": 66850
    },
    {
      "epoch": 3.714444444444444,
      "grad_norm": 0.207966148853302,
      "learning_rate": 1.2855555555555557e-05,
      "loss": 0.0024,
      "step": 66860
    },
    {
      "epoch": 3.715,
      "grad_norm": 0.23746857047080994,
      "learning_rate": 1.285e-05,
      "loss": 0.0022,
      "step": 66870
    },
    {
      "epoch": 3.7155555555555555,
      "grad_norm": 0.35541030764579773,
      "learning_rate": 1.2844444444444446e-05,
      "loss": 0.0032,
      "step": 66880
    },
    {
      "epoch": 3.716111111111111,
      "grad_norm": 0.2652258574962616,
      "learning_rate": 1.283888888888889e-05,
      "loss": 0.0036,
      "step": 66890
    },
    {
      "epoch": 3.716666666666667,
      "grad_norm": 0.44518691301345825,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0027,
      "step": 66900
    },
    {
      "epoch": 3.7172222222222224,
      "grad_norm": 0.2079441100358963,
      "learning_rate": 1.2827777777777778e-05,
      "loss": 0.0041,
      "step": 66910
    },
    {
      "epoch": 3.7177777777777776,
      "grad_norm": 0.14877517521381378,
      "learning_rate": 1.2822222222222222e-05,
      "loss": 0.0021,
      "step": 66920
    },
    {
      "epoch": 3.7183333333333333,
      "grad_norm": 0.06758690625429153,
      "learning_rate": 1.2816666666666669e-05,
      "loss": 0.0026,
      "step": 66930
    },
    {
      "epoch": 3.718888888888889,
      "grad_norm": 0.2987537980079651,
      "learning_rate": 1.2811111111111112e-05,
      "loss": 0.0022,
      "step": 66940
    },
    {
      "epoch": 3.7194444444444446,
      "grad_norm": 0.41569817066192627,
      "learning_rate": 1.2805555555555558e-05,
      "loss": 0.0029,
      "step": 66950
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.06120685487985611,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0035,
      "step": 66960
    },
    {
      "epoch": 3.7205555555555554,
      "grad_norm": 0.2382221668958664,
      "learning_rate": 1.2794444444444445e-05,
      "loss": 0.0024,
      "step": 66970
    },
    {
      "epoch": 3.721111111111111,
      "grad_norm": 0.14830869436264038,
      "learning_rate": 1.278888888888889e-05,
      "loss": 0.0025,
      "step": 66980
    },
    {
      "epoch": 3.7216666666666667,
      "grad_norm": 0.14924991130828857,
      "learning_rate": 1.2783333333333333e-05,
      "loss": 0.0025,
      "step": 66990
    },
    {
      "epoch": 3.7222222222222223,
      "grad_norm": 0.23703432083129883,
      "learning_rate": 1.2777777777777777e-05,
      "loss": 0.0027,
      "step": 67000
    },
    {
      "epoch": 3.722777777777778,
      "grad_norm": 0.23830391466617584,
      "learning_rate": 1.2772222222222222e-05,
      "loss": 0.0028,
      "step": 67010
    },
    {
      "epoch": 3.7233333333333336,
      "grad_norm": 0.3081139326095581,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0024,
      "step": 67020
    },
    {
      "epoch": 3.723888888888889,
      "grad_norm": 0.17907904088497162,
      "learning_rate": 1.2761111111111113e-05,
      "loss": 0.0035,
      "step": 67030
    },
    {
      "epoch": 3.7244444444444444,
      "grad_norm": 0.304555207490921,
      "learning_rate": 1.2755555555555556e-05,
      "loss": 0.0031,
      "step": 67040
    },
    {
      "epoch": 3.725,
      "grad_norm": 0.1484096795320511,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.0022,
      "step": 67050
    },
    {
      "epoch": 3.7255555555555553,
      "grad_norm": 0.17875707149505615,
      "learning_rate": 1.2744444444444445e-05,
      "loss": 0.0045,
      "step": 67060
    },
    {
      "epoch": 3.726111111111111,
      "grad_norm": 0.18243911862373352,
      "learning_rate": 1.2738888888888889e-05,
      "loss": 0.0031,
      "step": 67070
    },
    {
      "epoch": 3.7266666666666666,
      "grad_norm": 0.31138497591018677,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0037,
      "step": 67080
    },
    {
      "epoch": 3.727222222222222,
      "grad_norm": 0.17974984645843506,
      "learning_rate": 1.2727777777777778e-05,
      "loss": 0.003,
      "step": 67090
    },
    {
      "epoch": 3.727777777777778,
      "grad_norm": 0.26706454157829285,
      "learning_rate": 1.2722222222222221e-05,
      "loss": 0.0035,
      "step": 67100
    },
    {
      "epoch": 3.7283333333333335,
      "grad_norm": 0.10966745018959045,
      "learning_rate": 1.2716666666666668e-05,
      "loss": 0.0038,
      "step": 67110
    },
    {
      "epoch": 3.728888888888889,
      "grad_norm": 0.12027376145124435,
      "learning_rate": 1.2711111111111113e-05,
      "loss": 0.0029,
      "step": 67120
    },
    {
      "epoch": 3.7294444444444443,
      "grad_norm": 0.29736971855163574,
      "learning_rate": 1.2705555555555557e-05,
      "loss": 0.0031,
      "step": 67130
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.14902547001838684,
      "learning_rate": 1.27e-05,
      "loss": 0.002,
      "step": 67140
    },
    {
      "epoch": 3.7305555555555556,
      "grad_norm": 0.5617875456809998,
      "learning_rate": 1.2694444444444446e-05,
      "loss": 0.0044,
      "step": 67150
    },
    {
      "epoch": 3.7311111111111113,
      "grad_norm": 0.20842713117599487,
      "learning_rate": 1.268888888888889e-05,
      "loss": 0.0029,
      "step": 67160
    },
    {
      "epoch": 3.7316666666666665,
      "grad_norm": 0.32640284299850464,
      "learning_rate": 1.2683333333333333e-05,
      "loss": 0.0029,
      "step": 67170
    },
    {
      "epoch": 3.732222222222222,
      "grad_norm": 0.16940666735172272,
      "learning_rate": 1.2677777777777778e-05,
      "loss": 0.0022,
      "step": 67180
    },
    {
      "epoch": 3.7327777777777778,
      "grad_norm": 0.44534632563591003,
      "learning_rate": 1.2672222222222225e-05,
      "loss": 0.0035,
      "step": 67190
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.11882663518190384,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0026,
      "step": 67200
    },
    {
      "epoch": 3.733888888888889,
      "grad_norm": 0.29752543568611145,
      "learning_rate": 1.2661111111111112e-05,
      "loss": 0.0035,
      "step": 67210
    },
    {
      "epoch": 3.7344444444444447,
      "grad_norm": 0.1195591613650322,
      "learning_rate": 1.2655555555555557e-05,
      "loss": 0.0026,
      "step": 67220
    },
    {
      "epoch": 3.735,
      "grad_norm": 0.23171676695346832,
      "learning_rate": 1.2650000000000001e-05,
      "loss": 0.0026,
      "step": 67230
    },
    {
      "epoch": 3.7355555555555555,
      "grad_norm": 0.1788284331560135,
      "learning_rate": 1.2644444444444444e-05,
      "loss": 0.0022,
      "step": 67240
    },
    {
      "epoch": 3.736111111111111,
      "grad_norm": 0.059340257197618484,
      "learning_rate": 1.263888888888889e-05,
      "loss": 0.0028,
      "step": 67250
    },
    {
      "epoch": 3.736666666666667,
      "grad_norm": 0.47437727451324463,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.0024,
      "step": 67260
    },
    {
      "epoch": 3.737222222222222,
      "grad_norm": 0.39094558358192444,
      "learning_rate": 1.2627777777777777e-05,
      "loss": 0.0026,
      "step": 67270
    },
    {
      "epoch": 3.7377777777777776,
      "grad_norm": 0.0885256677865982,
      "learning_rate": 1.2622222222222224e-05,
      "loss": 0.0031,
      "step": 67280
    },
    {
      "epoch": 3.7383333333333333,
      "grad_norm": 0.4156913161277771,
      "learning_rate": 1.2616666666666669e-05,
      "loss": 0.0024,
      "step": 67290
    },
    {
      "epoch": 3.738888888888889,
      "grad_norm": 0.4152560830116272,
      "learning_rate": 1.2611111111111113e-05,
      "loss": 0.0033,
      "step": 67300
    },
    {
      "epoch": 3.7394444444444446,
      "grad_norm": 0.11329979449510574,
      "learning_rate": 1.2605555555555556e-05,
      "loss": 0.002,
      "step": 67310
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.177915558218956,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0028,
      "step": 67320
    },
    {
      "epoch": 3.7405555555555554,
      "grad_norm": 0.38565880060195923,
      "learning_rate": 1.2594444444444445e-05,
      "loss": 0.0027,
      "step": 67330
    },
    {
      "epoch": 3.741111111111111,
      "grad_norm": 0.543871283531189,
      "learning_rate": 1.2588888888888888e-05,
      "loss": 0.0028,
      "step": 67340
    },
    {
      "epoch": 3.7416666666666667,
      "grad_norm": 0.7964285016059875,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.0032,
      "step": 67350
    },
    {
      "epoch": 3.7422222222222223,
      "grad_norm": 0.07784868776798248,
      "learning_rate": 1.2577777777777777e-05,
      "loss": 0.0027,
      "step": 67360
    },
    {
      "epoch": 3.7427777777777775,
      "grad_norm": 0.12206586450338364,
      "learning_rate": 1.2572222222222224e-05,
      "loss": 0.0025,
      "step": 67370
    },
    {
      "epoch": 3.743333333333333,
      "grad_norm": 0.14842119812965393,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0029,
      "step": 67380
    },
    {
      "epoch": 3.743888888888889,
      "grad_norm": 0.326043039560318,
      "learning_rate": 1.2561111111111113e-05,
      "loss": 0.0026,
      "step": 67390
    },
    {
      "epoch": 3.7444444444444445,
      "grad_norm": 0.14915385842323303,
      "learning_rate": 1.2555555555555557e-05,
      "loss": 0.0028,
      "step": 67400
    },
    {
      "epoch": 3.745,
      "grad_norm": 0.2672119736671448,
      "learning_rate": 1.255e-05,
      "loss": 0.0021,
      "step": 67410
    },
    {
      "epoch": 3.7455555555555557,
      "grad_norm": 0.08968014270067215,
      "learning_rate": 1.2544444444444445e-05,
      "loss": 0.0025,
      "step": 67420
    },
    {
      "epoch": 3.746111111111111,
      "grad_norm": 0.47181904315948486,
      "learning_rate": 1.2538888888888889e-05,
      "loss": 0.0029,
      "step": 67430
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.1801505982875824,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0022,
      "step": 67440
    },
    {
      "epoch": 3.7472222222222222,
      "grad_norm": 0.03116820566356182,
      "learning_rate": 1.2527777777777778e-05,
      "loss": 0.0031,
      "step": 67450
    },
    {
      "epoch": 3.747777777777778,
      "grad_norm": 0.06039711833000183,
      "learning_rate": 1.2522222222222225e-05,
      "loss": 0.0026,
      "step": 67460
    },
    {
      "epoch": 3.748333333333333,
      "grad_norm": 0.2959490716457367,
      "learning_rate": 1.2516666666666668e-05,
      "loss": 0.0025,
      "step": 67470
    },
    {
      "epoch": 3.7488888888888887,
      "grad_norm": 0.2387012541294098,
      "learning_rate": 1.2511111111111112e-05,
      "loss": 0.0019,
      "step": 67480
    },
    {
      "epoch": 3.7494444444444444,
      "grad_norm": 0.11945874989032745,
      "learning_rate": 1.2505555555555557e-05,
      "loss": 0.0023,
      "step": 67490
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.356621652841568,
      "learning_rate": 1.25e-05,
      "loss": 0.0036,
      "step": 67500
    },
    {
      "epoch": 3.7505555555555556,
      "grad_norm": 0.4992591440677643,
      "learning_rate": 1.2494444444444444e-05,
      "loss": 0.003,
      "step": 67510
    },
    {
      "epoch": 3.7511111111111113,
      "grad_norm": 0.09172521531581879,
      "learning_rate": 1.248888888888889e-05,
      "loss": 0.0022,
      "step": 67520
    },
    {
      "epoch": 3.751666666666667,
      "grad_norm": 0.26747119426727295,
      "learning_rate": 1.2483333333333335e-05,
      "loss": 0.0027,
      "step": 67530
    },
    {
      "epoch": 3.752222222222222,
      "grad_norm": 0.36271214485168457,
      "learning_rate": 1.2477777777777778e-05,
      "loss": 0.0032,
      "step": 67540
    },
    {
      "epoch": 3.7527777777777778,
      "grad_norm": 0.3850993514060974,
      "learning_rate": 1.2472222222222223e-05,
      "loss": 0.002,
      "step": 67550
    },
    {
      "epoch": 3.7533333333333334,
      "grad_norm": 0.3075919449329376,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0023,
      "step": 67560
    },
    {
      "epoch": 3.753888888888889,
      "grad_norm": 0.29615360498428345,
      "learning_rate": 1.2461111111111112e-05,
      "loss": 0.0021,
      "step": 67570
    },
    {
      "epoch": 3.7544444444444443,
      "grad_norm": 0.14941804111003876,
      "learning_rate": 1.2455555555555556e-05,
      "loss": 0.0038,
      "step": 67580
    },
    {
      "epoch": 3.755,
      "grad_norm": 0.2685511112213135,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 0.0016,
      "step": 67590
    },
    {
      "epoch": 3.7555555555555555,
      "grad_norm": 0.2968480587005615,
      "learning_rate": 1.2444444444444445e-05,
      "loss": 0.0042,
      "step": 67600
    },
    {
      "epoch": 3.756111111111111,
      "grad_norm": 0.32658693194389343,
      "learning_rate": 1.2438888888888888e-05,
      "loss": 0.0029,
      "step": 67610
    },
    {
      "epoch": 3.756666666666667,
      "grad_norm": 0.45099952816963196,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0033,
      "step": 67620
    },
    {
      "epoch": 3.7572222222222225,
      "grad_norm": 0.031889695674180984,
      "learning_rate": 1.2427777777777779e-05,
      "loss": 0.0027,
      "step": 67630
    },
    {
      "epoch": 3.7577777777777777,
      "grad_norm": 0.14821085333824158,
      "learning_rate": 1.2422222222222222e-05,
      "loss": 0.0031,
      "step": 67640
    },
    {
      "epoch": 3.7583333333333333,
      "grad_norm": 0.17812326550483704,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.0029,
      "step": 67650
    },
    {
      "epoch": 3.758888888888889,
      "grad_norm": 0.06106362119317055,
      "learning_rate": 1.2411111111111113e-05,
      "loss": 0.0042,
      "step": 67660
    },
    {
      "epoch": 3.7594444444444446,
      "grad_norm": 0.11279793828725815,
      "learning_rate": 1.2405555555555556e-05,
      "loss": 0.0023,
      "step": 67670
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.5806772708892822,
      "learning_rate": 1.24e-05,
      "loss": 0.002,
      "step": 67680
    },
    {
      "epoch": 3.7605555555555554,
      "grad_norm": 0.24054750800132751,
      "learning_rate": 1.2394444444444445e-05,
      "loss": 0.0025,
      "step": 67690
    },
    {
      "epoch": 3.761111111111111,
      "grad_norm": 0.08983608335256577,
      "learning_rate": 1.238888888888889e-05,
      "loss": 0.002,
      "step": 67700
    },
    {
      "epoch": 3.7616666666666667,
      "grad_norm": 0.11948363482952118,
      "learning_rate": 1.2383333333333334e-05,
      "loss": 0.003,
      "step": 67710
    },
    {
      "epoch": 3.7622222222222224,
      "grad_norm": 0.17885033786296844,
      "learning_rate": 1.237777777777778e-05,
      "loss": 0.0024,
      "step": 67720
    },
    {
      "epoch": 3.762777777777778,
      "grad_norm": 0.08971267193555832,
      "learning_rate": 1.2372222222222223e-05,
      "loss": 0.0023,
      "step": 67730
    },
    {
      "epoch": 3.763333333333333,
      "grad_norm": 0.4908851981163025,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0027,
      "step": 67740
    },
    {
      "epoch": 3.763888888888889,
      "grad_norm": 0.07590433210134506,
      "learning_rate": 1.2361111111111112e-05,
      "loss": 0.0048,
      "step": 67750
    },
    {
      "epoch": 3.7644444444444445,
      "grad_norm": 0.08974341303110123,
      "learning_rate": 1.2355555555555557e-05,
      "loss": 0.0033,
      "step": 67760
    },
    {
      "epoch": 3.765,
      "grad_norm": 0.060925573110580444,
      "learning_rate": 1.235e-05,
      "loss": 0.0024,
      "step": 67770
    },
    {
      "epoch": 3.7655555555555553,
      "grad_norm": 0.5261918902397156,
      "learning_rate": 1.2344444444444444e-05,
      "loss": 0.0028,
      "step": 67780
    },
    {
      "epoch": 3.766111111111111,
      "grad_norm": 0.06386803090572357,
      "learning_rate": 1.233888888888889e-05,
      "loss": 0.002,
      "step": 67790
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.20761394500732422,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0035,
      "step": 67800
    },
    {
      "epoch": 3.7672222222222222,
      "grad_norm": 0.12061073631048203,
      "learning_rate": 1.2327777777777778e-05,
      "loss": 0.0036,
      "step": 67810
    },
    {
      "epoch": 3.767777777777778,
      "grad_norm": 0.17871792614459991,
      "learning_rate": 1.2322222222222223e-05,
      "loss": 0.0028,
      "step": 67820
    },
    {
      "epoch": 3.7683333333333335,
      "grad_norm": 0.21407614648342133,
      "learning_rate": 1.2316666666666667e-05,
      "loss": 0.0028,
      "step": 67830
    },
    {
      "epoch": 3.7688888888888887,
      "grad_norm": 0.060468640178442,
      "learning_rate": 1.2311111111111112e-05,
      "loss": 0.0029,
      "step": 67840
    },
    {
      "epoch": 3.7694444444444444,
      "grad_norm": 0.17298129200935364,
      "learning_rate": 1.2305555555555556e-05,
      "loss": 0.0029,
      "step": 67850
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.3867827355861664,
      "learning_rate": 1.23e-05,
      "loss": 0.0033,
      "step": 67860
    },
    {
      "epoch": 3.7705555555555557,
      "grad_norm": 0.17500746250152588,
      "learning_rate": 1.2294444444444444e-05,
      "loss": 0.0027,
      "step": 67870
    },
    {
      "epoch": 3.771111111111111,
      "grad_norm": 0.13537666201591492,
      "learning_rate": 1.228888888888889e-05,
      "loss": 0.0025,
      "step": 67880
    },
    {
      "epoch": 3.7716666666666665,
      "grad_norm": 0.0953943058848381,
      "learning_rate": 1.2283333333333335e-05,
      "loss": 0.0028,
      "step": 67890
    },
    {
      "epoch": 3.772222222222222,
      "grad_norm": 0.5648601651191711,
      "learning_rate": 1.2277777777777778e-05,
      "loss": 0.0026,
      "step": 67900
    },
    {
      "epoch": 3.772777777777778,
      "grad_norm": 0.1786322146654129,
      "learning_rate": 1.2272222222222222e-05,
      "loss": 0.0023,
      "step": 67910
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.00961272045969963,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0026,
      "step": 67920
    },
    {
      "epoch": 3.773888888888889,
      "grad_norm": 0.014815685339272022,
      "learning_rate": 1.2261111111111112e-05,
      "loss": 0.0022,
      "step": 67930
    },
    {
      "epoch": 3.7744444444444447,
      "grad_norm": 0.011266250163316727,
      "learning_rate": 1.2255555555555556e-05,
      "loss": 0.0041,
      "step": 67940
    },
    {
      "epoch": 3.775,
      "grad_norm": 0.09063612669706345,
      "learning_rate": 1.225e-05,
      "loss": 0.0027,
      "step": 67950
    },
    {
      "epoch": 3.7755555555555556,
      "grad_norm": 0.11891622096300125,
      "learning_rate": 1.2244444444444445e-05,
      "loss": 0.0032,
      "step": 67960
    },
    {
      "epoch": 3.776111111111111,
      "grad_norm": 0.27377888560295105,
      "learning_rate": 1.223888888888889e-05,
      "loss": 0.0034,
      "step": 67970
    },
    {
      "epoch": 3.7766666666666664,
      "grad_norm": 0.23751866817474365,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0033,
      "step": 67980
    },
    {
      "epoch": 3.777222222222222,
      "grad_norm": 0.059869833290576935,
      "learning_rate": 1.2227777777777779e-05,
      "loss": 0.0033,
      "step": 67990
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.03189549595117569,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 0.0048,
      "step": 68000
    },
    {
      "epoch": 3.7783333333333333,
      "grad_norm": 0.385343998670578,
      "learning_rate": 1.2216666666666668e-05,
      "loss": 0.0032,
      "step": 68010
    },
    {
      "epoch": 3.778888888888889,
      "grad_norm": 0.5074234008789062,
      "learning_rate": 1.2211111111111111e-05,
      "loss": 0.0026,
      "step": 68020
    },
    {
      "epoch": 3.7794444444444446,
      "grad_norm": 0.16573601961135864,
      "learning_rate": 1.2205555555555557e-05,
      "loss": 0.0021,
      "step": 68030
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.2970922291278839,
      "learning_rate": 1.22e-05,
      "loss": 0.0029,
      "step": 68040
    },
    {
      "epoch": 3.7805555555555554,
      "grad_norm": 0.21327519416809082,
      "learning_rate": 1.2194444444444444e-05,
      "loss": 0.0039,
      "step": 68050
    },
    {
      "epoch": 3.781111111111111,
      "grad_norm": 0.03195567801594734,
      "learning_rate": 1.218888888888889e-05,
      "loss": 0.0043,
      "step": 68060
    },
    {
      "epoch": 3.7816666666666667,
      "grad_norm": 0.2975548207759857,
      "learning_rate": 1.2183333333333334e-05,
      "loss": 0.0025,
      "step": 68070
    },
    {
      "epoch": 3.7822222222222224,
      "grad_norm": 0.013963889330625534,
      "learning_rate": 1.2177777777777778e-05,
      "loss": 0.0032,
      "step": 68080
    },
    {
      "epoch": 3.7827777777777776,
      "grad_norm": 0.01204440277069807,
      "learning_rate": 1.2172222222222223e-05,
      "loss": 0.0031,
      "step": 68090
    },
    {
      "epoch": 3.783333333333333,
      "grad_norm": 0.035699136555194855,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0032,
      "step": 68100
    },
    {
      "epoch": 3.783888888888889,
      "grad_norm": 0.38526758551597595,
      "learning_rate": 1.2161111111111112e-05,
      "loss": 0.0033,
      "step": 68110
    },
    {
      "epoch": 3.7844444444444445,
      "grad_norm": 0.2828853130340576,
      "learning_rate": 1.2155555555555555e-05,
      "loss": 0.003,
      "step": 68120
    },
    {
      "epoch": 3.785,
      "grad_norm": 0.7508224844932556,
      "learning_rate": 1.215e-05,
      "loss": 0.0034,
      "step": 68130
    },
    {
      "epoch": 3.785555555555556,
      "grad_norm": 0.08752962946891785,
      "learning_rate": 1.2144444444444444e-05,
      "loss": 0.0043,
      "step": 68140
    },
    {
      "epoch": 3.786111111111111,
      "grad_norm": 0.1505098193883896,
      "learning_rate": 1.213888888888889e-05,
      "loss": 0.0029,
      "step": 68150
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.2927267253398895,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.002,
      "step": 68160
    },
    {
      "epoch": 3.7872222222222223,
      "grad_norm": 0.09533864259719849,
      "learning_rate": 1.2127777777777778e-05,
      "loss": 0.0027,
      "step": 68170
    },
    {
      "epoch": 3.787777777777778,
      "grad_norm": 0.11899647116661072,
      "learning_rate": 1.2122222222222222e-05,
      "loss": 0.0027,
      "step": 68180
    },
    {
      "epoch": 3.788333333333333,
      "grad_norm": 0.03154706582427025,
      "learning_rate": 1.2116666666666669e-05,
      "loss": 0.0042,
      "step": 68190
    },
    {
      "epoch": 3.7888888888888888,
      "grad_norm": 0.27016088366508484,
      "learning_rate": 1.2111111111111112e-05,
      "loss": 0.0022,
      "step": 68200
    },
    {
      "epoch": 3.7894444444444444,
      "grad_norm": 0.4265516698360443,
      "learning_rate": 1.2105555555555556e-05,
      "loss": 0.0034,
      "step": 68210
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.0891072079539299,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.0026,
      "step": 68220
    },
    {
      "epoch": 3.7905555555555557,
      "grad_norm": 0.1788063645362854,
      "learning_rate": 1.2094444444444445e-05,
      "loss": 0.002,
      "step": 68230
    },
    {
      "epoch": 3.7911111111111113,
      "grad_norm": 0.12274539470672607,
      "learning_rate": 1.208888888888889e-05,
      "loss": 0.0022,
      "step": 68240
    },
    {
      "epoch": 3.7916666666666665,
      "grad_norm": 0.2092512845993042,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.0024,
      "step": 68250
    },
    {
      "epoch": 3.792222222222222,
      "grad_norm": 0.2967069745063782,
      "learning_rate": 1.2077777777777779e-05,
      "loss": 0.0022,
      "step": 68260
    },
    {
      "epoch": 3.792777777777778,
      "grad_norm": 0.23342914879322052,
      "learning_rate": 1.2072222222222222e-05,
      "loss": 0.0037,
      "step": 68270
    },
    {
      "epoch": 3.7933333333333334,
      "grad_norm": 0.17869992554187775,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0023,
      "step": 68280
    },
    {
      "epoch": 3.7938888888888886,
      "grad_norm": 0.32925984263420105,
      "learning_rate": 1.2061111111111113e-05,
      "loss": 0.0021,
      "step": 68290
    },
    {
      "epoch": 3.7944444444444443,
      "grad_norm": 0.11878406256437302,
      "learning_rate": 1.2055555555555556e-05,
      "loss": 0.0029,
      "step": 68300
    },
    {
      "epoch": 3.795,
      "grad_norm": 0.03294234722852707,
      "learning_rate": 1.205e-05,
      "loss": 0.0033,
      "step": 68310
    },
    {
      "epoch": 3.7955555555555556,
      "grad_norm": 0.38939860463142395,
      "learning_rate": 1.2044444444444445e-05,
      "loss": 0.0028,
      "step": 68320
    },
    {
      "epoch": 3.796111111111111,
      "grad_norm": 0.03362666815519333,
      "learning_rate": 1.203888888888889e-05,
      "loss": 0.0029,
      "step": 68330
    },
    {
      "epoch": 3.796666666666667,
      "grad_norm": 0.1488250344991684,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0026,
      "step": 68340
    },
    {
      "epoch": 3.7972222222222225,
      "grad_norm": 0.2393769770860672,
      "learning_rate": 1.2027777777777777e-05,
      "loss": 0.0035,
      "step": 68350
    },
    {
      "epoch": 3.7977777777777777,
      "grad_norm": 0.593086838722229,
      "learning_rate": 1.2022222222222223e-05,
      "loss": 0.003,
      "step": 68360
    },
    {
      "epoch": 3.7983333333333333,
      "grad_norm": 0.03356854245066643,
      "learning_rate": 1.2016666666666668e-05,
      "loss": 0.0032,
      "step": 68370
    },
    {
      "epoch": 3.798888888888889,
      "grad_norm": 0.26694026589393616,
      "learning_rate": 1.2011111111111111e-05,
      "loss": 0.003,
      "step": 68380
    },
    {
      "epoch": 3.799444444444444,
      "grad_norm": 0.03380351513624191,
      "learning_rate": 1.2005555555555557e-05,
      "loss": 0.0022,
      "step": 68390
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.14844036102294922,
      "learning_rate": 1.2e-05,
      "loss": 0.0028,
      "step": 68400
    },
    {
      "epoch": 3.8005555555555555,
      "grad_norm": 0.11922915279865265,
      "learning_rate": 1.1994444444444446e-05,
      "loss": 0.0033,
      "step": 68410
    },
    {
      "epoch": 3.801111111111111,
      "grad_norm": 0.2286677360534668,
      "learning_rate": 1.1988888888888889e-05,
      "loss": 0.0018,
      "step": 68420
    },
    {
      "epoch": 3.8016666666666667,
      "grad_norm": 0.08966906368732452,
      "learning_rate": 1.1983333333333334e-05,
      "loss": 0.0025,
      "step": 68430
    },
    {
      "epoch": 3.8022222222222224,
      "grad_norm": 0.061957404017448425,
      "learning_rate": 1.1977777777777778e-05,
      "loss": 0.0022,
      "step": 68440
    },
    {
      "epoch": 3.802777777777778,
      "grad_norm": 0.14929650723934174,
      "learning_rate": 1.1972222222222221e-05,
      "loss": 0.0026,
      "step": 68450
    },
    {
      "epoch": 3.8033333333333332,
      "grad_norm": 0.2631758451461792,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0029,
      "step": 68460
    },
    {
      "epoch": 3.803888888888889,
      "grad_norm": 0.5085933804512024,
      "learning_rate": 1.1961111111111112e-05,
      "loss": 0.0028,
      "step": 68470
    },
    {
      "epoch": 3.8044444444444445,
      "grad_norm": 0.20215733349323273,
      "learning_rate": 1.1955555555555556e-05,
      "loss": 0.0029,
      "step": 68480
    },
    {
      "epoch": 3.805,
      "grad_norm": 0.06090114638209343,
      "learning_rate": 1.195e-05,
      "loss": 0.0029,
      "step": 68490
    },
    {
      "epoch": 3.8055555555555554,
      "grad_norm": 0.1491803079843521,
      "learning_rate": 1.1944444444444446e-05,
      "loss": 0.0041,
      "step": 68500
    },
    {
      "epoch": 3.806111111111111,
      "grad_norm": 0.14835481345653534,
      "learning_rate": 1.193888888888889e-05,
      "loss": 0.0029,
      "step": 68510
    },
    {
      "epoch": 3.8066666666666666,
      "grad_norm": 0.2918475866317749,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0023,
      "step": 68520
    },
    {
      "epoch": 3.8072222222222223,
      "grad_norm": 0.17973513901233673,
      "learning_rate": 1.1927777777777778e-05,
      "loss": 0.0029,
      "step": 68530
    },
    {
      "epoch": 3.807777777777778,
      "grad_norm": 0.46653687953948975,
      "learning_rate": 1.1922222222222222e-05,
      "loss": 0.003,
      "step": 68540
    },
    {
      "epoch": 3.8083333333333336,
      "grad_norm": 0.8434943556785583,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.0037,
      "step": 68550
    },
    {
      "epoch": 3.8088888888888888,
      "grad_norm": 0.1488388627767563,
      "learning_rate": 1.1911111111111112e-05,
      "loss": 0.0028,
      "step": 68560
    },
    {
      "epoch": 3.8094444444444444,
      "grad_norm": 0.11885030567646027,
      "learning_rate": 1.1905555555555556e-05,
      "loss": 0.0024,
      "step": 68570
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.43100130558013916,
      "learning_rate": 1.19e-05,
      "loss": 0.0031,
      "step": 68580
    },
    {
      "epoch": 3.8105555555555557,
      "grad_norm": 0.08928724378347397,
      "learning_rate": 1.1894444444444445e-05,
      "loss": 0.002,
      "step": 68590
    },
    {
      "epoch": 3.811111111111111,
      "grad_norm": 0.356967568397522,
      "learning_rate": 1.188888888888889e-05,
      "loss": 0.0026,
      "step": 68600
    },
    {
      "epoch": 3.8116666666666665,
      "grad_norm": 0.007601229939609766,
      "learning_rate": 1.1883333333333334e-05,
      "loss": 0.003,
      "step": 68610
    },
    {
      "epoch": 3.812222222222222,
      "grad_norm": 0.5449427962303162,
      "learning_rate": 1.1877777777777777e-05,
      "loss": 0.0028,
      "step": 68620
    },
    {
      "epoch": 3.812777777777778,
      "grad_norm": 0.20510800182819366,
      "learning_rate": 1.1872222222222224e-05,
      "loss": 0.0026,
      "step": 68630
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.3550930917263031,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0047,
      "step": 68640
    },
    {
      "epoch": 3.813888888888889,
      "grad_norm": 0.09097664803266525,
      "learning_rate": 1.1861111111111111e-05,
      "loss": 0.0029,
      "step": 68650
    },
    {
      "epoch": 3.8144444444444443,
      "grad_norm": 0.12078173458576202,
      "learning_rate": 1.1855555555555556e-05,
      "loss": 0.0021,
      "step": 68660
    },
    {
      "epoch": 3.815,
      "grad_norm": 0.16467276215553284,
      "learning_rate": 1.185e-05,
      "loss": 0.0029,
      "step": 68670
    },
    {
      "epoch": 3.8155555555555556,
      "grad_norm": 0.012667720206081867,
      "learning_rate": 1.1844444444444445e-05,
      "loss": 0.0021,
      "step": 68680
    },
    {
      "epoch": 3.8161111111111112,
      "grad_norm": 0.6896463632583618,
      "learning_rate": 1.1838888888888889e-05,
      "loss": 0.0027,
      "step": 68690
    },
    {
      "epoch": 3.8166666666666664,
      "grad_norm": 0.26766225695610046,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.002,
      "step": 68700
    },
    {
      "epoch": 3.817222222222222,
      "grad_norm": 0.3264303207397461,
      "learning_rate": 1.1827777777777778e-05,
      "loss": 0.0034,
      "step": 68710
    },
    {
      "epoch": 3.8177777777777777,
      "grad_norm": 0.39460289478302,
      "learning_rate": 1.1822222222222223e-05,
      "loss": 0.0027,
      "step": 68720
    },
    {
      "epoch": 3.8183333333333334,
      "grad_norm": 0.5352159738540649,
      "learning_rate": 1.1816666666666668e-05,
      "loss": 0.0024,
      "step": 68730
    },
    {
      "epoch": 3.818888888888889,
      "grad_norm": 0.2374398559331894,
      "learning_rate": 1.1811111111111112e-05,
      "loss": 0.0023,
      "step": 68740
    },
    {
      "epoch": 3.8194444444444446,
      "grad_norm": 0.3561178743839264,
      "learning_rate": 1.1805555555555555e-05,
      "loss": 0.0028,
      "step": 68750
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.14871594309806824,
      "learning_rate": 1.18e-05,
      "loss": 0.0028,
      "step": 68760
    },
    {
      "epoch": 3.8205555555555555,
      "grad_norm": 0.11954963207244873,
      "learning_rate": 1.1794444444444446e-05,
      "loss": 0.0027,
      "step": 68770
    },
    {
      "epoch": 3.821111111111111,
      "grad_norm": 0.06061495468020439,
      "learning_rate": 1.178888888888889e-05,
      "loss": 0.0037,
      "step": 68780
    },
    {
      "epoch": 3.8216666666666668,
      "grad_norm": 0.44516149163246155,
      "learning_rate": 1.1783333333333333e-05,
      "loss": 0.003,
      "step": 68790
    },
    {
      "epoch": 3.822222222222222,
      "grad_norm": 0.08992644399404526,
      "learning_rate": 1.1777777777777778e-05,
      "loss": 0.0034,
      "step": 68800
    },
    {
      "epoch": 3.8227777777777776,
      "grad_norm": 0.4508925676345825,
      "learning_rate": 1.1772222222222223e-05,
      "loss": 0.0034,
      "step": 68810
    },
    {
      "epoch": 3.8233333333333333,
      "grad_norm": 0.26634326577186584,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0034,
      "step": 68820
    },
    {
      "epoch": 3.823888888888889,
      "grad_norm": 0.17777903378009796,
      "learning_rate": 1.1761111111111112e-05,
      "loss": 0.0024,
      "step": 68830
    },
    {
      "epoch": 3.8244444444444445,
      "grad_norm": 0.08994600921869278,
      "learning_rate": 1.1755555555555556e-05,
      "loss": 0.0038,
      "step": 68840
    },
    {
      "epoch": 3.825,
      "grad_norm": 0.09125878661870956,
      "learning_rate": 1.175e-05,
      "loss": 0.0029,
      "step": 68850
    },
    {
      "epoch": 3.825555555555556,
      "grad_norm": 0.03451919928193092,
      "learning_rate": 1.1744444444444446e-05,
      "loss": 0.0018,
      "step": 68860
    },
    {
      "epoch": 3.826111111111111,
      "grad_norm": 0.4098549485206604,
      "learning_rate": 1.173888888888889e-05,
      "loss": 0.0025,
      "step": 68870
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.007711751386523247,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0028,
      "step": 68880
    },
    {
      "epoch": 3.8272222222222223,
      "grad_norm": 0.2379467785358429,
      "learning_rate": 1.1727777777777779e-05,
      "loss": 0.0039,
      "step": 68890
    },
    {
      "epoch": 3.8277777777777775,
      "grad_norm": 0.10707394778728485,
      "learning_rate": 1.1722222222222224e-05,
      "loss": 0.004,
      "step": 68900
    },
    {
      "epoch": 3.828333333333333,
      "grad_norm": 0.11868482083082199,
      "learning_rate": 1.1716666666666667e-05,
      "loss": 0.0025,
      "step": 68910
    },
    {
      "epoch": 3.828888888888889,
      "grad_norm": 0.1815599501132965,
      "learning_rate": 1.1711111111111111e-05,
      "loss": 0.0025,
      "step": 68920
    },
    {
      "epoch": 3.8294444444444444,
      "grad_norm": 0.42572811245918274,
      "learning_rate": 1.1705555555555556e-05,
      "loss": 0.0028,
      "step": 68930
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.4149576723575592,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.0036,
      "step": 68940
    },
    {
      "epoch": 3.8305555555555557,
      "grad_norm": 0.11906509846448898,
      "learning_rate": 1.1694444444444445e-05,
      "loss": 0.0022,
      "step": 68950
    },
    {
      "epoch": 3.8311111111111114,
      "grad_norm": 0.17881938815116882,
      "learning_rate": 1.168888888888889e-05,
      "loss": 0.002,
      "step": 68960
    },
    {
      "epoch": 3.8316666666666666,
      "grad_norm": 0.3996286392211914,
      "learning_rate": 1.1683333333333334e-05,
      "loss": 0.0021,
      "step": 68970
    },
    {
      "epoch": 3.832222222222222,
      "grad_norm": 0.031880442053079605,
      "learning_rate": 1.1677777777777777e-05,
      "loss": 0.0029,
      "step": 68980
    },
    {
      "epoch": 3.832777777777778,
      "grad_norm": 0.326249361038208,
      "learning_rate": 1.1672222222222223e-05,
      "loss": 0.0022,
      "step": 68990
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.0892181247472763,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0035,
      "step": 69000
    },
    {
      "epoch": 3.8338888888888887,
      "grad_norm": 0.1780683547258377,
      "learning_rate": 1.1661111111111111e-05,
      "loss": 0.0029,
      "step": 69010
    },
    {
      "epoch": 3.8344444444444443,
      "grad_norm": 0.2966977059841156,
      "learning_rate": 1.1655555555555555e-05,
      "loss": 0.0033,
      "step": 69020
    },
    {
      "epoch": 3.835,
      "grad_norm": 0.08998367935419083,
      "learning_rate": 1.1650000000000002e-05,
      "loss": 0.0047,
      "step": 69030
    },
    {
      "epoch": 3.8355555555555556,
      "grad_norm": 0.09019532799720764,
      "learning_rate": 1.1644444444444446e-05,
      "loss": 0.0032,
      "step": 69040
    },
    {
      "epoch": 3.8361111111111112,
      "grad_norm": 0.21249592304229736,
      "learning_rate": 1.1638888888888889e-05,
      "loss": 0.0039,
      "step": 69050
    },
    {
      "epoch": 3.836666666666667,
      "grad_norm": 0.09467791020870209,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0032,
      "step": 69060
    },
    {
      "epoch": 3.837222222222222,
      "grad_norm": 0.4994157552719116,
      "learning_rate": 1.1627777777777778e-05,
      "loss": 0.004,
      "step": 69070
    },
    {
      "epoch": 3.8377777777777777,
      "grad_norm": 0.06555553525686264,
      "learning_rate": 1.1622222222222223e-05,
      "loss": 0.0033,
      "step": 69080
    },
    {
      "epoch": 3.8383333333333334,
      "grad_norm": 0.17845956981182098,
      "learning_rate": 1.1616666666666667e-05,
      "loss": 0.0021,
      "step": 69090
    },
    {
      "epoch": 3.838888888888889,
      "grad_norm": 0.1781173050403595,
      "learning_rate": 1.1611111111111112e-05,
      "loss": 0.0022,
      "step": 69100
    },
    {
      "epoch": 3.839444444444444,
      "grad_norm": 0.4159804582595825,
      "learning_rate": 1.1605555555555555e-05,
      "loss": 0.0028,
      "step": 69110
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.47906872630119324,
      "learning_rate": 1.16e-05,
      "loss": 0.0026,
      "step": 69120
    },
    {
      "epoch": 3.8405555555555555,
      "grad_norm": 0.09008663147687912,
      "learning_rate": 1.1594444444444446e-05,
      "loss": 0.0035,
      "step": 69130
    },
    {
      "epoch": 3.841111111111111,
      "grad_norm": 0.5085466504096985,
      "learning_rate": 1.158888888888889e-05,
      "loss": 0.0036,
      "step": 69140
    },
    {
      "epoch": 3.841666666666667,
      "grad_norm": 0.00853633601218462,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.0031,
      "step": 69150
    },
    {
      "epoch": 3.8422222222222224,
      "grad_norm": 0.1204129233956337,
      "learning_rate": 1.1577777777777778e-05,
      "loss": 0.0021,
      "step": 69160
    },
    {
      "epoch": 3.8427777777777776,
      "grad_norm": 0.0603167861700058,
      "learning_rate": 1.1572222222222224e-05,
      "loss": 0.0033,
      "step": 69170
    },
    {
      "epoch": 3.8433333333333333,
      "grad_norm": 0.23729833960533142,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.0032,
      "step": 69180
    },
    {
      "epoch": 3.843888888888889,
      "grad_norm": 0.23787836730480194,
      "learning_rate": 1.156111111111111e-05,
      "loss": 0.0022,
      "step": 69190
    },
    {
      "epoch": 3.8444444444444446,
      "grad_norm": 0.05584876611828804,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 0.0037,
      "step": 69200
    },
    {
      "epoch": 3.8449999999999998,
      "grad_norm": 0.03282967209815979,
      "learning_rate": 1.1550000000000001e-05,
      "loss": 0.0033,
      "step": 69210
    },
    {
      "epoch": 3.8455555555555554,
      "grad_norm": 0.09463569521903992,
      "learning_rate": 1.1544444444444445e-05,
      "loss": 0.0035,
      "step": 69220
    },
    {
      "epoch": 3.846111111111111,
      "grad_norm": 0.1827571988105774,
      "learning_rate": 1.153888888888889e-05,
      "loss": 0.0028,
      "step": 69230
    },
    {
      "epoch": 3.8466666666666667,
      "grad_norm": 0.12041059881448746,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0023,
      "step": 69240
    },
    {
      "epoch": 3.8472222222222223,
      "grad_norm": 0.00946373026818037,
      "learning_rate": 1.1527777777777779e-05,
      "loss": 0.0031,
      "step": 69250
    },
    {
      "epoch": 3.847777777777778,
      "grad_norm": 0.03258665278553963,
      "learning_rate": 1.1522222222222222e-05,
      "loss": 0.0036,
      "step": 69260
    },
    {
      "epoch": 3.8483333333333336,
      "grad_norm": 0.2662586569786072,
      "learning_rate": 1.1516666666666668e-05,
      "loss": 0.0017,
      "step": 69270
    },
    {
      "epoch": 3.848888888888889,
      "grad_norm": 0.17370980978012085,
      "learning_rate": 1.1511111111111111e-05,
      "loss": 0.0039,
      "step": 69280
    },
    {
      "epoch": 3.8494444444444444,
      "grad_norm": 0.1183844655752182,
      "learning_rate": 1.1505555555555555e-05,
      "loss": 0.0033,
      "step": 69290
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.4834247827529907,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0029,
      "step": 69300
    },
    {
      "epoch": 3.8505555555555553,
      "grad_norm": 0.3979474604129791,
      "learning_rate": 1.1494444444444445e-05,
      "loss": 0.0036,
      "step": 69310
    },
    {
      "epoch": 3.851111111111111,
      "grad_norm": 0.47465774416923523,
      "learning_rate": 1.1488888888888889e-05,
      "loss": 0.0026,
      "step": 69320
    },
    {
      "epoch": 3.8516666666666666,
      "grad_norm": 0.11869698762893677,
      "learning_rate": 1.1483333333333334e-05,
      "loss": 0.0029,
      "step": 69330
    },
    {
      "epoch": 3.852222222222222,
      "grad_norm": 0.41477978229522705,
      "learning_rate": 1.147777777777778e-05,
      "loss": 0.002,
      "step": 69340
    },
    {
      "epoch": 3.852777777777778,
      "grad_norm": 0.27964529395103455,
      "learning_rate": 1.1472222222222223e-05,
      "loss": 0.0035,
      "step": 69350
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.2443399876356125,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.003,
      "step": 69360
    },
    {
      "epoch": 3.853888888888889,
      "grad_norm": 0.08937980979681015,
      "learning_rate": 1.1461111111111112e-05,
      "loss": 0.0025,
      "step": 69370
    },
    {
      "epoch": 3.8544444444444443,
      "grad_norm": 0.26667705178260803,
      "learning_rate": 1.1455555555555555e-05,
      "loss": 0.0019,
      "step": 69380
    },
    {
      "epoch": 3.855,
      "grad_norm": 0.1780853569507599,
      "learning_rate": 1.145e-05,
      "loss": 0.0035,
      "step": 69390
    },
    {
      "epoch": 3.8555555555555556,
      "grad_norm": 0.032137759029865265,
      "learning_rate": 1.1444444444444446e-05,
      "loss": 0.0032,
      "step": 69400
    },
    {
      "epoch": 3.8561111111111113,
      "grad_norm": 0.36086350679397583,
      "learning_rate": 1.143888888888889e-05,
      "loss": 0.0021,
      "step": 69410
    },
    {
      "epoch": 3.8566666666666665,
      "grad_norm": 0.14964701235294342,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.002,
      "step": 69420
    },
    {
      "epoch": 3.857222222222222,
      "grad_norm": 0.03725791722536087,
      "learning_rate": 1.1427777777777778e-05,
      "loss": 0.0023,
      "step": 69430
    },
    {
      "epoch": 3.8577777777777778,
      "grad_norm": 0.31997713446617126,
      "learning_rate": 1.1422222222222223e-05,
      "loss": 0.0033,
      "step": 69440
    },
    {
      "epoch": 3.8583333333333334,
      "grad_norm": 0.3930390477180481,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.0031,
      "step": 69450
    },
    {
      "epoch": 3.858888888888889,
      "grad_norm": 0.03196476399898529,
      "learning_rate": 1.141111111111111e-05,
      "loss": 0.0029,
      "step": 69460
    },
    {
      "epoch": 3.8594444444444447,
      "grad_norm": 0.09049157053232193,
      "learning_rate": 1.1405555555555556e-05,
      "loss": 0.002,
      "step": 69470
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.08966569602489471,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0033,
      "step": 69480
    },
    {
      "epoch": 3.8605555555555555,
      "grad_norm": 0.06250882893800735,
      "learning_rate": 1.1394444444444445e-05,
      "loss": 0.004,
      "step": 69490
    },
    {
      "epoch": 3.861111111111111,
      "grad_norm": 0.17851750552654266,
      "learning_rate": 1.138888888888889e-05,
      "loss": 0.0034,
      "step": 69500
    },
    {
      "epoch": 3.861666666666667,
      "grad_norm": 0.26672524213790894,
      "learning_rate": 1.1383333333333333e-05,
      "loss": 0.0039,
      "step": 69510
    },
    {
      "epoch": 3.862222222222222,
      "grad_norm": 0.2728712260723114,
      "learning_rate": 1.1377777777777779e-05,
      "loss": 0.0035,
      "step": 69520
    },
    {
      "epoch": 3.8627777777777776,
      "grad_norm": 0.047959428280591965,
      "learning_rate": 1.1372222222222224e-05,
      "loss": 0.0033,
      "step": 69530
    },
    {
      "epoch": 3.8633333333333333,
      "grad_norm": 0.35586845874786377,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0033,
      "step": 69540
    },
    {
      "epoch": 3.863888888888889,
      "grad_norm": 0.08935253322124481,
      "learning_rate": 1.1361111111111111e-05,
      "loss": 0.0034,
      "step": 69550
    },
    {
      "epoch": 3.8644444444444446,
      "grad_norm": 0.3271104693412781,
      "learning_rate": 1.1355555555555556e-05,
      "loss": 0.0026,
      "step": 69560
    },
    {
      "epoch": 3.865,
      "grad_norm": 0.09602663666009903,
      "learning_rate": 1.1350000000000001e-05,
      "loss": 0.004,
      "step": 69570
    },
    {
      "epoch": 3.8655555555555554,
      "grad_norm": 0.11926797777414322,
      "learning_rate": 1.1344444444444445e-05,
      "loss": 0.0025,
      "step": 69580
    },
    {
      "epoch": 3.866111111111111,
      "grad_norm": 0.03133618086576462,
      "learning_rate": 1.1338888888888889e-05,
      "loss": 0.0033,
      "step": 69590
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.3857076168060303,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0023,
      "step": 69600
    },
    {
      "epoch": 3.8672222222222223,
      "grad_norm": 0.11946478486061096,
      "learning_rate": 1.1327777777777779e-05,
      "loss": 0.0028,
      "step": 69610
    },
    {
      "epoch": 3.8677777777777775,
      "grad_norm": 0.1493491679430008,
      "learning_rate": 1.1322222222222223e-05,
      "loss": 0.0035,
      "step": 69620
    },
    {
      "epoch": 3.868333333333333,
      "grad_norm": 0.20835287868976593,
      "learning_rate": 1.1316666666666668e-05,
      "loss": 0.0027,
      "step": 69630
    },
    {
      "epoch": 3.868888888888889,
      "grad_norm": 0.08951056003570557,
      "learning_rate": 1.1311111111111111e-05,
      "loss": 0.0028,
      "step": 69640
    },
    {
      "epoch": 3.8694444444444445,
      "grad_norm": 0.06204052269458771,
      "learning_rate": 1.1305555555555557e-05,
      "loss": 0.0025,
      "step": 69650
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.11890123784542084,
      "learning_rate": 1.13e-05,
      "loss": 0.0041,
      "step": 69660
    },
    {
      "epoch": 3.8705555555555557,
      "grad_norm": 0.17755189538002014,
      "learning_rate": 1.1294444444444445e-05,
      "loss": 0.0029,
      "step": 69670
    },
    {
      "epoch": 3.871111111111111,
      "grad_norm": 0.05993114411830902,
      "learning_rate": 1.1288888888888889e-05,
      "loss": 0.0025,
      "step": 69680
    },
    {
      "epoch": 3.8716666666666666,
      "grad_norm": 0.08923690766096115,
      "learning_rate": 1.1283333333333333e-05,
      "loss": 0.0039,
      "step": 69690
    },
    {
      "epoch": 3.8722222222222222,
      "grad_norm": 0.4634911119937897,
      "learning_rate": 1.127777777777778e-05,
      "loss": 0.0021,
      "step": 69700
    },
    {
      "epoch": 3.872777777777778,
      "grad_norm": 0.32694974541664124,
      "learning_rate": 1.1272222222222223e-05,
      "loss": 0.0035,
      "step": 69710
    },
    {
      "epoch": 3.873333333333333,
      "grad_norm": 0.049332406371831894,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0029,
      "step": 69720
    },
    {
      "epoch": 3.8738888888888887,
      "grad_norm": 0.14981615543365479,
      "learning_rate": 1.1261111111111112e-05,
      "loss": 0.0022,
      "step": 69730
    },
    {
      "epoch": 3.8744444444444444,
      "grad_norm": 0.17859257757663727,
      "learning_rate": 1.1255555555555557e-05,
      "loss": 0.0032,
      "step": 69740
    },
    {
      "epoch": 3.875,
      "grad_norm": 0.2079855352640152,
      "learning_rate": 1.125e-05,
      "loss": 0.0017,
      "step": 69750
    },
    {
      "epoch": 3.8755555555555556,
      "grad_norm": 0.2408977895975113,
      "learning_rate": 1.1244444444444444e-05,
      "loss": 0.0017,
      "step": 69760
    },
    {
      "epoch": 3.8761111111111113,
      "grad_norm": 0.3572535812854767,
      "learning_rate": 1.123888888888889e-05,
      "loss": 0.0026,
      "step": 69770
    },
    {
      "epoch": 3.876666666666667,
      "grad_norm": 0.11941453069448471,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0034,
      "step": 69780
    },
    {
      "epoch": 3.877222222222222,
      "grad_norm": 0.27677449584007263,
      "learning_rate": 1.1227777777777778e-05,
      "loss": 0.0021,
      "step": 69790
    },
    {
      "epoch": 3.8777777777777778,
      "grad_norm": 0.08926067501306534,
      "learning_rate": 1.1222222222222224e-05,
      "loss": 0.0039,
      "step": 69800
    },
    {
      "epoch": 3.8783333333333334,
      "grad_norm": 0.25170665979385376,
      "learning_rate": 1.1216666666666667e-05,
      "loss": 0.0028,
      "step": 69810
    },
    {
      "epoch": 3.878888888888889,
      "grad_norm": 0.44405415654182434,
      "learning_rate": 1.121111111111111e-05,
      "loss": 0.0022,
      "step": 69820
    },
    {
      "epoch": 3.8794444444444443,
      "grad_norm": 0.15126579999923706,
      "learning_rate": 1.1205555555555556e-05,
      "loss": 0.0023,
      "step": 69830
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.01663803681731224,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0029,
      "step": 69840
    },
    {
      "epoch": 3.8805555555555555,
      "grad_norm": 0.12160814553499222,
      "learning_rate": 1.1194444444444445e-05,
      "loss": 0.0038,
      "step": 69850
    },
    {
      "epoch": 3.881111111111111,
      "grad_norm": 0.38678741455078125,
      "learning_rate": 1.1188888888888888e-05,
      "loss": 0.0028,
      "step": 69860
    },
    {
      "epoch": 3.881666666666667,
      "grad_norm": 0.44226816296577454,
      "learning_rate": 1.1183333333333335e-05,
      "loss": 0.0022,
      "step": 69870
    },
    {
      "epoch": 3.8822222222222225,
      "grad_norm": 0.06490378826856613,
      "learning_rate": 1.1177777777777779e-05,
      "loss": 0.0029,
      "step": 69880
    },
    {
      "epoch": 3.8827777777777777,
      "grad_norm": 0.09076309949159622,
      "learning_rate": 1.1172222222222222e-05,
      "loss": 0.0029,
      "step": 69890
    },
    {
      "epoch": 3.8833333333333333,
      "grad_norm": 0.14953798055648804,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0023,
      "step": 69900
    },
    {
      "epoch": 3.883888888888889,
      "grad_norm": 0.03466698154807091,
      "learning_rate": 1.1161111111111111e-05,
      "loss": 0.0025,
      "step": 69910
    },
    {
      "epoch": 3.8844444444444446,
      "grad_norm": 0.2872682213783264,
      "learning_rate": 1.1155555555555556e-05,
      "loss": 0.0033,
      "step": 69920
    },
    {
      "epoch": 3.885,
      "grad_norm": 0.267732173204422,
      "learning_rate": 1.115e-05,
      "loss": 0.0036,
      "step": 69930
    },
    {
      "epoch": 3.8855555555555554,
      "grad_norm": 0.01374506950378418,
      "learning_rate": 1.1144444444444445e-05,
      "loss": 0.0026,
      "step": 69940
    },
    {
      "epoch": 3.886111111111111,
      "grad_norm": 0.26455581188201904,
      "learning_rate": 1.1138888888888889e-05,
      "loss": 0.0038,
      "step": 69950
    },
    {
      "epoch": 3.8866666666666667,
      "grad_norm": 0.00736872898414731,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0032,
      "step": 69960
    },
    {
      "epoch": 3.8872222222222224,
      "grad_norm": 0.3256414830684662,
      "learning_rate": 1.112777777777778e-05,
      "loss": 0.0027,
      "step": 69970
    },
    {
      "epoch": 3.887777777777778,
      "grad_norm": 0.326541543006897,
      "learning_rate": 1.1122222222222223e-05,
      "loss": 0.0025,
      "step": 69980
    },
    {
      "epoch": 3.888333333333333,
      "grad_norm": 0.23724456131458282,
      "learning_rate": 1.1116666666666666e-05,
      "loss": 0.0023,
      "step": 69990
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.1531296819448471,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.0025,
      "step": 70000
    },
    {
      "epoch": 3.8894444444444445,
      "grad_norm": 0.1208840012550354,
      "learning_rate": 1.1105555555555557e-05,
      "loss": 0.0023,
      "step": 70010
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.148239865899086,
      "learning_rate": 1.11e-05,
      "loss": 0.0025,
      "step": 70020
    },
    {
      "epoch": 3.8905555555555553,
      "grad_norm": 0.3559361398220062,
      "learning_rate": 1.1094444444444444e-05,
      "loss": 0.0026,
      "step": 70030
    },
    {
      "epoch": 3.891111111111111,
      "grad_norm": 0.12290108948945999,
      "learning_rate": 1.108888888888889e-05,
      "loss": 0.0025,
      "step": 70040
    },
    {
      "epoch": 3.8916666666666666,
      "grad_norm": 0.027830637991428375,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.0034,
      "step": 70050
    },
    {
      "epoch": 3.8922222222222222,
      "grad_norm": 0.013110392726957798,
      "learning_rate": 1.1077777777777778e-05,
      "loss": 0.0031,
      "step": 70060
    },
    {
      "epoch": 3.892777777777778,
      "grad_norm": 0.2084072232246399,
      "learning_rate": 1.1072222222222223e-05,
      "loss": 0.0036,
      "step": 70070
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.010292655788362026,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0033,
      "step": 70080
    },
    {
      "epoch": 3.8938888888888887,
      "grad_norm": 0.9324000477790833,
      "learning_rate": 1.106111111111111e-05,
      "loss": 0.0025,
      "step": 70090
    },
    {
      "epoch": 3.8944444444444444,
      "grad_norm": 0.031455136835575104,
      "learning_rate": 1.1055555555555556e-05,
      "loss": 0.0031,
      "step": 70100
    },
    {
      "epoch": 3.895,
      "grad_norm": 0.2677202820777893,
      "learning_rate": 1.1050000000000001e-05,
      "loss": 0.0029,
      "step": 70110
    },
    {
      "epoch": 3.8955555555555557,
      "grad_norm": 0.148765429854393,
      "learning_rate": 1.1044444444444444e-05,
      "loss": 0.0027,
      "step": 70120
    },
    {
      "epoch": 3.896111111111111,
      "grad_norm": 0.1194978803396225,
      "learning_rate": 1.103888888888889e-05,
      "loss": 0.0032,
      "step": 70130
    },
    {
      "epoch": 3.8966666666666665,
      "grad_norm": 0.17813102900981903,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.0024,
      "step": 70140
    },
    {
      "epoch": 3.897222222222222,
      "grad_norm": 0.11939693242311478,
      "learning_rate": 1.1027777777777779e-05,
      "loss": 0.003,
      "step": 70150
    },
    {
      "epoch": 3.897777777777778,
      "grad_norm": 0.08946458250284195,
      "learning_rate": 1.1022222222222222e-05,
      "loss": 0.0026,
      "step": 70160
    },
    {
      "epoch": 3.8983333333333334,
      "grad_norm": 0.06083571910858154,
      "learning_rate": 1.1016666666666667e-05,
      "loss": 0.0024,
      "step": 70170
    },
    {
      "epoch": 3.898888888888889,
      "grad_norm": 0.20788756012916565,
      "learning_rate": 1.1011111111111113e-05,
      "loss": 0.0029,
      "step": 70180
    },
    {
      "epoch": 3.8994444444444447,
      "grad_norm": 0.5042006969451904,
      "learning_rate": 1.1005555555555556e-05,
      "loss": 0.0019,
      "step": 70190
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.2372094839811325,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0027,
      "step": 70200
    },
    {
      "epoch": 3.9005555555555556,
      "grad_norm": 0.03585712984204292,
      "learning_rate": 1.0994444444444445e-05,
      "loss": 0.0024,
      "step": 70210
    },
    {
      "epoch": 3.901111111111111,
      "grad_norm": 0.23718322813510895,
      "learning_rate": 1.0988888888888889e-05,
      "loss": 0.0023,
      "step": 70220
    },
    {
      "epoch": 3.9016666666666664,
      "grad_norm": 0.4316408932209015,
      "learning_rate": 1.0983333333333334e-05,
      "loss": 0.0024,
      "step": 70230
    },
    {
      "epoch": 3.902222222222222,
      "grad_norm": 0.03167898580431938,
      "learning_rate": 1.0977777777777779e-05,
      "loss": 0.0029,
      "step": 70240
    },
    {
      "epoch": 3.9027777777777777,
      "grad_norm": 0.11854684352874756,
      "learning_rate": 1.0972222222222223e-05,
      "loss": 0.0022,
      "step": 70250
    },
    {
      "epoch": 3.9033333333333333,
      "grad_norm": 0.15334036946296692,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.0025,
      "step": 70260
    },
    {
      "epoch": 3.903888888888889,
      "grad_norm": 0.268414705991745,
      "learning_rate": 1.0961111111111113e-05,
      "loss": 0.0037,
      "step": 70270
    },
    {
      "epoch": 3.9044444444444446,
      "grad_norm": 0.031373970210552216,
      "learning_rate": 1.0955555555555557e-05,
      "loss": 0.0026,
      "step": 70280
    },
    {
      "epoch": 3.9050000000000002,
      "grad_norm": 0.14871104061603546,
      "learning_rate": 1.095e-05,
      "loss": 0.0046,
      "step": 70290
    },
    {
      "epoch": 3.9055555555555554,
      "grad_norm": 0.1783989667892456,
      "learning_rate": 1.0944444444444445e-05,
      "loss": 0.0024,
      "step": 70300
    },
    {
      "epoch": 3.906111111111111,
      "grad_norm": 0.033485040068626404,
      "learning_rate": 1.0938888888888889e-05,
      "loss": 0.0031,
      "step": 70310
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.11998837441205978,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0042,
      "step": 70320
    },
    {
      "epoch": 3.9072222222222224,
      "grad_norm": 0.38506031036376953,
      "learning_rate": 1.0927777777777778e-05,
      "loss": 0.0037,
      "step": 70330
    },
    {
      "epoch": 3.9077777777777776,
      "grad_norm": 0.32751238346099854,
      "learning_rate": 1.0922222222222223e-05,
      "loss": 0.0032,
      "step": 70340
    },
    {
      "epoch": 3.908333333333333,
      "grad_norm": 0.17950080335140228,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.0039,
      "step": 70350
    },
    {
      "epoch": 3.908888888888889,
      "grad_norm": 0.14938482642173767,
      "learning_rate": 1.0911111111111112e-05,
      "loss": 0.0027,
      "step": 70360
    },
    {
      "epoch": 3.9094444444444445,
      "grad_norm": 0.06069125607609749,
      "learning_rate": 1.0905555555555557e-05,
      "loss": 0.0026,
      "step": 70370
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.0905095636844635,
      "learning_rate": 1.09e-05,
      "loss": 0.0024,
      "step": 70380
    },
    {
      "epoch": 3.910555555555556,
      "grad_norm": 0.0996549054980278,
      "learning_rate": 1.0894444444444444e-05,
      "loss": 0.0032,
      "step": 70390
    },
    {
      "epoch": 3.911111111111111,
      "grad_norm": 0.1805524080991745,
      "learning_rate": 1.088888888888889e-05,
      "loss": 0.0041,
      "step": 70400
    },
    {
      "epoch": 3.9116666666666666,
      "grad_norm": 0.03163860738277435,
      "learning_rate": 1.0883333333333335e-05,
      "loss": 0.0018,
      "step": 70410
    },
    {
      "epoch": 3.9122222222222223,
      "grad_norm": 0.013468023389577866,
      "learning_rate": 1.0877777777777778e-05,
      "loss": 0.0034,
      "step": 70420
    },
    {
      "epoch": 3.912777777777778,
      "grad_norm": 0.4835708737373352,
      "learning_rate": 1.0872222222222222e-05,
      "loss": 0.0033,
      "step": 70430
    },
    {
      "epoch": 3.913333333333333,
      "grad_norm": 0.23723913729190826,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0031,
      "step": 70440
    },
    {
      "epoch": 3.9138888888888888,
      "grad_norm": 0.566091775894165,
      "learning_rate": 1.0861111111111112e-05,
      "loss": 0.0027,
      "step": 70450
    },
    {
      "epoch": 3.9144444444444444,
      "grad_norm": 0.32718825340270996,
      "learning_rate": 1.0855555555555556e-05,
      "loss": 0.0022,
      "step": 70460
    },
    {
      "epoch": 3.915,
      "grad_norm": 0.11980688571929932,
      "learning_rate": 1.0850000000000001e-05,
      "loss": 0.0027,
      "step": 70470
    },
    {
      "epoch": 3.9155555555555557,
      "grad_norm": 0.2369832992553711,
      "learning_rate": 1.0844444444444445e-05,
      "loss": 0.002,
      "step": 70480
    },
    {
      "epoch": 3.9161111111111113,
      "grad_norm": 0.23819176852703094,
      "learning_rate": 1.083888888888889e-05,
      "loss": 0.0028,
      "step": 70490
    },
    {
      "epoch": 3.9166666666666665,
      "grad_norm": 0.20777416229248047,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0017,
      "step": 70500
    },
    {
      "epoch": 3.917222222222222,
      "grad_norm": 0.17896780371665955,
      "learning_rate": 1.0827777777777779e-05,
      "loss": 0.004,
      "step": 70510
    },
    {
      "epoch": 3.917777777777778,
      "grad_norm": 0.28272178769111633,
      "learning_rate": 1.0822222222222222e-05,
      "loss": 0.0031,
      "step": 70520
    },
    {
      "epoch": 3.9183333333333334,
      "grad_norm": 0.04026414826512337,
      "learning_rate": 1.0816666666666666e-05,
      "loss": 0.0029,
      "step": 70530
    },
    {
      "epoch": 3.9188888888888886,
      "grad_norm": 0.2075708508491516,
      "learning_rate": 1.0811111111111113e-05,
      "loss": 0.0036,
      "step": 70540
    },
    {
      "epoch": 3.9194444444444443,
      "grad_norm": 0.0945039615035057,
      "learning_rate": 1.0805555555555556e-05,
      "loss": 0.004,
      "step": 70550
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.2674584984779358,
      "learning_rate": 1.08e-05,
      "loss": 0.0023,
      "step": 70560
    },
    {
      "epoch": 3.9205555555555556,
      "grad_norm": 0.039924439042806625,
      "learning_rate": 1.0794444444444445e-05,
      "loss": 0.0027,
      "step": 70570
    },
    {
      "epoch": 3.921111111111111,
      "grad_norm": 0.11963320523500443,
      "learning_rate": 1.078888888888889e-05,
      "loss": 0.0019,
      "step": 70580
    },
    {
      "epoch": 3.921666666666667,
      "grad_norm": 0.1290477216243744,
      "learning_rate": 1.0783333333333334e-05,
      "loss": 0.0025,
      "step": 70590
    },
    {
      "epoch": 3.9222222222222225,
      "grad_norm": 0.32106804847717285,
      "learning_rate": 1.0777777777777778e-05,
      "loss": 0.0032,
      "step": 70600
    },
    {
      "epoch": 3.9227777777777777,
      "grad_norm": 0.03786999359726906,
      "learning_rate": 1.0772222222222223e-05,
      "loss": 0.0023,
      "step": 70610
    },
    {
      "epoch": 3.9233333333333333,
      "grad_norm": 0.06326085329055786,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0038,
      "step": 70620
    },
    {
      "epoch": 3.923888888888889,
      "grad_norm": 0.14829802513122559,
      "learning_rate": 1.0761111111111112e-05,
      "loss": 0.0036,
      "step": 70630
    },
    {
      "epoch": 3.924444444444444,
      "grad_norm": 0.18184341490268707,
      "learning_rate": 1.0755555555555557e-05,
      "loss": 0.003,
      "step": 70640
    },
    {
      "epoch": 3.925,
      "grad_norm": 0.009827449917793274,
      "learning_rate": 1.075e-05,
      "loss": 0.0016,
      "step": 70650
    },
    {
      "epoch": 3.9255555555555555,
      "grad_norm": 0.09088891744613647,
      "learning_rate": 1.0744444444444444e-05,
      "loss": 0.0024,
      "step": 70660
    },
    {
      "epoch": 3.926111111111111,
      "grad_norm": 0.30004680156707764,
      "learning_rate": 1.073888888888889e-05,
      "loss": 0.0021,
      "step": 70670
    },
    {
      "epoch": 3.9266666666666667,
      "grad_norm": 0.06123761460185051,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0031,
      "step": 70680
    },
    {
      "epoch": 3.9272222222222224,
      "grad_norm": 0.19087858498096466,
      "learning_rate": 1.0727777777777778e-05,
      "loss": 0.0024,
      "step": 70690
    },
    {
      "epoch": 3.927777777777778,
      "grad_norm": 0.031767647713422775,
      "learning_rate": 1.0722222222222222e-05,
      "loss": 0.0043,
      "step": 70700
    },
    {
      "epoch": 3.9283333333333332,
      "grad_norm": 0.03190295398235321,
      "learning_rate": 1.0716666666666667e-05,
      "loss": 0.0022,
      "step": 70710
    },
    {
      "epoch": 3.928888888888889,
      "grad_norm": 0.060916490852832794,
      "learning_rate": 1.0711111111111112e-05,
      "loss": 0.0024,
      "step": 70720
    },
    {
      "epoch": 3.9294444444444445,
      "grad_norm": 0.47434496879577637,
      "learning_rate": 1.0705555555555556e-05,
      "loss": 0.0035,
      "step": 70730
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.1501195877790451,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.0029,
      "step": 70740
    },
    {
      "epoch": 3.9305555555555554,
      "grad_norm": 0.1488443911075592,
      "learning_rate": 1.0694444444444444e-05,
      "loss": 0.0032,
      "step": 70750
    },
    {
      "epoch": 3.931111111111111,
      "grad_norm": 0.05565780773758888,
      "learning_rate": 1.068888888888889e-05,
      "loss": 0.0037,
      "step": 70760
    },
    {
      "epoch": 3.9316666666666666,
      "grad_norm": 0.08933831751346588,
      "learning_rate": 1.0683333333333333e-05,
      "loss": 0.0025,
      "step": 70770
    },
    {
      "epoch": 3.9322222222222223,
      "grad_norm": 0.14540183544158936,
      "learning_rate": 1.0677777777777779e-05,
      "loss": 0.0034,
      "step": 70780
    },
    {
      "epoch": 3.932777777777778,
      "grad_norm": 0.35723572969436646,
      "learning_rate": 1.0672222222222222e-05,
      "loss": 0.0027,
      "step": 70790
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.14981745183467865,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0032,
      "step": 70800
    },
    {
      "epoch": 3.9338888888888888,
      "grad_norm": 0.24436889588832855,
      "learning_rate": 1.0661111111111113e-05,
      "loss": 0.003,
      "step": 70810
    },
    {
      "epoch": 3.9344444444444444,
      "grad_norm": 0.03474762663245201,
      "learning_rate": 1.0655555555555556e-05,
      "loss": 0.0026,
      "step": 70820
    },
    {
      "epoch": 3.935,
      "grad_norm": 0.4192934036254883,
      "learning_rate": 1.065e-05,
      "loss": 0.003,
      "step": 70830
    },
    {
      "epoch": 3.9355555555555557,
      "grad_norm": 0.17847730219364166,
      "learning_rate": 1.0644444444444445e-05,
      "loss": 0.0033,
      "step": 70840
    },
    {
      "epoch": 3.936111111111111,
      "grad_norm": 0.11906392127275467,
      "learning_rate": 1.063888888888889e-05,
      "loss": 0.0024,
      "step": 70850
    },
    {
      "epoch": 3.9366666666666665,
      "grad_norm": 0.0640302523970604,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0018,
      "step": 70860
    },
    {
      "epoch": 3.937222222222222,
      "grad_norm": 0.23827216029167175,
      "learning_rate": 1.0627777777777779e-05,
      "loss": 0.0036,
      "step": 70870
    },
    {
      "epoch": 3.937777777777778,
      "grad_norm": 0.1054411306977272,
      "learning_rate": 1.0622222222222223e-05,
      "loss": 0.0021,
      "step": 70880
    },
    {
      "epoch": 3.9383333333333335,
      "grad_norm": 0.2770063579082489,
      "learning_rate": 1.0616666666666668e-05,
      "loss": 0.0036,
      "step": 70890
    },
    {
      "epoch": 3.938888888888889,
      "grad_norm": 0.12023507058620453,
      "learning_rate": 1.0611111111111111e-05,
      "loss": 0.0025,
      "step": 70900
    },
    {
      "epoch": 3.9394444444444443,
      "grad_norm": 0.2190203070640564,
      "learning_rate": 1.0605555555555557e-05,
      "loss": 0.0033,
      "step": 70910
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.17836317420005798,
      "learning_rate": 1.06e-05,
      "loss": 0.0026,
      "step": 70920
    },
    {
      "epoch": 3.9405555555555556,
      "grad_norm": 0.26708027720451355,
      "learning_rate": 1.0594444444444444e-05,
      "loss": 0.0014,
      "step": 70930
    },
    {
      "epoch": 3.9411111111111112,
      "grad_norm": 0.17920343577861786,
      "learning_rate": 1.058888888888889e-05,
      "loss": 0.0025,
      "step": 70940
    },
    {
      "epoch": 3.9416666666666664,
      "grad_norm": 0.148830384016037,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.003,
      "step": 70950
    },
    {
      "epoch": 3.942222222222222,
      "grad_norm": 0.3576195240020752,
      "learning_rate": 1.0577777777777778e-05,
      "loss": 0.0031,
      "step": 70960
    },
    {
      "epoch": 3.9427777777777777,
      "grad_norm": 0.7141954898834229,
      "learning_rate": 1.0572222222222223e-05,
      "loss": 0.003,
      "step": 70970
    },
    {
      "epoch": 3.9433333333333334,
      "grad_norm": 0.20727765560150146,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0028,
      "step": 70980
    },
    {
      "epoch": 3.943888888888889,
      "grad_norm": 0.41517189145088196,
      "learning_rate": 1.0561111111111112e-05,
      "loss": 0.0025,
      "step": 70990
    },
    {
      "epoch": 3.9444444444444446,
      "grad_norm": 0.12036281824111938,
      "learning_rate": 1.0555555555555555e-05,
      "loss": 0.0022,
      "step": 71000
    },
    {
      "epoch": 3.945,
      "grad_norm": 0.1618226170539856,
      "learning_rate": 1.055e-05,
      "loss": 0.0033,
      "step": 71010
    },
    {
      "epoch": 3.9455555555555555,
      "grad_norm": 0.23813024163246155,
      "learning_rate": 1.0544444444444444e-05,
      "loss": 0.0033,
      "step": 71020
    },
    {
      "epoch": 3.946111111111111,
      "grad_norm": 0.4445020854473114,
      "learning_rate": 1.053888888888889e-05,
      "loss": 0.0019,
      "step": 71030
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.19942231476306915,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0024,
      "step": 71040
    },
    {
      "epoch": 3.947222222222222,
      "grad_norm": 0.21332350373268127,
      "learning_rate": 1.0527777777777778e-05,
      "loss": 0.0037,
      "step": 71050
    },
    {
      "epoch": 3.9477777777777776,
      "grad_norm": 0.4470033645629883,
      "learning_rate": 1.0522222222222222e-05,
      "loss": 0.0035,
      "step": 71060
    },
    {
      "epoch": 3.9483333333333333,
      "grad_norm": 0.03509962558746338,
      "learning_rate": 1.0516666666666667e-05,
      "loss": 0.0028,
      "step": 71070
    },
    {
      "epoch": 3.948888888888889,
      "grad_norm": 0.23946234583854675,
      "learning_rate": 1.0511111111111112e-05,
      "loss": 0.0028,
      "step": 71080
    },
    {
      "epoch": 3.9494444444444445,
      "grad_norm": 0.6150444149971008,
      "learning_rate": 1.0505555555555556e-05,
      "loss": 0.0024,
      "step": 71090
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.2960408329963684,
      "learning_rate": 1.05e-05,
      "loss": 0.0029,
      "step": 71100
    },
    {
      "epoch": 3.950555555555556,
      "grad_norm": 0.11869100481271744,
      "learning_rate": 1.0494444444444446e-05,
      "loss": 0.0028,
      "step": 71110
    },
    {
      "epoch": 3.951111111111111,
      "grad_norm": 0.2075461745262146,
      "learning_rate": 1.048888888888889e-05,
      "loss": 0.0035,
      "step": 71120
    },
    {
      "epoch": 3.9516666666666667,
      "grad_norm": 0.11906129866838455,
      "learning_rate": 1.0483333333333333e-05,
      "loss": 0.003,
      "step": 71130
    },
    {
      "epoch": 3.9522222222222223,
      "grad_norm": 0.13552135229110718,
      "learning_rate": 1.0477777777777779e-05,
      "loss": 0.0024,
      "step": 71140
    },
    {
      "epoch": 3.9527777777777775,
      "grad_norm": 0.23714250326156616,
      "learning_rate": 1.0472222222222222e-05,
      "loss": 0.0034,
      "step": 71150
    },
    {
      "epoch": 3.953333333333333,
      "grad_norm": 0.11847206950187683,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0039,
      "step": 71160
    },
    {
      "epoch": 3.953888888888889,
      "grad_norm": 0.3557594120502472,
      "learning_rate": 1.0461111111111111e-05,
      "loss": 0.0016,
      "step": 71170
    },
    {
      "epoch": 3.9544444444444444,
      "grad_norm": 0.23673076927661896,
      "learning_rate": 1.0455555555555556e-05,
      "loss": 0.0024,
      "step": 71180
    },
    {
      "epoch": 3.955,
      "grad_norm": 0.20840097963809967,
      "learning_rate": 1.045e-05,
      "loss": 0.0021,
      "step": 71190
    },
    {
      "epoch": 3.9555555555555557,
      "grad_norm": 0.177886962890625,
      "learning_rate": 1.0444444444444445e-05,
      "loss": 0.0026,
      "step": 71200
    },
    {
      "epoch": 3.9561111111111114,
      "grad_norm": 0.03274649381637573,
      "learning_rate": 1.043888888888889e-05,
      "loss": 0.0022,
      "step": 71210
    },
    {
      "epoch": 3.9566666666666666,
      "grad_norm": 0.2083551287651062,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.0027,
      "step": 71220
    },
    {
      "epoch": 3.957222222222222,
      "grad_norm": 0.32669419050216675,
      "learning_rate": 1.0427777777777778e-05,
      "loss": 0.0031,
      "step": 71230
    },
    {
      "epoch": 3.957777777777778,
      "grad_norm": 0.22682195901870728,
      "learning_rate": 1.0422222222222223e-05,
      "loss": 0.0027,
      "step": 71240
    },
    {
      "epoch": 3.9583333333333335,
      "grad_norm": 0.13351812958717346,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.003,
      "step": 71250
    },
    {
      "epoch": 3.9588888888888887,
      "grad_norm": 0.11246153712272644,
      "learning_rate": 1.0411111111111112e-05,
      "loss": 0.003,
      "step": 71260
    },
    {
      "epoch": 3.9594444444444443,
      "grad_norm": 0.12633417546749115,
      "learning_rate": 1.0405555555555555e-05,
      "loss": 0.0028,
      "step": 71270
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.03594702109694481,
      "learning_rate": 1.04e-05,
      "loss": 0.0028,
      "step": 71280
    },
    {
      "epoch": 3.9605555555555556,
      "grad_norm": 0.2673527002334595,
      "learning_rate": 1.0394444444444446e-05,
      "loss": 0.0028,
      "step": 71290
    },
    {
      "epoch": 3.9611111111111112,
      "grad_norm": 0.14817892014980316,
      "learning_rate": 1.038888888888889e-05,
      "loss": 0.0024,
      "step": 71300
    },
    {
      "epoch": 3.961666666666667,
      "grad_norm": 0.17677392065525055,
      "learning_rate": 1.0383333333333334e-05,
      "loss": 0.0023,
      "step": 71310
    },
    {
      "epoch": 3.962222222222222,
      "grad_norm": 0.23732149600982666,
      "learning_rate": 1.0377777777777778e-05,
      "loss": 0.0023,
      "step": 71320
    },
    {
      "epoch": 3.9627777777777777,
      "grad_norm": 0.20723484456539154,
      "learning_rate": 1.0372222222222222e-05,
      "loss": 0.0034,
      "step": 71330
    },
    {
      "epoch": 3.9633333333333334,
      "grad_norm": 0.2086496502161026,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0032,
      "step": 71340
    },
    {
      "epoch": 3.963888888888889,
      "grad_norm": 0.355589896440506,
      "learning_rate": 1.0361111111111112e-05,
      "loss": 0.0017,
      "step": 71350
    },
    {
      "epoch": 3.964444444444444,
      "grad_norm": 0.4758807420730591,
      "learning_rate": 1.0355555555555556e-05,
      "loss": 0.0043,
      "step": 71360
    },
    {
      "epoch": 3.965,
      "grad_norm": 0.178680881857872,
      "learning_rate": 1.035e-05,
      "loss": 0.0032,
      "step": 71370
    },
    {
      "epoch": 3.9655555555555555,
      "grad_norm": 0.06035211682319641,
      "learning_rate": 1.0344444444444446e-05,
      "loss": 0.0028,
      "step": 71380
    },
    {
      "epoch": 3.966111111111111,
      "grad_norm": 0.06232951954007149,
      "learning_rate": 1.033888888888889e-05,
      "loss": 0.0027,
      "step": 71390
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.4750346839427948,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.002,
      "step": 71400
    },
    {
      "epoch": 3.9672222222222224,
      "grad_norm": 0.009457172825932503,
      "learning_rate": 1.0327777777777778e-05,
      "loss": 0.0027,
      "step": 71410
    },
    {
      "epoch": 3.9677777777777776,
      "grad_norm": 0.14833961427211761,
      "learning_rate": 1.0322222222222224e-05,
      "loss": 0.0034,
      "step": 71420
    },
    {
      "epoch": 3.9683333333333333,
      "grad_norm": 0.06572555750608444,
      "learning_rate": 1.0316666666666667e-05,
      "loss": 0.003,
      "step": 71430
    },
    {
      "epoch": 3.968888888888889,
      "grad_norm": 0.08949365466833115,
      "learning_rate": 1.031111111111111e-05,
      "loss": 0.0023,
      "step": 71440
    },
    {
      "epoch": 3.9694444444444446,
      "grad_norm": 0.011389026418328285,
      "learning_rate": 1.0305555555555556e-05,
      "loss": 0.0019,
      "step": 71450
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 0.436423659324646,
      "learning_rate": 1.03e-05,
      "loss": 0.0016,
      "step": 71460
    },
    {
      "epoch": 3.9705555555555554,
      "grad_norm": 0.03552153334021568,
      "learning_rate": 1.0294444444444445e-05,
      "loss": 0.0023,
      "step": 71470
    },
    {
      "epoch": 3.971111111111111,
      "grad_norm": 0.06100555509328842,
      "learning_rate": 1.028888888888889e-05,
      "loss": 0.0032,
      "step": 71480
    },
    {
      "epoch": 3.9716666666666667,
      "grad_norm": 0.23736301064491272,
      "learning_rate": 1.0283333333333334e-05,
      "loss": 0.0026,
      "step": 71490
    },
    {
      "epoch": 3.9722222222222223,
      "grad_norm": 0.05633915215730667,
      "learning_rate": 1.0277777777777777e-05,
      "loss": 0.0023,
      "step": 71500
    },
    {
      "epoch": 3.972777777777778,
      "grad_norm": 0.1521969884634018,
      "learning_rate": 1.0272222222222224e-05,
      "loss": 0.0024,
      "step": 71510
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.17878246307373047,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0028,
      "step": 71520
    },
    {
      "epoch": 3.973888888888889,
      "grad_norm": 0.23694775998592377,
      "learning_rate": 1.0261111111111111e-05,
      "loss": 0.0029,
      "step": 71530
    },
    {
      "epoch": 3.9744444444444444,
      "grad_norm": 0.11916695535182953,
      "learning_rate": 1.0255555555555557e-05,
      "loss": 0.0017,
      "step": 71540
    },
    {
      "epoch": 3.975,
      "grad_norm": 0.479731023311615,
      "learning_rate": 1.025e-05,
      "loss": 0.003,
      "step": 71550
    },
    {
      "epoch": 3.9755555555555553,
      "grad_norm": 0.08949302136898041,
      "learning_rate": 1.0244444444444445e-05,
      "loss": 0.0033,
      "step": 71560
    },
    {
      "epoch": 3.976111111111111,
      "grad_norm": 0.2681404948234558,
      "learning_rate": 1.0238888888888889e-05,
      "loss": 0.003,
      "step": 71570
    },
    {
      "epoch": 3.9766666666666666,
      "grad_norm": 0.2100873738527298,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.0028,
      "step": 71580
    },
    {
      "epoch": 3.977222222222222,
      "grad_norm": 0.09109272807836533,
      "learning_rate": 1.0227777777777778e-05,
      "loss": 0.0019,
      "step": 71590
    },
    {
      "epoch": 3.977777777777778,
      "grad_norm": 0.17871743440628052,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 0.0017,
      "step": 71600
    },
    {
      "epoch": 3.9783333333333335,
      "grad_norm": 0.12054014205932617,
      "learning_rate": 1.0216666666666668e-05,
      "loss": 0.0042,
      "step": 71610
    },
    {
      "epoch": 3.978888888888889,
      "grad_norm": 0.09455908089876175,
      "learning_rate": 1.0211111111111112e-05,
      "loss": 0.0029,
      "step": 71620
    },
    {
      "epoch": 3.9794444444444443,
      "grad_norm": 0.2665155231952667,
      "learning_rate": 1.0205555555555555e-05,
      "loss": 0.0019,
      "step": 71630
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.17900975048542023,
      "learning_rate": 1.02e-05,
      "loss": 0.0027,
      "step": 71640
    },
    {
      "epoch": 3.9805555555555556,
      "grad_norm": 0.014578488655388355,
      "learning_rate": 1.0194444444444446e-05,
      "loss": 0.0025,
      "step": 71650
    },
    {
      "epoch": 3.9811111111111113,
      "grad_norm": 0.013393409550189972,
      "learning_rate": 1.018888888888889e-05,
      "loss": 0.0022,
      "step": 71660
    },
    {
      "epoch": 3.9816666666666665,
      "grad_norm": 0.1478853076696396,
      "learning_rate": 1.0183333333333333e-05,
      "loss": 0.0038,
      "step": 71670
    },
    {
      "epoch": 3.982222222222222,
      "grad_norm": 0.05050298571586609,
      "learning_rate": 1.0177777777777778e-05,
      "loss": 0.0025,
      "step": 71680
    },
    {
      "epoch": 3.9827777777777778,
      "grad_norm": 0.330643892288208,
      "learning_rate": 1.0172222222222223e-05,
      "loss": 0.0036,
      "step": 71690
    },
    {
      "epoch": 3.9833333333333334,
      "grad_norm": 0.24770177900791168,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0026,
      "step": 71700
    },
    {
      "epoch": 3.983888888888889,
      "grad_norm": 0.6731760501861572,
      "learning_rate": 1.0161111111111112e-05,
      "loss": 0.0026,
      "step": 71710
    },
    {
      "epoch": 3.9844444444444447,
      "grad_norm": 0.14897002279758453,
      "learning_rate": 1.0155555555555556e-05,
      "loss": 0.0021,
      "step": 71720
    },
    {
      "epoch": 3.985,
      "grad_norm": 0.033142995089292526,
      "learning_rate": 1.0150000000000001e-05,
      "loss": 0.0022,
      "step": 71730
    },
    {
      "epoch": 3.9855555555555555,
      "grad_norm": 0.41344618797302246,
      "learning_rate": 1.0144444444444445e-05,
      "loss": 0.0022,
      "step": 71740
    },
    {
      "epoch": 3.986111111111111,
      "grad_norm": 0.013968488201498985,
      "learning_rate": 1.013888888888889e-05,
      "loss": 0.0032,
      "step": 71750
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.03081720694899559,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0031,
      "step": 71760
    },
    {
      "epoch": 3.987222222222222,
      "grad_norm": 0.009543932043015957,
      "learning_rate": 1.0127777777777777e-05,
      "loss": 0.0031,
      "step": 71770
    },
    {
      "epoch": 3.9877777777777776,
      "grad_norm": 0.4164050221443176,
      "learning_rate": 1.0122222222222224e-05,
      "loss": 0.0026,
      "step": 71780
    },
    {
      "epoch": 3.9883333333333333,
      "grad_norm": 0.5038956999778748,
      "learning_rate": 1.0116666666666667e-05,
      "loss": 0.0027,
      "step": 71790
    },
    {
      "epoch": 3.988888888888889,
      "grad_norm": 0.13820864260196686,
      "learning_rate": 1.0111111111111111e-05,
      "loss": 0.0023,
      "step": 71800
    },
    {
      "epoch": 3.9894444444444446,
      "grad_norm": 0.11970365792512894,
      "learning_rate": 1.0105555555555556e-05,
      "loss": 0.0035,
      "step": 71810
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.17845705151557922,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.0027,
      "step": 71820
    },
    {
      "epoch": 3.9905555555555554,
      "grad_norm": 0.47444948554039,
      "learning_rate": 1.0094444444444445e-05,
      "loss": 0.0026,
      "step": 71830
    },
    {
      "epoch": 3.991111111111111,
      "grad_norm": 0.03312736749649048,
      "learning_rate": 1.0088888888888889e-05,
      "loss": 0.0023,
      "step": 71840
    },
    {
      "epoch": 3.9916666666666667,
      "grad_norm": 0.20549097657203674,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.0028,
      "step": 71850
    },
    {
      "epoch": 3.9922222222222223,
      "grad_norm": 0.09012208878993988,
      "learning_rate": 1.0077777777777777e-05,
      "loss": 0.0021,
      "step": 71860
    },
    {
      "epoch": 3.9927777777777775,
      "grad_norm": 0.0644521415233612,
      "learning_rate": 1.0072222222222223e-05,
      "loss": 0.0046,
      "step": 71870
    },
    {
      "epoch": 3.993333333333333,
      "grad_norm": 0.40581798553466797,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0025,
      "step": 71880
    },
    {
      "epoch": 3.993888888888889,
      "grad_norm": 0.12084925919771194,
      "learning_rate": 1.0061111111111112e-05,
      "loss": 0.0044,
      "step": 71890
    },
    {
      "epoch": 3.9944444444444445,
      "grad_norm": 0.08988766372203827,
      "learning_rate": 1.0055555555555555e-05,
      "loss": 0.0026,
      "step": 71900
    },
    {
      "epoch": 3.995,
      "grad_norm": 0.6813454031944275,
      "learning_rate": 1.005e-05,
      "loss": 0.0029,
      "step": 71910
    },
    {
      "epoch": 3.9955555555555557,
      "grad_norm": 0.10247217118740082,
      "learning_rate": 1.0044444444444446e-05,
      "loss": 0.0035,
      "step": 71920
    },
    {
      "epoch": 3.996111111111111,
      "grad_norm": 0.03597569465637207,
      "learning_rate": 1.0038888888888889e-05,
      "loss": 0.0032,
      "step": 71930
    },
    {
      "epoch": 3.9966666666666666,
      "grad_norm": 0.11855332553386688,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0027,
      "step": 71940
    },
    {
      "epoch": 3.9972222222222222,
      "grad_norm": 0.03777720406651497,
      "learning_rate": 1.0027777777777778e-05,
      "loss": 0.0027,
      "step": 71950
    },
    {
      "epoch": 3.997777777777778,
      "grad_norm": 0.3080943524837494,
      "learning_rate": 1.0022222222222223e-05,
      "loss": 0.0025,
      "step": 71960
    },
    {
      "epoch": 3.998333333333333,
      "grad_norm": 0.3615528643131256,
      "learning_rate": 1.0016666666666667e-05,
      "loss": 0.0031,
      "step": 71970
    },
    {
      "epoch": 3.9988888888888887,
      "grad_norm": 0.04062556102871895,
      "learning_rate": 1.0011111111111112e-05,
      "loss": 0.0025,
      "step": 71980
    },
    {
      "epoch": 3.9994444444444444,
      "grad_norm": 0.06435249745845795,
      "learning_rate": 1.0005555555555556e-05,
      "loss": 0.0025,
      "step": 71990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.06367597728967667,
      "learning_rate": 1e-05,
      "loss": 0.0033,
      "step": 72000
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.0027346634306013584,
      "eval_runtime": 118.5158,
      "eval_samples_per_second": 1519.595,
      "eval_steps_per_second": 37.995,
      "step": 72000
    },
    {
      "epoch": 4.000555555555556,
      "grad_norm": 0.2773076295852661,
      "learning_rate": 9.994444444444444e-06,
      "loss": 0.0019,
      "step": 72010
    },
    {
      "epoch": 4.001111111111111,
      "grad_norm": 0.11902415007352829,
      "learning_rate": 9.98888888888889e-06,
      "loss": 0.0019,
      "step": 72020
    },
    {
      "epoch": 4.001666666666667,
      "grad_norm": 0.2866610884666443,
      "learning_rate": 9.983333333333333e-06,
      "loss": 0.0034,
      "step": 72030
    },
    {
      "epoch": 4.002222222222223,
      "grad_norm": 0.2551330029964447,
      "learning_rate": 9.977777777777778e-06,
      "loss": 0.0026,
      "step": 72040
    },
    {
      "epoch": 4.002777777777778,
      "grad_norm": 0.062253549695014954,
      "learning_rate": 9.972222222222224e-06,
      "loss": 0.0035,
      "step": 72050
    },
    {
      "epoch": 4.003333333333333,
      "grad_norm": 0.3464434742927551,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0035,
      "step": 72060
    },
    {
      "epoch": 4.003888888888889,
      "grad_norm": 0.03199583664536476,
      "learning_rate": 9.96111111111111e-06,
      "loss": 0.002,
      "step": 72070
    },
    {
      "epoch": 4.004444444444444,
      "grad_norm": 0.24499431252479553,
      "learning_rate": 9.955555555555556e-06,
      "loss": 0.0051,
      "step": 72080
    },
    {
      "epoch": 4.005,
      "grad_norm": 0.03322619944810867,
      "learning_rate": 9.950000000000001e-06,
      "loss": 0.0038,
      "step": 72090
    },
    {
      "epoch": 4.0055555555555555,
      "grad_norm": 0.119989313185215,
      "learning_rate": 9.944444444444445e-06,
      "loss": 0.0025,
      "step": 72100
    },
    {
      "epoch": 4.006111111111111,
      "grad_norm": 0.08966701477766037,
      "learning_rate": 9.938888888888888e-06,
      "loss": 0.0034,
      "step": 72110
    },
    {
      "epoch": 4.006666666666667,
      "grad_norm": 0.1515669822692871,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.003,
      "step": 72120
    },
    {
      "epoch": 4.0072222222222225,
      "grad_norm": 0.1481369137763977,
      "learning_rate": 9.927777777777779e-06,
      "loss": 0.0036,
      "step": 72130
    },
    {
      "epoch": 4.007777777777778,
      "grad_norm": 0.04586420953273773,
      "learning_rate": 9.922222222222222e-06,
      "loss": 0.0027,
      "step": 72140
    },
    {
      "epoch": 4.008333333333334,
      "grad_norm": 0.267159640789032,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.0018,
      "step": 72150
    },
    {
      "epoch": 4.0088888888888885,
      "grad_norm": 0.06080137565732002,
      "learning_rate": 9.911111111111111e-06,
      "loss": 0.0028,
      "step": 72160
    },
    {
      "epoch": 4.009444444444444,
      "grad_norm": 0.17850077152252197,
      "learning_rate": 9.905555555555555e-06,
      "loss": 0.0028,
      "step": 72170
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.33125191926956177,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0022,
      "step": 72180
    },
    {
      "epoch": 4.010555555555555,
      "grad_norm": 0.030841508880257607,
      "learning_rate": 9.894444444444445e-06,
      "loss": 0.0032,
      "step": 72190
    },
    {
      "epoch": 4.011111111111111,
      "grad_norm": 0.4151756465435028,
      "learning_rate": 9.888888888888889e-06,
      "loss": 0.0025,
      "step": 72200
    },
    {
      "epoch": 4.011666666666667,
      "grad_norm": 0.0792776346206665,
      "learning_rate": 9.883333333333334e-06,
      "loss": 0.0033,
      "step": 72210
    },
    {
      "epoch": 4.012222222222222,
      "grad_norm": 0.00898286048322916,
      "learning_rate": 9.87777777777778e-06,
      "loss": 0.0032,
      "step": 72220
    },
    {
      "epoch": 4.012777777777778,
      "grad_norm": 0.11863264441490173,
      "learning_rate": 9.872222222222223e-06,
      "loss": 0.003,
      "step": 72230
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.5173812508583069,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0037,
      "step": 72240
    },
    {
      "epoch": 4.013888888888889,
      "grad_norm": 0.44307732582092285,
      "learning_rate": 9.861111111111112e-06,
      "loss": 0.0024,
      "step": 72250
    },
    {
      "epoch": 4.014444444444444,
      "grad_norm": 0.4205825626850128,
      "learning_rate": 9.855555555555555e-06,
      "loss": 0.0042,
      "step": 72260
    },
    {
      "epoch": 4.015,
      "grad_norm": 0.1370161771774292,
      "learning_rate": 9.85e-06,
      "loss": 0.0023,
      "step": 72270
    },
    {
      "epoch": 4.015555555555555,
      "grad_norm": 0.11960972100496292,
      "learning_rate": 9.844444444444446e-06,
      "loss": 0.0024,
      "step": 72280
    },
    {
      "epoch": 4.016111111111111,
      "grad_norm": 0.17851747572422028,
      "learning_rate": 9.83888888888889e-06,
      "loss": 0.0022,
      "step": 72290
    },
    {
      "epoch": 4.016666666666667,
      "grad_norm": 0.14869700372219086,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0021,
      "step": 72300
    },
    {
      "epoch": 4.017222222222222,
      "grad_norm": 0.17804782092571259,
      "learning_rate": 9.827777777777778e-06,
      "loss": 0.0034,
      "step": 72310
    },
    {
      "epoch": 4.017777777777778,
      "grad_norm": 0.4854338467121124,
      "learning_rate": 9.822222222222223e-06,
      "loss": 0.0016,
      "step": 72320
    },
    {
      "epoch": 4.0183333333333335,
      "grad_norm": 0.08954159915447235,
      "learning_rate": 9.816666666666667e-06,
      "loss": 0.0028,
      "step": 72330
    },
    {
      "epoch": 4.018888888888889,
      "grad_norm": 0.4160723388195038,
      "learning_rate": 9.81111111111111e-06,
      "loss": 0.0029,
      "step": 72340
    },
    {
      "epoch": 4.019444444444445,
      "grad_norm": 0.33131593465805054,
      "learning_rate": 9.805555555555557e-06,
      "loss": 0.0022,
      "step": 72350
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.2669811248779297,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0028,
      "step": 72360
    },
    {
      "epoch": 4.020555555555555,
      "grad_norm": 0.03795512020587921,
      "learning_rate": 9.794444444444445e-06,
      "loss": 0.0026,
      "step": 72370
    },
    {
      "epoch": 4.021111111111111,
      "grad_norm": 0.7197756767272949,
      "learning_rate": 9.78888888888889e-06,
      "loss": 0.0032,
      "step": 72380
    },
    {
      "epoch": 4.0216666666666665,
      "grad_norm": 0.0352521613240242,
      "learning_rate": 9.783333333333333e-06,
      "loss": 0.0027,
      "step": 72390
    },
    {
      "epoch": 4.022222222222222,
      "grad_norm": 0.11943027377128601,
      "learning_rate": 9.777777777777779e-06,
      "loss": 0.0016,
      "step": 72400
    },
    {
      "epoch": 4.022777777777778,
      "grad_norm": 0.09039690345525742,
      "learning_rate": 9.772222222222222e-06,
      "loss": 0.0028,
      "step": 72410
    },
    {
      "epoch": 4.023333333333333,
      "grad_norm": 0.23825626075267792,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.003,
      "step": 72420
    },
    {
      "epoch": 4.023888888888889,
      "grad_norm": 0.2667962610721588,
      "learning_rate": 9.761111111111111e-06,
      "loss": 0.0026,
      "step": 72430
    },
    {
      "epoch": 4.024444444444445,
      "grad_norm": 0.17760099470615387,
      "learning_rate": 9.755555555555556e-06,
      "loss": 0.0035,
      "step": 72440
    },
    {
      "epoch": 4.025,
      "grad_norm": 0.13709838688373566,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.0025,
      "step": 72450
    },
    {
      "epoch": 4.025555555555556,
      "grad_norm": 0.39115971326828003,
      "learning_rate": 9.744444444444445e-06,
      "loss": 0.0026,
      "step": 72460
    },
    {
      "epoch": 4.026111111111111,
      "grad_norm": 0.03776426985859871,
      "learning_rate": 9.738888888888889e-06,
      "loss": 0.0024,
      "step": 72470
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.08988571912050247,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0035,
      "step": 72480
    },
    {
      "epoch": 4.027222222222222,
      "grad_norm": 0.07465015351772308,
      "learning_rate": 9.727777777777779e-06,
      "loss": 0.0025,
      "step": 72490
    },
    {
      "epoch": 4.027777777777778,
      "grad_norm": 0.19198819994926453,
      "learning_rate": 9.722222222222223e-06,
      "loss": 0.0021,
      "step": 72500
    },
    {
      "epoch": 4.028333333333333,
      "grad_norm": 0.03574375435709953,
      "learning_rate": 9.716666666666666e-06,
      "loss": 0.0025,
      "step": 72510
    },
    {
      "epoch": 4.028888888888889,
      "grad_norm": 0.21233582496643066,
      "learning_rate": 9.711111111111111e-06,
      "loss": 0.002,
      "step": 72520
    },
    {
      "epoch": 4.029444444444445,
      "grad_norm": 0.014835306443274021,
      "learning_rate": 9.705555555555557e-06,
      "loss": 0.0028,
      "step": 72530
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.14895281195640564,
      "learning_rate": 9.7e-06,
      "loss": 0.0018,
      "step": 72540
    },
    {
      "epoch": 4.030555555555556,
      "grad_norm": 0.09540610015392303,
      "learning_rate": 9.694444444444446e-06,
      "loss": 0.0028,
      "step": 72550
    },
    {
      "epoch": 4.0311111111111115,
      "grad_norm": 0.29672423005104065,
      "learning_rate": 9.688888888888889e-06,
      "loss": 0.0029,
      "step": 72560
    },
    {
      "epoch": 4.031666666666666,
      "grad_norm": 0.48283013701438904,
      "learning_rate": 9.683333333333333e-06,
      "loss": 0.0017,
      "step": 72570
    },
    {
      "epoch": 4.032222222222222,
      "grad_norm": 0.14838923513889313,
      "learning_rate": 9.677777777777778e-06,
      "loss": 0.0019,
      "step": 72580
    },
    {
      "epoch": 4.032777777777778,
      "grad_norm": 0.15840332210063934,
      "learning_rate": 9.672222222222223e-06,
      "loss": 0.0032,
      "step": 72590
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 0.1486487239599228,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0016,
      "step": 72600
    },
    {
      "epoch": 4.033888888888889,
      "grad_norm": 0.207560732960701,
      "learning_rate": 9.66111111111111e-06,
      "loss": 0.0026,
      "step": 72610
    },
    {
      "epoch": 4.0344444444444445,
      "grad_norm": 0.03718806430697441,
      "learning_rate": 9.655555555555557e-06,
      "loss": 0.0022,
      "step": 72620
    },
    {
      "epoch": 4.035,
      "grad_norm": 0.24761535227298737,
      "learning_rate": 9.65e-06,
      "loss": 0.0048,
      "step": 72630
    },
    {
      "epoch": 4.035555555555556,
      "grad_norm": 0.14819228649139404,
      "learning_rate": 9.644444444444444e-06,
      "loss": 0.003,
      "step": 72640
    },
    {
      "epoch": 4.036111111111111,
      "grad_norm": 0.4498969614505768,
      "learning_rate": 9.63888888888889e-06,
      "loss": 0.0026,
      "step": 72650
    },
    {
      "epoch": 4.036666666666667,
      "grad_norm": 0.22292070090770721,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0034,
      "step": 72660
    },
    {
      "epoch": 4.037222222222222,
      "grad_norm": 0.30084648728370667,
      "learning_rate": 9.627777777777778e-06,
      "loss": 0.0029,
      "step": 72670
    },
    {
      "epoch": 4.0377777777777775,
      "grad_norm": 1.057867407798767,
      "learning_rate": 9.622222222222222e-06,
      "loss": 0.0029,
      "step": 72680
    },
    {
      "epoch": 4.038333333333333,
      "grad_norm": 0.32103824615478516,
      "learning_rate": 9.616666666666667e-06,
      "loss": 0.0019,
      "step": 72690
    },
    {
      "epoch": 4.038888888888889,
      "grad_norm": 0.23696164786815643,
      "learning_rate": 9.61111111111111e-06,
      "loss": 0.003,
      "step": 72700
    },
    {
      "epoch": 4.039444444444444,
      "grad_norm": 0.35678738355636597,
      "learning_rate": 9.605555555555556e-06,
      "loss": 0.0022,
      "step": 72710
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.23687978088855743,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0029,
      "step": 72720
    },
    {
      "epoch": 4.040555555555556,
      "grad_norm": 0.015784531831741333,
      "learning_rate": 9.594444444444445e-06,
      "loss": 0.0019,
      "step": 72730
    },
    {
      "epoch": 4.041111111111111,
      "grad_norm": 0.033083539456129074,
      "learning_rate": 9.588888888888888e-06,
      "loss": 0.0024,
      "step": 72740
    },
    {
      "epoch": 4.041666666666667,
      "grad_norm": 0.019124651327729225,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.0025,
      "step": 72750
    },
    {
      "epoch": 4.042222222222223,
      "grad_norm": 0.03956758603453636,
      "learning_rate": 9.577777777777779e-06,
      "loss": 0.0022,
      "step": 72760
    },
    {
      "epoch": 4.042777777777777,
      "grad_norm": 0.32091277837753296,
      "learning_rate": 9.572222222222222e-06,
      "loss": 0.0025,
      "step": 72770
    },
    {
      "epoch": 4.043333333333333,
      "grad_norm": 0.17872117459774017,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0017,
      "step": 72780
    },
    {
      "epoch": 4.043888888888889,
      "grad_norm": 0.11863735318183899,
      "learning_rate": 9.561111111111111e-06,
      "loss": 0.0028,
      "step": 72790
    },
    {
      "epoch": 4.044444444444444,
      "grad_norm": 0.059913091361522675,
      "learning_rate": 9.555555555555556e-06,
      "loss": 0.0034,
      "step": 72800
    },
    {
      "epoch": 4.045,
      "grad_norm": 0.3287774622440338,
      "learning_rate": 9.55e-06,
      "loss": 0.0027,
      "step": 72810
    },
    {
      "epoch": 4.045555555555556,
      "grad_norm": 0.5388417840003967,
      "learning_rate": 9.544444444444445e-06,
      "loss": 0.0022,
      "step": 72820
    },
    {
      "epoch": 4.046111111111111,
      "grad_norm": 0.14852632582187653,
      "learning_rate": 9.538888888888889e-06,
      "loss": 0.0022,
      "step": 72830
    },
    {
      "epoch": 4.046666666666667,
      "grad_norm": 0.21923881769180298,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0033,
      "step": 72840
    },
    {
      "epoch": 4.0472222222222225,
      "grad_norm": 0.364216148853302,
      "learning_rate": 9.52777777777778e-06,
      "loss": 0.0034,
      "step": 72850
    },
    {
      "epoch": 4.047777777777778,
      "grad_norm": 0.5357316136360168,
      "learning_rate": 9.522222222222223e-06,
      "loss": 0.0032,
      "step": 72860
    },
    {
      "epoch": 4.048333333333333,
      "grad_norm": 0.12009327858686447,
      "learning_rate": 9.516666666666666e-06,
      "loss": 0.0035,
      "step": 72870
    },
    {
      "epoch": 4.0488888888888885,
      "grad_norm": 0.6611194014549255,
      "learning_rate": 9.511111111111112e-06,
      "loss": 0.0033,
      "step": 72880
    },
    {
      "epoch": 4.049444444444444,
      "grad_norm": 0.14861758053302765,
      "learning_rate": 9.505555555555557e-06,
      "loss": 0.0027,
      "step": 72890
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.012110091745853424,
      "learning_rate": 9.5e-06,
      "loss": 0.0038,
      "step": 72900
    },
    {
      "epoch": 4.0505555555555555,
      "grad_norm": 0.05976961925625801,
      "learning_rate": 9.494444444444444e-06,
      "loss": 0.0034,
      "step": 72910
    },
    {
      "epoch": 4.051111111111111,
      "grad_norm": 0.5306488275527954,
      "learning_rate": 9.48888888888889e-06,
      "loss": 0.0024,
      "step": 72920
    },
    {
      "epoch": 4.051666666666667,
      "grad_norm": 0.0342782624065876,
      "learning_rate": 9.483333333333335e-06,
      "loss": 0.0023,
      "step": 72930
    },
    {
      "epoch": 4.052222222222222,
      "grad_norm": 0.3270840644836426,
      "learning_rate": 9.477777777777778e-06,
      "loss": 0.003,
      "step": 72940
    },
    {
      "epoch": 4.052777777777778,
      "grad_norm": 0.14890947937965393,
      "learning_rate": 9.472222222222223e-06,
      "loss": 0.0024,
      "step": 72950
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.1784205287694931,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0031,
      "step": 72960
    },
    {
      "epoch": 4.053888888888889,
      "grad_norm": 0.23657523095607758,
      "learning_rate": 9.461111111111112e-06,
      "loss": 0.0043,
      "step": 72970
    },
    {
      "epoch": 4.054444444444444,
      "grad_norm": 0.24007517099380493,
      "learning_rate": 9.455555555555556e-06,
      "loss": 0.0028,
      "step": 72980
    },
    {
      "epoch": 4.055,
      "grad_norm": 0.061468228697776794,
      "learning_rate": 9.450000000000001e-06,
      "loss": 0.0034,
      "step": 72990
    },
    {
      "epoch": 4.055555555555555,
      "grad_norm": 0.08928634971380234,
      "learning_rate": 9.444444444444445e-06,
      "loss": 0.0029,
      "step": 73000
    },
    {
      "epoch": 4.056111111111111,
      "grad_norm": 0.20214132964611053,
      "learning_rate": 9.438888888888888e-06,
      "loss": 0.0029,
      "step": 73010
    },
    {
      "epoch": 4.056666666666667,
      "grad_norm": 0.09722582995891571,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0029,
      "step": 73020
    },
    {
      "epoch": 4.057222222222222,
      "grad_norm": 0.208298921585083,
      "learning_rate": 9.427777777777779e-06,
      "loss": 0.0019,
      "step": 73030
    },
    {
      "epoch": 4.057777777777778,
      "grad_norm": 0.17750966548919678,
      "learning_rate": 9.422222222222222e-06,
      "loss": 0.0022,
      "step": 73040
    },
    {
      "epoch": 4.058333333333334,
      "grad_norm": 0.11918596923351288,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.0024,
      "step": 73050
    },
    {
      "epoch": 4.058888888888889,
      "grad_norm": 0.10851047188043594,
      "learning_rate": 9.411111111111113e-06,
      "loss": 0.0026,
      "step": 73060
    },
    {
      "epoch": 4.059444444444445,
      "grad_norm": 0.33045777678489685,
      "learning_rate": 9.405555555555556e-06,
      "loss": 0.0029,
      "step": 73070
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.23014619946479797,
      "learning_rate": 9.4e-06,
      "loss": 0.0027,
      "step": 73080
    },
    {
      "epoch": 4.060555555555555,
      "grad_norm": 0.2680571377277374,
      "learning_rate": 9.394444444444445e-06,
      "loss": 0.0021,
      "step": 73090
    },
    {
      "epoch": 4.061111111111111,
      "grad_norm": 0.22549216449260712,
      "learning_rate": 9.388888888888889e-06,
      "loss": 0.0023,
      "step": 73100
    },
    {
      "epoch": 4.0616666666666665,
      "grad_norm": 0.19810251891613007,
      "learning_rate": 9.383333333333334e-06,
      "loss": 0.0023,
      "step": 73110
    },
    {
      "epoch": 4.062222222222222,
      "grad_norm": 0.17845173180103302,
      "learning_rate": 9.377777777777779e-06,
      "loss": 0.0026,
      "step": 73120
    },
    {
      "epoch": 4.062777777777778,
      "grad_norm": 0.2437111735343933,
      "learning_rate": 9.372222222222223e-06,
      "loss": 0.0024,
      "step": 73130
    },
    {
      "epoch": 4.0633333333333335,
      "grad_norm": 0.16491279006004333,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0025,
      "step": 73140
    },
    {
      "epoch": 4.063888888888889,
      "grad_norm": 0.3054848909378052,
      "learning_rate": 9.361111111111111e-06,
      "loss": 0.0043,
      "step": 73150
    },
    {
      "epoch": 4.064444444444445,
      "grad_norm": 0.17848201096057892,
      "learning_rate": 9.355555555555557e-06,
      "loss": 0.0026,
      "step": 73160
    },
    {
      "epoch": 4.065,
      "grad_norm": 0.7213264107704163,
      "learning_rate": 9.35e-06,
      "loss": 0.0035,
      "step": 73170
    },
    {
      "epoch": 4.065555555555555,
      "grad_norm": 0.0602533258497715,
      "learning_rate": 9.344444444444444e-06,
      "loss": 0.0023,
      "step": 73180
    },
    {
      "epoch": 4.066111111111111,
      "grad_norm": 0.44552338123321533,
      "learning_rate": 9.338888888888889e-06,
      "loss": 0.0018,
      "step": 73190
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.010794847272336483,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0031,
      "step": 73200
    },
    {
      "epoch": 4.067222222222222,
      "grad_norm": 0.032467857003211975,
      "learning_rate": 9.327777777777778e-06,
      "loss": 0.0038,
      "step": 73210
    },
    {
      "epoch": 4.067777777777778,
      "grad_norm": 0.3861980140209198,
      "learning_rate": 9.322222222222223e-06,
      "loss": 0.0022,
      "step": 73220
    },
    {
      "epoch": 4.068333333333333,
      "grad_norm": 0.15889853239059448,
      "learning_rate": 9.316666666666667e-06,
      "loss": 0.0022,
      "step": 73230
    },
    {
      "epoch": 4.068888888888889,
      "grad_norm": 0.38495582342147827,
      "learning_rate": 9.311111111111112e-06,
      "loss": 0.0028,
      "step": 73240
    },
    {
      "epoch": 4.069444444444445,
      "grad_norm": 0.23748503625392914,
      "learning_rate": 9.305555555555555e-06,
      "loss": 0.0022,
      "step": 73250
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.018715960904955864,
      "learning_rate": 9.3e-06,
      "loss": 0.0022,
      "step": 73260
    },
    {
      "epoch": 4.070555555555556,
      "grad_norm": 0.17932730913162231,
      "learning_rate": 9.294444444444444e-06,
      "loss": 0.0035,
      "step": 73270
    },
    {
      "epoch": 4.071111111111111,
      "grad_norm": 0.3755621910095215,
      "learning_rate": 9.288888888888888e-06,
      "loss": 0.002,
      "step": 73280
    },
    {
      "epoch": 4.071666666666666,
      "grad_norm": 0.06105126440525055,
      "learning_rate": 9.283333333333335e-06,
      "loss": 0.0042,
      "step": 73290
    },
    {
      "epoch": 4.072222222222222,
      "grad_norm": 0.19481806457042694,
      "learning_rate": 9.277777777777778e-06,
      "loss": 0.0041,
      "step": 73300
    },
    {
      "epoch": 4.072777777777778,
      "grad_norm": 0.41252827644348145,
      "learning_rate": 9.272222222222222e-06,
      "loss": 0.002,
      "step": 73310
    },
    {
      "epoch": 4.073333333333333,
      "grad_norm": 0.11490358412265778,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0029,
      "step": 73320
    },
    {
      "epoch": 4.073888888888889,
      "grad_norm": 0.29797789454460144,
      "learning_rate": 9.261111111111112e-06,
      "loss": 0.0031,
      "step": 73330
    },
    {
      "epoch": 4.0744444444444445,
      "grad_norm": 0.08944003283977509,
      "learning_rate": 9.255555555555556e-06,
      "loss": 0.0032,
      "step": 73340
    },
    {
      "epoch": 4.075,
      "grad_norm": 0.09119146317243576,
      "learning_rate": 9.25e-06,
      "loss": 0.0019,
      "step": 73350
    },
    {
      "epoch": 4.075555555555556,
      "grad_norm": 0.17922493815422058,
      "learning_rate": 9.244444444444445e-06,
      "loss": 0.0033,
      "step": 73360
    },
    {
      "epoch": 4.0761111111111115,
      "grad_norm": 0.20909295976161957,
      "learning_rate": 9.23888888888889e-06,
      "loss": 0.0024,
      "step": 73370
    },
    {
      "epoch": 4.076666666666666,
      "grad_norm": 0.03154437988996506,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0033,
      "step": 73380
    },
    {
      "epoch": 4.077222222222222,
      "grad_norm": 0.35086214542388916,
      "learning_rate": 9.227777777777779e-06,
      "loss": 0.0027,
      "step": 73390
    },
    {
      "epoch": 4.0777777777777775,
      "grad_norm": 0.060550663620233536,
      "learning_rate": 9.222222222222222e-06,
      "loss": 0.0022,
      "step": 73400
    },
    {
      "epoch": 4.078333333333333,
      "grad_norm": 0.35540613532066345,
      "learning_rate": 9.216666666666666e-06,
      "loss": 0.0032,
      "step": 73410
    },
    {
      "epoch": 4.078888888888889,
      "grad_norm": 0.05997952073812485,
      "learning_rate": 9.211111111111111e-06,
      "loss": 0.0023,
      "step": 73420
    },
    {
      "epoch": 4.079444444444444,
      "grad_norm": 0.12007331103086472,
      "learning_rate": 9.205555555555556e-06,
      "loss": 0.0022,
      "step": 73430
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.17789499461650848,
      "learning_rate": 9.2e-06,
      "loss": 0.003,
      "step": 73440
    },
    {
      "epoch": 4.080555555555556,
      "grad_norm": 0.5008302927017212,
      "learning_rate": 9.194444444444445e-06,
      "loss": 0.0026,
      "step": 73450
    },
    {
      "epoch": 4.081111111111111,
      "grad_norm": 0.3833027780056,
      "learning_rate": 9.18888888888889e-06,
      "loss": 0.0013,
      "step": 73460
    },
    {
      "epoch": 4.081666666666667,
      "grad_norm": 0.06001310795545578,
      "learning_rate": 9.183333333333334e-06,
      "loss": 0.0035,
      "step": 73470
    },
    {
      "epoch": 4.082222222222223,
      "grad_norm": 0.7141463160514832,
      "learning_rate": 9.177777777777778e-06,
      "loss": 0.0024,
      "step": 73480
    },
    {
      "epoch": 4.082777777777777,
      "grad_norm": 0.06050233915448189,
      "learning_rate": 9.172222222222223e-06,
      "loss": 0.003,
      "step": 73490
    },
    {
      "epoch": 4.083333333333333,
      "grad_norm": 0.4209268093109131,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0032,
      "step": 73500
    },
    {
      "epoch": 4.083888888888889,
      "grad_norm": 0.09048150479793549,
      "learning_rate": 9.161111111111112e-06,
      "loss": 0.0021,
      "step": 73510
    },
    {
      "epoch": 4.084444444444444,
      "grad_norm": 0.11896402388811111,
      "learning_rate": 9.155555555555557e-06,
      "loss": 0.0027,
      "step": 73520
    },
    {
      "epoch": 4.085,
      "grad_norm": 0.11873578280210495,
      "learning_rate": 9.15e-06,
      "loss": 0.0021,
      "step": 73530
    },
    {
      "epoch": 4.085555555555556,
      "grad_norm": 0.17868660390377045,
      "learning_rate": 9.144444444444444e-06,
      "loss": 0.003,
      "step": 73540
    },
    {
      "epoch": 4.086111111111111,
      "grad_norm": 0.09030412882566452,
      "learning_rate": 9.13888888888889e-06,
      "loss": 0.0028,
      "step": 73550
    },
    {
      "epoch": 4.086666666666667,
      "grad_norm": 0.10725156217813492,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0025,
      "step": 73560
    },
    {
      "epoch": 4.0872222222222225,
      "grad_norm": 0.17760372161865234,
      "learning_rate": 9.127777777777778e-06,
      "loss": 0.0027,
      "step": 73570
    },
    {
      "epoch": 4.087777777777778,
      "grad_norm": 0.11912299692630768,
      "learning_rate": 9.122222222222222e-06,
      "loss": 0.0028,
      "step": 73580
    },
    {
      "epoch": 4.088333333333333,
      "grad_norm": 0.35690659284591675,
      "learning_rate": 9.116666666666667e-06,
      "loss": 0.0029,
      "step": 73590
    },
    {
      "epoch": 4.088888888888889,
      "grad_norm": 0.20758986473083496,
      "learning_rate": 9.111111111111112e-06,
      "loss": 0.0034,
      "step": 73600
    },
    {
      "epoch": 4.089444444444444,
      "grad_norm": 0.11963773518800735,
      "learning_rate": 9.105555555555556e-06,
      "loss": 0.0027,
      "step": 73610
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.03179604932665825,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.003,
      "step": 73620
    },
    {
      "epoch": 4.0905555555555555,
      "grad_norm": 0.11878934502601624,
      "learning_rate": 9.094444444444445e-06,
      "loss": 0.0017,
      "step": 73630
    },
    {
      "epoch": 4.091111111111111,
      "grad_norm": 0.01490369625389576,
      "learning_rate": 9.08888888888889e-06,
      "loss": 0.0037,
      "step": 73640
    },
    {
      "epoch": 4.091666666666667,
      "grad_norm": 0.09038253128528595,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.0029,
      "step": 73650
    },
    {
      "epoch": 4.092222222222222,
      "grad_norm": 0.1202712208032608,
      "learning_rate": 9.077777777777779e-06,
      "loss": 0.0024,
      "step": 73660
    },
    {
      "epoch": 4.092777777777778,
      "grad_norm": 0.059613171964883804,
      "learning_rate": 9.072222222222222e-06,
      "loss": 0.0026,
      "step": 73670
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.7442405819892883,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0025,
      "step": 73680
    },
    {
      "epoch": 4.0938888888888885,
      "grad_norm": 0.21503698825836182,
      "learning_rate": 9.061111111111113e-06,
      "loss": 0.0022,
      "step": 73690
    },
    {
      "epoch": 4.094444444444444,
      "grad_norm": 0.28855934739112854,
      "learning_rate": 9.055555555555556e-06,
      "loss": 0.0024,
      "step": 73700
    },
    {
      "epoch": 4.095,
      "grad_norm": 0.47541099786758423,
      "learning_rate": 9.05e-06,
      "loss": 0.0025,
      "step": 73710
    },
    {
      "epoch": 4.095555555555555,
      "grad_norm": 0.3317178785800934,
      "learning_rate": 9.044444444444445e-06,
      "loss": 0.0034,
      "step": 73720
    },
    {
      "epoch": 4.096111111111111,
      "grad_norm": 0.38117650151252747,
      "learning_rate": 9.03888888888889e-06,
      "loss": 0.0038,
      "step": 73730
    },
    {
      "epoch": 4.096666666666667,
      "grad_norm": 0.30788707733154297,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0023,
      "step": 73740
    },
    {
      "epoch": 4.097222222222222,
      "grad_norm": 0.12476954609155655,
      "learning_rate": 9.027777777777777e-06,
      "loss": 0.0024,
      "step": 73750
    },
    {
      "epoch": 4.097777777777778,
      "grad_norm": 0.19516891241073608,
      "learning_rate": 9.022222222222223e-06,
      "loss": 0.0029,
      "step": 73760
    },
    {
      "epoch": 4.098333333333334,
      "grad_norm": 0.19047871232032776,
      "learning_rate": 9.016666666666668e-06,
      "loss": 0.0024,
      "step": 73770
    },
    {
      "epoch": 4.098888888888889,
      "grad_norm": 0.4744361639022827,
      "learning_rate": 9.011111111111111e-06,
      "loss": 0.004,
      "step": 73780
    },
    {
      "epoch": 4.099444444444444,
      "grad_norm": 0.06004391983151436,
      "learning_rate": 9.005555555555557e-06,
      "loss": 0.003,
      "step": 73790
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.678355872631073,
      "learning_rate": 9e-06,
      "loss": 0.0021,
      "step": 73800
    },
    {
      "epoch": 4.100555555555555,
      "grad_norm": 0.4417073726654053,
      "learning_rate": 8.994444444444444e-06,
      "loss": 0.0027,
      "step": 73810
    },
    {
      "epoch": 4.101111111111111,
      "grad_norm": 0.39745041728019714,
      "learning_rate": 8.988888888888889e-06,
      "loss": 0.0018,
      "step": 73820
    },
    {
      "epoch": 4.101666666666667,
      "grad_norm": 0.11890967190265656,
      "learning_rate": 8.983333333333334e-06,
      "loss": 0.0022,
      "step": 73830
    },
    {
      "epoch": 4.102222222222222,
      "grad_norm": 0.1315569430589676,
      "learning_rate": 8.977777777777778e-06,
      "loss": 0.003,
      "step": 73840
    },
    {
      "epoch": 4.102777777777778,
      "grad_norm": 0.17782153189182281,
      "learning_rate": 8.972222222222221e-06,
      "loss": 0.0024,
      "step": 73850
    },
    {
      "epoch": 4.1033333333333335,
      "grad_norm": 0.20821049809455872,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0026,
      "step": 73860
    },
    {
      "epoch": 4.103888888888889,
      "grad_norm": 0.29710209369659424,
      "learning_rate": 8.961111111111112e-06,
      "loss": 0.003,
      "step": 73870
    },
    {
      "epoch": 4.104444444444445,
      "grad_norm": 0.2404293566942215,
      "learning_rate": 8.955555555555555e-06,
      "loss": 0.003,
      "step": 73880
    },
    {
      "epoch": 4.105,
      "grad_norm": 0.4337255358695984,
      "learning_rate": 8.95e-06,
      "loss": 0.0026,
      "step": 73890
    },
    {
      "epoch": 4.105555555555555,
      "grad_norm": 0.17866365611553192,
      "learning_rate": 8.944444444444444e-06,
      "loss": 0.0021,
      "step": 73900
    },
    {
      "epoch": 4.106111111111111,
      "grad_norm": 0.26849234104156494,
      "learning_rate": 8.93888888888889e-06,
      "loss": 0.0018,
      "step": 73910
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.14674662053585052,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0035,
      "step": 73920
    },
    {
      "epoch": 4.107222222222222,
      "grad_norm": 0.03932657092809677,
      "learning_rate": 8.927777777777778e-06,
      "loss": 0.0034,
      "step": 73930
    },
    {
      "epoch": 4.107777777777778,
      "grad_norm": 0.062130291014909744,
      "learning_rate": 8.922222222222222e-06,
      "loss": 0.003,
      "step": 73940
    },
    {
      "epoch": 4.108333333333333,
      "grad_norm": 0.14944274723529816,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.0026,
      "step": 73950
    },
    {
      "epoch": 4.108888888888889,
      "grad_norm": 0.061714205890893936,
      "learning_rate": 8.911111111111112e-06,
      "loss": 0.0021,
      "step": 73960
    },
    {
      "epoch": 4.109444444444445,
      "grad_norm": 0.015711018815636635,
      "learning_rate": 8.905555555555556e-06,
      "loss": 0.0028,
      "step": 73970
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.17875535786151886,
      "learning_rate": 8.9e-06,
      "loss": 0.0027,
      "step": 73980
    },
    {
      "epoch": 4.110555555555556,
      "grad_norm": 0.4375939965248108,
      "learning_rate": 8.894444444444445e-06,
      "loss": 0.0032,
      "step": 73990
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.1487514227628708,
      "learning_rate": 8.88888888888889e-06,
      "loss": 0.0031,
      "step": 74000
    },
    {
      "epoch": 4.111666666666666,
      "grad_norm": 0.41095033288002014,
      "learning_rate": 8.883333333333334e-06,
      "loss": 0.0031,
      "step": 74010
    },
    {
      "epoch": 4.112222222222222,
      "grad_norm": 0.04341328144073486,
      "learning_rate": 8.877777777777777e-06,
      "loss": 0.0032,
      "step": 74020
    },
    {
      "epoch": 4.112777777777778,
      "grad_norm": 0.1566450148820877,
      "learning_rate": 8.872222222222222e-06,
      "loss": 0.0021,
      "step": 74030
    },
    {
      "epoch": 4.113333333333333,
      "grad_norm": 0.3736731708049774,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0018,
      "step": 74040
    },
    {
      "epoch": 4.113888888888889,
      "grad_norm": 0.031360141932964325,
      "learning_rate": 8.861111111111111e-06,
      "loss": 0.0016,
      "step": 74050
    },
    {
      "epoch": 4.114444444444445,
      "grad_norm": 0.11978888511657715,
      "learning_rate": 8.855555555555556e-06,
      "loss": 0.003,
      "step": 74060
    },
    {
      "epoch": 4.115,
      "grad_norm": 0.06054453179240227,
      "learning_rate": 8.85e-06,
      "loss": 0.0027,
      "step": 74070
    },
    {
      "epoch": 4.115555555555556,
      "grad_norm": 0.060597147792577744,
      "learning_rate": 8.844444444444445e-06,
      "loss": 0.0023,
      "step": 74080
    },
    {
      "epoch": 4.1161111111111115,
      "grad_norm": 0.20774638652801514,
      "learning_rate": 8.838888888888889e-06,
      "loss": 0.0026,
      "step": 74090
    },
    {
      "epoch": 4.116666666666666,
      "grad_norm": 0.06127261742949486,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0025,
      "step": 74100
    },
    {
      "epoch": 4.117222222222222,
      "grad_norm": 0.23707272112369537,
      "learning_rate": 8.827777777777778e-06,
      "loss": 0.0035,
      "step": 74110
    },
    {
      "epoch": 4.1177777777777775,
      "grad_norm": 0.09348033368587494,
      "learning_rate": 8.822222222222223e-06,
      "loss": 0.0035,
      "step": 74120
    },
    {
      "epoch": 4.118333333333333,
      "grad_norm": 0.26688316464424133,
      "learning_rate": 8.816666666666668e-06,
      "loss": 0.0028,
      "step": 74130
    },
    {
      "epoch": 4.118888888888889,
      "grad_norm": 0.045341696590185165,
      "learning_rate": 8.811111111111112e-06,
      "loss": 0.0027,
      "step": 74140
    },
    {
      "epoch": 4.1194444444444445,
      "grad_norm": 0.296398401260376,
      "learning_rate": 8.805555555555555e-06,
      "loss": 0.0026,
      "step": 74150
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.23950272798538208,
      "learning_rate": 8.8e-06,
      "loss": 0.0022,
      "step": 74160
    },
    {
      "epoch": 4.120555555555556,
      "grad_norm": 0.23783521354198456,
      "learning_rate": 8.794444444444446e-06,
      "loss": 0.0034,
      "step": 74170
    },
    {
      "epoch": 4.121111111111111,
      "grad_norm": 0.20790310204029083,
      "learning_rate": 8.78888888888889e-06,
      "loss": 0.002,
      "step": 74180
    },
    {
      "epoch": 4.121666666666667,
      "grad_norm": 0.20734883844852448,
      "learning_rate": 8.783333333333335e-06,
      "loss": 0.003,
      "step": 74190
    },
    {
      "epoch": 4.122222222222222,
      "grad_norm": 0.11889401823282242,
      "learning_rate": 8.777777777777778e-06,
      "loss": 0.0021,
      "step": 74200
    },
    {
      "epoch": 4.122777777777777,
      "grad_norm": 0.20747032761573792,
      "learning_rate": 8.772222222222222e-06,
      "loss": 0.003,
      "step": 74210
    },
    {
      "epoch": 4.123333333333333,
      "grad_norm": 0.5081306099891663,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.0032,
      "step": 74220
    },
    {
      "epoch": 4.123888888888889,
      "grad_norm": 0.1485239863395691,
      "learning_rate": 8.761111111111112e-06,
      "loss": 0.0026,
      "step": 74230
    },
    {
      "epoch": 4.124444444444444,
      "grad_norm": 0.03262650594115257,
      "learning_rate": 8.755555555555556e-06,
      "loss": 0.0044,
      "step": 74240
    },
    {
      "epoch": 4.125,
      "grad_norm": 0.1078622043132782,
      "learning_rate": 8.75e-06,
      "loss": 0.003,
      "step": 74250
    },
    {
      "epoch": 4.125555555555556,
      "grad_norm": 0.38521039485931396,
      "learning_rate": 8.744444444444446e-06,
      "loss": 0.0022,
      "step": 74260
    },
    {
      "epoch": 4.126111111111111,
      "grad_norm": 0.5559372901916504,
      "learning_rate": 8.73888888888889e-06,
      "loss": 0.0042,
      "step": 74270
    },
    {
      "epoch": 4.126666666666667,
      "grad_norm": 0.2639128267765045,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0023,
      "step": 74280
    },
    {
      "epoch": 4.127222222222223,
      "grad_norm": 0.38549456000328064,
      "learning_rate": 8.727777777777779e-06,
      "loss": 0.0027,
      "step": 74290
    },
    {
      "epoch": 4.127777777777778,
      "grad_norm": 0.1199934184551239,
      "learning_rate": 8.722222222222224e-06,
      "loss": 0.0037,
      "step": 74300
    },
    {
      "epoch": 4.128333333333333,
      "grad_norm": 0.15499670803546906,
      "learning_rate": 8.716666666666667e-06,
      "loss": 0.0026,
      "step": 74310
    },
    {
      "epoch": 4.128888888888889,
      "grad_norm": 0.23702561855316162,
      "learning_rate": 8.711111111111111e-06,
      "loss": 0.0016,
      "step": 74320
    },
    {
      "epoch": 4.129444444444444,
      "grad_norm": 0.03555714711546898,
      "learning_rate": 8.705555555555556e-06,
      "loss": 0.0018,
      "step": 74330
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.009933356195688248,
      "learning_rate": 8.7e-06,
      "loss": 0.0027,
      "step": 74340
    },
    {
      "epoch": 4.1305555555555555,
      "grad_norm": 0.3262309432029724,
      "learning_rate": 8.694444444444445e-06,
      "loss": 0.0022,
      "step": 74350
    },
    {
      "epoch": 4.131111111111111,
      "grad_norm": 0.09042488038539886,
      "learning_rate": 8.68888888888889e-06,
      "loss": 0.0025,
      "step": 74360
    },
    {
      "epoch": 4.131666666666667,
      "grad_norm": 0.11916904896497726,
      "learning_rate": 8.683333333333334e-06,
      "loss": 0.0029,
      "step": 74370
    },
    {
      "epoch": 4.1322222222222225,
      "grad_norm": 0.38539329171180725,
      "learning_rate": 8.677777777777777e-06,
      "loss": 0.0035,
      "step": 74380
    },
    {
      "epoch": 4.132777777777778,
      "grad_norm": 0.12158934772014618,
      "learning_rate": 8.672222222222223e-06,
      "loss": 0.0031,
      "step": 74390
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.04616628959774971,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0028,
      "step": 74400
    },
    {
      "epoch": 4.1338888888888885,
      "grad_norm": 0.017163699492812157,
      "learning_rate": 8.661111111111111e-06,
      "loss": 0.0027,
      "step": 74410
    },
    {
      "epoch": 4.134444444444444,
      "grad_norm": 0.4745328426361084,
      "learning_rate": 8.655555555555555e-06,
      "loss": 0.0026,
      "step": 74420
    },
    {
      "epoch": 4.135,
      "grad_norm": 0.18042978644371033,
      "learning_rate": 8.65e-06,
      "loss": 0.0019,
      "step": 74430
    },
    {
      "epoch": 4.135555555555555,
      "grad_norm": 0.15186485648155212,
      "learning_rate": 8.644444444444445e-06,
      "loss": 0.0034,
      "step": 74440
    },
    {
      "epoch": 4.136111111111111,
      "grad_norm": 0.6515960097312927,
      "learning_rate": 8.638888888888889e-06,
      "loss": 0.0044,
      "step": 74450
    },
    {
      "epoch": 4.136666666666667,
      "grad_norm": 0.09186340868473053,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.0023,
      "step": 74460
    },
    {
      "epoch": 4.137222222222222,
      "grad_norm": 0.209219291806221,
      "learning_rate": 8.627777777777778e-06,
      "loss": 0.0034,
      "step": 74470
    },
    {
      "epoch": 4.137777777777778,
      "grad_norm": 0.10544934868812561,
      "learning_rate": 8.622222222222223e-06,
      "loss": 0.0026,
      "step": 74480
    },
    {
      "epoch": 4.138333333333334,
      "grad_norm": 0.06196950376033783,
      "learning_rate": 8.616666666666667e-06,
      "loss": 0.002,
      "step": 74490
    },
    {
      "epoch": 4.138888888888889,
      "grad_norm": 0.03360683098435402,
      "learning_rate": 8.611111111111112e-06,
      "loss": 0.0028,
      "step": 74500
    },
    {
      "epoch": 4.139444444444444,
      "grad_norm": 0.19978399574756622,
      "learning_rate": 8.605555555555555e-06,
      "loss": 0.0029,
      "step": 74510
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.17802827060222626,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0031,
      "step": 74520
    },
    {
      "epoch": 4.140555555555555,
      "grad_norm": 0.12199258804321289,
      "learning_rate": 8.594444444444446e-06,
      "loss": 0.0029,
      "step": 74530
    },
    {
      "epoch": 4.141111111111111,
      "grad_norm": 0.06041887775063515,
      "learning_rate": 8.58888888888889e-06,
      "loss": 0.0022,
      "step": 74540
    },
    {
      "epoch": 4.141666666666667,
      "grad_norm": 0.13110873103141785,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.0022,
      "step": 74550
    },
    {
      "epoch": 4.142222222222222,
      "grad_norm": 0.11887246370315552,
      "learning_rate": 8.577777777777778e-06,
      "loss": 0.0024,
      "step": 74560
    },
    {
      "epoch": 4.142777777777778,
      "grad_norm": 0.09471070021390915,
      "learning_rate": 8.572222222222224e-06,
      "loss": 0.003,
      "step": 74570
    },
    {
      "epoch": 4.1433333333333335,
      "grad_norm": 0.17826737463474274,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0043,
      "step": 74580
    },
    {
      "epoch": 4.143888888888889,
      "grad_norm": 0.1921289712190628,
      "learning_rate": 8.56111111111111e-06,
      "loss": 0.003,
      "step": 74590
    },
    {
      "epoch": 4.144444444444445,
      "grad_norm": 0.20846454799175262,
      "learning_rate": 8.555555555555556e-06,
      "loss": 0.0023,
      "step": 74600
    },
    {
      "epoch": 4.145,
      "grad_norm": 0.08961008489131927,
      "learning_rate": 8.550000000000001e-06,
      "loss": 0.0025,
      "step": 74610
    },
    {
      "epoch": 4.145555555555555,
      "grad_norm": 0.14917363226413727,
      "learning_rate": 8.544444444444445e-06,
      "loss": 0.0027,
      "step": 74620
    },
    {
      "epoch": 4.146111111111111,
      "grad_norm": 0.2670675814151764,
      "learning_rate": 8.53888888888889e-06,
      "loss": 0.0029,
      "step": 74630
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.06296751648187637,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0033,
      "step": 74640
    },
    {
      "epoch": 4.147222222222222,
      "grad_norm": 0.44841986894607544,
      "learning_rate": 8.527777777777777e-06,
      "loss": 0.0021,
      "step": 74650
    },
    {
      "epoch": 4.147777777777778,
      "grad_norm": 0.03827749565243721,
      "learning_rate": 8.522222222222222e-06,
      "loss": 0.0022,
      "step": 74660
    },
    {
      "epoch": 4.148333333333333,
      "grad_norm": 0.008471918292343616,
      "learning_rate": 8.516666666666668e-06,
      "loss": 0.0036,
      "step": 74670
    },
    {
      "epoch": 4.148888888888889,
      "grad_norm": 0.2077457457780838,
      "learning_rate": 8.511111111111111e-06,
      "loss": 0.0021,
      "step": 74680
    },
    {
      "epoch": 4.149444444444445,
      "grad_norm": 0.38637426495552063,
      "learning_rate": 8.505555555555555e-06,
      "loss": 0.0031,
      "step": 74690
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.033348191529512405,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0023,
      "step": 74700
    },
    {
      "epoch": 4.150555555555556,
      "grad_norm": 0.059884846210479736,
      "learning_rate": 8.494444444444445e-06,
      "loss": 0.0018,
      "step": 74710
    },
    {
      "epoch": 4.151111111111111,
      "grad_norm": 0.3850029408931732,
      "learning_rate": 8.488888888888889e-06,
      "loss": 0.0022,
      "step": 74720
    },
    {
      "epoch": 4.151666666666666,
      "grad_norm": 0.08952988684177399,
      "learning_rate": 8.483333333333334e-06,
      "loss": 0.0027,
      "step": 74730
    },
    {
      "epoch": 4.152222222222222,
      "grad_norm": 0.03090568445622921,
      "learning_rate": 8.477777777777778e-06,
      "loss": 0.0034,
      "step": 74740
    },
    {
      "epoch": 4.152777777777778,
      "grad_norm": 0.030646484345197678,
      "learning_rate": 8.472222222222223e-06,
      "loss": 0.0022,
      "step": 74750
    },
    {
      "epoch": 4.153333333333333,
      "grad_norm": 0.29671841859817505,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.003,
      "step": 74760
    },
    {
      "epoch": 4.153888888888889,
      "grad_norm": 0.031475771218538284,
      "learning_rate": 8.461111111111112e-06,
      "loss": 0.0032,
      "step": 74770
    },
    {
      "epoch": 4.154444444444445,
      "grad_norm": 0.38610246777534485,
      "learning_rate": 8.455555555555555e-06,
      "loss": 0.0018,
      "step": 74780
    },
    {
      "epoch": 4.155,
      "grad_norm": 0.14952953159809113,
      "learning_rate": 8.45e-06,
      "loss": 0.0034,
      "step": 74790
    },
    {
      "epoch": 4.155555555555556,
      "grad_norm": 0.09376132488250732,
      "learning_rate": 8.444444444444446e-06,
      "loss": 0.0026,
      "step": 74800
    },
    {
      "epoch": 4.1561111111111115,
      "grad_norm": 0.060996271669864655,
      "learning_rate": 8.43888888888889e-06,
      "loss": 0.0027,
      "step": 74810
    },
    {
      "epoch": 4.156666666666666,
      "grad_norm": 0.26988470554351807,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0023,
      "step": 74820
    },
    {
      "epoch": 4.157222222222222,
      "grad_norm": 0.14834080636501312,
      "learning_rate": 8.427777777777778e-06,
      "loss": 0.0033,
      "step": 74830
    },
    {
      "epoch": 4.157777777777778,
      "grad_norm": 0.3861762583255768,
      "learning_rate": 8.422222222222223e-06,
      "loss": 0.0024,
      "step": 74840
    },
    {
      "epoch": 4.158333333333333,
      "grad_norm": 0.01218092255294323,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.0038,
      "step": 74850
    },
    {
      "epoch": 4.158888888888889,
      "grad_norm": 0.1778751164674759,
      "learning_rate": 8.411111111111112e-06,
      "loss": 0.0032,
      "step": 74860
    },
    {
      "epoch": 4.1594444444444445,
      "grad_norm": 0.14880383014678955,
      "learning_rate": 8.405555555555556e-06,
      "loss": 0.0022,
      "step": 74870
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.21269267797470093,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0035,
      "step": 74880
    },
    {
      "epoch": 4.160555555555556,
      "grad_norm": 0.4194643199443817,
      "learning_rate": 8.394444444444444e-06,
      "loss": 0.002,
      "step": 74890
    },
    {
      "epoch": 4.161111111111111,
      "grad_norm": 0.08982162922620773,
      "learning_rate": 8.38888888888889e-06,
      "loss": 0.0024,
      "step": 74900
    },
    {
      "epoch": 4.161666666666667,
      "grad_norm": 0.20807838439941406,
      "learning_rate": 8.383333333333333e-06,
      "loss": 0.003,
      "step": 74910
    },
    {
      "epoch": 4.162222222222222,
      "grad_norm": 0.2382023185491562,
      "learning_rate": 8.377777777777779e-06,
      "loss": 0.0029,
      "step": 74920
    },
    {
      "epoch": 4.1627777777777775,
      "grad_norm": 0.34496647119522095,
      "learning_rate": 8.372222222222224e-06,
      "loss": 0.0029,
      "step": 74930
    },
    {
      "epoch": 4.163333333333333,
      "grad_norm": 0.2707951068878174,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0027,
      "step": 74940
    },
    {
      "epoch": 4.163888888888889,
      "grad_norm": 0.013306927867233753,
      "learning_rate": 8.361111111111111e-06,
      "loss": 0.0024,
      "step": 74950
    },
    {
      "epoch": 4.164444444444444,
      "grad_norm": 0.0893758088350296,
      "learning_rate": 8.355555555555556e-06,
      "loss": 0.0026,
      "step": 74960
    },
    {
      "epoch": 4.165,
      "grad_norm": 0.4448345899581909,
      "learning_rate": 8.350000000000001e-06,
      "loss": 0.0042,
      "step": 74970
    },
    {
      "epoch": 4.165555555555556,
      "grad_norm": 0.2074441909790039,
      "learning_rate": 8.344444444444445e-06,
      "loss": 0.0033,
      "step": 74980
    },
    {
      "epoch": 4.166111111111111,
      "grad_norm": 0.2076900750398636,
      "learning_rate": 8.338888888888888e-06,
      "loss": 0.0024,
      "step": 74990
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 0.23186419904232025,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0025,
      "step": 75000
    },
    {
      "epoch": 4.167222222222223,
      "grad_norm": 0.12192883342504501,
      "learning_rate": 8.327777777777779e-06,
      "loss": 0.0023,
      "step": 75010
    },
    {
      "epoch": 4.167777777777777,
      "grad_norm": 0.3848645091056824,
      "learning_rate": 8.322222222222223e-06,
      "loss": 0.0025,
      "step": 75020
    },
    {
      "epoch": 4.168333333333333,
      "grad_norm": 0.23825687170028687,
      "learning_rate": 8.316666666666668e-06,
      "loss": 0.0026,
      "step": 75030
    },
    {
      "epoch": 4.168888888888889,
      "grad_norm": 0.09537814557552338,
      "learning_rate": 8.311111111111111e-06,
      "loss": 0.0019,
      "step": 75040
    },
    {
      "epoch": 4.169444444444444,
      "grad_norm": 0.23723702132701874,
      "learning_rate": 8.305555555555555e-06,
      "loss": 0.0032,
      "step": 75050
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.2968013882637024,
      "learning_rate": 8.3e-06,
      "loss": 0.0032,
      "step": 75060
    },
    {
      "epoch": 4.170555555555556,
      "grad_norm": 0.14844690263271332,
      "learning_rate": 8.294444444444445e-06,
      "loss": 0.0027,
      "step": 75070
    },
    {
      "epoch": 4.171111111111111,
      "grad_norm": 0.09363415092229843,
      "learning_rate": 8.288888888888889e-06,
      "loss": 0.0021,
      "step": 75080
    },
    {
      "epoch": 4.171666666666667,
      "grad_norm": 0.060042817145586014,
      "learning_rate": 8.283333333333333e-06,
      "loss": 0.0035,
      "step": 75090
    },
    {
      "epoch": 4.1722222222222225,
      "grad_norm": 0.3270781338214874,
      "learning_rate": 8.27777777777778e-06,
      "loss": 0.0022,
      "step": 75100
    },
    {
      "epoch": 4.172777777777778,
      "grad_norm": 0.14795717597007751,
      "learning_rate": 8.272222222222223e-06,
      "loss": 0.0021,
      "step": 75110
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.059710316359996796,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0021,
      "step": 75120
    },
    {
      "epoch": 4.1738888888888885,
      "grad_norm": 0.3854363262653351,
      "learning_rate": 8.261111111111112e-06,
      "loss": 0.0026,
      "step": 75130
    },
    {
      "epoch": 4.174444444444444,
      "grad_norm": 0.4748130142688751,
      "learning_rate": 8.255555555555555e-06,
      "loss": 0.0044,
      "step": 75140
    },
    {
      "epoch": 4.175,
      "grad_norm": 0.1776878535747528,
      "learning_rate": 8.25e-06,
      "loss": 0.0021,
      "step": 75150
    },
    {
      "epoch": 4.1755555555555555,
      "grad_norm": 0.16355419158935547,
      "learning_rate": 8.244444444444444e-06,
      "loss": 0.002,
      "step": 75160
    },
    {
      "epoch": 4.176111111111111,
      "grad_norm": 0.2082756608724594,
      "learning_rate": 8.23888888888889e-06,
      "loss": 0.002,
      "step": 75170
    },
    {
      "epoch": 4.176666666666667,
      "grad_norm": 0.4195247292518616,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.0027,
      "step": 75180
    },
    {
      "epoch": 4.177222222222222,
      "grad_norm": 0.09025906771421432,
      "learning_rate": 8.227777777777778e-06,
      "loss": 0.0027,
      "step": 75190
    },
    {
      "epoch": 4.177777777777778,
      "grad_norm": 0.2965755760669708,
      "learning_rate": 8.222222222222223e-06,
      "loss": 0.0026,
      "step": 75200
    },
    {
      "epoch": 4.178333333333334,
      "grad_norm": 0.3258098363876343,
      "learning_rate": 8.216666666666667e-06,
      "loss": 0.0021,
      "step": 75210
    },
    {
      "epoch": 4.178888888888888,
      "grad_norm": 0.23750437796115875,
      "learning_rate": 8.21111111111111e-06,
      "loss": 0.0036,
      "step": 75220
    },
    {
      "epoch": 4.179444444444444,
      "grad_norm": 0.03129740431904793,
      "learning_rate": 8.205555555555556e-06,
      "loss": 0.0018,
      "step": 75230
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.16388918459415436,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0019,
      "step": 75240
    },
    {
      "epoch": 4.180555555555555,
      "grad_norm": 0.15066400170326233,
      "learning_rate": 8.194444444444445e-06,
      "loss": 0.0025,
      "step": 75250
    },
    {
      "epoch": 4.181111111111111,
      "grad_norm": 0.06095466390252113,
      "learning_rate": 8.188888888888888e-06,
      "loss": 0.0026,
      "step": 75260
    },
    {
      "epoch": 4.181666666666667,
      "grad_norm": 0.38892143964767456,
      "learning_rate": 8.183333333333333e-06,
      "loss": 0.0027,
      "step": 75270
    },
    {
      "epoch": 4.182222222222222,
      "grad_norm": 0.08933970332145691,
      "learning_rate": 8.177777777777779e-06,
      "loss": 0.0027,
      "step": 75280
    },
    {
      "epoch": 4.182777777777778,
      "grad_norm": 0.03158911317586899,
      "learning_rate": 8.172222222222222e-06,
      "loss": 0.0031,
      "step": 75290
    },
    {
      "epoch": 4.183333333333334,
      "grad_norm": 0.32722318172454834,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.0026,
      "step": 75300
    },
    {
      "epoch": 4.183888888888889,
      "grad_norm": 0.23720349371433258,
      "learning_rate": 8.161111111111111e-06,
      "loss": 0.002,
      "step": 75310
    },
    {
      "epoch": 4.184444444444445,
      "grad_norm": 0.4677964746952057,
      "learning_rate": 8.155555555555556e-06,
      "loss": 0.002,
      "step": 75320
    },
    {
      "epoch": 4.185,
      "grad_norm": 0.2962886393070221,
      "learning_rate": 8.15e-06,
      "loss": 0.0039,
      "step": 75330
    },
    {
      "epoch": 4.185555555555555,
      "grad_norm": 0.5539065599441528,
      "learning_rate": 8.144444444444445e-06,
      "loss": 0.0031,
      "step": 75340
    },
    {
      "epoch": 4.186111111111111,
      "grad_norm": 0.06025414541363716,
      "learning_rate": 8.138888888888889e-06,
      "loss": 0.0023,
      "step": 75350
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.34390825033187866,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0025,
      "step": 75360
    },
    {
      "epoch": 4.187222222222222,
      "grad_norm": 0.1051957979798317,
      "learning_rate": 8.12777777777778e-06,
      "loss": 0.0037,
      "step": 75370
    },
    {
      "epoch": 4.187777777777778,
      "grad_norm": 0.17836491763591766,
      "learning_rate": 8.122222222222223e-06,
      "loss": 0.0027,
      "step": 75380
    },
    {
      "epoch": 4.1883333333333335,
      "grad_norm": 0.08988139033317566,
      "learning_rate": 8.116666666666666e-06,
      "loss": 0.0038,
      "step": 75390
    },
    {
      "epoch": 4.188888888888889,
      "grad_norm": 0.14854492247104645,
      "learning_rate": 8.111111111111112e-06,
      "loss": 0.0036,
      "step": 75400
    },
    {
      "epoch": 4.189444444444445,
      "grad_norm": 0.0601583756506443,
      "learning_rate": 8.105555555555557e-06,
      "loss": 0.0016,
      "step": 75410
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.0901399552822113,
      "learning_rate": 8.1e-06,
      "loss": 0.0027,
      "step": 75420
    },
    {
      "epoch": 4.190555555555555,
      "grad_norm": 0.2375422865152359,
      "learning_rate": 8.094444444444444e-06,
      "loss": 0.0035,
      "step": 75430
    },
    {
      "epoch": 4.191111111111111,
      "grad_norm": 0.5410398840904236,
      "learning_rate": 8.08888888888889e-06,
      "loss": 0.0024,
      "step": 75440
    },
    {
      "epoch": 4.191666666666666,
      "grad_norm": 0.11890589445829391,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.0032,
      "step": 75450
    },
    {
      "epoch": 4.192222222222222,
      "grad_norm": 0.7669758200645447,
      "learning_rate": 8.077777777777778e-06,
      "loss": 0.0036,
      "step": 75460
    },
    {
      "epoch": 4.192777777777778,
      "grad_norm": 0.12008170783519745,
      "learning_rate": 8.072222222222223e-06,
      "loss": 0.0028,
      "step": 75470
    },
    {
      "epoch": 4.193333333333333,
      "grad_norm": 0.35628625750541687,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0021,
      "step": 75480
    },
    {
      "epoch": 4.193888888888889,
      "grad_norm": 0.03646451234817505,
      "learning_rate": 8.06111111111111e-06,
      "loss": 0.0029,
      "step": 75490
    },
    {
      "epoch": 4.194444444444445,
      "grad_norm": 0.08835233747959137,
      "learning_rate": 8.055555555555557e-06,
      "loss": 0.0043,
      "step": 75500
    },
    {
      "epoch": 4.195,
      "grad_norm": 0.20832030475139618,
      "learning_rate": 8.050000000000001e-06,
      "loss": 0.0025,
      "step": 75510
    },
    {
      "epoch": 4.195555555555556,
      "grad_norm": 0.11896437406539917,
      "learning_rate": 8.044444444444444e-06,
      "loss": 0.0031,
      "step": 75520
    },
    {
      "epoch": 4.196111111111112,
      "grad_norm": 0.010424942709505558,
      "learning_rate": 8.03888888888889e-06,
      "loss": 0.0022,
      "step": 75530
    },
    {
      "epoch": 4.196666666666666,
      "grad_norm": 0.061210863292217255,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0028,
      "step": 75540
    },
    {
      "epoch": 4.197222222222222,
      "grad_norm": 0.26738256216049194,
      "learning_rate": 8.027777777777778e-06,
      "loss": 0.0035,
      "step": 75550
    },
    {
      "epoch": 4.197777777777778,
      "grad_norm": 0.18302182853221893,
      "learning_rate": 8.022222222222222e-06,
      "loss": 0.0021,
      "step": 75560
    },
    {
      "epoch": 4.198333333333333,
      "grad_norm": 0.012733714655041695,
      "learning_rate": 8.016666666666667e-06,
      "loss": 0.0028,
      "step": 75570
    },
    {
      "epoch": 4.198888888888889,
      "grad_norm": 0.11989591270685196,
      "learning_rate": 8.01111111111111e-06,
      "loss": 0.0036,
      "step": 75580
    },
    {
      "epoch": 4.1994444444444445,
      "grad_norm": 0.20744635164737701,
      "learning_rate": 8.005555555555556e-06,
      "loss": 0.0017,
      "step": 75590
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.06097289174795151,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0037,
      "step": 75600
    },
    {
      "epoch": 4.200555555555556,
      "grad_norm": 0.03641877695918083,
      "learning_rate": 7.994444444444445e-06,
      "loss": 0.0023,
      "step": 75610
    },
    {
      "epoch": 4.2011111111111115,
      "grad_norm": 0.32927602529525757,
      "learning_rate": 7.988888888888888e-06,
      "loss": 0.0031,
      "step": 75620
    },
    {
      "epoch": 4.201666666666666,
      "grad_norm": 0.1780063509941101,
      "learning_rate": 7.983333333333334e-06,
      "loss": 0.0021,
      "step": 75630
    },
    {
      "epoch": 4.202222222222222,
      "grad_norm": 0.2964051365852356,
      "learning_rate": 7.977777777777779e-06,
      "loss": 0.0021,
      "step": 75640
    },
    {
      "epoch": 4.2027777777777775,
      "grad_norm": 0.09057771414518356,
      "learning_rate": 7.972222222222223e-06,
      "loss": 0.0028,
      "step": 75650
    },
    {
      "epoch": 4.203333333333333,
      "grad_norm": 0.5629005432128906,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0031,
      "step": 75660
    },
    {
      "epoch": 4.203888888888889,
      "grad_norm": 0.06955861300230026,
      "learning_rate": 7.961111111111111e-06,
      "loss": 0.003,
      "step": 75670
    },
    {
      "epoch": 4.204444444444444,
      "grad_norm": 0.184490367770195,
      "learning_rate": 7.955555555555557e-06,
      "loss": 0.0032,
      "step": 75680
    },
    {
      "epoch": 4.205,
      "grad_norm": 0.20787730813026428,
      "learning_rate": 7.95e-06,
      "loss": 0.0026,
      "step": 75690
    },
    {
      "epoch": 4.205555555555556,
      "grad_norm": 0.17821964621543884,
      "learning_rate": 7.944444444444445e-06,
      "loss": 0.0026,
      "step": 75700
    },
    {
      "epoch": 4.206111111111111,
      "grad_norm": 0.1203869879245758,
      "learning_rate": 7.938888888888889e-06,
      "loss": 0.0021,
      "step": 75710
    },
    {
      "epoch": 4.206666666666667,
      "grad_norm": 0.060861602425575256,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0026,
      "step": 75720
    },
    {
      "epoch": 4.207222222222223,
      "grad_norm": 0.060121189802885056,
      "learning_rate": 7.927777777777778e-06,
      "loss": 0.0018,
      "step": 75730
    },
    {
      "epoch": 4.207777777777777,
      "grad_norm": 0.35592007637023926,
      "learning_rate": 7.922222222222223e-06,
      "loss": 0.0023,
      "step": 75740
    },
    {
      "epoch": 4.208333333333333,
      "grad_norm": 0.06047377362847328,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.0024,
      "step": 75750
    },
    {
      "epoch": 4.208888888888889,
      "grad_norm": 0.2383279651403427,
      "learning_rate": 7.91111111111111e-06,
      "loss": 0.0027,
      "step": 75760
    },
    {
      "epoch": 4.209444444444444,
      "grad_norm": 0.29930004477500916,
      "learning_rate": 7.905555555555557e-06,
      "loss": 0.0022,
      "step": 75770
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.12052050232887268,
      "learning_rate": 7.9e-06,
      "loss": 0.0027,
      "step": 75780
    },
    {
      "epoch": 4.210555555555556,
      "grad_norm": 0.014658993110060692,
      "learning_rate": 7.894444444444444e-06,
      "loss": 0.0032,
      "step": 75790
    },
    {
      "epoch": 4.211111111111111,
      "grad_norm": 0.30756768584251404,
      "learning_rate": 7.88888888888889e-06,
      "loss": 0.0024,
      "step": 75800
    },
    {
      "epoch": 4.211666666666667,
      "grad_norm": 0.11915817111730576,
      "learning_rate": 7.883333333333335e-06,
      "loss": 0.0031,
      "step": 75810
    },
    {
      "epoch": 4.2122222222222225,
      "grad_norm": 0.2682527005672455,
      "learning_rate": 7.877777777777778e-06,
      "loss": 0.0018,
      "step": 75820
    },
    {
      "epoch": 4.212777777777778,
      "grad_norm": 0.05141812190413475,
      "learning_rate": 7.872222222222222e-06,
      "loss": 0.0025,
      "step": 75830
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.11900077015161514,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0027,
      "step": 75840
    },
    {
      "epoch": 4.213888888888889,
      "grad_norm": 0.4742238223552704,
      "learning_rate": 7.861111111111112e-06,
      "loss": 0.0032,
      "step": 75850
    },
    {
      "epoch": 4.214444444444444,
      "grad_norm": 0.08975690603256226,
      "learning_rate": 7.855555555555556e-06,
      "loss": 0.0032,
      "step": 75860
    },
    {
      "epoch": 4.215,
      "grad_norm": 0.2964344024658203,
      "learning_rate": 7.850000000000001e-06,
      "loss": 0.0027,
      "step": 75870
    },
    {
      "epoch": 4.2155555555555555,
      "grad_norm": 0.030986668542027473,
      "learning_rate": 7.844444444444445e-06,
      "loss": 0.002,
      "step": 75880
    },
    {
      "epoch": 4.216111111111111,
      "grad_norm": 0.20791871845722198,
      "learning_rate": 7.838888888888888e-06,
      "loss": 0.0035,
      "step": 75890
    },
    {
      "epoch": 4.216666666666667,
      "grad_norm": 0.2690374255180359,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.0016,
      "step": 75900
    },
    {
      "epoch": 4.217222222222222,
      "grad_norm": 0.4738577902317047,
      "learning_rate": 7.827777777777779e-06,
      "loss": 0.0017,
      "step": 75910
    },
    {
      "epoch": 4.217777777777778,
      "grad_norm": 0.35606804490089417,
      "learning_rate": 7.822222222222222e-06,
      "loss": 0.0025,
      "step": 75920
    },
    {
      "epoch": 4.218333333333334,
      "grad_norm": 0.20841751992702484,
      "learning_rate": 7.816666666666666e-06,
      "loss": 0.0025,
      "step": 75930
    },
    {
      "epoch": 4.2188888888888885,
      "grad_norm": 0.23742255568504333,
      "learning_rate": 7.811111111111113e-06,
      "loss": 0.0032,
      "step": 75940
    },
    {
      "epoch": 4.219444444444444,
      "grad_norm": 0.14818771183490753,
      "learning_rate": 7.805555555555556e-06,
      "loss": 0.0029,
      "step": 75950
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.35569438338279724,
      "learning_rate": 7.8e-06,
      "loss": 0.0026,
      "step": 75960
    },
    {
      "epoch": 4.220555555555555,
      "grad_norm": 0.2718542218208313,
      "learning_rate": 7.794444444444445e-06,
      "loss": 0.0023,
      "step": 75970
    },
    {
      "epoch": 4.221111111111111,
      "grad_norm": 0.23706366121768951,
      "learning_rate": 7.788888888888889e-06,
      "loss": 0.0031,
      "step": 75980
    },
    {
      "epoch": 4.221666666666667,
      "grad_norm": 0.09110987186431885,
      "learning_rate": 7.783333333333334e-06,
      "loss": 0.0022,
      "step": 75990
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.28750211000442505,
      "learning_rate": 7.777777777777777e-06,
      "loss": 0.0027,
      "step": 76000
    },
    {
      "epoch": 4.222777777777778,
      "grad_norm": 0.5552582144737244,
      "learning_rate": 7.772222222222223e-06,
      "loss": 0.0029,
      "step": 76010
    },
    {
      "epoch": 4.223333333333334,
      "grad_norm": 0.089775949716568,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0029,
      "step": 76020
    },
    {
      "epoch": 4.223888888888889,
      "grad_norm": 0.14917227625846863,
      "learning_rate": 7.761111111111112e-06,
      "loss": 0.0038,
      "step": 76030
    },
    {
      "epoch": 4.224444444444444,
      "grad_norm": 0.11851999908685684,
      "learning_rate": 7.755555555555557e-06,
      "loss": 0.0029,
      "step": 76040
    },
    {
      "epoch": 4.225,
      "grad_norm": 0.5346178412437439,
      "learning_rate": 7.75e-06,
      "loss": 0.0027,
      "step": 76050
    },
    {
      "epoch": 4.225555555555555,
      "grad_norm": 0.4090634882450104,
      "learning_rate": 7.744444444444444e-06,
      "loss": 0.0022,
      "step": 76060
    },
    {
      "epoch": 4.226111111111111,
      "grad_norm": 0.03112923353910446,
      "learning_rate": 7.738888888888889e-06,
      "loss": 0.0022,
      "step": 76070
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.41491374373435974,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0032,
      "step": 76080
    },
    {
      "epoch": 4.227222222222222,
      "grad_norm": 0.266814261674881,
      "learning_rate": 7.727777777777778e-06,
      "loss": 0.002,
      "step": 76090
    },
    {
      "epoch": 4.227777777777778,
      "grad_norm": 0.656500518321991,
      "learning_rate": 7.722222222222223e-06,
      "loss": 0.0024,
      "step": 76100
    },
    {
      "epoch": 4.2283333333333335,
      "grad_norm": 0.1806747317314148,
      "learning_rate": 7.716666666666667e-06,
      "loss": 0.0036,
      "step": 76110
    },
    {
      "epoch": 4.228888888888889,
      "grad_norm": 0.14858698844909668,
      "learning_rate": 7.711111111111112e-06,
      "loss": 0.0018,
      "step": 76120
    },
    {
      "epoch": 4.229444444444445,
      "grad_norm": 0.38579386472702026,
      "learning_rate": 7.705555555555556e-06,
      "loss": 0.0028,
      "step": 76130
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.2177344560623169,
      "learning_rate": 7.7e-06,
      "loss": 0.0038,
      "step": 76140
    },
    {
      "epoch": 4.230555555555555,
      "grad_norm": 0.34974223375320435,
      "learning_rate": 7.694444444444444e-06,
      "loss": 0.0019,
      "step": 76150
    },
    {
      "epoch": 4.231111111111111,
      "grad_norm": 0.05273229628801346,
      "learning_rate": 7.68888888888889e-06,
      "loss": 0.0045,
      "step": 76160
    },
    {
      "epoch": 4.2316666666666665,
      "grad_norm": 0.04548921808600426,
      "learning_rate": 7.683333333333335e-06,
      "loss": 0.0028,
      "step": 76170
    },
    {
      "epoch": 4.232222222222222,
      "grad_norm": 0.6232820749282837,
      "learning_rate": 7.677777777777778e-06,
      "loss": 0.003,
      "step": 76180
    },
    {
      "epoch": 4.232777777777778,
      "grad_norm": 0.3678492307662964,
      "learning_rate": 7.672222222222222e-06,
      "loss": 0.0029,
      "step": 76190
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 0.3566942512989044,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.003,
      "step": 76200
    },
    {
      "epoch": 4.233888888888889,
      "grad_norm": 0.11859721690416336,
      "learning_rate": 7.661111111111112e-06,
      "loss": 0.0023,
      "step": 76210
    },
    {
      "epoch": 4.234444444444445,
      "grad_norm": 0.2367652952671051,
      "learning_rate": 7.655555555555556e-06,
      "loss": 0.0031,
      "step": 76220
    },
    {
      "epoch": 4.235,
      "grad_norm": 0.35588160157203674,
      "learning_rate": 7.65e-06,
      "loss": 0.0027,
      "step": 76230
    },
    {
      "epoch": 4.235555555555556,
      "grad_norm": 0.06129343807697296,
      "learning_rate": 7.644444444444445e-06,
      "loss": 0.0038,
      "step": 76240
    },
    {
      "epoch": 4.236111111111111,
      "grad_norm": 0.6361842155456543,
      "learning_rate": 7.63888888888889e-06,
      "loss": 0.0038,
      "step": 76250
    },
    {
      "epoch": 4.236666666666666,
      "grad_norm": 0.05946587026119232,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.0033,
      "step": 76260
    },
    {
      "epoch": 4.237222222222222,
      "grad_norm": 0.37237903475761414,
      "learning_rate": 7.627777777777778e-06,
      "loss": 0.0024,
      "step": 76270
    },
    {
      "epoch": 4.237777777777778,
      "grad_norm": 0.29610130190849304,
      "learning_rate": 7.6222222222222225e-06,
      "loss": 0.003,
      "step": 76280
    },
    {
      "epoch": 4.238333333333333,
      "grad_norm": 0.17761258780956268,
      "learning_rate": 7.616666666666666e-06,
      "loss": 0.0037,
      "step": 76290
    },
    {
      "epoch": 4.238888888888889,
      "grad_norm": 0.1570700854063034,
      "learning_rate": 7.611111111111112e-06,
      "loss": 0.0038,
      "step": 76300
    },
    {
      "epoch": 4.239444444444445,
      "grad_norm": 0.38248011469841003,
      "learning_rate": 7.605555555555556e-06,
      "loss": 0.0023,
      "step": 76310
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.14867112040519714,
      "learning_rate": 7.6e-06,
      "loss": 0.0025,
      "step": 76320
    },
    {
      "epoch": 4.240555555555556,
      "grad_norm": 0.26773807406425476,
      "learning_rate": 7.5944444444444445e-06,
      "loss": 0.0033,
      "step": 76330
    },
    {
      "epoch": 4.2411111111111115,
      "grad_norm": 0.29698988795280457,
      "learning_rate": 7.58888888888889e-06,
      "loss": 0.0031,
      "step": 76340
    },
    {
      "epoch": 4.241666666666666,
      "grad_norm": 0.11221247911453247,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.0024,
      "step": 76350
    },
    {
      "epoch": 4.242222222222222,
      "grad_norm": 0.20795105397701263,
      "learning_rate": 7.577777777777778e-06,
      "loss": 0.002,
      "step": 76360
    },
    {
      "epoch": 4.2427777777777775,
      "grad_norm": 0.17873087525367737,
      "learning_rate": 7.572222222222222e-06,
      "loss": 0.0034,
      "step": 76370
    },
    {
      "epoch": 4.243333333333333,
      "grad_norm": 0.20776519179344177,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0018,
      "step": 76380
    },
    {
      "epoch": 4.243888888888889,
      "grad_norm": 0.26715219020843506,
      "learning_rate": 7.561111111111112e-06,
      "loss": 0.0029,
      "step": 76390
    },
    {
      "epoch": 4.2444444444444445,
      "grad_norm": 0.3189851641654968,
      "learning_rate": 7.555555555555556e-06,
      "loss": 0.0036,
      "step": 76400
    },
    {
      "epoch": 4.245,
      "grad_norm": 0.17636090517044067,
      "learning_rate": 7.55e-06,
      "loss": 0.0031,
      "step": 76410
    },
    {
      "epoch": 4.245555555555556,
      "grad_norm": 0.0340876504778862,
      "learning_rate": 7.544444444444444e-06,
      "loss": 0.0017,
      "step": 76420
    },
    {
      "epoch": 4.246111111111111,
      "grad_norm": 0.09016944468021393,
      "learning_rate": 7.538888888888889e-06,
      "loss": 0.0031,
      "step": 76430
    },
    {
      "epoch": 4.246666666666667,
      "grad_norm": 0.3556675314903259,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0022,
      "step": 76440
    },
    {
      "epoch": 4.247222222222222,
      "grad_norm": 0.26697543263435364,
      "learning_rate": 7.527777777777778e-06,
      "loss": 0.0023,
      "step": 76450
    },
    {
      "epoch": 4.247777777777777,
      "grad_norm": 0.3557256758213043,
      "learning_rate": 7.5222222222222226e-06,
      "loss": 0.0023,
      "step": 76460
    },
    {
      "epoch": 4.248333333333333,
      "grad_norm": 0.06392210721969604,
      "learning_rate": 7.516666666666668e-06,
      "loss": 0.0024,
      "step": 76470
    },
    {
      "epoch": 4.248888888888889,
      "grad_norm": 0.12129488587379456,
      "learning_rate": 7.511111111111112e-06,
      "loss": 0.002,
      "step": 76480
    },
    {
      "epoch": 4.249444444444444,
      "grad_norm": 0.1488616019487381,
      "learning_rate": 7.505555555555556e-06,
      "loss": 0.0033,
      "step": 76490
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.17921088635921478,
      "learning_rate": 7.5e-06,
      "loss": 0.0024,
      "step": 76500
    },
    {
      "epoch": 4.250555555555556,
      "grad_norm": 0.23767508566379547,
      "learning_rate": 7.494444444444445e-06,
      "loss": 0.0019,
      "step": 76510
    },
    {
      "epoch": 4.251111111111111,
      "grad_norm": 0.4447353482246399,
      "learning_rate": 7.48888888888889e-06,
      "loss": 0.0029,
      "step": 76520
    },
    {
      "epoch": 4.251666666666667,
      "grad_norm": 0.20729900896549225,
      "learning_rate": 7.483333333333334e-06,
      "loss": 0.0032,
      "step": 76530
    },
    {
      "epoch": 4.252222222222223,
      "grad_norm": 0.3265411853790283,
      "learning_rate": 7.477777777777778e-06,
      "loss": 0.0031,
      "step": 76540
    },
    {
      "epoch": 4.252777777777778,
      "grad_norm": 0.03487303853034973,
      "learning_rate": 7.472222222222222e-06,
      "loss": 0.0026,
      "step": 76550
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.12020918726921082,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0033,
      "step": 76560
    },
    {
      "epoch": 4.253888888888889,
      "grad_norm": 0.2376563549041748,
      "learning_rate": 7.461111111111112e-06,
      "loss": 0.0025,
      "step": 76570
    },
    {
      "epoch": 4.254444444444444,
      "grad_norm": 0.26730602979660034,
      "learning_rate": 7.455555555555556e-06,
      "loss": 0.0027,
      "step": 76580
    },
    {
      "epoch": 4.255,
      "grad_norm": 0.12319410592317581,
      "learning_rate": 7.45e-06,
      "loss": 0.0028,
      "step": 76590
    },
    {
      "epoch": 4.2555555555555555,
      "grad_norm": 0.1546197235584259,
      "learning_rate": 7.444444444444444e-06,
      "loss": 0.0024,
      "step": 76600
    },
    {
      "epoch": 4.256111111111111,
      "grad_norm": 0.2852616310119629,
      "learning_rate": 7.4388888888888895e-06,
      "loss": 0.0039,
      "step": 76610
    },
    {
      "epoch": 4.256666666666667,
      "grad_norm": 0.4799204170703888,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.0023,
      "step": 76620
    },
    {
      "epoch": 4.2572222222222225,
      "grad_norm": 0.24050217866897583,
      "learning_rate": 7.427777777777778e-06,
      "loss": 0.003,
      "step": 76630
    },
    {
      "epoch": 4.257777777777778,
      "grad_norm": 0.4440864026546478,
      "learning_rate": 7.422222222222222e-06,
      "loss": 0.0034,
      "step": 76640
    },
    {
      "epoch": 4.258333333333334,
      "grad_norm": 0.24902275204658508,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.0034,
      "step": 76650
    },
    {
      "epoch": 4.2588888888888885,
      "grad_norm": 0.13622578978538513,
      "learning_rate": 7.4111111111111115e-06,
      "loss": 0.0032,
      "step": 76660
    },
    {
      "epoch": 4.259444444444444,
      "grad_norm": 0.06066866219043732,
      "learning_rate": 7.405555555555556e-06,
      "loss": 0.0017,
      "step": 76670
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.0891459733247757,
      "learning_rate": 7.4e-06,
      "loss": 0.0043,
      "step": 76680
    },
    {
      "epoch": 4.260555555555555,
      "grad_norm": 0.03466246277093887,
      "learning_rate": 7.394444444444444e-06,
      "loss": 0.0021,
      "step": 76690
    },
    {
      "epoch": 4.261111111111111,
      "grad_norm": 0.0893513634800911,
      "learning_rate": 7.38888888888889e-06,
      "loss": 0.0029,
      "step": 76700
    },
    {
      "epoch": 4.261666666666667,
      "grad_norm": 0.17809151113033295,
      "learning_rate": 7.3833333333333335e-06,
      "loss": 0.0024,
      "step": 76710
    },
    {
      "epoch": 4.262222222222222,
      "grad_norm": 0.06137300655245781,
      "learning_rate": 7.377777777777778e-06,
      "loss": 0.002,
      "step": 76720
    },
    {
      "epoch": 4.262777777777778,
      "grad_norm": 0.26643240451812744,
      "learning_rate": 7.372222222222222e-06,
      "loss": 0.002,
      "step": 76730
    },
    {
      "epoch": 4.263333333333334,
      "grad_norm": 0.4141864776611328,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0027,
      "step": 76740
    },
    {
      "epoch": 4.263888888888889,
      "grad_norm": 0.1810508668422699,
      "learning_rate": 7.361111111111112e-06,
      "loss": 0.0024,
      "step": 76750
    },
    {
      "epoch": 4.264444444444444,
      "grad_norm": 0.1873965561389923,
      "learning_rate": 7.3555555555555555e-06,
      "loss": 0.0026,
      "step": 76760
    },
    {
      "epoch": 4.265,
      "grad_norm": 0.04737522453069687,
      "learning_rate": 7.35e-06,
      "loss": 0.0024,
      "step": 76770
    },
    {
      "epoch": 4.265555555555555,
      "grad_norm": 0.11863032728433609,
      "learning_rate": 7.344444444444445e-06,
      "loss": 0.0021,
      "step": 76780
    },
    {
      "epoch": 4.266111111111111,
      "grad_norm": 0.0898222029209137,
      "learning_rate": 7.33888888888889e-06,
      "loss": 0.0035,
      "step": 76790
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.35597091913223267,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0022,
      "step": 76800
    },
    {
      "epoch": 4.267222222222222,
      "grad_norm": 0.20858816802501678,
      "learning_rate": 7.3277777777777775e-06,
      "loss": 0.0037,
      "step": 76810
    },
    {
      "epoch": 4.267777777777778,
      "grad_norm": 0.4066545367240906,
      "learning_rate": 7.322222222222222e-06,
      "loss": 0.0032,
      "step": 76820
    },
    {
      "epoch": 4.2683333333333335,
      "grad_norm": 0.36125943064689636,
      "learning_rate": 7.316666666666667e-06,
      "loss": 0.0033,
      "step": 76830
    },
    {
      "epoch": 4.268888888888889,
      "grad_norm": 0.03204275295138359,
      "learning_rate": 7.311111111111112e-06,
      "loss": 0.0037,
      "step": 76840
    },
    {
      "epoch": 4.269444444444445,
      "grad_norm": 0.10971499979496002,
      "learning_rate": 7.305555555555556e-06,
      "loss": 0.0035,
      "step": 76850
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.545500636100769,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0024,
      "step": 76860
    },
    {
      "epoch": 4.270555555555555,
      "grad_norm": 0.09032224863767624,
      "learning_rate": 7.294444444444446e-06,
      "loss": 0.0025,
      "step": 76870
    },
    {
      "epoch": 4.271111111111111,
      "grad_norm": 0.036841150373220444,
      "learning_rate": 7.288888888888889e-06,
      "loss": 0.0018,
      "step": 76880
    },
    {
      "epoch": 4.2716666666666665,
      "grad_norm": 0.44894662499427795,
      "learning_rate": 7.283333333333334e-06,
      "loss": 0.0028,
      "step": 76890
    },
    {
      "epoch": 4.272222222222222,
      "grad_norm": 0.4150145947933197,
      "learning_rate": 7.277777777777778e-06,
      "loss": 0.0024,
      "step": 76900
    },
    {
      "epoch": 4.272777777777778,
      "grad_norm": 0.2969205379486084,
      "learning_rate": 7.272222222222222e-06,
      "loss": 0.002,
      "step": 76910
    },
    {
      "epoch": 4.273333333333333,
      "grad_norm": 0.20850148797035217,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.003,
      "step": 76920
    },
    {
      "epoch": 4.273888888888889,
      "grad_norm": 0.17501255869865417,
      "learning_rate": 7.261111111111111e-06,
      "loss": 0.0022,
      "step": 76930
    },
    {
      "epoch": 4.274444444444445,
      "grad_norm": 0.11887971311807632,
      "learning_rate": 7.255555555555556e-06,
      "loss": 0.0023,
      "step": 76940
    },
    {
      "epoch": 4.275,
      "grad_norm": 0.060570262372493744,
      "learning_rate": 7.25e-06,
      "loss": 0.0027,
      "step": 76950
    },
    {
      "epoch": 4.275555555555556,
      "grad_norm": 0.17859980463981628,
      "learning_rate": 7.244444444444445e-06,
      "loss": 0.0036,
      "step": 76960
    },
    {
      "epoch": 4.276111111111111,
      "grad_norm": 0.20829634368419647,
      "learning_rate": 7.23888888888889e-06,
      "loss": 0.0031,
      "step": 76970
    },
    {
      "epoch": 4.276666666666666,
      "grad_norm": 0.17809394001960754,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.003,
      "step": 76980
    },
    {
      "epoch": 4.277222222222222,
      "grad_norm": 0.060715146362781525,
      "learning_rate": 7.227777777777778e-06,
      "loss": 0.0029,
      "step": 76990
    },
    {
      "epoch": 4.277777777777778,
      "grad_norm": 0.03126845881342888,
      "learning_rate": 7.222222222222222e-06,
      "loss": 0.0021,
      "step": 77000
    },
    {
      "epoch": 4.278333333333333,
      "grad_norm": 0.011293978430330753,
      "learning_rate": 7.216666666666667e-06,
      "loss": 0.0028,
      "step": 77010
    },
    {
      "epoch": 4.278888888888889,
      "grad_norm": 0.3857092261314392,
      "learning_rate": 7.211111111111112e-06,
      "loss": 0.0024,
      "step": 77020
    },
    {
      "epoch": 4.279444444444445,
      "grad_norm": 0.03303081914782524,
      "learning_rate": 7.205555555555555e-06,
      "loss": 0.0026,
      "step": 77030
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.20843826234340668,
      "learning_rate": 7.2e-06,
      "loss": 0.0019,
      "step": 77040
    },
    {
      "epoch": 4.280555555555556,
      "grad_norm": 0.4147591292858124,
      "learning_rate": 7.194444444444445e-06,
      "loss": 0.0041,
      "step": 77050
    },
    {
      "epoch": 4.281111111111111,
      "grad_norm": 0.11911720782518387,
      "learning_rate": 7.188888888888889e-06,
      "loss": 0.0023,
      "step": 77060
    },
    {
      "epoch": 4.281666666666666,
      "grad_norm": 0.032520610839128494,
      "learning_rate": 7.183333333333334e-06,
      "loss": 0.0022,
      "step": 77070
    },
    {
      "epoch": 4.282222222222222,
      "grad_norm": 0.20828840136528015,
      "learning_rate": 7.177777777777778e-06,
      "loss": 0.0026,
      "step": 77080
    },
    {
      "epoch": 4.282777777777778,
      "grad_norm": 0.23680803179740906,
      "learning_rate": 7.172222222222223e-06,
      "loss": 0.0021,
      "step": 77090
    },
    {
      "epoch": 4.283333333333333,
      "grad_norm": 0.08977906405925751,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0029,
      "step": 77100
    },
    {
      "epoch": 4.283888888888889,
      "grad_norm": 0.4201156795024872,
      "learning_rate": 7.161111111111111e-06,
      "loss": 0.0035,
      "step": 77110
    },
    {
      "epoch": 4.2844444444444445,
      "grad_norm": 0.17820805311203003,
      "learning_rate": 7.155555555555556e-06,
      "loss": 0.0029,
      "step": 77120
    },
    {
      "epoch": 4.285,
      "grad_norm": 0.5704777240753174,
      "learning_rate": 7.15e-06,
      "loss": 0.003,
      "step": 77130
    },
    {
      "epoch": 4.285555555555556,
      "grad_norm": 0.089162677526474,
      "learning_rate": 7.144444444444445e-06,
      "loss": 0.0016,
      "step": 77140
    },
    {
      "epoch": 4.286111111111111,
      "grad_norm": 0.20288346707820892,
      "learning_rate": 7.13888888888889e-06,
      "loss": 0.0035,
      "step": 77150
    },
    {
      "epoch": 4.286666666666667,
      "grad_norm": 0.4600162208080292,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0032,
      "step": 77160
    },
    {
      "epoch": 4.287222222222222,
      "grad_norm": 0.1785845160484314,
      "learning_rate": 7.127777777777778e-06,
      "loss": 0.0028,
      "step": 77170
    },
    {
      "epoch": 4.2877777777777775,
      "grad_norm": 0.2068888396024704,
      "learning_rate": 7.122222222222223e-06,
      "loss": 0.0022,
      "step": 77180
    },
    {
      "epoch": 4.288333333333333,
      "grad_norm": 0.13589002192020416,
      "learning_rate": 7.116666666666667e-06,
      "loss": 0.0035,
      "step": 77190
    },
    {
      "epoch": 4.288888888888889,
      "grad_norm": 0.01492625754326582,
      "learning_rate": 7.111111111111112e-06,
      "loss": 0.003,
      "step": 77200
    },
    {
      "epoch": 4.289444444444444,
      "grad_norm": 0.08981765806674957,
      "learning_rate": 7.105555555555555e-06,
      "loss": 0.0029,
      "step": 77210
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.11862608790397644,
      "learning_rate": 7.1e-06,
      "loss": 0.0039,
      "step": 77220
    },
    {
      "epoch": 4.290555555555556,
      "grad_norm": 0.07120047509670258,
      "learning_rate": 7.094444444444445e-06,
      "loss": 0.0025,
      "step": 77230
    },
    {
      "epoch": 4.291111111111111,
      "grad_norm": 0.013260947540402412,
      "learning_rate": 7.0888888888888894e-06,
      "loss": 0.0028,
      "step": 77240
    },
    {
      "epoch": 4.291666666666667,
      "grad_norm": 0.03248362988233566,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.0026,
      "step": 77250
    },
    {
      "epoch": 4.292222222222223,
      "grad_norm": 0.4153195321559906,
      "learning_rate": 7.077777777777777e-06,
      "loss": 0.0032,
      "step": 77260
    },
    {
      "epoch": 4.292777777777777,
      "grad_norm": 0.09028782695531845,
      "learning_rate": 7.0722222222222235e-06,
      "loss": 0.0024,
      "step": 77270
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.6667385101318359,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0035,
      "step": 77280
    },
    {
      "epoch": 4.293888888888889,
      "grad_norm": 0.12106811255216599,
      "learning_rate": 7.0611111111111115e-06,
      "loss": 0.002,
      "step": 77290
    },
    {
      "epoch": 4.294444444444444,
      "grad_norm": 0.17832905054092407,
      "learning_rate": 7.055555555555556e-06,
      "loss": 0.0032,
      "step": 77300
    },
    {
      "epoch": 4.295,
      "grad_norm": 0.2971732020378113,
      "learning_rate": 7.049999999999999e-06,
      "loss": 0.0034,
      "step": 77310
    },
    {
      "epoch": 4.295555555555556,
      "grad_norm": 0.032551828771829605,
      "learning_rate": 7.0444444444444455e-06,
      "loss": 0.0037,
      "step": 77320
    },
    {
      "epoch": 4.296111111111111,
      "grad_norm": 0.06098084896802902,
      "learning_rate": 7.038888888888889e-06,
      "loss": 0.002,
      "step": 77330
    },
    {
      "epoch": 4.296666666666667,
      "grad_norm": 0.08955459296703339,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.0039,
      "step": 77340
    },
    {
      "epoch": 4.2972222222222225,
      "grad_norm": 0.09216693043708801,
      "learning_rate": 7.027777777777778e-06,
      "loss": 0.0028,
      "step": 77350
    },
    {
      "epoch": 4.297777777777778,
      "grad_norm": 0.3497937023639679,
      "learning_rate": 7.022222222222223e-06,
      "loss": 0.0024,
      "step": 77360
    },
    {
      "epoch": 4.298333333333334,
      "grad_norm": 0.20769721269607544,
      "learning_rate": 7.0166666666666675e-06,
      "loss": 0.0035,
      "step": 77370
    },
    {
      "epoch": 4.2988888888888885,
      "grad_norm": 0.08991527557373047,
      "learning_rate": 7.011111111111111e-06,
      "loss": 0.0042,
      "step": 77380
    },
    {
      "epoch": 4.299444444444444,
      "grad_norm": 0.237187922000885,
      "learning_rate": 7.0055555555555555e-06,
      "loss": 0.0029,
      "step": 77390
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.035710275173187256,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0026,
      "step": 77400
    },
    {
      "epoch": 4.3005555555555555,
      "grad_norm": 0.20769758522510529,
      "learning_rate": 6.994444444444445e-06,
      "loss": 0.0033,
      "step": 77410
    },
    {
      "epoch": 4.301111111111111,
      "grad_norm": 0.12632882595062256,
      "learning_rate": 6.9888888888888895e-06,
      "loss": 0.0025,
      "step": 77420
    },
    {
      "epoch": 4.301666666666667,
      "grad_norm": 0.11757302284240723,
      "learning_rate": 6.983333333333333e-06,
      "loss": 0.0027,
      "step": 77430
    },
    {
      "epoch": 4.302222222222222,
      "grad_norm": 0.03217347338795662,
      "learning_rate": 6.9777777777777775e-06,
      "loss": 0.0021,
      "step": 77440
    },
    {
      "epoch": 4.302777777777778,
      "grad_norm": 0.41507747769355774,
      "learning_rate": 6.972222222222223e-06,
      "loss": 0.0016,
      "step": 77450
    },
    {
      "epoch": 4.303333333333334,
      "grad_norm": 0.5546738505363464,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0029,
      "step": 77460
    },
    {
      "epoch": 4.303888888888888,
      "grad_norm": 0.12486463785171509,
      "learning_rate": 6.9611111111111116e-06,
      "loss": 0.0036,
      "step": 77470
    },
    {
      "epoch": 4.304444444444444,
      "grad_norm": 0.03338286280632019,
      "learning_rate": 6.955555555555555e-06,
      "loss": 0.0026,
      "step": 77480
    },
    {
      "epoch": 4.305,
      "grad_norm": 0.11964752525091171,
      "learning_rate": 6.950000000000001e-06,
      "loss": 0.0034,
      "step": 77490
    },
    {
      "epoch": 4.305555555555555,
      "grad_norm": 0.3258249759674072,
      "learning_rate": 6.944444444444445e-06,
      "loss": 0.002,
      "step": 77500
    },
    {
      "epoch": 4.306111111111111,
      "grad_norm": 0.14656497538089752,
      "learning_rate": 6.938888888888889e-06,
      "loss": 0.0029,
      "step": 77510
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.2670745253562927,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0029,
      "step": 77520
    },
    {
      "epoch": 4.307222222222222,
      "grad_norm": 0.31593412160873413,
      "learning_rate": 6.927777777777777e-06,
      "loss": 0.0023,
      "step": 77530
    },
    {
      "epoch": 4.307777777777778,
      "grad_norm": 0.5057662129402161,
      "learning_rate": 6.922222222222223e-06,
      "loss": 0.0028,
      "step": 77540
    },
    {
      "epoch": 4.308333333333334,
      "grad_norm": 0.26740917563438416,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.0037,
      "step": 77550
    },
    {
      "epoch": 4.308888888888889,
      "grad_norm": 0.3972141742706299,
      "learning_rate": 6.911111111111111e-06,
      "loss": 0.0024,
      "step": 77560
    },
    {
      "epoch": 4.309444444444445,
      "grad_norm": 0.06317492574453354,
      "learning_rate": 6.905555555555556e-06,
      "loss": 0.0029,
      "step": 77570
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.1202690377831459,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0018,
      "step": 77580
    },
    {
      "epoch": 4.310555555555555,
      "grad_norm": 0.4443148076534271,
      "learning_rate": 6.894444444444445e-06,
      "loss": 0.0026,
      "step": 77590
    },
    {
      "epoch": 4.311111111111111,
      "grad_norm": 0.3694581687450409,
      "learning_rate": 6.888888888888889e-06,
      "loss": 0.0031,
      "step": 77600
    },
    {
      "epoch": 4.3116666666666665,
      "grad_norm": 0.2089097648859024,
      "learning_rate": 6.883333333333333e-06,
      "loss": 0.0026,
      "step": 77610
    },
    {
      "epoch": 4.312222222222222,
      "grad_norm": 0.20922955870628357,
      "learning_rate": 6.877777777777778e-06,
      "loss": 0.0022,
      "step": 77620
    },
    {
      "epoch": 4.312777777777778,
      "grad_norm": 0.311613529920578,
      "learning_rate": 6.872222222222223e-06,
      "loss": 0.0037,
      "step": 77630
    },
    {
      "epoch": 4.3133333333333335,
      "grad_norm": 0.030637456104159355,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0022,
      "step": 77640
    },
    {
      "epoch": 4.313888888888889,
      "grad_norm": 0.4465281069278717,
      "learning_rate": 6.861111111111111e-06,
      "loss": 0.0025,
      "step": 77650
    },
    {
      "epoch": 4.314444444444445,
      "grad_norm": 0.177834153175354,
      "learning_rate": 6.855555555555555e-06,
      "loss": 0.0024,
      "step": 77660
    },
    {
      "epoch": 4.315,
      "grad_norm": 0.03208217769861221,
      "learning_rate": 6.8500000000000005e-06,
      "loss": 0.0033,
      "step": 77670
    },
    {
      "epoch": 4.315555555555555,
      "grad_norm": 0.34057262539863586,
      "learning_rate": 6.844444444444445e-06,
      "loss": 0.0022,
      "step": 77680
    },
    {
      "epoch": 4.316111111111111,
      "grad_norm": 0.1186879575252533,
      "learning_rate": 6.838888888888889e-06,
      "loss": 0.003,
      "step": 77690
    },
    {
      "epoch": 4.316666666666666,
      "grad_norm": 0.290359228849411,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0031,
      "step": 77700
    },
    {
      "epoch": 4.317222222222222,
      "grad_norm": 0.0781283751130104,
      "learning_rate": 6.827777777777779e-06,
      "loss": 0.0031,
      "step": 77710
    },
    {
      "epoch": 4.317777777777778,
      "grad_norm": 0.29965150356292725,
      "learning_rate": 6.8222222222222225e-06,
      "loss": 0.0039,
      "step": 77720
    },
    {
      "epoch": 4.318333333333333,
      "grad_norm": 0.09164666384458542,
      "learning_rate": 6.816666666666667e-06,
      "loss": 0.0019,
      "step": 77730
    },
    {
      "epoch": 4.318888888888889,
      "grad_norm": 0.11945407092571259,
      "learning_rate": 6.811111111111111e-06,
      "loss": 0.0033,
      "step": 77740
    },
    {
      "epoch": 4.319444444444445,
      "grad_norm": 0.4305350184440613,
      "learning_rate": 6.805555555555556e-06,
      "loss": 0.0025,
      "step": 77750
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.059871088713407516,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0026,
      "step": 77760
    },
    {
      "epoch": 4.320555555555556,
      "grad_norm": 0.2625025808811188,
      "learning_rate": 6.794444444444445e-06,
      "loss": 0.0034,
      "step": 77770
    },
    {
      "epoch": 4.321111111111112,
      "grad_norm": 0.12380159646272659,
      "learning_rate": 6.788888888888889e-06,
      "loss": 0.0027,
      "step": 77780
    },
    {
      "epoch": 4.321666666666666,
      "grad_norm": 0.5133282542228699,
      "learning_rate": 6.783333333333333e-06,
      "loss": 0.0029,
      "step": 77790
    },
    {
      "epoch": 4.322222222222222,
      "grad_norm": 0.2687045931816101,
      "learning_rate": 6.777777777777779e-06,
      "loss": 0.003,
      "step": 77800
    },
    {
      "epoch": 4.322777777777778,
      "grad_norm": 0.11945704370737076,
      "learning_rate": 6.772222222222223e-06,
      "loss": 0.0037,
      "step": 77810
    },
    {
      "epoch": 4.323333333333333,
      "grad_norm": 0.012020149268209934,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.0029,
      "step": 77820
    },
    {
      "epoch": 4.323888888888889,
      "grad_norm": 0.05984826758503914,
      "learning_rate": 6.761111111111111e-06,
      "loss": 0.0038,
      "step": 77830
    },
    {
      "epoch": 4.3244444444444445,
      "grad_norm": 0.30966076254844666,
      "learning_rate": 6.755555555555555e-06,
      "loss": 0.0022,
      "step": 77840
    },
    {
      "epoch": 4.325,
      "grad_norm": 0.5055716037750244,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.002,
      "step": 77850
    },
    {
      "epoch": 4.325555555555556,
      "grad_norm": 0.17909857630729675,
      "learning_rate": 6.744444444444445e-06,
      "loss": 0.0034,
      "step": 77860
    },
    {
      "epoch": 4.3261111111111115,
      "grad_norm": 0.32672545313835144,
      "learning_rate": 6.738888888888889e-06,
      "loss": 0.0031,
      "step": 77870
    },
    {
      "epoch": 4.326666666666666,
      "grad_norm": 0.060835108160972595,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0029,
      "step": 77880
    },
    {
      "epoch": 4.327222222222222,
      "grad_norm": 0.4238799512386322,
      "learning_rate": 6.727777777777779e-06,
      "loss": 0.0025,
      "step": 77890
    },
    {
      "epoch": 4.3277777777777775,
      "grad_norm": 0.23815792798995972,
      "learning_rate": 6.722222222222223e-06,
      "loss": 0.0036,
      "step": 77900
    },
    {
      "epoch": 4.328333333333333,
      "grad_norm": 0.21835196018218994,
      "learning_rate": 6.716666666666667e-06,
      "loss": 0.0015,
      "step": 77910
    },
    {
      "epoch": 4.328888888888889,
      "grad_norm": 0.11009752750396729,
      "learning_rate": 6.711111111111111e-06,
      "loss": 0.0029,
      "step": 77920
    },
    {
      "epoch": 4.329444444444444,
      "grad_norm": 0.14898262917995453,
      "learning_rate": 6.705555555555555e-06,
      "loss": 0.0028,
      "step": 77930
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.1483248472213745,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0019,
      "step": 77940
    },
    {
      "epoch": 4.330555555555556,
      "grad_norm": 0.42679524421691895,
      "learning_rate": 6.694444444444445e-06,
      "loss": 0.0042,
      "step": 77950
    },
    {
      "epoch": 4.331111111111111,
      "grad_norm": 0.17789886891841888,
      "learning_rate": 6.688888888888889e-06,
      "loss": 0.0023,
      "step": 77960
    },
    {
      "epoch": 4.331666666666667,
      "grad_norm": 0.033635739237070084,
      "learning_rate": 6.6833333333333334e-06,
      "loss": 0.0027,
      "step": 77970
    },
    {
      "epoch": 4.332222222222223,
      "grad_norm": 0.1495686173439026,
      "learning_rate": 6.677777777777779e-06,
      "loss": 0.0029,
      "step": 77980
    },
    {
      "epoch": 4.332777777777777,
      "grad_norm": 0.030417611822485924,
      "learning_rate": 6.672222222222223e-06,
      "loss": 0.0023,
      "step": 77990
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.2970080077648163,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0047,
      "step": 78000
    },
    {
      "epoch": 4.333888888888889,
      "grad_norm": 0.07095599919557571,
      "learning_rate": 6.661111111111111e-06,
      "loss": 0.0029,
      "step": 78010
    },
    {
      "epoch": 4.334444444444444,
      "grad_norm": 0.09883972257375717,
      "learning_rate": 6.655555555555556e-06,
      "loss": 0.0025,
      "step": 78020
    },
    {
      "epoch": 4.335,
      "grad_norm": 0.20846156775951385,
      "learning_rate": 6.650000000000001e-06,
      "loss": 0.0029,
      "step": 78030
    },
    {
      "epoch": 4.335555555555556,
      "grad_norm": 0.11995533853769302,
      "learning_rate": 6.644444444444445e-06,
      "loss": 0.0026,
      "step": 78040
    },
    {
      "epoch": 4.336111111111111,
      "grad_norm": 0.09094563126564026,
      "learning_rate": 6.638888888888889e-06,
      "loss": 0.0026,
      "step": 78050
    },
    {
      "epoch": 4.336666666666667,
      "grad_norm": 0.14900639653205872,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.0027,
      "step": 78060
    },
    {
      "epoch": 4.3372222222222225,
      "grad_norm": 0.29718005657196045,
      "learning_rate": 6.627777777777778e-06,
      "loss": 0.0018,
      "step": 78070
    },
    {
      "epoch": 4.337777777777778,
      "grad_norm": 0.12050031125545502,
      "learning_rate": 6.622222222222223e-06,
      "loss": 0.003,
      "step": 78080
    },
    {
      "epoch": 4.338333333333333,
      "grad_norm": 0.08926533162593842,
      "learning_rate": 6.616666666666667e-06,
      "loss": 0.0033,
      "step": 78090
    },
    {
      "epoch": 4.338888888888889,
      "grad_norm": 0.034056518226861954,
      "learning_rate": 6.611111111111111e-06,
      "loss": 0.0026,
      "step": 78100
    },
    {
      "epoch": 4.339444444444444,
      "grad_norm": 0.1789780855178833,
      "learning_rate": 6.605555555555557e-06,
      "loss": 0.0024,
      "step": 78110
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.20869798958301544,
      "learning_rate": 6.6e-06,
      "loss": 0.0027,
      "step": 78120
    },
    {
      "epoch": 4.3405555555555555,
      "grad_norm": 0.23240762948989868,
      "learning_rate": 6.594444444444445e-06,
      "loss": 0.0024,
      "step": 78130
    },
    {
      "epoch": 4.341111111111111,
      "grad_norm": 0.05986470356583595,
      "learning_rate": 6.588888888888889e-06,
      "loss": 0.0032,
      "step": 78140
    },
    {
      "epoch": 4.341666666666667,
      "grad_norm": 0.20716162025928497,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.0027,
      "step": 78150
    },
    {
      "epoch": 4.342222222222222,
      "grad_norm": 0.17801429331302643,
      "learning_rate": 6.577777777777779e-06,
      "loss": 0.0027,
      "step": 78160
    },
    {
      "epoch": 4.342777777777778,
      "grad_norm": 0.09007134288549423,
      "learning_rate": 6.572222222222222e-06,
      "loss": 0.0032,
      "step": 78170
    },
    {
      "epoch": 4.343333333333334,
      "grad_norm": 0.09069862216711044,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0023,
      "step": 78180
    },
    {
      "epoch": 4.343888888888889,
      "grad_norm": 0.4442054331302643,
      "learning_rate": 6.561111111111111e-06,
      "loss": 0.0031,
      "step": 78190
    },
    {
      "epoch": 4.344444444444444,
      "grad_norm": 0.415361225605011,
      "learning_rate": 6.555555555555556e-06,
      "loss": 0.0032,
      "step": 78200
    },
    {
      "epoch": 4.345,
      "grad_norm": 0.06147375702857971,
      "learning_rate": 6.550000000000001e-06,
      "loss": 0.0021,
      "step": 78210
    },
    {
      "epoch": 4.345555555555555,
      "grad_norm": 0.20752719044685364,
      "learning_rate": 6.544444444444444e-06,
      "loss": 0.0022,
      "step": 78220
    },
    {
      "epoch": 4.346111111111111,
      "grad_norm": 0.20825283229351044,
      "learning_rate": 6.538888888888889e-06,
      "loss": 0.0033,
      "step": 78230
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.2971798777580261,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0022,
      "step": 78240
    },
    {
      "epoch": 4.347222222222222,
      "grad_norm": 0.3553144037723541,
      "learning_rate": 6.5277777777777784e-06,
      "loss": 0.0015,
      "step": 78250
    },
    {
      "epoch": 4.347777777777778,
      "grad_norm": 0.14861629903316498,
      "learning_rate": 6.522222222222223e-06,
      "loss": 0.0023,
      "step": 78260
    },
    {
      "epoch": 4.348333333333334,
      "grad_norm": 0.11878305673599243,
      "learning_rate": 6.516666666666666e-06,
      "loss": 0.0022,
      "step": 78270
    },
    {
      "epoch": 4.348888888888889,
      "grad_norm": 0.1489439308643341,
      "learning_rate": 6.511111111111111e-06,
      "loss": 0.0044,
      "step": 78280
    },
    {
      "epoch": 4.349444444444444,
      "grad_norm": 0.11875097453594208,
      "learning_rate": 6.505555555555556e-06,
      "loss": 0.0017,
      "step": 78290
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.23769238591194153,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0031,
      "step": 78300
    },
    {
      "epoch": 4.350555555555555,
      "grad_norm": 0.29647326469421387,
      "learning_rate": 6.494444444444445e-06,
      "loss": 0.003,
      "step": 78310
    },
    {
      "epoch": 4.351111111111111,
      "grad_norm": 0.4148745536804199,
      "learning_rate": 6.488888888888888e-06,
      "loss": 0.0024,
      "step": 78320
    },
    {
      "epoch": 4.351666666666667,
      "grad_norm": 0.32634514570236206,
      "learning_rate": 6.4833333333333345e-06,
      "loss": 0.0026,
      "step": 78330
    },
    {
      "epoch": 4.352222222222222,
      "grad_norm": 0.06006736680865288,
      "learning_rate": 6.477777777777778e-06,
      "loss": 0.0024,
      "step": 78340
    },
    {
      "epoch": 4.352777777777778,
      "grad_norm": 0.09128264337778091,
      "learning_rate": 6.4722222222222225e-06,
      "loss": 0.0021,
      "step": 78350
    },
    {
      "epoch": 4.3533333333333335,
      "grad_norm": 0.24280235171318054,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0028,
      "step": 78360
    },
    {
      "epoch": 4.353888888888889,
      "grad_norm": 0.35641440749168396,
      "learning_rate": 6.4611111111111104e-06,
      "loss": 0.0039,
      "step": 78370
    },
    {
      "epoch": 4.354444444444445,
      "grad_norm": 0.01762630231678486,
      "learning_rate": 6.4555555555555565e-06,
      "loss": 0.0038,
      "step": 78380
    },
    {
      "epoch": 4.355,
      "grad_norm": 0.29626375436782837,
      "learning_rate": 6.45e-06,
      "loss": 0.0023,
      "step": 78390
    },
    {
      "epoch": 4.355555555555555,
      "grad_norm": 0.06144208461046219,
      "learning_rate": 6.4444444444444445e-06,
      "loss": 0.0031,
      "step": 78400
    },
    {
      "epoch": 4.356111111111111,
      "grad_norm": 0.3666342496871948,
      "learning_rate": 6.438888888888889e-06,
      "loss": 0.0023,
      "step": 78410
    },
    {
      "epoch": 4.3566666666666665,
      "grad_norm": 0.3731186091899872,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.0028,
      "step": 78420
    },
    {
      "epoch": 4.357222222222222,
      "grad_norm": 0.6990798711776733,
      "learning_rate": 6.4277777777777785e-06,
      "loss": 0.0026,
      "step": 78430
    },
    {
      "epoch": 4.357777777777778,
      "grad_norm": 0.2410144954919815,
      "learning_rate": 6.422222222222223e-06,
      "loss": 0.0035,
      "step": 78440
    },
    {
      "epoch": 4.358333333333333,
      "grad_norm": 0.033479947596788406,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.0027,
      "step": 78450
    },
    {
      "epoch": 4.358888888888889,
      "grad_norm": 0.0901588574051857,
      "learning_rate": 6.411111111111111e-06,
      "loss": 0.0028,
      "step": 78460
    },
    {
      "epoch": 4.359444444444445,
      "grad_norm": 0.03228672966361046,
      "learning_rate": 6.405555555555556e-06,
      "loss": 0.0021,
      "step": 78470
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.296886146068573,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0028,
      "step": 78480
    },
    {
      "epoch": 4.360555555555556,
      "grad_norm": 0.11874371767044067,
      "learning_rate": 6.394444444444445e-06,
      "loss": 0.0022,
      "step": 78490
    },
    {
      "epoch": 4.361111111111111,
      "grad_norm": 0.07789162546396255,
      "learning_rate": 6.3888888888888885e-06,
      "loss": 0.0024,
      "step": 78500
    },
    {
      "epoch": 4.361666666666666,
      "grad_norm": 0.0898565948009491,
      "learning_rate": 6.383333333333335e-06,
      "loss": 0.0023,
      "step": 78510
    },
    {
      "epoch": 4.362222222222222,
      "grad_norm": 0.03237811103463173,
      "learning_rate": 6.377777777777778e-06,
      "loss": 0.0023,
      "step": 78520
    },
    {
      "epoch": 4.362777777777778,
      "grad_norm": 0.19482983648777008,
      "learning_rate": 6.3722222222222226e-06,
      "loss": 0.0032,
      "step": 78530
    },
    {
      "epoch": 4.363333333333333,
      "grad_norm": 0.16696792840957642,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0021,
      "step": 78540
    },
    {
      "epoch": 4.363888888888889,
      "grad_norm": 0.06224245950579643,
      "learning_rate": 6.3611111111111105e-06,
      "loss": 0.0022,
      "step": 78550
    },
    {
      "epoch": 4.364444444444445,
      "grad_norm": 0.11225689947605133,
      "learning_rate": 6.355555555555557e-06,
      "loss": 0.0028,
      "step": 78560
    },
    {
      "epoch": 4.365,
      "grad_norm": 0.3257068693637848,
      "learning_rate": 6.35e-06,
      "loss": 0.002,
      "step": 78570
    },
    {
      "epoch": 4.365555555555556,
      "grad_norm": 0.3901045024394989,
      "learning_rate": 6.344444444444445e-06,
      "loss": 0.0031,
      "step": 78580
    },
    {
      "epoch": 4.3661111111111115,
      "grad_norm": 0.033038899302482605,
      "learning_rate": 6.338888888888889e-06,
      "loss": 0.0031,
      "step": 78590
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 0.09071212261915207,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0026,
      "step": 78600
    },
    {
      "epoch": 4.367222222222222,
      "grad_norm": 0.24880068004131317,
      "learning_rate": 6.327777777777779e-06,
      "loss": 0.0026,
      "step": 78610
    },
    {
      "epoch": 4.3677777777777775,
      "grad_norm": 0.4869767725467682,
      "learning_rate": 6.322222222222222e-06,
      "loss": 0.0025,
      "step": 78620
    },
    {
      "epoch": 4.368333333333333,
      "grad_norm": 0.1488659530878067,
      "learning_rate": 6.316666666666667e-06,
      "loss": 0.0027,
      "step": 78630
    },
    {
      "epoch": 4.368888888888889,
      "grad_norm": 0.031193969771265984,
      "learning_rate": 6.311111111111112e-06,
      "loss": 0.004,
      "step": 78640
    },
    {
      "epoch": 4.3694444444444445,
      "grad_norm": 0.010168851353228092,
      "learning_rate": 6.305555555555556e-06,
      "loss": 0.002,
      "step": 78650
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.08979637920856476,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0021,
      "step": 78660
    },
    {
      "epoch": 4.370555555555556,
      "grad_norm": 0.6663475036621094,
      "learning_rate": 6.294444444444444e-06,
      "loss": 0.0029,
      "step": 78670
    },
    {
      "epoch": 4.371111111111111,
      "grad_norm": 0.2667590081691742,
      "learning_rate": 6.288888888888889e-06,
      "loss": 0.0037,
      "step": 78680
    },
    {
      "epoch": 4.371666666666667,
      "grad_norm": 0.1200440376996994,
      "learning_rate": 6.283333333333334e-06,
      "loss": 0.0025,
      "step": 78690
    },
    {
      "epoch": 4.372222222222222,
      "grad_norm": 0.060920942574739456,
      "learning_rate": 6.277777777777778e-06,
      "loss": 0.0037,
      "step": 78700
    },
    {
      "epoch": 4.372777777777777,
      "grad_norm": 0.2416849285364151,
      "learning_rate": 6.272222222222223e-06,
      "loss": 0.0032,
      "step": 78710
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.09374022483825684,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0025,
      "step": 78720
    },
    {
      "epoch": 4.373888888888889,
      "grad_norm": 0.0604667067527771,
      "learning_rate": 6.261111111111112e-06,
      "loss": 0.0021,
      "step": 78730
    },
    {
      "epoch": 4.374444444444444,
      "grad_norm": 0.18048249185085297,
      "learning_rate": 6.255555555555556e-06,
      "loss": 0.0036,
      "step": 78740
    },
    {
      "epoch": 4.375,
      "grad_norm": 0.010312413796782494,
      "learning_rate": 6.25e-06,
      "loss": 0.0021,
      "step": 78750
    },
    {
      "epoch": 4.375555555555556,
      "grad_norm": 0.11893104016780853,
      "learning_rate": 6.244444444444445e-06,
      "loss": 0.0041,
      "step": 78760
    },
    {
      "epoch": 4.376111111111111,
      "grad_norm": 0.1497087925672531,
      "learning_rate": 6.238888888888889e-06,
      "loss": 0.0037,
      "step": 78770
    },
    {
      "epoch": 4.376666666666667,
      "grad_norm": 0.14933110773563385,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0031,
      "step": 78780
    },
    {
      "epoch": 4.377222222222223,
      "grad_norm": 0.06064632907509804,
      "learning_rate": 6.227777777777778e-06,
      "loss": 0.0032,
      "step": 78790
    },
    {
      "epoch": 4.377777777777778,
      "grad_norm": 0.21561786532402039,
      "learning_rate": 6.222222222222222e-06,
      "loss": 0.0023,
      "step": 78800
    },
    {
      "epoch": 4.378333333333333,
      "grad_norm": 0.23624613881111145,
      "learning_rate": 6.2166666666666676e-06,
      "loss": 0.0022,
      "step": 78810
    },
    {
      "epoch": 4.378888888888889,
      "grad_norm": 0.24154485762119293,
      "learning_rate": 6.211111111111111e-06,
      "loss": 0.0025,
      "step": 78820
    },
    {
      "epoch": 4.379444444444444,
      "grad_norm": 0.14883612096309662,
      "learning_rate": 6.205555555555556e-06,
      "loss": 0.0029,
      "step": 78830
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.09152408689260483,
      "learning_rate": 6.2e-06,
      "loss": 0.0033,
      "step": 78840
    },
    {
      "epoch": 4.3805555555555555,
      "grad_norm": 0.293089896440506,
      "learning_rate": 6.194444444444445e-06,
      "loss": 0.0036,
      "step": 78850
    },
    {
      "epoch": 4.381111111111111,
      "grad_norm": 0.6319534182548523,
      "learning_rate": 6.18888888888889e-06,
      "loss": 0.0017,
      "step": 78860
    },
    {
      "epoch": 4.381666666666667,
      "grad_norm": 0.14886146783828735,
      "learning_rate": 6.183333333333333e-06,
      "loss": 0.003,
      "step": 78870
    },
    {
      "epoch": 4.3822222222222225,
      "grad_norm": 0.2968367338180542,
      "learning_rate": 6.177777777777778e-06,
      "loss": 0.0023,
      "step": 78880
    },
    {
      "epoch": 4.382777777777778,
      "grad_norm": 0.14901627600193024,
      "learning_rate": 6.172222222222222e-06,
      "loss": 0.003,
      "step": 78890
    },
    {
      "epoch": 4.383333333333334,
      "grad_norm": 0.09037904441356659,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0025,
      "step": 78900
    },
    {
      "epoch": 4.3838888888888885,
      "grad_norm": 0.2079085111618042,
      "learning_rate": 6.161111111111112e-06,
      "loss": 0.0016,
      "step": 78910
    },
    {
      "epoch": 4.384444444444444,
      "grad_norm": 0.14907227456569672,
      "learning_rate": 6.155555555555556e-06,
      "loss": 0.0034,
      "step": 78920
    },
    {
      "epoch": 4.385,
      "grad_norm": 0.17918260395526886,
      "learning_rate": 6.15e-06,
      "loss": 0.0019,
      "step": 78930
    },
    {
      "epoch": 4.385555555555555,
      "grad_norm": 0.3853936195373535,
      "learning_rate": 6.144444444444445e-06,
      "loss": 0.0017,
      "step": 78940
    },
    {
      "epoch": 4.386111111111111,
      "grad_norm": 0.04025363549590111,
      "learning_rate": 6.138888888888889e-06,
      "loss": 0.0043,
      "step": 78950
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.14892512559890747,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0027,
      "step": 78960
    },
    {
      "epoch": 4.387222222222222,
      "grad_norm": 0.2960391640663147,
      "learning_rate": 6.127777777777778e-06,
      "loss": 0.0039,
      "step": 78970
    },
    {
      "epoch": 4.387777777777778,
      "grad_norm": 0.29203757643699646,
      "learning_rate": 6.1222222222222224e-06,
      "loss": 0.0035,
      "step": 78980
    },
    {
      "epoch": 4.388333333333334,
      "grad_norm": 0.12372355163097382,
      "learning_rate": 6.116666666666667e-06,
      "loss": 0.0042,
      "step": 78990
    },
    {
      "epoch": 4.388888888888889,
      "grad_norm": 0.20832061767578125,
      "learning_rate": 6.111111111111111e-06,
      "loss": 0.0032,
      "step": 79000
    },
    {
      "epoch": 4.389444444444444,
      "grad_norm": 0.12044166028499603,
      "learning_rate": 6.105555555555556e-06,
      "loss": 0.0022,
      "step": 79010
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.010949320159852505,
      "learning_rate": 6.1e-06,
      "loss": 0.003,
      "step": 79020
    },
    {
      "epoch": 4.390555555555555,
      "grad_norm": 0.1781366914510727,
      "learning_rate": 6.094444444444445e-06,
      "loss": 0.0033,
      "step": 79030
    },
    {
      "epoch": 4.391111111111111,
      "grad_norm": 0.23787535727024078,
      "learning_rate": 6.088888888888889e-06,
      "loss": 0.0025,
      "step": 79040
    },
    {
      "epoch": 4.391666666666667,
      "grad_norm": 0.060900285840034485,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.0028,
      "step": 79050
    },
    {
      "epoch": 4.392222222222222,
      "grad_norm": 0.03660132735967636,
      "learning_rate": 6.077777777777778e-06,
      "loss": 0.0028,
      "step": 79060
    },
    {
      "epoch": 4.392777777777778,
      "grad_norm": 0.08985760062932968,
      "learning_rate": 6.072222222222222e-06,
      "loss": 0.0026,
      "step": 79070
    },
    {
      "epoch": 4.3933333333333335,
      "grad_norm": 0.09362233430147171,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.002,
      "step": 79080
    },
    {
      "epoch": 4.393888888888889,
      "grad_norm": 0.238608255982399,
      "learning_rate": 6.061111111111111e-06,
      "loss": 0.002,
      "step": 79090
    },
    {
      "epoch": 4.394444444444445,
      "grad_norm": 0.10736259073019028,
      "learning_rate": 6.055555555555556e-06,
      "loss": 0.0021,
      "step": 79100
    },
    {
      "epoch": 4.395,
      "grad_norm": 0.2945307493209839,
      "learning_rate": 6.0500000000000005e-06,
      "loss": 0.0024,
      "step": 79110
    },
    {
      "epoch": 4.395555555555555,
      "grad_norm": 0.31233102083206177,
      "learning_rate": 6.044444444444445e-06,
      "loss": 0.0034,
      "step": 79120
    },
    {
      "epoch": 4.396111111111111,
      "grad_norm": 0.29670652747154236,
      "learning_rate": 6.038888888888889e-06,
      "loss": 0.003,
      "step": 79130
    },
    {
      "epoch": 4.3966666666666665,
      "grad_norm": 0.1486901491880417,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0019,
      "step": 79140
    },
    {
      "epoch": 4.397222222222222,
      "grad_norm": 0.2518334984779358,
      "learning_rate": 6.027777777777778e-06,
      "loss": 0.0017,
      "step": 79150
    },
    {
      "epoch": 4.397777777777778,
      "grad_norm": 0.050730325281620026,
      "learning_rate": 6.0222222222222225e-06,
      "loss": 0.003,
      "step": 79160
    },
    {
      "epoch": 4.398333333333333,
      "grad_norm": 0.06962741166353226,
      "learning_rate": 6.016666666666667e-06,
      "loss": 0.002,
      "step": 79170
    },
    {
      "epoch": 4.398888888888889,
      "grad_norm": 0.11939969658851624,
      "learning_rate": 6.011111111111111e-06,
      "loss": 0.0021,
      "step": 79180
    },
    {
      "epoch": 4.399444444444445,
      "grad_norm": 0.1788371354341507,
      "learning_rate": 6.005555555555556e-06,
      "loss": 0.0026,
      "step": 79190
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.352139949798584,
      "learning_rate": 6e-06,
      "loss": 0.0028,
      "step": 79200
    },
    {
      "epoch": 4.400555555555556,
      "grad_norm": 0.24305860698223114,
      "learning_rate": 5.9944444444444446e-06,
      "loss": 0.0035,
      "step": 79210
    },
    {
      "epoch": 4.401111111111111,
      "grad_norm": 0.12032181769609451,
      "learning_rate": 5.988888888888889e-06,
      "loss": 0.0021,
      "step": 79220
    },
    {
      "epoch": 4.401666666666666,
      "grad_norm": 0.39736658334732056,
      "learning_rate": 5.983333333333334e-06,
      "loss": 0.0025,
      "step": 79230
    },
    {
      "epoch": 4.402222222222222,
      "grad_norm": 0.07251136749982834,
      "learning_rate": 5.977777777777778e-06,
      "loss": 0.0025,
      "step": 79240
    },
    {
      "epoch": 4.402777777777778,
      "grad_norm": 0.08943270891904831,
      "learning_rate": 5.972222222222223e-06,
      "loss": 0.0023,
      "step": 79250
    },
    {
      "epoch": 4.403333333333333,
      "grad_norm": 0.1778966635465622,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0011,
      "step": 79260
    },
    {
      "epoch": 4.403888888888889,
      "grad_norm": 0.06101434305310249,
      "learning_rate": 5.961111111111111e-06,
      "loss": 0.0024,
      "step": 79270
    },
    {
      "epoch": 4.404444444444445,
      "grad_norm": 0.060174863785505295,
      "learning_rate": 5.955555555555556e-06,
      "loss": 0.0022,
      "step": 79280
    },
    {
      "epoch": 4.405,
      "grad_norm": 0.08981788903474808,
      "learning_rate": 5.95e-06,
      "loss": 0.0023,
      "step": 79290
    },
    {
      "epoch": 4.405555555555556,
      "grad_norm": 0.1784459799528122,
      "learning_rate": 5.944444444444445e-06,
      "loss": 0.0016,
      "step": 79300
    },
    {
      "epoch": 4.406111111111111,
      "grad_norm": 0.3857145607471466,
      "learning_rate": 5.938888888888889e-06,
      "loss": 0.0034,
      "step": 79310
    },
    {
      "epoch": 4.406666666666666,
      "grad_norm": 0.1188889816403389,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0027,
      "step": 79320
    },
    {
      "epoch": 4.407222222222222,
      "grad_norm": 0.2074238359928131,
      "learning_rate": 5.927777777777778e-06,
      "loss": 0.002,
      "step": 79330
    },
    {
      "epoch": 4.407777777777778,
      "grad_norm": 0.11885657906532288,
      "learning_rate": 5.922222222222223e-06,
      "loss": 0.003,
      "step": 79340
    },
    {
      "epoch": 4.408333333333333,
      "grad_norm": 0.14880314469337463,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.0016,
      "step": 79350
    },
    {
      "epoch": 4.408888888888889,
      "grad_norm": 0.06082086265087128,
      "learning_rate": 5.9111111111111115e-06,
      "loss": 0.0028,
      "step": 79360
    },
    {
      "epoch": 4.4094444444444445,
      "grad_norm": 0.11386366188526154,
      "learning_rate": 5.905555555555556e-06,
      "loss": 0.0023,
      "step": 79370
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.12492910027503967,
      "learning_rate": 5.9e-06,
      "loss": 0.0027,
      "step": 79380
    },
    {
      "epoch": 4.410555555555556,
      "grad_norm": 0.267521470785141,
      "learning_rate": 5.894444444444445e-06,
      "loss": 0.0037,
      "step": 79390
    },
    {
      "epoch": 4.411111111111111,
      "grad_norm": 0.2514030933380127,
      "learning_rate": 5.888888888888889e-06,
      "loss": 0.0021,
      "step": 79400
    },
    {
      "epoch": 4.411666666666667,
      "grad_norm": 0.060469746589660645,
      "learning_rate": 5.8833333333333335e-06,
      "loss": 0.0037,
      "step": 79410
    },
    {
      "epoch": 4.412222222222222,
      "grad_norm": 0.2497924119234085,
      "learning_rate": 5.877777777777778e-06,
      "loss": 0.0025,
      "step": 79420
    },
    {
      "epoch": 4.4127777777777775,
      "grad_norm": 0.20774014294147491,
      "learning_rate": 5.872222222222223e-06,
      "loss": 0.002,
      "step": 79430
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.0328768752515316,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0031,
      "step": 79440
    },
    {
      "epoch": 4.413888888888889,
      "grad_norm": 0.26695284247398376,
      "learning_rate": 5.861111111111112e-06,
      "loss": 0.0019,
      "step": 79450
    },
    {
      "epoch": 4.414444444444444,
      "grad_norm": 0.444722443819046,
      "learning_rate": 5.8555555555555555e-06,
      "loss": 0.002,
      "step": 79460
    },
    {
      "epoch": 4.415,
      "grad_norm": 0.20777824521064758,
      "learning_rate": 5.850000000000001e-06,
      "loss": 0.0028,
      "step": 79470
    },
    {
      "epoch": 4.415555555555556,
      "grad_norm": 0.33182668685913086,
      "learning_rate": 5.844444444444445e-06,
      "loss": 0.0022,
      "step": 79480
    },
    {
      "epoch": 4.416111111111111,
      "grad_norm": 0.03285808488726616,
      "learning_rate": 5.838888888888889e-06,
      "loss": 0.0039,
      "step": 79490
    },
    {
      "epoch": 4.416666666666667,
      "grad_norm": 0.1246597021818161,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0037,
      "step": 79500
    },
    {
      "epoch": 4.417222222222223,
      "grad_norm": 0.13090841472148895,
      "learning_rate": 5.8277777777777775e-06,
      "loss": 0.0036,
      "step": 79510
    },
    {
      "epoch": 4.417777777777777,
      "grad_norm": 0.11901813000440598,
      "learning_rate": 5.822222222222223e-06,
      "loss": 0.0029,
      "step": 79520
    },
    {
      "epoch": 4.418333333333333,
      "grad_norm": 0.32648828625679016,
      "learning_rate": 5.816666666666667e-06,
      "loss": 0.0026,
      "step": 79530
    },
    {
      "epoch": 4.418888888888889,
      "grad_norm": 0.06577256321907043,
      "learning_rate": 5.8111111111111116e-06,
      "loss": 0.003,
      "step": 79540
    },
    {
      "epoch": 4.419444444444444,
      "grad_norm": 0.4023548364639282,
      "learning_rate": 5.805555555555556e-06,
      "loss": 0.0027,
      "step": 79550
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.14851072430610657,
      "learning_rate": 5.8e-06,
      "loss": 0.0025,
      "step": 79560
    },
    {
      "epoch": 4.420555555555556,
      "grad_norm": 0.1314290463924408,
      "learning_rate": 5.794444444444445e-06,
      "loss": 0.0028,
      "step": 79570
    },
    {
      "epoch": 4.421111111111111,
      "grad_norm": 0.2966606318950653,
      "learning_rate": 5.788888888888889e-06,
      "loss": 0.0028,
      "step": 79580
    },
    {
      "epoch": 4.421666666666667,
      "grad_norm": 0.06090591475367546,
      "learning_rate": 5.783333333333334e-06,
      "loss": 0.0034,
      "step": 79590
    },
    {
      "epoch": 4.4222222222222225,
      "grad_norm": 0.4836757183074951,
      "learning_rate": 5.777777777777778e-06,
      "loss": 0.0032,
      "step": 79600
    },
    {
      "epoch": 4.422777777777778,
      "grad_norm": 0.03167827054858208,
      "learning_rate": 5.772222222222222e-06,
      "loss": 0.0035,
      "step": 79610
    },
    {
      "epoch": 4.423333333333334,
      "grad_norm": 0.38751205801963806,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0026,
      "step": 79620
    },
    {
      "epoch": 4.4238888888888885,
      "grad_norm": 0.20814131200313568,
      "learning_rate": 5.761111111111111e-06,
      "loss": 0.0028,
      "step": 79630
    },
    {
      "epoch": 4.424444444444444,
      "grad_norm": 0.03201691806316376,
      "learning_rate": 5.755555555555556e-06,
      "loss": 0.0023,
      "step": 79640
    },
    {
      "epoch": 4.425,
      "grad_norm": 0.16834770143032074,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.0025,
      "step": 79650
    },
    {
      "epoch": 4.4255555555555555,
      "grad_norm": 0.03700317442417145,
      "learning_rate": 5.744444444444444e-06,
      "loss": 0.0027,
      "step": 79660
    },
    {
      "epoch": 4.426111111111111,
      "grad_norm": 0.06019773334264755,
      "learning_rate": 5.73888888888889e-06,
      "loss": 0.002,
      "step": 79670
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.23781287670135498,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0035,
      "step": 79680
    },
    {
      "epoch": 4.427222222222222,
      "grad_norm": 0.11979465931653976,
      "learning_rate": 5.727777777777778e-06,
      "loss": 0.002,
      "step": 79690
    },
    {
      "epoch": 4.427777777777778,
      "grad_norm": 0.11866553127765656,
      "learning_rate": 5.722222222222223e-06,
      "loss": 0.0023,
      "step": 79700
    },
    {
      "epoch": 4.428333333333334,
      "grad_norm": 0.37873077392578125,
      "learning_rate": 5.7166666666666664e-06,
      "loss": 0.0037,
      "step": 79710
    },
    {
      "epoch": 4.428888888888888,
      "grad_norm": 0.3851388394832611,
      "learning_rate": 5.711111111111112e-06,
      "loss": 0.0027,
      "step": 79720
    },
    {
      "epoch": 4.429444444444444,
      "grad_norm": 0.08912034332752228,
      "learning_rate": 5.705555555555555e-06,
      "loss": 0.0021,
      "step": 79730
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.09125972539186478,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0028,
      "step": 79740
    },
    {
      "epoch": 4.430555555555555,
      "grad_norm": 0.08957338333129883,
      "learning_rate": 5.694444444444445e-06,
      "loss": 0.0026,
      "step": 79750
    },
    {
      "epoch": 4.431111111111111,
      "grad_norm": 0.38636043667793274,
      "learning_rate": 5.688888888888889e-06,
      "loss": 0.0025,
      "step": 79760
    },
    {
      "epoch": 4.431666666666667,
      "grad_norm": 0.14930884540081024,
      "learning_rate": 5.683333333333334e-06,
      "loss": 0.0023,
      "step": 79770
    },
    {
      "epoch": 4.432222222222222,
      "grad_norm": 0.06542430818080902,
      "learning_rate": 5.677777777777778e-06,
      "loss": 0.0028,
      "step": 79780
    },
    {
      "epoch": 4.432777777777778,
      "grad_norm": 0.031224248930811882,
      "learning_rate": 5.6722222222222225e-06,
      "loss": 0.0032,
      "step": 79790
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 0.14851170778274536,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0021,
      "step": 79800
    },
    {
      "epoch": 4.433888888888889,
      "grad_norm": 0.23753610253334045,
      "learning_rate": 5.661111111111111e-06,
      "loss": 0.0027,
      "step": 79810
    },
    {
      "epoch": 4.434444444444445,
      "grad_norm": 0.49067872762680054,
      "learning_rate": 5.655555555555556e-06,
      "loss": 0.0036,
      "step": 79820
    },
    {
      "epoch": 4.435,
      "grad_norm": 0.11924871802330017,
      "learning_rate": 5.65e-06,
      "loss": 0.003,
      "step": 79830
    },
    {
      "epoch": 4.435555555555555,
      "grad_norm": 0.061319515109062195,
      "learning_rate": 5.6444444444444445e-06,
      "loss": 0.0033,
      "step": 79840
    },
    {
      "epoch": 4.436111111111111,
      "grad_norm": 0.29632577300071716,
      "learning_rate": 5.63888888888889e-06,
      "loss": 0.0018,
      "step": 79850
    },
    {
      "epoch": 4.4366666666666665,
      "grad_norm": 0.42394721508026123,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0026,
      "step": 79860
    },
    {
      "epoch": 4.437222222222222,
      "grad_norm": 0.14833152294158936,
      "learning_rate": 5.6277777777777786e-06,
      "loss": 0.0023,
      "step": 79870
    },
    {
      "epoch": 4.437777777777778,
      "grad_norm": 0.1489092856645584,
      "learning_rate": 5.622222222222222e-06,
      "loss": 0.0039,
      "step": 79880
    },
    {
      "epoch": 4.4383333333333335,
      "grad_norm": 0.3822828233242035,
      "learning_rate": 5.6166666666666665e-06,
      "loss": 0.0029,
      "step": 79890
    },
    {
      "epoch": 4.438888888888889,
      "grad_norm": 0.5049641728401184,
      "learning_rate": 5.611111111111112e-06,
      "loss": 0.0033,
      "step": 79900
    },
    {
      "epoch": 4.439444444444445,
      "grad_norm": 0.17995311319828033,
      "learning_rate": 5.605555555555555e-06,
      "loss": 0.0024,
      "step": 79910
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.20756356418132782,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0019,
      "step": 79920
    },
    {
      "epoch": 4.440555555555555,
      "grad_norm": 0.08984505385160446,
      "learning_rate": 5.594444444444444e-06,
      "loss": 0.0033,
      "step": 79930
    },
    {
      "epoch": 4.441111111111111,
      "grad_norm": 0.03690347820520401,
      "learning_rate": 5.588888888888889e-06,
      "loss": 0.0028,
      "step": 79940
    },
    {
      "epoch": 4.441666666666666,
      "grad_norm": 0.2975551187992096,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.0037,
      "step": 79950
    },
    {
      "epoch": 4.442222222222222,
      "grad_norm": 0.7410646677017212,
      "learning_rate": 5.577777777777778e-06,
      "loss": 0.0024,
      "step": 79960
    },
    {
      "epoch": 4.442777777777778,
      "grad_norm": 0.2879199981689453,
      "learning_rate": 5.572222222222223e-06,
      "loss": 0.0019,
      "step": 79970
    },
    {
      "epoch": 4.443333333333333,
      "grad_norm": 0.10202201455831528,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0027,
      "step": 79980
    },
    {
      "epoch": 4.443888888888889,
      "grad_norm": 0.27957025170326233,
      "learning_rate": 5.561111111111111e-06,
      "loss": 0.0022,
      "step": 79990
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.10039151459932327,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.003,
      "step": 80000
    },
    {
      "epoch": 4.445,
      "grad_norm": 0.23816953599452972,
      "learning_rate": 5.55e-06,
      "loss": 0.003,
      "step": 80010
    },
    {
      "epoch": 4.445555555555556,
      "grad_norm": 0.23784185945987701,
      "learning_rate": 5.544444444444445e-06,
      "loss": 0.0028,
      "step": 80020
    },
    {
      "epoch": 4.446111111111112,
      "grad_norm": 0.03391876071691513,
      "learning_rate": 5.538888888888889e-06,
      "loss": 0.0041,
      "step": 80030
    },
    {
      "epoch": 4.446666666666666,
      "grad_norm": 0.3102944493293762,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0025,
      "step": 80040
    },
    {
      "epoch": 4.447222222222222,
      "grad_norm": 0.031925637274980545,
      "learning_rate": 5.527777777777778e-06,
      "loss": 0.0017,
      "step": 80050
    },
    {
      "epoch": 4.447777777777778,
      "grad_norm": 0.23729553818702698,
      "learning_rate": 5.522222222222222e-06,
      "loss": 0.002,
      "step": 80060
    },
    {
      "epoch": 4.448333333333333,
      "grad_norm": 0.23764315247535706,
      "learning_rate": 5.5166666666666675e-06,
      "loss": 0.0024,
      "step": 80070
    },
    {
      "epoch": 4.448888888888889,
      "grad_norm": 0.17824386060237885,
      "learning_rate": 5.511111111111111e-06,
      "loss": 0.0036,
      "step": 80080
    },
    {
      "epoch": 4.4494444444444445,
      "grad_norm": 0.48242101073265076,
      "learning_rate": 5.505555555555556e-06,
      "loss": 0.0024,
      "step": 80090
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.1777992695569992,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.0018,
      "step": 80100
    },
    {
      "epoch": 4.450555555555556,
      "grad_norm": 0.05997345224022865,
      "learning_rate": 5.494444444444444e-06,
      "loss": 0.0029,
      "step": 80110
    },
    {
      "epoch": 4.4511111111111115,
      "grad_norm": 0.27205508947372437,
      "learning_rate": 5.4888888888888895e-06,
      "loss": 0.0022,
      "step": 80120
    },
    {
      "epoch": 4.451666666666666,
      "grad_norm": 0.03178388997912407,
      "learning_rate": 5.483333333333333e-06,
      "loss": 0.0023,
      "step": 80130
    },
    {
      "epoch": 4.452222222222222,
      "grad_norm": 0.09956136345863342,
      "learning_rate": 5.477777777777778e-06,
      "loss": 0.0027,
      "step": 80140
    },
    {
      "epoch": 4.4527777777777775,
      "grad_norm": 0.14982685446739197,
      "learning_rate": 5.472222222222223e-06,
      "loss": 0.0028,
      "step": 80150
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.03227310627698898,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0016,
      "step": 80160
    },
    {
      "epoch": 4.453888888888889,
      "grad_norm": 0.20828785002231598,
      "learning_rate": 5.4611111111111115e-06,
      "loss": 0.0028,
      "step": 80170
    },
    {
      "epoch": 4.454444444444444,
      "grad_norm": 0.11903936415910721,
      "learning_rate": 5.455555555555556e-06,
      "loss": 0.0019,
      "step": 80180
    },
    {
      "epoch": 4.455,
      "grad_norm": 0.14488624036312103,
      "learning_rate": 5.45e-06,
      "loss": 0.0023,
      "step": 80190
    },
    {
      "epoch": 4.455555555555556,
      "grad_norm": 0.5649763345718384,
      "learning_rate": 5.444444444444445e-06,
      "loss": 0.0031,
      "step": 80200
    },
    {
      "epoch": 4.456111111111111,
      "grad_norm": 0.38512614369392395,
      "learning_rate": 5.438888888888889e-06,
      "loss": 0.0028,
      "step": 80210
    },
    {
      "epoch": 4.456666666666667,
      "grad_norm": 0.17936080694198608,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.0035,
      "step": 80220
    },
    {
      "epoch": 4.457222222222223,
      "grad_norm": 0.0312676876783371,
      "learning_rate": 5.427777777777778e-06,
      "loss": 0.0026,
      "step": 80230
    },
    {
      "epoch": 4.457777777777777,
      "grad_norm": 0.2968558073043823,
      "learning_rate": 5.422222222222222e-06,
      "loss": 0.0026,
      "step": 80240
    },
    {
      "epoch": 4.458333333333333,
      "grad_norm": 0.23726621270179749,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.0042,
      "step": 80250
    },
    {
      "epoch": 4.458888888888889,
      "grad_norm": 0.20791448652744293,
      "learning_rate": 5.411111111111111e-06,
      "loss": 0.0028,
      "step": 80260
    },
    {
      "epoch": 4.459444444444444,
      "grad_norm": 0.404474675655365,
      "learning_rate": 5.405555555555556e-06,
      "loss": 0.0023,
      "step": 80270
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.031222157180309296,
      "learning_rate": 5.4e-06,
      "loss": 0.0024,
      "step": 80280
    },
    {
      "epoch": 4.460555555555556,
      "grad_norm": 0.033968448638916016,
      "learning_rate": 5.394444444444445e-06,
      "loss": 0.0026,
      "step": 80290
    },
    {
      "epoch": 4.461111111111111,
      "grad_norm": 0.00912420079112053,
      "learning_rate": 5.388888888888889e-06,
      "loss": 0.0025,
      "step": 80300
    },
    {
      "epoch": 4.461666666666667,
      "grad_norm": 0.5054179430007935,
      "learning_rate": 5.383333333333333e-06,
      "loss": 0.003,
      "step": 80310
    },
    {
      "epoch": 4.4622222222222225,
      "grad_norm": 0.3849254548549652,
      "learning_rate": 5.3777777777777784e-06,
      "loss": 0.0021,
      "step": 80320
    },
    {
      "epoch": 4.462777777777778,
      "grad_norm": 0.04238772764801979,
      "learning_rate": 5.372222222222222e-06,
      "loss": 0.0031,
      "step": 80330
    },
    {
      "epoch": 4.463333333333333,
      "grad_norm": 0.1193133220076561,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.0025,
      "step": 80340
    },
    {
      "epoch": 4.463888888888889,
      "grad_norm": 0.1194131150841713,
      "learning_rate": 5.361111111111111e-06,
      "loss": 0.0031,
      "step": 80350
    },
    {
      "epoch": 4.464444444444444,
      "grad_norm": 0.04431736469268799,
      "learning_rate": 5.355555555555556e-06,
      "loss": 0.0034,
      "step": 80360
    },
    {
      "epoch": 4.465,
      "grad_norm": 0.17843101918697357,
      "learning_rate": 5.3500000000000004e-06,
      "loss": 0.0022,
      "step": 80370
    },
    {
      "epoch": 4.4655555555555555,
      "grad_norm": 0.2497287094593048,
      "learning_rate": 5.344444444444445e-06,
      "loss": 0.0021,
      "step": 80380
    },
    {
      "epoch": 4.466111111111111,
      "grad_norm": 0.33747556805610657,
      "learning_rate": 5.338888888888889e-06,
      "loss": 0.0024,
      "step": 80390
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.0907282754778862,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0016,
      "step": 80400
    },
    {
      "epoch": 4.467222222222222,
      "grad_norm": 0.12432423233985901,
      "learning_rate": 5.327777777777778e-06,
      "loss": 0.0032,
      "step": 80410
    },
    {
      "epoch": 4.467777777777778,
      "grad_norm": 0.3262474834918976,
      "learning_rate": 5.3222222222222225e-06,
      "loss": 0.0028,
      "step": 80420
    },
    {
      "epoch": 4.468333333333334,
      "grad_norm": 0.35350966453552246,
      "learning_rate": 5.316666666666667e-06,
      "loss": 0.0024,
      "step": 80430
    },
    {
      "epoch": 4.468888888888889,
      "grad_norm": 0.11897942423820496,
      "learning_rate": 5.311111111111111e-06,
      "loss": 0.0025,
      "step": 80440
    },
    {
      "epoch": 4.469444444444444,
      "grad_norm": 0.2662486433982849,
      "learning_rate": 5.305555555555556e-06,
      "loss": 0.0033,
      "step": 80450
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.06183159723877907,
      "learning_rate": 5.3e-06,
      "loss": 0.0024,
      "step": 80460
    },
    {
      "epoch": 4.470555555555555,
      "grad_norm": 0.09044546633958817,
      "learning_rate": 5.294444444444445e-06,
      "loss": 0.003,
      "step": 80470
    },
    {
      "epoch": 4.471111111111111,
      "grad_norm": 0.09282291680574417,
      "learning_rate": 5.288888888888889e-06,
      "loss": 0.0034,
      "step": 80480
    },
    {
      "epoch": 4.471666666666667,
      "grad_norm": 0.36372965574264526,
      "learning_rate": 5.283333333333334e-06,
      "loss": 0.0025,
      "step": 80490
    },
    {
      "epoch": 4.472222222222222,
      "grad_norm": 0.23771101236343384,
      "learning_rate": 5.277777777777778e-06,
      "loss": 0.0032,
      "step": 80500
    },
    {
      "epoch": 4.472777777777778,
      "grad_norm": 0.17846466600894928,
      "learning_rate": 5.272222222222222e-06,
      "loss": 0.0033,
      "step": 80510
    },
    {
      "epoch": 4.473333333333334,
      "grad_norm": 0.23873412609100342,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0028,
      "step": 80520
    },
    {
      "epoch": 4.473888888888889,
      "grad_norm": 0.4509240686893463,
      "learning_rate": 5.261111111111111e-06,
      "loss": 0.0028,
      "step": 80530
    },
    {
      "epoch": 4.474444444444444,
      "grad_norm": 0.11840355396270752,
      "learning_rate": 5.255555555555556e-06,
      "loss": 0.0027,
      "step": 80540
    },
    {
      "epoch": 4.475,
      "grad_norm": 0.17826658487319946,
      "learning_rate": 5.25e-06,
      "loss": 0.0021,
      "step": 80550
    },
    {
      "epoch": 4.475555555555555,
      "grad_norm": 0.031774044036865234,
      "learning_rate": 5.244444444444445e-06,
      "loss": 0.004,
      "step": 80560
    },
    {
      "epoch": 4.476111111111111,
      "grad_norm": 0.29944464564323425,
      "learning_rate": 5.238888888888889e-06,
      "loss": 0.0029,
      "step": 80570
    },
    {
      "epoch": 4.476666666666667,
      "grad_norm": 0.1472923755645752,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.0021,
      "step": 80580
    },
    {
      "epoch": 4.477222222222222,
      "grad_norm": 0.06161905452609062,
      "learning_rate": 5.227777777777778e-06,
      "loss": 0.003,
      "step": 80590
    },
    {
      "epoch": 4.477777777777778,
      "grad_norm": 0.12101009488105774,
      "learning_rate": 5.2222222222222226e-06,
      "loss": 0.002,
      "step": 80600
    },
    {
      "epoch": 4.4783333333333335,
      "grad_norm": 0.2470335215330124,
      "learning_rate": 5.216666666666667e-06,
      "loss": 0.0014,
      "step": 80610
    },
    {
      "epoch": 4.478888888888889,
      "grad_norm": 0.17918023467063904,
      "learning_rate": 5.211111111111111e-06,
      "loss": 0.0036,
      "step": 80620
    },
    {
      "epoch": 4.479444444444445,
      "grad_norm": 0.08969562500715256,
      "learning_rate": 5.205555555555556e-06,
      "loss": 0.0025,
      "step": 80630
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.06037471070885658,
      "learning_rate": 5.2e-06,
      "loss": 0.0023,
      "step": 80640
    },
    {
      "epoch": 4.480555555555555,
      "grad_norm": 0.060159847140312195,
      "learning_rate": 5.194444444444445e-06,
      "loss": 0.0026,
      "step": 80650
    },
    {
      "epoch": 4.481111111111111,
      "grad_norm": 0.03137810528278351,
      "learning_rate": 5.188888888888889e-06,
      "loss": 0.0033,
      "step": 80660
    },
    {
      "epoch": 4.4816666666666665,
      "grad_norm": 0.06089617684483528,
      "learning_rate": 5.183333333333333e-06,
      "loss": 0.002,
      "step": 80670
    },
    {
      "epoch": 4.482222222222222,
      "grad_norm": 0.38531842827796936,
      "learning_rate": 5.177777777777778e-06,
      "loss": 0.0032,
      "step": 80680
    },
    {
      "epoch": 4.482777777777778,
      "grad_norm": 0.2671094834804535,
      "learning_rate": 5.172222222222223e-06,
      "loss": 0.0054,
      "step": 80690
    },
    {
      "epoch": 4.483333333333333,
      "grad_norm": 0.14946703612804413,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.0028,
      "step": 80700
    },
    {
      "epoch": 4.483888888888889,
      "grad_norm": 0.1775897890329361,
      "learning_rate": 5.161111111111112e-06,
      "loss": 0.0035,
      "step": 80710
    },
    {
      "epoch": 4.484444444444445,
      "grad_norm": 0.05964922904968262,
      "learning_rate": 5.155555555555555e-06,
      "loss": 0.003,
      "step": 80720
    },
    {
      "epoch": 4.485,
      "grad_norm": 0.17746445536613464,
      "learning_rate": 5.15e-06,
      "loss": 0.0013,
      "step": 80730
    },
    {
      "epoch": 4.485555555555556,
      "grad_norm": 0.26648274064064026,
      "learning_rate": 5.144444444444445e-06,
      "loss": 0.0019,
      "step": 80740
    },
    {
      "epoch": 4.486111111111111,
      "grad_norm": 0.03270100802183151,
      "learning_rate": 5.138888888888889e-06,
      "loss": 0.0019,
      "step": 80750
    },
    {
      "epoch": 4.486666666666666,
      "grad_norm": 0.0594840943813324,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0024,
      "step": 80760
    },
    {
      "epoch": 4.487222222222222,
      "grad_norm": 0.472685843706131,
      "learning_rate": 5.127777777777778e-06,
      "loss": 0.0027,
      "step": 80770
    },
    {
      "epoch": 4.487777777777778,
      "grad_norm": 0.1198967918753624,
      "learning_rate": 5.122222222222223e-06,
      "loss": 0.002,
      "step": 80780
    },
    {
      "epoch": 4.488333333333333,
      "grad_norm": 0.17816321551799774,
      "learning_rate": 5.116666666666667e-06,
      "loss": 0.0024,
      "step": 80790
    },
    {
      "epoch": 4.488888888888889,
      "grad_norm": 0.06293471157550812,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 0.0025,
      "step": 80800
    },
    {
      "epoch": 4.489444444444445,
      "grad_norm": 0.0313403494656086,
      "learning_rate": 5.105555555555556e-06,
      "loss": 0.0031,
      "step": 80810
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.41889122128486633,
      "learning_rate": 5.1e-06,
      "loss": 0.003,
      "step": 80820
    },
    {
      "epoch": 4.490555555555556,
      "grad_norm": 0.1355379968881607,
      "learning_rate": 5.094444444444445e-06,
      "loss": 0.0021,
      "step": 80830
    },
    {
      "epoch": 4.4911111111111115,
      "grad_norm": 0.3978278338909149,
      "learning_rate": 5.088888888888889e-06,
      "loss": 0.0016,
      "step": 80840
    },
    {
      "epoch": 4.491666666666666,
      "grad_norm": 0.11890845745801926,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.0023,
      "step": 80850
    },
    {
      "epoch": 4.492222222222222,
      "grad_norm": 0.5323438048362732,
      "learning_rate": 5.077777777777778e-06,
      "loss": 0.0027,
      "step": 80860
    },
    {
      "epoch": 4.4927777777777775,
      "grad_norm": 0.32104888558387756,
      "learning_rate": 5.072222222222222e-06,
      "loss": 0.002,
      "step": 80870
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.5933849811553955,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0025,
      "step": 80880
    },
    {
      "epoch": 4.493888888888889,
      "grad_norm": 0.11767783761024475,
      "learning_rate": 5.061111111111112e-06,
      "loss": 0.0031,
      "step": 80890
    },
    {
      "epoch": 4.4944444444444445,
      "grad_norm": 0.03600635752081871,
      "learning_rate": 5.0555555555555555e-06,
      "loss": 0.0024,
      "step": 80900
    },
    {
      "epoch": 4.495,
      "grad_norm": 0.044642817229032516,
      "learning_rate": 5.050000000000001e-06,
      "loss": 0.0028,
      "step": 80910
    },
    {
      "epoch": 4.495555555555556,
      "grad_norm": 0.26657333970069885,
      "learning_rate": 5.044444444444444e-06,
      "loss": 0.0027,
      "step": 80920
    },
    {
      "epoch": 4.496111111111111,
      "grad_norm": 0.41517478227615356,
      "learning_rate": 5.038888888888889e-06,
      "loss": 0.0019,
      "step": 80930
    },
    {
      "epoch": 4.496666666666667,
      "grad_norm": 0.20765018463134766,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0033,
      "step": 80940
    },
    {
      "epoch": 4.497222222222222,
      "grad_norm": 0.6792243123054504,
      "learning_rate": 5.0277777777777775e-06,
      "loss": 0.0029,
      "step": 80950
    },
    {
      "epoch": 4.497777777777777,
      "grad_norm": 0.03099920228123665,
      "learning_rate": 5.022222222222223e-06,
      "loss": 0.0022,
      "step": 80960
    },
    {
      "epoch": 4.498333333333333,
      "grad_norm": 0.5046054124832153,
      "learning_rate": 5.016666666666666e-06,
      "loss": 0.0034,
      "step": 80970
    },
    {
      "epoch": 4.498888888888889,
      "grad_norm": 0.03136254474520683,
      "learning_rate": 5.011111111111112e-06,
      "loss": 0.0027,
      "step": 80980
    },
    {
      "epoch": 4.499444444444444,
      "grad_norm": 0.30259624123573303,
      "learning_rate": 5.005555555555556e-06,
      "loss": 0.0024,
      "step": 80990
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.2370685338973999,
      "learning_rate": 5e-06,
      "loss": 0.003,
      "step": 81000
    },
    {
      "epoch": 4.500555555555556,
      "grad_norm": 0.11872819811105728,
      "learning_rate": 4.994444444444445e-06,
      "loss": 0.003,
      "step": 81010
    },
    {
      "epoch": 4.501111111111111,
      "grad_norm": 0.2673127353191376,
      "learning_rate": 4.988888888888889e-06,
      "loss": 0.0029,
      "step": 81020
    },
    {
      "epoch": 4.501666666666667,
      "grad_norm": 0.07056734710931778,
      "learning_rate": 4.983333333333334e-06,
      "loss": 0.0032,
      "step": 81030
    },
    {
      "epoch": 4.502222222222223,
      "grad_norm": 0.15669776499271393,
      "learning_rate": 4.977777777777778e-06,
      "loss": 0.0025,
      "step": 81040
    },
    {
      "epoch": 4.502777777777778,
      "grad_norm": 0.2903009355068207,
      "learning_rate": 4.9722222222222224e-06,
      "loss": 0.0027,
      "step": 81050
    },
    {
      "epoch": 4.503333333333333,
      "grad_norm": 0.4741738736629486,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0028,
      "step": 81060
    },
    {
      "epoch": 4.503888888888889,
      "grad_norm": 0.20769886672496796,
      "learning_rate": 4.961111111111111e-06,
      "loss": 0.0024,
      "step": 81070
    },
    {
      "epoch": 4.504444444444444,
      "grad_norm": 0.3575948476791382,
      "learning_rate": 4.955555555555556e-06,
      "loss": 0.002,
      "step": 81080
    },
    {
      "epoch": 4.505,
      "grad_norm": 0.5637065172195435,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.0031,
      "step": 81090
    },
    {
      "epoch": 4.5055555555555555,
      "grad_norm": 0.20816224813461304,
      "learning_rate": 4.9444444444444444e-06,
      "loss": 0.0013,
      "step": 81100
    },
    {
      "epoch": 4.506111111111111,
      "grad_norm": 0.06232057139277458,
      "learning_rate": 4.93888888888889e-06,
      "loss": 0.0031,
      "step": 81110
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.08945481479167938,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0023,
      "step": 81120
    },
    {
      "epoch": 4.5072222222222225,
      "grad_norm": 0.06087963283061981,
      "learning_rate": 4.927777777777778e-06,
      "loss": 0.0023,
      "step": 81130
    },
    {
      "epoch": 4.507777777777778,
      "grad_norm": 0.1653764694929123,
      "learning_rate": 4.922222222222223e-06,
      "loss": 0.0014,
      "step": 81140
    },
    {
      "epoch": 4.508333333333333,
      "grad_norm": 0.05989345535635948,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.0022,
      "step": 81150
    },
    {
      "epoch": 4.5088888888888885,
      "grad_norm": 0.20884372293949127,
      "learning_rate": 4.911111111111112e-06,
      "loss": 0.0023,
      "step": 81160
    },
    {
      "epoch": 4.509444444444444,
      "grad_norm": 0.17821112275123596,
      "learning_rate": 4.905555555555555e-06,
      "loss": 0.0032,
      "step": 81170
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.03288287669420242,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0033,
      "step": 81180
    },
    {
      "epoch": 4.510555555555555,
      "grad_norm": 0.14812177419662476,
      "learning_rate": 4.894444444444445e-06,
      "loss": 0.0021,
      "step": 81190
    },
    {
      "epoch": 4.511111111111111,
      "grad_norm": 0.015563206747174263,
      "learning_rate": 4.888888888888889e-06,
      "loss": 0.0028,
      "step": 81200
    },
    {
      "epoch": 4.511666666666667,
      "grad_norm": 0.20791128277778625,
      "learning_rate": 4.883333333333334e-06,
      "loss": 0.0024,
      "step": 81210
    },
    {
      "epoch": 4.512222222222222,
      "grad_norm": 0.23707720637321472,
      "learning_rate": 4.877777777777778e-06,
      "loss": 0.0039,
      "step": 81220
    },
    {
      "epoch": 4.512777777777778,
      "grad_norm": 0.24041296541690826,
      "learning_rate": 4.8722222222222225e-06,
      "loss": 0.0022,
      "step": 81230
    },
    {
      "epoch": 4.513333333333334,
      "grad_norm": 0.29624369740486145,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0029,
      "step": 81240
    },
    {
      "epoch": 4.513888888888889,
      "grad_norm": 0.2962888777256012,
      "learning_rate": 4.861111111111111e-06,
      "loss": 0.0018,
      "step": 81250
    },
    {
      "epoch": 4.514444444444445,
      "grad_norm": 0.009722819551825523,
      "learning_rate": 4.855555555555556e-06,
      "loss": 0.0035,
      "step": 81260
    },
    {
      "epoch": 4.515,
      "grad_norm": 0.14829935133457184,
      "learning_rate": 4.85e-06,
      "loss": 0.0023,
      "step": 81270
    },
    {
      "epoch": 4.515555555555555,
      "grad_norm": 0.23712597787380219,
      "learning_rate": 4.8444444444444446e-06,
      "loss": 0.0023,
      "step": 81280
    },
    {
      "epoch": 4.516111111111111,
      "grad_norm": 0.1781727373600006,
      "learning_rate": 4.838888888888889e-06,
      "loss": 0.0023,
      "step": 81290
    },
    {
      "epoch": 4.516666666666667,
      "grad_norm": 0.013478807173669338,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0014,
      "step": 81300
    },
    {
      "epoch": 4.517222222222222,
      "grad_norm": 0.2070055603981018,
      "learning_rate": 4.827777777777779e-06,
      "loss": 0.0024,
      "step": 81310
    },
    {
      "epoch": 4.517777777777778,
      "grad_norm": 0.08940568566322327,
      "learning_rate": 4.822222222222222e-06,
      "loss": 0.0022,
      "step": 81320
    },
    {
      "epoch": 4.5183333333333335,
      "grad_norm": 0.2669200301170349,
      "learning_rate": 4.816666666666667e-06,
      "loss": 0.0032,
      "step": 81330
    },
    {
      "epoch": 4.518888888888889,
      "grad_norm": 0.29745426774024963,
      "learning_rate": 4.811111111111111e-06,
      "loss": 0.0028,
      "step": 81340
    },
    {
      "epoch": 4.519444444444445,
      "grad_norm": 0.4819062352180481,
      "learning_rate": 4.805555555555555e-06,
      "loss": 0.0026,
      "step": 81350
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.1193171814084053,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0032,
      "step": 81360
    },
    {
      "epoch": 4.520555555555555,
      "grad_norm": 0.030851831659674644,
      "learning_rate": 4.794444444444444e-06,
      "loss": 0.0025,
      "step": 81370
    },
    {
      "epoch": 4.521111111111111,
      "grad_norm": 0.06661497801542282,
      "learning_rate": 4.7888888888888894e-06,
      "loss": 0.0027,
      "step": 81380
    },
    {
      "epoch": 4.5216666666666665,
      "grad_norm": 0.11906856298446655,
      "learning_rate": 4.783333333333333e-06,
      "loss": 0.0025,
      "step": 81390
    },
    {
      "epoch": 4.522222222222222,
      "grad_norm": 0.17865686118602753,
      "learning_rate": 4.777777777777778e-06,
      "loss": 0.0024,
      "step": 81400
    },
    {
      "epoch": 4.522777777777778,
      "grad_norm": 0.090964674949646,
      "learning_rate": 4.772222222222223e-06,
      "loss": 0.0028,
      "step": 81410
    },
    {
      "epoch": 4.523333333333333,
      "grad_norm": 0.17746679484844208,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.0017,
      "step": 81420
    },
    {
      "epoch": 4.523888888888889,
      "grad_norm": 0.14801542460918427,
      "learning_rate": 4.7611111111111115e-06,
      "loss": 0.0026,
      "step": 81430
    },
    {
      "epoch": 4.524444444444445,
      "grad_norm": 0.2674247920513153,
      "learning_rate": 4.755555555555556e-06,
      "loss": 0.0042,
      "step": 81440
    },
    {
      "epoch": 4.525,
      "grad_norm": 0.03204113990068436,
      "learning_rate": 4.75e-06,
      "loss": 0.0027,
      "step": 81450
    },
    {
      "epoch": 4.525555555555556,
      "grad_norm": 0.23676463961601257,
      "learning_rate": 4.744444444444445e-06,
      "loss": 0.0035,
      "step": 81460
    },
    {
      "epoch": 4.526111111111111,
      "grad_norm": 0.03308802843093872,
      "learning_rate": 4.738888888888889e-06,
      "loss": 0.0025,
      "step": 81470
    },
    {
      "epoch": 4.526666666666666,
      "grad_norm": 0.563685417175293,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.003,
      "step": 81480
    },
    {
      "epoch": 4.527222222222222,
      "grad_norm": 0.10826554149389267,
      "learning_rate": 4.727777777777778e-06,
      "loss": 0.0026,
      "step": 81490
    },
    {
      "epoch": 4.527777777777778,
      "grad_norm": 0.11862578243017197,
      "learning_rate": 4.722222222222222e-06,
      "loss": 0.003,
      "step": 81500
    },
    {
      "epoch": 4.528333333333333,
      "grad_norm": 0.2971212863922119,
      "learning_rate": 4.7166666666666675e-06,
      "loss": 0.0019,
      "step": 81510
    },
    {
      "epoch": 4.528888888888889,
      "grad_norm": 0.8327400088310242,
      "learning_rate": 4.711111111111111e-06,
      "loss": 0.0036,
      "step": 81520
    },
    {
      "epoch": 4.529444444444445,
      "grad_norm": 0.06280416995286942,
      "learning_rate": 4.705555555555556e-06,
      "loss": 0.0023,
      "step": 81530
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.06434892863035202,
      "learning_rate": 4.7e-06,
      "loss": 0.0029,
      "step": 81540
    },
    {
      "epoch": 4.530555555555556,
      "grad_norm": 0.1777763068675995,
      "learning_rate": 4.694444444444444e-06,
      "loss": 0.0023,
      "step": 81550
    },
    {
      "epoch": 4.531111111111111,
      "grad_norm": 0.1494559794664383,
      "learning_rate": 4.6888888888888895e-06,
      "loss": 0.0019,
      "step": 81560
    },
    {
      "epoch": 4.531666666666666,
      "grad_norm": 0.012570919468998909,
      "learning_rate": 4.683333333333333e-06,
      "loss": 0.0026,
      "step": 81570
    },
    {
      "epoch": 4.532222222222222,
      "grad_norm": 0.03343972563743591,
      "learning_rate": 4.677777777777778e-06,
      "loss": 0.002,
      "step": 81580
    },
    {
      "epoch": 4.532777777777778,
      "grad_norm": 0.29560336470603943,
      "learning_rate": 4.672222222222222e-06,
      "loss": 0.0023,
      "step": 81590
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.012483332306146622,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0028,
      "step": 81600
    },
    {
      "epoch": 4.533888888888889,
      "grad_norm": 0.14801561832427979,
      "learning_rate": 4.6611111111111116e-06,
      "loss": 0.0024,
      "step": 81610
    },
    {
      "epoch": 4.5344444444444445,
      "grad_norm": 0.7798712253570557,
      "learning_rate": 4.655555555555556e-06,
      "loss": 0.0045,
      "step": 81620
    },
    {
      "epoch": 4.535,
      "grad_norm": 0.2667270302772522,
      "learning_rate": 4.65e-06,
      "loss": 0.0029,
      "step": 81630
    },
    {
      "epoch": 4.535555555555556,
      "grad_norm": 0.355561226606369,
      "learning_rate": 4.644444444444444e-06,
      "loss": 0.0017,
      "step": 81640
    },
    {
      "epoch": 4.536111111111111,
      "grad_norm": 0.031947433948516846,
      "learning_rate": 4.638888888888889e-06,
      "loss": 0.0027,
      "step": 81650
    },
    {
      "epoch": 4.536666666666667,
      "grad_norm": 0.030907249078154564,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0023,
      "step": 81660
    },
    {
      "epoch": 4.537222222222223,
      "grad_norm": 0.2439766526222229,
      "learning_rate": 4.627777777777778e-06,
      "loss": 0.0036,
      "step": 81670
    },
    {
      "epoch": 4.5377777777777775,
      "grad_norm": 0.03159251809120178,
      "learning_rate": 4.622222222222222e-06,
      "loss": 0.0034,
      "step": 81680
    },
    {
      "epoch": 4.538333333333333,
      "grad_norm": 0.6154196262359619,
      "learning_rate": 4.616666666666667e-06,
      "loss": 0.0028,
      "step": 81690
    },
    {
      "epoch": 4.538888888888889,
      "grad_norm": 0.23708570003509521,
      "learning_rate": 4.611111111111111e-06,
      "loss": 0.0021,
      "step": 81700
    },
    {
      "epoch": 4.539444444444444,
      "grad_norm": 0.03161142021417618,
      "learning_rate": 4.605555555555556e-06,
      "loss": 0.0024,
      "step": 81710
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.47501513361930847,
      "learning_rate": 4.6e-06,
      "loss": 0.0027,
      "step": 81720
    },
    {
      "epoch": 4.540555555555556,
      "grad_norm": 0.2667720317840576,
      "learning_rate": 4.594444444444445e-06,
      "loss": 0.0024,
      "step": 81730
    },
    {
      "epoch": 4.541111111111111,
      "grad_norm": 0.3257873058319092,
      "learning_rate": 4.588888888888889e-06,
      "loss": 0.0024,
      "step": 81740
    },
    {
      "epoch": 4.541666666666667,
      "grad_norm": 0.07314442098140717,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.0028,
      "step": 81750
    },
    {
      "epoch": 4.542222222222223,
      "grad_norm": 0.29752853512763977,
      "learning_rate": 4.5777777777777785e-06,
      "loss": 0.0019,
      "step": 81760
    },
    {
      "epoch": 4.542777777777777,
      "grad_norm": 0.17837482690811157,
      "learning_rate": 4.572222222222222e-06,
      "loss": 0.0024,
      "step": 81770
    },
    {
      "epoch": 4.543333333333333,
      "grad_norm": 0.3269074857234955,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0024,
      "step": 81780
    },
    {
      "epoch": 4.543888888888889,
      "grad_norm": 0.03585372865200043,
      "learning_rate": 4.561111111111111e-06,
      "loss": 0.0019,
      "step": 81790
    },
    {
      "epoch": 4.544444444444444,
      "grad_norm": 0.14910757541656494,
      "learning_rate": 4.555555555555556e-06,
      "loss": 0.0026,
      "step": 81800
    },
    {
      "epoch": 4.545,
      "grad_norm": 0.0627572312951088,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 0.0018,
      "step": 81810
    },
    {
      "epoch": 4.545555555555556,
      "grad_norm": 0.23545074462890625,
      "learning_rate": 4.544444444444445e-06,
      "loss": 0.003,
      "step": 81820
    },
    {
      "epoch": 4.546111111111111,
      "grad_norm": 0.08088120818138123,
      "learning_rate": 4.538888888888889e-06,
      "loss": 0.0027,
      "step": 81830
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.11866948008537292,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0032,
      "step": 81840
    },
    {
      "epoch": 4.5472222222222225,
      "grad_norm": 0.14864413440227509,
      "learning_rate": 4.527777777777778e-06,
      "loss": 0.0041,
      "step": 81850
    },
    {
      "epoch": 4.547777777777778,
      "grad_norm": 0.14936435222625732,
      "learning_rate": 4.5222222222222225e-06,
      "loss": 0.0036,
      "step": 81860
    },
    {
      "epoch": 4.548333333333334,
      "grad_norm": 0.2087099403142929,
      "learning_rate": 4.516666666666667e-06,
      "loss": 0.002,
      "step": 81870
    },
    {
      "epoch": 4.5488888888888885,
      "grad_norm": 0.18077947199344635,
      "learning_rate": 4.511111111111111e-06,
      "loss": 0.0033,
      "step": 81880
    },
    {
      "epoch": 4.549444444444444,
      "grad_norm": 0.015174751169979572,
      "learning_rate": 4.505555555555556e-06,
      "loss": 0.0025,
      "step": 81890
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.09014573693275452,
      "learning_rate": 4.5e-06,
      "loss": 0.0015,
      "step": 81900
    },
    {
      "epoch": 4.5505555555555555,
      "grad_norm": 0.20938488841056824,
      "learning_rate": 4.4944444444444445e-06,
      "loss": 0.002,
      "step": 81910
    },
    {
      "epoch": 4.551111111111111,
      "grad_norm": 0.35639962553977966,
      "learning_rate": 4.488888888888889e-06,
      "loss": 0.0029,
      "step": 81920
    },
    {
      "epoch": 4.551666666666667,
      "grad_norm": 0.08976124227046967,
      "learning_rate": 4.483333333333334e-06,
      "loss": 0.0021,
      "step": 81930
    },
    {
      "epoch": 4.552222222222222,
      "grad_norm": 0.17822027206420898,
      "learning_rate": 4.477777777777778e-06,
      "loss": 0.0035,
      "step": 81940
    },
    {
      "epoch": 4.552777777777778,
      "grad_norm": 0.09000150859355927,
      "learning_rate": 4.472222222222222e-06,
      "loss": 0.0019,
      "step": 81950
    },
    {
      "epoch": 4.553333333333334,
      "grad_norm": 0.266954630613327,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0035,
      "step": 81960
    },
    {
      "epoch": 4.553888888888888,
      "grad_norm": 0.03502229228615761,
      "learning_rate": 4.461111111111111e-06,
      "loss": 0.0026,
      "step": 81970
    },
    {
      "epoch": 4.554444444444444,
      "grad_norm": 0.2202792763710022,
      "learning_rate": 4.455555555555556e-06,
      "loss": 0.0021,
      "step": 81980
    },
    {
      "epoch": 4.555,
      "grad_norm": 0.3858473300933838,
      "learning_rate": 4.45e-06,
      "loss": 0.0024,
      "step": 81990
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.00901475828140974,
      "learning_rate": 4.444444444444445e-06,
      "loss": 0.0026,
      "step": 82000
    },
    {
      "epoch": 4.556111111111111,
      "grad_norm": 0.22711044549942017,
      "learning_rate": 4.4388888888888886e-06,
      "loss": 0.0022,
      "step": 82010
    },
    {
      "epoch": 4.556666666666667,
      "grad_norm": 0.29609471559524536,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0029,
      "step": 82020
    },
    {
      "epoch": 4.557222222222222,
      "grad_norm": 0.07331114262342453,
      "learning_rate": 4.427777777777778e-06,
      "loss": 0.0034,
      "step": 82030
    },
    {
      "epoch": 4.557777777777778,
      "grad_norm": 0.01648230291903019,
      "learning_rate": 4.422222222222223e-06,
      "loss": 0.0017,
      "step": 82040
    },
    {
      "epoch": 4.558333333333334,
      "grad_norm": 0.23711824417114258,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.0022,
      "step": 82050
    },
    {
      "epoch": 4.558888888888889,
      "grad_norm": 0.034221101552248,
      "learning_rate": 4.411111111111111e-06,
      "loss": 0.0028,
      "step": 82060
    },
    {
      "epoch": 4.559444444444445,
      "grad_norm": 0.26807475090026855,
      "learning_rate": 4.405555555555556e-06,
      "loss": 0.0028,
      "step": 82070
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.0611591637134552,
      "learning_rate": 4.4e-06,
      "loss": 0.0027,
      "step": 82080
    },
    {
      "epoch": 4.560555555555555,
      "grad_norm": 0.14915136992931366,
      "learning_rate": 4.394444444444445e-06,
      "loss": 0.0013,
      "step": 82090
    },
    {
      "epoch": 4.561111111111111,
      "grad_norm": 0.11972974985837936,
      "learning_rate": 4.388888888888889e-06,
      "loss": 0.0024,
      "step": 82100
    },
    {
      "epoch": 4.5616666666666665,
      "grad_norm": 0.14865723252296448,
      "learning_rate": 4.3833333333333334e-06,
      "loss": 0.002,
      "step": 82110
    },
    {
      "epoch": 4.562222222222222,
      "grad_norm": 0.3174048066139221,
      "learning_rate": 4.377777777777778e-06,
      "loss": 0.0019,
      "step": 82120
    },
    {
      "epoch": 4.562777777777778,
      "grad_norm": 0.15363477170467377,
      "learning_rate": 4.372222222222223e-06,
      "loss": 0.0024,
      "step": 82130
    },
    {
      "epoch": 4.5633333333333335,
      "grad_norm": 0.3191460967063904,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.003,
      "step": 82140
    },
    {
      "epoch": 4.563888888888889,
      "grad_norm": 0.0897132083773613,
      "learning_rate": 4.361111111111112e-06,
      "loss": 0.0029,
      "step": 82150
    },
    {
      "epoch": 4.564444444444445,
      "grad_norm": 0.08996012806892395,
      "learning_rate": 4.3555555555555555e-06,
      "loss": 0.0018,
      "step": 82160
    },
    {
      "epoch": 4.5649999999999995,
      "grad_norm": 0.06002132222056389,
      "learning_rate": 4.35e-06,
      "loss": 0.0024,
      "step": 82170
    },
    {
      "epoch": 4.565555555555555,
      "grad_norm": 0.03781084343791008,
      "learning_rate": 4.344444444444445e-06,
      "loss": 0.0033,
      "step": 82180
    },
    {
      "epoch": 4.566111111111111,
      "grad_norm": 0.04418107122182846,
      "learning_rate": 4.338888888888889e-06,
      "loss": 0.0032,
      "step": 82190
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 0.11902043223381042,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0029,
      "step": 82200
    },
    {
      "epoch": 4.567222222222222,
      "grad_norm": 0.26764971017837524,
      "learning_rate": 4.3277777777777775e-06,
      "loss": 0.0021,
      "step": 82210
    },
    {
      "epoch": 4.567777777777778,
      "grad_norm": 0.33137086033821106,
      "learning_rate": 4.322222222222223e-06,
      "loss": 0.0027,
      "step": 82220
    },
    {
      "epoch": 4.568333333333333,
      "grad_norm": 0.26911264657974243,
      "learning_rate": 4.316666666666667e-06,
      "loss": 0.0029,
      "step": 82230
    },
    {
      "epoch": 4.568888888888889,
      "grad_norm": 0.03382432088255882,
      "learning_rate": 4.3111111111111115e-06,
      "loss": 0.0026,
      "step": 82240
    },
    {
      "epoch": 4.569444444444445,
      "grad_norm": 0.06127185374498367,
      "learning_rate": 4.305555555555556e-06,
      "loss": 0.0026,
      "step": 82250
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.4242710769176483,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0024,
      "step": 82260
    },
    {
      "epoch": 4.570555555555556,
      "grad_norm": 0.23773135244846344,
      "learning_rate": 4.294444444444445e-06,
      "loss": 0.0022,
      "step": 82270
    },
    {
      "epoch": 4.571111111111112,
      "grad_norm": 0.1187964528799057,
      "learning_rate": 4.288888888888889e-06,
      "loss": 0.0023,
      "step": 82280
    },
    {
      "epoch": 4.571666666666666,
      "grad_norm": 0.3558807373046875,
      "learning_rate": 4.2833333333333335e-06,
      "loss": 0.0036,
      "step": 82290
    },
    {
      "epoch": 4.572222222222222,
      "grad_norm": 0.2465561330318451,
      "learning_rate": 4.277777777777778e-06,
      "loss": 0.0023,
      "step": 82300
    },
    {
      "epoch": 4.572777777777778,
      "grad_norm": 0.1381593942642212,
      "learning_rate": 4.272222222222222e-06,
      "loss": 0.0031,
      "step": 82310
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.04214458540081978,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0025,
      "step": 82320
    },
    {
      "epoch": 4.573888888888889,
      "grad_norm": 0.15698879957199097,
      "learning_rate": 4.261111111111111e-06,
      "loss": 0.003,
      "step": 82330
    },
    {
      "epoch": 4.5744444444444445,
      "grad_norm": 0.29703575372695923,
      "learning_rate": 4.2555555555555556e-06,
      "loss": 0.0025,
      "step": 82340
    },
    {
      "epoch": 4.575,
      "grad_norm": 0.11878837645053864,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.0025,
      "step": 82350
    },
    {
      "epoch": 4.575555555555556,
      "grad_norm": 0.1584276258945465,
      "learning_rate": 4.244444444444444e-06,
      "loss": 0.0029,
      "step": 82360
    },
    {
      "epoch": 4.5761111111111115,
      "grad_norm": 0.21253421902656555,
      "learning_rate": 4.238888888888889e-06,
      "loss": 0.0032,
      "step": 82370
    },
    {
      "epoch": 4.576666666666666,
      "grad_norm": 0.1486952304840088,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.0029,
      "step": 82380
    },
    {
      "epoch": 4.577222222222222,
      "grad_norm": 0.1490662544965744,
      "learning_rate": 4.227777777777778e-06,
      "loss": 0.0027,
      "step": 82390
    },
    {
      "epoch": 4.5777777777777775,
      "grad_norm": 0.14791180193424225,
      "learning_rate": 4.222222222222223e-06,
      "loss": 0.0027,
      "step": 82400
    },
    {
      "epoch": 4.578333333333333,
      "grad_norm": 0.03153976798057556,
      "learning_rate": 4.216666666666666e-06,
      "loss": 0.0029,
      "step": 82410
    },
    {
      "epoch": 4.578888888888889,
      "grad_norm": 0.022941216826438904,
      "learning_rate": 4.211111111111112e-06,
      "loss": 0.0022,
      "step": 82420
    },
    {
      "epoch": 4.579444444444444,
      "grad_norm": 0.031746864318847656,
      "learning_rate": 4.205555555555556e-06,
      "loss": 0.0028,
      "step": 82430
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.08992689102888107,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0022,
      "step": 82440
    },
    {
      "epoch": 4.580555555555556,
      "grad_norm": 0.17851601541042328,
      "learning_rate": 4.194444444444445e-06,
      "loss": 0.0023,
      "step": 82450
    },
    {
      "epoch": 4.581111111111111,
      "grad_norm": 0.2674030065536499,
      "learning_rate": 4.188888888888889e-06,
      "loss": 0.0025,
      "step": 82460
    },
    {
      "epoch": 4.581666666666667,
      "grad_norm": 0.22991539537906647,
      "learning_rate": 4.183333333333334e-06,
      "loss": 0.0033,
      "step": 82470
    },
    {
      "epoch": 4.582222222222223,
      "grad_norm": 0.41446641087532043,
      "learning_rate": 4.177777777777778e-06,
      "loss": 0.0023,
      "step": 82480
    },
    {
      "epoch": 4.582777777777777,
      "grad_norm": 0.3843784034252167,
      "learning_rate": 4.1722222222222225e-06,
      "loss": 0.002,
      "step": 82490
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 0.5617904663085938,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0029,
      "step": 82500
    },
    {
      "epoch": 4.583888888888889,
      "grad_norm": 0.17862142622470856,
      "learning_rate": 4.161111111111111e-06,
      "loss": 0.0025,
      "step": 82510
    },
    {
      "epoch": 4.584444444444444,
      "grad_norm": 0.0899742841720581,
      "learning_rate": 4.155555555555556e-06,
      "loss": 0.0025,
      "step": 82520
    },
    {
      "epoch": 4.585,
      "grad_norm": 0.1252584457397461,
      "learning_rate": 4.15e-06,
      "loss": 0.0023,
      "step": 82530
    },
    {
      "epoch": 4.585555555555556,
      "grad_norm": 0.012408399023115635,
      "learning_rate": 4.1444444444444445e-06,
      "loss": 0.0021,
      "step": 82540
    },
    {
      "epoch": 4.586111111111111,
      "grad_norm": 0.275141179561615,
      "learning_rate": 4.13888888888889e-06,
      "loss": 0.0024,
      "step": 82550
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.05956970155239105,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0021,
      "step": 82560
    },
    {
      "epoch": 4.5872222222222225,
      "grad_norm": 0.23748241364955902,
      "learning_rate": 4.127777777777778e-06,
      "loss": 0.0029,
      "step": 82570
    },
    {
      "epoch": 4.587777777777777,
      "grad_norm": 0.17809335887432098,
      "learning_rate": 4.122222222222222e-06,
      "loss": 0.0027,
      "step": 82580
    },
    {
      "epoch": 4.588333333333333,
      "grad_norm": 0.23745635151863098,
      "learning_rate": 4.1166666666666665e-06,
      "loss": 0.0022,
      "step": 82590
    },
    {
      "epoch": 4.588888888888889,
      "grad_norm": 0.17817527055740356,
      "learning_rate": 4.111111111111112e-06,
      "loss": 0.004,
      "step": 82600
    },
    {
      "epoch": 4.589444444444444,
      "grad_norm": 0.20767545700073242,
      "learning_rate": 4.105555555555555e-06,
      "loss": 0.0024,
      "step": 82610
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.17888708412647247,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.0026,
      "step": 82620
    },
    {
      "epoch": 4.5905555555555555,
      "grad_norm": 0.5926951766014099,
      "learning_rate": 4.094444444444444e-06,
      "loss": 0.0028,
      "step": 82630
    },
    {
      "epoch": 4.591111111111111,
      "grad_norm": 0.29682499170303345,
      "learning_rate": 4.088888888888889e-06,
      "loss": 0.002,
      "step": 82640
    },
    {
      "epoch": 4.591666666666667,
      "grad_norm": 0.2984308898448944,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.0028,
      "step": 82650
    },
    {
      "epoch": 4.592222222222222,
      "grad_norm": 0.0169870276004076,
      "learning_rate": 4.077777777777778e-06,
      "loss": 0.0021,
      "step": 82660
    },
    {
      "epoch": 4.592777777777778,
      "grad_norm": 0.031116310507059097,
      "learning_rate": 4.0722222222222226e-06,
      "loss": 0.0023,
      "step": 82670
    },
    {
      "epoch": 4.593333333333334,
      "grad_norm": 0.20932622253894806,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0029,
      "step": 82680
    },
    {
      "epoch": 4.593888888888889,
      "grad_norm": 0.2081308662891388,
      "learning_rate": 4.061111111111111e-06,
      "loss": 0.0022,
      "step": 82690
    },
    {
      "epoch": 4.594444444444444,
      "grad_norm": 0.021956603974103928,
      "learning_rate": 4.055555555555556e-06,
      "loss": 0.0024,
      "step": 82700
    },
    {
      "epoch": 4.595,
      "grad_norm": 0.11982046812772751,
      "learning_rate": 4.05e-06,
      "loss": 0.0038,
      "step": 82710
    },
    {
      "epoch": 4.595555555555555,
      "grad_norm": 0.024914449080824852,
      "learning_rate": 4.044444444444445e-06,
      "loss": 0.0026,
      "step": 82720
    },
    {
      "epoch": 4.596111111111111,
      "grad_norm": 0.20701876282691956,
      "learning_rate": 4.038888888888889e-06,
      "loss": 0.0029,
      "step": 82730
    },
    {
      "epoch": 4.596666666666667,
      "grad_norm": 0.07333261519670486,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0028,
      "step": 82740
    },
    {
      "epoch": 4.597222222222222,
      "grad_norm": 0.41574957966804504,
      "learning_rate": 4.027777777777779e-06,
      "loss": 0.0031,
      "step": 82750
    },
    {
      "epoch": 4.597777777777778,
      "grad_norm": 0.28761622309684753,
      "learning_rate": 4.022222222222222e-06,
      "loss": 0.003,
      "step": 82760
    },
    {
      "epoch": 4.598333333333334,
      "grad_norm": 0.2378181368112564,
      "learning_rate": 4.0166666666666675e-06,
      "loss": 0.0026,
      "step": 82770
    },
    {
      "epoch": 4.598888888888889,
      "grad_norm": 0.1486036479473114,
      "learning_rate": 4.011111111111111e-06,
      "loss": 0.0018,
      "step": 82780
    },
    {
      "epoch": 4.599444444444444,
      "grad_norm": 0.6820365190505981,
      "learning_rate": 4.005555555555555e-06,
      "loss": 0.003,
      "step": 82790
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.3260122239589691,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.002,
      "step": 82800
    },
    {
      "epoch": 4.600555555555555,
      "grad_norm": 0.060589443892240524,
      "learning_rate": 3.994444444444444e-06,
      "loss": 0.0028,
      "step": 82810
    },
    {
      "epoch": 4.601111111111111,
      "grad_norm": 0.2074306756258011,
      "learning_rate": 3.9888888888888895e-06,
      "loss": 0.0025,
      "step": 82820
    },
    {
      "epoch": 4.601666666666667,
      "grad_norm": 0.11934950202703476,
      "learning_rate": 3.983333333333333e-06,
      "loss": 0.0027,
      "step": 82830
    },
    {
      "epoch": 4.602222222222222,
      "grad_norm": 0.41548874974250793,
      "learning_rate": 3.977777777777778e-06,
      "loss": 0.003,
      "step": 82840
    },
    {
      "epoch": 4.602777777777778,
      "grad_norm": 0.1610907018184662,
      "learning_rate": 3.972222222222223e-06,
      "loss": 0.0019,
      "step": 82850
    },
    {
      "epoch": 4.6033333333333335,
      "grad_norm": 0.5890589356422424,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.0031,
      "step": 82860
    },
    {
      "epoch": 4.603888888888889,
      "grad_norm": 0.12115488946437836,
      "learning_rate": 3.9611111111111115e-06,
      "loss": 0.0021,
      "step": 82870
    },
    {
      "epoch": 4.604444444444445,
      "grad_norm": 0.3848019540309906,
      "learning_rate": 3.955555555555555e-06,
      "loss": 0.0024,
      "step": 82880
    },
    {
      "epoch": 4.605,
      "grad_norm": 0.1781294196844101,
      "learning_rate": 3.95e-06,
      "loss": 0.002,
      "step": 82890
    },
    {
      "epoch": 4.605555555555555,
      "grad_norm": 0.1195945292711258,
      "learning_rate": 3.944444444444445e-06,
      "loss": 0.0023,
      "step": 82900
    },
    {
      "epoch": 4.606111111111111,
      "grad_norm": 0.26815590262413025,
      "learning_rate": 3.938888888888889e-06,
      "loss": 0.002,
      "step": 82910
    },
    {
      "epoch": 4.6066666666666665,
      "grad_norm": 0.06065022945404053,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0027,
      "step": 82920
    },
    {
      "epoch": 4.607222222222222,
      "grad_norm": 0.0977804958820343,
      "learning_rate": 3.927777777777778e-06,
      "loss": 0.0035,
      "step": 82930
    },
    {
      "epoch": 4.607777777777778,
      "grad_norm": 0.14774946868419647,
      "learning_rate": 3.922222222222222e-06,
      "loss": 0.0029,
      "step": 82940
    },
    {
      "epoch": 4.608333333333333,
      "grad_norm": 0.11707337945699692,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.0038,
      "step": 82950
    },
    {
      "epoch": 4.608888888888889,
      "grad_norm": 0.14878766238689423,
      "learning_rate": 3.911111111111111e-06,
      "loss": 0.0021,
      "step": 82960
    },
    {
      "epoch": 4.609444444444445,
      "grad_norm": 0.3426686227321625,
      "learning_rate": 3.905555555555556e-06,
      "loss": 0.0031,
      "step": 82970
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.09363264590501785,
      "learning_rate": 3.9e-06,
      "loss": 0.0022,
      "step": 82980
    },
    {
      "epoch": 4.610555555555555,
      "grad_norm": 0.14956702291965485,
      "learning_rate": 3.894444444444444e-06,
      "loss": 0.0036,
      "step": 82990
    },
    {
      "epoch": 4.611111111111111,
      "grad_norm": 0.061669327318668365,
      "learning_rate": 3.888888888888889e-06,
      "loss": 0.0031,
      "step": 83000
    },
    {
      "epoch": 4.611666666666666,
      "grad_norm": 0.07205014675855637,
      "learning_rate": 3.883333333333333e-06,
      "loss": 0.0031,
      "step": 83010
    },
    {
      "epoch": 4.612222222222222,
      "grad_norm": 0.014629613608121872,
      "learning_rate": 3.877777777777778e-06,
      "loss": 0.0026,
      "step": 83020
    },
    {
      "epoch": 4.612777777777778,
      "grad_norm": 0.12244515120983124,
      "learning_rate": 3.872222222222222e-06,
      "loss": 0.0023,
      "step": 83030
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.26834169030189514,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.003,
      "step": 83040
    },
    {
      "epoch": 4.613888888888889,
      "grad_norm": 0.3128270208835602,
      "learning_rate": 3.861111111111112e-06,
      "loss": 0.002,
      "step": 83050
    },
    {
      "epoch": 4.614444444444445,
      "grad_norm": 0.2368074208498001,
      "learning_rate": 3.855555555555556e-06,
      "loss": 0.0026,
      "step": 83060
    },
    {
      "epoch": 4.615,
      "grad_norm": 0.23703232407569885,
      "learning_rate": 3.85e-06,
      "loss": 0.0028,
      "step": 83070
    },
    {
      "epoch": 4.615555555555556,
      "grad_norm": 0.2961944341659546,
      "learning_rate": 3.844444444444445e-06,
      "loss": 0.002,
      "step": 83080
    },
    {
      "epoch": 4.6161111111111115,
      "grad_norm": 0.20891600847244263,
      "learning_rate": 3.838888888888889e-06,
      "loss": 0.0028,
      "step": 83090
    },
    {
      "epoch": 4.616666666666667,
      "grad_norm": 0.011078104376792908,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.004,
      "step": 83100
    },
    {
      "epoch": 4.617222222222222,
      "grad_norm": 0.630312979221344,
      "learning_rate": 3.827777777777778e-06,
      "loss": 0.0024,
      "step": 83110
    },
    {
      "epoch": 4.6177777777777775,
      "grad_norm": 0.06057026982307434,
      "learning_rate": 3.8222222222222224e-06,
      "loss": 0.0031,
      "step": 83120
    },
    {
      "epoch": 4.618333333333333,
      "grad_norm": 0.2974107563495636,
      "learning_rate": 3.816666666666667e-06,
      "loss": 0.0035,
      "step": 83130
    },
    {
      "epoch": 4.618888888888889,
      "grad_norm": 0.06207849830389023,
      "learning_rate": 3.8111111111111112e-06,
      "loss": 0.0026,
      "step": 83140
    },
    {
      "epoch": 4.6194444444444445,
      "grad_norm": 0.01111202035099268,
      "learning_rate": 3.805555555555556e-06,
      "loss": 0.0028,
      "step": 83150
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.1067446768283844,
      "learning_rate": 3.8e-06,
      "loss": 0.0032,
      "step": 83160
    },
    {
      "epoch": 4.620555555555556,
      "grad_norm": 0.17865362763404846,
      "learning_rate": 3.794444444444445e-06,
      "loss": 0.0017,
      "step": 83170
    },
    {
      "epoch": 4.621111111111111,
      "grad_norm": 0.12424909323453903,
      "learning_rate": 3.788888888888889e-06,
      "loss": 0.0033,
      "step": 83180
    },
    {
      "epoch": 4.621666666666667,
      "grad_norm": 0.011321705766022205,
      "learning_rate": 3.7833333333333333e-06,
      "loss": 0.0021,
      "step": 83190
    },
    {
      "epoch": 4.622222222222222,
      "grad_norm": 0.14869289100170135,
      "learning_rate": 3.777777777777778e-06,
      "loss": 0.0022,
      "step": 83200
    },
    {
      "epoch": 4.622777777777777,
      "grad_norm": 0.237864688038826,
      "learning_rate": 3.772222222222222e-06,
      "loss": 0.0023,
      "step": 83210
    },
    {
      "epoch": 4.623333333333333,
      "grad_norm": 0.14867515861988068,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0034,
      "step": 83220
    },
    {
      "epoch": 4.623888888888889,
      "grad_norm": 0.20747341215610504,
      "learning_rate": 3.7611111111111113e-06,
      "loss": 0.0026,
      "step": 83230
    },
    {
      "epoch": 4.624444444444444,
      "grad_norm": 0.16248242557048798,
      "learning_rate": 3.755555555555556e-06,
      "loss": 0.0036,
      "step": 83240
    },
    {
      "epoch": 4.625,
      "grad_norm": 0.17935939133167267,
      "learning_rate": 3.75e-06,
      "loss": 0.0034,
      "step": 83250
    },
    {
      "epoch": 4.625555555555556,
      "grad_norm": 0.1448623687028885,
      "learning_rate": 3.744444444444445e-06,
      "loss": 0.0021,
      "step": 83260
    },
    {
      "epoch": 4.626111111111111,
      "grad_norm": 0.09079456329345703,
      "learning_rate": 3.738888888888889e-06,
      "loss": 0.0029,
      "step": 83270
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.06063565984368324,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0017,
      "step": 83280
    },
    {
      "epoch": 4.627222222222223,
      "grad_norm": 0.5368160009384155,
      "learning_rate": 3.727777777777778e-06,
      "loss": 0.0025,
      "step": 83290
    },
    {
      "epoch": 4.627777777777778,
      "grad_norm": 0.06063643470406532,
      "learning_rate": 3.722222222222222e-06,
      "loss": 0.0029,
      "step": 83300
    },
    {
      "epoch": 4.628333333333333,
      "grad_norm": 0.15044263005256653,
      "learning_rate": 3.716666666666667e-06,
      "loss": 0.0022,
      "step": 83310
    },
    {
      "epoch": 4.628888888888889,
      "grad_norm": 0.20829002559185028,
      "learning_rate": 3.711111111111111e-06,
      "loss": 0.0023,
      "step": 83320
    },
    {
      "epoch": 4.629444444444444,
      "grad_norm": 0.11943932622671127,
      "learning_rate": 3.7055555555555557e-06,
      "loss": 0.0033,
      "step": 83330
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.2052609920501709,
      "learning_rate": 3.7e-06,
      "loss": 0.0034,
      "step": 83340
    },
    {
      "epoch": 4.6305555555555555,
      "grad_norm": 0.18005870282649994,
      "learning_rate": 3.694444444444445e-06,
      "loss": 0.0035,
      "step": 83350
    },
    {
      "epoch": 4.631111111111111,
      "grad_norm": 0.20861083269119263,
      "learning_rate": 3.688888888888889e-06,
      "loss": 0.0025,
      "step": 83360
    },
    {
      "epoch": 4.631666666666667,
      "grad_norm": 0.17905040085315704,
      "learning_rate": 3.6833333333333338e-06,
      "loss": 0.0022,
      "step": 83370
    },
    {
      "epoch": 4.6322222222222225,
      "grad_norm": 0.47467929124832153,
      "learning_rate": 3.6777777777777778e-06,
      "loss": 0.003,
      "step": 83380
    },
    {
      "epoch": 4.632777777777778,
      "grad_norm": 0.14166779816150665,
      "learning_rate": 3.6722222222222226e-06,
      "loss": 0.0027,
      "step": 83390
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 0.09012421220541,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0025,
      "step": 83400
    },
    {
      "epoch": 4.6338888888888885,
      "grad_norm": 0.06589475274085999,
      "learning_rate": 3.661111111111111e-06,
      "loss": 0.0019,
      "step": 83410
    },
    {
      "epoch": 4.634444444444444,
      "grad_norm": 0.20714794099330902,
      "learning_rate": 3.655555555555556e-06,
      "loss": 0.0022,
      "step": 83420
    },
    {
      "epoch": 4.635,
      "grad_norm": 0.17770254611968994,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 0.0021,
      "step": 83430
    },
    {
      "epoch": 4.635555555555555,
      "grad_norm": 0.012623876333236694,
      "learning_rate": 3.6444444444444446e-06,
      "loss": 0.0016,
      "step": 83440
    },
    {
      "epoch": 4.636111111111111,
      "grad_norm": 0.12014871090650558,
      "learning_rate": 3.638888888888889e-06,
      "loss": 0.0025,
      "step": 83450
    },
    {
      "epoch": 4.636666666666667,
      "grad_norm": 0.03247439116239548,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.0024,
      "step": 83460
    },
    {
      "epoch": 4.637222222222222,
      "grad_norm": 0.1595742255449295,
      "learning_rate": 3.627777777777778e-06,
      "loss": 0.0033,
      "step": 83470
    },
    {
      "epoch": 4.637777777777778,
      "grad_norm": 0.14871041476726532,
      "learning_rate": 3.6222222222222226e-06,
      "loss": 0.0023,
      "step": 83480
    },
    {
      "epoch": 4.638333333333334,
      "grad_norm": 0.060067519545555115,
      "learning_rate": 3.6166666666666666e-06,
      "loss": 0.0024,
      "step": 83490
    },
    {
      "epoch": 4.638888888888889,
      "grad_norm": 0.32039886713027954,
      "learning_rate": 3.611111111111111e-06,
      "loss": 0.0031,
      "step": 83500
    },
    {
      "epoch": 4.639444444444445,
      "grad_norm": 0.4042128920555115,
      "learning_rate": 3.605555555555556e-06,
      "loss": 0.0032,
      "step": 83510
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.14003731310367584,
      "learning_rate": 3.6e-06,
      "loss": 0.0028,
      "step": 83520
    },
    {
      "epoch": 4.640555555555555,
      "grad_norm": 0.2679429054260254,
      "learning_rate": 3.5944444444444447e-06,
      "loss": 0.0021,
      "step": 83530
    },
    {
      "epoch": 4.641111111111111,
      "grad_norm": 0.059443384408950806,
      "learning_rate": 3.588888888888889e-06,
      "loss": 0.0021,
      "step": 83540
    },
    {
      "epoch": 4.641666666666667,
      "grad_norm": 0.47413116693496704,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.0018,
      "step": 83550
    },
    {
      "epoch": 4.642222222222222,
      "grad_norm": 0.03180520609021187,
      "learning_rate": 3.577777777777778e-06,
      "loss": 0.0018,
      "step": 83560
    },
    {
      "epoch": 4.642777777777778,
      "grad_norm": 0.21321511268615723,
      "learning_rate": 3.5722222222222227e-06,
      "loss": 0.0029,
      "step": 83570
    },
    {
      "epoch": 4.6433333333333335,
      "grad_norm": 0.16220031678676605,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.0024,
      "step": 83580
    },
    {
      "epoch": 4.643888888888889,
      "grad_norm": 0.08992218971252441,
      "learning_rate": 3.5611111111111115e-06,
      "loss": 0.0027,
      "step": 83590
    },
    {
      "epoch": 4.644444444444445,
      "grad_norm": 0.2070329189300537,
      "learning_rate": 3.555555555555556e-06,
      "loss": 0.0029,
      "step": 83600
    },
    {
      "epoch": 4.645,
      "grad_norm": 0.38325417041778564,
      "learning_rate": 3.55e-06,
      "loss": 0.0029,
      "step": 83610
    },
    {
      "epoch": 4.645555555555555,
      "grad_norm": 0.2075766772031784,
      "learning_rate": 3.5444444444444447e-06,
      "loss": 0.0023,
      "step": 83620
    },
    {
      "epoch": 4.646111111111111,
      "grad_norm": 0.31617456674575806,
      "learning_rate": 3.5388888888888887e-06,
      "loss": 0.0033,
      "step": 83630
    },
    {
      "epoch": 4.6466666666666665,
      "grad_norm": 0.17842450737953186,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0027,
      "step": 83640
    },
    {
      "epoch": 4.647222222222222,
      "grad_norm": 0.18861596286296844,
      "learning_rate": 3.527777777777778e-06,
      "loss": 0.0023,
      "step": 83650
    },
    {
      "epoch": 4.647777777777778,
      "grad_norm": 0.06065914034843445,
      "learning_rate": 3.5222222222222228e-06,
      "loss": 0.0026,
      "step": 83660
    },
    {
      "epoch": 4.648333333333333,
      "grad_norm": 0.19448751211166382,
      "learning_rate": 3.5166666666666667e-06,
      "loss": 0.0014,
      "step": 83670
    },
    {
      "epoch": 4.648888888888889,
      "grad_norm": 0.07227937877178192,
      "learning_rate": 3.5111111111111116e-06,
      "loss": 0.0029,
      "step": 83680
    },
    {
      "epoch": 4.649444444444445,
      "grad_norm": 0.23805607855319977,
      "learning_rate": 3.5055555555555555e-06,
      "loss": 0.0023,
      "step": 83690
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.7077587246894836,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0042,
      "step": 83700
    },
    {
      "epoch": 4.650555555555556,
      "grad_norm": 0.17791958153247833,
      "learning_rate": 3.4944444444444448e-06,
      "loss": 0.0025,
      "step": 83710
    },
    {
      "epoch": 4.651111111111111,
      "grad_norm": 0.17178887128829956,
      "learning_rate": 3.4888888888888888e-06,
      "loss": 0.0032,
      "step": 83720
    },
    {
      "epoch": 4.651666666666666,
      "grad_norm": 0.27080902457237244,
      "learning_rate": 3.4833333333333336e-06,
      "loss": 0.0023,
      "step": 83730
    },
    {
      "epoch": 4.652222222222222,
      "grad_norm": 0.03954644873738289,
      "learning_rate": 3.4777777777777776e-06,
      "loss": 0.0032,
      "step": 83740
    },
    {
      "epoch": 4.652777777777778,
      "grad_norm": 0.11973615735769272,
      "learning_rate": 3.4722222222222224e-06,
      "loss": 0.0019,
      "step": 83750
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.13063809275627136,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0025,
      "step": 83760
    },
    {
      "epoch": 4.653888888888889,
      "grad_norm": 1.0351452827453613,
      "learning_rate": 3.4611111111111116e-06,
      "loss": 0.0023,
      "step": 83770
    },
    {
      "epoch": 4.654444444444445,
      "grad_norm": 0.283529669046402,
      "learning_rate": 3.4555555555555556e-06,
      "loss": 0.0025,
      "step": 83780
    },
    {
      "epoch": 4.655,
      "grad_norm": 0.17934753000736237,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 0.0031,
      "step": 83790
    },
    {
      "epoch": 4.655555555555556,
      "grad_norm": 0.10482991486787796,
      "learning_rate": 3.4444444444444444e-06,
      "loss": 0.0022,
      "step": 83800
    },
    {
      "epoch": 4.656111111111111,
      "grad_norm": 0.2388516217470169,
      "learning_rate": 3.438888888888889e-06,
      "loss": 0.0018,
      "step": 83810
    },
    {
      "epoch": 4.656666666666666,
      "grad_norm": 0.0895213931798935,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0031,
      "step": 83820
    },
    {
      "epoch": 4.657222222222222,
      "grad_norm": 0.32651442289352417,
      "learning_rate": 3.4277777777777776e-06,
      "loss": 0.0023,
      "step": 83830
    },
    {
      "epoch": 4.657777777777778,
      "grad_norm": 0.12089430540800095,
      "learning_rate": 3.4222222222222224e-06,
      "loss": 0.0032,
      "step": 83840
    },
    {
      "epoch": 4.658333333333333,
      "grad_norm": 0.1486303061246872,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.003,
      "step": 83850
    },
    {
      "epoch": 4.658888888888889,
      "grad_norm": 0.08968613296747208,
      "learning_rate": 3.4111111111111113e-06,
      "loss": 0.003,
      "step": 83860
    },
    {
      "epoch": 4.6594444444444445,
      "grad_norm": 0.23732109367847443,
      "learning_rate": 3.4055555555555557e-06,
      "loss": 0.002,
      "step": 83870
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.4479750692844391,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0029,
      "step": 83880
    },
    {
      "epoch": 4.660555555555556,
      "grad_norm": 0.06071792170405388,
      "learning_rate": 3.3944444444444445e-06,
      "loss": 0.0024,
      "step": 83890
    },
    {
      "epoch": 4.661111111111111,
      "grad_norm": 0.12139283120632172,
      "learning_rate": 3.3888888888888893e-06,
      "loss": 0.0024,
      "step": 83900
    },
    {
      "epoch": 4.661666666666667,
      "grad_norm": 0.1778922826051712,
      "learning_rate": 3.3833333333333337e-06,
      "loss": 0.0016,
      "step": 83910
    },
    {
      "epoch": 4.662222222222223,
      "grad_norm": 0.15005506575107574,
      "learning_rate": 3.3777777777777777e-06,
      "loss": 0.0018,
      "step": 83920
    },
    {
      "epoch": 4.6627777777777775,
      "grad_norm": 0.21008847653865814,
      "learning_rate": 3.3722222222222225e-06,
      "loss": 0.0031,
      "step": 83930
    },
    {
      "epoch": 4.663333333333333,
      "grad_norm": 0.20791247487068176,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.0032,
      "step": 83940
    },
    {
      "epoch": 4.663888888888889,
      "grad_norm": 0.06193845346570015,
      "learning_rate": 3.3611111111111113e-06,
      "loss": 0.0038,
      "step": 83950
    },
    {
      "epoch": 4.664444444444444,
      "grad_norm": 0.14801624417304993,
      "learning_rate": 3.3555555555555557e-06,
      "loss": 0.0023,
      "step": 83960
    },
    {
      "epoch": 4.665,
      "grad_norm": 0.5434492826461792,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 0.0023,
      "step": 83970
    },
    {
      "epoch": 4.665555555555556,
      "grad_norm": 0.3451322615146637,
      "learning_rate": 3.3444444444444445e-06,
      "loss": 0.0024,
      "step": 83980
    },
    {
      "epoch": 4.666111111111111,
      "grad_norm": 0.14938022196292877,
      "learning_rate": 3.3388888888888893e-06,
      "loss": 0.0017,
      "step": 83990
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.3553865849971771,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0021,
      "step": 84000
    },
    {
      "epoch": 4.667222222222223,
      "grad_norm": 0.09098681062459946,
      "learning_rate": 3.327777777777778e-06,
      "loss": 0.0025,
      "step": 84010
    },
    {
      "epoch": 4.667777777777777,
      "grad_norm": 0.06144951283931732,
      "learning_rate": 3.3222222222222226e-06,
      "loss": 0.0023,
      "step": 84020
    },
    {
      "epoch": 4.668333333333333,
      "grad_norm": 0.11890597641468048,
      "learning_rate": 3.3166666666666665e-06,
      "loss": 0.0028,
      "step": 84030
    },
    {
      "epoch": 4.668888888888889,
      "grad_norm": 0.11998758465051651,
      "learning_rate": 3.3111111111111114e-06,
      "loss": 0.0028,
      "step": 84040
    },
    {
      "epoch": 4.669444444444444,
      "grad_norm": 0.4335431158542633,
      "learning_rate": 3.3055555555555553e-06,
      "loss": 0.0032,
      "step": 84050
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.09041280299425125,
      "learning_rate": 3.3e-06,
      "loss": 0.0029,
      "step": 84060
    },
    {
      "epoch": 4.670555555555556,
      "grad_norm": 0.17830535769462585,
      "learning_rate": 3.2944444444444446e-06,
      "loss": 0.0028,
      "step": 84070
    },
    {
      "epoch": 4.671111111111111,
      "grad_norm": 0.1794537752866745,
      "learning_rate": 3.2888888888888894e-06,
      "loss": 0.0028,
      "step": 84080
    },
    {
      "epoch": 4.671666666666667,
      "grad_norm": 0.2589140236377716,
      "learning_rate": 3.2833333333333334e-06,
      "loss": 0.0033,
      "step": 84090
    },
    {
      "epoch": 4.6722222222222225,
      "grad_norm": 0.3568926155567169,
      "learning_rate": 3.277777777777778e-06,
      "loss": 0.0018,
      "step": 84100
    },
    {
      "epoch": 4.672777777777778,
      "grad_norm": 0.060255274176597595,
      "learning_rate": 3.272222222222222e-06,
      "loss": 0.003,
      "step": 84110
    },
    {
      "epoch": 4.673333333333334,
      "grad_norm": 0.010797176510095596,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0025,
      "step": 84120
    },
    {
      "epoch": 4.6738888888888885,
      "grad_norm": 0.20756196975708008,
      "learning_rate": 3.2611111111111114e-06,
      "loss": 0.0028,
      "step": 84130
    },
    {
      "epoch": 4.674444444444444,
      "grad_norm": 0.1786738783121109,
      "learning_rate": 3.2555555555555554e-06,
      "loss": 0.0027,
      "step": 84140
    },
    {
      "epoch": 4.675,
      "grad_norm": 0.09235142171382904,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.0022,
      "step": 84150
    },
    {
      "epoch": 4.6755555555555555,
      "grad_norm": 0.08180456608533859,
      "learning_rate": 3.244444444444444e-06,
      "loss": 0.0026,
      "step": 84160
    },
    {
      "epoch": 4.676111111111111,
      "grad_norm": 0.08921711891889572,
      "learning_rate": 3.238888888888889e-06,
      "loss": 0.003,
      "step": 84170
    },
    {
      "epoch": 4.676666666666667,
      "grad_norm": 0.01570175401866436,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0025,
      "step": 84180
    },
    {
      "epoch": 4.677222222222222,
      "grad_norm": 0.28026899695396423,
      "learning_rate": 3.2277777777777783e-06,
      "loss": 0.0033,
      "step": 84190
    },
    {
      "epoch": 4.677777777777778,
      "grad_norm": 0.11928089708089828,
      "learning_rate": 3.2222222222222222e-06,
      "loss": 0.0019,
      "step": 84200
    },
    {
      "epoch": 4.678333333333334,
      "grad_norm": 0.08953358232975006,
      "learning_rate": 3.216666666666667e-06,
      "loss": 0.0022,
      "step": 84210
    },
    {
      "epoch": 4.678888888888888,
      "grad_norm": 0.8954159021377563,
      "learning_rate": 3.2111111111111115e-06,
      "loss": 0.0027,
      "step": 84220
    },
    {
      "epoch": 4.679444444444444,
      "grad_norm": 0.47471505403518677,
      "learning_rate": 3.2055555555555555e-06,
      "loss": 0.0024,
      "step": 84230
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.23947103321552277,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0033,
      "step": 84240
    },
    {
      "epoch": 4.680555555555555,
      "grad_norm": 0.0914609357714653,
      "learning_rate": 3.1944444444444443e-06,
      "loss": 0.0022,
      "step": 84250
    },
    {
      "epoch": 4.681111111111111,
      "grad_norm": 0.7841545939445496,
      "learning_rate": 3.188888888888889e-06,
      "loss": 0.0022,
      "step": 84260
    },
    {
      "epoch": 4.681666666666667,
      "grad_norm": 0.11957994103431702,
      "learning_rate": 3.1833333333333335e-06,
      "loss": 0.0016,
      "step": 84270
    },
    {
      "epoch": 4.682222222222222,
      "grad_norm": 0.3575379550457001,
      "learning_rate": 3.1777777777777783e-06,
      "loss": 0.0026,
      "step": 84280
    },
    {
      "epoch": 4.682777777777778,
      "grad_norm": 0.6762866973876953,
      "learning_rate": 3.1722222222222223e-06,
      "loss": 0.0024,
      "step": 84290
    },
    {
      "epoch": 4.683333333333334,
      "grad_norm": 0.14830856025218964,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0025,
      "step": 84300
    },
    {
      "epoch": 4.683888888888889,
      "grad_norm": 0.14942732453346252,
      "learning_rate": 3.161111111111111e-06,
      "loss": 0.0022,
      "step": 84310
    },
    {
      "epoch": 4.684444444444445,
      "grad_norm": 0.11853421479463577,
      "learning_rate": 3.155555555555556e-06,
      "loss": 0.0026,
      "step": 84320
    },
    {
      "epoch": 4.6850000000000005,
      "grad_norm": 0.3162623941898346,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 0.0031,
      "step": 84330
    },
    {
      "epoch": 4.685555555555555,
      "grad_norm": 0.5268027186393738,
      "learning_rate": 3.1444444444444443e-06,
      "loss": 0.0026,
      "step": 84340
    },
    {
      "epoch": 4.686111111111111,
      "grad_norm": 0.46013087034225464,
      "learning_rate": 3.138888888888889e-06,
      "loss": 0.0036,
      "step": 84350
    },
    {
      "epoch": 4.6866666666666665,
      "grad_norm": 0.23708242177963257,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0024,
      "step": 84360
    },
    {
      "epoch": 4.687222222222222,
      "grad_norm": 0.11956249177455902,
      "learning_rate": 3.127777777777778e-06,
      "loss": 0.003,
      "step": 84370
    },
    {
      "epoch": 4.687777777777778,
      "grad_norm": 0.032404642552137375,
      "learning_rate": 3.1222222222222224e-06,
      "loss": 0.0021,
      "step": 84380
    },
    {
      "epoch": 4.6883333333333335,
      "grad_norm": 0.08959071338176727,
      "learning_rate": 3.1166666666666668e-06,
      "loss": 0.0024,
      "step": 84390
    },
    {
      "epoch": 4.688888888888889,
      "grad_norm": 0.06221276894211769,
      "learning_rate": 3.111111111111111e-06,
      "loss": 0.0024,
      "step": 84400
    },
    {
      "epoch": 4.689444444444445,
      "grad_norm": 0.060544710606336594,
      "learning_rate": 3.1055555555555556e-06,
      "loss": 0.0014,
      "step": 84410
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 0.060417789965867996,
      "learning_rate": 3.1e-06,
      "loss": 0.0017,
      "step": 84420
    },
    {
      "epoch": 4.690555555555555,
      "grad_norm": 0.08978379517793655,
      "learning_rate": 3.094444444444445e-06,
      "loss": 0.0019,
      "step": 84430
    },
    {
      "epoch": 4.691111111111111,
      "grad_norm": 0.23742066323757172,
      "learning_rate": 3.088888888888889e-06,
      "loss": 0.0021,
      "step": 84440
    },
    {
      "epoch": 4.691666666666666,
      "grad_norm": 0.23840434849262238,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.0032,
      "step": 84450
    },
    {
      "epoch": 4.692222222222222,
      "grad_norm": 0.17918555438518524,
      "learning_rate": 3.077777777777778e-06,
      "loss": 0.0026,
      "step": 84460
    },
    {
      "epoch": 4.692777777777778,
      "grad_norm": 0.4434854984283447,
      "learning_rate": 3.0722222222222224e-06,
      "loss": 0.0026,
      "step": 84470
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.1333981603384018,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0032,
      "step": 84480
    },
    {
      "epoch": 4.693888888888889,
      "grad_norm": 0.17766480147838593,
      "learning_rate": 3.0611111111111112e-06,
      "loss": 0.0029,
      "step": 84490
    },
    {
      "epoch": 4.694444444444445,
      "grad_norm": 0.08978801220655441,
      "learning_rate": 3.0555555555555556e-06,
      "loss": 0.0029,
      "step": 84500
    },
    {
      "epoch": 4.695,
      "grad_norm": 0.3264264464378357,
      "learning_rate": 3.05e-06,
      "loss": 0.0027,
      "step": 84510
    },
    {
      "epoch": 4.695555555555556,
      "grad_norm": 0.12009266763925552,
      "learning_rate": 3.0444444444444444e-06,
      "loss": 0.0018,
      "step": 84520
    },
    {
      "epoch": 4.696111111111112,
      "grad_norm": 0.060754597187042236,
      "learning_rate": 3.038888888888889e-06,
      "loss": 0.0023,
      "step": 84530
    },
    {
      "epoch": 4.696666666666666,
      "grad_norm": 0.10230249166488647,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.0035,
      "step": 84540
    },
    {
      "epoch": 4.697222222222222,
      "grad_norm": 0.575384795665741,
      "learning_rate": 3.027777777777778e-06,
      "loss": 0.0029,
      "step": 84550
    },
    {
      "epoch": 4.697777777777778,
      "grad_norm": 0.09660639613866806,
      "learning_rate": 3.0222222222222225e-06,
      "loss": 0.0019,
      "step": 84560
    },
    {
      "epoch": 4.698333333333333,
      "grad_norm": 0.13332027196884155,
      "learning_rate": 3.016666666666667e-06,
      "loss": 0.0039,
      "step": 84570
    },
    {
      "epoch": 4.698888888888889,
      "grad_norm": 0.12588320672512054,
      "learning_rate": 3.0111111111111113e-06,
      "loss": 0.0033,
      "step": 84580
    },
    {
      "epoch": 4.6994444444444445,
      "grad_norm": 0.12026838213205338,
      "learning_rate": 3.0055555555555557e-06,
      "loss": 0.0019,
      "step": 84590
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.11936545372009277,
      "learning_rate": 3e-06,
      "loss": 0.0036,
      "step": 84600
    },
    {
      "epoch": 4.700555555555556,
      "grad_norm": 0.12947030365467072,
      "learning_rate": 2.9944444444444445e-06,
      "loss": 0.0025,
      "step": 84610
    },
    {
      "epoch": 4.7011111111111115,
      "grad_norm": 0.11987373977899551,
      "learning_rate": 2.988888888888889e-06,
      "loss": 0.0029,
      "step": 84620
    },
    {
      "epoch": 4.701666666666666,
      "grad_norm": 0.3267030119895935,
      "learning_rate": 2.9833333333333333e-06,
      "loss": 0.0026,
      "step": 84630
    },
    {
      "epoch": 4.702222222222222,
      "grad_norm": 0.15020127594470978,
      "learning_rate": 2.977777777777778e-06,
      "loss": 0.0026,
      "step": 84640
    },
    {
      "epoch": 4.7027777777777775,
      "grad_norm": 0.09117517620325089,
      "learning_rate": 2.9722222222222225e-06,
      "loss": 0.0033,
      "step": 84650
    },
    {
      "epoch": 4.703333333333333,
      "grad_norm": 0.03213345631957054,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0024,
      "step": 84660
    },
    {
      "epoch": 4.703888888888889,
      "grad_norm": 0.03343159332871437,
      "learning_rate": 2.9611111111111113e-06,
      "loss": 0.003,
      "step": 84670
    },
    {
      "epoch": 4.704444444444444,
      "grad_norm": 0.14945726096630096,
      "learning_rate": 2.9555555555555557e-06,
      "loss": 0.0038,
      "step": 84680
    },
    {
      "epoch": 4.705,
      "grad_norm": 0.23820389807224274,
      "learning_rate": 2.95e-06,
      "loss": 0.0025,
      "step": 84690
    },
    {
      "epoch": 4.705555555555556,
      "grad_norm": 0.4898907542228699,
      "learning_rate": 2.9444444444444445e-06,
      "loss": 0.0019,
      "step": 84700
    },
    {
      "epoch": 4.706111111111111,
      "grad_norm": 0.1781437247991562,
      "learning_rate": 2.938888888888889e-06,
      "loss": 0.0028,
      "step": 84710
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.23740629851818085,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0017,
      "step": 84720
    },
    {
      "epoch": 4.707222222222223,
      "grad_norm": 0.08925983309745789,
      "learning_rate": 2.9277777777777777e-06,
      "loss": 0.0022,
      "step": 84730
    },
    {
      "epoch": 4.707777777777777,
      "grad_norm": 0.0899496003985405,
      "learning_rate": 2.9222222222222226e-06,
      "loss": 0.0015,
      "step": 84740
    },
    {
      "epoch": 4.708333333333333,
      "grad_norm": 0.17827878892421722,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0028,
      "step": 84750
    },
    {
      "epoch": 4.708888888888889,
      "grad_norm": 0.2960275411605835,
      "learning_rate": 2.9111111111111114e-06,
      "loss": 0.0028,
      "step": 84760
    },
    {
      "epoch": 4.709444444444444,
      "grad_norm": 0.3045492470264435,
      "learning_rate": 2.9055555555555558e-06,
      "loss": 0.0035,
      "step": 84770
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.2599593698978424,
      "learning_rate": 2.9e-06,
      "loss": 0.0036,
      "step": 84780
    },
    {
      "epoch": 4.710555555555556,
      "grad_norm": 0.5470871925354004,
      "learning_rate": 2.8944444444444446e-06,
      "loss": 0.0025,
      "step": 84790
    },
    {
      "epoch": 4.711111111111111,
      "grad_norm": 0.12483533471822739,
      "learning_rate": 2.888888888888889e-06,
      "loss": 0.0021,
      "step": 84800
    },
    {
      "epoch": 4.711666666666667,
      "grad_norm": 0.17828357219696045,
      "learning_rate": 2.8833333333333334e-06,
      "loss": 0.0019,
      "step": 84810
    },
    {
      "epoch": 4.7122222222222225,
      "grad_norm": 0.058080922812223434,
      "learning_rate": 2.877777777777778e-06,
      "loss": 0.0018,
      "step": 84820
    },
    {
      "epoch": 4.712777777777777,
      "grad_norm": 0.03411882743239403,
      "learning_rate": 2.872222222222222e-06,
      "loss": 0.0023,
      "step": 84830
    },
    {
      "epoch": 4.713333333333333,
      "grad_norm": 0.027920911088585854,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0028,
      "step": 84840
    },
    {
      "epoch": 4.713888888888889,
      "grad_norm": 0.062285542488098145,
      "learning_rate": 2.8611111111111114e-06,
      "loss": 0.0037,
      "step": 84850
    },
    {
      "epoch": 4.714444444444444,
      "grad_norm": 0.008745810948312283,
      "learning_rate": 2.855555555555556e-06,
      "loss": 0.0016,
      "step": 84860
    },
    {
      "epoch": 4.715,
      "grad_norm": 0.44504034519195557,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 0.0026,
      "step": 84870
    },
    {
      "epoch": 4.7155555555555555,
      "grad_norm": 0.3935467600822449,
      "learning_rate": 2.8444444444444446e-06,
      "loss": 0.003,
      "step": 84880
    },
    {
      "epoch": 4.716111111111111,
      "grad_norm": 0.30556634068489075,
      "learning_rate": 2.838888888888889e-06,
      "loss": 0.0016,
      "step": 84890
    },
    {
      "epoch": 4.716666666666667,
      "grad_norm": 0.11911219358444214,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.0026,
      "step": 84900
    },
    {
      "epoch": 4.717222222222222,
      "grad_norm": 0.20822860300540924,
      "learning_rate": 2.827777777777778e-06,
      "loss": 0.0023,
      "step": 84910
    },
    {
      "epoch": 4.717777777777778,
      "grad_norm": 0.11945810168981552,
      "learning_rate": 2.8222222222222223e-06,
      "loss": 0.0037,
      "step": 84920
    },
    {
      "epoch": 4.718333333333334,
      "grad_norm": 0.06098076328635216,
      "learning_rate": 2.8166666666666667e-06,
      "loss": 0.0023,
      "step": 84930
    },
    {
      "epoch": 4.718888888888889,
      "grad_norm": 0.26801660656929016,
      "learning_rate": 2.811111111111111e-06,
      "loss": 0.0031,
      "step": 84940
    },
    {
      "epoch": 4.719444444444444,
      "grad_norm": 0.33859002590179443,
      "learning_rate": 2.805555555555556e-06,
      "loss": 0.0019,
      "step": 84950
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.20880158245563507,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0027,
      "step": 84960
    },
    {
      "epoch": 4.720555555555555,
      "grad_norm": 0.14906461536884308,
      "learning_rate": 2.7944444444444447e-06,
      "loss": 0.0023,
      "step": 84970
    },
    {
      "epoch": 4.721111111111111,
      "grad_norm": 0.22240109741687775,
      "learning_rate": 2.788888888888889e-06,
      "loss": 0.0041,
      "step": 84980
    },
    {
      "epoch": 4.721666666666667,
      "grad_norm": 0.061482984572649,
      "learning_rate": 2.7833333333333335e-06,
      "loss": 0.0014,
      "step": 84990
    },
    {
      "epoch": 4.722222222222222,
      "grad_norm": 0.14747372269630432,
      "learning_rate": 2.777777777777778e-06,
      "loss": 0.003,
      "step": 85000
    },
    {
      "epoch": 4.722777777777778,
      "grad_norm": 0.48832395672798157,
      "learning_rate": 2.7722222222222223e-06,
      "loss": 0.0044,
      "step": 85010
    },
    {
      "epoch": 4.723333333333334,
      "grad_norm": 0.20841215550899506,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.0025,
      "step": 85020
    },
    {
      "epoch": 4.723888888888889,
      "grad_norm": 0.14723075926303864,
      "learning_rate": 2.761111111111111e-06,
      "loss": 0.0019,
      "step": 85030
    },
    {
      "epoch": 4.724444444444444,
      "grad_norm": 0.32381951808929443,
      "learning_rate": 2.7555555555555555e-06,
      "loss": 0.0019,
      "step": 85040
    },
    {
      "epoch": 4.725,
      "grad_norm": 0.17855535447597504,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.0032,
      "step": 85050
    },
    {
      "epoch": 4.725555555555555,
      "grad_norm": 0.0318511500954628,
      "learning_rate": 2.7444444444444448e-06,
      "loss": 0.0022,
      "step": 85060
    },
    {
      "epoch": 4.726111111111111,
      "grad_norm": 0.12056437879800797,
      "learning_rate": 2.738888888888889e-06,
      "loss": 0.0021,
      "step": 85070
    },
    {
      "epoch": 4.726666666666667,
      "grad_norm": 0.21208402514457703,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0018,
      "step": 85080
    },
    {
      "epoch": 4.727222222222222,
      "grad_norm": 0.09768068045377731,
      "learning_rate": 2.727777777777778e-06,
      "loss": 0.0024,
      "step": 85090
    },
    {
      "epoch": 4.727777777777778,
      "grad_norm": 0.45437195897102356,
      "learning_rate": 2.7222222222222224e-06,
      "loss": 0.0024,
      "step": 85100
    },
    {
      "epoch": 4.7283333333333335,
      "grad_norm": 0.064926378428936,
      "learning_rate": 2.7166666666666668e-06,
      "loss": 0.0028,
      "step": 85110
    },
    {
      "epoch": 4.728888888888889,
      "grad_norm": 0.1418716162443161,
      "learning_rate": 2.711111111111111e-06,
      "loss": 0.0021,
      "step": 85120
    },
    {
      "epoch": 4.729444444444445,
      "grad_norm": 0.060606326907873154,
      "learning_rate": 2.7055555555555556e-06,
      "loss": 0.0037,
      "step": 85130
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.14910413324832916,
      "learning_rate": 2.7e-06,
      "loss": 0.003,
      "step": 85140
    },
    {
      "epoch": 4.730555555555555,
      "grad_norm": 0.06197961047291756,
      "learning_rate": 2.6944444444444444e-06,
      "loss": 0.0027,
      "step": 85150
    },
    {
      "epoch": 4.731111111111111,
      "grad_norm": 0.541292667388916,
      "learning_rate": 2.6888888888888892e-06,
      "loss": 0.0019,
      "step": 85160
    },
    {
      "epoch": 4.7316666666666665,
      "grad_norm": 0.39026954770088196,
      "learning_rate": 2.6833333333333336e-06,
      "loss": 0.0031,
      "step": 85170
    },
    {
      "epoch": 4.732222222222222,
      "grad_norm": 0.17786981165409088,
      "learning_rate": 2.677777777777778e-06,
      "loss": 0.002,
      "step": 85180
    },
    {
      "epoch": 4.732777777777778,
      "grad_norm": 0.41522231698036194,
      "learning_rate": 2.6722222222222224e-06,
      "loss": 0.0032,
      "step": 85190
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.032720185816287994,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0024,
      "step": 85200
    },
    {
      "epoch": 4.733888888888889,
      "grad_norm": 0.013379623182117939,
      "learning_rate": 2.6611111111111112e-06,
      "loss": 0.0026,
      "step": 85210
    },
    {
      "epoch": 4.734444444444445,
      "grad_norm": 0.26009970903396606,
      "learning_rate": 2.6555555555555556e-06,
      "loss": 0.002,
      "step": 85220
    },
    {
      "epoch": 4.735,
      "grad_norm": 0.01939546689391136,
      "learning_rate": 2.65e-06,
      "loss": 0.003,
      "step": 85230
    },
    {
      "epoch": 4.735555555555555,
      "grad_norm": 0.20758235454559326,
      "learning_rate": 2.6444444444444444e-06,
      "loss": 0.0028,
      "step": 85240
    },
    {
      "epoch": 4.736111111111111,
      "grad_norm": 0.1483076810836792,
      "learning_rate": 2.638888888888889e-06,
      "loss": 0.0029,
      "step": 85250
    },
    {
      "epoch": 4.736666666666666,
      "grad_norm": 0.28967562317848206,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.0019,
      "step": 85260
    },
    {
      "epoch": 4.737222222222222,
      "grad_norm": 0.13495652377605438,
      "learning_rate": 2.627777777777778e-06,
      "loss": 0.0021,
      "step": 85270
    },
    {
      "epoch": 4.737777777777778,
      "grad_norm": 0.14944131672382355,
      "learning_rate": 2.6222222222222225e-06,
      "loss": 0.0019,
      "step": 85280
    },
    {
      "epoch": 4.738333333333333,
      "grad_norm": 0.5211207866668701,
      "learning_rate": 2.616666666666667e-06,
      "loss": 0.0031,
      "step": 85290
    },
    {
      "epoch": 4.738888888888889,
      "grad_norm": 0.2672690749168396,
      "learning_rate": 2.6111111111111113e-06,
      "loss": 0.0039,
      "step": 85300
    },
    {
      "epoch": 4.739444444444445,
      "grad_norm": 0.037254877388477325,
      "learning_rate": 2.6055555555555557e-06,
      "loss": 0.0024,
      "step": 85310
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.20867350697517395,
      "learning_rate": 2.6e-06,
      "loss": 0.0021,
      "step": 85320
    },
    {
      "epoch": 4.740555555555556,
      "grad_norm": 0.2685668468475342,
      "learning_rate": 2.5944444444444445e-06,
      "loss": 0.0026,
      "step": 85330
    },
    {
      "epoch": 4.7411111111111115,
      "grad_norm": 0.06027597934007645,
      "learning_rate": 2.588888888888889e-06,
      "loss": 0.0021,
      "step": 85340
    },
    {
      "epoch": 4.741666666666667,
      "grad_norm": 0.17847782373428345,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.0026,
      "step": 85350
    },
    {
      "epoch": 4.742222222222222,
      "grad_norm": 0.06025264039635658,
      "learning_rate": 2.5777777777777777e-06,
      "loss": 0.0023,
      "step": 85360
    },
    {
      "epoch": 4.7427777777777775,
      "grad_norm": 0.5005783438682556,
      "learning_rate": 2.5722222222222225e-06,
      "loss": 0.0021,
      "step": 85370
    },
    {
      "epoch": 4.743333333333333,
      "grad_norm": 0.04765629023313522,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.0025,
      "step": 85380
    },
    {
      "epoch": 4.743888888888889,
      "grad_norm": 0.03928273916244507,
      "learning_rate": 2.5611111111111113e-06,
      "loss": 0.0019,
      "step": 85390
    },
    {
      "epoch": 4.7444444444444445,
      "grad_norm": 0.06042240560054779,
      "learning_rate": 2.5555555555555557e-06,
      "loss": 0.0029,
      "step": 85400
    },
    {
      "epoch": 4.745,
      "grad_norm": 0.20741139352321625,
      "learning_rate": 2.55e-06,
      "loss": 0.0016,
      "step": 85410
    },
    {
      "epoch": 4.745555555555556,
      "grad_norm": 0.09028104692697525,
      "learning_rate": 2.5444444444444446e-06,
      "loss": 0.0038,
      "step": 85420
    },
    {
      "epoch": 4.746111111111111,
      "grad_norm": 0.12024440616369247,
      "learning_rate": 2.538888888888889e-06,
      "loss": 0.0042,
      "step": 85430
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.17794214189052582,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0028,
      "step": 85440
    },
    {
      "epoch": 4.747222222222222,
      "grad_norm": 0.3280473053455353,
      "learning_rate": 2.5277777777777778e-06,
      "loss": 0.0025,
      "step": 85450
    },
    {
      "epoch": 4.747777777777777,
      "grad_norm": 0.14905713498592377,
      "learning_rate": 2.522222222222222e-06,
      "loss": 0.0023,
      "step": 85460
    },
    {
      "epoch": 4.748333333333333,
      "grad_norm": 0.17796924710273743,
      "learning_rate": 2.516666666666667e-06,
      "loss": 0.0026,
      "step": 85470
    },
    {
      "epoch": 4.748888888888889,
      "grad_norm": 0.02240273542702198,
      "learning_rate": 2.5111111111111114e-06,
      "loss": 0.0025,
      "step": 85480
    },
    {
      "epoch": 4.749444444444444,
      "grad_norm": 0.17777884006500244,
      "learning_rate": 2.505555555555556e-06,
      "loss": 0.0025,
      "step": 85490
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.14947360754013062,
      "learning_rate": 2.5e-06,
      "loss": 0.0022,
      "step": 85500
    },
    {
      "epoch": 4.750555555555556,
      "grad_norm": 0.03188079968094826,
      "learning_rate": 2.4944444444444446e-06,
      "loss": 0.0025,
      "step": 85510
    },
    {
      "epoch": 4.751111111111111,
      "grad_norm": 0.2696252465248108,
      "learning_rate": 2.488888888888889e-06,
      "loss": 0.0027,
      "step": 85520
    },
    {
      "epoch": 4.751666666666667,
      "grad_norm": 0.11862247437238693,
      "learning_rate": 2.4833333333333334e-06,
      "loss": 0.0028,
      "step": 85530
    },
    {
      "epoch": 4.752222222222223,
      "grad_norm": 0.2072429060935974,
      "learning_rate": 2.477777777777778e-06,
      "loss": 0.0021,
      "step": 85540
    },
    {
      "epoch": 4.752777777777778,
      "grad_norm": 0.0981832891702652,
      "learning_rate": 2.4722222222222222e-06,
      "loss": 0.0019,
      "step": 85550
    },
    {
      "epoch": 4.753333333333333,
      "grad_norm": 0.11856269836425781,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0019,
      "step": 85560
    },
    {
      "epoch": 4.753888888888889,
      "grad_norm": 0.11903324723243713,
      "learning_rate": 2.4611111111111115e-06,
      "loss": 0.0024,
      "step": 85570
    },
    {
      "epoch": 4.754444444444444,
      "grad_norm": 0.14872640371322632,
      "learning_rate": 2.455555555555556e-06,
      "loss": 0.0021,
      "step": 85580
    },
    {
      "epoch": 4.755,
      "grad_norm": 0.2605375349521637,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 0.0029,
      "step": 85590
    },
    {
      "epoch": 4.7555555555555555,
      "grad_norm": 0.14865931868553162,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 0.0027,
      "step": 85600
    },
    {
      "epoch": 4.756111111111111,
      "grad_norm": 0.06305903196334839,
      "learning_rate": 2.438888888888889e-06,
      "loss": 0.0028,
      "step": 85610
    },
    {
      "epoch": 4.756666666666667,
      "grad_norm": 0.1849336475133896,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0032,
      "step": 85620
    },
    {
      "epoch": 4.7572222222222225,
      "grad_norm": 0.45894068479537964,
      "learning_rate": 2.427777777777778e-06,
      "loss": 0.003,
      "step": 85630
    },
    {
      "epoch": 4.757777777777778,
      "grad_norm": 0.08998789638280869,
      "learning_rate": 2.4222222222222223e-06,
      "loss": 0.0025,
      "step": 85640
    },
    {
      "epoch": 4.758333333333333,
      "grad_norm": 0.0603824257850647,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.002,
      "step": 85650
    },
    {
      "epoch": 4.7588888888888885,
      "grad_norm": 0.08970880508422852,
      "learning_rate": 2.411111111111111e-06,
      "loss": 0.0027,
      "step": 85660
    },
    {
      "epoch": 4.759444444444444,
      "grad_norm": 0.09158605337142944,
      "learning_rate": 2.4055555555555555e-06,
      "loss": 0.0037,
      "step": 85670
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.2085445374250412,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0028,
      "step": 85680
    },
    {
      "epoch": 4.760555555555555,
      "grad_norm": 0.1791606992483139,
      "learning_rate": 2.3944444444444447e-06,
      "loss": 0.002,
      "step": 85690
    },
    {
      "epoch": 4.761111111111111,
      "grad_norm": 0.7571902275085449,
      "learning_rate": 2.388888888888889e-06,
      "loss": 0.0019,
      "step": 85700
    },
    {
      "epoch": 4.761666666666667,
      "grad_norm": 0.012675825506448746,
      "learning_rate": 2.3833333333333335e-06,
      "loss": 0.0018,
      "step": 85710
    },
    {
      "epoch": 4.762222222222222,
      "grad_norm": 0.11906138807535172,
      "learning_rate": 2.377777777777778e-06,
      "loss": 0.0031,
      "step": 85720
    },
    {
      "epoch": 4.762777777777778,
      "grad_norm": 0.052092041820287704,
      "learning_rate": 2.3722222222222223e-06,
      "loss": 0.0027,
      "step": 85730
    },
    {
      "epoch": 4.763333333333334,
      "grad_norm": 0.35554108023643494,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.0025,
      "step": 85740
    },
    {
      "epoch": 4.763888888888889,
      "grad_norm": 0.040478937327861786,
      "learning_rate": 2.361111111111111e-06,
      "loss": 0.0026,
      "step": 85750
    },
    {
      "epoch": 4.764444444444445,
      "grad_norm": 0.09024225175380707,
      "learning_rate": 2.3555555555555555e-06,
      "loss": 0.0022,
      "step": 85760
    },
    {
      "epoch": 4.765,
      "grad_norm": 0.38519981503486633,
      "learning_rate": 2.35e-06,
      "loss": 0.0031,
      "step": 85770
    },
    {
      "epoch": 4.765555555555555,
      "grad_norm": 0.0624864362180233,
      "learning_rate": 2.3444444444444448e-06,
      "loss": 0.0019,
      "step": 85780
    },
    {
      "epoch": 4.766111111111111,
      "grad_norm": 0.1938517838716507,
      "learning_rate": 2.338888888888889e-06,
      "loss": 0.0028,
      "step": 85790
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 0.03209446743130684,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0032,
      "step": 85800
    },
    {
      "epoch": 4.767222222222222,
      "grad_norm": 0.2666080594062805,
      "learning_rate": 2.327777777777778e-06,
      "loss": 0.0021,
      "step": 85810
    },
    {
      "epoch": 4.767777777777778,
      "grad_norm": 0.11856380105018616,
      "learning_rate": 2.322222222222222e-06,
      "loss": 0.0041,
      "step": 85820
    },
    {
      "epoch": 4.7683333333333335,
      "grad_norm": 0.1514335572719574,
      "learning_rate": 2.316666666666667e-06,
      "loss": 0.0025,
      "step": 85830
    },
    {
      "epoch": 4.768888888888889,
      "grad_norm": 0.17766182124614716,
      "learning_rate": 2.311111111111111e-06,
      "loss": 0.0021,
      "step": 85840
    },
    {
      "epoch": 4.769444444444445,
      "grad_norm": 0.348361611366272,
      "learning_rate": 2.3055555555555556e-06,
      "loss": 0.0021,
      "step": 85850
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.20746879279613495,
      "learning_rate": 2.3e-06,
      "loss": 0.0024,
      "step": 85860
    },
    {
      "epoch": 4.770555555555555,
      "grad_norm": 0.09347165375947952,
      "learning_rate": 2.2944444444444444e-06,
      "loss": 0.0035,
      "step": 85870
    },
    {
      "epoch": 4.771111111111111,
      "grad_norm": 0.06003081426024437,
      "learning_rate": 2.2888888888888892e-06,
      "loss": 0.0034,
      "step": 85880
    },
    {
      "epoch": 4.7716666666666665,
      "grad_norm": 0.20813806354999542,
      "learning_rate": 2.2833333333333336e-06,
      "loss": 0.0033,
      "step": 85890
    },
    {
      "epoch": 4.772222222222222,
      "grad_norm": 0.062104858458042145,
      "learning_rate": 2.277777777777778e-06,
      "loss": 0.0028,
      "step": 85900
    },
    {
      "epoch": 4.772777777777778,
      "grad_norm": 0.2376396507024765,
      "learning_rate": 2.2722222222222224e-06,
      "loss": 0.0021,
      "step": 85910
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.06479210406541824,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0021,
      "step": 85920
    },
    {
      "epoch": 4.773888888888889,
      "grad_norm": 0.17832399904727936,
      "learning_rate": 2.2611111111111112e-06,
      "loss": 0.0029,
      "step": 85930
    },
    {
      "epoch": 4.774444444444445,
      "grad_norm": 0.010036161169409752,
      "learning_rate": 2.2555555555555557e-06,
      "loss": 0.0017,
      "step": 85940
    },
    {
      "epoch": 4.775,
      "grad_norm": 0.06041746214032173,
      "learning_rate": 2.25e-06,
      "loss": 0.0021,
      "step": 85950
    },
    {
      "epoch": 4.775555555555556,
      "grad_norm": 0.03256526589393616,
      "learning_rate": 2.2444444444444445e-06,
      "loss": 0.0025,
      "step": 85960
    },
    {
      "epoch": 4.776111111111111,
      "grad_norm": 0.17944420874118805,
      "learning_rate": 2.238888888888889e-06,
      "loss": 0.0022,
      "step": 85970
    },
    {
      "epoch": 4.776666666666666,
      "grad_norm": 0.013831707648932934,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0031,
      "step": 85980
    },
    {
      "epoch": 4.777222222222222,
      "grad_norm": 0.16898228228092194,
      "learning_rate": 2.227777777777778e-06,
      "loss": 0.0029,
      "step": 85990
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.13751083612442017,
      "learning_rate": 2.2222222222222225e-06,
      "loss": 0.0042,
      "step": 86000
    },
    {
      "epoch": 4.778333333333333,
      "grad_norm": 0.22069625556468964,
      "learning_rate": 2.216666666666667e-06,
      "loss": 0.0032,
      "step": 86010
    },
    {
      "epoch": 4.778888888888889,
      "grad_norm": 0.14954878389835358,
      "learning_rate": 2.2111111111111113e-06,
      "loss": 0.0024,
      "step": 86020
    },
    {
      "epoch": 4.779444444444445,
      "grad_norm": 0.09847551584243774,
      "learning_rate": 2.2055555555555557e-06,
      "loss": 0.002,
      "step": 86030
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.27562323212623596,
      "learning_rate": 2.2e-06,
      "loss": 0.0029,
      "step": 86040
    },
    {
      "epoch": 4.780555555555556,
      "grad_norm": 0.23703356087207794,
      "learning_rate": 2.1944444444444445e-06,
      "loss": 0.0026,
      "step": 86050
    },
    {
      "epoch": 4.781111111111111,
      "grad_norm": 0.01609308458864689,
      "learning_rate": 2.188888888888889e-06,
      "loss": 0.0013,
      "step": 86060
    },
    {
      "epoch": 4.781666666666666,
      "grad_norm": 0.08999478071928024,
      "learning_rate": 2.1833333333333333e-06,
      "loss": 0.0024,
      "step": 86070
    },
    {
      "epoch": 4.782222222222222,
      "grad_norm": 0.17809933423995972,
      "learning_rate": 2.1777777777777777e-06,
      "loss": 0.0022,
      "step": 86080
    },
    {
      "epoch": 4.782777777777778,
      "grad_norm": 0.29059943556785583,
      "learning_rate": 2.1722222222222226e-06,
      "loss": 0.0029,
      "step": 86090
    },
    {
      "epoch": 4.783333333333333,
      "grad_norm": 0.08962522447109222,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.0024,
      "step": 86100
    },
    {
      "epoch": 4.783888888888889,
      "grad_norm": 0.015461510978639126,
      "learning_rate": 2.1611111111111114e-06,
      "loss": 0.0021,
      "step": 86110
    },
    {
      "epoch": 4.7844444444444445,
      "grad_norm": 0.18295571208000183,
      "learning_rate": 2.1555555555555558e-06,
      "loss": 0.0016,
      "step": 86120
    },
    {
      "epoch": 4.785,
      "grad_norm": 0.6689412593841553,
      "learning_rate": 2.1499999999999997e-06,
      "loss": 0.003,
      "step": 86130
    },
    {
      "epoch": 4.785555555555556,
      "grad_norm": 0.20812533795833588,
      "learning_rate": 2.1444444444444446e-06,
      "loss": 0.0023,
      "step": 86140
    },
    {
      "epoch": 4.786111111111111,
      "grad_norm": 0.17857927083969116,
      "learning_rate": 2.138888888888889e-06,
      "loss": 0.0033,
      "step": 86150
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.09586219489574432,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0027,
      "step": 86160
    },
    {
      "epoch": 4.787222222222223,
      "grad_norm": 0.18274442851543427,
      "learning_rate": 2.1277777777777778e-06,
      "loss": 0.002,
      "step": 86170
    },
    {
      "epoch": 4.7877777777777775,
      "grad_norm": 0.1682974100112915,
      "learning_rate": 2.122222222222222e-06,
      "loss": 0.0026,
      "step": 86180
    },
    {
      "epoch": 4.788333333333333,
      "grad_norm": 0.0316927395761013,
      "learning_rate": 2.1166666666666666e-06,
      "loss": 0.0035,
      "step": 86190
    },
    {
      "epoch": 4.788888888888889,
      "grad_norm": 0.20726735889911652,
      "learning_rate": 2.1111111111111114e-06,
      "loss": 0.0024,
      "step": 86200
    },
    {
      "epoch": 4.789444444444444,
      "grad_norm": 0.15779201686382294,
      "learning_rate": 2.105555555555556e-06,
      "loss": 0.0029,
      "step": 86210
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.14885364472866058,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.0028,
      "step": 86220
    },
    {
      "epoch": 4.790555555555556,
      "grad_norm": 0.11911127716302872,
      "learning_rate": 2.0944444444444446e-06,
      "loss": 0.0026,
      "step": 86230
    },
    {
      "epoch": 4.791111111111111,
      "grad_norm": 0.1601707637310028,
      "learning_rate": 2.088888888888889e-06,
      "loss": 0.0034,
      "step": 86240
    },
    {
      "epoch": 4.791666666666667,
      "grad_norm": 0.20798957347869873,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.0022,
      "step": 86250
    },
    {
      "epoch": 4.792222222222223,
      "grad_norm": 0.14629527926445007,
      "learning_rate": 2.077777777777778e-06,
      "loss": 0.003,
      "step": 86260
    },
    {
      "epoch": 4.792777777777777,
      "grad_norm": 0.061522454023361206,
      "learning_rate": 2.0722222222222222e-06,
      "loss": 0.0028,
      "step": 86270
    },
    {
      "epoch": 4.793333333333333,
      "grad_norm": 0.062185242772102356,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0028,
      "step": 86280
    },
    {
      "epoch": 4.793888888888889,
      "grad_norm": 0.32613909244537354,
      "learning_rate": 2.061111111111111e-06,
      "loss": 0.0024,
      "step": 86290
    },
    {
      "epoch": 4.794444444444444,
      "grad_norm": 0.14831626415252686,
      "learning_rate": 2.055555555555556e-06,
      "loss": 0.0027,
      "step": 86300
    },
    {
      "epoch": 4.795,
      "grad_norm": 0.23725523054599762,
      "learning_rate": 2.0500000000000003e-06,
      "loss": 0.0017,
      "step": 86310
    },
    {
      "epoch": 4.795555555555556,
      "grad_norm": 0.2780665457248688,
      "learning_rate": 2.0444444444444447e-06,
      "loss": 0.0023,
      "step": 86320
    },
    {
      "epoch": 4.796111111111111,
      "grad_norm": 0.092286616563797,
      "learning_rate": 2.038888888888889e-06,
      "loss": 0.0023,
      "step": 86330
    },
    {
      "epoch": 4.796666666666667,
      "grad_norm": 0.4152386486530304,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.002,
      "step": 86340
    },
    {
      "epoch": 4.7972222222222225,
      "grad_norm": 0.1483793705701828,
      "learning_rate": 2.027777777777778e-06,
      "loss": 0.0026,
      "step": 86350
    },
    {
      "epoch": 4.797777777777778,
      "grad_norm": 0.06894785165786743,
      "learning_rate": 2.0222222222222223e-06,
      "loss": 0.0014,
      "step": 86360
    },
    {
      "epoch": 4.798333333333334,
      "grad_norm": 0.4464755654335022,
      "learning_rate": 2.0166666666666667e-06,
      "loss": 0.0017,
      "step": 86370
    },
    {
      "epoch": 4.7988888888888885,
      "grad_norm": 0.28774192929267883,
      "learning_rate": 2.011111111111111e-06,
      "loss": 0.0024,
      "step": 86380
    },
    {
      "epoch": 4.799444444444444,
      "grad_norm": 0.5931922197341919,
      "learning_rate": 2.0055555555555555e-06,
      "loss": 0.0028,
      "step": 86390
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.30668991804122925,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0034,
      "step": 86400
    },
    {
      "epoch": 4.8005555555555555,
      "grad_norm": 0.11906048655509949,
      "learning_rate": 1.9944444444444447e-06,
      "loss": 0.0025,
      "step": 86410
    },
    {
      "epoch": 4.801111111111111,
      "grad_norm": 0.09600533545017242,
      "learning_rate": 1.988888888888889e-06,
      "loss": 0.0022,
      "step": 86420
    },
    {
      "epoch": 4.801666666666667,
      "grad_norm": 0.0894085243344307,
      "learning_rate": 1.9833333333333335e-06,
      "loss": 0.0028,
      "step": 86430
    },
    {
      "epoch": 4.802222222222222,
      "grad_norm": 0.4710536003112793,
      "learning_rate": 1.9777777777777775e-06,
      "loss": 0.0025,
      "step": 86440
    },
    {
      "epoch": 4.802777777777778,
      "grad_norm": 0.010435295291244984,
      "learning_rate": 1.9722222222222224e-06,
      "loss": 0.0021,
      "step": 86450
    },
    {
      "epoch": 4.803333333333334,
      "grad_norm": 0.6928864121437073,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.0028,
      "step": 86460
    },
    {
      "epoch": 4.803888888888888,
      "grad_norm": 0.0323156900703907,
      "learning_rate": 1.961111111111111e-06,
      "loss": 0.0025,
      "step": 86470
    },
    {
      "epoch": 4.804444444444444,
      "grad_norm": 0.45124712586402893,
      "learning_rate": 1.9555555555555556e-06,
      "loss": 0.0029,
      "step": 86480
    },
    {
      "epoch": 4.805,
      "grad_norm": 0.3261778652667999,
      "learning_rate": 1.95e-06,
      "loss": 0.0032,
      "step": 86490
    },
    {
      "epoch": 4.805555555555555,
      "grad_norm": 0.3937292695045471,
      "learning_rate": 1.9444444444444444e-06,
      "loss": 0.0029,
      "step": 86500
    },
    {
      "epoch": 4.806111111111111,
      "grad_norm": 0.17818768322467804,
      "learning_rate": 1.938888888888889e-06,
      "loss": 0.0015,
      "step": 86510
    },
    {
      "epoch": 4.806666666666667,
      "grad_norm": 0.11872833222150803,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0024,
      "step": 86520
    },
    {
      "epoch": 4.807222222222222,
      "grad_norm": 0.19792494177818298,
      "learning_rate": 1.927777777777778e-06,
      "loss": 0.003,
      "step": 86530
    },
    {
      "epoch": 4.807777777777778,
      "grad_norm": 0.12006061524152756,
      "learning_rate": 1.9222222222222224e-06,
      "loss": 0.0022,
      "step": 86540
    },
    {
      "epoch": 4.808333333333334,
      "grad_norm": 0.2673719823360443,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.0033,
      "step": 86550
    },
    {
      "epoch": 4.808888888888889,
      "grad_norm": 0.3527126908302307,
      "learning_rate": 1.9111111111111112e-06,
      "loss": 0.0024,
      "step": 86560
    },
    {
      "epoch": 4.809444444444445,
      "grad_norm": 0.3848373591899872,
      "learning_rate": 1.9055555555555556e-06,
      "loss": 0.0024,
      "step": 86570
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 0.09451654553413391,
      "learning_rate": 1.9e-06,
      "loss": 0.0025,
      "step": 86580
    },
    {
      "epoch": 4.810555555555555,
      "grad_norm": 0.06097494065761566,
      "learning_rate": 1.8944444444444444e-06,
      "loss": 0.0025,
      "step": 86590
    },
    {
      "epoch": 4.811111111111111,
      "grad_norm": 0.11961327493190765,
      "learning_rate": 1.888888888888889e-06,
      "loss": 0.0027,
      "step": 86600
    },
    {
      "epoch": 4.8116666666666665,
      "grad_norm": 0.05786478519439697,
      "learning_rate": 1.8833333333333334e-06,
      "loss": 0.0025,
      "step": 86610
    },
    {
      "epoch": 4.812222222222222,
      "grad_norm": 0.2369975596666336,
      "learning_rate": 1.877777777777778e-06,
      "loss": 0.0031,
      "step": 86620
    },
    {
      "epoch": 4.812777777777778,
      "grad_norm": 0.035685908049345016,
      "learning_rate": 1.8722222222222225e-06,
      "loss": 0.003,
      "step": 86630
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.11930021643638611,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.003,
      "step": 86640
    },
    {
      "epoch": 4.813888888888889,
      "grad_norm": 0.06323834508657455,
      "learning_rate": 1.861111111111111e-06,
      "loss": 0.0024,
      "step": 86650
    },
    {
      "epoch": 4.814444444444445,
      "grad_norm": 0.011698026210069656,
      "learning_rate": 1.8555555555555555e-06,
      "loss": 0.0028,
      "step": 86660
    },
    {
      "epoch": 4.8149999999999995,
      "grad_norm": 0.2221957892179489,
      "learning_rate": 1.85e-06,
      "loss": 0.0029,
      "step": 86670
    },
    {
      "epoch": 4.815555555555555,
      "grad_norm": 0.03210273012518883,
      "learning_rate": 1.8444444444444445e-06,
      "loss": 0.0036,
      "step": 86680
    },
    {
      "epoch": 4.816111111111111,
      "grad_norm": 0.3933292627334595,
      "learning_rate": 1.8388888888888889e-06,
      "loss": 0.002,
      "step": 86690
    },
    {
      "epoch": 4.816666666666666,
      "grad_norm": 0.03460461273789406,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0037,
      "step": 86700
    },
    {
      "epoch": 4.817222222222222,
      "grad_norm": 0.06045771762728691,
      "learning_rate": 1.827777777777778e-06,
      "loss": 0.0025,
      "step": 86710
    },
    {
      "epoch": 4.817777777777778,
      "grad_norm": 0.22425465285778046,
      "learning_rate": 1.8222222222222223e-06,
      "loss": 0.0042,
      "step": 86720
    },
    {
      "epoch": 4.818333333333333,
      "grad_norm": 0.14838625490665436,
      "learning_rate": 1.816666666666667e-06,
      "loss": 0.002,
      "step": 86730
    },
    {
      "epoch": 4.818888888888889,
      "grad_norm": 0.48098286986351013,
      "learning_rate": 1.8111111111111113e-06,
      "loss": 0.0027,
      "step": 86740
    },
    {
      "epoch": 4.819444444444445,
      "grad_norm": 0.45413851737976074,
      "learning_rate": 1.8055555555555555e-06,
      "loss": 0.0025,
      "step": 86750
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.0893469750881195,
      "learning_rate": 1.8e-06,
      "loss": 0.0021,
      "step": 86760
    },
    {
      "epoch": 4.820555555555556,
      "grad_norm": 0.26782137155532837,
      "learning_rate": 1.7944444444444445e-06,
      "loss": 0.0036,
      "step": 86770
    },
    {
      "epoch": 4.821111111111112,
      "grad_norm": 0.5191394686698914,
      "learning_rate": 1.788888888888889e-06,
      "loss": 0.003,
      "step": 86780
    },
    {
      "epoch": 4.821666666666666,
      "grad_norm": 0.4152601957321167,
      "learning_rate": 1.7833333333333333e-06,
      "loss": 0.0026,
      "step": 86790
    },
    {
      "epoch": 4.822222222222222,
      "grad_norm": 0.14836673438549042,
      "learning_rate": 1.777777777777778e-06,
      "loss": 0.002,
      "step": 86800
    },
    {
      "epoch": 4.822777777777778,
      "grad_norm": 0.10937312990427017,
      "learning_rate": 1.7722222222222224e-06,
      "loss": 0.0041,
      "step": 86810
    },
    {
      "epoch": 4.823333333333333,
      "grad_norm": 0.15050950646400452,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0039,
      "step": 86820
    },
    {
      "epoch": 4.823888888888889,
      "grad_norm": 0.06490468978881836,
      "learning_rate": 1.7611111111111114e-06,
      "loss": 0.002,
      "step": 86830
    },
    {
      "epoch": 4.8244444444444445,
      "grad_norm": 0.17839287221431732,
      "learning_rate": 1.7555555555555558e-06,
      "loss": 0.0017,
      "step": 86840
    },
    {
      "epoch": 4.825,
      "grad_norm": 0.11929504573345184,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.0019,
      "step": 86850
    },
    {
      "epoch": 4.825555555555556,
      "grad_norm": 0.14842930436134338,
      "learning_rate": 1.7444444444444444e-06,
      "loss": 0.003,
      "step": 86860
    },
    {
      "epoch": 4.8261111111111115,
      "grad_norm": 0.090999074280262,
      "learning_rate": 1.7388888888888888e-06,
      "loss": 0.0022,
      "step": 86870
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.08928298950195312,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0025,
      "step": 86880
    },
    {
      "epoch": 4.827222222222222,
      "grad_norm": 0.19615571200847626,
      "learning_rate": 1.7277777777777778e-06,
      "loss": 0.002,
      "step": 86890
    },
    {
      "epoch": 4.8277777777777775,
      "grad_norm": 0.2243266999721527,
      "learning_rate": 1.7222222222222222e-06,
      "loss": 0.0035,
      "step": 86900
    },
    {
      "epoch": 4.828333333333333,
      "grad_norm": 0.036570798605680466,
      "learning_rate": 1.7166666666666668e-06,
      "loss": 0.0024,
      "step": 86910
    },
    {
      "epoch": 4.828888888888889,
      "grad_norm": 0.48532217741012573,
      "learning_rate": 1.7111111111111112e-06,
      "loss": 0.0028,
      "step": 86920
    },
    {
      "epoch": 4.829444444444444,
      "grad_norm": 0.19753192365169525,
      "learning_rate": 1.7055555555555556e-06,
      "loss": 0.0025,
      "step": 86930
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.1496078222990036,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.0034,
      "step": 86940
    },
    {
      "epoch": 4.830555555555556,
      "grad_norm": 0.39501625299453735,
      "learning_rate": 1.6944444444444446e-06,
      "loss": 0.0028,
      "step": 86950
    },
    {
      "epoch": 4.831111111111111,
      "grad_norm": 0.03128490597009659,
      "learning_rate": 1.6888888888888888e-06,
      "loss": 0.0026,
      "step": 86960
    },
    {
      "epoch": 4.831666666666667,
      "grad_norm": 0.061712734401226044,
      "learning_rate": 1.6833333333333332e-06,
      "loss": 0.0026,
      "step": 86970
    },
    {
      "epoch": 4.832222222222223,
      "grad_norm": 0.3360852301120758,
      "learning_rate": 1.6777777777777779e-06,
      "loss": 0.0035,
      "step": 86980
    },
    {
      "epoch": 4.832777777777777,
      "grad_norm": 0.03346622735261917,
      "learning_rate": 1.6722222222222223e-06,
      "loss": 0.0034,
      "step": 86990
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 0.3559301495552063,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0037,
      "step": 87000
    },
    {
      "epoch": 4.833888888888889,
      "grad_norm": 0.11960090696811676,
      "learning_rate": 1.6611111111111113e-06,
      "loss": 0.0022,
      "step": 87010
    },
    {
      "epoch": 4.834444444444444,
      "grad_norm": 0.14809836447238922,
      "learning_rate": 1.6555555555555557e-06,
      "loss": 0.002,
      "step": 87020
    },
    {
      "epoch": 4.835,
      "grad_norm": 0.35578063130378723,
      "learning_rate": 1.65e-06,
      "loss": 0.0023,
      "step": 87030
    },
    {
      "epoch": 4.835555555555556,
      "grad_norm": 0.32544562220573425,
      "learning_rate": 1.6444444444444447e-06,
      "loss": 0.0032,
      "step": 87040
    },
    {
      "epoch": 4.836111111111111,
      "grad_norm": 0.12889666855335236,
      "learning_rate": 1.638888888888889e-06,
      "loss": 0.0018,
      "step": 87050
    },
    {
      "epoch": 4.836666666666667,
      "grad_norm": 0.2093578726053238,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0025,
      "step": 87060
    },
    {
      "epoch": 4.8372222222222225,
      "grad_norm": 0.2984400689601898,
      "learning_rate": 1.6277777777777777e-06,
      "loss": 0.0022,
      "step": 87070
    },
    {
      "epoch": 4.837777777777777,
      "grad_norm": 0.0898427739739418,
      "learning_rate": 1.622222222222222e-06,
      "loss": 0.0019,
      "step": 87080
    },
    {
      "epoch": 4.838333333333333,
      "grad_norm": 0.8709977269172668,
      "learning_rate": 1.6166666666666667e-06,
      "loss": 0.0019,
      "step": 87090
    },
    {
      "epoch": 4.838888888888889,
      "grad_norm": 0.35523009300231934,
      "learning_rate": 1.6111111111111111e-06,
      "loss": 0.003,
      "step": 87100
    },
    {
      "epoch": 4.839444444444444,
      "grad_norm": 0.09002204239368439,
      "learning_rate": 1.6055555555555557e-06,
      "loss": 0.0025,
      "step": 87110
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.032450124621391296,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0039,
      "step": 87120
    },
    {
      "epoch": 4.8405555555555555,
      "grad_norm": 0.11915935575962067,
      "learning_rate": 1.5944444444444445e-06,
      "loss": 0.0021,
      "step": 87130
    },
    {
      "epoch": 4.841111111111111,
      "grad_norm": 0.09008672088384628,
      "learning_rate": 1.5888888888888892e-06,
      "loss": 0.0023,
      "step": 87140
    },
    {
      "epoch": 4.841666666666667,
      "grad_norm": 0.4251052439212799,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.0027,
      "step": 87150
    },
    {
      "epoch": 4.842222222222222,
      "grad_norm": 0.17840245366096497,
      "learning_rate": 1.577777777777778e-06,
      "loss": 0.0032,
      "step": 87160
    },
    {
      "epoch": 4.842777777777778,
      "grad_norm": 0.2077682763338089,
      "learning_rate": 1.5722222222222222e-06,
      "loss": 0.0027,
      "step": 87170
    },
    {
      "epoch": 4.843333333333334,
      "grad_norm": 0.14603926241397858,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0027,
      "step": 87180
    },
    {
      "epoch": 4.843888888888889,
      "grad_norm": 0.4686249792575836,
      "learning_rate": 1.5611111111111112e-06,
      "loss": 0.0016,
      "step": 87190
    },
    {
      "epoch": 4.844444444444444,
      "grad_norm": 0.6598577499389648,
      "learning_rate": 1.5555555555555556e-06,
      "loss": 0.0024,
      "step": 87200
    },
    {
      "epoch": 4.845,
      "grad_norm": 0.5773999094963074,
      "learning_rate": 1.55e-06,
      "loss": 0.0018,
      "step": 87210
    },
    {
      "epoch": 4.845555555555555,
      "grad_norm": 0.1494414508342743,
      "learning_rate": 1.5444444444444446e-06,
      "loss": 0.0023,
      "step": 87220
    },
    {
      "epoch": 4.846111111111111,
      "grad_norm": 0.11887233704328537,
      "learning_rate": 1.538888888888889e-06,
      "loss": 0.0022,
      "step": 87230
    },
    {
      "epoch": 4.846666666666667,
      "grad_norm": 0.09119639545679092,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0029,
      "step": 87240
    },
    {
      "epoch": 4.847222222222222,
      "grad_norm": 0.07497861236333847,
      "learning_rate": 1.5277777777777778e-06,
      "loss": 0.0025,
      "step": 87250
    },
    {
      "epoch": 4.847777777777778,
      "grad_norm": 0.29649847745895386,
      "learning_rate": 1.5222222222222222e-06,
      "loss": 0.0024,
      "step": 87260
    },
    {
      "epoch": 4.848333333333334,
      "grad_norm": 0.034331053495407104,
      "learning_rate": 1.5166666666666668e-06,
      "loss": 0.0029,
      "step": 87270
    },
    {
      "epoch": 4.848888888888889,
      "grad_norm": 0.35609468817710876,
      "learning_rate": 1.5111111111111112e-06,
      "loss": 0.0028,
      "step": 87280
    },
    {
      "epoch": 4.849444444444444,
      "grad_norm": 0.09184646606445312,
      "learning_rate": 1.5055555555555556e-06,
      "loss": 0.0025,
      "step": 87290
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.39217063784599304,
      "learning_rate": 1.5e-06,
      "loss": 0.0026,
      "step": 87300
    },
    {
      "epoch": 4.850555555555555,
      "grad_norm": 0.03324684128165245,
      "learning_rate": 1.4944444444444444e-06,
      "loss": 0.0029,
      "step": 87310
    },
    {
      "epoch": 4.851111111111111,
      "grad_norm": 0.33398064970970154,
      "learning_rate": 1.488888888888889e-06,
      "loss": 0.0024,
      "step": 87320
    },
    {
      "epoch": 4.851666666666667,
      "grad_norm": 0.11926108598709106,
      "learning_rate": 1.4833333333333335e-06,
      "loss": 0.0029,
      "step": 87330
    },
    {
      "epoch": 4.852222222222222,
      "grad_norm": 0.20701497793197632,
      "learning_rate": 1.4777777777777779e-06,
      "loss": 0.0027,
      "step": 87340
    },
    {
      "epoch": 4.852777777777778,
      "grad_norm": 0.03361177444458008,
      "learning_rate": 1.4722222222222223e-06,
      "loss": 0.003,
      "step": 87350
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.34917259216308594,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0026,
      "step": 87360
    },
    {
      "epoch": 4.853888888888889,
      "grad_norm": 0.14831925928592682,
      "learning_rate": 1.4611111111111113e-06,
      "loss": 0.0027,
      "step": 87370
    },
    {
      "epoch": 4.854444444444445,
      "grad_norm": 0.46390923857688904,
      "learning_rate": 1.4555555555555557e-06,
      "loss": 0.0037,
      "step": 87380
    },
    {
      "epoch": 4.855,
      "grad_norm": 0.09053325653076172,
      "learning_rate": 1.45e-06,
      "loss": 0.0023,
      "step": 87390
    },
    {
      "epoch": 4.855555555555555,
      "grad_norm": 0.20846790075302124,
      "learning_rate": 1.4444444444444445e-06,
      "loss": 0.0032,
      "step": 87400
    },
    {
      "epoch": 4.856111111111111,
      "grad_norm": 0.48052340745925903,
      "learning_rate": 1.438888888888889e-06,
      "loss": 0.0026,
      "step": 87410
    },
    {
      "epoch": 4.8566666666666665,
      "grad_norm": 0.09227592498064041,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.0021,
      "step": 87420
    },
    {
      "epoch": 4.857222222222222,
      "grad_norm": 0.03305795416235924,
      "learning_rate": 1.427777777777778e-06,
      "loss": 0.003,
      "step": 87430
    },
    {
      "epoch": 4.857777777777778,
      "grad_norm": 0.07344818115234375,
      "learning_rate": 1.4222222222222223e-06,
      "loss": 0.003,
      "step": 87440
    },
    {
      "epoch": 4.858333333333333,
      "grad_norm": 0.04038083553314209,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.0026,
      "step": 87450
    },
    {
      "epoch": 4.858888888888889,
      "grad_norm": 0.07012899219989777,
      "learning_rate": 1.4111111111111111e-06,
      "loss": 0.0034,
      "step": 87460
    },
    {
      "epoch": 4.859444444444445,
      "grad_norm": 0.480739563703537,
      "learning_rate": 1.4055555555555555e-06,
      "loss": 0.0023,
      "step": 87470
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.17934860289096832,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0033,
      "step": 87480
    },
    {
      "epoch": 4.860555555555555,
      "grad_norm": 0.41833123564720154,
      "learning_rate": 1.3944444444444446e-06,
      "loss": 0.0025,
      "step": 87490
    },
    {
      "epoch": 4.861111111111111,
      "grad_norm": 0.2665189802646637,
      "learning_rate": 1.388888888888889e-06,
      "loss": 0.0031,
      "step": 87500
    },
    {
      "epoch": 4.861666666666666,
      "grad_norm": 0.6413118243217468,
      "learning_rate": 1.3833333333333334e-06,
      "loss": 0.0025,
      "step": 87510
    },
    {
      "epoch": 4.862222222222222,
      "grad_norm": 0.05978640913963318,
      "learning_rate": 1.3777777777777778e-06,
      "loss": 0.0023,
      "step": 87520
    },
    {
      "epoch": 4.862777777777778,
      "grad_norm": 0.1193942055106163,
      "learning_rate": 1.3722222222222224e-06,
      "loss": 0.0036,
      "step": 87530
    },
    {
      "epoch": 4.863333333333333,
      "grad_norm": 0.26675090193748474,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0021,
      "step": 87540
    },
    {
      "epoch": 4.863888888888889,
      "grad_norm": 0.21354936063289642,
      "learning_rate": 1.3611111111111112e-06,
      "loss": 0.0031,
      "step": 87550
    },
    {
      "epoch": 4.864444444444445,
      "grad_norm": 0.5108123421669006,
      "learning_rate": 1.3555555555555556e-06,
      "loss": 0.0024,
      "step": 87560
    },
    {
      "epoch": 4.865,
      "grad_norm": 0.2861674726009369,
      "learning_rate": 1.35e-06,
      "loss": 0.0024,
      "step": 87570
    },
    {
      "epoch": 4.865555555555556,
      "grad_norm": 0.06064050644636154,
      "learning_rate": 1.3444444444444446e-06,
      "loss": 0.0024,
      "step": 87580
    },
    {
      "epoch": 4.8661111111111115,
      "grad_norm": 0.20867998898029327,
      "learning_rate": 1.338888888888889e-06,
      "loss": 0.0013,
      "step": 87590
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.2667195796966553,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0029,
      "step": 87600
    },
    {
      "epoch": 4.867222222222222,
      "grad_norm": 0.11864784359931946,
      "learning_rate": 1.3277777777777778e-06,
      "loss": 0.0034,
      "step": 87610
    },
    {
      "epoch": 4.8677777777777775,
      "grad_norm": 0.2678191363811493,
      "learning_rate": 1.3222222222222222e-06,
      "loss": 0.0027,
      "step": 87620
    },
    {
      "epoch": 4.868333333333333,
      "grad_norm": 0.020512448623776436,
      "learning_rate": 1.3166666666666668e-06,
      "loss": 0.0021,
      "step": 87630
    },
    {
      "epoch": 4.868888888888889,
      "grad_norm": 0.3648791015148163,
      "learning_rate": 1.3111111111111112e-06,
      "loss": 0.0023,
      "step": 87640
    },
    {
      "epoch": 4.8694444444444445,
      "grad_norm": 0.5305045247077942,
      "learning_rate": 1.3055555555555556e-06,
      "loss": 0.0025,
      "step": 87650
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.20797795057296753,
      "learning_rate": 1.3e-06,
      "loss": 0.0033,
      "step": 87660
    },
    {
      "epoch": 4.870555555555556,
      "grad_norm": 0.015060295350849628,
      "learning_rate": 1.2944444444444445e-06,
      "loss": 0.0015,
      "step": 87670
    },
    {
      "epoch": 4.871111111111111,
      "grad_norm": 0.20714087784290314,
      "learning_rate": 1.2888888888888889e-06,
      "loss": 0.0027,
      "step": 87680
    },
    {
      "epoch": 4.871666666666667,
      "grad_norm": 0.20816203951835632,
      "learning_rate": 1.2833333333333335e-06,
      "loss": 0.0018,
      "step": 87690
    },
    {
      "epoch": 4.872222222222222,
      "grad_norm": 0.8630369305610657,
      "learning_rate": 1.2777777777777779e-06,
      "loss": 0.002,
      "step": 87700
    },
    {
      "epoch": 4.872777777777777,
      "grad_norm": 0.17773163318634033,
      "learning_rate": 1.2722222222222223e-06,
      "loss": 0.0029,
      "step": 87710
    },
    {
      "epoch": 4.873333333333333,
      "grad_norm": 0.11904877424240112,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0028,
      "step": 87720
    },
    {
      "epoch": 4.873888888888889,
      "grad_norm": 0.16234613955020905,
      "learning_rate": 1.261111111111111e-06,
      "loss": 0.0019,
      "step": 87730
    },
    {
      "epoch": 4.874444444444444,
      "grad_norm": 0.09913517534732819,
      "learning_rate": 1.2555555555555557e-06,
      "loss": 0.0023,
      "step": 87740
    },
    {
      "epoch": 4.875,
      "grad_norm": 0.20516808331012726,
      "learning_rate": 1.25e-06,
      "loss": 0.003,
      "step": 87750
    },
    {
      "epoch": 4.875555555555556,
      "grad_norm": 0.11964363604784012,
      "learning_rate": 1.2444444444444445e-06,
      "loss": 0.0023,
      "step": 87760
    },
    {
      "epoch": 4.876111111111111,
      "grad_norm": 0.12152563780546188,
      "learning_rate": 1.238888888888889e-06,
      "loss": 0.0029,
      "step": 87770
    },
    {
      "epoch": 4.876666666666667,
      "grad_norm": 0.08911404758691788,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0022,
      "step": 87780
    },
    {
      "epoch": 4.877222222222223,
      "grad_norm": 0.2668537497520447,
      "learning_rate": 1.227777777777778e-06,
      "loss": 0.0035,
      "step": 87790
    },
    {
      "epoch": 4.877777777777778,
      "grad_norm": 0.48054400086402893,
      "learning_rate": 1.2222222222222223e-06,
      "loss": 0.0021,
      "step": 87800
    },
    {
      "epoch": 4.878333333333333,
      "grad_norm": 0.29800736904144287,
      "learning_rate": 1.2166666666666667e-06,
      "loss": 0.0031,
      "step": 87810
    },
    {
      "epoch": 4.878888888888889,
      "grad_norm": 0.14855632185935974,
      "learning_rate": 1.2111111111111111e-06,
      "loss": 0.0025,
      "step": 87820
    },
    {
      "epoch": 4.879444444444444,
      "grad_norm": 0.20839229226112366,
      "learning_rate": 1.2055555555555555e-06,
      "loss": 0.0015,
      "step": 87830
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.44452354311943054,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0026,
      "step": 87840
    },
    {
      "epoch": 4.8805555555555555,
      "grad_norm": 0.09047012031078339,
      "learning_rate": 1.1944444444444446e-06,
      "loss": 0.0026,
      "step": 87850
    },
    {
      "epoch": 4.881111111111111,
      "grad_norm": 0.012982997111976147,
      "learning_rate": 1.188888888888889e-06,
      "loss": 0.0024,
      "step": 87860
    },
    {
      "epoch": 4.881666666666667,
      "grad_norm": 0.1780029684305191,
      "learning_rate": 1.1833333333333334e-06,
      "loss": 0.003,
      "step": 87870
    },
    {
      "epoch": 4.8822222222222225,
      "grad_norm": 0.1507321149110794,
      "learning_rate": 1.1777777777777778e-06,
      "loss": 0.0034,
      "step": 87880
    },
    {
      "epoch": 4.882777777777778,
      "grad_norm": 0.4989525377750397,
      "learning_rate": 1.1722222222222224e-06,
      "loss": 0.0015,
      "step": 87890
    },
    {
      "epoch": 4.883333333333333,
      "grad_norm": 0.013462336733937263,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.002,
      "step": 87900
    },
    {
      "epoch": 4.8838888888888885,
      "grad_norm": 0.23878780007362366,
      "learning_rate": 1.161111111111111e-06,
      "loss": 0.0023,
      "step": 87910
    },
    {
      "epoch": 4.884444444444444,
      "grad_norm": 0.08972372114658356,
      "learning_rate": 1.1555555555555556e-06,
      "loss": 0.003,
      "step": 87920
    },
    {
      "epoch": 4.885,
      "grad_norm": 0.6230772137641907,
      "learning_rate": 1.15e-06,
      "loss": 0.0037,
      "step": 87930
    },
    {
      "epoch": 4.885555555555555,
      "grad_norm": 0.2370307445526123,
      "learning_rate": 1.1444444444444446e-06,
      "loss": 0.0026,
      "step": 87940
    },
    {
      "epoch": 4.886111111111111,
      "grad_norm": 0.061788346618413925,
      "learning_rate": 1.138888888888889e-06,
      "loss": 0.0025,
      "step": 87950
    },
    {
      "epoch": 4.886666666666667,
      "grad_norm": 0.11999247968196869,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0018,
      "step": 87960
    },
    {
      "epoch": 4.887222222222222,
      "grad_norm": 0.23903460800647736,
      "learning_rate": 1.1277777777777778e-06,
      "loss": 0.0019,
      "step": 87970
    },
    {
      "epoch": 4.887777777777778,
      "grad_norm": 0.05552201345562935,
      "learning_rate": 1.1222222222222222e-06,
      "loss": 0.0024,
      "step": 87980
    },
    {
      "epoch": 4.888333333333334,
      "grad_norm": 0.21065786480903625,
      "learning_rate": 1.1166666666666666e-06,
      "loss": 0.0026,
      "step": 87990
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.2968965470790863,
      "learning_rate": 1.1111111111111112e-06,
      "loss": 0.0027,
      "step": 88000
    },
    {
      "epoch": 4.889444444444445,
      "grad_norm": 0.14950641989707947,
      "learning_rate": 1.1055555555555557e-06,
      "loss": 0.0023,
      "step": 88010
    },
    {
      "epoch": 4.89,
      "grad_norm": 0.27777066826820374,
      "learning_rate": 1.1e-06,
      "loss": 0.003,
      "step": 88020
    },
    {
      "epoch": 4.890555555555555,
      "grad_norm": 0.17850519716739655,
      "learning_rate": 1.0944444444444445e-06,
      "loss": 0.0018,
      "step": 88030
    },
    {
      "epoch": 4.891111111111111,
      "grad_norm": 0.06069430336356163,
      "learning_rate": 1.0888888888888889e-06,
      "loss": 0.0029,
      "step": 88040
    },
    {
      "epoch": 4.891666666666667,
      "grad_norm": 0.14841674268245697,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.003,
      "step": 88050
    },
    {
      "epoch": 4.892222222222222,
      "grad_norm": 0.1492406725883484,
      "learning_rate": 1.0777777777777779e-06,
      "loss": 0.0031,
      "step": 88060
    },
    {
      "epoch": 4.892777777777778,
      "grad_norm": 0.17983122169971466,
      "learning_rate": 1.0722222222222223e-06,
      "loss": 0.0031,
      "step": 88070
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.11941325664520264,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0018,
      "step": 88080
    },
    {
      "epoch": 4.893888888888889,
      "grad_norm": 0.237420454621315,
      "learning_rate": 1.061111111111111e-06,
      "loss": 0.0025,
      "step": 88090
    },
    {
      "epoch": 4.894444444444445,
      "grad_norm": 0.2960524559020996,
      "learning_rate": 1.0555555555555557e-06,
      "loss": 0.0033,
      "step": 88100
    },
    {
      "epoch": 4.895,
      "grad_norm": 0.20865218341350555,
      "learning_rate": 1.0500000000000001e-06,
      "loss": 0.0019,
      "step": 88110
    },
    {
      "epoch": 4.895555555555555,
      "grad_norm": 0.35681965947151184,
      "learning_rate": 1.0444444444444445e-06,
      "loss": 0.0038,
      "step": 88120
    },
    {
      "epoch": 4.896111111111111,
      "grad_norm": 0.1255812793970108,
      "learning_rate": 1.038888888888889e-06,
      "loss": 0.0025,
      "step": 88130
    },
    {
      "epoch": 4.8966666666666665,
      "grad_norm": 0.29655224084854126,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.003,
      "step": 88140
    },
    {
      "epoch": 4.897222222222222,
      "grad_norm": 0.11996136605739594,
      "learning_rate": 1.027777777777778e-06,
      "loss": 0.0027,
      "step": 88150
    },
    {
      "epoch": 4.897777777777778,
      "grad_norm": 0.23801405727863312,
      "learning_rate": 1.0222222222222223e-06,
      "loss": 0.0028,
      "step": 88160
    },
    {
      "epoch": 4.898333333333333,
      "grad_norm": 0.060419950634241104,
      "learning_rate": 1.0166666666666665e-06,
      "loss": 0.0028,
      "step": 88170
    },
    {
      "epoch": 4.898888888888889,
      "grad_norm": 0.43403178453445435,
      "learning_rate": 1.0111111111111111e-06,
      "loss": 0.0022,
      "step": 88180
    },
    {
      "epoch": 4.899444444444445,
      "grad_norm": 0.08959212154150009,
      "learning_rate": 1.0055555555555556e-06,
      "loss": 0.0017,
      "step": 88190
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.009880859404802322,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0023,
      "step": 88200
    },
    {
      "epoch": 4.900555555555556,
      "grad_norm": 0.03214988112449646,
      "learning_rate": 9.944444444444446e-07,
      "loss": 0.0032,
      "step": 88210
    },
    {
      "epoch": 4.901111111111111,
      "grad_norm": 0.3564292788505554,
      "learning_rate": 9.888888888888888e-07,
      "loss": 0.0023,
      "step": 88220
    },
    {
      "epoch": 4.901666666666666,
      "grad_norm": 0.4843273162841797,
      "learning_rate": 9.833333333333334e-07,
      "loss": 0.0033,
      "step": 88230
    },
    {
      "epoch": 4.902222222222222,
      "grad_norm": 0.11967039108276367,
      "learning_rate": 9.777777777777778e-07,
      "loss": 0.0028,
      "step": 88240
    },
    {
      "epoch": 4.902777777777778,
      "grad_norm": 0.14914532005786896,
      "learning_rate": 9.722222222222222e-07,
      "loss": 0.0026,
      "step": 88250
    },
    {
      "epoch": 4.903333333333333,
      "grad_norm": 0.3893651068210602,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.0027,
      "step": 88260
    },
    {
      "epoch": 4.903888888888889,
      "grad_norm": 0.1491168886423111,
      "learning_rate": 9.611111111111112e-07,
      "loss": 0.0019,
      "step": 88270
    },
    {
      "epoch": 4.904444444444445,
      "grad_norm": 0.5991628766059875,
      "learning_rate": 9.555555555555556e-07,
      "loss": 0.0029,
      "step": 88280
    },
    {
      "epoch": 4.905,
      "grad_norm": 0.6282598376274109,
      "learning_rate": 9.5e-07,
      "loss": 0.003,
      "step": 88290
    },
    {
      "epoch": 4.905555555555556,
      "grad_norm": 0.03430166468024254,
      "learning_rate": 9.444444444444445e-07,
      "loss": 0.0031,
      "step": 88300
    },
    {
      "epoch": 4.906111111111111,
      "grad_norm": 0.09066621214151382,
      "learning_rate": 9.38888888888889e-07,
      "loss": 0.003,
      "step": 88310
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.23761212825775146,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0024,
      "step": 88320
    },
    {
      "epoch": 4.907222222222222,
      "grad_norm": 0.14836609363555908,
      "learning_rate": 9.277777777777777e-07,
      "loss": 0.003,
      "step": 88330
    },
    {
      "epoch": 4.907777777777778,
      "grad_norm": 0.11993023753166199,
      "learning_rate": 9.222222222222222e-07,
      "loss": 0.0028,
      "step": 88340
    },
    {
      "epoch": 4.908333333333333,
      "grad_norm": 0.26794829964637756,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.0026,
      "step": 88350
    },
    {
      "epoch": 4.908888888888889,
      "grad_norm": 0.11879268288612366,
      "learning_rate": 9.111111111111112e-07,
      "loss": 0.0026,
      "step": 88360
    },
    {
      "epoch": 4.9094444444444445,
      "grad_norm": 0.11895313858985901,
      "learning_rate": 9.055555555555557e-07,
      "loss": 0.0027,
      "step": 88370
    },
    {
      "epoch": 4.91,
      "grad_norm": 0.27573898434638977,
      "learning_rate": 9e-07,
      "loss": 0.0031,
      "step": 88380
    },
    {
      "epoch": 4.910555555555556,
      "grad_norm": 0.1376110464334488,
      "learning_rate": 8.944444444444445e-07,
      "loss": 0.0024,
      "step": 88390
    },
    {
      "epoch": 4.911111111111111,
      "grad_norm": 0.3214312195777893,
      "learning_rate": 8.88888888888889e-07,
      "loss": 0.0023,
      "step": 88400
    },
    {
      "epoch": 4.911666666666667,
      "grad_norm": 0.26679298281669617,
      "learning_rate": 8.833333333333334e-07,
      "loss": 0.0025,
      "step": 88410
    },
    {
      "epoch": 4.912222222222223,
      "grad_norm": 0.33620932698249817,
      "learning_rate": 8.777777777777779e-07,
      "loss": 0.0022,
      "step": 88420
    },
    {
      "epoch": 4.9127777777777775,
      "grad_norm": 0.03559017926454544,
      "learning_rate": 8.722222222222222e-07,
      "loss": 0.0018,
      "step": 88430
    },
    {
      "epoch": 4.913333333333333,
      "grad_norm": 0.03696690499782562,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0028,
      "step": 88440
    },
    {
      "epoch": 4.913888888888889,
      "grad_norm": 0.09139559417963028,
      "learning_rate": 8.611111111111111e-07,
      "loss": 0.0028,
      "step": 88450
    },
    {
      "epoch": 4.914444444444444,
      "grad_norm": 0.3053915798664093,
      "learning_rate": 8.555555555555556e-07,
      "loss": 0.0021,
      "step": 88460
    },
    {
      "epoch": 4.915,
      "grad_norm": 0.11817917227745056,
      "learning_rate": 8.500000000000001e-07,
      "loss": 0.0023,
      "step": 88470
    },
    {
      "epoch": 4.915555555555556,
      "grad_norm": 0.4160078763961792,
      "learning_rate": 8.444444444444444e-07,
      "loss": 0.0036,
      "step": 88480
    },
    {
      "epoch": 4.916111111111111,
      "grad_norm": 0.20794016122817993,
      "learning_rate": 8.388888888888889e-07,
      "loss": 0.0022,
      "step": 88490
    },
    {
      "epoch": 4.916666666666667,
      "grad_norm": 0.09901715815067291,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0021,
      "step": 88500
    },
    {
      "epoch": 4.917222222222223,
      "grad_norm": 0.29723992943763733,
      "learning_rate": 8.277777777777778e-07,
      "loss": 0.0028,
      "step": 88510
    },
    {
      "epoch": 4.917777777777777,
      "grad_norm": 0.09004335850477219,
      "learning_rate": 8.222222222222223e-07,
      "loss": 0.003,
      "step": 88520
    },
    {
      "epoch": 4.918333333333333,
      "grad_norm": 0.20799770951271057,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0021,
      "step": 88530
    },
    {
      "epoch": 4.918888888888889,
      "grad_norm": 0.2968928813934326,
      "learning_rate": 8.11111111111111e-07,
      "loss": 0.0023,
      "step": 88540
    },
    {
      "epoch": 4.919444444444444,
      "grad_norm": 0.17818181216716766,
      "learning_rate": 8.055555555555556e-07,
      "loss": 0.0039,
      "step": 88550
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.20692229270935059,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0027,
      "step": 88560
    },
    {
      "epoch": 4.920555555555556,
      "grad_norm": 0.06060278043150902,
      "learning_rate": 7.944444444444446e-07,
      "loss": 0.0025,
      "step": 88570
    },
    {
      "epoch": 4.921111111111111,
      "grad_norm": 0.08950276672840118,
      "learning_rate": 7.88888888888889e-07,
      "loss": 0.002,
      "step": 88580
    },
    {
      "epoch": 4.921666666666667,
      "grad_norm": 0.08950470387935638,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.0045,
      "step": 88590
    },
    {
      "epoch": 4.9222222222222225,
      "grad_norm": 0.7021108269691467,
      "learning_rate": 7.777777777777778e-07,
      "loss": 0.0022,
      "step": 88600
    },
    {
      "epoch": 4.922777777777778,
      "grad_norm": 0.3095512092113495,
      "learning_rate": 7.722222222222223e-07,
      "loss": 0.003,
      "step": 88610
    },
    {
      "epoch": 4.923333333333334,
      "grad_norm": 0.20776978135108948,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0018,
      "step": 88620
    },
    {
      "epoch": 4.9238888888888885,
      "grad_norm": 0.17829403281211853,
      "learning_rate": 7.611111111111111e-07,
      "loss": 0.0023,
      "step": 88630
    },
    {
      "epoch": 4.924444444444444,
      "grad_norm": 0.06090408191084862,
      "learning_rate": 7.555555555555556e-07,
      "loss": 0.0023,
      "step": 88640
    },
    {
      "epoch": 4.925,
      "grad_norm": 0.07092288136482239,
      "learning_rate": 7.5e-07,
      "loss": 0.0028,
      "step": 88650
    },
    {
      "epoch": 4.9255555555555555,
      "grad_norm": 0.03495483100414276,
      "learning_rate": 7.444444444444445e-07,
      "loss": 0.0015,
      "step": 88660
    },
    {
      "epoch": 4.926111111111111,
      "grad_norm": 0.28263047337532043,
      "learning_rate": 7.388888888888889e-07,
      "loss": 0.0026,
      "step": 88670
    },
    {
      "epoch": 4.926666666666667,
      "grad_norm": 0.11916860938072205,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0021,
      "step": 88680
    },
    {
      "epoch": 4.927222222222222,
      "grad_norm": 0.09224376827478409,
      "learning_rate": 7.277777777777778e-07,
      "loss": 0.0018,
      "step": 88690
    },
    {
      "epoch": 4.927777777777778,
      "grad_norm": 0.3926743268966675,
      "learning_rate": 7.222222222222222e-07,
      "loss": 0.003,
      "step": 88700
    },
    {
      "epoch": 4.928333333333334,
      "grad_norm": 0.23961208760738373,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.003,
      "step": 88710
    },
    {
      "epoch": 4.928888888888888,
      "grad_norm": 0.32666343450546265,
      "learning_rate": 7.111111111111112e-07,
      "loss": 0.0019,
      "step": 88720
    },
    {
      "epoch": 4.929444444444444,
      "grad_norm": 0.11901082843542099,
      "learning_rate": 7.055555555555556e-07,
      "loss": 0.0025,
      "step": 88730
    },
    {
      "epoch": 4.93,
      "grad_norm": 0.2166653871536255,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0029,
      "step": 88740
    },
    {
      "epoch": 4.930555555555555,
      "grad_norm": 0.0813150480389595,
      "learning_rate": 6.944444444444445e-07,
      "loss": 0.0021,
      "step": 88750
    },
    {
      "epoch": 4.931111111111111,
      "grad_norm": 0.10963910073041916,
      "learning_rate": 6.888888888888889e-07,
      "loss": 0.0032,
      "step": 88760
    },
    {
      "epoch": 4.931666666666667,
      "grad_norm": 0.4452238380908966,
      "learning_rate": 6.833333333333334e-07,
      "loss": 0.0021,
      "step": 88770
    },
    {
      "epoch": 4.932222222222222,
      "grad_norm": 0.0700143426656723,
      "learning_rate": 6.777777777777778e-07,
      "loss": 0.0023,
      "step": 88780
    },
    {
      "epoch": 4.932777777777778,
      "grad_norm": 0.2611455023288727,
      "learning_rate": 6.722222222222223e-07,
      "loss": 0.003,
      "step": 88790
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.11882446706295013,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0029,
      "step": 88800
    },
    {
      "epoch": 4.933888888888889,
      "grad_norm": 0.25675979256629944,
      "learning_rate": 6.611111111111111e-07,
      "loss": 0.0015,
      "step": 88810
    },
    {
      "epoch": 4.934444444444445,
      "grad_norm": 0.06121063977479935,
      "learning_rate": 6.555555555555556e-07,
      "loss": 0.0026,
      "step": 88820
    },
    {
      "epoch": 4.9350000000000005,
      "grad_norm": 0.298206090927124,
      "learning_rate": 6.5e-07,
      "loss": 0.0022,
      "step": 88830
    },
    {
      "epoch": 4.935555555555555,
      "grad_norm": 0.060593847185373306,
      "learning_rate": 6.444444444444444e-07,
      "loss": 0.0029,
      "step": 88840
    },
    {
      "epoch": 4.936111111111111,
      "grad_norm": 0.20955930650234222,
      "learning_rate": 6.388888888888889e-07,
      "loss": 0.0027,
      "step": 88850
    },
    {
      "epoch": 4.9366666666666665,
      "grad_norm": 0.26888442039489746,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.0037,
      "step": 88860
    },
    {
      "epoch": 4.937222222222222,
      "grad_norm": 0.0458010770380497,
      "learning_rate": 6.277777777777778e-07,
      "loss": 0.0032,
      "step": 88870
    },
    {
      "epoch": 4.937777777777778,
      "grad_norm": 0.17465753853321075,
      "learning_rate": 6.222222222222223e-07,
      "loss": 0.0029,
      "step": 88880
    },
    {
      "epoch": 4.9383333333333335,
      "grad_norm": 0.2081521451473236,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.0024,
      "step": 88890
    },
    {
      "epoch": 4.938888888888889,
      "grad_norm": 0.08939547836780548,
      "learning_rate": 6.111111111111112e-07,
      "loss": 0.0019,
      "step": 88900
    },
    {
      "epoch": 4.939444444444445,
      "grad_norm": 0.11878027021884918,
      "learning_rate": 6.055555555555556e-07,
      "loss": 0.0031,
      "step": 88910
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.17844194173812866,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.0025,
      "step": 88920
    },
    {
      "epoch": 4.940555555555555,
      "grad_norm": 0.12010525166988373,
      "learning_rate": 5.944444444444445e-07,
      "loss": 0.002,
      "step": 88930
    },
    {
      "epoch": 4.941111111111111,
      "grad_norm": 0.12017831206321716,
      "learning_rate": 5.888888888888889e-07,
      "loss": 0.0027,
      "step": 88940
    },
    {
      "epoch": 4.941666666666666,
      "grad_norm": 0.17821455001831055,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.0036,
      "step": 88950
    },
    {
      "epoch": 4.942222222222222,
      "grad_norm": 0.3268570005893707,
      "learning_rate": 5.777777777777778e-07,
      "loss": 0.0014,
      "step": 88960
    },
    {
      "epoch": 4.942777777777778,
      "grad_norm": 0.2671933174133301,
      "learning_rate": 5.722222222222223e-07,
      "loss": 0.0031,
      "step": 88970
    },
    {
      "epoch": 4.943333333333333,
      "grad_norm": 0.032347410917282104,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.0029,
      "step": 88980
    },
    {
      "epoch": 4.943888888888889,
      "grad_norm": 0.03557860478758812,
      "learning_rate": 5.611111111111111e-07,
      "loss": 0.0028,
      "step": 88990
    },
    {
      "epoch": 4.944444444444445,
      "grad_norm": 0.14812582731246948,
      "learning_rate": 5.555555555555556e-07,
      "loss": 0.0026,
      "step": 89000
    },
    {
      "epoch": 4.945,
      "grad_norm": 0.2074912041425705,
      "learning_rate": 5.5e-07,
      "loss": 0.0028,
      "step": 89010
    },
    {
      "epoch": 4.945555555555556,
      "grad_norm": 0.06341231614351273,
      "learning_rate": 5.444444444444444e-07,
      "loss": 0.0026,
      "step": 89020
    },
    {
      "epoch": 4.946111111111112,
      "grad_norm": 0.23769329488277435,
      "learning_rate": 5.388888888888889e-07,
      "loss": 0.0028,
      "step": 89030
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.09016981720924377,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0022,
      "step": 89040
    },
    {
      "epoch": 4.947222222222222,
      "grad_norm": 0.20727410912513733,
      "learning_rate": 5.277777777777779e-07,
      "loss": 0.003,
      "step": 89050
    },
    {
      "epoch": 4.947777777777778,
      "grad_norm": 0.2081422656774521,
      "learning_rate": 5.222222222222223e-07,
      "loss": 0.0021,
      "step": 89060
    },
    {
      "epoch": 4.948333333333333,
      "grad_norm": 0.03274795785546303,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.0024,
      "step": 89070
    },
    {
      "epoch": 4.948888888888889,
      "grad_norm": 0.14881812036037445,
      "learning_rate": 5.111111111111112e-07,
      "loss": 0.0026,
      "step": 89080
    },
    {
      "epoch": 4.9494444444444445,
      "grad_norm": 0.1099756509065628,
      "learning_rate": 5.055555555555556e-07,
      "loss": 0.002,
      "step": 89090
    },
    {
      "epoch": 4.95,
      "grad_norm": 0.2504933774471283,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0031,
      "step": 89100
    },
    {
      "epoch": 4.950555555555556,
      "grad_norm": 0.20735155045986176,
      "learning_rate": 4.944444444444444e-07,
      "loss": 0.0017,
      "step": 89110
    },
    {
      "epoch": 4.9511111111111115,
      "grad_norm": 0.11910480260848999,
      "learning_rate": 4.888888888888889e-07,
      "loss": 0.0019,
      "step": 89120
    },
    {
      "epoch": 4.951666666666666,
      "grad_norm": 0.15057002007961273,
      "learning_rate": 4.833333333333334e-07,
      "loss": 0.0019,
      "step": 89130
    },
    {
      "epoch": 4.952222222222222,
      "grad_norm": 0.3557959794998169,
      "learning_rate": 4.777777777777778e-07,
      "loss": 0.0034,
      "step": 89140
    },
    {
      "epoch": 4.9527777777777775,
      "grad_norm": 0.08907445520162582,
      "learning_rate": 4.7222222222222226e-07,
      "loss": 0.0032,
      "step": 89150
    },
    {
      "epoch": 4.953333333333333,
      "grad_norm": 0.12055317312479019,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.0026,
      "step": 89160
    },
    {
      "epoch": 4.953888888888889,
      "grad_norm": 0.12020296603441238,
      "learning_rate": 4.611111111111111e-07,
      "loss": 0.0024,
      "step": 89170
    },
    {
      "epoch": 4.954444444444444,
      "grad_norm": 0.17825660109519958,
      "learning_rate": 4.555555555555556e-07,
      "loss": 0.0017,
      "step": 89180
    },
    {
      "epoch": 4.955,
      "grad_norm": 0.1220954954624176,
      "learning_rate": 4.5e-07,
      "loss": 0.0031,
      "step": 89190
    },
    {
      "epoch": 4.955555555555556,
      "grad_norm": 0.06043803691864014,
      "learning_rate": 4.444444444444445e-07,
      "loss": 0.0027,
      "step": 89200
    },
    {
      "epoch": 4.956111111111111,
      "grad_norm": 0.03169761598110199,
      "learning_rate": 4.3888888888888895e-07,
      "loss": 0.0023,
      "step": 89210
    },
    {
      "epoch": 4.956666666666667,
      "grad_norm": 0.11917818337678909,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.003,
      "step": 89220
    },
    {
      "epoch": 4.957222222222223,
      "grad_norm": 0.23785842955112457,
      "learning_rate": 4.277777777777778e-07,
      "loss": 0.0031,
      "step": 89230
    },
    {
      "epoch": 4.957777777777777,
      "grad_norm": 0.5753171443939209,
      "learning_rate": 4.222222222222222e-07,
      "loss": 0.0021,
      "step": 89240
    },
    {
      "epoch": 4.958333333333333,
      "grad_norm": 0.4149792492389679,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.0023,
      "step": 89250
    },
    {
      "epoch": 4.958888888888889,
      "grad_norm": 0.23695923388004303,
      "learning_rate": 4.111111111111112e-07,
      "loss": 0.0018,
      "step": 89260
    },
    {
      "epoch": 4.959444444444444,
      "grad_norm": 0.4278579354286194,
      "learning_rate": 4.055555555555555e-07,
      "loss": 0.0029,
      "step": 89270
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.23681889474391937,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.0026,
      "step": 89280
    },
    {
      "epoch": 4.960555555555556,
      "grad_norm": 0.11916733533143997,
      "learning_rate": 3.944444444444445e-07,
      "loss": 0.0019,
      "step": 89290
    },
    {
      "epoch": 4.961111111111111,
      "grad_norm": 0.27735018730163574,
      "learning_rate": 3.888888888888889e-07,
      "loss": 0.0023,
      "step": 89300
    },
    {
      "epoch": 4.961666666666667,
      "grad_norm": 0.41542115807533264,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 0.0027,
      "step": 89310
    },
    {
      "epoch": 4.9622222222222225,
      "grad_norm": 0.11937188357114792,
      "learning_rate": 3.777777777777778e-07,
      "loss": 0.0024,
      "step": 89320
    },
    {
      "epoch": 4.962777777777777,
      "grad_norm": 0.33256784081459045,
      "learning_rate": 3.7222222222222226e-07,
      "loss": 0.0015,
      "step": 89330
    },
    {
      "epoch": 4.963333333333333,
      "grad_norm": 0.11928096413612366,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.0023,
      "step": 89340
    },
    {
      "epoch": 4.963888888888889,
      "grad_norm": 0.1746651828289032,
      "learning_rate": 3.611111111111111e-07,
      "loss": 0.0026,
      "step": 89350
    },
    {
      "epoch": 4.964444444444444,
      "grad_norm": 0.06033530831336975,
      "learning_rate": 3.555555555555556e-07,
      "loss": 0.003,
      "step": 89360
    },
    {
      "epoch": 4.965,
      "grad_norm": 0.35148999094963074,
      "learning_rate": 3.5000000000000004e-07,
      "loss": 0.0037,
      "step": 89370
    },
    {
      "epoch": 4.9655555555555555,
      "grad_norm": 0.0599200613796711,
      "learning_rate": 3.4444444444444444e-07,
      "loss": 0.0027,
      "step": 89380
    },
    {
      "epoch": 4.966111111111111,
      "grad_norm": 0.060476597398519516,
      "learning_rate": 3.388888888888889e-07,
      "loss": 0.0022,
      "step": 89390
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 0.1778566688299179,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.0032,
      "step": 89400
    },
    {
      "epoch": 4.967222222222222,
      "grad_norm": 0.10641889274120331,
      "learning_rate": 3.277777777777778e-07,
      "loss": 0.0025,
      "step": 89410
    },
    {
      "epoch": 4.967777777777778,
      "grad_norm": 0.23073554039001465,
      "learning_rate": 3.222222222222222e-07,
      "loss": 0.0017,
      "step": 89420
    },
    {
      "epoch": 4.968333333333334,
      "grad_norm": 0.5634722709655762,
      "learning_rate": 3.1666666666666667e-07,
      "loss": 0.0021,
      "step": 89430
    },
    {
      "epoch": 4.968888888888889,
      "grad_norm": 0.11902853101491928,
      "learning_rate": 3.111111111111111e-07,
      "loss": 0.0028,
      "step": 89440
    },
    {
      "epoch": 4.969444444444444,
      "grad_norm": 0.25865620374679565,
      "learning_rate": 3.055555555555556e-07,
      "loss": 0.0024,
      "step": 89450
    },
    {
      "epoch": 4.97,
      "grad_norm": 0.3078768849372864,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.0024,
      "step": 89460
    },
    {
      "epoch": 4.970555555555555,
      "grad_norm": 0.18251127004623413,
      "learning_rate": 2.9444444444444444e-07,
      "loss": 0.0019,
      "step": 89470
    },
    {
      "epoch": 4.971111111111111,
      "grad_norm": 0.32543668150901794,
      "learning_rate": 2.888888888888889e-07,
      "loss": 0.0027,
      "step": 89480
    },
    {
      "epoch": 4.971666666666667,
      "grad_norm": 0.15688984096050262,
      "learning_rate": 2.8333333333333336e-07,
      "loss": 0.0021,
      "step": 89490
    },
    {
      "epoch": 4.972222222222222,
      "grad_norm": 0.2374286949634552,
      "learning_rate": 2.777777777777778e-07,
      "loss": 0.0031,
      "step": 89500
    }
  ],
  "logging_steps": 10,
  "max_steps": 90000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9733333333333334,
  "eval_steps": 500,
  "global_step": 37000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 1.059812068939209,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0072,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.6069688200950623,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0077,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.7446404099464417,
      "learning_rate": 4.996e-05,
      "loss": 0.0062,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 1.056039810180664,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0036,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.4996369183063507,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0038,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.7692309021949768,
      "learning_rate": 4.992e-05,
      "loss": 0.0041,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.28987646102905273,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0029,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.2602183222770691,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0034,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.8706716299057007,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0035,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.5319051146507263,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0026,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.3709770143032074,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0026,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.5119967460632324,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0025,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.3244156539440155,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.0023,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.5485076904296875,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0031,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.28672847151756287,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0024,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.6820136308670044,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0031,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.21936504542827606,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.0022,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.17908428609371185,
      "learning_rate": 4.976e-05,
      "loss": 0.0029,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 1.1639606952667236,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0028,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.8374425768852234,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0027,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 1.1783438920974731,
      "learning_rate": 4.972e-05,
      "loss": 0.0024,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.7172598242759705,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0028,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.6773176193237305,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0029,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.46428555250167847,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.002,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.3020167052745819,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0022,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.5240236520767212,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.0025,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.5521891117095947,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.003,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.25291913747787476,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0029,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.05036431923508644,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0021,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.36717548966407776,
      "learning_rate": 4.96e-05,
      "loss": 0.0024,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.182940274477005,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0025,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.15219427645206451,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0021,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.9656050801277161,
      "learning_rate": 4.956e-05,
      "loss": 0.0028,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.2844858467578888,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0032,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.5029634237289429,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0023,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.4644206762313843,
      "learning_rate": 4.952e-05,
      "loss": 0.0029,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.11386203020811081,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0029,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.9232047200202942,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0034,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.9603371620178223,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0029,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.42436715960502625,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0026,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.3883577883243561,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0018,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.34745851159095764,
      "learning_rate": 4.944e-05,
      "loss": 0.0026,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.5633381605148315,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0016,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.21910670399665833,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0018,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.47362396121025085,
      "learning_rate": 4.94e-05,
      "loss": 0.0012,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.0804646834731102,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0017,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.1557466685771942,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0021,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.3118736445903778,
      "learning_rate": 4.936e-05,
      "loss": 0.0011,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.28862830996513367,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0021,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.35993558168411255,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0013,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.18976044654846191,
      "learning_rate": 4.932e-05,
      "loss": 0.0016,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.38255444169044495,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0019,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.21496661007404327,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0021,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.283731073141098,
      "learning_rate": 4.928e-05,
      "loss": 0.0017,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.2829826772212982,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0014,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.25388747453689575,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0023,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 1.3472404479980469,
      "learning_rate": 4.924e-05,
      "loss": 0.0029,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.44829466938972473,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0024,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.5176795125007629,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0025,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.23082014918327332,
      "learning_rate": 4.92e-05,
      "loss": 0.0014,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.5199032425880432,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.0022,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.7644081711769104,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0015,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.2459067553281784,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0026,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.35484665632247925,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0026,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.37939944863319397,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0017,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.41709381341934204,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.002,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.2469605803489685,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.0013,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.28505972027778625,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.002,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.11421292275190353,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0018,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.861753523349762,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0028,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.2924383580684662,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0017,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.22565090656280518,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.0015,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.17601941525936127,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0016,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.31049102544784546,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0013,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.17051883041858673,
      "learning_rate": 4.9e-05,
      "loss": 0.0025,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.18275073170661926,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0022,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.16531053185462952,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0021,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.39541342854499817,
      "learning_rate": 4.896e-05,
      "loss": 0.0028,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.24790582060813904,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.0017,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.3704140782356262,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0021,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.41217881441116333,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.0022,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.9578334093093872,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.0018,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.1133781224489212,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0019,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.3267688751220703,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0018,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.8055340051651001,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0021,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.15378884971141815,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0019,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.469332754611969,
      "learning_rate": 4.884e-05,
      "loss": 0.0017,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.5254361629486084,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0029,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.3022765815258026,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0019,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.43320581316947937,
      "learning_rate": 4.88e-05,
      "loss": 0.0016,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.3057672381401062,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0021,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.516840398311615,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0022,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.27872905135154724,
      "learning_rate": 4.876e-05,
      "loss": 0.0021,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.12040306627750397,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0014,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.2491835653781891,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0023,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.5526683926582336,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.002,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.3612803518772125,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0019,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.5290868282318115,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0019,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.24702806770801544,
      "learning_rate": 4.868e-05,
      "loss": 0.0021,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.6063419580459595,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0012,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.2323961704969406,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.0013,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.2317373901605606,
      "learning_rate": 4.864e-05,
      "loss": 0.0017,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.19316856563091278,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0016,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.45132750272750854,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.0018,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.11163822561502457,
      "learning_rate": 4.86e-05,
      "loss": 0.002,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.41910332441329956,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0023,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.47229957580566406,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.002,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.1761588156223297,
      "learning_rate": 4.856e-05,
      "loss": 0.0015,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.39197733998298645,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0014,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.5323113799095154,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.002,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.3042120635509491,
      "learning_rate": 4.852e-05,
      "loss": 0.0016,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.15959331393241882,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0028,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.09275304526090622,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.002,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.4758303463459015,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0021,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.4783317446708679,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0018,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.3641161620616913,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0019,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.09186884015798569,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0024,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.6422461271286011,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0021,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.5234509110450745,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0018,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.2936343252658844,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0021,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.13857533037662506,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0017,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.22331006824970245,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0019,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.22561727464199066,
      "learning_rate": 4.836e-05,
      "loss": 0.0022,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.17069923877716064,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.0018,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.2121584415435791,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0018,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.25169822573661804,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0009,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.26193681359291077,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0023,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.31967952847480774,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0018,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.3213638961315155,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0023,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.3917107880115509,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.002,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.15244610607624054,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0027,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.354848712682724,
      "learning_rate": 4.824e-05,
      "loss": 0.0018,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.2921662926673889,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0022,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.3493441641330719,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0011,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.38676732778549194,
      "learning_rate": 4.82e-05,
      "loss": 0.0019,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.5464226603507996,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.002,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.26970505714416504,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0022,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.5114212036132812,
      "learning_rate": 4.816e-05,
      "loss": 0.0015,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.16592733561992645,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0025,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.15641170740127563,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0017,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.0,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0012,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.2529388666152954,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0029,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.3434649705886841,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0015,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.1629551500082016,
      "learning_rate": 4.808e-05,
      "loss": 0.0015,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.18278992176055908,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0017,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.05576279014348984,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0025,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.6243715286254883,
      "learning_rate": 4.804e-05,
      "loss": 0.0023,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.23689274489879608,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0021,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.07417382299900055,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0013,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.25672295689582825,
      "learning_rate": 4.8e-05,
      "loss": 0.0024,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.24438492953777313,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.0013,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.2679620683193207,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0014,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.09255179762840271,
      "learning_rate": 4.796e-05,
      "loss": 0.0011,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.22759345173835754,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0017,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.07157821953296661,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.002,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.34477442502975464,
      "learning_rate": 4.792e-05,
      "loss": 0.0024,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 0.1737586110830307,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0017,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.2591981887817383,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0029,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.3658919632434845,
      "learning_rate": 4.788e-05,
      "loss": 0.0017,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.20184294879436493,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0027,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.18554562330245972,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0012,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.1398981362581253,
      "learning_rate": 4.784e-05,
      "loss": 0.0017,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.14325639605522156,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0021,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.6633538603782654,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0014,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.1583012342453003,
      "learning_rate": 4.78e-05,
      "loss": 0.0014,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.20379674434661865,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0023,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.39779919385910034,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.0016,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.31983548402786255,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0026,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.2920391261577606,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0022,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.10030566900968552,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0011,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.13409751653671265,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0013,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.3054955303668976,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0015,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.2401193231344223,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.0015,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.3079219162464142,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0015,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.1557205468416214,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0013,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.3686986267566681,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0015,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.455685555934906,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0022,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.14539700746536255,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0021,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.4878537952899933,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0021,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.2933332324028015,
      "learning_rate": 4.76e-05,
      "loss": 0.0017,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.22484910488128662,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0028,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.16817627847194672,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0018,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.6149879097938538,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0012,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.4343946576118469,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.002,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.23681436479091644,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0006,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.06849811971187592,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0016,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0013,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.12056498974561691,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0017,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.29116472601890564,
      "learning_rate": 4.748e-05,
      "loss": 0.0015,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.17899799346923828,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0025,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.18752093613147736,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.002,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.5097025036811829,
      "learning_rate": 4.744e-05,
      "loss": 0.0024,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.16764968633651733,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0016,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.11850914359092712,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0018,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.1932068020105362,
      "learning_rate": 4.74e-05,
      "loss": 0.0016,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.3371942937374115,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.0012,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.21148350834846497,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0016,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.18039532005786896,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0014,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.23483173549175262,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0016,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.30981263518333435,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0025,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.2822193205356598,
      "learning_rate": 4.732e-05,
      "loss": 0.0024,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.25270771980285645,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0019,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.2613469660282135,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0026,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.1924182027578354,
      "learning_rate": 4.728e-05,
      "loss": 0.0017,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.28839656710624695,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0017,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.6111916303634644,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0023,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.2694184184074402,
      "learning_rate": 4.724e-05,
      "loss": 0.0021,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.22845809161663055,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.002,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.16132232546806335,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0031,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.4514246881008148,
      "learning_rate": 4.72e-05,
      "loss": 0.0012,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.1171804666519165,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0021,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.33296486735343933,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.0012,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.25157496333122253,
      "learning_rate": 4.716e-05,
      "loss": 0.0023,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.10564132034778595,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0019,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.15639378130435944,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0021,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.20102649927139282,
      "learning_rate": 4.712e-05,
      "loss": 0.0018,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.23066844046115875,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0014,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.15607985854148865,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0019,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.706801176071167,
      "learning_rate": 4.708e-05,
      "loss": 0.0023,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.2839409112930298,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0025,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.22543585300445557,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0019,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.12054562568664551,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.0027,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.30958402156829834,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.0022,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.1978500932455063,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.002,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.4153967499732971,
      "learning_rate": 4.7e-05,
      "loss": 0.0011,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.3056415319442749,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0022,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.1900501847267151,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.001,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.10936478525400162,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0014,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.14452989399433136,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.0026,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.33433398604393005,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0015,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.4123678207397461,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0024,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.47735610604286194,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0015,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.2371882051229477,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0023,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.3710820972919464,
      "learning_rate": 4.688e-05,
      "loss": 0.0011,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.295331746339798,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0013,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.16506575047969818,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.0026,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.44504037499427795,
      "learning_rate": 4.684e-05,
      "loss": 0.0027,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.323036253452301,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0018,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.23778608441352844,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0024,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.6650975346565247,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.002,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.20032799243927002,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0022,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.23351693153381348,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0021,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.22246715426445007,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0026,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.3532465994358063,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0021,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.25017720460891724,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0015,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.2347354143857956,
      "learning_rate": 4.672e-05,
      "loss": 0.0025,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.2652879059314728,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.0025,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.35517677664756775,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0011,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.335516095161438,
      "learning_rate": 4.668e-05,
      "loss": 0.0022,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.4953628480434418,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0017,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.8341356515884399,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0017,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.41875624656677246,
      "learning_rate": 4.664e-05,
      "loss": 0.0024,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.32559841871261597,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0036,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.21307416260242462,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0018,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.20262356102466583,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0016,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.2645806670188904,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0016,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.22104521095752716,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0018,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.5893562436103821,
      "learning_rate": 4.656e-05,
      "loss": 0.0035,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.3631651997566223,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0017,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.3291851282119751,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0016,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.16016851365566254,
      "learning_rate": 4.652e-05,
      "loss": 0.0025,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.28620341420173645,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0018,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.14048053324222565,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.0021,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.1989734023809433,
      "learning_rate": 4.648e-05,
      "loss": 0.0017,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.20204854011535645,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0018,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.1734599769115448,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.0016,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.2029668092727661,
      "learning_rate": 4.644e-05,
      "loss": 0.0012,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.17669515311717987,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0017,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.4110761880874634,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0016,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.27882158756256104,
      "learning_rate": 4.64e-05,
      "loss": 0.0015,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.23189300298690796,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0021,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.2015679031610489,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0022,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.19366976618766785,
      "learning_rate": 4.636e-05,
      "loss": 0.0017,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.07818954437971115,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0013,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.3529379069805145,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0012,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.6208836436271667,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0019,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.1628505289554596,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.0016,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.23830243945121765,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0016,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.3548828959465027,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0023,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.18947620689868927,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0017,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.5161760449409485,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0022,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.21715526282787323,
      "learning_rate": 4.624e-05,
      "loss": 0.0023,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.1912698745727539,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.0017,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.2914075255393982,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.0023,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.159600168466568,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0021,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.18122485280036926,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0021,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.13589482009410858,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0015,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.18401677906513214,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.001,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.22351129353046417,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.0021,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.6584680080413818,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0016,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.3230162560939789,
      "learning_rate": 4.612e-05,
      "loss": 0.0019,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.30018559098243713,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0018,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.8110292553901672,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0026,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.4199329912662506,
      "learning_rate": 4.608e-05,
      "loss": 0.0032,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.43091028928756714,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0016,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.24120143055915833,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0015,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.23292797803878784,
      "learning_rate": 4.604e-05,
      "loss": 0.002,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.40812426805496216,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0013,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.10830030590295792,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0013,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.32615751028060913,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0019,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.19427122175693512,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0022,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.29885420203208923,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0011,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.08567483723163605,
      "learning_rate": 4.596e-05,
      "loss": 0.0011,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.3071555495262146,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0012,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.12341627478599548,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0013,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.14150981605052948,
      "learning_rate": 4.592e-05,
      "loss": 0.0016,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.45346546173095703,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.0019,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.17020124197006226,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.001,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.2578687369823456,
      "learning_rate": 4.588e-05,
      "loss": 0.0014,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.2781873345375061,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0009,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.18764498829841614,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0021,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.17840340733528137,
      "learning_rate": 4.584e-05,
      "loss": 0.0023,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.2613335847854614,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.0014,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.2795170545578003,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0014,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.13960686326026917,
      "learning_rate": 4.58e-05,
      "loss": 0.0031,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.1561322957277298,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0008,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.18546076118946075,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0015,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.18072234094142914,
      "learning_rate": 4.576e-05,
      "loss": 0.0017,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.21945081651210785,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0018,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.29334068298339844,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0022,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.17011067271232605,
      "learning_rate": 4.572e-05,
      "loss": 0.0015,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.35437583923339844,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0019,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.35968685150146484,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0021,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.21423505246639252,
      "learning_rate": 4.568e-05,
      "loss": 0.0023,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.3777692914009094,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0023,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.20650871098041534,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.0019,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.14903314411640167,
      "learning_rate": 4.564e-05,
      "loss": 0.0012,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.06882745027542114,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0016,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.11931084841489792,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0013,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.2879886031150818,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0029,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.1259537786245346,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0019,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.48359474539756775,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0019,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.27738580107688904,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0023,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.676454484462738,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0023,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.2880755066871643,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.002,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.14596684277057648,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0022,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.1626075953245163,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0023,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.4583325684070587,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0014,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.4169618785381317,
      "learning_rate": 4.548e-05,
      "loss": 0.0012,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.13254232704639435,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0012,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.09242498129606247,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0026,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.7979061603546143,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0029,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.25486958026885986,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0019,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.14616386592388153,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0017,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.3882778584957123,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.001,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.11034524440765381,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0011,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.1544569879770279,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.0019,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.1725768744945526,
      "learning_rate": 4.536e-05,
      "loss": 0.0022,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.5575828552246094,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0017,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.6473851203918457,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.002,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.49438297748565674,
      "learning_rate": 4.532e-05,
      "loss": 0.0023,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.28685006499290466,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0016,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.13619717955589294,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0021,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.22300556302070618,
      "learning_rate": 4.528e-05,
      "loss": 0.0012,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.20902884006500244,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0018,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.14731355011463165,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0016,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.13938961923122406,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0009,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.7391897439956665,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0021,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.43980133533477783,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0009,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.4368399381637573,
      "learning_rate": 4.52e-05,
      "loss": 0.0014,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.23346169292926788,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.0022,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.25158947706222534,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.0024,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.42114365100860596,
      "learning_rate": 4.516e-05,
      "loss": 0.0023,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.11742344498634338,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0017,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.20672312378883362,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0017,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.3180656433105469,
      "learning_rate": 4.512e-05,
      "loss": 0.0026,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.11864060163497925,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.002,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.1994510293006897,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0013,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.1410149782896042,
      "learning_rate": 4.508e-05,
      "loss": 0.0009,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.0999191403388977,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0014,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.3580726087093353,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.002,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.29778966307640076,
      "learning_rate": 4.504e-05,
      "loss": 0.0022,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.16463804244995117,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0011,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.1220826581120491,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.0017,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.14622923731803894,
      "learning_rate": 4.5e-05,
      "loss": 0.0014,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0015,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.17111894488334656,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0017,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.18846985697746277,
      "learning_rate": 4.496e-05,
      "loss": 0.0014,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.08448649197816849,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0021,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.25107699632644653,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0024,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.1446419656276703,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0009,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.2686622440814972,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0014,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.25515562295913696,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.002,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.1303016096353531,
      "learning_rate": 4.488e-05,
      "loss": 0.0012,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.1337793618440628,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0018,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.18772146105766296,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.0012,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.2430673986673355,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0029,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.1507367491722107,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0018,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.3471932113170624,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0014,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.17278806865215302,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0023,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.2754209339618683,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0027,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.19309712946414948,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0015,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.15604567527770996,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0016,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.32575252652168274,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0022,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.39640581607818604,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.001,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.10144250094890594,
      "learning_rate": 4.472e-05,
      "loss": 0.002,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.19964191317558289,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.0009,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.14455638825893402,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0012,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.37107235193252563,
      "learning_rate": 4.468e-05,
      "loss": 0.0025,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.6216257214546204,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0016,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.3796056807041168,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0012,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.6241713762283325,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.0017,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.7111450433731079,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.0012,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.3708886504173279,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0023,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.5928679704666138,
      "learning_rate": 4.46e-05,
      "loss": 0.0016,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.26732566952705383,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0019,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.34181728959083557,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0023,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.5016070604324341,
      "learning_rate": 4.456e-05,
      "loss": 0.0021,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.6924854516983032,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0018,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.5440824031829834,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0016,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.3992495834827423,
      "learning_rate": 4.452e-05,
      "loss": 0.0018,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.12700997292995453,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0016,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.10934843122959137,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.002,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.26662832498550415,
      "learning_rate": 4.448e-05,
      "loss": 0.0023,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.36894354224205017,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0018,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.30160045623779297,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0014,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.16812896728515625,
      "learning_rate": 4.444e-05,
      "loss": 0.0013,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.13379846513271332,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.0013,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.2129662036895752,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0012,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.20366348326206207,
      "learning_rate": 4.44e-05,
      "loss": 0.0018,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.18877455592155457,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.0021,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.3004581928253174,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0013,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.20871640741825104,
      "learning_rate": 4.436e-05,
      "loss": 0.0016,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.153203547000885,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.0016,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.14095646142959595,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0013,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.14009450376033783,
      "learning_rate": 4.432e-05,
      "loss": 0.0018,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.4065653383731842,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0018,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.2160891443490982,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0014,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.4110742509365082,
      "learning_rate": 4.428e-05,
      "loss": 0.0024,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.285643994808197,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0016,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.2658282518386841,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0011,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.1283174306154251,
      "learning_rate": 4.424e-05,
      "loss": 0.0016,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.14942975342273712,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0015,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.8425927758216858,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0016,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.22184084355831146,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0019,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.2621639370918274,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.0014,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.5141098499298096,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0013,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.3031558096408844,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0017,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.2599395215511322,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0014,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.3734759986400604,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0019,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.21064314246177673,
      "learning_rate": 4.412e-05,
      "loss": 0.0028,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.22796617448329926,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.0016,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.4385080933570862,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.0028,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.25633180141448975,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.002,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.11960741132497787,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.003,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.17634117603302002,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0024,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.20091408491134644,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0014,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.24855771660804749,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0009,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.08777058869600296,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0014,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.17746601998806,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0019,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.15771302580833435,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0012,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.5270122289657593,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0022,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.485855370759964,
      "learning_rate": 4.396e-05,
      "loss": 0.0012,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.13058899343013763,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0008,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.3621373772621155,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0018,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.5870986580848694,
      "learning_rate": 4.392e-05,
      "loss": 0.0011,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.24669614434242249,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0016,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.06893865764141083,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.001,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.2960718870162964,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0011,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.1137482076883316,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.002,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.19173091650009155,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0013,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.1525202989578247,
      "learning_rate": 4.384e-05,
      "loss": 0.0009,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.2526216208934784,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0008,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.08858760446310043,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.0022,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.1140921413898468,
      "learning_rate": 4.38e-05,
      "loss": 0.0013,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.1136901006102562,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0019,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.12520472705364227,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0021,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.159373939037323,
      "learning_rate": 4.376e-05,
      "loss": 0.0013,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.13424280285835266,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0017,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.1547073870897293,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0013,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.177095428109169,
      "learning_rate": 4.372e-05,
      "loss": 0.0028,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.297575980424881,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0006,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.1512729376554489,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0012,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.22057421505451202,
      "learning_rate": 4.368e-05,
      "loss": 0.0018,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.738353967666626,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0018,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.30403077602386475,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0014,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.23505091667175293,
      "learning_rate": 4.364e-05,
      "loss": 0.0008,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.06352463364601135,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0012,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.1476714015007019,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0024,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.08468981087207794,
      "learning_rate": 4.36e-05,
      "loss": 0.0012,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.1450888067483902,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0015,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.30562925338745117,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0017,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.11755792796611786,
      "learning_rate": 4.356e-05,
      "loss": 0.0018,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.24350227415561676,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0018,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0013,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.12435164302587509,
      "learning_rate": 4.352e-05,
      "loss": 0.0009,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.1608724445104599,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.002,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.295939177274704,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0019,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.2737772464752197,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0036,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.4973556399345398,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0015,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.19507922232151031,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0021,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.0,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0015,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.26175206899642944,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0017,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.367221862077713,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0014,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.3053324818611145,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0023,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.2324633002281189,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0012,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.33591902256011963,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.0017,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.25236666202545166,
      "learning_rate": 4.336e-05,
      "loss": 0.0024,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.13819366693496704,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0014,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.06790929287672043,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0013,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.0,
      "learning_rate": 4.332e-05,
      "loss": 0.0011,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.5830972194671631,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0015,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.2265767604112625,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.0021,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.25726318359375,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.0026,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.3742733597755432,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0025,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.35156315565109253,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0021,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.2872534692287445,
      "learning_rate": 4.324e-05,
      "loss": 0.0027,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.2921535074710846,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0013,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.17255862057209015,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0014,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.12925322353839874,
      "learning_rate": 4.32e-05,
      "loss": 0.0019,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.34798330068588257,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.001,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.2110978364944458,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0018,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.17134641110897064,
      "learning_rate": 4.316e-05,
      "loss": 0.0022,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.22865700721740723,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0013,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.29963919520378113,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0012,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.17563672363758087,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0015,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.31907227635383606,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0021,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.33924806118011475,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0018,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.13620825111865997,
      "learning_rate": 4.308e-05,
      "loss": 0.0016,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.18590328097343445,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0012,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.2377329170703888,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0019,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.2315918207168579,
      "learning_rate": 4.304e-05,
      "loss": 0.0013,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.14758455753326416,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0012,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.314441055059433,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0016,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.1688186079263687,
      "learning_rate": 4.3e-05,
      "loss": 0.0026,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.15666107833385468,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.0007,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.352636456489563,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0021,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.12512770295143127,
      "learning_rate": 4.296e-05,
      "loss": 0.0016,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.09137504547834396,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0009,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.5022721290588379,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0017,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.30846095085144043,
      "learning_rate": 4.292e-05,
      "loss": 0.0014,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.13299313187599182,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0015,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.29416507482528687,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0018,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.16963978111743927,
      "learning_rate": 4.288e-05,
      "loss": 0.0014,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.3754965662956238,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0016,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.12252622097730637,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0022,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.314635306596756,
      "learning_rate": 4.284e-05,
      "loss": 0.0013,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.28872549533843994,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0018,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.12381338328123093,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0016,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.4270872473716736,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0013,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.14085988700389862,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0013,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.1111493855714798,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0022,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.38519662618637085,
      "learning_rate": 4.276e-05,
      "loss": 0.0014,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.22170913219451904,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0015,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.20664438605308533,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0016,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.06007973477244377,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.001,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.14808928966522217,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.0015,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.47645705938339233,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0015,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.11924175918102264,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0016,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.10884977132081985,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.001,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.09869443625211716,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0021,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.30584123730659485,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.0013,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.15083979070186615,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0014,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.5411344170570374,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0019,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.3518214523792267,
      "learning_rate": 4.26e-05,
      "loss": 0.0019,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.27594125270843506,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0011,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.16474425792694092,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0012,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.23785565793514252,
      "learning_rate": 4.256e-05,
      "loss": 0.0013,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.24340634047985077,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0014,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.33000367879867554,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0012,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.9492914080619812,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0015,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.07275481522083282,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0014,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.36906781792640686,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0018,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.1257559061050415,
      "learning_rate": 4.248e-05,
      "loss": 0.0012,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.29608312249183655,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0013,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.26038259267807007,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.0022,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.27135854959487915,
      "learning_rate": 4.244e-05,
      "loss": 0.0017,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.20816387236118317,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0012,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.29092103242874146,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0012,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.2402462363243103,
      "learning_rate": 4.24e-05,
      "loss": 0.0018,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.10294922441244125,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0009,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 0.2821531593799591,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0024,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.44144129753112793,
      "learning_rate": 4.236e-05,
      "loss": 0.0018,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.18138140439987183,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0011,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.5898331999778748,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0013,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.07931490242481232,
      "learning_rate": 4.232e-05,
      "loss": 0.0021,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.1540512591600418,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0017,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.14298896491527557,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0013,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.13835276663303375,
      "learning_rate": 4.228e-05,
      "loss": 0.0012,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.2729833722114563,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0018,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.42632046341896057,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0017,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.3271445035934448,
      "learning_rate": 4.224e-05,
      "loss": 0.0016,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.3817877471446991,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0019,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.176996648311615,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0021,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.2673482298851013,
      "learning_rate": 4.22e-05,
      "loss": 0.0017,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.26212450861930847,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0025,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.15909360349178314,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0009,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.16686655580997467,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0015,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.2114846110343933,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0016,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.2363010048866272,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0023,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.33438053727149963,
      "learning_rate": 4.212e-05,
      "loss": 0.0009,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.48445096611976624,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0013,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.3530465066432953,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0008,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.4895651340484619,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.001,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.2833331227302551,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0013,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.38324227929115295,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0015,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.3323636054992676,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0021,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.526960015296936,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.0026,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.18980169296264648,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0014,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.12097186595201492,
      "learning_rate": 4.2e-05,
      "loss": 0.0013,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.4256748855113983,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0019,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.2810082733631134,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0017,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.20082078874111176,
      "learning_rate": 4.196e-05,
      "loss": 0.001,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.38636791706085205,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0016,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.33126822113990784,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0012,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.11027706414461136,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0011,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.35957905650138855,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0027,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.310014009475708,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0011,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.0552724152803421,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0013,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.46999311447143555,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0016,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.6725565195083618,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0019,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.4516756534576416,
      "learning_rate": 4.184e-05,
      "loss": 0.0021,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.4746604561805725,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0016,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.3823752999305725,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0011,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.29059654474258423,
      "learning_rate": 4.18e-05,
      "loss": 0.0012,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.2981984615325928,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0014,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.21475858986377716,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0016,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.18147951364517212,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.0017,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.2500811219215393,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0015,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.2729334235191345,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0024,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.30623647570610046,
      "learning_rate": 4.172e-05,
      "loss": 0.0015,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.4149118661880493,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0024,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.07213085889816284,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0015,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.14524388313293457,
      "learning_rate": 4.168e-05,
      "loss": 0.0008,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.1233275830745697,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0021,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.25678813457489014,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0009,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.28840309381484985,
      "learning_rate": 4.164e-05,
      "loss": 0.0018,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.1617593765258789,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0022,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.3753912150859833,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.002,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1833735704421997,
      "learning_rate": 4.16e-05,
      "loss": 0.0021,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.5757991075515747,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0021,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.3308732509613037,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0021,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.3146064579486847,
      "learning_rate": 4.156e-05,
      "loss": 0.0006,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.25027257204055786,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0015,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.5361234545707703,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0023,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.11704067885875702,
      "learning_rate": 4.152e-05,
      "loss": 0.0015,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.13331137597560883,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0005,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.4455726742744446,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0015,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.1544598489999771,
      "learning_rate": 4.148e-05,
      "loss": 0.0013,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.3580586016178131,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0019,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.1615557223558426,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.0016,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.20086751878261566,
      "learning_rate": 4.144e-05,
      "loss": 0.0018,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.3069019317626953,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0005,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.052137069404125214,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0007,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.18146763741970062,
      "learning_rate": 4.14e-05,
      "loss": 0.0024,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.1499941349029541,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0007,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.344164103269577,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0012,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.24480438232421875,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0021,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.09152531623840332,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.0017,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.44307637214660645,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0019,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.2295004278421402,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.0017,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.08207698166370392,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0015,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.7460024356842041,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.0021,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.49256396293640137,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0018,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.10028526186943054,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.002,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.3859323263168335,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0021,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.3658202290534973,
      "learning_rate": 4.124e-05,
      "loss": 0.0011,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.23575620353221893,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0017,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.1699935495853424,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.0017,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.2548026144504547,
      "learning_rate": 4.12e-05,
      "loss": 0.0016,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.14264316856861115,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.0015,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.31454208493232727,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.0015,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.27571210265159607,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0014,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.4401112496852875,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.0012,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.4462226927280426,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0024,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.22207678854465485,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0014,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.34963440895080566,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0017,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.23167802393436432,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.0021,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.18809746205806732,
      "learning_rate": 4.108e-05,
      "loss": 0.0014,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.4366428554058075,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0027,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.22061040997505188,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0015,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.1424335241317749,
      "learning_rate": 4.104e-05,
      "loss": 0.0013,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.16337953507900238,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0007,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.13402733206748962,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0009,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.23462960124015808,
      "learning_rate": 4.1e-05,
      "loss": 0.001,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.3751049041748047,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0017,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.22619666159152985,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.0015,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.2625161409378052,
      "learning_rate": 4.096e-05,
      "loss": 0.0016,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.20583279430866241,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0012,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.28518882393836975,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0011,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.23960168659687042,
      "learning_rate": 4.092e-05,
      "loss": 0.0013,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.1989487260580063,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0014,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.31046557426452637,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.002,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.28217267990112305,
      "learning_rate": 4.088e-05,
      "loss": 0.0021,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.46355003118515015,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0017,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.3168416917324066,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0017,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.7561533451080322,
      "learning_rate": 4.084e-05,
      "loss": 0.0015,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.2851845920085907,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0006,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.7516540288925171,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0017,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.5271733999252319,
      "learning_rate": 4.08e-05,
      "loss": 0.0014,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.12987922132015228,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0013,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.2035134881734848,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0018,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.09891755133867264,
      "learning_rate": 4.076e-05,
      "loss": 0.0017,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.16253367066383362,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.0012,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.5585610866546631,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0015,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.35147929191589355,
      "learning_rate": 4.072e-05,
      "loss": 0.0016,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.10241913795471191,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0016,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.1354226917028427,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.0014,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.17223405838012695,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0017,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.17060048878192902,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.001,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.202396422624588,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0011,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.3457490801811218,
      "learning_rate": 4.064e-05,
      "loss": 0.0014,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.26078346371650696,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0022,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.07082104682922363,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0013,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.49014371633529663,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0011,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.24364639818668365,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0016,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.247825026512146,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0016,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.1771421730518341,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.002,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.4348696172237396,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0016,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.20853443443775177,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0021,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.06396643072366714,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.0014,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.5305231809616089,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0012,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.6529163718223572,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0013,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.3456515371799469,
      "learning_rate": 4.048e-05,
      "loss": 0.0021,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.40937340259552,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0011,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.16427414119243622,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0013,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.21613693237304688,
      "learning_rate": 4.044e-05,
      "loss": 0.0009,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.18650296330451965,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0008,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.1940654069185257,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0015,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.21231000125408173,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.001,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.12816119194030762,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0024,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.3401321768760681,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0015,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.47433406114578247,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.0016,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.13322439789772034,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0009,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.2540617883205414,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0011,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.2493206113576889,
      "learning_rate": 4.032e-05,
      "loss": 0.002,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.20937898755073547,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0022,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.659763753414154,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0017,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.5349336266517639,
      "learning_rate": 4.028e-05,
      "loss": 0.0011,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.13511468470096588,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0013,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.16206254065036774,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0016,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.4127085208892822,
      "learning_rate": 4.024e-05,
      "loss": 0.0018,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.7981961369514465,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0023,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.2418251931667328,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0021,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.21430547535419464,
      "learning_rate": 4.02e-05,
      "loss": 0.0016,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.15634369850158691,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.001,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.23813308775424957,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0019,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.3071671426296234,
      "learning_rate": 4.016e-05,
      "loss": 0.0011,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.2925663888454437,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0014,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0008,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.09860923141241074,
      "learning_rate": 4.012e-05,
      "loss": 0.001,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.38136348128318787,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.0022,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.3662945330142975,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0016,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.14579355716705322,
      "learning_rate": 4.008e-05,
      "loss": 0.0016,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.11565713584423065,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0022,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.2769434154033661,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0017,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.31659430265426636,
      "learning_rate": 4.004e-05,
      "loss": 0.0016,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.2830907702445984,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.002,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.30246207118034363,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0018,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.36882463097572327,
      "learning_rate": 4e-05,
      "loss": 0.0024,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.2632492184638977,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0013,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.18950879573822021,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0016,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.38646453619003296,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.001,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.1865912228822708,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0016,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.1702207773923874,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0009,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.4084342420101166,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0014,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.6332597732543945,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0012,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.1861664056777954,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0014,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.07261434197425842,
      "learning_rate": 3.988e-05,
      "loss": 0.0009,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.2032845914363861,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0011,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.07249115407466888,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0017,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.2055436670780182,
      "learning_rate": 3.984e-05,
      "loss": 0.0011,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.263147234916687,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0016,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.2365848869085312,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0015,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.4144590497016907,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0019,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.0688294842839241,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0018,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.07204855978488922,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.0016,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.42147770524024963,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0023,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.17060524225234985,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0006,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.26791292428970337,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.002,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.2772673964500427,
      "learning_rate": 3.972e-05,
      "loss": 0.002,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.09758860617876053,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0012,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.0878615453839302,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0014,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.2054317444562912,
      "learning_rate": 3.968e-05,
      "loss": 0.0014,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.283600389957428,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.002,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.20025166869163513,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0009,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.4592174291610718,
      "learning_rate": 3.964e-05,
      "loss": 0.0015,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.165066197514534,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.0016,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.16303767263889313,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0017,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.7827902436256409,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0016,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.15712189674377441,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0015,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.1140425056219101,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.0013,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.3965974748134613,
      "learning_rate": 3.956e-05,
      "loss": 0.0013,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.07688478380441666,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0014,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.08173435926437378,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0014,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.0,
      "learning_rate": 3.952e-05,
      "loss": 0.0004,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.2942150831222534,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0015,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.163193941116333,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0014,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.24599668383598328,
      "learning_rate": 3.948e-05,
      "loss": 0.0011,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.36137640476226807,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0015,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.5551990866661072,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0014,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.626440703868866,
      "learning_rate": 3.944e-05,
      "loss": 0.0014,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.7300647497177124,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.002,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.7804096341133118,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0011,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.2697025537490845,
      "learning_rate": 3.94e-05,
      "loss": 0.0017,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.22610343992710114,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0019,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.5402924418449402,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0019,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.19603011012077332,
      "learning_rate": 3.936e-05,
      "loss": 0.0016,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.4795607924461365,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0023,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.09284725040197372,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.002,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.41190874576568604,
      "learning_rate": 3.932e-05,
      "loss": 0.0012,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.33826184272766113,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.0017,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.32543233036994934,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0015,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.34409576654434204,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0017,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.1444966346025467,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0018,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.27483659982681274,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0013,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.20896382629871368,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0015,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.22705911099910736,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0016,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.2112126499414444,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0019,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.15738609433174133,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.002,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.1557454913854599,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0008,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.16261380910873413,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.0011,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.25650542974472046,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.002,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.14417710900306702,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0019,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.8390770554542542,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0024,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.20305058360099792,
      "learning_rate": 3.912e-05,
      "loss": 0.0016,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.06459956616163254,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0014,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.1750340461730957,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.0013,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.09446458518505096,
      "learning_rate": 3.908e-05,
      "loss": 0.0011,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.36915123462677,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0021,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.0990123450756073,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0015,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.27797362208366394,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0017,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0016,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0016,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.27035120129585266,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0017,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.1798936277627945,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.0018,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.21373742818832397,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0015,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.29703202843666077,
      "learning_rate": 3.896e-05,
      "loss": 0.0016,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.13639532029628754,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0016,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.4452509582042694,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0016,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.39222487807273865,
      "learning_rate": 3.892e-05,
      "loss": 0.0015,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.46445268392562866,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.002,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.25722435116767883,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.001,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.18789373338222504,
      "learning_rate": 3.888e-05,
      "loss": 0.0008,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.0998634323477745,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0016,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.3116486072540283,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0021,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.1291646808385849,
      "learning_rate": 3.884e-05,
      "loss": 0.0016,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.16112838685512543,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0013,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.26792997121810913,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0016,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.22713269293308258,
      "learning_rate": 3.88e-05,
      "loss": 0.0011,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.22749273478984833,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0011,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.21451613306999207,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.0009,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.14692215621471405,
      "learning_rate": 3.876e-05,
      "loss": 0.0011,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.4030153155326843,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0015,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.2681908905506134,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0016,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.12264205515384674,
      "learning_rate": 3.872e-05,
      "loss": 0.001,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.18670456111431122,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0009,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.10736562311649323,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0009,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.27181243896484375,
      "learning_rate": 3.868e-05,
      "loss": 0.0011,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.07559527456760406,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0008,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.19643567502498627,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0014,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.18122310936450958,
      "learning_rate": 3.864e-05,
      "loss": 0.0018,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.14593768119812012,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0017,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.08141116797924042,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0013,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.0887959823012352,
      "learning_rate": 3.86e-05,
      "loss": 0.0016,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.1605963259935379,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.0013,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.36044231057167053,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0023,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.129328653216362,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0014,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.20374323427677155,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0009,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.14415349066257477,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0018,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.3687868118286133,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0012,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.4835551977157593,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0016,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.23946036398410797,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0016,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.3955870270729065,
      "learning_rate": 3.848e-05,
      "loss": 0.002,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.38928595185279846,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0015,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.31482428312301636,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0017,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.14132292568683624,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0011,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.18652740120887756,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0015,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.22959324717521667,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.001,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.536287248134613,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0019,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.2911362648010254,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0007,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.17012497782707214,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0016,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.0,
      "learning_rate": 3.836e-05,
      "loss": 0.0008,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.17252056300640106,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0009,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.2673192024230957,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0018,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.3480331003665924,
      "learning_rate": 3.832e-05,
      "loss": 0.0016,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.2300717979669571,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0009,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.08833900094032288,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0009,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.2727195620536804,
      "learning_rate": 3.828e-05,
      "loss": 0.0021,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.30947259068489075,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0016,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.09217619895935059,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0013,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.4527609646320343,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.0014,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.5670892596244812,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0016,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.9848908185958862,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0012,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.3162878453731537,
      "learning_rate": 3.82e-05,
      "loss": 0.0012,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.10049208998680115,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0019,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.5707842707633972,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0014,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.05635258927941322,
      "learning_rate": 3.816e-05,
      "loss": 0.0014,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.4907856285572052,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0009,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.620529055595398,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0009,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.1388346254825592,
      "learning_rate": 3.812e-05,
      "loss": 0.0007,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.2771073281764984,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0007,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.16121187806129456,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0017,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.21189431846141815,
      "learning_rate": 3.808e-05,
      "loss": 0.0008,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.28981277346611023,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0008,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.25886061787605286,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0013,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.28833872079849243,
      "learning_rate": 3.804e-05,
      "loss": 0.0018,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.3934777081012726,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0012,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.29248112440109253,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0018,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.15370683372020721,
      "learning_rate": 3.8e-05,
      "loss": 0.0011,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.22553065419197083,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0008,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.27244657278060913,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0012,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.07115927338600159,
      "learning_rate": 3.796e-05,
      "loss": 0.0013,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.24389122426509857,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0015,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.29842737317085266,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0011,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.0,
      "learning_rate": 3.792e-05,
      "loss": 0.0014,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.15162208676338196,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0017,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.4928562343120575,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0014,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.4517434239387512,
      "learning_rate": 3.788e-05,
      "loss": 0.0008,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.27529236674308777,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0011,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.29978156089782715,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.001,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.22411295771598816,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0022,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.19068437814712524,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0016,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.3399558961391449,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0014,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.20643439888954163,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0019,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.5288785696029663,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.0006,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.2261829525232315,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0014,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.2058500200510025,
      "learning_rate": 3.776e-05,
      "loss": 0.001,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.23457707464694977,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0012,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.12004197388887405,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.002,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.1396198570728302,
      "learning_rate": 3.772e-05,
      "loss": 0.0013,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.22036652266979218,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0015,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.1438792198896408,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0008,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.19722245633602142,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.002,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.4709509313106537,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0012,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.10439407080411911,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.0016,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.10040387511253357,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0014,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.14045347273349762,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0015,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.13714255392551422,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0014,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.23768776655197144,
      "learning_rate": 3.76e-05,
      "loss": 0.0018,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.12965446710586548,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.0019,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.43418624997138977,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0011,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.19609856605529785,
      "learning_rate": 3.756e-05,
      "loss": 0.0013,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.15629597008228302,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.0017,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.36454153060913086,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0016,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.4251319468021393,
      "learning_rate": 3.752e-05,
      "loss": 0.0015,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.5151246786117554,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0012,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.5256772637367249,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.0019,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.14878420531749725,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0016,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.22860555350780487,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0017,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.10259763151407242,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0007,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.7479169964790344,
      "learning_rate": 3.744e-05,
      "loss": 0.001,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.4692477583885193,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0013,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.43315809965133667,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0016,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.21180108189582825,
      "learning_rate": 3.74e-05,
      "loss": 0.0017,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.742538571357727,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.0018,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.13224735856056213,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0005,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.367977112531662,
      "learning_rate": 3.736e-05,
      "loss": 0.0015,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.2586083710193634,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0016,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.20826058089733124,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0018,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.37080830335617065,
      "learning_rate": 3.732e-05,
      "loss": 0.002,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.22371137142181396,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.0006,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.08320426195859909,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.0014,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.10573757439851761,
      "learning_rate": 3.728e-05,
      "loss": 0.0009,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.23365770280361176,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0013,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.38174524903297424,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0013,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.698397159576416,
      "learning_rate": 3.724e-05,
      "loss": 0.002,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.5716807246208191,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0007,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.4962786138057709,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0013,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.19430473446846008,
      "learning_rate": 3.72e-05,
      "loss": 0.001,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.3004932999610901,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0021,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.1460086703300476,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0009,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.33283287286758423,
      "learning_rate": 3.716e-05,
      "loss": 0.0018,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.2126762419939041,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0022,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.5089898109436035,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0013,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.48481133580207825,
      "learning_rate": 3.712e-05,
      "loss": 0.0014,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.2090030014514923,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0003,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.1670149713754654,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0014,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.19594085216522217,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.0018,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.13687162101268768,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0019,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.06257673352956772,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.0018,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.6301828026771545,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0013,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.06228826940059662,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0008,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.16666638851165771,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0013,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.2896067798137665,
      "learning_rate": 3.7e-05,
      "loss": 0.0012,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.13986511528491974,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0009,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.585435152053833,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0023,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.08196839690208435,
      "learning_rate": 3.696e-05,
      "loss": 0.0011,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.2278042733669281,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0007,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.6963745355606079,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0014,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.13074617087841034,
      "learning_rate": 3.692e-05,
      "loss": 0.0007,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.430612713098526,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0017,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.18675722181797028,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.0013,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.2868960499763489,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.0008,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.11069294065237045,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0018,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.20008817315101624,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0012,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.08098448812961578,
      "learning_rate": 3.684e-05,
      "loss": 0.0017,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.4829624891281128,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0023,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.26712238788604736,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0014,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.48344093561172485,
      "learning_rate": 3.68e-05,
      "loss": 0.0013,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.2271919548511505,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0016,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.2733962833881378,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0016,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.37507864832878113,
      "learning_rate": 3.676e-05,
      "loss": 0.0014,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.3717821538448334,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0016,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.19982485473155975,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0013,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.20717130601406097,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0014,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.26995599269866943,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.001,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.19848744571208954,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0019,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.2067960500717163,
      "learning_rate": 3.668e-05,
      "loss": 0.0014,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.284608393907547,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0015,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.33707067370414734,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0016,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.3743399679660797,
      "learning_rate": 3.664e-05,
      "loss": 0.0015,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.11395249515771866,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0012,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.25217726826667786,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0016,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.2501475214958191,
      "learning_rate": 3.66e-05,
      "loss": 0.0019,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.1613856852054596,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.0016,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.32201072573661804,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0012,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.30416858196258545,
      "learning_rate": 3.656e-05,
      "loss": 0.0013,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.3952264189720154,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.0017,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.3201634883880615,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.001,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.11779791861772537,
      "learning_rate": 3.652e-05,
      "loss": 0.0008,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.16846878826618195,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0006,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.08548294752836227,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0005,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.2407969981431961,
      "learning_rate": 3.648e-05,
      "loss": 0.0014,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.2593871057033539,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0008,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.15839530527591705,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.0012,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.3083430528640747,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.0023,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.7759751081466675,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.0014,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.1384894698858261,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0015,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.17390486598014832,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.001,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.30029261112213135,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.001,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.15323477983474731,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0013,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.25598233938217163,
      "learning_rate": 3.636e-05,
      "loss": 0.0011,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.1239926815032959,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0009,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.07498559355735779,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0012,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.11927644163370132,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.001,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.21363727748394012,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0018,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.12211853265762329,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0014,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.8701483011245728,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0012,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.3722005784511566,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0012,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.22374653816223145,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0023,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.22519302368164062,
      "learning_rate": 3.624e-05,
      "loss": 0.0017,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.20723706483840942,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.002,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.16212791204452515,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0022,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.3657955825328827,
      "learning_rate": 3.62e-05,
      "loss": 0.0007,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.649493396282196,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0007,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.3346143066883087,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0014,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.3008476793766022,
      "learning_rate": 3.616e-05,
      "loss": 0.0019,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.16534170508384705,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0011,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.16859839856624603,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0009,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.5677992701530457,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0009,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.23083578050136566,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0018,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.1681620478630066,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0012,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.07873375713825226,
      "learning_rate": 3.608e-05,
      "loss": 0.0014,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.10225294530391693,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0008,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.44390565156936646,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0013,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.530890703201294,
      "learning_rate": 3.604e-05,
      "loss": 0.0015,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.3473045825958252,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0012,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.7603787779808044,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0012,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.5580527782440186,
      "learning_rate": 3.6e-05,
      "loss": 0.0012,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.39266714453697205,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0012,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.4817371666431427,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0013,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.13939784467220306,
      "learning_rate": 3.596e-05,
      "loss": 0.0015,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.14924198389053345,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0015,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.001,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.15043018758296967,
      "learning_rate": 3.592e-05,
      "loss": 0.0015,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.3990965783596039,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.0011,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.5224295258522034,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0022,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.6029701828956604,
      "learning_rate": 3.588e-05,
      "loss": 0.002,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.15879984200000763,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0019,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.5485687851905823,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0015,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.22649522125720978,
      "learning_rate": 3.584e-05,
      "loss": 0.0013,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.23531480133533478,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.001,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.23194283246994019,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0009,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.27542346715927124,
      "learning_rate": 3.58e-05,
      "loss": 0.0007,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.5533347725868225,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0011,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.22155678272247314,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.0014,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.08596209436655045,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0005,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.1189621165394783,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0008,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.23724199831485748,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0007,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.20093044638633728,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0009,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.49512356519699097,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0016,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.21574729681015015,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0011,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.4197690784931183,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0006,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.2826175391674042,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0015,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.32644936442375183,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0012,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.35180214047431946,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0012,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.08587366342544556,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0012,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.15957897901535034,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0013,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.18981404602527618,
      "learning_rate": 3.56e-05,
      "loss": 0.001,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.2948964536190033,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0005,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.12158238142728806,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0011,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.20649191737174988,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.0011,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.11762528866529465,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.0017,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.09927670657634735,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0007,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.19094502925872803,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.001,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.2651049494743347,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.0016,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.08327316492795944,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.0008,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.20471793413162231,
      "learning_rate": 3.548e-05,
      "loss": 0.0007,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.1717357337474823,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0015,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.21381565928459167,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0008,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.17886091768741608,
      "learning_rate": 3.544e-05,
      "loss": 0.0015,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.13509757816791534,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0013,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.41949358582496643,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0012,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.14338956773281097,
      "learning_rate": 3.54e-05,
      "loss": 0.0012,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.12218520045280457,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.001,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.41328901052474976,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0012,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.20922806859016418,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0006,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.20514507591724396,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0013,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.11220599710941315,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0009,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.18547430634498596,
      "learning_rate": 3.532e-05,
      "loss": 0.0021,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.1547086238861084,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0015,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.1634775847196579,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0008,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.07801627367734909,
      "learning_rate": 3.528e-05,
      "loss": 0.0008,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.11358692497015,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0012,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.07055262476205826,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.0007,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.14668667316436768,
      "learning_rate": 3.524e-05,
      "loss": 0.0008,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.26878687739372253,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0004,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.14071281254291534,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0019,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.2615262269973755,
      "learning_rate": 3.52e-05,
      "loss": 0.0026,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.2865091860294342,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.001,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.2253720909357071,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0016,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.36031606793403625,
      "learning_rate": 3.516e-05,
      "loss": 0.0009,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.4208626449108124,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.002,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.40441209077835083,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0011,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.30641838908195496,
      "learning_rate": 3.512e-05,
      "loss": 0.0008,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.31135088205337524,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0012,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.1101573258638382,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0017,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.15854428708553314,
      "learning_rate": 3.508e-05,
      "loss": 0.0012,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.23486950993537903,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.002,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.9186968207359314,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0014,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.23953069746494293,
      "learning_rate": 3.504e-05,
      "loss": 0.0012,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.19110675156116486,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.001,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.3263777494430542,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0017,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.07125803083181381,
      "learning_rate": 3.5e-05,
      "loss": 0.0016,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.5637092590332031,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0016,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.31954270601272583,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0015,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.11071112751960754,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0011,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.2331811636686325,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0012,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.39967086911201477,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0011,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.32798057794570923,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0011,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.37217533588409424,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.0015,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.17255061864852905,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0009,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.15663932263851166,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0012,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.36296889185905457,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0023,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.23500995337963104,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0012,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.32689324021339417,
      "learning_rate": 3.484e-05,
      "loss": 0.0014,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.22869518399238586,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.001,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.15646468102931976,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.001,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.5623471140861511,
      "learning_rate": 3.48e-05,
      "loss": 0.0008,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.5193647146224976,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0015,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.6893144249916077,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0014,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.4034157693386078,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0016,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.42286062240600586,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0011,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.1865565925836563,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0021,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.31247320771217346,
      "learning_rate": 3.472e-05,
      "loss": 0.0015,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.30987945199012756,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0017,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.29635652899742126,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0012,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.1956835687160492,
      "learning_rate": 3.468e-05,
      "loss": 0.0011,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.16153617203235626,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0011,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.10656116157770157,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0015,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.20058338344097137,
      "learning_rate": 3.464e-05,
      "loss": 0.001,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.4388105869293213,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.001,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.24555133283138275,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0005,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.3939419984817505,
      "learning_rate": 3.46e-05,
      "loss": 0.0007,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.4000571370124817,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.002,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.34567564725875854,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0012,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.24283398687839508,
      "learning_rate": 3.456e-05,
      "loss": 0.0012,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.14100989699363708,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0013,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.1952887773513794,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0013,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.2671683132648468,
      "learning_rate": 3.452e-05,
      "loss": 0.0011,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.15095704793930054,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.0018,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.10632771998643875,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0014,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.3362734913825989,
      "learning_rate": 3.448e-05,
      "loss": 0.0007,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.21586301922798157,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0011,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.38347309827804565,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0012,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 1.100172758102417,
      "learning_rate": 3.444e-05,
      "loss": 0.001,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.1456475853919983,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0009,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.20442532002925873,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0006,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.1137581542134285,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0018,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.11779562383890152,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0012,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.3891381621360779,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0008,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.09291442483663559,
      "learning_rate": 3.436e-05,
      "loss": 0.0012,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.33705922961235046,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0007,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.2324167788028717,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0008,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.349132776260376,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0005,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.20482875406742096,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0009,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.3191632032394409,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0011,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.11076956242322922,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0008,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.18319325149059296,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0011,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.24967411160469055,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0017,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.151254341006279,
      "learning_rate": 3.424e-05,
      "loss": 0.0009,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.062195029109716415,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.0009,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.2895810008049011,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0008,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.16229400038719177,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0012,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.1357877403497696,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.0019,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.21143725514411926,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.0008,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.2771065831184387,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0012,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.179478257894516,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0013,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.20459453761577606,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0014,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.23972970247268677,
      "learning_rate": 3.412e-05,
      "loss": 0.0008,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.30099132657051086,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0012,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.2275811731815338,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0016,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.38851404190063477,
      "learning_rate": 3.408e-05,
      "loss": 0.0014,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.20051667094230652,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0008,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.0961124449968338,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0004,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.3570629358291626,
      "learning_rate": 3.404e-05,
      "loss": 0.0014,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.4031173586845398,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.0007,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.3954605460166931,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0008,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10679973661899567,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0013,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.357621967792511,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0014,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.34023502469062805,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0013,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.10310542583465576,
      "learning_rate": 3.396e-05,
      "loss": 0.0011,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 1.0220859050750732,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0011,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.20551854372024536,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0018,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.1680508702993393,
      "learning_rate": 3.392e-05,
      "loss": 0.0011,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.2961542308330536,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0009,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.10812999308109283,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0015,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.12391912192106247,
      "learning_rate": 3.388e-05,
      "loss": 0.0009,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.21375232934951782,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0012,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.09166119247674942,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.0016,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.16234220564365387,
      "learning_rate": 3.384e-05,
      "loss": 0.0017,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.15198081731796265,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.0009,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.3688163757324219,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.002,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.12462569028139114,
      "learning_rate": 3.38e-05,
      "loss": 0.001,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.12854689359664917,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0006,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.2314198911190033,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0011,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.24133308231830597,
      "learning_rate": 3.376e-05,
      "loss": 0.0015,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.001,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.10707107931375504,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.001,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.2510888874530792,
      "learning_rate": 3.372e-05,
      "loss": 0.001,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.6609331965446472,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0009,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.4924800395965576,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.0016,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.7159043550491333,
      "learning_rate": 3.368e-05,
      "loss": 0.0016,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.4135504364967346,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0015,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.6049647927284241,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0017,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.9266130328178406,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0013,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.6141232252120972,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.001,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0013,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.2072754204273224,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0017,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.32397595047950745,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0012,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.4186440110206604,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.0012,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.07925054430961609,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.0012,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.4040779173374176,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0011,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.15442989766597748,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0014,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.09698843210935593,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.001,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.20246902108192444,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0009,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.39320287108421326,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0015,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.18474504351615906,
      "learning_rate": 3.348e-05,
      "loss": 0.0017,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.09164906293153763,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0016,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.11864027380943298,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.001,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.2046433687210083,
      "learning_rate": 3.344e-05,
      "loss": 0.0016,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.08420175313949585,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0018,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.31607890129089355,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0007,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.08977975696325302,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0008,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.20925383269786835,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.0011,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.12637899816036224,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.0008,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.27700570225715637,
      "learning_rate": 3.336e-05,
      "loss": 0.0017,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.10443182289600372,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.0015,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.3184525966644287,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0008,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.2625844478607178,
      "learning_rate": 3.332e-05,
      "loss": 0.0009,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.1370144933462143,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.001,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.4459793269634247,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0014,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.13332772254943848,
      "learning_rate": 3.328e-05,
      "loss": 0.0017,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.38825172185897827,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0007,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.4130096137523651,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0009,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.2931918501853943,
      "learning_rate": 3.324e-05,
      "loss": 0.0011,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.3555240333080292,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0006,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.19789478182792664,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0019,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.2984085977077484,
      "learning_rate": 3.32e-05,
      "loss": 0.0008,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.17144735157489777,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.0007,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.2416219413280487,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0014,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.8895310759544373,
      "learning_rate": 3.316e-05,
      "loss": 0.0012,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.27425020933151245,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0012,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.4116964340209961,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.001,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.42479464411735535,
      "learning_rate": 3.312e-05,
      "loss": 0.0009,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.15010036528110504,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.001,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.4212329387664795,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0014,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.21081691980361938,
      "learning_rate": 3.308e-05,
      "loss": 0.0012,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.24772489070892334,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0014,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.17861288785934448,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0014,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.18651141226291656,
      "learning_rate": 3.304e-05,
      "loss": 0.0017,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.18441744148731232,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.001,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.12095865607261658,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0008,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.0,
      "learning_rate": 3.3e-05,
      "loss": 0.0009,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0005,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.09079734236001968,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.0009,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.31163614988327026,
      "learning_rate": 3.296e-05,
      "loss": 0.0014,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.1594124287366867,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0013,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.19569353759288788,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0008,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.24000681936740875,
      "learning_rate": 3.292e-05,
      "loss": 0.0013,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.5139943957328796,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.002,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.39188432693481445,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0012,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.36454811692237854,
      "learning_rate": 3.288e-05,
      "loss": 0.0013,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 1.2604365348815918,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0018,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 1.2931783199310303,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0017,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.9440960884094238,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0016,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.11071714758872986,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.001,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.29293137788772583,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0009,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.1715060919523239,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0008,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.14046338200569153,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0009,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.28246423602104187,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.001,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.13313771784305573,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.0008,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.17530883848667145,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0011,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.127288818359375,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0009,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.19367951154708862,
      "learning_rate": 3.272e-05,
      "loss": 0.0008,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.18377284705638885,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0014,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.25621530413627625,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0006,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.07972399145364761,
      "learning_rate": 3.268e-05,
      "loss": 0.0009,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.23213785886764526,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0007,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.2991805374622345,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.001,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.10651355981826782,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.0004,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.12273108959197998,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.0016,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.27679890394210815,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0009,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.4673970937728882,
      "learning_rate": 3.26e-05,
      "loss": 0.0016,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.224570631980896,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.001,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.38384175300598145,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0015,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.523715615272522,
      "learning_rate": 3.256e-05,
      "loss": 0.0011,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.48225101828575134,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0022,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.8983853459358215,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0014,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.1438581943511963,
      "learning_rate": 3.252e-05,
      "loss": 0.0011,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.12173593044281006,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0012,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.15614841878414154,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0011,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.08521965891122818,
      "learning_rate": 3.248e-05,
      "loss": 0.0011,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0007,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.19478997588157654,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0011,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.14905165135860443,
      "learning_rate": 3.244e-05,
      "loss": 0.0008,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.19537657499313354,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.0009,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.17687541246414185,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.001,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.35295242071151733,
      "learning_rate": 3.24e-05,
      "loss": 0.0014,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.23647701740264893,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0014,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.13597968220710754,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0014,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.4371366798877716,
      "learning_rate": 3.236e-05,
      "loss": 0.0009,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.1675628274679184,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0014,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.25786757469177246,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0013,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.7430538535118103,
      "learning_rate": 3.232e-05,
      "loss": 0.0015,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.38710248470306396,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0011,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.09363269060850143,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.001,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.33003997802734375,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0008,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.2611640393733978,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0008,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.22307397425174713,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0018,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.3507213294506073,
      "learning_rate": 3.224e-05,
      "loss": 0.0009,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.187469944357872,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0015,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.3928506374359131,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0007,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.5257143378257751,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.001,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.12979736924171448,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0017,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0015,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.17736533284187317,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0008,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.26149722933769226,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0005,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.24376016855239868,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.001,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.2015337198972702,
      "learning_rate": 3.212e-05,
      "loss": 0.001,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.2135021835565567,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0015,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.1270752251148224,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.0006,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.17947371304035187,
      "learning_rate": 3.208e-05,
      "loss": 0.0008,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.17659726738929749,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0014,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.2711411416530609,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.001,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.15854749083518982,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.0011,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.1879778504371643,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0011,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.16794627904891968,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0007,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.16496144235134125,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0007,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.12467550486326218,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0012,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.7034496068954468,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0012,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.10771304368972778,
      "learning_rate": 3.196e-05,
      "loss": 0.0009,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.1542043536901474,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.0013,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.26105794310569763,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0024,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.3617883026599884,
      "learning_rate": 3.192e-05,
      "loss": 0.0008,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.08604662120342255,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0005,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.20579788088798523,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.0011,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.0,
      "learning_rate": 3.188e-05,
      "loss": 0.0011,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.3144737482070923,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0004,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.13659203052520752,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0006,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.22106561064720154,
      "learning_rate": 3.184e-05,
      "loss": 0.0017,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.6677219867706299,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.0015,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.1495697945356369,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0008,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.1034654900431633,
      "learning_rate": 3.18e-05,
      "loss": 0.0005,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.34072667360305786,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0013,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.15919886529445648,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0014,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.7990580797195435,
      "learning_rate": 3.176e-05,
      "loss": 0.0009,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.17035214602947235,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0009,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.27753153443336487,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0011,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.1729191243648529,
      "learning_rate": 3.172e-05,
      "loss": 0.0011,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.08930543810129166,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0009,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.5983515381813049,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0017,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.5209975242614746,
      "learning_rate": 3.168e-05,
      "loss": 0.0006,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.16904217004776,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0007,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.5283662676811218,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.001,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.2132149189710617,
      "learning_rate": 3.164e-05,
      "loss": 0.0008,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.21375614404678345,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.0011,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.08271859586238861,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0014,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.10613410174846649,
      "learning_rate": 3.16e-05,
      "loss": 0.0013,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.14886029064655304,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.001,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.3670404553413391,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0012,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.08213438093662262,
      "learning_rate": 3.156e-05,
      "loss": 0.0011,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.16451723873615265,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.0013,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.13784682750701904,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0006,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.1992369443178177,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0007,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.1865362823009491,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0021,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.23126839101314545,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0014,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.12700149416923523,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0022,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.20069824159145355,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0008,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.14130601286888123,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0007,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.18182899057865143,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.0007,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.23196068406105042,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0013,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.15735018253326416,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0009,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.11137204617261887,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0011,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.19707347452640533,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0007,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0013,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.15727955102920532,
      "learning_rate": 3.136e-05,
      "loss": 0.001,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.4045353829860687,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.001,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.21456660330295563,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0011,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.24366451799869537,
      "learning_rate": 3.132e-05,
      "loss": 0.0013,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.2090994417667389,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.001,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.29719027876853943,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0009,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.10815644264221191,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0008,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.09120938181877136,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0009,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.103587806224823,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0007,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.09502021223306656,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0012,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.21120397746562958,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0011,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.20636329054832458,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0011,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.6749953031539917,
      "learning_rate": 3.12e-05,
      "loss": 0.0009,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.29502639174461365,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.0008,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.06321515887975693,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0008,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.1581154614686966,
      "learning_rate": 3.116e-05,
      "loss": 0.0011,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.11719650775194168,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0009,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.2977115213871002,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0009,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.4014202654361725,
      "learning_rate": 3.112e-05,
      "loss": 0.0014,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.20161841809749603,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0011,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.12777447700500488,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0008,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.18069563806056976,
      "learning_rate": 3.108e-05,
      "loss": 0.0009,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.2794886529445648,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0018,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.17615142464637756,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0007,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.09809744358062744,
      "learning_rate": 3.104e-05,
      "loss": 0.0007,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.19170288741588593,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0011,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.6689661741256714,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0012,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.3119887411594391,
      "learning_rate": 3.1e-05,
      "loss": 0.0012,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.12287063896656036,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0011,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.2455253303050995,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.0008,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.1610027551651001,
      "learning_rate": 3.096e-05,
      "loss": 0.0021,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.19216863811016083,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.001,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.17963534593582153,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0008,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.36610981822013855,
      "learning_rate": 3.092e-05,
      "loss": 0.0014,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.12554213404655457,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0006,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.3850393295288086,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.001,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.0815797969698906,
      "learning_rate": 3.088e-05,
      "loss": 0.0008,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.24173147976398468,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0008,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.34405437111854553,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0007,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.1803184449672699,
      "learning_rate": 3.084e-05,
      "loss": 0.001,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.5473829507827759,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0005,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.14028896391391754,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.001,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.08142565190792084,
      "learning_rate": 3.08e-05,
      "loss": 0.0007,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.07678957283496857,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0011,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.27564966678619385,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0011,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.3166654109954834,
      "learning_rate": 3.076e-05,
      "loss": 0.0011,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.21119728684425354,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.0015,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.23350758850574493,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0013,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.6539866924285889,
      "learning_rate": 3.072e-05,
      "loss": 0.0006,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.678865909576416,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0011,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.5184303522109985,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.001,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.6825547814369202,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0008,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.38633447885513306,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0012,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.18934406340122223,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0015,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.07993273437023163,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0008,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.5159530639648438,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0013,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.10557692497968674,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0014,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.07472467422485352,
      "learning_rate": 3.06e-05,
      "loss": 0.0006,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.1994374543428421,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.001,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.20976781845092773,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0009,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.21083469688892365,
      "learning_rate": 3.056e-05,
      "loss": 0.0008,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.45286792516708374,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0009,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.17867761850357056,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0013,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.2725670039653778,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.0007,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.4516119062900543,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0012,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.2696079909801483,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.0009,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.1378113478422165,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.0005,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.30913296341896057,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0012,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.2610744535923004,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0013,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.2678842842578888,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0015,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.30456721782684326,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0008,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.14241349697113037,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.001,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.3346257209777832,
      "learning_rate": 3.04e-05,
      "loss": 0.0014,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.16922272741794586,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0012,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.3629935085773468,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0007,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.23278366029262543,
      "learning_rate": 3.036e-05,
      "loss": 0.0008,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.3965567648410797,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0007,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.11333013325929642,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.001,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.09492234885692596,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0012,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.16683897376060486,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0015,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.14711615443229675,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0004,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.16365090012550354,
      "learning_rate": 3.028e-05,
      "loss": 0.0008,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.1802709698677063,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.001,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.3018726110458374,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0008,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.12501105666160583,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0007,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.10605843365192413,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0012,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.16848792135715485,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0016,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.2967255711555481,
      "learning_rate": 3.02e-05,
      "loss": 0.0013,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.19378909468650818,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0009,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.2741360068321228,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0008,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.5959452986717224,
      "learning_rate": 3.016e-05,
      "loss": 0.0011,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.17402295768260956,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.0009,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0005,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.14610593020915985,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0007,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.16324643790721893,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.0017,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.24145115911960602,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0008,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.27334854006767273,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0008,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.2366286814212799,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0015,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.17914384603500366,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.001,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.13209019601345062,
      "learning_rate": 3.004e-05,
      "loss": 0.0012,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.3177914023399353,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0008,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 1.0384495258331299,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.001,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5663380026817322,
      "learning_rate": 3e-05,
      "loss": 0.0016,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.19605353474617004,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.0015,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0009,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.0,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0002,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.3571208119392395,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.0011,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.20736481249332428,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0015,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.20221249759197235,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0013,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.2871105372905731,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0008,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.0746968686580658,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0007,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.12438257783651352,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0014,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.2601762115955353,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0007,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.4063257873058319,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.001,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.0,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0009,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.2889597415924072,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0005,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.2728744149208069,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0012,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.10314192622900009,
      "learning_rate": 2.98e-05,
      "loss": 0.0004,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.10008087009191513,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.0008,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.3324511647224426,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0008,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.12290287017822266,
      "learning_rate": 2.976e-05,
      "loss": 0.0008,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.29999321699142456,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0009,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.25866422057151794,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.001,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.10724761337041855,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0008,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.3343769907951355,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0004,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.3972082734107971,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0015,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.24291931092739105,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0011,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.28569698333740234,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0008,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.08526137471199036,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0005,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.13964824378490448,
      "learning_rate": 2.964e-05,
      "loss": 0.0013,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.12842386960983276,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0007,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0014,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.20910024642944336,
      "learning_rate": 2.96e-05,
      "loss": 0.0006,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.11271417140960693,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.001,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.29560619592666626,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0013,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.4940512776374817,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0014,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.30146002769470215,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.001,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.19891731441020966,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0004,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.2259036749601364,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.0018,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.10165538638830185,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0009,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.3555595278739929,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0007,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.08903723210096359,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0014,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.40734320878982544,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0014,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.138203963637352,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.0014,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.48502179980278015,
      "learning_rate": 2.944e-05,
      "loss": 0.0017,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.33734390139579773,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0013,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.22550849616527557,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0018,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.19715476036071777,
      "learning_rate": 2.94e-05,
      "loss": 0.0005,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.3310040831565857,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0011,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.43983885645866394,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.001,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.24733181297779083,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0013,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0005,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0009,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.3473033607006073,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0008,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.12206093966960907,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0017,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.05711700767278671,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0008,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.15556123852729797,
      "learning_rate": 2.928e-05,
      "loss": 0.0012,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.08248093724250793,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0009,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.17726925015449524,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0011,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.10919779539108276,
      "learning_rate": 2.924e-05,
      "loss": 0.0012,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.1296810358762741,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.0021,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.21239301562309265,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0013,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.6271438598632812,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.001,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.38121113181114197,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.0008,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.27210718393325806,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0017,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.14101983606815338,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0008,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.30704402923583984,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0014,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.2694040834903717,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0006,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.0,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.0008,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.2713654637336731,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0007,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.328172504901886,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.0011,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.1771516501903534,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0007,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.23823899030685425,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0012,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.1732672154903412,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.0007,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.10963427275419235,
      "learning_rate": 2.904e-05,
      "loss": 0.0017,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.10497891902923584,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0007,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.16620001196861267,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.0005,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.20250044763088226,
      "learning_rate": 2.9e-05,
      "loss": 0.001,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.1789323091506958,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0007,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.13986626267433167,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.0008,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.4872496724128723,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.001,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.19887709617614746,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0011,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.21213047206401825,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0009,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.2572271525859833,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0009,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.36186495423316956,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0014,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.23587001860141754,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0012,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.4464198052883148,
      "learning_rate": 2.888e-05,
      "loss": 0.001,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.21595443785190582,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0009,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.24524646997451782,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0008,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.1790178120136261,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.001,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.5253353714942932,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0009,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.37137049436569214,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.001,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.23991206288337708,
      "learning_rate": 2.88e-05,
      "loss": 0.0015,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.13871046900749207,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0009,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.27830496430397034,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0005,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.2736838459968567,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.001,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.5210342407226562,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0005,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.0835004448890686,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0008,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.1651557832956314,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0011,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.13913613557815552,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.0012,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.21498672664165497,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0008,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.2923889458179474,
      "learning_rate": 2.868e-05,
      "loss": 0.0009,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.1192045584321022,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0013,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.3315316438674927,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.001,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.15484368801116943,
      "learning_rate": 2.864e-05,
      "loss": 0.0011,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.4607841372489929,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.0009,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.6213606595993042,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0008,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.23798297345638275,
      "learning_rate": 2.86e-05,
      "loss": 0.0014,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.332701712846756,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0007,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.20925094187259674,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0009,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.18927446007728577,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0013,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.16275112330913544,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0009,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.06823365390300751,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0007,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.41605299711227417,
      "learning_rate": 2.852e-05,
      "loss": 0.0012,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.12676794826984406,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.0009,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.33333295583724976,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.0004,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.516749918460846,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0018,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.36649516224861145,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0011,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.33662062883377075,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0014,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.3867073357105255,
      "learning_rate": 2.844e-05,
      "loss": 0.0009,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.16496291756629944,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.0005,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.35118603706359863,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0008,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.0,
      "learning_rate": 2.84e-05,
      "loss": 0.0006,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.09306959062814713,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0004,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.3228197693824768,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0004,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.17144612967967987,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0013,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.09914378821849823,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0009,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.2988084554672241,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0007,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.4718315005302429,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0014,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.16178520023822784,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0011,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.8072137236595154,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.001,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.14883869886398315,
      "learning_rate": 2.828e-05,
      "loss": 0.0006,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.36941686272621155,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0013,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.13723315298557281,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.0008,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.4160212576389313,
      "learning_rate": 2.824e-05,
      "loss": 0.0012,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.08738608658313751,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0005,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.0659521222114563,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.0009,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.0,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0008,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.48486196994781494,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0012,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.2655717730522156,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0008,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.19071611762046814,
      "learning_rate": 2.816e-05,
      "loss": 0.0013,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.1657116413116455,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0005,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.21846310794353485,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.002,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.18639543652534485,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0009,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.09362030774354935,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0008,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.1220390796661377,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0012,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.5602936148643494,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0006,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.2774958908557892,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0009,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.1379261612892151,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0007,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.3017374873161316,
      "learning_rate": 2.804e-05,
      "loss": 0.0018,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.34313902258872986,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0011,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.2618691623210907,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.001,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.088003970682621,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.001,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.47369325160980225,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0012,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.23637007176876068,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0014,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.09999264776706696,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0009,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.14400622248649597,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.0006,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.166448712348938,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0013,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.17925842106342316,
      "learning_rate": 2.792e-05,
      "loss": 0.0006,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.16693907976150513,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.0003,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.28122586011886597,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.001,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.2601710855960846,
      "learning_rate": 2.788e-05,
      "loss": 0.0009,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0005,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.17885656654834747,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0013,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.17890895903110504,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0008,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.14448535442352295,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0004,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.21507640182971954,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0012,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.2486574798822403,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0011,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.31727319955825806,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0014,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.11208192259073257,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.001,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.3607984185218811,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.0009,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.2615492045879364,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.001,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0011,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.13598106801509857,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.001,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.07954437285661697,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0009,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.19042648375034332,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0012,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.33787012100219727,
      "learning_rate": 2.768e-05,
      "loss": 0.0019,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.36786457896232605,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0017,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.13751626014709473,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0008,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.16744978725910187,
      "learning_rate": 2.764e-05,
      "loss": 0.0005,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.2656874358654022,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.0009,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.21122656762599945,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.001,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.3016503155231476,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0007,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.5672500729560852,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0011,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.09425818175077438,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0022,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.29218417406082153,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0012,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.1016220822930336,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0014,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.18814946711063385,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0009,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.17786093056201935,
      "learning_rate": 2.752e-05,
      "loss": 0.001,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.6180617809295654,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0011,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.09711353480815887,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.0008,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.31341004371643066,
      "learning_rate": 2.748e-05,
      "loss": 0.001,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.2617899477481842,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0005,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.14704281091690063,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.0009,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.15094628930091858,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0011,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.08143477141857147,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0015,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.1878894418478012,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.0009,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.26665496826171875,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.001,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.0006,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.3953039348125458,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0022,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.28872719407081604,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.001,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.1614425927400589,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0009,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.3293548822402954,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0005,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.3329814672470093,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0012,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.10803474485874176,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0013,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.12330394238233566,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.001,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.11857451498508453,
      "learning_rate": 2.728e-05,
      "loss": 0.001,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.09280350059270859,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0005,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.3150653839111328,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0009,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.0,
      "learning_rate": 2.724e-05,
      "loss": 0.0009,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.23654146492481232,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0012,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.3383219242095947,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0012,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.2876962423324585,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0012,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.11123925447463989,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0007,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.11369814723730087,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.0007,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.19646577537059784,
      "learning_rate": 2.716e-05,
      "loss": 0.0006,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.1610482633113861,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.0006,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.20778781175613403,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.001,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.1604723185300827,
      "learning_rate": 2.712e-05,
      "loss": 0.001,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.250995397567749,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0011,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.14915241301059723,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0005,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.1792844980955124,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.0013,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.16959090530872345,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0011,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.5639711618423462,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0008,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.2299676537513733,
      "learning_rate": 2.704e-05,
      "loss": 0.0007,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.44016724824905396,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0011,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.47336769104003906,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0016,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.20162805914878845,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0012,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.11354146152734756,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0007,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.2619166374206543,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.001,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.23776355385780334,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0008,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.11014549434185028,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0008,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.07444620132446289,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0009,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.44639691710472107,
      "learning_rate": 2.692e-05,
      "loss": 0.0011,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.3424091339111328,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0016,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.4115915298461914,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0006,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.2523152828216553,
      "learning_rate": 2.688e-05,
      "loss": 0.0013,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.17238467931747437,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0008,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.383037269115448,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0007,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.24493178725242615,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.0004,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.1294214278459549,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.001,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.1696554273366928,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.001,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.13733547925949097,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0009,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.13411356508731842,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0011,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.23931634426116943,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0012,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.2947717607021332,
      "learning_rate": 2.676e-05,
      "loss": 0.0004,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.27046653628349304,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.0003,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.1606193631887436,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0007,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.4133530557155609,
      "learning_rate": 2.672e-05,
      "loss": 0.0008,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.22728854417800903,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0013,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.13113771378993988,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0006,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.20237289369106293,
      "learning_rate": 2.668e-05,
      "loss": 0.0011,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.6380124092102051,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0013,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.3029671311378479,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0013,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.20000310242176056,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.0008,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.606567919254303,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.0011,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.16219353675842285,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.0013,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.3282764256000519,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0014,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.33902546763420105,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0012,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.3277393579483032,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.0014,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.1139318197965622,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.001,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.4100494980812073,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.001,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.18263956904411316,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0014,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.11934677511453629,
      "learning_rate": 2.652e-05,
      "loss": 0.001,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.18864168226718903,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0009,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.4079348146915436,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0007,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.4046650230884552,
      "learning_rate": 2.648e-05,
      "loss": 0.0012,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.1882774531841278,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0009,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.3627474904060364,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0007,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.18221208453178406,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.0007,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0009,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.1285094916820526,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.001,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.18935103714466095,
      "learning_rate": 2.64e-05,
      "loss": 0.0015,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.14624564349651337,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0008,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.22725524008274078,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0007,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.13994792103767395,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0012,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.0997963696718216,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0012,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0009,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.22060881555080414,
      "learning_rate": 2.632e-05,
      "loss": 0.0009,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.11533304303884506,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0006,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.16813616454601288,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.001,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.09187457710504532,
      "learning_rate": 2.628e-05,
      "loss": 0.0003,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.0930388867855072,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.001,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.2758497893810272,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0011,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.39596885442733765,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0008,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.17965441942214966,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0008,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.2864473760128021,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0008,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.17814062535762787,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0006,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.17235520482063293,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0008,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.15580956637859344,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0013,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.06728300452232361,
      "learning_rate": 2.616e-05,
      "loss": 0.001,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.10146464407444,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0012,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.24118506908416748,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0013,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.3190734386444092,
      "learning_rate": 2.612e-05,
      "loss": 0.0006,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.26559507846832275,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0009,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.2546149790287018,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.001,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.16839146614074707,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0011,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.15758392214775085,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0009,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.21297778189182281,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0006,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.1368626207113266,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.0008,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.10613393038511276,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0009,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.31012478470802307,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0014,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.25202348828315735,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0008,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.6612563133239746,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0011,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.22110091149806976,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0007,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.11191311478614807,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.001,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.20823624730110168,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0011,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0006,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.0,
      "learning_rate": 2.592e-05,
      "loss": 0.0009,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.30240127444267273,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0009,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.18313075602054596,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0011,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.26702576875686646,
      "learning_rate": 2.588e-05,
      "loss": 0.0003,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.17715179920196533,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0007,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.342475563287735,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0009,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.09964866191148758,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0008,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.11810614168643951,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0005,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.13071632385253906,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0006,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.12322725355625153,
      "learning_rate": 2.58e-05,
      "loss": 0.0009,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.10691186785697937,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0011,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.3502935469150543,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0013,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.6971992254257202,
      "learning_rate": 2.576e-05,
      "loss": 0.0011,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.329597145318985,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0011,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.30058810114860535,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0011,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.14386345446109772,
      "learning_rate": 2.572e-05,
      "loss": 0.0011,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.2814895808696747,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0009,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.8371949195861816,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0011,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.10018936544656754,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0009,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.2568052411079407,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0005,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.1541173905134201,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.0009,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.07846138626337051,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0008,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.1454944759607315,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0008,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.1805894672870636,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0003,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.5066353678703308,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.001,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.5717074275016785,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.0011,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.1744716912508011,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.0011,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.22474898397922516,
      "learning_rate": 2.556e-05,
      "loss": 0.001,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.20045974850654602,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0009,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.5098023414611816,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0009,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.24484047293663025,
      "learning_rate": 2.552e-05,
      "loss": 0.001,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.09853854030370712,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.0012,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.3954286575317383,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.001,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.1776050329208374,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0009,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.1977158933877945,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0011,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.08766549080610275,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0003,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.17504210770130157,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0012,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.15217258036136627,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.001,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.16002719104290009,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0014,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.13282160460948944,
      "learning_rate": 2.54e-05,
      "loss": 0.001,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.2802738845348358,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0011,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.21638904511928558,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0012,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.25089597702026367,
      "learning_rate": 2.536e-05,
      "loss": 0.001,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.15711773931980133,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0012,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.4139285385608673,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0018,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.1468522697687149,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0009,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.5032572150230408,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0012,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.1675909012556076,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0011,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.1512041985988617,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.001,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.14421038329601288,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0004,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.2934788465499878,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.0006,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.07524630427360535,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0006,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.39460816979408264,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0007,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.39170458912849426,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0004,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.20594513416290283,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0013,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.16674622893333435,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0012,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.22538430988788605,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0011,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.2787121832370758,
      "learning_rate": 2.516e-05,
      "loss": 0.0014,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.2873328626155853,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.0008,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.11065122485160828,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0012,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.3889133632183075,
      "learning_rate": 2.512e-05,
      "loss": 0.001,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.1744646281003952,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.0005,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.21573539078235626,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0008,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.15722249448299408,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0004,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.1730291098356247,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0011,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.21523375809192657,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.0011,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.3801681399345398,
      "learning_rate": 2.504e-05,
      "loss": 0.0009,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.39172160625457764,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0007,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.08947727829217911,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0011,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4901706874370575,
      "learning_rate": 2.5e-05,
      "loss": 0.001,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0013507000403478742,
      "eval_runtime": 129.9748,
      "eval_samples_per_second": 1154.07,
      "eval_steps_per_second": 28.852,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.250286728143692,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0012,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.24134978652000427,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0015,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.15566492080688477,
      "learning_rate": 2.496e-05,
      "loss": 0.001,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.45389503240585327,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0016,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.24478743970394135,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0012,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.32079294323921204,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.0015,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.11085871607065201,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0006,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.21297197043895721,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0013,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.10323825478553772,
      "learning_rate": 2.488e-05,
      "loss": 0.0007,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.19862531125545502,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0007,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.07811644673347473,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0007,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.07283643633127213,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0011,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.21790145337581635,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0014,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.15434594452381134,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0012,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.17117805778980255,
      "learning_rate": 2.48e-05,
      "loss": 0.0004,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.07683131843805313,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.001,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.3476825952529907,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0011,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.09433986991643906,
      "learning_rate": 2.476e-05,
      "loss": 0.001,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.5261143445968628,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0006,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.16861870884895325,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0011,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.4199109375476837,
      "learning_rate": 2.472e-05,
      "loss": 0.0013,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.11455310881137848,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0008,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.13401027023792267,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.0004,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.18405036628246307,
      "learning_rate": 2.468e-05,
      "loss": 0.0007,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0008,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.2134067416191101,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0007,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.1565604954957962,
      "learning_rate": 2.464e-05,
      "loss": 0.0009,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.0008,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.1536850482225418,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0008,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.11548241972923279,
      "learning_rate": 2.46e-05,
      "loss": 0.001,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.18541556596755981,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0003,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.27670857310295105,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0004,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.5042861700057983,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.001,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.2396743893623352,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0011,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.1623532921075821,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0008,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.21221281588077545,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0008,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.32120442390441895,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.001,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.19439075887203217,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0007,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.18416181206703186,
      "learning_rate": 2.448e-05,
      "loss": 0.0006,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.23477356135845184,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0006,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.2980049252510071,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0011,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.7381560802459717,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0012,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.8128584027290344,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0006,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.8098232746124268,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0012,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.20094294846057892,
      "learning_rate": 2.44e-05,
      "loss": 0.0016,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.638695240020752,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.0013,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.9639939069747925,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0012,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.7949791550636292,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0011,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.33337414264678955,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.001,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.2671302855014801,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0007,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.20865517854690552,
      "learning_rate": 2.432e-05,
      "loss": 0.0009,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.12124207615852356,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0007,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.4031315743923187,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0006,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.3230789601802826,
      "learning_rate": 2.428e-05,
      "loss": 0.0008,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.19249340891838074,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0006,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.3358877897262573,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0006,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.24657930433750153,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0014,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.14417675137519836,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.0012,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.15245173871517181,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0008,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.1271577775478363,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0008,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.14026278257369995,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.0003,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.07497728615999222,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.0005,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.3337880074977875,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0008,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.10510289669036865,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0006,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.11285419017076492,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0008,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.1353640854358673,
      "learning_rate": 2.412e-05,
      "loss": 0.0008,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.5646523833274841,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.0007,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.49484673142433167,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0015,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.14188972115516663,
      "learning_rate": 2.408e-05,
      "loss": 0.0015,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.21206021308898926,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0014,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.2589606046676636,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0013,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.16589489579200745,
      "learning_rate": 2.404e-05,
      "loss": 0.0016,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.10981451719999313,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0008,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.0,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.0006,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.09859952330589294,
      "learning_rate": 2.4e-05,
      "loss": 0.0015,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.23906685411930084,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.001,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.4134468138217926,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0006,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.10093097388744354,
      "learning_rate": 2.396e-05,
      "loss": 0.0012,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.3212968707084656,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0015,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.24428512156009674,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0016,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.12498298287391663,
      "learning_rate": 2.392e-05,
      "loss": 0.0007,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.3555794060230255,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0009,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.10404456406831741,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.0012,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.17285238206386566,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.001,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.10614632815122604,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0011,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.17036601901054382,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0008,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.13255178928375244,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0013,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.1041816994547844,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.0009,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.14022967219352722,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0007,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.0,
      "learning_rate": 2.38e-05,
      "loss": 0.0014,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.22325408458709717,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0016,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.10510879009962082,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0009,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.09308701008558273,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0009,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.4619717001914978,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.001,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.39011111855506897,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0004,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.1160397008061409,
      "learning_rate": 2.372e-05,
      "loss": 0.0007,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.14151781797409058,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0013,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.35069355368614197,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0009,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.11104628443717957,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0005,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.12100959569215775,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0011,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.17427591979503632,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0012,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.17869360744953156,
      "learning_rate": 2.364e-05,
      "loss": 0.0005,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.0007,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.21480315923690796,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.001,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.1395464539527893,
      "learning_rate": 2.36e-05,
      "loss": 0.0011,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.18078754842281342,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0011,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.0003,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.0,
      "learning_rate": 2.356e-05,
      "loss": 0.0012,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.13552559912204742,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0009,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.16706961393356323,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0003,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.15319101512432098,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0013,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.2156626582145691,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0009,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.08059553056955338,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0011,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.22941578924655914,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0004,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0009,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.1505952924489975,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0011,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.20779825747013092,
      "learning_rate": 2.344e-05,
      "loss": 0.0006,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.06995214521884918,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.0015,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.13046129047870636,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0011,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.24884247779846191,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0013,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.1702270656824112,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0007,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.2580564618110657,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.0011,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.3074178397655487,
      "learning_rate": 2.336e-05,
      "loss": 0.0011,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.41519877314567566,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0006,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0007,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.13911311328411102,
      "learning_rate": 2.332e-05,
      "loss": 0.0006,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.19444283843040466,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0014,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.16174985468387604,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.0012,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.4643753170967102,
      "learning_rate": 2.328e-05,
      "loss": 0.0005,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.386728435754776,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0008,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.28858932852745056,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.0012,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.13444004952907562,
      "learning_rate": 2.324e-05,
      "loss": 0.0003,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.13842669129371643,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0008,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.29544925689697266,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0011,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.08881676942110062,
      "learning_rate": 2.32e-05,
      "loss": 0.0004,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.10264632105827332,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.0003,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.3802933692932129,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0009,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.09883738309144974,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0011,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.30792471766471863,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0017,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.5056560039520264,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0012,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.16723422706127167,
      "learning_rate": 2.312e-05,
      "loss": 0.0015,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.20612221956253052,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0007,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.12808963656425476,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0008,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.14945437014102936,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0012,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.2964351177215576,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.001,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.47711730003356934,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.0007,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.18875902891159058,
      "learning_rate": 2.304e-05,
      "loss": 0.0006,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.24560332298278809,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.0009,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.3177167773246765,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0013,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.2876470685005188,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0008,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.2226482629776001,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0004,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.1990772932767868,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0008,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.15177354216575623,
      "learning_rate": 2.296e-05,
      "loss": 0.0009,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.2326854020357132,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0006,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0015,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.252985417842865,
      "learning_rate": 2.292e-05,
      "loss": 0.0014,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.2268448919057846,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0007,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.15659427642822266,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0008,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.1172468438744545,
      "learning_rate": 2.288e-05,
      "loss": 0.0011,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.20651239156723022,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0013,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.1533573716878891,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.001,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.18786005675792694,
      "learning_rate": 2.284e-05,
      "loss": 0.0011,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.1148686558008194,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0008,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.1943085491657257,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0004,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.1988663673400879,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0008,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.1887015700340271,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.0016,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.2994939386844635,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.0014,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.6098525524139404,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.001,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.39104610681533813,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.002,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0004,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.0940091609954834,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0013,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.3980898857116699,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.0008,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.09009425342082977,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0007,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.08661843836307526,
      "learning_rate": 2.268e-05,
      "loss": 0.0012,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.48390424251556396,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0014,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.18189865350723267,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0007,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.32379910349845886,
      "learning_rate": 2.264e-05,
      "loss": 0.001,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.2317788153886795,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0006,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.43688148260116577,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0008,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.22223304212093353,
      "learning_rate": 2.26e-05,
      "loss": 0.001,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.2606453001499176,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.001,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.2466917186975479,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0008,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.20752698183059692,
      "learning_rate": 2.256e-05,
      "loss": 0.0019,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.20169581472873688,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0009,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.0968271866440773,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0009,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.14930790662765503,
      "learning_rate": 2.252e-05,
      "loss": 0.0008,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.1715097427368164,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0007,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.10728326439857483,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0011,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.2661665976047516,
      "learning_rate": 2.248e-05,
      "loss": 0.0005,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.16744235157966614,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0013,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.1878950595855713,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0012,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.3937608301639557,
      "learning_rate": 2.244e-05,
      "loss": 0.0011,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.3124992251396179,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.001,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.1454123556613922,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0016,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.4619443118572235,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0013,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.3637327253818512,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0008,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.11359420418739319,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.0016,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.09708354622125626,
      "learning_rate": 2.236e-05,
      "loss": 0.0004,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.290703147649765,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.0006,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.283566951751709,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0012,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.23617473244667053,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.001,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.1338941752910614,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0011,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.09908368438482285,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0013,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.19453267753124237,
      "learning_rate": 2.228e-05,
      "loss": 0.0006,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.31807366013526917,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0006,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.09636758267879486,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0009,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.1465427428483963,
      "learning_rate": 2.224e-05,
      "loss": 0.0009,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.16061927378177643,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0004,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.1914042979478836,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.001,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.22069181501865387,
      "learning_rate": 2.22e-05,
      "loss": 0.0012,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.16080546379089355,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.0013,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.23639683425426483,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0008,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.18493132293224335,
      "learning_rate": 2.216e-05,
      "loss": 0.0011,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.7581769824028015,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.001,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.1371920257806778,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0006,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.10882683098316193,
      "learning_rate": 2.212e-05,
      "loss": 0.0004,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.13766750693321228,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0007,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.19543053209781647,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.0005,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.2932165861129761,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0009,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.4231586456298828,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0004,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.2914908528327942,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0007,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.23496954143047333,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0006,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.17185750603675842,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.0008,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.7282532453536987,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.0009,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.0,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.001,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.1385689228773117,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0006,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.33014804124832153,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0011,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.299038827419281,
      "learning_rate": 2.196e-05,
      "loss": 0.0005,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.2571089267730713,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0011,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.16136294603347778,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0009,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.20505285263061523,
      "learning_rate": 2.192e-05,
      "loss": 0.0007,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.1528354436159134,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.0008,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.1788499802350998,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0011,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.07912235707044601,
      "learning_rate": 2.188e-05,
      "loss": 0.0003,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.31702104210853577,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0017,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.216989204287529,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0006,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.12752752006053925,
      "learning_rate": 2.184e-05,
      "loss": 0.0007,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.18243373930454254,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0008,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.13881593942642212,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0004,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.15382911264896393,
      "learning_rate": 2.18e-05,
      "loss": 0.0013,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.18163879215717316,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0015,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.08508466184139252,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0011,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.14228405058383942,
      "learning_rate": 2.176e-05,
      "loss": 0.001,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.13387294113636017,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0008,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.27778035402297974,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0004,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.11327748000621796,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0006,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.18536342680454254,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.0007,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.09308366477489471,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0011,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.19402959942817688,
      "learning_rate": 2.168e-05,
      "loss": 0.0005,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.3762909173965454,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.001,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.13005496561527252,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0009,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.32819876074790955,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0006,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.14851117134094238,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0011,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.239839568734169,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0006,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.14030230045318604,
      "learning_rate": 2.16e-05,
      "loss": 0.0007,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.1692003756761551,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.001,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.18018758296966553,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0011,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.12859386205673218,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0004,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.26374319195747375,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0015,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.2830968499183655,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0009,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.36061742901802063,
      "learning_rate": 2.152e-05,
      "loss": 0.0004,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.05088863894343376,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0009,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.28895026445388794,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0011,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.14507131278514862,
      "learning_rate": 2.148e-05,
      "loss": 0.0005,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.28492245078086853,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0007,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.10531830042600632,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0006,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.30397576093673706,
      "learning_rate": 2.144e-05,
      "loss": 0.0004,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.08400274813175201,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0011,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.18155163526535034,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.0011,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.18553774058818817,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0002,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.17378480732440948,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.001,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.13046912848949432,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.0019,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.13449253141880035,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0004,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.271488219499588,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0005,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.3887125253677368,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.001,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.17701826989650726,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.0008,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.212437242269516,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.001,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.10326928645372391,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.0013,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.10479999333620071,
      "learning_rate": 2.128e-05,
      "loss": 0.0008,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.2842928469181061,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0007,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.18636462092399597,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.001,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.27331480383872986,
      "learning_rate": 2.124e-05,
      "loss": 0.0009,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.3684946894645691,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.0008,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.3241123557090759,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0008,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.26305779814720154,
      "learning_rate": 2.12e-05,
      "loss": 0.0003,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.1688547283411026,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.001,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.08886965364217758,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0005,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.3161928355693817,
      "learning_rate": 2.116e-05,
      "loss": 0.001,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.13849101960659027,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.001,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.1477186232805252,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0004,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.30603736639022827,
      "learning_rate": 2.112e-05,
      "loss": 0.0012,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.29384616017341614,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0014,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.14676880836486816,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.001,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.31665265560150146,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0007,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.47822025418281555,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.001,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.20544882118701935,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0007,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.36377274990081787,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0007,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.2594304084777832,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0012,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.37092575430870056,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.0006,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.24871359765529633,
      "learning_rate": 2.1e-05,
      "loss": 0.001,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.17134670913219452,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.0005,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.23774242401123047,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.0008,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.09694947302341461,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.001,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.001,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.5329983830451965,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.001,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.2115945667028427,
      "learning_rate": 2.092e-05,
      "loss": 0.0008,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.2816562354564667,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.0008,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.34985053539276123,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0011,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.14882458746433258,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0004,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.3139093220233917,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0004,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.40046727657318115,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.0012,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.09819219261407852,
      "learning_rate": 2.084e-05,
      "loss": 0.0008,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.14707067608833313,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0005,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.6054941415786743,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0007,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.36344099044799805,
      "learning_rate": 2.08e-05,
      "loss": 0.0008,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.2319493293762207,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.0006,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.08757922798395157,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.0007,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.15212267637252808,
      "learning_rate": 2.076e-05,
      "loss": 0.0009,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.1323925405740738,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0003,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.1520354300737381,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0006,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.2143707424402237,
      "learning_rate": 2.072e-05,
      "loss": 0.0008,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.1574072241783142,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0011,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.11357346177101135,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0007,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.24029773473739624,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.001,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.1137179434299469,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.001,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.11777740716934204,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.0009,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.08457306027412415,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0012,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.14923124015331268,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0006,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.0006,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.19250331819057465,
      "learning_rate": 2.06e-05,
      "loss": 0.0007,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.40750494599342346,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0016,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.24489863216876984,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0009,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.3606983423233032,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0016,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.07917170226573944,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.001,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.5592603087425232,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0012,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.2256026417016983,
      "learning_rate": 2.052e-05,
      "loss": 0.0005,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.1931503862142563,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.0005,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.34345611929893494,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0018,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.16982696950435638,
      "learning_rate": 2.048e-05,
      "loss": 0.0016,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.31805869936943054,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.001,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.1993228644132614,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0009,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.07489525526762009,
      "learning_rate": 2.044e-05,
      "loss": 0.0009,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.09312429279088974,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0008,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.10116470605134964,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0006,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.11008875072002411,
      "learning_rate": 2.04e-05,
      "loss": 0.0005,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0007,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0009,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.13028854131698608,
      "learning_rate": 2.036e-05,
      "loss": 0.0006,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.16007156670093536,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0014,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.662960410118103,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0008,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.08776292204856873,
      "learning_rate": 2.032e-05,
      "loss": 0.0005,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.11414019018411636,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0009,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.08466640114784241,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0014,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.0,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0007,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.19001777470111847,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0006,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.11570446938276291,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0007,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.38622334599494934,
      "learning_rate": 2.024e-05,
      "loss": 0.0013,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.4880273640155792,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0017,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.6106237769126892,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.001,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.17999979853630066,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0008,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.08317850530147552,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0008,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.3481588065624237,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.001,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.2652983069419861,
      "learning_rate": 2.016e-05,
      "loss": 0.0006,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.2642681300640106,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0006,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.14723464846611023,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.001,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.35126495361328125,
      "learning_rate": 2.012e-05,
      "loss": 0.0009,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.3995850384235382,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0004,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.2108159065246582,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0008,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.12719149887561798,
      "learning_rate": 2.008e-05,
      "loss": 0.0005,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.001,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.19306740164756775,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.0009,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.22752796113491058,
      "learning_rate": 2.004e-05,
      "loss": 0.0009,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.4177044630050659,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.0009,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.18093548715114594,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0005,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.16235806047916412,
      "learning_rate": 2e-05,
      "loss": 0.0006,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.15887996554374695,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0007,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.2487339973449707,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.001,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.16249360144138336,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0013,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.15610149502754211,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.0009,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.18095475435256958,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0005,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.4342631995677948,
      "learning_rate": 1.992e-05,
      "loss": 0.0003,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.43882811069488525,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0007,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.44644901156425476,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0007,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.2907274663448334,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.001,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.29033106565475464,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0005,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.2673591077327728,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0012,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.14704692363739014,
      "learning_rate": 1.984e-05,
      "loss": 0.0009,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.2419009506702423,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.001,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.14444878697395325,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0013,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.18120825290679932,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0004,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.10545625537633896,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.0005,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.5906427502632141,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0006,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.21064426004886627,
      "learning_rate": 1.976e-05,
      "loss": 0.0012,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.26382532715797424,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0008,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.13641147315502167,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0006,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.15599918365478516,
      "learning_rate": 1.972e-05,
      "loss": 0.0006,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.12778916954994202,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.0005,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.3725482225418091,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0008,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.2694602310657501,
      "learning_rate": 1.968e-05,
      "loss": 0.001,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.16229982674121857,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0005,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.16602523624897003,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0006,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.21251614391803741,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.0008,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.16931229829788208,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0005,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.22997893393039703,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0015,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.2687288820743561,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0011,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.19938910007476807,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0007,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.15774787962436676,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0013,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.0988844484090805,
      "learning_rate": 1.956e-05,
      "loss": 0.0012,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.17029425501823425,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.001,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.3399065136909485,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0009,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.5266199707984924,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.0011,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.14460664987564087,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0005,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.1380670964717865,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0013,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.37835603952407837,
      "learning_rate": 1.948e-05,
      "loss": 0.0007,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.14105407893657684,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0008,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.17128857970237732,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0003,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.3859105706214905,
      "learning_rate": 1.944e-05,
      "loss": 0.0011,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.21592648327350616,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0007,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.652204155921936,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.0007,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.8119356036186218,
      "learning_rate": 1.94e-05,
      "loss": 0.0009,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.1850188672542572,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0013,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.21383772790431976,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0006,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.18447312712669373,
      "learning_rate": 1.936e-05,
      "loss": 0.0016,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.0813802033662796,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0007,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.11319111287593842,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0005,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.0787554606795311,
      "learning_rate": 1.932e-05,
      "loss": 0.0009,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.09885215014219284,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.001,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.15125489234924316,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0006,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.1721695512533188,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.0004,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.1951388269662857,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.001,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.18591640889644623,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.0008,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.18021687865257263,
      "learning_rate": 1.924e-05,
      "loss": 0.0005,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.14779171347618103,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.001,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.30172765254974365,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0015,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.12725594639778137,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0007,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.4499128758907318,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.0007,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.21598617732524872,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.001,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.16742175817489624,
      "learning_rate": 1.916e-05,
      "loss": 0.0007,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.16194258630275726,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.001,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.15826672315597534,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0007,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.26362287998199463,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.0006,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.13713832199573517,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.001,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.20947107672691345,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0006,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.16037972271442413,
      "learning_rate": 1.908e-05,
      "loss": 0.0007,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.2265165150165558,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0009,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.08732923120260239,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0008,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.11728324741125107,
      "learning_rate": 1.904e-05,
      "loss": 0.0007,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.14218904078006744,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0008,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.1881980150938034,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0003,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.21648655831813812,
      "learning_rate": 1.9e-05,
      "loss": 0.0009,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.1452232152223587,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0008,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.11207405477762222,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.0009,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.113025963306427,
      "learning_rate": 1.896e-05,
      "loss": 0.001,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.09692445397377014,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0011,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.18945424258708954,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0007,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.33922773599624634,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.001,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.646825909614563,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.0008,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.1963002234697342,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.001,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.28239706158638,
      "learning_rate": 1.888e-05,
      "loss": 0.0012,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.18829156458377838,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.001,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.17784813046455383,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0004,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.3499177396297455,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0011,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.12345586717128754,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0008,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.4207214415073395,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.0005,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.18073605000972748,
      "learning_rate": 1.88e-05,
      "loss": 0.0006,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.1940426230430603,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0004,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.17656497657299042,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0011,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.20364579558372498,
      "learning_rate": 1.876e-05,
      "loss": 0.0006,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.18457132577896118,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.0005,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.3243653178215027,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0007,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.0,
      "learning_rate": 1.872e-05,
      "loss": 0.0003,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.18405050039291382,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0005,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.31460919976234436,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0007,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.1567937135696411,
      "learning_rate": 1.868e-05,
      "loss": 0.0019,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.6163658499717712,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0006,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.11100646108388901,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0006,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.45992982387542725,
      "learning_rate": 1.864e-05,
      "loss": 0.0012,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.10425294935703278,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0012,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.19267119467258453,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0003,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.3779303729534149,
      "learning_rate": 1.86e-05,
      "loss": 0.0013,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.2231907993555069,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0006,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.2593761384487152,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0005,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.17020194232463837,
      "learning_rate": 1.856e-05,
      "loss": 0.0007,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.10436859726905823,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0006,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.1109267994761467,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0009,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.1271066814661026,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0005,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.07814960181713104,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.0009,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.10986968129873276,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0006,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.09760057926177979,
      "learning_rate": 1.848e-05,
      "loss": 0.0008,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.16915945708751678,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0004,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.12396260350942612,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0012,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.07298439741134644,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0014,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.08520536124706268,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.0009,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.16101975739002228,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.001,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.14701588451862335,
      "learning_rate": 1.84e-05,
      "loss": 0.0008,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.13030238449573517,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0008,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.15731367468833923,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0016,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.41380253434181213,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0008,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.11641235649585724,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0005,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.13824565708637238,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0005,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.21848413348197937,
      "learning_rate": 1.832e-05,
      "loss": 0.0015,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.2107025682926178,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0007,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.20655599236488342,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0008,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.15211381018161774,
      "learning_rate": 1.828e-05,
      "loss": 0.0004,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.3616589307785034,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0005,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.14617830514907837,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.0015,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.15025262534618378,
      "learning_rate": 1.824e-05,
      "loss": 0.0019,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.3693501949310303,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.0009,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.2111898809671402,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0004,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.09223873913288116,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0003,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.23917505145072937,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0006,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.23262175917625427,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0007,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.07852749526500702,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0005,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.09651003032922745,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.001,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.06563932448625565,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0006,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.03953680396080017,
      "learning_rate": 1.812e-05,
      "loss": 0.0011,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.1924763321876526,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.0005,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.3215709626674652,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.001,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.19625146687030792,
      "learning_rate": 1.808e-05,
      "loss": 0.0009,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.536100447177887,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0014,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.1853390336036682,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0012,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.10255319625139236,
      "learning_rate": 1.804e-05,
      "loss": 0.0005,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.4880498945713043,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0007,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.12569937109947205,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0006,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.3248894512653351,
      "learning_rate": 1.8e-05,
      "loss": 0.001,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.258235901594162,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.0013,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.4962829351425171,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0011,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.32677894830703735,
      "learning_rate": 1.796e-05,
      "loss": 0.0008,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.18292942643165588,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0008,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.5394376516342163,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0013,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.1420850306749344,
      "learning_rate": 1.792e-05,
      "loss": 0.001,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.0004,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.23567834496498108,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0005,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.5363749265670776,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0011,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0002,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.15259793400764465,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0006,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.4404037594795227,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0006,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.283141553401947,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0006,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.0,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0004,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.19861304759979248,
      "learning_rate": 1.78e-05,
      "loss": 0.0006,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.30508115887641907,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0006,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.265392929315567,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0003,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.09296716749668121,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.0009,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.12011624127626419,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0007,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.08234398812055588,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0005,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.15027141571044922,
      "learning_rate": 1.772e-05,
      "loss": 0.0005,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.2923884689807892,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0013,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.001,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.3434545695781708,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0005,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.23241467773914337,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0005,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.3217311203479767,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0011,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.10550710558891296,
      "learning_rate": 1.764e-05,
      "loss": 0.0006,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.2154836803674698,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0006,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.03203392028808594,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0002,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.10175976902246475,
      "learning_rate": 1.76e-05,
      "loss": 0.0007,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.38433021306991577,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.001,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.1163642555475235,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.0005,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.3399585783481598,
      "learning_rate": 1.756e-05,
      "loss": 0.0013,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.09011030197143555,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0003,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.38993334770202637,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0007,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.4284061789512634,
      "learning_rate": 1.752e-05,
      "loss": 0.0008,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.22321823239326477,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.0006,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.3879361152648926,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0005,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.26801598072052,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.0008,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.1173674464225769,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0008,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.12049984186887741,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0007,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.18059681355953217,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0005,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.03396850451827049,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.0006,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.10899515450000763,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0008,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.2093982696533203,
      "learning_rate": 1.74e-05,
      "loss": 0.0004,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.14917339384555817,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0004,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.5630869269371033,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0005,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.15154939889907837,
      "learning_rate": 1.736e-05,
      "loss": 0.0004,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0006,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0014,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.34675610065460205,
      "learning_rate": 1.732e-05,
      "loss": 0.001,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.46771475672721863,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0011,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.24362078309059143,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0009,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.1608268916606903,
      "learning_rate": 1.728e-05,
      "loss": 0.0011,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.3048307001590729,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0007,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.1806701421737671,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0011,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.16952699422836304,
      "learning_rate": 1.724e-05,
      "loss": 0.0006,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.24770985543727875,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0007,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.3183036744594574,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0011,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.2362421303987503,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0007,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.3033496141433716,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0009,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.15568846464157104,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0004,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.12273463606834412,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.001,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.17056728899478912,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0009,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0005,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.18328699469566345,
      "learning_rate": 1.712e-05,
      "loss": 0.001,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.36485716700553894,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0006,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.10247725248336792,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0009,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.09120307862758636,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0006,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.17583470046520233,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.0005,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.2330888956785202,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.0011,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.33886194229125977,
      "learning_rate": 1.704e-05,
      "loss": 0.0012,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.2217174917459488,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.0016,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.21355564892292023,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.001,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.16955018043518066,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0006,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.09222854673862457,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0006,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.19430416822433472,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.0014,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.21093417704105377,
      "learning_rate": 1.696e-05,
      "loss": 0.0009,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.28123223781585693,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0014,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.34829312562942505,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0006,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.21235515177249908,
      "learning_rate": 1.692e-05,
      "loss": 0.0008,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.5585794448852539,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.001,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.2035151720046997,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0013,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.18347810208797455,
      "learning_rate": 1.688e-05,
      "loss": 0.0006,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.09841539710760117,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0011,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.3730667531490326,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0006,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.10857851803302765,
      "learning_rate": 1.684e-05,
      "loss": 0.0007,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.3073742687702179,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0008,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.17893056571483612,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0008,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.17274180054664612,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0007,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.5056935548782349,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.001,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.2572808563709259,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.002,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.10603956133127213,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.0009,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.1600310206413269,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0003,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.2376660257577896,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0007,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.6329545378684998,
      "learning_rate": 1.672e-05,
      "loss": 0.0007,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.11129027605056763,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0012,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.11977541446685791,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0011,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.4975557029247284,
      "learning_rate": 1.668e-05,
      "loss": 0.0008,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.1820286512374878,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0002,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.26742321252822876,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0013,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.24186424911022186,
      "learning_rate": 1.664e-05,
      "loss": 0.0008,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0005,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.2315940111875534,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.0005,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.0,
      "learning_rate": 1.66e-05,
      "loss": 0.0008,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.12123104184865952,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0009,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.22408165037631989,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0012,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.4196512699127197,
      "learning_rate": 1.656e-05,
      "loss": 0.0006,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.2211356908082962,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0006,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.19676928222179413,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0003,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.28898748755455017,
      "learning_rate": 1.652e-05,
      "loss": 0.0005,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.36480194330215454,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0005,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.17946085333824158,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0006,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.09626690298318863,
      "learning_rate": 1.648e-05,
      "loss": 0.0007,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.1698358654975891,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0006,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.28061315417289734,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0002,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.41325801610946655,
      "learning_rate": 1.644e-05,
      "loss": 0.0008,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.32829102873802185,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.0005,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.3106653690338135,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0008,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.4659498631954193,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0009,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.2841777503490448,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.0009,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.08626103401184082,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0007,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.2922526001930237,
      "learning_rate": 1.636e-05,
      "loss": 0.0008,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.24773703515529633,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0014,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.1512281894683838,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0007,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.17949102818965912,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0008,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.12582577764987946,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0006,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.07412555068731308,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.0014,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.18620698153972626,
      "learning_rate": 1.628e-05,
      "loss": 0.0012,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.2318379282951355,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0005,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0009,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.4067501723766327,
      "learning_rate": 1.624e-05,
      "loss": 0.0011,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.16447420418262482,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0006,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.0006,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.5585321187973022,
      "learning_rate": 1.62e-05,
      "loss": 0.0007,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.2418336570262909,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0008,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.33036938309669495,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.0009,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.21126557886600494,
      "learning_rate": 1.616e-05,
      "loss": 0.0018,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.6525601148605347,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0007,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.46997976303100586,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0005,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.40292882919311523,
      "learning_rate": 1.612e-05,
      "loss": 0.0004,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.30210036039352417,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0007,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.2443525493144989,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0015,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.20943492650985718,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0007,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.29706844687461853,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0009,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.3909042179584503,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0009,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.1934255212545395,
      "learning_rate": 1.604e-05,
      "loss": 0.0009,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.318112313747406,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.001,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.12003888189792633,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0012,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.17687280476093292,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0011,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.21100075542926788,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0008,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.320334792137146,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0005,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.15002787113189697,
      "learning_rate": 1.596e-05,
      "loss": 0.0009,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.19086766242980957,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0002,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.12307587265968323,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0004,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.1247761994600296,
      "learning_rate": 1.592e-05,
      "loss": 0.0003,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.10188274830579758,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.0004,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.19689588248729706,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0011,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.3495548665523529,
      "learning_rate": 1.588e-05,
      "loss": 0.001,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.2394562065601349,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0008,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.5610452890396118,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0004,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.26740196347236633,
      "learning_rate": 1.584e-05,
      "loss": 0.0007,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.2583000063896179,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0005,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.21230468153953552,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0006,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.11838837713003159,
      "learning_rate": 1.58e-05,
      "loss": 0.0004,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.11449456959962845,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0004,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.12492763996124268,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0004,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.0816580206155777,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0005,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.34066134691238403,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0008,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.19595979154109955,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.001,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.0,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0005,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.10081023722887039,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.0006,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.1443454474210739,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0006,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.12291853874921799,
      "learning_rate": 1.568e-05,
      "loss": 0.0005,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.1719159632921219,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0004,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.2034677267074585,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0009,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.1207776665687561,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0002,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.32389241456985474,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0005,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.2797321081161499,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0006,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.4986913204193115,
      "learning_rate": 1.56e-05,
      "loss": 0.0005,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.20469209551811218,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0008,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.679530143737793,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0006,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.1848899871110916,
      "learning_rate": 1.556e-05,
      "loss": 0.0005,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.21852795779705048,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0008,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.21565496921539307,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.001,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.3207593560218811,
      "learning_rate": 1.552e-05,
      "loss": 0.0003,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.21652327477931976,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0015,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.15028126537799835,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.0003,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.10648425668478012,
      "learning_rate": 1.548e-05,
      "loss": 0.0004,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.14797185361385345,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.001,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.23653100430965424,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0006,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.07042338699102402,
      "learning_rate": 1.544e-05,
      "loss": 0.0004,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.2688220739364624,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0012,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.31420817971229553,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.0016,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.19802992045879364,
      "learning_rate": 1.54e-05,
      "loss": 0.0012,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.19714801013469696,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0004,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.2824433743953705,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.001,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.06337732821702957,
      "learning_rate": 1.536e-05,
      "loss": 0.0014,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.46162793040275574,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0006,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.28435423970222473,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0006,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.10339903086423874,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0009,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.24771519005298615,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0012,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.11304469406604767,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.0005,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.21005560457706451,
      "learning_rate": 1.528e-05,
      "loss": 0.0009,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.11778810620307922,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0008,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.28201553225517273,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0011,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.21928752958774567,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0003,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.15583840012550354,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0009,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.11893194168806076,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0002,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.2701212167739868,
      "learning_rate": 1.52e-05,
      "loss": 0.0007,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.2304394394159317,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0006,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.1938951164484024,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0006,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.11951947212219238,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0004,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.1403440237045288,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0012,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0005,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.6914467215538025,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0014,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.19852596521377563,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0004,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.08590424060821533,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.0007,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.11517318338155746,
      "learning_rate": 1.508e-05,
      "loss": 0.0004,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.17250500619411469,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0003,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.1678386926651001,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0008,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.20862242579460144,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.001,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.46353772282600403,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0005,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.3099936246871948,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0006,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.1770784854888916,
      "learning_rate": 1.5e-05,
      "loss": 0.0013,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 0.22768418490886688,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0004,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.3154456913471222,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0007,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.34951239824295044,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.0015,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.2253352552652359,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0005,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.31419432163238525,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0008,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.7044052481651306,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0007,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.27033016085624695,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0009,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.3663002550601959,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0005,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.19873808324337006,
      "learning_rate": 1.488e-05,
      "loss": 0.001,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.09057463705539703,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0011,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0006,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.193526491522789,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0005,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.0888250470161438,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0008,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.24483388662338257,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0005,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.17643287777900696,
      "learning_rate": 1.48e-05,
      "loss": 0.0008,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.5186058878898621,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.0007,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.18761180341243744,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0009,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.2241809368133545,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0012,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.038722530007362366,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.0009,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.14858584105968475,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.001,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.10165854543447495,
      "learning_rate": 1.472e-05,
      "loss": 0.0009,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.3005097508430481,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.001,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.0805010125041008,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0004,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.11950821429491043,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0004,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.39983075857162476,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0005,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.10227705538272858,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.0006,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.30459675192832947,
      "learning_rate": 1.464e-05,
      "loss": 0.0005,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.1076776310801506,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0009,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.2781924307346344,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.0009,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.0,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.001,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.09630060195922852,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0009,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.28488948941230774,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0007,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.21571478247642517,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0006,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.0,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0001,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.16592977941036224,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0009,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.43853670358657837,
      "learning_rate": 1.452e-05,
      "loss": 0.0006,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.5507192611694336,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.0009,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.26146769523620605,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0013,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.26580294966697693,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.0007,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.2785128057003021,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0012,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.17430952191352844,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0006,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.08666250109672546,
      "learning_rate": 1.444e-05,
      "loss": 0.0009,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.1629679799079895,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.001,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.34001898765563965,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0006,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.23572097718715668,
      "learning_rate": 1.44e-05,
      "loss": 0.0015,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.14191177487373352,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0007,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.10163803398609161,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.0008,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.18753142654895782,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0007,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.18712976574897766,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.001,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.1500021070241928,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0015,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.377677857875824,
      "learning_rate": 1.432e-05,
      "loss": 0.0005,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.3808649778366089,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.0009,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.11281188577413559,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0005,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.0,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.001,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.3007175922393799,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0007,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.001,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.0,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.0006,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.198863223195076,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0014,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.11696091294288635,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0003,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.15005101263523102,
      "learning_rate": 1.42e-05,
      "loss": 0.001,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.11417567729949951,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0012,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.14148327708244324,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0007,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.17058594524860382,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.001,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.19579973816871643,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0009,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.1283026486635208,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0008,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.17768464982509613,
      "learning_rate": 1.412e-05,
      "loss": 0.0008,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.2191074788570404,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0013,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.12130708992481232,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.0003,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.0,
      "learning_rate": 1.408e-05,
      "loss": 0.0003,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.08610513806343079,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0004,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.2250012904405594,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0004,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.16452260315418243,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0003,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.1303585171699524,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0007,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.18950234353542328,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0004,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.2593715786933899,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0008,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.26521363854408264,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.0006,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.10558875650167465,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0007,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.11160080134868622,
      "learning_rate": 1.396e-05,
      "loss": 0.0004,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.08806958794593811,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.0005,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.11297579854726791,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0007,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.22510936856269836,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0005,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.11253006011247635,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0009,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.20403584837913513,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0008,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.28188657760620117,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0006,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.4656866788864136,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0011,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.05812627822160721,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0011,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.11340206861495972,
      "learning_rate": 1.384e-05,
      "loss": 0.001,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.23739421367645264,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0006,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.12828391790390015,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0003,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.26888999342918396,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0008,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.6044797897338867,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.001,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.12400788068771362,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0008,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.15472614765167236,
      "learning_rate": 1.376e-05,
      "loss": 0.001,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.33714592456817627,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0006,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.06780456751585007,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0008,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.21722723543643951,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0008,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.17302769422531128,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0009,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.3356145918369293,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0005,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.12707023322582245,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0006,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.12088871002197266,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0012,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.15923072397708893,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0007,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.1570405811071396,
      "learning_rate": 1.364e-05,
      "loss": 0.0006,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.11462868750095367,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0003,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0007,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.19442184269428253,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0013,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.1860625147819519,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0009,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0005,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.2822660505771637,
      "learning_rate": 1.356e-05,
      "loss": 0.0005,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.4595971405506134,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0013,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.19271540641784668,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0008,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.20977634191513062,
      "learning_rate": 1.352e-05,
      "loss": 0.0007,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.08884278684854507,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0014,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.5265812873840332,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0007,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.2795734703540802,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0012,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.2272396981716156,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0005,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.18163107335567474,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.0006,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.2521393597126007,
      "learning_rate": 1.344e-05,
      "loss": 0.0006,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.07837904244661331,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.0012,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.6095388531684875,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0008,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.33512040972709656,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0003,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.1575891524553299,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.001,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.19785119593143463,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0009,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.29359838366508484,
      "learning_rate": 1.336e-05,
      "loss": 0.0013,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.1103530302643776,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.001,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.21719758212566376,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0005,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.22566092014312744,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0007,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0012,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.2629343569278717,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0013,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.22836869955062866,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.0009,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.3269190788269043,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0013,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.3754361569881439,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.0011,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.20728905498981476,
      "learning_rate": 1.324e-05,
      "loss": 0.0019,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.5426722764968872,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0008,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.4059829115867615,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0006,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.0,
      "learning_rate": 1.32e-05,
      "loss": 0.0003,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.1148754209280014,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0003,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.29990848898887634,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.001,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.18211698532104492,
      "learning_rate": 1.316e-05,
      "loss": 0.0005,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.22362644970417023,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.0004,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.05993828549981117,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0005,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.13959857821464539,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0005,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0004,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.10686949640512466,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0002,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.0,
      "learning_rate": 1.308e-05,
      "loss": 0.0009,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0007,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.17711636424064636,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0004,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.208131805062294,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0006,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.12141913175582886,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0005,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.3096088767051697,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0007,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.2504356801509857,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0008,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.20503248274326324,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0006,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.2322271168231964,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0007,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.19473443925380707,
      "learning_rate": 1.296e-05,
      "loss": 0.0004,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.1898612678050995,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0009,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.25284242630004883,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0008,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.18891559541225433,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0011,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.17911837995052338,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0014,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.16266821324825287,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0006,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.18170994520187378,
      "learning_rate": 1.288e-05,
      "loss": 0.0004,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.22411058843135834,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0011,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.1583852767944336,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0009,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.17717960476875305,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0004,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.3504498600959778,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0009,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.4053593873977661,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0007,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.24466605484485626,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0006,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.15386007726192474,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0008,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.0723189190030098,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0006,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.3225138187408447,
      "learning_rate": 1.276e-05,
      "loss": 0.0007,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.1883658468723297,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0002,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.16178983449935913,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0006,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.15792520344257355,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0008,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.14444005489349365,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0004,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.4827330410480499,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0007,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.3585008978843689,
      "learning_rate": 1.268e-05,
      "loss": 0.0006,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.263445109128952,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0008,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.1674925535917282,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0005,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.14622317254543304,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.0005,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.10834476351737976,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0007,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.11917899549007416,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0005,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.21974901854991913,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0007,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.08972110599279404,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0011,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.12000288814306259,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0009,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.20355869829654694,
      "learning_rate": 1.256e-05,
      "loss": 0.0006,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 0.2873455286026001,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0004,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.49245375394821167,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0009,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.2836069166660309,
      "learning_rate": 1.252e-05,
      "loss": 0.0009,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.15405742824077606,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0006,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.092256098985672,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.0006,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.2619248330593109,
      "learning_rate": 1.248e-05,
      "loss": 0.0002,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.14698749780654907,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0006,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.17155779898166656,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0007,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.345907062292099,
      "learning_rate": 1.244e-05,
      "loss": 0.0008,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.2795887887477875,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0005,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.15838390588760376,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0007,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.29959923028945923,
      "learning_rate": 1.24e-05,
      "loss": 0.0004,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.2771511971950531,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.001,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.14688777923583984,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0004,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.42856672406196594,
      "learning_rate": 1.236e-05,
      "loss": 0.0004,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.001,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.11643943935632706,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0007,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.095772884786129,
      "learning_rate": 1.232e-05,
      "loss": 0.0007,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.09857998788356781,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0011,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0005,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.0,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0006,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.24925847351551056,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0007,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.2919962704181671,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0006,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.3526841998100281,
      "learning_rate": 1.224e-05,
      "loss": 0.0007,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.06325636804103851,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0006,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.21625497937202454,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0009,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.21196889877319336,
      "learning_rate": 1.22e-05,
      "loss": 0.0005,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.05280108004808426,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0007,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0007,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.2983416020870209,
      "learning_rate": 1.216e-05,
      "loss": 0.0005,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.45466703176498413,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0007,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.171571284532547,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0011,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.21787194907665253,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0002,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.19380170106887817,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0008,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.11066559702157974,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0002,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.11872148513793945,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0004,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.30239149928092957,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0003,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.10228627175092697,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0009,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.1531103551387787,
      "learning_rate": 1.204e-05,
      "loss": 0.0009,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.26085299253463745,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.0007,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.1676657646894455,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0008,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.11155996471643448,
      "learning_rate": 1.2e-05,
      "loss": 0.0007,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.2319989651441574,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.0006,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.32173681259155273,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0008,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.09619608521461487,
      "learning_rate": 1.196e-05,
      "loss": 0.0005,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.4682503342628479,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0004,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.1804107427597046,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.001,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.0461355596780777,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0007,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.18745681643486023,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.0005,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.3920884132385254,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0004,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.2585318088531494,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0006,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.18725955486297607,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0007,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.0875849798321724,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0004,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.3415004014968872,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0002,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.2487407773733139,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0005,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.001,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.14225278794765472,
      "learning_rate": 1.18e-05,
      "loss": 0.0007,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.40493568778038025,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0017,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.17370884120464325,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0008,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.26451027393341064,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0006,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.08370012789964676,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.001,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0007,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.1277623325586319,
      "learning_rate": 1.172e-05,
      "loss": 0.0006,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.11937489360570908,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0008,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.31136250495910645,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0007,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.12898589670658112,
      "learning_rate": 1.168e-05,
      "loss": 0.0009,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.12727747857570648,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0007,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.21307922899723053,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0008,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.14888620376586914,
      "learning_rate": 1.164e-05,
      "loss": 0.0005,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.0,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0006,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.1438502073287964,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.001,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.16296662390232086,
      "learning_rate": 1.16e-05,
      "loss": 0.0007,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.08734363317489624,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0003,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.1314706653356552,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.001,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.08741647005081177,
      "learning_rate": 1.156e-05,
      "loss": 0.0004,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.1862146109342575,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0006,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.12737637758255005,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0002,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.18898732960224152,
      "learning_rate": 1.152e-05,
      "loss": 0.0007,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.28277286887168884,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0005,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0008,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.3172508180141449,
      "learning_rate": 1.148e-05,
      "loss": 0.001,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.29347825050354004,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0008,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.4318546652793884,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0008,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.10547538846731186,
      "learning_rate": 1.144e-05,
      "loss": 0.0007,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.24590663611888885,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.0007,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.21499556303024292,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0007,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.1450001746416092,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0009,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.2389945685863495,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.0007,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.22167088091373444,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0006,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.21476948261260986,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.0005,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.18192905187606812,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0009,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.3864077031612396,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0007,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.16315653920173645,
      "learning_rate": 1.132e-05,
      "loss": 0.0006,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.1715320646762848,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0008,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.2286742776632309,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.001,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.09161438792943954,
      "learning_rate": 1.128e-05,
      "loss": 0.0007,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.09839769452810287,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0011,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.178264319896698,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0005,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.061026766896247864,
      "learning_rate": 1.124e-05,
      "loss": 0.0002,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.18939833343029022,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.0008,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.31389352679252625,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0006,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.1195102334022522,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0003,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.12613515555858612,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0003,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.12785141170024872,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.0005,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.10372535139322281,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.0003,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.18019689619541168,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.0005,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.14587046205997467,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0003,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.11587658524513245,
      "learning_rate": 1.112e-05,
      "loss": 0.0003,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.21989430487155914,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0008,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.2822660803794861,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0011,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.19581818580627441,
      "learning_rate": 1.108e-05,
      "loss": 0.0005,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.1511726677417755,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0009,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.16039077937602997,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0005,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.14749102294445038,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0007,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.11706078797578812,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0007,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.2704625129699707,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.001,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.08866745978593826,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0004,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.08702808618545532,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0007,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.22308757901191711,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.0002,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.15242213010787964,
      "learning_rate": 1.096e-05,
      "loss": 0.0006,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.14643141627311707,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0003,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.32822227478027344,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0009,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.27643340826034546,
      "learning_rate": 1.092e-05,
      "loss": 0.0008,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.28188198804855347,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0008,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.14685465395450592,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.001,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.2702105641365051,
      "learning_rate": 1.088e-05,
      "loss": 0.0005,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.11401800066232681,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0007,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.11626919358968735,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0003,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.1101650595664978,
      "learning_rate": 1.084e-05,
      "loss": 0.0005,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.11582386493682861,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0007,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.3336441218852997,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0011,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.1779859960079193,
      "learning_rate": 1.08e-05,
      "loss": 0.0009,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.42424991726875305,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.0007,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.10845061391592026,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.0008,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.3314070999622345,
      "learning_rate": 1.076e-05,
      "loss": 0.0005,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.1251705288887024,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0008,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.14601333439350128,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0006,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.15831637382507324,
      "learning_rate": 1.072e-05,
      "loss": 0.0004,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.08863480389118195,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0007,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.12072227150201797,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0004,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.11159638315439224,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0008,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.23390130698680878,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0001,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.0,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0003,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.04612990468740463,
      "learning_rate": 1.064e-05,
      "loss": 0.0004,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.08269154280424118,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0009,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.25711938738822937,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0005,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.3512003421783447,
      "learning_rate": 1.06e-05,
      "loss": 0.0012,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.11735216528177261,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0009,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.12911471724510193,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0008,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.15380626916885376,
      "learning_rate": 1.056e-05,
      "loss": 0.0006,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.19520734250545502,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0009,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.1660708785057068,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0007,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.4143863618373871,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0016,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.3426192104816437,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.001,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.10223917663097382,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.0009,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.37599948048591614,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0003,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0007,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.1079055443406105,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.001,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.24038901925086975,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0001,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.06375706195831299,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0007,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.3114199936389923,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0009,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.11548818647861481,
      "learning_rate": 1.04e-05,
      "loss": 0.0008,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.09334219992160797,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.0012,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.23316313326358795,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0002,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.1375909149646759,
      "learning_rate": 1.036e-05,
      "loss": 0.0002,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0004,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.15483304858207703,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.001,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.09105344116687775,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0009,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.28632307052612305,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0004,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.23652707040309906,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0004,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.37376296520233154,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0006,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.19146054983139038,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0006,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.18317656219005585,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0005,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.3110924959182739,
      "learning_rate": 1.024e-05,
      "loss": 0.0003,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.33320608735084534,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.0005,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.23599617183208466,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0005,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.22697950899600983,
      "learning_rate": 1.02e-05,
      "loss": 0.0007,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.07160196453332901,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0009,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.11507058143615723,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.0001,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.1624671220779419,
      "learning_rate": 1.016e-05,
      "loss": 0.0008,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.16678625345230103,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0008,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.16364091634750366,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0004,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.16984587907791138,
      "learning_rate": 1.012e-05,
      "loss": 0.0006,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.18074944615364075,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0006,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0004,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.09624642878770828,
      "learning_rate": 1.008e-05,
      "loss": 0.0003,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0003,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.22358006238937378,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0011,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.15492460131645203,
      "learning_rate": 1.004e-05,
      "loss": 0.0005,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.15740974247455597,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.0006,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.20655405521392822,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0006,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.2025614231824875,
      "learning_rate": 1e-05,
      "loss": 0.0009,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.11319935321807861,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0007,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.28912514448165894,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.0004,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.14079122245311737,
      "learning_rate": 9.96e-06,
      "loss": 0.0007,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.4463724195957184,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0005,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.14042535424232483,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0003,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.24641627073287964,
      "learning_rate": 9.92e-06,
      "loss": 0.0007,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.23529556393623352,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.0006,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.14122354984283447,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0001,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.22821828722953796,
      "learning_rate": 9.88e-06,
      "loss": 0.0003,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.0,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0003,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.21175217628479004,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0004,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.1354658454656601,
      "learning_rate": 9.84e-06,
      "loss": 0.0007,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.16582150757312775,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0005,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.12331654876470566,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0008,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.1686975657939911,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0006,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.2132672220468521,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0007,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.18555614352226257,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.0004,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.2930584251880646,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0006,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.10840308666229248,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0008,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.3108208477497101,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0007,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.3367052376270294,
      "learning_rate": 9.72e-06,
      "loss": 0.0014,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.21831366419792175,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0007,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.12764842808246613,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0003,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.28136205673217773,
      "learning_rate": 9.68e-06,
      "loss": 0.0005,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.13634639978408813,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0005,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.12714548408985138,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0006,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.06935472786426544,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.001,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.11077792942523956,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.001,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.18268844485282898,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0004,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.40779808163642883,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0009,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.09790892153978348,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0007,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.3013114035129547,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0003,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.30822354555130005,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.0003,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.0855124443769455,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0005,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.3791621923446655,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.001,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.08206828683614731,
      "learning_rate": 9.52e-06,
      "loss": 0.0008,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.4992285966873169,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0007,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.4749343991279602,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0008,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.20941618084907532,
      "learning_rate": 9.48e-06,
      "loss": 0.0001,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.11058944463729858,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0006,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.3017812669277191,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0002,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.3083341121673584,
      "learning_rate": 9.44e-06,
      "loss": 0.0005,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.09449446201324463,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.0004,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.1102917492389679,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0004,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.29366347193717957,
      "learning_rate": 9.4e-06,
      "loss": 0.0009,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.2824415862560272,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0006,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.07767977565526962,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0002,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.11820674687623978,
      "learning_rate": 9.36e-06,
      "loss": 0.0004,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.1469905823469162,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0007,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.10089704394340515,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0007,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.12778525054454803,
      "learning_rate": 9.32e-06,
      "loss": 0.0011,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.16797716915607452,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0011,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.1888905018568039,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0006,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.0829756036400795,
      "learning_rate": 9.28e-06,
      "loss": 0.0008,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.0,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0011,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.1394587606191635,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0007,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.20454448461532593,
      "learning_rate": 9.24e-06,
      "loss": 0.0007,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.13167929649353027,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0003,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.15207599103450775,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0005,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.20400914549827576,
      "learning_rate": 9.2e-06,
      "loss": 0.0006,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.18254490196704865,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.0008,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.21455079317092896,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0006,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.26375406980514526,
      "learning_rate": 9.16e-06,
      "loss": 0.0006,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.463617205619812,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0006,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.2211834043264389,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0011,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.5888775587081909,
      "learning_rate": 9.12e-06,
      "loss": 0.0004,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.14147301018238068,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0007,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.17915859818458557,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0008,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.11370410025119781,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0007,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.0,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0006,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.19883733987808228,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.0008,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.12268678843975067,
      "learning_rate": 9.04e-06,
      "loss": 0.0005,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.16088993847370148,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.0009,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.08080246299505234,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0002,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.14976313710212708,
      "learning_rate": 9e-06,
      "loss": 0.0008,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.1737566739320755,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0006,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.09890374541282654,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.001,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.11578268557786942,
      "learning_rate": 8.96e-06,
      "loss": 0.0007,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.10315219312906265,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0001,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.09265223890542984,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.001,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.14493092894554138,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0004,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.2824257016181946,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.0007,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.23280753195285797,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.001,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.315467894077301,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0008,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.4681486487388611,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0003,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.22156266868114471,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.0002,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.12580238282680511,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0009,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.3443305492401123,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0009,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.1001334860920906,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0007,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.23603513836860657,
      "learning_rate": 8.8e-06,
      "loss": 0.0004,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.10113970935344696,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0007,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.25608646869659424,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0006,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.24546973407268524,
      "learning_rate": 8.76e-06,
      "loss": 0.001,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.08628880977630615,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0011,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.06054532900452614,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0007,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.21074043214321136,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0007,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.16690143942832947,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0004,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.10926935076713562,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0008,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.1626655012369156,
      "learning_rate": 8.68e-06,
      "loss": 0.0008,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.2619616985321045,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.001,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.3270864486694336,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0008,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.11693010479211807,
      "learning_rate": 8.64e-06,
      "loss": 0.0002,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.18325211107730865,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0004,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.14904870092868805,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0005,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.07082020491361618,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0012,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.1812087893486023,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0003,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.281889945268631,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0008,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.2393864095211029,
      "learning_rate": 8.56e-06,
      "loss": 0.0007,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.34877339005470276,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0003,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.4658972918987274,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0008,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.19093306362628937,
      "learning_rate": 8.52e-06,
      "loss": 0.0005,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.1726301908493042,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.001,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.1566573679447174,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0007,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.19174574315547943,
      "learning_rate": 8.48e-06,
      "loss": 0.0007,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.35636383295059204,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0007,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.12000096589326859,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0006,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.13750863075256348,
      "learning_rate": 8.44e-06,
      "loss": 0.0006,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.18290650844573975,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0003,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.07150880247354507,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0008,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.0,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0007,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.31523290276527405,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0005,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.08489618450403214,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0005,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.12714481353759766,
      "learning_rate": 8.36e-06,
      "loss": 0.0012,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.16083307564258575,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.0006,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.13868869841098785,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0011,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.11787420511245728,
      "learning_rate": 8.32e-06,
      "loss": 0.0005,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.21865719556808472,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0012,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.18225105106830597,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0009,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.11131659150123596,
      "learning_rate": 8.28e-06,
      "loss": 0.0006,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.14232082664966583,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0007,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.14490747451782227,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.001,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.2110385000705719,
      "learning_rate": 8.24e-06,
      "loss": 0.001,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.2593684196472168,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.0009,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.18281596899032593,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0004,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.10426633805036545,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0008,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.171169251203537,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0007,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.277275025844574,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0009,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.12923365831375122,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0009,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.08541060984134674,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0009,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.17124776542186737,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0009,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.09172660857439041,
      "learning_rate": 8.12e-06,
      "loss": 0.0004,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.0,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0005,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.1183626800775528,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0013,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.17391030490398407,
      "learning_rate": 8.08e-06,
      "loss": 0.0013,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.1546654999256134,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0004,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.37878623604774475,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0003,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.4599175751209259,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0009,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.0,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0001,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.14349479973316193,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0007,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.13314571976661682,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0007,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.22474758327007294,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.0005,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0008,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.3392944633960724,
      "learning_rate": 7.96e-06,
      "loss": 0.0008,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.12497001886367798,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0003,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.19595535099506378,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0004,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.24889066815376282,
      "learning_rate": 7.92e-06,
      "loss": 0.0007,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.16204838454723358,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0004,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.11486657708883286,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0007,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.20725177228450775,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0008,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.12404952198266983,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0004,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.12764865159988403,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.0005,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.10869856178760529,
      "learning_rate": 7.84e-06,
      "loss": 0.0004,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.28739413619041443,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.0003,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.11013581603765488,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.0004,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.16115374863147736,
      "learning_rate": 7.8e-06,
      "loss": 0.0004,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.08374258130788803,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0009,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0005,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.15878824889659882,
      "learning_rate": 7.76e-06,
      "loss": 0.0006,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.0,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0002,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.0,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0001,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.0932595506310463,
      "learning_rate": 7.72e-06,
      "loss": 0.0007,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.10950039327144623,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0004,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.11569077521562576,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0004,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.19183652102947235,
      "learning_rate": 7.68e-06,
      "loss": 0.001,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.13273976743221283,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0009,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.31758764386177063,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0002,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.25551003217697144,
      "learning_rate": 7.64e-06,
      "loss": 0.0007,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.19401893019676208,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0009,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.25992149114608765,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0006,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.09644059091806412,
      "learning_rate": 7.6e-06,
      "loss": 0.0003,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.47438937425613403,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0013,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.14472506940364838,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0004,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.09908513724803925,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0007,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.0963335856795311,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0006,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.1275326907634735,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.001,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.09263208508491516,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0008,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.2384931445121765,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0005,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.1997501403093338,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0004,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.21663148701190948,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0009,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.12169645726680756,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.001,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.15154296159744263,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0007,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.15737365186214447,
      "learning_rate": 7.44e-06,
      "loss": 0.0003,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.13316263258457184,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0007,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0006,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.27418869733810425,
      "learning_rate": 7.4e-06,
      "loss": 0.0006,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.1795366108417511,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.0007,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.0931461900472641,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0011,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.23222875595092773,
      "learning_rate": 7.36e-06,
      "loss": 0.0009,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.14375941455364227,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0007,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.2982361912727356,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0008,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.13675937056541443,
      "learning_rate": 7.32e-06,
      "loss": 0.0008,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.21605409681797028,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0007,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.30879342555999756,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0004,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.0,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.0008,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.17784997820854187,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0011,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.12469390034675598,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.0007,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.25146400928497314,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.0009,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.11813081800937653,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.0004,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.1568651795387268,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0006,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.1557537317276001,
      "learning_rate": 7.2e-06,
      "loss": 0.0004,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.2972925007343292,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0002,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.20135398209095,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.0008,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.4857334792613983,
      "learning_rate": 7.16e-06,
      "loss": 0.0009,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.22800402343273163,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0004,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.36576828360557556,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0004,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.20250385999679565,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.0004,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.10890252143144608,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.0007,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.17442826926708221,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0006,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.24455180764198303,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0009,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.14271259307861328,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0008,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.5369724631309509,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0005,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.20150284469127655,
      "learning_rate": 7.04e-06,
      "loss": 0.0006,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.08763664215803146,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0007,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.0008,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.08076192438602448,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0006,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.10461096465587616,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0006,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.0,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0006,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.0,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0006,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.2092912644147873,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.001,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.08560077100992203,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0005,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.2056531012058258,
      "learning_rate": 6.92e-06,
      "loss": 0.001,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.16757452487945557,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0014,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.13358916342258453,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0004,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.12264184653759003,
      "learning_rate": 6.88e-06,
      "loss": 0.0002,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.2675955593585968,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0009,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.13641197979450226,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0011,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.18309344351291656,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0007,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.13819389045238495,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.0011,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.10213668644428253,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0004,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.17055954039096832,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0007,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.2386191338300705,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0011,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.02939811907708645,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0007,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.0,
      "learning_rate": 6.76e-06,
      "loss": 0.0005,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.1071331724524498,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.0012,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.11703645437955856,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0005,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.08849021047353745,
      "learning_rate": 6.72e-06,
      "loss": 0.0002,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.1853918582201004,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.0008,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.1828152984380722,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0011,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.19753232598304749,
      "learning_rate": 6.68e-06,
      "loss": 0.0007,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.0,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0008,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.11458911001682281,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.0007,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.11754368245601654,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0004,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.11298071593046188,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0009,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.29395708441734314,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.0009,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.2546387016773224,
      "learning_rate": 6.6e-06,
      "loss": 0.0009,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.09178691357374191,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0004,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.12290874868631363,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0004,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.1932741403579712,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0005,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.1055716872215271,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.001,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.0871417224407196,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0008,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.1915423423051834,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.0004,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.0,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0004,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.23221537470817566,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0006,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.15061253309249878,
      "learning_rate": 6.48e-06,
      "loss": 0.0006,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.2537998855113983,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0004,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.08722057938575745,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0006,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.1822725236415863,
      "learning_rate": 6.44e-06,
      "loss": 0.0007,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.11643999069929123,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0008,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.1696443110704422,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.0004,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.13157519698143005,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.001,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.1959364414215088,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.0002,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.12778215110301971,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0008,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.21900255978107452,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0008,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.07097478210926056,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0007,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.13915207982063293,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0004,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.14210203289985657,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0003,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.1896074265241623,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0005,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.2022889405488968,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.0009,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.17564478516578674,
      "learning_rate": 6.28e-06,
      "loss": 0.0006,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.06202450394630432,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0014,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.14431601762771606,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0003,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.0980951339006424,
      "learning_rate": 6.24e-06,
      "loss": 0.0003,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.2041916400194168,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0005,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.1865442842245102,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0003,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.1298973262310028,
      "learning_rate": 6.2e-06,
      "loss": 0.0003,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.3047075867652893,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.0006,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.09612205624580383,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0008,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.14047209918498993,
      "learning_rate": 6.16e-06,
      "loss": 0.0009,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.08100643754005432,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0003,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.11567558348178864,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0005,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.13265998661518097,
      "learning_rate": 6.12e-06,
      "loss": 0.0008,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.154420867562294,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0005,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.11574165523052216,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0006,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.10132670402526855,
      "learning_rate": 6.08e-06,
      "loss": 0.0008,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.4159661531448364,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0003,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.4336283504962921,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0002,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.1448703408241272,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0009,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.2587233781814575,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0007,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.4169667065143585,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0005,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.18273043632507324,
      "learning_rate": 6e-06,
      "loss": 0.0003,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.23226463794708252,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.0013,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.1219625398516655,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.0003,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.2280411422252655,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0005,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.2289288341999054,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0005,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.5912312865257263,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0004,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.21516655385494232,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0009,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.2969200909137726,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0005,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.15603308379650116,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0013,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.17909227311611176,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0003,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.16637389361858368,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0005,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.22507621347904205,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0003,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.17339934408664703,
      "learning_rate": 5.84e-06,
      "loss": 0.0009,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.13790956139564514,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0007,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.08378060907125473,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0007,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.2608624994754791,
      "learning_rate": 5.8e-06,
      "loss": 0.0009,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.1307365596294403,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0001,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.34412121772766113,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0003,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.08072817325592041,
      "learning_rate": 5.76e-06,
      "loss": 0.0005,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.13281464576721191,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.0009,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.21905656158924103,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0011,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.2555815875530243,
      "learning_rate": 5.72e-06,
      "loss": 0.0006,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.1647452563047409,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0009,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.2589990496635437,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0014,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.40671396255493164,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0005,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.17561130225658417,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0007,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.19146573543548584,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.0006,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.10745055228471756,
      "learning_rate": 5.64e-06,
      "loss": 0.0006,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.12744568288326263,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0002,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.2910822331905365,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0009,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.4549533724784851,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0007,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.12440985441207886,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0011,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.18879394233226776,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.0008,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.1308852881193161,
      "learning_rate": 5.56e-06,
      "loss": 0.0004,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.13774242997169495,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0003,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.23927128314971924,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0006,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.10297325253486633,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0006,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.244407519698143,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0004,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.18764391541481018,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0003,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.08665218204259872,
      "learning_rate": 5.48e-06,
      "loss": 0.0008,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.1124536320567131,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0007,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.16927330195903778,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.0002,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.1409665048122406,
      "learning_rate": 5.44e-06,
      "loss": 0.0003,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.10481400042772293,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.001,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.1725442260503769,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0004,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.13553766906261444,
      "learning_rate": 5.4e-06,
      "loss": 0.0004,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.15280501544475555,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0003,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.20229633152484894,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0009,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.19367335736751556,
      "learning_rate": 5.36e-06,
      "loss": 0.0007,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.24791760742664337,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0002,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.20161545276641846,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0004,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.11997804790735245,
      "learning_rate": 5.32e-06,
      "loss": 0.0004,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.14981694519519806,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0004,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.25206756591796875,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0007,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.1272697001695633,
      "learning_rate": 5.28e-06,
      "loss": 0.0011,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.4646386206150055,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0008,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.12918132543563843,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0003,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.08591007441282272,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0012,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.1329263299703598,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.0008,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.1882847249507904,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0004,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.16213344037532806,
      "learning_rate": 5.2e-06,
      "loss": 0.0005,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.12517577409744263,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0004,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.25097545981407166,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0009,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.1911768764257431,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0005,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.0774231106042862,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0003,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.3078584372997284,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0006,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.12634658813476562,
      "learning_rate": 5.12e-06,
      "loss": 0.0006,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.3677375316619873,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0004,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.3757760524749756,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0006,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.18392229080200195,
      "learning_rate": 5.08e-06,
      "loss": 0.0005,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.07544943690299988,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0007,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.19803322851657867,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.0009,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.06632701307535172,
      "learning_rate": 5.04e-06,
      "loss": 0.0002,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.1487126499414444,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0002,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.1215241551399231,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0005,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.13986292481422424,
      "learning_rate": 5e-06,
      "loss": 0.0004,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.18664442002773285,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0008,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.09585589915513992,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0005,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.07823850959539413,
      "learning_rate": 4.96e-06,
      "loss": 0.0004,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.09741383790969849,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.0007,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.1712634265422821,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0008,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.18648678064346313,
      "learning_rate": 4.92e-06,
      "loss": 0.0004,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.13489945232868195,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0006,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.23738570511341095,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.0009,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.37305930256843567,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.0012,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.18760527670383453,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0006,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.13089360296726227,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0002,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.30952921509742737,
      "learning_rate": 4.84e-06,
      "loss": 0.0004,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.1892111450433731,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0007,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.3179946541786194,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0007,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.13240520656108856,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0004,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.11675216257572174,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.0003,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.12978453934192657,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.0008,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.09058818966150284,
      "learning_rate": 4.76e-06,
      "loss": 0.0006,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.19728602468967438,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.0007,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0008,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.0,
      "learning_rate": 4.72e-06,
      "loss": 0.0006,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.08181465417146683,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0008,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.2808878719806671,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0006,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.10544198006391525,
      "learning_rate": 4.68e-06,
      "loss": 0.0008,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.1168031170964241,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0006,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0005,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.0,
      "learning_rate": 4.64e-06,
      "loss": 0.0005,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.15656131505966187,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0007,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.20391295850276947,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.0008,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.09909404814243317,
      "learning_rate": 4.6e-06,
      "loss": 0.0005,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.15233679115772247,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0003,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.13610196113586426,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0009,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.2138485610485077,
      "learning_rate": 4.56e-06,
      "loss": 0.0002,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.2710307836532593,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0008,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.1537248194217682,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0003,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.11790981143712997,
      "learning_rate": 4.52e-06,
      "loss": 0.0012,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.3909820318222046,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0004,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.1674446314573288,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.0001,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.087164506316185,
      "learning_rate": 4.48e-06,
      "loss": 0.0005,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.14562731981277466,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0004,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.19092516601085663,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0007,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.24279329180717468,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0004,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.45484626293182373,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0008,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.30517321825027466,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0011,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.11927871406078339,
      "learning_rate": 4.4e-06,
      "loss": 0.0003,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0005,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.16650152206420898,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0006,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.10685616731643677,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0004,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.1776919960975647,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0003,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.1397397667169571,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0003,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.250814825296402,
      "learning_rate": 4.32e-06,
      "loss": 0.0004,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.18540708720684052,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0006,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.13616132736206055,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0008,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.18677236139774323,
      "learning_rate": 4.28e-06,
      "loss": 0.0009,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.13762938976287842,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0003,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.114957295358181,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0004,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.1382104605436325,
      "learning_rate": 4.24e-06,
      "loss": 0.0004,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.0006,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.10260465741157532,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0007,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.1632138341665268,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0009,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.17528508603572845,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.0011,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.07147423177957535,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0006,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.0,
      "learning_rate": 4.16e-06,
      "loss": 0.0005,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.1312440186738968,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0003,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.1430930495262146,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0006,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.1828429102897644,
      "learning_rate": 4.12e-06,
      "loss": 0.0004,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.0,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0008,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.332887202501297,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0004,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.10185285657644272,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0005,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.15597227215766907,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0006,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.1445353925228119,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0003,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.09795268625020981,
      "learning_rate": 4.04e-06,
      "loss": 0.0006,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.15961019694805145,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0009,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.18607980012893677,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.001,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.2203795164823532,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0004,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0004,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0006,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.0167173370718956,
      "learning_rate": 3.96e-06,
      "loss": 0.0008,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.1534392088651657,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.001,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.13394787907600403,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0009,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.1885460913181305,
      "learning_rate": 3.92e-06,
      "loss": 0.0009,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.09819924831390381,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0006,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.10797818005084991,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0005,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.0,
      "learning_rate": 3.88e-06,
      "loss": 0.0006,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.27324509620666504,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0005,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.04785962030291557,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.001,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.01752409152686596,
      "learning_rate": 3.84e-06,
      "loss": 0.0007,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.10671702772378922,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0008,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.13161435723304749,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0005,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.09951748698949814,
      "learning_rate": 3.8e-06,
      "loss": 0.0005,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.13927172124385834,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0005,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.3017328679561615,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0006,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.3059713542461395,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0007,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.25462988018989563,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0012,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.31578028202056885,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.001,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.21282894909381866,
      "learning_rate": 3.72e-06,
      "loss": 0.0008,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.199596107006073,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0005,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.2955656945705414,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0007,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.13319160044193268,
      "learning_rate": 3.68e-06,
      "loss": 0.0004,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.0,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0005,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.1653677076101303,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0007,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.11405704915523529,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.0008,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.26077061891555786,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0013,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.1411493718624115,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0001,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.0,
      "learning_rate": 3.6e-06,
      "loss": 0.0002,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.3270919620990753,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0005,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.1226481944322586,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.0008,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.17505495250225067,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0005,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.14571689069271088,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.001,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.14818662405014038,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0005,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.29024356603622437,
      "learning_rate": 3.52e-06,
      "loss": 0.0005,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.12968161702156067,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.001,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.0,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.0005,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.07499675452709198,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0008,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.10367242246866226,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0006,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.11142431944608688,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0012,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.0,
      "learning_rate": 3.44e-06,
      "loss": 0.0005,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.2204713672399521,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0004,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.07560506463050842,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0007,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.16344720125198364,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.001,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.18942826986312866,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0005,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.2140878587961197,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0005,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.14322517812252045,
      "learning_rate": 3.36e-06,
      "loss": 0.0006,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.2762337625026703,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0006,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.21473532915115356,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0007,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.12454112619161606,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.001,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.10345491766929626,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0005,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.04975351691246033,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0006,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.4452531039714813,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0005,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.15838667750358582,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0006,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.14579305052757263,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.001,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.11823530495166779,
      "learning_rate": 3.24e-06,
      "loss": 0.0004,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.09674619138240814,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0003,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.08746901899576187,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0004,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.25537607073783875,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0004,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.16327007114887238,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.001,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.09558712691068649,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0004,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.1506142020225525,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.0009,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.12028844654560089,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0005,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.33212101459503174,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0004,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.17431294918060303,
      "learning_rate": 3.12e-06,
      "loss": 0.0009,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.2007141411304474,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0006,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.11698994785547256,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0002,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.2297908365726471,
      "learning_rate": 3.08e-06,
      "loss": 0.0008,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.22493574023246765,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0007,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.10148251056671143,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0005,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.0,
      "learning_rate": 3.04e-06,
      "loss": 0.0005,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.11223973333835602,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0011,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.09248177707195282,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0012,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.11809319257736206,
      "learning_rate": 3e-06,
      "loss": 0.0004,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0008,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.18378087878227234,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0008,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.11550844460725784,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.0009,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.18972186744213104,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0002,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.21528096497058868,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0007,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.08368457108736038,
      "learning_rate": 2.92e-06,
      "loss": 0.0005,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.43739891052246094,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0007,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.14144328236579895,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0005,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.0,
      "learning_rate": 2.88e-06,
      "loss": 0.0004,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.2438782900571823,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0005,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.0,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0005,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.33750173449516296,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0005,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.08370159566402435,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0003,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0006,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.129177987575531,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0003,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.0002,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.09377181529998779,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0001,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.4092246890068054,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0007,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.17480087280273438,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0006,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.10865301638841629,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0002,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.26659461855888367,
      "learning_rate": 2.72e-06,
      "loss": 0.0007,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.10120225697755814,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0009,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.25889891386032104,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0009,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.06400950998067856,
      "learning_rate": 2.68e-06,
      "loss": 0.0005,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.15736635029315948,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0011,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.1806531399488449,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0005,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.12208892405033112,
      "learning_rate": 2.64e-06,
      "loss": 0.0004,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.20849332213401794,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0008,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0005,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.1724657416343689,
      "learning_rate": 2.6e-06,
      "loss": 0.0002,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.19297672808170319,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.0003,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.13174743950366974,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0004,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.3622351288795471,
      "learning_rate": 2.56e-06,
      "loss": 0.0006,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.0,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0006,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0007,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.17168164253234863,
      "learning_rate": 2.52e-06,
      "loss": 0.0007,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0003,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.1376814842224121,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0003,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.2551749348640442,
      "learning_rate": 2.48e-06,
      "loss": 0.0006,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.1006522998213768,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0008,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.24402298033237457,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0009,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.11045119911432266,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0006,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.1592835783958435,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0008,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.18115539848804474,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0004,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.16000314056873322,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0001,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.16857632994651794,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0005,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.2439807504415512,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0007,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.18521246314048767,
      "learning_rate": 2.36e-06,
      "loss": 0.0003,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.10362863540649414,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0009,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.17129814624786377,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0004,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.1640678197145462,
      "learning_rate": 2.32e-06,
      "loss": 0.0005,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.08294329047203064,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.0013,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.15316690504550934,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0002,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.10765776038169861,
      "learning_rate": 2.28e-06,
      "loss": 0.0004,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.24167637526988983,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0003,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.25437602400779724,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0005,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.08540086448192596,
      "learning_rate": 2.24e-06,
      "loss": 0.0007,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.12876394391059875,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0006,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0008,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.18329671025276184,
      "learning_rate": 2.2e-06,
      "loss": 0.001,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0004,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.19810912013053894,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.0007,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.3598218858242035,
      "learning_rate": 2.16e-06,
      "loss": 0.0012,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.11563758552074432,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0011,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.24737343192100525,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0004,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.12357047200202942,
      "learning_rate": 2.12e-06,
      "loss": 0.001,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.09950577467679977,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0006,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.21616306900978088,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0004,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.1334751695394516,
      "learning_rate": 2.08e-06,
      "loss": 0.0003,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.0715717300772667,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.16275905072689056,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.0004,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.12941676378250122,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0002,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.1415775716304779,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.0006,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.09261348098516464,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0004,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.12961767613887787,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0006,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.10641966015100479,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.001,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.11026792228221893,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0002,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.15725316107273102,
      "learning_rate": 1.96e-06,
      "loss": 0.0006,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.11022049188613892,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0005,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.18547837436199188,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0002,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.11066421121358871,
      "learning_rate": 1.92e-06,
      "loss": 0.0011,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.24694032967090607,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0014,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0004,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.2013714760541916,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0004,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.1114133968949318,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0009,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.18903066217899323,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0004,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.26674777269363403,
      "learning_rate": 1.84e-06,
      "loss": 0.0004,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.18775378167629242,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0006,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.1019081398844719,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.0005,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.10239369422197342,
      "learning_rate": 1.8e-06,
      "loss": 0.0009,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.10545359551906586,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0007,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.13867664337158203,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0006,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.09428136050701141,
      "learning_rate": 1.76e-06,
      "loss": 0.0004,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.16813655197620392,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0006,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.20142722129821777,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0005,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.21404005587100983,
      "learning_rate": 1.72e-06,
      "loss": 0.0001,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.12741178274154663,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0006,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.1530344933271408,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0005,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.08551197499036789,
      "learning_rate": 1.68e-06,
      "loss": 0.0003,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.18211500346660614,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0003,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.1685165911912918,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.0006,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.24169591069221497,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0006,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.2867942452430725,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.0011,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.23791831731796265,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0005,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.19864416122436523,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0005,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.11280786991119385,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0006,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.3180266320705414,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0007,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.0,
      "learning_rate": 1.56e-06,
      "loss": 0.0006,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.3408459424972534,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0003,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.10650953650474548,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0003,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.13892251253128052,
      "learning_rate": 1.52e-06,
      "loss": 0.0003,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.22573599219322205,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0006,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.39062416553497314,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0001,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.1274697482585907,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0006,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.12819381058216095,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0003,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.0001,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.22429494559764862,
      "learning_rate": 1.44e-06,
      "loss": 0.0007,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.0009,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.2498982846736908,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0003,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.14695140719413757,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0003,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.13165532052516937,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.0008,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.16182993352413177,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0004,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.2513735592365265,
      "learning_rate": 1.36e-06,
      "loss": 0.001,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.11588560044765472,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0005,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0003,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.10032670199871063,
      "learning_rate": 1.32e-06,
      "loss": 0.0002,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.19914215803146362,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0007,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.12691330909729004,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0009,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.08560769259929657,
      "learning_rate": 1.28e-06,
      "loss": 0.0003,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.2025679349899292,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0004,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.20237009227275848,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.0003,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.0,
      "learning_rate": 1.24e-06,
      "loss": 0.001,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.18291939795017242,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0004,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.11978030949831009,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.0006,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.21877984702587128,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0003,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.16378062963485718,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0006,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.17738279700279236,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0011,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.11746581643819809,
      "learning_rate": 1.16e-06,
      "loss": 0.0004,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.0,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.0002,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.14598014950752258,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0008,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.1220443844795227,
      "learning_rate": 1.12e-06,
      "loss": 0.0009,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.32957443594932556,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.0007,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0005,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.19564129412174225,
      "learning_rate": 1.08e-06,
      "loss": 0.0006,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.19855722784996033,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0009,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.2522861361503601,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.0004,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.13964159786701202,
      "learning_rate": 1.04e-06,
      "loss": 0.0003,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.0003,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.13108709454536438,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0007,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.0,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0008,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 0.1100962907075882,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0008,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.10141456872224808,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.0014,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.09510847926139832,
      "learning_rate": 9.6e-07,
      "loss": 0.0003,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.11663012951612473,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.12952804565429688,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0004,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.1247163638472557,
      "learning_rate": 9.2e-07,
      "loss": 0.0006,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.11045219004154205,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0001,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.042212918400764465,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.0008,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.2327299565076828,
      "learning_rate": 8.8e-07,
      "loss": 0.0004,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.10588112473487854,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0007,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.1250060498714447,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.0008,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.1728253811597824,
      "learning_rate": 8.4e-07,
      "loss": 0.001,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.0,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0011,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.19148872792720795,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0004,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.10510896146297455,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0006,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.0892503559589386,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.0006,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.13362403213977814,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0008,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.1114618331193924,
      "learning_rate": 7.6e-07,
      "loss": 0.0008,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.11230488121509552,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0003,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.1799660623073578,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0003,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.08355405926704407,
      "learning_rate": 7.2e-07,
      "loss": 0.0004,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.16578160226345062,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0004,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.1308731734752655,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.0007,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.20935401320457458,
      "learning_rate": 6.8e-07,
      "loss": 0.0004,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.13634152710437775,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0004,
      "step": 37000
    }
  ],
  "logging_steps": 10,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

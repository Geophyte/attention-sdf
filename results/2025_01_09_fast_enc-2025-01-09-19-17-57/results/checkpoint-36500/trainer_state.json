{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9466666666666668,
  "eval_steps": 500,
  "global_step": 36500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 0.09411536157131195,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0128,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.11777599900960922,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0053,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.06749992817640305,
      "learning_rate": 4.996e-05,
      "loss": 0.0038,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.12739239633083344,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0045,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.18859319388866425,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0031,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.1866399049758911,
      "learning_rate": 4.992e-05,
      "loss": 0.0032,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0033,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.04862592741847038,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0037,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.10657472908496857,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0049,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.1449624001979828,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0036,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.1888507604598999,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0021,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.016718508675694466,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0041,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.398092657327652,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.0035,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.06872797757387161,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0029,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.030015572905540466,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0036,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.23614399135112762,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0045,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.1880355328321457,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.0031,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.07591351866722107,
      "learning_rate": 4.976e-05,
      "loss": 0.0036,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.1878928244113922,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0044,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.18928055465221405,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0034,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.1262340396642685,
      "learning_rate": 4.972e-05,
      "loss": 0.0034,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.14549431204795837,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0042,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.18969693779945374,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0046,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.0,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.0026,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.07327879220247269,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0018,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.18708913028240204,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.004,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.18347185850143433,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0046,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.86432945728302,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0047,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.30491510033607483,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0039,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.7962618470191956,
      "learning_rate": 4.96e-05,
      "loss": 0.0035,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.12760330736637115,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0035,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.034555088728666306,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0023,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.1330125629901886,
      "learning_rate": 4.956e-05,
      "loss": 0.0031,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.18568076193332672,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0038,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.06163012981414795,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0035,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.2463637739419937,
      "learning_rate": 4.952e-05,
      "loss": 0.0039,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.09076705574989319,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0041,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.4375598430633545,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.004,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.7235539555549622,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0032,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.20068359375,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.004,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.19603906571865082,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.003,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.06025077402591705,
      "learning_rate": 4.944e-05,
      "loss": 0.0028,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.18419349193572998,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0032,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.18816085159778595,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0033,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.06830014288425446,
      "learning_rate": 4.94e-05,
      "loss": 0.0021,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.18173851072788239,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0023,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.24839258193969727,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0041,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.12711280584335327,
      "learning_rate": 4.936e-05,
      "loss": 0.0022,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.10010430961847305,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0033,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.12701445817947388,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0037,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.07190098613500595,
      "learning_rate": 4.932e-05,
      "loss": 0.0026,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.2423427850008011,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0037,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.18848752975463867,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0032,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.06924550235271454,
      "learning_rate": 4.928e-05,
      "loss": 0.0027,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0023,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.18244144320487976,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0035,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.6775214672088623,
      "learning_rate": 4.924e-05,
      "loss": 0.0038,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.13753202557563782,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0037,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.12504422664642334,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0036,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.21635371446609497,
      "learning_rate": 4.92e-05,
      "loss": 0.0035,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.1848183423280716,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.004,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.6047077775001526,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0032,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.057680029422044754,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0028,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.2477426677942276,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0034,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.0688064768910408,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0024,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.06824474781751633,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.0029,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.07813181728124619,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.003,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.12025560438632965,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.0032,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.4327063262462616,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0032,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.6700074672698975,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.004,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.10843734443187714,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0031,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.07234402000904083,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.0032,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.13373754918575287,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0032,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.06870832294225693,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0029,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.2498111128807068,
      "learning_rate": 4.9e-05,
      "loss": 0.0031,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.21651668846607208,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0036,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.051109034568071365,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0039,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.12237141281366348,
      "learning_rate": 4.896e-05,
      "loss": 0.0036,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.07121367007493973,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.0026,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.06561516225337982,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0029,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.06941714882850647,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.0038,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.17807281017303467,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.0036,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.040690917521715164,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0035,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.3676750063896179,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0025,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.12263280898332596,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0039,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.05958467349410057,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.003,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.0,
      "learning_rate": 4.884e-05,
      "loss": 0.0026,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.3571471571922302,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0048,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.17795781791210175,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0035,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.06836560368537903,
      "learning_rate": 4.88e-05,
      "loss": 0.0025,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.12929126620292664,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0032,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.044434987008571625,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0029,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.0838530957698822,
      "learning_rate": 4.876e-05,
      "loss": 0.003,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.0466499961912632,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0039,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.0630832090973854,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0035,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.47025102376937866,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.0039,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.12444598227739334,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0039,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.062463369220495224,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0032,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.04088876023888588,
      "learning_rate": 4.868e-05,
      "loss": 0.0023,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0018,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.12085603922605515,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.003,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.18440377712249756,
      "learning_rate": 4.864e-05,
      "loss": 0.0031,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.11883615702390671,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0028,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.030747516080737114,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.0039,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.061692774295806885,
      "learning_rate": 4.86e-05,
      "loss": 0.0031,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.03081449493765831,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.004,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.24438121914863586,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0033,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.2846508324146271,
      "learning_rate": 4.856e-05,
      "loss": 0.003,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.12288674712181091,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.003,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.2379210889339447,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0027,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.2935313880443573,
      "learning_rate": 4.852e-05,
      "loss": 0.0026,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.18727336823940277,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0035,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.06596642732620239,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0028,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.060318246483802795,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0037,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.013889963738620281,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0028,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.12433423846960068,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0031,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.12454959750175476,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0031,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.09657663851976395,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0028,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.06521817296743393,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0031,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.07822622358798981,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0044,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.3051673173904419,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0024,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.08080118894577026,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0026,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.04235384985804558,
      "learning_rate": 4.836e-05,
      "loss": 0.0026,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.18830151855945587,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.0031,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.13151773810386658,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0033,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.12339209765195847,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0028,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.08140451461076736,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.003,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.30381065607070923,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0037,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.18190599977970123,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0029,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.06288846582174301,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0034,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.036362338811159134,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0039,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.0940968468785286,
      "learning_rate": 4.824e-05,
      "loss": 0.0029,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.11819566041231155,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0035,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.030754877254366875,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0026,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.08196435123682022,
      "learning_rate": 4.82e-05,
      "loss": 0.003,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.3761711120605469,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.0033,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.1293625831604004,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0026,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.07367052882909775,
      "learning_rate": 4.816e-05,
      "loss": 0.0038,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.1476663202047348,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0037,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.06605684012174606,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0028,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.06265652179718018,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0029,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.2344326227903366,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0039,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.09791082888841629,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0022,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.11912095546722412,
      "learning_rate": 4.808e-05,
      "loss": 0.0035,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.0658663958311081,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0022,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0035,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.18512141704559326,
      "learning_rate": 4.804e-05,
      "loss": 0.0041,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.18700875341892242,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0031,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.06660684198141098,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0021,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.17815516889095306,
      "learning_rate": 4.8e-05,
      "loss": 0.0039,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.2375733107328415,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.003,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0025,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.0,
      "learning_rate": 4.796e-05,
      "loss": 0.0026,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.1776374727487564,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0019,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.2569642663002014,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0029,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.1845409870147705,
      "learning_rate": 4.792e-05,
      "loss": 0.0034,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0026,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.1215110719203949,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0039,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.18383894860744476,
      "learning_rate": 4.788e-05,
      "loss": 0.0026,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.19833654165267944,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0037,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.25460341572761536,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0026,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.1945667862892151,
      "learning_rate": 4.784e-05,
      "loss": 0.0019,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.14161524176597595,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0025,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.1187795102596283,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.003,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.06704498082399368,
      "learning_rate": 4.78e-05,
      "loss": 0.0021,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.18218842148780823,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0027,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.08395674079656601,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.003,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.2977175712585449,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.005,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.12964046001434326,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0026,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.17868809401988983,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0022,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.18348771333694458,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0023,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.03053734265267849,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0036,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.06725159287452698,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.003,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.1333475261926651,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0029,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.06786920875310898,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0023,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.2570524513721466,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0027,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.05843738093972206,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0034,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.06555449962615967,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0035,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0022,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.23501232266426086,
      "learning_rate": 4.76e-05,
      "loss": 0.0047,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.1345311999320984,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0042,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.19153913855552673,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0026,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.12614992260932922,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0027,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.3644896149635315,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.003,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.17877152562141418,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0018,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.06208105385303497,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0031,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.13164594769477844,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0021,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.11698917299509048,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0028,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.10553135722875595,
      "learning_rate": 4.748e-05,
      "loss": 0.0028,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.07278431206941605,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0029,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.06274255365133286,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.0028,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.07749796658754349,
      "learning_rate": 4.744e-05,
      "loss": 0.0037,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.04247117042541504,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0022,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.13595865666866302,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0033,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.1167377158999443,
      "learning_rate": 4.74e-05,
      "loss": 0.0027,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.23455582559108734,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.0029,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.09714335948228836,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0024,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.09599293768405914,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0024,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.5390619039535522,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0027,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.1852078139781952,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0039,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.11768003553152084,
      "learning_rate": 4.732e-05,
      "loss": 0.0031,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.3787533640861511,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0029,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.49953895807266235,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0037,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.13158315420150757,
      "learning_rate": 4.728e-05,
      "loss": 0.0024,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.06303507834672928,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0024,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.19032052159309387,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0033,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.19087675213813782,
      "learning_rate": 4.724e-05,
      "loss": 0.0025,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.07016792893409729,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0029,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.4120239019393921,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0033,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.18353278934955597,
      "learning_rate": 4.72e-05,
      "loss": 0.0021,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0028,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.07023356109857559,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.0019,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.1761331409215927,
      "learning_rate": 4.716e-05,
      "loss": 0.003,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.1182534471154213,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0035,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.14624594151973724,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0044,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.12113059312105179,
      "learning_rate": 4.712e-05,
      "loss": 0.0025,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.16356472671031952,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0021,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.12358198314905167,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0036,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.07001825422048569,
      "learning_rate": 4.708e-05,
      "loss": 0.0023,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.07170133292675018,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0027,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.15826088190078735,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0029,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.07224167138338089,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.004,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.12276110798120499,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.0036,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.08192130923271179,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.0031,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.13012482225894928,
      "learning_rate": 4.7e-05,
      "loss": 0.0026,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.10023386776447296,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0032,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.11806543171405792,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.0017,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.1295044869184494,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0026,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.1805155873298645,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.0033,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.1799449324607849,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0029,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.23257243633270264,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0029,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.07724392414093018,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0024,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.08919132500886917,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0045,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.24006372690200806,
      "learning_rate": 4.688e-05,
      "loss": 0.0025,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.23471491038799286,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0027,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.062357429414987564,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.003,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.04222314804792404,
      "learning_rate": 4.684e-05,
      "loss": 0.0036,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.06659387052059174,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0023,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.14600197970867157,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0032,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.13263766467571259,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0028,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.1736706793308258,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0036,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.11598073691129684,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0033,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.24227364361286163,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.004,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.24232465028762817,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0031,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.07796750217676163,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0018,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.046368058770895004,
      "learning_rate": 4.672e-05,
      "loss": 0.0031,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.12367705255746841,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.0037,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.06357160210609436,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.002,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.11662322282791138,
      "learning_rate": 4.668e-05,
      "loss": 0.0034,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.2593001425266266,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0024,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.1474130004644394,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0024,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.08756694197654724,
      "learning_rate": 4.664e-05,
      "loss": 0.0027,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.30749720335006714,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0041,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.06148410588502884,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0024,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.0,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.002,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.10016483068466187,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0025,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.13646316528320312,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.002,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.19571968913078308,
      "learning_rate": 4.656e-05,
      "loss": 0.004,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.06117496266961098,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0025,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.1755593717098236,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0014,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.12013623863458633,
      "learning_rate": 4.652e-05,
      "loss": 0.003,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.06428509205579758,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0022,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.17911824584007263,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.0034,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.4298337996006012,
      "learning_rate": 4.648e-05,
      "loss": 0.0028,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.12441897392272949,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.003,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.12650436162948608,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.0021,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.09699215739965439,
      "learning_rate": 4.644e-05,
      "loss": 0.0026,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.13191388547420502,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0021,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.2444954365491867,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0025,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.1251756101846695,
      "learning_rate": 4.64e-05,
      "loss": 0.0033,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.12611423432826996,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0027,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.11020547151565552,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0029,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.29324227571487427,
      "learning_rate": 4.636e-05,
      "loss": 0.0024,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.06160558760166168,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0021,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.07694895565509796,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0023,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.12038331478834152,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0029,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.1386575549840927,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.0017,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0025,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.18557819724082947,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0029,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.26870232820510864,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0033,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.12580177187919617,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0029,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.1629888415336609,
      "learning_rate": 4.624e-05,
      "loss": 0.0028,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.040939345955848694,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.0032,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.2505241632461548,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.0034,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.12202493101358414,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0027,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.11711157858371735,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0024,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.08406267315149307,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0022,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.06103385239839554,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0021,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.0616195946931839,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.0027,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.0667693242430687,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0023,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.05247209221124649,
      "learning_rate": 4.612e-05,
      "loss": 0.0018,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.07467038929462433,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0022,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.1354362517595291,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0027,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.13336938619613647,
      "learning_rate": 4.608e-05,
      "loss": 0.0045,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.06799554079771042,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0026,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.1360315978527069,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0018,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.051255688071250916,
      "learning_rate": 4.604e-05,
      "loss": 0.0025,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.08918418735265732,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0015,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.119315005838871,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0024,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.06020376831293106,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0021,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.13481636345386505,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0027,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.1451019048690796,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0029,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.1298183649778366,
      "learning_rate": 4.596e-05,
      "loss": 0.002,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.059686269611120224,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0022,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.12350460141897202,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0022,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.1317061334848404,
      "learning_rate": 4.592e-05,
      "loss": 0.0021,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.15208184719085693,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.0027,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.06514639407396317,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0023,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.1234477311372757,
      "learning_rate": 4.588e-05,
      "loss": 0.0018,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.18727053701877594,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0019,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.0839032381772995,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0028,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.055624932050704956,
      "learning_rate": 4.584e-05,
      "loss": 0.0033,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.07732141762971878,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.0018,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.19807659089565277,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0025,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.07958604395389557,
      "learning_rate": 4.58e-05,
      "loss": 0.0038,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0011,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.13375109434127808,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.002,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.5332381129264832,
      "learning_rate": 4.576e-05,
      "loss": 0.0024,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.09038803726434708,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.002,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.18239161372184753,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0033,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.1420745998620987,
      "learning_rate": 4.572e-05,
      "loss": 0.0025,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.12384810298681259,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0019,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.17844036221504211,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0032,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.17618435621261597,
      "learning_rate": 4.568e-05,
      "loss": 0.003,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.25007835030555725,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0023,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.15301194787025452,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.0026,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.25574421882629395,
      "learning_rate": 4.564e-05,
      "loss": 0.0023,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.07679641246795654,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.002,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.12025052309036255,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0016,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.44552093744277954,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0042,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.07573720067739487,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0029,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.1764507293701172,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0018,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.1961020976305008,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0028,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.02527090720832348,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0029,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.04195677489042282,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0023,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.18438862264156342,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0024,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.059314996004104614,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0025,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.4473210275173187,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0023,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.24447178840637207,
      "learning_rate": 4.548e-05,
      "loss": 0.0017,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.08006595820188522,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0015,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.030064307153224945,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0033,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.09038066864013672,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0037,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.06656977534294128,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0032,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.07963494211435318,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0023,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.17343467473983765,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0019,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.12718161940574646,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0018,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.09166138619184494,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.0029,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.05008912459015846,
      "learning_rate": 4.536e-05,
      "loss": 0.0028,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.018236881121993065,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0024,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.06952973455190659,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0023,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.05204619839787483,
      "learning_rate": 4.532e-05,
      "loss": 0.0028,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.2542761266231537,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0022,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.1119823083281517,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0028,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.0819048136472702,
      "learning_rate": 4.528e-05,
      "loss": 0.0023,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.13178510963916779,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0022,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0021,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.05275094136595726,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0014,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0027,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.06956162303686142,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0017,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.07539169490337372,
      "learning_rate": 4.52e-05,
      "loss": 0.0021,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.12891675531864166,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.002,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.11866600066423416,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.0025,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.07712573558092117,
      "learning_rate": 4.516e-05,
      "loss": 0.0032,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.1206996887922287,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0024,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.1200505867600441,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0015,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.0749310553073883,
      "learning_rate": 4.512e-05,
      "loss": 0.0029,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.19658894836902618,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.0022,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.07096321880817413,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0014,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.04869282245635986,
      "learning_rate": 4.508e-05,
      "loss": 0.0018,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.06328301876783371,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0021,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.18162448704242706,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0023,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.3132162094116211,
      "learning_rate": 4.504e-05,
      "loss": 0.0028,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.10351935029029846,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0019,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.04726389795541763,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.0024,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.06779011338949203,
      "learning_rate": 4.5e-05,
      "loss": 0.0017,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.07280462235212326,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0023,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.07600146532058716,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.002,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.05048469081521034,
      "learning_rate": 4.496e-05,
      "loss": 0.0023,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.24718226492404938,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0027,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.41268905997276306,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.003,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.161330908536911,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0014,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.07063078880310059,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0019,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.0904335081577301,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0026,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.0393931083381176,
      "learning_rate": 4.488e-05,
      "loss": 0.0026,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.3430500626564026,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0023,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.1453268676996231,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.002,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.2945632040500641,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0036,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.10088571161031723,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.003,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.08597216755151749,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0028,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.2567993104457855,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0023,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.04399358108639717,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0026,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.18722304701805115,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.002,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.24207811057567596,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0025,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.13076522946357727,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0022,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.06368758529424667,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0014,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.18616315722465515,
      "learning_rate": 4.472e-05,
      "loss": 0.0028,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.057118698954582214,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.0018,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.06405415385961533,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0021,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.0744224563241005,
      "learning_rate": 4.468e-05,
      "loss": 0.0027,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.32903528213500977,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.003,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.12778156995773315,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0013,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.07086040079593658,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.0016,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.035053350031375885,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.0015,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.14944420754909515,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0026,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.10162636637687683,
      "learning_rate": 4.46e-05,
      "loss": 0.0017,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.07634665817022324,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.002,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0023,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.0785127729177475,
      "learning_rate": 4.456e-05,
      "loss": 0.0021,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.11868854612112045,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0023,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.15114517509937286,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0017,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.20822259783744812,
      "learning_rate": 4.452e-05,
      "loss": 0.0022,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.0935049057006836,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0016,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.07402274012565613,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.002,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.334768682718277,
      "learning_rate": 4.448e-05,
      "loss": 0.0033,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.2517912983894348,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0024,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.07521640509366989,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.002,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.07534337043762207,
      "learning_rate": 4.444e-05,
      "loss": 0.002,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.11515497416257858,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.0015,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.12690867483615875,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0015,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.18749575316905975,
      "learning_rate": 4.44e-05,
      "loss": 0.002,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.002,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0014,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.3121320903301239,
      "learning_rate": 4.436e-05,
      "loss": 0.0017,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.24554161727428436,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.002,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.06274806708097458,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0023,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.11777956038713455,
      "learning_rate": 4.432e-05,
      "loss": 0.0029,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.2005218118429184,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0025,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.13779115676879883,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0023,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.08656549453735352,
      "learning_rate": 4.428e-05,
      "loss": 0.0034,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.19777965545654297,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0017,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.1321832537651062,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0015,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.033642929047346115,
      "learning_rate": 4.424e-05,
      "loss": 0.0016,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.06003260239958763,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0021,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.07195284217596054,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0019,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.0,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0015,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.12584830820560455,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.002,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.06471974402666092,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0019,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.031777165830135345,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0017,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.21034489572048187,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.002,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.3070579171180725,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0028,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.07982105016708374,
      "learning_rate": 4.412e-05,
      "loss": 0.003,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.12385643273591995,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.002,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.6403548717498779,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.003,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.1980542689561844,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0026,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.4460630714893341,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0035,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.07937970757484436,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0032,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.1517028957605362,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0024,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.0996069610118866,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0023,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.15608248114585876,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0021,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.06333350390195847,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0025,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.1240282654762268,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0016,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.06573078781366348,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0031,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.10875809192657471,
      "learning_rate": 4.396e-05,
      "loss": 0.0015,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.07037774473428726,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.001,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.2091691941022873,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0021,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.24916906654834747,
      "learning_rate": 4.392e-05,
      "loss": 0.0017,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.1333577185869217,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0018,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0014,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.057921312749385834,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0014,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.24837984144687653,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0021,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.17735303938388824,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0018,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.2552734613418579,
      "learning_rate": 4.384e-05,
      "loss": 0.0019,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.2470269352197647,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0019,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.06322232633829117,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.0029,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.12164381891489029,
      "learning_rate": 4.38e-05,
      "loss": 0.0015,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.1200750321149826,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0019,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.19873802363872528,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0027,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.12711353600025177,
      "learning_rate": 4.376e-05,
      "loss": 0.0024,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.24310116469860077,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.002,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.18669626116752625,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0017,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.20485346019268036,
      "learning_rate": 4.372e-05,
      "loss": 0.0034,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.001,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.16589714586734772,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0019,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.10912937670946121,
      "learning_rate": 4.368e-05,
      "loss": 0.002,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.08251869678497314,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0019,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.1361856758594513,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0016,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.0748477429151535,
      "learning_rate": 4.364e-05,
      "loss": 0.0012,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.0625249594449997,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0016,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.1847769320011139,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0025,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.18784384429454803,
      "learning_rate": 4.36e-05,
      "loss": 0.0016,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.12646864354610443,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0019,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.06037572771310806,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0017,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.07679994404315948,
      "learning_rate": 4.356e-05,
      "loss": 0.0019,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.07687386870384216,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0023,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.06462577730417252,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0016,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.08088097721338272,
      "learning_rate": 4.352e-05,
      "loss": 0.001,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.2976590096950531,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0025,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.08757438510656357,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0018,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.09697510302066803,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0033,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.17528371512889862,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0016,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0025,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.13871952891349792,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0018,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.1175742968916893,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0021,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.17261315882205963,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0016,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.08490252494812012,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0026,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.10096053034067154,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0014,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.1297813504934311,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.0022,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.45963823795318604,
      "learning_rate": 4.336e-05,
      "loss": 0.0028,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.05107271671295166,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0019,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.0,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0013,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.20711860060691833,
      "learning_rate": 4.332e-05,
      "loss": 0.0015,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.25032296776771545,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0015,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.1804637759923935,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.0021,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.0567687526345253,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.003,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.08669106662273407,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0029,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.07370832562446594,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0024,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.12747615575790405,
      "learning_rate": 4.324e-05,
      "loss": 0.0026,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.36856669187545776,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0015,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.07543062418699265,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0012,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.07169515639543533,
      "learning_rate": 4.32e-05,
      "loss": 0.0021,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.138260155916214,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0011,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.14887499809265137,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0025,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.1697826087474823,
      "learning_rate": 4.316e-05,
      "loss": 0.0023,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.08192956447601318,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0019,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.16580815613269806,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0018,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.060758523643016815,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0015,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.1152491420507431,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0022,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.032861337065696716,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0022,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.07277054339647293,
      "learning_rate": 4.308e-05,
      "loss": 0.0019,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.12454123795032501,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0016,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.07385633140802383,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0017,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.07281104475259781,
      "learning_rate": 4.304e-05,
      "loss": 0.0013,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.050134219229221344,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0017,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.04275747761130333,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0017,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.07565370202064514,
      "learning_rate": 4.3e-05,
      "loss": 0.0029,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.071502186357975,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.0013,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0022,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.12504370510578156,
      "learning_rate": 4.296e-05,
      "loss": 0.0024,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.11839500069618225,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0011,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.09458333253860474,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0021,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.14579488337039948,
      "learning_rate": 4.292e-05,
      "loss": 0.0019,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.13264818489551544,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0018,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.14157767593860626,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0025,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.15675483644008636,
      "learning_rate": 4.288e-05,
      "loss": 0.002,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.18094363808631897,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.002,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.2110120803117752,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0026,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.11575643718242645,
      "learning_rate": 4.284e-05,
      "loss": 0.0019,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.04698650538921356,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0017,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.07227500528097153,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0019,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.20498380064964294,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0015,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.5069917440414429,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.002,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.26971444487571716,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0028,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.0,
      "learning_rate": 4.276e-05,
      "loss": 0.0017,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.1232847049832344,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0016,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.18207503855228424,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0017,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.07378076016902924,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0011,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.1417263150215149,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.0019,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0013,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.30140531063079834,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0023,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.11308908462524414,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0011,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.07180047780275345,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0024,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.08394347131252289,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.002,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.13790790736675262,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0019,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.07524298876523972,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0018,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.18989743292331696,
      "learning_rate": 4.26e-05,
      "loss": 0.0023,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.07645637542009354,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0017,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.047405458986759186,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0016,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.04144994542002678,
      "learning_rate": 4.256e-05,
      "loss": 0.0014,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.18585972487926483,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0014,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0017,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.08591146767139435,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0016,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.1430438756942749,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0016,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.10224533826112747,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0017,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.1512959897518158,
      "learning_rate": 4.248e-05,
      "loss": 0.0017,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0014,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.22033452987670898,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.0025,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.1991344541311264,
      "learning_rate": 4.244e-05,
      "loss": 0.0019,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.24561534821987152,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0018,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.11993084847927094,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0017,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.3790380656719208,
      "learning_rate": 4.24e-05,
      "loss": 0.0028,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.14365221560001373,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0015,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 0.06704816967248917,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.003,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.2382144033908844,
      "learning_rate": 4.236e-05,
      "loss": 0.0022,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.16418813169002533,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0015,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.19228506088256836,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0018,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.03018190525472164,
      "learning_rate": 4.232e-05,
      "loss": 0.0022,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.12526845932006836,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0018,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0021,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.12185703963041306,
      "learning_rate": 4.228e-05,
      "loss": 0.0015,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.17840440571308136,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.002,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.050861407071352005,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0017,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.08421021699905396,
      "learning_rate": 4.224e-05,
      "loss": 0.0023,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.0674474686384201,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0023,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.06399800628423691,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0027,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.13313543796539307,
      "learning_rate": 4.22e-05,
      "loss": 0.0018,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.1447862684726715,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0031,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.13596902787685394,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0011,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.09483295679092407,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0018,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.20550543069839478,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0019,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.07388454675674438,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0021,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.06201649084687233,
      "learning_rate": 4.212e-05,
      "loss": 0.0019,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.0,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0017,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.0,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0011,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.15387359261512756,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.0012,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.07545032352209091,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0012,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0015,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.1276669055223465,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0019,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.295368492603302,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.0023,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.0728812962770462,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0018,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0549442432820797,
      "learning_rate": 4.2e-05,
      "loss": 0.0019,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.23017695546150208,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0021,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.1273437887430191,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0013,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.14115314185619354,
      "learning_rate": 4.196e-05,
      "loss": 0.0016,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.3013084828853607,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0025,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.09692873060703278,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0014,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.24482402205467224,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0015,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.132017582654953,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0024,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.15431112051010132,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0015,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.078250452876091,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0015,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.12477719038724899,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0018,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.19931989908218384,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0024,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.18805021047592163,
      "learning_rate": 4.184e-05,
      "loss": 0.0025,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.1739443689584732,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0022,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.20733512938022614,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0017,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.037751637399196625,
      "learning_rate": 4.18e-05,
      "loss": 0.0011,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.1997133195400238,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0018,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.07351408898830414,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0016,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.25433263182640076,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.0016,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.1999119073152542,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0027,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.2017017900943756,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0025,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.12306475639343262,
      "learning_rate": 4.172e-05,
      "loss": 0.0014,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.1299791783094406,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0023,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.09964020550251007,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0019,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.05456804111599922,
      "learning_rate": 4.168e-05,
      "loss": 0.0016,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05892898514866829,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0022,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.08144152909517288,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0017,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.1359822303056717,
      "learning_rate": 4.164e-05,
      "loss": 0.0019,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.09165738523006439,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0025,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.16177891194820404,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0023,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.17967510223388672,
      "learning_rate": 4.16e-05,
      "loss": 0.0025,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.1633078008890152,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0015,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.323903888463974,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0022,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.07274601608514786,
      "learning_rate": 4.156e-05,
      "loss": 0.001,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.08804722875356674,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0017,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.1267455518245697,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0027,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.07636091113090515,
      "learning_rate": 4.152e-05,
      "loss": 0.0017,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.1515171229839325,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0009,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.12906117737293243,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0019,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.07364164292812347,
      "learning_rate": 4.148e-05,
      "loss": 0.0016,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.13577765226364136,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0021,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.14932677149772644,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.0018,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.13075222074985504,
      "learning_rate": 4.144e-05,
      "loss": 0.0021,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.121530681848526,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.001,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.12202707678079605,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0007,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.12353187054395676,
      "learning_rate": 4.14e-05,
      "loss": 0.0023,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.37770992517471313,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0011,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.19300243258476257,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0016,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.27133670449256897,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0023,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.08820270001888275,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.002,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0019,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.15191161632537842,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.002,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.07751916348934174,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0013,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.1285107582807541,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.0023,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.11443494260311127,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0014,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.15764841437339783,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0017,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.07684461772441864,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0024,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.10953247547149658,
      "learning_rate": 4.124e-05,
      "loss": 0.0013,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.06497064977884293,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0021,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.1237461268901825,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.002,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.0683961734175682,
      "learning_rate": 4.12e-05,
      "loss": 0.0013,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.13580268621444702,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.0017,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.25557470321655273,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.0019,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.10275331139564514,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0016,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.156093031167984,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.0014,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.1572018414735794,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0024,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.31769007444381714,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0019,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.0978061705827713,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0019,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.18633264303207397,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.002,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.13520222902297974,
      "learning_rate": 4.108e-05,
      "loss": 0.0019,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0024,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.12021621316671371,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0015,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.12199278920888901,
      "learning_rate": 4.104e-05,
      "loss": 0.0013,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.07054170221090317,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0009,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.07167183607816696,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0015,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.19848792254924774,
      "learning_rate": 4.1e-05,
      "loss": 0.0012,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.13928073644638062,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0016,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.07305172085762024,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.0018,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.3183600604534149,
      "learning_rate": 4.096e-05,
      "loss": 0.0027,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.18073874711990356,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0018,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.07844778150320053,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0016,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.10369662940502167,
      "learning_rate": 4.092e-05,
      "loss": 0.0015,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.13730661571025848,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.002,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.08839200437068939,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0022,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.08384068310260773,
      "learning_rate": 4.088e-05,
      "loss": 0.0022,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.19810423254966736,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0023,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.08575527369976044,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0016,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.07127733528614044,
      "learning_rate": 4.084e-05,
      "loss": 0.0016,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.1014288067817688,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0013,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.1939077377319336,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.002,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.06890475749969482,
      "learning_rate": 4.08e-05,
      "loss": 0.0013,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.1358252614736557,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0016,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.07900267839431763,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0022,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.14985471963882446,
      "learning_rate": 4.076e-05,
      "loss": 0.0019,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.13606959581375122,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.0016,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.0,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0015,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.0,
      "learning_rate": 4.072e-05,
      "loss": 0.0016,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.5444571375846863,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0017,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.0636352077126503,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.0014,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.10432679951190948,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0018,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.04327850043773651,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0012,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.05873391032218933,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0017,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.08559815585613251,
      "learning_rate": 4.064e-05,
      "loss": 0.0018,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.08583240211009979,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0022,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.13534675538539886,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0015,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.17692668735980988,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0014,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.20624323189258575,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0017,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.3261849284172058,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.002,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.130042165517807,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.0022,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.05997715890407562,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0017,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.06660114228725433,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0024,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.10259627550840378,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.0014,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.08140357583761215,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0015,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.07651615887880325,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0016,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.09010518342256546,
      "learning_rate": 4.048e-05,
      "loss": 0.0019,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.30197644233703613,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0014,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.1843239814043045,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0014,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.07301631569862366,
      "learning_rate": 4.044e-05,
      "loss": 0.0013,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.14386175572872162,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0012,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.19683748483657837,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0016,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.19357508420944214,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0015,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.3655318021774292,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.003,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.41645926237106323,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0014,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.3609989583492279,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.002,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.20310480892658234,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0015,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.1911969929933548,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0013,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.3172205686569214,
      "learning_rate": 4.032e-05,
      "loss": 0.0027,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.20580871403217316,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0026,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.08798406273126602,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0017,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.0,
      "learning_rate": 4.028e-05,
      "loss": 0.0011,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.07052788883447647,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0013,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.33655667304992676,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0023,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.1287567913532257,
      "learning_rate": 4.024e-05,
      "loss": 0.0016,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.1710391491651535,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0022,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.09186189621686935,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0025,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.0,
      "learning_rate": 4.02e-05,
      "loss": 0.0021,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.07646314054727554,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0014,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.13693565130233765,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0025,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.06952736526727676,
      "learning_rate": 4.016e-05,
      "loss": 0.0013,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.05258408933877945,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0016,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.3911823034286499,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0013,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.07047504186630249,
      "learning_rate": 4.012e-05,
      "loss": 0.0015,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.02825399488210678,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.0021,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.06473328918218613,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0018,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.20496225357055664,
      "learning_rate": 4.008e-05,
      "loss": 0.0016,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.25767314434051514,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0026,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.1558731496334076,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0025,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.2664029598236084,
      "learning_rate": 4.004e-05,
      "loss": 0.002,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.14874131977558136,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.0021,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.10393297672271729,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0019,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.15081964433193207,
      "learning_rate": 4e-05,
      "loss": 0.0024,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.0,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0016,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.06935697793960571,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0019,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.14408765733242035,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.0011,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.125986248254776,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0016,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0007,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.11184755712747574,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0025,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.03652549162507057,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0014,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.043295424431562424,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0011,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.2582681179046631,
      "learning_rate": 3.988e-05,
      "loss": 0.0015,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.08782926946878433,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0015,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.06772984564304352,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.002,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.09521730244159698,
      "learning_rate": 3.984e-05,
      "loss": 0.0013,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.08544710278511047,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.002,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.11428992450237274,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0019,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.05857406184077263,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0017,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.062359388917684555,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.002,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.1188521757721901,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.0016,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.35840198397636414,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0022,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.07616087794303894,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0008,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.1966465413570404,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0022,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.08985317498445511,
      "learning_rate": 3.972e-05,
      "loss": 0.0018,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.13299858570098877,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0011,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.08079076558351517,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0011,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.18799813091754913,
      "learning_rate": 3.968e-05,
      "loss": 0.0015,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.1439884603023529,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0021,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0009,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.11682002991437912,
      "learning_rate": 3.964e-05,
      "loss": 0.002,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.2493632435798645,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.002,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.08532249927520752,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0019,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.1434069722890854,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0012,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.04559379443526268,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0021,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.07398798316717148,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.0017,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.24606962502002716,
      "learning_rate": 3.956e-05,
      "loss": 0.0013,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.06125959753990173,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.002,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.1302907019853592,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0016,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.0,
      "learning_rate": 3.952e-05,
      "loss": 0.0005,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.44708025455474854,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0017,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.060414157807826996,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0012,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.078775554895401,
      "learning_rate": 3.948e-05,
      "loss": 0.0017,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.19431090354919434,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0017,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.12945684790611267,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0015,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.06470464915037155,
      "learning_rate": 3.944e-05,
      "loss": 0.0015,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.19773943722248077,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.0018,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.13504108786582947,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0012,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.059146489948034286,
      "learning_rate": 3.94e-05,
      "loss": 0.0013,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.09575773030519485,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0015,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.07637129724025726,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0018,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.250112920999527,
      "learning_rate": 3.936e-05,
      "loss": 0.0014,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.16413640975952148,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0024,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.30244824290275574,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0027,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.0706886574625969,
      "learning_rate": 3.932e-05,
      "loss": 0.0009,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.1868000626564026,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.0016,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0017,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.23742303252220154,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0016,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.27496740221977234,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0019,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.12297304719686508,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0013,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.15298089385032654,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0022,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.28816136717796326,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.002,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.06538582593202591,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0019,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.15229864418506622,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.002,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.07535535842180252,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0008,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.19099199771881104,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.0014,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.06742111593484879,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.0019,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.1282358020544052,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0019,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.2076246440410614,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0031,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.0,
      "learning_rate": 3.912e-05,
      "loss": 0.0015,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.08218033611774445,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0012,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.104312002658844,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.0022,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.15396468341350555,
      "learning_rate": 3.908e-05,
      "loss": 0.0014,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.07782131433486938,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.002,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.1110394224524498,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0016,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.06283994764089584,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0022,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.07142849266529083,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0017,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.13931405544281006,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0018,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.3043668568134308,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0019,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.10088808089494705,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.002,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.07091811299324036,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0023,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.13694222271442413,
      "learning_rate": 3.896e-05,
      "loss": 0.0018,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.06883876770734787,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0015,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.2491713911294937,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0018,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.2230798751115799,
      "learning_rate": 3.892e-05,
      "loss": 0.0017,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.17257682979106903,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.0024,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.0,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0009,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.14674684405326843,
      "learning_rate": 3.888e-05,
      "loss": 0.0009,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.1284601390361786,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0021,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.19559654593467712,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0028,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.08424470573663712,
      "learning_rate": 3.884e-05,
      "loss": 0.0018,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.18529543280601501,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0021,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.32739001512527466,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0019,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.12474959343671799,
      "learning_rate": 3.88e-05,
      "loss": 0.0011,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.41609689593315125,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0017,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.07504348456859589,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.0012,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.10099124163389206,
      "learning_rate": 3.876e-05,
      "loss": 0.0013,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0019,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.12970241904258728,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0021,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.08246445655822754,
      "learning_rate": 3.872e-05,
      "loss": 0.0012,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.16223736107349396,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0015,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.1899966597557068,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0014,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.0,
      "learning_rate": 3.868e-05,
      "loss": 0.0018,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.19509448111057281,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0012,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.12985047698020935,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0021,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.13189192116260529,
      "learning_rate": 3.864e-05,
      "loss": 0.002,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.08258841186761856,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0019,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.10419730097055435,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0015,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.04703165963292122,
      "learning_rate": 3.86e-05,
      "loss": 0.0025,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.10910119116306305,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.0017,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.20036478340625763,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0026,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.18849505484104156,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0022,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.05485506355762482,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0014,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.1299898624420166,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0025,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.08394935727119446,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0016,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.25414279103279114,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0023,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.16877786815166473,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0017,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.27177783846855164,
      "learning_rate": 3.848e-05,
      "loss": 0.0017,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.24817363917827606,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0016,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.20722126960754395,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0018,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.07140049338340759,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0012,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.12304574996232986,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0022,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.0012,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.13459564745426178,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0021,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.18600186705589294,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0013,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.2813846170902252,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0018,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.07132977992296219,
      "learning_rate": 3.836e-05,
      "loss": 0.0007,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.07523658126592636,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0009,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.14516206085681915,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0021,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.2863023579120636,
      "learning_rate": 3.832e-05,
      "loss": 0.002,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.1809815913438797,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0013,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.12912097573280334,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0012,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.2519916892051697,
      "learning_rate": 3.828e-05,
      "loss": 0.0021,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.07857133448123932,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0024,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.12278757989406586,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0014,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.13937798142433167,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.0013,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.1433756947517395,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0015,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.12619708478450775,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0016,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.25182557106018066,
      "learning_rate": 3.82e-05,
      "loss": 0.0014,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.07020281255245209,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0018,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.19500169157981873,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0013,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.19553853571414948,
      "learning_rate": 3.816e-05,
      "loss": 0.0014,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.0,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.001,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.25924912095069885,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0019,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.0,
      "learning_rate": 3.812e-05,
      "loss": 0.0005,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.15387262403964996,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0013,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.18743769824504852,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0023,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.07858799397945404,
      "learning_rate": 3.808e-05,
      "loss": 0.0012,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.10509968549013138,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0009,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.08209192007780075,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0018,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.0987466499209404,
      "learning_rate": 3.804e-05,
      "loss": 0.0024,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.14929914474487305,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0017,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.15975920855998993,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0027,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.045930080115795135,
      "learning_rate": 3.8e-05,
      "loss": 0.0015,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.0947209894657135,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0009,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.07248905301094055,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0013,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.0,
      "learning_rate": 3.796e-05,
      "loss": 0.0015,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.1340371072292328,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0018,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.07809759676456451,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0012,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.03828859329223633,
      "learning_rate": 3.792e-05,
      "loss": 0.0013,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.08492071181535721,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0016,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.3196789026260376,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0022,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.0,
      "learning_rate": 3.788e-05,
      "loss": 0.0011,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.07188927382230759,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0016,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.20263732969760895,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0014,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.04611724242568016,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0023,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.07619890570640564,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0016,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.12657682597637177,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0014,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.21202680468559265,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0024,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.0009,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.1878269612789154,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0019,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.062328897416591644,
      "learning_rate": 3.776e-05,
      "loss": 0.002,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.07691148668527603,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0013,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.1351565271615982,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0022,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.16729982197284698,
      "learning_rate": 3.772e-05,
      "loss": 0.0014,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.08194296061992645,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0019,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.13830383121967316,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0011,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.2404320240020752,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0027,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.06719156354665756,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0011,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.13304723799228668,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.002,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.12787240743637085,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0011,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.13671398162841797,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0017,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0015,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.07962322235107422,
      "learning_rate": 3.76e-05,
      "loss": 0.0024,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.21155422925949097,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.0022,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.23802272975444794,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0013,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.07887950539588928,
      "learning_rate": 3.756e-05,
      "loss": 0.0016,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.07068247348070145,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.0018,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.0,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0018,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.1194128766655922,
      "learning_rate": 3.752e-05,
      "loss": 0.0018,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.13454021513462067,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.001,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.07783440500497818,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.002,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.2001703530550003,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0018,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.24509219825267792,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0018,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.34997159242630005,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0009,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.15261368453502655,
      "learning_rate": 3.744e-05,
      "loss": 0.0014,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.17391397058963776,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0014,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.10520339757204056,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0016,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.07228188216686249,
      "learning_rate": 3.74e-05,
      "loss": 0.0016,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.3103431165218353,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.0013,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.1348590850830078,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0011,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.14474596083164215,
      "learning_rate": 3.736e-05,
      "loss": 0.0015,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.19420136511325836,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0017,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.18799537420272827,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0018,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.14349661767482758,
      "learning_rate": 3.732e-05,
      "loss": 0.0019,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.0857166051864624,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.0011,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.0018,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.19412416219711304,
      "learning_rate": 3.728e-05,
      "loss": 0.0012,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.08667626976966858,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0014,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.19189752638339996,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0017,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.08736547082662582,
      "learning_rate": 3.724e-05,
      "loss": 0.0018,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.08970806002616882,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0006,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.15324099361896515,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0011,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.0,
      "learning_rate": 3.72e-05,
      "loss": 0.0012,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.08082593232393265,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0019,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.0664040744304657,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.001,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.1605917364358902,
      "learning_rate": 3.716e-05,
      "loss": 0.0018,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.06837453693151474,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0021,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.12888772785663605,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0016,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.13799557089805603,
      "learning_rate": 3.712e-05,
      "loss": 0.0014,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.22090013325214386,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0004,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.04040714353322983,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0015,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.12885896861553192,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.0022,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.19307386875152588,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0024,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.1958070546388626,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.0016,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.065749391913414,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0014,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.08046027272939682,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0008,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.08465884625911713,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0015,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.0777197852730751,
      "learning_rate": 3.7e-05,
      "loss": 0.0015,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.06674301624298096,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.001,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.07023988664150238,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0019,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.09131774306297302,
      "learning_rate": 3.696e-05,
      "loss": 0.0013,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.18898797035217285,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0011,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.08723995834589005,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0018,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.07645716518163681,
      "learning_rate": 3.692e-05,
      "loss": 0.0011,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.09160859882831573,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0025,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.15646354854106903,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.0018,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.12426925450563431,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.0017,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.06729323416948318,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0022,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.10583839565515518,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0014,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.03665197640657425,
      "learning_rate": 3.684e-05,
      "loss": 0.0018,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.22236740589141846,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0028,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.08859551697969437,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0015,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.06450853496789932,
      "learning_rate": 3.68e-05,
      "loss": 0.0018,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.14047527313232422,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.002,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.2707129120826721,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.002,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.05774443969130516,
      "learning_rate": 3.676e-05,
      "loss": 0.002,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.1969175934791565,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.002,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.083633653819561,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0015,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.0,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0018,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.06122709438204765,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.0011,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.06763587146997452,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0019,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.12178652733564377,
      "learning_rate": 3.668e-05,
      "loss": 0.0016,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0021,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.06304275244474411,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0018,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.45080244541168213,
      "learning_rate": 3.664e-05,
      "loss": 0.0018,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.12767505645751953,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0016,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.20286646485328674,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.002,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.062209777534008026,
      "learning_rate": 3.66e-05,
      "loss": 0.002,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.056473322212696075,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.0014,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0013,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.11156803369522095,
      "learning_rate": 3.656e-05,
      "loss": 0.0014,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.0661788061261177,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.002,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.06657097488641739,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0009,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.06870345771312714,
      "learning_rate": 3.652e-05,
      "loss": 0.0011,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.08080049604177475,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0009,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.07403764873743057,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0008,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.06627022475004196,
      "learning_rate": 3.648e-05,
      "loss": 0.0016,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.10661742091178894,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0011,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.15702186524868011,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.0018,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.038429588079452515,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.0031,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.06867900490760803,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.0008,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.06589090824127197,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0013,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.09442101418972015,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0011,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.2830410897731781,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0015,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.09427232295274734,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0016,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.306254506111145,
      "learning_rate": 3.636e-05,
      "loss": 0.0009,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.06749111413955688,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0016,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.13023138046264648,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0015,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.0,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.001,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.1442670375108719,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0019,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.19194595515727997,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0016,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.12234245240688324,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0016,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.07686632126569748,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.002,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.18879829347133636,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0029,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.1460447907447815,
      "learning_rate": 3.624e-05,
      "loss": 0.0019,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.06900176405906677,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.0019,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.07194288074970245,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0029,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.10841020196676254,
      "learning_rate": 3.62e-05,
      "loss": 0.0012,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.31375497579574585,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.001,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.06460995972156525,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0013,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.06450918316841125,
      "learning_rate": 3.616e-05,
      "loss": 0.0026,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.0894470140337944,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0012,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.2727420926094055,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.001,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.13148896396160126,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0015,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.1056639775633812,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0019,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.09357992559671402,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0015,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.058992549777030945,
      "learning_rate": 3.608e-05,
      "loss": 0.0019,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.15084128081798553,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0011,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.2568757236003876,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0017,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.15191979706287384,
      "learning_rate": 3.604e-05,
      "loss": 0.0017,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.04883376508951187,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0015,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.11537840962409973,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0012,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.11793370544910431,
      "learning_rate": 3.6e-05,
      "loss": 0.0013,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.04926478862762451,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0009,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.3462088704109192,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0018,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.08017807453870773,
      "learning_rate": 3.596e-05,
      "loss": 0.0013,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.2009834200143814,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0018,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0009,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.1279478669166565,
      "learning_rate": 3.592e-05,
      "loss": 0.0015,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.08025391399860382,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.0007,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.07935856282711029,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0023,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.0968153178691864,
      "learning_rate": 3.588e-05,
      "loss": 0.0023,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.1546436995267868,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0022,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0013,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.18018868565559387,
      "learning_rate": 3.584e-05,
      "loss": 0.0016,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.129257470369339,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.0015,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.13871529698371887,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0014,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.14261743426322937,
      "learning_rate": 3.58e-05,
      "loss": 0.0013,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.14554336667060852,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0011,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.06967582553625107,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.0013,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.0,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0009,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.06439853459596634,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0006,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.1200457513332367,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0014,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.1969849169254303,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0015,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.09024711698293686,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0019,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.09172060340642929,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0017,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.07179886847734451,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0015,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.060066983103752136,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.002,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.3199678659439087,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0016,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.0,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0014,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.08911308646202087,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0018,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.08304034173488617,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0022,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.07616987079381943,
      "learning_rate": 3.56e-05,
      "loss": 0.0012,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0007,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.06541717797517776,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0017,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.10272059589624405,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.0015,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.25308093428611755,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.0016,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.13359154760837555,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0013,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.11627302318811417,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0013,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.24609003961086273,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.002,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.07876759767532349,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.0014,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.2589075267314911,
      "learning_rate": 3.548e-05,
      "loss": 0.0013,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.06903897970914841,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0016,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.2548423707485199,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0019,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.14077775180339813,
      "learning_rate": 3.544e-05,
      "loss": 0.0021,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.1349254846572876,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0016,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.05371531844139099,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0015,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.19587646424770355,
      "learning_rate": 3.54e-05,
      "loss": 0.0016,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.13457083702087402,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.0018,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.1550481766462326,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0013,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.08271254599094391,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0008,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.2033962458372116,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0017,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.07607367634773254,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0011,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.06560594588518143,
      "learning_rate": 3.532e-05,
      "loss": 0.0022,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.08289557695388794,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0012,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.06944926828145981,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0007,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.13153386116027832,
      "learning_rate": 3.528e-05,
      "loss": 0.0013,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.10285557806491852,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0011,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.07739495486021042,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.001,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.0,
      "learning_rate": 3.524e-05,
      "loss": 0.001,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.08596376329660416,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0006,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.2626611888408661,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0019,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.12169923633337021,
      "learning_rate": 3.52e-05,
      "loss": 0.0026,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.12040086090564728,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.001,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.06935808807611465,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0017,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.08644109219312668,
      "learning_rate": 3.516e-05,
      "loss": 0.0015,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.1668132245540619,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.002,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.06954487413167953,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0015,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.21694184839725494,
      "learning_rate": 3.512e-05,
      "loss": 0.0013,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.07928095757961273,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0012,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.11412995308637619,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0019,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.22178980708122253,
      "learning_rate": 3.508e-05,
      "loss": 0.0015,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.10651631653308868,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0022,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.31045612692832947,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0018,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.18762490153312683,
      "learning_rate": 3.504e-05,
      "loss": 0.0016,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.07159309089183807,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.0017,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0021,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.07150907814502716,
      "learning_rate": 3.5e-05,
      "loss": 0.0016,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.09824783354997635,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0025,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.20302319526672363,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0021,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.0,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0012,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.09313313663005829,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0015,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.20939742028713226,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0009,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.18670439720153809,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0012,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.2601853311061859,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.0017,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.06696142256259918,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0013,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.16849271953105927,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0015,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.07859063148498535,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0026,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.05777020752429962,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0011,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.1020871102809906,
      "learning_rate": 3.484e-05,
      "loss": 0.0017,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.1188715323805809,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.0011,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.07377476245164871,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0013,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.13957886397838593,
      "learning_rate": 3.48e-05,
      "loss": 0.0011,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.1949092000722885,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0017,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.060389384627342224,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0018,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.1774650514125824,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0015,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0007,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.13687624037265778,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0025,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.13552477955818176,
      "learning_rate": 3.472e-05,
      "loss": 0.0028,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.08177626878023148,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0017,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.06395509093999863,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0013,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.062443807721138,
      "learning_rate": 3.468e-05,
      "loss": 0.0013,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.065633125603199,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0017,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.1653861254453659,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0018,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.07791066914796829,
      "learning_rate": 3.464e-05,
      "loss": 0.001,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.08711560815572739,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.001,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.07293014228343964,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.001,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.13854682445526123,
      "learning_rate": 3.46e-05,
      "loss": 0.001,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.4979814887046814,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.0021,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.07172849774360657,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0013,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.09958475083112717,
      "learning_rate": 3.456e-05,
      "loss": 0.0009,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.06779414415359497,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0017,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.1386910080909729,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0022,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.12524478137493134,
      "learning_rate": 3.452e-05,
      "loss": 0.0019,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.12544508278369904,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.002,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.0680268406867981,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0014,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.1181144118309021,
      "learning_rate": 3.448e-05,
      "loss": 0.0007,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.1373738944530487,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0015,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.06165057793259621,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0013,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.07291758060455322,
      "learning_rate": 3.444e-05,
      "loss": 0.001,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.13351193070411682,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0014,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.17620903253555298,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0009,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.0950208380818367,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0019,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.07673012465238571,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0016,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.06990218162536621,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0011,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.3115510642528534,
      "learning_rate": 3.436e-05,
      "loss": 0.0017,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0014,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.1243610605597496,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0015,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.07729513198137283,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.001,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.14867286384105682,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0012,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.09864833205938339,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0012,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.24478155374526978,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0015,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.07554622739553452,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0012,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.12447239458560944,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0015,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.0,
      "learning_rate": 3.424e-05,
      "loss": 0.0011,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.10598088800907135,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.0009,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.09213490039110184,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0009,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.08901670575141907,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0011,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.13091148436069489,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.002,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.047294165939092636,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.0008,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.03769754618406296,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0017,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.12695620954036713,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0013,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.06618638336658478,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0016,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.26018500328063965,
      "learning_rate": 3.412e-05,
      "loss": 0.0012,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.14506158232688904,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0017,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.09143465012311935,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0024,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.07075930386781693,
      "learning_rate": 3.408e-05,
      "loss": 0.0013,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0012,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0008,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.10335827618837357,
      "learning_rate": 3.404e-05,
      "loss": 0.0015,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.16557666659355164,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.0008,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.077427938580513,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.001,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.0,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0012,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.0898958221077919,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0017,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.1319846659898758,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0015,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.0,
      "learning_rate": 3.396e-05,
      "loss": 0.0009,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.12705093622207642,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0009,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.11701816320419312,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0026,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.09902581572532654,
      "learning_rate": 3.392e-05,
      "loss": 0.0011,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.1536390632390976,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0011,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.18288026750087738,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0016,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.10693810880184174,
      "learning_rate": 3.388e-05,
      "loss": 0.0013,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.06995654106140137,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0015,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.15388815104961395,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.0018,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.06640788912773132,
      "learning_rate": 3.384e-05,
      "loss": 0.002,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.14175307750701904,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.0006,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.07867229729890823,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.002,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.07987983524799347,
      "learning_rate": 3.38e-05,
      "loss": 0.0009,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.13458450138568878,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0011,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.08896064758300781,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0013,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.2631007134914398,
      "learning_rate": 3.376e-05,
      "loss": 0.0016,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.11620446294546127,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0017,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.09582274407148361,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0012,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.050934191793203354,
      "learning_rate": 3.372e-05,
      "loss": 0.0012,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.15715859830379486,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0017,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.045963138341903687,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.0017,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.14687463641166687,
      "learning_rate": 3.368e-05,
      "loss": 0.0016,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.1388995349407196,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0018,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.10014502704143524,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0012,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.0,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0007,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.08149634301662445,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.0014,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.1636674851179123,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0013,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.0,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0019,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.09126326441764832,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0011,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.06861002743244171,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.0012,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.1088971197605133,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.0014,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.0842941403388977,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0009,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.14784926176071167,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0015,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.08282652497291565,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.0009,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.19222640991210938,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0015,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.08341237902641296,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0017,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.0970551073551178,
      "learning_rate": 3.348e-05,
      "loss": 0.0019,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.08302786946296692,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0018,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.07479981333017349,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0012,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.06577602028846741,
      "learning_rate": 3.344e-05,
      "loss": 0.0016,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0017,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.17056086659431458,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0008,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.06803315877914429,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0009,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.15761350095272064,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.001,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.0009,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.26211458444595337,
      "learning_rate": 3.336e-05,
      "loss": 0.0024,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.06859990209341049,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.002,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0009,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.17879259586334229,
      "learning_rate": 3.332e-05,
      "loss": 0.0012,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.15768671035766602,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.002,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.12937699258327484,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0018,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.13785363733768463,
      "learning_rate": 3.328e-05,
      "loss": 0.002,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.0869484469294548,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0011,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.21193000674247742,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0013,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.20067407190799713,
      "learning_rate": 3.324e-05,
      "loss": 0.0013,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.12540023028850555,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0011,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.13690471649169922,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0025,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.16606873273849487,
      "learning_rate": 3.32e-05,
      "loss": 0.0011,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.07054496556520462,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.001,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.15901419520378113,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0016,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.1987336128950119,
      "learning_rate": 3.316e-05,
      "loss": 0.002,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.10506627708673477,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0019,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.1494423747062683,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0011,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.14527547359466553,
      "learning_rate": 3.312e-05,
      "loss": 0.0012,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.07466370612382889,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.0012,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.06304342299699783,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0013,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.09036901593208313,
      "learning_rate": 3.308e-05,
      "loss": 0.0016,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.19877460598945618,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0016,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.15551431477069855,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0021,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.08958671241998672,
      "learning_rate": 3.304e-05,
      "loss": 0.0015,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.16710315644741058,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.001,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.15702128410339355,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0009,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.20622216165065765,
      "learning_rate": 3.3e-05,
      "loss": 0.0013,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.20505979657173157,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0005,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.25006842613220215,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.0015,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.1653243750333786,
      "learning_rate": 3.296e-05,
      "loss": 0.0019,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.2182091623544693,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0016,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.08348063379526138,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0014,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.17300471663475037,
      "learning_rate": 3.292e-05,
      "loss": 0.0015,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.08629994839429855,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0025,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.10633476078510284,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0018,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.1681843250989914,
      "learning_rate": 3.288e-05,
      "loss": 0.0019,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.2071509212255478,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0013,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.10167758911848068,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0015,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.07055239379405975,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0012,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.20161445438861847,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.0016,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.19567744433879852,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0021,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.26092204451560974,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0011,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.26826658844947815,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0015,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.18426060676574707,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0013,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.13465702533721924,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.0008,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.324701189994812,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0013,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.08793025463819504,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0016,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.0,
      "learning_rate": 3.272e-05,
      "loss": 0.0012,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.09716083854436874,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0016,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.07272334396839142,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0012,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.0,
      "learning_rate": 3.268e-05,
      "loss": 0.0015,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.13465061783790588,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0012,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.08085629343986511,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.0014,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.12962312996387482,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.001,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.059165049344301224,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.0019,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.05006152763962746,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0015,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.20912209153175354,
      "learning_rate": 3.26e-05,
      "loss": 0.0021,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.14238496124744415,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.0008,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.2866617441177368,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0022,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.080182284116745,
      "learning_rate": 3.256e-05,
      "loss": 0.0014,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.20402882993221283,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0025,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.13834188878536224,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0013,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.19633141160011292,
      "learning_rate": 3.252e-05,
      "loss": 0.001,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.09530998766422272,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0017,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0013,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.0,
      "learning_rate": 3.248e-05,
      "loss": 0.0012,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0008,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.05074157565832138,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0015,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.09800884127616882,
      "learning_rate": 3.244e-05,
      "loss": 0.0008,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.1842338740825653,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.001,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0009,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.4904876947402954,
      "learning_rate": 3.24e-05,
      "loss": 0.002,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.10603228956460953,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0016,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.2316872924566269,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0017,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.09330117702484131,
      "learning_rate": 3.236e-05,
      "loss": 0.0014,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.09398484975099564,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0017,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0015,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.09033937007188797,
      "learning_rate": 3.232e-05,
      "loss": 0.0018,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.08081291615962982,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.001,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.07630319148302078,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.0011,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.0,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0006,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.001,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.08439967036247253,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0018,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.13387709856033325,
      "learning_rate": 3.224e-05,
      "loss": 0.0011,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.2642817199230194,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0016,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.08059752732515335,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0012,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.1573898047208786,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0011,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.04215478152036667,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0024,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.09314970672130585,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0014,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.19478823244571686,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0012,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.13550575077533722,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0008,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.08170075714588165,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0011,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.12155932188034058,
      "learning_rate": 3.212e-05,
      "loss": 0.0009,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.06586115807294846,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0015,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.1705358624458313,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.001,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.13371551036834717,
      "learning_rate": 3.208e-05,
      "loss": 0.0008,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.16174207627773285,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0015,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.13559308648109436,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.0013,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.08011037111282349,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.0011,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.1787966936826706,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0012,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.12660250067710876,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0009,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.1950419396162033,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0007,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.07557167857885361,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0012,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.10578367114067078,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0017,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.07607372105121613,
      "learning_rate": 3.196e-05,
      "loss": 0.0014,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.1971856951713562,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.0015,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.19578880071640015,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0025,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.036501459777355194,
      "learning_rate": 3.192e-05,
      "loss": 0.0011,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.09130146354436874,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0011,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.2167579084634781,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.0013,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.0830484926700592,
      "learning_rate": 3.188e-05,
      "loss": 0.001,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0005,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.06676891446113586,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0008,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.14852720499038696,
      "learning_rate": 3.184e-05,
      "loss": 0.0018,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.061914268881082535,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.0012,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.08427112549543381,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.001,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.07794240117073059,
      "learning_rate": 3.18e-05,
      "loss": 0.0008,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.2091287523508072,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0015,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.16146238148212433,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0018,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.0,
      "learning_rate": 3.176e-05,
      "loss": 0.0008,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.07376186549663544,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0011,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.001,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.0,
      "learning_rate": 3.172e-05,
      "loss": 0.0011,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.06930244714021683,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0012,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.1470305621623993,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0018,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.04945890232920647,
      "learning_rate": 3.168e-05,
      "loss": 0.0008,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.07349233329296112,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0009,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0014,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.25567328929901123,
      "learning_rate": 3.164e-05,
      "loss": 0.0011,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.10112779587507248,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.001,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.2039628028869629,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0017,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.06328647583723068,
      "learning_rate": 3.16e-05,
      "loss": 0.0016,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.08314578980207443,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0011,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.07934335619211197,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0019,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.08834853768348694,
      "learning_rate": 3.156e-05,
      "loss": 0.0015,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.07915352284908295,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.002,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0007,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.1269453763961792,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.001,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.15055888891220093,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0019,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.10938625782728195,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0017,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.10260769724845886,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0021,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.001,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.2789882719516754,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0011,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.0757882222533226,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.001,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.07543151080608368,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0015,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.0902489423751831,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0008,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.05600149184465408,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0012,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.13149550557136536,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0011,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.08622410893440247,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0015,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.07659649103879929,
      "learning_rate": 3.136e-05,
      "loss": 0.0012,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.09422025084495544,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0012,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.25108131766319275,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0012,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.06393932551145554,
      "learning_rate": 3.132e-05,
      "loss": 0.0013,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.08589979261159897,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.0017,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.07675915211439133,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0011,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.0,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0013,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.07415260374546051,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.001,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.08791687339544296,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.001,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.20063140988349915,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0016,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.13610966503620148,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0012,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.1552266925573349,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0014,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.09462586045265198,
      "learning_rate": 3.12e-05,
      "loss": 0.0012,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.10016228258609772,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.0009,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.0723070576786995,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0012,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.15242137014865875,
      "learning_rate": 3.116e-05,
      "loss": 0.0021,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.14365369081497192,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0014,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.2585546672344208,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0014,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.09380702674388885,
      "learning_rate": 3.112e-05,
      "loss": 0.0017,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.08735164254903793,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0015,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.28607815504074097,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0015,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.08575848489999771,
      "learning_rate": 3.108e-05,
      "loss": 0.0011,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.16370199620723724,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0019,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.14242972433567047,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0011,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.06941795349121094,
      "learning_rate": 3.104e-05,
      "loss": 0.0013,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.14783316850662231,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0012,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.1553562432527542,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0011,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.09211710840463638,
      "learning_rate": 3.1e-05,
      "loss": 0.002,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.06660813838243484,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0012,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.10213220864534378,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.001,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.1292605996131897,
      "learning_rate": 3.096e-05,
      "loss": 0.0026,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.07963893562555313,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0013,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.13176807761192322,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0014,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.09076537191867828,
      "learning_rate": 3.092e-05,
      "loss": 0.0015,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0015,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.08220242708921432,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.0013,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.06263288855552673,
      "learning_rate": 3.088e-05,
      "loss": 0.0011,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.1132061779499054,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.001,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.14202181994915009,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0011,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.08158621937036514,
      "learning_rate": 3.084e-05,
      "loss": 0.0013,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.07395024597644806,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0006,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.07623246312141418,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0014,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.0765242949128151,
      "learning_rate": 3.08e-05,
      "loss": 0.0013,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.07705419510602951,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0011,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.2135366052389145,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0013,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.13597482442855835,
      "learning_rate": 3.076e-05,
      "loss": 0.0013,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.10577214509248734,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.0015,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.09475310146808624,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0011,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.2811586856842041,
      "learning_rate": 3.072e-05,
      "loss": 0.0009,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.10602805763483047,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0008,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.3247842490673065,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0012,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.06699234247207642,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0007,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.14572060108184814,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0016,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.05640041083097458,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0016,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.11219564825296402,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0011,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.0440661758184433,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0013,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.06291261315345764,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0016,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.19360105693340302,
      "learning_rate": 3.06e-05,
      "loss": 0.0013,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.08164651691913605,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.0011,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.07739146053791046,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0015,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.258212685585022,
      "learning_rate": 3.056e-05,
      "loss": 0.0011,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.072936050593853,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0009,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.0,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0016,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.08737779408693314,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.001,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.1403719186782837,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0014,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.14171360433101654,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.0013,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.13022612035274506,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.001,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.07404720038175583,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.002,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.11566535383462906,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0015,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.1439674347639084,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0016,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.05402541905641556,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.001,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.0774235799908638,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0013,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.25737306475639343,
      "learning_rate": 3.04e-05,
      "loss": 0.0016,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.32535263895988464,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0014,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.0503876619040966,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0008,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.0,
      "learning_rate": 3.036e-05,
      "loss": 0.0009,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.14369279146194458,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0011,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.08033797144889832,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0015,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.0,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0013,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.10308825969696045,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0016,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.08066083490848541,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0009,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.08759746700525284,
      "learning_rate": 3.028e-05,
      "loss": 0.0008,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.15711069107055664,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0012,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.33427003026008606,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0012,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.0790887102484703,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.001,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.21692059934139252,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0014,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.09002046287059784,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0018,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.06547076255083084,
      "learning_rate": 3.02e-05,
      "loss": 0.0015,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.3439260423183441,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0009,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0011,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.22761346399784088,
      "learning_rate": 3.016e-05,
      "loss": 0.0016,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.1102898120880127,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.0011,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.08510388433933258,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0007,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.1155681163072586,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0011,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.002,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.07439874112606049,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0014,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.06700188666582108,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0015,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.07902009040117264,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0021,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.07601740956306458,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.0016,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.09261307865381241,
      "learning_rate": 3.004e-05,
      "loss": 0.0016,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.11443190276622772,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0012,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.0737229436635971,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.001,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.09482221305370331,
      "learning_rate": 3e-05,
      "loss": 0.0019,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.10119356215000153,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.0016,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.07320264726877213,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0009,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.0,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0003,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.001,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.18142805993556976,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0018,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.08881078660488129,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0016,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.14006663858890533,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0009,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0011,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.10207269340753555,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0018,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.31912434101104736,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.001,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.18364685773849487,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0011,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.19771534204483032,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0013,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.12136171758174896,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0009,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.07094589620828629,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0011,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.0,
      "learning_rate": 2.98e-05,
      "loss": 0.0004,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.09487875550985336,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.001,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.08723714202642441,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0013,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.07246649265289307,
      "learning_rate": 2.976e-05,
      "loss": 0.0014,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.10380608588457108,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.001,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.16661494970321655,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0013,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.0,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0011,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.10558226704597473,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0004,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.14944683015346527,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0018,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.13579416275024414,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0013,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.19042690098285675,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0011,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.06987476348876953,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0008,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.10552551597356796,
      "learning_rate": 2.964e-05,
      "loss": 0.0016,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.1488654911518097,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0013,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.13162119686603546,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0013,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.09791046380996704,
      "learning_rate": 2.96e-05,
      "loss": 0.0012,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.1013876274228096,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.0013,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.1334131360054016,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0015,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.06465469300746918,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0013,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.27660128474235535,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.0009,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.2337145060300827,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0006,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.03997018560767174,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.0023,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.1349932849407196,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0009,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.13479922711849213,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0015,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.0,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0017,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.21503020823001862,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0018,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.14238522946834564,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.002,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.10894164443016052,
      "learning_rate": 2.944e-05,
      "loss": 0.0017,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.14941860735416412,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0013,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.2776165306568146,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0021,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.06975962221622467,
      "learning_rate": 2.94e-05,
      "loss": 0.0009,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.15894147753715515,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.001,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.0663774386048317,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.001,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.23887889087200165,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0015,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.08977213501930237,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0006,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.1510811299085617,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0013,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.07533711194992065,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0009,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.22381463646888733,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0024,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.06647146493196487,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0016,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.26928994059562683,
      "learning_rate": 2.928e-05,
      "loss": 0.0013,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.0716957077383995,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0008,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.14372260868549347,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0011,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.04229331016540527,
      "learning_rate": 2.924e-05,
      "loss": 0.0013,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.1488693803548813,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.0025,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.07973168045282364,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0013,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.13336482644081116,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.002,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.07826697081327438,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.0012,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.07504508644342422,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0021,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.08652802556753159,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0015,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.18277721107006073,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0018,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.11138513684272766,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0006,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.08536646515130997,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.0009,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.08165960013866425,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0007,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.14888565242290497,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.001,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.16774730384349823,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0009,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.1338934451341629,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0011,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.07313250750303268,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.001,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.0,
      "learning_rate": 2.904e-05,
      "loss": 0.0023,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0008,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.19733239710330963,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.0014,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.0,
      "learning_rate": 2.9e-05,
      "loss": 0.0013,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.07957262545824051,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0012,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.048313066363334656,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.001,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.13118302822113037,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.0013,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0008,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.001,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.0,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0013,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.061842892318964005,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0015,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.2097671926021576,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0012,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.076499342918396,
      "learning_rate": 2.888e-05,
      "loss": 0.0009,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0011,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.1291225701570511,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0019,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.2600751221179962,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.0021,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.14557600021362305,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0009,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.06866586953401566,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.001,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.2195056974887848,
      "learning_rate": 2.88e-05,
      "loss": 0.002,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.13755646347999573,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0014,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.07185859978199005,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0003,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.0,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.0012,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.08230739086866379,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0007,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.04364089295268059,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.001,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.07237063348293304,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0013,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.3030782639980316,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.0015,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.27978941798210144,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0013,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.0,
      "learning_rate": 2.868e-05,
      "loss": 0.001,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.07491131871938705,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0013,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.15196189284324646,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0019,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.1882418394088745,
      "learning_rate": 2.864e-05,
      "loss": 0.0015,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.15065865218639374,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.0011,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.030738383531570435,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0012,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.08628074079751968,
      "learning_rate": 2.86e-05,
      "loss": 0.0013,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.07400182634592056,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0011,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.20100243389606476,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0009,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.13810531795024872,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0011,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.001,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.12379614263772964,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0009,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.14180471003055573,
      "learning_rate": 2.852e-05,
      "loss": 0.001,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.114956334233284,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.0009,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.08314061909914017,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.0005,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.10686347633600235,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0018,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.13638833165168762,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0008,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.3279089033603668,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0013,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.06476402282714844,
      "learning_rate": 2.844e-05,
      "loss": 0.0007,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.061775095760822296,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.0006,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0012,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.3280726373195648,
      "learning_rate": 2.84e-05,
      "loss": 0.0011,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.06995867192745209,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0012,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.2532537877559662,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0007,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.05977233126759529,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0018,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.03391192853450775,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0011,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.07973485440015793,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.001,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.20539167523384094,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0018,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.12194843590259552,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0009,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.13808591663837433,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.0012,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.08610277622938156,
      "learning_rate": 2.828e-05,
      "loss": 0.0012,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.15171818435192108,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0014,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.07930557429790497,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.001,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.20305617153644562,
      "learning_rate": 2.824e-05,
      "loss": 0.0012,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.09513996541500092,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0009,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.09083133935928345,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.0008,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.08574115484952927,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0009,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.34246426820755005,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0016,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.09042900055646896,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0011,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.15466320514678955,
      "learning_rate": 2.816e-05,
      "loss": 0.0014,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.140090212225914,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0008,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.119434654712677,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0019,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.09643346816301346,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0013,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.11995599418878555,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.001,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.07401186972856522,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0014,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.07000889629125595,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0009,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.38452252745628357,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0016,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.12639176845550537,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0012,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.09607529640197754,
      "learning_rate": 2.804e-05,
      "loss": 0.0018,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.06808306276798248,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0016,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.15961028635501862,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0011,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.06193730980157852,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.001,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.19805891811847687,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0012,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.13995830714702606,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0018,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.031772680580616,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0013,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.10146763175725937,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.0008,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.09781385213136673,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0012,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.0,
      "learning_rate": 2.792e-05,
      "loss": 0.001,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.14950627088546753,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.001,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.12604151666164398,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0015,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.07748749107122421,
      "learning_rate": 2.788e-05,
      "loss": 0.0013,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.08183832466602325,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0005,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.12823982536792755,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0016,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.13329815864562988,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0013,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.07992783933877945,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0008,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.27804356813430786,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0014,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.0,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.001,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0015,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.09170840680599213,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.0014,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.12072911858558655,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.001,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.13759329915046692,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.0008,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.08386406302452087,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0016,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.13988231122493744,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.0013,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0011,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.14932209253311157,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0016,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.16858835518360138,
      "learning_rate": 2.768e-05,
      "loss": 0.0019,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.19208931922912598,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0014,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.20764358341693878,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0008,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.08341479301452637,
      "learning_rate": 2.764e-05,
      "loss": 0.0008,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.06482374668121338,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.001,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.2431558519601822,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0016,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.1495230793952942,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0015,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.0696449875831604,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0013,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.19709263741970062,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0021,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.20042204856872559,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0012,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.09846282750368118,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0014,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0011,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.0798785611987114,
      "learning_rate": 2.752e-05,
      "loss": 0.0009,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.1287471354007721,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0011,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.07641690969467163,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.001,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.1411008983850479,
      "learning_rate": 2.748e-05,
      "loss": 0.0014,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0005,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.14842040836811066,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.001,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.5701593160629272,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0017,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.06145235151052475,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0019,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.06560122966766357,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.0014,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.059363506734371185,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0012,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.0008,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.11046405881643295,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0023,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.1639176309108734,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0012,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.1031893789768219,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0009,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.10894037038087845,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0011,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.2026730179786682,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0017,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.09315387159585953,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0017,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.06135549768805504,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.0008,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.10682550817728043,
      "learning_rate": 2.728e-05,
      "loss": 0.0016,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0007,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.1078740730881691,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0009,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.13181966543197632,
      "learning_rate": 2.724e-05,
      "loss": 0.0015,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.15539532899856567,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0015,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.13109396398067474,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0009,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.1366732120513916,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0015,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.09710218012332916,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0008,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.24016045033931732,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.0012,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.15010766685009003,
      "learning_rate": 2.716e-05,
      "loss": 0.0009,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.08184879273176193,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.0011,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.2099045217037201,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0013,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.07288496196269989,
      "learning_rate": 2.712e-05,
      "loss": 0.0011,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.24849186837673187,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0009,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.20388922095298767,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0004,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.33960774540901184,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.002,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.12288136035203934,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0009,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.08859935402870178,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0011,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.08703751862049103,
      "learning_rate": 2.704e-05,
      "loss": 0.0019,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.038220662623643875,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0013,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0011,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.08579766005277634,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0014,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.09420781582593918,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0009,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.12418576329946518,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0012,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.0706901028752327,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0011,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.2697143852710724,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.001,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.001,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.42192214727401733,
      "learning_rate": 2.692e-05,
      "loss": 0.0013,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.09422962367534637,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0021,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.19209450483322144,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0012,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.1870856136083603,
      "learning_rate": 2.688e-05,
      "loss": 0.0013,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0009,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.21345867216587067,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0011,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.08950090408325195,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.0007,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.10914681106805801,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0012,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.09613586217164993,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.001,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.21121957898139954,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0012,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.08070572465658188,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.001,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.20107243955135345,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0016,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.1470787674188614,
      "learning_rate": 2.676e-05,
      "loss": 0.0012,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.0008,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.10835889726877213,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.001,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.1635413020849228,
      "learning_rate": 2.672e-05,
      "loss": 0.001,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.09137212485074997,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.001,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.14439435303211212,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0009,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.16871054470539093,
      "learning_rate": 2.668e-05,
      "loss": 0.0017,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.1773584485054016,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0017,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0011,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.21920999884605408,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.001,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.08950179070234299,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.0014,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.0511760450899601,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.0017,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.07574998587369919,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0018,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.19253048300743103,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.001,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.1487363576889038,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.0015,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.09733856469392776,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0019,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.1990361213684082,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0014,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.11259566247463226,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0013,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.0,
      "learning_rate": 2.652e-05,
      "loss": 0.0013,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.2356545776128769,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0018,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.12775439023971558,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0006,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.12702637910842896,
      "learning_rate": 2.648e-05,
      "loss": 0.0012,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.13653390109539032,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0008,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.040251605212688446,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0008,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.0,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.001,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.09621836245059967,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0015,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.08442594856023788,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0011,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.20256023108959198,
      "learning_rate": 2.64e-05,
      "loss": 0.0016,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.12193825095891953,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0017,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.13365086913108826,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0009,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.09859749674797058,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0014,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.0870763435959816,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0012,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0008,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.08321865648031235,
      "learning_rate": 2.632e-05,
      "loss": 0.0011,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.08521716296672821,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0007,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.0013,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.19009648263454437,
      "learning_rate": 2.628e-05,
      "loss": 0.0009,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0016,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.0806315541267395,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0018,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.0,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0017,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0008,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.14701680839061737,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0012,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.14522899687290192,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0011,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.14113923907279968,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0013,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.0806412324309349,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0013,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.13695093989372253,
      "learning_rate": 2.616e-05,
      "loss": 0.0013,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.15092170238494873,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0013,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.1898898184299469,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0018,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.15137815475463867,
      "learning_rate": 2.612e-05,
      "loss": 0.0012,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.1227894276380539,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0012,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.16704800724983215,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0011,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.19434143602848053,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0016,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.30365684628486633,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0012,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.15859530866146088,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0011,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.0,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.001,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.4787795841693878,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0015,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.14549756050109863,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0014,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.0980723425745964,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.001,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.11639784276485443,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0014,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.07549700886011124,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0008,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.07344745844602585,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.0013,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.2072586715221405,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0013,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0006,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.06158125028014183,
      "learning_rate": 2.592e-05,
      "loss": 0.0011,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0014,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.1600254327058792,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0013,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.09429356455802917,
      "learning_rate": 2.588e-05,
      "loss": 0.0007,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.14932243525981903,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0013,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.08067865669727325,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0013,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.09630483388900757,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0012,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.09647095948457718,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.001,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.1772061139345169,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0009,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.17510615289211273,
      "learning_rate": 2.58e-05,
      "loss": 0.0018,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.08768659085035324,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0018,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.0892629399895668,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0019,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.0,
      "learning_rate": 2.576e-05,
      "loss": 0.0013,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.0750003233551979,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0008,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0012,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.0,
      "learning_rate": 2.572e-05,
      "loss": 0.0008,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.2023938000202179,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0014,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.07814223319292068,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0009,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.18836259841918945,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0012,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.1310402899980545,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0008,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.08625878393650055,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.0012,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.09329882264137268,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0009,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.09010998159646988,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0011,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.3014208674430847,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0005,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.0,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0012,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.2807482182979584,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.0017,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.16824592649936676,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.001,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.13371050357818604,
      "learning_rate": 2.556e-05,
      "loss": 0.0009,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.20396754145622253,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0016,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.07841727882623672,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0008,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.15440869331359863,
      "learning_rate": 2.552e-05,
      "loss": 0.001,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.08428936451673508,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.0013,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.09987056255340576,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0009,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.1540674865245819,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0009,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.07083726674318314,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0014,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.09505319595336914,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0009,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.0823950320482254,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0017,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.0719509944319725,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0015,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.09083180129528046,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0012,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.1837873011827469,
      "learning_rate": 2.54e-05,
      "loss": 0.0012,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.10519909858703613,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0014,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0014,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.1033996194601059,
      "learning_rate": 2.536e-05,
      "loss": 0.0013,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.13079455494880676,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0012,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.08582880347967148,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0023,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.1590123027563095,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0009,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.11221308261156082,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0013,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.1379813700914383,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0012,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.12455276399850845,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.0008,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.2083696573972702,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0009,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.2038191258907318,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.0009,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.10259360074996948,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0009,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.09420963376760483,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0008,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0004,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.10213807225227356,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0012,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.08757378160953522,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0014,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.11159902811050415,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0011,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.09704550355672836,
      "learning_rate": 2.516e-05,
      "loss": 0.002,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.20123375952243805,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.001,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.07383107393980026,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0017,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.08304505050182343,
      "learning_rate": 2.512e-05,
      "loss": 0.0011,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.15507328510284424,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.0006,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.15253637731075287,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0009,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.0,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0007,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.083774134516716,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0015,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.09013047069311142,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.001,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.13358362019062042,
      "learning_rate": 2.504e-05,
      "loss": 0.0011,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.15233105421066284,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0009,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.1559801548719406,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0018,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.07721610367298126,
      "learning_rate": 2.5e-05,
      "loss": 0.0014,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0014093085192143917,
      "eval_runtime": 117.6199,
      "eval_samples_per_second": 1275.295,
      "eval_steps_per_second": 31.882,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0015,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.14188753068447113,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0012,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.1630437672138214,
      "learning_rate": 2.496e-05,
      "loss": 0.0008,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.09010585397481918,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0015,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.0809912458062172,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0012,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.08829405158758163,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.0017,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.09489912539720535,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0008,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.1572776585817337,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0017,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.14681701362133026,
      "learning_rate": 2.488e-05,
      "loss": 0.001,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.1079394519329071,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.001,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.08635500818490982,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.001,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.0,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0019,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.06891509890556335,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0012,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.09257657825946808,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0021,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.08805447816848755,
      "learning_rate": 2.48e-05,
      "loss": 0.0009,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0015,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0016,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.08247492462396622,
      "learning_rate": 2.476e-05,
      "loss": 0.001,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.2702990174293518,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0008,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.15329043567180634,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0011,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.0,
      "learning_rate": 2.472e-05,
      "loss": 0.0017,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.0691392570734024,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0011,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.2406933456659317,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.0011,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.12399625033140182,
      "learning_rate": 2.468e-05,
      "loss": 0.0009,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0007,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.11262395977973938,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0011,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.1469513177871704,
      "learning_rate": 2.464e-05,
      "loss": 0.0013,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.001,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.07650532573461533,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.001,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.15949493646621704,
      "learning_rate": 2.46e-05,
      "loss": 0.0011,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0007,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.06901269406080246,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0006,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.2103745937347412,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.0013,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.17690233886241913,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0015,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.07313298434019089,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0009,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.06850463896989822,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0009,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.15406794846057892,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.0012,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.09584339708089828,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0009,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.15928258001804352,
      "learning_rate": 2.448e-05,
      "loss": 0.0011,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.05860384553670883,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0009,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0012,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.26165786385536194,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0011,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.07867559045553207,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0006,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.11805203557014465,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0015,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.0,
      "learning_rate": 2.44e-05,
      "loss": 0.0011,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.08957219868898392,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.001,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.12701305747032166,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0013,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.1540614813566208,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0008,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.09813419729471207,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.0009,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.0740586370229721,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0009,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.08130946755409241,
      "learning_rate": 2.432e-05,
      "loss": 0.0014,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.104294054210186,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0012,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.08902708441019058,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0004,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.0,
      "learning_rate": 2.428e-05,
      "loss": 0.001,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.13778237998485565,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0009,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.0943019837141037,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0007,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.061645615845918655,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0013,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.0804857611656189,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.0013,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.0,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0013,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.2174498736858368,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.001,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.1070498675107956,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.0004,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.15049505233764648,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.001,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.14333166182041168,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0012,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0006,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.07930552959442139,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0006,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.0893244743347168,
      "learning_rate": 2.412e-05,
      "loss": 0.0007,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.23855693638324738,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.001,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.0972025915980339,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0015,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.05556613579392433,
      "learning_rate": 2.408e-05,
      "loss": 0.0017,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.11354485154151917,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0015,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.07036729902029037,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0014,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.06315360963344574,
      "learning_rate": 2.404e-05,
      "loss": 0.0018,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0006,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.0,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.001,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.16080527007579803,
      "learning_rate": 2.4e-05,
      "loss": 0.0017,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0013,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.06327180564403534,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.001,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.0,
      "learning_rate": 2.396e-05,
      "loss": 0.001,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.21683359146118164,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0017,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.0830487608909607,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0016,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.2542499303817749,
      "learning_rate": 2.392e-05,
      "loss": 0.0007,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.14520443975925446,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0011,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.1716328263282776,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.0012,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.09046519547700882,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0013,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.06451646983623505,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.001,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.16282868385314941,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0013,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.33089011907577515,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0015,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.001,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.16641975939273834,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0011,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.10699896514415741,
      "learning_rate": 2.38e-05,
      "loss": 0.0015,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.19027207791805267,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0018,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.08646462112665176,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0014,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.07653991878032684,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0012,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.07105007022619247,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.0011,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.08665972203016281,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0004,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.14893625676631927,
      "learning_rate": 2.372e-05,
      "loss": 0.0011,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.10626493394374847,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0016,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0013,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.09169521927833557,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0004,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.11284863948822021,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0011,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.06867925822734833,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0012,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.1495886743068695,
      "learning_rate": 2.364e-05,
      "loss": 0.0012,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.1524321436882019,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.0013,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.08914150297641754,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0014,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.07762575894594193,
      "learning_rate": 2.36e-05,
      "loss": 0.0011,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.12861864268779755,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0015,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.08450771123170853,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.0005,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.0,
      "learning_rate": 2.356e-05,
      "loss": 0.0015,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.08188756555318832,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0019,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.08726988732814789,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0004,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.07904594391584396,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0015,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0009,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.06720263510942459,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0012,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.20687516033649445,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0004,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.2228168100118637,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0012,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.12792201340198517,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0018,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.2313511222600937,
      "learning_rate": 2.344e-05,
      "loss": 0.0009,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.1510312557220459,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.0015,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.1197308897972107,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0011,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.0,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0015,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.14199420809745789,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0009,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.0009,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.1060626208782196,
      "learning_rate": 2.336e-05,
      "loss": 0.0017,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0008,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.10483060777187347,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0009,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.1674191951751709,
      "learning_rate": 2.332e-05,
      "loss": 0.001,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.08266188949346542,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0015,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.06436119228601456,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.0012,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.12601019442081451,
      "learning_rate": 2.328e-05,
      "loss": 0.0008,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.1354978382587433,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0013,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.13886615633964539,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.0017,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.16918137669563293,
      "learning_rate": 2.324e-05,
      "loss": 0.0006,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.31216520071029663,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0013,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.3508862555027008,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0014,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.11673840135335922,
      "learning_rate": 2.32e-05,
      "loss": 0.0012,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.07182914018630981,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.001,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.07671118527650833,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0013,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.0835469514131546,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0015,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.10711752623319626,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0019,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.16529524326324463,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0008,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.14491702616214752,
      "learning_rate": 2.312e-05,
      "loss": 0.0017,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.10964706540107727,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.001,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.07947398722171783,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.001,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.07503176480531693,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0009,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.0738319531083107,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.001,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.0,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.0011,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.07595602422952652,
      "learning_rate": 2.304e-05,
      "loss": 0.0005,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.09763739258050919,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.0014,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.08255449682474136,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0016,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.14674299955368042,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0008,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.25305938720703125,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0008,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.250937283039093,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0012,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.08957523107528687,
      "learning_rate": 2.296e-05,
      "loss": 0.0011,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.08028966933488846,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0007,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.09708935022354126,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0013,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.11190938204526901,
      "learning_rate": 2.292e-05,
      "loss": 0.0016,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.13630203902721405,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0012,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.10360075533390045,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0006,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.07988617569208145,
      "learning_rate": 2.288e-05,
      "loss": 0.0012,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.16651678085327148,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.002,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.1468355804681778,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.001,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.1358788162469864,
      "learning_rate": 2.284e-05,
      "loss": 0.0014,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0007,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.089958555996418,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0006,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.12199807912111282,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0013,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.11443199217319489,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.0013,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.0013,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.167194202542305,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.0011,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.15717178583145142,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.0021,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.09140756726264954,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0008,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.06630774587392807,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.001,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.055894408375024796,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.001,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.09516044706106186,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0011,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.13488242030143738,
      "learning_rate": 2.268e-05,
      "loss": 0.0014,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.09088998287916183,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0017,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.0,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0008,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.11192753165960312,
      "learning_rate": 2.264e-05,
      "loss": 0.0012,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.14535629749298096,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0012,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.17012344300746918,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0007,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.1023857519030571,
      "learning_rate": 2.26e-05,
      "loss": 0.0017,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.13707569241523743,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.0014,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.20744356513023376,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.001,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.22675511240959167,
      "learning_rate": 2.256e-05,
      "loss": 0.0023,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.13760948181152344,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0012,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.001,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.10582230240106583,
      "learning_rate": 2.252e-05,
      "loss": 0.0008,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.14906816184520721,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0008,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.08041533082723618,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0009,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.09212911874055862,
      "learning_rate": 2.248e-05,
      "loss": 0.0007,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.05611034482717514,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0012,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.06904523074626923,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0012,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.0833270400762558,
      "learning_rate": 2.244e-05,
      "loss": 0.0014,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.2190108746290207,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0008,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.08077224344015121,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0021,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.09077277034521103,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0011,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.29283371567726135,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0009,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.07409307360649109,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.0023,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.08609373867511749,
      "learning_rate": 2.236e-05,
      "loss": 0.0008,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.0011,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.12115233391523361,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0018,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.08967325091362,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.0012,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.09621672332286835,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0013,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.08646129816770554,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0012,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.0685666874051094,
      "learning_rate": 2.228e-05,
      "loss": 0.0006,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.11276435852050781,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.001,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.07630633562803268,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0011,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.21702226996421814,
      "learning_rate": 2.224e-05,
      "loss": 0.0011,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.1755739003419876,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0007,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.15608550608158112,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.001,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.16603149473667145,
      "learning_rate": 2.22e-05,
      "loss": 0.0013,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.0018,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.09319304674863815,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0015,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.09703154861927032,
      "learning_rate": 2.216e-05,
      "loss": 0.0016,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.4230377674102783,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.0009,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.19500397145748138,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.001,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.08361511677503586,
      "learning_rate": 2.212e-05,
      "loss": 0.0005,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.07797782868146896,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0006,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.07037439942359924,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.0006,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.1621156483888626,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0017,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.06934533268213272,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0009,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.08080555498600006,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0013,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.09470321983098984,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0009,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.37613925337791443,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.0015,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.2563173770904541,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.001,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.0,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0016,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.09449417889118195,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0012,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.19777613878250122,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0013,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.0768243819475174,
      "learning_rate": 2.196e-05,
      "loss": 0.001,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.13063368201255798,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0012,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.07849325984716415,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0009,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.14733947813510895,
      "learning_rate": 2.192e-05,
      "loss": 0.0011,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.07924415171146393,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.001,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.1074829176068306,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0012,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.06448033452033997,
      "learning_rate": 2.188e-05,
      "loss": 0.0012,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.3047775626182556,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0019,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.2646401524543762,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0009,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.1799306422472,
      "learning_rate": 2.184e-05,
      "loss": 0.0006,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.14384648203849792,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0009,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.10741513222455978,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0006,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.059149373322725296,
      "learning_rate": 2.18e-05,
      "loss": 0.0012,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.20065678656101227,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0016,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0019,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.05509205907583237,
      "learning_rate": 2.176e-05,
      "loss": 0.0008,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.09924150258302689,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0012,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.15978454053401947,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0006,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.15572164952754974,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0008,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.10791922360658646,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.001,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.07195670157670975,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0014,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.19314537942409515,
      "learning_rate": 2.168e-05,
      "loss": 0.001,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0009,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.07104088366031647,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0009,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.14002984762191772,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0007,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.13168953359127045,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0013,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.15810605883598328,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0011,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.09397485852241516,
      "learning_rate": 2.16e-05,
      "loss": 0.0008,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.15027295053005219,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0012,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.28115856647491455,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0013,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.0,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0004,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.06830720603466034,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0015,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.3854672312736511,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0012,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.12856702506542206,
      "learning_rate": 2.152e-05,
      "loss": 0.0004,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.07205791771411896,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0012,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.19502177834510803,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0011,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.09539928287267685,
      "learning_rate": 2.148e-05,
      "loss": 0.0011,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.14177775382995605,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0012,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0009,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.07610845565795898,
      "learning_rate": 2.144e-05,
      "loss": 0.0008,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.08683715760707855,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0015,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.07900068908929825,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.001,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.0978570505976677,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0004,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.09398987889289856,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0011,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.0808621495962143,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.0013,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.0,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0007,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.1681269407272339,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0007,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.2072601616382599,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0011,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.07448840886354446,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.0007,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.10133033245801926,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0012,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.0,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.0012,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.10885059088468552,
      "learning_rate": 2.128e-05,
      "loss": 0.0012,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.22200648486614227,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0014,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.08692361414432526,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.0012,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.08522098511457443,
      "learning_rate": 2.124e-05,
      "loss": 0.0012,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.10662612318992615,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.0008,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.05132313817739487,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0007,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.14477521181106567,
      "learning_rate": 2.12e-05,
      "loss": 0.0007,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.10195956379175186,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.0014,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.08112075179815292,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0005,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.1485053449869156,
      "learning_rate": 2.116e-05,
      "loss": 0.0011,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.08227463066577911,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.0008,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0004,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.1715330183506012,
      "learning_rate": 2.112e-05,
      "loss": 0.0017,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.08140584826469421,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0012,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.09502438455820084,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.001,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.0734160915017128,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0007,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.14636114239692688,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0011,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.17585758864879608,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0011,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.17582514882087708,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0011,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.1455468088388443,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0013,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.08597852289676666,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.0007,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.16850075125694275,
      "learning_rate": 2.1e-05,
      "loss": 0.0012,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.13831880688667297,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.0008,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.3242229223251343,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.0008,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.09198316931724548,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.0012,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0009,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0009,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.27902844548225403,
      "learning_rate": 2.092e-05,
      "loss": 0.0009,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.07695312798023224,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.001,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.08108236640691757,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0012,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.16454237699508667,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0005,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.11526624113321304,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0007,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.07866836339235306,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.0013,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.11986086517572403,
      "learning_rate": 2.084e-05,
      "loss": 0.001,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0003,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.001,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.15573883056640625,
      "learning_rate": 2.08e-05,
      "loss": 0.0009,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.11680399626493454,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.001,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.18047188222408295,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.001,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.07969116419553757,
      "learning_rate": 2.076e-05,
      "loss": 0.001,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.1544804573059082,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0005,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.15699677169322968,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0006,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.08370939642190933,
      "learning_rate": 2.072e-05,
      "loss": 0.0017,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.10114328563213348,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0016,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.09549208730459213,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0009,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.0,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.001,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.16291876137256622,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0009,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.09072510153055191,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.001,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.17346148192882538,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0016,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.07809638231992722,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0012,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.09360119700431824,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.001,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.08285015821456909,
      "learning_rate": 2.06e-05,
      "loss": 0.0007,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.2626701593399048,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0012,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.06764024496078491,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.001,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.0,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0014,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.08962903916835785,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0011,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.17488886415958405,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0008,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.0,
      "learning_rate": 2.052e-05,
      "loss": 0.0008,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.1433112621307373,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.0009,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.5512744784355164,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0023,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.16546663641929626,
      "learning_rate": 2.048e-05,
      "loss": 0.0017,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0011,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.17463891208171844,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0009,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.0,
      "learning_rate": 2.044e-05,
      "loss": 0.0009,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0007,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.10831664502620697,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0011,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.11261492967605591,
      "learning_rate": 2.04e-05,
      "loss": 0.0007,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.09195330739021301,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0013,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.09441684931516647,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0012,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.0,
      "learning_rate": 2.036e-05,
      "loss": 0.0007,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.09463689476251602,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0016,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.08419865369796753,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.001,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.0,
      "learning_rate": 2.032e-05,
      "loss": 0.0005,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.15891717374324799,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0012,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0016,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.08708745241165161,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0011,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.001,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.17462259531021118,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.001,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.3078019618988037,
      "learning_rate": 2.024e-05,
      "loss": 0.0015,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.09439784288406372,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0015,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.0668710246682167,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.0009,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.16598789393901825,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.001,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.22990897297859192,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0012,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.28363338112831116,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0012,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.0,
      "learning_rate": 2.016e-05,
      "loss": 0.0009,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.153283029794693,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0014,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.09295718371868134,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.001,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.0,
      "learning_rate": 2.012e-05,
      "loss": 0.001,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.11549562215805054,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0004,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.09045372903347015,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0014,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.09606219083070755,
      "learning_rate": 2.008e-05,
      "loss": 0.0008,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0011,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.0009,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.0,
      "learning_rate": 2.004e-05,
      "loss": 0.0012,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.10049717128276825,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.0009,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.1179831400513649,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0007,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.16570809483528137,
      "learning_rate": 2e-05,
      "loss": 0.001,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.05378420278429985,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0016,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.08559983223676682,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0012,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.10780640691518784,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0015,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.07932793349027634,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.001,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.13679443299770355,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0003,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.22662563621997833,
      "learning_rate": 1.992e-05,
      "loss": 0.0006,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.22810500860214233,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0007,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.15526436269283295,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0009,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.10687324404716492,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.0012,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.15294301509857178,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0009,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.1787971705198288,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0011,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.15165846049785614,
      "learning_rate": 1.984e-05,
      "loss": 0.0011,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.08062591403722763,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.0015,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.24647317826747894,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0019,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.0,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0011,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.0006,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.1805804967880249,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0011,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.06859752535820007,
      "learning_rate": 1.976e-05,
      "loss": 0.0013,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.06495426595211029,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0007,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.0923241451382637,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0007,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.13427716493606567,
      "learning_rate": 1.972e-05,
      "loss": 0.0015,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.0951952412724495,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.001,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.14660032093524933,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0013,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.16576750576496124,
      "learning_rate": 1.968e-05,
      "loss": 0.0013,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.0809398740530014,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0007,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.3331029415130615,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0012,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.0,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.001,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.10100945830345154,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0007,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.14076830446720123,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0015,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.06983693689107895,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0011,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.2854764461517334,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0011,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0015,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.1046617403626442,
      "learning_rate": 1.956e-05,
      "loss": 0.0012,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.09818560630083084,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.0013,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.12272007763385773,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0014,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.22578981518745422,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.0013,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.07300397008657455,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0009,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.09571768343448639,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0013,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.11003546416759491,
      "learning_rate": 1.948e-05,
      "loss": 0.0008,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.14605429768562317,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0014,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.0827942043542862,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0006,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.07345882803201675,
      "learning_rate": 1.944e-05,
      "loss": 0.0011,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.22027063369750977,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0011,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.0760684385895729,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.0008,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.2178218960762024,
      "learning_rate": 1.94e-05,
      "loss": 0.0009,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.35776740312576294,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0017,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0009,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.265863835811615,
      "learning_rate": 1.936e-05,
      "loss": 0.002,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.08668248355388641,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0008,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.4190205931663513,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0018,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.06698910146951675,
      "learning_rate": 1.932e-05,
      "loss": 0.0012,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.09615837782621384,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0016,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.2339257150888443,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0013,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.0,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.0009,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.07589033991098404,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0013,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.2296481430530548,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.0005,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.15157125890254974,
      "learning_rate": 1.924e-05,
      "loss": 0.001,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.0009,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.1082061156630516,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0018,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.2843109667301178,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0011,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.14264333248138428,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.001,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.07086168229579926,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0009,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.0,
      "learning_rate": 1.916e-05,
      "loss": 0.001,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.10408603399991989,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0011,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.20919448137283325,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0008,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.10917703807353973,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.0005,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.14824038743972778,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.0013,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.08591275662183762,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0008,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.08404721319675446,
      "learning_rate": 1.908e-05,
      "loss": 0.0008,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.06646817177534103,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0012,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.10077238827943802,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0012,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.08343526721000671,
      "learning_rate": 1.904e-05,
      "loss": 0.0008,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0009,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.18420927226543427,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0005,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.19236479699611664,
      "learning_rate": 1.9e-05,
      "loss": 0.0009,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.22125501930713654,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0012,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.079006128013134,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.0012,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.06601189821958542,
      "learning_rate": 1.896e-05,
      "loss": 0.0007,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0012,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.10162816941738129,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0014,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.207700714468956,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.0011,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.09653829783201218,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.0011,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.10195118188858032,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0013,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.08928167819976807,
      "learning_rate": 1.888e-05,
      "loss": 0.0013,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.09264218062162399,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0009,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.16220936179161072,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0004,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.2574646770954132,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0015,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.15071551501750946,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0017,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.11803560703992844,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.0007,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.12153049558401108,
      "learning_rate": 1.88e-05,
      "loss": 0.0009,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.03850783407688141,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0009,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.15918727219104767,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0013,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.15950420498847961,
      "learning_rate": 1.876e-05,
      "loss": 0.0015,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.08108476549386978,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.0009,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.12349195033311844,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0011,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.0,
      "learning_rate": 1.872e-05,
      "loss": 0.0009,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.09141499549150467,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0008,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.08087468892335892,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0006,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.053643252700567245,
      "learning_rate": 1.868e-05,
      "loss": 0.002,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.14772765338420868,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0007,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.08174493908882141,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.001,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.08487756550312042,
      "learning_rate": 1.864e-05,
      "loss": 0.0012,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.2854897677898407,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0018,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.10513515770435333,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0007,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.1754569709300995,
      "learning_rate": 1.86e-05,
      "loss": 0.0015,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.137197807431221,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0009,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.1408715397119522,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0014,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.2200116366147995,
      "learning_rate": 1.856e-05,
      "loss": 0.0008,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.10231900215148926,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.001,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.001,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.21722352504730225,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0011,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.16159982979297638,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.001,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.16824620962142944,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0006,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.0,
      "learning_rate": 1.848e-05,
      "loss": 0.0011,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.19719301164150238,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0012,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.10380642116069794,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0008,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.0795966386795044,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0017,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.06537111103534698,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.0009,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.25645148754119873,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0008,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.09281135350465775,
      "learning_rate": 1.84e-05,
      "loss": 0.0006,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.37004896998405457,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0014,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.1413983255624771,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0013,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.0,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0006,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.09057741612195969,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0006,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.2123565673828125,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0007,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.07212192565202713,
      "learning_rate": 1.832e-05,
      "loss": 0.0022,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.06929542869329453,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0008,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.0991535484790802,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0011,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.10368005186319351,
      "learning_rate": 1.828e-05,
      "loss": 0.0006,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.14487682282924652,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0008,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.13634075224399567,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.0014,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.13523103296756744,
      "learning_rate": 1.824e-05,
      "loss": 0.0014,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.07367618381977081,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.0012,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.08983530104160309,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0011,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.1947137862443924,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0003,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.15179088711738586,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.001,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.106040358543396,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0012,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.0,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0008,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.001,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.07813288271427155,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.001,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.0,
      "learning_rate": 1.812e-05,
      "loss": 0.0013,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.08951679617166519,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.0006,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.07990538328886032,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0008,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.0677335262298584,
      "learning_rate": 1.808e-05,
      "loss": 0.0004,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.16064979135990143,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0014,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.0443132109940052,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0012,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.1371891051530838,
      "learning_rate": 1.804e-05,
      "loss": 0.0011,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.10960475355386734,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.001,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.0,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0008,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.2870114743709564,
      "learning_rate": 1.8e-05,
      "loss": 0.0013,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.06998760998249054,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.0012,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.11828168481588364,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0013,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.23471000790596008,
      "learning_rate": 1.796e-05,
      "loss": 0.0014,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.12992145121097565,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0009,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.4507380723953247,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0014,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.0,
      "learning_rate": 1.792e-05,
      "loss": 0.0009,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.0005,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0006,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.21902261674404144,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.001,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0005,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.001,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.11681251227855682,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0011,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.08603951334953308,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0006,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.08537326753139496,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0006,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.18294186890125275,
      "learning_rate": 1.78e-05,
      "loss": 0.0008,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0012,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.10402415692806244,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0002,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.10978367924690247,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.002,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.001,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0004,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.09325819462537766,
      "learning_rate": 1.772e-05,
      "loss": 0.0004,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.11162614077329636,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0016,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.05324997007846832,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.0011,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.0,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0008,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.16377346217632294,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0009,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.17249280214309692,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0012,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.0755893886089325,
      "learning_rate": 1.764e-05,
      "loss": 0.0007,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0007,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.1013779267668724,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0005,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.11290737241506577,
      "learning_rate": 1.76e-05,
      "loss": 0.0012,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.2340557873249054,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0012,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.15419632196426392,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.001,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.3348064422607422,
      "learning_rate": 1.756e-05,
      "loss": 0.0019,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.06663130223751068,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0005,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.18955905735492706,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0014,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.19781549274921417,
      "learning_rate": 1.752e-05,
      "loss": 0.0012,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.11732546985149384,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.001,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.1691637635231018,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0007,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.2163715362548828,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.0009,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.020549878478050232,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0015,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.16964933276176453,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.001,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.09918175637722015,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0009,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.08503842353820801,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.0007,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.09257662296295166,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0013,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.09683173149824142,
      "learning_rate": 1.74e-05,
      "loss": 0.0007,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.07777664810419083,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0008,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.17699390649795532,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0009,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.102537602186203,
      "learning_rate": 1.736e-05,
      "loss": 0.0011,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.09445317089557648,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0008,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.10162230581045151,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0017,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.29522421956062317,
      "learning_rate": 1.732e-05,
      "loss": 0.0013,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.34426698088645935,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0013,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.15535549819469452,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0011,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.1386314481496811,
      "learning_rate": 1.728e-05,
      "loss": 0.0011,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.29014360904693604,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0009,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.143643319606781,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0009,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.0,
      "learning_rate": 1.724e-05,
      "loss": 0.0006,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.13493631780147552,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0012,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.06754039227962494,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0011,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.22779357433319092,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0011,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.24599602818489075,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0012,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.09982915222644806,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0007,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.08591511100530624,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0013,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.09000588208436966,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0013,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.07514321058988571,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0008,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.08436305820941925,
      "learning_rate": 1.712e-05,
      "loss": 0.0012,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.1627172827720642,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0007,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.09584463387727737,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0006,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.15944921970367432,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0007,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.1105787605047226,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.0008,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.1519286036491394,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.0014,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.05052108317613602,
      "learning_rate": 1.704e-05,
      "loss": 0.0015,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.14787033200263977,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.0015,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.14833538234233856,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0015,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.1805541068315506,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0008,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.14640028774738312,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0008,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.10178946703672409,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.0013,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.1116272360086441,
      "learning_rate": 1.696e-05,
      "loss": 0.0016,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0009,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0009,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.08444046974182129,
      "learning_rate": 1.692e-05,
      "loss": 0.0013,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.2239542007446289,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0014,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.21457979083061218,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0015,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.1585029661655426,
      "learning_rate": 1.688e-05,
      "loss": 0.0012,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.08267869800329208,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0014,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.1324801743030548,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0006,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.14171822369098663,
      "learning_rate": 1.684e-05,
      "loss": 0.0011,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.21195140480995178,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0014,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.14389756321907043,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0008,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.19508899748325348,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0011,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.09152596443891525,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0016,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.29939892888069153,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.002,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.10894078761339188,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.001,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.07786062359809875,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0005,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.13690286874771118,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0011,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.15642300248146057,
      "learning_rate": 1.672e-05,
      "loss": 0.0012,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.22384753823280334,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0015,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0012,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.10607180744409561,
      "learning_rate": 1.668e-05,
      "loss": 0.0007,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.11267898976802826,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0006,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.13846459984779358,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0013,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.1663462221622467,
      "learning_rate": 1.664e-05,
      "loss": 0.0009,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.18329812586307526,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.001,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.0005,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.0,
      "learning_rate": 1.66e-05,
      "loss": 0.0007,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.22463469207286835,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0012,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.08847694098949432,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0012,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.16795453429222107,
      "learning_rate": 1.656e-05,
      "loss": 0.0012,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.09561511129140854,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0006,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.10631659626960754,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0003,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.14628615975379944,
      "learning_rate": 1.652e-05,
      "loss": 0.0009,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.070085309445858,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.001,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0006,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.15103156864643097,
      "learning_rate": 1.648e-05,
      "loss": 0.001,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0008,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.23595087230205536,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0004,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.10347665846347809,
      "learning_rate": 1.644e-05,
      "loss": 0.0013,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.09662523120641708,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.001,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0007,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.22052650153636932,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0011,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.0007,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.001,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.128327876329422,
      "learning_rate": 1.636e-05,
      "loss": 0.001,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.09148920327425003,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0015,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.22943086922168732,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.001,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.15001365542411804,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0012,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.15585818886756897,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0007,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.14137934148311615,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.0018,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.0851786807179451,
      "learning_rate": 1.628e-05,
      "loss": 0.0011,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.1162494421005249,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0006,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.09793936461210251,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.001,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.10445156693458557,
      "learning_rate": 1.624e-05,
      "loss": 0.0012,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.09156625717878342,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0007,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.09397734701633453,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.0006,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.21719308197498322,
      "learning_rate": 1.62e-05,
      "loss": 0.0009,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.10949065536260605,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0014,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.1046207994222641,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.0012,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.14447316527366638,
      "learning_rate": 1.616e-05,
      "loss": 0.0024,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0007,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.07244709879159927,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0006,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.0,
      "learning_rate": 1.612e-05,
      "loss": 0.0005,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.08377590775489807,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0007,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.276995450258255,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0019,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.18074658513069153,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0012,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.1335076093673706,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0011,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.07058332860469818,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0009,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.0,
      "learning_rate": 1.604e-05,
      "loss": 0.0012,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.2560020089149475,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.0014,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.07894827425479889,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.001,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.08911613374948502,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.001,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.08032281696796417,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0012,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.1958884447813034,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0008,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.0,
      "learning_rate": 1.596e-05,
      "loss": 0.001,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.1382095366716385,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0006,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0004,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.15670251846313477,
      "learning_rate": 1.592e-05,
      "loss": 0.001,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.0679163932800293,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.0004,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.0571383535861969,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.001,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.23350469768047333,
      "learning_rate": 1.588e-05,
      "loss": 0.0011,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.10529614984989166,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0009,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.08649224787950516,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0005,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.13795727491378784,
      "learning_rate": 1.584e-05,
      "loss": 0.0011,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.08447030186653137,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0005,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.0961698442697525,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0006,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.07624021917581558,
      "learning_rate": 1.58e-05,
      "loss": 0.0006,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.15406356751918793,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0006,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.25879907608032227,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0004,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.0,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0004,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.16675220429897308,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0012,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.07716284692287445,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0011,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.07992775738239288,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0006,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.10440254211425781,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.0009,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.14057192206382751,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0009,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.16969798505306244,
      "learning_rate": 1.568e-05,
      "loss": 0.0008,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.10064565390348434,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0003,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.10074270516633987,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0007,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.23690885305404663,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0009,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.18857811391353607,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.001,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.16444122791290283,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0011,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.21698668599128723,
      "learning_rate": 1.56e-05,
      "loss": 0.0008,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.09214680641889572,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0013,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.0895528569817543,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0005,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.1970611959695816,
      "learning_rate": 1.556e-05,
      "loss": 0.0005,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.1453568935394287,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.001,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.13393378257751465,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0012,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.1569422334432602,
      "learning_rate": 1.552e-05,
      "loss": 0.0008,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.2912147343158722,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0018,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.22472766041755676,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.0007,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.0,
      "learning_rate": 1.548e-05,
      "loss": 0.0009,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.09811054170131683,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0016,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0007,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.054419513791799545,
      "learning_rate": 1.544e-05,
      "loss": 0.0007,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.1403650939464569,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0016,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.14896728098392487,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.0019,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.07646515965461731,
      "learning_rate": 1.54e-05,
      "loss": 0.001,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0007,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.08397600799798965,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0013,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.11676733195781708,
      "learning_rate": 1.536e-05,
      "loss": 0.0015,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0004,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.12627805769443512,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0007,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.1734790951013565,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0012,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.14310956001281738,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0012,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.0726047158241272,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.0007,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.16666030883789062,
      "learning_rate": 1.528e-05,
      "loss": 0.001,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.10087776184082031,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0011,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.0685202032327652,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.001,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.08939366042613983,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0004,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.0929591953754425,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0011,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0004,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.0,
      "learning_rate": 1.52e-05,
      "loss": 0.001,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.3820353150367737,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0011,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.1275109499692917,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0008,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.10764514654874802,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0004,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.09764552116394043,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0013,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0008,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.09170389920473099,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0016,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0006,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.0,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.0007,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.0,
      "learning_rate": 1.508e-05,
      "loss": 0.0007,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0006,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.16289174556732178,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0012,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.18173474073410034,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0012,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.24393156170845032,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0009,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.13230332732200623,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0013,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.21336326003074646,
      "learning_rate": 1.5e-05,
      "loss": 0.0015,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 0.10831496119499207,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0006,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.05845329165458679,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0005,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.12413612753152847,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.0019,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.09395355731248856,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0006,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.13638530671596527,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0013,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.0,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0009,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.08782169967889786,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0008,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.10032095015048981,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.001,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.0,
      "learning_rate": 1.488e-05,
      "loss": 0.0008,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.21931852400302887,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.001,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.1414376050233841,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0011,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.11294001340866089,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0003,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.24066083133220673,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0012,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.07462367415428162,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0008,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.0,
      "learning_rate": 1.48e-05,
      "loss": 0.0007,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.1036573126912117,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.001,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.20303933322429657,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0015,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.07400057464838028,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.001,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.07277953624725342,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.001,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.14957372844219208,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0012,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.20979885756969452,
      "learning_rate": 1.472e-05,
      "loss": 0.0016,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.06500554829835892,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.0014,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.08727355301380157,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0006,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.0,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0004,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0005,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.09965364634990692,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.001,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.17397020757198334,
      "learning_rate": 1.464e-05,
      "loss": 0.0005,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.20671360194683075,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0013,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.001,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.0,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0008,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.07387924939393997,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0007,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.10042478144168854,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0007,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.0,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0005,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.10226640850305557,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0004,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.07202031463384628,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0017,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.10891590267419815,
      "learning_rate": 1.452e-05,
      "loss": 0.0006,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.10161984711885452,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.0011,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.08553402870893478,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0015,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.17127951979637146,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.0011,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.08213189244270325,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0014,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.3899139165878296,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0009,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.0707964301109314,
      "learning_rate": 1.444e-05,
      "loss": 0.0014,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.0829213410615921,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.0011,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.24631065130233765,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0008,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.07640651613473892,
      "learning_rate": 1.44e-05,
      "loss": 0.0015,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.1903040111064911,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0009,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.15193916857242584,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.0012,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.24748247861862183,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0012,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.19492320716381073,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0012,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.10049771517515182,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0013,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.3241615891456604,
      "learning_rate": 1.432e-05,
      "loss": 0.0008,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.2077256739139557,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.001,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.1133129671216011,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0007,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.09598424285650253,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0012,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.1336284726858139,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0012,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.10120227187871933,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0009,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.0,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.0006,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.184469535946846,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0019,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.07990006357431412,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0006,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.10430022329092026,
      "learning_rate": 1.42e-05,
      "loss": 0.0012,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.14670608937740326,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0015,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.12605422735214233,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0008,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.09398794919252396,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.0012,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.15706099569797516,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0015,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0006,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.0,
      "learning_rate": 1.412e-05,
      "loss": 0.0009,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.3122086822986603,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0012,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.0007,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.0,
      "learning_rate": 1.408e-05,
      "loss": 0.0007,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.1394897997379303,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.001,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.16656358540058136,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0009,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.09501487761735916,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0007,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.08178091049194336,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0007,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.11566071212291718,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0009,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.0,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0011,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.11687387526035309,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.001,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.14779995381832123,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0009,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.24000103771686554,
      "learning_rate": 1.396e-05,
      "loss": 0.0007,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.23718784749507904,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.0007,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.15312153100967407,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0012,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.14539241790771484,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0005,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.10673994570970535,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0008,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.09673009067773819,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0011,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.09040455520153046,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0009,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.07426521182060242,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0015,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.20736747980117798,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0011,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.14518339931964874,
      "learning_rate": 1.384e-05,
      "loss": 0.0012,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.07100829482078552,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0008,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.0779159814119339,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0004,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.24363070726394653,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0008,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.17047233879566193,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.001,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.0762433260679245,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0009,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.2112123668193817,
      "learning_rate": 1.376e-05,
      "loss": 0.0009,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.09875083714723587,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0007,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.19352219998836517,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0014,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.11007105559110641,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0009,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.23243406414985657,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0012,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.1167055070400238,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0009,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.1711343228816986,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0006,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.08834739774465561,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0012,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.0900968387722969,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0007,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.09504643827676773,
      "learning_rate": 1.364e-05,
      "loss": 0.0008,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.14339470863342285,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0012,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.11342339217662811,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0009,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.06671907007694244,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0016,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.15065938234329224,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0016,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.08182631433010101,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0006,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.09213181585073471,
      "learning_rate": 1.356e-05,
      "loss": 0.0007,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0014,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.08795791864395142,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0012,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.08561106026172638,
      "learning_rate": 1.352e-05,
      "loss": 0.001,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.10094200074672699,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0014,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.10449634492397308,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0009,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.22518755495548248,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0014,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.15679854154586792,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0013,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.0007,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.13977700471878052,
      "learning_rate": 1.344e-05,
      "loss": 0.0011,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.09534434974193573,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.001,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.09429768472909927,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0012,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.22187423706054688,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0006,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.07734943181276321,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0011,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0011,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.10464347898960114,
      "learning_rate": 1.336e-05,
      "loss": 0.0014,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.08211197704076767,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.0011,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.2749459743499756,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0012,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.2411375194787979,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0012,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.07914011180400848,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0012,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.19745898246765137,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0017,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.11957661062479019,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.001,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.09338690340518951,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0013,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.18045161664485931,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.002,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.10128822177648544,
      "learning_rate": 1.324e-05,
      "loss": 0.0015,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.11512840539216995,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0014,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.1602853685617447,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0008,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.0,
      "learning_rate": 1.32e-05,
      "loss": 0.0003,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0004,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.08301618695259094,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.0009,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.09543239325284958,
      "learning_rate": 1.316e-05,
      "loss": 0.0005,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.1033889502286911,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.0006,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.1801450550556183,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0005,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.17070528864860535,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0004,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0004,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.05990886315703392,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0006,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.09925485402345657,
      "learning_rate": 1.308e-05,
      "loss": 0.0012,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0012,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.1476164311170578,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0006,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.0,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0008,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.1855255663394928,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0013,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.09454244375228882,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0011,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.08417493104934692,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0014,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.12268746644258499,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0011,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.12181361764669418,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.001,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.1940358728170395,
      "learning_rate": 1.296e-05,
      "loss": 0.0013,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.07397312670946121,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0007,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.06703753024339676,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0013,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.14586737751960754,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0008,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.2147788405418396,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0022,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.09662491828203201,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.001,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.08788270503282547,
      "learning_rate": 1.288e-05,
      "loss": 0.0005,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.09835577011108398,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0012,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.06172371283173561,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0013,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.09896649420261383,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0008,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.0770164504647255,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0007,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.1683124154806137,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0006,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.17088738083839417,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0006,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.0942736566066742,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0013,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.08866820484399796,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0007,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.20773068070411682,
      "learning_rate": 1.276e-05,
      "loss": 0.0006,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.07023324072360992,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0008,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.07711628079414368,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0007,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.21741622686386108,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0016,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.1500394493341446,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0011,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.20300109684467316,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0006,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.0728391781449318,
      "learning_rate": 1.268e-05,
      "loss": 0.0004,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.06978665292263031,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0012,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.1542866826057434,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0004,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.1009017825126648,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.0009,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.258590430021286,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0013,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.001,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.0,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0008,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.08509702980518341,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0013,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.0847284272313118,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0015,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.14071723818778992,
      "learning_rate": 1.256e-05,
      "loss": 0.0007,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0003,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.19401413202285767,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0014,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.2154211401939392,
      "learning_rate": 1.252e-05,
      "loss": 0.0008,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.12648122012615204,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0006,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.10057337582111359,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.0011,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.0,
      "learning_rate": 1.248e-05,
      "loss": 0.0003,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.08854816108942032,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0008,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.22135880589485168,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.001,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.253825306892395,
      "learning_rate": 1.244e-05,
      "loss": 0.0009,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.0,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0007,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.09615300595760345,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0009,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.09763064980506897,
      "learning_rate": 1.24e-05,
      "loss": 0.0004,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.001,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.1709698587656021,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0007,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.16597136855125427,
      "learning_rate": 1.236e-05,
      "loss": 0.0003,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.16030225157737732,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.0012,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.07777750492095947,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0006,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.17419849336147308,
      "learning_rate": 1.232e-05,
      "loss": 0.001,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.14761137962341309,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0012,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0006,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.0,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0012,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0012,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.06943006813526154,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0006,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.09611295163631439,
      "learning_rate": 1.224e-05,
      "loss": 0.0008,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.0,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0005,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.195176362991333,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0014,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.09102071076631546,
      "learning_rate": 1.22e-05,
      "loss": 0.0008,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.15028801560401917,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0008,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.20786701142787933,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0011,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.0,
      "learning_rate": 1.216e-05,
      "loss": 0.0007,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.1478603035211563,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0008,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.001,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.11581489443778992,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0004,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.001,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.24283772706985474,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0008,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.08817905932664871,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0006,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.11594575643539429,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0006,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0009,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.08747853338718414,
      "learning_rate": 1.204e-05,
      "loss": 0.0008,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.11116454750299454,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.001,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.08593977242708206,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0012,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.06963423639535904,
      "learning_rate": 1.2e-05,
      "loss": 0.001,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.1093447282910347,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.0007,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.10912048816680908,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0014,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.07886194437742233,
      "learning_rate": 1.196e-05,
      "loss": 0.0008,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.16763213276863098,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0007,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.15811090171337128,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.001,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.07691360265016556,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0008,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.0761488676071167,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.0005,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.07792914658784866,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0005,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.09712710231542587,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0005,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.09707355499267578,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.001,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.10189192742109299,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0006,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.16604822874069214,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0004,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.0,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0009,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.001,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.15612155199050903,
      "learning_rate": 1.18e-05,
      "loss": 0.0009,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.0696934312582016,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0015,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.11306814849376678,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0006,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.09693612903356552,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0009,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0008,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.09486272931098938,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0006,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.17303653061389923,
      "learning_rate": 1.172e-05,
      "loss": 0.0006,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.11641200631856918,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0009,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.06778479367494583,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0008,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.08163657784461975,
      "learning_rate": 1.168e-05,
      "loss": 0.0011,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.20515689253807068,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0012,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0007,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.0,
      "learning_rate": 1.164e-05,
      "loss": 0.001,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.09690388292074203,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0009,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.15744365751743317,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.001,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.08779642730951309,
      "learning_rate": 1.16e-05,
      "loss": 0.0008,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.07679738849401474,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0005,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.10068008303642273,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.0011,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.0,
      "learning_rate": 1.156e-05,
      "loss": 0.0008,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0005,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.1267060786485672,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0004,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.16946876049041748,
      "learning_rate": 1.152e-05,
      "loss": 0.001,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0008,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0015,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.11835736036300659,
      "learning_rate": 1.148e-05,
      "loss": 0.0011,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.07147135585546494,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.001,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.09925758838653564,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0007,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.0,
      "learning_rate": 1.144e-05,
      "loss": 0.0005,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.08555033057928085,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.0009,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.11086473613977432,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0006,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.07448984682559967,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0009,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.1740884780883789,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.0015,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.07882736623287201,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0008,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.09384246915578842,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.001,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0008,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.1497305929660797,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0006,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.07333867996931076,
      "learning_rate": 1.132e-05,
      "loss": 0.0007,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0011,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.08635550737380981,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.001,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.07650680840015411,
      "learning_rate": 1.128e-05,
      "loss": 0.0006,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.2245195358991623,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0016,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.16080361604690552,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0013,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.0,
      "learning_rate": 1.124e-05,
      "loss": 0.0002,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.08999630063772202,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.0008,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.08191113919019699,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0006,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.0,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0009,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.1050473228096962,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0003,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.0004,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.0,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.0008,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.09433286637067795,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.0007,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.11180432885885239,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0003,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.0961274653673172,
      "learning_rate": 1.112e-05,
      "loss": 0.0004,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.23788364231586456,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0008,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.08340340107679367,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.001,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.0,
      "learning_rate": 1.108e-05,
      "loss": 0.0007,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.0802663043141365,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0015,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.13102221488952637,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0005,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.21826554834842682,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0012,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0009,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.11807139217853546,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0009,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.11825060099363327,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0006,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.1567726582288742,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0009,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.4020633399486542,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.0006,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.07544997334480286,
      "learning_rate": 1.096e-05,
      "loss": 0.0006,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.09214837849140167,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0004,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.16269560158252716,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.001,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.38594669103622437,
      "learning_rate": 1.092e-05,
      "loss": 0.0012,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.09464725852012634,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0009,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.14326123893260956,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.0014,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.18367639183998108,
      "learning_rate": 1.088e-05,
      "loss": 0.0009,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0007,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.0,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0006,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.10928219556808472,
      "learning_rate": 1.084e-05,
      "loss": 0.0006,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0011,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.10720603168010712,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.001,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.10255886614322662,
      "learning_rate": 1.08e-05,
      "loss": 0.0014,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.0014,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.16480670869350433,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.0012,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.0850440263748169,
      "learning_rate": 1.076e-05,
      "loss": 0.0006,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.1987849771976471,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0005,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.08382850885391235,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0007,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.12472307682037354,
      "learning_rate": 1.072e-05,
      "loss": 0.0009,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.3703635036945343,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0007,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.07070589810609818,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0008,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.106532521545887,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0011,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.10941272228956223,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0005,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.0,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0008,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.0,
      "learning_rate": 1.064e-05,
      "loss": 0.0007,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.0631781741976738,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0009,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0004,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.1008359044790268,
      "learning_rate": 1.06e-05,
      "loss": 0.0011,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.09979435801506042,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0012,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.1576172113418579,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0011,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.0,
      "learning_rate": 1.056e-05,
      "loss": 0.0007,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.2145739048719406,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.001,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.13991662859916687,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0011,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.07790914922952652,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.002,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.12484533339738846,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.0009,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.05689126253128052,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.001,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.0,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0005,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.09664841741323471,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0007,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.12465019524097443,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0011,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.10464691370725632,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0006,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.13703885674476624,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0008,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.19198155403137207,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0014,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.0867646336555481,
      "learning_rate": 1.04e-05,
      "loss": 0.0007,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.06962153315544128,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.0012,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.07855702936649323,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0004,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.187325581908226,
      "learning_rate": 1.036e-05,
      "loss": 0.0005,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.22869111597537994,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0005,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.07313954085111618,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0008,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.10463447868824005,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0014,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.0,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0006,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.23374398052692413,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0006,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.2075629085302353,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.001,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.1051940843462944,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.001,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.09691062569618225,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0007,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.14818331599235535,
      "learning_rate": 1.024e-05,
      "loss": 0.0006,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.19734732806682587,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.0007,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.10610485076904297,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0008,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.0,
      "learning_rate": 1.02e-05,
      "loss": 0.0009,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.1081773191690445,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0008,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.07514321058988571,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.0003,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.17457661032676697,
      "learning_rate": 1.016e-05,
      "loss": 0.0012,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.32508400082588196,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.001,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0004,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.22791096568107605,
      "learning_rate": 1.012e-05,
      "loss": 0.0011,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.21206970512866974,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0008,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.08046037703752518,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0006,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.11238980293273926,
      "learning_rate": 1.008e-05,
      "loss": 0.0008,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.09792062640190125,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0007,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.17179128527641296,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0011,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.07769183814525604,
      "learning_rate": 1.004e-05,
      "loss": 0.0008,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.09752510488033295,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.001,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.07854954898357391,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0009,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.1071053147315979,
      "learning_rate": 1e-05,
      "loss": 0.0008,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.1595693677663803,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0008,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.16347824037075043,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.0009,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.17052993178367615,
      "learning_rate": 9.96e-06,
      "loss": 0.0008,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.14813655614852905,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0007,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.07146274298429489,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.001,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.13124388456344604,
      "learning_rate": 9.92e-06,
      "loss": 0.0007,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.0,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.0008,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.0,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0006,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.09770365804433823,
      "learning_rate": 9.88e-06,
      "loss": 0.0006,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.0,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0004,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.0980636477470398,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0008,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.07938119769096375,
      "learning_rate": 9.84e-06,
      "loss": 0.001,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.10832957178354263,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0004,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.12366893887519836,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0011,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.2722935080528259,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0011,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.10936113446950912,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0007,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.15755148231983185,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.0005,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.11614836752414703,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0012,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.07964841276407242,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0011,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.0,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0009,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.10363193601369858,
      "learning_rate": 9.72e-06,
      "loss": 0.0017,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.17154572904109955,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0007,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.083448126912117,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0008,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.1674664169549942,
      "learning_rate": 9.68e-06,
      "loss": 0.0008,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.0,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0006,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.0,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0008,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.1677834838628769,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.0009,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.08689074963331223,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.001,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.30045899748802185,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0009,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.08148802071809769,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0011,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.16083383560180664,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0009,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.1033901795744896,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0009,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.0,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.0004,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.10347650945186615,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0006,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.1287204474210739,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0016,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.17483550310134888,
      "learning_rate": 9.52e-06,
      "loss": 0.0014,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.08156511187553406,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0009,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.0,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0011,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.4610545337200165,
      "learning_rate": 9.48e-06,
      "loss": 0.0002,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.0796075090765953,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0006,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.17778156697750092,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0009,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.10589805990457535,
      "learning_rate": 9.44e-06,
      "loss": 0.0007,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.0,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.0006,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.09283597767353058,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0006,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.10391312092542648,
      "learning_rate": 9.4e-06,
      "loss": 0.0012,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.298292875289917,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0007,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.0727047324180603,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0003,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.06250303983688354,
      "learning_rate": 9.36e-06,
      "loss": 0.0006,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.07718242704868317,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0006,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.14436067640781403,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0014,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.0,
      "learning_rate": 9.32e-06,
      "loss": 0.0008,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.16204120218753815,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0012,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.10944050550460815,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0007,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.17829939723014832,
      "learning_rate": 9.28e-06,
      "loss": 0.0012,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.07499898225069046,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0014,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.11438242346048355,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0007,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.0,
      "learning_rate": 9.24e-06,
      "loss": 0.0008,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.1107519268989563,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0006,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.10490336269140244,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.001,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.1581810563802719,
      "learning_rate": 9.2e-06,
      "loss": 0.0007,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.10890127718448639,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.0012,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.1471349447965622,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0013,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.18713627755641937,
      "learning_rate": 9.16e-06,
      "loss": 0.0009,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.0892285630106926,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0007,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.09447818994522095,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0016,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.0,
      "learning_rate": 9.12e-06,
      "loss": 0.0006,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.15840578079223633,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.001,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.2139768749475479,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.001,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.0,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0007,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.0,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0006,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.24232864379882812,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.0007,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.09510114043951035,
      "learning_rate": 9.04e-06,
      "loss": 0.0004,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.23809833824634552,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.0007,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.10419213771820068,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0005,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.08140356838703156,
      "learning_rate": 9e-06,
      "loss": 0.0013,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.20929402112960815,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0007,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.0,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0008,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.11588813364505768,
      "learning_rate": 8.96e-06,
      "loss": 0.001,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.08867818862199783,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0004,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.0,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0009,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.0,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0007,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.10180002450942993,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.0014,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.08713754266500473,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.001,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.1737983077764511,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.001,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.07977128773927689,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0005,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.1633516401052475,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.0003,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.048583004623651505,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0014,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.06924733519554138,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0012,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.1342008411884308,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0008,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.16726337373256683,
      "learning_rate": 8.8e-06,
      "loss": 0.0004,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.08394993841648102,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0013,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.08399175852537155,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0009,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.39070549607276917,
      "learning_rate": 8.76e-06,
      "loss": 0.0015,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.12284631282091141,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0016,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.2578977942466736,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0012,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.13112394511699677,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0007,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.0,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0003,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.18925930559635162,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0008,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.05699091777205467,
      "learning_rate": 8.68e-06,
      "loss": 0.001,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.0,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0012,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.2636895477771759,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0008,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.0,
      "learning_rate": 8.64e-06,
      "loss": 0.0005,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.16876089572906494,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0008,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.19556930661201477,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0007,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.06616629660129547,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0012,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.09637925028800964,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0007,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.16893115639686584,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0007,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.21305598318576813,
      "learning_rate": 8.56e-06,
      "loss": 0.0006,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.0,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0008,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.07334235310554504,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0011,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.24760496616363525,
      "learning_rate": 8.52e-06,
      "loss": 0.0008,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.11308398097753525,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0016,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.0,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0009,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.22465254366397858,
      "learning_rate": 8.48e-06,
      "loss": 0.0009,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.14749018847942352,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0009,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.1740158051252365,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0009,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.09033627063035965,
      "learning_rate": 8.44e-06,
      "loss": 0.0009,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.108054980635643,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0004,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.1621139943599701,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0006,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.0811578705906868,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.001,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.14832669496536255,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0009,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.1836644858121872,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0009,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.13638748228549957,
      "learning_rate": 8.36e-06,
      "loss": 0.0012,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.07922644168138504,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.0008,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.16898825764656067,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0009,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.08487486094236374,
      "learning_rate": 8.32e-06,
      "loss": 0.0007,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.11042588949203491,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0012,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.1205662414431572,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0009,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.16109834611415863,
      "learning_rate": 8.28e-06,
      "loss": 0.0006,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.07193015515804291,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0008,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.1762278974056244,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.001,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.1663948893547058,
      "learning_rate": 8.24e-06,
      "loss": 0.0014,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.1350385695695877,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.0009,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.159848153591156,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0009,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.12526297569274902,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.001,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.07767727971076965,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0006,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.0,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.001,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.0827273428440094,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0012,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.300071656703949,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0012,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.09899523109197617,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0009,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.12243357300758362,
      "learning_rate": 8.12e-06,
      "loss": 0.0006,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.2537945508956909,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0011,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.12687763571739197,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0015,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.07485929876565933,
      "learning_rate": 8.08e-06,
      "loss": 0.0016,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.10365591198205948,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0007,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.236593097448349,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0007,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.11909040808677673,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0007,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.1656017303466797,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0004,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.10239938646554947,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0005,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.09633589535951614,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0007,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.0,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.0009,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.16155168414115906,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.001,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.22650854289531708,
      "learning_rate": 7.96e-06,
      "loss": 0.0009,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.16914702951908112,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0005,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.0,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0004,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.2708927392959595,
      "learning_rate": 7.92e-06,
      "loss": 0.0013,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.11088000237941742,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0008,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.0,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0008,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.09246016293764114,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0012,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.08906906098127365,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0006,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.09144508093595505,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.0006,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.07421316206455231,
      "learning_rate": 7.84e-06,
      "loss": 0.0008,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.11321188509464264,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.0004,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.0007,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.0,
      "learning_rate": 7.8e-06,
      "loss": 0.0004,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.0,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0007,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0007,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.07364354282617569,
      "learning_rate": 7.76e-06,
      "loss": 0.001,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.10620908439159393,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0004,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.0,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0004,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.14771680533885956,
      "learning_rate": 7.72e-06,
      "loss": 0.0013,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.18377690017223358,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0007,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.0,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0004,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.0,
      "learning_rate": 7.68e-06,
      "loss": 0.0012,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.25445112586021423,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0015,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.07197730243206024,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0005,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.10884501785039902,
      "learning_rate": 7.64e-06,
      "loss": 0.0007,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.2026548832654953,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.001,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.2172805368900299,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0013,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.0743674784898758,
      "learning_rate": 7.6e-06,
      "loss": 0.0006,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.3763136863708496,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0014,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.21406865119934082,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0006,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.0733722597360611,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0008,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.09195572137832642,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0004,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.1000821515917778,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0012,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.0,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0008,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.09256313741207123,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0008,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.06601810455322266,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0004,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.13517753779888153,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0013,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.08140622079372406,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0013,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.0,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.001,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.22963838279247284,
      "learning_rate": 7.44e-06,
      "loss": 0.0009,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.0,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0007,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0007,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.2531598210334778,
      "learning_rate": 7.4e-06,
      "loss": 0.0009,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.36505258083343506,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.0012,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.08085359632968903,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0015,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.07854641228914261,
      "learning_rate": 7.36e-06,
      "loss": 0.0012,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.0721348375082016,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0011,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.08232180774211884,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0011,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.0,
      "learning_rate": 7.32e-06,
      "loss": 0.0006,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.10678060352802277,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0014,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.0,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0005,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.12250547856092453,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.001,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.0,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0011,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.1658264696598053,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.001,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.15444912016391754,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.0011,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.1724715232849121,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.0004,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.23384803533554077,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0009,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.1675463765859604,
      "learning_rate": 7.2e-06,
      "loss": 0.0011,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.0,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0005,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.09884750843048096,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.0008,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.07016178965568542,
      "learning_rate": 7.16e-06,
      "loss": 0.0005,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.08419127762317657,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0005,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.0,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0004,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.11797955632209778,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.0006,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.0,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.0007,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.09454764425754547,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0011,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.10725945979356766,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0011,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.16897495090961456,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.001,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.0,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0006,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.18311592936515808,
      "learning_rate": 7.04e-06,
      "loss": 0.001,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.08344005048274994,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0008,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.08888528496026993,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.001,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.1620265543460846,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0008,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.0,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0011,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.12020836025476456,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.001,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.0,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0008,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.08462051302194595,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0007,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.2619663178920746,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0008,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.16473039984703064,
      "learning_rate": 6.92e-06,
      "loss": 0.0011,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.12059808522462845,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0014,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.0900762528181076,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0008,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.11588434875011444,
      "learning_rate": 6.88e-06,
      "loss": 0.0006,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.07189376652240753,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0011,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.0830271765589714,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0011,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.09336889535188675,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0011,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.1417255699634552,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.001,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.0,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0005,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.09623563289642334,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0007,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.07419263571500778,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0011,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.1527833193540573,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.001,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.16303282976150513,
      "learning_rate": 6.76e-06,
      "loss": 0.0009,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.1584184616804123,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.0013,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.11665724962949753,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0008,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.09905266761779785,
      "learning_rate": 6.72e-06,
      "loss": 0.0005,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.09386840462684631,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.001,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.26321297883987427,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0012,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.13311640918254852,
      "learning_rate": 6.68e-06,
      "loss": 0.0009,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.14886817336082458,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0011,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.12557369470596313,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.001,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.0,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0003,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.2024376094341278,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0007,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.24493464827537537,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.0009,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.2015468329191208,
      "learning_rate": 6.6e-06,
      "loss": 0.0014,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.0,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0006,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.0,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0003,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.07303383946418762,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0008,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.08980073779821396,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.001,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.08819495141506195,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0011,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.10143010318279266,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.001,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.0,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0008,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.0,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0012,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.09412622451782227,
      "learning_rate": 6.48e-06,
      "loss": 0.001,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.14101186394691467,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0007,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.09225806593894958,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.001,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.1449464112520218,
      "learning_rate": 6.44e-06,
      "loss": 0.0014,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.12714925408363342,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0009,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.10428409278392792,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.001,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.08699442446231842,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0013,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.08443114161491394,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.0006,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.04387912154197693,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0012,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.0827348455786705,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0006,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.13745737075805664,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.001,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.08597459644079208,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0004,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.17793206870555878,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0005,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.09951938688755035,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0006,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.09671393036842346,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.0011,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.1813943237066269,
      "learning_rate": 6.28e-06,
      "loss": 0.0006,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.10149743407964706,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0011,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.17263548076152802,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0006,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.0,
      "learning_rate": 6.24e-06,
      "loss": 0.0005,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.0994701236486435,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0011,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.2195717692375183,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0003,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.1397671103477478,
      "learning_rate": 6.2e-06,
      "loss": 0.0004,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.266666054725647,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.001,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.07636188715696335,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.001,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.08091482520103455,
      "learning_rate": 6.16e-06,
      "loss": 0.0008,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.0,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0004,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.0980064868927002,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0006,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.14544306695461273,
      "learning_rate": 6.12e-06,
      "loss": 0.0013,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.07468889653682709,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0007,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.06572065502405167,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0006,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.18340618908405304,
      "learning_rate": 6.08e-06,
      "loss": 0.0011,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.0,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0003,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.0,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0004,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.07379650324583054,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0013,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.13291867077350616,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0013,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.1362408697605133,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0006,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.09466052800416946,
      "learning_rate": 6e-06,
      "loss": 0.0007,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.18632112443447113,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.0013,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.13441073894500732,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.0007,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.1015239953994751,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0008,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.15094640851020813,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0006,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.07134786993265152,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0006,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.18268586695194244,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0014,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.27043065428733826,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0006,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.22471068799495697,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0012,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.0,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0002,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.17731155455112457,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0009,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.3078363537788391,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0006,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.050399236381053925,
      "learning_rate": 5.84e-06,
      "loss": 0.001,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.0775160938501358,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.001,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.18421700596809387,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0009,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.03513140231370926,
      "learning_rate": 5.8e-06,
      "loss": 0.0009,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.09358106553554535,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0005,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.09308668226003647,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0006,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.0,
      "learning_rate": 5.76e-06,
      "loss": 0.0006,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.1160033792257309,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.0009,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.0,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0015,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.10970966517925262,
      "learning_rate": 5.72e-06,
      "loss": 0.0011,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.0,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0007,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.14460891485214233,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0014,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.28620871901512146,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0007,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.0,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0006,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.3005909025669098,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.0006,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.09250407665967941,
      "learning_rate": 5.64e-06,
      "loss": 0.0009,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.0,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0004,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.15038219094276428,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0012,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.1288566291332245,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.001,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.11715933680534363,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0013,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.1576414853334427,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.0011,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.08781727403402328,
      "learning_rate": 5.56e-06,
      "loss": 0.0009,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.08964592218399048,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0003,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.17312681674957275,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0011,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.2336197942495346,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.001,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.09516605734825134,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0009,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.09691420197486877,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0004,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.07245391607284546,
      "learning_rate": 5.48e-06,
      "loss": 0.0009,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.08863087743520737,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0008,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.08146442472934723,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.0004,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.0,
      "learning_rate": 5.44e-06,
      "loss": 0.0004,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.15275278687477112,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.0012,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.1618686318397522,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.001,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.08606688678264618,
      "learning_rate": 5.4e-06,
      "loss": 0.0009,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.15505889058113098,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0008,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.12402297556400299,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0008,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.28373846411705017,
      "learning_rate": 5.36e-06,
      "loss": 0.0012,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.0,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0005,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.1451195776462555,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0005,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.0,
      "learning_rate": 5.32e-06,
      "loss": 0.0003,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.15757720172405243,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0006,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.09146488457918167,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0009,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.13109517097473145,
      "learning_rate": 5.28e-06,
      "loss": 0.0012,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.11560079455375671,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0006,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.0,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0005,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.08975125104188919,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0012,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.0,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.001,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.08238968998193741,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0006,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.0,
      "learning_rate": 5.2e-06,
      "loss": 0.0007,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.1063573881983757,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0004,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.12235457450151443,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0012,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.18825861811637878,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0006,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.10622388124465942,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0007,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.0850653424859047,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0007,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.08047648519277573,
      "learning_rate": 5.12e-06,
      "loss": 0.001,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.057604849338531494,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0007,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.08287674933671951,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0008,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.10740542411804199,
      "learning_rate": 5.08e-06,
      "loss": 0.0008,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.082705557346344,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0009,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.16183005273342133,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.0008,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.11504219472408295,
      "learning_rate": 5.04e-06,
      "loss": 0.0005,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.0,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0004,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.09639989584684372,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0005,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.07312184572219849,
      "learning_rate": 5e-06,
      "loss": 0.0009,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.22377659380435944,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.001,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.15539562702178955,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0008,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.0859694704413414,
      "learning_rate": 4.96e-06,
      "loss": 0.0004,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.08866619318723679,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.001,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.11288615316152573,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0008,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.21019376814365387,
      "learning_rate": 4.92e-06,
      "loss": 0.0008,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.11331785470247269,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0009,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.10011738538742065,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.001,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.07674252986907959,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.0012,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.0,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0006,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0007,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.0,
      "learning_rate": 4.84e-06,
      "loss": 0.0005,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.1508883535861969,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.001,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.09832664579153061,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0009,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.0,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0004,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.07919780910015106,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.001,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.0404980294406414,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.001,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.0,
      "learning_rate": 4.76e-06,
      "loss": 0.0011,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.05444639176130295,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.0008,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.10109246522188187,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0012,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.0,
      "learning_rate": 4.72e-06,
      "loss": 0.0011,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.0,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0008,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.0767611414194107,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0007,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.0,
      "learning_rate": 4.68e-06,
      "loss": 0.0008,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.0,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0008,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0006,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.11459862440824509,
      "learning_rate": 4.64e-06,
      "loss": 0.0009,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.07681088149547577,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0007,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.09462611377239227,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.0009,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.07838049530982971,
      "learning_rate": 4.6e-06,
      "loss": 0.0005,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.09598225355148315,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0005,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.20123131573200226,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0009,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.1971028596162796,
      "learning_rate": 4.56e-06,
      "loss": 0.0005,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.10561485588550568,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0007,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0004,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.0,
      "learning_rate": 4.52e-06,
      "loss": 0.0016,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.09590192884206772,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0005,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.08476553857326508,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.0004,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.0,
      "learning_rate": 4.48e-06,
      "loss": 0.0006,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.17330744862556458,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0003,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.0,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0005,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.10492569953203201,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0008,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.07034043222665787,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0009,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.14428800344467163,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0011,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.11435410380363464,
      "learning_rate": 4.4e-06,
      "loss": 0.0004,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.10700336843729019,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0005,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.12653577327728271,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0007,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.0,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0002,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.0,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0007,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.11417505145072937,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0004,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.08364282548427582,
      "learning_rate": 4.32e-06,
      "loss": 0.0004,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.0,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.001,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.10026432573795319,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0009,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.2914638817310333,
      "learning_rate": 4.28e-06,
      "loss": 0.0013,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.1715201586484909,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.001,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.06512857973575592,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0008,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.09539281576871872,
      "learning_rate": 4.24e-06,
      "loss": 0.0007,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.09117274731397629,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.0005,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.09038631618022919,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.001,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.0,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0009,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.1530488133430481,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.001,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.056486207991838455,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0008,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.0,
      "learning_rate": 4.16e-06,
      "loss": 0.001,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.11988319456577301,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.001,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.13789533078670502,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0007,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.16979369521141052,
      "learning_rate": 4.12e-06,
      "loss": 0.0005,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.0,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0006,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.31030964851379395,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0009,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.16347430646419525,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0005,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.2592688798904419,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0006,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.18778908252716064,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0002,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.08139680325984955,
      "learning_rate": 4.04e-06,
      "loss": 0.001,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.08663920313119888,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0009,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.14838315546512604,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0012,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.18287216126918793,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0004,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0006,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.10743797570466995,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0008,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.07133473455905914,
      "learning_rate": 3.96e-06,
      "loss": 0.001,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.1759205311536789,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0012,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.1389980912208557,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0012,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.10168000310659409,
      "learning_rate": 3.92e-06,
      "loss": 0.0008,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.0,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0006,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.0,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0007,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.0,
      "learning_rate": 3.88e-06,
      "loss": 0.0006,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.16458705067634583,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0007,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.06732633709907532,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0009,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.0803142860531807,
      "learning_rate": 3.84e-06,
      "loss": 0.0006,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0013,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.17563632130622864,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0012,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.08274827897548676,
      "learning_rate": 3.8e-06,
      "loss": 0.001,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.10831209272146225,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.001,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.166050523519516,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0007,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.09704520553350449,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0007,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0012,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.1801663488149643,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0009,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.10494464635848999,
      "learning_rate": 3.72e-06,
      "loss": 0.0013,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.18151241540908813,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0008,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.2994205355644226,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0007,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.09322452545166016,
      "learning_rate": 3.68e-06,
      "loss": 0.0009,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.0,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0005,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.22607111930847168,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0006,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.10266626626253128,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.0006,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.10832010209560394,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0015,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.08457046747207642,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0007,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.06467551738023758,
      "learning_rate": 3.6e-06,
      "loss": 0.0005,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.0,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0006,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.07878809422254562,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.001,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.09946786612272263,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0008,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.10712703317403793,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0015,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.15135221183300018,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0005,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.0713101178407669,
      "learning_rate": 3.52e-06,
      "loss": 0.0008,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.0,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0011,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.0,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.0005,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.09775488823652267,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0011,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0009,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.08049418777227402,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0016,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.0,
      "learning_rate": 3.44e-06,
      "loss": 0.0007,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.14673662185668945,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0007,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.08614255487918854,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0007,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.12747079133987427,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0011,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.11550126224756241,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0008,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.15911145508289337,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0005,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.08121929317712784,
      "learning_rate": 3.36e-06,
      "loss": 0.0008,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.12276019155979156,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0014,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.09559665620326996,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.001,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.16124853491783142,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0011,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.0,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0003,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.07319857180118561,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0008,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.18064434826374054,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0007,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.19081951677799225,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0012,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.06448573619127274,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.0011,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.08802738785743713,
      "learning_rate": 3.24e-06,
      "loss": 0.0007,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.0,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0004,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.12465453892946243,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0006,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.1219632625579834,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0007,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.13262438774108887,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.001,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.08209379762411118,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0008,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.045337773859500885,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.0008,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.15875393152236938,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0008,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.24850238859653473,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0007,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.12084432691335678,
      "learning_rate": 3.12e-06,
      "loss": 0.0012,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.07512637227773666,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0005,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.09089510887861252,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0002,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.14499931037425995,
      "learning_rate": 3.08e-06,
      "loss": 0.0019,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.09738977998495102,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0009,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.2201530933380127,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0009,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.0,
      "learning_rate": 3.04e-06,
      "loss": 0.0005,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.0864085778594017,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0012,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.15293419361114502,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0016,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.0,
      "learning_rate": 3e-06,
      "loss": 0.0009,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.1020052507519722,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0008,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.08285827934741974,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0015,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.1241442859172821,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.0006,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0004,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.14823733270168304,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0009,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.03196265175938606,
      "learning_rate": 2.92e-06,
      "loss": 0.0009,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.18833871185779572,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0007,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.08217089623212814,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0007,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.10605965554714203,
      "learning_rate": 2.88e-06,
      "loss": 0.0007,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.18838749825954437,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0008,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.18909819424152374,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0008,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.40229710936546326,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0011,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.07322722673416138,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0004,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.19276319444179535,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.001,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.10044273734092712,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0005,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.0005,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.10064448416233063,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0001,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.11815982311964035,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0009,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.0,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0007,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.16822010278701782,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0006,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.1049949899315834,
      "learning_rate": 2.72e-06,
      "loss": 0.0007,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.08802174031734467,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0009,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.14095501601696014,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0009,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.11964433640241623,
      "learning_rate": 2.68e-06,
      "loss": 0.0007,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.2049930989742279,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0015,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.1478426456451416,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0004,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.09000474214553833,
      "learning_rate": 2.64e-06,
      "loss": 0.0007,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.0899437963962555,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0011,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0005,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.09913161396980286,
      "learning_rate": 2.6e-06,
      "loss": 0.0004,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.1281946748495102,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.0007,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.19009822607040405,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0005,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.24299676716327667,
      "learning_rate": 2.56e-06,
      "loss": 0.0012,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.09889733046293259,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0006,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0007,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.08517450839281082,
      "learning_rate": 2.52e-06,
      "loss": 0.0008,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.12091302871704102,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0008,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.0,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0008,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.11373188346624374,
      "learning_rate": 2.48e-06,
      "loss": 0.0006,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.001,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.18667897582054138,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0011,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.0,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0007,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.16408056020736694,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0012,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.08625853061676025,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0009,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.0,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0002,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.0,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0007,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.33902186155319214,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.001,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.19910946488380432,
      "learning_rate": 2.36e-06,
      "loss": 0.0006,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.08292206376791,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0012,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.12078368663787842,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0004,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.1144443079829216,
      "learning_rate": 2.32e-06,
      "loss": 0.0007,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.059834904968738556,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.0015,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.08283426612615585,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0003,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.10783901065587997,
      "learning_rate": 2.28e-06,
      "loss": 0.0005,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.0,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0006,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.13880136609077454,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0008,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.0,
      "learning_rate": 2.24e-06,
      "loss": 0.0013,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.07292479276657104,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.001,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.0,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.001,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.08947379887104034,
      "learning_rate": 2.2e-06,
      "loss": 0.0008,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.08427037298679352,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0005,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.19158941507339478,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.001,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.15790043771266937,
      "learning_rate": 2.16e-06,
      "loss": 0.0011,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.22354356944561005,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.001,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.10895795375108719,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0008,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.0,
      "learning_rate": 2.12e-06,
      "loss": 0.0013,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.10873491317033768,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0005,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.23702095448970795,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0006,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.08501525223255157,
      "learning_rate": 2.08e-06,
      "loss": 0.0006,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.09165363013744354,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0004,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.08880772441625595,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.0005,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.06711161136627197,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0002,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.05756637454032898,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.0006,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.0,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0006,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.13283909857273102,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0009,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.0011,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.11897556483745575,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0003,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.07227741181850433,
      "learning_rate": 1.96e-06,
      "loss": 0.0006,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.06942565739154816,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0006,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0005,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.0,
      "learning_rate": 1.92e-06,
      "loss": 0.0014,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.19150692224502563,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0016,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.12217695266008377,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0006,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.09562765806913376,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0007,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.09024070203304291,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0006,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.250996470451355,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0006,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.0,
      "learning_rate": 1.84e-06,
      "loss": 0.0008,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.17294354736804962,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0007,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.10820268094539642,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.0007,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.1420217752456665,
      "learning_rate": 1.8e-06,
      "loss": 0.0012,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0008,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.0,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0007,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.06012856587767601,
      "learning_rate": 1.76e-06,
      "loss": 0.0005,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.0,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0006,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.1164313480257988,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0008,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.11270761489868164,
      "learning_rate": 1.72e-06,
      "loss": 0.0003,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.09134136885404587,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0009,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.09489306062459946,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0008,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.0,
      "learning_rate": 1.68e-06,
      "loss": 0.0006,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.08916054666042328,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0002,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.15403498709201813,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.001,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.0,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0006,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.1528291255235672,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.0017,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.12915724515914917,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0009,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.08917451649904251,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0003,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0007,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.09370339661836624,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0008,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.0,
      "learning_rate": 1.56e-06,
      "loss": 0.001,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.15389281511306763,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.001,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.07622305303812027,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0007,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.12540127336978912,
      "learning_rate": 1.52e-06,
      "loss": 0.0005,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.0,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.001,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.24611347913742065,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0001,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.10799039900302887,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0013,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.11517537385225296,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0005,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.0,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.0002,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.06787577271461487,
      "learning_rate": 1.44e-06,
      "loss": 0.001,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.06473805010318756,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.001,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.08640006184577942,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0006,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.0,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0003,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.0,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.0007,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.08130554109811783,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0005,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.09261564165353775,
      "learning_rate": 1.36e-06,
      "loss": 0.0008,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.08848915994167328,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0007,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.0,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.001,
      "step": 36500
    }
  ],
  "logging_steps": 10,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

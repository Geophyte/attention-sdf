{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9733333333333334,
  "eval_steps": 500,
  "global_step": 37000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 0.16228075325489044,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0022,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.6336237788200378,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0034,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.6088259816169739,
      "learning_rate": 4.996e-05,
      "loss": 0.0033,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.19537419080734253,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0027,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.23564021289348602,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0024,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.8731986880302429,
      "learning_rate": 4.992e-05,
      "loss": 0.0026,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.8727937340736389,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0024,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.7255638241767883,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0033,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.9163646697998047,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0036,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.7639572620391846,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0037,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.07903634756803513,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0042,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.6756222248077393,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0033,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.20982569456100464,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.0029,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.9384710192680359,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0037,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 1.0341664552688599,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0051,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 1.2200685739517212,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0036,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.5492845177650452,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.0049,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.45108476281166077,
      "learning_rate": 4.976e-05,
      "loss": 0.0029,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.37776851654052734,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0037,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.5586029291152954,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0032,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.6128696203231812,
      "learning_rate": 4.972e-05,
      "loss": 0.0036,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.9107626676559448,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0041,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.27704912424087524,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0029,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.4862616956233978,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.0046,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.7087898850440979,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0035,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.6377111077308655,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.0038,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 1.0618762969970703,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0044,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.1465086191892624,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0041,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.6117877960205078,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0039,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.26565974950790405,
      "learning_rate": 4.96e-05,
      "loss": 0.0029,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.22181712090969086,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0028,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.5096977949142456,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.003,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.7050331234931946,
      "learning_rate": 4.956e-05,
      "loss": 0.0034,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 1.0364630222320557,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0025,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.15388518571853638,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0029,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.30955740809440613,
      "learning_rate": 4.952e-05,
      "loss": 0.0034,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.5024361610412598,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.003,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.908814013004303,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0029,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.9847410321235657,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0046,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.18534018099308014,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0027,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.7316298484802246,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0034,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 1.2257212400436401,
      "learning_rate": 4.944e-05,
      "loss": 0.0034,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.13785572350025177,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0033,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.43052199482917786,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0035,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.2491564750671387,
      "learning_rate": 4.94e-05,
      "loss": 0.0042,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.5794606804847717,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0038,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.40543413162231445,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0028,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.7922031283378601,
      "learning_rate": 4.936e-05,
      "loss": 0.0051,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 1.1779485940933228,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0031,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.1360313892364502,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0034,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 1.3804184198379517,
      "learning_rate": 4.932e-05,
      "loss": 0.0035,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.11931031942367554,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0044,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.4491283893585205,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.004,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.9139630794525146,
      "learning_rate": 4.928e-05,
      "loss": 0.0041,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.7011880278587341,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0039,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.10297747701406479,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0047,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.465444415807724,
      "learning_rate": 4.924e-05,
      "loss": 0.0043,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.2878667712211609,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0045,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.5405372977256775,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0031,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.2106880098581314,
      "learning_rate": 4.92e-05,
      "loss": 0.0045,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.7237352132797241,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.0044,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.334239661693573,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0029,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.376579225063324,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0026,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.30766984820365906,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0032,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.8731878399848938,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0025,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.666671097278595,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.0026,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.2513681948184967,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.0038,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.3321424722671509,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.0036,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.6013772487640381,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.004,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.23430235683918,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.004,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.09263621270656586,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0029,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.10756596177816391,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.0036,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.55081707239151,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0035,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.5877659320831299,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0036,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.7277271747589111,
      "learning_rate": 4.9e-05,
      "loss": 0.0037,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.15193240344524384,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0025,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.7485698461532593,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0028,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.712946355342865,
      "learning_rate": 4.896e-05,
      "loss": 0.0039,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.4914931356906891,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.003,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.2281665951013565,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0039,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.17701902985572815,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.0041,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.7270632982254028,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.0028,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.5090845227241516,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0019,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.41633832454681396,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0026,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.49514731764793396,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0027,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.3172515630722046,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0035,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.6454076170921326,
      "learning_rate": 4.884e-05,
      "loss": 0.0026,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.2026507705450058,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0035,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.2631634473800659,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0031,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.28357502818107605,
      "learning_rate": 4.88e-05,
      "loss": 0.0037,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.561821699142456,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0026,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.2007886916399002,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0024,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.37870508432388306,
      "learning_rate": 4.876e-05,
      "loss": 0.003,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.7777050733566284,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0032,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.9121527075767517,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0037,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.14204861223697662,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.0029,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.5390146970748901,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0029,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.22321027517318726,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0022,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.1583450436592102,
      "learning_rate": 4.868e-05,
      "loss": 0.0023,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.13020646572113037,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.003,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.7367950081825256,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.0038,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.5482726693153381,
      "learning_rate": 4.864e-05,
      "loss": 0.0019,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.2925189733505249,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0032,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.20318078994750977,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.0024,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.7605319619178772,
      "learning_rate": 4.86e-05,
      "loss": 0.0029,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.1323423832654953,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0031,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.2501012682914734,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0032,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.631406843662262,
      "learning_rate": 4.856e-05,
      "loss": 0.003,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.46547532081604004,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0028,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.27939796447753906,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0021,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.6058846712112427,
      "learning_rate": 4.852e-05,
      "loss": 0.0026,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.574813961982727,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0024,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.5192779898643494,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0029,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.7692464590072632,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0031,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.5931359529495239,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.003,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.10198111832141876,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0028,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.7803165912628174,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0032,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.13978153467178345,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0027,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.5099263191223145,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0027,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.7493675947189331,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0034,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.6079575419425964,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0026,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.30088868737220764,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0026,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.8425427675247192,
      "learning_rate": 4.836e-05,
      "loss": 0.0036,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.2189842313528061,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.0032,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.6088135242462158,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0032,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.09374941885471344,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0025,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.7287036776542664,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0028,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 1.0784815549850464,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0032,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.4558509886264801,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0023,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.15770907700061798,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0035,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.32390230894088745,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0023,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.1926095187664032,
      "learning_rate": 4.824e-05,
      "loss": 0.0024,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.2944246828556061,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0029,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.10765726864337921,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0027,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.23406554758548737,
      "learning_rate": 4.82e-05,
      "loss": 0.0029,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.5224606394767761,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.0023,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.5227038860321045,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0038,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.3562995493412018,
      "learning_rate": 4.816e-05,
      "loss": 0.0052,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.6404721140861511,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0046,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.8148260712623596,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0033,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.3735455572605133,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0038,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.27163469791412354,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0036,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.36177536845207214,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0034,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.35303017497062683,
      "learning_rate": 4.808e-05,
      "loss": 0.0026,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.5441962480545044,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0028,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.34666338562965393,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0035,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.42743590474128723,
      "learning_rate": 4.804e-05,
      "loss": 0.0031,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.15781906247138977,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0026,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.2788410782814026,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0026,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.31177371740341187,
      "learning_rate": 4.8e-05,
      "loss": 0.0028,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.3959556519985199,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.0034,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.44540733098983765,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0042,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.3507002294063568,
      "learning_rate": 4.796e-05,
      "loss": 0.0027,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.40636417269706726,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0031,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.4849030673503876,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0032,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.16386519372463226,
      "learning_rate": 4.792e-05,
      "loss": 0.0034,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 0.27397072315216064,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0036,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.4945908486843109,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0023,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.6450510025024414,
      "learning_rate": 4.788e-05,
      "loss": 0.0032,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.45080330967903137,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0024,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.08404189348220825,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0037,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.8105529546737671,
      "learning_rate": 4.784e-05,
      "loss": 0.0026,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.13389958441257477,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0017,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.24492985010147095,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0031,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.6307096481323242,
      "learning_rate": 4.78e-05,
      "loss": 0.0027,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.37596169114112854,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0029,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.7363422513008118,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.003,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.2617837190628052,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0034,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.37762346863746643,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0023,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.2706087827682495,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0031,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.5796851515769958,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0023,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.4966621994972229,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0025,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.13335539400577545,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.0022,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.17744940519332886,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0025,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.5117694139480591,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0036,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.23955166339874268,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0021,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.4392443597316742,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.003,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.18758252263069153,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0032,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.23483116924762726,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0028,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.15100592374801636,
      "learning_rate": 4.76e-05,
      "loss": 0.0025,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.08251143991947174,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0023,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.9056216478347778,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0031,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.25214242935180664,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.003,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.40960395336151123,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.0028,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.09686976671218872,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0039,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.2820039689540863,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0031,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.5411835312843323,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0024,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.32887277007102966,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0023,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.10251449793577194,
      "learning_rate": 4.748e-05,
      "loss": 0.0021,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.36120885610580444,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.003,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.48246338963508606,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.0024,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.4919487535953522,
      "learning_rate": 4.744e-05,
      "loss": 0.0033,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.1474219709634781,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0035,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.5537270903587341,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0029,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.6384204030036926,
      "learning_rate": 4.74e-05,
      "loss": 0.0042,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.17361514270305634,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.0023,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 1.2521008253097534,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0023,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.888714611530304,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0021,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.9978775382041931,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0029,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.5883423089981079,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0028,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.5098579525947571,
      "learning_rate": 4.732e-05,
      "loss": 0.0034,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.10266075283288956,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0028,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.12832869589328766,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0028,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.30891844630241394,
      "learning_rate": 4.728e-05,
      "loss": 0.0026,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.2801200747489929,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0028,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.200412780046463,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0019,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.8039039373397827,
      "learning_rate": 4.724e-05,
      "loss": 0.0028,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.19501903653144836,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0025,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.4795837104320526,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0026,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.1308974027633667,
      "learning_rate": 4.72e-05,
      "loss": 0.0028,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.46487897634506226,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0025,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.4308378994464874,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.0023,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.3202787935733795,
      "learning_rate": 4.716e-05,
      "loss": 0.0031,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.37897202372550964,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0035,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.4081994891166687,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0038,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.17641934752464294,
      "learning_rate": 4.712e-05,
      "loss": 0.0019,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.31590181589126587,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0022,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.3730185925960541,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.002,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.6574583053588867,
      "learning_rate": 4.708e-05,
      "loss": 0.0026,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.5280961990356445,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0028,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.4654904901981354,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0038,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.12442981451749802,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.0037,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.17796199023723602,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.003,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.6965906620025635,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.002,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.4699860215187073,
      "learning_rate": 4.7e-05,
      "loss": 0.0036,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.8182145953178406,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.003,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.49223393201828003,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.0022,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.1338696926832199,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0031,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.5948728919029236,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.0025,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.2793181836605072,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0024,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.10732356458902359,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0028,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.2140362411737442,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0023,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.5804224014282227,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0022,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.18601126968860626,
      "learning_rate": 4.688e-05,
      "loss": 0.002,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.31739291548728943,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0025,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.32097434997558594,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.0026,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.31505855917930603,
      "learning_rate": 4.684e-05,
      "loss": 0.0026,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.861988365650177,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0025,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.2229277342557907,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0025,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5032564401626587,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.003,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.10525139421224594,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0023,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.5139890313148499,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0026,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.3810097575187683,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0037,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.33477795124053955,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0026,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.3205893933773041,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0023,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.2028372883796692,
      "learning_rate": 4.672e-05,
      "loss": 0.0024,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.37655460834503174,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.003,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.564168393611908,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0024,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.38896724581718445,
      "learning_rate": 4.668e-05,
      "loss": 0.0031,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.4421233832836151,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0025,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.48931974172592163,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0031,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.673157274723053,
      "learning_rate": 4.664e-05,
      "loss": 0.003,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.4778578579425812,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0033,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.14580687880516052,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0028,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.3557685315608978,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0027,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.27507203817367554,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0027,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.31926560401916504,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0026,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.2557083070278168,
      "learning_rate": 4.656e-05,
      "loss": 0.0028,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.2686769962310791,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0026,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.5732027292251587,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0022,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.14066362380981445,
      "learning_rate": 4.652e-05,
      "loss": 0.0027,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.42380180954933167,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0029,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.20815345644950867,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.003,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.3495723307132721,
      "learning_rate": 4.648e-05,
      "loss": 0.003,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.26621291041374207,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0027,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.24527935683727264,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.003,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.2069244086742401,
      "learning_rate": 4.644e-05,
      "loss": 0.0026,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.3949367105960846,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0029,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.39268773794174194,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.002,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.18725746870040894,
      "learning_rate": 4.64e-05,
      "loss": 0.0032,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.5559359192848206,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0032,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.18671514093875885,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0024,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.354667067527771,
      "learning_rate": 4.636e-05,
      "loss": 0.0018,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.5073001980781555,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0022,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.47962313890457153,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0023,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.16460363566875458,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0031,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.638954758644104,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.0022,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.2015567421913147,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0027,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.3931092917919159,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0022,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 1.1305341720581055,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0034,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.5723303556442261,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0031,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.7465258836746216,
      "learning_rate": 4.624e-05,
      "loss": 0.0029,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.26327139139175415,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.0026,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.1923564076423645,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.0027,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.35724619030952454,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0023,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.5682566165924072,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0029,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.5119673013687134,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.003,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.4045066833496094,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0028,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.10904666781425476,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.002,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.3256993591785431,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0024,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.39275094866752625,
      "learning_rate": 4.612e-05,
      "loss": 0.0021,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.2668552100658417,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0023,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.10621379315853119,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.002,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.1418682485818863,
      "learning_rate": 4.608e-05,
      "loss": 0.0024,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.36581599712371826,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.002,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.12177416682243347,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0027,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.5130150318145752,
      "learning_rate": 4.604e-05,
      "loss": 0.0028,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.6249338388442993,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0023,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.21828532218933105,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0027,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.08267711848020554,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0021,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.1524174064397812,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0028,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.21323026716709137,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0033,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.63323575258255,
      "learning_rate": 4.596e-05,
      "loss": 0.0044,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.4333871901035309,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0034,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.3994736671447754,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0022,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.39446449279785156,
      "learning_rate": 4.592e-05,
      "loss": 0.0026,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.1629544496536255,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.003,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.8301949501037598,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0033,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.8613213896751404,
      "learning_rate": 4.588e-05,
      "loss": 0.0027,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.5979753732681274,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0025,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.324457585811615,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0033,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.36775922775268555,
      "learning_rate": 4.584e-05,
      "loss": 0.003,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.393898606300354,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.002,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.13069994747638702,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0026,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.304596871137619,
      "learning_rate": 4.58e-05,
      "loss": 0.0028,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.22397851943969727,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0027,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.47356685996055603,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0026,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.7966552376747131,
      "learning_rate": 4.576e-05,
      "loss": 0.0022,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.9540399312973022,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0031,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.6640323400497437,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0023,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.37395453453063965,
      "learning_rate": 4.572e-05,
      "loss": 0.0027,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.3334890902042389,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0025,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.5622698664665222,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0031,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.31334230303764343,
      "learning_rate": 4.568e-05,
      "loss": 0.0032,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.14062710106372833,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0029,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.5796827673912048,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.0027,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.40946251153945923,
      "learning_rate": 4.564e-05,
      "loss": 0.0025,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.10607178509235382,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0027,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.13091109693050385,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0021,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.6419046521186829,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.003,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.29967740178108215,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0023,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.8208682537078857,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0031,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.41536521911621094,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0021,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.27639979124069214,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0035,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.23994918167591095,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0026,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.12214840203523636,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0026,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.15512871742248535,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0027,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.7639144659042358,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0027,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.33693158626556396,
      "learning_rate": 4.548e-05,
      "loss": 0.003,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.16105790436267853,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0027,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.8202841877937317,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0024,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.17126363515853882,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0029,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.4937652349472046,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0025,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.122304767370224,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0029,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.09573496133089066,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0022,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.35814401507377625,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0036,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.17173856496810913,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.0029,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.13896313309669495,
      "learning_rate": 4.536e-05,
      "loss": 0.0019,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.3281806707382202,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0025,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.47032099962234497,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0031,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.1878330558538437,
      "learning_rate": 4.532e-05,
      "loss": 0.0026,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.21170946955680847,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0026,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.725851833820343,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0025,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.5984870195388794,
      "learning_rate": 4.528e-05,
      "loss": 0.0029,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.11777708679437637,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0028,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.42267394065856934,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0024,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.5083007216453552,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0022,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.1583721935749054,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0027,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.08708526939153671,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0027,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.7493064403533936,
      "learning_rate": 4.52e-05,
      "loss": 0.0026,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.3849208652973175,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.003,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.20002694427967072,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.0021,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.7902815341949463,
      "learning_rate": 4.516e-05,
      "loss": 0.0028,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.39641469717025757,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0027,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.49767202138900757,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0027,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.08324847370386124,
      "learning_rate": 4.512e-05,
      "loss": 0.0027,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.21269194781780243,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.0027,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.6150638461112976,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0036,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.4223891496658325,
      "learning_rate": 4.508e-05,
      "loss": 0.0026,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 1.113584041595459,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0025,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.38983598351478577,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0025,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.615909218788147,
      "learning_rate": 4.504e-05,
      "loss": 0.0029,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.6027609705924988,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0023,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.6205601096153259,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.0028,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.19233641028404236,
      "learning_rate": 4.5e-05,
      "loss": 0.0028,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.897445559501648,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0027,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.8151419162750244,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0032,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.6646756529808044,
      "learning_rate": 4.496e-05,
      "loss": 0.0028,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.898461639881134,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0035,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.2119522988796234,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.003,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.3674832582473755,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0029,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.7559965252876282,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0025,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.3721245229244232,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0029,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.5261216163635254,
      "learning_rate": 4.488e-05,
      "loss": 0.0024,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.13094337284564972,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0029,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.28423210978507996,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.0034,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.7563912868499756,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0025,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.21293863654136658,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0024,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.4112027883529663,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0024,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.4745129644870758,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0036,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.2520627975463867,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.003,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.1554802656173706,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0026,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.5720181465148926,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0036,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.8105778098106384,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0024,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.4618730843067169,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0026,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.4993413984775543,
      "learning_rate": 4.472e-05,
      "loss": 0.0031,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.5831425189971924,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.0024,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.7055626511573792,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0031,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.288421630859375,
      "learning_rate": 4.468e-05,
      "loss": 0.0029,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.3343777656555176,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.003,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.48648571968078613,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0033,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.2071574330329895,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.0026,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.10412970930337906,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.0029,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.8244183659553528,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0031,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.5086327791213989,
      "learning_rate": 4.46e-05,
      "loss": 0.0031,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.19557219743728638,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0033,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.2726418077945709,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.003,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.8835409283638,
      "learning_rate": 4.456e-05,
      "loss": 0.0025,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.7992144823074341,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.003,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.2945936918258667,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0023,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.26132267713546753,
      "learning_rate": 4.452e-05,
      "loss": 0.0034,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.7001864910125732,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0022,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.308078795671463,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.0031,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.43626540899276733,
      "learning_rate": 4.448e-05,
      "loss": 0.0037,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.35537999868392944,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0036,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.5338316559791565,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0022,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.3390375077724457,
      "learning_rate": 4.444e-05,
      "loss": 0.0024,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.166920468211174,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.0031,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.548265814781189,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0035,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.16672827303409576,
      "learning_rate": 4.44e-05,
      "loss": 0.0033,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.09044698625802994,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.0034,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.5901986956596375,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0026,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.4803468883037567,
      "learning_rate": 4.436e-05,
      "loss": 0.0031,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.7881708741188049,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.003,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.19576424360275269,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0028,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.16115839779376984,
      "learning_rate": 4.432e-05,
      "loss": 0.003,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 1.0674943923950195,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0023,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.7969151735305786,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0025,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.19650320708751678,
      "learning_rate": 4.428e-05,
      "loss": 0.0031,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.25836697220802307,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0024,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.11689932644367218,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0023,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.5997998118400574,
      "learning_rate": 4.424e-05,
      "loss": 0.0028,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.1481359899044037,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0026,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.5162442922592163,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.003,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.5163375735282898,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0028,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.7490279078483582,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.0023,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.2939715087413788,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0027,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.5843685269355774,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0024,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.9026954770088196,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0026,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.14811411499977112,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0025,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.0870039090514183,
      "learning_rate": 4.412e-05,
      "loss": 0.0034,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.2231115847826004,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.0026,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.17460840940475464,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.0024,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.10157555341720581,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0019,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.8542423844337463,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0019,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.29051825404167175,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0032,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.6335978507995605,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0032,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.5751587748527527,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0031,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.821513295173645,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.002,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.0058016777038574,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0025,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.8211973309516907,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0021,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.2696197032928467,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0026,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.4604376554489136,
      "learning_rate": 4.396e-05,
      "loss": 0.0024,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.7438417673110962,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0024,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.5561848282814026,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0032,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.15127533674240112,
      "learning_rate": 4.392e-05,
      "loss": 0.002,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.5932006239891052,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0024,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.6637548208236694,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0026,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.849528431892395,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0027,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.11311540752649307,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0033,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.2424887865781784,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0027,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.5577514171600342,
      "learning_rate": 4.384e-05,
      "loss": 0.0037,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.3841931223869324,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0029,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.1254589855670929,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.002,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.35565823316574097,
      "learning_rate": 4.38e-05,
      "loss": 0.0023,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.11223465204238892,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0025,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.3542836904525757,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0025,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.12629348039627075,
      "learning_rate": 4.376e-05,
      "loss": 0.0026,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.5283936858177185,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0028,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.25304096937179565,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0028,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.539560854434967,
      "learning_rate": 4.372e-05,
      "loss": 0.0032,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.5326464176177979,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0029,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.310172438621521,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0023,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.48867854475975037,
      "learning_rate": 4.368e-05,
      "loss": 0.0021,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.1944936215877533,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0026,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.2587008774280548,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0023,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.20892426371574402,
      "learning_rate": 4.364e-05,
      "loss": 0.0023,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.6308549046516418,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0019,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.2625614404678345,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0023,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 1.126777172088623,
      "learning_rate": 4.36e-05,
      "loss": 0.003,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.5507404208183289,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0019,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.624648928642273,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0022,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.08166016638278961,
      "learning_rate": 4.356e-05,
      "loss": 0.0029,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.29906928539276123,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0022,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.6316792964935303,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0023,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.7091938257217407,
      "learning_rate": 4.352e-05,
      "loss": 0.0028,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.5511513948440552,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0023,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.42021647095680237,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0022,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.8115218877792358,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0023,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.5724450349807739,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0027,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.24734796583652496,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0028,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.604868471622467,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0029,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.13974887132644653,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0025,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.22847090661525726,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0028,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.6230884194374084,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0027,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.13376106321811676,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0032,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.1597122848033905,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.0036,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.10961562395095825,
      "learning_rate": 4.336e-05,
      "loss": 0.0027,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.3748272657394409,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.002,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.7102338075637817,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0023,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.7972278594970703,
      "learning_rate": 4.332e-05,
      "loss": 0.0024,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.4624832570552826,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.003,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.6285310983657837,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.0047,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.16364726424217224,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.0029,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.4980933666229248,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.003,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.17818833887577057,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0024,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.3689863681793213,
      "learning_rate": 4.324e-05,
      "loss": 0.0029,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.30652952194213867,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0023,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.23760424554347992,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0027,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.28161898255348206,
      "learning_rate": 4.32e-05,
      "loss": 0.0027,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.7255752086639404,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0029,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.4541652202606201,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0029,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.5446967482566833,
      "learning_rate": 4.316e-05,
      "loss": 0.0027,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.2804705500602722,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0031,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.6545495986938477,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0026,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.42760902643203735,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0022,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.42665430903434753,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0022,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.1438542902469635,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0026,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.3040829300880432,
      "learning_rate": 4.308e-05,
      "loss": 0.0022,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.3941880166530609,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0026,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.4340587556362152,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0025,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.43555957078933716,
      "learning_rate": 4.304e-05,
      "loss": 0.0021,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.20236901938915253,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0022,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.11873986572027206,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0017,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.26507511734962463,
      "learning_rate": 4.3e-05,
      "loss": 0.0024,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.14701826870441437,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.0024,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.10640472918748856,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0026,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.3950071930885315,
      "learning_rate": 4.296e-05,
      "loss": 0.0028,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.8223202228546143,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0019,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.6215930581092834,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0027,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.22648656368255615,
      "learning_rate": 4.292e-05,
      "loss": 0.0028,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.32872506976127625,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0024,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.15517674386501312,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0029,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.3464546203613281,
      "learning_rate": 4.288e-05,
      "loss": 0.0038,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.0831143781542778,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0025,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.3657152056694031,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0026,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.49687841534614563,
      "learning_rate": 4.284e-05,
      "loss": 0.0027,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.25864177942276,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0024,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.39073315262794495,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0027,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.9923192262649536,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0033,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.22599907219409943,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0037,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.6321772933006287,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0031,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.24704937636852264,
      "learning_rate": 4.276e-05,
      "loss": 0.0026,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.2050207257270813,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0034,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.44789689779281616,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0046,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.13774539530277252,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0025,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.600657045841217,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.003,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.9485122561454773,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0028,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.6086698770523071,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0035,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.760888397693634,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0028,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.5067268013954163,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0024,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.19112235307693481,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.0021,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.7855547070503235,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0028,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.333156555891037,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0033,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.0952233299612999,
      "learning_rate": 4.26e-05,
      "loss": 0.002,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.09365792572498322,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0022,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.48106881976127625,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0045,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.4220721125602722,
      "learning_rate": 4.256e-05,
      "loss": 0.0031,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.32918062806129456,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0026,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.7635456323623657,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0029,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 1.0661948919296265,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0022,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.625129759311676,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0032,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.5318737030029297,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0029,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.4944673478603363,
      "learning_rate": 4.248e-05,
      "loss": 0.0025,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.7211872935295105,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0021,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 1.1524981260299683,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.0029,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.15510715544223785,
      "learning_rate": 4.244e-05,
      "loss": 0.0026,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.29846876859664917,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0022,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.26644444465637207,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0026,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.06417098641395569,
      "learning_rate": 4.24e-05,
      "loss": 0.0031,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.5560386180877686,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.003,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 0.26958543062210083,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0025,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.55862957239151,
      "learning_rate": 4.236e-05,
      "loss": 0.0023,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.2605457007884979,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0021,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.23912620544433594,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0027,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.9147278666496277,
      "learning_rate": 4.232e-05,
      "loss": 0.0023,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.4992517828941345,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0022,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.5430758595466614,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0024,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.7688999176025391,
      "learning_rate": 4.228e-05,
      "loss": 0.0027,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.7760552167892456,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0032,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.30459287762641907,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0033,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.28206712007522583,
      "learning_rate": 4.224e-05,
      "loss": 0.0029,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.1603274643421173,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0025,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.14701591432094574,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.003,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.22973456978797913,
      "learning_rate": 4.22e-05,
      "loss": 0.0027,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.32362663745880127,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0027,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.2417345643043518,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0019,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.2953108251094818,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0019,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.2976391911506653,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0022,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.5527003407478333,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0025,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.19848725199699402,
      "learning_rate": 4.212e-05,
      "loss": 0.0029,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.6290238499641418,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0021,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.14532017707824707,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0027,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.24548596143722534,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.0032,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.6980917453765869,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0027,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.19919779896736145,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.002,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.32103431224823,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0022,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.25286009907722473,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.0022,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.5368257761001587,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0028,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.10803712159395218,
      "learning_rate": 4.2e-05,
      "loss": 0.0024,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.8898023366928101,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0019,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.43617838621139526,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0029,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.6540278196334839,
      "learning_rate": 4.196e-05,
      "loss": 0.0022,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.4763270914554596,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0028,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.12232164293527603,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0023,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.21764761209487915,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0026,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.41185277700424194,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0022,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.086476169526577,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0025,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.4069640338420868,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0022,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.41365018486976624,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0032,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.5448378920555115,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0035,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.3257533013820648,
      "learning_rate": 4.184e-05,
      "loss": 0.0021,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.3828177750110626,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0025,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.4584200978279114,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0021,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.31328684091567993,
      "learning_rate": 4.18e-05,
      "loss": 0.0026,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.24155664443969727,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0025,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.49147507548332214,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0021,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.33244335651397705,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.0023,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.6301300525665283,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0028,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.4117222726345062,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0029,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.21679440140724182,
      "learning_rate": 4.172e-05,
      "loss": 0.003,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.7747967839241028,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0026,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.6174554228782654,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0028,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.8783817291259766,
      "learning_rate": 4.168e-05,
      "loss": 0.0028,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.15550954639911652,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0019,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.3999587595462799,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0028,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.3790316581726074,
      "learning_rate": 4.164e-05,
      "loss": 0.0034,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.1792028844356537,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0021,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.37210145592689514,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0024,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.5245617628097534,
      "learning_rate": 4.16e-05,
      "loss": 0.0023,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.5016447901725769,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0031,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.9186266660690308,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0031,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 1.0206749439239502,
      "learning_rate": 4.156e-05,
      "loss": 0.0033,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.7011423707008362,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0022,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.3840945363044739,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0026,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.3405347764492035,
      "learning_rate": 4.152e-05,
      "loss": 0.0027,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.7199695706367493,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0024,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 1.2448949813842773,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0036,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.3000970780849457,
      "learning_rate": 4.148e-05,
      "loss": 0.0029,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.21278558671474457,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0032,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.163404643535614,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.0026,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.495797723531723,
      "learning_rate": 4.144e-05,
      "loss": 0.0026,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.17195555567741394,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0028,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.7192797660827637,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0022,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.22300711274147034,
      "learning_rate": 4.14e-05,
      "loss": 0.002,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.5762017965316772,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0024,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.10822590440511703,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0024,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.3905375003814697,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0031,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.3845435678958893,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.0032,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.25320562720298767,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0028,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.5654251575469971,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.0025,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.16906476020812988,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0028,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.47757488489151,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.0028,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.7350267767906189,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0023,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.7021167278289795,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0034,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.11279600113630295,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0024,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.6834073662757874,
      "learning_rate": 4.124e-05,
      "loss": 0.0023,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.1529751718044281,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0033,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.8591165542602539,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.0024,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.601525068283081,
      "learning_rate": 4.12e-05,
      "loss": 0.0025,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.7705175280570984,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.0031,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 1.4033647775650024,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.0033,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.15953445434570312,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0031,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.22671744227409363,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.003,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.08072225004434586,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0024,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.5367351770401001,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0024,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.25437429547309875,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0025,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.19480711221694946,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.0034,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.095769964158535,
      "learning_rate": 4.108e-05,
      "loss": 0.0031,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.2159920483827591,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.003,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.15317252278327942,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0023,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.4499026834964752,
      "learning_rate": 4.104e-05,
      "loss": 0.0026,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.44686222076416016,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0032,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.3259638249874115,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0024,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.34350132942199707,
      "learning_rate": 4.1e-05,
      "loss": 0.0034,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.2117992341518402,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0027,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.8742280602455139,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.0016,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.21890297532081604,
      "learning_rate": 4.096e-05,
      "loss": 0.0021,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.4717634916305542,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0026,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.3168574273586273,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0028,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.6860703825950623,
      "learning_rate": 4.092e-05,
      "loss": 0.0023,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.4363194406032562,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0033,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.21649661660194397,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0025,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.296367347240448,
      "learning_rate": 4.088e-05,
      "loss": 0.0033,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.4683193862438202,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0031,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.8219646215438843,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0028,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.28336960077285767,
      "learning_rate": 4.084e-05,
      "loss": 0.0029,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.14133712649345398,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0026,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.12450254708528519,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0029,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.3997077941894531,
      "learning_rate": 4.08e-05,
      "loss": 0.0035,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.12955568730831146,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0021,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.7053068280220032,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0025,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.3173843026161194,
      "learning_rate": 4.076e-05,
      "loss": 0.0028,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.9628307819366455,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.0032,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.27735596895217896,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0029,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.22878892719745636,
      "learning_rate": 4.072e-05,
      "loss": 0.0045,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.39989203214645386,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0029,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.31327685713768005,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.0024,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.25548428297042847,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0027,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.10562986135482788,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0026,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.40665116906166077,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0025,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.14349497854709625,
      "learning_rate": 4.064e-05,
      "loss": 0.0029,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.272751122713089,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0028,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.40613096952438354,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0031,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.2373124659061432,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0028,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.6897313594818115,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.002,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.5384554862976074,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0032,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.35590431094169617,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.0024,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.3962998688220978,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0031,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.41639384627342224,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0032,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.19002856314182281,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.0027,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.27385836839675903,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0022,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.1165490373969078,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0026,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.17048171162605286,
      "learning_rate": 4.048e-05,
      "loss": 0.0025,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.06432076543569565,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0027,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.11336438357830048,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0031,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.7763426303863525,
      "learning_rate": 4.044e-05,
      "loss": 0.003,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.11155907809734344,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0034,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.4617844223976135,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0033,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.5077078342437744,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0027,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.12455732375383377,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0037,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.45450496673583984,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0024,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.4733230769634247,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.0025,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.14605547487735748,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.003,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.4711341857910156,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0028,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.30274900794029236,
      "learning_rate": 4.032e-05,
      "loss": 0.0027,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.5826767683029175,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0025,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.32847434282302856,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0028,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.17064633965492249,
      "learning_rate": 4.028e-05,
      "loss": 0.0027,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.4910958409309387,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0024,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.8367209434509277,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0031,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.30337345600128174,
      "learning_rate": 4.024e-05,
      "loss": 0.0022,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.20693765580654144,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0029,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.3303794264793396,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0023,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.5068126320838928,
      "learning_rate": 4.02e-05,
      "loss": 0.0027,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.37952274084091187,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0024,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.38216182589530945,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0031,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.15450526773929596,
      "learning_rate": 4.016e-05,
      "loss": 0.003,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.5229586362838745,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0028,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.3581080138683319,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0024,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.21266485750675201,
      "learning_rate": 4.012e-05,
      "loss": 0.0026,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.6324783563613892,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.0026,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.23267658054828644,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0032,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.13983429968357086,
      "learning_rate": 4.008e-05,
      "loss": 0.0036,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.5262097716331482,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0032,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.08457235246896744,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0021,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.087025947868824,
      "learning_rate": 4.004e-05,
      "loss": 0.0025,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.4191064238548279,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.0028,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.31298068165779114,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0026,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.17370770871639252,
      "learning_rate": 4e-05,
      "loss": 0.0037,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.3501471281051636,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0021,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.07247819006443024,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0026,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.21723471581935883,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.0024,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.6025151014328003,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0023,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.7959572076797485,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0025,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.11623503267765045,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0023,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.11251504719257355,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0024,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.27884116768836975,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0028,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.06242160126566887,
      "learning_rate": 3.988e-05,
      "loss": 0.0025,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.21362558007240295,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0021,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.2605961263179779,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0024,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.6346266865730286,
      "learning_rate": 3.984e-05,
      "loss": 0.003,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.1811048537492752,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0023,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.16901057958602905,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0034,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.10639522224664688,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0031,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.36777132749557495,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0027,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.7349234223365784,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.0022,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.21536128222942352,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0029,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.5304067730903625,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0028,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.392821729183197,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0029,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.368113249540329,
      "learning_rate": 3.972e-05,
      "loss": 0.0027,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.14810211956501007,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0029,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.11812745779752731,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0029,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.6021175384521484,
      "learning_rate": 3.968e-05,
      "loss": 0.0028,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 1.261640191078186,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0021,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.8194525241851807,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0025,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.27401208877563477,
      "learning_rate": 3.964e-05,
      "loss": 0.0028,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.37059539556503296,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.0024,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 1.1268072128295898,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0024,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.35325193405151367,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0018,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.4341951906681061,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0019,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.30519866943359375,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.002,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.4586361348628998,
      "learning_rate": 3.956e-05,
      "loss": 0.0027,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.6812667846679688,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0025,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.24960875511169434,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0026,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.7575073838233948,
      "learning_rate": 3.952e-05,
      "loss": 0.0023,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.7797759175300598,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0033,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.29484623670578003,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.003,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.3474361300468445,
      "learning_rate": 3.948e-05,
      "loss": 0.0033,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.5167973041534424,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0023,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.4351749122142792,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0023,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.10311207920312881,
      "learning_rate": 3.944e-05,
      "loss": 0.0028,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.18246935307979584,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.0028,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.3985418677330017,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0027,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.17990437150001526,
      "learning_rate": 3.94e-05,
      "loss": 0.0032,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.4826144576072693,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0022,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.25250181555747986,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0026,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.11412885785102844,
      "learning_rate": 3.936e-05,
      "loss": 0.0026,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.1816595494747162,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0023,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.5712348818778992,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0027,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.5139080286026001,
      "learning_rate": 3.932e-05,
      "loss": 0.0024,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.23268161714076996,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.003,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.08548923581838608,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0028,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.3310772180557251,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0024,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.2934936583042145,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0029,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.8850798010826111,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0021,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.5370977520942688,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0025,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.8433681130409241,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0032,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.4016721248626709,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0034,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.25013288855552673,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0028,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 1.4817793369293213,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0031,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.1886647641658783,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.002,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.6466467380523682,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.0027,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.10232995450496674,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0027,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.6558778285980225,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0031,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.8691113591194153,
      "learning_rate": 3.912e-05,
      "loss": 0.0022,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.18711161613464355,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0026,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.36062607169151306,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.0029,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.24114671349525452,
      "learning_rate": 3.908e-05,
      "loss": 0.0023,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.44175177812576294,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0027,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.2200932502746582,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0023,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.16685187816619873,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0021,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.3771710693836212,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0025,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.18368403613567352,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0026,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.5275084972381592,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0027,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.9674719572067261,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.0034,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.679909884929657,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0026,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.24713349342346191,
      "learning_rate": 3.896e-05,
      "loss": 0.0025,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.2366473525762558,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.002,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.5871521234512329,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0023,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.3831760883331299,
      "learning_rate": 3.892e-05,
      "loss": 0.0026,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.6823698878288269,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.0025,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.09718535095453262,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0026,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.27854984998703003,
      "learning_rate": 3.888e-05,
      "loss": 0.0029,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.5795250535011292,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0025,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.10675731301307678,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0026,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.22842496633529663,
      "learning_rate": 3.884e-05,
      "loss": 0.0026,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.35470232367515564,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0024,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.29505375027656555,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0023,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.2298034131526947,
      "learning_rate": 3.88e-05,
      "loss": 0.0028,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.585577666759491,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0032,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.519389808177948,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.0022,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.6622294783592224,
      "learning_rate": 3.876e-05,
      "loss": 0.0022,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.14282847940921783,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0026,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.7334581017494202,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0027,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.3738531172275543,
      "learning_rate": 3.872e-05,
      "loss": 0.0025,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.28804194927215576,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0035,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.7179915308952332,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0021,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.10242003947496414,
      "learning_rate": 3.868e-05,
      "loss": 0.0021,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.10640071332454681,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0026,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.6863716840744019,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0018,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.6632287502288818,
      "learning_rate": 3.864e-05,
      "loss": 0.0023,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.24427343904972076,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0024,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.254331111907959,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0026,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.3290300667285919,
      "learning_rate": 3.86e-05,
      "loss": 0.0027,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.37602585554122925,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.0038,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.21488413214683533,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0024,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.1912737637758255,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0026,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.56360924243927,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0023,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.1486814171075821,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0026,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.5515058636665344,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0029,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.5642108917236328,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0028,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.18726375699043274,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0024,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.18234708905220032,
      "learning_rate": 3.848e-05,
      "loss": 0.0027,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.18988725543022156,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0029,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.8231385946273804,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0025,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.18467742204666138,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0026,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.09906332194805145,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0022,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.6664577126502991,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.0026,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.46406790614128113,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0034,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.4385609030723572,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0036,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.3209628164768219,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0029,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.4428767263889313,
      "learning_rate": 3.836e-05,
      "loss": 0.0023,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.320304811000824,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0032,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.5891900062561035,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.003,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.3944993019104004,
      "learning_rate": 3.832e-05,
      "loss": 0.0031,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.4868881404399872,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0024,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.2604322135448456,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0031,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.22423213720321655,
      "learning_rate": 3.828e-05,
      "loss": 0.0021,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.3019312620162964,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0021,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.3136819303035736,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0028,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.1707870215177536,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.0024,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.23849090933799744,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0024,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.33239099383354187,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0023,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.897789716720581,
      "learning_rate": 3.82e-05,
      "loss": 0.0029,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.15480434894561768,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0028,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.5262984037399292,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0022,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.9320629835128784,
      "learning_rate": 3.816e-05,
      "loss": 0.0024,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.5476534366607666,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0029,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.21851292252540588,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.004,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.3041526973247528,
      "learning_rate": 3.812e-05,
      "loss": 0.003,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.17862221598625183,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0034,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.3777759373188019,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0021,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.2580827474594116,
      "learning_rate": 3.808e-05,
      "loss": 0.0023,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.11702000349760056,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0033,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.24610185623168945,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0026,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.1326456218957901,
      "learning_rate": 3.804e-05,
      "loss": 0.002,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.38817915320396423,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0023,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.32702526450157166,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0024,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.2176159769296646,
      "learning_rate": 3.8e-05,
      "loss": 0.002,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.48875677585601807,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0024,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.42165902256965637,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0025,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.06898339837789536,
      "learning_rate": 3.796e-05,
      "loss": 0.0018,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.22637653350830078,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0026,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.13088421523571014,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0029,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.32422080636024475,
      "learning_rate": 3.792e-05,
      "loss": 0.0026,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.1248423159122467,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0025,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.09861548244953156,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0023,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.5419115424156189,
      "learning_rate": 3.788e-05,
      "loss": 0.003,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.3592078685760498,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0022,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.6177285313606262,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0024,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.7500630021095276,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0024,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.9422703385353088,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0024,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 1.106895923614502,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0034,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.3589836657047272,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0023,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.3656865060329437,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.002,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.11389205604791641,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0028,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.7501925826072693,
      "learning_rate": 3.776e-05,
      "loss": 0.0029,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.1357423961162567,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0021,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.2504923641681671,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.002,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.2066468745470047,
      "learning_rate": 3.772e-05,
      "loss": 0.0027,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.33511969447135925,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0023,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.6939315795898438,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0021,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.34133288264274597,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0021,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.3733794391155243,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0024,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.2537367045879364,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.002,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.7375986576080322,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0028,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.5566498041152954,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.002,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.16452562808990479,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0024,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.4075564742088318,
      "learning_rate": 3.76e-05,
      "loss": 0.0024,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.645365297794342,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.0027,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.4916503131389618,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0029,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.29910746216773987,
      "learning_rate": 3.756e-05,
      "loss": 0.0027,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.14632074534893036,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.0029,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.5027857422828674,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0029,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.37729132175445557,
      "learning_rate": 3.752e-05,
      "loss": 0.0021,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.540225088596344,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0022,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.6550266742706299,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.0027,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.13678394258022308,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0021,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.24821485579013824,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0022,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.22802406549453735,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0026,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.5950594544410706,
      "learning_rate": 3.744e-05,
      "loss": 0.0029,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.46192190051078796,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0027,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.40245676040649414,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0027,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.33627480268478394,
      "learning_rate": 3.74e-05,
      "loss": 0.0022,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.1297154277563095,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.0027,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.277387410402298,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0032,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.4123331606388092,
      "learning_rate": 3.736e-05,
      "loss": 0.0023,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.2886753976345062,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0025,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.1536722332239151,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0023,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.413761705160141,
      "learning_rate": 3.732e-05,
      "loss": 0.002,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.3553932309150696,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.0031,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.34183230996131897,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.002,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.26804736256599426,
      "learning_rate": 3.728e-05,
      "loss": 0.0022,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.26976943016052246,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0028,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.21963375806808472,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0024,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.7756059765815735,
      "learning_rate": 3.724e-05,
      "loss": 0.0021,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.22230125963687897,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.003,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.5793077945709229,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0034,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.1532142013311386,
      "learning_rate": 3.72e-05,
      "loss": 0.0028,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.5638129711151123,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0028,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.3476250171661377,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0022,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.9725548028945923,
      "learning_rate": 3.716e-05,
      "loss": 0.0024,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.25250136852264404,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0028,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.5095416903495789,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0025,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.1049782857298851,
      "learning_rate": 3.712e-05,
      "loss": 0.0022,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.3559626042842865,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0025,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.6885954737663269,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.003,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.3189687430858612,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.003,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.5400663614273071,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0024,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.22373858094215393,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.0027,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.24216441810131073,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0025,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.07590901106595993,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0025,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.2716415822505951,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.002,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.137928307056427,
      "learning_rate": 3.7e-05,
      "loss": 0.0027,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.07444842159748077,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0027,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.15566135942935944,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.002,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.16962824761867523,
      "learning_rate": 3.696e-05,
      "loss": 0.0026,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.2665784955024719,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0023,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.3889073431491852,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0022,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.05523621663451195,
      "learning_rate": 3.692e-05,
      "loss": 0.0024,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.46100321412086487,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.002,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.8032365441322327,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.002,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.24685534834861755,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.0031,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.10765016078948975,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0032,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.5087139010429382,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0031,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.7741591930389404,
      "learning_rate": 3.684e-05,
      "loss": 0.0026,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.4057798981666565,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0026,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.3097991943359375,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0023,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.1571715772151947,
      "learning_rate": 3.68e-05,
      "loss": 0.0025,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.08038055151700974,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0023,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.6732036471366882,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0031,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.6327844262123108,
      "learning_rate": 3.676e-05,
      "loss": 0.0031,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.34606271982192993,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0027,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.14057745039463043,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.003,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.31631484627723694,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0019,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.21227414906024933,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.0025,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.7692321538925171,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0033,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.3010351061820984,
      "learning_rate": 3.668e-05,
      "loss": 0.0034,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.13434329628944397,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0029,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.2680894136428833,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0026,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.18113620579242706,
      "learning_rate": 3.664e-05,
      "loss": 0.0033,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.6519647240638733,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0029,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.18014557659626007,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0027,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.26595595479011536,
      "learning_rate": 3.66e-05,
      "loss": 0.0032,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.3358396589756012,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.0022,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.9394407868385315,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0028,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.17934387922286987,
      "learning_rate": 3.656e-05,
      "loss": 0.0029,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.150925874710083,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.0027,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.06336208432912827,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0023,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.35778120160102844,
      "learning_rate": 3.652e-05,
      "loss": 0.0018,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.3034191429615021,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0025,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.5661076307296753,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0026,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.08179208636283875,
      "learning_rate": 3.648e-05,
      "loss": 0.0027,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.14509668946266174,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0022,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.7863433957099915,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.002,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.5895661115646362,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.002,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.5622073411941528,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.0029,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.781442403793335,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0022,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.3267146944999695,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0029,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.5307389497756958,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0032,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.26133084297180176,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0021,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.09649974852800369,
      "learning_rate": 3.636e-05,
      "loss": 0.0033,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.11903611570596695,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0025,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.6169143319129944,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0022,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.48145928978919983,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.0026,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.571398138999939,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0024,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.05768049880862236,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0026,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.36867302656173706,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0032,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.12667614221572876,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0035,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.34940198063850403,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0027,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.5093534588813782,
      "learning_rate": 3.624e-05,
      "loss": 0.0025,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.2554183602333069,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.0029,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.08387868106365204,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0023,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.42005422711372375,
      "learning_rate": 3.62e-05,
      "loss": 0.0023,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.5291349291801453,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0018,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.5867465734481812,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0029,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.4452723562717438,
      "learning_rate": 3.616e-05,
      "loss": 0.0024,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.31247204542160034,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0024,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.3716461956501007,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0027,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.30049383640289307,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0021,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.6068991422653198,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0027,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.2660543620586395,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0036,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.2030244916677475,
      "learning_rate": 3.608e-05,
      "loss": 0.0029,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.0866709053516388,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0027,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.7466484308242798,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0024,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.16236978769302368,
      "learning_rate": 3.604e-05,
      "loss": 0.0024,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.3789079785346985,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0026,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.16756710410118103,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0021,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.0769587755203247,
      "learning_rate": 3.6e-05,
      "loss": 0.0027,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.5903830528259277,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0023,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.6010887622833252,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0022,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.3395842909812927,
      "learning_rate": 3.596e-05,
      "loss": 0.0034,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.299371600151062,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0021,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.9886401295661926,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0021,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.25410982966423035,
      "learning_rate": 3.592e-05,
      "loss": 0.0029,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.32113903760910034,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.0027,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.39181625843048096,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0025,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 1.204394817352295,
      "learning_rate": 3.588e-05,
      "loss": 0.0025,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.4763169288635254,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.002,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.7256430983543396,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0023,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.7148059606552124,
      "learning_rate": 3.584e-05,
      "loss": 0.002,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.5430100560188293,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.0029,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.11713056266307831,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0021,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.13903261721134186,
      "learning_rate": 3.58e-05,
      "loss": 0.0029,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.4986005127429962,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0023,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.09268936514854431,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.0027,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.14097650349140167,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.003,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.7184547185897827,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0022,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.14741043746471405,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0021,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.5281299352645874,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0017,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.5211887955665588,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0019,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.3202260732650757,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0027,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.6389433741569519,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0021,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.3373033106327057,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0029,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.3783204257488251,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0019,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.24653545022010803,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0019,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.1447111964225769,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0025,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.15851883590221405,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0021,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.7707083225250244,
      "learning_rate": 3.56e-05,
      "loss": 0.0022,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.15111404657363892,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0028,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.6953871250152588,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0025,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.23113231360912323,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.0027,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.17687499523162842,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.0028,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.4838356077671051,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0022,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.49486982822418213,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0031,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.12171661853790283,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.003,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.3796023726463318,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.003,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.39961254596710205,
      "learning_rate": 3.548e-05,
      "loss": 0.0021,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.23518651723861694,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0028,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.41233009099960327,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0032,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.6609963178634644,
      "learning_rate": 3.544e-05,
      "loss": 0.002,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.21384954452514648,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0024,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.22852163016796112,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0022,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.15441539883613586,
      "learning_rate": 3.54e-05,
      "loss": 0.0024,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.3949291706085205,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.0026,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.13196805119514465,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0033,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.14116118848323822,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0021,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.4378301799297333,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0031,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.19799143075942993,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0026,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.316191166639328,
      "learning_rate": 3.532e-05,
      "loss": 0.0021,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.6657355427742004,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0021,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.21813338994979858,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0026,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.7522289156913757,
      "learning_rate": 3.528e-05,
      "loss": 0.0032,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.16087329387664795,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0019,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.16426189243793488,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.0026,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.6424588561058044,
      "learning_rate": 3.524e-05,
      "loss": 0.0024,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.09427013993263245,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0027,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.18684837222099304,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0022,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.07617441564798355,
      "learning_rate": 3.52e-05,
      "loss": 0.0021,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.8027487993240356,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.0026,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.21221809089183807,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0024,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.19673150777816772,
      "learning_rate": 3.516e-05,
      "loss": 0.002,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.2602473795413971,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.002,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.4878804683685303,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0027,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.2667035758495331,
      "learning_rate": 3.512e-05,
      "loss": 0.0027,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.3480317294597626,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0023,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.46416962146759033,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0017,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.42946597933769226,
      "learning_rate": 3.508e-05,
      "loss": 0.0017,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.5617691874504089,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0023,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.39180633425712585,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0023,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.4043407440185547,
      "learning_rate": 3.504e-05,
      "loss": 0.0026,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.10494913160800934,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.0019,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.13790327310562134,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0022,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.39899930357933044,
      "learning_rate": 3.5e-05,
      "loss": 0.0024,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.22592981159687042,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0024,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.3379765450954437,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0023,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.2432815581560135,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0026,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.3964512348175049,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0027,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.5469657778739929,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.003,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 1.1237661838531494,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0022,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.9719569087028503,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.003,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.8113671541213989,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0025,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.07482018321752548,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0019,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.4556277394294739,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0023,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.2643257677555084,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0024,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.47782135009765625,
      "learning_rate": 3.484e-05,
      "loss": 0.0021,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.5735381841659546,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.0026,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.17025309801101685,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0027,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.3439694941043854,
      "learning_rate": 3.48e-05,
      "loss": 0.0025,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.340556263923645,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0023,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.28510841727256775,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0025,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.46348315477371216,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0022,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.12255197763442993,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0021,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.545082151889801,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0024,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.18056610226631165,
      "learning_rate": 3.472e-05,
      "loss": 0.0024,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.5684136152267456,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0026,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.6856950521469116,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0024,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.6669042706489563,
      "learning_rate": 3.468e-05,
      "loss": 0.0031,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.6311606168746948,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0028,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.3414612114429474,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0019,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.12546269595623016,
      "learning_rate": 3.464e-05,
      "loss": 0.0026,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.141620934009552,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.0031,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.5310572385787964,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0021,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.25125551223754883,
      "learning_rate": 3.46e-05,
      "loss": 0.0021,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.8111728429794312,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.0024,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.1805131882429123,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0023,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.1731317639350891,
      "learning_rate": 3.456e-05,
      "loss": 0.0024,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.6419877409934998,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0028,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.40621477365493774,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0029,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.9855972528457642,
      "learning_rate": 3.452e-05,
      "loss": 0.0022,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.11791790276765823,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.0021,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.2730625867843628,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0024,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.41805827617645264,
      "learning_rate": 3.448e-05,
      "loss": 0.0022,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.31926217675209045,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0021,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.22987550497055054,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0031,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.16275976598262787,
      "learning_rate": 3.444e-05,
      "loss": 0.0024,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.21429993212223053,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0022,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.08798445016145706,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0024,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.815622866153717,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0023,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.45284560322761536,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0023,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.4190960228443146,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0029,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.10352813452482224,
      "learning_rate": 3.436e-05,
      "loss": 0.0025,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.3924012780189514,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0028,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.6737782955169678,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0036,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.13812744617462158,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0023,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.6517927646636963,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0018,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.26566246151924133,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0023,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.33268681168556213,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0021,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.0735345333814621,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0022,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.2865425944328308,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0023,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.47150126099586487,
      "learning_rate": 3.424e-05,
      "loss": 0.0022,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.11924832314252853,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.002,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.5609789490699768,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0018,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.0971936285495758,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0024,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.30422505736351013,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.0022,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.4606229364871979,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.0028,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.11148262023925781,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0019,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.40845227241516113,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0026,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.35438135266304016,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0028,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.11892411857843399,
      "learning_rate": 3.412e-05,
      "loss": 0.002,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.3363998830318451,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0021,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.3283020853996277,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0028,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.2742767333984375,
      "learning_rate": 3.408e-05,
      "loss": 0.0028,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.25255608558654785,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0024,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.32836660742759705,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0022,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.15592807531356812,
      "learning_rate": 3.404e-05,
      "loss": 0.0028,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.21775025129318237,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.0023,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.5836994051933289,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0021,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.7219825983047485,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0021,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 1.1058248281478882,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0025,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.6461328864097595,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0019,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.2776697278022766,
      "learning_rate": 3.396e-05,
      "loss": 0.0024,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.5245559215545654,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0026,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.20519505441188812,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0029,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.6541891098022461,
      "learning_rate": 3.392e-05,
      "loss": 0.0032,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.24766823649406433,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0031,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.4194512367248535,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0026,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.11848653852939606,
      "learning_rate": 3.388e-05,
      "loss": 0.0022,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.4392551779747009,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0022,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.23208178579807281,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.0029,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.6583613753318787,
      "learning_rate": 3.384e-05,
      "loss": 0.0024,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.4192487299442291,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.002,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.5138259530067444,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.0019,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.6811152696609497,
      "learning_rate": 3.38e-05,
      "loss": 0.0028,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.09017645567655563,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0025,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.6003484725952148,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0023,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.3517303168773651,
      "learning_rate": 3.376e-05,
      "loss": 0.0028,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.2165772169828415,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0023,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.5136138796806335,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.002,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.22272735834121704,
      "learning_rate": 3.372e-05,
      "loss": 0.0026,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.19958998262882233,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0024,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.3883000612258911,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.0022,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.8348330855369568,
      "learning_rate": 3.368e-05,
      "loss": 0.003,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.3925880193710327,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0023,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.1397695690393448,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0025,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.2571866810321808,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0022,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.15322813391685486,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.0021,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.34599703550338745,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0021,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.23212891817092896,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0019,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.6235650777816772,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0023,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.4458663761615753,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.0021,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.6004752516746521,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.0017,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.23033910989761353,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0025,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.9116142988204956,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0024,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.6064170598983765,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.002,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.4223334789276123,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0026,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.30826810002326965,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0025,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.2692137658596039,
      "learning_rate": 3.348e-05,
      "loss": 0.0033,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.2864859104156494,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0026,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.09823579341173172,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0029,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.13902254402637482,
      "learning_rate": 3.344e-05,
      "loss": 0.0027,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.1597413271665573,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0024,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.174890398979187,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0022,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.6010185480117798,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0023,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.6052899956703186,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.0022,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.32961830496788025,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.0023,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.10785787552595139,
      "learning_rate": 3.336e-05,
      "loss": 0.0027,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.16652293503284454,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.0028,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.11638075858354568,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0022,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.3852175176143646,
      "learning_rate": 3.332e-05,
      "loss": 0.0024,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.07294532656669617,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.0019,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.1633487492799759,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0018,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.13198257982730865,
      "learning_rate": 3.328e-05,
      "loss": 0.0024,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.3617580235004425,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0029,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.5042997598648071,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0021,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.20320147275924683,
      "learning_rate": 3.324e-05,
      "loss": 0.0022,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.13519981503486633,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0021,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.38883835077285767,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0027,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.4913632869720459,
      "learning_rate": 3.32e-05,
      "loss": 0.0027,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.2841988503932953,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.0021,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.24908262491226196,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.002,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.061533693224191666,
      "learning_rate": 3.316e-05,
      "loss": 0.0029,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.10069708526134491,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0025,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.49123355746269226,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0023,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.31693142652511597,
      "learning_rate": 3.312e-05,
      "loss": 0.0027,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.3169761002063751,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.0025,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.739434540271759,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0021,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.345317542552948,
      "learning_rate": 3.308e-05,
      "loss": 0.0022,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.27892088890075684,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0023,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.36768198013305664,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0025,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.23807303607463837,
      "learning_rate": 3.304e-05,
      "loss": 0.003,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.4578436017036438,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.0024,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.9660142064094543,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0024,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.9800466299057007,
      "learning_rate": 3.3e-05,
      "loss": 0.0028,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.1762305498123169,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0026,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.32738709449768066,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.0022,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.6985872387886047,
      "learning_rate": 3.296e-05,
      "loss": 0.0017,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.6787628531455994,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0024,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.2911669611930847,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.002,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.19266995787620544,
      "learning_rate": 3.292e-05,
      "loss": 0.0025,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.6499008536338806,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0028,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.28300660848617554,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0033,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.12400556355714798,
      "learning_rate": 3.288e-05,
      "loss": 0.0029,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.16986393928527832,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0021,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.2992156445980072,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0029,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.2872217297554016,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0029,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.31375232338905334,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.0024,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.11074337363243103,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0024,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.08550900220870972,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0024,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.1377357691526413,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0027,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.09611638635396957,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0026,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.2684866189956665,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.0022,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.17021307349205017,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0025,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.4238086938858032,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0021,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.5989921689033508,
      "learning_rate": 3.272e-05,
      "loss": 0.0021,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.651993453502655,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0018,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.2566259503364563,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0033,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.5861757397651672,
      "learning_rate": 3.268e-05,
      "loss": 0.0033,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.5189005732536316,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0025,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.6561194658279419,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.003,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.2683885991573334,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.0021,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.10067460685968399,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.002,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.12859559059143066,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0022,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.13376717269420624,
      "learning_rate": 3.26e-05,
      "loss": 0.0026,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.20368748903274536,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.0026,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.3518019914627075,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0024,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.492499440908432,
      "learning_rate": 3.256e-05,
      "loss": 0.0023,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.11464271694421768,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0028,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.3671018183231354,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0033,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.12966366112232208,
      "learning_rate": 3.252e-05,
      "loss": 0.0022,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.19511862099170685,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0023,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.19714118540287018,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0027,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.40750521421432495,
      "learning_rate": 3.248e-05,
      "loss": 0.0031,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.26234206557273865,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0029,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.544745147228241,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0026,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.3582375943660736,
      "learning_rate": 3.244e-05,
      "loss": 0.0028,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.17496606707572937,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.0022,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.7894817590713501,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0025,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.22857566177845,
      "learning_rate": 3.24e-05,
      "loss": 0.002,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.3834536373615265,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0021,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.10131367295980453,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0028,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.0966346487402916,
      "learning_rate": 3.236e-05,
      "loss": 0.0023,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.08414918184280396,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0021,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.24013981223106384,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0029,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.18301662802696228,
      "learning_rate": 3.232e-05,
      "loss": 0.0022,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.712630033493042,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0025,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.6372234225273132,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.0028,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.2969375550746918,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0025,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.3846445381641388,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0024,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.09467562288045883,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0022,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.19771245121955872,
      "learning_rate": 3.224e-05,
      "loss": 0.002,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.15462985634803772,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0022,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.09778881818056107,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0022,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.33020296692848206,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0021,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.21599464118480682,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0022,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.04308456927537918,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0028,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.5169304609298706,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0021,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.1651400625705719,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0023,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.8199205994606018,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0025,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.22996026277542114,
      "learning_rate": 3.212e-05,
      "loss": 0.0019,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.47838422656059265,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0025,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.19821442663669586,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.0022,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.5996087789535522,
      "learning_rate": 3.208e-05,
      "loss": 0.002,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.5260949730873108,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0029,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.4827840328216553,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.0027,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.23058368265628815,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.002,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.7071915864944458,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0024,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.11370398849248886,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0028,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.09291715174913406,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0023,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.1225205510854721,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0022,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.2261337786912918,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0019,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.5942472815513611,
      "learning_rate": 3.196e-05,
      "loss": 0.0027,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.2068188637495041,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.0025,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.44591957330703735,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0026,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.2904614806175232,
      "learning_rate": 3.192e-05,
      "loss": 0.0029,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.27831652760505676,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0026,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.4523106813430786,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.0024,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 1.241083025932312,
      "learning_rate": 3.188e-05,
      "loss": 0.0031,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.2241857647895813,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.003,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.3176651895046234,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0024,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.4299890100955963,
      "learning_rate": 3.184e-05,
      "loss": 0.0029,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.8613691329956055,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.003,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.08097084611654282,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0021,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.39111706614494324,
      "learning_rate": 3.18e-05,
      "loss": 0.0027,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.602739155292511,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0024,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.13461898267269135,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0031,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.23365868628025055,
      "learning_rate": 3.176e-05,
      "loss": 0.0022,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.4321843087673187,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0021,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.08569592237472534,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0023,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.6249895691871643,
      "learning_rate": 3.172e-05,
      "loss": 0.0018,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.19334103167057037,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0028,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.33435702323913574,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0022,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.4621167480945587,
      "learning_rate": 3.168e-05,
      "loss": 0.0022,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.3039712607860565,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0023,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.7227002382278442,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0022,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.9542833566665649,
      "learning_rate": 3.164e-05,
      "loss": 0.002,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.5717464089393616,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.0023,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.2550572156906128,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0025,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.20027261972427368,
      "learning_rate": 3.16e-05,
      "loss": 0.0024,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.3493651747703552,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0021,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.3130868077278137,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0025,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.6186877489089966,
      "learning_rate": 3.156e-05,
      "loss": 0.0026,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.25035062432289124,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.0024,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.19550864398479462,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.002,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.8318385481834412,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0029,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.2031855434179306,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0035,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.9067081212997437,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0021,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.7475478649139404,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0023,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.2073335349559784,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0025,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.24014614522457123,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.002,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.843431293964386,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.0023,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.06578000634908676,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0028,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.09525987505912781,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0019,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.4002057611942291,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0023,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.2607153654098511,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0031,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.5026589035987854,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0023,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.78232342004776,
      "learning_rate": 3.136e-05,
      "loss": 0.0017,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.40608251094818115,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0024,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.4505780041217804,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.002,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.34533199667930603,
      "learning_rate": 3.132e-05,
      "loss": 0.0024,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.11457427591085434,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.0019,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.25566786527633667,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0019,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.18320220708847046,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0026,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.5201078057289124,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0025,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.36464226245880127,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0023,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.16673733294010162,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0025,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.1898287534713745,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0021,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.22237779200077057,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0027,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.22712677717208862,
      "learning_rate": 3.12e-05,
      "loss": 0.0025,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.12640810012817383,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.002,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.2048799693584442,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0026,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.10237091779708862,
      "learning_rate": 3.116e-05,
      "loss": 0.0022,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.2744419574737549,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0022,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.593536913394928,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0019,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.19755017757415771,
      "learning_rate": 3.112e-05,
      "loss": 0.0019,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.13919612765312195,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0023,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.17650796473026276,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0023,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.7391290664672852,
      "learning_rate": 3.108e-05,
      "loss": 0.002,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.3076287806034088,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0023,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.12536074221134186,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0018,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.19152776896953583,
      "learning_rate": 3.104e-05,
      "loss": 0.0023,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.20124797523021698,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0024,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.8504419922828674,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0021,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.42606088519096375,
      "learning_rate": 3.1e-05,
      "loss": 0.0027,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.4929862916469574,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0019,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.8188884854316711,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.0021,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.12316235154867172,
      "learning_rate": 3.096e-05,
      "loss": 0.0024,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.13580374419689178,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0024,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.26596012711524963,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0026,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.4576385021209717,
      "learning_rate": 3.092e-05,
      "loss": 0.0031,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.22878901660442352,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0028,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.15051576495170593,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.002,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.3119862973690033,
      "learning_rate": 3.088e-05,
      "loss": 0.0027,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.16000452637672424,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0025,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.17304570972919464,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0025,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.15663495659828186,
      "learning_rate": 3.084e-05,
      "loss": 0.0027,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.320340633392334,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0023,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.15829896926879883,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0023,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.35578346252441406,
      "learning_rate": 3.08e-05,
      "loss": 0.0023,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.41313108801841736,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0026,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.1383834034204483,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0027,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.17062537372112274,
      "learning_rate": 3.076e-05,
      "loss": 0.0022,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.1951749622821808,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.0019,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.32405486702919006,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0018,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.0934586152434349,
      "learning_rate": 3.072e-05,
      "loss": 0.0024,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.08361484855413437,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0022,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.42934203147888184,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0026,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.48713991045951843,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0025,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.3774194121360779,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.002,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.3890814185142517,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0027,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.41120272874832153,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0032,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.10039857029914856,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0016,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.19228293001651764,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0028,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.30050358176231384,
      "learning_rate": 3.06e-05,
      "loss": 0.0022,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.37312689423561096,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.0019,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.13934344053268433,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0017,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.22427086532115936,
      "learning_rate": 3.056e-05,
      "loss": 0.0028,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.3923521339893341,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0021,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.21557612717151642,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0021,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.23524852097034454,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.0023,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.08358821272850037,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0026,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.1126370057463646,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.002,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.08097556978464127,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.0023,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.13182009756565094,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0019,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.7086764574050903,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0025,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.27221640944480896,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0026,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.09687402844429016,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0026,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.22337408363819122,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0026,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.20349785685539246,
      "learning_rate": 3.04e-05,
      "loss": 0.0025,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.8359521627426147,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0026,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.4851858913898468,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0022,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.4457996189594269,
      "learning_rate": 3.036e-05,
      "loss": 0.0021,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.3192604184150696,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0024,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.3768952786922455,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0021,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.2234259396791458,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0024,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.12233548611402512,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0027,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.29746001958847046,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0021,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.5813372135162354,
      "learning_rate": 3.028e-05,
      "loss": 0.0026,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.48493489623069763,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0026,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.6194521188735962,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0027,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.3480871915817261,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0023,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.10565969347953796,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0026,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.19003081321716309,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0022,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.23993928730487823,
      "learning_rate": 3.02e-05,
      "loss": 0.0025,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.47049325704574585,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0025,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.12592333555221558,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0031,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.16077277064323425,
      "learning_rate": 3.016e-05,
      "loss": 0.0028,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.1361703723669052,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.0025,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.6798694133758545,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0026,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.2935406267642975,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0022,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.7147207260131836,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.0022,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.5033732056617737,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0022,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.6276780962944031,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0023,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.17251260578632355,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.002,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.724324107170105,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.003,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.37320780754089355,
      "learning_rate": 3.004e-05,
      "loss": 0.0021,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.1168844923377037,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0028,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.5506376028060913,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.0029,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.40438035130500793,
      "learning_rate": 3e-05,
      "loss": 0.0024,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.152990460395813,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.002,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.2826065421104431,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0026,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.23382388055324554,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0023,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.36098018288612366,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.0023,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.45701175928115845,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0025,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.4741416275501251,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0019,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.11546289175748825,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0019,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.42909935116767883,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0023,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.5522526502609253,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0025,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.174650177359581,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0022,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.687393844127655,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0024,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.12237869203090668,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0021,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.2924356162548065,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0019,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.463076114654541,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0021,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.2723458707332611,
      "learning_rate": 2.98e-05,
      "loss": 0.0026,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.08717332780361176,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.0025,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.5071628093719482,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0022,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.07129409909248352,
      "learning_rate": 2.976e-05,
      "loss": 0.0022,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.15827368199825287,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0018,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.6594900488853455,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0024,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.2358781397342682,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0024,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.1733485907316208,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0025,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.5722762942314148,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0037,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.17836803197860718,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0025,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.23624368011951447,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0025,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.30428916215896606,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0028,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.844458818435669,
      "learning_rate": 2.964e-05,
      "loss": 0.0027,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.08405029773712158,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0023,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.17602387070655823,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0023,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.15229646861553192,
      "learning_rate": 2.96e-05,
      "loss": 0.0023,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.30162313580513,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.0028,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.28835487365722656,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0025,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.27972668409347534,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0024,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.22528162598609924,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.0021,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.4093396067619324,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0025,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.23212218284606934,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.003,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.3898618817329407,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0023,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.13689938187599182,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0027,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.21484462916851044,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0026,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.40318790078163147,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0027,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.4395100772380829,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.0028,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.13397406041622162,
      "learning_rate": 2.944e-05,
      "loss": 0.0019,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.09150587767362595,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0023,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.08900822699069977,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0021,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.2131529450416565,
      "learning_rate": 2.94e-05,
      "loss": 0.0025,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.13938888907432556,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0025,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.16796839237213135,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.0022,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.3438052535057068,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0021,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.11701745539903641,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0023,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.43270349502563477,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0021,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.27280357480049133,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0025,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.8488890528678894,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0022,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.15667757391929626,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0022,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.5384663343429565,
      "learning_rate": 2.928e-05,
      "loss": 0.0019,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.5168288350105286,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.002,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.27449026703834534,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0022,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.39416882395744324,
      "learning_rate": 2.924e-05,
      "loss": 0.0024,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.5996467471122742,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.003,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.463113933801651,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.002,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.4876933991909027,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0019,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.5587485432624817,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.002,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.43232306838035583,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0025,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.1877104490995407,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0021,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.24128301441669464,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0021,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.7692825198173523,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.002,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.1676715612411499,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.002,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.0755731463432312,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.003,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.740566611289978,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.0025,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.09745859354734421,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0022,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.0696793794631958,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0026,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.2670000195503235,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.0018,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.2058422863483429,
      "learning_rate": 2.904e-05,
      "loss": 0.0027,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.7928218841552734,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0025,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.8798953890800476,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.0024,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.6141201257705688,
      "learning_rate": 2.9e-05,
      "loss": 0.0022,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.40829113125801086,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.002,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.3418084383010864,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.0021,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.27762219309806824,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.0025,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.5612961649894714,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0026,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.7921887636184692,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.002,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.0986403152346611,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.002,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.08659181743860245,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0021,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.39998435974121094,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0023,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.29379016160964966,
      "learning_rate": 2.888e-05,
      "loss": 0.0023,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.18455733358860016,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0027,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.08184440433979034,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0024,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.25031742453575134,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.0025,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.34291672706604004,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0024,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.45034682750701904,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.0021,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.18835891783237457,
      "learning_rate": 2.88e-05,
      "loss": 0.0021,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.7402819395065308,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0024,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.7830780744552612,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.002,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.5802791714668274,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.0027,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.21104243397712708,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0026,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.31225886940956116,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0029,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.09151298552751541,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0025,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.3423604965209961,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.0022,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.31096863746643066,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0022,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.7586507797241211,
      "learning_rate": 2.868e-05,
      "loss": 0.0024,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.5201966166496277,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0026,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.837426483631134,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0021,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.44565364718437195,
      "learning_rate": 2.864e-05,
      "loss": 0.0026,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.29539772868156433,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.0024,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.17696943879127502,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0022,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.07367997616529465,
      "learning_rate": 2.86e-05,
      "loss": 0.0022,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.4028245508670807,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0027,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.3402548134326935,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.003,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.3530307710170746,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0027,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.18038460612297058,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0021,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.5692574977874756,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0027,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.6073582768440247,
      "learning_rate": 2.852e-05,
      "loss": 0.002,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.2114211469888687,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.0023,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.23493197560310364,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.0021,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.5784772038459778,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0026,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.19026006758213043,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0028,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.3114396929740906,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0027,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.19447258114814758,
      "learning_rate": 2.844e-05,
      "loss": 0.0037,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.2217867225408554,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.002,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.47882702946662903,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0023,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.7082579135894775,
      "learning_rate": 2.84e-05,
      "loss": 0.0026,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.9618104696273804,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0027,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.36765751242637634,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0022,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.2604447305202484,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0027,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.3590645492076874,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0029,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.15439487993717194,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0027,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.35896608233451843,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0021,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.4775819778442383,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0032,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.6312574744224548,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.0023,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.6976194381713867,
      "learning_rate": 2.828e-05,
      "loss": 0.0025,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.423019140958786,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.002,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.14223703742027283,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.002,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.5139802098274231,
      "learning_rate": 2.824e-05,
      "loss": 0.002,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.0725455954670906,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0026,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.7336530089378357,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.0019,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.24293404817581177,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0024,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.060680195689201355,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0028,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.41231799125671387,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0026,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.12139309197664261,
      "learning_rate": 2.816e-05,
      "loss": 0.002,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.5366722941398621,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0026,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.11748528480529785,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.002,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.28670987486839294,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0027,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.09222950041294098,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0023,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.21835480630397797,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.002,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.31551775336265564,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0023,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.30475395917892456,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0022,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.17880138754844666,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0031,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.13907240331172943,
      "learning_rate": 2.804e-05,
      "loss": 0.0025,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.3941153585910797,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.002,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.17077510058879852,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0025,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.23613810539245605,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.002,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.44218307733535767,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0021,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.3648552894592285,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0022,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.6940519213676453,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0022,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.11673874408006668,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.0024,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.08701932430267334,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0027,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.11213391274213791,
      "learning_rate": 2.792e-05,
      "loss": 0.0024,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.38374799489974976,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.0019,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.2672463059425354,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0027,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.10235033184289932,
      "learning_rate": 2.788e-05,
      "loss": 0.0022,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.6431860327720642,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0027,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.42368414998054504,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0021,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.20851626992225647,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0023,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.13947151601314545,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0026,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.18956972658634186,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0022,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.07390137016773224,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0023,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.539593517780304,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0024,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.1659044325351715,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.0027,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.17787590622901917,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.0027,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.2641802430152893,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.0022,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.4476630687713623,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0023,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.7762600779533386,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.0024,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.5004069805145264,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0028,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.6899610757827759,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0024,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.3101159930229187,
      "learning_rate": 2.768e-05,
      "loss": 0.0018,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.1334393173456192,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0026,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.24155886471271515,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0022,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.13801278173923492,
      "learning_rate": 2.764e-05,
      "loss": 0.002,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.0919976457953453,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.0024,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.2985673248767853,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0033,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.22592255473136902,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0019,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.48283398151397705,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0023,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.3022608458995819,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0027,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.20926296710968018,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.002,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.4744517207145691,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0022,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.16989757120609283,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0023,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.4421178996562958,
      "learning_rate": 2.752e-05,
      "loss": 0.0017,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.13282941281795502,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0023,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.4624997079372406,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.002,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.13143104314804077,
      "learning_rate": 2.748e-05,
      "loss": 0.0023,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.258987158536911,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0024,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.8084277510643005,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.0024,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.2905771732330322,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0021,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.21513506770133972,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.002,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.19112886488437653,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.0025,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.6094327569007874,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0019,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.09066307544708252,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.002,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.06880194693803787,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0019,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.3416891396045685,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0027,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.8659802079200745,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0019,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.346962571144104,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0023,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.5005462765693665,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.002,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.07126352936029434,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0024,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.16538842022418976,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.0024,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.6642910838127136,
      "learning_rate": 2.728e-05,
      "loss": 0.0023,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.33916807174682617,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0029,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.34562817215919495,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0018,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.7855948209762573,
      "learning_rate": 2.724e-05,
      "loss": 0.0025,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.36913809180259705,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.002,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.2428080290555954,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0021,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.3085975646972656,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0025,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.4153374433517456,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0021,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.4192024767398834,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.0024,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.4336240291595459,
      "learning_rate": 2.716e-05,
      "loss": 0.002,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.5589233040809631,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.0022,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.11001390218734741,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0025,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.3139730393886566,
      "learning_rate": 2.712e-05,
      "loss": 0.002,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.7787299752235413,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0023,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.12861986458301544,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0025,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.8131375908851624,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.0026,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.21969102323055267,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0018,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.27909788489341736,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0024,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.2618371248245239,
      "learning_rate": 2.704e-05,
      "loss": 0.0018,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.6678251028060913,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0029,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.13115748763084412,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0026,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.08080440759658813,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0019,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.5930188298225403,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0019,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.27416402101516724,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0024,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.163047656416893,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0018,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.6056787967681885,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0032,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.5190441608428955,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0021,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.20494771003723145,
      "learning_rate": 2.692e-05,
      "loss": 0.0028,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.2966928780078888,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0024,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.22684282064437866,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0022,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.44978591799736023,
      "learning_rate": 2.688e-05,
      "loss": 0.0026,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.7805339694023132,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.002,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.08017067611217499,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0028,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.43872523307800293,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.0021,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.37748587131500244,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0021,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.08940498530864716,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.002,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.3153992295265198,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0021,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.2422674000263214,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0024,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.08867370337247849,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0021,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.06586739420890808,
      "learning_rate": 2.676e-05,
      "loss": 0.0019,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.5305188894271851,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.0017,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.24953250586986542,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0027,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.46683230996131897,
      "learning_rate": 2.672e-05,
      "loss": 0.0023,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.8137219548225403,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0028,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.5318824052810669,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0025,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.11371110379695892,
      "learning_rate": 2.668e-05,
      "loss": 0.0026,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.37395763397216797,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0026,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.18018053472042084,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0027,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.16343919932842255,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.0023,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.6431220769882202,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.0025,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.3812785744667053,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.002,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.13428238034248352,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0021,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.13166587054729462,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0028,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.09306750446557999,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.002,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.1694623976945877,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0021,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.18165747821331024,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0022,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.20997460186481476,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0029,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.33168825507164,
      "learning_rate": 2.652e-05,
      "loss": 0.002,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.33045902848243713,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0023,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.1658935695886612,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0023,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.41451752185821533,
      "learning_rate": 2.648e-05,
      "loss": 0.0025,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.12637481093406677,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.002,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.5140430331230164,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0017,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.6149857044219971,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.0023,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.7218660116195679,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0022,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.22397343814373016,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0025,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.14369143545627594,
      "learning_rate": 2.64e-05,
      "loss": 0.0018,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.4576367139816284,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0022,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.8114770650863647,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0025,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.3303295373916626,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0023,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.3327203691005707,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0025,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.28005877137184143,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0025,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.09075362980365753,
      "learning_rate": 2.632e-05,
      "loss": 0.0024,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.43027573823928833,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0021,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.1662071794271469,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.0023,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.20971651375293732,
      "learning_rate": 2.628e-05,
      "loss": 0.0021,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.564449667930603,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0025,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.5323697328567505,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0026,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.1871597319841385,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0019,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 1.077402949333191,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0026,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.6508671641349792,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0023,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.20577619969844818,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0022,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.27953317761421204,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0025,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.13119621574878693,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0019,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.43651923537254333,
      "learning_rate": 2.616e-05,
      "loss": 0.0024,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.18960605561733246,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0025,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.4685390591621399,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0019,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.31495919823646545,
      "learning_rate": 2.612e-05,
      "loss": 0.0024,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.17149823904037476,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0023,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.6360960602760315,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0027,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.21590514481067657,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0018,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.46362969279289246,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0033,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.20446617901325226,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0026,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.5293412208557129,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.0021,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.8908668160438538,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0024,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.19430120289325714,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0024,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.8602743744850159,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0024,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.13651025295257568,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0024,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.16176877915859222,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0027,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.42583855986595154,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.0026,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.07562590390443802,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0024,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.21496886014938354,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0025,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.8347660899162292,
      "learning_rate": 2.592e-05,
      "loss": 0.0021,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.08522310107946396,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0027,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.0946880653500557,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.003,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.2579197287559509,
      "learning_rate": 2.588e-05,
      "loss": 0.0028,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.09204267710447311,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0025,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.702496349811554,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.002,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.41078564524650574,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0024,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.5236199498176575,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0021,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.18069709837436676,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0024,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.07920227199792862,
      "learning_rate": 2.58e-05,
      "loss": 0.0028,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.12008063495159149,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0025,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.14109112322330475,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0026,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.12946978211402893,
      "learning_rate": 2.576e-05,
      "loss": 0.002,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.21308833360671997,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.002,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.25109490752220154,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0023,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.30723002552986145,
      "learning_rate": 2.572e-05,
      "loss": 0.0023,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.15340977907180786,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0021,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.3052385747432709,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0023,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.27995675802230835,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.002,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.35208311676979065,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.002,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.43478333950042725,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.0027,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.24620486795902252,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0018,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.10534420609474182,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0025,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.6783321499824524,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0021,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.4113222062587738,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0025,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.3212718665599823,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.0026,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.44246482849121094,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.0023,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.8052831292152405,
      "learning_rate": 2.556e-05,
      "loss": 0.0028,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.4836525321006775,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0027,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.6745706796646118,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.002,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.32923147082328796,
      "learning_rate": 2.552e-05,
      "loss": 0.0026,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.10465482622385025,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.0028,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.12937718629837036,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0022,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.154144749045372,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0028,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.11783286184072495,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0032,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.21382270753383636,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.002,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.19162946939468384,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0022,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.6053047776222229,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0027,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.41987141966819763,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0025,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.3960886001586914,
      "learning_rate": 2.54e-05,
      "loss": 0.002,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.7907098531723022,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0023,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.4768419563770294,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0025,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.10489628463983536,
      "learning_rate": 2.536e-05,
      "loss": 0.0025,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.10962322354316711,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0023,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.2490282952785492,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0021,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.3044082820415497,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0026,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.2537636160850525,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0025,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.6704111099243164,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0021,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.20979472994804382,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.002,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.28449469804763794,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0024,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.11733238399028778,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.0025,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.15540950000286102,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.002,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.09845027327537537,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0022,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.0714997723698616,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0018,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.41124197840690613,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0027,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.5511408448219299,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.002,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.1430402398109436,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0027,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.42117905616760254,
      "learning_rate": 2.516e-05,
      "loss": 0.0023,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.6247774958610535,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.0022,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.13379625976085663,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0024,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.21661707758903503,
      "learning_rate": 2.512e-05,
      "loss": 0.0021,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.20073126256465912,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.002,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.08089511096477509,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.002,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.5510864853858948,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0025,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.3113566040992737,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0019,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.1270395666360855,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.0023,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.12175538390874863,
      "learning_rate": 2.504e-05,
      "loss": 0.002,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.25482574105262756,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0022,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.7982085943222046,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0027,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7537744641304016,
      "learning_rate": 2.5e-05,
      "loss": 0.0025,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.002313608303666115,
      "eval_runtime": 108.3927,
      "eval_samples_per_second": 1383.856,
      "eval_steps_per_second": 34.596,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 1.0666489601135254,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0023,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.07823537290096283,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0024,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.11029788851737976,
      "learning_rate": 2.496e-05,
      "loss": 0.0021,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.46448779106140137,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0019,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.6968131065368652,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0019,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.21236149966716766,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.0027,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.7125865817070007,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0022,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.102008156478405,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0019,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.09184470027685165,
      "learning_rate": 2.488e-05,
      "loss": 0.0023,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.08331304788589478,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.002,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.10461457073688507,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0024,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.23567847907543182,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0026,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.6034177541732788,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0023,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.20409630239009857,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0025,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.2523878812789917,
      "learning_rate": 2.48e-05,
      "loss": 0.0026,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.1541643887758255,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0018,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.13795122504234314,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0025,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.11059370636940002,
      "learning_rate": 2.476e-05,
      "loss": 0.0018,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.06528668850660324,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0019,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.1581413298845291,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0027,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.14837077260017395,
      "learning_rate": 2.472e-05,
      "loss": 0.0018,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.25188764929771423,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0025,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.38024085760116577,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.002,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.5089230537414551,
      "learning_rate": 2.468e-05,
      "loss": 0.0024,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.32501164078712463,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0022,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.3675476014614105,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0022,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.09913406521081924,
      "learning_rate": 2.464e-05,
      "loss": 0.0021,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.13025972247123718,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.0021,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.504544734954834,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0023,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.20977845788002014,
      "learning_rate": 2.46e-05,
      "loss": 0.0025,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.08502063900232315,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0028,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.47831854224205017,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0018,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.2987198829650879,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.002,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.3251766264438629,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0019,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.6157810688018799,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0024,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.14092078804969788,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0025,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.07393117994070053,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.0021,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.2942974865436554,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0024,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.7076961994171143,
      "learning_rate": 2.448e-05,
      "loss": 0.002,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.18470971286296844,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0022,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.19369502365589142,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0023,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.13002057373523712,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0019,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.26371288299560547,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0024,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.12685562670230865,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.003,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.47323665022850037,
      "learning_rate": 2.44e-05,
      "loss": 0.0022,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.5090391039848328,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.0021,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.17523573338985443,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0026,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.13604149222373962,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0021,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.3739684522151947,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.0018,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.5238627195358276,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0021,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.14795991778373718,
      "learning_rate": 2.432e-05,
      "loss": 0.0025,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.29580217599868774,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0021,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.06698144972324371,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0019,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.49671030044555664,
      "learning_rate": 2.428e-05,
      "loss": 0.0025,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.20858833193778992,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0022,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.4263138473033905,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0021,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.3282017409801483,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0025,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.3516123592853546,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.0024,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.5976245999336243,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0021,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.269814133644104,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0028,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.1067059114575386,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.002,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.1115466058254242,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.0024,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.1474430412054062,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0022,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.11762376129627228,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0023,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.8000739812850952,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0019,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.2601475715637207,
      "learning_rate": 2.412e-05,
      "loss": 0.002,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.3940483629703522,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.0031,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.5866937041282654,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0023,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.08466771990060806,
      "learning_rate": 2.408e-05,
      "loss": 0.0021,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.2348133623600006,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0022,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.5255343914031982,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0026,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.09482719749212265,
      "learning_rate": 2.404e-05,
      "loss": 0.0025,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.10401513427495956,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0022,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.17710547149181366,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.0027,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.11281344294548035,
      "learning_rate": 2.4e-05,
      "loss": 0.0023,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.15482498705387115,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0025,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.2683345079421997,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0018,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.1394827663898468,
      "learning_rate": 2.396e-05,
      "loss": 0.0023,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.08931100368499756,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0023,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.3627466559410095,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.002,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.2965086102485657,
      "learning_rate": 2.392e-05,
      "loss": 0.0021,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.34046605229377747,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0016,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.46510130167007446,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.0026,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.2610672116279602,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0024,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.29403263330459595,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0024,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.14963571727275848,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0019,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.1796291321516037,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0023,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.12637488543987274,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.002,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.11841508001089096,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0019,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.1426403522491455,
      "learning_rate": 2.38e-05,
      "loss": 0.0021,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.6153639554977417,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.002,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.391034334897995,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0019,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.6208599209785461,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0023,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.2760160565376282,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.0024,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.5146113038063049,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0027,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.4953058958053589,
      "learning_rate": 2.372e-05,
      "loss": 0.0022,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.3369279205799103,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0024,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.25451362133026123,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0023,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.22775574028491974,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0018,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.19461598992347717,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0023,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.49306827783584595,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0026,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.2906600832939148,
      "learning_rate": 2.364e-05,
      "loss": 0.0024,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.15614889562129974,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.0022,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.3315846025943756,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0021,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.209430992603302,
      "learning_rate": 2.36e-05,
      "loss": 0.002,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.7237997651100159,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.002,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.5444064736366272,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.002,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.19165289402008057,
      "learning_rate": 2.356e-05,
      "loss": 0.0028,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.10272270441055298,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0023,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.7744047045707703,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0023,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.11686047166585922,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0019,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.3062893748283386,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0022,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.42056089639663696,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0022,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.38316744565963745,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0022,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.4626692831516266,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0023,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.0760788545012474,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0021,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.76320880651474,
      "learning_rate": 2.344e-05,
      "loss": 0.0021,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.1720167100429535,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.0024,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.07447071373462677,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0025,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.5135733485221863,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0023,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.2348800152540207,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0025,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.450660765171051,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.0023,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.20429633557796478,
      "learning_rate": 2.336e-05,
      "loss": 0.0023,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.5292262434959412,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0021,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.43676072359085083,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0019,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.4446036219596863,
      "learning_rate": 2.332e-05,
      "loss": 0.0024,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.31999996304512024,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.002,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.39312365651130676,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.0017,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.11547411978244781,
      "learning_rate": 2.328e-05,
      "loss": 0.0022,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.2658537030220032,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0022,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.2991706132888794,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.0019,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.243903249502182,
      "learning_rate": 2.324e-05,
      "loss": 0.0027,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.1809266060590744,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.002,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.2729821503162384,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0018,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.06890949606895447,
      "learning_rate": 2.32e-05,
      "loss": 0.0025,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.45523691177368164,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.0024,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.06476334482431412,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.003,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.5061862468719482,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.003,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.26725271344184875,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0019,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.1061827689409256,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0023,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.14227187633514404,
      "learning_rate": 2.312e-05,
      "loss": 0.0018,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.22159667313098907,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0022,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.4486987590789795,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0024,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.42114412784576416,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0018,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.13529297709465027,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.002,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.3350036144256592,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.0019,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.17038021981716156,
      "learning_rate": 2.304e-05,
      "loss": 0.0022,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.456627756357193,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.0023,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.10457368940114975,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0022,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.7569237351417542,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0024,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.12802252173423767,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0022,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.2972187399864197,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0019,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.2985340654850006,
      "learning_rate": 2.296e-05,
      "loss": 0.0026,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.2758195400238037,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.002,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.5331274271011353,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0023,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.5969971418380737,
      "learning_rate": 2.292e-05,
      "loss": 0.0024,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.11796821653842926,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0023,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.6346287727355957,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0028,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.08114305138587952,
      "learning_rate": 2.288e-05,
      "loss": 0.002,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.3557329475879669,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0025,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.09680941700935364,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.0024,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.5235430598258972,
      "learning_rate": 2.284e-05,
      "loss": 0.0019,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.24254542589187622,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0024,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.45006465911865234,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0028,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.4902210831642151,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0021,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.33960792422294617,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.0022,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.09714175015687943,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.0018,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.25431180000305176,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.002,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.5564829111099243,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.0027,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.2651958465576172,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0024,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.36799871921539307,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0021,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.2666885554790497,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.0021,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.3836511969566345,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.003,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.6796882748603821,
      "learning_rate": 2.268e-05,
      "loss": 0.0026,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.10580731183290482,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0017,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.13856567442417145,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0026,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.5186958312988281,
      "learning_rate": 2.264e-05,
      "loss": 0.0019,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.3602532744407654,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0029,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.3925689160823822,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.002,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.08462782949209213,
      "learning_rate": 2.26e-05,
      "loss": 0.0021,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.17556814849376678,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.003,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.1953164041042328,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0024,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.14496080577373505,
      "learning_rate": 2.256e-05,
      "loss": 0.002,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.1563829928636551,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0022,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.10150404274463654,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0021,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.12562961876392365,
      "learning_rate": 2.252e-05,
      "loss": 0.0021,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.36314091086387634,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0024,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.20721237361431122,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0024,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.36714932322502136,
      "learning_rate": 2.248e-05,
      "loss": 0.002,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.7135667204856873,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0024,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.33284991979599,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0018,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.33698469400405884,
      "learning_rate": 2.244e-05,
      "loss": 0.0031,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.3526615798473358,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0024,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.1182999387383461,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0023,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.13858424127101898,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0018,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.0644044578075409,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0021,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.46237218379974365,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.002,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.18849831819534302,
      "learning_rate": 2.236e-05,
      "loss": 0.0022,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.1985384076833725,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.0026,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 1.0562018156051636,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0025,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.522193968296051,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.0024,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.17027056217193604,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0019,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.3613636791706085,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0022,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.07499226927757263,
      "learning_rate": 2.228e-05,
      "loss": 0.002,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.24583330750465393,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0027,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.16887961328029633,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0027,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.42700353264808655,
      "learning_rate": 2.224e-05,
      "loss": 0.002,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.6701842546463013,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0021,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.3252143859863281,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.0026,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.10921402275562286,
      "learning_rate": 2.22e-05,
      "loss": 0.0023,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.4851071238517761,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.0022,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.14276951551437378,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0025,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.4745402932167053,
      "learning_rate": 2.216e-05,
      "loss": 0.0019,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.13363738358020782,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.0022,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.18469524383544922,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0018,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.21102778613567352,
      "learning_rate": 2.212e-05,
      "loss": 0.0025,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.2690647840499878,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0022,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.4145445227622986,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.002,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.5289920568466187,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0027,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.1275913268327713,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0026,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.09797273576259613,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0027,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.34984466433525085,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0019,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.2689855396747589,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.0022,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.20889756083488464,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.003,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.17043614387512207,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.002,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.5120245218276978,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0022,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.124994657933712,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0019,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.92580246925354,
      "learning_rate": 2.196e-05,
      "loss": 0.002,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.7166897058486938,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0023,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.37664592266082764,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0022,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.36221975088119507,
      "learning_rate": 2.192e-05,
      "loss": 0.0022,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.2751234471797943,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.0021,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.6315683126449585,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0024,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.1604005992412567,
      "learning_rate": 2.188e-05,
      "loss": 0.0019,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.11287017911672592,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0023,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.12448272109031677,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0017,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.21755023300647736,
      "learning_rate": 2.184e-05,
      "loss": 0.0021,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.18510845303535461,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0023,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.30309996008872986,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0019,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.15975284576416016,
      "learning_rate": 2.18e-05,
      "loss": 0.0023,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.3210732340812683,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0021,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.2236429750919342,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0024,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.07819929718971252,
      "learning_rate": 2.176e-05,
      "loss": 0.0016,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.41076189279556274,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0022,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.10595134645700455,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0024,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.28551363945007324,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0023,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.12760323286056519,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.0024,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.3663161098957062,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0022,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.3116135597229004,
      "learning_rate": 2.168e-05,
      "loss": 0.0021,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.13141824305057526,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0035,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.8344037532806396,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0021,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.23349031805992126,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0033,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.3567836880683899,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0025,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.36609509587287903,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0027,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.20707659423351288,
      "learning_rate": 2.16e-05,
      "loss": 0.0018,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.07870199531316757,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0024,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.31470632553100586,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0022,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.10222138464450836,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0018,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.49292901158332825,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0025,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.3650418221950531,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0019,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.3192063271999359,
      "learning_rate": 2.152e-05,
      "loss": 0.0026,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.2546707093715668,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0022,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.7078961133956909,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0027,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.4272611737251282,
      "learning_rate": 2.148e-05,
      "loss": 0.002,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.11259205639362335,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0026,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.1566474586725235,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0025,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.27425700426101685,
      "learning_rate": 2.144e-05,
      "loss": 0.0021,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.28091418743133545,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0019,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.27799782156944275,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.0021,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.9170626401901245,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0026,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.3109307587146759,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0024,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.21230055391788483,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.002,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.21333874762058258,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0021,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.10846957564353943,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0024,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.40472158789634705,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0023,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.21852156519889832,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.0024,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.8722501993179321,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0021,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.7500683069229126,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.0024,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.09807447344064713,
      "learning_rate": 2.128e-05,
      "loss": 0.0019,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.2533894181251526,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.002,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.25478535890579224,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.0025,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.46085450053215027,
      "learning_rate": 2.124e-05,
      "loss": 0.0019,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.17225444316864014,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.0022,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.25977566838264465,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0027,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.5892214179039001,
      "learning_rate": 2.12e-05,
      "loss": 0.0022,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.6574661731719971,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.0024,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.1579214632511139,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0021,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.6134065389633179,
      "learning_rate": 2.116e-05,
      "loss": 0.0024,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.4021720588207245,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.0022,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.12541398406028748,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.002,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.2369975596666336,
      "learning_rate": 2.112e-05,
      "loss": 0.0017,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.24887308478355408,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0024,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.4179936945438385,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.0025,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.4390426576137543,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0021,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.16388535499572754,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0022,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.6408535242080688,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0024,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.39875200390815735,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0019,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.1756838709115982,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0028,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.3631575107574463,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.0023,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.14748771488666534,
      "learning_rate": 2.1e-05,
      "loss": 0.0023,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.11084848642349243,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.0023,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.10598881542682648,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.0025,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.4066712260246277,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.0021,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.2532399892807007,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0021,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.059697479009628296,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0018,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.39607661962509155,
      "learning_rate": 2.092e-05,
      "loss": 0.0025,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.7266004085540771,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.0018,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.06835935264825821,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0022,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.21549652516841888,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0017,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.08404863625764847,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0023,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.09652084112167358,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.0021,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.8532446026802063,
      "learning_rate": 2.084e-05,
      "loss": 0.0022,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.06867128610610962,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0018,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.47502586245536804,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0024,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.7559531927108765,
      "learning_rate": 2.08e-05,
      "loss": 0.0018,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.266769140958786,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.0026,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.2770003080368042,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.0021,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.07211896777153015,
      "learning_rate": 2.076e-05,
      "loss": 0.0021,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.32647424936294556,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0023,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.30253005027770996,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0023,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.22454959154129028,
      "learning_rate": 2.072e-05,
      "loss": 0.0019,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.08989833295345306,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0021,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.16470980644226074,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0022,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.4107253849506378,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.0027,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.33016934990882874,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0018,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.23082369565963745,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.002,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.16420742869377136,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0027,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.40560173988342285,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0024,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.21351176500320435,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.0025,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.3687443435192108,
      "learning_rate": 2.06e-05,
      "loss": 0.002,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.2989524006843567,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0023,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.3081878125667572,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0023,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.11271660029888153,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0024,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.13265632092952728,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0026,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.24021990597248077,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0021,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.361570805311203,
      "learning_rate": 2.052e-05,
      "loss": 0.0021,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.16684305667877197,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.002,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.3772679269313812,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.003,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.5839120149612427,
      "learning_rate": 2.048e-05,
      "loss": 0.0021,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.6679489612579346,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0027,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.36106738448143005,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.002,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.37218111753463745,
      "learning_rate": 2.044e-05,
      "loss": 0.0021,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.5663359761238098,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0023,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.2894690930843353,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0019,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.18089869618415833,
      "learning_rate": 2.04e-05,
      "loss": 0.002,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.528433620929718,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0024,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.46742555499076843,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0021,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.2484886348247528,
      "learning_rate": 2.036e-05,
      "loss": 0.0022,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.28405702114105225,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0021,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.15485279262065887,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0022,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.17830714583396912,
      "learning_rate": 2.032e-05,
      "loss": 0.0027,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.3747504651546478,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0018,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.3600958287715912,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.002,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.39916279911994934,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0023,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.38262224197387695,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0026,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.5470338463783264,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0022,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.5731862187385559,
      "learning_rate": 2.024e-05,
      "loss": 0.0026,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.0975489392876625,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0018,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.10065864771604538,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.002,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.4957258999347687,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0022,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.22493743896484375,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0022,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.19821983575820923,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0022,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.1245337575674057,
      "learning_rate": 2.016e-05,
      "loss": 0.0026,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.38838961720466614,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.002,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.360156774520874,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0025,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.12710373103618622,
      "learning_rate": 2.012e-05,
      "loss": 0.0022,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.164133682847023,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0023,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.19687700271606445,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0023,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.22570155560970306,
      "learning_rate": 2.008e-05,
      "loss": 0.0027,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.3027244508266449,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0024,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.3201736509799957,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.0023,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.25273749232292175,
      "learning_rate": 2.004e-05,
      "loss": 0.0024,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.23878327012062073,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.0026,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.07480957359075546,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0027,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.3148641586303711,
      "learning_rate": 2e-05,
      "loss": 0.0019,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.3011901378631592,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0023,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.28049638867378235,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0024,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.08073370903730392,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0019,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.2745897173881531,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.0019,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.6295948624610901,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0021,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.37545353174209595,
      "learning_rate": 1.992e-05,
      "loss": 0.0021,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.11686503887176514,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0021,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.2826458215713501,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.003,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.3706349730491638,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.0018,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.34491944313049316,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0027,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.2643212378025055,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0019,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.27599164843559265,
      "learning_rate": 1.984e-05,
      "loss": 0.0028,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.26501327753067017,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.0024,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.4775424599647522,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0021,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.7432648539543152,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0018,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.18304497003555298,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.002,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.15920665860176086,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0025,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.2558179795742035,
      "learning_rate": 1.976e-05,
      "loss": 0.0019,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.2129303514957428,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0017,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.4882243871688843,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0018,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.09016276895999908,
      "learning_rate": 1.972e-05,
      "loss": 0.0025,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.560965895652771,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.0021,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.26624342799186707,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0019,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.4961259365081787,
      "learning_rate": 1.968e-05,
      "loss": 0.0022,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.5105230212211609,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0023,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.40780144929885864,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0024,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.12044838815927505,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.0023,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.13945317268371582,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0027,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.5425463318824768,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0022,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.1299886554479599,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0022,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.23450158536434174,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0023,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.09334629029035568,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0022,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.31820911169052124,
      "learning_rate": 1.956e-05,
      "loss": 0.002,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.5012950301170349,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.0022,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.5094301104545593,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0023,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.42883118987083435,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.002,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.08943042904138565,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.002,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.27931487560272217,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0023,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.19196103513240814,
      "learning_rate": 1.948e-05,
      "loss": 0.0022,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.27791208028793335,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0019,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.5721384286880493,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0019,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.6483351588249207,
      "learning_rate": 1.944e-05,
      "loss": 0.0022,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.21351757645606995,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0026,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.13140088319778442,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.002,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.07163948565721512,
      "learning_rate": 1.94e-05,
      "loss": 0.0019,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.35786572098731995,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0023,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.07261494547128677,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0019,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.4395158886909485,
      "learning_rate": 1.936e-05,
      "loss": 0.0024,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.3432861268520355,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0021,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.1927575021982193,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0021,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.16692480444908142,
      "learning_rate": 1.932e-05,
      "loss": 0.0019,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.1277015507221222,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0022,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.1362907588481903,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0021,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.26378676295280457,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.0027,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.22712042927742004,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0023,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.40034219622612,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.0024,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.7250956296920776,
      "learning_rate": 1.924e-05,
      "loss": 0.0023,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.5397036075592041,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.0022,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.18749414384365082,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.002,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.1879204660654068,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0018,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.49879345297813416,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.0027,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.5957792401313782,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0022,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.3668595850467682,
      "learning_rate": 1.916e-05,
      "loss": 0.0024,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.396184504032135,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0024,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.12301598489284515,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0024,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.4893442392349243,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.002,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.568535566329956,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.0025,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.28792670369148254,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0017,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.4396422207355499,
      "learning_rate": 1.908e-05,
      "loss": 0.0028,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.32881826162338257,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0027,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.2964487373828888,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0027,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.23073409497737885,
      "learning_rate": 1.904e-05,
      "loss": 0.0018,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.1687052696943283,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0027,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.13746434450149536,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0024,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.24127785861492157,
      "learning_rate": 1.9e-05,
      "loss": 0.0018,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.2237083613872528,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0023,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.23615005612373352,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.0027,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.41053450107574463,
      "learning_rate": 1.896e-05,
      "loss": 0.0024,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.11974895000457764,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0024,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.2501678466796875,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0026,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.3270496726036072,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.0019,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.2193249762058258,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.0021,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.10754507035017014,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0021,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.0676417127251625,
      "learning_rate": 1.888e-05,
      "loss": 0.002,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.35921505093574524,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0017,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.35024264454841614,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.002,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.21186858415603638,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0023,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.1508798748254776,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0027,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.34480804204940796,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.0018,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.13407209515571594,
      "learning_rate": 1.88e-05,
      "loss": 0.0023,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.32292404770851135,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0021,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.5178793668746948,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0027,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.14489643275737762,
      "learning_rate": 1.876e-05,
      "loss": 0.0025,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.541389524936676,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.0021,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.6166738867759705,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0024,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.26906198263168335,
      "learning_rate": 1.872e-05,
      "loss": 0.0024,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.14027641713619232,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.002,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.09132487326860428,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0019,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.3705764412879944,
      "learning_rate": 1.868e-05,
      "loss": 0.0018,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.6201316714286804,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0021,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.4762980341911316,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0023,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.07547718286514282,
      "learning_rate": 1.864e-05,
      "loss": 0.0022,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.0831681564450264,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0023,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.2390647530555725,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0022,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.11688614636659622,
      "learning_rate": 1.86e-05,
      "loss": 0.002,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.07985308766365051,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0023,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.2750721275806427,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0019,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.31262117624282837,
      "learning_rate": 1.856e-05,
      "loss": 0.0017,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.6688013076782227,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.002,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.34620940685272217,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0021,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.051412783563137054,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0022,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.09917524456977844,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.0021,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.19189080595970154,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0017,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.1157093346118927,
      "learning_rate": 1.848e-05,
      "loss": 0.0026,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.6162824630737305,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0021,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.5339810848236084,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0022,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.43228814005851746,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0022,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.6090795397758484,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.0023,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.46512019634246826,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0022,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.7961805462837219,
      "learning_rate": 1.84e-05,
      "loss": 0.0021,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.2928970754146576,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0021,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.24751047790050507,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0021,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.32178956270217896,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0021,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.33002719283103943,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0025,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.12562337517738342,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0023,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.44934338331222534,
      "learning_rate": 1.832e-05,
      "loss": 0.0027,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.4131852388381958,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0023,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.36613479256629944,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0027,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.38013729453086853,
      "learning_rate": 1.828e-05,
      "loss": 0.0026,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.18912816047668457,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.002,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.25006985664367676,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.002,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.45820027589797974,
      "learning_rate": 1.824e-05,
      "loss": 0.002,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.18892110884189606,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.0025,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.1320774108171463,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0017,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.0872630625963211,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.002,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.21383315324783325,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0019,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.2746864855289459,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.002,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.8780685663223267,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0022,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.37905412912368774,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.0022,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.4571962356567383,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0024,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.35564908385276794,
      "learning_rate": 1.812e-05,
      "loss": 0.0025,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.6604182720184326,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.0027,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.33574366569519043,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0023,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.1383633315563202,
      "learning_rate": 1.808e-05,
      "loss": 0.0018,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.1318126618862152,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0019,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.15903009474277496,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0019,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.45603761076927185,
      "learning_rate": 1.804e-05,
      "loss": 0.0026,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.24947501718997955,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0024,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.33940690755844116,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0024,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.5094503164291382,
      "learning_rate": 1.8e-05,
      "loss": 0.0019,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.264466255903244,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.0024,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.5568182468414307,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0024,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.14789091050624847,
      "learning_rate": 1.796e-05,
      "loss": 0.0021,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.2870911955833435,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0023,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.1997518539428711,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0025,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.5234477519989014,
      "learning_rate": 1.792e-05,
      "loss": 0.0021,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.31004753708839417,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.002,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.25343388319015503,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0022,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.07687786221504211,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0019,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.2608357071876526,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0023,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.10044372826814651,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0024,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.21504124999046326,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0021,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.695399820804596,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.002,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.10922446846961975,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0019,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.05842532590031624,
      "learning_rate": 1.78e-05,
      "loss": 0.0017,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.5614511370658875,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0021,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.18167749047279358,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0019,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.21852649748325348,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.002,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.4126604497432709,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0024,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.32323235273361206,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0021,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.39840731024742126,
      "learning_rate": 1.772e-05,
      "loss": 0.0023,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.09253431111574173,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0022,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.11134148389101028,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.002,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.29390770196914673,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0023,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.10677268356084824,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0021,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.42489349842071533,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0018,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.15418067574501038,
      "learning_rate": 1.764e-05,
      "loss": 0.002,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.4782311022281647,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0024,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.13843078911304474,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0025,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.25700119137763977,
      "learning_rate": 1.76e-05,
      "loss": 0.0019,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.07210681587457657,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0021,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.22114577889442444,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.0019,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.5462508797645569,
      "learning_rate": 1.756e-05,
      "loss": 0.0018,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.4095517098903656,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.002,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.2070564329624176,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0021,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.1426243633031845,
      "learning_rate": 1.752e-05,
      "loss": 0.0021,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.36676427721977234,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.0019,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.7570556998252869,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0023,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.3456326723098755,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.002,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.3627362847328186,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0028,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.5396053194999695,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0017,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.6672742366790771,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.002,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.4063250720500946,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.002,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.5441983938217163,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0021,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.1375245451927185,
      "learning_rate": 1.74e-05,
      "loss": 0.0019,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.5721725821495056,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0026,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.14443258941173553,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0021,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.07535210996866226,
      "learning_rate": 1.736e-05,
      "loss": 0.0016,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.4131987392902374,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0021,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.06397348642349243,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0024,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.550477147102356,
      "learning_rate": 1.732e-05,
      "loss": 0.0023,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.37956640124320984,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0024,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.1416788250207901,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0017,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.09503104537725449,
      "learning_rate": 1.728e-05,
      "loss": 0.0028,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.24172045290470123,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0018,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.38497236371040344,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0023,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.21571266651153564,
      "learning_rate": 1.724e-05,
      "loss": 0.0027,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.4263554811477661,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.002,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.19378717243671417,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0021,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.5886544585227966,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0016,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.1864079385995865,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0019,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.3954397141933441,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.002,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.5613390803337097,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0019,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.38931477069854736,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0023,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.1172395721077919,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0026,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.39202621579170227,
      "learning_rate": 1.712e-05,
      "loss": 0.0021,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.5421990156173706,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0026,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.16847744584083557,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0021,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.7283613681793213,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0026,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.6157961487770081,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.002,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.3241139054298401,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.002,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.33974236249923706,
      "learning_rate": 1.704e-05,
      "loss": 0.0017,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.5787990093231201,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.002,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.3111398220062256,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0018,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.7180463671684265,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.002,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.12029827386140823,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0019,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.5873265862464905,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.0025,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.0711115151643753,
      "learning_rate": 1.696e-05,
      "loss": 0.0023,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.12712019681930542,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0027,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.16204899549484253,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0021,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.6828574538230896,
      "learning_rate": 1.692e-05,
      "loss": 0.0022,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.187290757894516,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0024,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.17342545092105865,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0028,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.15277278423309326,
      "learning_rate": 1.688e-05,
      "loss": 0.0016,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.1518530249595642,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0019,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.2757140100002289,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.002,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.3140721023082733,
      "learning_rate": 1.684e-05,
      "loss": 0.0023,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.18092621862888336,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0025,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.23651227355003357,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0021,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.3139210641384125,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0022,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.22193920612335205,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0024,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.22595322132110596,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.0023,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.19488808512687683,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.002,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.6923359036445618,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0021,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.08936238288879395,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0028,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.49745410680770874,
      "learning_rate": 1.672e-05,
      "loss": 0.002,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.5202582478523254,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0021,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.5291963219642639,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0018,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.12008290737867355,
      "learning_rate": 1.668e-05,
      "loss": 0.0021,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.48609229922294617,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0017,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.11400966346263885,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0018,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.10059473663568497,
      "learning_rate": 1.664e-05,
      "loss": 0.0024,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.45492687821388245,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0022,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.07402853667736053,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.002,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.5256865620613098,
      "learning_rate": 1.66e-05,
      "loss": 0.0018,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.3166734576225281,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0023,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.09324999153614044,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0025,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.14160317182540894,
      "learning_rate": 1.656e-05,
      "loss": 0.002,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.2795719504356384,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0021,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.39872536063194275,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.002,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.28259170055389404,
      "learning_rate": 1.652e-05,
      "loss": 0.0019,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.4202006757259369,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0021,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.20195139944553375,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0021,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.3226979076862335,
      "learning_rate": 1.648e-05,
      "loss": 0.002,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.28066322207450867,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0026,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.10911785066127777,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.002,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.3981262445449829,
      "learning_rate": 1.644e-05,
      "loss": 0.0023,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.14689378440380096,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.0022,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.2475890815258026,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0022,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.5319859981536865,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0019,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.5970376133918762,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.0026,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.4505263864994049,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0021,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.477089524269104,
      "learning_rate": 1.636e-05,
      "loss": 0.0018,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.6531283259391785,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0018,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.6145707964897156,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.002,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.35681992769241333,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0024,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.6502541899681091,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0019,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.42972567677497864,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.002,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.2363908588886261,
      "learning_rate": 1.628e-05,
      "loss": 0.0021,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.17829672992229462,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0019,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.16936379671096802,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0025,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.18059134483337402,
      "learning_rate": 1.624e-05,
      "loss": 0.0025,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.2964523434638977,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0022,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.2617286145687103,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.002,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.07162387669086456,
      "learning_rate": 1.62e-05,
      "loss": 0.0026,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.11897784471511841,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0017,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.6998069882392883,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.0027,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.7815226316452026,
      "learning_rate": 1.616e-05,
      "loss": 0.0019,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.6398736834526062,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0021,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.32592758536338806,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0019,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.161255344748497,
      "learning_rate": 1.612e-05,
      "loss": 0.0018,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.398355633020401,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0022,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.6073628664016724,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0019,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.07587125152349472,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0022,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.20018228888511658,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0018,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.5303324460983276,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0024,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.19410614669322968,
      "learning_rate": 1.604e-05,
      "loss": 0.0025,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.6166513562202454,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.0019,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.4715403616428375,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0021,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.1717505007982254,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.002,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.25438234210014343,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0021,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.07756657153367996,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0023,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.6733354330062866,
      "learning_rate": 1.596e-05,
      "loss": 0.0024,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.4426293671131134,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0025,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.205100879073143,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0023,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.9931985139846802,
      "learning_rate": 1.592e-05,
      "loss": 0.0022,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.150194451212883,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.0024,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.5026326179504395,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0025,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.11681391298770905,
      "learning_rate": 1.588e-05,
      "loss": 0.0021,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.11407676339149475,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0022,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.357185423374176,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0018,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.3102046847343445,
      "learning_rate": 1.584e-05,
      "loss": 0.0022,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.2775397002696991,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0019,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.05559913441538811,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0023,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.2052401304244995,
      "learning_rate": 1.58e-05,
      "loss": 0.0026,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.451188325881958,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0027,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.8205645084381104,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0022,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.15587083995342255,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0021,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.1656486988067627,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0019,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.4295734167098999,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0023,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.4478231966495514,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0027,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.4061031937599182,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.0023,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.3153311014175415,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0026,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.07398504763841629,
      "learning_rate": 1.568e-05,
      "loss": 0.0022,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.6926539540290833,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0019,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.14409539103507996,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0024,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.3962661921977997,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0018,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.07153952121734619,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0025,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.1322057545185089,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0019,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.407208114862442,
      "learning_rate": 1.56e-05,
      "loss": 0.0021,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.09158392995595932,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0019,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.4294391870498657,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0017,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.5176138877868652,
      "learning_rate": 1.556e-05,
      "loss": 0.0023,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.3000926375389099,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0026,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.34978359937667847,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0024,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.23997896909713745,
      "learning_rate": 1.552e-05,
      "loss": 0.0022,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.185115247964859,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.002,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.38263973593711853,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.0027,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.49717748165130615,
      "learning_rate": 1.548e-05,
      "loss": 0.0019,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.37582165002822876,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0019,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.2477240264415741,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0025,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.2087070643901825,
      "learning_rate": 1.544e-05,
      "loss": 0.0021,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.15340620279312134,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0021,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.09118632227182388,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.0021,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.3907945156097412,
      "learning_rate": 1.54e-05,
      "loss": 0.0019,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.10090267658233643,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0022,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.3544580638408661,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0018,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.2850346863269806,
      "learning_rate": 1.536e-05,
      "loss": 0.0023,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.3203813135623932,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0019,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.19974109530448914,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0024,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.13316528499126434,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0021,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.08642596751451492,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0019,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.36989039182662964,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.002,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.08177271485328674,
      "learning_rate": 1.528e-05,
      "loss": 0.002,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.13924984633922577,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0022,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.23125578463077545,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0024,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.3141464591026306,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0023,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.1696973443031311,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0024,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.4084415137767792,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.002,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.11675254255533218,
      "learning_rate": 1.52e-05,
      "loss": 0.0028,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.20422714948654175,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.002,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.12680058181285858,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0021,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.20305955410003662,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0019,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.18825693428516388,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0019,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.335915744304657,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0022,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.6502861380577087,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0027,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.45794469118118286,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0016,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.5898078680038452,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.0021,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.15548591315746307,
      "learning_rate": 1.508e-05,
      "loss": 0.0022,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.29423803091049194,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0017,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.17220883071422577,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0019,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.43538665771484375,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0028,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.4756465256214142,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0023,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.24891617894172668,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0023,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.7727137207984924,
      "learning_rate": 1.5e-05,
      "loss": 0.0022,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 0.24489101767539978,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.002,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.18727202713489532,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.002,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.46863120794296265,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.002,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.5707443952560425,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0023,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.5932445526123047,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.002,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.2009093314409256,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0019,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.3129251301288605,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0022,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.23941537737846375,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0021,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.13222631812095642,
      "learning_rate": 1.488e-05,
      "loss": 0.002,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.18515537679195404,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0021,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.18607831001281738,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0019,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.3119997978210449,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0024,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.2978612780570984,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0018,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.09924749284982681,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0023,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.17428110539913177,
      "learning_rate": 1.48e-05,
      "loss": 0.0021,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.42617151141166687,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.002,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.4247320890426636,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.002,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.38894617557525635,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0023,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.2804555296897888,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.002,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.33819782733917236,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0017,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.15705162286758423,
      "learning_rate": 1.472e-05,
      "loss": 0.0019,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.10700371116399765,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.002,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.6807828545570374,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.002,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.1777154952287674,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0025,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.16914212703704834,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0023,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.14969736337661743,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.0019,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.18436165153980255,
      "learning_rate": 1.464e-05,
      "loss": 0.0015,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.21907202899456024,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0022,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.13569164276123047,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.002,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.1317306011915207,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0027,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.27716347575187683,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0023,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.24318286776542664,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0017,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.3755074739456177,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0017,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.4316275715827942,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0019,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.2380792647600174,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0018,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.535749077796936,
      "learning_rate": 1.452e-05,
      "loss": 0.0021,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.24570123851299286,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.002,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.5425168871879578,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0024,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.08680283278226852,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.0024,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.11539608240127563,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0021,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.13671815395355225,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0019,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.4996374547481537,
      "learning_rate": 1.444e-05,
      "loss": 0.0022,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.5679935216903687,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.002,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.299445778131485,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0022,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.23047290742397308,
      "learning_rate": 1.44e-05,
      "loss": 0.0021,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.2712012827396393,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0022,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.10586165636777878,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.0021,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.21262551844120026,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0027,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.31129592657089233,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0021,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.6144589781761169,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0022,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.405001163482666,
      "learning_rate": 1.432e-05,
      "loss": 0.002,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.11174854636192322,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.0018,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.3750104010105133,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0023,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.7280989289283752,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0019,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.6616636514663696,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0023,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.09161382913589478,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0021,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.5704408288002014,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.002,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.09009529650211334,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0025,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.1080145537853241,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0022,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.19946542382240295,
      "learning_rate": 1.42e-05,
      "loss": 0.0022,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.23974697291851044,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0018,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.16425193846225739,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0021,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.4637364149093628,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.0026,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.8132288455963135,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0026,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.5230815410614014,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.002,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.19784918427467346,
      "learning_rate": 1.412e-05,
      "loss": 0.0033,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.37970271706581116,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0026,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.07313678413629532,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.0019,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.09764403849840164,
      "learning_rate": 1.408e-05,
      "loss": 0.002,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.5850331783294678,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0021,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.13024893403053284,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0019,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.35298725962638855,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0019,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.643501877784729,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0018,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.3891577124595642,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0021,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.5823084712028503,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0023,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.3703256845474243,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.0018,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.39515218138694763,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0026,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.3171064257621765,
      "learning_rate": 1.396e-05,
      "loss": 0.002,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.0950394943356514,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.002,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.08768635988235474,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0022,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.24209870398044586,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0019,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.10557782649993896,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0021,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.14144746959209442,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0017,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.2632431983947754,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0018,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.44044366478919983,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0024,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.16672126948833466,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0023,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.29006803035736084,
      "learning_rate": 1.384e-05,
      "loss": 0.002,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.5667153000831604,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0019,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.10864421725273132,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.002,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.4943835735321045,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0018,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.5563439726829529,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.002,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.09781438112258911,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0019,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.2014838606119156,
      "learning_rate": 1.376e-05,
      "loss": 0.0023,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.21669764816761017,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0024,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.437165230512619,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0021,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.4461941123008728,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0019,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.15791413187980652,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0018,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.6110793948173523,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0018,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.14873643219470978,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0021,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.19465085864067078,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0025,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.45587262511253357,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0022,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.42116779088974,
      "learning_rate": 1.364e-05,
      "loss": 0.0022,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.33818894624710083,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0021,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.6471620202064514,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0021,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.511070966720581,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0022,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.5448474884033203,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0021,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.36174845695495605,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0019,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.1519550234079361,
      "learning_rate": 1.356e-05,
      "loss": 0.0021,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.7015597224235535,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0022,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.09440546482801437,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0024,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.7289797067642212,
      "learning_rate": 1.352e-05,
      "loss": 0.003,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.4268510639667511,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0022,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.19557452201843262,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0026,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.1174737736582756,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0024,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.2301660031080246,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0024,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.4018261432647705,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.002,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.41276928782463074,
      "learning_rate": 1.344e-05,
      "loss": 0.0027,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.12366963177919388,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.0023,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.1239212304353714,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.002,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.10019709914922714,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0019,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.17390893399715424,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0023,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.17203213274478912,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0021,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.4952186942100525,
      "learning_rate": 1.336e-05,
      "loss": 0.002,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.41851675510406494,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.0019,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.5353403091430664,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0021,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.5246387720108032,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0022,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.6207577586174011,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0016,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.6346897482872009,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0022,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.4621768295764923,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.0019,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.3269931674003601,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0026,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.16895636916160583,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.0025,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.609782874584198,
      "learning_rate": 1.324e-05,
      "loss": 0.0022,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.6501682996749878,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0024,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.5288513898849487,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0018,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.5362038016319275,
      "learning_rate": 1.32e-05,
      "loss": 0.0018,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.5922290682792664,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0021,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.14523717761039734,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.0021,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.09874323755502701,
      "learning_rate": 1.316e-05,
      "loss": 0.0021,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.24119620025157928,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.0022,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.200684055685997,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.002,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.3056768774986267,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.002,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.5217915177345276,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.002,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.7703075408935547,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0025,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.09126979857683182,
      "learning_rate": 1.308e-05,
      "loss": 0.002,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.4561324715614319,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0025,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.3094758987426758,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0019,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.2745357155799866,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0021,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.28257834911346436,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0022,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.10983949154615402,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0024,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.4144716262817383,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0018,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.2567613124847412,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0022,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.1686735302209854,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0019,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.3516730070114136,
      "learning_rate": 1.296e-05,
      "loss": 0.0023,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.4881567656993866,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0021,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.2362096905708313,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0021,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.12635037302970886,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0019,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.46724584698677063,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0021,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.1489509493112564,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0019,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.1937539428472519,
      "learning_rate": 1.288e-05,
      "loss": 0.0022,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.42266204953193665,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0018,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.4183809161186218,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0019,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.10459281504154205,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.002,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.3483368456363678,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0021,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.6830787658691406,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0022,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.28760918974876404,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0016,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.5877931118011475,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0023,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.275958389043808,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.002,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.22161756455898285,
      "learning_rate": 1.276e-05,
      "loss": 0.0024,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.22593961656093597,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0021,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.15330356359481812,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0021,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.4647165834903717,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0022,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.32087743282318115,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0024,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.16488289833068848,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0018,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.4266265034675598,
      "learning_rate": 1.268e-05,
      "loss": 0.002,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.4728930592536926,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0017,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.12955614924430847,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0019,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.2765498757362366,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.002,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.25599074363708496,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.002,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.09896072000265121,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0017,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.10673030465841293,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.002,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.2574369013309479,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0019,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.20935048162937164,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0027,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.26485082507133484,
      "learning_rate": 1.256e-05,
      "loss": 0.002,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 0.15481775999069214,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.002,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.15596118569374084,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0019,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.1594650000333786,
      "learning_rate": 1.252e-05,
      "loss": 0.002,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.22562584280967712,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0018,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.6864680051803589,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.002,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.7799639105796814,
      "learning_rate": 1.248e-05,
      "loss": 0.0019,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.157944455742836,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0021,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.18892428278923035,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0021,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.25383660197257996,
      "learning_rate": 1.244e-05,
      "loss": 0.0021,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.23831580579280853,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0021,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.11866532266139984,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.002,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.07078070193529129,
      "learning_rate": 1.24e-05,
      "loss": 0.0021,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.48175227642059326,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.0019,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.37308192253112793,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0019,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.10049008578062057,
      "learning_rate": 1.236e-05,
      "loss": 0.0018,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.41062068939208984,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.0022,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.6135063767433167,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0019,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.3574424684047699,
      "learning_rate": 1.232e-05,
      "loss": 0.0019,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.9096850156784058,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.002,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.13905246555805206,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0017,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.4681828022003174,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.002,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.08749508112668991,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0025,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.4488687813282013,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0018,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.565672755241394,
      "learning_rate": 1.224e-05,
      "loss": 0.0023,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.2514471709728241,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0018,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.663347601890564,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.002,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.2673187255859375,
      "learning_rate": 1.22e-05,
      "loss": 0.0022,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.17324402928352356,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0021,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.22404752671718597,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0019,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.3721461892127991,
      "learning_rate": 1.216e-05,
      "loss": 0.0019,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.12771864235401154,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.002,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.4106215834617615,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.002,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.20204612612724304,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0022,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.49064141511917114,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0021,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.311807781457901,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0019,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.6300343871116638,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0021,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.2825740575790405,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0024,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.18143212795257568,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0027,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.1717378795146942,
      "learning_rate": 1.204e-05,
      "loss": 0.0019,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.17841266095638275,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.0022,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.2764165997505188,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0021,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.4250985085964203,
      "learning_rate": 1.2e-05,
      "loss": 0.0025,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.05040675774216652,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.002,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.14332503080368042,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0024,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.15583030879497528,
      "learning_rate": 1.196e-05,
      "loss": 0.0019,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.2670845687389374,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0018,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.551345705986023,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0026,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.2898399531841278,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0017,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.43180710077285767,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.0019,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.3946570158004761,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0026,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.15739339590072632,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0019,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.19697260856628418,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0023,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.16822874546051025,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0022,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.12308483570814133,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0026,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.41004684567451477,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0018,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.17772041261196136,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.0022,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.1269071251153946,
      "learning_rate": 1.18e-05,
      "loss": 0.0025,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.39226406812667847,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0016,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.47978726029396057,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0024,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.24242660403251648,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0025,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.4961217939853668,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0021,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.28854072093963623,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0022,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.34718137979507446,
      "learning_rate": 1.172e-05,
      "loss": 0.0019,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.16251924633979797,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0017,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.24599646031856537,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.002,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.5058217644691467,
      "learning_rate": 1.168e-05,
      "loss": 0.0019,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.45636528730392456,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0018,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.20455363392829895,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0021,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.539831280708313,
      "learning_rate": 1.164e-05,
      "loss": 0.0021,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.6909258961677551,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0018,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.4828324317932129,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0021,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.07339740544557571,
      "learning_rate": 1.16e-05,
      "loss": 0.002,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.06452108174562454,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0022,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.13548552989959717,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.0021,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.15139761567115784,
      "learning_rate": 1.156e-05,
      "loss": 0.0021,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.3935139775276184,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0018,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.15456132590770721,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0023,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 1.0090062618255615,
      "learning_rate": 1.152e-05,
      "loss": 0.0024,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.6090426445007324,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0027,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.2546672821044922,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0026,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.2359076589345932,
      "learning_rate": 1.148e-05,
      "loss": 0.0021,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.3162851333618164,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.002,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.22421380877494812,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0023,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.130921870470047,
      "learning_rate": 1.144e-05,
      "loss": 0.0021,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.09850412607192993,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.0024,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.09742990881204605,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0027,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.2113129198551178,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0026,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.8630459308624268,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.0022,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.08217941969633102,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0017,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.08867701143026352,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.0018,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.1280898004770279,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0022,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.1345948874950409,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0022,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.22134564816951752,
      "learning_rate": 1.132e-05,
      "loss": 0.002,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.2675313651561737,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0021,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.9306904077529907,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.0021,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.09523932635784149,
      "learning_rate": 1.128e-05,
      "loss": 0.0017,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.30015331506729126,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0018,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.5939954519271851,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0026,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.26630696654319763,
      "learning_rate": 1.124e-05,
      "loss": 0.0018,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.37188589572906494,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.002,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.5362701416015625,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0025,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.3293381631374359,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.002,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.4894331395626068,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0019,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.07104092836380005,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.002,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.07391958683729172,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.002,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.18873804807662964,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.002,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.20251305401325226,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.002,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.4399498999118805,
      "learning_rate": 1.112e-05,
      "loss": 0.0018,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.4281204640865326,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0019,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.215754896402359,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0021,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.2936767637729645,
      "learning_rate": 1.108e-05,
      "loss": 0.0019,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.7492520809173584,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0022,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.4138757884502411,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.002,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.27755117416381836,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.002,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.07905455678701401,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0018,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.1717854142189026,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0018,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.2828686535358429,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.002,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.528817892074585,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0019,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.25137779116630554,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.0019,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.3147914409637451,
      "learning_rate": 1.096e-05,
      "loss": 0.0021,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.47193124890327454,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0021,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.2298707515001297,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0018,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.24213604629039764,
      "learning_rate": 1.092e-05,
      "loss": 0.0023,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.2706499695777893,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0021,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.29139238595962524,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.0018,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.6944031119346619,
      "learning_rate": 1.088e-05,
      "loss": 0.0018,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.531875729560852,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0019,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.31934115290641785,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0025,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.37112194299697876,
      "learning_rate": 1.084e-05,
      "loss": 0.0018,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.8407958149909973,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0017,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.3825753927230835,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0023,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.38339531421661377,
      "learning_rate": 1.08e-05,
      "loss": 0.0018,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.4558873474597931,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.0027,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.23767608404159546,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.0018,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.22895337641239166,
      "learning_rate": 1.076e-05,
      "loss": 0.0023,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.48405033349990845,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.002,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.23385024070739746,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0023,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.1627681851387024,
      "learning_rate": 1.072e-05,
      "loss": 0.0026,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.2672397494316101,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0028,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.18426239490509033,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0022,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.24902620911598206,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0018,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.14934797585010529,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0019,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.11291718482971191,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0021,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.1252269148826599,
      "learning_rate": 1.064e-05,
      "loss": 0.0018,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.1578247994184494,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0019,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.5562676787376404,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0018,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.5266291499137878,
      "learning_rate": 1.06e-05,
      "loss": 0.0019,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.24069881439208984,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.002,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.16076411306858063,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0024,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.12977510690689087,
      "learning_rate": 1.056e-05,
      "loss": 0.002,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.0969129130244255,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0017,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.09470387548208237,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0018,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.5114444494247437,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0023,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.39461201429367065,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.0023,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.36607787013053894,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.0022,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.3523986041545868,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0022,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.20120419561862946,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.002,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.49651890993118286,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0021,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.331645131111145,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0019,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.5083891749382019,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0018,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.5487736463546753,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0022,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.2483062893152237,
      "learning_rate": 1.04e-05,
      "loss": 0.0023,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.3094048500061035,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.0021,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.28633517026901245,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0019,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.2942679226398468,
      "learning_rate": 1.036e-05,
      "loss": 0.0026,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.3283042311668396,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0025,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.6724362373352051,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0019,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.4350033700466156,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0024,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.15805545449256897,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0024,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.2823928892612457,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0028,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.2657862603664398,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0023,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.1607520878314972,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0021,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.18000207841396332,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0022,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.22875094413757324,
      "learning_rate": 1.024e-05,
      "loss": 0.0017,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.38864442706108093,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.0017,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.37100258469581604,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0021,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.08775069564580917,
      "learning_rate": 1.02e-05,
      "loss": 0.0018,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.1316329836845398,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0022,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.18297365307807922,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.0022,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.5466486811637878,
      "learning_rate": 1.016e-05,
      "loss": 0.0022,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.7019345164299011,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0021,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.6335154175758362,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0023,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.26471516489982605,
      "learning_rate": 1.012e-05,
      "loss": 0.0026,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.0808672159910202,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0016,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.30485260486602783,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0019,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.11385374516248703,
      "learning_rate": 1.008e-05,
      "loss": 0.0023,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.21843253076076508,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0025,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.09777447581291199,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0018,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.21854084730148315,
      "learning_rate": 1.004e-05,
      "loss": 0.0022,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.47466087341308594,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.0022,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.6257346868515015,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0019,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.0068752765655518,
      "learning_rate": 1e-05,
      "loss": 0.0022,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.18941274285316467,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0019,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.3601762652397156,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.002,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.3035276234149933,
      "learning_rate": 9.96e-06,
      "loss": 0.0019,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.12059690058231354,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.002,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.12880127131938934,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0026,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.19967947900295258,
      "learning_rate": 9.92e-06,
      "loss": 0.0023,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.2410702258348465,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.002,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.3293038606643677,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0023,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.18234653770923615,
      "learning_rate": 9.88e-06,
      "loss": 0.0024,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.12181009352207184,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0017,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.1482013463973999,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0029,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.3079131841659546,
      "learning_rate": 9.84e-06,
      "loss": 0.002,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.1603488326072693,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0023,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.2811625003814697,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0019,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.38970836997032166,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0021,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.11351865530014038,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0018,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.3019576668739319,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.0018,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.1710696518421173,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0017,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.16572657227516174,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.002,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.25618648529052734,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0022,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.10569494217634201,
      "learning_rate": 9.72e-06,
      "loss": 0.002,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.09047241508960724,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0017,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.26189014315605164,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.002,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.7961776852607727,
      "learning_rate": 9.68e-06,
      "loss": 0.0022,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.3532499670982361,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0024,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.6409902572631836,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0018,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.6035513877868652,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.0027,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.22687143087387085,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.0018,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.06445744633674622,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0019,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.33764246106147766,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.002,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.14411823451519012,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0019,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.4447019696235657,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0018,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.12039077281951904,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.002,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.192007914185524,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.002,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.292316734790802,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0021,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.07526201009750366,
      "learning_rate": 9.52e-06,
      "loss": 0.002,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.11560327559709549,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0017,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.3034789562225342,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0021,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.8325226902961731,
      "learning_rate": 9.48e-06,
      "loss": 0.0022,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.2163529396057129,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0019,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.06901990622282028,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0017,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.2323881834745407,
      "learning_rate": 9.44e-06,
      "loss": 0.0021,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.10912979394197464,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.0018,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.5417894124984741,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0017,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.2982082962989807,
      "learning_rate": 9.4e-06,
      "loss": 0.0019,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.6114134788513184,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0022,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.47642984986305237,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0026,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.10281319171190262,
      "learning_rate": 9.36e-06,
      "loss": 0.0021,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.09245097637176514,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0017,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.28412386775016785,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0019,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.12600071728229523,
      "learning_rate": 9.32e-06,
      "loss": 0.0021,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.20516785979270935,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0022,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.43464478850364685,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.002,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.24746666848659515,
      "learning_rate": 9.28e-06,
      "loss": 0.0022,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.36216607689857483,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0018,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.4671943485736847,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0019,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.141648069024086,
      "learning_rate": 9.24e-06,
      "loss": 0.0019,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.24950264394283295,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0019,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.06760689616203308,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0026,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.23634611070156097,
      "learning_rate": 9.2e-06,
      "loss": 0.0028,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.22397483885288239,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.0021,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.4522673189640045,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0018,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.32150596380233765,
      "learning_rate": 9.16e-06,
      "loss": 0.0021,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.08919686824083328,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.002,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.29880350828170776,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0021,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.21272866427898407,
      "learning_rate": 9.12e-06,
      "loss": 0.0021,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.34044313430786133,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0021,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.15736331045627594,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0022,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.5618748068809509,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.002,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.1858544796705246,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0021,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.64261394739151,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.0025,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.344275563955307,
      "learning_rate": 9.04e-06,
      "loss": 0.0019,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.3828525245189667,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.002,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.2642262876033783,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0024,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.10589508712291718,
      "learning_rate": 9e-06,
      "loss": 0.0023,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.34636595845222473,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0022,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.6182767152786255,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0017,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.07366146892309189,
      "learning_rate": 8.96e-06,
      "loss": 0.0026,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.09986609220504761,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.002,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.2783070206642151,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0021,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.22844929993152618,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.002,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.11995898187160492,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.0018,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.4440394341945648,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.0016,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.25907471776008606,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0022,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.31373709440231323,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0018,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.3913775384426117,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.0021,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.4579050540924072,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0022,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.12187173962593079,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0018,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.07279080897569656,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0018,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.15705734491348267,
      "learning_rate": 8.8e-06,
      "loss": 0.0017,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.16909661889076233,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0023,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.27413210272789,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0022,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.2094058096408844,
      "learning_rate": 8.76e-06,
      "loss": 0.0027,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.18824584782123566,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0018,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.14182934165000916,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0023,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.09592181444168091,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0023,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.09631916880607605,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0022,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.8652991056442261,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0024,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.11980215460062027,
      "learning_rate": 8.68e-06,
      "loss": 0.0017,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.2184988111257553,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0023,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.17989301681518555,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0024,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.38674384355545044,
      "learning_rate": 8.64e-06,
      "loss": 0.0029,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.28058236837387085,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0019,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.36474940180778503,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0021,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.1026776134967804,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0021,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.22631336748600006,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0018,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.14562645554542542,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0021,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.620553195476532,
      "learning_rate": 8.56e-06,
      "loss": 0.0016,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.56766277551651,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.002,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.44148024916648865,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0019,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.11651581525802612,
      "learning_rate": 8.52e-06,
      "loss": 0.0019,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.3952835500240326,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0019,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.33307403326034546,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0019,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.2152191549539566,
      "learning_rate": 8.48e-06,
      "loss": 0.0019,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.06678394973278046,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.002,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.37028631567955017,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0024,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.05802498757839203,
      "learning_rate": 8.44e-06,
      "loss": 0.0019,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.13502255082130432,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0021,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.12103282660245895,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0021,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.29318442940711975,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0022,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.15163302421569824,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0018,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.48887890577316284,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0023,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.5546231269836426,
      "learning_rate": 8.36e-06,
      "loss": 0.0022,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.33611181378364563,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.0018,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.14103835821151733,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.002,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.2773790955543518,
      "learning_rate": 8.32e-06,
      "loss": 0.0019,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.12890177965164185,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0021,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.44429922103881836,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0018,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.12292671948671341,
      "learning_rate": 8.28e-06,
      "loss": 0.0025,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.3040015697479248,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.002,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.11004936695098877,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.0017,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.4817564785480499,
      "learning_rate": 8.24e-06,
      "loss": 0.0023,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.5627838969230652,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.002,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.2703559100627899,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0027,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.17965862154960632,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0021,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.4105070233345032,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0017,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.11629433184862137,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0022,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.11476338654756546,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0019,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.08261317014694214,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0017,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.354788213968277,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.003,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.20514126121997833,
      "learning_rate": 8.12e-06,
      "loss": 0.0021,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.07548678666353226,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0022,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.2866764962673187,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0027,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.050425924360752106,
      "learning_rate": 8.08e-06,
      "loss": 0.0019,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.22217142581939697,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.002,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.10022950172424316,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0021,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.3691975474357605,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0016,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.3134831488132477,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0021,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.2766988277435303,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0021,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.08815348148345947,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0022,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.07455163449048996,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.0018,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.45020902156829834,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0018,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.6760249137878418,
      "learning_rate": 7.96e-06,
      "loss": 0.0021,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.20208902657032013,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0021,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.1618586629629135,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0018,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.28101322054862976,
      "learning_rate": 7.92e-06,
      "loss": 0.0022,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.39311957359313965,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0023,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.8767569661140442,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.002,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.2392224818468094,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0018,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.6671572923660278,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0018,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.2996756434440613,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.0019,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.3345714211463928,
      "learning_rate": 7.84e-06,
      "loss": 0.0019,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.1907361000776291,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.0018,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.22873598337173462,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.002,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.27054139971733093,
      "learning_rate": 7.8e-06,
      "loss": 0.0023,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.1345299929380417,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.002,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.10481361299753189,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.002,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.26959288120269775,
      "learning_rate": 7.76e-06,
      "loss": 0.0018,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.14442883431911469,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0019,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.19315685331821442,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.002,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.11176558583974838,
      "learning_rate": 7.72e-06,
      "loss": 0.002,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.08995665609836578,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0019,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.11783140152692795,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.002,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.6646156311035156,
      "learning_rate": 7.68e-06,
      "loss": 0.0021,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.24908891320228577,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.002,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.7994722127914429,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0025,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.4518556296825409,
      "learning_rate": 7.64e-06,
      "loss": 0.0023,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.7317222952842712,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0017,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.24773654341697693,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0019,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.5045248866081238,
      "learning_rate": 7.6e-06,
      "loss": 0.002,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.30201077461242676,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0018,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.16378216445446014,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0017,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.8286468386650085,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0019,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.3726029098033905,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.002,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.5313443541526794,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.002,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.18959271907806396,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0019,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.09220089018344879,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0019,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.14421069622039795,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.002,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.36390453577041626,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0022,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.05519978702068329,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.002,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.13507620990276337,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0016,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.24411222338676453,
      "learning_rate": 7.44e-06,
      "loss": 0.0016,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.1367146372795105,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0018,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.14110895991325378,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0021,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.4608426094055176,
      "learning_rate": 7.4e-06,
      "loss": 0.0019,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.5223991274833679,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.002,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.5424433350563049,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0026,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.1529294103384018,
      "learning_rate": 7.36e-06,
      "loss": 0.0019,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.26246124505996704,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0022,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.480774849653244,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0021,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.12417279928922653,
      "learning_rate": 7.32e-06,
      "loss": 0.0021,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.33965036273002625,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0025,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.3098642826080322,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0019,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.300790011882782,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.002,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.17570243775844574,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0021,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.1842743307352066,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.002,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.3233303725719452,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.0023,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.2323533594608307,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.0022,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.7092322707176208,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0026,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.19788306951522827,
      "learning_rate": 7.2e-06,
      "loss": 0.0018,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.34516212344169617,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0021,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.09352145344018936,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.002,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.3507632911205292,
      "learning_rate": 7.16e-06,
      "loss": 0.0021,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.18951982259750366,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0018,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.4147944450378418,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0018,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.37841853499412537,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.0017,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.5215842723846436,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.0025,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.42871662974357605,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0022,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.5062082409858704,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0024,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.24527354538440704,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0024,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.32424408197402954,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0019,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.2990923523902893,
      "learning_rate": 7.04e-06,
      "loss": 0.0018,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.27857697010040283,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0023,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.13175177574157715,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.0019,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.8075593113899231,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.002,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.17135314643383026,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0019,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.2054031640291214,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0026,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.46312621235847473,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0017,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.4245748519897461,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0018,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.43862390518188477,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0021,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.2424813061952591,
      "learning_rate": 6.92e-06,
      "loss": 0.002,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.2993994951248169,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.002,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.21221669018268585,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0018,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.18719616532325745,
      "learning_rate": 6.88e-06,
      "loss": 0.0019,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.16136601567268372,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.002,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.3081150949001312,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.002,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.22730322182178497,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.002,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.1873849779367447,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.002,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.45084232091903687,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0021,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.46530547738075256,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.002,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.48403722047805786,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.002,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.5967892408370972,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0024,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.15931643545627594,
      "learning_rate": 6.76e-06,
      "loss": 0.0021,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.10723686963319778,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.0022,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.32865267992019653,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0024,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.14139389991760254,
      "learning_rate": 6.72e-06,
      "loss": 0.002,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.2719261646270752,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.0016,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.16647160053253174,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0023,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.3150467872619629,
      "learning_rate": 6.68e-06,
      "loss": 0.0021,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.6380148530006409,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0022,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.08334904909133911,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.0018,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.5006293654441833,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0023,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.10346124321222305,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0021,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.45665693283081055,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.0024,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.4884493947029114,
      "learning_rate": 6.6e-06,
      "loss": 0.0019,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.36689141392707825,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0022,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.14303307235240936,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0016,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.14364300668239594,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0019,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.5606977939605713,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0019,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.29192835092544556,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.002,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.20421230792999268,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.0021,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.10015407204627991,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0021,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.4728330969810486,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0023,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.09095055609941483,
      "learning_rate": 6.48e-06,
      "loss": 0.0029,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.12440375238656998,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0021,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.4419271945953369,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0023,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.2453286051750183,
      "learning_rate": 6.44e-06,
      "loss": 0.0018,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.32417792081832886,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.002,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.25620096921920776,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.002,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.3491126298904419,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0024,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.2405204325914383,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.002,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.09472969174385071,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.002,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.16475161910057068,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0019,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.08682236075401306,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0022,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.356010764837265,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0016,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.07187449932098389,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0016,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.3109511435031891,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0024,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.49917343258857727,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.0021,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.332507461309433,
      "learning_rate": 6.28e-06,
      "loss": 0.0017,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.10323917865753174,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0019,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.21973615884780884,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0022,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.2600556015968323,
      "learning_rate": 6.24e-06,
      "loss": 0.002,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.30025792121887207,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0018,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.24725927412509918,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0023,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.43458154797554016,
      "learning_rate": 6.2e-06,
      "loss": 0.002,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.19238020479679108,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.002,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.19563253223896027,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0018,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.12795770168304443,
      "learning_rate": 6.16e-06,
      "loss": 0.0022,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.12432708591222763,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0018,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.2914305627346039,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0021,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.23827888071537018,
      "learning_rate": 6.12e-06,
      "loss": 0.002,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.4420660734176636,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0018,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.23022595047950745,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0022,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.1234336569905281,
      "learning_rate": 6.08e-06,
      "loss": 0.0019,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.23597201704978943,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0023,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.12851887941360474,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0023,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.08912546187639236,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0019,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.26125067472457886,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.002,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.3988651931285858,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0021,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.19854162633419037,
      "learning_rate": 6e-06,
      "loss": 0.0018,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.3114532232284546,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.002,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.3286856412887573,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.0018,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.16605135798454285,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0022,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.2219489961862564,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0016,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.42698317766189575,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0019,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.22238163650035858,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0019,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.3595171868801117,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0019,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.40403473377227783,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0019,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.22747252881526947,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0017,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.11493825912475586,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0024,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.33158260583877563,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0021,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.2529512643814087,
      "learning_rate": 5.84e-06,
      "loss": 0.0018,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.20785143971443176,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0021,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.16146498918533325,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0021,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.15992920100688934,
      "learning_rate": 5.8e-06,
      "loss": 0.002,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.12359362840652466,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0019,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.130599707365036,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0025,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.1895165592432022,
      "learning_rate": 5.76e-06,
      "loss": 0.0019,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.18396516144275665,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.002,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.5170176029205322,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0019,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.09614044427871704,
      "learning_rate": 5.72e-06,
      "loss": 0.0017,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.3959697484970093,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0021,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.34788528084754944,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0018,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.13907627761363983,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0018,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.5041399002075195,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.002,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.18489228188991547,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.0018,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.19132132828235626,
      "learning_rate": 5.64e-06,
      "loss": 0.0021,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.09417887032032013,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.002,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.18637791275978088,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0017,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.08696799725294113,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0019,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.18801310658454895,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0019,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.07513430714607239,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.002,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.08856423944234848,
      "learning_rate": 5.56e-06,
      "loss": 0.0018,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.14045631885528564,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0022,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.3019501864910126,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0021,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.2234085351228714,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0023,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.34662067890167236,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0024,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.28496333956718445,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0017,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.12473265826702118,
      "learning_rate": 5.48e-06,
      "loss": 0.0021,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.2725365459918976,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0023,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.11817163228988647,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.002,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.1575901061296463,
      "learning_rate": 5.44e-06,
      "loss": 0.0022,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.2531168758869171,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.0018,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.11788497865200043,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0021,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.10286914557218552,
      "learning_rate": 5.4e-06,
      "loss": 0.0017,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.1270892322063446,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0019,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.09656422585248947,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0021,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.3948962688446045,
      "learning_rate": 5.36e-06,
      "loss": 0.002,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.13675636053085327,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0018,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.2633323669433594,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0018,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.2890063226222992,
      "learning_rate": 5.32e-06,
      "loss": 0.0027,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.43598318099975586,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0018,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.38548505306243896,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0027,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.13448311388492584,
      "learning_rate": 5.28e-06,
      "loss": 0.0022,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.3486971855163574,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0022,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.22141210734844208,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0018,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.2782880663871765,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.002,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.27141356468200684,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.0021,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.18299324810504913,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0022,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.11449732631444931,
      "learning_rate": 5.2e-06,
      "loss": 0.002,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.25529006123542786,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0022,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.5546436905860901,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.002,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.14199042320251465,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0018,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.4490300118923187,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0019,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.31840184330940247,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.002,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.1308242380619049,
      "learning_rate": 5.12e-06,
      "loss": 0.0018,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.4060133397579193,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0022,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.3170015811920166,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0018,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.39194270968437195,
      "learning_rate": 5.08e-06,
      "loss": 0.0021,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.36810094118118286,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0024,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.31395143270492554,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.0018,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.17425617575645447,
      "learning_rate": 5.04e-06,
      "loss": 0.0022,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.3509622812271118,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.002,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.16960136592388153,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0018,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.5145726203918457,
      "learning_rate": 5e-06,
      "loss": 0.0017,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.3926681578159332,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0024,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.34835630655288696,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0018,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.10126853734254837,
      "learning_rate": 4.96e-06,
      "loss": 0.0019,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.0842403918504715,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.0019,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.26522296667099,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0019,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.32148265838623047,
      "learning_rate": 4.92e-06,
      "loss": 0.0023,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.4077736735343933,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0018,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.21903382241725922,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.0018,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.4156988561153412,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.0024,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.11470276117324829,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0018,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.1620541214942932,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0016,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.33287370204925537,
      "learning_rate": 4.84e-06,
      "loss": 0.0019,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.16293667256832123,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0019,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.1371515691280365,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0021,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.5053499937057495,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0021,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.09445984661579132,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.0018,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.18394677340984344,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.002,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.5098686814308167,
      "learning_rate": 4.76e-06,
      "loss": 0.0024,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.20427946746349335,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.0022,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.09561075270175934,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.002,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.19496305286884308,
      "learning_rate": 4.72e-06,
      "loss": 0.0017,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.35284140706062317,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0016,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.2643706202507019,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0022,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.261037141084671,
      "learning_rate": 4.68e-06,
      "loss": 0.002,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.1993129402399063,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0019,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.10309923440217972,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0017,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.2158154845237732,
      "learning_rate": 4.64e-06,
      "loss": 0.0017,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.5973289608955383,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0018,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.3196066617965698,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.0021,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.22039099037647247,
      "learning_rate": 4.6e-06,
      "loss": 0.0018,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.1165001317858696,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0022,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.9119405150413513,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0022,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.17575858533382416,
      "learning_rate": 4.56e-06,
      "loss": 0.0018,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.25106334686279297,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0017,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.2339620590209961,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0018,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.1457739919424057,
      "learning_rate": 4.52e-06,
      "loss": 0.002,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.629342257976532,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.002,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.11429634690284729,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.0018,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.13765108585357666,
      "learning_rate": 4.48e-06,
      "loss": 0.0021,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.2842731773853302,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.002,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.08047892153263092,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0018,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.7576802968978882,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0021,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.16724556684494019,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0018,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.10664279758930206,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0017,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.4207485318183899,
      "learning_rate": 4.4e-06,
      "loss": 0.0021,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.36550042033195496,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0021,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.18052417039871216,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.002,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.31429773569107056,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0017,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.13544481992721558,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0021,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.12471221387386322,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0022,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.26186805963516235,
      "learning_rate": 4.32e-06,
      "loss": 0.0026,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.18179981410503387,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0019,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.27610477805137634,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0022,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.25624707341194153,
      "learning_rate": 4.28e-06,
      "loss": 0.0019,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.21090495586395264,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0019,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.14722225069999695,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0019,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.4667297601699829,
      "learning_rate": 4.24e-06,
      "loss": 0.002,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.23344407975673676,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.0019,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.2391715943813324,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0022,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.19217821955680847,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0021,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.11477899551391602,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.002,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.4030233323574066,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0018,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.24795910716056824,
      "learning_rate": 4.16e-06,
      "loss": 0.0018,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.18351337313652039,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0023,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.271201491355896,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0019,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.1450035125017166,
      "learning_rate": 4.12e-06,
      "loss": 0.0019,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.12779557704925537,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0018,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.08235874772071838,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.002,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.4548807144165039,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0019,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.3457743525505066,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0019,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.5858997106552124,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0019,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.09168347716331482,
      "learning_rate": 4.04e-06,
      "loss": 0.0017,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.346357136964798,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0021,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.23538711667060852,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0022,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.4533325135707855,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0018,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.11530599743127823,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0017,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.5816370248794556,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0023,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.3157922625541687,
      "learning_rate": 3.96e-06,
      "loss": 0.0017,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.2912016212940216,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0018,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.5046675205230713,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0016,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.20609192550182343,
      "learning_rate": 3.92e-06,
      "loss": 0.002,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.46711477637290955,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0017,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.6686623692512512,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0022,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.26726657152175903,
      "learning_rate": 3.88e-06,
      "loss": 0.0019,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.27375364303588867,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.002,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.228728249669075,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0021,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.10357200354337692,
      "learning_rate": 3.84e-06,
      "loss": 0.0019,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.1060885339975357,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0019,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.6629629731178284,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0024,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.3867860734462738,
      "learning_rate": 3.8e-06,
      "loss": 0.0022,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.08828891068696976,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0017,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.06540276855230331,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0021,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.29962095618247986,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0017,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.1531040519475937,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0024,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.123013935983181,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0024,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.5398683547973633,
      "learning_rate": 3.72e-06,
      "loss": 0.0019,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.25375881791114807,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0022,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.1659242957830429,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0017,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.3743259906768799,
      "learning_rate": 3.68e-06,
      "loss": 0.0021,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.2837750017642975,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0019,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.3296162188053131,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0019,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.12344085425138474,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.002,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.5812015533447266,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0022,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.10278061777353287,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.002,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.2783910036087036,
      "learning_rate": 3.6e-06,
      "loss": 0.0017,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.13010457158088684,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.002,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.3612631559371948,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.002,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.5490548610687256,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0024,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.09805071353912354,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0023,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.1797059029340744,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0021,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.1926564872264862,
      "learning_rate": 3.52e-06,
      "loss": 0.002,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.1704368144273758,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0017,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.29108524322509766,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.002,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.7235918641090393,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0018,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.5171946883201599,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0016,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.260163277387619,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0021,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.07267016917467117,
      "learning_rate": 3.44e-06,
      "loss": 0.0019,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.6881005764007568,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0015,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.3769375681877136,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.002,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.22734467685222626,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0017,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.2832717299461365,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0019,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.20488570630550385,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0025,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.2051861435174942,
      "learning_rate": 3.36e-06,
      "loss": 0.0017,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.5253251791000366,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0021,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.42225226759910583,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0027,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.3560294806957245,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0016,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.3462381660938263,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0019,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.4957176744937897,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0021,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.07174748182296753,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.002,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.24009943008422852,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0019,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.2762536406517029,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.0018,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.440228670835495,
      "learning_rate": 3.24e-06,
      "loss": 0.0019,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.49612629413604736,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0021,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.12082540988922119,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0018,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.1347307413816452,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0017,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.12550856173038483,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.0018,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.14111441373825073,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0026,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.31961509585380554,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.002,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.30385512113571167,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0021,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.22274957597255707,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0019,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.0775131806731224,
      "learning_rate": 3.12e-06,
      "loss": 0.0018,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.5788167715072632,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0018,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.07907942682504654,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0026,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.10764317959547043,
      "learning_rate": 3.08e-06,
      "loss": 0.0021,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.18813352286815643,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0021,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.3411843180656433,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0016,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.5673823356628418,
      "learning_rate": 3.04e-06,
      "loss": 0.002,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.06410270929336548,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0015,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.11977284401655197,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0019,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.7124152183532715,
      "learning_rate": 3e-06,
      "loss": 0.0021,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.05480877310037613,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0021,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.17838044464588165,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0023,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.23039467632770538,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.002,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.29821082949638367,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0021,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.6079505085945129,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.002,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.11409546434879303,
      "learning_rate": 2.92e-06,
      "loss": 0.0019,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.10153103619813919,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0022,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.2140423059463501,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0019,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.407962441444397,
      "learning_rate": 2.88e-06,
      "loss": 0.0019,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.2611224949359894,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0019,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.457071989774704,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0021,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.17999309301376343,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0019,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.3119809627532959,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0017,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.2858998477458954,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0022,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.35017427802085876,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.002,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.18978120386600494,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.002,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.09746981412172318,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0019,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.3364581763744354,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0018,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.40127354860305786,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0019,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.1540205478668213,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0018,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.40003424882888794,
      "learning_rate": 2.72e-06,
      "loss": 0.0022,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.33748260140419006,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0021,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.14228326082229614,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0024,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.4057393968105316,
      "learning_rate": 2.68e-06,
      "loss": 0.0019,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.4528258144855499,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.002,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.14914816617965698,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0022,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.09903551638126373,
      "learning_rate": 2.64e-06,
      "loss": 0.0017,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.27770018577575684,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0021,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.26477327942848206,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0018,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.26114434003829956,
      "learning_rate": 2.6e-06,
      "loss": 0.0017,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.11651009321212769,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.0019,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.3479701578617096,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0021,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.18618616461753845,
      "learning_rate": 2.56e-06,
      "loss": 0.0016,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.19144460558891296,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0019,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.060593534260988235,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0022,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.3366294205188751,
      "learning_rate": 2.52e-06,
      "loss": 0.0021,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.11025162786245346,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0023,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.11880772560834885,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0021,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.35325169563293457,
      "learning_rate": 2.48e-06,
      "loss": 0.0019,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.16742448508739471,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0023,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.12656019628047943,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0019,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.24418456852436066,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0018,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.37069231271743774,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0021,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.32625648379325867,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0021,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.29814228415489197,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0018,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.377227246761322,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0019,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.2687036991119385,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0018,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.41009289026260376,
      "learning_rate": 2.36e-06,
      "loss": 0.002,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.4241992235183716,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0018,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.45411497354507446,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.002,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.18955686688423157,
      "learning_rate": 2.32e-06,
      "loss": 0.0019,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.4369554817676544,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.0021,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.3798661231994629,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0017,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.4465138912200928,
      "learning_rate": 2.28e-06,
      "loss": 0.0022,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.10981293022632599,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.002,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.20883013308048248,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0019,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.28847289085388184,
      "learning_rate": 2.24e-06,
      "loss": 0.002,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.10433711856603622,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0019,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.28794318437576294,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0017,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.42562374472618103,
      "learning_rate": 2.2e-06,
      "loss": 0.002,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.23809997737407684,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0018,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.14103999733924866,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.002,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.12584610283374786,
      "learning_rate": 2.16e-06,
      "loss": 0.002,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.1461721956729889,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0019,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.1806277483701706,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0017,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.15635107457637787,
      "learning_rate": 2.12e-06,
      "loss": 0.0017,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.20959898829460144,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0018,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.17011037468910217,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0022,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.08426199108362198,
      "learning_rate": 2.08e-06,
      "loss": 0.0021,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.3635301887989044,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0018,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.09884117543697357,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.0017,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.2829648554325104,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0017,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.5841019749641418,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.002,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.2685597240924835,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0018,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.1694604903459549,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0017,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.1528046578168869,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.002,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.142994686961174,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.002,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.36597731709480286,
      "learning_rate": 1.96e-06,
      "loss": 0.0024,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.0717732161283493,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0018,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.24696631729602814,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0021,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.07979099452495575,
      "learning_rate": 1.92e-06,
      "loss": 0.0023,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.15605488419532776,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0021,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.38039883971214294,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0023,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.3649665415287018,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0018,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.13934774696826935,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.002,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.2993277907371521,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0018,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.1927531659603119,
      "learning_rate": 1.84e-06,
      "loss": 0.0019,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.6617767810821533,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0021,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.13070456683635712,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.0017,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.3784932792186737,
      "learning_rate": 1.8e-06,
      "loss": 0.0026,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.349570095539093,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0026,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.4325049817562103,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0019,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.10978798568248749,
      "learning_rate": 1.76e-06,
      "loss": 0.0018,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.5638564825057983,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.002,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.28778353333473206,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0018,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.37296026945114136,
      "learning_rate": 1.72e-06,
      "loss": 0.0018,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.18083424866199493,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0019,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.1440107673406601,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0017,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.0892816111445427,
      "learning_rate": 1.68e-06,
      "loss": 0.002,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.27494826912879944,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0018,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.42432254552841187,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.002,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.16088667511940002,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0019,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.2070876508951187,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.0019,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.5435220003128052,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.002,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.34228911995887756,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.002,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.4673965275287628,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0019,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.12053826451301575,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0019,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.4450986087322235,
      "learning_rate": 1.56e-06,
      "loss": 0.0019,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.48376885056495667,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0018,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.27279072999954224,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0022,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.12944790720939636,
      "learning_rate": 1.52e-06,
      "loss": 0.0018,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.2338152527809143,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0023,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.157537579536438,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0023,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.3137216567993164,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0018,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.617580235004425,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.002,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.44395339488983154,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.0023,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.16172155737876892,
      "learning_rate": 1.44e-06,
      "loss": 0.0021,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.12819446623325348,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.0017,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.16237816214561462,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0018,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.47258231043815613,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0019,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.2882704436779022,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.0018,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.1417742669582367,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0015,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.16292211413383484,
      "learning_rate": 1.36e-06,
      "loss": 0.0019,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.15508590638637543,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0019,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.34958407282829285,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.002,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.2628640830516815,
      "learning_rate": 1.32e-06,
      "loss": 0.0017,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.15317432582378387,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0019,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.2692907452583313,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0021,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.40190306305885315,
      "learning_rate": 1.28e-06,
      "loss": 0.002,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.08641629666090012,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0017,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.3937503397464752,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.002,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.1017744168639183,
      "learning_rate": 1.24e-06,
      "loss": 0.0017,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.2331751435995102,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0018,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.21655243635177612,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.002,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.12387760728597641,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0017,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.13600735366344452,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0021,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.3698521852493286,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0017,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.28369131684303284,
      "learning_rate": 1.16e-06,
      "loss": 0.0016,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.0589982233941555,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.0017,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.12589356303215027,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0024,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.15206776559352875,
      "learning_rate": 1.12e-06,
      "loss": 0.0019,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.07223479449748993,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.0022,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.25064554810523987,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.002,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.13687406480312347,
      "learning_rate": 1.08e-06,
      "loss": 0.0018,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.3713664710521698,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0019,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.13755393028259277,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.002,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.3242969512939453,
      "learning_rate": 1.04e-06,
      "loss": 0.0016,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.5494619011878967,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.0018,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.1875469982624054,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0017,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.254686176776886,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0021,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 0.0970870852470398,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0017,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.19529850780963898,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.0016,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.25420066714286804,
      "learning_rate": 9.6e-07,
      "loss": 0.0022,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.11336521059274673,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.002,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.2115754336118698,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0018,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.18515098094940186,
      "learning_rate": 9.2e-07,
      "loss": 0.0018,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.27733033895492554,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0021,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.18981878459453583,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.002,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.2612048089504242,
      "learning_rate": 8.8e-07,
      "loss": 0.0017,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.4763430953025818,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0019,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.1196317970752716,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.0021,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.20130959153175354,
      "learning_rate": 8.4e-07,
      "loss": 0.0018,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.31175780296325684,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0021,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.3016093969345093,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0015,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.2201787680387497,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0021,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.13144585490226746,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.002,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.1056768074631691,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0018,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.43005943298339844,
      "learning_rate": 7.6e-07,
      "loss": 0.002,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.1909826397895813,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0018,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.5574795603752136,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0022,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.4423333704471588,
      "learning_rate": 7.2e-07,
      "loss": 0.002,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.10710588842630386,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0018,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.22432401776313782,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.0018,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.5051037073135376,
      "learning_rate": 6.8e-07,
      "loss": 0.0019,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.2029302418231964,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0019,
      "step": 37000
    }
  ],
  "logging_steps": 10,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}
